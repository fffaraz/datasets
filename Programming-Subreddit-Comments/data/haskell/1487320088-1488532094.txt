Note: for those struggling to understand what this even means, this intuition might be helpful: I realized after staring at it for a long time that `:**:` is roughly a type-level [`bimap`](https://hackage.haskell.org/package/bifunctors-3.2.0.1/docs/Data-Bifunctor.html#v:bimap) on tuples. The value-level (tuple) `bimap` is pretty simple, and it's defined like this: bimap f g (x, y) = (f x, g y) Similarly, the "separated conjunction" is more-or-less doing this: newtype TypeBimap f g '(x, y) = TypeBimap (f x, g y) Just replace the left `TypeBimap` with `:**:`, replace the right `TypeBimap` with `:&amp;&amp;:`, add some kind annotations to make the compiler happy, rewrite it all using generalized-algebraic-data-type notation, and voila, you have u/Iceland_jack's version. (Note that `'(x, y)` is a type-level tuple, as contrasted with a tuple type. A tuple type is a type. A type-level tuple is a pair of types, which is not itself a type, just like a pair of `Int`s is not itself an `Int`. It's some kind (wink wink) of GHC extension voodoo involving `DataKinds`. Also note that the tuple on the right hand side does *not* have a `'` symbol, so it's an actual tuple type, not a type-level tuple. This is probably the biggest way in which the analogy between `bimap` and `:**:` breaks down, since a "true" type-level `bimap` would both consume *and produce* a type-level tuple)
Can I suggest that you have a look at the CIS194 course? Or spend a few minutes googling and let us know what you've found and what doesn't seem clear about it? It took me a while to learn to dig for answers to these kinds of questions, but learning how to chase down answers (or at least leads or new keywords to search with) has paid off enormously. You'll probably also get more in depth answers from folks here if you've shown that you've had a search around first :) 
I tried to google it but could not find something informative about function type constructor. That because I posted my question here :-) 
No there isn't I just run it in parallel in a terminal :D But now there are some nice extensinos using `intero` which are pretty cool. Inline error reporting and things like "jump to definition". "Haskero" for instance is one of those extensions.
Couple things: 1) The `fun` outside and inside the `do` block are two separate things (the two identical names point to distinct memory objects), in fact GHC will complain about the inner one "shadowing" an external binding. The type of what's returned by `get` within `f` is established at its use site, the `Map.lookup` call 2) You need `execState` around the `do` blocks to "execute" them, i.e. fish out the value contained within: f = execState $ do ... 3) The State newtype transforms state and can return a value of a distinct type from the state; you retrieve both using `runState :: State s a -&gt; s -&gt; (a, s)`, as /u/kqr shows. The actual type `s` depends on what `get` and `put` pass around, whereas the result type `a` depends on what you pass to `return`
I don't know of any books, sorry. I read a handful of online tutorials on parsec when I tried to write my first parser in haskell. Unfortunately it was awhile ago, so I don't remember what they were, but this is another benefit to trying to write a parser in haskell as opposed to some other projects: at the time I did it there were a few different online resources to learn from. This stands in contrast to lots of other possible tasks where the resources available to learn from online are limited and you are often forced to just read code, which I think is tougher without more exposure to the language. Thus, my suggestion is to look up tutorials on parsec.
I think it's a great task to demonstrate the potential strengths of the language and to practice the (for most of us coming from other languages) paradigm shift involved in writing it.
&gt; The subject just confuses me. In that case, before going with "a book" (which is always a good idea, time permitting!), begin with https://two-wrongs.com/parser-combinators-parsing-for-haskell-beginners which I found super-approachable. A neat way to get hooked on parsing is starting not by actually parsing, but defining your "AST" or "language" as you'd normally, via recursive `data` constructors, and just using the `Text.Read` package for the actual "parsing", ie. you write out "the program" (just a value of your AST type) in a text file and `readMaybe` it. Not a solid long-term solution and before long you'll want "a nicer syntax, less parens" etc but by now you're actually "hooked" which makes learning a breeze. For example this way I "created a mini-language" in one HTML renderer (the "iterator") that comes my static site generator, letting you compose any number of primitive/trival SQL-like modifications to a data set before rendering: https://metaleap.github.io/haxtatic/xtypes/hax.iterator.html#Modifiers for some types of apps, this kind of stuff is actually "sufficiently powerful" for the user, doesn't require a PhD for them to write (or you, to "parse"), and is in its *core* (ignoring the typical refactorings/refinements/etc) a matter of an hour or 3, and super-smoothly extensible! Mostly you're dealing with eval'ing not parsing. So why do I mention it? Again, if you weren't hooked on parsing before, adopting such techniques for a while *will* (`read` does have its limits) ;)
Hearthstone example is a pretty good third sample to read after Haskell calculator example. If it used only reflex, and no JS ffi calls, it would be an ideal next step.
I have a list here: http://beerendlauwers.be/project-ideas.html
Thanks. I'll try that and, hopefully, have an eureka moment and everything will make sense. It's like the notion of pointers in c, once you get it, everything just makes sense and the memory management of data structures becomes a breeze. 
That looks really awesome, bit i'm always missing some sort of conversion function for those advanced representations. Without the possibility of converting, it gets difficult to write a parser... I haven't much time currently, but next week i'll like to play around with this stuff more, maybe i can come up with one ;)
It doesn't have to be a Map, but yes, that's how state is managed. The `fun` at top level is used as a starting state by `runState`.
If I have to combine two global states then I do that in the 'getter' ? Assume it is a produce of 'rows' and 'columns'. State machines ? Update : I meant simple games like board games. Is there some specific material to learn this type of state management ?
Agreed; also (and one day I will find and link to the origin of this (very good) advice, but for now I'll just paraphrase): solve a *real* problem you have, *completely*, using only the things you *already know* in Haskell. The emphases are because, respectively: you need to be motivated to get to the end; you have to see how all the real-world corner cases get dealt with in Haskell; and you will be better able to understand advanced features once you've experienced the problems they exist to fix. Your program will not be awesome at first, but you will have several "Aha!" moments, which will set you well on the way to awesomeness.
I've pushed a strongly scoped version of the untyped lambda calculus with [a scope checker](https://github.com/gallais/potpourri/blob/master/haskell/strongly-scoped/Lambda.hs#L38) for raw terms. Using applicative combinators, the definition is really clean.
*lots of type level stuff i don't fully understand* but that looks like what i wanted
It took me months to come up with the right sort of setup at the type level. It is pretty scary but it does the job. I should maybe add longer documentation recording the reasons why type families &amp; type class instances are setup that way. One day when I'll find the time, maybe.
If you can do it with `unsafePerformIO`, you can do it with your `safePerformIO`, because by definition they are the same function. Even if it's type safe, it's not purity-safe, and since GHC assumes every function is pure and can be cached this can lead to some obnoxiously wacky behaviour.
If you take [this implementation](https://gist.github.com/ppetr/3693348) of `coerce` in terms of `unsafePerformIO` and try to use `safePerformIO` instead, it won't compile. My question is whether that's true for all implementations.
This could be what you're looking for http://webcache.googleusercontent.com/search?q=cache:EZVbxYEOgiAJ:www.cse.chalmers.se/~atze/papers/keymonad.pdf+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=hr
[More people need to know about openTOC](http://www.sigplan.org/OpenTOC/haskell16.html)
FYI, a self-contained problem file (with all appropriate imports, language extensions, etc.) will yield more answers (because people can try out something they're not too sure about).
This is awesome!
Ideally, find something you think is worthwhile. A linter, formatter, or parser for some language could work. If you go the parser way, I'd say try to use a library that supports monad transformers, so when you get comfortable with it all, you can try that out (to log errors while parsing, or whatever). My first sizable project was a [CSS parser and minifier](https://github.com/contivero/hasmin) (cheeky self-promotion :p).
Edit: The help command for haddock within stack is: $ stack exec -- haddock --help Where is haddock ? $ stack exec -- which haddock /home/arch/.stack/programs/x86_64-linux/ghc-8.0.2/bin/haddock 
I develop Haskell on OSX and FreeBSD. No trouble at all.
Ah, then I can have another rule, indentented at the same level of stmt(IF:ts) to do each rule. Repeat for all the rules. Eg stmt (WHILE : ts) = do -- parse the statement after WHILE ts &lt;- stmt ts -- This seem right?
I should also mention that THEN : ts &lt;- expr ts is a shortcut that doesn't work with `Either`, I just did it because it's shorter. If the pattern doesn't match an exception will be thrown, rather than the result evaluating to a `Left` value. So to be correct you will need to use `case` explicitly.
You generally try to avoid state in the first place. Either by using a different algorithm/approach or passing parameteres around manually. I only tend to switch over to using a State monad when I have a fairly complicated state that I need to maintain across a wide variety of functions/modules. Note that the functions you provided can be simplified with `modify`: modify f :: (s -&gt; s) -&gt; State s () modify f = do s &lt;- get put (f s) which allows you to write f r c = modify (Map.insert r c)
Other sources of exceptions are indeed deterministic. `error`, `undefined`, pattern match failures.
Fair enough, I was curious if you were using this for stream processing in Haskell. I'll take a closer look later.
You should read [Monadic Parser Combinators](http://www.cs.nott.ac.uk/~pszgmh/pearl.pdf). It's a really great paper that serves as a decent introduction to monads and purely functional parsing. It's a pretty easy read.
It does violate referential transparency though. A function can error or not depending on the environment, not just its inputs. foo :: () foo = safePerformIO $ do now &lt;- getCurrentTime when (isEvenSecond now) $ throwIO ... return () Whether `foo` is `()` or an error depends on the time it is evaluated, which is bad for purity
I'm using them, as seen in the instance code block. The problem is that in order to use them I need the type in scope, and for that I need `InstanceSigs`. The error message is about the verification if the signature I gave is equivalent to the necessary one.
I'm gonna try and report back. Edit: That made the trick! I actually had to add a proxy for `f` too, but after that it went ok. Thanks!
"If it compiles, it chats."
 main = do let x = safePerformIO (print "Safe!") -- "Safe!" to be printed once, twice or zero times? return (eval x x) eval :: () -&gt; () -&gt; () eval () () = () 
This avoids the proxy for `f`, though mixing the styles may be somewhat ugly. :) domCartesian p = domCartesian @f p
What I meant to ask was, if I already know that IO inside transactions can be retried, is there anything else I should worry about? Can it break STM's internal invariants, not just ping the bank 739 times?
The canonical example is `unsafeLaunchMissiles`, by the way.
The [A reflection on types](https://www.seas.upenn.edu/~sweirich/papers/wadlerfest2016.pdf) paper and the related [talk](https://www.youtube.com/watch?v=asdABzBUoGM) by Stephanie Weirich touch on that.
Haskell `Float` and `Double` types correspond to the standard single and double precision [IEEE standards](https://en.wikipedia.org/wiki/IEEE_754-1985). That is where the weird `+0`, `-0`, `-Infinity`, `+Infinity` come from. In general, `x == y --&gt; f x == f y` should always be true, but really it just boils down to the particular `Eq` instance. In fact, modulo weird numeric types, I generally _do_ assume that `x == y --&gt; f x == f y`.
Yes, that's exactly what I was thinking of! It's a great talk.
&gt;NaN /= NaN Wow, that's even more irksome. &gt;Because IEEE floats fucking suck. So this will only happen with floating point weirdness (like `maximum [0,0.2 .. 1] == 1` returning `False`)? Other than weird float stuff, can I rely on `x == y --&gt; f x == f y` being valid? Thanks!
Do you know what is a type constructor? E.g., in the following examples, can you find the type constructor? Maybe Int Either String Int String `Either` Int String :~: Int String -&gt; Int
You're not preventing any kind of IO by restricting the return value to `()`. I mean, `launchMissles :: IO ()`, right? 
Thanks! I didn't know the difference between sync and async exceptions until now. With some hacking I was able to make your second example work like the first: safePerformIO :: IO () -&gt; () safePerformIO io = unsafePerformIO (catch io handler) where handler :: SomeException -&gt; IO () handler exn = do threadId &lt;- myThreadId throwTo threadId exn I'm not sure if that's a good idea, though.
&gt; if I already know that IO inside transactions can be retried, is there anything else I should worry about? Can I break STM's internal invariants? I'm not sure to what extent this can be done. I do know that `atomically` will throw an exception if it is called within another `atomically`. e.g. atomically (safePerformIO (atomically (return ())) `seq` return ()) Something like this will throw *** Exception: Control.Concurrent.STM.atomically was nested I'm pretty sure the design of `stm`, combined with forbidding nested `atomically` calls, protects you from doing any actual damage to the STM invariants.
[This talk](https://channel9.msdn.com/Series/C9-Lectures-Erik-Meijer-Functional-Programming-Fundamentals/C9-Lectures-Dr-Erik-Meijer-Functional-Programming-Fundamentals-Chapter-8-of-13) on parsers by Erik Mejier is good. Starts around minute 6. ([All the talks](https://channel9.msdn.com/Series/C9-Lectures-Erik-Meijer-Functional-Programming-Fundamentals) in the series are good.) There's also the [monadic parser combinators](http://unpetitaccident.com/pub/compeng/languages/Haskell/monparsing.pdf) paper. 
That's a nifty trick, i thought the ghc could figure that out by itself. EDIT: still has the same error on f :( Expected type: a1 -&gt; Term a1 Actual type: a -&gt; Term a
Ideally, we could write the instance: instance (Applicative f, Monoid a) =&gt; Monoid (f a) where mempty = pure mempty mappend = liftA2 mappend but alas, due to technical reasons we cannot/should not. I'd expect every concrete Applicative instance to provide this instance, though.
I like that ... perhaps that should be the motto :D
If someone implements a reddit backend why not ;)
Uh, it compiles fine here. I'm not sure what's up. I just added an annotation with `type` instead of `newtype`, looks a bit more lightweight.
Oh I see. GADTs implies [MonoLocalBinds](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#ghc-flag--XMonoLocalBinds) for some reason. The fix is to annotate the binding of `f` with a polymorphic type (`f :: forall a. TermF a`).
Yes, that's definitely a good idea. Also `{-# NOINLINE safePerformIO #-}` might be useful. Since you clearly understand this better than me, do you see any other gotchas?
ok now that already looks quite good and normalizes `k i` to `\a.\b.b` but it won't normalize `s k k` to `\x.x` because the case for Lam should be `Lam (nf' . f)` but that has more of these ugly type errors: http://lpaste.net/8068633881340805120
At last a safe way to launch missiles!
Even informally `Lam (nf' . f) :: forall a. TermF a` doesn't seem to typecheck. Given `x :: a`, `f x :: TermF a` with a rigid `a`, whereas `nf'` expects a polymorphic term: it would like `a` to be flexible. Instead, you can call `nf'` at line 82 just after substituting. &gt; is there a way to get around all those ugly hacks? or do i need to get a better type system for that? (something like idris/agda/coq) I guess you need a better type system. But I'm not familiar enough with type inference to know whether there may be something to be done about this within GHC, nor whether the existing dependently typed languages can handle this kind of thing more easily.
`nf $ App skk i` == `i` it works, nice thanks EDIT: PHOAS is quite difficult to work with in Haskell, i might go back to old style unsafe Expr and renaming... manually EDIT2: w = Lam $ \x -&gt; App (Var x) (Var x) `k i (ww)` == `i` now i need a typesystem to prevent ww from being used EDIT3: STLC was quite easy: http://lpaste.net/4212266008498405376 i might add some more sophisticated typesystems later and may implement PHOAS in my new language &gt;:D
so glad to see this! chat bots are, in my opinion, a great learning project because they cover a variety of real-world scenarios.
Good question. The type constructor is: Maybe Int But Either String Int could be too, but is not? 
It seems like the biggest problem happens with `bracket`, as described in [this thread](https://mail.haskell.org/pipermail/haskell-cafe/2014-February/112555.html).
As many people have said, it's due to IEEE's floats being weird. Not much Haskell can do about it since approximating the reals is kinda hard to say the least. Also you might notice other kinds of weirdnesses with IEEE Floats like `x == x` not being true for `NaN` or the transitivity of `&lt;` being broken by some values. I think a lot of the weirdness comes from the fact that floats really need 3-valued logic for comparison and orderings to make sense on them, but `(==)` and similar return Bools, so that doesn't mesh well together.
The goal of standard floating-point arithmetic is to provide sane defaults for standard programmers. I do know of projective geometry, I don't think that's what people usually _want_...
Why would NaN == NaN? It is the "numeric" representation of something which is not a number. If NaN was equal to itself that would kinda be saying everything which is not a number is the same.
One useful definition of equality is substitutability. Afaik, all NaNs are substitutable with each other within IEEE floating semantics, so it would make sense for them to be equal. There are other perspectives, but I'd submit that this one has largely won over time. See also SQL NULL, which is generally logically sensible on its own terms, but has truth tables we generally wouldn't pick today. It also sort of wreaks havoc on a lot of things when x != x, like hash tables or sets. (Corrections welcome on the substitution claim. I do know not all NaNs are bit-pattern identical which is why I qualify with "within IEEE.")
Actually, there is something even weirder with your reasoning that because you end up with an undefined form the starting equality can't be right, and that's because you multiply both sides by zero, so let's see what happens if I do that with another equality: 1 = 1 1 * 0 = 1 * 0 1 = 1 * (0 / 0) But 0 / 0 is undefined, so the statement 1 = 1 is a paradox.
Wow, thanks for your help! How would I go about doing that? stmt (IF : ts) = do -- parse the expression between IF (consumed above) and THEN (consumed below) THEN : ts &lt;- expr ts -- parse the statement between THEN and ELSE ELSE : ts &lt;- stmt ts -- parse the statement after ELSE stmt ts Also how do I 'return' at the end of a recurse. For example, the production INPUT ID, would this be stmt (INPUT : ts) = do stmt ts = Right ts -- or just Right ts? I need to grab the ID. Or in the case of productions like this: --factor -&gt; LPAR expr RPAR -- | ID -- | NUM -- | SUB NUM. factor:: [Tokens] -&gt; Either String [Tokens] factor (LPAR : ts) = do -- parse the expression between LPAR and RPAR RPAR : ts &lt;- expr ts Right ts -- I need an expression here, for the do operator factor (ID : ts) = do Right ts -- this doesnt make sense because there is nothing to the right, we're at the end? factor (t : ts) = case t of (ID) -&gt; Right ts -- this doesnt make sense because there is nothing to the right, we're at the end? (NUM) -&gt; Right ts -- this doesnt make sense because there is nothing to the right, we're at the end? otherwise -&gt; Left $ "Expecting ID or NUM, instead got" ++ show t factor (SUB : ts) = do (NUM) : Right ts Or empty rules. --exprprime -&gt; addop exprprime | empty exprprime:: [Tokens] -&gt; Either String [Tokens] exprprime ts = case eth_str_tok of Right toks -&gt; exprprime toks Left errormsg -&gt; Left errormsg where eth_str_tok = addop ts exprprime ts = Right ts --is this ambiguous? How do I handle empty when the rule starts with a terminal 
Oh, here it is: http://code.haskell.org/~Saizan/ST/ST.agda. The only thing he needed to postulate was a free theorem, which we can't prove in Agda.
Yes; that's similar to what I meant by the "truth tables" in SQL. What I'm only mostly sure of (rather than entirely sure of) is that there is no _other_ way to distinguish two NaNs within the IEEE rules. (As I said, I _know_ there are if you leave the IEEE domain and start looking at bits... there's a lot of different bits that spell NaN and a lot of JIT compilers will stuff things in them.) This being the internet, I figured I'd hedge and hope the inevitable correction would be that much gentler. 
 GHCi&gt; -1/(-0) &gt; 1000000/(-0) True GHCi&gt; (1/0) Infinity GHCi&gt; 0*(1/0) NaN GHCi&gt; (-0)*(1/0) NaN GHCi&gt; number-of-weirdnesses-possible Infinity 
Not a dog != Not a dog. This is sensible.
Dividing by zero in ordinary arithmetic is an undefined operation. So it might be a little bit surprising that IEEE 754 defines the result to be positive or negative infinity, rather than NaN or throwing an exception. However, think of it from the perspective of limits. That is, you are performing division with an ever shrinking (approaching zero) denominator. As the denominator approaches zero, from either side, your result approaches positive or negative infinity. Eventually the quantized nature of floating point numbers means a shrinking divisor becomes zero; it can't shrink forever like a real number would. The designers of IEEE 754 thought it was more useful if the result became positive or negative infinity, rather than NaN. Their belief was that seeing a positive or negative infinity result would help the programmer debug their code better than a NaN. That is, if you see a positive infinity and you have a divide in your code, you should suspect that either you overflowed from max_float or you divided by positive zero. Similar story if you have a negative infinity. Anyway, here the reasoning is as a mathy picture: http://imgur.com/a/0UUfV See page 10 of [Professor Kahan's lecture notes](https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF) for more details. Page 11 has a nice motivating example. Another example of a reason for having a negative infinity is log(0).
I think your view is rather lacking in nuance. There are reasons for the various features in IEEE 754 floating point standard, including positive and negative infinities, NaNs not being reflexive, etc. Some compromise is needed when attempting to represent the real number line with a finite number of bits.
Well, the problem is that `Ord` instances are used to order things, and if the instance isn't law-abiding, things like `sort` and `Map` don't work properly. And I'd rather not have to define a newtype to fix IEEE's mistakes.
I wrote only the parser. Before posting a HTML example I guess I need to include a writer for `DocTable` elements.
Persistent recently needed this to give examples of how a query would modify different SQL tables (show one table, then the query, then the result table). Don't know anything about the implementation but sounds like a good feature to have.
I use Hipchat at work but outside a work environment I really don't get these new web based IRC replacements.
Maybe they're part of a religion that believes in sacrificing RAM to their gods?
 stmt (IF : ts) = do THEN : ts &lt;- expr ts ELSE : ts &lt;- stmt ts stmt ts desugars to stmt (IF : ts) = do ts1 &lt;- expr ts case ts1 of THEN : ts2 -&gt; do ts3 &lt;- stmt ts2 case ts3 of ELSE : ts4 -&gt; stmt ts4 _ -&gt; fail "pattern match failure" _ -&gt; fail "pattern match failure" What I mean is to replace `fail` with `Left`. --- &gt; stmt (INPUT : ts) = do &gt; stmt ts = Right ts -- or just Right ts? I need to grab the ID. The rule is `stmt = INPUT ID`, with nothing between `INPUT` and `ID`, so you can pattern match both tokens at once. The pattern `INPUT : ID _ : ts` matches non-empty lists (`:`) whose head is `INPUT`, and whose tail is also non-empty (`:`), the head of the tail is `ID _`, for any contained value, and the tail of the tail is `ts`. stmt (INPUT : ID _ : ts) = Right ts If you want to also return the ID string, or more generally a representation of the statement being parsed, you will need to change `stmt` to also return a value, and to modify the definition to construct such a value (typically an AST). stmt :: [Token] -&gt; Either String ([Token], Stmt) -- for some definition of Stmt data Stmt = ... Note that handling the token stream explicitly like this quickly becomes quite unwieldly, and things like monadic parser combinators that ElvishJerrico mentions make parsing much more manageable. --- factor (LPAR : ts) = do -- parse the expression between LPAR and RPAR RPAR : ts &lt;- expr ts Right ts -- I need an expression here, for the do operator factor (ID : ts) = do Right ts -- this doesnt make sense because there is nothing to the right, we're at the end? `do` is just syntactic sugar. factor (LPAR : ts) = case expr ts of Left e -&gt; Left e Right (RPAR : ts) -&gt; Right ts -- this is why the last statement must be an expression Right _ -&gt; fail "pattern match failure" factor (ID : ts) = Right ts -- a do with a single statement (= expression) is just the expression. In the *grammar rule* there isn't anything to the right of `ID`, a factor can be just an `ID`. But in the *token stream* there is something to the right. `factor` thus consumes the `ID` token corresponding to a factor, and returns the rest of the stream, `ts`. --- exprprime ts = Right ts --is this ambiguous? How do I handle empty when the rule starts with a terminal If there is an empty rule there is nothing to match and you can just return the token stream unmodified. Ambiguity is a property of the grammar, not of a single rule, so don't understand what you are asking.
Ooh, that makes sense. Thanks!
Not much to be honest! I'd like to add an option (configuration) to trim the content of a post in `index.html`, an option to limit the amount of posts on `index.html` and have multiple pages for that, and also a configuration for input path/output path so directories won't be hard coded. If you have other suggestions, feel free to suggest them and we'll talk about it. I don't know how much I'll work/maintain this because i just wrote it for something I needed, so if you find me not as cooperative as you'd like feel free to fork it and change/add the features you want :)
This is a really good question. I have this problem too and I thought of this exact solution but I haven't used it : my gut feeling is it based to rely on "external" solutions (like CPP). But it's just my instinct. I'm also interested in buying what other people think. At the moment, I just use a bigger prelude and do qualified import manually. Using a "bigger prelude" reduces the import section enough.
Could someone give me please an example of function type constructor?
&gt; then I realized that this way I won't be able to distinguish my imports based on module qualification, e.g. Set.fromList vs Map.fromList to name a common one. Are there many collisions? Perhaps you could simply define `setFromList` and `mapFromList` in the imports file. Another common source of collisions are functions like "length" or "null" for monomorphic containers like lazy/strict text and lazy/strict bytestring. I sometimes use [monoid-subclasses](http://hackage.haskell.org/package/monoid-subclasses-0.4.3.1) to get a common interface (`Null`, `Factorial` and `TextualMonoid` are particularly helpful).
As a counterexample consider the type `Maybe Int`. You now have `Nothing` to represent errors, but this way no algebraic laws are broken. You still have `x == x` and `x == y -&gt; f x == f y`. This extends the `Int` type with an error value in a much more natural way than the IEEE semantics. And you wouldn't need [any tricks](http://stackoverflow.com/questions/10710328/comparing-numpy-arrays-containing-nan) writing test-code with arrays containing error values. EDIT: Well, some algebraic laws would break, for example `0 * err = err * 0 = err`. But that's also the case with NaN and at least you could recover reflexivity and structural equality.
Yes, you can do that, but you have to use `unsafeCoerce`. Storing the values themselves is not difficult, any dictionary data structure like `Data.Map` will do. The real problem is storing and retrieving the right types. If you want to do it purely functionally, and you want to do it in full generality, then you have to resort to `unsafeCoerce`. My [`vault`][1] packages implements something like that. [1]: http://hackage.haskell.org/package/vault
There's https://gitter.im/dataHaskell/Lobby at least (gitter at least lets you connect from irc, so it's possible to use without getting Yet Another Client). EDIT: also https://www.reddit.com/r/haskell/comments/5gt8j0/gitter_haskell_community_chat/
There is a [stackoverflow answer](http://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values/1573715#1573715) by a member of the IEEE committee. According to his answer there were two main reasons for this design: * To reuse `x - y == 0` on the hardware level for `x == y` comparison and * so that languages wouldn't have to wait for the adoption of `isnan()` and instead allow testing for NaNs by the inequality `isnan(x) = x =/= x`.
I never understood why haskell lacks a decent way of dealing with overloaded symbols. Why can't I just import modules and all their symbols and have ghc only require me to qualify if there's a conflict? Why can't it resolve potential conflicts based on type inference, at least in trivial cases?
I've previously done my own bit of a research project like this called [Fraxl](https://github.com/ElvishJerricco/fraxl), which is on Hackage. It's purpose was geared more towards exploring the possibilities and making it more composable, so it's not *quite* as simple of an interface for users. But stuff like caching and supporting multiple data sources are just substitution layers in Fraxl, so you can strip it down to being quite simple if you ignore such things. The current status of the project is that it won't compile on GHC 8. The `CoRec` type that I used comes from a package which doesn't work on GHC 8, and the author hasn't updated it. So I'm currently in the midst of a bit of a rewrite to both simplify things and improve performance. As it stands, Fraxl is about 4 times slower than Haxl. Getting it that close was a difficult task with the generality I'm going for (it comes down to a particularly fast free applicative). But I've found that I can actually rival Haxl's performance if I use the church encoded free monad, and concede that the free monad transformer is too slow. I also found it important to execute commands in an arbitrary monad. This is why I made it a transformer initially. But you can still execute commands in your choice of monad with a non-transformer during the interpreter phase.
I like the simplicity of this library compared to Haxl. So yeah, it would be useful to have it in Hackage. 
Genius comment. End of discussion incoming.
What you are really suggesting is not support for partial functions, but automatic deriving of complete functions from unmatched cases. Sounds bad.
I have used approach of 3. in several largish projects now; I also have [a package for it on github](https://github.com/lspitzner/qualified-prelude). Some pros/cons are in the README. In general I would cautiously recommend this general approach, if with that package or a copy that you tune per-project (e.g. in brittany i use a [project-specific CPP prelude](https://github.com/lspitzner/brittany/blob/master/srcinc/prelude.inc)). There is tooling around that does not work well with CPP so this can be a blocker. Apart from that I see no disadvantages. Most importantly, compilations times are not at all affected by using CPP in general. (I never put qualified-prelude on hackage because previously i had used some evil CPP stuff that simply was not compatible at all with the PVP. But I recently removed these evil parts and added reexporting instead, so I think this could go on hackage now..) I don't like classy approaches because error messages are sufficiently hard already without making stuff more ambiguous.
I agree with your general sentiment. In fact, I was mostly trying to make the point that there are a lot of complicated issues and you are going to give up something. IEEE 754 largely gives up things numerical analysts are comfortable giving up in the context of a low level standard to support portable numerical code and that is intended for fast hardware implementation. And it includes some things that numerical analysts want to use in specialized situations, but that might seem odd to others, e.g. signed zero. It also has some historical quirks that make less sense if your language fully supports IEEE, but were hard to avoid given the poor IEEE support in most languages for many years, e.g. the fact that x =/= x has served as the only portable test for NaN in many languages. Reflexivity, on the other hand, is usually not critical to writing portable numerical code. I'm not arguing that giving up reflexivity is good, but it isn't something numerical code usually depends on, and the decision does have some justification, especially considering the historical context. My point on sqrt(-2) == sqrt(-1) was in the context of defining an equality based on considering distinct NaNs nonequal. I was responding to a post that had some discussion of whether all NaNs are equivalent in IEEE 754. They are not equivalent, but I doubt you really want to use what is expected to be a numerical equality operator to distinguish different types of NaN. I think the partial order aspect is a large part of the problem: IEEE defines something closer to a partial order (however, not quite, since &lt;= is still not reflexive for NaN), but comparisons have boolean results. It's not a nice fit. As a slightly higher level interface to IEEE 754, I actually like the idea of making lack of ordering explicit with comparisons of type `Double -&gt; Double -&gt; Maybe Bool`. But I would still prefer that `NaN == NaN` evaluate to `Nothing` instead of `Just True`, with separate operations for comparing diagnostic payloads if that is needed. 
University of Oregon's is also open to applications now! 
Agreed. I've joined several discords, but usually when there is no alternative like irc. At my work we started using mattermost. So me and a few coworkers made a terminal mode client for mattermost. You can find it here: https://github.com/matterhorn-chat/matterhorn And yes, it's pure Haskell.
I hope this will prove to be a tooling issue, as I expect we will see a piece of tooling that can add/modify `import` lines based on the code we write. It needs quite a bit of work to have this in Haskell land, but it sure exists for other languages. I've worked with Java IDEs that had this feature implemented quite well.
You mean the [Oregon Programming Language Summer School (OPLSS)](https://www.cs.uoregon.edu/research/summerschool/summer17/). The deadline for registration is April 1, 2017. Note that the two summer schools cover completely different content: OPLSS is much more on the theory of programming languages (with a focus on reasoning about programs, including type systems), this one is about language implementation with performance in mind. I would expect that there is no overlap in content between the two.
[removed]
I agree will all your criticisms of IRC and still think that Hipchat/Gitter/Slack/Discord only solve some of the problems of IRC, and in some ways these new ones are worse than IRC. Here in Australia, on my really shitty (and expensive) ADSL connection, IRC with its distributed set of servers is far more reliable (ie stays connected) than Hipchat or Giitter (the two I've tired). Yes, I'd love to have better scrollback in IRC. I also find having to use a web browser as a client instead of a dedicated client a real pain in the neck as well as being a huge resource hog. I use the standalone Hipchat client at work (I know its chrome underneath) and I've connected to Gitter with my usual IRC client (which was appallingly un-reliable, sometimes being unreachable for days other times dropping out every 5-10 minutes). I definitely do not want to have to run a separate client for each of the networks I'm connected to. 
This is basically what we already know how to do in OCaml, but it requires partiality, which we didn't find satisfying.
FWIW, Discord's model in particular solves a lot of these issues (it's the only thing I recommend to friends). It's closer to the IRC model where one client has views into many actual "servers" (rather than 'rooms' or 'companies' or 'hangouts'), which can be created on a whim and are geographically located. Individual servers do not federate or distribute (though you could likely build sort-of federation capabilities between servers using the API). And they at least have a Sydney endpoint so you create servers in Australia... It's also far less of a resource hog. Slack is using 1GB of working set on startup on this machine. Discord uses ~100mb despite also being a Chrome-powered application and me using it on 5 or 6 fairly active servers. I believe this is a selling point for them and part of their design, due to them marketing it for video game players (i.e. they want low overhead). Of course Discord cannot solve the one problem IRC has always solved, which is that it's open. I desperately want something open and federated but with the quality/design of something like Discord. It's far better than any alternative in its space, but the lack of openness is a killer for some uses.
Because the extensibility and ease of deployment/use of basically everything that came after IRC was utter trash. If all I want is a bunch of chatrooms for a technicality adept, small scale user base, IRC is far and above the easiest way to do that. The problem with the rest of the offerings is that nobody ever reimplemented the same core featureset on new software - Everyone thought they had a brand new better user experience that was worth getting rid of x, where x was usually either the concept of chat rooms, the ease of deployment, or the ability to host your own server on hardware of your choice.
Works for me. SHA256: 58:9B:60:BE:3A:B0:96:F5:A6:93:BA:5E:A8:34:CF:9D:CD:74:B2:3E:68:6C:85:53:FA:92:75:47:95:48:E4:57 Check out fingerprint of that revoked certificate in https properties - should be somewhere in address bar under lock icon. It's either stuck in cache of CDN, which should fix itself in a couple of days, or someone's fucked up while spoofing you - that's where you should question your ISP. Try to access it from proxy\vpn. If it works - something wrong with your pc\connection.
What would the type of `f` be, or `applyTwice`? `(f f) x` means to apply `f` as the first argument to `f`, and `x` as the second argument to `f`. It's not possible in Haskell for a function to have a type that allows itself to be one of its arguments.
To expand on this, values/functions passed into a function become monomorphic, in the sense that even if they look polymorphic they can only be used in exactly one way within the function. So you might think: selfApply f = f f should typecheck since it works with `id`: id :: a -&gt; a id id :: a -&gt; a But the issue is, is that each of the two `id` functions are instantiated to different types: (id :: (a -&gt; a) -&gt; a -&gt; a) (id :: a -&gt; a) To make the above type of thing work, you have to allow for inputs to functions to be used polymorphically, such as with `RankNTypes`: selfApply :: (forall a. a -&gt; a) -&gt; a -&gt; a selfApply f = f f selfApply id :: a -&gt; a 
I mean I would say that `transitivity`, `symmetry` and `identity` are all laws for the `Eq` class, any types that break it I would consider buggy. Hence why I hate `Double` and `Float`, but I mean they break basically all laws, certain ones out of necessity (associativity of addition), certain ones out of being sucky (reflexitivity).
There is no perfect way, but I still thing `x /= x` is WAY more surprising than `error1 == error2`.
 Prelude&gt; let z :: Rational; z = 0 in 1/z == 2/z *** Exception: Ratio has zero denominator 
For lists, there are advantages to the current `Monoid` instance (List is kind of a special case, being the free Monoid). For monomorphic types the Monoid instance is also fine. For `Data.Map`, a recursive Monoid would have been a huge improvement and would remove the need for much of its clunky API. For `Maybe` the recursive `Monoid` instance makes more sense than the data-losing `Alternative` instance. It's nice to have both, though. There's no advantage to the 2 instances agreeing and a huge loss of usefulness. 
&gt; the batching, which can be done very well within IO, without yet another monad. by accumulating jobs in a list/structure until (condition) holds? Or do you mean there's some kinda-sorta "built-in helper" useful for batching needs? &gt; Do you think that your monad simplifies anything? There is a radical idea propagated by deplorable traitors that posit that programming is an activity intended to solve problems, not to create them. Hehe. Hey it's an open-source offering, take it or leave it right? I half sympathize with your sentiment as "I've been there", still am somewhat. However do take note: whether it was the case in OP's scenario or not (don't have the time to judge), generally speaking as you keep purely-functionally-programming trivial patterns emerge that you then later find out have a some academic moniker and a lot of verbiage to them. Can't fault others for having found some common terminology right? Not fair either. Anyway, especially with Monoid/Functor/Applicative these are necessarily emerging patterns in FP and it's the way people in that community share concepts and logic. These are every much as convenient and fatefully-inevitable as `&amp;&amp;` or lambdas! In practical terms, you get reusable predefined operators for handier briefer notations so you don't end up with multiple dozens of custom operators to memorize (unless you import some lens package with 100s of them ;) 
I can sort of maybe see that argument. But really I personally think my explanation that nan is basically like Nothing in that it is an error, and comparing two functions that both give a (catchable / non-bottom) error equal is IMO just fine. Likewise if nan did have extra payload information it would be like Either String and thus two different errors would not compare equal. 
Same on the nexus 6, cannot zoom out further
I am confuse about right and left association! Lambda calculus is left associative(application is left associative) and type class constraints is the right associative?
Thanks, this looks really nice!
OK. A Double is conceptually similar to something like: data D = NotANaN Double | NaN Sign Payload and as you say for Either, Eq instances for other sum types compare both options. That makes sense on some level, but I still don't think it fits in well in the context of writing portable and efficient numerical code, which is what IEEE 754 was all about. If you include error comparison in your equality, suddenly an operation that in the vast majority of cases is intended to be used for numerical equality in code that does minimal error handling becomes dependent on how library authors chose to handle errors. Those sorts of errors would have been hard to standardize and it all would have been very bad for portability. The alternative of declaring all NaNs to be equal regardless of payload seems fairly limiting and you lose the one universally portable way to check for NaN on platforms with poor IEEE support. It all seems simpler to me, given that the expectation in numerical code that equality is numerical equality, to just decide that == is numerical equality and errors shouldn't be compared for numerical equality. As a higher level interface, if your comparisons make this explicit by having type `Double -&gt; Double -&gt; Maybe Bool`, this even forces you to handle errors cases separately, which is usually considered a good thing in Haskell. At a lower level, the IEEE approach seems like a reasonable approximation to this in that == is intended to be used for numerical equality and x =/= x provides a quick and highly portable check for the presence of NaN. In any event, I don't think there is one perfect answer here. Any choice will have trade-offs. Having higher level interfaces to IEEE and providing different comparison operators with different properties (including reflexivity and returning Maybe Bool) seems entirely reasonable to me. 
Application is left-associative in HS, but `-&gt;`, the function type thing, is right-associative. That means that `f :: a -&gt; a -&gt; b :: a -&gt; (a -&gt; b)` but `f x y = (f x) y`. But that's actually irrelevant. `applyTwice f x = (f f) x`, so if `f :: t -&gt; u`, you would have `t ~ t -&gt; v` (for some `v`; `a ~ b` is type equality) so that you can take `f f`. But then `t ~ (t -&gt; v0) -&gt; v1`, so `t ~ (... -&gt; vn)-&gt;vn1` would be infinite, which Haskell doesn't allow. As someone else noted, you could fix this by making `f` polymorphic, like `id`, so that `f :: forall a. a -&gt; a`, which would be valid (though it would require an extension).
Hi Andy, I would be interested in your paper that compares Haxl with the Remote Monad. One interesting difference between the Remote Monad and my Batcher Monad is that I don't distinguish between synchronous and asynchronous commands. The Remote Monad has synchronous Commands and asynchronous Procedures. With the Batcher Monad this distinction isn't made in the API of the library. However, the user is free to perform certain commands synchronously and others asynchronously by providing a custom [Worker](https://github.com/basvandijk/monad-batcher/blob/master/src/Control/Monad/Batcher.hs#L111).
If you were only running unit tests and all your repos were public and part of hackage, would you consider leaning exclusively on uploading to hackage and watching nixpkgs public jobset output/status? Do you see any drawbacks to this approach? It feels like a nice minimalist alternative to relying on Travis or Stackage. 
I don't think that's a good idea for a few reasons, there are two huge issues: What happens if you make a mistake: you've just released a broken version to Hackage. Our Hydra server has just completed it's 80000th build. Although lots of these are our own packages, pushing thousands of releases to Hackage isn't responsible. The latency is huge, nixpkgs tracks Hackage with up to a week's delay! Basically, Hackage and nixpkgs are for releases, and not for development.
&gt; Monadic parsers are a more verbose, typed and compositional alternative to regular expressions. A monadic parser can almost surely handle some non-regular languages as well, so I think characterizing it as just a replacement for regular expressions sells them a little bit short.
&gt;by accumulating jobs in a list/structure until (condition) holds? Or do you mean there's some kinda-sorta "built-in helper" useful for batching needs? I can imagine many ways to do it. Nothing that would make an average java programmer tremble.
So from this you deduce that the main functionality is batching, even if it does not mention it in the description.....
From that I deduce it's *a* core functionality, and honestly the only one that makes it interesting to me.
Thanks, fixed.
Which ones could be easily parallelized using ApplicativeDo?
Ok I was totally wrong about that: it was the maxing of the two variables in your example that needs to be independent (which with dice it typically is, and in the real world, typically is not). The monadic structure is fine, yes. I will downvote my original comment in shame. EDIT: Thinking about this some more kinda bothers me. Isn't the whole point of stuff like `Applicative` to enable writing code like `max &lt;$&gt; p1 &lt;*&gt; p2`? And yet that's clearly "wrong" here (it's not wrong in the sense that it does give you *a* probability distribution, but wrong in the sense that it gives you the *wrong* distribution). That thought doesn't make me comfortable with how freely I assume that patterns like the above work just because "oh hey, I have an Applicative here!"
Yea it was inaccurate to say "most." Those two can be monoids if the API is somewhat restructured, and parsers (when parsing context free grammars, which are most common) are doable with Applicative.
In the last example you meant SVG instead of JavaScript, right?
Hi metafunctor, I'm always interested in making things simpler. Could you give an example of what your imagining? 
The `blank-canvas` library generates JavaScript that manipulates a `&lt;canvas&gt;` element. It allows you to use a very very limited set of JavaScript functions.
I've tried changing up the definitions many times but I just can't seem to get it to work.I'm not sure what you mean by correcting the definition of treeFold
thank you this helped so much. One last question I have though is on the possibility of doing a left or a right fold on the tree, I have been looking on the internet and everywhere I've looked seems to indicate that there's only one possible way to fold over a tree as opposed to how lists can be folded through foldl and foldr
I don't think left folds generalize well to other data structures. You can't really "invert" a binary tree. You certainly can define the same fold as above in a tail-recursive fashion, but it doesn't make sense to do that in Haskell or any other language without artificial constraints on stack depth, as it just trades stack space for heap space. If you're interested in how to systematically define folds (and other interesting higher-order functions) on various data structures, the key phrase to look up is "recursion schemes".
Plus the parser mirrors the type you parse into instead of the format that is parsed. Always wanted to get into earley, thanks for the reminder!
Well if you were to go about doing that same function fold that you just showed, but instead folded left and folded right, how would that go about? 
Thank you all! Very good explanations! :D
&gt; I don't think left folds generalize well to other data structures. The concept of "catamorphism" generalizes right folds. Catamorphisms destroy structures "upwards from the leaves". I believe left folds are generalized by functions that process a structure "from the root downwards". These functions can be constructed by making a regular catamorphism return a function that builds the final result, in a similar way to how [foldl can be defined in terms of foldr](https://wiki.haskell.org/Foldl_as_foldr). For example, this function decorates each node in a tree with its path towards the root: inherit :: Tree a -&gt; Tree (NonEmpty a) inherit tree = foldTree algebra tree [] where algebra :: a -&gt; [[a] -&gt; Tree (NonEmpty a)] -&gt; [a] -&gt; Tree (NonEmpty a) algebra a fs as = Node (a:|as) (fs &lt;*&gt; [a:as]) I'm using the [foldTree](http://hackage.haskell.org/package/containers-0.5.10.1/docs/Data-Tree.html#v:foldTree) catamorphism. Notice that the return type of the algebra is the function `[a] -&gt; Tree (NonEmpty a)`.
Also`Canvas` 
I don't think I understand your question. Could you perhaps show the implementation?
I'm not sure I see how this generalizes to fixed points of arbitrary strictly positive functors. Would you care to elaborate or throw a link to a paper my way? I'm quite interested in this topic, but I don't recall seeing it treated generally.
The foldl-as-foldr thing is mentionend in sections 5 &amp; 5.1 of [A tutorial on the universality and expressiveness of fold](http://www.cs.nott.ac.uk/~pszgmh/fold.pdf). There's also [this paper](http://www.staff.science.uu.nl/~jeuri101/homepage/Publications/ags.pdf) about translating attribute grammars (with both inherited and synthesized attributes) to catamorphisms.
The statement isn't infinite. The issue is that `applyTwice`'s type would be infinite. I mean, what type should it have? Write it out.
Well nothing *has* to be a monad. You can always just stop at `Applicative` and choose not to define a `Monad` instance =P But if we say that something *must* be a monad if it *can* be a monad, then all the most common monads definitely must be monads. `State`, `IO`, and `Reader` all *have* to be monads. The more interesting question is which things *can't* be monads, which I think is a well covered topic.
[removed]
Yes. For your example, use the IO monad: main = do putStrLn "Welcome to the example of IO, the secret weapon of Haskell" putStrLn "Please type your name " putStrLn "and age." name &lt;- getLine age &lt;- getLine putStrLn $ "Hello " ++ name putStrLn $ age ++ " is not that old!" It is not so complicated. One may feel miserable by doing something that everyone understand at first sight and do not expand our immense ego. That humility is a sin for postmodern metrosexuals. but with practice one can accept it as normal. Even if it makes you feel uncomfortable at the beginning. But wait... looking at this snippet, a newbie haskeller could have the impression that Haskell is not so complicated after all and this is danger that every expert haskeller must avoid to preserve his self esteem. This problem can be solved by keeping this IO code secret for use only in production. If this fails, play the political card; Pretend that you are anti-racist even if you are white and live in Japan. For problems where there are many queries : &gt; Our program was sometimes reading or writing multiple registers after each other. It's more efficient to group multiple reads or writes into a single batch. Simply group updates in a List. In a state monad for example. Don't use GADT's, put directly the methods in the state list. Why the hell you need an intermediate representation? When you need a query result, execute all the pending updates and the query. For single threaded cases like yours, instead of a state monad, an IORef that store the lists is enough
Thank you. I'm familiar with Hutton's paper, it's specifically about folds on lists; the other one looks more promising, but it looks like it's gonna be quite chewy.
And again, I'm telling you to come up with a type for it. What type will `f` be? What type will `x` be? What is the return type? I guess another good question is what a "type" means to you. What does `f :: a -&gt; b` mean? If `f x` is ok and `x :: a`, what can you say about `f`? Is it possible for `f x` and `f y` to be valid, but for `x` and `y` to have different types? When?
Instead of using fold, can you use foldl and foldr?
Excellent write-up!
Books will not get you there. Need to write code. After two books you should have more than enough :)
[removed]
In the current implementation of Typeable - yes, read the paper below and you see the new version does not need it.
You can't meaningfully ask for the maximum of the probability distributions of a draw and a dependent, second draw, because that's effectively a single distribution of an ordered pair. Perhaps the nasty trap is thinking there's a monadic equivalent of `max &lt;$&gt; p &lt;*&gt; p`, when there really isn't. The distribution of possible values in the second drawing, after some arbitrary piece of paper has been removed, is equal to the distribution of possible values in the first drawing. &gt; let p = uniform [1,2] &gt; p &gt;&gt;= (\a -&gt; uniform $ map (a,) $ filter (/= a) [1,2]) fromFreqs [((1,2),0.5),((2,1),0.5)] But that's a single distribution of pairs of ints, not a pair of distributions of single ints.
You know enough to start building applications, programs, and libraries. Write an interpreter for a basic programming language. A basic procedural language is a great start. If you can parse and evaluate the following bit of code, you'll feel awesome. x := 1; y := 2; print (x + y); while (x &lt;= y) { x := x + 1; print(x); } Build a blog. Nothing crazy, just Posts + Comments. Use Yesod or Snap or whatever. Allow users to login/signup. Figure out how to allow users to have privacy settings. Allow users to follow each other. Write a simple job control system. Have an Input thread, a Controller thread, and some toy jobs. Have the Controller thread scale up the worker threads if it's not processing them at the predetermined rate. Let the controller have some retry/failure capacity. Write a scraper/bot that logs into your OKCupid account and hides people that have a &lt;70% match with you. They weren't compatible anyway.
I'm using this now and it's been great, thanks! FYI I have a fork where I'm layering my own awful hacks, but one adds support for dynamic sorting and filtering which you might find useful.
I'd think it was declining numerically rather than literally but I guess the number might be inscribed somewhere. Prodding aside, you're right although the current options suck.
Slightly tangential, but I think there's some pedagogical advantage in teaching the monadic interface basically as a means to `do` notation, at least at first. I remember Erik Meijer approaching it this way when I did FP101x a while back (beginning with parsers -- the monadic power is meaningful here). (Related aside: I'd love it if there were a more comprehensive and baked-in hierarchy of collection or ringad typeclasses, in large part because of the potential for nice associated comprehension/query/grouping notation.)
I agree it's less confusing.
Some people really dislike "big" IDEs. Others think they really help. Either way, wanted to show how easy it is to get everything up and running using the intellij-haskell plugin. 
Thank you for asking this! A parse error bugged me today for an hour or so. I fixed it without knowing how, just by removing some parentheses. Now I understand, you don't have to use parens for pattern matching inside case expressions! 
dude just won't let up with the discord chat, spamming it everywhere
&gt; Does this same trap occur whenever your two probability distributions aren't independent? This is the difference between maxOf2Dice = do x &lt;- die y &lt;- die return (max x y) and notMaxOfTwoDice = do x &lt;- die let y = x return (max x y) -- here we only rolled 1 die and looked at its value twice You are correct that every call to `maxOfTwoDice` is independent, and if you wanted a dependent version you would have to enhance it somehow (with arguments specifying the probability distribution, or some form of state like in your version of drawing without replacement) A useful function for these is `uniformPick :: [a] -&gt; Prob (a, [a])` which selects one element from the list and returns it along with the remainder of unselected elements. You can construct many dependent distributions using this function repeatedly or recursively.
Sure, but these formalistic probability scenarios like the marbles in the jar aren't really what worry me: what worries me is `isTall :: Prob Bool` and `speaksDutch :: Prob Bool` and doing `(&amp;&amp;) &lt;$&gt; isTall &lt;*&gt; speaksDutch :: Prob Bool`. These two things are [not independent](http://www.telegraph.co.uk/travel/maps-and-graphics/the-tallest-and-shortest-countries-in-the-world/), so this will yield a very wrong answer, no? That's what concerns me.
Also if someone knows a way to do 'rot13stor' without an intermediate 'ByteString' (i.e. as quick as a plain old rot13) that would be fantastic. The current implementation also munges tags, but I think that's intentional.
You say no parentheses and I raise you `\case { (Just x) : xs -&gt; expr }`
&gt; Well I'd contend that a constant distribution of one value is still a distribution It is, but finding the distribution of maximums over two distributions requires, well, two distributions :-) &gt;  just put a third item in the jar. That, alas, doesn't fix the problem, no matter how many items you have in the jar if you do a dependent second drawing you wind up with one distribution of probability of pairs of drawn values, but `max` has arity two. 
Ok if you want to be pedantic =P There's still no parentheses around the top level pattern though.
The Yesod book is quite specific, it really is first and foremost about Yesod. Which is great if you have already decided to build a web application with Yesod, but for a more general goal, I'd pick something else. The other two I haven't read, but they look thorough, and I've heard good things about them. On top of the books you've listed, and the other recommendations here, I'll add one more: [Write Yourself A Scheme In 48 Hours](https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours). The material contained in the book itself is probably a bit too simple for you at this point, but it provides countless kickstarter opportunities for self-driven learning - the Scheme you're implementing in the book is very simple, and many aspects of a proper Scheme are brushed over, but you can go into much more depth on your own. In terms of online resources, the Haskell Wiki on haskell.org has a lot of useful pointers to interesting reads. I do second the common notion here that the most important thing to do at this point is to get your hands dirty and write some code. As far as "essential" goes, this is pretty much it. I'd also recommend *reading* some code. For the latter, good candidates would be: - The **Prelude** (even better: implement Prelude functions yourself before looking at the real implementation, and then try to understand the difference) - **Pandoc**. Great reading, because the code is straightforward enough to follow along, well structured, and because Pandoc is basically a compiler, it covers all the important areas of program design and structure, however the compilation targets are textual, so it is easier to follow and requires less background knowledge than an actual compiler that targets some binary machine language. Also, Pandoc is relatively easy and very rewarding to play with yourself. Finally, I highly recommend hanging out on freenode (#haskell) - one of the most active programming IRC channels, and full of super helpful, friendly, and highly intelligent haskellers.
But `isTall` and `speaksDutch` are not probability distributions, they are functions of a particular object. The right way to think about this is do p &lt;- population return (isTall p &amp;&amp; speaksDutch p) Now, you can have `isTallPop = isTall &lt;$&gt; population` and `speaksDutchPop = speaksDutch &lt;$&gt; population` and you would be totally wrong to combine those two probability distributions unless you realize that you are talking about two *most likely different* random people.
Hi metafunctor, I think you're missing the point and that's probably my fault since I gave such a bad example. I don't have time yet to write a better example so let me refer you to [one of the Haxl examples](https://github.com/facebook/Haxl/blob/master/example/sql/readme.md) dealing with batching that will hopefully provide a good motivation for abstractions like the Haxl, Batcher or any of the other monads implementing the remote-monad design pattern. Let me know if you do or don't get the example. BTW I'm considering switching our application to haxl because I'm beginning to like its generality; haxl allows you to schedule commands of different types while my Batcher monad only allows scheduling commands of the same type. Though I'm not 100% convinced yet since I can imagine you can build this extra generality on top of my Batcher monad.
That's why I find myself using applicative parsers more often. Still good! 
&gt; and you would be totally wrong to combine those two probability distributions unless you realize that you are talking about two most likely different random people. Exactly! And this is what was bothering me in my above post: it's so intuitive to just write `f &lt;$&gt; a1 &lt;*&gt; a2`, but that intuition is almost certainly *wrong* with respect to stats (in any non-independent case). It's such a bad trap because yes, probability does form an applicative, and you can combine any two (even dependent!) distributions and get a valid distribution with respect to the Applicative laws, but that distribution is intuitively *not* the distribution that most (presumably) people think it is -- hence, my statement above that it gives "a distribution", but it's the "wrong" distribution. Also, `f &lt;$&gt; p1 &lt;*&gt; p2` just looks so pretty and elegant and haskelly. It can't possibly be wrong, right?
very good idea
Which paper? The Key monad one?
Can you give a real example of where you'd actually benefit from switching `&lt;|&gt;` to `&lt;&gt;` and get the same semantics?
Monadic parsers can parse recursively enumerable languages.
Reminds me of [reactive values](https://hackage.haskell.org/package/keera-hails-reactivevalues/docs/Data-ReactiveValue.html), but after taking a closer look I am not so sure if they are related. I'll leave this here anyway.
No, it's a terrible idea. Curly brace languages use curly braces to determine scope, not indentation. Violating all common style guidelines to pretend they don't doesn't change that.
If I wanted to make a function also work on bytestrings or unboxed vectors but still want concatenation semantics for all types. 
And now they can use indentation to determine scope instead? that's what I'm getting from this. It seems like a cool idea but I'm not sure because I don't know how it would hold up in use-cases. EDIT: Oh actually nevermind, I thought this does something else.
Does anyone know what the main differences between haskforce and intellij-haskell are?
Yeah. That's the thing isn't it. It doesn't actually do anything other than make tracing the actual scope difficult.
&gt; and have it executed efficiently. So we need an abstraction that automatically batches independent commands and executes each batch efficiently for example by running a single query or by running the queries in parallel. That is a problem for Haxl, but not for you, since you are happy with a single thread. This does not justify a new monad. It is justified in Haxl, which is more ambitious, and solves different problems elegantly. 1,2 So you gain nothing using applicatives. A `for` with a distinctive signature is enough and do not mislead the users thinking that there are unknown powers in that monad. There isn't any. 3 - Who would code that way having a way to do it efficiently with a `for`? only an academic or some newbie that would never use your monad. And, if you still want to have your monad perhaps as an initial step for something more useful, remember that you hardly will need an intermediate representation of GADT's.
Why not just `IO T`? You get the functor instance for `IO` for free. sharedState :: IO T sharedState = readIORef _t 
To be yet more pedantic, in your example the parens are not needed because that's how it's parsed. As normal.
For the simplest case where the only supported operation is reading this is fine. In practice we usually want to support multiple operations, like `takeMVar` and `readMVar`.
Every time I hear `Coyoneda`, I think of the Simpsons' [Canyonero](http://simpsons.wikia.com/wiki/Canyonero). [*Coyonedaaaaaa*](https://www.youtube.com/watch?v=PI_Jl5WFQkA) [*Cooyoooneeeedaaaa*](https://www.youtube.com/watch?v=PI_Jl5WFQkA)
We have that in Emacs too, bound to `M-x shrooms`
I was going to post an explanation *why* this typechecking error is prefixed with "Occurs check", but I couldn't write something satisfactory. I had something like: &gt; "It's called the occurs check because during the type unification phase, the compiler tries to unify a type with a subset of itself, say `a` and `[a]`." Does anyone have anything more formal or well-reasoned?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/haskell] [Why is it called the "occurs check"?](https://np.reddit.com/r/haskell/comments/5v474c/why_is_it_called_the_occurs_check/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
I actually just found this: https://mail.haskell.org/pipermail/haskell-cafe/2005-March/009314.html &gt; If I remember correctly, an "occurs check" is checking whether a type variable (call it 't') occurs as an argument to a tuple constructor or function arrow in a type expression to be substituted for `t`, as above or in `t = t -&gt; t1`. &gt; &gt;Such occurences would lead to an infinite type, hence are forbidden. Is this still accurate?
Awesome. Thanks for sharing. Does it use intero haskell package to provide completion ?
Not available in 23.1 sadly. Also WTF, our university workplaces have emacs v 23.1
This is fascinating. I don't think it's going to catch on though. At least, I'm not going to recommend it for any Java projects. Java has a fairly consistent set of style guidelines and that's important.
Well, I'll admit I can see how that might be a useful optimization in some cases. But if Haskell really moves in this direction it still seems to me like it is not a simple thing to just declare NaN==NaN. You then would want to adjust Ord suitably and there are a lot of decisions to be made. (aside from having to decide on error payloads, IEEE 754-2008 already does have a totalOrder predicate and it distinguishes between +/- NaN.) I would hope that anyone wanting to make these sorts of changes would take a close look at what is already in the IEEE standard, think deeply about how to move forward, and then make sure that usual IEEE operations are still available and usable, even if they aren't in Eq and Ord. &gt;And if the RealFloat thing is such a big deal then yeah that can be &gt;rethought. It does sort of revolve around the idea that you are &gt;interacting with real floating point numbers. So maybe we just &gt;need a new class for things that are only relevant to real floats but &gt;that also work on complex real floats or other extensions of your &gt;typical IEEE double. Kind of lame but unless we can implement all &gt;of RealFloat for Complex numbers it might be necessary. It's a big deal for me. Close to 100% of anything I would ever write is numerical code that I want to work for both real and complex types. Although I'll admit, that since `Data.Complex` doesn't define a type class that would allow conjugation and other operations on real types, I am already using my own type class for pretty basic operations on complex numbers.
Other difference is that intellij-haskell supports navigation to library sources.
[removed]
It comes from unification terminology. An occurs check causes unification of a variable V and a structure S to fail if V _occurs_ inside S. E.g. `v` and `(Int,v)`. These can't be unified.
Since functions are pure, can't GHC just execute one million core instructions, and see if it terminates or not? This would avoid annotations, and it would retain a deterministic compiler. I guess memory consumption is a problem, since it might be possible to allocate a lot of memory with few instructions. And then it would be a problem, since memory consumption in the compiler is probably not specified, so if this made termination-searching stop, you'd get a term that reduces to a constant with one compiler release, but not with the next.
What I think I want is something like import Data.Map.Lazy import Data.Text.Lazy foo = with Data.Text.Lazy empty with empty defaulting to `Text` in the `with` block.
Came here to say this.
Here is the question Bartosz references at the end: [Monoid under Day convolution and lax monoidal functor: Is strength necessary?](http://mathoverflow.net/q/260635/34579)
Please do offer up pull requests! It sounds like there is more than enough interest to warrant putting it up on Hackage but first some cleanup is in order I think.
Hello /r/haskell! I'm Andrey, the host of this podcast. In this episode we explored features of state-of-the-art type systems. We went into Interfaces, Generics, ADT, Type Classes and Dependent Types. Talked with people with functional &amp; OOP backgrounds. In general, had a great time  If you're interested in the topic, join the discussion and ask guests questions on our forum: https://discuss.codepodcast.com/t/episode-5-type-systems Hope you enjoy the show! Please let me know what you think afterwards! Andrey
The goal is to see what's popular (or simply makes sense), improve it on issues, and place it somewhere else or deprecate with instructions on how to replace it. I can imagine it working, and can also imagine it not working.
But you get different semantics. `&lt;|&gt;` is not concatenation, but more of a `catch` operation. So if you wanted concatenation you shouldn't have been using `&lt;|&gt;` in the first place?
[removed]
Actually Morte was first and some ideas were carried over to Dhall. Ability to refer to fragments of configurations makes perfect sense in Dhall.
A reflection on types (link just below) about Typeable implementation.
/u/simonmar, I just added support for [throwing and catching exceptions](https://github.com/basvandijk/monad-batcher/blob/support-exceptions/src/Control/Monad/Batcher.hs#L54) to the Batcher monad. Interestingly I don't seem to need the [Throw constructor of Result](https://github.com/facebook/Haxl/blob/master/Haxl/Core/Monad.hs#L196) that Haxl uses. I just utilise the throw and catch operations of the underlying monad. Is there a particular reason you need the Throw constructor and can't use a similar scheme as in monad-batcher? It could also be that I'm missing something very obvious since I haven't even tested this approach yet. 
I'm no IDE fan, but that is straight-up gorgeous. Now that Vim 8 supports asynchronous checking, here's hoping this will sting someone into writing an intero backing for vim. Alas that my vimscript-fu is too weak to attempt it myself.
Link not displaying below in my browser at least, but I've got it now ;)
I discovered the reason: when I immediately throw an exception in the base monad like: throwM ex = Batcher $ \_ref -&gt; throwM ex the exception is thrown too soon, i.e. it will be thrown before the execution of blocked commands that preceded it. One attempt to fix it is to postpone the throwing by turning it into a blocked computation: throwM ex = Batcher $ \_ref -&gt; pure $ Blocked $ Batcher $ \_ref -&gt; throwM ex However this causes the exception to be thrown to late, i.e. after the execution of blocked commands that follow it. I believe the solution is to incorporate Throw as part of the Result type exactly like you do in Haxl so that you have precise control of when the handle exceptions.
vault is extremely cool.
Someone's shadowbanned... Reddit says there's 2 comments, but I only see 1.
They could just make floating points totally ordered, just like how `Ord a =&gt; Maybe a` is, despite it having a failure value.
Why it is going to apply again and again. In my opinion: ((_ -&gt; x -&gt; y) -&gt; x -&gt; y) -&gt; x -&gt; y should be finished. Thanks
I believe OP needs to _use_ REST APIs, rather than provide a REST API.
&gt; Is haxl or some other similar library a good option for the "retrieving results from diverse sources" problem? What problem do you want to solve exactly? It would help with implicit parallelism (using applicative functors in your frontend code and some concurrency mechanism in your data sources), but you won't be able to use its caching or batching features if all you do is send N requests to N different data sources.
Since you post in /r/haskell, could you sum up why I would prefer your hypothetical language over Haskell? I see some stuff about dependent types, but that's covered by Richard Eisenberg's work.
`const fn`, first class labels and named arguments, for example. IMO the most interesting thing is the relationship between variable arguments and row polymorphism. Although I haven't written that part, it should be obvious in its context.
Also, I'm glad to see someone in this sub actually read the whole thing :D
For lists, it is `catch` in the same sense that the empty list represents failure (`empty`/`&lt;|&gt;` as in `mzero`/`mplus`).
I like it. One thought occurred to me though: exceptions. The library doesn't have any explicit support for exceptions currently. This has historically been a very tricky area to get right in Haxl. Do you want to be able throw and catch exceptions? What happens when a data source throws exceptions? What about asynchronous exceptions? Lifting arbitrary IO into Batcher could lead to surprises when things get reordered. This is why Haxl calls the lifting operation `unsafeLiftIO` and doesn't expose it through the normal API.
It is a type annotation (possibly inline), so as far as I can tell it's saying that `id 1` is fully applied and returns (evaluates to) something that is an instance of `Num`. I believe `a` will be inferred depending on the surrounding context.
It constrains `a` to be of the typeclass `Num`. See https://en.wikibooks.org/wiki/Haskell/Type_basics_II
You've asked a lot of questions here lately. May I suggest /r/haskellquestions? Also, what resources have you been trying to use to learn Haskell? I think most of these questions should be answered by any decent book. Maybe we can help you find something to make learning these things easier.
I bought the http://haskellbook.com/ and struggling on the chapter 5 with types. 
After the REST interface: exp &lt;- parse input serv1 exp &lt;&gt; serv2 exp &lt;&gt; serv3 exp The Monoid opertor (`&lt;&gt;`) would collect all the results and concatenate them. More generally: exp &lt;- parse input foldr (&lt;&gt;) mempty $ map (\serv -&gt; serv exp) services where exp :: Expression services :: [Service] type Service= Expression -&gt; IO ByteString If the monad support concurrency, like Haxl or Transient, you can invoke all the services simultaneously. To parse the input you might use a parser like attoparsec or parsec
[removed]
[removed]
I dont't know the implementation of `kessel`, but the only one that is possible is: kessel a b = a and you restricted the type by yourself, while it could be `a -&gt; b -&gt; a`, without constraints. It goes by `const` on the Prelude. To understand the signature, the compiler will roughly ask the following: - `kessel` is being used, what do I need on its arguments? Ok, `(Ord a, Num b)`. - `1` is being used as an argument, what is it type? `Num a =&gt; a`. What is needed for it to be used as the argument? It should be `Ord` too. - That means that for the first argument we have something of type `(Ord a, Num a) =&gt; a`, namely `1` and the second argument must have obey `Num b`. Putting everything together we have `(Num a, Num b, Ord a)`.
What i hoped this was before clicking on the link and seeing what it actually was, was some sort of 'preprocessor' for c like languages that insert braces/semicolons according to the usual haskell rules so you can use layout instead, so it would convert the output without the semicolons/braces in the link, to the input. So if someone could just, get on that?
Be sure to watch Bartosz Milewski's lectures that are based on this book: [https://www.youtube.com/watch?v=N6sOMGYsvFA](https://www.youtube.com/watch?v=N6sOMGYsvFA)
`1` does not have a concrete type like `Int` or anything. When you pass it `1`, GHCi is trying really hard to give it the most generic type possible. Since any type with a `Num` instance can be created from a number literal like `1`, GHCi waits for you to tell it that you want a specific numeric type before specializing `a`.
oh wow, that is awesome!
Hi David, Do you know of any examples of Monads which thread state?
Isn't `foldr (&lt;&gt;) mempty $ map f xs` the same as `foldMap f xs`?
Another "have you looked at related work X?" comment, but have you looked at [Ur/Web](http://www.impredicative.com/ur/)? It has row polymorphism and ML functors, which combine to make something that feels kind of like a typed macro system. Ur/Web also has separate phases of compile-time and run-time code, requiring polymorphism and function arguments to be instantiated at compile time. However, it's all figured out by the compiler's aggressive inlining, and there's no way for the programmer to directly specify phases. This turns out not to be a problem in practice unless you want some sort of dynamic dispatch (for which you want closures at runtime), and the payoffa runtime with no garbage collectionis worth it for Ur/Web's use case.
I haven't dug through all of the lens internals, but I've seen a lot of them. I'd say the best way to figure out what about 50% of them are up to is to just try to implement a lens library yourself. Many of these types don't have some meaning larger than being useful to implement a particular aspect of the library (or, if they do, I don't know of them and they're not seriously hinted at by the names). The remaining 50% do have some deeper theory. I'd divide these into two groups: identifications of external concepts that just happened to show up in Lens and Really Useful Repeatable Patterns which Lens is founded upon. The latter I can talk about briefly: You should really figure out what Profunctors are and how they related to lens. If you're really interested, take note that you can implement lens using the formulation `type Optic c s t a b = forall p . c p =&gt; p a b -&gt; p s t` where `type Iso = Optic Profunctor`, `type Lens = Optic Strong` and `type Prism = Optic Choice` where Strong and Choice are subclasses of Profunctor. You should figure out what's going on with Bazaar. It's an extension of the "Store" comonad and things like `Market` and `Bizarre` are all cute puns on this. The underlying idea here is the "indexed kleene store". If you're used to the idea that `type Store s a = (s, s -&gt; a)` is like a type with a "pointer" into that type, then an indexed store is `type Store s t a = (s, t -&gt; a)` which is the same idea when `s ~ t` but allows for a little variance, and the "kleene" bit refers to it being a collection of indexed pointers instead of just one. This helps you characterize a Traversal which has 0-to-many targets within a type. What makes `Bazaar` extra complicated is that `lens` often transposes between "final" and "initial" encodings of these types for performance and composition reasons. Initial encodings are sort of like free monads and final encodings are what happen when you represent a type exclusively by "how it can be used". I don't have the space to really describe this, but you'll hear these terms all over the place and also will need to derive or take it on faith that certain types are all isomorphic to one another despite looking very weird. It might be worth going over Twan van Laarhoven's blog where he did some of the earliest exploration of these ideas: - http://www.twanvl.nl/blog/haskell/overloading-functional-references - http://www.twanvl.nl/blog/haskell/cps-functional-references - http://www.twanvl.nl/blog/haskell/non-regular1 - http://www.twanvl.nl/blog/haskell/non-regular2 The comments are also important to look at as people used them to elaborate on the ideas, extend them, prove some laws. (Zemyla's comment on non-regular1 introduces the indexing idea and also starts to note how an indexed store relates to profunctors, traversals, applicatives, and explicitly references Bazaar. Then again, it was written 7 years later, so it's really only connecting the dots the same way you could.) Finally, worth noting that there are two kinds of "indexing" going on. On the one hand, there's the "indexing" as I just described above which relates to the extension of `Store s a` to `Store s t a` providing an extra "type index" and more flexibility. On the other hand, `lens` is very concerned with the ability to pipe indexes around so that you can have "indexed lenses" which let you, e.g., have a traversal into a `Map k v` which interacts with both the value being stored and its index. Very practically important, but as far as I'm aware it's got fewer interesting theoretical facets. The essential way that indexed optics work is that optics often interact with variations on functions like `(a -&gt; f b) -&gt; (s -&gt; f t)` where the user of the lens will have a chance to describe how `a`s become `b`s. We want to *also* have access to the index when we make that transformation, so we'd like the function to look instead like `(i -&gt; a -&gt; f b) -&gt; (i -&gt; s -&gt; f t)` for some index type `i`. Instead of manually piping that everywhere and having to deal with circumstances where it's not useful, much of lens is written generically in the arrow type so that we can write `IndexedArrow i a b = i -&gt; a -&gt; b` and just use that in place of `-&gt;` when needed. This overlaps neatly with the general technique of working over unspecified Profunctors instead of specific arrows (`IndexedArrow` is clearly a `Profunctor`) so these two techniques get mashed together. *Edit:* One last big trick that's worth keeping in mind. What does it mean for `f` to be both `Functor` and `Contravariant`? As it turns out, it means that `f a` doesn't "contain" any `a`s at all. Try writing `fcoerce :: (Functor f, Contravariant f) =&gt; f a -&gt; f b` to prove it to yourself. This trick along with a clever use of the applicative instance of `Const` gives rise to how lens handles certain kinds of subclassing and multiplicity. Keep an eye out for it and you'll be able to mentally erase a lot of noise.
I know little about ML modules. Can you elaborate? I've heard of Ur/Web, but don't know anything about it. CTFE is different from inlining, though, but I guess it does both. How does it achieve type-safety without GC? Is it similar to Rust? I think the biggest problem to make a functional general-purpose language would be mutable references because it seems incompatible with referential transparency. Or does it just clone the objects everywhere.
I don't understand the question. If you're wondering about duplicating state, consider the `[]` monad.
This is a great reply. Tangentially, I personally find the profunctor optic story so satisfying that I want to use that and only that, and absolutely recommend it as a way to explore the idea of building a lens library yourself. It should go without saying that the `lens` package is a tremendous achievement, but the relative opacity of the concepts is not one its merits.
Thanks! And I totally agree. It seems clear that profunctor transformations are a better foundational theory for lenses than the `a -&gt; f b` business, but compatibility with `traverse` and generally the easier story for traversals also seems to have won out.
[Functional Geometry (pdf)](http://eprints.soton.ac.uk/257577/1/funcgeo2.pdf) is indeed a really nice read.
I'm not really helping in the cleanup department with my changes unfortunately, but I will see if I can put together a PR when I'm done fiddling with things.
&gt; One could write a class for monads that can thread the state, You wrote that. I'm wondering if there are examples of such a pattern.
This compiles (7.10.3): {-# LANGUAGE DeriveDataTypeable, ScopedTypeVariables #-} import Data.Typeable import Control.Exception data Exception1 = Exception1 String deriving (Show, Typeable) data Exception2 = Exception2 String deriving (Show, Typeable) instance Exception Exception1 instance Exception Exception2 action :: IO () action = throwIO $ Exception2 "exc. 2 msg" main :: IO () main = action `catches` [ Handler (\(err :: Exception1) -&gt; putStrLn $ "ex1: " ++ show err), Handler (\(err :: Exception2) -&gt; putStrLn $ "ex2: " ++ show err), Handler (\(err :: SomeException) -&gt; putStrLn $ "ex. other: " ++ show err) ] 
.. and Indexed optics.
Using catches is an option, but the main point of my question is that the Control.Exception documentation recommends NOT using catch/catches unless dealing with async exceptions.
For examples of similar classes, look at `mtl`. One possible implementation might be `newtype Noisy a = Noisy (StateT NoiseState IO a)`, but I don't know enough about the problem domain to say how you should do it.
Do you understand the Functor typeclass? One way to generalize Functor is to be a Functor on two type arguments class Bifunctor p where bimap :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; p a c -&gt; p b d first :: (a -&gt; b) -&gt; p a c -&gt; p b c second :: (b -&gt; c) -&gt; p a b -&gt; p a c instance Bifunctor (,) where bimap f g ~(a, b) = (f a, g b) instance Bifunctor Either where bimap f _ (Left a) = Left (f a) bimap _ g (Right b) = Right (g b) Basically, with Bifunctor you can map over both sides of a Either or Tuple, or any similar sort of type. There's another sort of way to generalize a Functor, though - flipping around the function that you pass around to fmap, which gives you a "Contravariant Functor": class Contravariant f where contramap :: (a -&gt; b) -&gt; f b -&gt; f a Basically, in a Functor, the function `a -&gt; b` is applied to data that's contained "inside" the Functor or is the result of a computation that the Functor represents - for example, to the data inside a list, or to the result of an IO computation. By contrast, a Contravariant functor, the function `a -&gt; b` gets applied to the *input* to the computation the Contravariant represents. One of the simplest Contravariants to understand is the Contravariant for Predicates, `a -&gt; Bool`. In that case, you essentially have something like contramap :: (b -&gt; a) -&gt; (a -&gt; Bool) -&gt; (b -&gt; Bool) A Profunctor is what you get if you smash those 2 generalizations together. It's a bifunctor that's contravariant on the first type. Basically, it's some type that you can map over both input *and* output. The simplest type that maps to this is `-&gt;`. There's a number of other Profunctors, though. For example, you can define a Profunctor for `newtype Kleisli m a b = Kleisli { runKleisli :: a -&gt; m b }`. Or for `data LeftFold a b = forall s. L (s -&gt; b) (s -&gt; a -&gt; s) s`. edit: fixed contramap's signature
There's actually nothing particularly difficult about `Profunctor`, although the name is intimidating. It's just two ideas smashed into one. I assume you understand `Functor` already, which is one of the ideas. The other idea is expressed by the [`Contravariant`](https://hackage.haskell.org/package/contravariant-0.1.0/docs/Data-Functor-Contravariant.html) class: class Contravariant f where contramap :: (a -&gt; b) -&gt; f b -&gt; f a `Contravariant` itself may seem a bit surprising at first. The usual idea is that if `f` is contravariant, then its type argument only appears in a *negative* position. What does that mean? That it appears on the left side of an odd number of function arrows. For instance: a -- positive position a -&gt; Bool -- negative position (a -&gt; Bool) -&gt; Bool -- positive position ((a -&gt; Bool) -&gt; Bool) -&gt; Bool -- negative position a -&gt; Bool -&gt; Bool = a -&gt; (Bool -&gt; Bool) -- negative position Once you've gotten a feel for `Contravariant`, you can think about `Profunctor`. A profunctor is a type `* -&gt; * -&gt; *` that is *contravariant* (like `Contravariant`) in its first parameter and *covariant* (like `Functor`) in its second parameter. `(-&gt;)` is indeed the archetypal profunctor, but you can find plenty of others. Perhaps you can gain some good intuition with a `Stream a b` type that processes elements of type `a` and produces ones of type `b`. You can map *contravariantly* over the input, and *covariantly* over the output.
---and if anyone wants to download them all (ie only all `*.hs`) in one go for offline play: https://github.com/metaleap/rosetta-haskell-dump/ ) Some of them I noticed don't seem to compile with GHC 8 outright --- I'll automate their detection "one of these days" (actually have a need coming up) and ditch those
Functors all the way down! Slowly turning into a "functorial programmer"..
I think LYAH, as the first ever book I read about functional programming gave me a lot, but the late chapters indeed felt a bit "in one ear out the other"
This looks interesting. But also in a way similar to RWH, and maybe also slightly to Haskell from first principles. May I ask how does it differ from these?
You emphasize front end work..for what reason exactly? Are you saying that static typing makes sense for "backend" work but not for front end? That's such an artificial divide, it's silly.
I figured the combining the results from the different insurance providers might be a monoid but wasn't sure because it's really just putting their responses in a list and returning the list, not really combining them into a single thing in the sense 1 + 1 = 2 sense.
Since we have no use for caching or batching Haxl might be overkill then. Thanks for pointing this out.
It's not arbitrary. Front end work is, by its nature, close to humans. Humans do a lot of fuzzy crap. A language that allows graceful duck typing can be a huge advantage that allows you to avoid a ton of boilerplate. I have certainly written my share of code where I wasn't terribly happy with the behavior, but by volume, my experience has been that duck typing saves more headache than it inflicts when dealing strictly with user input. Whether or not that's true for anyone else's code is a question of how they like to do things, and their use case, hence my question - given the recent popularity, it seems likely that others are doing things differently in this sphere, so it made sense to ask about it.
I think that would lead to nasty surprises, and incorrect behavior. Let's assume NaN is Equal to NaN in your definition. So what's 0/0 compared to log(-1)? Both of those produce NaN, so comparing those two expressions would result in Equal, which is completely nonsensical. In reality, log(-1) is the complex number i*Pi, while 0/0 is indeterminate, so claiming they're equal would just be wrong. Instead, what about a partially ordered set? Obviously, NaN is incomparable to every other IEEE 754 floating point value. Let's make the typeclass match reality, instead of trying to bend reality to the typeclass.
&gt; even though that is there semantics. Their*
Everyone else is correct that profunctors are bifunctors (i.e., functors in two different ways) where one "way" is covariant and the other is contravariant. But while technically accurate, that may not be so helpful for building intuitions... Intuitively, a profunctor is trying to generalize the notion of "collections of 'functions'" where I use the term 'function' extremely loosely. The canonical profunctor is `(-&gt;)`, where each `a -&gt; b` is the set of functions from `a` to `b`. There are three ways to generalize this: (1) instead of being a *set* of functions, we could ask that it has some additional structure like being a monoid or a CPO. That way lies enriched/weighted category theory. (2) instead of being functions, they can be some other form of mapping. An obvious example here is morphisms in a Kleisli category, i.e. `(_ -&gt; m _)` for some fixed `m`. This is what the "`Profunctor`" class in Haskell is good for and what everybody's description is getting at. (3) we could allow for `a` and `b` to "live in different places". For `(-&gt;)` they both live in the same place, namely they are both proper types (aka: objects of the category Hask). However, there's nothing that forces us to have that restriction. This third form of generalization is [what profunctors are about in category theory](https://ncatlab.org/nlab/show/profunctor). For this to make sense, you need to understand how functors (in category theory) are mappings from one category to another (unlike the "`Functor`" class in Haskell which is only for mappings from the category of Haskell's proper types to itself). Assuming you do have that background, every functor `F : C -&gt; D` induces two profunctors: `D(F(c),d)` and `D(d,F(c))` where `c:C`, `d:D`, and `D(_,_)` is the hom-profunctor for category `D`. As a more concrete example of the latter, consider the gadt: data Vect : * -&gt; Nat -&gt; * where Nil :: Vect a 0 Cons :: a -&gt; Vect a n -&gt; Vect a (n+1) If we fix the type of the elements to something suitable, this gives us a category theoretic functor `Vect R : Nat -&gt; Hask` (or `Vect R :: Nat -&gt; *` if you prefer). The morphism map is the usual one for vector spaces: -- Category-theoretic notation fmap : Nat(m,n) -&gt; Hask(Vect R m, Vect R n) -- Haskell notation fmap :: (Fin m -&gt; Fin n) -&gt; Vect R m -&gt; Vect R n fmap f xs = accumVect [(f i, x) | (i,x) &lt;- xs] -- aka: fmap f xs = Vect $ \j -&gt; sum [ xs!i | i &lt;- f^{-1}(j) ] (Or if you don't like that one, you can consider it's other functor interpretation: `Vect A : Nat^op -&gt; Hask`, which has no restrictions on `A`: fmap : Nat^op(m,n) -&gt; Hask(Vect A m, Vect A n) fmap :: (Fin n -&gt; Fin m) -&gt; Vect A m -&gt; Vect A n fmap f xs = Vect $ \i -&gt; xs ! f i But I'll ignore this one since the "op" introduces unnecessary confusion about what's covariant vs contravariant.) From `Vect R` we get two profunctors. Their action on objects is: -- aka "(-&gt;)(Id, Vect R)" P1 : Hask^op * Nat -&gt; Hask P1(a,n) = a -&gt; Vect R n -- aka "(-&gt;)(Vect R, Id)" P2 : Nat^op * Hask -&gt; Hask P2(n,a) = Vect R n -&gt; a and the action on morphisms should be obvious: P1 : Hask^op(a,b) * Nat(m,n) -&gt; Hask(a -&gt; Vect R m, b -&gt; Vect R n) P1(f, g) = \h -&gt; fmap g . h . f P2 : Nat^op(m,n) * Hask(a,b) -&gt; Hask(Vect R m -&gt; a, Vect R n -&gt; b) P2(f, g) = \h -&gt; g . h . fmap f The `P1` form looks a lot like Kleisli arrows, except that `n` lives in `Nat` rather than living in `Hask` (aka `*`)! But even though the `n` and `a` live in different places, we can still think of `P1(a,n)` as " the collection of mappings from `a` to `n`". And you can imagine cases where it'd be nice to capture this set of mappings. E.g., if we're doing machine learning and want to talk about feature functions which take in some value of type `a` and spit out a vector of feature values (and we actually care about keeping track of the dimensions of vectors in our types). Recognizing the profunctorial structure of the `P1` type could help clean up code. Or, we may decide that `(-&gt;)` or `Vect R` aren't exactly right so we want some other representations for "the same idea", and we want to be sure client code doesn't care too much about the representation.
&gt; Let's not supply an Ord instance for Floats or Doubles (and don't make a partially ordered instance either). Well back in the real world, we can't just throw away useful things because they are imperfect. We just want to make them as good as we can. &gt; Or don't mislead people by giving them a thing that breaks the laws. I never suggested breaking any laws that `==` requires. If you would like to give me an example of how something I suggested would break a law required of an equivalence relation then let me know. But until then stop accusing me of things incorrectly. My implementation for `Eq` is COMPLETELY LAWFUL, you may not like how `0/0 == log(-1)` works, but just to reiterate, NO LAWS REQUIRED OF `Eq` HAVE BEEN BROKEN. And actually even for `Ord`, if we put `NaN` at the bottom, we would still not be breaking any laws required of a total order. a &lt;= b &amp;&amp; b &lt;= a =&gt; a = b Holds just fine. a &lt;= b &amp;&amp; b &lt;= c =&gt; a &lt;= c Yep, again holds fine. a &lt;= b || b &lt;= a Also holds just fine, now that we have decided `NaN == NaN`, and `x /= NaN =&gt; NaN &lt;= x`.
To be honest, I solve that problem by having an SSH server with an IRC client running on it, and then just connect to the server. You stay connected perpetually, keeping scrollbacks and mentions all the while, but only use the bandwidth for the things you look at. Also, it allows me to access the same chats whether I'm on my computer, my phone, or a library computer, without anyone on the other end being able to tell the difference, and no one noticing when my connection to the ssh server goes down.
Nice docs! :)
&gt; COMPLETELY LAWFUL ... NO LAWS REQUIRED OF Eq HAVE BEEN BROKEN First, let's calm down a bit. I have yet to see an internet discussion that is enhanced by raising one's digital voice. &gt; I never suggested breaking any laws that == requires Let me rephrase myself then. Doing meaningful things (comparing) with meaningless values (NaN) is misleading to the user. It's swallowing the exception that NaN is supposed to signal to the user ('Something went wrong! This result is wrong!') and binning every possible method for producing that exception into the same bucket. You can do it and make a total ordering, but when the special case occurs, it will be surprising to many/most of the users who aren't aware of the special case. It may lead to bugs in their program. So in your system, with NaN less than everything else, it's also less than negative infinity? Sqrt(-1) is less than negative infinity? That's going to be confusing. &gt; we can't just throw away useful things because they are imperfect We might choose to because it's misleading. Or we might choose to define a type of NaN-aware Ord, e.g. ````compare :: Double -&gt; Double -&gt; Maybe Ordering````. Anyway, you should be pleased to note that today's definition of Ord for Doubles and Floats puts NaN as greater than everything else: compare (log(-1)) (1/0) GT 
Well instead of ByteString, you can use some data structure with a more sophisticated Monoid definition
Maybe, but the foldr expression is more expressive and impressive ;)
How should I read the book? Every time I encounter something that I do not understand, then just annoying people with my questions? How should I do, keep reading or ask, when I do not understand something, even when it will mention in the next chapter. 
That's a really clean and clever haskell integration. How do you achieve a good autocompletion with the . char. I'm writing a haskell language server and the . is so ambiguous when you don't want to write a full parser. How to distinguish between f.g (where g is a local function) and an import qualified f.putStrLn for instance. For now, i propose qualifier first, and default completion if it's empty.
&gt; Let me rephrase myself then. Doing meaningful things (comparing) with meaningless values (NaN) is misleading to the user. It's swallowing the exception that NaN is supposed to signal to the user ('Something went wrong! This result is wrong!') and binning every possible method for producing that exception into the same bucket. I understand that point, and I do agree it is an issue. But please note that I am not really attempting to solve that. Since my suggested change does not make that situation any worse OR better. Either way `==` can be surprising, but at least my way `==` is lawful. I don't currently have a good solution to the `NaN` problem as a whole. Perhaps more liberal use of `Maybe`, or hell even `_|_`. But I just want to fix `Eq` and make `a == a =&gt; True` a god damn law haha.
Thanks so much. 
Each of these functions (`round` and `fromIntegral`) does one thing and does it well. Let's see the information: &gt; :i round class (Real a, Fractional a) =&gt; RealFrac a where ... round :: Integral b =&gt; a -&gt; b ... -- Defined in GHC.Real &gt; :i fromIntegral fromIntegral :: (Integral a, Num b) =&gt; a -&gt; b -- Defined in GHC.Real The `round` function is not about `Floating`, it's about any `Fractional` type. When you implement a `Floating` type you make the best and optimal `round` for it. On the other hand, `fromIntegral` works for any `Num`, so when you instantiate your own `Num` you implement the best and optimal `fromInteger` for it. Therefore you don't need to have different rounding functions, like roundDouble :: Double -&gt; Double roundFloat :: Float -&gt; Float roundRatio :: Integral a =&gt; Ratio a -&gt; Ratio a ... Instead you get one universal composition fromIntegral . round :: (RealFrac a, Num c) =&gt; a -&gt; c which chooses the best implementation for any suitable type: fromIntegral . round :: Double -&gt; Double fromIntegral . round :: Float -&gt; Float fromIntegral . round :: Integral a =&gt; Ratio a -&gt; Ratio a ... I think it is not so difficult to make an alias roundDouble :: Double -&gt; Double roundDouble = fromIntegral . round in the program, which uses rounding heavily. &gt; roundDouble pi 3.0 &gt; roundDouble (exp 100) 2.6881171418161356e43 &gt; roundDouble (exp (-100)) 0.0 
I think the point was not about polluting the namespace with roundWhatever. It was about the (lack of) necessity to enforce stricter class of the result.
 fromIntegral . round :: (RealFrac a, Num c) =&gt; a -&gt; c The question is why `round` doesn't have this more general signature in the first place. &gt; which chooses the best implementation for any suitable type: It's generally false that if two functions have the "best" implementation, then their composition also gives the "best" one (e.g., a function and its inverse).
Perhaps he meant it offers nothing :p
You're about to get a butt load of "reasons" for the type of `round` in the Haskell standard prelude, but the *true* reason is that the designers of the standard prelude cocked up floating point big time, due to lack of experience and too much hubris to go down the hall and ask an expert. There is a darn good reason `round` and friends in `libc` return floating point types, and those reasons apply equally well to numeric programming in Haskell. Below is a taste of the sordid situation. $ ghci GHCi, version 8.0.1: http://www.haskell.org/ghc/ :? for help Prelude&gt; :t round round :: (RealFrac a, Integral b) =&gt; a -&gt; b Prelude&gt; let inf = 1/0 Prelude&gt; inf Infinity Prelude&gt; -inf -Infinity Prelude&gt; round inf 179769313486231590772930519078902473361797697894230657273430081157732675805500963132708477322407536021120113879871393357658789768814416622492847430639474124377767893424865485276302219601246094119453082952085005768838150682342462881473913110540827237163350510684586298239947245938479716304835356329624224137216 Prelude&gt; round (-inf) -179769313486231590772930519078902473361797697894230657273430081157732675805500963132708477322407536021120113879871393357658789768814416622492847430639474124377767893424865485276302219601246094119453082952085005768838150682342462881473913110540827237163350510684586298239947245938479716304835356329624224137216 Prelude&gt; let nan = 0/0 Prelude&gt; nan NaN Prelude&gt; -nan NaN Prelude&gt; round nan -269653970229347386159395778618353710042696546841345985910145121736599013708251444699062715983611304031680170819807090036488184653221624933739271145959211186566651840137298227914453329401869141179179624428127508653257226023513694322210869665811240855745025766026879447359920868907719574457253034494436336205824 Prelude&gt; round (-nan) 269653970229347386159395778618353710042696546841345985910145121736599013708251444699062715983611304031680170819807090036488184653221624933739271145959211186566651840137298227914453329401869141179179624428127508653257226023513694322210869665811240855745025766026879447359920868907719574457253034494436336205824 Prelude&gt; round inf :: Int 0 Prelude&gt; round nan :: Int 0 Prelude&gt; last [0..9.5] 10.0 Prelude&gt; :quit Leaving GHCi. 
Thanks. Using doctest makes some things a little uglier than they could be otherwise, but to me the convenience of knowing that the examples I provide actually build outweighs this.
 I'm not sure. Opaleye also makes use of Category, Arrow, and another abstraction called product profunctors that to my understanding was specifically invented for opaleye. All of these type classes are things that a Colonnade is definitely not ( for example, think of trying to define id for a colonnade). But, maybe the common use of profunctor is no coincidence. I'm not sure, and I think I would need to actually use opaleye to figure out if the instance is doing the same thing in its case. 
Paging /u/pcapriotti . Is this possible? Thanks in advance!
A minor caveat: `fromIntegral . round` has the problem of the intermediate integral type being underspecified, and relying upon defaulting. `fromInteger . round` does not. In theory it might have been better a couple of decades ago to name an outside of the class helper that does the `fromInteger . round` just like we provide `fromIntegral = fromInteger . toInteger`, and/or to have the member function return an `Integer` rather than an arbitrary `Integral`, after all we supply `toInteger` not `fromIntegral` in `Integral` itself. The slight change in behavior if `fromIntegral` was the member of `Integral` rather than `toInteger` is rather illuminating. We embrace `Integer` as a canonical choice of intermediary there. `round` returning an `Integral` is kind of backwards in that `Integral` is about "being a good source of whole numbers", and every instance looks like some sort of truncation of `Integer`. So it is odd to return something of this sort, when its major reason for existing is that you can get an Integer out of it. If you look at how it got that way, though, it is pretty clear that it had to do with the `RealFrac (Ratio a)` instance. The notion that someone was trying to avoid expanding intermediate variables beyond the register size seems apparent to me. Now, `Ratio Int` is pretty much always a bad idea -- It overflows alarmingly quickly -- but it illuminates the sort of fuzzy thinking in this area. By returning the result in an arbitrary `Integral` we could choose to do the result calculations in the current Integral used by the numerator and denominator, then `fromIntegral` at the end. Of course nothing says which to do or of either one is better than the other. rounding Ratio Int -&gt; Integer should probably work in the target Integral, while Ratio Integer -&gt; Int probably should work in the source Integral, lest the calculation overflow in one and not the other. Also the lack of laws on the class are such that these are not in general going to return the same answer! [Ignorable Aside: If you make up some laws to try to make this whole thing sane then`toInteger` wants to be a nice ring homomorphism, but risks the intermediate operations overflowing before you get there. `fromInteger`on the other hand is well behaved, but their composition still has the properties that you can't assume bad, or at least different, things won't happen if you'd overflow on either side.] We're left with a pattern that would force us to do the wrong thing in one of these two cases or to take a third, more moral, high road and convert the intermediaries to Integer _anyways_, then work there for all our `div`/`mod` work, then `fromIntegral` at the end. So which one do we do in base? I leave it as an exercise for the reader to figure out. (I do so, because honestly I don't remember, and I don't have GHC handy where I am.) Also note the dictionary noise from passing `Integral` in to do the work on the result number type overwhelms any concrete wins in representation made by using smaller intermediaries unless both `a` and `b` are fixed concrete types and the code gets inlined. Having `round` produce `Integer` probably would have been saner all around, then having a helper to the final `fromInteger`, but then we'd have to take two names. Having it produce a `Num` would have made it clear you need to do the intermediary work yourself before doing that final conversion via `fromInteger` that everyone seems to need in all their instances. I don't think it was obvious that both of the other options were sort of "broken" at first. In a world where folks were more upset about this I'd kind of like to see the constraint on round relaxed to class (Real a, Fractional a) =&gt; RealFrac a where round :: Num b =&gt; a -&gt; b ... but this would necessitate fixing whatever user instances out there that currently try to do their `div`s and `mod`s in the output `b` type directly rather than more safely in an `Integer` intermediary. As it is, it probably isn't worth breaking all the instances to generalize the signature seen by the user of those methods. The refactoring that'd have to be done for user instances is kind of tedious and often needs explicit type signatures to pick Integer for the intermediary, making instances for `RealFrac` even more verbose, and this boilerplate gets repeated multiple times due to all the members.
Oh my god. I had no idea the situation was so bad with NaNs &amp; co. Is there any mature numeric library in Haskell that does these things the way they should be ?
BTW, Repa's fusion mechanism depends heavily on the simplifier getting it right, and inlining everything correctly. This requires fairly intricate tuning when it doesn't work. And it's sometimes brittle across GHC versions. That's why the PLS group at UNSW (who are the main people behind Repa) are focusing more on their other current parallelism project accelerate[-llvm] these days for CPU as well as GPU parallelism. 
Cool, that seems like what I had in mind. Thanks!
Reading that I have to conclude that with this kind of generalization we would need specialized libraries for fast numeric calculation. (What may already be the case, I wouldn't know.) It's probably a good thing. Most people can ignore this stuff and get the no-surprises default. People that need speed would have to replace the Prelude with something else, and on the process explicitly select what rules they are willing to lose.
Is that a subtle table-flip at the end? ;)
Case in point: from the (limited amount) I understand of Tensorflow's new "XLA" backend for post-training inference (e.g. on Mobile), it is essentially a compiler from your trained model directly to native code. The prior version kind of 'interpreted' the model from what I understand. Haskell would clearly be good at this kind of task. Tensorflow is also clearly symbolic in its programming model, and Haskell can be a very good fit for some of these problems too, of course. However, I strongly suggest every Haskell programmer who wants to compete in this space: avoid your inclinations to spend endless amounts of time abstracting things... You mention "advanced type level tricks" to do things like embedded DSLs. I would be very careful of this reasoning if I were you. The problem is those techniques are often extremely powerful but they are not "lightweight" (mostly in a cognitive sense). Machine learning is already a field rife with "black boxes". Do not add another one that requires an almost entirely separate set of cognitive burdens and domain expertise, without careful restraint. If your goal is niche adoption for a power-user tool among FPers, this can work fine. If your goal is to actually "get" these tools into the space in a large way, I think this thought process will largely hamstring you. I am not saying this because I'm a super hardcore machine learning scientist with years of experience. I am sure that many Haskell abstractions are useful for it. I am saying this has someone with a rather large amount of experience with the language, though. Those tricks aren't free from a maintenance or user POV. It's also not leveraging your tools correctly and playing to their strengths, really, if you ask me. Python is a language that's hard to "get right after the fact". It's hard to change with confidence. You need tests for everything and it can be hard to trace dynamic code. It's costly to pay for runtime errors. I'm sure there has been immense amount of time spent designing its APIs. Haskell programmers I think are more agile here: refactoring is safer and picking careful APIs is often easier, because many things like `data` have very low overhead and are cheap. It's ergonomically pleasing and easy to separate `Int` from `CustomerId` from `OrderId`. Just do that. In contrast -- highly experimental features like closed type families and injective families and all that stuff? It regresses and breaks people's programs *all of the time*! It's what you need for some of the most high-powered techniques, but it is not free. And that's literally the reason you chose Haskell, so it didn't break later! Don't hurt yourself like this, unless you have reason to believe it is absolutely necessary. Don't spend 40 years designing a perfect numeric hierarchy so you can define Tensors in a completely theoretically sound way. Define tensors, and work the other way -- refactor aggressively and keep your APIs as well contained as possible to insulate users. This is a form of modularity, all on its own (principle of least power), rather than a kind of "high ceiling" modularity where the typechecker either explodes on you or somehow does magic, or where you need a deep hierarchy of interconnected ideas to get things done (Haskell is much better at 'large' typeclass interfaces than 'small' ones I think, at least for now) Sure, Haskell may be a better symbolic language than the imitation in Python that Tensorflow uses or `sympy` or whatever -- but the difference is that people can actually *work with Tensorflow today*. You can educate people endlessly about how types will save them -- but the domain is already really messy (e.g. optimization of a model is basically magic?) and ultimately this will probably fall on deaf ears. There is a level of respect to be had for the fact people need a tool to get work done. As an example, I have played with Tensorflow over the past week to dip my feet in a little bit of ML. But I would probably never ever use Repa or Accelerate anything for this unless I specifically wanted to implement an optimization or algorithm in detail. And I probably would never bother contributing substantially to create some kind of clone. Why bother? I'll just use Judah's TF binding. My goal is not to abuse Haskell into doing ML, it is *to do ML*. Haskell is my mechanism -- it is not *my goal*. I think Haskell people can easily lose sight of that, especially in "fun" spaces like this. Just my 0.02c from some nobody who can grasp "softmax" at only the baby level, but still actually used TF to train a model.
Thanks a lot for the reply! I'll find some way around it then. Perhaps you should answer on stackoverflow too? That way, the next person trying to do this might save a few minutes. (I might copy-paste your answer to there too, if you don't have an account or something)
Yeah, the web site I linked to is just an executive summary. You have to click the ["View on Github"](https://github.com/Peaker/lamdu) link at the top to get a tutorial and installation instructions.
The point is that in a correct `Fractional` instance infinities are **not** equal. The instances for IEEE FP types are full of surprising and bizarre behaviors due to the weird properties of IEEE FP, and this is one of them. It has been proposed many times to wrap IEEE FP in Haskell with a layer that allows them to be better behaved semantic citizens of the Haskell world. But that has not happened, for a simple reason: IEEE FP types exist in Haskell only because of their efficiency, and any such wrapper would make it harder to write really fast FP code. So at the end of the day, the IEEE FP types are a kind of low-level optimization that exposes hardware functionality. Like all low-level optimizations, it makes your code uglier. But if your use case requires the optimization, you are willing to pay that price.
What? Why do you need a way to convert to Integral? Num is a superclass of Integral so if you convert to a Num then you automatically have any Integral.`Num a =&gt; a` is a strictly better/more general type than `Integral a =&gt; a`.
You betcha! $ ghci GHCi, version 8.0.1: http://www.haskell.org/ghc/ :? for help Prelude&gt; let inf=1/0 Prelude&gt; let nan=inf-inf Prelude&gt; round nan - round (-nan) -539307940458694772318791557236707420085393093682691971820290243473198027416502889398125431967222608063360341639614180072976369306443249867478542291918422373133303680274596455828906658803738282358359248856255017306514452047027388644421739331622481711490051532053758894719841737815439148914506068988872672411648 Prelude&gt; compare nan inf GT Prelude&gt; compare inf nan GT Prelude&gt; compare 0 nan GT Prelude&gt; compare nan 0 GT Prelude&gt; compare nan nan GT Prelude&gt; :quit Leaving GHCi. 
&gt; There is a darn good reason round and friends in libc return floating point types, and those reasons apply equally well to numeric programming in Haskell. Exactly - the standard prelude is for general use. It's not intended to be optimized for numeric programming. The designers of the standard prelude - who were the members of a committee that included some of the top computer scientists of the 1990s - actually knew quite a lot about types; probably a lot more than your "experts down the hall". And more than you think about numeric programming too, but that is not the point of the prelude. There are quite a lot of numeric-oriented libraries in Haskell - check them out. But I think everyone agrees that there is a lot more work to be done in this direction. If you have a design for types that would work well for numerics and it's not out there yet, then by all means, go for it. Not that the numeric types and classes are a perfect design for general use, either. It was cutting edge and experimental at the time, and a great job. But I think everyone agrees that it could be improved given what we now know. There just isn't a consensus about exactly how. If you feel that you can contribute to that discussion too, then please do.
Your proposal is in fact a decrease in power. It is an increase in polymorphism, which loses preciseness of the type, and therefore loses semantic information at the type level. Just because you *can* always apply `fromIntegral` does not prove that you don't lose information by doing so.
This makes sense if your goal is to make something that gets popular with non-Haskellers, but that's by no means the only goal. If that's what you're interested in, fine, but I don't even think it's a good goal. After all, anybody who wants a "worse-is-better TensorFlow" *will simply use TensorFlow*. You may as well just contribute to the [existing Haskell bindings](https://github.com/tensorflow/haskell) at that point. On the other hand, making an elegant, theoretically well-founded design fully taking advantage of Haskell is useful *because that doesn't exist yet*. Haskellers working on projects like this aren't "losing sight" of anything; they're prioritizing interesting design space rather than popularity.
[removed]
Great paper. There is also a Haskell implementation (which is slightly different): http://projects.haskell.org/diagrams/gallery/SquareLimit.html
looks like it is now much easier to integrate with stack! stoked.
&gt; the standard prelude is for general use. It's not intended to be optimized for numeric programming. IEEE Floating Point had, as a primary design goal, making naive code do as well as possible. Experts can check for NaNs and overflows and handle such cases carefully. But by default, the system should do something reasonable. In deviating from those design decisions, the standard prelude is not somehow catering for the more naive general user. Quite the reverse: it is putting caltrops in their path.
Hi Simon, thanks for the response. Like I mentioned above I figured out two *wrong* ways of implementing exceptions. I think the right way is how you do it in Haxl which I think I'm just going to copy in monad-batcher.
Have you heard about the [Haskell Book](http://haskellbook.com)? Highly recommended.
Good to know, is this variability observable by testing Repa with different versions of GHC? Like has someone recorded it? 
You could argue that it is a decrease in precision. Just like how fmap is less precise than liftM. But it is absolutely an increase in power, flexibility, and expressiveness.
Last I checked Int was within `Num a =&gt; a`. So you still get it no different than before. With my suggestion `round 0.5 :: Int` works IDENTICALLY to before. I don't know why people aren't understanding the OP, did I word it poorly?
&gt; but I do miss a good resource where you could read from beginning to end without much ado Looked into much material and honestly think the most-"programmer-friendly" material (if you have already a few years of coding experience) to first-get-hooked (the rest then follows) is [the Wikibook](https://en.wikibooks.org/wiki/Haskell/) for some reason. Has that distinct "coding tutorial series" flair of easing into things at the right pace. Also unlike text-books, they don't try to make you attack especially-contrived (or overly mathy/puzzly) "problems" as "projects to be completed upon book completion" or some such notion --- if you have coded for a while, you'll have plenty of your own ideas (aka, "the motivating kind") in your head already. The Wikibooks format covers the principal building blocks in a "this is the essence of it, here's the motivation for it too, and here a brief glance at the full spectrum of expressive power it gives you *potentially*" kinda flow. Once "into it", most every book and article out there brings *something* of worth to the table of the eager student. By now I have a reading list that could fill a full sabbatical year.. hope they'd stop writing books as of *right now*! (Except "Haskell from first principles" aka "ze Haskell Book", which last I checked is stuck at 0.12 ;) **Also a word of caution**: the ecosystem is awash with many (certainly stimulating but *initially distracting and confusing*) "diversions": type system experiments, language extensions, expressing every problem in category-theory terms and concepts, expressing everything even URL routes "in a compile-time type-safe manner", etc etc pp many such "higher-order gimmicks". One must keep watch at first to not lose sight of what is "essential Haskell" and what is simply seasoned enthusiasts and intellectuals looking to further push the boundaries, lest one be hopelessly overwhelmed by stuff that one had no immediate pressing need for *in the first place* (remember people doing a PhD have at least 3 years to freely allocate to their pursuits ;). Get an early (or repeat) grasp on the [elegant simplicity](https://www.youtube.com/watch?v=eis11j_iGMs) of Lambda calculus, then understand that all Haskell (no matter how fancy the extensions and gimmicks used) first compiles to almost (not quite) a similarly compact intermediate format just to give you a basic idea of what everything always boils down to. This helps one ignore all the "exciting promising" (aka super-time-consuming) things apparently happening everywhere in this space --- at least when one feels one hasn't progressed much for days of reading =) If you programmed in anything like modern JS, C#, Python in the last 5 years you know of anonymous functions, functions-as-(mostly/almost)-first-class-values, function application and variables anyway. The rest is practice, practice, practice (getting out of the non-purely-functional mindset can be achieved no other way), stopping by /r/haskellquestions anytime, read a bit when in the mood to further your arsenal, looking up library/module/built-in-function names, and the minor logistics related to builds etc
I'm personally pretty big on the whole maximizing polymorphism thing, but I guess that would also work. 
About two weeks ago, yeah.
But why that over `Num a =&gt; Rational -&gt; a` which is what I am suggesting? It's strictly more powerful. 
I just reinstalled Arch on my laptop and hadn't used XMonad in forever. I was using some (probably dated) configs, and having trouble with xmobar not displaying properly (would be covered by windows). I had to modify my config a bit as described [here](http://xmonad.org/xmonad-docs/xmonad-contrib/XMonad-Hooks-ManageDocks.html), notably adding the `docks` function: main = do xmproc &lt;- spawnPipe "xmobar ~/.xmonad/xmobar.hs" xmonad . docks $ defaults { logHook = dynamicLogWithPP $ myDefaultPP { ppOutput = hPutStrLn xmproc } , manageHook = manageDocks &lt;+&gt; myManageHook } 
&gt; which makes the user unable to copy+paste. What do you mean ? Do you want the user to be able to copy and paste the program output in the console ? If it is the case, I suggest replace the cmd.exe program for a better alternative since this one is pretty terrible. One of the best terminal emulators I have found is Emacs that has clickable history, can have multi line input and history and allows copy and paste. The only disadvantage is the lack of tab completion like in a real terminal.
You are not alone, a majority of blog posts I just don't understand and I've been doing Haskell for a few years. But I don't care, because I probably wouldn't use something a simpleton like myself couldn't understand in production (app-level) code. Just too cognitively loading to be comfortable. Save that stuff for libraries full of black magic. Simple Haskell code can do anything and is **waay** more straightforward to read than "simple" code in any other language.
I'm a recent XMonad convert, thanks for this piece of awesome!
Seriously don't do this though. The `These` parser is quite benign if they're flags or options, but this does create a positional, so it could be very confusing (or even broken) if there are other positional arguments in the parser. 
&gt; Simple Haskell code can do anything and is waay more straightforward to read than "simple" code in any other language. Agree &amp; I think that's absolutely the spirit =) I also propose a corollary: &gt; Simple Haskell code can do anything and is waay more straightforward to read than "simple" code monad-transformed-lensed-Kleisli'd-memptied-back-and-forth via `&lt;~+=&lt;&lt;`
Thanks. I had it with just `manageDocks` at first and that's when I had issues, hadn't tried removing it.
That's a good one! -- | An important event in history. Is also funny as the first comment to be read.
I guess it was just *a matter of time*.
This and the OP seem like a lot of extra machinery just to avoid typing 8080. I've always found smart constructors cleaner and more informative than the use of the lawless Default type class. newtype Port = Port Int defPort = Port 8080 data Foo = Foo { _port :: Port, _name :: Maybe Text } defFoo = Foo defPort Nothing
Compare: [withDef| data Options = Options { manager = Left defaultManagerSettings :: Mgr , proxy = Nothing :: Maybe Proxy , auth = Nothing :: Maybe Auth , headers = [("User-Agent", userAgent)] :: [Header] , params = [] :: [(Text, Text)] , redirects = 10 :: Int , cookies = Just (HTTP.createCookieJar []) :: Maybe CookieJar , checkResponse = Nothing :: Maybe ResponseChecker } deriving (Typeable) where userAgent = "haskell wreq-" &lt;&gt; Char8.pack (showVersion version) |] With: data Options = Options { manager :: Mgr , proxy :: Maybe Proxy , auth :: Maybe Auth , headers :: [Header] , params :: [(Text, Text)] , redirects :: Int , cookies :: Maybe CookieJar , checkResponse :: Maybe ResponseChecker } deriving (Typeable) newtype OptsHeaders = OptsHeaders [Header] newtype OptsRedirects = OptsRedirects Int newtype OptsCookies = OptsCookies (Maybe CookieJar) instance Default Mgr where def = Left defaultManagerSettings instance Default OptsHeaders where def = OptsHeaders [("User-Agent", userAgent)] userAgent = "haskell wreq-" &lt;&gt; Char8.pack (showVersion version) instance Default OptsRedirects where def = OptsRedirects 10 instance Default OptsCookies where def = OptsCookies (Just (HTTP.createCookieJar [])) And with the plain old separate instance: data Options = Options { manager :: Mgr , proxy :: Maybe Proxy , auth :: Maybe Auth , headers :: [Header] , params :: [(Text, Text)] , redirects :: Int , cookies :: Maybe CookieJar , checkResponse :: Maybe ResponseChecker } deriving (Typeable) instance Default Options where def = Options { manager = Left defaultManagerSettings , proxy = Nothing , auth = Nothing , headers = [("User-Agent", userAgent)] , params = [] , redirects = 10 , cookies = Just (HTTP.createCookieJar []) , checkResponse = Nothing } where userAgent = "haskell wreq-" &lt;&gt; Char8.pack (showVersion version) (Example taken from [wreq](https://hackage.haskell.org/package/wreq-0.5.0.0/docs/src/Network-Wreq-Internal.html#defaults)) &amp;nbsp; To me it seems the first one is the easiest to read, write, maintain, and contains the least amount of duplication and boilerplate. Does my question seem more agreeable with these examples?
Very much so when there is only one, two, or maybe at most three options I want to enable to be configurable, (while most of the time, in most of the use-cases they just get used with the default values). In that case doing what you describe can make tremendous sense. Especially in comparison to all this extra boilerplate. &amp;nbsp; However, what if the number of these options is 8, as in [this example I posted in another comment](https://www.reddit.com/r/haskell/comments/5vm792/anyone_knows_of_a_thqq_function_that_lets_me/de3b0oc/)? Or even as high as 16? Would you still go with the approach as you are describing, and have a function with 8-16 parameters?
I personally liked [this website] (https://crypto.stanford.edu/~blynn/haskell/). I'd recommend supplementing it with something else to learn the absolute basics but it shows plenty of applications for graphics and neural nets.
Thanks for pointing that out. This is now fixed in the newest hackage release. I had forgotten that the data constructors for `Headed` and `Headless` are on rare occasion actually useful for end users.
&gt; Requiring my prospective users to install a terminal emulator to use a script that I wrote for them is not a tenable option for most of my use cases - Getting any of my co-workers convinced that a command line program is useful in the first place is an uphill battle, a terminal emulator as a requirement would be right out. Command line application are very productive, customizable and scriptable. The problem is that Windows makes very hard to use them due the old and crippled cmd.exe. I really can't understand why MS doesn't improve it or port some Unix terminal for Windows. After I used many terminal emulators on Linux, I can't stand cmd.exe. On Windows Emacs saves from that. Making the people use the cmd.exe will make them angry with command line. Nobody should be suffer to use it. There are many terminal emulators better than cmd.exe that can provide a better CLI experience and even portable apps with no install.
Ok. Thanks. Lots of pitfalls to remember. 
I was actually wondering where this talk of profunctors came from, because I was looking at this on Hackage and 1.0.0 does not depend on profunctors. But I see you also realized what was wrong at about the same time. :)
I can't seem to reproduce this behavior.. If you can consistently, could you provide a small example and make a ticket at https://ghc.haskell.org/trac/ghc/ and I'd be happy to take a look. edit: ah, I understand now, you mean when a haskell program is the one to create the console. e.g. launching ghci on it's own instead of launching it from within a session already. I don't know what could be causing this behavior but If you can make a ticket I'll take a look and keep you updated there.
It was co authored by the guy in the video you posted fwiw
&gt; If you don't get something at first, don't struggle to understand it. This is important to learning almost anything. It's really easy to get caught up in the details, but fail to learn because you never apply it (due to spending too much time trying to understand and never progressing). Ideally you would just see this used a bunch, try it out, see what happens, and slowly form a picture in your head or what it means or does. Then you're either correct and can move on, or you are incorrect and can now ask a question related to your misunderstanding. (I'm currently learning Japanese and it's exactly like this).
This is really neat. Could be very useful in a lot of the boardgame apps I try to write. It's nice to have undo/redo functionality. Now if only I can convince you to provide instances of PersistField and PersistFieldSql for me :p
Some of your posts have helped me, though. Thanks for taking the time.
I am following the Haskell book and had not looked at wikibook. Thanks for pointing to that.
&gt; the Wikibook for some reason I know /u/wolftune put effort into improving it as he read through the first bits. I encourage everyone else to do the same :)
It's recommended right here at /r/haskell in the sidebar!
The tutorial on Github also just introduces its most basic features.
I have implemented exactly this now: https://hackage.haskell.org/package/free-functors-0.7.2/docs/Data-Functor-HHFree.html
I think [this haskell website](https://haskell-lang.org) contains quite a few very good tutorials and links to quite a few more, including [hpffp](http://haskellbook.com). Frankly at this point i think there's a decent amount of good tutorials on Haskell itself. I wish there was more effort on writing domain specific tutorials using Haskell :)
Please explain how erasing useful information is an increase in power or expressiveness.
Woah, amazing! I'm heading back to xmonad. Thanks Peter!
Somewhat! I went i3 -&gt; KDE -&gt; XFCE -&gt; XMonad. I can't really stay anywhere :)
XMonad is awesome. For some XMonad inspiration, checkout this [XMonad Demo](https://www.youtube.com/watch?v=70IxjLEmomg) video and the corresponding [configuration examples](https://github.com/altercation/dotfiles-tilingwm/tree/c9c6684381215a865116523868790eeae7e4fd2f/.xmonad).
This is neat! I'm a bit of a Haskell noob, so I can't lay out the code for you, but the way my mind tackled this problem was to partition the answers of step N into two parts - those that start with 1 and those that start with 0 - and use the answer of the step N-1 to try and answer each of those cases in turn. For the first case, I can prepend 1 to the start of *all* of my answers of step N-1 to obtain valid answers for step N. For the second case, I can prepend 0 to only those answers which started with a 1 in step N-1 (to avoid a '00' at the start). We previously established that the size of answers in step N, that start with 1, was all of the answers from step N-1. Thus the number of answers for step N-1, that start with 1, was all the answers from step N-2. Thus, the number of answers for step N, that start with 0, was all the answers from step N-2. Sticking the two partitions together again gives us Step N = (Step N-1) + (Step N-2), which probably looks familiar to you! 
I'll ignore the input and output parsing, because I believe you should be able to figure that part out on your own, and focus on the core problem, finding lawful strings of ones and zeroes. Let's apply the usual recursive reasoning. What we want is a function that, given a string length `n`, gives us a list of all the strings that do not contain contiguous zeroes. So: lawfulStrings :: Int -&gt; [String] We'll start with the base case, `n == 0`: lawfulStrings 0 = [""] There is exactly one lawful string of length 0, the empty string. So far, so good. Now the recursive trick of pretending that we know the answer for `n - 1` while writing the answer for `n`. I'm using the list monad here, but you could do the same with plain old `concatMap` (remember that the list monad does exactly that): lawfulStrings n = do -- we're pretending we already know how to make lawful strings: str &lt;- lawfulStrings (n - 1) -- inspect the lawful tail we've received to figure out valid -- prefixes: prefix &lt;- case str of -- starts with a '0': can only prepend '1' ('0':_) -&gt; ['1'] -- starts with a '1', or is empty: can prepend either '1' or '0' _ -&gt; ['0', '1'] return $ prefix:str **HOWEVER.** The problem statement does not require you to actually list the answers; it just wants you to calculate how many there are. Also, the code I gave you gets really slow at string lengths of 30 and up, so using it to solve for n = 104 is a no-go. If you look at the solutions for the first 10 lenghts or so, however, you should notice a pattern: the Fibonacci sequence. [This page](https://wiki.haskell.org/The_Fibonacci_sequence) gives you an exhaustive list of possible implementations in Haskell, with explanation, ranging from naive through efficient to esoteric. Pick your poison.
You can do this with a bit of induction. Create a function that, given a string of length n that obeys the rule, create all the strings of length (n+1) that obey the rule. Now start with an empty string. Apply that function to it. You now have all the strings of length 1. Apply the function to all those strings. You now have a list of lists of all the strings of length two. Concatenate to a single list and recurse until you have all the strings of the length you want.
Once you've done that you can start to see the patterns. Each string will either have one or two 'children'. [] -&gt; [0,1] = 1 [0,1]-&gt; [01,10,11] = 3 [01,10,11] -&gt; [010,011,101,110,111] = 5 [010,011,101,110,111] -&gt;[0101,0110,0111,1010,1011,1101,1110,1111] = 8 In each new list there will be the same number that end in '1' as there were in the previous list. But there will only be as many that end in '0' as there were in the list before that, because it takes a string ending in '0' two generations to have a child ending in '0' again. So f(n) = f(n-1) + f(n-2) Which is the fibonacci sequence. That was an amusing ten-minute diversion. Thank you. 
[removed]
ByteString isn't really a string, it's just poorly named.
Thanks for sharing that video, always love to see breakdowns of more in depth setups so I can steal what catches my eye.
Hmm when I write some code in Haskell I'm never trying to make it "get into the space real good" or whatever, I'm trying to write code that allows me to learn and code that helps me do my tasks without annoying bugs. Specifically one annoying bug in ML is matrix dimensions and this comes over to Haskell but things are so bad in Python that you can run a program and then be notified that you used a variable that you didnt even bind ever in the program and have everything crash. It's terrible to get that result after 10 minutes of actual CPU labor. I'll use as many type tricks as I need to get rid of the errors I hate honestly, but I agree that using more makes no sense.
For a basic HTML5 multi-select box: {-# LANGUAGE OverloadedStrings #-} module MultiSelect where import Text.Blaze.Html.Renderer.Pretty import Text.Blaze.Html5 import qualified Text.Blaze.Html5.Attributes as A myCountries = [("IE", "Ireland"), ("IT", "Italy"), ("JP", "Japan")] myMultiSelect = select ! A.multiple "multiple" $ mapM_ renderCountry myCountries where renderCountry (code, name) = option ! A.value code $ name render = renderHtml myMultiSelect `render` will generate &lt;select multiple="multiple"&gt; &lt;option value="IE"&gt; Ireland &lt;/option&gt; &lt;option value="IT"&gt; Italy &lt;/option&gt; &lt;option value="JP"&gt; Japan &lt;/option&gt; &lt;/select&gt; Edit: There's a bit of type inference going on in the example above. If `myCountries` had a type of `[(String, String)]`, you'd have to convert the types like so: myCountries :: [(String, String)] myCountries = [("IE", "Ireland"), ("IT", "Italy"), ("JP", "Japan")] myMultiSelect = select ! A.multiple "multiple" $ mapM_ renderCountry myCountries where renderCountry (code, name) = option ! A.value (stringValue code) $ toHtml name
"n" is the length of the bitstring, not the maximum binary interpretation of the bitstring. length $ replicateM n "01" &gt;&gt;= guard . not . ("00" `isInfixOf`) Not this is going to be far too slow.
Turning on [type families](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#type-families) or [GADTs](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#generalised-algebraic-data-types-gadts) (neither of which implies the other) implies [mono local binds](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#ghc-flag--XMonoLocalBinds).
Ah, I see. Having read the monomorphism restriction of the Haskell report more carefully, does the restriction *not* apply here then because `check` has a function type?
Yes. ByteStyring is actually a stream of bytes and each "character" is an ascii char no UTF8 like Data.Text.
Watched a bit of the monad transformer talk and finally realized that the idea behind transformers is super simple. If we nest transformers we can't abstract over composition using our existing definitions anymore. If we instead compose the monads themselves and make sure that `t = m (n (...))` is a monad we just get the normal `(b -&gt; t c) -&gt; (a -&gt; t b) -&gt; (a -&gt; t c)` kleisli arrow stuff! And then we can just implement everything as transformers and use Identity or IO at the end and abuse typeclasses to lift to the correct layer. Looking at it that way monad transformers are the most obvious solution to compose monads. Anyway, not sure if all that theory is the ideal way to start wit haskell. For some people it probably works out great but there probably are also a lot of people that don't have the same determination as you and who would give up half way through out of sheer frustration.
I'm probably the wrong person to ask as I avoid TH like the plague (I like my compile times too much). With respect to the manually written instances though, I think future me appreciates this boilerplatey variety still for a few reasons: - If I'm using multiple default values in one function, I'd suspect that using Default would require explicit type annotations. At this point, I don't find `def :: Port` any less boilerplatey than `defPort`. - Speaking of boilerplate, you still have to write the instances, in which case, writing a function requires slightly less boiler plate than writing a type class instance. - I often think of type classes as a way to abstract over some common, but not universal idiom. The only operation I can perform with a Default instance is def, which just returns an inhabitant of some type, which doesn't strike me as very helpful without accompanying laws. What I mean is, what does saying `Default a =&gt; a -&gt; b` tell me that `Port -&gt; b` doesn't? I can't think of too many useful functions that work on _any_ default value, after all. Please tell me (anyone) if I've missed something Default gets me other than a cute, reusable function name.
Seeing this, I was reminded of my previous question (https://www.reddit.com/r/haskell/comments/5ttycg/weird_error_in_ghc_compilation/) Seems like almost every time someone thinks that there is a bug in GHC, the real problem involves the Monomorphism Restriction. :)
Huh? You are erasing from the type the information that the result must be an integer. It's not an arbitrary constraint. It's conveying useful information at the type level.
Well, it has improved. Prelude&gt; :{ Prelude| data Status = Running | Start | Stop Prelude| Prelude| x = 100 Prelude| y = 300 Prelude| Prelude| add :: Int -&gt; Int -&gt; Int Prelude| add x y = 2 * x + y Prelude| Prelude| Prelude| prompt :: String -&gt; IO String Prelude| prompt question = putStr question &gt;&gt; getLine Prelude| Prelude| :} It wasn't possible to paste multiple chuncks of code like this in older versions of ghci. $ stack exec -- ghci --version The Glorious Glasgow Haskell Compilation System, version 8.0.1 But the approach of ;; double semi colon would be better. The Idris REPL runs new chuncks of code in this way too.
It is much more updated than RWH with respect to modern Haskell techniques, like lenses. I don't know Haskell from first principles too much, but looking at its ToC it looks that my book gets into more advanced topics like GADTs.
I'd also recommend looking at [Lucid](https://hackage.haskell.org/package/lucid/docs/Lucid.html) which IMO is a slighly better templating DSL for HTML.
How to contact team administrator for an invitation?
Appreciate you writing this, thanks! I would love to do shows more often. It's not easy. There's a way you can help and get episodes earlier. If you're interested: https://gist.github.com/filipovskii/f12685bc74a425ba651c736fb5e3e5ae
I don't have time just yet but you can use this function to integrate Lucid templates: https://github.com/arianvp/scotty-lucid/blob/master/Web/Scotty/Lucid.hs Just add `lucid` to your cabal dependencies and `import Lucid`. The template code will look very similar to Blaze HTML but with less boilerplate. Here are some examples: http://chrisdone.github.io/lucid/Lucid.html
If you have trouble with the infix operators, there is sort of a loose logic associated with the vast majority of them so even if you see a particular infix operator for the first time, you can make a pretty intuitive guess at what it does. ----- - `.` - Single Value - `..` - List of Values - `?` - return value wrapped in Maybe (replaces the `.` in an operator) Used when viewing things that can fail - `?!` - Identical to `?` except instead of Just/Nothing wrapper, it provides the plain value and throws an exception in the Nothing case (a.k.a. unsafe version of `?`) - `@` - Indicates that you will be passing an index (used for working with list-like structures) - `^` - View the value - `~` - Edit the value - `=` - Edit value in the current state type - `&lt;` - Return new value in a tuple - `&lt;&lt;` - Return old value in a tuple - `%` - Modify with a user supplied function There is also a bunch of 'standard' functions that have infix operators built by default. - `||` - Boolean or - `&amp;&amp;` - Boolean and - `-` - Subtraction - `*` - Multiplication - etc. In the form of: ` `/`&lt;`/`&lt;&lt;` + One of those standard functions + `~`/`=` which makes up the bulk of the operators defined just by making one for every combination. So for example, when you see the infix operator `&lt;-~` You can break it down into three pieces: - `&lt;` - Result will be a tuple, the left value is the the new value, the right value is the updated structure - `-` - Doing plain old subtraction - `~` - The result of that subtraction will be used to set the value So for a random couple of Lenses from [Control.Lens.Operators](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Operators.html) so you can see the pattern: &gt; ( a , b ) &amp; _1 -~ c ( a - c , b ) &gt; ( a , b ) &amp; _1 &lt;-~ c ( a - c , ( a - c , b ) ) &gt; ( a , b ) &amp; _1 &lt;&lt;-~ c ( a , ( a - c , b ) ) &gt; ( a , b ) ^. _1 a &gt; ( a , b ) ^? _1 Just a
For more reason WHY this happens, check out https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tldi10-vytiniotis.pdf
Heehee: This works: numNoZeroes n = round $ ((a**nf) - (b**nf)) / (root5) where a = (1.0+root5)/2 b = (1.0-root5)/2 nf = fromIntegral n root5 = sqrt 5 Do you know why?
Down for me **EDIT:** u/jwiegly could you look into this? I'd love to read your article, but even for newartisans.com I see only an nginx test page. **EDIT2:** And suddenly it works
-EDIT- Forgot about the 'of x length' restraint. After giving it some thought: import Data.Bits import qualified Data.Vector.Unboxed as V valRange :: Int -&gt; [Int] valRange 0 = [1] valRange 1 = [1,2] valRange n = let upper = (2^n - 1) lower = (2^(n - 2)) in [lower..upper] noTwoContig :: Int -&gt; Bool noTwoContig b | (b == 0) = True | (b .&amp;. 3) == 0 = False | otherwise = noTwoContig $ (flip shiftR) 1 b V.length . V.filter noTwoContig . V.fromList . valRange $ n
That's a pretty light dependency footprint there.
&gt; The flag `-XMonoLocalBinds` is implied by `-XTypeFamilies` and `-XGADTs`. You can switch it off again with `-XNoMonoLocalBinds` but type inference becomes less predicatable if you do so. (Read the papers!) Interesting. Anyone want to try and give a tl;dr on this? What is meant by "less predictable" here?
About boilerplate, Default can often be derived generically (there's indeed a default implementation). It's also possible to add options to tweak the behavior for some irregular fields, though I know of no existing implementation of this. I admit it's not the most compelling argument ever.
Thanks! Looks like that fixed the code I was having trouble with. I forgot that you had to turn that on.
For sure. The ReadP parser combinator module was designed as a successor to parsec meant to be very fast with almost-deterministic grammars by eliminating backtracking.
Yep.
The tl;dr is something on the lines of "complete and unrestricted type inference in the presence of lots of additional features is really hard". I don't personally know exactly what specifically makes things hard in these circumstances. [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.3144&amp;rep=rep1&amp;type=pdf) is the paper on it. For one example where lack of MonoLocalBinds break things (due to Overlapping instances instead of TypeFamilies / GADTs, so Overlaps and friends should probably enable MonoLocalBinds). Look at my stackoverflow question [here](http://stackoverflow.com/questions/38880858/moving-function-call-into-where-clause-breaks-type-checker-with-overlappinginsta).
I seize the chance to ask: if I want to parse an Integer using ReadP, first I have to use `reads :: Read a =&gt; ReadS a` and then convert it with `readS_to_P`. Is there a simpler way?
That would work, but would be very slow (`readS_to_P` should only be used as a last resort due to terrible performance huge memory use). For arbitrary types implementing Read, you can use `readPrec_to_P readPrec 0`, which returns the internal ReadP used to create the Read instance (the precedence value is necessary due to infix constructors). For positive integers, there is a specialized version `readDecP` in `Text.Read.Lex`.
To extend slightly on great awpri's answer the problem is known as [Ticket #367](https://ghc.haskell.org/trac/ghc/ticket/367) which recently got a few words in the [user's manual](https://git.haskell.org/ghc.git/commitdiff/4c55f14d0ab0f2b099c475a9810012f645bb059e) **-fno-omit-yields** is one of the workarounds to insert rescheduling point into non-allocating loops.
At the expense of completely ruining your ability to reason about coherence. =( For a single top level system configuration that shall never, ever, be changed, it works okay. For anything else, it is pretty dangerous. I keep meaning to demote that to a `reflection-extras` package.
Say you're on the other side and you're making the class. If I suspect that instances might require some additional context, would it be better to bake those facilities into the class itself, or to keep the class clean but require reflection for extra context?
I told you to look at the library functions that you called, look at the equivalent code in Scala, figure out what it gets compiled to, and then it would answer your question as to "Why does this in Haskell run slower than this other thing that does sort of the same thing in Scheme" (which you DID ask - if you don't believe me, read what you wrote). I'm asking you to manually parse your code correctly to figure out exactly what it's doing, but if you can't parse my comment to figure out what I was trying to tell you, then you should probably just give up programming altogether.
You forgot the screenshot! :)
Is there any guide on how to use https://perf.haskell.org/ghc/ and how to make sense of the number and graphs?
[removed]
Yep. The only thing that kind of guides it at all is that there are some rules about using the "newer" one that make sure `ImplicitParams` get plumbed correctly, but when you merge constraints from different contexts, which you get is very much an artifact of the implementation, not any specification.
I'm afraid not at the moment; I pretty much hacked it up over a weekend out of frustration with having no long-term visibility into GHC's performance characteristics. I've been meaning to return to it for quite some time now; unfortunately it never makes it to the top of the pile. The gist of it is that you select the branch you want to examine in the right pane. Then select the tests you would like to plot on the left. Basic substring filtering on test name is provided by the text box at the top of the tests list. After selecting some tests (and a rather long wait due to the rather [inefficient queries produced by Postgrest](https://github.com/begriffs/postgrest/issues/621)) you should see some curves plotted in the middle. Below the plots there is a tabulation of commit-to-commit changes above the indicated relative change threshold. These large-delta commits are also marked with arrows in the plot.
Is this measuring how fast GHC compiles or how fast the code emitted after compilation is?
It takes data from our `nofib` benchmark suite, which measures both characteristics.
About `pure`, it's `return` that you can get for free. About `&lt;*&gt;` and `fmap`, I can't imagine how they could be already defined. Since you must create an instance for `Applicative` and `Functor` before creating a `Monad`, they can't depend on `&gt;&gt;=`.
So, this this what I'm looking for (the section titled "Compile times"): https://gist.github.com/bgamari/ff4592ae89c5b7d7751180b967e55590#file-nofib-2016-06-txt-L2987-L3572 And the stats at the bottom of the section claim that the compilation times have been degrading since 7.4.2? Would anyone be able to explain why this perf-degradation been happening consistently with every new GHC release? And what can others (non GHC hackers) do to help.
It's more of a string than Text, or String. The problem is there is plenty of insane mixing everywhere with those concepts, so when one comes into a language where they are completely separated confusion is ensured.
Interestingly, this is what `liftM` (or `liftA`) and `ap` are for. Also return defaults to equal to pure now, so if you only care about modern GHCs you can use: import Control.Applicative instance Functor (State s) where fmap = liftM instance Applicative (State s) where pure x = State (\s -&gt; (x,s)) (&lt;*&gt;) = ap instance Monad (State s) where State rs &gt;&gt;= f = ... And if you don't you can always write the extra `return = pure` line. Why isn't telling it the Monad instance enough to get the superclass members for free? Well, what would happen if we had a similar class, `Comonad`? And knowing `Comonad` was also enough to tell you how to define `Functor` for the same type, `fmap = liftW`. That would also want to fill in the same definition but differently. (We do, and it does want to.) What about something completely different, like Traversable. If I can `traverse` with any `Applicative` effect, then I can traverse with the `Identity` effect. `fmap = fmapDefault` w/ `fmapDefault` from `Data.Traversable`. `Functor` is a superclass of `Traversable` because of this relationship. Also, this doesn't address the case that sometimes the Monad will have additional constraints that the simpler instances won't. instance Functor ((,) e) instance Monoid e =&gt; Applicative ((,) e) instance Monoid e =&gt; Monad ((,) e) so liftM :: Monoid e =&gt; (a -&gt; b) -&gt; (e, a) -&gt; (e, b) is less useful than fmap :: (a -&gt; b) -&gt; (e, a) -&gt; (e, b) And finally, these definitions are law abiding, but not necessarily the most efficient. The `liftM` definition above for `(,) e` is going to effectively take whatever value you have for e and turn it into 'mappend e mempty'. This is equal to e, assuming your monoid is legal but now there is an extra computation step. For some monoids this computational effort may be non-trivial. For `Functor` at least you can use `deriving Functor` and the `DeriveFunctor` extension as the "best" legal definition is easily algorithmically derived, but the pattern above holds for all sorts of other cases that don't have special tricks baked into the compiler.
Didn't know about that extension. Seems very handy.
Maybe this will help. Here are two profunctors that I've stumbled across in real life: [Foldl](http://hackage.haskell.org/package/foldl-1.2.3/docs/Control-Foldl.html) and [Colonnade](http://hackage.haskell.org/package/colonnade-1.1.0/docs/Colonnade.html). Disclaimer: I wrote the second package.
&gt; Would anyone be able to explain why this perf-degradation been happening consistently with every new GHC release? There is no simple answer to this. Compiler performance changes due to a variety of reasons, * avoidable regressions due to lack of attention to performance in implementation * new optimizations which require effort by the compiler, but in exchange produce better code (e.g. the new join points work) * new features which simply require more of the compiler (e.g. `TypeInType`, type-indexed `Typeable`) Also, there are a few reasons outside of GHC itself which weigh in here, * users increasingly relying on deriving and generics, both of which tend to produce significant quantities of code which require quite some effort from the compiler to make fast. * libraries being peppered with `INLINE` and `INLINEABLE` pragmas, which demand that the compiler do work (e.g. reading unfoldings from interface files, performing the inlining, and then simplifying the resulting program) * users compiling with `-O1` and `-O2`, regardless of whether optimization is desired To be clear, I'm not suggesting that these are problems; I'm merely pointing out that only a fraction of the slow-down that users report is due to GHC itself and there is no such thing as a free lunch. &gt; And what can others (non GHC hackers) do to help. Contribute performance testcases with minimal dependencies. While I've been trying to improve visibility into the testcases (with the tool above and the introduction of per-pass timing output in `ghc`) that we have, few of these cases are representative of modern Haskell code. Until this is resolved it is hard to know how much of the slow-down reported by users is due to GHC and how much is due to changing usage of the compiler. There are several reasons for the "minimal dependencies" requirement, * `nofib` can't use `cabal-install`, complicating installation * upstream libraries can shift beneath us, compromising result stability * Maintaining the benchmarks themselves carries a significant cost which we would like to minimize One option that has been floated in the past is to periodically collect a small set of widely used libraries and their transitive dependencies and freeze them. This set could be maintained as a stable benchmark collection. However, this would require someone to step up to do the work.
&gt; And the stats at the bottom of the section claim that the compilation times have been degrading since 7.4.2? One other point: wall times (as seen in the `Compile times` section of the `nofib-analyse` output I linked to) tend to be rather unstable. It's generally better to look at allocations (the next section in the output), which generally co-varies with time.
&gt; If `nofib` compilation is consistently getting slower, is adding more compiler performance test cases really going to help? Will more tests improve compiler performance? Of course not. Nothing other than working on the compiler itself is going to improve compiler performance on `nofib`. Will more tests help GHC developers better target efforts to improve compiler performance? Absolutely. Moreover, it's hard to overstate the importance of this: performance issues tend to be very time-consuming to track down and there is a lot of educated guesswork involved in choosing which paths to follow. Having more evidence regarding what performance issues actual users are seeing is invaluable.
Right! But not the whole story. The actual reason ReadP in particular is used is that it could be made compatible with the original type ReadS. Which means that hand-written, backtracking parsers using ReadS can be freely (and invisibly) mixed with ReadP parsers. This was needed for backwards compatibility. The compiler generates ReadP parsers when driving Read, in which users' own ReadS parsers can be used with only paying a local performance penalty. (For example when parsing a list of objects of type MyType, for which the user has written their own ReadS function.) I wish ReadS had been an abstract type, so compilers could have chosen even better implementations than ReadP, but this is not the case. Users should really use ReadP everywhere, even for manual instances. But not everyone knows about it, and it would not be compatible with other compilers. 
One more update for anyone who's curious, after more investigation we found the slowdown was due to how PostgreSQL's query planner treated a join query in this particular application. Adjusting a database setting (cpu_tuple_cost) allowed the database to better utilize indices. This removed the delay that we previously thought might be caused by either PostgREST's use of CTEs or in-database JSON serialization.
What I meant is that since there is already a benchmark the devs are happy to run that reliably shows a problem, maybe a better use of contributor time would be something like helping with the tooling that you mentioned you don't have time to work on. If we get to the point where `nofib` looks great, but users are experiencing an issue, then there would clearly be a pointed need for contributions of repeatable tests that exhibit the problem.
This is one of the common variants of proposals out there. It still has some ambiguities. They are maybe not insurmountable, but they do seem to get in the way of a "perfect" solution. Ultimately, the issue you get is that folks find that whatever you'd give them it won't cover all the things that are missing in the current design space. It is really common to want to be able to automatically define all the instances for superclasses that aren't already specified, e.g. instance Monad Foo where fmap = ... return = ... (&gt;&gt;=) = ... and just have it plug in Functor and Applicative as it goes. You could invert something like the default signatures syntax to give default definitions to the superclass members getting defined this way. It even has the benefit that code defined in such a manner doesn't have to be as tightly 'knit' as the stuff using the existing backwards default signature scheme! This has a number of very good implications for code reduction for narrow deep class hierarchies. e.g. We could quietly break Monad into Semimonad and Monad, leaving 'return' in Monad. Modulo some imports, nobody would care! All existing instances would continue to compile. All existing types would continue to work. This sort of thing would be completely awesome for refactorings that are otherwise rather awkward, like the Semigroup-as-a-superclass-of-Monoid changes that are going through slowly as we speak. But it isn't without faults. data Foo a instance Traversable Foo instance Monad Foo Keep in mind instances aren't read top-to-bottom, but we parse the module, then run them. Now Functor has to come from one of those two definitions and it isn't clear which would be the supplier. (Foldable and Applicative would also be generated) Keep in mind you can define data Foo instance Monad Foo instance Applicative Foo instance Functor Foo so it isn't known until you've gathered all the instances in the module if anything is really missing, and in the first case, which one supplies the missing superclasses is up for discussion. So now the proposal picks up warts to try to be explicit about which instances are supplying which superclasses, but you need to be careful. Then there is the problem of the conditional theories stuff I mentioned. As your deep lattice of classes grows _wider_ you're going to run into more problems, as the 'who supplies this superclass method' problem becomes more and more common as you get more paths up through the class hierarchy to the same point. Richard Eisenberg has a variation on this proposal that is good for this sort of thing, but its bad at the splitting off of superclasses. Mainly because he'd prefer to be explicit, so unpatched code would be a twisty maze of loud warnings. But if you're being more explicit about the superclasses being simultaneously defined then this proposal doesn't help with the "split class" problem. User code still has to change for us to refine the "real" class hierarchy. Right now you have a lot of different proposals from folks who all have different agendas, most of whom are tired of the discussion anyways, and some of those agendas are mutually exclusive. And none of the solutions if "obviously perfect." This is a pretty good path to paralysis. I personally want some way to be able to refine a class hierarchy without breaking all user code. I think that is the major barrier to building good _accurate_ abstractions in Haskell today. Few people are going to go define instances for Traversable1WithIndex because they have to define TraversableWithIndex, FoldableWithIndex, Foldable1WithIndex, Traversable1, Foldable1, Traversable, Foldable and Functor instances as well. If the definition of all of those was a single definition using the most specific combinator? People would. But note, that example doesn't fit the single argument "all instances have the same shape" pattern because whatever index type is also included in some of those instances, just like the `MonadReader` example I gave earlier, so your final overly explicit alternate syntax doesn't work. 
As far as I know there's not one on hackage. But, I think you should be able to make a "NormedTreeMap" that has O(lg n) lookup, but you'll probably lose most of your sharing and I think have to deal with O(n) insert.
&gt; NormedTreeMap This is what I'm currently trying, but I'm stuck at "how do I put this tree map in normal form in a remotely efficient way?" The best I've come up with is pulling all the keys, sorting them, then creating a new tree by traversing that list and putting each key from the old tree into the new one. And that sounds embarassingly bad!
I see HaskellDB inside of there. Did you debate any alternatives before choosing HaskellDB? (I'm generating for MYSQL, or else I would look a little deeper at Opaleye.)
I can't even think of a way to have a type list with a normal form. How do you order arbitrary types?
You could store the tree in a Braun tree. You'd still have to pay O(n) to rebuild the Braun tree after each insertion, but it'd give you a O(log n) time lookup, and a unique normal form. This has the benefit that it "looks" like the kind of tree you are looking for. It is all balanced and stuff. They go back to a paper by Braun and Rem from 1983, but the idea is that the left sub-tree at every level is always _exactly_ the same size as the right sub-tree or exactly 1 element larger. This prescribes things so that there is precisely one tree for every given number of elements, while trees are always of minimum height. The latter property gives you the O(log n) lookup time, and the minimum height property is a nice hard guarantee. The highly prescriptive nature of the shape also means you don't have to store any meta-data in the tree to work with it! Conversion to and from a list of elements is able to be done in linear time. This gives you an O(n) construction you can adapt from https://www.eecs.northwestern.edu/~robby/courses/395-495-2013-fall/three-algorithms-on-braun-trees.pdf There are plenty of other structures though with the same asymptotics, but they all have that unfortunate cost that inserting into them may have to rebuild the whole thing. Other examples of such structures with a unique normal form are the skew binary random access lists Okasaki talks about in Purely Functional Data Structures or the [Leonardo random access lists](https://www.schoolofhaskell.com/user/edwardk/fibonacci/leonardo) I mentioned in an article a while back. These don't have an arbitrary 'insert' operation, but I do have a nice algorithm for O(log n) drop on either one. They give you an O(1) cons operation, which can be used in the fromDiscinctAscList and insertion algorithms. These have logarithmic (but not minimal) height, and lean funny. In practice what you probably want is something a bit different than what you asked for: 1.) A "sloppy" tree form you use when inserting elements, which doesn't have the nice unique shape property that you want. 2.) And then a normal form you convert to for equality comparison or for when you tag something with the set of items you've inserted. This lets you avoid paying linear costs for each insert, just one linear cost at the end after a bunch of logarithmic time inserts, turning the construction of an n element set from an O(n^2 ) time operation to O(n log n). All you need now is to write a type level discrimination library, and you can get construction down to O(n). ;) Though, that would need type level mutable arrays and ST. o_O Yikes. Nevermind.
You need to have a ordering on the types, which means picking up some other kind of constraint you can employ like a type family for (&lt;=) with the unchecked property that it correctly provides the total order you want.
I started, but never finished or released, a library for syntax-safe, type-safe and type-inferred interaction with postgres via TH of SQL expressions [0]. The idea is that you declare your tables in Haskell, and then write raw SQL that fails at compile-time if your syntax is wrong, or the return types or interpolation variables have the wrong type. Sort of a much safer `postgresql-simple`. Extensible records (I used `bookkeeper`) turn out to be the perfect representation for tables and query types, in my opinion. I won't have much time to work on it, but if someone wants to lead developement, I'm certainly able to help. It already works for some basic things, and shouldn't be all that much more work. [0] https://github.com/turingjump/forum
Are you proposing `round :: (Num a, Num b) =&gt; a -&gt; b` ? or `Num a =&gt; a -&gt; a` ? The later doesn't allow to convert to an Int. I'm not sure if the former is always possible (but I am probably wrong).
&gt; In that last little bit I mentioned, where you should probably have two structures, one normalized, and one not I'm sorta doing this I think; at least, I'm delaying normalisation as long as possible and not doing it on each insert. My multiply op on units is basically `x * y = normalise (mergeUnitTree x y)`, where `mergeUnitTree` does the work of inserting/adding, so I'm at least not performing normalisation on every single insert. I'll read up on these structures and see if I can get anything better.
What exactly will the use-case be? This is something I spent some time thinking about (which is not to say I've come up with a great solution), since it's what the core of [`bookkeeper`](https://hackage.haskell.org/package/bookkeeper) is . The currently-released version is pretty inefficient, since it checks that every operation (e.g. insert) is in fact taking as argument a type-level map in normal form, and returns one in normal form. The big insight that /u/fizruk pointed out to me during Haskell Exchange was that it is instead much more efficient to never try to prove to the compiler these normal form facts. Instead, you just expose an API that always maintains normal form (e.g., `insert` inserts in the right place, and removes duplicate keys). This made a *huge* difference. Previously even maps with ten keys led to slow compile times. Now maps that are as large as I imagine myself using compile fairly quickly. The only remaining problem is that the user should have access to a type function that converts a map into normal form, which in this case mostly means sorting. And that gets slow for maps with ~ 60 keys (ugh! so funny to imagine computers having a hard time sorting such small lists!). I still haven't released the improved version, since I had to wait for 8.0.2 (8.0.1 had a bug). I guess I could do it now...
I don't think it has to be *quite* that bad. I'd end up wanting to do some dependently typed stuff to make sure I didn't flub the implementation. But, basically, there's only one place to grow the tree, and you have to figure out the correct element to put there on the downward pass (leaving "holes"), and then fix up parents and siblings on the way back up. There's a "dumb" way that should be fairly easy to implement, but gives O(n) inserts because it might end up traversing the majority of the tree while fixing things up. I'm not sure, but there might be a "smart" way that gives O(lg n) insert but I'm not 100% sure that even exists.
&gt; Still, I'm still wary of this "as large as I imagine myself using", as being constrained often limits the ideas that you think of in the first place. Absolutely. On the other hand, optimizing for use-cases you haven't imagined can often be a waste of time! That said, there is always the possibility of using a compiler plug-in. I'd imagine that making everything much faster.
I made a proof-of-concept relational library that allows you to expression the tables in the type system. https://github.com/stepcut/relatable The example usage starts here: https://github.com/stepcut/relatable/blob/master/Relatable.hs#L163 Here we define two tables, `User` and `Todo` -- | 'User' Schema type User = Book '[ "userid" :=&gt; Integer , "username" :=&gt; Text ] -- | 'Todo' Schema type Todo = Book '[ "todoid" :=&gt; Integer , "ownerid" :=&gt; Integer , "todo" :=&gt; Text ] Here is a complex query: -- | find all the todos for userid == i projectTodosFor :: Integer -&gt; Query Tables [Project '["username", "todo"] (Cross User Todo)] projectTodosFor i = querySt (Project (AddKey #username (AddKey #todo NoKeys)) (Select (Eq (VField #userid) (VLit i)) (Select (Eq (VField #userid) (VField #ownerid)) (Cross (Table users) (Table todos))))) This is a bit ugly because I am using the constructors directly. A little bit of sugar would help clean it up. In relational notation that query would look more like: _{#username, #todo} (_{#userid == 1} (_{#userid == #ownerid} (users  todos))) In my case, I opted to use [acid-state](https://hackage.haskell.org/package/acid-state) to implement the backend. But it would be easy to adapt to generate SQL instead. The nice thing about this implementation is that it prevents you from doing stupid things like projecting fields that do not exist. This is 'simple' in the sense that it is only a few hundred lines of code. But it is also incomplete as it does not implement some of the more advanced relation algebra operations yet. Nor does it have a query optimizer. Though, in theory, the types should help ensure that the optimization rules are correct.
Hmm. It's interesting how classes work out both as constraints on the values of some type as well as constraints on the types themselves. I always expected we'd need some kind of "lifted" version of typeclasses.
 ExBound : Type -&gt; Type ExBound = Lowest : ExBound k | InBound { bound : k } : Exbound k | Highest : ExBound k instance Ord a =&gt; Ord (ExBound a) where compare (InBound l) (InBound r) = compare l r compare Lowest Lowest = EQ compare Highest Highest = EQ compare Lowest _ = LT compare _ Lowest = GT compare Highest _ = GT compare _ Highest = LT Tree : k -&gt; v -&gt; ExBound k -&gt; Nat -&gt; ExBound k -&gt; Type Tree = Empty : {lb &lt; ub} -&gt; Tree k v lb Z ub | NIL { left : Tree k v lb n p, val : v, right : Tree k v p n ub } : {lb &lt; p} -&gt; {p &lt; ub} -&gt; Tree k v lb (S (n + n)) ub | NIR { left : Tree k v lb (S n) p, val : v, right : Tree k v p n ub } : {lb &lt; p} -&gt; {p &lt; ub} -&gt; Tree k v lb (S (S (n + n))) ub That formulation should at least guarantee correctness and normality, though it would certainly admit the `fromVector . sort . ((k, v) ::) . toVector` approach, so it doesn't guarantee performance.
That is the Braun heap I was suggesting as your non-normalized intermediate form. It satisfies the heap property, and gives a heap with log time operations, the shape of the tree is fixed, the placement of the values is not in order, so it isn't a binary search tree.
(from [another thread](https://www.reddit.com/r/haskell/comments/5vz9sw/maplike_data_structure_with_normal_form/de6951q/)) &gt; Braun tree Well, that's my ignorance of the literature shown. That's the name for a good version of whatever I posted.
I am proposing: round :: (RealFrac a, Num b) =&gt; a -&gt; b Which is always possible due to the `fromInteger` / `fromIntegral` functions. 
The following paper seems to be related. https://www.microsoft.com/en-us/research/publication/using-destination-passing-style-compile-functional-language-efficient-low-level-code/
Yep! It is basically spinning the path it takes through the tree as it walks down. It is an absolutely gorgeous algorithm.
The end run around the open world policy is to forbid orphans and exploit the partial order you get on modules based on which modules import which other modules. From there, for the simple non-flexible single parameter type class case, you can show that it'll always be one of the modules or the other, based on this import order and which winds up importing the other transitively. Dealing with multiple-parameters start to expose some limitations with the design of Haskell-style module systems, and cabal-style package management. "Necessary orphans" can be fixed by some changes in this area, but again the language starts to look very, very non-haskelly. In the more interesting dependent type setting where you're slowly accumulating proof obligations of equivalence of, say, liftM and liftW for something that is both a Monad and a Comonad but only when the context for both is available, then it is interesting to me that the proof obligations have precisely one module to live in as well.
?
I'd love to debug this, as you're not the only one having troubles. The IPv4 for the server is 208.82.102.85, and the IPv6 is 2607:f2e0:f:712::2.
I was actually saying the opposite. If you want to use functions with types, you need a separate construct: type families. To use classes with types instead of values you don't need some lifted version of typeclasses. 
There's also [hexml](http://hackage.haskell.org/package/hexml), which I use for parsing all day long. It is significantly faster than Hexpat for my use cases. 
The commit where most of the work seems to be done: https://github.com/rainbyte/frag/commit/e379b620a2ae1209d6e5a7dd209ec9d1562b4b5a I think it's really cool that a "newbie" can take a project like this and bring it up through two GHC major versions. Well done.
The API seems much better than the original process. If it didn't have too many dependencies (conduit in particular), it may be a very good standard replacement.
Good job! It is nice to see this live again.
Another important difference is that intellij-haskell doesn't support Windows.
If you inlined all of the dependency code, you could just release a new major version and existing projects *should* be pinning their versions appropriately. Seems like something Haskell needs in its core, because as you say, the current has annoying cross-platform bugs. *Just imagine how many bugs you're going to fix across the board by fixing bugs in a core part of Haskell.* Since it is easy to pin the version, I would support merging the changes into `process` using the existing module name.
Finally, I understand contravariant functors! Thanks! 
&gt; I also never understood why it doesn't have a "void"-type member Just an oversight on my part really. Maybe I'll add one in the future. It turns out not to be especially useful though.
This is the composable type-safe API I've always wished process had. This example was rather confusing until I dug into it. It's using a lazy ByteString, but all stderr gets buffered before withProcess returns, AIUI. So, if the process outputs a lot of errors, the consumer uses a lot of memory. err &lt;- withProcess_ procConf $ atomically . getStderr putStrLn "\n\n\nCaptured the following stderr:\n\n" L8.putStrLn err Why use lazy ByteString at all, when it's fully buffered? (PS, Data.ByteString.Lazy.Char8 is super bad in most uses cases, and using it in examples is not a good idea, as it will lead to people using it in production.)
I originally ran Bake almost in production at one previous job, but haven't used it since then. Bake is definitely cleverer than other CI solutions, but I'd probably avoid it if any of the following hold: * You don't want to help develop it. * Your tests run in &lt; 10 mins. * You can throw compute power (e.g. AWS) at your tests. * Your development team is &lt; 5. It has a niche, and I think that niche is actually most mid-sized development teams in large organisations, but it doesn't have the niceties that more established things like Jenkins have, and it doesn't really have any active development or community to speak of.
You don't need type families, or at least they don't need to be syntactically distinguished from functions.
Good idea! I wanted to see how it turned out first, but now the code is settled down, I'll send a pull request with what I need!
&gt; It builds on top of the veritable process package **venerable** process package? 
&gt; In addition, a few people have raised questions on the process issue tracker whose simplest answer is IMO "use typed-process." Am I the only one who sees this as a hackage tragedy? Someone comes to trackage, investigates `process`, but there is *no way* the user can understand that `typed-process` is superior. - No "see also" section - No comments - No stars - No reviews - No errata or known issues No actionable information for deciding on whether another more useful package exists. 
Pssk the compiler does most of the work anyways. 
Very happy to see this. The question of how to communicate this to the general community (via notes or depreciations, etc.) is a tough one, but it's been tough for a long time. I'd love to see better answers to that in general.
That's true in the sense that it would be near impossible to do in some scripting language. But a human still has to do the research to find out why things don' t compile. That's a good amount of work if you don't know about it already.
 &gt;If I could do what I do for Opaleye via Generics then I would prefer it, but I don't think I can. Generics has problems with two type parameters. Why is that a problem? I did not mean that the profunctors should be generic. only the data handled by it, which is how one-liner uses Generics. 
Hi cocreature, can you ping me on IRC in #haskell, and we'll get to the bottom of this? I have a feeling it's a DNS issue. I had some hiccups when I switched to the current server a few months ago.
I really like what you've done here /u/snoyman, it's a great way of enriching a fundamental building block with the structure of types. My vote is for adding a mention to the `process` page on Hackage, or proposing the addition of `System.Process.Typed` (or something similar) to the libraries mailing list.
The real question is why does Haskell implement `Eq` for floats when the very first thing you learn about them is not to compare them for equality? To be fair, this criticism can be made of every language.
Nice, looking forward to it.
&gt; Am I the only one who sees this as a hackage tragedy? Maybe, but if it is it doesn't need to be solved by Hackage. We just need a site http://www.use-this-haskell-package-instead.com which maps defunct best replacement. (cf http://haskelliseasy.com)
Thanks! That worked: https://hackage.haskell.org/package/product-profunctors-0.8.0.3/docs/Data-Profunctor-Product-TH.html (I find Haddock extremely confusing)
At this point I don't investigate anything on Hackage and haven't for years. The index is too big and the categories too vague to find anything. Now even the haddocks are on Stackage. My only interaction with Hackage is to upload packages, and if I could upload them directly to Stackage I would do that instead. 
Do you mean type checking? If so, that is usually done on the ast after you have done your parsing, so I'd suggest changing `parseProgram` to return a `Program` on success and doing the type checking after that. (As an aside, errors are normally put in the Left constructor with Either, so it's probably worth making that change. If the error type stays the same, it means you can use the monad instance / do-notation to easily compose together functions that might fail, with the end result returning the first error or the eventual success.) For the type checking, I'd recommend looking at `MonadReader` and the `local` function. Unless you're dealing with something about as fancy a substructural type system (linear types etc), you can often get away with using `local` to manage the context (which variables have which types). After that, I would usually write something like: type Context = Map Identifier Type inferInstruction :: (MonadReader Context, MonadError TypeError) =&gt; Instruction -&gt; m Type inferInstruction = ... checkInstruction :: (MonadReader Context, MonadError TypeError) =&gt; Instruction - &gt; Type -&gt; m () checkInstruction = ... along with similar functions for the other pieces of your AST. These will be recursive functions, and will potentially mutually recursive (often you write `check` as `infer` followed by a test for equality between the inferred type and the argument to `check`). You can often get away with defining `inferInstruction` by cases - one case for `WhileLoop`, one case for `ForLoop` etc - although that is definitely easier if you have `inferExpression` written first. I'm on my phone at the moment - if you want more details / clarification / further info let me know and I'll do what I can when I'm near my laptop. Edit: if the `MonadReader` / `MonadError` thing is new to you, the above functions are equivalent to inferInstruction :: Context -&gt; Instruction -&gt; Either TypeError Type checkInstruction :: Context -&gt; Instruction -&gt; Type -&gt; Either TypeError () except you don't have to pass the `Context` around explicitly or check for errors at every step. 
Since the author of the package is the maintainer of `process` its easy enough for them to add information regarding it to the documentation, in this case.
The discussion seems very interesting. Would you mind explaining the use case a bit more. 
Haha, I thought this comment was a bit harsh and then I noticed the author. Straight from the horses mouth.
Any insight on how other compilers, say rust, purescript, golang, or elm solve for this issue?
Cool :) BTW I have Archlinux and an nvidia adapter, and it failed with Configuring OpenGLRaw-3.2.4.0... Cabal-simple_mPHDZzAJ_1.24.2.0_ghc-8.0.2: Missing dependency on a foreign library: * Missing C library: GL This problem can usually be solved by installing the system package that provides this library (you may need the "-dev" version). If the library is already installed but in a non-standard location then you can use the flags --extra-include-dirs= and --extra-lib-dirs= to specify where it is. I just followed the adivce and added `--extra-lib-dirs=/usr/lib/nvidia` to `stack build`.
Agreed. Just to be clear - by design xml-conduit represents the entire XML as tree structure in memory. For use cases where you need that, xml-conduit is very fast. It's a great library. But if you want fast streaming processing of a large XML, you need a SAX-style parser.
amazing! Thank you very much for setting up the bounty and thanks to the people involved in making this happen.
Those languages do not have all the type-foo-bars that GHC has. Add in TH, rewrite rules and you have enough to keep the compiler engaged for some time. Any compiler that agressively optimises is going to be slow (See for example mlton or stalin). GHC is not a whole program optimiser like mlton for example but with rewrite rules and inlines it is pretty close. 
I see but do you mean to say that one can now put a computation in the type field Like for example append :: Vector n a -&gt; Vector m b -&gt; Vector (n + m) b Is this possible in GHC? 
If I understood the bookkeeper usage correctly the `KeyMap String Type` will be at the type level. If complicated functions are unpleasant how does /u/jkarni handle combining of fields etc. GHC is able to do that computation ? 
re 2: you can't do that? you get the GHC_PACKAGE_PATH env warning.
just to have complete context, are you learning haskell along with maintaining this code base? what is the LOC of the codebase and what's the pr problem domain?
i'm not exactly learning haskell from scratch while maintaining - took a coursera course last year and i've read through most of LYAH. but yes i've never shipped haskell before. you can take a look at the code yourself since it's [open source](https://github.com/databrary/databrary).
On IDEs, if you're a VIM person, intero + spacemacs (not vanilla Emacs) is *definitely* for you. Try reading the spacemacs docs. I switched from vim to spacemacs specifically for Haskell.
* Cabal hell (of course) They claim they've solved that with cabal new-build, but I've never tested it. Nix is pretty cool too. * Stack purgatory No idea, don't use it. * No IDEs I've never used an IDE to program in any language. I use Emacs. Not sure whether that makes me lucky or unlucky. &gt; So if I change something somewhere that's a defered computation I have to get its type exactly right (no coercion or duck typing) but I have no idea what it'll end up being because I have no idea where the computation will actually end up being run What? Can you give an example? That doesn't make any sense to me. &gt; I can't distinguish function identifiers and arguments just by looking Sure you can. The function is the leftmost thing. &gt; User defined operators Meh. Sometimes they're a little bit too much. Sometimes they're a godsend. If you spot one that's the former, file an issue. &gt; So many purity hacks Umm ... they're not hacks. &gt; Inconsistent whitespace rules No clue. Emacs indents things the right amount for me. Rarely had a problem with layout rules. &gt; Single letter variable names. Are you nuts? They're great! &gt; Dog slow compilation times Meh &gt; Dead packages http://haskelliseasy.com &gt; TH shenanigans. This codebase I've inherited does I/O during compile time Often inadvisable, often critically useful. If you spot a bad usage file an issue. What's wrong with what postgresql-typed does? &gt; Compile time sys paths No clue about this. 
Definitely would recommend picking up "Haskell from first principles" and plowing through that like a motherfucker. If you've inherited the codebase you want to get up to speed on Haskell asap and that's the route of least "I just spent 5 hours fucking around on 2 lines of code because the learning exercise was too vague" that I've seen out there so far. It'll take you through every in lyah, cis192, and the NICU course (probably spelled that wrong. They're linked on the "how to learn Haskell" GitHub repo everyone constantly references). Lyah is also considered old and varying degrees of inaccurate; one of the pain points of the language for sure. Hell, I'm still writing c++ -ansi code for my programming class right now; you can't even write 5 year old Haskell code half the time anymore. Still, it's usually for the better (from what I've seen).
Just to say what powerful types and abstrations can do to compile time https://arxiv.org/pdf/1401.7694.pdf Granted not compilation time but whatever.
Haskell plugins for Atom work very well with both cabal and stack. A little trick to help with the general types is to specialize locally by adding type signatures to intermediate variables. You can also use type application in ghci to fix type variables. Finally, have a conversation with GHC don't just treat it as some final step. For example, use typed holes to see the type of the missing pieces.
The shortest I can find is [here](https://github.com/databrary/databrary/commit/24b56064966878123584e5028f18ce8cb60f6f43#diff-7577c1cb950f6c487202033979aea10dR21). Why do I need `liftIO` there? Why can't I just use the bind sugar directly? Because the first argument to `webRegenerate` isn't run until later (in its body) and so the type of `nodeModulesPath` without `liftIO` is monad transformer (because bind attempts to be polymorphic/is polymorphic).
&gt;Sure you can. The function is the leftmost thing. umm what about function composition? &gt;What? Can you give an example? That doesn't make any sense to me. The shortest I can find is [here](https://github.com/databrary/databrary/commit/24b56064966878123584e5028f18ce8cb60f6f43#diff-7577c1cb950f6c487202033979aea10dR21). Why do I need `liftIO` there? Why can't I just use the bind sugar directly? Because the first argument to `webRegenerate` isn't until later (in its body) and so the type of `nodeModulesPath` without `liftIO` is monad transformer (because bind attempts to be polymorphic/is polymorphic). &gt;Often inadvisable, often critically useful. If you spot a bad usage file an issue. What's wrong with what postgresql-typed does? it hits a postgres instance during compile time to simultaneously create types and pg enums. so i can't decouple compilation from having a running postgres instance (which is playing havoc with deploying on docker). &gt;http://haskelliseasy.com lol there are like only 20 libraries there.
| Leksah isn't maintained Leksah seems pretty actively maintained. The last commit was [5 days ago](https://github.com/leksah/leksah/commit/441e6aedb1807f26a3ed828466678472b1135df4) and the last I looked it had a graphical debugger.
I'll give you my take in a point-by-point manner. Often times this is me saying "huh, problem?" and trying to talk through my work style, hope that help or gives us a starting discussion. &gt; Cabal hell Extremely rare for me, particularly when the project has few dependencies. My MO is to create (or clone) a project, `cabal sandbox`, and `cabal install`. If you are relying on a global package database for many large projects I could imagine this problem cropping up more. Also, when I was learning there was no cabal and therefore no cabal hell. Depending on what you are doing to learn this is an entirely valid path. (I later see you are thrown into the deep end of the pool, which is itself a problem) &gt; Stack purgatory. I generally don't use stack and don't know what stack purgatory is. Pick one of the two tools and learn it more completely, to suit your needs, before deciding to not know or like two tools ;-). &gt; What seems like the perfect storm of unmaintainable features ... if I change something somewhere that's a deferred computation I have to get its type exactly right You always have to have things well typed - re-examine your implication that this has anything to do with lazy evaluation. If you aren't using explicit type signatures frequently enough then an expression with an unexpected type can be attributed as a type error somewhere much different - use more type signatures, it helps with readability anyway. &gt; I end up having to understand completely all of the type classes my type implements and exactly where the thunk gets run. But you don't say _why_ you believe you must understand this. I don't tend to care what instances exist for a type so long as the instance I'm using exists. Can you provide an example of when this is necessary? EDIT: Your comments keep pointing out a use of `liftIO`. This is an issue with you making well typed expressions. For example. you have `f :: IO a` and want to call it in a context that requires `m a` where `m` has `MonadIO` instance. So lucky you, you can say "I want to escape the confines of `m` and do some arbitrary IO" but you do actually have to know the expected and type of the expression and the actual type of the procedure you wish to call. &gt; I can't distinguish function identifiers and arguments just by looking. Really? It's a matter of position . `a b c` - there `a` is the function applied to `b` and `c`. `d e f \`symbol\` x y z` there `d` is a function, `x` is a function, and `symbol` is a function used in infix notation. Parsing Haskell should not be a difficulty; perhaps some interactive discussions on IRC can help clear up any pain points? EDIT: Your comments mention functions-as-arguments. Those functions aren't being applied at that point, they are just arguments so why are you trying to distinguish the types of your values? You can use editor integration to inspect the types if you so desire, but determining which function is being applied should be trivial. &gt; I can't tell which are the functions in an expression because I have no idea where they end. Where the expression ends? Can you link to an example where this issue crops up. Come to think of it, a lot of these "I can't read Haskell" comments could benefit from examples and more discussion. &gt; User defined operator ... like ?$&gt;...? What are the semantics of that? As you noted, it is user defined. I often avoid libraries that over-use crazy operators, especially if the semantics can't be extracted from the haddocks. You're the programmer, which libraries you use are up to you. &gt; And if it's not imported selectively I have to go looking in each wholesale import for its definition. Or use better editor integration of things like ghc-mod, hdevtools, etc. These tools can tell you from whence any symbol originate. Also you could just `cabal repl` and `:info symbol`. &gt; Inconsistent whitespace rules. Any amount of indentation to indicate scope is fine except for with lets. Why??? Is that a rule consistency issue or just a flexibility you don't like because it means there is less style consistency? Either way, this does fall into a "reading Haskell" issue and I understand how that can be frustrating but is also something that can be worked though. &gt; Single letter variable names. Are you nuts? Heh, I've ran into this a good bit recently with code translated from papers where the code took the paper's symbols exclusively. OTOH if the variable doesn't actually have any further meaning is this an issue? For example, `go (x:xs) = f x : go xs` - it is clear to most Haskell programmers that there is no information about `x` beyond it being some element of a list and `xs` being a list of possibly many elements. &gt; Dog slow compilation times. I understand this is a nontrivial problem but still. Yep yep, I'm with you. Consider `repl` or explicit use of `ghci` to avoid that in the normal case. Also `new-build` avoids some of the unnecessary compilations but certainly isn't an end-all. Finally I saw below you are using (or packages that are using) significant TH which as dramatic negative impact on compile time - feel free to avoid that or factor it into a module that won't need recompiled frequently. &gt; Dead packages. So many packages that haven't been updated in years. Like this (yes I know globals...) where the only thing that needs to be done is version bumps. It should be removed from hackage or something. This is a big community issue and if you have culling suggestions or UX then I encourage discussion here or on cafe@h.o. &gt; TH shenanigans. This codebase I've inherited does I/O during compile time. How is that even possible??? More importantly why should that even be possible. For the curious the culprit is postgresql-typed. Whoa whoa whoa, it sounds like you jumped (or got thrown) into the deep end way too quickly. I don't think I'd like to maintain a larger Perl code-base right now, not remembering any Perl, and suggest if you _want_ to learn Haskell (vs need to maintain a code base for pay) that you start somewhere simpler. &gt; Compile time sys paths. I think the Paths_package module is a terrible hack solution to the configuration problem. Maybe no one uses but if someone does and the next person wants to get away from it (like me) it's really annoying. I'm curious what your alternative is here. Just CPP and a packager-provided directory a la apt? Environmental variables for the "win"?
&gt; &gt; It's nothing to do with anything being deferred. &gt; Tell me how that should be clear from that block of code? Well, why would it be to do with something being deferred? &gt; Case in point: if WebGenerator implemented IO I wouldn't need to lift. It does implement IO, which is why you *can* lift.
&gt; Because the first argument to webRegenerate isn't until later (in its body) No. &gt; and so the type of nodeModulesPath without liftIO is monad transformer No. &gt; (because bind attempts to be polymorphic/is polymorphic). No. Your Haskell knowledge seems quite messed up. May I suggest you to settle down and actually be patient with learning a completely different language? It is normal for one learning Haskell to feel like he/she is learning programming from scratch. And it is, I dare to say, *impossible* for a imperative programmer to just expect to 'take up' Haskell and start going, whereas you might be more than ready to maintain a Java project with a good amount of C++ knowledge. And of course, it's impossible to be productive with something you are completely unfamiliar with. If you cannot learn Haskell properly, my advice is that yes, rewrite the entire codebase in Go.
Cute name. I'll give you that, but since I had never heard of it I just assumed OP meant Shake but had a typo. :)
&gt;Extremely rare for me, particularly when the project has few dependencies. My MO is to create (or clone) a project, cabal sandbox, and cabal install. If you are relying on a global package database for many large projects I could imagine this problem cropping up more. The first thing I did was ask on #haskell what the deployment story was and someone said either cabal sandbox or stack. I chose stack because sandboxes still play with global installs (and so I could get into cabal hell). &gt;Where the expression ends? Can you link to an example where this issue crops up. Come to think of it, a lot of these "I can't read Haskell" comments could benefit from examples and more discussion. I can't tell where the function ends because I don't know the number of args it takes. Now I just realized in thinking about responding to you that `a b c` can't be `a` applied `b` applied to `c` because function application has highest precedence but for example `$` plays havoc with that. So `a $ b c` does work and in an expression with many `$`s it's easy to get lost. For example if `f :: a -&gt; String -&gt; c` then `f x $ do getStr` is confusing. &gt;Or use better editor integration of things like ghc-mod, hdevtools, etc. These tools can tell you from whence any symbol originate. Also you could just cabal repl and :info symbol. I'm in vim. It doesn't make sense to jump to the terminal to run whatever commandline tool to find the file and line number then jump back to vim. I've been doing it (actually haskell-vim-now with ctags works fine) but it's painful. &gt;x:xs Of course for example like that I wouldn't complain; for (i = 0; i &lt; 10; i++) I don't complain about either. How about [this](https://github.com/databrary/databrary/blob/master/Databrary/Web/Generate.hs#L53)? &gt;Whoa whoa whoa It's fine. I'm a big boy and I can handle it but I'm arguing that shouldn't be allowed. I can't decouple compilation from a service that's a part of the app. How does that make sense even if I'm a haskell expert? &gt;I'm curious what your alternative is here. Just CPP and a packager-provided directory a la apt? Environmental variables for the "win"? The obvious answer: globals.
&gt;Well, why would it be to do with something being deferred? Because I can't tell what's happening from the block of code. I have to know about webRegenerate. &gt;It does implement IO, which is why you can lift. This is now the opposite of what was explained to me in #haskell. The explanation I was given was that the IO monad is at the bottom of the stack and what I'm actually lifting is that computational context.
&gt; I've ran into this a good bit recently with code translated from papers where the code took the paper's symbols exclusively. I love me some pretty latex symbols, but I've always been mildly infuriated when people replace haskell operators with a more mathematical one in papers. My brain gets used to the ascii version and doesn't recognize the more mathy one as the same operator. Kind of a weird reverse skeuomorphism or something.
&gt;No. No what? I'm wrong? The first arg to webRegenerate isn't lazy? As far as I can tell (by the absence of `!` in the definition of webRegenerate it is. &gt;No. I assure you that if you remove `liftIO` what the right hand side of that `&lt;-` will be is a monad transformer. You are welcome to remove it and compile. &gt;No. Again no what? I'm wrong? bind isn't polymorphic? Any monad (implements `&gt;&gt;=`) can use `&lt;-` right? That's not the definition of polymorphism? &gt;Your Haskell knowledge seems quite messed up. My knowledge isn't quite messed up - it's incomplete. Despite what everyone would have you believe there's nothing really confusing about monads, functors, etc. My argument is that the way that Haskell glues them together is messy. &gt;It is normal for one learning Haskell to feel like he/she is learning programming from scratch. I do not feel that way. I feel like I am fighting a convoluted system that's convoluted simply for the sake for formal "beauty". &gt;If you cannot learn Haskell properly, my advice is that yes, rewrite the entire codebase in Go. This summarizes my point very well actually if you substitute *completely* for *properly*, and I might agree, but I haven't encountered another language (except maybe C++) where I have to know the entire language before I'm productive.
&gt; I'm wrong? You're *not even* wrong. The objections you are making have nothing whatsoever to do with the problem you're having.
As a Nix user and contributor, I wouldn't recommend Nix if you don't plan to invest quite some time into getting used to it. It really requires a big investment, and with the current toolset (`stack` or `cabal new-build`) it isn't that necessary anymore. To use Nix effectively, you need to learn a whole new language, and documentation is *even worse* than Haskell's. 
&gt; TH shenanigans. This codebase I've inherited does I/O during compile time. How is that even possible??? More importantly why should that even be possible. For the curious the culprit is postgresql-typed. "*To help ensure safety, it uses the PostgreSQL server to parse every query and statement in your code to infer types at compile-time.*" So that's pretty much the whole purpose of `postgresql-typed`. It sounds a lot of your complaints aren't really with Haskell but the choices the developer of the project you are working on made. &gt; Granted I'm a novice - only three weeks. That's pretty much nothing. You have to realize the reason it's so easy to jump between languages like Python, Ruby, Go, Lua, etc is because they're pretty much just "skins" for the underlying imperative core. Haskell is a *substantially* different paradigm. You can't think about solving problems the same way as you can in imperative languages - it's very much about types, data flow, etc. A metaphor I like to use is that Haskell is a lot like designing a regexp, you think about how data flows and is transformed not a set of steps like a recipe. It took me substantially more than a year before I considered myself productive. After 5 or so years with it as my main programming language, I wouldn't consider myself an expert. If you want instant gratification like jumping between imperative languages, it's probably not for you. &gt; No parens for function application, currying, and 9? different levels of fixity. So I can't distinguish function identifiers and arguments just by looking. Plus you guys hate parentheses like they beat your mother or something and use pointfree style. Stuff like this will get a lot easier with practice. Tools that show you the types of symbols are also very useful. If you know the type of a symbol, it'll be pretty obvious how it can be used even with stuff like currying in play. &gt; But honestly how can this ever be a productive language? Well, obviously there are people that are productive in it, large projects solving complex problems, etc. You seem to be assuming that since *you* aren't productive after 3 weeks that it's impossible. Now, it absolutely is true that some of the problems you listed are not easy to fix with like slow compilation times. However, there are ways of dealing with them, such as relying on the interpreter, etc. Reloading modules in GHCI is much faster than compiling. If you doing test compiles, it's worthwhile setting up Cabal flags to turn down optimization, disabling compilation through LLVM if you use that (it's quite slow), etc.
To clear up that `liftIO` thing: the reason you need / don't need it has nothing to do with laziness. In fact, both cases use `liftIO`: * in the `webRegenerate` case, the `liftIO` is done inside of that function: https://github.com/databrary/databrary/search?utf8=%E2%9C%93&amp;q=WebGenerator * in the new code, you don't call `webRegenerate` anymore, so you'll have to call `liftIO` yourself The reason why liftIO is necessary is that the `do` block is of type `WebGeneratorM something`, but you want to do IO, which has type `IO something`. Because `WebGeneratorM` is an instance of `MonadIO`, you can use `liftIO :: IO something -&gt; WebGeneratorM something` to "lift" the IO action into the WebGeneratorM monad. 
I'm not entirely familar with them, but I don't believe that they *require* it. But that's one of the few cases where Nix is actually superior to the current Haskell tooling, because those packages require complex *non-Haskell* dependencies (in particular, `ghcjs` and `webkit` for the `ghc` backend of `reflex-dom`). I'm not sure if `stack` is able to manage `ghcjs` for you yet, but at least with cabal, you'll have to go through the process of installing `ghcjs` yourself, with `Nix` it does that for you. 
&gt; Granted I'm a novice - only three weeks. Learning Haskell with a background in imperative programming can be a bit like learning programming all over again. It's often best to forget as much as you can about how to program in order to ease the process. In that context, I'd say give it more time. After all, if you had never programmed before at all, you would probably be quite confused by a similar Java codebase after only three weeks. Hope it works out!
Great to hear back from this thread, thanks a lot for this note. The attack criticism is interesting and I'm excited to read about how they avoid it.
&gt; I have to know about webRegenerate Not really. The type of `generateStylusCSS ` (which you are defining) is given as `WebGenerator`. `WebGenerator` is a type synonym for `(WebFilePath, Maybe WebFileInfo) -&gt; WebGeneratorM Bool` (*this* is the part you have to know). The definition starts with `generateStylusCSS fo@(f, _) = do `. So from the type above we know that the parameter `fo` has the type `(WebFilePath, Maybe WebFileInfo)` and the `do` block on the right must have the type `WebGeneratorM Bool`. And now there is a simple rule: if a `do` block has some type `m a` then every expression on the right of `&lt;-` must have a type of the form `m b`. Then, the identifier on the left gets type `b`. In this case, every expression on the right of `&lt;-` must have as type of the form `WebGeneratorM b` for some type `b`. `Conf.get "node.modules.path" &lt;$&gt; Conf.getConfig` has type `IO String`, which is not of this form.That means, it cannot be used as an expression on the right of `&lt;-` directly. The way forward is to use a function, which converts a value of type `IO String` to a value of type `WebGeneratorM String`. `liftIO` does exactly that. This has nothing to do with `webRegenerate`. And it certainly has nothing to do with lazy evaluation or some value being deferred. It is just a result of strict typing.
The answer to many of your questions is that it comes down to experience: short variable names, operators, Haskell syntax, types and monadic effects are all massively useful once you get the hang of using them. Parens vs `$` is a great case study: it's might have been weird at the beginning, but after I acclimated to it, code with `$` became both easier to read *and* to write than code with lots of nested parentheses. Parentheses add unnecessary noise to your expression, and you have to mentally keep track of how things nest. `$` avoids the nesting problem and gives your expressions more syntactic room to breathe. After you've seen and written code with `$` enough times, it gets so intuitive you won't even notice it any more: I take it completely for granted now. Part of it is mastering your tools: I use Emacs, Nix and cabal and, while it took some up-front setup, it's an incredibly productive environment now. Some O(1) work and learning for a massive O(n) payoff. A few of the other things are real problems, where we're productive *despite* real issues, largely because Haskell is such an expressive language. Long compilation times, bad build systems, missing libraries and poorly maintained Hackage packages are all legitimately annoyingbut worth it to use Haskell or, equivalently, to not use not-Haskell. The TH thing is a total niche. I suspect what it's doing is along the lines of reading database schemas to generate types and codeno different from ORMs and a reasonable thing to do.
&gt; &gt;Sure you can. The function is the leftmost thing. &gt; &gt; umm what about function composition? If there is an operator, the operator is the function. So in the expression `f . g` the function is `.` and the arguments are `f` and `g`. And in the expression `(f . g) x` the same is true and also `f . g` is a function and `x` is its argument. Do you have a specific example of an expression you have problems with? I find Haskell's syntax for function applications quite regular, many other programming languages have a more complex syntax.
&gt; The obvious answer: globals. Is it that obvious? If I gave a global path of `\Docs and Settings\My App\data` would everyone be happy?
&gt; Single letter variable names. Are you nuts? Not at all. Often you're writing a function where the only think you know about an argument is that it belongs to some type classes. What would be a sensible name for the patterns to a function like `mconcat :: Monoid m =&gt; [m] -&gt; m`?
global set at runtime.
Name was not my idea - Andy Adams-Moran came up with that. Really Shake and Bake have the names the wrong way round, but too late by the time I was writing Bake to fix that :)
You can use the `--no-ghc-package-path` flag to avoid that.
Still nothing to do with laziness. It is connected with delaying evaluation, but only accidentally, and any type system issues are unconnected.
&gt; &gt; Sure you can. The function is the leftmost thing. &gt; &gt; umm what about function composition? What about it?
Regarding your edit: let's look at a simplified code, because there is much unnecessary noise. generateStylusCSS1 = do path &lt;- liftIO $ Conf.get "path" &lt;$&gt; Conf.getConfig webRegenerate (callProcess path) generateStylusCSS2 = do let ioAction = do path &lt;- Conf.get "path" &lt;$&gt; Conf.getConfig callProcess path webRegenerate ioAction Your confusion might resolve (or increase), if I show you a third implementation: generateStylusCSS3 = do let ioAction = do path &lt;- Conf.get "path" &lt;$&gt; Conf.getConfig return path path &lt;- liftIO ioAction webRegenerate (callProcess path) This last implementation is equivalent to the first one. You were told that the difference between `generateStylusCSS1` and `generateStylusCSS2` is that the config lookup is done immediately with the former and deferred with the latter. This is correct, but it is not because of `liftIO`. Neither does `liftIO` force anything, nor does `let` have anything to do with deferring. You need `liftIO` in the first case, because you are in the context of `WebGenerateM` (which is disguised by `WebGenerator`). In the second case, you are in the context of `IO` (`ioAction` has type `IO ()`), so you don't need it. So in part it has something to do with `webRegenerate`, but only with its type. If `webRegenerate` would take a parameter of type `WebGenerateM ()` instead of `IO ()`, you would still need `liftIO`.
&gt; Why do I need liftIO there? Let's look at [the type signature for bind](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.9.0.0/Control-Monad.html#v:-62--62--61-): (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b The key point is the three `m`s in that type signature. Every use of `m` has to be the same type. In your example, `m` is `WebGenerator`. So the type signature is: (&gt;&gt;=) :: WebGenerator a -&gt; (a -&gt; WebGenerator b) -&gt; WebGenerator b Without the `liftIO` you were doing (&gt;&gt;=) :: WebGenerator a -&gt; (a -&gt; IO b) -&gt; WebGenerator b ...which doesn't type check. I talked about this more in [a talk I gave awhile back](https://vimeo.com/59215336).
/r/Haskell gets its comeuppance...whew, that is a hefty list of grievances.
I'd say my cases are a bit mixed: - CI does include long-running jobs - Compute power available is so-so - Teams range from &lt; 5 to &gt; 5 depending on projects I'd be interested in developing, but my haskell skill is caveman-level and this would be for work, where stopping a task and bugfixing isn't ideal. So it sounds like maybe it's not the right route for my use cases.
Ah, ok, I haven't really used `stack exec cabal` but I'd kind of expect to run into problems with that approach. If you have any questions about how to achieve things with `stack` I'd suggest the `haskell-stack` tag on StackOverflow A cursory reading of https://docs.haskellstack.org/en/stable/GUIDE/ might also help.
[removed]
Nice! Not to derail, but for a similar approach, see also `org-drill` for `org-mode` in Emacs.
It sounds like it would suit you, if you had development skills within the company or wanted to gain them. That said, a CI server isn't the easiest project to start with.
&gt; I've been wrestling with an inherited haskell code base for 3 weeks... part of the reason I took the job was because I would get to learn haskell. You took a job as maintainer of a code base because you "would get to learn haskell"? And now you come here and rant about how awful everything is? People are being very gracious with you.
Thanks to the type system the 'different levels of fixity' are less of a problem than in C: if you guess the precedence wrong in Haskell, the compiler will likely complain.
You mean explicit semicolons and curly braces?
Yeah. They are in the language to pacify some complaints (and to make life easier for automatic code generators)---but nobody ever uses them. ;)
You can also get running pretty quickly with atom editor plus ide-haskell and stack install the following: - ghc-mod - hlint - stylish-haskell
&gt; if you're a VIM person I don't see someone who prefers vim being happy with atom. Atom is perfectly fine if you haven't learned vim or emacs, though even it's useful to learn one of the two.
You might find *Programming in Haskell* a better fit. I evaluated both that and *Haskell Programming from First Principles* for teaching a Haskell class at work, and they seem to cover the core ideas equally well but in different styles, with *Programming in Haskell* adopting a significantly terser style.
&gt; For now is there a way to just remove stack from my system. Simply delete the exectable and the `~/.stack` folder.
You need to learn the basics. Are you in either USA or Australia?
Wouldn't MonadState be more appropriate here since you actually need to modify the state to keep track of which variables are in/out of scope, as well as keep track of variables of the same name that have different types in different scopes?
&gt; `Integral`. Not integer. `Integral` is the Haskell class that represents the notion of a mathematical integer in any of its various machine representations. So the exact semantics of the type round :: (RealFrac a, Integral b) =&gt; a -&gt; b is: "`round` is function which maps a machine representation of a real number to a machine representation of an integer." This is exactly the correct type, with exactly the correct amount of generality. &gt; And since Num is a superclass of `Integral`, the second type is strictly more general and powerful. That is a *non sequitur*. More polymorphic types are not strictly more powerful. They lose information at the type level, and in that sense are weaker. &gt; You also don't lose the information that b is an integer value, as the only values that inhabit `Num a =&gt; a` are integers. Due to the type of `fromInteger :: Num a =&gt; Integer -&gt; a`. `fromInteger` has nothing to do with it. That just means that there is required to be a map of `Integer` into the type; that map is neither required to be surjective nor injective. The only reason `Num a =&gt; a` has any inhabitants at all is because of the types of Haskell's built in numeric literals, which is a side issue. And anyway, the inhabitants of a type at the value level do not determine its meaning - and therefore usefulness - at the type level.
I will be in Boulder in May, teaching the basics. I do it for my job in Australia. Want to catch up?
I'm responding to you because you wrote an earnest and sincere reply, so I'd like to be earnest and sincere in kind. &gt;It's a journey, not a destination. Most of us are here because we enjoy the journey. If that's not you, then you might be better off switching back to go. It was this line that made me concede. As I mentioned below I took the job because I was interested in Haskell and the journey. But this is strange to me &gt; That would be a loss for the Haskell community Do you feel this way because adoption of Haskell has been weak? It's odd to me that the hopes and dreams of a community might rest on the success or failure of some arbitrary codebase. It's also silly to me, if it is in fact the case that Haskell is in such a precarious position that my codebase matters, that some people in this thread are dismissive of pointless hurdles. I'm going to illustrate what I mean by taking bits from your responses but plenty of others exist here: &gt;This tells me you're not thinking about the language in the right way. We have no coercion or duck typing because that destroys the whole point of having a powerful type system. I'm not at all confused about the point of a type system - note I said go and not python because I want to work in a language with types. I was merely saying that in this particular instance, when types aren't explicit and extremely strong, it's difficult. &gt; When the compiler gives you a type error, it's because your code doesn't make sense. It wouldn't make sense if you wrote it in Ruby or Python either. I would never argue to the contrary. I've written code in C, C++, Java, and Python. Only one of those has a weak type system. It's a little disingenuous to willfully misconstrue what I meant in order to paint me as someone who's completely clueless about the advantages of a type system. What I was saying was determining what the correct type at that moment wasn't natural - for example the compiler error was something about monad transformers - not WebGenerator. &gt;This is completely wrong. It has nothing to do with when the thunk gets run. This has been cleared up for me but you have to admit confusing deferred evaluation and laziness isn't so far-fetched. &gt;That's because your brain isn't used to thinking about it in the right way. This is simply not true. Please tell me how easy [this](https://github.com/databrary/databrary/blob/master/Databrary/HTTP/Path/JS.hs#L25) is to parse? Have you gotten so used to it that you can tell me immediately what's happening? How about the rest of that function? What's a reasonable amount of time for how long it should take to understand it? Comments like that one of yours and &gt;You can't think about solving problems the same way as you can in imperative languages - it's very much about types, data flow, etc. A metaphor I like to use is that Haskell is a lot like designing a regexp, you think about how data flows and is transformed not a set of steps like a recipe. from /u/Vulpyne are condescending. I have no problem with functional idioms - map, reduce, filter, lambda, transformation over mutation, recursion. Those are not the problems I'm having. Notice none of my gripes were about the kinds of problem people coming from imperative languages usually have ("where's the for loop", "where's the branch", "where mutation"). &gt;I'd suggest you take a step back. Regroup, adjust your expectations. Write some code, when you don't understand something, ask a pointed question about a specific issue. The community is overwhelmingly helpful. Once you understand that issue, wash, rinse, repeat. You'll make more progress this way than by venting your frustration with vague rants. It's true I was ranting out of frustration. I admit it. And it's true this thread is itself a reflection of how helpful the community is, and one of the reasons I'm reconsidering the rewrite. The broader question in my mind though is why is it important that this app get written in Haskell (other than for my own learning experience). Am I not first and foremost responsible to the stakeholders of the project to deliver a working application? Is a programming language simply not a tool in that effort? Would you really argue that Go is not a better tool for web than Haskell?
I'm a longtime emacs user (10 years) and I am pretty happy with atom. I go back and forth, honestly. I was responding more to the original poster who seemed to indicate that IDEs are in a terrible state in Haskell-land. Aside from a pretty fully configured emacs, I've never used a full "IDE", though, so I don't know what someone who expects an IDE-like experience is really seeking.
&gt; Please tell me how easy this is to parse? Actually, that is something that you can parse instantly, once you know the trick of it: "tl $ filter (liftM2 (&amp;&amp;) isAscii isAlphaNum) $ show $ typeOf a" is equivalent to "tl (filter (liftM2 (&amp;&amp;) isAscii isAlphaNum) (show (typeOf a)))"; but it's easier to read as a pipeline of functions: "first compute typeOf a, then show it, then filter it, then tl it"
It still has nothing to do with laziness, and I'm still not clear on why you think otherwise. Let binding has nothing to do with deferring computations. Your problem was that you were binding (`&lt;-`), thus using an IO action (`IO ...`), where a `WebGenerator` was expected. Nothing more or less. It's just a type error.
Yep, that's exactly right. Partial application is exactly one of those things that is kind of mindblowing when you first learn about it, but then you start using it *all the time.*
This is exactly what I mean when I say people are missing the point: I have no problem with partial application or currying - I have a problem when people say that `$` is simple - it's just function application of lowest precedence. Something more complicated is happening (a partial application and then application) and it's unclear. Not to mention there are two functions just adjacent to each other right there! `liftM2 (&amp;&amp;) isAscii isAlphaNum` Are `isAscii` and `isAlphaNum` monads? Because the signature of `liftM2` is `(a1 -&gt; a2 -&gt; r) -&gt; m a1 -&gt; m a2 -&gt; m r`
Cute
All Haskell functions take one argument, always, and without exception.
That's because the types aren't identical, not because of laziness. Here's [getConfig](https://github.com/databrary/databrary/blob/24b56064966878123584e5028f18ce8cb60f6f43/Databrary/Store/Config.hs#L72): getConfig :: IO Config And here's the type of [WebGenerator](https://github.com/databrary/databrary/blob/24b56064966878123584e5028f18ce8cb60f6f43/Databrary/Web/Types.hs#L39): type WebGeneratorM a = ExceptT String (StateT WebFileMap IO) a type WebGenerator = (WebFilePath, Maybe WebFileInfo) -&gt; WebGeneratorM Bool So, in your monadic syntax, the type of the containing `do` expression is ultimately `ExceptT String (StateT WebFileMap IO) a`, which is not the same as `IO Config` -- and recall that `IO Config` is the type of the expression `Conf.get "node.modules.path" &lt;$&gt; Conf.getConfig`. So, you need some way to go from `IO Config` to `ExceptT String (StateT WebFileMap IO) a` (otherwise known as `WebGeneratorM a`). That's what [`liftIO`](https://hackage.haskell.org/package/transformers-0.4.2.0/docs/Control-Monad-IO-Class.html) does for you (note the `MonadIO (ExceptT e m)` instance). So `liftIO` here is instantiated as `IO Config -&gt; ExceptT String (StateT WebFileMap IO) Config`. If you squint, you can see that it's "lifting" (that is, taking something to a higher level of abstraction) the argument from the `IO` monad to the `ExceptT String (StateT WebFileMap IO)` monad, thus the name. This took me about 5 minutes to dig through your source, find references on hackage (I don't have any of this memorized, so I had to do a quick google search), and format this reply. If you're not familiar with monad transformers (or more generally, the type level stuff going on here), I'd recommend fleshing out your understanding first before casting judgement. A couple years ago, I wouldn't know what's going on here either, but with study and practice it becomes natural -- the same can be said of, say, meta-programming in Ruby/Python/Smalltalk, Lisp macros, C++ templates, OOP, and just about every other programming skill. Stick with it -- you'll find that it clicks, I promise (as opposed to something like e.g. PHP. where things are inconsistent and require rote memorization of all the weird edge cases in order to get by -- can't promise that'll ever click).
&gt; Please tell me how easy this is to parse? I found it fairly easy to parse. Not instant, but I got there within 30 seconds or so. _But_ I'd add the caveat that I find it to be poorly-written code. The `liftM2` code golf is unnecessary, not to mention the use of single and double letter names everywhere. &gt; Would you really argue that Go is not a better tool for web than Haskell? As someone with experience in both, I'd say it depends on your goals. Obviously 'web' is a very broad category! Go has a leg up with built-in HTTP, though Warp (with WAI) is excellent. Haskell has a leg up in the abstractions you can build, and the amount of confidence you can have when refactoring. EDIT: I should say, I'm sympathetic to your plight. I love Haskell, but so often I see its power used for evil rather than good :p
I don't agree with everything you said but this is a very detailed and informative post so thanks for taking the time to write it up. &gt; --- Strongly typed, lazy, and polymorphic. So if I change something somewhere that's a defered computation I have to get its type exactly right (no coercion or duck typing) but I have no idea what it'll end up being because I have no idea where the computation will actually end up being run (without following the call stack by hand). So I end up having to understand completely all of the type classes my type implements and exactly where the thunk gets run. This one I actually strongly disagree with. If anything, this is good because it stops you from making a mistake. Honestly this seems to be the main point of Haskell, or having a strongly typed language in general. I had to pick up a codebase in Python and I actually really like Python but it was very difficult to maintain because unless you had very good test coverage, anytime you changed something you weren't sure if/where/when it would break. In Haskell if you do something you aren't supposed to, the compiler stops you. &gt; --- User defined operators. Ya'll are crazy for this. Whose brilliant idea was it to let people define operators like ?$&gt;...? What are the semantics of that? And if it's not imported selectively I have to go looking in each wholesale import for its definition. I also hate this. Not user defined operators themselves which are nice for math libraries and the such but when people don't import them explicitly.
Here is a section I wrote on [IDE support](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#ide-support) in the "State of the Haskell ecosystem"
Why is a global variable better than the `Paths_*` solution?
I'm also using Arch, it is really cool :) This does not happen with Mesa OpenGL drivers, it seems to be something specific to nvidia. Thanks for pointing it out. 
Eh, I learned vim, and use it primarily for most editing tasks, but for code beyond around 50 lines I tend to switch to Atom or VSCode now, and for Java I definitely use IntelliJ over vim. Vim's new background processing capabilities may close the gap somewhat, but I find that it's still not _quite_ there for languages where I really need brief docs right at hand. Haskell seems to drive me to docs a bit less, but it's still nice to see the types in the editor as I go.
&gt; Well one reason I feel this was is there many fewer mature web libraries in Haskell. I'm not sure that's enough information to draw the conclusion. Haskell is a fairly small community, so there may just be less users interested in web stuff. Or there's already a web package that does what most people want and they aren't motivated to reinvent the wheel, etc. &gt; Everything else you've said I generally concede on. I'm glad we're reaching an understanding! I'd certainly like to see Haskell become more popular but I think it's important to be realistic about the effort required to get there. And hey, it's taken me years to get to this point. If you could do it in a couple weeks then either you're a genius or I'm a candidate for losing games of mental skill to garden veggies. I'd like to think I'm smarter than the average cucumber.
/u/ice109 PM me and I'll give you my Skype/Google Hangouts/whatever. We can pair program for a bit, you can bounce questions of off me, whatever.
What do you mean by "change where the assets [are]"? Do you mean change their path relative to the project root?
&gt; Are isAscii and isAlphaNum monads? Yes, functions have a monad instance (and yes, that can be very surprising the first time you see it). This... liftM2 (&amp;&amp;) isAscii isAlphaNum ... is a function that takes a `Char`, passes it to both `isAscii` and `isAlphaNum` and combines the two results thus obtained with `(&amp;&amp;)`. In other words, it is the same thing as... \c -&gt; isAscii c &amp;&amp; isAlphaNum c How does that relate to the type of `liftM2`? When you pass `isAscii` to it, the `m` becomes `Char -&gt;`, the type constructor of functions that take a `Char`.^1 Specialising `liftM2` according to the type of the three arguments it is given leads to this type: (Bool -&gt; Bool -&gt; Bool) -&gt; (Char -&gt; Bool) -&gt; (Char -&gt; Bool) -&gt; (Char -&gt; Bool) You can see that process taking place by playing with `:t` in GHCi: GHCi&gt; import Control.Monad GHCi&gt; import Data.Char GHCi&gt; :t liftM2 liftM2 :: Monad m =&gt; (a1 -&gt; a2 -&gt; r) -&gt; m a1 -&gt; m a2 -&gt; m r GHCi&gt; :t liftM2 (&amp;&amp;) liftM2 (&amp;&amp;) :: Monad m =&gt; m Bool -&gt; m Bool -&gt; m Bool GHCi&gt; :t liftM2 (&amp;&amp;) isAscii liftM2 (&amp;&amp;) isAscii :: (Char -&gt; Bool) -&gt; Char -&gt; Bool GHCi&gt; :t liftM2 (&amp;&amp;) isAscii isAlphaNum liftM2 (&amp;&amp;) isAscii isAlphaNum :: Char -&gt; Bool With GHC 8, you can also enable `TypeApplications` and replace the type parameters in `liftM2` directly: GHCi&gt; :set -XTypeApplications GHCi&gt; :t liftM2 liftM2 :: Monad m =&gt; (a1 -&gt; a2 -&gt; r) -&gt; m a1 -&gt; m a2 -&gt; m r GHCi&gt; :t liftM2 @((-&gt;) Char) liftM2 @((-&gt;) Char) :: (a1 -&gt; a2 -&gt; r) -&gt; (Char -&gt; a1) -&gt; (Char -&gt; a2) -&gt; Char -&gt; r GHCi&gt; :t liftM2 @((-&gt;) Char) @Bool @Bool @Bool liftM2 @((-&gt;) Char) @Bool @Bool @Bool :: (Bool -&gt; Bool -&gt; Bool) -&gt; (Char -&gt; Bool) -&gt; (Char -&gt; Bool) -&gt; Char -&gt; Bool (Footnote 1: Note that, as there aren't sections for type operators, `Char -&gt;` is usually written in prefix style as `(-&gt;) Char`, as that is how you would actually write it in a program.)
FWIW I agree with you, but you're picking on a fairly minor problem. It was confusing, but now you know what it means and you can read it easily. It's a one-time cost and once you know how it works, it's easy. It's no different to doing: tl( filter( lift_and(LiftTypes.is_ascii, LiftTypes.is_alpha_num), show( typeof("string")))) in Python. But it's obviously clearer in Haskell. And I say that as a Python programmer. I mean, this is like saying LISP is better than Python because the syntax is simpler. Python's syntactic sugar may require you learn more structures, but it is also what makes it good.
So `cabal` does give you the option to change the directory prefix where you store the assets using the `data-dir` option. However, if you change the asset names or the organization underneath the prefix then I believe both approaches would require you to make code changes
I would recommend Nix + Hydra. Hydra is a bit annoying to set up in various ways, but it pays off. I am pretty excited for [Hercules](https://github.com/hercules-ci/hercules), but it has a long way to go.
&gt; Do you feel this way because adoption of Haskell has been weak? It's odd to me that the hopes and dreams of a community might rest on the success or failure of some arbitrary codebase. It's also silly to me, if it is in fact the case that Haskell is in such a precarious position that my codebase matters, that some people in this thread are dismissive of pointless hurdles. I didn't specify how much of a loss it would be. Just that it would be a non-zero loss. Communities are built one person at a time. If you take over this Haskell project now, a bunch of follow-on things will be more likely to happen. You'll probably learn the language better, maintain the project over time, in the process contribute improvements to other libraries, perhaps discover that you enjoy the language, pass this project on to another maintainer, mentor them in the process, and eventually you might be responsible for bringing Haskell into use in places it otherwise wouldn't have been. So losing you could potentially be a great loss to the community, not because of how much you would hurt the community, but because of how much you could conceivably have helped. It's a bit more noteworthy because it sounds like there's already a significant existing Haskell codebase there, so we're not just talking about some random onlooker expressing interest. &gt; Comments like that one of yours and from [Vulpyne] are condescending. I'm sorry if it came across that way. That was not my intention. There's nothing wrong with not knowing something. The main point of my whole comment was to say that the concepts here are so different you can't expect to pick them up quickly. I can parse that code snippet that you linked to pretty much instantly. This is not because I'm smarter than you, it's because I've been doing it longer than you. This is why I said "your brain isn't used to thinking about it in the right way". It's just not! And that's fine. That pattern exists in no other language you're likely to have used, so I wouldn't expect you to be used to thinking about it in that way. You say you have no problem with functional idioms, but yet you're clearly having enough problems that you took the time to write this fairly lengthy post. And it's clear from all the comments here that you have had significant misunderstandings. My point and that of many other people here is that this is all to be expected. It's not you. It's not Haskell. It's that Haskell is different...much more different than you were probably expecting. I can't tell you whether continuing to use Haskell for this project is the right decision. I hope it is, because I think it would be a benefit to the Haskell community to have more Haskell code in real world use. But I can tell you that if you put a significant effort into learning Haskell, I believe it will pay off for you in the long term. Here's my advice. Try not to get frustrated with the pace of progress. Get comfortable with not knowing things. Re-examine what you think you know. Pretend you're learning programming again for the first time. Put your energy into solving specific problems. Ask for help. Try to work with other people who are more experienced than you. Do this for an extended period of time, and don't compare your progress to what you would have been doing in Go. It will take effort, but if you really want it, you'll get there.
It's not "cute" - that's *exactly* how it works. Like, that's the correct way of thinking about it.
I'm a pretty unsentimental person but you (and others here) have done a good job inspiring me and making this a little more meaningful than just some code. I'll have a conversation tomorrow with my manager and see what he says - because even if I'm prepared to reinvest, at the end of the day, it's a business decision. Thank you.
&gt; it's already been explained that what I confused was deferred computation and laziness Could you elaborate on what you mean here? I ask because, as those terms are commonly understood in the Haskell world, *neither* are an explanation for the necessity of `liftIO` in the example you posted ([the comment in which I explain](https://www.reddit.com/r/haskell/comments/5wb5qw/how_do_you_guys_get_anything_done/de9b7p3/), for reference). It's not deferred computation nor laziness -- the types just don't unify. With another week or so of earnest practice, it'll be as immediately apparent why you need `liftIO` in your example as why `1 + 2 + "3"` won't compile while `1 + 2 + read "3"` will (can't add strings and numbers -- need to parse the string first). &gt; &lt;- doesn't mean thus IO, it means thus monad right? a &lt;- ma b &lt;- mb ...stuff... desugars as `ma &gt;&gt;= \a -&gt; mb &gt;&gt;= \b -&gt; ...stuff...`, and because `&gt;&gt;=` will work for anything that is an instance of `Monad`, yes, the `a &lt;- ma` syntax will work for any instance of `Monad`. *However*, it's *absolutely crucial* that you understand that both `ma`, `mb` and everything else in `...stuff...` ***must*** be *the same* `Monad`. So if `ma` is `IO Config`, the Monad is `IO`, and thus `mb` must be `IO whatever`. In the example you posted, without the `liftIO`, you'd be dealing with two *different* Monad types: `IO` vs `WebGeneratorM`. If you were trying to mix `MaybeT Identity a` and `Identity a` you'd have the same problem (docs: [`MaybeT`](https://www.stackage.org/haddock/lts-8.2/transformers-0.5.2.0/Control-Monad-Trans-Maybe.html#t:MaybeT), [`Identity`](https://www.stackage.org/haddock/lts-8.2/base-4.9.1.0/Data-Functor-Identity.html#t:Identity)), but here it's more evident that these monads don't explicitly capture any notion of defferal (the underlying value is immediatly available, thunks aside -- unlike `IO`). You'd need some way to lift from `Identity a` up to `MaybeT Identity a` so the two types would unify.
in fact i took a considerable pay cut in taking the job
 i'm talking about this https://www.reddit.com/r/haskell/comments/5wb5qw/how_do_you_guys_get_anything_done/de8wstk/
Looks like hdevtools needed stack to work? It was giving me errors about stack not existing when I deleted the executable. I reinstalled it with cabal and now it just doesn't work but also doesn't give an error message. Is there any way I can just delete all of Haskell and related stuff from my system and just start over. I am pretty sure I have a shit ton of global packages and things I don't need. I just want to start over with the newest stable haskell release and cabal and install exactly what I need, then use sandboxes.
First of all I wanted to say that I've been somewhat in your shoes and can understand where you're coming from. That being said... ... it would seem that you are taking people's suggestions a tad personal. Yes some could be interpreted as condescending but mostly if you're looking for it since mostly people here just want to help. Though I don't blame some for being defensive given how you've phrased your post. By that I mean the very title is asking for people to be defensive. So far the complaints you've raised could be answered with a something along the lines of "practice and studying". Which is what I had to do and now things are much smoother. Is haskell a perfect language? I would argue no. But it is at its core much different then most popular languages and as such basically requires you to re-learn how to program. If you haven't already I recommend reading the haskell book (http://haskellbook.com/) it costs money but is oh so worth it IMHO. Best of luck in whatever you decide to do.
I have been using Haskell full-time, professionally for over a decade -- so I guess I qualify as someone who gets stuff done. &gt; Cabal hell (of course) I use Nix. Tough to learn, but awesome once you master it. Prior to using Nix I never really experienced this so call cabal hell. From what I understand the latest `cabal` with the `new-build` really solves what remnants of cabal hell still exist. &gt; Stack purgatory I use Nix. It seems to offer everything that stack does and more, so I have never bothered to even try using stack on top of nix. &gt; No IDEs I use emacs. Having never polluted my mind with a modern IDE, I have no idea what I am missing. Ignorance is bliss. &gt; So if I change something somewhere that's a defered computation I have no idea what you are talking about. I suspect that your issue is due to an incomplete understanding of the language, and so you are getting tripped up by a priori expectations about how things should work. This, of course, certainly sucks when you are trying to learn the language. But once you know the language, it is not an ongoing issue. &gt; No parens for function application Once again, I have no idea what you are talking about. I can not identify with this issue. I suspect that this is once again just an issue of inexperience. After you have written and read more code you will start to see patterns that you are currently missing and things that seem inscrutable now will become second nature. &gt; User defined operators A user defined operator is just a user defined function that happens to default to infix application. We could restrict you to lisp-like `(+ 3 4)` but that hardly seems like an improvement. &gt; So many purity hacks `unsafePerformIO` is a purity hack. `Functor`, `Applicative`, and `Monad` are well defined classes with laws and used primarily for things which are unarguably pure. &gt; Inconsistent whitespace rules I long ago adopted a coding style that is also congruent with the white space rules and I probably don't even know what is even actually allowed. Regardless of what language you learn, you will certainly adopt a coding standard that is a subset of what is allowable. Now, it is true that some languages, like Python, try to limit you to the One True Style. I'm not going to say that Haskell's more flexible rules are better. Just that once you have more experience, they are insignificant. &gt; Single letter variable names I personally tend to use longer variable names these days. But I don't find single variable names that offensive. In many cases a single letter is used because there isn't really much of importance you can say about a value. If I have a function like: &gt; reverse :: [a] -&gt; [a] &gt; reverse [] = [] &gt; reverse (a:as) = reverse as ++ [a] How much more is there I can really say about `a`. I could say `element` -- but that is pretty clear from the types. &gt; Dog slow compilation times true. Gives me more time to catch up on reddit while the code compiles. &gt; Dead packages Better than some languages with no packages. Haskell does have the paradox of choice when it comes to libraries. &gt; TH shenanigans TH could be a lot better. Some uses are great, some are benign, some are terrible. Do you find `deriving` to be offensive? Aka, data Foo = Foo deriving (Eq, Read, Show) That is baked in compile time code generation. But, what if you want to derive something else that the GHC authors do not know about? Enter template haskell! deriveMyThing ''Foo The syntax is not as pretty, but the end result is the same -- code is being generated at runtime which saves you time and prevents errors. Compile time occasionally I/O is useful, but potentially at odds with reproducability. I don't remember the last time I wrote TH code. So, while it has it's uses, it is not really a major obstacle to getting stuff done. TH is definitely *not* an example of something Haskell got right. Fortunately, the only obstacle towards creating an alternative runtime code generation solution is your willingness to do it. TH was an experiment that got some things right and some things wrong. If someone invents something better, then people would use it instead. I do not think anyone is in love with TH. People use it because it can get the job done. But, people also try to avoid using it because it isn't fun. &gt; Compile time sys paths I use it, and it seems to get the job done. It provides a working default value, and you can override the default values using environment variables. I have no strong feelings about replacing it with something better. I can't say it has caused me much trouble. 
However you decide to install your Haskell toolchain, you'll likely want to wipe away your user package database and cabal configuration: `rm ~/.cabal ~/.ghc`
Yeah I figured as much. So what Haskell related things should be in my Path? I assume I should install hlint and hdevtools globally though right? Or not even those?
Yeah I have done that. My current issue is that I keep getting `sudo: /Library/Haskell/bin/uninstall-hs: command not found`. I think brew cask is completely screwed ATM, I can't install, uninstall, reinstall or interact with haskell-platform AT ALL. Because of that error. What the hell am I supposed to do, besides just not get in this situation in the first place? This is getting me ridiculously pissed off. EDIT: Ok I was able to fix it with a combination of `brew cask info` and `rm -rf`. So now it looks like I am finally in a coherent-ish position. I decided to just install the Haskell platform.
Looks like I got past that, now I keep getting `Cabal error: ghc-pkg dump failed`.
How do I get hdevtools to recognize my sandboxes? They keep complaining about missing dependencies.
Possible to share a high level summary in plain English. This is the paper's abstract, and it makes no sense to me whatsoever :) &gt; We describe our experience implementing a broad categorytheory library in Coq. Category theory and computational performance are not usually mentioned in the same breath, but we have needed substantial engineering effort to teach Coq to cope with large categorical constructions without slowing proof script processing unacceptably. In this paper, we share the lessons we have learned about how to represent very abstract mathematical objects and arguments in Coq and how future proof assistants might be designed to better support such reasoning. One particular encoding trick to which we draw attention allows category-theoretic arguments involving duality to be internalized in Coqs logic with definitional equality. Ours may be the largest Coq development to date that uses the relatively new Coq version developed by homotopy type theorists, and we reflect on which new features were especially helpful. Any particular section of the paper that makes the core point immediately obvious?
Is there a GHC flag which turns OFF all funky/advanced stuff that may be taking the compiler a lot of time to do? One can probably use that for on-the-fly type-checking in the IDE and get the full compilation suite during a proper build. Or is that flag simply `-O0`?
I like using vim, so I think I'm gonna skip that second part. But what do you mean "just use stack"?
if you use stack you don't need the haskell platform (or separate downloads of ghc/cabal). This is a common point of confusion unfortunately.
Alright that sounds pretty convenient, how do I get hdevtools and hlint and such to work with stack? Currently I have them cabal installed globally, and actually I'm having some trouble getting hdevtools to recognize cabal sandboxes. It keeps saying `Cabal error: Encountered missing dependencies: mtl ==2.2.*` which is bugging me a lot.
use stack instead of haskell-platform
Is it even worth trying anything else when ghc-mod (in my project) is broken? Doesn't everything depend on it?
I mean this in the nicest way possible: What the heck? First, Nixpkgs on OSX doesn't actually have a good reputation compared to Nixpkgs on Linux. In fact I've heard it's downright bad, has this changed? Secondly, why would your recommend that a beginner leave the standard path by so much? What's he going to do when he wants to do the most basic things in Haskell, like go from a `cabal.config` file to a running an application with those dependencies? The answer is that he'll have to find custom tools to do this, because since Nixpkgs isn't built for Haskell dev it doesn't have them built in. Keep in mind: this is a person who hasn't discovered _cabal sandboxes_ yet. This is a bad idea. (Disclaimer: I'm on NixOS. I love Nix. I'm not saying it's bad, I'm saying that it's a bad recommendation for standard Haskell use at this time.)
Can we please stop recommending the Haskell Platform on haskell.org? Cabal sandboxes or stack are both far better to use and far more principled solutions. I know some people have made this suggestion in a mean way, which I didn't like at the time and still don't, but just because some people were mean about how they suggested something doesn't mean it's not the right thing to do.
Every multi-arg function is actually a bunch of nested functions: f = (\input1 -&gt; (\input2 -&gt; (\input3 -&gt; (input1 + input2 - input3) ) ) is exactly the same as f input1 = (\input2 -&gt; (\input3 -&gt; (input1 + input2 - input3) ) ) and so on until you reach: f input1 input2 input3 = input1 + input2 - input3 This last version is syntactic sugar. Purely syntactic sugar.
What about "0-ary" functions that just produce a constant? f = 5 ? EDIT: Thanks to everyone who replied! It's clear to me now. I must have mentally copied this over from my lessons on church encoding, but it has no value in Haskell itself. 
&gt; Am I not first and foremost responsible to the stakeholders of the project to deliver a working application? Is a programming language simply not a tool in that effort? Would you really argue that Go is not a better tool for web than Haskell? I'd really, really like to push back against this statement. If you view programming as a cut and dry process of a "get requirements, fulfill obligations, next assignment" cycle then you ignore the many, many, many other facets of programming. Code needs to be stable (push it to production and be confident that it won't go down), code needs to be maintainable (both to adding small bits of functionality and deep refactors that change the nature of the program because new fundamental requirements are discovered). Code needs to be debuggable (when errors do happen, it should be painless to track down where the error occurred and to fix it quickly). There are many other requirements that code needs. If you view coding as a simple exercise of getting from Point A to Point B, I think you need to deeply reconsider what it means to write code. These are _even more important_ on the web, because on the web you are responsible 24/7 for the stability of your service, and attack vectors are much larger.
Sounds good, best of luck! I will be honest and say that my editor integration is broken at the moment, so I'm not the person to advise on that particular problem=)
Have you tried using `nix` to install `ghc-mod`? The last I checked it's building successfully: http://hydra.nixos.org/job/nixpkgs/trunk/haskellPackages.ghc-mod.x86_64-linux/all
Some IDEs have plugins based on `intero` instead of `ghc-mod`. Note that `intero` requires `stack`.
Alright, I will uninstall haskell-platform and do stuff with stack. Should I install stack before I even install ghc? Do I still need to install ghc by itself?
Thanks, I'll take another look then :).
What's the need for wrapping `a -&gt; b` in an "opaque" type like `p a b`? The former is equally typesafe, right? I'm assuming this is the simplest form and it can actually be extended to something more esoteric. Any example of the esoteric stuff?
Build of the ghc-mod is not an issue, it crashes when trying to load some library. I heard the name nix, but always paired with how difficult and complex it is. When I have some free time I will check it, thanks for the tip :).
There is a nice construction where you get a profunctor from any monad `m` by defining `p a b` to be `a -&gt; m b`. So your profunctor could potentially involve any kind of monadic computation (such as side effects like ElvishJerricco mentioned below).
Needs more comonads
f is not a function.
Continuing what other people said, stack is the way to go. I think [this get started page](https://haskell-lang.org/get-started) will provide you with what you need to know to get started using stack (do read the next steps section as well). Regarding the editor, I switched to using Emacs from vim after having a taste of haskell-mode in spacemacs. I did give up on spacemacs as it wasn't to my liking but I made my own emacs+evil config and I'm pretty happy with it. One nice thing in particular is that you don't need to install any additional application for haskell-mode to work. So depending on how married you are to vim, you might want to consider emacs+evil+ haskell-mode/intero or spacemacs. Editing code is not very different when you use evil-mode but i found the development tools to be superior in emacs.
These are values, not functions. Functions are values.
Nowadays I will say `stack` is the solution; if you want hassle free. Just download stack and unzip the executable into any $PATH location. Then you can get a global ghc installation via running `stack setup` from any folder that doesn't have a `stack.yml`; and you also get per project versions, for those projects that have a `stack.yml` file. After that you can run ghc with `stack ghc`, ghci with `stack ghci`. Other programs you might install with stack globally will all be placed in ~/.local/bin (so if that isn't defined in your $PATH variable I suggest you do). edit: [sample installation of hlint globally](http://pastebin.com/RjRSyNz8)
I don't think it would be a big loss for the Haskell community given OP's ignorance and unwillingness to learn. At the same time Go target audience is people who don't want to learn new ways to develop software even if that can boost their productivity. Sad but true.
Ok I took everyones advice and switched to stack. I did stack init and stack build and so on. But I keep getting various errors when trying to actually use the hdevtools I installed via stack: &gt; Cabal error: The program 'ghc' version &gt;=6.4 is required but it could not be found. Is one, which was on a completely fresh project. Where I did `stack new`, `cd`, `stack setup`, `stack build`, and then `stack install hdevtools`. I tried `stack install ghc` but it still doesn't work. 
&gt; I got errors about not having the nightly version of the compiler and then once I got rustup to give me nightly I got another compiler error and I said screw it with all of the immature nonsense I've found the Rust community to be incredibly, positive to the point where things like that bother me less. Yes, it's sub-ideal and I wouldn't use it in production. But stuff like that usually can be fixed pretty quickly and they're definitely responsive to bug reports. 
As others have already said, that is not a function. This post is an interesting look at the (false) belief that everything is a function: http://conal.net/blog/posts/everything-is-a-function-in-haskell
&gt; I find even this rm ~/.cabal ~/.ghc business silly - users should not have to muck around with their filesystem in this way to reset their configuration. They don't have to - unless the muck up their system. Trust me, I can muck up your system in a way that you'll have to do that for stack, too.
perhaps if you give haskell as much time as c (or to a lesser extent, java, php or javascript) has had, it will be solving a few more problems...
&gt; Lots of profunctors are also arrows, and every arrow can be made a profunctor This probably answers your question right? In the same wat that lots of applicatives are also monads, and every monad can be made into an applicative. 
Have you ever looked at the GHC code base itself? They're used pretty heavily there.
It is not straightforward, but on the other hand, everyone using Haskell for a bit longer knows about partial application and `$`. They are everywhere. I agree though that the code you linked is not easy to understand. That is due to short variable names and literally no comments. I'd consider that bad form in any language.
The only problem it has solved is for inept morons to criticize others and feel good about it.
Not really because I have programmed in Haskell.
I managed to get it working. I think the thing that finally fixed it was adding the following to my `.bash_profile`. export PATH="$HOME/.stack/programs/x86_64-osx/ghc-8.0.2/bin:${PATH}" Or something on those lines, I did a bunch of things sort of at once and eventually everything sort of worked itself out. So looks like I finally have a nice dev environment. Thanks a ton for the help! Everyone here has been incredibly useful. I really wish it wasn't such a pain to begin with, but I guess I don't have to worry about it for quite a long time now, which is nice.
&gt; There's a reason why most projects that start with Haskell switch to something else. [Citation needed]
Yes, it has solved much more problems than it has created because of one bug. Tell me when you're done writing your bug-free Cloudfare competitor in Haskell. Oh but you won't. You'd rather complain about how bad everything else is because it is not practical to build anything useful in Haskell... which is why no one has.
Well, it just shifts the goalpost to "what are arrows" :) And, Opaleye is already **using arrows** for it's query DSL. Why would it also need to use profunctors?
&gt; Please tell me how easy this is to parse? Personally I find `$` unpleasant, and this usage of `liftM2` doesn't help clarity. I would have written av (R.PlaceholderValueParameter a) = (tl . filter (\c -&gt; isAscii c &amp;&amp; isAlphaNum c) . show . typeOf) a 
After you punk
I find the example given for "military typing" kinda weak. Since any operation on the `XP` and `YP` datatypes ends up being done through a `lift*` operation of some sort, what kind of type safety does that give you *exactly*, in contrast with the same lifts functions with a more direct implementation ? And you can still pattern-match on the XP / YP datatypes and wreck havoc if you like. Maybe a more interesting design point would be to hide the XP / YP constructors.
Somehow this made me curious about the overall story. After all, I can imagine the last thing management would want to hear is to have the newly hired person (that was supposedly hired to maintain the codebase) to propose a rewrite in anger :) Maybe you should make a write-up when it settles down.
Firstly, if you're having to understand what a `ProductProfunctor` is to understand Opaleye that's rather unfortunate. Could you [file an issue](https://github.com/tomjaguarpaw/haskell-opaleye/issues) to explain what your exact problem is? &gt; What really is `Profunctor p =&gt; p a b` ... Do you understand what `Monad m =&gt; ... m a ...` means? It means that the definition works for any `Monad`, not just a specific one. Likewise, `Profunctor p =&gt; ... p a b ...` means it works for any `Profunctor`, not just a specific one. &gt; `dimap :: ProductProfunctor p =&gt; (user -&gt; tuple) -&gt; (tuple -&gt; user) -&gt; p tuple tuple -&gt; p user user` It means that `p tuple tuple` has a `tuple` coming in (on the left) and a `tuple` coming out (on the right). With `user -&gt; tuple` I can make a `user` come in on the left instead, and with `tuple -&gt; user` I can make a user come out on the right. This gives me a `tuple user user`. &gt; someone who doesn't understand category theory? (As an aside, [Please Don't Learn Category Theory](https://jozefg.bitbucket.io/posts/2013-10-14-please-dont-learn-cat-theory.html). It's somewhat unfortunate (although understandable, and I don't have any better alternatives) that `Monad`, `Profunctor`, `Contravariant` and friends have been given categorical names, but you don't have to understand *any* category theory to learn how to use them. After all, you don't have to learn algebraic number theory to learn how to do algebra.) &gt; What's another noun that can be used in place of `ProductProfunctor` to explain what `p` is? A `Profunctor` `p` can be considered as a "bridge" where things come in on one side and go out on the other. A `p a b` can be thought of as taking in `a`s and emitting `b`s. A `ProductProfunctor` is the same, plus it gives you a way to combine two `p`s into one. If you have `p a b` and `p a' b'` (allowing you to consume `a`s and emit `b`s, and consume `a'`s and emit `b'`s respectively) you can create `p (a, a') (b, b')` which consumes `(a, a')`s and emits `(b, b')`s. &gt; Finally, is there a way to write a profunctor without dealing with tuples? **IMPORTANT:** You are *not* writing a profunctor. You are writing a `Default` instance and an "adaptor" `pFoo` for your `User` type. These work *with* (product) profunctors. `User` itself is *not* a (product) profunctor. But yes, the latest `product-profunctors` will work with any number of fields. &gt; Why is the first argument contravariant and the second argument variant? Because that's what a `Profunctor` *is*! You may as well ask why the argument to a `Functor` is covariant! &gt; if you have a function `a -&gt; b` how do you really do this transformation - `p b c -&gt; p a c` (because you don't really have a `b -&gt; a`)! That's `lmap` (which is `\p -&gt; dimap p id`). 
Because most of the `Profunctor`s that Opaleye uses aren't actually `Arrow`s. I think /u/ElvishJerrico's assertion that &gt; It's very rare for a profunctor which is useful in an industrial setting to not be an Arrow is not actually correct. It's simply that most developers have not got used to noticing `Profunctor`s in the same way that they are used to noticing `Functor`s, `Applicative`s and `Monad`s.
I dont think its redundant at all but sadly Im not aware of a library that implements these ideas. If you combine something like [pipes](https://hackage.haskell.org/package/pipes) with [pipes-async](https://hackage.haskell.org/package/pipes-async) you get something similar but the big problem with this approach is that `pipes` doesnt play well with non-linear control flow so you can build a linear pipeline but not branch and merge. (Yes I know there are some workarounds but I havent seen one that is clean and simple).
&gt; Do you understand what Monad m =&gt; ... m a ... means? It means that the definition works for any Monad, not just a specific one. Likewise, Profunctor p =&gt; ... p a b ... means it works for any Profunctor, not just a specific one. Tom, I think you misunderstood my question. I really want to build an intuition for `p a b` and when (product) profunctors can be used. For example, here are my personal intuitions for functors and monads: * Functor is a "wrapper" that can wrap any other data-type AND it allows you to run functions on the elements that it is wrapping over, giving you back another functor with the same general "shape" * Monad allows you to hide function arguments, this giving you the ability to pass implicit state. &gt; A Profunctor p can be considered as a "bridge" where things come in on one side and go out on the other. A p a b can be thought of as taking in as and emitting bs. Just confirming, would the `pipes` or `conduits` library also be using Profunctors internally? The way you explain, it seems pretty much like a pipe or conduit to me. &gt; A Profunctor p can be considered as a "bridge" where things come in on one side and go out on the other. A p a b can be thought of as taking in as and emitting bs. A ProductProfunctor is the same, plus it gives you a way to combine two ps into one. If you have p a b and p a' b' (allowing you to consume as and emit bs, and consume a's and emit b's respectively) you can create p (a, a') (b, b') which consumes (a, a')s and emits (b, b')s. Hang on, if I am finally understanding this correctly `Profunctor p =&gt; p a b` is the basic building block of Opaleye, with `a = Column PGInt8` and `b = Int64`. Is that correct? And because one is dealing with records, which *may* be represented as tuples, one composes these `p a b` units into larger units by using ProductProfunctors. So, if I have a record with two fields -- `(Int64, Text)` -- and a `p (Column PGInt8) Int64` and `p (Column PGText) Text` -- I need a ProductProfunctor to get -- `p (Column PGInt8, Column PGText) (Int64, Text)`. Is this understanding correct? And if this understanding is correct, is it possible to write a Profunctor directly of the form `p {field1 :: Column PGInt8, field2 :: Column PGText} {field1 :: Int64, field2 : Text}` without involving ProductProfunctors at all? So basically, in the case of Opaleye, are product-profunctors being used only because of the ease in which they can combine multiple profunctors through the use of tuples?
Forgive me. I am very unfamiliar with Opaleye. But that sounds more like a job for the `ST` style usage of `RankNTypes`, does it not? If you're just trying to prevent columns from being accessible outside of a certain scope, you can tag the column type with a type variable that ensures they're used within the correct rank-n setting.
I think I'm finally getting it. Do take a look at https://www.reddit.com/r/haskell/comments/5wf49a/didnt_struggle_with_monads_as_much_as_were/de9sow3/ -- am I on the right track? In your example above, the analogy understandably breaks down because in real life one will be unable to take a mixed language sentence and summarize it using multiple single-language summarizers. In the general sense this would mean, that `p a b` and `p a' d'` have to be absolutely independent of each other to be able to produce `p (a, a') (b b')`. Is this correct?
and Binaries
Hey, thanks for the reply. I was hoping someone knew a way to grab it automatically from the `stack.yaml` (by running `stack` with some command line argument I couldn't find). Going into the Dockerfile and updating the version isn't so big a deal I'd want to introduce something as heavy as `m4` to do it, but thanks anyway. :)
at least you're getting good resume experience 
&gt; Well one reason I feel this was is there many fewer mature web libraries in Haskell. You might be interested in the [state of the Haskell ecosystem](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md).
Thanks, I'm fairly new to docker so I assumed this was possible using just the Dockerfile but apparently not.
I used reflex too. I just used `stack` and tweaked the `stack.yaml` file to use GHCjs. That's all that was required (that and waiting a bit for the compiler to build).
No, most of them are library ecosystems.
Did you learn Go completely? Probably not. Did you learn how the basics of Go? Of course you should. You haven't learned the basics of Haskell. That's far away from 'complete', so far that none of your points make sense. This is my last response in this whole thread. It wasn't quite worth it.
Also, please realize that not all categories are as important, and the rating system not complacent. For example, "databases and data stores" is ranked immature because of the lack of bindings to commercial databases. While this can be a showstopper, I wouldn't have classified it that harshly. Some other domains are clearly hugely important and problematic (debugging and IDE support for example). However, Haskell is "best in class" with regards to maintenance, whereas most other languages are between immature and bad. That is, for me, the main drive behind my use of Haskell in a professional setting.
&gt; most other languages are between immature and bad I just had to work on javascript. That judgement was a bit harsh :)
4/7.
Pretty much, yes. `data` of a single field is pretty much useless.
To be fair, `tl` is not a very good function name if you ask me, even if the definition is nearby. Neither is `av`. It would be nice if line 20 were actually meaningful without needing to inspect all the subsequent definitions, like `second (\r -&gt; bQuote &lt;&gt; r &lt;&gt; bQuote) $ formatPathValues 0 el` so I know at a glance that we're formatting the arguments of `el` and then surrounding them with quotes. I can then make an informed decision about whether I'm interested in `bQuote` or `formatPathValues` or whatever.
&gt; In the general sense this would mean, that p a b and p a' d' have to be absolutely independent of each other to be able to produce p (a, a') (b b'). Is this correct? I don't think that means anything precise so it's not possible to give a precise answer to this. I think this example is helping you build the right intuition though.
You're welcome!
&gt; Short variable names are never a good idea, except for throwaway vars with very small scope. Short variable names also clarify code, especially for known concepts that are relatively generic. foo xs = case xs of x : xs' -&gt; .. [] -&gt; .. bar n = .. 
"Don't do it because these guys did a terrible job of it" is basically your argument? Anyway, I already added the suggestion to simply implement the services as packages to be mounted in his router, so I'm not sure why you're disagreeing.
I imagined regimented keyboard use when I read the title.
If that's the case, how about we get rid of `newtype` and have `data` of a single field always become what `newtype` is now?
yea i've been to Jane Street before actually so OCaml was one of the first things that came to mind when considering a rewrite - but I feel like it's even a smaller community than Haskell.
Ok, fair enough.
&gt; 4/7 I would ignore categories you don't care and readjust this fraction with the remaining. It'll probably be higher.
Yeah I think that all makes sense, thank you! So basically without orphans we shouldn't get ourselves into too awful of a situation, but that genuine ambiguity will sometimes come about and we just want it to be as quick and clear to deal with as possible.
Except that he complained that the compilation times were long, and that he was digging around in a huge codebase that he did not understand. Microservices add complexity, but it's a very manageable complexity layer. In web applications it makes a lot of sense to have a bunch of uncoupled services behind a reverse proxy or a service gateway. Why have things that are not related to eachother in the same codebase?
Yeah, I don't understand that either. For all my purposes REST is fine. But I don't run a huge consumer internet scale application.
Or slightly reformulated: there are values that are not functions.
You are conflating runtime things with the 'meaning' of the language. ` name arg1 arg2 = ... ` is just a short-hand for ` name = \arg1 -&gt; \arg2 -&gt; ... ` That there's a particularly efficient compilation available when targeting x86 when you give all arguments is just an implementation detail.
Haskell has state, too. It's just tagged with IO or ST.
I'm fluent in several ML-like languages and I still won't touch Haskell, mostly for reasons included in the OP. Just because someone is a newcomer doesn't mean their griefs are invalid.
AFAIK, the ergonomic debugging of lazy languages is an open problem.
Haskell is *lingua franca* in academic FP, so it's a good idea to know the language and keep up with the news even if you don't write it.
Yes, that flag is `-O0`. In the case of IDE tooling you can also (in many cases) pass `-fno-code` to tell the compiler that it doesn't need to do any code generation. Unfortunately, this doesn't interact well with Template Haskell (see [#11425](https://ghc.haskell.org/trac/ghc/ticket/11425)).
&gt; Run cabal2nix? As I noted the path to enlightenment is long and difficult. `cabal2nix` spits out a Nix expression with the Nixpkgs names of Haskell packages. Crucially, it does nothing to get their versions right, the versions will be whatever is currently in your Nixpkgs repo. It's not a bad tool, but it is an extremely simple one, and insufficient for getting reproducible builds from a Cabal file.
I'm not sure it's relevant but maybe it can help building your intuition. If you think of morphisms between type constructors, you'll get something like newtype Hom f g a b = Hom { runHom :: f a -&gt; g b } Then you'll easily have Functor g =&gt; Functor Hom f g a and Profunctor is "exactly" the class you get when both f and g are functors so you have (Functor f, Functor g) =&gt; Profunctor (Hom f g)
Yes, it's really quite simple. For a big data project using Cloud Haskell we implemented a "big message" channel. The API is much like a normal channel but it's designed for sending GB sized messages (like map/reduce results). All the implementation needs to do is to break the message up into chunks (which is the natural result of serialisation anyway) and use a little acknowledgement protocol to say when the consumer has received a chunk. You can set this up so that you have N chunks in flight at once so that you're not introducing extra latency. This is a really simple form of a window algorithm like is used in TCP for back-pressure. So the sender starts by sending N chunks and then it goes into a loop to wait for an ack before sending another chunk.
Mmm I just realized that my [catchBatcher](https://github.com/basvandijk/monad-batcher/blob/master/src/Control/Monad/Batcher.hs#L94) is very likely broken. I'm thinking about a fix. This stuff is tricky!
The biggest problem with floats in Haskell is the broken `Eq` instances. At least remove them! They break any code silently that relies on `Eq` specifying an equivalence relation.
[This version](https://github.com/basvandijk/monad-batcher/blob/fix-exceptions/src/Control/Monad/Batcher.hs#L94) should behave better I believe. I'm working on a few test cases.
First check and make sure that your version of Haskell Platform didn't already install stack for you (it does on Windows). Does: &gt; stack --version do anything? If so you can just proceed with `stack setup` to download and install the default GHC (probably 8 at this point). I have a full Haskell setup on Mac for testing via Stack and I didn't need to do anything with `nix`. EDIT: Just a word of warning, `stack` can upgrade itself with `stack upgrade`. But if you're behind more than a couple of versions (the latest is 1.3.2), it will pull down an absolutely *ginormous* tree of dependencies. At this point you're just better off uninstalling Platform and downloading the [installer](https://docs.haskellstack.org/en/stable/install_and_upgrade/).
&gt; And interestingly enough C and C++ are not "strongly typed" while python is. That way, Haskell is not strongly typed either due to `unsafeCoerce`, you can cause the very same damage you would in C with a misused pointer cast.
I love how this comment doesn't get downvoted in /r/haskell :)
when you're given code that is quantified over forall Monad m =&gt; .... -&gt; m whatever without any other constraints on m, you can know it doesn't do anything other than plumb the effects you give it around and map some pure functions. It only has access to whatever 'm effects' you pass that function as arguments. (Well, those, and, due to historical weirdnesses, `fail`.) I exploited this in this blog post: http://comonad.com/reader/2011/searching-infinity/
Why doesn't it use `liftA2` instead?
This is actually a interesting example. Since eliminating the monomorphism restriction by default, this actually does get compiled into a function -- but not an 0-ary one. It becomes a function from a `Num` dictionary for some type to a value of that type, with a body much like `dict_fromInteger dict $(primNumericTokenToInteger "5"#)` `f = "Hello, world!"` gets a similar treatment if `OverloadedStrings` is on. Even `f = []` gets this treatment is `OverloadedLists` is on. `f = True` is definitely NOT a function, it is just a constant definition.
I'm fairly comfortable with profunctor *usage*, but I rarely see profunctors abstracted over. Do you have examples of when it comes in handy to have a type class for them opposed to just calling dimap on things like functions (and function like things)?
You can represent [lenses in terms of profunctors](https://github.com/purescript-contrib/purescript-profunctor-lenses) in a way that uses the classes for abstraction. There's a couple of useful combinators you can write for Arrows as well.
It still needs to be gathered in one location for discoverability. Hackage needs to grow some hooks - just some javascript to gather data from other services could be enough.
Curious what ML language you're using. I imagine F# is really nice from a tooling and ecosystem perspective, but as someone who has to use Standard ML quite a bit, I spend a lot of time wishing it was Haskell.
One thing I should add is that it doesn't have to necessarily be impure in the "has real side effects" / `IO a` type of impure. It can also be things possible to code purely in Haskell like `State`, `Reader`, `[]`, `Writer`, and `Kleisli`/`Cokleisli` wrappings of such types.
What makes cabal different from stack in an organizational sense? Is cabal "owned" by the haskell committee or something? I don't think the Haskell Platform can be replaced on haskell.org unless the people managing that site also manage stack.
I'm really sorry, but you totally lost me here. I'm suggesting that the Haskell Platform doesn't serve a purpose in modern haskell, and that users would be better off with the recommendation to use `cabal-install` + sandboxes or `stack`. What would stop haskell.org from changing their recommendation to those two options?
&gt; Can't multiply horizontal meters by vertical meters, but can add horizontal meters to horizontal square meters! Yay! The problem, for both of those, is `(*) :: a -&gt; a -&gt; a` 
Fair enough. If Cabal did sandboxing by default, I think getting rid of the Haskell Platform would be a no-brainer.
&gt; You'd rather complain about how bad everything else is because it is not practical to build anything useful in Haskell... which is why no one has. Pandoc and xmonad are both written in Haskell. And so is yesod. 
&gt; What really is Profunctor p =&gt; p a b Notice that all occurrences of that should include one in 'negative' position so its really two parts 'hey p is a profunctor: Profunctor p =&gt; .... and then you need a supply of `p`s' somewhere usually passed into the function for the user, and it probably has to give one out at the end. dimap :: Profunctor p =&gt; (a -&gt; b) -&gt; (c -&gt; d) -&gt; p b c -&gt; p a d doesn't give you any way to construct a 'p' from whole cloth, merely the ability to tweak one you're given. From there we can slowly ramp up what we're allowed to do with the type variables on both sides. class Profunctor p =&gt; Strong p where first' :: p a b -&gt; p (a, c) (b, c) let's you start moving information in the form of products from the 'input' side to the output side. class Profunctor p =&gt; Choice p where left' :: p a b -&gt; p (Either a c) (Either b c) let's you move sums. In another reply in this post I gave a bunch of profunctor examples. It is instructive to consider which ones allow Strong and which one allow Choice instances. Not all do, and "why" is interesting. With the product profunctor machinery you get some way to construct such a `p` from whole cloth, and the ability to put them together. class Profunctor p =&gt; ProductProfunctor where empty :: p () () (***!) :: p a b -&gt; p c d -&gt; p (a, c) (b, d) With that you can now combine these sort of wiring diagrams side by side. Note: I don't actually use this last abstraction myself, hence why it is packaged up by tomjaguarpaw and not me.
Hey, I just moved to NYC. Is the group on Meetup?
&gt; newtype X a b = a -&gt; a -&gt; b is a profunctor, which is equivalent to Costar ((-&gt;) Bool) a b This equivalence has me stumped. Is there a chance it's a typo and should be `newtype X a b = Bool -&gt; a -&gt; b` ? 
This. I've seen some pieces of code recently, that use comparisons like `x == 1.0` and `x /= 1.0`, which I suppose are not right. edit: I think they could be replaced with something like `abs(x - 1.0) &gt; epsilon` (?)
My understanding is that Hydra and Hercules are not general CI systems, but just for testing Nix stuff. I've never seen anyone use it for anything else.
hydra is capable of building anything that has a nix package and it is capable of testing anything that can be tested under a nix build nixos test cases are handled by just booting an entire vm under qemu and ensuring it passes the tests
Languages are still considered strongly typed if the contain an explicit way to convert one type to another. Python has str() to convert stuff to strings, but that does not break strong typing.
Yes it is true, and you could potentially do that, although I am guessing that such algorithms have probably already been implemented, if they are particularly useful. The big win with laziness is that you automatically get such algorithms when chaining lots of different algorithms and functions together, so you basically get composability, which you often don't have in strict languages, which forces you to do a bunch of hand fusing.
`O(kn)` is slower than `O(n log n)` and degrades to `O(n^2)` as k approaches n. That would be the complexity for any type of selection / insertion sort. Merge sort, etc. would still require `O(n log n)` time because, in the example of merge sort, the entire array would have to be sorted recursively, and the short circuiting would not occur until the final merge step. So it would run linear with respect to k but *also* with respect to n, which is really the same thing as quadratic time. It's not possible to sort in linear time.
Reread the op: "for a constant k". So yeah `O(nk)` = `O(n)`. Also it's actually faster than `O(nk)` since, in the case where `k = n` the example in the OP takes `O(n log n)`.
It is folklore that lazy mergesort can find the first element of a list in O(1) time. Saying that the function runs in O(n) for constant k isn't very interesting, because a function that ran in O(nk) to select the kth element would have this asymptotic, but this clearly would result in an O(n^2) algorithm if run to the end. How to carry out this asymptotic analysis is an interesting problem. There's some empirical analysis at http://stackoverflow.com/questions/12057658/lazy-evaluation-and-time-complexity but I don't know of a good formal analysis.
&gt; although I am guessing that such algorithms have probably already been implemented True! But it is interesting to see the correspondence between lazy/strict algorithms. &gt; The big win with laziness is that you automatically get such algorithms when chaining lots of different algorithms and functions together Yes! The beauty of Haskell is that one can express such complicated semantics simply and neatly. If you want to express the same thing (preserving the laziness) in a strict language, that would be more complicated and uglier. &gt; which forces you to do a bunch of hand fusing I am not talking about hand crafting, I am talking about an automated conversion. In the worst case, one can write an embedded Haskell interpreter in Java and run the Haskell code, but there must be smarter ways. 
This clearly does not result in `O(n^2)` for `k=n`. Sorting the whole thing takes `O(n log n)` and `take n` takes linear time, so the total is still `O(n log n)`.
The algorithm doesn't have lower complexity in a lazy language. Rather, it is the case that the partial result of the sort algorithm (which has the same complexity) has a lower complexity than the full result, while in a strict language you can't get just the "partial result" and so would need to write a different algorithm for "top k" in order to obtain the same complexity. The (or one at least) standard way to do complexity analysis on algorithms with lazy computation is the "bankers method" as described by Okasaki in his thesis: https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf (as well has his subsequent book of the same title, which is a classic that I highly recommend). Generally algorithms which can yield important partial results are called "online" algorithms and the "stream-like" nature of haskell list processing lends itself to them. There is also a result that in a purely functional language with no mutation (i.e. not any language in common use), certain algorithms have higher complexity than in a lazy language, as the lazy one "sneaks in" a form of mutation behind the scenes: http://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Pure%20Versus%20Impure%20LISP.pdf There's also the famous "repmin" problem that introduces how "circular programming" can eliminate multiple traversals -- it was first discussed by Bird in '84 but I don't think that paper's free online, so here's some notes on it to get you started: http://web.cecs.pdx.edu/~sheard/course/CS457-557/Notes/Laziness-6up.pdf There's certainly research in "program transformation" on how one can turn such circular programs into "typical" imperative ones with the same performance characteristics. 
You've got the parentheses wrong. You want: `(Bool -&gt; a) -&gt; b` `Bool -&gt; a` is isomorphic to `(a, a)`, then curry. tup f = (f False, f True) untup (a, _) False = a untup (_, b) True = b tup . untup = id untup . tup = id
Yes. All I am saying is that there should be a tighter bound. Maybe it is O(n log k)
Cheers, it makes sense now.
Someone correct me if I'm wrong, wouldn't this depend on the implementation of the sort, i.e. whether it streams or not? For example, if sort is a selection sort along the lines of sort lst = findMin lst : sort (removeMin lst) where findMin and removeMin do a linear scan of the list, then head k will only force the first k elements of `sort lst` which is then O(kn) which if k is constant, then it's O(n). Of course, fully sorting the list this way is O( n^2 ). If my sort function was instead sort lst = sort (removeMax lst) ++ (findMax lst) Then it would have to removeMax from the list n times before it could `take k` which would be O( n^2 ) even though I'm only taking the first k elements. But the [sort function in Data.List](http://hackage.haskell.org/package/base-4.9.1.0/docs/src/Data.OldList.html#sort) is some variant of merge sort which splits the list into ascending and descending runs then merges them. I don't know how to tell if it streams or not. I'm pretty sure it doesn't though since in the worst case, it should have to merge all but the final two lists before producing any output, but maybe someone else knows for sure? 
Yes. https://www.meetup.com/NY-Haskell/
Yeah I know you aren't talking about that. But in general that is what you normally have to do. What you are essentially trying to do is model laziness in Java. Which may or may not be practical, I'm honestly not sure. 
Well, these terms are quite soft, in that everyone defines them for themselves. But the best definition I've seen, and the one that's most widely accepted, as far as I can see, is that if a language is strongly typed, you shouldn't be able to cause undefined behavior in it. You can call `str()` in Python, or anything else for that matter, but at worst, you'll get a **well defined**, deterministic exception. In Haskell, with `unsafeCoerce` and in C with a wrong pointer cast, you can corrupt memory, and cause mayhem. This is not a bad thing IMHO, it shows that the language is built for professionals :)
Im pretty sure the merging is done lazily. So if you have `minimum = head . sort`, how much of the output will be forced to produce the result depends on the contents of the input, but it generally wont be the whole thing. You can play around with this using `:sprint` in GHCi to get a feel for it.
Also, `Arrow` is a terrible class. The `arr` method in particular means you have to be able to embed all Haskell functions into your profunctor; which is far too much to ask. That method is set up to explicitly exclude any DSL which intentionally tries to keep itself restricted; thereby making the class useless for its intended purpose, imo
The awesome-* lists tend to be *curated* lists: hand-picked commonly used packages that are important for newcomers to discover. If we're just auto-generating, there's not much value compared to Hackage.
`ProductProfunctor` is necessary because `Arrow` is broken. The `arr` method of `Arrow` makes far too strong of demands, and therefore excludes the vast majority of DSLs I've worked on. `ProductProfunctor` is the mathematically correct abstraction; it's the bugfix for `Arrow`
&gt; it can be shown that any sorting algorithm must run in at least O(n log n) time That's [not accurate](https://hackage.haskell.org/package/discrimination-0.2.1/docs/Data-Discrimination.html) (see [this video](https://www.youtube.com/watch?v=cB8DapKQz-I) for details). If all you give me is a pairwise compare, then yes, you'll get nlogn bounds.
Like this? https://hackage.haskell.org/packages/top Something like that might be useful, but I'm doubtful that anything completely automatically generated will really provide much more than Hackage. 
Which parameters are you planning to compare IDE support on? Here are the params from the Rust link: * Syntax highlighting (.rs) * Snippets * Code Completion * Linting * Code Formatting * Go-to Definition * Debugging * Documentation Tooltips What does "snippets" mean, in this list? Some more suggestions: * Go-to definition when the current file doesn't compile due to a type-error * Displaying type variable-names in documentation tooltips * Display expanded TH splices, like macro-expand in Lisp * Hot code reloading -- very important for webapp development 
Regarding Editors plugins, is there any way to connect a vim plugin to a Haskell environment running in a Virtual machine?
I highly recommend reading [Purely Functional Data Structures](https://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504/ref=pd_sbs_14_img_0?_encoding=UTF8&amp;psc=1&amp;refRID=6VDRA13GFWM5KVXF2JG2), which dives into how to properly deal with amortized analysis in a functional setting where objects are persistent, and then how to modify that analysis to allow for a lazy functional setting where thunks can share work. His analysis techniques are the only real ones I know for working with lazy functional code. **tl;dr** In general, you can't earn 'credit' when someone can cash it in twice, but you can make a thunk and suspend it in a structure to be paid off with 'debit's before forcing it, and pay it off multiple times. The worst case scenario then is that things are cheaper than your analysis predicts! e.g. consider immutable maps in a strict pure language w/ say, scrape-goat trees, which would try to rebalance in worst-case O(n) but amortized O(log n). You can capture the map right before it rebalances, then every insert into that map is O(n). Fortunately, there are lots of general techniques that can fit this form of analysis. There are plenty of algorithms in practice where you can win a log'ish factor back by using a lazy pure language vs. a strict pure language. And there is a theorem that says that strict pure languages can lose a log factor over strict impure languages with references, but that theorem doesn't apply to lazy pure languages. You can always explicitly do what the lazy algorithm is going to do in a language with mutable references by emulating your own thunks, though, so the overall speedup of lazy pure code over strict pure code is bounded by that log factor. We don't have a theorem that says we can _always_ match the strict impure bound, though. In fact we have some bad bounds about the minimum cost of [confluent persistence](https://en.wikipedia.org/wiki/Persistent_data_structure#Confluently_persistent) even with mutable pointer structures and restriction to bounded in-degree, and confluent persistence is a weaker claim than functional persistence, so I don't think we'll ever get that one.
&gt; `Arrow` is broken. I don't agree. "Broken" would imply it doesn't do what it was intended to do. `Arrow` does exactly what it was intended to do. It was meant to be a layer sitting between `Applicative` and `Monad`, with more granularity in the powers presented. &gt; The `arr` method of `Arrow` makes far too strong of demands, and therefore excludes the vast majority of DSLs I've worked on. `Arrow` is not *for* DSLs. It's meant to serve a very similar purpose to `Monad` and monadic do-notation. It gives you a powerful way to abstract about impure effects and use them with standard Haskell pure functions. This goal would be destroyed without `arr`. &gt; `ProductProfunctor` is the mathematically correct abstraction I see very little mathematically correct about `ProductProfunctor`. There are a number of mathematical concepts backing `Arrow`. `Arrow`s are monoids in the category of strong profunctors. `Arrow` is `Strong`+`Category`. `Arrow` begets `Applicative` and `Monad` begets `Arrow`. I could go on. `Arrow` has an enormous number of mathematical niceties. `ProductProfunctor` is just warping the idea of a `Profunctor` to fit an engineering need. --- I do get your point though. `Arrow` is bad if you're goal is to create a DSL completely distinct from Haskell within Haskell. But that's asking for something more like a Cartesian Closed Category, which is a completely different mathematical construct. It'd be nice if we had a good representation of those, and if they had a do-notation style DSL. But it's certainly far more abstract than `Arrow`, and would break from the norm in terms of what Haskell's do-notations are designed for. If you want a nice syntax for your DSL, I would recommend Template Haskell and Quasi Quoters. I like arrows quite a lot because they represent something with almost as much power as `Monad`, but with a more abusable structure. I'll hopefully be releasing an article on what I mean by this later this week.
I specifically said "anything useful".
I'm not sure. I think it needs hand-curation of some sort, perhaps as driven by reverse-deps? Maybe that's supposed to come from user input? I'm not sure how the system is supposed to work in general.
I think youre right, Ive been meaning to check that out but havent gotten around to it so far.
I'm using those items, and added a Hoogle column. Yo can see the preview in the repo. Also, I've added levels of support to the chart, from "unknown" to "best in class": `` `` `` `` "Snippets" refer to pre-made pieces of code (as in emacs yasnippet), so you avoid repeating common construct in the language (such as `if _ then _ else _`, `case _ of; _ -&gt; _`, etc). I'll take note about those suggestions... It would be even better if you can contribute to the discussion on the PR :)
Hmm, I do not use Vim myself, but it would be great if someone could bring some insight about it's level of Haskell support.
I've added `haskell-mode` for syntax highlight. If I'm right, intero provides: - Code completion - Goto def - Doc. tooltips (e.g. type of an expresion) - Lint Is that correct? edit: also, can you tell me how well are those features supported? (is config easy? does it work as expected?)
Cool... I'll update the chart with that info, thanks! About completion... I think that an user would expect "intelligent completion" (with support for modules, types, context, etc), so I'll rate that item as inmature for now.
Will add it for sure, I just need the data. Do you know what is the prefered config? (features supported and plug-ins needed)
I agree, but to be honest, I had never heard about tsv before writing this so I assumed "csv with tabs" would be more familar to people than "tsv". I think the csv standard even allows other separators. Changed it in the description though.
I wrote up a [sketch of a proof][1] some time ago. To do a completely rigorous analysis, one would have to use the [debit method][2]. This shouldn't be too difficult, though, as the previous argument already tells you where you have to sprinkle the debits in your data structure's value. [1]: http://apfelmus.nfshost.com/articles/quicksearch.html [2]: http://apfelmus.nfshost.com/articles/debit-method.html
For VSCode, the Haskero plugin provides what I would call "mature" support for Code Completion, Goto Def, Find Usages, and Doc. Tooltips. 
Nice, didn't know about this before! Vim user though... To be fair, I think I mostly wrote this because of NIH and because I wanted to do something more fun than actually learning my words. There is also [Clanki](https://github.com/marcusbuffett/Clanki), which is written in haskell as well. There might be a problem with SuperMemo2, the algorithm to decide the interval between the repetetions though (see [this issue](https://github.com/marcusbuffett/Clanki/issues/3)), and its data format is not meant to be human readable AFAIK.
Thank you! That is a lovely blog post.
Thanks for your help! I've added sublime, preview is [here](https://github.com/rainbyte/haskell-ide-chart)
Wow, you've defined a matrix type with homogeneous rows and heterogeneous columns. That's even more pointless than the OP. Try multiplying or inverting them!
I think he refers to those as parser directives: %left PLUS MINUS %left TIMES DIV The lines seem to be ordered by ascending precedence.
Oh, I see! Thanks.
Idk about `--profile` specifically, but I also had to write a utility to delete all my `.stack-work` files. It's also nice for zipping code bases
Great! I was using [Try Haskell](http://tryhaskell.org) (slow) and [Coding Ground](http://tutorialspoint.com/compile_haskell_online.php) (batch only). This is a nice surprise. 1) Can I measure time and memory use? 2) Can I have multiple imports? 
I don't know if it feels this way to anybody else, but your black circle looks like a negative, ie "unsupported" to me, opposite of the full-yellow. If anyone else complains, maybe consider switching the meaning of those.
OK, thanks. I was wondering whether I was forgetting about some other case.
I think you are confusing two problems: - The fact that Eq is not a true equivalence relation, because it is not reflexive (NaN /= NaN). - The fact that comparing floating-point results for equality is dangerous because of roundoff errors (notably: floating-point addition is NOT associative, so pretty trivial equalities on real numbers do not hold in floating-point arithmetic). For the latter situation, your approach does not work either. Small roundoff errors can be amplified by your computation and make comparisons break for any epsilon.
Very nice! One thing I have had issues with in the past is defining a parser for a language where some sub-expressions may have a different type. A typical example is a (typed) lambda calculus presented in a bidirectional style: data Val = Lam Name Val | Neu Neu data Neu = Var Name | Cut Val Type | App Neu Val The type of `chainl` is not polymorphic enough for this setting but it is possible to define a more general version `hchainl` which makes it possible to parse applications: hchainl :: Parser a -&gt; Parser b -&gt; Parser (a -&gt; b -&gt; a) -&gt; Parser a Do you think (a variant of) your setup could handle this type of use case? I have the impression that we ought to be able to have something like (where `All` is a type family such that `All P as` gives you a `P a` for each `a` in `as`): type ModularParser as a = All Parser as -&gt; All Parser as -&gt; Parser a and then take the fixpoint if we are able to build `All (ModularParser as) as`.
Can you add case splitting as a feature to track? Emacs and atom have this, but it's one of the main things missing in Haskero
I have used drag and drop in gtk2hs ([gtk3](http://hackage.haskell.org/package/gtk3) library). That works, but is not that easy. GTK is suited for bigger applications, but it's interface in Haskell is not that nice as it is a very low level binding, so you need also to learn GTK first. But you also have Glade as a UI Designer available.
Since it connects to a server initially, I assume its just a remote GHCi session.
Yay, more usergroup videos! Its a shame so many talks are never released to a larger audience.
&gt; How complicated is it to get `HscNothing` to support `TemplateHaskell`? The problem is that `HscNothing` asks GHC to only typecheck your program, termination compilation before code generation (or more precisely, the Core pipeline). However, `TemplateHaskell` requires that we have code for certain imported modules in order to typecheck. &gt; Does someone already know how to do it? /u/ezyang has proposed that we track import phasing more explicitly. This, for instance, would allow the user to write, {-# LANGUAGE TemplateHaskell #-} module Hello where import {-# TH #-} ModuleA import ModuleB {-# ... #-} In this case, the user could only use declarations from `ModuleA` in TH splices. In the case of `HscNothing`, GHC would then know to produce code for `ModuleA`, anticipating that it will be needed in typechecking `Hello`. I suspect this wouldn't be that difficult to implement, although you never know until you try. Moreover, I don't think anyone has given much thought to what the the backwards compatibility story would look like here. 
I liked the `PrimQuery'` type. I know Join isn't strictly necessary as a primitive, but I think it should be included as you have chosen. I have a clear direction on how to approach using untyped PrimQuery, but I am still trying to understand the various old (HaskellDB record) and new approaches (Bookkeeper) to extensible records and type-checking at compile time and run time. I suspect that I will not be able to reuse anything above PrimQuery, as I am starting with an external DSL.
Sorry if you're getting tired of this example, but I think nobody else mentioned some steps that are helpful to understand how the behavior of `$` really is following the usual rules. Thinking of `$` turning straight into an application is great for everyday use, but it helped me to see all the detail once or twice. Starting from tl $ filter (liftM2 (&amp;&amp;) isAscii isAlphaNum) $ show $ typeOf a First, `$` is a low-precedence right-associative operator, and no operator has higher precedence than plain application, so it groups like this: tl $ (filter (liftM2 (&amp;&amp;) isAscii isAlphaNum) $ (show $ (typeOf a))) Second, turn the infix syntax into a normal prefix application. ($) tl (($) (filter (liftM2 (&amp;&amp;) isAscii isAlphaNum) (($) show (typeOf a))) Now that it's in prefix form we can replace `$` with it's definition (there's no infix syntax for applying a whole expression). The definition `f $ x = f x` is equivalent to `($) = \f x -&gt; f x`: (\f x -&gt; f x) tl ((\f x -&gt; f x) (filter (liftM2 (&amp;&amp;) isAscii isAlphaNum) ((\f x -&gt; f x) show (typeOf a))) Finally, reducing these applications of the lambda `(\f x -&gt; f x)` turns things into actual applications: tl ((\f x -&gt; f x) (filter (liftM2 (&amp;&amp;) isAscii isAlphaNum) ((\f x -&gt; f x) show (typeOf a))) tl ((filter (liftM2 (&amp;&amp;) isAscii isAlphaNum) ((\f x -&gt; f x) show (typeOf a))) tl ((filter (liftM2 (&amp;&amp;) isAscii isAlphaNum) (show (typeOf a))) Whew, let's not do that often! We reduced things before lazy evaluation demands them, but here it can't actually change any results so it's okay (omitting the detailed argument). Incidentally, there actually is one way that ($) is special, but that's only in cases involving "higher-rank types", to allow `runST $ do ...` to work whenever `runST (do ...)` is allowed. And yes, the `liftM2 (&amp;&amp;) isAscii isAlphaNum` looks like somewhat poor style to me too.
Even if I/we are biased, Rust is on the right side of them - Designed with an awareness of ML and maybe even some research from the last 20 years - Takes types seriously, and applies them to a serious problem (lifetimes) - Targets a domain that anybody halfway objective has to admit Haskell doesn't cover, specifically GC-less code and direct control over memory layout (with due respect to ekmett's struct experiments).
I also find these symbols confusing. Instead of switching the meaning which I'd expect to confuse others, please use a different set of symbols or something verbal like Tekmo's labels "best in class", "mature" etc.
By complete coincedence I ported a [drag-and-drop demo](https://github.com/deech/fltkhs-demos/blob/master/src/Examples/howto-drag-and-drop.hs) for FLTKHS last night and just pushed it up so you could look at it. First [compile and install FLTK](https://github.com/deech/fltkhs#quick-install) and then the [demo](https://github.com/deech/fltkhs-demos#installation) package, followed by `stack exec fltkhs-howto-drag-and-drop` to run the example.
I'd be happy to lend a hand in maintenance or take over a smaller package or two!
In that Reflex app, was the entire stack in Haskell, and did you use Snap for the backend?
Sorry to hear about your RSI. I've dealt with various kinds of repetitive stress and chronic overuse issues in music and typing for most of my life as well. If you're looking for any advice there, I've found Alexander Technique to be helpful.
It would be nice to see a list of the available packages.
Hey AWS seems to be having issues and it's affecting us. We'll hopefully be back up soon.
Shameless plug: I'm working on [Aelve Guide](https://guide.aelve.com/haskell), which tries to solve roughly the same problem (Hackage being a hard-to-navigate mess of packages), except that it's * hand-curated (i.e. a user-editable wiki) * tries to provide recommendations instead of just being a list of links (and more in-depth comparisons than number of stars) * but doesn't actually show number of stars yet :P (it's a planned feature) (Even more shameless plug: if you want to contribute to an open-source Haskell project  which Guide is  write me.)
Yes, and yes.
Thanks for your review! What do you think about using this alternatives? From worst to better: 1.    2.    3.    4.    5.    6.    Any other suggestion?
I'll considere it... Do you know which plug-ins provide this feature on emacs and vim?
Thanks! I have actually never used a type family like `All` ranging over types, only over type classes. Could you show me how you'd define it for this example? I have tried the following but I get stuck: type family All (f :: * -&gt; *) (xs :: [*]) type instance All f '[] = () type instance All f (x ': xs) = (f x, All f xs) type ModularParser' as a = All Parser as -&gt; All Parser as -&gt; Parser a choiceOrNextP' :: [ModularParser' as a] -&gt; Parser1 a choiceOrNextP' ps nextP = _ GHC won't accept `ModularParser' as a`:  Couldn't match type All Parser as with All Parser as0 Expected type: [ModularParser' as a] -&gt; Parser1 a Actual type: [ModularParser' as0 a] -&gt; Parser1 a NB: All is a type function, and may not be injective The type variable as0 is ambiguous  In the ambiguity check for choiceOrNextP' To defer the ambiguity check to use sites, enable AllowAmbiguousTypes In the type signature: choiceOrNextP' :: [ModularParser' as a] -&gt; Parser1 a
Don't worry, as long as S3 is down no one will even notice. :P
The migration is now completed. The old server will continue to forward to the new for some time. Please report any issue you have with the new hackage server.
Will that talk be recorded?
The content there is good so far. Every wiki I have ever seen degenerates into a pile of disorganized comments. The only exception is Wikipedia, and that's because a lot of manpower goes into it. Have you considered just leaving it in non-wiki form?
There are lots of areas of Haskell that I simply don't have any expertise on and thus can't write about  somebody else would have to write about them. Now that we have established that supporting several editors is needed no matter what, the only remaining question is how hard should it be to get editing rights :) And given that Every wiki I have ever seen degenerates into a pile of disorganized comments hasn't been my experience with wikis so far, I'd rather start with a lax editing policy and make it stricter if needed.
Thanks for the links, that's a lot to read! @__@ For the prisms technique, nice, that's actually too much modularity for my current use, haha! But it seems really nice for building incremental language features, or do things in the style of Datatypes  la Carte / Metatheory  la Carte.
Why is this so downvoted *(-4 at time of writing)*? Downvotes are not a constructive response to innocent questions... Intero is pretty heavily intertwined with Stack, isn't it? Would that make it harder?
I was unable to find any documentation on what `sm_inline` does. Could someone please explain?
Most of ghc's internal workings are documented in the extensive comments. Here is the relevant one: {- Note [Simplifying rules] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ When simplifying a rule LHS, refrain from /any/ inlining or applying of other RULES. Doing anything to the LHS is plain confusing, because it means that what the rule matches is not what the user wrote. c.f. Trac #10595, and #10528. Moreover, inlining (or applying rules) on rule LHSs risks introducing Ticks into the LHS, which makes matching trickier. Trac #10665, #10745. Doing this to either side confounds tools like HERMIT, which seek to reason about and apply the RULES as originally written. See Trac #10829. Note [No eta expansion in stable unfoldings] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ If we have a stable unfolding f :: Ord a =&gt; a -&gt; IO () -- Unfolding template -- = /\a \(d:Ord a) (x:a). bla we do not want to eta-expand to f :: Ord a =&gt; a -&gt; IO () -- Unfolding template -- = (/\a \(d:Ord a) (x:a) (eta:State#). bla eta) |&gt; co because not specialisation of the overloading doesn't work properly (see Note [Specialisation shape] in Specialise), Trac #9509. So we disable eta-expansion in stable unfoldings. Note [Inlining in gentle mode] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Something is inlined if (i) the sm_inline flag is on, AND (ii) the thing has an INLINE pragma, AND (iii) the thing is inlinable in the earliest phase. Example of why (iii) is important: {-# INLINE [~1] g #-} g = ... {-# INLINE f #-} f x = g (g x) If we were to inline g into f's inlining, then an importing module would never be able to do f e --&gt; g (g e) ---&gt; RULE fires because the stable unfolding for f has had g inlined into it. On the other hand, it is bad not to do ANY inlining into an stable unfolding, because then recursive knots in instance declarations don't get unravelled. However, *sometimes* SimplGently must do no call-site inlining at all (hence sm_inline = False). Before full laziness we must be careful not to inline wrappers, because doing so inhibits floating e.g. ...(case f x of ...)... ==&gt; ...(case (case x of I# x# -&gt; fw x#) of ...)... ==&gt; ...(case x of I# x# -&gt; case fw x# of ...)... and now the redex (f x) isn't floatable any more. The no-inlining thing is also important for Template Haskell. You might be compiling in one-shot mode with -O2; but when TH compiles a splice before running it, we don't want to use -O2. Indeed, we don't want to inline anything, because the byte-code interpreter might get confused about unboxed tuples and suchlike. Note [Simplifying inside stable unfoldings] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ We must take care with simplification inside stable unfoldings (which come from INLINE pragmas). First, consider the following example let f = \pq -&gt; BIG in let g = \y -&gt; f y y {-# INLINE g #-} in ...g...g...g...g...g... Now, if that's the ONLY occurrence of f, it might be inlined inside g, and thence copied multiple times when g is inlined. HENCE we treat any occurrence in a stable unfolding as a multiple occurrence, not a single one; see OccurAnal.addRuleUsage. Second, we do want *do* to some modest rules/inlining stuff in stable unfoldings, partly to eliminate senseless crap, and partly to break the recursive knots generated by instance declarations. However, suppose we have {-# INLINE &lt;act&gt; f #-} f = &lt;rhs&gt; meaning "inline f in phases p where activation &lt;act&gt;(p) holds". Then what inlinings/rules can we apply to the copy of &lt;rhs&gt; captured in f's stable unfolding? Our model is that literally &lt;rhs&gt; is substituted for f when it is inlined. So our conservative plan (implemented by updModeForStableUnfoldings) is this: ------------------------------------------------------------- When simplifying the RHS of an stable unfolding, set the phase to the phase in which the stable unfolding first becomes active ------------------------------------------------------------- That ensures that a) Rules/inlinings that *cease* being active before p will not apply to the stable unfolding, consistent with it being inlined in its *original* form in phase p. b) Rules/inlinings that only become active *after* p will not apply to the stable unfolding, again to be consistent with inlining the *original* rhs in phase p. For example, {-# INLINE f #-} f x = ...g... {-# NOINLINE [1] g #-} g y = ... {-# RULE h g = ... #-} Here we must not inline g into f's RHS, even when we get to phase 0, because when f is later inlined into some other module we want the rule for h to fire. Similarly, consider {-# INLINE f #-} f x = ...g... g y = ... and suppose that there are auto-generated specialisations and a strictness wrapper for g. The specialisations get activation AlwaysActive, and the strictness wrapper get activation (ActiveAfter 0). So the strictness wrepper fails the test and won't be inlined into f's stable unfolding. That means f can inline, expose the specialised call to g, so the specialisation rules can fire. A note about wrappers ~~~~~~~~~~~~~~~~~~~~~ It's also important not to inline a worker back into a wrapper. A wrapper looks like wraper = inline_me (\x -&gt; ...worker... ) Normally, the inline_me prevents the worker getting inlined into the wrapper (initially, the worker's only call site!). But, if the wrapper is sure to be called, the strictness analyser will mark it 'demanded', so when the RHS is simplified, it'll get an ArgOf continuation. -} 
I won't be able to help, sorry, but I wanted to make a quick analysis because I think this is an interesting case. When I started learning and writing Haskell I found your name all over the places. I was amazed by your prolificacy. Now I'm sad to see that you don't have anymore time to work on these projects, but I think making a step back, and getting more people involved, is the right move. Some OSS maintainers just don't reply to issues or PRs anymore and, in my opinion, that's just a wasteful behavior.
I'll be glad to see you again this year! :)
Right now the vim row doesn't say what plugins are used. Can you add that info?
So you consider spewing gigabytes of user info over months "useful" but not changing document formats? 
Why don't you just do a port of Opaleye to MySQL? Shouldn't be too hard. Time consuming, but not hard.
&gt; `Costar f a b = f a -&gt; b` is a profunctor if `f` is a functor. ~~Doesn't `f` need to be contravariant instead of covariant here?~~ Nevermind.
I would definitely consider it, but for my particular problem I wouldn't gain completely from such a port, since I need run time type checking because we are providing an external DSL to users.
$3500 for airfare! *sigh*
I'm not convinced that a non-Hackage solution would even be *good*.
That's really cool. Haskell has come far. We can now be full stack web developers in Haskell only. No JS needed! (Although I guess we still need HTML and CSS.)
Thanks for bringing this to my [and our] attention. It looks like a good book.
Why don't they finish [their other book](http://haskellbook.com/) first?
I thought it was finished by now. Is it not?
I'd assume they'd publish a v1.0 at some point.
Does [Graphics.Rendering.Chart.Easy#bars](https://hackage.haskell.org/package/Chart-1.8.2/docs/Graphics-Rendering-Chart-Easy.html#v:bars) address the concern about `Chart` not handling bar plots @ ~27m? Or is the issue about making it easier? 
Not officially. 
[Yes, he has](http://chrisdone.com/posts/emacs-key-analysis). He also had a serious shot at setting up [a third way](http://chrisdone.com/posts/god-mode). (Incidentally, I'm aware of these posts, having read them only a few days ago, mostly because Chris' Intero has been tempting me to switch *from* Vim...)
But it is the same Julie 
My head canon for how this happened is Julie accidentally emailed the wrong Chris about how the book was going and he, confused, pumped out some chapters. Realizing her mistake she was too embarassed to say anything, and a second Haskell book was born. I'm sure the real story is both more and less interesting.
Julie is fast becoming the Paul Erds of modern Haskell literature.
Does it bring something more to the table than the book she wrote with Christopher Allen? I have the 0.12 version of the Haskell book and was hoping they do a final release with may be some more stuff?!
Happy cake day :) I too have The Haskell Book (in-so-far as it exists) and wonder what this other book (with a different Chris) (but the same Julie) might offer.
Can I ask which country is this from? O_O A quick check on flights.google.com for 8th of June to 12th of June from {SFO, Sydney, Bankok, Tokyo, Delhi, Juneau} to Zurich always showed cheapest ticket below 1000USD.
Like you mentioned, we are still ramping up. Our employees do maintain a lot of public packages that are under heavy use at Takt but for the most part our work has been very domain specific. In a couple months we'll most likely be pruning client references and generalizing in order to make some meaningful contributions.
Performance pianists have this issue a fair amount, and there are some resources to help them. If you have any music schools or conservatories nearby, you can try asking them and they'll probably be able to point you in the right direction. Might even learn some basic piano while you're at it!
Sure, there's a lot to praise about Rust, but that doesn't change my point. Try to go to another language subreddit and post the same comment. See how long it takes before the comment is buried.
Then I don't see how python is stronger typed than C++. In python you have the truthiness mess, and in C++ you have numeric &lt;-&gt; pointer conversions etc. They're both well defined, and in both languages you can take measures (by not using those types naked but passing them around in a wrapped form) to avoid them from taking place. And in C++, you can actually enforce those measures at compile time. Anyway, it's pointless arguing over vaguely defined terms.
&gt; However, *sometimes* SimplGently must do no call-site inlining at all (hence sm_inline = False). But this is now no longer the case? Why can it now be equal to `True` ?
Using the reflex-dom functions or lucid/blaze with clay as the css generator, you can even pretend those don't exist!
Nice, I think this will be a good reading together with Algebra of programming of R. Bird. An easier introduction to math concepts behind Haskell is welcome before diving into the details of abstract algebra needed for a true understanding of why some solution or another must work in a given context.
If you are not already seeing a good physiotherapist I highly recommend that you find one ASAP. I started getting some pretty serious RSI around August last year and it slowly escalated until I did finally see a physio in December. I've had a about 6 visits so far, all but the first included acupuncture. Homework was a regime of strengthening exercises that I undertake 3 times a day. According to the physio, the longer you leave it before getting proper treatment the longer it will take to recover. Two months after my first phyisio session my condition is vastly improved and still getting better. The physio says it will probably be 3-6 months before I am back to normal and that I will need to continue the strengthening twice a week until I stop using a keyboard. Feel free to PM for more info. 
What are co-maintainers? The dual of a maintainer, someone who adds crap to your project and makes it harder to maintain?
100 participants registered in 12hours. We'll be closing registration in one day if that trend continues ;-)
Plus and minus zeros.
Hi, nice chart! You could add a column : show errors/warnings in code, as it's an important feature. ps: the haskero link in broken and points to haskelly :)
hi, haskero dev here, could you open a feature reaquest here (https://gitlab.com/vannnns/haskero/issues) I'm interested to know what to you mean by a case spliting feature.
Great, I'll take any advice and experience reports. Taking this to PM.
Bears has very cool algebraic properties. However, I'm rather confused by it. What do you do if you want to combine tables that have different keys?
Seconding this. When I had RSI I had a lot of physio and I switched to the Dvorak keyboard layout. Some combination of these seems to have cured it (and I suspect the physio did more good than the keyboard).
Stack-related issues do contribute a good amount to issues opened. I think that probably the GHC team has enough on their plate, and that the actual part using the GHC API, the intero binary itself (which is indeed a fork of GHCi), has [relatively few bug issues](https://github.com/commercialhaskell/intero/issues?q=is%3Aissue+is%3Aopen+label%3A%22type%3A+bug%22) (maybe 10 out of 97) opened on it (but those are the more interesting ones). Most things are build system issues, and some Emacs issues. 
I have been taking the advice of this [saxaphonist](https://www.rsitips.com/healing-rsi-through-exercise/), but that's a good idea, local experienced pianists probably have great pointers on this issue. Thanks!
Thanks, I'll do some research on that.
Just (tried) to register, I hope there is still room! It would be a shame to lose it since I live so close :)
Theoretical Computer Scientists are the ones who brought us monads, you know.
ZuriHac is great.
Might be interesting to rewrite your JS example in TS? [TypeScript has ADTs](https://www.typescriptlang.org/docs/handbook/advanced-types.html#discriminated-unions), so you can do "pattern matching". TypeScript 2.0 even does some exhaustiveness checking for the constructors of your ADT.
I would like to see a refactoring section added with various refactoring related entries (e.g. safe renaming, move to new module, convert to type class, etc.).
&gt; /u/dons did the last update ... in 2008
Does **any** Haskell plugin/IDE has any refactoring functionality whatsoever?
The semantics can become a bit wonky; it becomes unclear which effects are kept and which are discarded. Imagine you use `concurrently` with two `StateT s IO` computations. Which state should the result have? And how should `concurrently` interact with `ExceptT`? If one action fails, should it wait for the others to finish? 
Yes. You're missing the entire complicated API and how to actually instantiate `MonadBaseControl`. The principle is simple. The implementation and the usage is not.
Donated $100. Good luck!
Fair enough. Is there a better alternative? I don't want to litter my code with `liftIO`s so I was using that package. I never needed to instantiate `MonadBaseControl`, I guess the part of the API that I used didn't actually need `MonadBaseControl` so I wasn't aware of that problem.
Are you going Tom? I want to work with opaleye :-) 
Since `` is a bifunctor, you can map over the left argument with ` -&gt; 1` and then use the left unitor given by the monoidal structure? Edit: Actually that function ` -&gt; 1` is not always definable, for example in the category of endofunctors where that would be equivalent to writing the function `forall f a . Functor f =&gt; f a -&gt; a` `1` would need to be the terminal object, which would make the tensor operation the product in `a`
Yay ZuriHac! The LumiGuide team is also coming (if our registration has come in time). As last year we'll be working on [haskell-opencv](https://github.com/LumiGuide/haskell-opencv). Who wants to help out?
This is what I meant when I said I could use the `Comonoid` implied by cartesian structure on the tensor to get to `1`, but this does not do the right thing when the tensor is coproduct like. In that case, I would like to use the `Monoid` instance over the product tensor to manufacture a default value to handle the case where the coproduct is a `Left` not `Right` then use the `Semigroup` instance with respect to the coproduct (also known as `codiagonal   +   `) to pull out the value. The problem is an abstraction that makes the decision based on the tensor because I cannot just encode it directly as it would require overlapping instances. I would rather not come up with something ad hoc as that would defeat the purpose. &gt;for example in the category of endofunctors where that would be equivalent to writing the function forall f a . Functor f =&gt; f a -&gt; a Depends on what tensor one equips the category of endofunctors with. 
They have laws that are easy to reason about.
They are heavily used in the lens library. they directly give rise to all the nice composable properties of lenses and prisms etc. It's probably the best example I can come up with right now. Lens is an extremely powerful and useful package for data manipulation. However we decided to avoid it because it does require more extensive knowledge about Haskell to be able to grasp the error messages it spits out.
I think that might solve my concern. My mistake was that I saw a Histogram module that didn't fit my needs so I didn't think to look elsewhere
The key can also be part of the row, so if you have two `Table`s that you want to combine you can use `Data.Foldable.toList` to extract their underlying rows, combine them appropriately, and then reindex them to convert back to a `Table`
Thanks for this inspiring post! Can you explain on which (open source) Haskell libraries your Jobmachine service basically relies on? Or if you have written most of it by yourself: During your journey, which libraries helped you most? Here's why I ask: I find it really compelling to read you story, because your needs sound familiar. I have just a little (outdated) Haskell experience since I was confronted with it at university. Nowadays, working mostly with Ruby, I really would like to learn how to employ Haskell for some tasks because I'm attracted by its elegance.
Thanks for your review! Could you create a new issue to discuss this?
Even if any IDE supports these features, it would be great to document the absense
Yeah, that's the main problem, no tests, I've been doing some myself, but a lot of code has no tests.
Hence why I figured you wouldn't have a problem expanding the maintainer group. ;) The alternate plan is to ping hvr and get him to add him directly, but then the process would be for him and hvr to email you and wait a couple of weeks if you didn't reply, etc. Tedium ensues.
See [this thread on Haskelly](https://www.reddit.com/r/haskell/comments/5u63l7/introducing_haskelly_haskell_extension_for_visual/) from two weeks ago. No offense, but you could have found that with a simple google search.
All functors should be lazy in their polymorphic fields otherwise they do not pass the `Functor` laws. Many `Functor` instances do not forcing the Prelude is to not include rewrite rules to optimize subsequent calls to `fmap` into a single call with the function being mapped over the structure being composed.
I tried haskell with vs code just today so I can't tell you a definitive answer. Mostly they seem fairly similar. Haskero has some additional features like finding all usages or inserting type signature. Opening hackage docs and repl are planned but not yet implemented. Haskelly on the other hand can build, run, and launch ghci or quickcheck tests for the current file/project. The error reporting seems to be broken, though. You might also want to have a look at [phoityne](https://marketplace.visualstudio.com/items?itemName=phoityne.phoityne-vscode) which lets you use ghci's debugger in a much nicer way. The interactive debugger repl might help with the lack of one in Haskero as well.
We can play with different mallocs at runtime with LD_PRELOAD right? I know I've tried that in the past with a web server and didn't observe any interesting differences, but it's possible I didn't know what I was doing.
Errrrm, I guess I think of that as a different community, but I suppose it is theoretical computer science. It resembles logic and semantics more than the rest of TCS IMO.
If I understand them correctly arrows are basically composable bifunctors. Are there practical bifunctors that aren't arrows?
They're pretty commononce you're familiar with them, you spot them more often. Things that take inputs and produce outputs are often Profunctors, which includes functions, arrows and other abstractions that represents flows, pipelines and processes. When you have some kind of generic process abstraction, "prepending" a function to the input and "appending" a function to the output are really common operationsand exactly what Profunctor gets you.
This is great. A total score column at the end might be useful. Thanks for putting in the effort.
Same as most replies to [this survey](https://www.reddit.com/r/haskell/comments/3n0vaw/survey_how_do_you_use_hackage/)  it doesn't follow standard web app conventions, and breaks pretty much all the UX best practices. 
I didn't try to grok the comment joehillen posted, but does this result suggest that inline-more-of-the-things would be an effective way to optimize GHC? In my very limited experience looking and working with the GHC codebase I noticed very few INLINE pragmas, and strictness annotations (esp. on data types for -funbox-strict-fields), compared to what I'm used to seeing and writing myself in libraries.
Looks pretty amazing but way over my head at this point! :-) I just watched ekmett's [talk on discrimination](https://www.youtube.com/watch?v=cB8DapKQz-I) though so I do already believe that this abstract stuff can be applied in quite tangible ways.
"Easy" if you thrive rote-memorizing dozens of rules and can trust all code involved abiding by them. (And I'm curious, how often do instances-abiding-by-them actually "buy"/"deliver" anything substantive?) The appeal seems to be "if we can coerce our lib/API into this 'well-known' corset of abstract/theoretical classes/laws (and there are enough of them by now it is surely .. doable), we don't need to document it, the user will be able to reconstruct the rules of engagement with *just a little category-theory*" =)
Can I ask what keyboard you use? A good friend of mine got RSI early in college and threw money at it, with a keyboard. Helped him tremendously. You probably already have a good keyboard, but if you're curious, the one in question is the [Kinesis Advantage](https://www.kinesis-ergo.com/shop/advantage2/). I've been using mine for nearly 5 years every day and it's still fantastic. I don't have wrist injuries but it still feels much better than anything else.
On a haskell board people are likely to prefer using haskell over XYZ flavor of scheme, but why do you ask? It sounds like you might have interesting ideas in mind already.
You might be interested in [Hackett](https://lexi-lambda.github.io/blog/2017/01/05/rascal-is-now-hackett-plus-some-answers-to-questions/) I've personally dreamed of a system that is as configurable and customizable as Emacs, but with a rigid type-system and functional purity that keeps everything working. Unfortunately, every time I think about it for more than a second the idea seems more impossible.
The reason for needing all these separate type classes is because the functor construction provided in the prelude isn't general enough to accommodate them under one notion of Functor. And yes they are very common and useful. It is quite common for a type to be functorial in more than one argument. Even if they weren't common, that just means people haven't explored these abstractions enough. There was after all a time when monads were considered curiosities.
I'm not going but perhaps we can collaborate remotely.
There are `Profunctor`s that aren't `Category`s or `Arrow`s. One I've worked with would be data ReadShow a b = ReadShow { rsRead :: Int -&gt; ReadS b, rsShow :: Int -&gt; a -&gt; ShowS } You separate the values specifically so you can map them independently. This also gives you useful combinators like string :: String -&gt; ReadShow a String string str = ReadShow (\_ s -&gt; [(str, s') | (l, s') &lt;- lex s, l == str]) (\_ _ -&gt; showString str) (***) :: ReadShow a b -&gt; ReadShow c d -&gt; ReadShow (a, c) (b, d) ReadShow rb sa *** ReadShow rd sc = ReadShow (\p s -&gt; [((b, d), s'') | (b, s') &lt;- rb p s, (d, s'') &lt;- rd p s']) (\p (a, c) -&gt; sa p a . sc p c) (+++) :: ReadShow a b -&gt; ReadShow c d -&gt; ReadShow (Either a c) (Either b d) ReadShow rb sa +++ ReadShow rd sc = ReadShow (\p s -&gt; [(Left b, s') | (b, s') &lt;- rb p s] ++ [(Right d, s') | (d, s') &lt;- rd p s]) (\p -&gt; either (sa p) (sc p)) These allow you to concurrently build up both a `Read` and a `Show` instance for a type, keeping them both in sync. But they have no meaningful instances for anything in the `Profunctor` package besides `Profunctor` itself (so no `Strong`, `Choice`, `Closed`, etc).
ghci can be emulated with the [`hint`](http://hackage.haskell.org/package/hint) package, but it doesn't run in ghcjs for some reason.
We shouldn't have to coerce. Sometimes, code naturally fits into these abstract models, and that's when we implement it as instances of these general structures.
Not to mention writing a large amount of integration and unit tests takes much more time and effort than just using Haskell, since if anything Haskell takes negative effort if you are comparing it to using Ruby (assuming you already know both). That's not to say unit and integration tests shouldn't be used in Haskell, but you get the idea.
[removed]
&gt; "Easy" if you thrive rote-memorizing dozens of rules and can trust all code involved abiding by them. The person implementing the instance must take the laws into account (just like we must respect the `equals` and `hashCode` contracts when working in Java). But from the perspective of the user, they usually boil down to "do the least surprising thing". &gt; And I'm curious, how often do instances-abiding-by-them actually "buy"/"deliver" anything substantive? Finding a valid Monoid/Applicative/Alternative instance for a type opens many new ways of [composing](http://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Compose.html) it with other code, and lets you reuse a multitude of existing functions. Traversable is hugely useful as well. &gt; The appeal seems to be "if we can coerce our lib/API into this 'well-known' corset of abstract/theoretical classes/laws (and there are enough of them by now it is surely .. doable), we don't need to document it That's actually a problem, yes. People should document better what the class methods do for particular instances. OTOH, you get "intuition reuse": being able to apply a common set of expectations across very different-seeming problems.
&gt; How did anyone get anything done three weeks after they started programming? The answer is that they didn't. Not really true, especially not in the context of picking up an *additional* language (which is what we're talking about here), not learning to code for the first time (which is what your anecdote is about). There are any number of languages where a competent working programmer can jump into a codebase in that language and be doing small tasks within hours to days, without any prior experience of the language. That's not to say that they'll be anything like expert, or that they won't bang their heads on things, but even making mistakes, they'll make progress towards a goal, and learn as they go. Haskell is very much not one of those languages.
The point isn't about reasoning, it's about code reuse. If I have something which is a monad which abides by the laws, then there's a huge amount of code I don't have to write because all the monadic code has been written based on the assumption (i.e. The precondition on the types) that types which implement Monad abide by the rules. This is the same for Applicative, Functor, Profunctor, Category, Bifunctor, ~~Cofunctor~~ Contravariant(functor) etc. etc. This is the fundamental reason why you do these things, it's is the pragmatic goal of code reuse, not (just) an academic exercise. 
There are two branches of theoretical CS, the complexity branch and the semantics branch. That's a coarse division but its approximately correct and . Many semantics people have been using monads for nearly 30 years. Complexity people have lagged behind (they're too obsessed with automata).
I was looking at the [slides for this talk](https://github.com/Gabriel439/slides/blob/master/lambdaconf/data/data.md) today while I watched the video, and I have to say that I am a huge fan of /u/tekmo 's style of importing stuff. I think it would be really awesome (and helpful to newcomers) if more people tried write Haskell in this style.
When I look at a data type that has two type parameters and they aren't what I want them to be I can either call `bimap` and make the change or I can pollute my namespace with more and more one off combinators. If I need some form of applicative side effect, maybe that'll be `bitraverse`. Without looking at the data type I can do a lot of reasoning with just those combinators and simply knowing that there is an instance available. When I have 3-4 different things that I might want to pass in, if all I'm doing is changing that one parameter I may be able to drop reference to the concrete things being manipulated entirely and get more general code, rather than write 3-4 different versions of the same function that all call one-off versions of a more general concept. Now, I could make up my own class here, but thats more names and cognitive overhead to add, and it penalizes library interoperability. Also, I probably won't pick a "good" set of laws with nice properties. If I work parametrically with `Functor` rather than with a concrete data type I'm saying a lot about what aspects of the thing I'm manipulating that I use, rather than hey, every possible thing that could ever be done with an `IntMap` or `[]` or whatever could be happening here, watch out. If you give me a function with a signature `Functor f =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; b -&gt; f c`. I can reason out exactly what that function does without ever looking at the code to a high degree of confidence. Change f to [], and that goes out the window. Give me a function (a -&gt; a) and it is one of two things. Give me a function Int -&gt; Int and it could do anything to that number. These abstractions may be borrowed from weird places, but they happen to have the benefit that they have withstood, in some cases, 70 years worth of extensive use in a large number of areas of mathematics, where they were used to explain ideas that were common but lacked common vocabulary. In computer science we can only really point at FORTRAN and LISP with that sort of pedigree. Unlike FORTRAN (and to a lesser extent LISP) these abstractions have a lot of practitioners still excited about their usage, who are still finding new things out. And frankly, neither of those is really focused on abstraction.
Yup, good tests require good code.
Then, how is it relevant here?
 newtype Tagged s a = Tagged a is both a Bifunctor and Profunctor that is not an Arrow. ;) *ducks* 
Re: infix symbol operators, I recommend hayoo (my favorite) or hoogle if you don't know of them. Google searching those tends to suck in my experience. 
It's about reasoning and reuse.
You want something like https://hackage.haskell.org/package/monad-control
The real story is less interesting, I'm afraid. Originally, I just wanted this to be a (long) appendix for haskellbook, to cover the extensions. But I have expanded the scope...as one does. I asked (this) Chris to join me because I liked blog posts like [this](https://chris-martin.org/2014/unwanted-haskell-triangle) and the fact that he keeps PRing Haskell docs all over the place. It seems, so far, like a good fit. :)
That book is basically complex. It was content complete a while ago and iirc they're only doing final tiny edits for a release 
In case you are wondering, I think others are downvoting because /r/haskell has seen many similar posts in the last months.
It should, `liftM2` should be deprecated.
Do you happen to know what the problem with providing quickcheck properties along with your typeclasses to enforce laws is? It seems so simple that I'm sure people have talked about it before. Is it just a code complexity/inertia thing?
Oh, so it's just a cultural issue then.
Oh I agree, but in convincing that it's something worthwhile, code reuse is a good practical of why, and it is only possible to reuse that code because of that ability to reason. DRY through preconditions and properties.
I've seen like 2. Are people watching new or something?
I sometimes check /new since the volume is usually low.
QuickCheck can't prove anything, but it covers most of the ground.
Maybe adding a nice Haskell FFI to the Haskell port of [Shen](https://hackage.haskell.org/package/shentong) would work for you. Unlike Chicken Scheme, Shen has a pretty flexible (optional) type system.
We should probably give up general recursion. When I say give up general recursion that doesn't mean one won't be able to do it. Code that can't be proven to terminate would be in a monad like any other side effect, so being total allows us to reason about total functions. This allows a whole host of optimizations. It also gives strictness analysis significantly more room to optimize. Furthermore, I would argue that the majority does not require general recursion and would be better written using higher order recursive functions that have already been proven to terminate. Also, the desire to make these language total comes from making the proofs logically consistent. With general recursion, it is trivial to prove any proposition. Haskell is already inconsistent as a logic, so adding dependent types, which would let one do this, will not automatically require Haskell to be total.
It would be even better if data instances could end in kind `Constraint` as well as `Type`. Then, one could write class instances. This would essentially make the distinction between data families and type families to be that of saturated and unsaturated arrows.
&gt; Cofunctor Cofunctor is the same thing as Functor.
Is it just shitty error messages and wonky operator symbols (I swear some of them are literally emoticons) that prevent the lens library from being easier to grasp? The operator symbols is just a documentation thing, but the error messages. Well, if that's all that's preventing lens from being more widely understood, what's preventing that from being fixed?
Thanks for the link, but I think Norman seems to agree with me: &gt; The opposite of "strongly typed" is "weakly typed", which means you can work around the type system. This is precisely what `unsafeCoerce` does, and similarly what `unsafePerformIO` does too, since Haskell's type system is advertised to make side effects explicit and `unsafePerformIO` can be used to violate this. Python is strongly typed, because everything in Python is an Object, and there's nothing you can do to violate this (short of C FFI obviously). Put differently, there's no way in Python you could put your program in an undefined inconsistent state. I think you'd have to try really hard to place Haskell and C categorically differently in the strong-vs-weak typing distinctions. However, how often you need to work around the type system in these languages is another matter entirely. In C, passing `void *`s around is the primary way of achieving polymorphism, but in Haskell, you only ever need `unsafeCoerce` buried deep inside libraries doing advanced stuff.
What exactly do you want to change? The haddocks themselves? Or just the layer of organization around displaying packages and package metadata?
Just the user interface: new navigation, re-prioritize which information is shown and when, search.
It seems good to me. One important feature which is gaining traction for one year is the Language server protocol (if you don't know : https://code.visualstudio.com/blogs/2016/06/27/common-language-protocol) One column to say if the editor is supporting the language server protocol would be nice. And, as a language server protocol defines all features a IDE can provide you can draw inspiration from it (especially the document section: https://github.com/Microsoft/language-server-protocol/blob/master/protocol.md)
Thanks for your correction. But actually it's incorrect too. See the edit to my original post.
Other people have tried to redesign haddock. It was not received well (even though I think the new interface looked much better, similar to the elixir hexdocs). I don't think an Elm UI will receive much love either. People will cry about it not being a static website, requiring JavaScript etc. Which I think is BS but whatever. I'll certainly give feedback on any effort which attempts to bring Haskell websites to the 21st century (in terms of design / UX).
I tried both and end up using Haskero because I was looking for a VS Code intero extension and Haskero just worked. Haskelly is trying to do more, compiling non stack projects, running tests etc, but the type hints and autocomplete wasn't working for me(a couple of weeks ago). That said, the Haskelly project is being built as a final year project by some university students who appear to be putting a lot of time and effort into it, so best of luck to them and who knows where it will be in a few months.
Just curious, what do you mean when you say that a function `a -&gt; a` can be one of two things? What else can it be besides the identity function?
Could be `undefined`.
You can see what GHC means by optimization [in this page](http://www.aosabook.org/en/ghc.html), if you search for "Optimisation". It seems to be a colloquial usage of "optimizing compiler".
Need to refactor some ruby code in order to make it suitable for unit testing. What do?
+1
I think how it's received is dependent on the changes themselves, esp as the OP isn't going to eliminate the current site for those who like it. I would welcome a better interface. 
Yes I know how `haxl` is translated but even in the applicative version: myPutStrLn "Hello" *&gt; throwM MyException *&gt; myPutStrLn "World!" I would expect that only "Hello" is printed. Ideally the semantics of `GenHaxl` or `Batcher` are exactly like `IO` except that it's operationally more efficient. To implement this desired behaviour I think we need to somehow turn `throw` into a `dataFetch` (or scheduled command in Batcher parlance). Then the worker that executes commands can break the list of commands at the first throw and execute the first batch. Then it should make sure the exception is thrown, that the batch of commands after the throw is written back to the IORef and that the blocked continuations belonging to the commands after the throw are handled properly. That last part is were I'm currently stuck.
Donated $200 as well. Hope this works!
Data races are not an issue in Haskell because everything is immutable. The entire state of our application is a data structure which is updated (not in-place) by a single thread that runs an event loop. Snapshots of the state are periodically written to durable storage (Redis), and all events since the last snapshot are also kept in durable storage. There is one potential issue where a client makes a decision based on an outdated version of the state, such as restarting a failed job which has been restarted and completed already. All events are designed in such a way that this does not cause problems; they are facts rather than commands. We also use QuickCheck to replay random events against random states, and we caught a few edge cases in this way, such as a worker reporting that it completed a non-existing job.
I personally love the `streaming` package, and am using it wherever I can. It has a very list-like interface - no custom composition operators... it's just function application! So far at work we use it in two places: 1. A large batch processing job that refreshes an internal db against an external API. 2. Generating zip files for customers, where each file in the zip is fetched concurrently (up to a max bound of concurrency). I also have a library [`streaming-postgresql-simple`](http://hackage.haskell.org/package/streaming-postgresql-simple) that gives you two ways to actually stream a PostgreSQL query (either single row mode or with cursors, depending on your requirements). I like this because `streaming-utils` gives adapters back to `pipes`, so I get some extra potential users for free. For 1, we consume a `Stream` at the top-level: updateFoos Metrics{..} connectionPool apiKey allIds = do setGauge metric'foosRefreshed 0 S.effects (listsOf fooRequestSize allIds &amp; mapLoggingErrors lookupIds tshow logUnreachable &amp; flip S.for S.each &amp; S.mapM refreshFoo) And for 2, uploadZipToS3 :: Text -&gt; Text -&gt; S.Stream (S.Of (ExceptT Text m Zip.Entry)) m () -&gt; m SignedUrl uploadZipToS3 fileName objectPrefix entries = do now &lt;- liftIO (fmap round getPOSIXTime) availableWorker &lt;- newQSem maxWorkers (failures, zipEntries) &lt;- do entryBuilders &lt;- S.toList_ (S.map (\io -&gt; Concurrently (bracket_ (waitQSem availableWorker) (signalQSem availableWorker) (runExceptT (tryAny io &gt;&gt;= liftEither . bimap tshow id)))) entries) fmap partitionEithers (runConcurrently (sequenceA entryBuilders)) traverse_ (logErrorNS "uploadZipToS3") failures let zipBytes = Zip.fromArchive (foldr (Zip.addEntryToArchive . (\e -&gt; e {Zip.eLastModified = now})) Zip.emptyArchive zipEntries) contentDisposition = "attachment; filename=\"" &lt;&gt; fileName &lt;&gt; ".zip\"" s3Path = objectPrefix &lt;&gt; "/" &lt;&gt; showDigest (sha256 zipBytes) &lt;&gt; ".zip" _ &lt;- do assetsBucket &lt;- awsS3AssetsBucket handlerAws (putObject assetsBucket s3Path (RequestBodyLBS zipBytes)) {poContentDisposition = Just contentDisposition} publicUrl s3Path That is, you give `uploadZipToS3` a stream of monadic actions that will form each zip file, and it runs these actions `Concurrently`, partitions errors, and then uploads a zip of what it can.
Thanks for sharing that, i guess GHC's allocator is just up to its task.
You're welcome, FYI it's actually part of my job ; ) 
I am still waiting for computer scientist to use adjunctions instead. To me (did a master in maths) they are much more insightful/basic than monads...
So in the first example is &amp; operating similarly to &gt;-&gt; in pipes? One way I like using (maybe abusing) pipes is it gives me an approximation to chaining of effectful operations without thinking too much about monad stacks. It seems like that might work a bit differently here. For example, I may have a sequence of computations which also write intermediate results/output to stdout or a log somewhere. Have you noticed a performance improvement relative to pipes and conduit?
wait the application pointed them to a trac page? that's pretty bad :/ is it online somewhere?
&gt; I personally love the streaming package, and am using it wherever I can Same here! Used it extensively when I worked for Signal Vine.
Oh, `&amp;` is just reverse function application - https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Function.html#v:-38-. &gt; It seems like that might work a bit differently here. For example, I may have a sequence of computations which also write intermediate results/output to stdout or a log somewhere I think this might be orthogonal - I use `mtl` type classes on the `m` in my `Stream (Of a) m`, and `Stream` itself implements most `mtl` classes, so I can easily mix effects into setting. &gt; Have you noticed a performance improvement relative to pipes and conduit? People worry far too much about performance. The moment you actually start doing IO that's irrelevant - IO will completely dominate your benchmarks. All I care about is that they don't leak space, which is a requirement all libraries satisfy.
I'm working for the infrastructure team of didi@china, not many people use haskell at work here but i do use them to build various things ; )
i understand your point about performance, but for statistcal computation and machine learning it does matter. IO is not happening on every iteration.
You could also move the need of forking from your implementation into your free interpreter.
Literally lol :)
In terms of adoption - conduit and pipes are both very mainstream, heavily used, stable, well documented, and very well supported. It's interesting to hear that some people have used also streaming in production. As a heavy user of conduits, it's hard for me to see from the docs for streaming what it is trying to improve over conduits. Conduit also provides a full set of Data-List-like combinators. The syntax is only slightly different than what you use for normal lists, and easy to learn, understand, and use in practice. Same for pipes iiuc. Making it even more similar doesn't seem like such a huge gain. In terms of performance, I can say we are very satisfied with conduits. They are deeply and aggressively tuned for performance. You mentioned benchmarks that claim that streaming is even faster, but didn't give any links. Or links to responses from the maintainers of conduits and pipes.
streaming allows expressing "group operations" in a very natural way, without requiring each group to accumulate in memory. It was more cumbersome to do that in conduit, IIRC.
Despite its heavy-duty type machinery, I find generics-sop more intuitive than regular generics.
As someone who's tried, and failed, to get into lens, I felt it was mainly the complexity of the types, and the level of abstraction you need to think at. The error messages are a consequence of that.
&gt;I have found to prefer the quality of the OCaml community to the size of the JavaScript community. I don't understand what you're saying here. that even though the ocaml community is small its quality makes it as useful as javascript's sizeable community?
What are your questions?
First, while a value of type `IOArray Int Int` is a mutable array, a value of type `IO (IOArray Int Int)` is not: it's an IO computation which produces an array. So `funarray` is not a map from string to arrays, it's a map from string to IO computations. In an imperative language, an `IO (IOArray Int Int)` would be represented as a function with no arguments returning an array. Second, while your `funarray` is indeed a globally-accessible value, it is still an immutable value, which means that you will not be able to insert new values into it, and it will remain empty. In particular, if you call both `runState putmutablearray funarray` and `runState getmutablearray funarray`, the first call will *not* mutate `funarray` and so the second call will *not* see the "value-table" key. Instead, `runState putmutablearray funarray` returns a *new* Map which contains this key, while the original `funarray` remains empty. So if you want `getmutablearray` to see the key, you have to manually pass the new Map to `getmutablearray`, like this: let ((), funarray') = runState putmutablearray funarray in runState getmutablearray funarray Or equivalently, let putThenGet = do putmutablearray getmutablearray in runState putThenGet funarray In fact, the whole point of State is to allow you to write stateful-looking code using do notation, without having to manually pass the new value to the next function on every single line. Under the hood, though, those values are still being passed around, and at the end you still get a new Map while the original one remains unmodified. Finally, if you want to use an `IOArray Int Int`, you will have to call `readArray` etc. from an IO computation like `produceValue :: BoardState -&gt; IO Int`, not from a pure function like `value :: BoardState -&gt; Int`. The good news is that unlike State computations, IO computations really do perform mutations, they don't return a new `IOArray Int Int`. I recommend to practice your understanding of State and IO computations by writing a program consisting of only State computations (except for `main`), and then by writing a program consisting of only IO computations. Only then will you be ready to mix the two in the same program.
Pretty much any time you care about preserving the hierarchal structure of your streaming functions. Let's say every `X` yields many `Y`s, every `Y` yields many `Z`s, and you want `Map X (Map Y (Map Z Content))` as a result. Each `Z` needs to stream up its `Content`, and each `Y` needs to stream up its `Map Z Content`.
Here's how this problem is solved in the Yesod ecosystem: Yesod [provides a function](http://hackage.haskell.org/package/yesod-core-1.4.32/docs/Yesod-Core-Handler.html#g:25) `forkHandler`, which forks a thread but in the library's `Handler` monad stack rather than in `IO`. It required some work for the yesod library developers to set that up, but it's mostly straightforward. They call `forkIO` with a function whose arguments are the user-provided function plus whatever else is needed to set up the monadic environment (state, etc.). When doing this exercise, you may come across parts of the monadic state that you really don't want to pass into a forked thread. For example, in a web application, you probably don't want the request body in there, because the thread may outlive the request-response and you will end up with a memory leak. And you probably want a copy of the response, not the actual response, for the same reason. It's easy for a user to override those defaults though if that's what they really want. (Pass the request body as a parameter. Wait on the thread before sending the response, and use its result to modify the actual response.) In any case, it's an exercise you only need to do once. After you write your monad-specific fork once, you can use it anywhere in your application.
If it's just about caching, you can just as well use a normal web server and set the appropriate caching headers. There is no need to use electron.
&gt; pretend its coherent Seems like a good way to make your logic (i.e. the logic corresponding to your type system) inconsistent. If you do that, you might violate any safety / correctness that your proof terms ensure.
&gt; https://hackage.haskell.org/package/streaming Any responses from the conduit or pipes teams? Those benchmarks are tiny operations. I'd like to see a more realistic benchmark. Like streaming a 10 MB text file, performing some operation on each line, and writing it back out. Or streaming 10 MB to or from the network.
Unsurprisingly, because the Haddock codebase is in horrible shape. The person who did the redesign about a year ago must have put a lot of effort into understanding the code and rewriting it. If anyone plans to do a redesign, it might be better to start with a clean slate. And start with the design/UX first.
That's cool. Thanks!
When benchmarking `tdigest` package, I used `machines` to do the benchmark, as I wanted to print logging messages in between. Generating uniform numbers, foldl' average: 0.75sec Generating normal distributed, foldl' average: 1.39sec Generating normal distributed, machines based average: 1.78sec Generating normal distributed, machines tdigest-median: 2.09sec Generating normal distributed, foldl' tdigest-median: 1.84sec Generating uniform numbers, foldl' tdigest-median: 1.13sec I don't know what I try to say with this numbers, but seems that statistical computation will still dominate the CPU time, not the feeding the numbers thru the pipeline. In my case, generating more sophisticated inputs adds more time than using tdigest or (and!) machines. *EDIT* and if machines is *the* slowest one of streaming libraries, than I second /u/ocharles opinion, the streaming library you choose won't dominate your CPU usage. And if you have to squeeze everything from hardrware, than maybe the composability of streaming libraries is something you can sacrifice later, when you have your pipeline working.
Because Haskell is so different from mainstream languages that the new programmer analogy is actually more relevant than the case of picking up an additional language.
Great initiative!
Thank you sharing this - the two tutorials look great!
It processes around 26k entries and takes a few hours.
My post above uses this grouping: S.effects (listsOf octopartRequestSize allCacheIds &amp; mapLoggingErrors lookupCacheIds tshow logOctopartUnreachable &amp; flip S.for S.each &amp; S.mapM refreshPartWithCategory) With -- | Chunk a stream into non-streaming lists. listsOf :: Monad m =&gt; Int -&gt; S.Stream (S.Of a) m r -&gt; S.Stream (S.Of [a]) m r listsOf n = S.chunksOf n &gt;&gt;&gt; -- S.Stream (S.Stream (S.Of a)) m r S.mapped S.toList -- S.Stream (S.Of [a]) m r Is one case of grouping, though I immediately flatten each group into a concrete list. mapLoggingErrors :: (MonadCatch m) =&gt; (a -&gt; ExceptT e m b) -&gt; (SomeException -&gt; e) -&gt; (e -&gt; m c) -&gt; S.Stream (S.Of a) m r -&gt; S.Stream (S.Of b) m r mapLoggingErrors m handleException handleError = S.mapM (fmap (join . bimap handleException id) . try . runExceptT . m) &gt;&gt;&gt; -- :: S.Stream (S.Of (Either e b)) m r S.partitionEithers &gt;&gt;&gt; -- :: S.Stream (S.Of e) (S.Stream (S.Of b) m r) S.mapM_ (lift . handleError) -- :: S.Stream (S.Of b) m r Also does this "grouping" - look at the type of `partitionEithers`: partitionEithers :: Monad m =&gt; Stream (Of (Either a b)) m r -&gt; Stream (Of a) (Stream (Of b) m) r It gives a `Stream` of failures and then a `Stream` of successes. In my `mapLoggingErrors` I immediately eliminate the errors with `mapM_ (lift . handleError)`, and then return the `Stream` of success.
I use `conduit` almost exclusively at this point. `pipes` has a nicer API and documentation, but `conduit`'s focus on resource management/finalizers make me feel more confident about my software. I have not found a compelling reason to switch to `streaming`, though it does seem to have the most elegant implementation. Additionally, [`stm-conduit`](https://hackage.haskell.org/package/stm-conduit) is a really great library for getting asynchronous pipelines. Our conduits at work are mostly dominated by HTTP requests, DB inserts, and SMTP sends. Substituting a single operator `=$=&amp;` and getting really good concurrency for free was a massive performance benefit.
Well, Mateusz was working on it quite regularly but he simply lost all his free time and effort, and nobody has regularly maintained it directly since. For GHC developers, Haddock is literally almost never on the critical path -- there's about 10,000,000 better things to do. In fact I just found the patch you are referring to, and it [really does not look that incredibly intimidating](https://github.com/haskell/haddock/compare/master...lamefun:pisces). There's actually a relatively small amount of Haskell changes. "rewriting it" may only be true in a narrow way (the UX, the Haskell is fairly in tact). However, I cannot find this PR upstream anywhere -- so the author may have also, simply, run out of steam before submitting it. That happens a lot. That said I don't disagree Haddock's code could be better. I mean, sure, everything could be better. But I think it really has little to do with some old code and more to do with "literally nobody who was doing has time anymore". Haddock has not seen much development since the authors of the redesign stopped working on it. Most if it is just keeping up with new GHC features. Updating it would require merging in some stuff for that, but I don't think it's ridiculously overwhelming and you probably don't entirely *have* to throw the baby out with the bathwater. There's also the fact working on documentation generators IMO (having worked on Haddock), and shit like this -- is ridiculously non-fun work and is literally never appreciated by anyone it feels. If you look at the prior threads, literally half of it was just constant back-and-forth about UX changes. It's draining. This kind of stuff is important, extremely underappreciated, and almost always ends up disappointing *someone*, so I don't find it surprising most people don't want to do boring work. EDIT: I can see how the last passage might seem accusatory (as if you're not appreciating it). It isn't! I haven't touched Haddock in a while, it's just not fun, is what I was getting at.
&gt; new navigation, re-prioritize which information is shown and when Although I'm sure there is room for improvement, it's hard to do a redesign that a majority of people find prefererable. And often baked into the re-prioritizion of information are preferences by the designer that are not universally by the developers. For example, how are you deciding what to re-prioritize? Should typeclass instances be more or less visible, etc... I've also often seen a tendency to make docs look more "uniform" and "minimal", especially using muted colors.. etc.. this often decreases readability at the expense of pleasing UX to the designer's eye. I may be wrong on this, but it looks like stackage even reverted their css at some point to look more like the hackage docs, than the font &amp; style changes they'd done in the previous version?
I really like `streaming` and have been using it more lately; not just for the types of things that `pipes` and `conduit` do, but also as a replacement for `FreeT`. However, `streaming` does have a smaller ecosystem, and there is currently no built-in support for writing things like consumers or "pipes". It's very production-oriented at the moment, so the idiom is that you take a producer as an argument and draw from it directly, much like pipes' `next` function. In some contexts this is simpler, and clearer; but in others it's not quite as natural as using `pipes`.
Separate cases can be made for Constraint and #. The concern with Constraint is ensuring you don't violent coherence by letting users pack it up themselves. Heck, there are times when I want to be able to inject them in closed kinds as well. e.g. postulating the existence of a type so I can hang instances off of it. I know this is wrong and evil, but its useful. ;)
&gt; +#interface .top &gt; .arguments &gt; table &gt; tbody &gt; tr &gt; td.doc { As a web/frontend developer, I find that *very* intimidating. And it makes me think why the changes to the Haskell code were so superficial (as you correctly observed, those aren't that complex). Maybe the Haskell code was so hard to understand/change that he found it easier to write 6-times nested child selectors in CSS. Probably. Because nobody uses this deep selector nesting on purpose. And I disagree that work on documentation is underappreciated. I know someone who did some improvements on the docgen of another language and his contributions were very well received.
In this context does that mean it fails to return a value at all?
&gt; I've always been mildly infuriated when people replace haskell operators with a more mathematical one in papers. While I do sympathize, you can technically use the more mathematical operators in your source code, too, by enabling ```-XUnicodeSyntax``` and importing a few modules. Haskell source is Unicode (even without the extension) and both names and operators can include Unicode characters. The following are all perfectly valid Haskell: liftIO  MonadIO m  IO   m  do { x  readLn; ... } x  [3, 5, 7] You need the extension to use Unicode for built-in syntax (``````, ``````, ``````, ``````), but the name `````` (epsilon) and the operator `````` (```elem```, element of) are valid without any extensions. More info here: https://wiki.haskell.org/Unicode-symbols
`{-# LANGUAGE AgdaSyntax #-}` :)
Prelude&gt; :t concat @[] Also, using `Control.Monad.join` would have caught this error. 
These instances are surprising in their behavior even if they are legal. Without them certain semantic errors are type errors, which is one of the primary advantages of static typing. We shouldn't sacrifice this advantage in the name of theoretical completeness.
Well, as a counterexample, let's take a binary radix sort, over k bits. It makes k passes, each with O(n) time, for O(nk) total time. Now, if you assume that each value is unique, then you have n  2^k which means that k  log n, so in the case of the longest list possible, you have O(n log n). That bound is pretty difficult to escape without parallelizing (as in sorting networks or something similar).
[removed]
Can you give me an example of what you mean?
I have Text so I am needing: decode :: FromJSON a =&gt; Data.Text -&gt; Maybe a This seems to be an inefficient dance when I've already taken the trouble to decodeUtf8 the data into the Text; so would I be better off creating a ByteString to hold the data? Then I will need: decode :: FromJSON a =&gt; ByteString -&gt; Maybe a which is also close to Data.Aeson.decode (modulo Lazy)
To some people, `Foldable` means "this acts exactly like a list", rather than "this contains 0 or more values which can be aggregated by a function".
Here is your modified decode function: import Data.Text.Encoding (encodeUtf8Builder) import Data.ByteString.Builder(toLazyByteString) import Data.Aeson (decode) decode'' :: FromJSON a =&gt; Text -&gt; Maybe a decode'' = decode . toLazyByteString . encodeUtf8Builder As for efficiency, you could just use decode'', profile your application, and if this really happens to be a bottleneck simply use a `ByteString` to hold your data. This is the type `aeson` uses and there's no way around it. If it's a strict bytestring, you can convert it to a lazy one with [fromStrict](https://hackage.haskell.org/package/bytestring-0.10.8.1/docs/Data-ByteString-Lazy.html), which shouldn't be too expensive.
And this is why `Traversal`s and `Fold`s are superior to `Traversable` and `Foldable` instances.
&gt; Also keep in mind that (apparently) only objects and arrays are valid top-level json so encode/decodewill (I think) fail on them. This was actually changed in 0.9: https://github.com/bos/aeson/blob/master/changelog.md#0900. &gt; decode "123" :: Maybe Int Just 123 The limitation wasn't required by the original ECMA 404 or the latest RFC, so I'm glad they fixed it.
Wow, what a horrible style. John Harrop is right in that functional programming is more difficult to read when doing real world programs. I do not agree. It is a matter to create significative variable names and partition the expression in understandable let and ins, instead of trying to demosntrate such horrible compositions 
I did remove my upvote from the one I don't agree with. I just figured I'd jump out with some summaries of the sides to get things going. Please forgive me if I did a bit of straw-manning.
I dont know if streaming is a category in itself. Haskell is naturally stream oriented so I don'k know if we are creating an artificial barrier if a library is specifically for streaming, when streaming is part of a problem and not the most important part of it. The second example is what would be called network and concurrent programming rather that streaming proper. This is how it would look like using transient, shortening the less interesting bits. Since transient deals with streaming *and* concurrency *and* exception handling among other effects, it can raise a bit the level of composition: uploadzips3 :: Text -&gt; Text -&gt; TransIO Text -&gt; [Url] uploadzips3 fileName objectPrefix entries= onException $ \(e:: SomeException) -&gt; liftIO $ print e urls &lt;- threads maxWorkers $ foldr (&lt;&gt;) mempty processOne entries return urls where processOne entry= do zipEntry &lt;- async entry url &lt;- liftIO $ s3Stuff zipEntry fileName objectPrefix return [url] Both the upload *and* the S3Stuff is done in parallel. The concurrency only is resolved at the very end when the results need to gathered. so there is more work done in parallel mode. This would not succeed if some of the entries fails, but it can be arranged to make the workers independent by catching the error in the worker and returning the error as a list element that can be added to the result by the monoid ( `:` can be used in the fold expression instead of `&lt;&gt;`) processOne entry= do ezipEntry &lt;- async $ try entry case ezipEntry of Left e -&gt; return [show e] Right zipEntry -&gt; do url &lt;-liftIO $ s3Stuff zipEntry fileName objectPrefix return [url] 
I'm almost certain this is where we'll end up. Haskell type classes are only really well-behaved and future-proof when they are unique and Traversable/Foldable instances only unique when they are trivial. When you are thinking of a particular traversal (even the the natural one) then you should probably just pass it around, using ReaderT or something if you need to. I can see a future where something like: Unique :: (k -&gt; Type) -&gt; Type Unique p = Unique { witness :: Exists p , universal :: (another :: Exists p) -&gt; witness == another } gets special treatment by the type checker and used instead of type class instances or implicit arguments. And, we take everything that isn't provably unique and pass it around explicitly (even if we use ReaderT to make the syntax nicer, it's still explicit). It may take a little work, but we should be able use higher inductive types to even express uniqueness "up to isomorphism" and use this for "morally" unique values. (recall that:) Exists :: (k -&gt; Type) -&gt; Type Exists p = Exists { value :: k , proof :: p k } 
Yes, RFC 7159 defines JSON in terms of Unicode code points, not bytes. (You can tell because the `unescaped` production contains `%x5D-10FFFF` on the right hand side.) However, most of the interesting bits of JSON are in ASCII, and every JSON value can be represented without using non-ASCII characters. (Unicode is of course allowed in member identifiers and string values, but you can always use unicode escapes.) Also, attoparsec up to 0.9.1.2 didn't really have support for Unicode parsing and attoparsec is part of what gives Aeson it's speed. So, Aeson operates on ByteStrings -- I'm not sure what encoding it assumes when it hits a literal non-ASCII character in a string value or member identifier. (Probably UTF-8.)
&gt; Haskell type classes are only really well-behaved and future-proof when they are unique and Traversable/Foldable instances only unique when they are trivial. It is often useful to create instances for typeclasses when they aren't unique. For example, the monad instance for `[]` is not unique.
But you could use a `Fold` instead.
Read the title of this post as "Deprecate Foldable in favor of Either" and was all wtf.
Thanks! I've only been using Opaleye for like 48 hours. A suggestion: one thing that I think immediately helps clear up intuition is to dispense with the notion of "read types" and "write types" or whatever when dealing with queries. It's a lot simpler to simply think of Opaleye queries as a kind of *function that operates on SQL queries*, and when you think of it like this, the sort of "compositionality" Opaleye promises is very obvious. Functions obviously compose. Opaleye just represents "SQL queries" (pure functions from dataset -&gt; result set) as Haskell functions. Here's what I've done in my code: -- | The "Query Arrow" -- -- The following is true: -- &gt;&gt;&gt; Query a = () :~&gt; a type (:~&gt;) a b = QueryArr a b Now, you can think of an ordinary `Query` as a "wobbly function" from the type `() :~&gt; a`. `()` is unit, but for a query, unit really means "I need no input to construct this query". It's kind of more like ML, in that sense (where you represent "actions" like `IO Int` as a function like `unit -&gt; int`.) Columns are directly represented by a data type. A value of this data type represents a row from the database: data Birthday' a b = Birthday { _bdName :: a , _bdDay :: b } type Birthday = Birthday' String Day type BirthdayColumn = Birthday' (Column PGText) (Column PGDate) So if I have a `Birthday` in hand, that was actually a row from the database I got (or that I'm going to put in). The record type directly reflects the columns. Or you could call it the "Column type" Now we have queries. birthdayQ :: () :~&gt; BirthdayColumn -- Query BirthdayColumn birthdayQ = queryTable birthdayTable Anything which has a wobbly function type is actually a *transformation from a column type, to another column type*. The unit type `()` represents "no column type", i.e. the above function is really more of a value. The traditional type, `Query BirthdayColumn`, makes it look like a value, but it is really a *transformation*, still. Thus, if we are transforming column types, we are really *shaping* the results of the database query. For example, what if we want to change the "shape" of `BirthdayColumn` to only return the name of the person? With arrow notation: :t birthdayQ &gt;&gt;^ view bdName birthdayQ &gt;&gt;^ view bdName :: QueryArr () (Column PGText) Note the type. That's the same thing as `() :~&gt; Column PGText`, so it is wobbly. But that directly composed them. What if I wanted to represent this "shape transformation" as its own wobbly type: bdNameQ :: BirthdayColumn :~&gt; Column PGText bdNameQ = proc row -&gt; do returnA -&lt; view bdName row Or, `bdNameQ = arr (view bdName)`, as `arr` can lift any pure function into a wobbly function, something like `arr :: (b -&gt; c) -&gt; (b :~&gt; c)` Obviously, if I have `() :~&gt; BirthdayColumn` and `BirthdayColumn ~:&gt; Column PGText` -- I can just smash them together with a compose operator, and get `() :~&gt; Column PGText` So "wobbly functions" are very similar to ordinary functions, but instead your lambda has to be a bit different (using `proc`) and you use the slightly different arrow notation. Translation to ordinary `do`: - `proc (x, y, z) -&gt; do ...` means `\x y z -&gt; do ...` (i.e. you must curry your "wobbly functions") - `f -&lt; (x, y, z)` means "Apply `f` to the tuple `(x, y, z)`". It's like saying `f x y z :: Monad m =&gt; m ...`, just with "arrow type" instead of monadic type. - `&lt;-` is mostly the same. `v &lt;- f -&lt; (x, y, z)` for arrows is the same as `v &lt;- f x y z` in ordinary do notation, so it just binds a value. - `return` is `returnA`, and it must also have arguments applied with `-&lt;` notation. Also, once you think of them this way, you can begin to look at types like this: ```f :: a -&gt; (b :~&gt; c)``` -- what does that mean? Well, you actually have a function which takes an argument and returns a *row transformation* in a sense. Really, you have a type here operating at two levels, over different domains; one is the domain of "ordinary" Haskell values, while another (the returned value) is in the domain of "queries". Similarly, what does `k :: (b :~&gt; c) -&gt; a` mean? It must be something that has to otherwise execute or "extract" an actual Haskell value from a transformation. This correspondence between "wobbly types" and "ordinary types" also hints to the key projection between a `Birthday` and a `BirthdayColumn` -- Columns are wobbly, and transformations of columns are wobbly functions, but ordinary types are not. But this is the basic essence of Opaleye from what I can tell, and why it's composable: because it generalizes the notion of function arrows to "query arrows" or "wobbly functions" which operate as a transformation over columns and the result set. Opaleye then "compiles" these wobbly functions into SQL queries -- when you shape the result of your query by composing "wobbly functions", the resulting "shape" of the SQL query is changed too. I was thinking of submitting my thoughts upstream because I think the documentation for `QueryArr`, which mentions "`QueryArr a b` is like the Haskell function `a -&gt; b`" should be one of the key points for introducing things, and hammered in early on, IMO. Many things immediately clicked from this insight.
&gt; A quick Hoogle search shows nothing with this name https://www.stackage.org/lts-7.8/hoogle?q=Fold First hit: https://www.stackage.org/haddock/lts-7.8/lens-4.14/Control-Lens-Fold.html &gt; I have never ever run into issues other than sometimes cryptic build error messages where Foldable instances would be causing problem `toList (1, 2) = [2]` is a pretty enormous problem.
It really says something about the clarity of RFC 7159 when we have to deduce what the BNF is actually talking about _from the values themselves_. Argh!
Not what I was trying to say; my issue was that haskell can't import definitions from multiple modules with the same name and select them based on context (i.e. required types, arity) and only require disambiguation at the call site when inference isn't possible. i.e. why can't ghc figure out that `lookup k m` when `m` is a `Data.Map.Strict.Map` means `Data.Map.Strict.lookup` and `lookup k m` when `m` is a `Data.Map.Lazy.Map` means `Data.Map.Lazy.lookup` (without using type classes)? And in cases where inference can't clearly distinguish this, why can't I just be explicit at the call site and say `Data.Map.Strict.lookup k m` instead of having to import the module at the top and coming up with some strange additional name and multi-letter abbreviation for it?
Did you guys look at `tisch` at all? The [article describing the motivation for it](http://ren.zone/articles/opaleye-sot) is one of my favorite Haskell-related posts. I briefly tried using it in a project but I ran into a compiler error that I found very difficult to parse and then I switched to a different stackage snapshot and all the package bounds in [`tisch.cabal`](https://github.com/k0001/tisch/blob/master/tisch.cabal) wouldn't resolve for me anymore. I've been thinking about trying again because it seems like an interesting approach to removing Opaleye boilerplate.
Well, honestly, it might be made clear elsewhere in the prose, I was skimming it and that was the first thing that caught my eye to indicate a Unicode definition. There's a fairly strong recommendation to use UTF-8 for JSON documents, but certainly not a requirement. It at least mentions UTF-16 as an option. (If your data is text heavy and that text is CJK or Emoji heavy, then UTF-16 might be smaller, faster to parse, or both but probably not.)
I think problem here is the name of the function not what it actually does. What it does is completely sensible because it is consistent, but its semantic meaning is not always a conversion to a list.
Yes, that's exactly what I meant. When it comes to communities, I would prefer a smaller community with smart people than a larger one with all kinds of people. I guess there is some minimal reasonable community size for a language, but my experience (in OCaml, Factor and Io) has been that the smaller communities are pushing very hard to be as helpful as possible.
To be fair, this has been sort of a long standing gripe of theirs. New management decided to take it a bit more seriously, I guess.
In years past we were able to have a little more room to enter ideas page link stuff and linked to 2-3 different places. Whittled down to just a trac full of 10 year old ideas? Yeah, that looks terrible.
What's your opinion on Yesod?
Dude... Look at my suggestion all right? round :: (RealFrac a, Num b) =&gt; a -&gt; b We can implement that now with fromIntegral, we will call it `round'`: round' :: (RealFrac a, Num b) =&gt; a -&gt; b round' = fromIntegral . round -- indirect way of getting more or less what I want -- might not be as efficient as fixing it at the source -- but this has the EXACT type I suggested Now we put in your little function: f :: Int -&gt; Double -&gt; Int f i x = i + round' x It compiles just fine, and works exactly as intended. Can you please just admit that your first point was completely wrong, everything works EXACTLY the same as before for that specific function. Like with the type signature you provided and the function you created this change is a complete no-op. Like I don't mean to be rude but you are seriously pissing me off, and top it all off by being condescending after being called out for being wrong. Why would you comment on something you don't seem to understand, half of your previous comment was complete BS, because it literally works EXACTLY THE SAME.
Yes. I think the option being bandied about at present is instance IsString s =&gt; MonadFail (Either s) This instance doesn't require FlexibleInstances on its own, so infers well, and will allow folks to use `Text` or other things as well. `fail` will no longer infect the entire `Monad` for Either, so it can incur additional constraints.
&gt; Opaleye then "compiles" these wobbly functions into SQL queries One thing I never understood about Opaleye, but also haven't looked into, is how does it (if it does) really restrict the `QueryArr` world to *just* the things that can be compiled to SQL? Arrows have `arr :: Arrow a =&gt; (b -&gt; c) -&gt; a b c`. But that means I can lift *any* Haskell function to `QueryArr`. Which in turn means (it would seem) that a `QueryArr a b` may not be a pure-SQL arrow, but may be roundtripping through my application. Which seems like it could be bad. (`hasql` is also in a similar situation, I think.) I believe there are fairly complicated workarounds, but the ones I can think of are not very nice...
That was my initial impression on what `MonadFail` ought to be, but I encountered (what I think may be well deserved) skepticism about the defaults, namely that a parser like attoparsec may be incorrectly prohibited by the instance. For instance, you might want something like `eitherP "foo" "bar"` to have type `Either (Parser Foo) (Parser Bar)` to exist and not have unintended conflicts. When I was presented with I demurred; was I too conservative?
It shouldn't, but I'm guessing it will for backwards compatibility.
Get better soon! Our community can't lose your valuable contributions like these design insights.
Thank you for the thoughtful comment. Three building blocks of Opalaye that have taken me very long to understand, yet I don't understand them completely - * QueryArr and Query and how Opaleye composes/transforms and abstract representing of an SQL query in Haskell-land. * The QueryRunner which finally converts a Query to an SQL string; and how it interacts with the `constant` functions * How Opalaye converts SQL rows back into Haskell data types and what the `Default` class has to do with it. I'm undecided if explaining these concepts upfront would have been better, or only after showing how to do basic CRUD applications. are these internals that not everyone needs to bother with, or necessary concepts without which you can't use Opaleye effectively?
Having to import lens for something so trivial would be extremely annoying.
I feel like you could make this exact same argument against all the `Functor`, `Applicative`, and `Monad` instances on trivial types like `Either` and `Maybe`. You can always replace type classes with explicit dictionary passing. 
This is one of the major complaints about `Arrow` in general (that it requires the ability to embed any Haskell function, which is majorly restrictive). I don't know and admit I haven't looked into it, either. I just write the queries! Paging /u/tomejaguar, as I can only provide so much needless dribble.
You know the size of a tuple at compile-time, but it's difficult to write a type family that can compute the length of an arbitrary tuple because you must supply an instance for every possible tuple type and moreover you're restricted by length to 62 columns because that's the limit in GHC. Yes, you can do it with TH, but that's not such a clean solution. See [here](https://gist.github.com/rahulmutt/840e2b67796c131d2345be4df7fa7858) for how to compute the length of an HList generically.
I suspect the amount of time people have spent bickering about this is far greater than the amount of time anyone has spent confused by it. Compare the few seconds the student took to ask about it vs. the collective man hours invested in this mail thread alone.
To go into a little more depth, suppose you have some transformer like type App = StateT S (ReaderT R IO) And now you want to use a callback function, like: forkIO :: IO a -&gt; IO ThreadId Well, you can't just `forkIO` an `App a`, because `forkIO` only takes an `IO`. So you need to convert an `App a` into an `IO` somehow. One way to do that is to hoist the action out. makeIOAction :: App a -&gt; App (IO a) makeIOAction action = do s &lt;- get r &lt;- ask pure (runStateT (runReaderT action r) s) which would allow you to write something like forkApp :: App a -&gt; App ThreadId forkApp action = do ioAction &lt;- makeIOAction action liftIO (forkIO ioAction) It gets worse, though. Consider something like: withFile :: FilePath -&gt; (Handle -&gt; IO a) -&gt; IO a Well, this is a little more awkward. We can make another variant of `makeIOAction`: makeIOCallback :: (a -&gt; App b) -&gt; App (a -&gt; IO b) makeIOCallback callback = do (s, r) &lt;- (,) &lt;$&gt; get &lt;*&gt; ask pure $ \a -&gt; runStateT (runReaderT (callback a) r) s But this quickly becomes unsatisfying, as now you need to either uncurry all your functions to use with this, or you have to define even more of these functions. Instead of making a function `App a -&gt; App (IO a)`, maybe we can embed the IO action more directly. The essence of this pattern is given by `control`, with a simplified type signature: control :: ((forall a. App a -&gt; IO a) -&gt; IO a) -&gt; App a which is typically used like: foo :: App Text foo = do control $ \runInIO -&gt; -- `control` is in App withFile "wat.txt" $ \handle -&gt; -- now we're in IO! runInIO $ process handle -- back in app process :: Handle -&gt; App Text 
Well, there's already an HTTP REST API to hackage, if I understand it right. So this is the target scenario for a front-end framework like elm. (?)
Yep, definitely an installed app based in something like Electron would work too. But that sounds like more work. Also, a higher burden for adoption because of harder deploy and upgrades.
&gt; Although I'm sure there is room for improvement, it's hard to do a redesign that a majority of people find prefererable. I agree. But it'll be easy to make something I and a small band of others like. If the crowd likes some of the ideas, maybe they'll be picked up by https://hackage.haskell.org. 
Thanks - excellent work. The chart has me considering installing the Microsoft editor on my Mac, which I've been resistant to doing.
&gt; Haskell type classes are only really well-behaved and future-proof when they are unique Correct me if I'm wrong, but the current `Foldable Either` instance is pretty darn unique isn't it? So this seems like a very strange time to bring this whole thing up.
*Insighftul* dribble.
The real mistake was returning a tuple, which is for heterogeneous collections, from `mostImportantTwoIDs`, it is an inherently homogenous result and I would immediately get mad at the person who returned a tuple instead of blaming the only lawful instance of foldable for tuples.
No, they can't. Just look at the type of `toList` toList :: Foldable f =&gt; f b -&gt; [b] So for tuples: instance Foldable ((,) a) -- b definitely not in scope So: f = ((,) a) toList :: (,) a b -&gt; [b] I think you can use something like `MonoFoldable` to make it work. But IMO that is the absolute wrong way to go, tuples are heterogenous data structures, if you want a homogenous structure use a list, or define your own type like `data List2 a = List2 a a`.
There are no global data structures in Haskell; variables have lexical scope (i.e. inner function scopes can see the variables of their calling context), which personally I find easier to reason about. In your code above, start by getting rid of all the type annotations. The compiler will infer more general versions of the types, which become more specialized as you add pieces (for example, if you say `(x, y) &lt;- get`, the pertinent pieces of State will have to pass around a tuple of things rather than a thing alone). The "IO monad" is a placeholder name for the functions that may perform input/output computations. "Computations" are more general forms of "functions", in the sense that the binding environment may change from one computation to the next, which is why things like _mutable vector_ functions (https://hackage.haskell.org/package/vector-0.12.0.0/docs/Data-Vector-Mutable.html) _return in a monad_ rather than returning pure values.
But you still need to depend on lens...
For better or worse, Haskell isn't symmetric w.r.t. order of arguments in functions and type constructors. Privileging the second argument of Either is an example of that. It kinda goes against my math intuition, but it's ingrained in the language by now. I wonder how a more symmetric language could look like.
I actually have used the foldable instance of Either as well as the traversable instance, the same way I would have for Maybe. Mainly with functions of the form `traverse_ print $ someFunctionThatCanFail someInput`, particularly with things like lexers and parsers and semantic checkers and so on. So I would not be happy if they got removed. I agree with Edward that this is a teaching moment, and not a moment for a cool and sometimes useful feature to be removed.
I would say, _due to_ its heavy-duty type machinery, `generics-sop` is more intuitive than regular generics. But then I am biased :-D Either way, glad you like it :)
Your code looks more like OCaml than Haskell. Without advances FFI/unsafe tricks, you still have to explicitly pass values you want to be mutable, even if they are global. writeSomeValues :: IOArray Int Int -&gt; IO () ... readSomeValues :: IOArray Int Int -&gt; IO [Int] ... type MyMap = IORef (Map String (IOArray Int Int)) writeSomeMore :: MyMap -&gt; IO () .... new :: IO MyMap new = newIORef mempty main = do value &lt;- new writeSomeMore value Forget about the State monad for now, until you are more familiar with the language and just experiment with IO and IORefs. Later you can use the state monad to do this without IORefs to turn writeSome :: Map String (IOArray Int Int) -&gt; IO (SomeResult, Map String (IOArray Int Int)) into writeSome :: StateT IO (Map String (IOArray Int Int)) SomeResult Although IORefs are the right abstraction for this use case in my opinion, not StateT.
Donated 200$ and got Google match the donation with another 200$ ;) 