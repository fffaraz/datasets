 #!/usr/bin/env bash # Use gnu time, not shell builtin time env time -o foo.txt --append ld "$@" Just replace `ld` with whatever linker you actually want to use. Then you just have to instruct GHC or your build tool to use this as the linker.
IO manager http://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Event.html, its what everything in haskell uses in base
&gt; My current approach is to start an async recv on all the sockets, add a brief thread delay, then poll them all I would instead use [`waitAny`](http://hackage.haskell.org/package/async-2.2.1/docs/Control-Concurrent-Async.html#v:waitAny) to block until one of those calls complete. &gt; and cancel the ones which did not return. I would not cancel them, as you may lose data if some of the other calls have also completed. Instead, I would replace the async call which has completed, and call `waitAny` again. You can use [`delete`](https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-List.html#v:delete) to remove the async which completed from the list.
Mostly I chaff because I worked in the book business. So much like mp3s, charging $60 when there is no 40% store markup, no 25% distributor markup, and no manufacturing cost or warehouse delivery cost is crazy. I buy $20 ebooks all the time, but for $60 I want a book-book. Even at $20 the author share would be double or quadruple a print book rate. YMMV, but it certainly rules it out for me.
You want to calculate two things: the sum of the list, and whether any partial sum was negative. “Two things” suggests using a pair of an integer and a Boolean as the accumulator of your fold; “any” suggests you need to combine the Booleans with logical OR. Try filling in this template: sumGoesNegative :: [Integer] -&gt; Bool sumGoesNegative input = extractResult (foldl step start input) where -- What should the result be if the input is empty? start :: (Integer, Bool) start = (_, _) -- How do you update the accumulator with the next number? step :: (Integer, Bool) -&gt; Integer -&gt; (Integer, Bool) step (previousSum, hasGoneNegative) nextNumber = let currentSum = _ in (_, _) -- What final result do you want to extract from the accumulator? extractResult :: (Integer, Bool) -&gt; Bool extractResult (_, _) = _ Of course, you can simplify and shorten this code considerably by inlining the definitions in the `where` clause, I just wanted to give them names and type signatures for clarity’s sake. This is also a good demonstration of how laziness affects performance: if you enter `:set +s` in GHCi, it will show the memory usage and time taken to evaluate each expression you enter; try `sumGoesNegative` on lists of increasing size like `[1..10]`, `[1..1000]`, `[1..100000]` and see how the efficiency scales. What do you think you could change about the definition of `sumGoesNegative` or `step` to improve this? Finally, there’s an even simpler and more idiomatic solution using the standard functions `scanl` (suggested by “partial sum”) and `any`—see if you can find it.
I don't think it makes sense to call that a special case. That would be similar to implying that: data Foo = Foo { foo :: !Foo } Desugars to data Foo = Foo { foo :: !~Foo } Which is special cased to get rid of the `~`. IMO a better way to think about it is that a `!` in the left hand side of a let binding adds a `seq` dependency between that value and its immediate parent, and that values are otherwise lazy by default. Thus for /u/iitalics to get a runtime error in the second case they would need to change it to: let !((), !2) = ((), 3) in 2 Such that strictness reaches all the way to the top.
GHCJS has it's own GC, so I'm assuming there is already some sort of solution to this problem in GHCJS's runtime.
I also can only assume that javascript's GC is far from optimal for most non-JavaScript languages (particularly radically different ones like Haskell).
Here's my experience with trifecta: it uses the "parsers" package, which provides a bunch of typeclass instances for e.g. wrapping a parser with StateT. I keep two Naturals of state: the indent level, and the number of levels consumed. This lets me be very precise about indents, e.g. two tabs will become two indents. This is useful in my problem domain but might be too precise for what you want, though. 
thanks!
I use a function like this, jwiegley’s “my-auto-format” https://github.com/jwiegley/dot-emacs/blob/master/init.el#L1864
If anyone here has tried this and wants to use it outside US-East-1 here's a guide for that I threw together: [https://blog.alainodea.com/en/article/511/haskell-aws-lambda-runtime-outside-us-east-1](https://blog.alainodea.com/en/article/511/haskell-aws-lambda-runtime-outside-us-east-1)
I'm not the author or affiliated with them. If you want to make your own layer I think you can start with my guide to building and publishing theirs. I suspect you can add libs to the bootstrap. I think you might have to link them statically or do some dance to embed them in it and extract them onto LIBPATH at runtime before running your Haskell as a subprocess. Your libs will need to be compatible with x86\_64 Amazon Linux. Firecracker might be a good place to try things first: [https://github.com/firecracker-microvm/firecracker](https://github.com/firecracker-microvm/firecracker) I think it's a pretty pared down GHC with some wrapping to allow AWS Lambda invocation to work. The runtime is 22MB (4.7MB zipped). The example Lambda is 11.75MB (2.2MB zipped). It's a clean stack build of this run on circleci/python:2.7-jessie: [https://github.com/theam/aws-lambda-haskell-runtime/](https://github.com/theam/aws-lambda-haskell-runtime/) My version is a clean stack build on fpco/stack-build since I'm relying on stack's docker support to provide a usable build even though I'm running macOS.
Fantastic answer, I really like how you talked through everything without fully giving away the solution. 
You could always fork the blocking operation into its own thread.
Definitely worth a book to include all that content and present them systematicly. Thanks for the link.
Gorgeous website! Looks like a lot of fun, too; unfortunately, I need to stay in Portland OR for the foreseeable future, but best of luck in the hiring process :)
&gt; I'm going to update the post and add this approach there if you don't mind me crediting your reddit username I don't mind :) Feel free to include the suggestion! &gt; it can be really simple and easy to write APIs with Haskell That's true. But understaind of type-level tricks requires some time... Though it gives you a lot of power! &gt; Did you see what I did with the TaskState? Too much code to grasp immediately. But I spotted one thing to think about. Here is your `TaskWithState` data type: newtype TaskWithState = TWS { getTask :: forall ident state. WithID ident (Complete (Task state)) } The place where you're using `forall` is important here. Compare the following two data types in Haskell: ghci&gt; data Foo = forall a b . Foo (a, b) ghci&gt; :t Foo Foo :: (a, b) -&gt; Foo ghci&gt; data Bar = Bar { bar :: forall a b . (a, b) } ghci&gt; :t Bar Bar :: (forall a b. (a, b)) -&gt; Bar The only difference between those two data types is that `forall` for `Bar` is nested deeper. That's why you might have problems with `-XImpredicativeTypes`. Having `Some` constructor for `TaskState` at least feels wrong because you **should** know the state of every task, so there should be some way to not introduce `Some` constructor. 
&gt; The only difference between those two data types is that forall for Bar is nested deeper. That's why you might have problems with -XImpredicativeTypes. Having Some constructor for TaskState at least feels wrong because you should know the state of every task, so there should be some way to not introduce Some constructor. The thing is you actually don't what state the task is in when it comes from the DB, which is why I had to add that `Some` constructor. I think there are other ways I could have solved it. Also I ended up using the `ImpredicativeTypes` extension due to the inability to include the simpler `Foo` example directly in the `TodoAPI` type, IIRC. I think what I can do is that since I split it up into `Task`, `TaskF`, and `TaskFWithState` I can remove the `Some` constructor and force conversions when necessary -- then I can use the simpler `Foo` version everywhere.
The most basic way is, of course, doing a fold, probably will look something like this: g = snd . foldl (\(acc, bool) a -&gt; (acc + a, not bool || acc + a &lt; 0)) (0, True) It's not that bad, certainly a bit bulky, but it'll pass, right? &gt; ... depending on if that sum becomes negative _at any time_ during the foldl. Well, here's the problem: folds need to go through the whole list to give you the result, that certainly isn't the most optimal way, is it? Thankfully, there are scans to help: f = and . map (&gt;= 0) . scanl (+) 0 A scan is practically the same as a fold, except it returns all of the intermediate results. Then all we need is to just compare each of those little results to zero and if any of them is less than zero, it shall quit prematurely.
Writing a compiler in Haskell is a awesome experience. Very different from other languages, where you play around with lex and yacc for lexer and parser, which Haskell follows a novel approach with Algebraic data structures and pattern matching. Also sequencing using monads is a good way to learn about side effects and how List, Maybe, State, IO etc are modelled as monads. I am a haskell beginner like you, its been a year since I started learning haskell. Hope this helps. :) &amp;#x200B;
 foldl' (\acc x -&gt; if acc &lt; 0 then acc else acc + x) 0 [1,2,-4,5,undefined] &lt; 0 The fold terminates as soon as the result becomes negative and returns that result. If the result never became negative it will return the final sum. You can then check the result whether it is negative or not to return True/False. There is a common misconception that left folds cannot terminate early and right folds can. Left folds can terminate if the expression that you are returning is free from the variable holding the next element to be folded. For example in the expression above we are returning `acc` when it is less than 0, so the fold will terminate as there is no `x` in that expression.
That's interesting. Here's the Opaleye version. See the generated code run at http://sqlfiddle.com/#!17/d3997e/1/0. ``` {-# LANGUAGE Arrows #-} -- Extensions only needed for the generated code. Will disappear in a -- future release. -- { {-# LANGUAGE TypeOperators #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE UndecidableInstances #-} {-# LANGUAGE FlexibleContexts #-} -- } import Opaleye import Control.Arrow (returnA) import Data.Profunctor.Product (p2) -- Imports only needed for the generated code. Will disappear in a -- future release. -- { import Data.Profunctor as P import Opaleye.TypeFamilies import Data.Profunctor.Product as PP import Data.Profunctor.Product.Default as D import Data.Profunctor.Product.Default (Default) -- } data Item f = Item { iName :: TableField f String SqlText NN Req , iPrice :: TableField f Double SqlFloat8 NN Req } data Purchase f = Purchase { pDate :: TableField f Int SqlInt4 NN Req , pName :: TableField f String SqlText NN Req , pItemN :: TableField f String SqlText NN Req , pUnits :: TableField f Double SqlFloat8 NN Req } items :: Table (Item W) (Item O) items = table "items" (Item &lt;$&gt; lmap iName (tableColumn "name") &lt;*&gt; lmap iPrice (tableColumn "price")) purchases :: Table (Purchase W) (Purchase O) purchases = table "purchases" (Purchase &lt;$&gt; lmap pDate (tableColumn "date") &lt;*&gt; lmap pName (tableColumn "name") &lt;*&gt; lmap pItemN (tableColumn "item") &lt;*&gt; lmap pUnits (tableColumn "units")) query :: Select (Column SqlFloat8) query = aggregate avg $ fmap snd $ aggregate (p2 (groupBy, Opaleye.sum)) $ proc () -&gt; do item &lt;- selectTable items -&lt; () purchase &lt;- selectTable purchases -&lt; () restrict -&lt; iName item .== pItemN purchase restrict -&lt; pDate purchase .&lt;= 6 restrict -&lt; pItemN purchase ./= sqlString "legal fees (1 hour)" returnA -&lt; (pName purchase, pUnits purchase * iPrice item) sql :: IO () sql = case showSql query of Nothing -&gt; return () Just s -&gt; putStrLn s -- Generated code. Will disappear in a future release. -- { pPurchase :: PP.ProductProfunctor p =&gt; Purchase (p :&lt;$&gt; a :&lt;*&gt; b) -&gt; p (Purchase a) (Purchase b) pPurchase (Purchase a b c d) = Purchase PP.***$ P.lmap pDate a PP.**** P.lmap pName b PP.**** P.lmap pItemN c PP.**** P.lmap pUnits d pItem :: PP.ProductProfunctor p =&gt; Item (p :&lt;$&gt; a :&lt;*&gt; b) -&gt; p (Item a) (Item b) pItem (Item a b) = Item PP.***$ P.lmap iName a PP.**** P.lmap iPrice b instance ( PP.ProductProfunctor p , Default p (TableField a Int SqlInt4 NN Req) (TableField b Int SqlInt4 NN Req) , Default p (TableField a String SqlText NN Req) (TableField b String SqlText NN Req) , Default p (TableField a Double SqlFloat8 NN Req) (TableField b Double SqlFloat8 NN Req)) =&gt; Default p (Purchase a) (Purchase b) where def = pPurchase (Purchase D.def D.def D.def D.def) instance ( PP.ProductProfunctor p , Default p (TableField a String SqlText NN Req) (TableField b String SqlText NN Req) , Default p (TableField a Double SqlFloat8 NN Req) (TableField b Double SqlFloat8 NN Req)) =&gt; Default p (Item a) (Item b) where def = pItem (Item D.def D.def) -- } -- http://sqlfiddle.com/#!17/d3997e/1/0 {- CREATE TABLE items( name text, price float8 ); CREATE TABLE purchases( date int4, name text, item text, units float8 ); INSERT INTO items (name, price) VALUES ('computer' , 1000), ('car' , 5000), ('legal fees (1 hour)' , 400); INSERT INTO purchases (date, name, item, units) VALUES (7 , 'bob' , 'car' , 1), (5 , 'alice' , 'car' , 1), (4 , 'bob' , 'legal fees (1 hour)' , 20), (3 , 'alice' , 'computer' , 2), (1 , 'bob' , 'computer' , 1); -} ``` 
I use haskell-vim-now, you can find my vim config here: [https://github.com/harendra-kumar/config-vim](https://github.com/harendra-kumar/config-vim) . It can be installed automatically if you want to try, though it may take a while if you install everything.
I actually applied to Futurice (the ad didn't say haskell then), but stopped half way since didn't want to work full time. BUT all I saw was good, the interview rounds, how they worked, how the office functioned perk-wise, personal development, mindful people.
Could this be done remotely? Best 
Given the topic at hand it is absurd to reach a point where someone honestly believe a comment about what responsibilities decent people have, and caring about people starving has relevance to the discussion. In my estimation a peak in the landscape of more or less strange reactions. I think the best way to understand that is as an indicator of people talking past each other, which in turn should be viewed in the light of some political overtones, which tends to bring out the worst in people. 
Sorry for the naive question, but when building an executable via `stack` (or even `cabal` for that matter), I don't invoke `ld` manually. How does this script really work?
As /u/BurningWitness noted, this is a job for a scan: foo = any (&lt;0) . scanl' (+) 0 But since scans usually get optimized to folds we can express the same thing with either. The (library specified) optimization rule is somewhat confusing, though. If we use a right scan it is pretty easy: foo ls = case foldr step False ls of of (a,b) -&gt; b || a &lt; 0 where step cur (acc, b) = (cur + acc, b || acc &lt; 0) The left scan is rather more cryptic: foo ls = foldr step (const False) ls 0 where step b g = oneShot (\x -&gt; let b' = x + b in b' &lt; 0 || g b') Which becomes a nice imperative loop when combined with a list producer. 
https://stackoverflow.com/questions/43243322/how-to-link-with-the-gnu-gold-linker-instead-of-ld-in-haskell/43243323#43243323 Just replace `lld` or `gold` with the wrapper script.
You're obviously not old enough. ;)
Added perk: working along some _very_ talented Haskellers, e.g. Oleg (`phadej`)
Wow! I didn't know that. Thanks for sharing. Any known bugs with using `lld` or `gold`? If they're fast _and_ stable, why is the default choice still `ld`?
`gold` has gotten pretty stable as a proper `ld` replacement, as I understand it. `lld` is *pretty good* at having the same CLI as `ld`, but still lacks some things that may come up. But I believe that using `clang` as the driver instead of `gcc` works pretty much perfectly with `lld`, and `clang` is almost a perfect substitute for `gcc`. So other than historical baggage, the only reason `lld` isn’t the default is because `clang` isn’t the default. As for `gold`, I’m not sure why it’s not default. I’m guessing there are still some minor compatibility quirks with niche use cases or something. Or maybe it targets fewer platforms.
ELI5: Why does `jaguar` type check and why doesn't it when you remove the type? 
damn just checked said persons github. 4000~ commits in a year and I haven't even got 1 :o 
The biggest problem with HIE for me is that there isn't any sort of (visible) roadmap. It's not clear what the objectives are or what the priorities are and that demotivates me from contributing. I thought this proposal was going to be exactly that, a list of objectives for HIE that could be fleshed out and worked on for the next year. 
Does Templste Haskell grant type safety because it preprocess templates to code that is then type checked at the subsequent compile stage?
There are some roadmaps but they're heavily outdated. And there is only a couple of people working on hie. Meanwhile there are plenty of issues and even well defined tasks to do waiting. I guess situation will remain the same while everyone wait until someone else will do something like roadmap or personal invitation or something. Sorry if this sound rude.
That is consistency and small commits for you (okay, and some luck in the genetic lottery). Not speaking out of experience, but hope ;)
Yeah, use new-update. In general, use the new-* command if it's available `new-repl --build-depends` opens up a ghci repl with the dependencies you specified (they get cached, cabal will build them only the first time). It's especially useful when you want to quickly test some library If you don't want to specify the deps each time, you can use `new-install --lib tidal` to make the tidal library available to all future ghci sessions by default (it gets added to the default ghc environment)
Well yes. More importantly, you can _use_ the generated code in a type safe manner afterwards.
Formatting your code so it's readable... {-# LANGUAGE Arrows #-} -- Extensions only needed for the generated code. Will disappear in a -- future release. -- { {-# LANGUAGE TypeOperators #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE UndecidableInstances #-} {-# LANGUAGE FlexibleContexts #-} -- } import Opaleye import Control.Arrow (returnA) import Data.Profunctor.Product (p2) -- Imports only needed for the generated code. Will disappear in a -- future release. -- { import Data.Profunctor as P import Opaleye.TypeFamilies import Data.Profunctor.Product as PP import Data.Profunctor.Product.Default as D import Data.Profunctor.Product.Default (Default) -- } data Item f = Item { iName :: TableField f String SqlText NN Req , iPrice :: TableField f Double SqlFloat8 NN Req } data Purchase f = Purchase { pDate :: TableField f Int SqlInt4 NN Req , pName :: TableField f String SqlText NN Req , pItemN :: TableField f String SqlText NN Req , pUnits :: TableField f Double SqlFloat8 NN Req } items :: Table (Item W) (Item O) items = table "items" (Item &lt;$&gt; lmap iName (tableColumn "name") &lt;*&gt; lmap iPrice (tableColumn "price")) purchases :: Table (Purchase W) (Purchase O) purchases = table "purchases" (Purchase &lt;$&gt; lmap pDate (tableColumn "date") &lt;*&gt; lmap pName (tableColumn "name") &lt;*&gt; lmap pItemN (tableColumn "item") &lt;*&gt; lmap pUnits (tableColumn "units")) query :: Select (Column SqlFloat8) query = aggregate avg $ fmap snd $ aggregate (p2 (groupBy, Opaleye.sum)) $ proc () -&gt; do item &lt;- selectTable items -&lt; () purchase &lt;- selectTable purchases -&lt; () restrict -&lt; iName item .== pItemN purchase restrict -&lt; pDate purchase .&lt;= 6 restrict -&lt; pItemN purchase ./= sqlString "legal fees (1 hour)" returnA -&lt; (pName purchase, pUnits purchase * iPrice item) sql :: IO () sql = case showSql query of Nothing -&gt; return () Just s -&gt; putStrLn s -- Generated code. Will disappear in a future release. -- { pPurchase :: PP.ProductProfunctor p =&gt; Purchase (p :&lt;$&gt; a :&lt;*&gt; b) -&gt; p (Purchase a) (Purchase b) pPurchase (Purchase a b c d) = Purchase PP.***$ P.lmap pDate a PP.**** P.lmap pName b PP.**** P.lmap pItemN c PP.**** P.lmap pUnits d pItem :: PP.ProductProfunctor p =&gt; Item (p :&lt;$&gt; a :&lt;*&gt; b) -&gt; p (Item a) (Item b) pItem (Item a b) = Item PP.***$ P.lmap iName a PP.**** P.lmap iPrice b instance ( PP.ProductProfunctor p , Default p (TableField a Int SqlInt4 NN Req) (TableField b Int SqlInt4 NN Req) , Default p (TableField a String SqlText NN Req) (TableField b String SqlText NN Req) , Default p (TableField a Double SqlFloat8 NN Req) (TableField b Double SqlFloat8 NN Req)) =&gt; Default p (Purchase a) (Purchase b) where def = pPurchase (Purchase D.def D.def D.def D.def) instance ( PP.ProductProfunctor p , Default p (TableField a String SqlText NN Req) (TableField b String SqlText NN Req) , Default p (TableField a Double SqlFloat8 NN Req) (TableField b Double SqlFloat8 NN Req)) =&gt; Default p (Item a) (Item b) where def = pItem (Item D.def D.def) -- } -- http://sqlfiddle.com/#!17/d3997e/1/0 {- CREATE TABLE items( name text, price float8 ); CREATE TABLE purchases( date int4, name text, item text, units float8 ); INSERT INTO items (name, price) VALUES ('computer' , 1000), ('car' , 5000), ('legal fees (1 hour)' , 400); INSERT INTO purchases (date, name, item, units) VALUES (7 , 'bob' , 'car' , 1), (5 , 'alice' , 'car' , 1), (4 , 'bob' , 'legal fees (1 hour)' , 20), (3 , 'alice' , 'computer' , 2), (1 , 'bob' , 'computer' , 1); -} 
I'm not entirely sure, but I don't think that GHC can infer it, but it can validate that it is a correct type (just as it could, say, (a -&gt; a) -&gt; (a -&gt; a), and disvalidate a -&gt; a -&gt; a).
That's a totally valid perspective, if that works for you then the community benefits from your work! Personally though, I have a lot of other projects, research, ghc work even, I don't have the energy to contribute to a project if it feels like i'm spinning in place. Knowing that these issues form part of a greater thread helps, otherwise it feels like plugging holes in a leaking ship.
Thank you. Triple backticks doesn't work in old-style Reddit.
Sadly, current amount of contributors in hie only allows plugging holes as you say. So it's a vicious cycle.
Ahh. Hadn't realised they'd changed things. RES kills triple backquotes too.
This was a great explanation, thanks a lot for going into such detail to help me out!
Without an annotation GHC tries to infer a simple type without quantifiers, by setting the type of `jaguar` to a unification variable `t` it sees that, since `jaguar` is applied to `jaguar`, `t` must be a function type, with argument type `t`: `t ~ (t -&gt; ?)` which results in an infinite type. With the annotation, GHC is able to instantiate the two occurences of `jaguar` differently.
Frankly, I think someone needs to put some real life money into it. IDEs are big, difficult projects. If any company thinks their developers would be more productive with a better editor experience, and has enough capital to invest that far into their future, I think HIE would be a very good investment, and a perfect way to give back to the community. I have a hard time believing this is going to get to where it needs to be without a little bit of full-time work on it.
I've never worked at a company which could invest or contribute to HIE because of it's license situation.
Oh? Could you elaborate?
Ah, right, [the AGPL argument](https://github.com/DanielG/ghc-mod/issues/638). I have to say, I don't see any argument whatsoever that you can't use AGPL'd software as a dev tool without AGPL'ing the code it operates on. That seems completely unrelated to the purpose of AGPL, which is about GPL'ing SaaS products. And using HIE as a dev tool is *not* making it a SaaS product for your customers; they do not interact with it through a computer network. So any licensing issue is only that a corporation has decided to adopt a general policy against (A)GPL software. Is that a common policy? It's certainly an unnecessary one.
What about `Sock`s
I don't think that fold terminates when the accumulator turns negative, it keeps evaluating `if`s until the end. And it should hang with an infinite list.
¯\\\_(ツ)\_/¯ 
&gt; IDEs are big, difficult projects. They are! But a Haskell language server that you can hook into existing IDEs is at least a lot less difficult. I'd happily use MS VSCode with Haskell if the autocomplete/auto-import/cross-linking/error-reporting/formatting/refactorings were accurate and snappy. Honestly, I after using TypeScript in VSCode for a while I now I find writing Haskell very tedious. Do I really need to type in all this stuff myself? :)
HIE is licensed under BSD3, and haskell-lsp under MIT. There are indeed components with GPL licenses included, but if PRs were presented with alternatives these would be accepted, possibly using a feature flag, as for libgmp and GHC. 
Few motivated students / college-grads can probably do a kickstarter campaign for this. Come up with a well-defined 1 year roadmap and commit to the project. (probably publish a few papers as a side-effect). I will contribute to the kickstarter campaign.
&gt; There is a common misconception that left folds cannot terminate early and right folds can. This is not a misconception. `foldl` (and `foldl'`) cannot terminate early (on lists). Even taking your example, it will hang on an infinite list, or, put another way: &gt;&gt;&gt; foldl (\acc x -&gt; if acc &lt; 0 then acc else acc + x) 0 (1:2: -4:5:6:undefined) &lt; 0 *** Exception: Prelude.undefined Compare this to /u/BurningWitness 's answer, which uses `and` (which is implemented in terms of `foldr`): &gt;&gt;&gt; (not . and . map (&gt;= 0) . scanl (+) 0) (1:2: -4:5:6:undefined) True The `undefined` in your example is ignored because the expression constructed doesn't scrutinize it: the closure is still *applied* to undefined, though. In other words, for a list of length `n`, `foldl f b` will *always* scrutinize (and therefore pay for) `f` `n` times.
Happy to see cool Haskell companies in Europe; should come in handy when I plan to move there in about 2 years, though I'm happy with my current Haskell job.
whoa, that is the most hardcore nerd stack overflow answer I've ever seen. Wow. thanks!
Cool. Now hurry up and unite their work on probabilistic self-reflection into a general theorem corresponding to Lawvere's Fixed Point Theorem. I tried to nag them to do it two years ago and they didn't.
&gt; Do I really need to type in all this stuff myself? Nice thing about Haskell is that even when you're typing everything by yourself you still can be very productive :)
Thanks for the counter example. I know scan can be used for termination but I incorrectly thought foldl can terminate too. It does not evaluate the values beyond the negative result but still goes through the spine of the list and evaluates the if branch for each value. Thinking a bit more, the way foldl works there is no user control over recursion, it is driven from within the fold, whereas in foldr recursion is driven by the user supplied fold expression, by evaluating the tail, so you can end it by finishing the tail anytime, supplying the zero. So it indeed is not a misconception.
&gt; As for MIRI, despite enjoying EY's writings, I think the basilisk-style AI risk is silly (though I do think there is AI risk -- it's already here, just look at the shameful ways we use the AI we have), but hey, if the organization is supporting this kind of work, then credit where credit is due: this is great! Hot take: EY sounding silly is a sign of how *AI as a field* is fucking silly. In no other field would people say, "We build uncontrollable black-boxes and then give them vast amounts of free reign within society, and That's Good Actually." Or at least, not in any other *engineering* field. In engineering, you are usually required to demonstrate that bridges you build will stand up, robots you build will not harm factory workers, and software systems you run in the wild will shut down when CTRL-C is pushed.
You might think it's unnecessary, but most companies, e.g. Google, disagree with you: [https://www.theregister.co.uk/2011/03/31/google\_on\_open\_source\_licenses/](https://www.theregister.co.uk/2011/03/31/google_on_open_source_licenses/). It's a very common policy.
Yes, but that requires a level of commitment beyond just sending a patch, instead rewriting a huge section, and when you combine the components the only sensible legal approach is to adopt all the licenses together.
Source on "most companies"? Google bans it because their whole product line is SaaS, so they just don't want the trouble of making sure they don't use AGPL code in their services (plus they're incentivized to prevent AGPL's use as much as possible, so they they can use as many open source projects as possible in their services). But the "most companies" bit seems fairly unlikely.
Having to compile HIE myself on macOS was a significant speed bump in my own adoption of it. Pre-made binaries could lead to swifter adoption, more exposure, and possibly more parties interested in hacking on it. Sure that's a stretch, but removing barriers like that cannot hurt. 
Tbh I don't think any of this matters unless someone has a proposal for *funding* a Haskell IDE
Hi @phadej, thanks for reaching out! These questions help a lot :) * The layer itself is nothing more than a proper dispatcher, we'd love to have the possibility of compilation in the future, but currently it reads parameters from AWS Lambda, places them in the \`Context\` value and passes it to the appropriate handler. * Speaking of handlers, your project is not stored in the same layer as the runtime, so you can store the libraries in your project. Could you be more specific? Maybe I can help better. * \`configureLambda\` basically looks for all the files that contain a handler and creates a main file that dispatches based on a call from the runtime. You could write it yourself by hand, but basically what happen are qualified calls to the handlers. Thats why importing them qualified is needed. * The docker image that is being used is not arbitrary, in fact it is the official image that Stack uses underneath, there are no strings attached :) I see that you are developing a runtime too, why don't we join forces? :D
Thanks for all this info @AlainODea, we are going to fix the issue with the region ASAP! :)
No problem. It turned into a worthwhile AWS Lambda Layers learning opportunity for me :)
&gt;Rich lets almost no one contribute to core, but conversely, when he does add stuff to core is often just a more "Rich-y" solution to something the community had already come up with solutions for. &gt; &gt;So rather than either adopting or recommending Schema, we get Spec, which is cryptic and harder to use. Instead of standardizing on lein/boot for a build tool, suddenly we have clj, which isn't even feature complete compared to either of those. Can I add other things? The new "REBL", that needs more Protocols, instead of contributing to UNREPL (that already works with some REPLs out there, and would allow people to add support for more editors, web platforms) Also, \`clj\` is indeed the greates NIH *ever*...
Not a Haskell/data science expert, but here's a pragmatic solution for your problem, assuming your data is stored in CSV files. Libraries: cassava, text, bytestring, vector, containers. &amp;#x200B; {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE DeriveGeneric #-} {-# LANGUAGE OverloadedStrings #-} module Main where import qualified Data.ByteString.Lazy as BL import Data.Csv import qualified Data.Vector as V import Data.Text (Text) import qualified Data.Text as T import Data.Map.Strict (Map) import qualified Data.Map.Strict as M import GHC.Generics (Generic) data Price = Price { item :: Text , price :: Integer } deriving (Generic, Show) data Purchase = Purchase { date :: Integer , person :: Text , itemBought :: Text , unitsBought :: Integer } deriving (Generic, Show) instance FromRecord Price instance FromRecord Purchase main :: IO () main = do -- Read CSV files pricesData &lt;- BL.readFile "Prices.csv" purchaseData &lt;- BL.readFile "Purchases.csv" putStrLn . show $ do -- Parse CSVs (prices :: V.Vector Price) &lt;- decode HasHeader pricesData (purchases :: V.Vector Purchase) &lt;- decode HasHeader purchaseData -- Create price mapping let priceMapping = M.fromList . map (\(Price i p) -&gt; (T.strip i, p)) . V.toList $ prices -- Map purchases with prices let mapWithPrice purchase = (\price -&gt; (purchase, price * unitsBought purchase)) &lt;$&gt; M.lookup (T.strip $ itemBought purchase) priceMapping let pricedPurchases = V.mapMaybe mapWithPrice purchases -- Filter non-legal purchases let nonLegalPurchases = V.filter (not . T.isInfixOf "legal fees" . itemBought . fst) pricedPurchases -- Filter purchases before or at time 6 let purchasesBeforeDateSix = V.takeWhile ((&lt;= 6) . date . fst) . V.reverse $ nonLegalPurchases -- Accumulate purchases per person let sumPerPerson = V.foldr' (\(purchase, price) acc -&gt; M.insertWith (+) (T.strip . person $ purchase) price acc) M.empty purchasesBeforeDateSix -- Calculate average over all persons (who have valid purchases before or at time 6) let values = map snd . M.toList $ sumPerPerson let averageOverAllPersons = (fromIntegral . sum $ values) / (fromIntegral . length $ values) return (sumPerPerson, averageOverAllPersons) &amp;#x200B;
See https://github.com/haskell/haskell-ide-engine/commit/a7b120f96d53d0fbf97d93a07559e4df28adebff
&gt;I mean, there are really important differences between schema and spec. Same with deps, lein and boot. what are really the differences between schema and spec? (Honest question here)
I don't really agree: look at the first nuclear tests or Castle Bravo or even the entire concept of doing atmospheric nuclear testing. Look at the way we trash up the ocean with plastics and the atmosphere with carbon dioxide. The idea that in every other field we think twice and carefully measure the impact of what we do is just not true. We have done irrecoverable damage to the planet and continue to press on doing so because the alternative - lower quarterly profit reports - is apparently intolerable. Further, EY is not merely pushing for a crusade against sloppy software engineering practices; the contention is there's a real risk of an AI "escaping the box" and achieving god-like intelligence and potentially wrecking human civilization while pursuing its utility function. That is not, in my estimation, a serious risk, especially when there are actual serious risks sitting right at our doorstep and we do nothing at all to address them. Even in AI, as I noted in my original comment, there seems to be no concern at all at MIRI for the present damage AI is doing: as far as I can see, we as a civilization have done literally nothing good with it. We use it to maximize addiction on mobile apps and social media at demonstrable expense to the population's psychological health and we use it to construct panopticons to give our unashamedly corrupt governments orders of magnitude more power than they had before. Seriously, what good do we do with this technology? The problem is not some god-AI sacrificing human civilization for its nonsensical goals; the problem is *us* sacrificing human civilization for our nonsensical goals.
It says "This module should be considered GHC internal" and it looks extremely low-level, I wouldn't use that directly. By "everything in haskell", are you referring to specific higher-level libraries, perhaps one of which would be more appropriate in this situation?
My favorite part of AoC is writing up solutions and documenting them so that someone else could get something out of them. I hope you find it beneficial to check some of them out. If you ever have any questions you can reach me and many other participants on IRC: [Freenode #haskell](http://webchat.freenode.net?channels=%23haskell) !
&gt;Google bans it because their whole product line is SaaS, so they just don't want the trouble of making sure they don't accidentally use AGPL code in their services by integrating some previously internal tool. Sure, a company that doesn't write any software to provide services is not affected by this. But frankly, this isn't very common these days. Outside of gaming, the retail software market is pretty much a thing of the past.
But most companies who write services aren't writing such a clusterfuck of them and their own internal tooling that they need to worry about accidentally deploying AGPL code by accident. I really don't think this policy is necessary in the vast majority of companies, and for the ones that have it, surely they can make an exception for something like editor integration, which is never going to accidentally wind up in their completely unrelated SaaS
Yes I noticed that cancelling was losing me data, so I changed by design to use nonblocking streams. waitAny doesn't work in my case, as the nodes need to do work when there is no communication, so whatever queuing mechanism I use cannot block.
I'll take a look at this next time I try to write something like this.
The problem I have is that the main thread of each node needs to take action when there is no communication also. I sort of implemented this using a promise which is passed around each iteration until it can be evaluated.
It’s literally what you should use. Or the high level wrappers in control concurrent which give you simple mvars or stm tvar wrappers to the same info. But either way, if your io code isn’t indirectly using this stuff via sleep until ready code using tvars or mvars, your code is gonna be wrong 
I'd rather see work on getting individual tools working better. For instance ghc-mod is usually several versions behind in support, only works with the specific version it was compiled with, and isn't in stackage at all. I'm skeptical about full-ide style solutions, without a large team working on them they'll never be well supported, and using one in a text editor would require a massive amount of extra work on my vimrc, to the point where I'm not inclined to even try.
A lot of monad tutorials are written by new users whomst believe they have just grasped the true essence of monads.
This is great. I aspire to such fold-fu. &amp;#x200B; Did you use foldr' because a big dataset could produce an unevaluably big thunk if the fold was lazy? Similarly for Data.Map.Strict? &amp;#x200B; I \[rewrote the code\]([https://github.com/JeffreyBenjaminBrown/data-haskell-challenge/tree/master/rindenmulch](https://github.com/JeffreyBenjaminBrown/data-haskell-challenge/tree/master/rindenmulch)) to make the types more explicit, and put it in a repo with a .cabal file with all the needed libraries. I intend to do the same for all other responses; starting now with the Opaleye code.
Strictly speaking the only way to interrupt a `recv` system call in blocking mode on Linux is to send a signal (probably SIGUSR1 or one of the real time signals) to the thread using `pthread_kill` or the lower level system call `tkill` (you have to set an empty signal handler as well) to cause it to return with `EINTR` but that's hugely messy, probably tramples on territory used by the GHC runtime internals and it is best to avoid that anyway by opening all your sockets in `O_NONBLOCK` (set with http://hackage.haskell.org/package/network-2.8.0.0/docs/Network-Socket.html#v:setNonBlockIfNeeded ) so you don't need to bother with worrying about it anyway.
Elm. Friends don't leave friends alone with a haskell compiler until they are comfortable.
ha
What does this problem look like in python?
What I meant was that not everyone who is working in the West has spare money, and not everyone who isn't is broke. It contained invalid assumtion, and I (also invalidly) assumed it aimed to touch on todays anti-west anti-straight anti-white sentiments. Since you offered philosophy you value, let me point you this way: [Baruch_Spinoza](https://en.wikipedia.org/wiki/Baruch_Spinoza). This gentleman should help you get rid of all the unnecessary shackles that can be found at where you pointed, while allowing you to keep all the positive parts.
That’s how we’ve brought a few boot camp coders into the FP world. Including now coding Haskell.
I'm as relaxed as ever, but, similarly to antifa, "I'll bash the fash" wherevr they might show. But since I'm just using words, the worst that can happen are some butthurt feelings (inconsequential) or (worse, even), that people will hear things they are not comfortable with. BIG. FUCKING. DEAL. Did all the idiots trying to infringe upon our (as in every-fucking-ones) fundamental rights stop dead in their tracks because i offend me and millions of others ? No. Well, we are not gonna stop either, and some on the other side might learn that being offended IS NOT THE EFFIN' END OF THE EFFIN' WORLD. Get offended. Whoopty-fucking-doo. That's your effin' right. Or _DON'T_ get offended. Tha't your effin' privilege. You need to start caring, bozo.
&lt;*&gt; on functions is interesting since it and const can be used to define any term in the untyped lambda calculus (they're the SK combinators) 
&gt; Haskellers occassionally will glibly state “a monad is just a monoid in the category of endofunctors, what’s the problem.” I strongly believe the vast majority, if not all cases, of this statement are meant to self-mock. But let’s pretend for a moment that this information was stated in this tone to a new user, who has never seen monads, monoids, categories, or endofunctors before. This *massively* fails the empathy test. I think concern about the monad joke is as played as the monad joke itself by now. Monads and functors can be familiar to Haskell newbies; they were to me. That's why this line of complaint always irks me. Isn't it another case of lack of empathy to assume _all_ new users have the same background, someone familiar with imperative programming but not algebra? &gt; And making these assumptions about new users who don’t fit this description perfectly doesn’t usually cause any kind of major negative impact Yes it can. The thing folks don't seem to realize is just how much knowledge about programming is already assumed in books and tutorials. I had a very difficult time even identifying what it was that I didn't know that I didn't know. The tools are still difficult and were even more difficult to use back when I started. The command line was daunting. The focus on lazy evaluation was confusing; thinking about a "runtime" was baffling. There's tons of confusing and ill-defined terminology that gets thrown at you that has nothing to do with algebra. For someone like me, it was the mathematical expositions that kept me interested enough to keep at it. I had to learn on the job with no mentorship and the extreme stress of a startup atmosphere. Connecting Haskell to elegant, universal, algebraic concepts helped me immensely.
Here's [one solution](https://github.com/JeffreyBenjaminBrown/data-haskell-challenge/tree/f4ea9c72abe8d1363503ca4f42cf84c096181799/python). It's nice and readable, but horribly unsafe.
The reason I took so long to learn Haskell is because I didn’t even know it existed. Once I finally discovered it and opened LYAH, I knew within 15 minutes this was for me, but before that it just never showed up on my radar loud enough for me to notice. The two things that caused me to notice it were Rust developers (way back in beta) arguing about how stuff was done in Haskell and a meme mocking Haskell programmers for living in an ivory tower, which sounded right up my alley. So clearly, the best way to attract more new people is to spend time on r/rust telling them they’re doing it wrong and make memes.
&gt;&gt; But I’ve found that the following traits generally apply to someone trying out Haskell for the first time. And making these assumptions about new users who don’t fit this description perfectly doesn’t usually cause any kind of major negative impact: &gt; &gt; Yes it can. The traits listed there are "skeptical", "impatient", "curious", "smart, but distracted" and "eager to succeed". Familiar with programming in other languages is not one of them. So I'm not sure which of these traits you feel shouldn't be on the list, i.e., assuming it can lead to a major negative impact...
`printf` from `Text.Printf` has type `printf :: PrintfType r =&gt; String -&gt; r` where `PrintfType` somehow facilitates taking as many arguments as needed. To be precise: &gt; The PrintfType class provides the variable argument magic for printf. Its implementation is intentionally not visible from this module. That's fine I don't need to know the specifics. But I'd like to know: 1. How do I check that say `Int`, `Float` are instances of `PrintfType`? The only Instances I could find are: IsChar c =&gt; PrintfType [c] -- that can't be it a ~ () =&gt; PrintfType (IO a) (PrintfArg a, PrintfType r) =&gt; PrintfType (a -&gt; r) 2. If not answered by (1.), what does the tilde do in `a ~ ()`?
I know you're only joking but let's do the mirror test: &gt; So clearly, the best way to attract more new people is to spend time on [r/haskell](https://www.reddit.com/r/haskell) telling them they’re doing it wrong and make memes.
What's the simplest idiomatic way of testing parsers? (ParsecT) What about without incurring a library dependency?
But see it doesn’t work that direction because we’re not doing it wrong.
&gt; 1. Ensure there’s a clear start location to point them to &gt; 2. Reduce the number of choices they need to make to get started .. in light of these insights, and of who the author is, one really has to wonder: Why, exactly, do we have two websites (haskell.org, haskell-lang.org), and two "main" buildtools (cabal-install, stack)? Of course I am no newcomer to this community, so I might have a rough idea of the answer to that question. But we are talking about newcomers, right? And to those, I dare predict this mess in this ecosystem is a major turn-off. Yes, the fpcomplete blog has/had one or three articles about "why stack". Unfortunately, none of them do _any_ good at explaining why improving cabal was not an option. The two websites don't mention each other either. As long as that is the case, as long as to any newcomer the situation is as opaque as it is, I have to say: &gt; Dear Mr. Snoyman, to me you are the face of the main divide in the haskell community, deserved or not. As a consequence your post is one heavy ingot of irony. &gt; &gt; I get that you probably cannot explain what caused this divide; I suspect a long series of miscommunication and conflicting goals of different parties. Would be hard to explain objectively, and it is not your job either. I cannot judge you, I cannot blame you for trolls that nor can I demand that you provide any explanations to me or others. &gt; &gt; Nonetheless, I frankly do not think you are in a position to give advice on such matters to the haskell community.
As someone new-ish to the community that wouldn’t be here without his work(use Yesod and stack at work), how has he caused a divide? I honestly don’t know and I’m genuinely interested in your point of view.
It was pretty obvious if you were paying attention: at the time, haskell.org and cabal-install weren't doing the job. h.o couldn't offer simple getting started instructions and c-i couldn't install things reliably.
If you don't agree that the standard monad in-joke is demeaning to new users, then let's talk instead about the [Zygohistomorphic_prepromorphisms page](https://wiki.haskell.org/Zygohistomorphic_prepromorphisms) on the Haskell wiki. I hear it's from a joke (?) but the first time I saw it I took it at face value. It gave me the impression that Haskell was over-abstracted, under-documented, and generally just not worth bothering with. If you want to champion new users, start by deleting that page. I'm serious.
First of all, -3 is not a natural number, it’s an integer. Also, why are you using `Word`?
That's a bug in base, in my view.
That's the point: the read should throw exception, but it doesn't.
They said they expected -3 to raise an exception exactly for that reason. And `Word` is a reasonable approximation for natural numbers if you know it fits your range.
&gt; the contention is there's a real risk of an AI "escaping the box" On a long enough timeline, there is a risk in this area. Reasonable people can and do disagree on what consitutes a long timeline, however. Extrapolation is hard. Yudkowsky's core research question of how would you even state a utility function given infinite computing resources that doesn't go horribly wrong is a legitimate question to ask, and one that has spat out a lot of counter-intuitive results along the way. I'm inclined to keep exploring this space given that a.) the costs are absurdly low relative to the potential scale of the problem and b.) it is continuing to produce novel results. It has caused me personally on more than one occasion to reverse my "common sense" beliefs about the safety and effectiveness of certain approaches to things. &gt; there seems to be no concern at all at MIRI for the present damage AI is doing There is room in this industry for more than one actor and more than one focus. MIRI is focused very much on a blend of the longer term problem and on industry coordination. When you are the size MIRI is, splitting your focus is a great way to not do anything well. On the other hand, organizations like the Future of Humanity Institute out of Oxford have been taking a shorter term focus and trying to work better with insurance companies, and governments and the like working to help figure out policies. For that matter, I've helped [ought.org](https://ought.org/careers) find at least one developer, and they are carrying out a very non-MIRI-centric research program. Argumentation that starts with "why are you worried about X when I think issue Y is so much more important" tends to be a bit of a non-starter for me. If we have to wait until world hunger or world peace or poverty or breast cancer or some other thing to be fixed to an arbitrary [Procrustean](https://www.schoolofhaskell.com/user/edwardk/editorial/procrustean-mathematics) standard before we're allowed to spend any effort on something else, then we'll be potentially blocked indefinitely on anything you don't think is important. Real solutions tend to involve lots of folks making progress on many little fronts over time. Across the entire sector if you consider all people devoted to AI risk or ethics in _any_ capacity including the shorter term concerns you espouse above, its far sub 1%. MIRI has something like ~14 researchers all told. It is a comparable fraction of that fraction of a percent. &gt; The problem is not some god-AI sacrificing human civilization for its nonsensical goals; the problem is us sacrificing human civilization for our nonsensical goals. I'm not sure I have any real way to impact the way the latter works. I _might_ have something I can do about the former. Both strike me as legitimate concerns. I'd sure as heck hate to solve one and lose everything to the other due to having too myopic a focus, however.
Perhaps, I should use a parser to read a natural number.
Has anyone been able to get the Haskell Language Server up and running on VS Code on Mac OS X?? I continually get "/bin/sh: ghc: command not found" when attempting to open a Haskell project. I've combed through every single issue on the GitHub page of people with the same problems but none of the "fixes" have worked...
What editor do you use with HIE for Mac? I've been trying for days to get it working with VS Code but nothing has worked
I recommend [megaparsec](http://hackage.haskell.org/package/megaparsec): import Data.Void import Text.Megaparsec import Text.Megaparsec.Char.Lexer -- | -- &gt;&gt;&gt; parseTest word "1234" -- 1234 -- &gt;&gt;&gt; parseTest word "-3" -- 1:1: -- | -- 1 | -3 -- | ^ -- unexpected '-' -- expecting integer word :: Parsec Void String Word word = decimal 
1) If you're looking for the types that are formattable look at the instances for `PrintfArg`. Informally, the three instances for `PrintfType` collectively state that a value whose type has an instance of `PrintfType` is a function that takes some number of arguments (possibly 0), all of which have instances of `PrintfArg`, and returns either an `IO ()` or a list of characters (`IsChar`). 2) The tilde here is an equality constraint. Basically `a ~ ()` says that `a` must be `()`. For example, you could write the function myNot :: (a ~ Bool) =&gt; a -&gt; a myNot True = False myNot False = True Of course, this example isn't very useful because it's the exact same as if you had written `Bool -&gt; Bool`, but it turns out there is a subtle difference between instance PrintfType (IO ()) where ... and instance a ~ () =&gt; PrintfType (IO a) where ... [This blog post](https://chrisdone.com/posts/haskell-constraint-trick) is an excellent explanation of why the equality constraint is necessary here.
What is megaparsec for? How does it compare with trifecta, attoparsec, and parsec?
&gt; I strongly believe the vast majority, if not all cases, of this statement are meant to self-mock. This is the assumption/belief with which I was disagreeing but I wrote unclearly. Not all cases of use of the quote "a monad is just a monoid in the category of endofunctors, what’s the problem" are meant in mock, either of the teller or listener. Individual cases vary.
I don't use the Wiki. It never seemed helpful to me. What do you think about pointing newbies to Stephen Diehl's [What I wish I knew when learning Haskell](http://dev.stephendiehl.com/hask/) instead?
Come on, now. You can golf and obfuscate pretty much any language, and creative use of Haskell ought to be praised. If someone gets a panic attack because of the linked page then so be it. You don't see C programmers decrying fast inverse square root as a unholy evil.
https://github.com/mrkkrp/megaparsec#comparison-with-other-solutions
Since you specify “in industry” I suppose some of the big areas will be web and app dev. Web dev with Haskell is great! Servant is fantastic for backend/api stuff, persistent and esqueleto are great for DB stuff. Miso is absolutely amazing for front end Haskell web development, and I’m excited to start playing around with reflex for app dev.
[Here is my solution](https://gist.github.com/gelisam/ba755361fdb34c952776e79b8bf02602). I was initially planning to use cassava, but ended up writing my own csv parser in order to accommodate your non-standard format and in order to make sure my solution would stream.
Purely functional data structures pretty much laid out the problem, figured out what you needed to analyze solutions in the space, and then built most of the interesting data structures that are suited to his approach. The major things I know of since that have contributed interesting results are: * Hinze's paper on [bootstrapping flexible arrays](https://www.cs.ox.ac.uk/ralf.hinze/publications/ICFP02.pdf), * [Brodal-Okasaki heaps](http://www.brics.dk/RS/96/37/BRICS-RS-96-37.pdf) * Brodal et al.'s [Purely functional worst-case constant catenable sorted lists](http://tildeweb.au.dk/au121/papers/esa06trees.pdf) * [Reflection without Remorse](http://okmij.org/ftp/Haskell/zseq.pdf) makes heavy use of Okasaki style data structures. I'm not sure if any of it would qualify as "seminal" but I've used ideas from Okasaki personally in several projects, mostly his number systems stuff: * [Cache Oblivious Maps](https://www.youtube.com/watch?v=OJLcGJqvWgQ) * [Leonardo Random Access Lists](https://www.schoolofhaskell.com/user/edwardk/fibonacci/leonardo) * After we beat on it for a couple of months, and long after I personally gave up, Alex Lang and James Diekun managed to finally implement the incredibly poorly documented [Tarjan-Mihaescu deque](https://github.com/alang9/deque/blob/master/Data/Deque/Cat.hs) in Haskell. Outside of the Okasaki'ish toolbox, the main additions to the functional programming toolbox have probably been: * Hash-Array Mapped Tries (by the late Phil Bagwell), as implemented in Scala, Clojure and Haskell's `unordered-containers`. * [RRB-Trees](https://infoscience.epfl.ch/record/169879/files/RMTrees.pdf) are a reasonably efficient array-like structure using 32-way trees, also due to Phil Bagwell. They work great if you ignore the nonsensical claims in the paper about being operations "effectively constant time"; log base 32 is pretty good, but it isn't that good. N can easily exceed main memory size while retaining a tiny representation and become enormous as you can simply grow them by repeated doubling. * Transients by Rich Hickey give a general framework for 'temporarily mutable' data, with O(1) conversion to/from mutable form. These are well suited to implementation in Haskell through ST. 
Returning a type and generating a brand new type are two fairly different things. You could check the input to see whether you want to return `[Int]` or `(Bool, Bool)`. But you can’t check the input to see whether you want to create `data Foo = ...` or `data Bar = ...`. The key difference is that `data` and `TH` and the like are actually extending `Type`, whereas everywhere else you are just returning values within that type. 
Discussion around "right-bias" on Either, as far as I recall, came from Scala, where type-constructors are uncurried. I don't it really pertains to Haskell. I think it is more accurate (Haskell) to say that Either is a type function, taking one argument (like all functions), which is a type, returning a type constructor :: * -&gt; *. 
Fast inverse square root is probably a bad example. Wikipedia shows the code as having a comment specifying that it is "evil floating point bit level having", also a comment that just states "what the fuck". Even the maintainers of that code believed it was unholy evil, or at least not the kind of idiomatic C that you normally see.
Or really just &gt; make memes
Surely it is time to work together.
Should I be using [Data.Bytestring.Conversion](https://hackage.haskell.org/package/bytestring-conversion-0.3.1/docs/Data-ByteString-Conversion.html)? I need to be able to convert floating point numbers into bytestrings (to send them over a websocket). This package has many downloads, but a pretty low rating. The most concerning thing is that when I run `runParser' parser $ toByteString 1.2` in `ghci` I get `Left "Trailing input". I'm not sure how to interpret this error message, and it's strange because `Double` is an instance of both `toByteString` and `fromByteString`. Am I doing something wrong? If not, is there are reliable tool that I can use with Haskell to quickly convert floats into bytestrings? (32-bit floats optimally, but 64-bit will do)
&gt; it’s quite possible we have more mtfs than biological females Not quite! Tallying up responses to "What is your gender?" and "Do you identify as transgender?" from the provided CSV, there are 26 "Female"/"No"s and 19 "Female"/"Yes"es.
(I think) I get it, just contesting the appropriateness ;) (which may or may not be justified)
The use of right folds with strict accumulators on finite sequences and of strict maps is more or less something I've taken along as best practice for avoiding space leaks while reading this subreddit ;-) &amp;#x200B;
&gt; how has he caused a divide? Because a couple individuals strongly dislike him and his work but are very vocal about it which fuels the divide: https://github.com/tonymorris/do-not-use-stack/issues/1
&gt; haskell.org and cabal-install weren't doing the job. h.o couldn't offer simple getting started instructions and c-i couldn't install things reliably. ...why are you using the past tense? According to archive.org there hasn't been any significant changes to the newcomer pages on haskell.org over the last 2 years. And as far as I know cabal new-build is still considered an opt-in experimental beta feature and keeps lagging behind Stack in terms of newcomer friendliness and features. 
Yes, but it still doesn't explain why they decided to build a new tool instead of fixing what's already there.
Please anyone who know eclipse che and haskell, work on this : https://github.com/haskell/haskell-ide-engine/issues/516 https://github.com/eclipse/che/issues/10568 che is used by the online IDE of codenvy.com. With 3 GB for free to begin with, I can build docker images, develop instantly with an haskell docker images, run web programs etc. the only thing missing for that superb IDE alternative is che integration. 
Who is this guy and why are people interacting with him?
In 2014, when I started learning the language, I did use the wiki a lot instead, even though some pages are frustratingly terse. Back then I'd have to use whatever resource I could find to peel apart the many layers of jargon, lore and technicality. WIWIK didn't even exist at the time.
Seriously? Don't you know Michael Snoyman? He's single handedly implemented a huge part of the ecosystem we use (Stack, Stackage, Intero, Yesod, Conduit)
They are, features get ported to each other all the time and Stack uses Cabal-the-Haskell-library as its foundation, it's all already shared
I'm sorry but do you even hear yourself? this is far too much fanboy-ism. Languages are tools and and placing too much focus on the language itself aka a tool is the wrong kind of developer perhaps an immature one.
LGTM
I ported the lens library to C# a few years back for my work (we also have an immutable type code generator to make it actually useful).. Sadly my employer has no intention of open sourcing any of it, but it's probably saved me thousands of hours of work at this point. Sadly coding in pure FP programming in C# is a bit of a dead end career-wise.. 
&gt; MIRI is focused very much on a blend of the longer term problem and on industry coordination. When you are the size MIRI is, splitting your focus is a great way to not do anything well. On the other hand, organizations like the Future of Humanity Institute out of Oxford have been taking a shorter term focus and are trying to work better with insurance companies, and governments and the like working to help figure out policies. Unfortunately, I don't have much faith in the efficacy of such negotiations because the root of the problem is our measurement of a society's success by short-term monetary metrics. Until we scrap that kind of thinking (which amusingly, all of our religions warned us about) and replace it with a social paradigm more grounded metrics like ecological, demographic, and genetic stability, I fear such negotiation efforts are just going to result in a myriad of new ways to sacrifice these things for money. Case in point: BPA is a material used in plastics with all sorts of disruptive effects on sexual hormone regulation, so the environmental activists rallied and got the governments to regulate BPA, and the corps responded appropriately by finding a BPA substitute. Great, the system worked! Except [the substitute turned out to be just as bad as the original](https://news.wsu.edu/2018/09/13/wsu-researchers-see-new-plastics-causing-reproductive-woes-old-plastics/) (which should be unsurprising, given it was, well, a valid chemical substitute) and the whole thing was a gigantic waste of money that did jack all to help the environment. And it's not like this is the only example - practically everything I bother investigating turns out this way, because the systematic incentives are just fundamentally wrong. Negotiating with Facebook is like... that scene from How to Train Your Dragon, when Hiccup's dad exasperatingly says "The problem is... this" *gestures at all of Hiccup* &gt; Argumentation that starts with "why are you worried about X when I think issue Y is so much more important" tends to be a bit of a non-starter for me. If we have to wait until world hunger or world peace or poverty or breast cancer or some other thing to be fixed to an arbitrary Procrustean standard before we're allowed to spend any effort on something else, then we'll be potentially blocked indefinitely on anything you don't think is important. I agree, you're right - I can be a bit forward sometimes. Not everyone has to focus on what I think is important, and even if I think more people should, the sensible course of action is to inspire more people to join my cause, not complain about people joining someone else's. Part of why MIRI annoys me is that we agree on enough that the remaining dissonances are magnified. It's sorta like how it's more infuriating to program in Elm than JS, despite Elm being much closer to what we want than JS. Elm gets "enough right" to bait you into trying to write Haskell and then getting frustrated when you can't, whereas JS just flat out has no capacity to even attempt that and so you don't frustrate yourself trying. That's kind of how I feel about MIRI. In any case, as I said in my original comment, this expansion of MIRI efforts to include more general comp sci research greatly increases my respect for the organization. Even just the fact that they somehow knew to look for Haskell devs is a healthy sign.
I meant Tony Morris.
You input an expression with an ambiguous type and type defaulting assigned **wrong** type here. &gt;&gt;&gt;:set -Wall &gt;&gt;&gt;runParser' parser $ toByteString 1.2 &lt;interactive&gt;:31:1: warning: [-Wtype-defaults] • Defaulting the following constraints to type ‘Integer’ (Show a0) arising from a use of ‘print’ at &lt;interactive&gt;:31:1-36 (FromByteString a0) arising from a use of ‘it’ at &lt;interactive&gt;:31:1-36 • In a stmt of an interactive GHCi command: print it &lt;interactive&gt;:31:21: warning: [-Wtype-defaults] • Defaulting the following constraints to type ‘Double’ (ToByteString a0) arising from a use of ‘toByteString’ at &lt;interactive&gt;:31:21-36 (Fractional a0) arising from the literal ‘1.2’ at &lt;interactive&gt;:31:34-36 • In the second argument of ‘($)’, namely ‘toByteString 1.2’ In the expression: runParser' parser $ toByteString 1.2 In an equation for ‘it’: it = runParser' parser $ toByteString 1.2 [...] Left "Trailing input" You see? There are two ambiguous types. (a) what type the result should be, and (b) what type the literal `1.2` is. (a) is defaulted to `Integer`, which we didn't expect. Specifying them clears the error here. &gt;&gt;&gt;runParser' parser $ toByteString (1.2 :: Double) :: Either String Double Right 1.2 But sorry, I don't know what library you should use.
It was tongue-in-cheek. Rust obviously has different design goals than Haskell, and even aside from that, there's tons of stuff Haskell gets wrong anyway. That said, the balance we have between theory and actually being useful is, in my estimation, substantially better than anything else currently available.
&gt; That said, the balance we have between theory and actually being useful is, in my estimation, substantially better than anything else currently available. That's fine, I think each engineer will have their own estimation in regards to how pragmatic Haskell "is" as this will be subjective based on a lot of external factors. My take is this: a tool _(e.g. Haskell, Prolog, White Space whatever)_ may work for you, it may even feel like its the right solution for problem X,Y and Z, but that doesn't make it so, it just makes it so for you as an individual and your context. The danger then is to project that subjective evaluation of "usefulness" to become a general statement. Worse still is when you are inside an echo chamber, where biases are simply magnified. I personally have no attachment for any language _(I dislike them all for different reasons)_. I frequent many language communities and its super interesting (at least for me) to get a feel for the atmosphere of each language community, I guess I'm a language nomad in that regards, and I hear the same things every community thinks their one is more "useful", "productive" etc.
So, is it primarily an organisational shift that's left?
T-shirt real-estate: The one thing that I can match with (or ever surpass) edwardkmett.
wooosh
I'd expect the behaviour because of two reasons. 1) (0 :: Word) - 3 will have this result 2) Never trust read If you want something close to Natural there's Numeric.Natural :)
Usually parser combinator libraries have function of type: parse :: Text -&gt; Parser a -&gt; Either ErrorMessage a So you can just run parser over your textual inputs and just check that it results into `Right myObject`. Very simple unit tests.
Pretty much. That's where most of the rub is, too
For something small like this attoparsec is pretty nice to get going with [https://github.com/FintanH/aocrc/blob/master/day1/src/Frequencies.hs#L85-L101](https://github.com/FintanH/aocrc/blob/master/day1/src/Frequencies.hs#L85-L101)
&gt; (if someone wants to twitter this to the author, feel free. i have only targeted the reddit bubble.) What a cowardly request. I hope that, upon reflection, you see how this is part of the problem.
The comments in this thread do a great job of confirming the concerns of the author.
As a Haskell newbie my impression is that you (the Haskell community) focus too much on the mathematical names. My (current) POV is that there are some abstractions (or techniques) that functional programmers have discovered over time that happen to be useful when programming in Haskell. And it happens also that most of those abstractions exist and have names in different mathematical fields and that haskellers have pervasively reused those names. But it is my intuition that in practice, the relation to the mathematical concept ends just there, in most cases you don't really use those mathematical concepts in order to build higher abstractions when programming. Take the classic "a monad is just a monoid in the category of endofunctors". Is that knowledge useful for the day to day Haskell programmer? I don't think so. IMO, when programming, "Monad" is just a name for a programming pattern. Also, compare it to the OOP world, they use names from the everyday world to name their abstractions (Factory, Builder, Command, Adapter, Bridge, etc.). People can much easily relate to those abstractions. Using the mathematical names just induce newbies to believe that they must understand the mathematical concepts behind those names in order to program in Haskell effectively which as far as I can see is completely wrong. Take another view, if you wanted to teach somebody to program in assembly, would you start by telling then that the processor operates with the 𝚭₈ 𝚭₁₆ 𝚭₃₂ 𝚭₆₄ rings and then proceed to classify its operators according to ring mathematical properties and such?
It seems attoparsec is similar to megaparsec.
look I get now that his response is also a "joke": * after your "wooosh" reference * which is often used to imitate the sound of something flying over ones head * which itself alluding (in a mocking way) that some one didn't grasp the original comment. * I would have preferred "he was joking mate", instead of the more mocking style of "wooosh". But I forgive you I will not hunt you down like a wild dog and feast on your liver cooked over a make shift camp fire. but without any ":)" its difficult to parse the emotional intent when reading plain text.
 &lt;interactive&gt;:8:3: warning: [-Woverflowed-literals] Literal -1 is negative but Natural only supports positive numbers *** Exception: arithmetic underflow &gt; (-1) :: Word &lt;interactive&gt;:9:3: warning: [-Woverflowed-literals] Literal -1 is out of the Word range 0..18446744073709551615 18446744073709551615 It's probably consistent in the sense that numbers are desugared with `fromInteger` and that shows the same behavior as `read` (See [here](http://hackage.haskell.org/package/base-4.12.0.0/docs/src/GHC.Read.html).) and `Read` is supposed to *read in* Haskell values with correct syntax. There might be some practical reasons why you want negative numbers to be represented: &gt; (-1) :: Word 18446744073709551615 &gt; 18446744073709551615 + 1 :: Word 0 As you can see `(-1)` is actually the additive inverse of `1`. Therefore `Word` is not simply a limited version of the natural numbers because natural numbers do not have an inverse element. The wider issue here is that `Natural` is put in a hierarchy that it doesn't belong to.
Ya they're all quite similar. You could even just describe the operation using the parsers interface (https://hackage.haskell.org/package/parsers) and drop in and out of different solutions. I would personally just opt for attoparsec because it's lightweight but megaparsec is also great! I guess it depends on how much I would be parsing :)
If you have a matching pretty-printer, QuickCheck that you parsing will undo the pretty-printing. Great for finding "used `\` to escape things but forgot to escape `\`" bugs.
Tony Morris had a beer and lunch with Michael Snoyman today.
I'm sad to see the downvotes on this one. New users, pretty much by definition, are not fully able to prioritize concepts and figure out what is essential to learning Haskell vs. what is cruft. There are effectively no applications of the concept ([https://stackoverflow.com/questions/5057136/real-world-applications-of-zygohistomorphic-prepromorphisms](https://stackoverflow.com/questions/5057136/real-world-applications-of-zygohistomorphic-prepromorphisms)). It exists as a joke at best, or passphrase to identify yourself as belonging to an in-group at worst. I won't speak to "what we should do" as a community, but as a former educator I would burn this stuff down with extreme prejudice.
I use VSCode on mac, and I remember it took me a while too. Maybe I can help? Could you describe your setup in more detail, i.e do you use stack, how did you install HIE, did vscode show any error messages etc?
And there are occasional "121 contributions on Sep 24, 2018" or "64 contributions on March 9, 2018"... there is a pattern ;)
To your second point: This is a bit different though, since it interfaces with the Language Server Protocol, so any dev effort will pay off for every editor that can use LSP. In your case, you'll be fine without spending much time on changing your vimrc, the instructions in the HIE readme are pretty usable.
In a project with multiple packages, should we have one orphan module per package, or a single module for the whole project? Many of the listed advantages come from the fact that there is only a _single_ module, so a single module for the whole project sounds good, but this in turn causes the project which hosts that module to have a lot of dependencies. For example, in our project that module contains orphan `MonadHandler` instances, which in turn causes our utility libraries to indirectly depend on yesod even though they're not defining webservers.
We went with a single orphan module for our entire project, which includes a hand full of packages. There is the downside of forcing some dependencies lower in the stack, but I prefer the conceptual simplicity. If you eventually want to open source a package, then you'll likely be shedding the orphan instances as well, so you then just need to decide how important dependency hygiene is for your internal packages.
In fact i was just complaining that two’s complement isn’t taught as relating to the general theory of p-adic numbers!
&gt; Take the classic "a monad is just a monoid in the category of endofunctors". Is that knowledge useful for the day to day Haskell programmer? Do you realise that even mathematicians don't use that as a definition?
what if i wanted to stick to prelude and not use any other functions?
What kind of skills do people who write functional code (but Haskell in particular) professionally have? My goal in life isn't to be a Haskell developer, but if I happen to know some things that would *enable* me to be a Haskell developer, well, that would be pretty nice.
What sort of beer did he have? 
&gt;as far as I know cabal new-build is still considered an opt-in experimental beta feature and keeps lagging behind Stack in terms of newcomer friendliness and features. What kind of features do you have in mind?
You could use [Text.ParserCombinators.ReadP](http://hackage.haskell.org/package/base-4.12.0.0/docs/Text-ParserCombinators-ReadP.html) from base to avoid additional dependencies. word :: ReadP Word word = fmap read . many . choice . map char $ ['0'..'9'] parse :: ReadP a -&gt; String -&gt; Maybe a parse = fmap f . readP_to_S where f = \case [] -&gt; Nothing xs -&gt; Just . fst . last $ xs λ&gt; parse word "123456" Just 123456 λ&gt; parse word "-123456" Just *** Exception: Prelude.read: no parse 
Then you'll have to implement chunksOf yourself or come up with a different solution I guess 😄
I've looked into doing it before, it's actually a lot of work. You can't replace your current config with the lsp stuff because not all languages have an lsp/not all machines will have it installed. You end up with a whole bunch of conditional logic and now whenever you make a configuration change you need to do it in two or three places. It also involves hours of time in the documentation figuring out what things need to be configured.
Did they tear each other to pieces like the fanboys dream?
Your rebuttal only applies so far as your feeling is generalizable, in which case I ask: should all newcomers have to figure this one out by themselves? It seems like a lesser instance of broken stair to me.
The trouble is that the RISC-V LLVM backend contributed by lowRISC looks nothing like the other architecture backends so adding GHC's calling convention is now considerably harder. It would be great if someone could pick this up.
None.
s/Haskell/high-school
How did you make the hierarchy work without multiple inheritance? Did you use implicit conversions?
I use VSCode. Far as I can tell, it does require a stack project to work, or at least the VSCode integration does. I haven't used it (or Haskell in general) in anger on a serious project, so I couldn't tell you where the rough edges are.
Well, I know enough maths to understand what "a monad is just a monoid in the category of endofunctors" means. In any case my point is that for me, the core of maths is that given some knowledge (a set of axioms) we are able to derive new knowledge, and from there more knowledge, etc. So, in the case, of monads, given whatever definition mathematicians use to define monads nowadays, they are able to derive that a monad must be "a monoid in the category of endofunctors", and maybe from there derive other knowledge that they may find useful. My impression so far, as a Haskell newbie, is that Haskell uses the fancy mathematical names and that's all. It doesn't really take advantage of the knowledge we have of the mathematical objects (or at least not too much - and that is probably fine, BTW). My first impression when I approached Haskell a couple of months ago was: "at some point I may need to get really familiar with category theory in order to become an expert haskeller". Currently I don't think so anymore... Obviously I may be wrong (feel free to correct me if so) I am just trying here to give you my impressions as a newbie trying to learn Haskell. &amp;#x200B;
While I haven't been involved in scaled-up Haskell development, I would tend to isolate them to only those packages that produce an executable (which might be all of them!), at least as a starting point.
I think you dropped this --&gt; /s
&gt; As a Haskell newbie my impression is that you (the Haskell community) focus too much on the mathematical names. I'm glad Haskell doesn't obscure concepts by creating new names for things that *already have* ^mathematical *names*.
I'm very interested to know why you thought you might need to learn *more* category theory despite already understanding what the very advanced concepts comprising "a monad is just a monoid in the category of endofunctors" mean!
To be woefully earnest, I've found increasingly that this perspective is important, especially as it relates to thinking of "generalized monads" in 2-categorical settings where the base category is not Cat. Your really then do need to start with an abstracted notion of a higher monoid object to make the definitions work! (C.f. Street's absolutely beautiful "The Formal Theory of Monads" paper). 
&gt; The correct literal range for Word should therefore be -18446744073709551615 .. 18446744073709551615. No. `Word` is specifically unsigned whereas `Int` is signed.
Ah, thanks. This library should be fine, at least for the time being.
\str -&gt; let v = read str in if v &lt; 0 then Nothing else Just v
I was not rebutting. Nor do I generalize my experience, it's mine. &gt; should all newcomers have to go down this particular dead end? No. That's why I'd recommend newcomers go to better resources than the Wiki, like WIWIK...
If you dig into historical stuff a bit, there is a rich history of conflict between Snoyman and the Haskell.org, including ostensibly irreconcilable process and communication issues. I think it would be uncouth to pick a side in this thread, but I think time and hindsight have not been kind to some of the parties involved, and the whole conflict has obviously set Haskell back vs. an ideal world where everyone managed to successfully collaborate. I'll let you find out for yourself what is what.
I used interfaces and extension methods, so in essence I had multiple inheritance: - ILens implements IGetter and ITraversal - IPrism implements ITraversal, - IGetter implements IFold - ITraversal implements IFold and ISetter
Kingfisher.
I don't really have an opinion on whether a quota system makes sense for Trudeau's cabinet or GHC, etc. I think you're probably right that that isn't a great idea. But I think your argument comes from an over-abundance of concern for and faith in an idea of meritocracy that I've come to believe is basically a myth. Once you get to a certain level of achievement, you have a pool of people who have a certain amount of excellence and all of whom have something very useful to contribute. At that point distinguishing among them (for the purpose of promotion, awards, selection for conferences, etc) becomes in large part arbitrary, or just not something humans do particularly well. Or put another way I believe you can strive for "equality of outcomes", as you put it, while finding that the quality of your result is undiminished or improves. I'm thinking here of my 20yrs experience in the classical music world (competitions, auditions, etc), and also my limited experience at tech conferences: e.g. at Strange Loop 2017 my favorite talks were (IIRC) mostly by people who were not white men. I'm certain building a conf as (relatively) diverse as Strange Loop takes a lot more work in terms of outreach and years of building trust and reputation, with diversity as a conscious goal and benchmark. And the outcome (for me at least) was a better conf. That's not to say merit isn't real of course, or that you could institute any arbitrary quota system without outcomes suffering. But I think focusing on diversity directly as a goal and yardstick leads to better outcomes and does not meaningfully diminish opportunities for those of "higher merit".
https://code.world is also a good introduction to Haskell; it pares it down to just the essentials, and has nicer error messages.
\[Here\]([https://gist.github.com/chessai/e5a04ddcbc6c6708333e187ee8ae41a3](https://gist.github.com/chessai/e5a04ddcbc6c6708333e187ee8ae41a3)) is the same solution, but rewritten to use streaming, and uses `siphon` instead of `cassava`. I find `cassava` to be rather inflexible and its usage of typeclasses is annoying. `siphon` is built on top of the `colonnade` library. `colonnade` is a profunctor with a very useful `Monoid` instance that allows you to cleanly compose columnar data. A `Colonnade` is a _producer_ of columnar data. A `Siphon` (from the `siphon` library) is a _consumer_ of columnar data, particularly CSVs, and `Siphon`s also compose nicely; `Siphon` is just a specialisation of the free applicative, so you use applicative to compose them. 
So in a concrete `class MyLens : ILens`, do you have to manually implement all of the `IGetter` and `ITraversal` methods? That's what I'm using default interface methods for in the post: you just implement the minimum `ILens` interface and get the other methods for free. Sounds like you're codegen-ing your lens classes so maybe not an issue in practice?
&gt; It's not clear to me where microaggressions begin and end Terms like "microaggression" are critical tools, they are ways both for people in discriminated groups, say, to have a framework for understanding, identifying, and talking about things they experience, and also for people in a dominant group to think more critically about their own biases and prejudices, and to strive for more empathy and intelligence. It's not an objective classification and that's okay.
I can't get HIE to work with VS Code on Windows either...that's after going through the pain of just getting it to build
Yeah, I have a a static method off the concrete class, so to create a new lens you do something like: &gt; Lens.Create&lt;ConcreteType,ConcreteType&gt;(get: t =&gt; t, set: (t, f) =&gt; f(t)); I also found type inference breaks down when trying to go 2 levels deep in interfaces, so for practical purposes I mostly use traversals over ImmutableList&lt;T&gt; instead of IEnumerable&lt;T&gt;, since C# can infer the type of 'T' in ITraversal&lt;ImmutableList&lt;T&gt;&gt;, but not ITraversal&lt;IEnumerable&lt;T&gt;&gt;. Do you have a repo link? I'd be happy to submit a PR.
Hey I opened an issue! https://github.com/alanz/vscode-hie-server/issues/126
&gt; none of them do any good at explaining why improving cabal was not an option In my opinion, it was a matter of iteration time and control over what improvements *could* be made. Contributing features to cabal-install would take more time and effort to gain consensus, would have to adhere to cabal-install's "minimal dependencies" philosophy, would have to maintain compatibility with existing cabal workflows, etc. Here's one example. Stack will install ghc for you. I tend to doubt cabal-install will ever do this. &gt; But we are talking about newcomers, right? And to those, I dare predict this mess in this ecosystem is a major turn-off. Personally, I will be pointing newcomers to haskell-lang.org and stack first, because I think they are better suited to newcomers. I don't think we need to force haskell.org to be opinionated in the same way as haskell-lang.org, though I agree it would be nice for them to refer to each other at least as a "see also".
[Here's one way](https://github.com/gagandeepb/frames-explore/blob/master/src/Lib.hs) to do it using Frames, foldl and Pipes. It uses streaming in one place, in-memory when it needs to sort, group etc. and finally makes use of foldl to compute summary statistics in one pass. 
Suspect you're suffering from invariance. `ILens&lt;Foo, ImmutableList&lt;Bar&gt;&gt;` is not a subtype of `ILens&lt;Foo, IEnumerable&lt;Bar&gt;&gt;` because the second parameter appears as both an output (of `Get`) and an input (of `Set`). You need to use the four parameter version if you want variance (`ILens&lt;in S, out T, out A, in B&gt;`).
/u/gelisam thank you so much for the information - sorry it took me so long to reply; work is busy. I appreciate the response.
/u/_kostas thank you!
I think that a great loaf the Haskell Wiki is useless and even confusing, as well as many other publications for newbies since the last -censored- changes in applicatives and semigroups
&gt; I'm not for a political purge of anyone, which I think is a grotesque idea. I may have written too forcefully. I don't want to purge people from the community based on their political views. What I should probably have said instead is that I would discourage people from bringing up radical politics and using them as a basis for harassing others. As far as the "radical left" goes, there are degrees of radicalism, aren't there? There are certainly groups like Antifa and those that believe "capitalism" is central to the world's problems and believe violence against capitalism is justified. Besides that extreme, there do seem to be some other beliefs I see as troubling arising from the Left, such as: * speech can be violence * harmful ideas and harmful people need to be deplatformed * what constitutes bigotry is not up for debate, and anyone who believes to the contrary is a bigot themselves I think it's important to remember that the radical left is responsible for greater atrocities in the 20th century than the radical right, if measured by number of deaths.
the tuple constructor is a function btw λ: liftA2 (,) [1,2,3] [1,3,5,7,9,10,13,15,16] [(1,1),(1,3),(1,5),(1,7),(1,9),(1,10),(1,13),(1,15),(1,16),(2,1),(2,3),(2,5),(2,7),(2,9),(2,10),(2,13),(2,15),(2,16),(3,1),(3,3),(3,5),(3,7),(3,9),(3,10),(3,13),(3,15),(3,16)]
makes sense to me
Oh, that's a shame, I'll have to try it out on my neovim install to see for myself then.
ok
:\^)
I agree with this (having also taken that page at face value), and can't imagine why this is being downvoted.
fp complete waste of time /jk
Thanks I'll give it a try.
Changing the behavior of the function necessarily breaks the callers if you have type erasure. If you want to go from a function that takes an `Int` to a function that takes an `Int` or a `String` - then the difference in the input has to be communicated to the function *somehow*. It's impossible to avoid it and achieve sane behaviour. If you *don't* have type erasure, then you are always paying the price for that, in a different and arguably much worse way. There's no free lunch.
Right, because Perl, Python, and PHP have never changed anything incompatibly, and certainly every Python 2 program works flawlessly under Python 3. Even C and C++ have introduced incompatible changes. C99 required variable length stack arrays to be supported; C11 made it an optional feature. C++98 allowed "auto" with the C semantics, C++11 disallowed "auto", C++14 introduced new semantics for "auto". Take a perfectly reasonable older program, and a newer compiler may not compile it. Some programs that were thread-safe under Java 1.4 were no longer safe under the Java 1.5 memory model. They still compiled, just broke silently but only when under load! I do wish we had a standards committee instead of just the GHC development process. But, I'd still like never versions of the language and libraries to be able to evolve. I'd also love if there was a simple compiler (or build system?) option to use an older language/library standard, but since I'm not paying for a GHC developer I don't think I'm justified in demanding they support the Haskell '98 that I learned. (And, honestly, an older stackage LTS generally serves well.)
Eh. When you’re in charge of an organization, especially a small to medium sized one (10s-100s), publicly articulated opinions matter. There’s a lot of social norms that favor folks doing a “no disagreements ” public image. Which means irrespective of avowals to the contrary, senior staff Members public opinions that relate to matters of the businesses domain, will be reflective of norms of that org. -these opinions are my own and in no way reflect whatever weird stuff my employer may have officially. Cause it’s so huge you could fit every functional programming language user on the planet into their giant ass headcount 
Dude thanks. I'm new to haskell too, and learning this kind of tips (liftA2 and tuple constructor) is very pleasant. Thanks.
Does that mean cabal is the way now?
[removed]
It depends really. If the `x` in this example is really some concrete type, then yes, I suppose you could consider it to be an easing of requirements for the caller, from a certain point of view. However, include any polymorphism, and it's no longer an easing of requirements under any possible viewpoint. `foo :: c x =&gt; x -&gt; [x]` already quite possibly includes the possibility of `foo :: Maybe x -&gt; [Maybe x]`, and `foo :: x -&gt; [x]` *definitely* includes it. By making the `Maybe` explicit however, you are ruling out all other possible types - it is not in any way an easing of requirements, from either end.
You might be interested in the "Being Lazy With Class" essay, which is about the history of the language and design decisions (and is an interesting, easy read).
Yea I don't see the point in this post. He's still in charge; his opinions still influence everything there. Doesn't really matter that his opinions aren't the single source of truth for FPCo and all its employees, they're still a major factor. And that's a good thing, but it kinda defeats the point of the article.
&gt; I feel almost silly writing up this blog post, [...] However, it seems like something like this needed to be said. No, you were right the first time. It really did not need to be said, nor posted here with your troll account week after week. This is /r/haskell, not /r/michaelsnoyman.
Thanks for posting this here, /u/AlpMestan! I tried to do it myself earlier, but screwed it up somehow ;) I have a question for everyone: Have you come across united monoids? If yes, could you please point me to examples, papers, existing libraries, or share any other observations? Thank you!
[snoyjerk is not Snoyman](https://www.snoyman.com/blog/2018/05/i-am-not-snoyjerk).
In `Numeric`, we have [`readDec`](https://hackage.haskell.org/package/base-4.12.0.0/docs/Numeric.html#v:readDec), which only reads positive numbers. This is the standard function defined by the Report for this purpose. In GHC Haskell, it's a better idea to use `ReadP` functions over `ReadS` functions. To that end, you should use [`Text.Read.Lex.readDecP`](https://hackage.haskell.org/package/base-4.12.0.0/docs/Text-Read-Lex.html#v:readDecP).
We are all snoyjerk.
I can't say I've come across them before, but I may have another example: data SemiLatticeSet a = SemilatticeSet (Set a) instance Semigroup (SemilatticeSet a) where SemilatticeSet as &lt;&gt; SemilatticeSet bs = SemilatticeSet (Set.union as bs) instance Monoid (SemilatticeSet a) where mempty = Set.mempty instance Semilattice (SemilatticeSet a) instance Semilattice a =&gt; United (SemilatticeSet a) where connect ass@(SemilatticeSet as) bss@(SemilatticeSet bs) = ass &lt;&gt; bss &lt;&gt; SemilatticeSet (Set.map (uncurry (&lt;&gt;)) $ Set.cartesianProduct as bs) I haven't tested the code, but I'm pretty sure the laws hold. By the way, I'm the author and maintainer of [monoid-subclasses](http://hackage.haskell.org/package/monoid-subclasses). I'd be open to a pull request adding Semilattice and United to it, especially if it comes with a maintainer. ;) 
I have the 4 parameter version implemented, but usually end up using the same for A and B. Honestly I don't want to use IEnumerable directly since it's not really "safe", and really if I want to read a value I'm not going to try to convince my coworkers that using point-free style is valid reason to not do it in LINQ. Traversal are really great for updating records that are like 5 lists deep though. &amp;#x200B; If you don't mind me asking, why make a distinction between Lens and ComposedLens (and all the other types)? If you follow all the right rules, you should be able to just compose the getter and setter functions of the two lenses to get the composite lens (and likewise for all the other types).
Spend fine minutes looking at their comment history. He’s not even trying hard to hide it.
I'm getting [a "No instance for (Default Unpackspec (Item O) (Item O))" error](https://github.com/JeffreyBenjaminBrown/data-haskell-challenge/issues/1).
&gt; But I think your argument comes from an over-abundance of concern I think my level of concern does not even come close to "over-abundance". My original post in that thread was a response to [this post](https://old.reddit.com/r/haskell/comments/a3mz2a/guidelines_for_respectful_communication/eb7mxac/) someone who rather clumsily raised concerns about the guidelines. I tired to help clarify by raising Equality of Opportunity vs Outcomes. I then reposted that that comment attention nometa which someone else then called "extremely conservative" which eventually caused me to suggest that Equality of Outcomes was "far left". By not being aware of subtle differences in historical context and the laws of the US and Australia (laws which are both called Affirmative Action btw) I managed to dig myself into a hole that I would never find myself in under normal circumstances. The problem is that in the US, Equality of Outcomes (and least with regards to race) is the law (which given the historical context is justified) and any opposition to it can legitimately be called "extremely conservative". However, in Australia, Equality of Opportunity is the law and any move to replace that with Equality of Outcomes could only come from the far left. 
And calling others cowards is a sure way to resolving this unspecified problem you mention, I take it? As far as I can see, the main and only way of providing feedback presented on a website is twitter. I don't want to use twitter. If you see the need to throw stupid rhetoric at me as a consequence.. feel free to waste your time.
Hold up, I did not say he caused it. I am not sure who caused it, and I was very careful not to put blame. The thing is, a couple years ago there was one build-tool and one website. Now there are two, and fpcomplete/snoyman appear to be the driving factor behind this duplication. And there appear to be two corresponding "factions" that have mildly strong opinions on the matter. Now, people are free to create duplication, right? That does not make them dividers. It just makes them people with enough motivation to do such a thing. As I said, I have no right to demand an explanation. And duplication is not even necessarily a bad thing either - I completely admit that. Nonetheless, if you publish and promote an alternative without providing this explanation; if you mention only what the new tool does better and give close to no insight to the general public as to why improving existing systems was decided against, if you lack that bit of transparency, then I dare say that you do risk (in a predicable manner) that people are getting confused, and that a userbase and downstream tooling becomes fractured.
Again, I was very careful not to put blame. I'd appreciate it if you did not bend the meaning of what I said in such a manner.
Again, that is how you represent it but not how you write the literals (e.g., `fromInteger @Word 18446744073709551615 == fromInteger (-1)`). You could obviously make such literals illegal (as `negate` can be used) but then `fromInteger @Word` should throw an exception as well when receiving a wrong argument.
From a categorical standpoint, an object that is both initial and terminal is a _zero_ object, and a category with a zero object such that products and coproducts coincide (up to iso) is called an _additive category_ (https://en.wikipedia.org/wiki/Additive_category). The exemplary additive category is `Ab`, the category of abelian groups (https://en.wikipedia.org/wiki/Category_of_abelian_groups). This should relate further to the topological approach, but I confess I don't really understand all the details here.
One other thing to mention.. If you're planning on using this library, you probably want to implement some sort of code generation for algebraic data types. C# doesn't really have a way to represent sum types out of the box (at least none that I know of). 
Also I may point out this bit of history: ["haskell.org and the Evil Cabal"](https://web.archive.org/web/20170422201404/http://www.snoyman.com/blog/2016/08/haskell-org-evil-cabal) which, as far as I know, was silently unpublished. My none else than snoyman. Consider that, and judge whether the accusations within are sufficiently backed up by facts that are available to the general public.
Actually, the `fromInteger` is always done on the unsigned part, and `negate` is used to handle the `-` part, in standard Haskell. The `-` is only processed together with the digits when you enable the [`NegativeLiterals`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#negative-literals) extension. Finally, `Word` (and `Int` and explicitly-sized variants etc.) all wrap (and arithmetic is done modulo some power of 2), so `500 :: Word8` and `99999999999999999999 :: Word` are both accepted. That doesn't mean the warning is wrong; the warning is specifically there so that your literal prints the same as it's input, which excludes all negative values for `Word`. Prelude&gt; import Data.Word Prelude Data.Word&gt; 500 :: Word8 &lt;interactive&gt;:3:1: warning: [-Woverflowed-literals] Literal 500 is out of the Word8 range 0..255 244 Prelude Data.Word&gt; 99999999999999999999 :: Word &lt;interactive&gt;:4:1: warning: [-Woverflowed-literals] Literal 99999999999999999999 is out of the Word range 0..18446744073709551615 7766279631452241919 
If only there were some sort of magic software that allowed anyone to edit things to update them as things evolved. Hmm... documentation that people can contribute editorial changes to... I maybe a future generation will solve that. For now, we're stuck with this wiki.
sorry, I don't really understand what this has to do with what I said. Yes, stack does things differently, and arguably a good deal better (especially when it was first published). But that is missing my main point. I am not saying stack is bad. Or that innovation is bad. Or that duplication is bad. I say that as long as you cause fragmentation, without being really transparent as to why, while making attacks on elusive (to the general public) grounds ("cabal/haskell.org is evil" etc.), you are not in a position to lecture the community in these social matters. And yes, that stuff is in the past, but I will continue to take it into account, especially when neither side has provided any sort of apology. Again, I am refusing to place myself on one side here. If, lets say, hvr would write a similar blogpost, I'd throw my criticism at them just as well :p
I don't like the direction Elm is heading, but unless I can spend time with someone and help unstick him or her, or I know that they're highly motivated, then I will usually point them at Elm.
If someone named snoyberg's trying to make an account look like someone else, why would they name it snoyjerk? 
It's not an 'alternative', it's a separate system with different goals built on the same infrastructure. See: [Here](https://www.snoyman.com/blog/2018/11/stackage-history-philosophy-future) For the explanation you claim wasn't provided. I get that this is a strange experience for new users and you have valid points, but there is no great mistake here, just a series of actions that each made sense in their own contexts. Haskell has a non traditional organizational structure, it's not backed by a specific company or strong personality like most other popular languages. There is no one organization to blame here.
I used to make it a soft prerequisite. Now, its a hard prerequisite, given how quickly people can be to judge. Instead, I adjust how much time I suggest to use Elm before starting Haskell based on the potential learner's background and motivation.
I don't know how you understand the term "alternative", but let me just quote what you linked: &gt; Releasing it as a competitor to cabal-install was another matter. So I'll just safely retract my choice of wording, and replace "alternative" with "competitor". I find that even a bit stronger, but whatever. &gt; For the explanation you claim wasn't provided. I have to plainly disagree. It does not provide the explanation as to why cabal-install could not be improved instead. And haskell-lang is not mentioned in any way either. I get it, people were unable or unwilling to cooperate, and making a fresh new start aligned to exactly what was perceived as the proper goals was considered the reasonable path instead. It makes sense. Nothing to blame. But if you make that kind of decision, with its predictable impact on the community, then I will not refrain from voicing my concern with this kind of disconnect of the author's history and the message they appear to send in their blogpost. 
Ok but now I really want to know why FP Complete hates nix so much
That's exactly what I was going to write!!
Thank you! Yes, this does look like a valid instance. I think it turns into the simplicial complex instance from the blog post if you pick `a = Simplex b`, so your instance is more general/useful. Your `monoid-subclasses` package is pretty cool :) I haven't yet thought about what to do with united monoids; most likely I'll just build it into the `algebraic-graphs` library. I'll get in touch once I figure this out!
Thank you, I haven't come across `Ab` or any other additive categories before. It will take me some time to grok the wiki pages from your comment, as I keep branching out while reading them... :)
My apologies for misrepresenting you, it was just the vibe I got from yours (and other) comments. So to summarize: snowmen wanted to take cabal-install and other tools in a direction that the committee didn’t like so he just created an extra “layer” of tooling on top to take in the direction he wanted, but people are unhappy that he didn’t just make adjustments to the original tools or at least provide a transparent description of what happened, preferably with explanations of the different tools strengths and weaknesses. Something like that?
The entire 'History of Stack' section focuses almost exclusively on the reasons that the design direction of `cabal-install` was incompatible with their idea of reproducible builds, so, I am really not sure where you're getting this idea that there is no explanation of why `cabal-install` wasn't improved instead. I am honestly confused as to what open questions you might have about this, given that you seem to be possessed of at least some knowledge as to the history of events as not recorded by these blogs. It is certainly a valid historical interpretation to view the story of 'Cabal vs. Stack', such as it is, as the history of two opinionated groups of developers engaged in a turf war over the end user experience of building a haskell package. There is plenty of factual basis to back up such an interpretation. But I think it's a pretty useless interpretation. There was never a world in which FPComplete would've made a pull request to cabal to add support for stackage snapshots or something like it, and had that pull request accepted. That possibility did not exist, and still doesn't. That's not because they're giant manchildren stuck in a knuckle-dragging fight to the death on the internet, it's because stack and cabal don't have the same design goals. The drama is just window dressing on an irreconcilable design conflict, and it's not worth beating the dead horse. Haskell-lang.org vs. Haskell.org, on the other hand, is an obnoxious situation that should really be fixed. I have no defense for that, and don't think one is possible.
Is there a reason `trifecta` does not have a `Rope` instance for `Text`? It appears that this parsing library is limited to `String` and `ByteString`... or am I missing something?
The module should be called `Orphanage.hs`. :)
FWIW, I've recently written up some (conjectured) laws for the normal form of grates @ https://hub.darcs.net/roconnor/lens-family/browse/src/Lens/Family2/Unchecked.hs#163 grater ($ s) === grater (\k -&gt; f (k . grater)) === grater (\k -&gt; f ($ k)) But I haven't gotten around to publicizing them yet.
Orphan Instances (or rather the way they are currently handled) are IMO one of the biggest issues with Haskell today, and while the OP solution works it is far from elegant. We need some sort of theoretically sound system for delegating where to find orphans for various type / class combinations. Both on the package level in the cabal file, and on the module level in the various `.hs` files. My current preference would be the ability to `export` modules without importing them, either unconditionally or conditionally. For example: ``` module Foo ( Foo(Foo) , module Foo.Show ) where data Foo = Foo ``` ``` module Foo.Show () where import Foo instance Show Foo where ... ``` Now if you added the needed `import` statement above in `Foo` you would have a circular import (which as a side note really should be automatically handled by GHC with Haskell's love of mutual recursion), but with my suggestion since you only export the module, and don't import it, this module export should essentially be thought of as "if you import `Foo` you automatically also import `Foo.Show`" and GHC would therefore not warn about orphans. Now this allows us to break up instances into multiple modules without any issues, although we need more to tackle the real issue of orphans, which is instances not being made because neither package wants to directly depend on the other. To do that we need to extend the above to allow for the ability to only export modules if some other module/class/type exists. This I have not settled on a precise design for, as it's a little more tricky, but something somewhat like the following might work: ``` module Foo ( Foo(Foo) , if Bar module Foo.Bar ) where data Foo = Foo ``` ``` module Bar ( Bar ) where class Bar a where ... ``` ``` module Foo.Bar ( ) where import Foo import Bar instance Bar Foo where ... ``` So basically `Foo` will pull in `Foo.Bar` if `Bar` exists. Thus `Foo` wouldn't have to depend on `Bar` directly, it just has to acknowledge its existence, a much weaker requirement. This then would mean that the `foo` package would contain `Foo`, and it would conditionally export package `foo-bar` if package `bar` existed, or another option (both would ideally be supported) is that `Foo.Bar` actually is in the `foo` package, but the `Foo.Bar` module is only exported if `bar` exists, thus you can use `foo`/`Foo` without pulling in `bar` as long as you don't import `Foo.Bar`. So the package level configuration would look a lot like the module level configuration, just in the cabal file instead. I am not 100% sure that the above is the right way to go, and I would really welcome criticism / people poking holes and pointing out edge cases. However something like the above could really really improve the numerous situations I have encountered where a library does not create a perfectly reasonable instance because they (understandably) don't want to pull in a third party dependency.
The problem with "easily relate" and "everyday world" is they give a satisfying but false sense of understanding. Take inheritance for example: class inheritance seems similar to genetic inheritance, but none of my knowledge of genetic inheritance gives me any insight on how or when to use class inheritance. If I try to apply any rigor in defining inheritance so I know how to work with it, things get complicated quickly. covariant and contravariant polymorphism(both ad-hoc and parametric) in the face of subtyping is no easy topic. Another great example of friendly name abuse is git. Git uses familiar terms from other revision control systems to seem more accessible, but the git operations are subtly (sometimes not so subtly) different leading to confusion and costly mistakes. Contrast this with Monoid - I didn't know monoids when I started Haskell, but the unfamiliar name clued me in that I was learning something new and the simple rigorous definition made it easy to reason about.
One idea I've raised in the past is to offer a pragma stating where an instance must be if it exists. data Foo a = ... {-# INSTANCE_IN package1:Module1 package2:Module2.Bar (Foo a) #-} class Baz a where ... {-# INSTANCE_IN package3:Module3 Baz package4:Module4.Quux #-} Note that the `Bar` class and the `Quux` type do *not* have to be visible to the pragma. The pragma must appear in the module defining a class or a type, and explains where the instance corresponding to the given instance head is allowed to appear. If it appears anywhere else, that's an error. So now neither package has to depend on the other; their authors only have to agree on a third package that should contain the instance.
I would prefer for a way to avoid being required to mention the package name in the `.hs` file, as you don't need to (except in the case of ambiguities) currently, I also would like to avoid a pragma. But besides those issues that seems very reasonable.
Why are you even posting this? It’s like you are desperately attempting to keep the divide in the community instead of working towards fixing things...
Why? I simply do not see why size magically makes a difference in that regard? I work at a smaller company, and my opinions are certainly not that of my colleagues or employer. They might align some times, they might not others, but I am here in my capacity as a private person. I don’t think it’s constructive to have this arbitrary line drawn somewhere in the sand, based on your seniority and company size.
`Orphanarium.hs` is also acceptable.
Obviously so that simpletons like us wouldn't notice. ^^^/tinfoil
Nice, these look like the laws I found too. law1 : forall a : A, a = grate (fun ab : A -&gt; B =&gt; ab a) law2 : forall (ib : (A -&gt; B) -&gt; B) (i : A -&gt; B), ib i = i (grate ib) law2b : forall g : (((A -&gt; B) -&gt; B) -&gt; B) -&gt; B, grate (fun i : A -&gt; B =&gt; g (fun f : (A -&gt; B) -&gt; B =&gt; i (grate f))) = grate (fun i : A -&gt; B =&gt; g (fun f : (A -&gt; B) -&gt; B =&gt; f i)) ______________________________________(1/1) a = grate (fun ab : A -&gt; B =&gt; ab a) I just don't know how to convince Coq that `law2b` (yours) is enough, so I have mine (`law2`, but with a predicate restriction). --- Also you have the third **MatchMatch** rule for Prism. That's very good. Let's see what Edward says when I make a `lens` `Prism` documentation update. 
I think root of the problem is "over-hyping" on FP Complete. There is Ocaml community and Jane Street Capital. And there is Haskell with FP Complete and people often made parallels, but I think they are wrong: Ocaml can not be compared to Haskell as well as JSC to FPC: one is serious successful company with very big science capacity and another one is small consulting company without any big success stories. So, opinion of FP Complete (about Nix or anything else) should not be taken seriously - somebody in the Web said something, nothing more. Somebody likes Nix, another one - does not, FP Complete is not standard and can have personal opinion (as well all their employees: "We have technical disagreements on our team"), better is to discuss Nix features impersonally then to talk about "Snoyman", "FP Complete" or something similar
perhaps by making web sites traversable
Yeah, it is the way to... (•_•) ( •_•)&gt;⌐■-■ (⌐■_■) ... Cabal Hell!
lol, that's a weird way to misspell "cabal" 
Nah, it's all good! I saw them making out. =)
https://gitlab.com/haskell-hr/basic/blob/master/src/Data/Basic/Example.hs There is a worrying amount of `unsafeCoerce` in that file. I'm pretty sure that can't be safe, particularly since as far as I am aware `unsafeCoerce ()` can potentially cause UB even if not directly read.
Python, PHP etc could do so because they don´t need to attract new users. The problem is that Haskell is an elitist cult and most of the haskellers want to keep the cult going.
yeah I'll wait with doing anything with this until they get decentralized staking on the main net.. 
I think the identity for `connect` is `SemilatticeSet (Set.singleton mempty)`, actually.
You have endless alt accounts, I see that. How old are you, 12?
From looking at the source, trifecta operates on byte sequences only (via ropes). The `Reducer [Char] Rope` instance simply encodes strings as UTF-8. You could do the same for Text, though this doesn't really buy you much. instance Reducer Text Rope where unit = unit . strand . encodeUtf8 cons = cons . strand . encodeUtf8 snoc r = snoc r . strand . encodeUtf8
Thanks for catching that. It's an old piece of code (that happens to work in this case), but we've moved to a more principled approach to "empty" values since then and the generated code is now completely safe (as in, it doesn't use unsafe functions anywhere).
Great to hear! Thanks!
By using so many separate accounts you are robbing this subreddit of genuine character. Hint: stylometry.
Another sock puppet. I am downloading the comment history of all your fake accounts and will be bringing them and the stylometry results to the moderators. 
Another Michael Snoyman sock puppet. You have to be more careful with your trolling. 
&gt; We have one member of our committee due for retirement -- our chair Gershom Bazerman. !!!
Fair enough, but the idea here is to give people time to get their head around Plutus and Haskell, while the other bits and pieces are being rolled out. 
To provide some context: the Plutus Platform is a development and execution environment using Haskell for contract development on the Cardano blockchain. The link takes you to the Plutus Playground, a Web-based environment to write and run Plutus code on a blockchain emulator for exploration and testing. Enjoy!
Are you going to address what I said, or are you going to continue repeating the same thing as if it were true? Calling someone who disagrees with you 12... really does not help your case. But you do you, I suppose.
This reddit submission is a buffet of Michael Snoyman's sock puppet accounts. There are probably a dozen more. I have identified the following: * duplode * madelinja * Tehnix * MidnightMoment * snoyberg * hastor * mnbvas * snoyjerk * ybx These accounts are all regularly used to discuss the same topics of Stackage, Haskell, Michael himself, Hackage, WAI, Rust, Amazon, C++, etc. More clearly, they exhibit strong stylometry results using [JGAAP](https://github.com/evllabs/JGAAP), which identifies for common words, phrases, beginning of sentences, punctuation, rare words and more. Spend 5 minutes to conduct this experiment on simple ngrams yourself: 1. Use the reddit API to download each comment history of the above accounts: https://www.reddit.com/user/snoyberg/comments/.json?limit=100 2. Download histories of other verified accounts that you know are real. 3. Write the body of the comments to separate files. You only need about 5,000 words but these files average 6,000 to 12,000. 4. Create authors in the JGAAP tool via the steps in [Running Your First Experiment](https://github.com/evllabs/JGAAP/blob/master/docs/JGAAP_User_Guide.pdf). Output for `hastor` produces this: hastor.txt hastor.txt Canonicizers: Normalize ASCII, Normalize Whitespace, Punctuation Separator EventDriver: Character NGrams n : 2 Analysis: Nearest Neighbor Driver with metric Cosine Distance 1. snoyberg - snoyberg.txt 0.00563363896190960 2. edwardkmett - edwardkmett.txt 0.01170601080902711 3. cdsmith - cdsmith.txt 0.01432472235099802 4. Tekmo - Tekmo.txt 0.01808146570085989 It's twice as likely that `hastor` is `snoyberg` than the other sample authors. Most of the others identified above display concordant results. Be careful to avoid authors that quote other authors in the sample text, which can skew the result.
I have addressed what you said [here](https://old.reddit.com/r/haskell/comments/a5ksbn/fp_completes_opinion/eboybun/).
Right sorry I missed the [link](https://testnet.iohkdev.io/plutus/tools/playground/)
No problem :)
nice, very detailed, thanks!
As far as I can see, the two following syntaxes are equivalent f : (1 x : Nat) -&gt; String f : Nat -o String 
I actually know multiple of the people on that list in person so I'm confident you're wrong. They just aren't super interested in commenting on technical things. Some people are here for the Haskell *community*, not the tech.
Hey! We have just deployed our runtime to all regions. Feel free to add this info to your guide /u/AlainODea :)
Done. I added a note to the top of my article explaining your update and recommending it only if people are curious about building the layer themselves.
The numbers speak for themselves. The similarity of writing style and coincidence that they all chime in at the same time [in the same thread](https://www.reddit.com/r/haskell/comments/a5ksbn/fp_completes_opinion/ebnlckm/), repeatedly, is evidence. So I'm confident you're wrong. Which accounts do you think are real? Do you have any evidence? You, for example, I can link back to a GitHub account referring to your reddit submissions. Out of 8 accounts, I am happy to be wrong about one or two. 
I shouldn't have to compromise the privacy and trust of my friends and colleagues to tell you that the burden of proof is on you. Your analysis only proves that these accounts *might* be owned by people with aligning interests. It's not unreasonable to think that there are several people in the world who don't care to spend their free time on technical things, but who do get quite riled up over social controversies. That's a reasonable class to expect multiple people to exist in.
Is it conflict of interest to upvote something I've worked on? 🤔 Ahh well.
&gt; This is not healthy for /r/haskell Says the doctor no one asked.
My usual solution to "it's taking a long time to get feedback/confirmation/approval." is "speak up." Very often people are just busy and they aren't ignoring you on purpose. They just need a friendly nudge. Part of being a blocking component of a process is understanding that people need your time to get unblocked and getting nudged is not a bother, but a gracious act.
&gt; Haskell is an elitist cult and most of the haskellers want to keep the cult going. Please be more respectful in your communication.
The whole point of the language server protocol is to hook into existing IDEs, and not have to re-invent the wheel.
&gt; the former allows defining linear return types I'm not even sure that's true. Since the annotations are relative to the use of the return value. An annotation on the return value would be a recursive (and not well-founded) definition.
I agree that this was very much on the edge between “is just a bug” and “should go through the GHC proposal process”. And in the context of this issue, a one month delay to get committee acceptance is unfortunate … but the process is tailored to language extension proposals, and a certain slow down here is, I think, a feature.
The containment axiom can equivalently be written ab = ab + a + b. That's almost proven directly in the post, but I did spend thirty seconds wondering if there was a potential generalisation (no), so might be worth writing down somewhere.
&gt; I say that as long as you cause fragmentation, without being really transparent as to why... I was trying to shed some light on the motivation for creating stack instead of improving cabal.
Quite right. Although in this case Neil did precisely this; he was always gracious in his nudging. Indeed we have been quite busy with various changes in infrastructure (e.g. changes in CI, a move to GitLab, the final push for Hadrian, cleaning up various bits of technical debt in the codebase) but hopefully things will quiet soon, we will begin seeing the dividends of our efforts, and we can return to the work of making GHC a better compiler.
(In fear of making it a routine gesture) Thanks for all your work on GHC! Infrastructure is not the most glamorous work, but it really makes a difference for developer happiness and hopefully this initiative will unlock some new contributors.
Code blocks reformatted for old reddit: module Foo ( Foo(Foo) , module Foo.Show ) where data Foo = Foo --- module Foo.Show () where import Foo instance Show Foo where ... --- module Foo ( Foo(Foo) , if Bar module Foo.Bar ) where data Foo = Foo --- module Bar ( Bar ) where class Bar a where ... --- module Foo.Bar ( ) where import Foo import Bar instance Bar Foo where ...
The type annotations given are also misleading. `forever` frees up a type parameter, but the annotations pin it back to something that already appears earlier in the type, making it seem as though the caller will actually get a value to use, when in fact: ``` -- Applicative f =&gt; Observable f a r -&gt; (a -&gt; f r) -&gt; f r follow :: Applicative m =&gt; ContT r m a1 -&gt; (a1 -&gt; m a2) -&gt; m r -- Applicative f =&gt; Observable f a r -&gt; (a -&gt; f r) -&gt; f r subscribe :: Applicative f =&gt; ContT a1 f a2 -&gt; (a2 -&gt; f a1) -&gt; f b -- Applicative f =&gt; Observable f a r -&gt; (a -&gt; f r) -&gt; f r watch :: Applicative f =&gt; ContT a1 f a2 -&gt; (a2 -&gt; f a3) -&gt; f b ``` `subscribe` and `watch` will never terminate normally (`forall b . f b`). `follow` and `watch` take continuations which never terminate `(forall x . a -&gt; m x)` and somehow use it to produce the result of the continuation (in case of `follow`), or to again never terminate (in case of `watch`). These more general types make me wonder how useful the definitions really are. This library seems to be about side-effecting-callback-style programming in Haskell, which is fine, but I don't think it gives anything substantial. It's just some synonyms for `Control.Monad.Trans.Cont`.
Right, I'd also like to chime in... I am increasingly growing fond of \`hadrian\`, for example. GitLab also looks pretty good so far.
Not an easy question to answer, being so open-ended. For coding in general, the ability to derive and create abstractions is probably the most critical skill to have, especially the abstraction of *composition*. Whether one arrives at it through formal education or personal intuition, "being an abstract thinker" with the skill of putting together and taking apart abstractions is a big part of it. Other great "soft" skills to have would be curiosity and patience, which work in tandem as you navigate the vast amount of learning materials in the FP world that is often written for people more advanced than you. Beyond the very basics of tutorials, your learning material consists largely of blog posts and papers, which makes for a steep and rocky learning curve with lots of setbacks along the way. It takes patience to sort them out and wait until things "click" when you can absorb the rest of the paper, and curiosity to explore the problem further and find the right people to ask questions. As professional skills go, I speak only as a developer in far less principled languages (I last paid my bills with Perl), but I can't understate how important communication skills are, both verbal and written. Not only does it help everyone else on the team, but explicitly stating problems in precise and clear terms will immensely help you with your own process too.
This was very enlightening. Thanks. In my head generating definitions was just a nice trick to automatically generate functions. But now I see that adding definitions amounts actually to extending the type system. It seems to me the real problem is that type definitions allows to get new constructors and consequently new eliminators (or new patterns if you prefer). Is there something else I am missing? 
Nope. This is not making it to the subreddit. 😊
Interestingly, this axiom defines the Cartesian product for the category of partial functions...
Could you spell that out further?
It's not clear what you tried from this description. What was in your `stack.yaml`? What command did you run? For example, on my machine, this works just fine: stack --resolver lts-12.21 build wreq Or if you want to play with that in the repl: stack --resolver lts-12.21 ghci --package wreq
And 7%-19% better in terms of performance per Euro.
Do you have some profiling data? I *think* what you need to do is make Cell more strict, and maybe even unbox your Vectors, but I prefer making performance related changes only have looking at the data.
I tried to edit my `stack.yaml` by adding `wreq` to the `extra-deps` field. Afterwards I ran `stack build` to build my package. However, when I do that it starts screaming that it is missing `aeson` as a package for example. What I was hoping for was that it was like Yarn or NPM (for the JavaScript stack) where I could just add a dependency and it handled all the underwater stuff. I looked into Nix, but I haven't fully figured out yet what it does.
It's really weird that compiling Stack didn't benefit from high core count CPUs. Stack has a lot of dependencies.. I guess there's too much interdependency between the dependencies! :D I wonder if parallelism could be improved via some sort of "header-like" (but automatic) mechanism, similar to how C/C++ builds are extremely parallel…
You are having two seprate problem: ADT, and type constructor. You can has ADT without having the later, or have type constructor without ADT. If you want pure ADT, then you could have \`\`\` newtype TCon = TC String \[TVar\] Type data Type = TVar TVar | TArr Type Type | TCon TCon \[Type\] \`\`\`
No, it shouldn't, because the only part that determines is an instance overlaps another is the instance head, not the context. `MonadFail (ExceptT e m)` can unify with `MonadFail (ExceptT String Identity)` when `e ~ String` and `m ~ Identity`. You might be able to use overlapping / overlappable pragmas in order to enable this instance, but I'm not sure.
In Haskell terms, the category whose arrows are `a -&gt; Maybe b` has as its Cartesian product the type `Either a (Either b (a, b))`, more popularly known as `These a b`. A proof of this, generalized over Set(oid)s can be found here: https://github.com/jwiegley/category-theory/blob/master/Instance/Sets/Part.v
Right, and this is really neat. But this isn't the above formula! the `ab` on the left and right sides of the containment axion are the same `ab` while in yours you really want `a ⊗ b` on the left I think?
Right you are! Thanks for the clarity, sclv. 
I am in a similar situation. I was especially encouraged by this talk by Ryan Trinkle: [Full-Stack Haskell, from Prototype to Production](https://www.youtube.com/watch?v=riJuXDIUMA0). Unfortunately my experience with [obelisk](https://github.com/obsidiansystems/obelisk) so far has been a little disappointing, but I'm sure with more active maintainers it could be great!
I don't know what your `stack.yaml` looked like in the first place, you still haven't provided it, but you shouldn't need to include `wreq` at all. `wreq` is included in the Stackage snapshots, and so will be available. You just need to add it as a dependency in your cabal file or package.yaml file. This document may help: https://docs.haskellstack.org/en/stable/stack_yaml_vs_cabal_package_file/
An important part of the GHC Haskell puzzle is cross module inlining and for that you need to have the compiled IR to be available... Maybe this can be improved upon though. For typechecking you should just need the signatures of modules you depend on.
Check out this series: (this is part 3) https://vadosware.io/post/rest-ish-services-in-haskell-part-3/ Magicbane is the Haskell version of Dropwizard: https://hackage.haskell.org/package/magicbane David Johnson's Intro to Servant is great and its what I started with: https://www.youtube.com/watch?v=gMDiKOuwLXw 
In my experience, you need to keep the cores that GHC used to a minimum; not greater than 4. [This blog post explains some old and some remaining problems with GHC parallelism](https://trofi.github.io/posts/193-scaling-ghc-make.html). The OP sets `-qn8` unilaterally, which might cause a ton of time to be wasted in parallel garbage collection (the parallel garbage collector needs to be used with great care in my experience). In the past, I've tested the time it takes Nix to build Stack from scratch, varying only the number of parallel builds allowed, and the `-j` argument provided to GHC. The `-j` argument almost never improved build times after `-j2`, but increasing the number of parallel builds made a massive difference for about the first half ondependencies, then only marginal difference for the second half. Unfortunately I don't have a writeup and a bunch of recorded data like the OP does.
Yeah, I'm not talking GHC parallelism, I didn't even know it's a thing :) I'm talking about the module-level parallelism only. I have an 8-core Ryzen, and indeed it's &gt; about the first half of dependencies well, on some of my projects, it felt like more than half actually.
There's some interesting [commentary](https://github.com/isovector/thinking-with-types#commentary) in the readme.
Inlining is the only thing GHC does resembling LTO, as far as I'm aware. Which is sad IMO. It's probably too late to change this now, but I wonder if you couldn't reduce total compile time dramatically by translating everything to unoptimized core, storing that, then optimizing at link time from top down before translating to STG. Currently, GHC has to be kind of pessimistic about how core will be optimized, doing a lot of unnecessary or redundant work just in case it's needed (or not doing work that would have been beneficial to the program because it can't know that it would be until later). It would increase the link time substantially, but I think you could cache nodes in the optimization graph to make sure only the first link is so long.
By GHC parallelism, I think I meant "module parallelism." Isn't that what `ghc --make -j` controls?
Yeah, ThinLTO is pretty good. GHC should be able to leverage it as-is (all it needs to do is produce object files with bitcode, and add linker flags). Is cross-module inlining happening in non-optimized builds as well?
&gt;*If I were to do this project again, knowing what I know now,* I would write the entire book as a series of Haskell modules. I'd use quasiquoters to write inline prose and build meaningful abstractions in a principled, well-understood language. In essence, I'd write a book DSL, and then write interpretations of that into my eventual desired formats. Is this kind of like what the Racket people try to do with Scribble and Pollen?
I wonder how the 8400 would perform. It’s a lot cheaper and lacks unnecessary features such as hyper threading, so maybe it would be a good choice. Also interesting that memory speed made so little difference. No reason to throw money at fast ram I guess.
Yes, you are right: \`ab = ab + a + b\`, \`ab = ab + a\` and \`ab = ab + b\` are all equivalent. I have updated the blog post to include this more symmetric version.
Thank you very much! :)
Is there any particular reason you're using Stack? It seems to me it would be much easier if you simply use `cabal` as the [Tutorial](http://www.serpentine.com/wreq/tutorial.html) suggests: &gt; To use the wreq package, simply use cabal, the standard Haskell package management command. The combination of [GhcUp](https://github.com/haskell/ghcup) with Cabal is much easier to use. You can find an introduction at https://haskell-at-work.com/episodes/2018-05-13-introduction-to-cabal.html if you need it.
This book is absolutely brilliant, and y'all should really buy it.
I was more hinting at borrowing the ThinLTO ideas and applying them to the Core IR of GHC. Like /u/ElvishJerricco describes.
This doesn't seem to be the case: λ&gt; a = SemiLatticeSet (Set.singleton (Set.singleton "a")) λ&gt; b = SemiLatticeSet (Set.singleton (Set.singleton "b")) λ&gt; a &lt;+&gt; b SemiLatticeSet (fromList [fromList ["a"],fromList ["b"]]) λ&gt; a &lt;.&gt; b SemiLatticeSet (fromList [fromList ["a"],fromList ["a","b"],fromList ["b"]]) λ&gt; e = SemiLatticeSet (Set.singleton mempty) λ&gt; a SemiLatticeSet (fromList [fromList ["a"]]) λ&gt; a &lt;.&gt; empty SemiLatticeSet (fromList [fromList ["a"]]) λ&gt; a &lt;.&gt; e SemiLatticeSet (fromList [fromList [],fromList ["a"]]) As you can see, `empty` does work as identity for `&lt;.&gt;`, but `e = SemiLatticeSet (Set.singleton mempty)` does not.
I doubt you'd get much benefit from ThinLTO with GHC's LLVM backend. Maybe I'm wrong, but I'm guessing GHC would need its own core-level LTO. And no, I don't believe inling ever occurs in unoptimized builds, except maybe(?) functions marked explicitly with `INLINE`
If it's in the same package, then there's no need. But most of the big problems (aside from `base`) are cross-package.
Yeah that is more or less it. They are creating brand new top level types / constructors. The key thing to think about is the aspect of generativity. Newtypes and data declarations are generative, which means that you cannot substitute equals for equals. Whatever construct you come up with must utilize that. So something like `foo = generateNewTypeOver Int` will not work. You need a `&lt;-` or some other non-equals-for-equals syntax.
Well, without code nobody can help you :D
Yeah I'm on mobile give me a sec I'll comment it 
So I was thinking that the package-level aspect would be handled in the cabal file, via conditional dependencies and conditional modules and what have you. The `.hs` files would solely deal with the module level aspect. 
type Point = (Float, Float) xs = \[\] arrange\_in\_pairs :: Point -&gt; Point -&gt; \[Point\] -&gt; \[(Point, Point)\] arrange\_in\_pairs (x, y) (xi, yi) points | length xs == 0 = xs ++ \[((x, y), (points !! 0))\] | (length xs - 1) /= length points = xs ++ \[((points !! (length xs - 1)), (points !! length xs))\] arrange\_in\_pairs (x, y) (xi, yi) points | otherwise = xs +
I realise this isn't probably clear. a Point is meant to be a co-ordinate like on a graph. the function is meant to group these co-ordinates together, so (x, y) is the starting point, (xi, yi) is the ending point and "points" is a list of points that are in between the start and end. the function is meant to group them together in tuples of size 2, so the starting point and first supporting point are in a tuple, then the first supporting point and the second supporting point, then the second and third etc.
I wonder how this would break down in US dollars. 8700k is $369.99 and 2700x is $289.99 and comes with a usable cooler on Amazon right now.
Still, even though the two products are different in your example, it's an interesting observation, thank you!
Currently reading the book. Got to it with the expectation of it being dense and a hard read but it's well written and very approachable! Thank you for writing this, I enjoy reading it very much!
Without more info it's hard to tell but there are some things that usually help significantly with numeric code: - flatten the vectors - use an unboxed vector For the first one create a new lookup function like type Vec = Unboxed.Vector Int read :: Vec -&gt; (Int, Int) -&gt; Maybe Int read v (x, y) = v !? (x * rowLen + y) If you can, make the indexing in-bounds by construction and use unsafeRead once you are confident in the code. 
Just from glancing at your code, there are a couple things that jump out: * You're using a lot of `foldr'`. Since you're using it on maps and vectors (rather than lists) this isn't actually any worse than `foldl'`, but I wonder why you're reaching for it so often. It probably *doesn't* do what you think it does. * You're duplicating functions that already exist (`Vec.foldr' (Vec.++) ` -&gt; `Vec.concat`.) Looking at it a little more, here are my suggestions: first, don't fill the vector with cells, fill it with poer levels! You're always computing the power level anyway, so storing the full cell information (especially the serial number, 
Thanks for writing this book. I'm currently making my way through it, and it's kicking my .... but I'm learning a LOT! Highly recommend. I'd read it over and over. It's fulfilling while making me realize how much I really don't know. I also love that you included the breadcrumb trail for people (like myself) who like to read papers on things you spoke about in the book, for those of us who like rabbit holes! :)
Just from glancing at your code, there are a couple things that jump out: * You're using a lot of `foldr'`. Since you're using it on maps and vectors (rather than lists) this isn't actually any worse than `foldl'`, but I wonder why you're reaching for it so often. It probably *doesn't* do what you think it does. * You're duplicating functions that already exist (`Vec.foldr' (Vec.++) ` -&gt; `Vec.concat`.) Looking at it a little more, here are my suggestions: first, don't fill the vector with cells, fill it with power levels! You're always computing the power level anyway, so storing the full cell information is redundant. The real big improvement you can make, though, is with memoization. In Haskell, you don't have to explicitly handle the caching (like you're doing here): because of laziness, values will be computed only when they're demanded, so if you just make a call into the cache *as you build it*, everything will work out. Here, for instance, we need to cache squares and sizes. That means we want a function `x -&gt; y -&gt; size -&gt; area` that recurses on the size, until it gets to 1, and the returning the power level for x and y at that point. Instead of figuring out the tricky cache-filling logic, you can just fill the array (or matrix) *assuming it's already filled* as you go: matrix :: Array (Int,Int,Int) Int matrix = array ((1, 1, 1), (300, 300, 300)) [ ((x, y, s), fn x y s) | x &lt;- [1 .. 300] , y &lt;- [1 .. 300] , s &lt;- [1 .. 300] ] where fn x y 1 = powerLevel x y fn x y s = let t = sum [ matrix ! (cx, y, 1) | cx &lt;- [x .. x + s - 1] ] l = sum [ matrix ! (x, cy, 1) | cy &lt;- [y + 1 .. y + s - 1] ] in t + l + matrix ! (x + 1, y + 1, s - 1) I'm using `Data.Array` instead of vector here because it handles multiple dimensions automatically. 
I don't see how that works. Suppose module `p1.C` defines a class `C` and package `p2.T` defines a type `T`. We want to specify a single correct location for `instance C T`. If we put the pragma in `p1.C`, and only specify `T.T`, which `T` is that supposed to refer to? We surely have a very specific one in mind, but there could be many packages with modules called `T`, and we're referring to one that's likely not even in scope. That sounds like a mess. The same problem occurs if we only specify a module name (and not a package name) for the instance location. One nagging concern: how can versioning work? If we update the class or type (or, most especially, if we change the way we interpret them), then we need to be sure that users update the target package. But that requires version bounds on *reverse* dependencies, which I suspect may cause the solver to fail even if there's a way to make things work. But perhaps that's an acceptable price to pay.
I bought the book, it is a great read for intermediate Haskell programmers who know the basics and want to move on. Very inspirational, thank you!
How does it compare to Simplicity?
Is the full index avaiable?
er, by module I meant to say package
hm. Is that because of GHC emitting continuation passing style code?
GHC doesn't emit continuation passing style code.
A big part of Hindley-Milner type inference is unification. I'm sure you're familiar with this process: when the two type expressions are the same node, unify their subexpressions, and when one of the expressions is a variable, set it to the other expression after performing the occurs check. For example, unifying `Int -&gt; a` and `b -&gt; Int` would entail first seeing that both types are `TArr`s, then unifying both `a` and `b` with `Int`. We can extend this design to handle type application expressions. Example: `Maybe a` and `Maybe Int` would entail first visiting the `TTypApply` nodes of the two types, then visiting the type constructor and argument subexpressions of the nodes. At those steps, `Maybe` unifies with `Maybe` and `a` unifies with `Int`. If you want to support higher-kinded types, you'll also need to associate type variables with kinds and write a function that computes the kind of a type expression. 
Right, those 7%-19% numbers are for full system costs. For the upgrade-only path, which assumes that your old system uses DDR3, you would need to buy: CPU+Motherboard+Memory+Cooler for Intel. I actually forgot the CPU cooler for Intel in the upgrade path, and I've adjusted the calculations accordingly. So AMD is always the cheaper option, where Intel is 33% more expensive for the upgrade path, and 9% more expensive for the full system path. For the compile benchmarks, AMD is actually 2%-7% better in terms of performance per Euro, and 13%-20% better for the test suite benchmarks.
I made a difference for the Ryzen 7 2700X, which has a higher core count, and so perhaps a higher memory pressure. But yeah, as you've noticed, it didn't seem to make a difference for the i7-7700K, which has only 4 cores. I can't make any claims for the i7-8700K though, as we haven't actually benchmarked it with slower RAM.
Hi, thank you for taking the time to reply. Dropwizard is definitely interesting, however it is not really the technical side of web dev in Haskell I am interested in but rather but the business side. For example convincing someone to let me build them a web app in Python/JS is much easier because they know someone else can come in for relatively cheap to work on the project after me. But with Haskell that might be an issue for them so other negotiations, like a long maintenance period etc. might be wanted.
Thanks for the insights! I'll play with the \`-qnX\` flag some more to see whether e.g. setting it to \`-qn1\` improves things. Perhaps I'm misunderstanding the user guide, but the way I understood it is that GC is already parallel equal to the number of threads you set with \`-jN\`, but with \`qnX\` you set an upper bound for the GC threads. i.e. \`--make -j2 -qn8\` should still only use 2 GC threads, is my understanding wrong? &amp;#x200B; I'm happy to apply more optimisations to the benchmark script that favour CPUs with higher core counts, especially as the number of cores increases every CPU generation; and as we say in the blog post, the Ryzen 7 2700X is actually the CPU that we have in our new PC, so anything that can make it go faster would be appreciated.
Your book looks *really* nice! Did you consider releasing a print edition? I'd totally buy that!
Hm, the user guide doesn't actually say what happens when your `-qn` is greater than your `-j`. I guess it probably just uses the lower number, like you say. But still, I don't often find it a good thing to have high `-qn`, even when `-j` is high. So when your `-j` is 8, I think your choice is likely going to cause problems.
&gt; passphrase to identify yourself as belonging to an in-group at worst One should probably know the word "shibboleth" when talking about shibboleths ;) 
Massiv would also be fun to play with :) I'm unsure as to it's dynamic stencil(?) support, however
&gt; I doubt you'd get much benefit from ThinLTO with GHC's LLVM backend. Maybe I'm wrong, but I'm guessing GHC would need its own core-level LTO. LTO generally seems unnecessary for a high-level language like Haskell; I have always viewed it as a crutch for C's lack of anything resembling a module system, precluding any sensible optimisation across compilation units at compile time. GHC, on the other hand, can pull up the interface file for any module it wants on demand.
I think you could do a lot with core-level LTO. The biggest thing would be better specialization. But also I think inlining could be a lot smarter with whole-world knowledge of what's used and where. Inlining right now isn't as good as I want it to be (though certainly leagues better than almost any other language)
&gt; shibboleth Dang, I *always* miss my chance to use this one!
is it this? https://github.com/ghc-proposals/ghc-proposals/pull/180
The second law is intended to be fully polymorphic. Given three instances of the same grate family: grate01 :: ((s0 -&gt; a0) -&gt; a1) -&gt; s1 grate12 :: ((s1 -&gt; a1) -&gt; a2) -&gt; s2 grate02 :: ((s0 -&gt; a0) -&gt; a2) -&gt; s2 and given any f :: (((s0 -&gt; a0) -&gt; a1) -&gt; a1) -&gt; a2 we require that grater12 (\k -&gt; f (k . grater01)) === grater02 (\k -&gt; f ($ k)) (I think I got those types right).
For the record, the hbc compiler is not written in C, it’s written in LML. And the LML compiler is written in LML. So it’s another island. 
&gt; We surely have a very specific one in mind, but there could be many packages with modules called T I mean that applies pretty much equally to regular-old imports. You generally have a pretty specific package in mind, and occasionally another package comes along that reimplements a heavily overlapping or equivalent interface. So just like normal you want to make sure that any module you reference only exists in one of the packages mentioned in your cabal file. &gt; but there could be many packages with modules called T, and we're referring to one that's likely not even in scope There shouldn't be "many packages [in your cabal file] with [exposed] modules called T", as that causes problems in regular Haskell without this new feature. &gt; But that requires version bounds on reverse dependencies Yes there would be version bounds on reverse dependencies: if bar &gt;=0 &amp;&amp; &lt;1 then foo-bar &gt;=0 &amp;&amp; &lt;1 if bar &gt;=0 &amp;&amp; &lt;1 then Foo.Bar &gt; But that requires version bounds on reverse dependencies, which I suspect may cause the solver to fail even if there's a way to make things work. Yeah the solver will have to be thought out a little, whether effort should be made to actually hit the above `if` statement or not, as technically `bar-2` would be perfectly compatible, you just don't get `foo-bar`, but people may not want to be ever given `bar-2` due to the instance stuff.
I've never had issues with Intero. If my code has errors, it tends to be able to display them piecemeal through integrating with flycheck, and then you can pop up a handy buffer to see a list of all current errors and warnings. What's intellisense, I've never used it and it sounds like your errors stem from that?
Thanks. I'm trying to absorb and write code with the mantra that `Text` is for human-readable characters, `ByteString` is for wire formats, and `String` is an accident of history best forgotten. So it seemed odd that a parsing library would not work on `Text`. But I suppose the aim of the library was byte sequence parsing (only).
I think you've got something backwards. The whole point is to guarantee that *no matter what packages are in use*, there will be *at most one* instance of the class for the given type. Suppose I depend on package `c` defining class `C.C` and package `t` defining type `T.T`, and suppose one (or both) of those specifies that the `C T` instance is in module `I`. Suppose also that packages `i1` and `i2` each contain a module called `I` containing an instance of `C T`. If I depend on `i1` *or* `i2`, then everything is fine. But if I depend on *both* of them, everything breaks. This is *exactly the same problem* that orphan instances cause today. If, however, the pragma specifies the instance is in `i1.I`, then `i2` is simply *broken*; it can't be compiled at all, so its author is forced to fix it by removing the prohibited instance. working + working = working, which is a nice law to have. I don't understand your hypothetical version bound language, so I can't comment on that.
So there is a key aspect of how cabal files work that I am relying on to avoid specifying a package, and that is the following: If you specify a module in a Haskell package, only the packages in that package’s cabal file will be looked at to find that module. So in your example case you would have to have made the silly decision to specify both `i1` and `i2` **directly** in the cabal file to have both `i1.I' and `i2.I` as feasible choices. Even in that situation GHC would complain about an ambiguity error and would refuse to compile. One thing to keep in mind would be that it should be an error for the `I` referenced in `C` to be in a different package than the `I` referenced in `T`, if both decide to mention `I`. My version bound cabal file basically states that “if version X of package Y is being used (by a package that depends on me), also add package/module Z to my dependencies” This is probably the most complex part of the setup as it involves something not normally seen, which is bringing more libraries into scope based on what libraries a dependent package uses. However I do not know a good way around that, as fundamentally we want any usage of T and C to fairly automatically bring in third party separate thing I, but with near-zero overhead when only one of the two is used. 
[removed]
Is there a standard mutable vector that has a fast push operation? For the recent AOC problem, I ended up rolling my own because pushing individual elements by calling `grow` followed by `write` gave me absymal performance, even when compared to Python, so I'm guessing it does what it is told literally, instead of multiply capacity by a constant strategy (because it doesn't track capacity).
Oh how i wish brick be ported on Windows :(
Ah, thanks. Da war der Wunsch Vater des Gedanken. I fixed that section. 
Yes, every `grow` call indeed creates a new copy. [source code](https://www.stackage.org/haddock/lts-12.22/vector-0.12.0.2/src/Data.Vector.Generic.Mutable.Base.html) I don't know whether there is a library or not (I couldn't find), but rolling my own version based on MVector was not a difficult task. &lt;SpoilerAlert&gt; [This is my version](https://github.com/viercc/acyclic-dfa/blob/ed35619023dce95cb17eda8017ade3f036b6fbc9/src/Data/Vector/Growable.hs). &lt;/SpoilerAlert&gt;
I'm going through following code sample and found it hard to figure out how to use (-&gt;) and (Star f) once they implemented 'first' and became a member of Cartisian. Could anyone provide some easy to understand examples for those? Thanks. -- Intuitively a profunctor is cartesian if it can pass around additional -- context in the form of a pair. class Profunctor p =&gt; Cartesian p where first :: p a b -&gt; p (a, c) (b, c) first = dimap swapP swapP . second second :: p a b -&gt; p (c, a) (c, b) second = dimap swapP swapP . first instance Cartesian (-&gt;) where first :: (a -&gt; b) -&gt; (a, c) -&gt; (b, c) first f = f `cross` id instance Functor f =&gt; Cartesian (Star f) where first :: Star f a b -&gt; Star f (a, c) (b, c) first (Star f) = Star $ (\(fx, y) -&gt; (, y) &lt;$&gt; fx) . (f `cross` id) 
I don't think you're being balanced **example:** do other communities/languages say "hey look man I'm using a monoid with an identity and associated binary operator!" when all they are doing is adding two flipping numbers? I see a similar behavior in science papers, when authors will use incredibly obtuse language and formalisation to make themselves sound more "academic", but that doesn't actually tell you any more if you used simple everyday language. The Haskell community sometimes remind me of this quote from Friends: &gt; Joey: &gt; Oh, "They're warm, nice people with big hearts." &gt; Chandler: &gt; And that became, "They're humid, prepossessing Homo sapiens with full-sized aortic pumps." Now don't twist my words, sometimes using mathematical terms is necessary, I just don't think its all the time (because if you want to play that game, anything could be expressed in a complex manor)
FYI, are you aware of the on-going work on https://github.com/haskell-CI/haskell-ci ? That auto-generates travis.yaml magic from your "tested-with" entries in the cabal file and supports all sorts of nice things.
&gt; GHC, on the other hand, can pull up the interface file for any module it wants on demand. I'm confused, what do you mean with that? The header files are available during compilation of C code too. Are you referring to the fact that GHC chooses to put _some_ unoptimized function definitions into interface files? Isn't this like putting the function definition into the header file ("header only")? What is the difference between me putting `INLINABLE` on every definition in every package I'm using and a whole program optimizer?
“There are no linux generic builds provided, and instead we recommend use of the ghcup tool (https://github.com/haskell/ghcup/) in combination with the stack install script. We feel this gives a smoother and better experience than the existing install process, being less invasive (not requiring root.)” I hadn't heard of ghcup before. Less invasive! More flexible! 🎉 Woo! 🎉
You should have two files: `stack.yaml` and `package.yaml`. Your `stack.yaml` should look like this: `resolver: lts-12.20` and nothing else. In your `package.yaml` you should add `- wreq` under `dependencies:`. This should just work, if you are having trouble please ask.
C cannot inline the compiled code of one .c file into another .c file. Haskell can do that, because the Haskell interface files (`.hi`) contain the core (semi-compiled Haskell) code. Thus C can only inline header files, while GHC can inline any code into other code.
I am sorry but I am not following. What do you mean by *you cannot substitute equals for equals*?
Unfortunately `base`, and Haskell in general, is totally bugged when it comes to silent integer overflow in places where it really shouldn't be. See https://github.com/haskell/bytestring/issues/144 for another example. This kind of stuff needs to be eradicated in my opinion, we need to be much stricter when it comes to non-type-level correctness issues because these screw you over hard in production. See http://haskell.1045720.n5.nabble.com/Deprecating-fromIntegral-tt5864578.html#a5864638 for one of many bugs with `fromIntegral`(which is fundamentally flawed and we should do it the way like Rust does it).
From earlier readings, it looked like Plutus would be quite different from Haskell - but looking at the code it looks identical. What am I missing?
Rather than an instance for each arity, you could use one instance for functions and one for nonfunctions: instance {-# OVERLAPPABLE #-} NFData b =&gt; GenNF b where genNF = error "TODO" instance (ArbNF a, GenNF b) =&gt; GenNF (a -&gt; b) where genNF fs = genNF [f x | f &lt;- fs] where x = getXFromSomewhere --- Other than random testing, functions can also be tested by enumeration. In particular, if the function is lazy, we can also drive the enumeration by looking at the bits of the input the function needs. See [lazysmallcheck](https://hackage.haskell.org/package/lazysmallcheck) and [lazy-search](https://hackage.haskell.org/package/lazy-search). Do you have plans to also try those approaches for benchmarking?
Another option could be this: use a modern GHC to compile one of the old simple GHC's to Core. Then implement a simple Core interpreter (say in C). Then run that old GHC using the interpreter, bootstrapping your way back up to modern GHC. (fun exercise: check that you get byte-for-byte the same output as the modern GHC you started with.) You might say that the Core is still "generated files", but at least it's somewhat legible generated code (and for the purposes of bootstrapping, could be cleaned up to be more legible still).
The problem is not "roll your own", that's fine. It is that 1. The rich API of Vector is no longer available for use with your new vector, so you need to go through defining proper ports of every function. 2. If you implement it as a simple "flat" pair, there is going to be an additional pointer hop for accessing the underlying array. This feels unnecessary and slightly off-putting given the fact that vector is designed for performance, but building on top of the package in the simplest way possible gives you an API which is _not_ as fast as it could be.
&gt; Da war der Wunsch Vater des Gedanken. Explained as: &gt; Sagt man, wenn jemand eine bestimmte Sache so sehen will, wie er sie sich vorstellt, und nicht, wie diese tatsächlich in der Realität ist. which means &gt; One says this, when someone wants to regard a certain matter in the way he imagines it to be, and not how it is in reality.
Thanks for your reply! To construct benchmarks using Criterion, I use something similar to: nf :: NFData b =&gt; (a -&gt; b) -&gt; a -&gt; Benchmarkable This means that I can't *fully* apply each function `f`. However, I could slightly modify your solution: instance (ArbNF a, NFData b) =&gt; GenNF (a -&gt; b) where genNF fs = error "to-do" instance {-# OVERLAPPING #-} (ArbNF a, GenNF (b -&gt; c)) =&gt; GenNF (a -&gt; b -&gt; c) where genNF fs = genNF [f x | f &lt;- fs] where x = getXFromSomewhere What do you think to this? &amp;#x200B; My main reason for choosing random testing over enumeration was overall test time: I experimented with smallcheck and found that trying all possible inputs up to a certain depth took quite a while. This is something I want to come back to, though. I haven't seen lazy-search before, so I'll take a look. Thankyou! &amp;#x200B; Best, Martin
But might some ancient version not be dependent on a bug in the compiler? I don't know GHC, but I presume it is full of impure and unsafe stuff. The tiniest modification to evaluation order might produce segfaults.
Hi! I’ve worked since ~2002 doing “web dev” in various forms (from JS/PHP to flash, to mobile, etc). I think my career is similar to what you’re describing: usually just me consulting for clients. Lately we have grown a little and I have 2 employees. I’ve also been involved as principal engineer or CTO in 3 startups, and I’m about to start another. Lately we use Haskell on everything, but not for the front end. While I think it’s a great idea, the tools for front end Haskell development aren’t mature enough to be very effective choices. It’s possible, sure, but you have to make serious compromises. We use Elm for frontend. While it lacks many features I miss from Haskell, the designers are so focused on the real problems of web dev, it’s a joy to work with. In regards to getting clients, the key is to sell the entire solution: You aren’t just providing programming services, you’re solving a business problem for them. Then you can choose Haskell because it’s your secret weapon, one of the reasons you can ship faster and better than everyone else. However, it takes time to grow your reputation, skills, and network where clients will let you do this. I still usually have to have a difficult conversation with a client to convince them Haskell is ok (they’re often worried about hiring). Also, It’s much easier to find consulting work as an “iOS contractor” than to find whole big jobs where you can choose the tech. Go for big projects. If you focus on small web apps and compete with rails guys you will lose. Haskell shines on projects that take 3 months or more. Let me know if you have any questions!
Thanks for the analysis. I updated my code and will update my post with more data.
You are not missing anything. We decided that the best course of action would be to repurpose Haskell to use as a contract language, instead of inventing yet another functional programming language (for really no good reason).
Ah that makes sense, thanks!
Thanks! I updated the post with new data and pushed some changes to the code so it's now using `Array` for the 2D matrix and unboxed vectors.
Uploaded profiling data and new code!
Oh yes, that looks like a good workaround!
Hm, sorry, I misunderstood. And you are right. The example I gave is exactly what you said, a simple pair `STRef s (Int, MVector s a)`. It got hit by both of your problems. And I totally didn't think about the performance cost! I'm not sure, but I think it's worth to try `{-# UNPACK #-}` pragma. 
I learned last year from Guillaume Boisseau that the very natural and obvious versions of the laws of [Van Laaroven's isomorphism lenses](https://twanvl.nl/blog/haskell/isomorphism-lenses) are, in fact, incorrect. Taking lenses as an example: Lens s a := Exists r. (s -&gt; (r * a)) * ((r * a) -&gt; s) Given a pair of function `(&amp;alpha;, &amp;beta;) : Lens s a` we naively expect that the laws should be 1. `&amp;beta; . &amp;alpha; = id` 1. `&amp;alpha; . &amp;beta; = id` The first law is fine, both sides having type `s -&gt; s`. The second law is problematic. Our first sign of trouble is that it allegedly has type `(r * a) -&gt; (r * a)`, but this is a type error because, as GHC would put it, "type variable ‘r’ would escape its scope". Moreover, like universally quantified types, existentially quantified types are also subject to parametricty. For existentially quantified types, this parametricity relation manifests itself as an equivalence relation between members of the type. In our particular case, parametricity require that for every function `f :: r1 -&gt; r2`, `&amp;alpha; : s -&gt; (r1 * a)` and `&amp;beta; : (r2 * a) -&gt; s` that the pairs `(first f . &amp;alpha, &amp;beta)` and `(&amp;alpha, &amp;beta . first f)` of type `Lens s a` are equal. My understanding is that imposing the second law as written would discriminate between equal values of an existential type. Anyhow, I haven't really got this all figured out. If you can, you should ask Guillaume Boisseau about this issue, and try to get a copy of his thesis.
If that counts, then why do this on an _old_ GHC? Compile the current GHC to core, and use that to build your stage 0 GHC compiler.
Yeah I realized using UNPACK will get you similar performance (hence removed part of my original comment), but the API wrapping is annoying so I was just wondering if somebody had already done it in an easy to use form :P.
Apologies for the off-topic response then. I think /u/embwbam's reply is what you're looking for :) - https://www.reddit.com/r/haskell/comments/a5kmce/haskell_webdev_experience/ebrk2ge/
I love these posts about and around Smos. Thanks for sharing!
[removed]
&gt; we recommend use of the ghcup tool (https://github.com/haskell/ghcup/) in combination with the stack install script This sounds like very bad piece of advice as Stack installs the correct GHC for you. There's no benefit combining Stack with this redundant ghcup tool.
I honestly think blockly is a very bad fit for visualizing functional languages. It's all about data flow. An arrow based interface nodes and dataflows is probably a lot more intuitive to a non-programmer. See also BPML and Petrinets
Yes, I tend to agree. We are not sure about blockly either and are keen to get feedback. So, thanks!
That would indeed be a bit better than using GHC to compile GHC to C (which is what you typically do when you want to bootstrap GHC on a new architecture). But I the Core is not source code (as in, “preferred form of modification”), so it would not qualify for the purposes of https://bootstrappable.org/
[removed]
If one is using ghcup or the Platform, odds are they aren't using Stack as their main Haskell environment. Nonetheless, they might still want to use Stack to build some of their projects, or Stack-oriented projects from other people. In such a scenario, that advice is entirely sensible.
If this language is called Marlowe, then the other one shouldn’t be called Plutus, it should be called Peytone.
Intriguing that nobody has tried with GCC 2.95 like the author of the linked post mentions. I have fond memories because I remember Doom for Linux was compiled with that version initially.
The advice seems odd, but put it in context with what the Haskell Platform core installer typically provides: ghc, cabal-install, and stack. ghcup can install ghc and cabal-install for you, so the stack-install script is the final piece to the puzzle of "getting you everything the Haskell Platform core installer gave you." Of course there are workflows for using stack to install all 3, so yes, if we're being honest, ghcup is mainly for the convenience of non-stack users. It looks quite sensible; perhaps stack should reimplement its ghc version management to leverage ghcup and have one less thing to maintain.
And a comparable interface to similar tools for other languages. This is nice.
&gt; The header files are available during compilation of C code too. Are you referring to the fact that GHC chooses to put some unoptimized function definitions into interface files? The difference is that under the C model the user is required to make a conscious decision of which functions they want inlined and include these in the header. If the user decided not to include a usefully-inlineable function in their header file then it cannot be inlined except by way of LTO. In GHC this is up to the compiler and configurable by the user with a compile-time flag. You could fairly easily build a whole-program optimizing compiler on top of GHC in this way. [Clash](https://clash-lang.org) does just this, in fact.
Sadly true. Unfortunately, as you conclude in your post, I suspect it's probably the best we could do. My first thought while reading the post was: &gt; GHC is big, but so much of it is due to typechecking. Perhaps by assuming your program is well-typed you could do away with much of this complexity. For the purposes of bootstrapping just assume that the program is well-typed. Oh, but what about instance resolution? [waving hands furiously] Surely there must be some simple, brute-force instance resolution scheme one could use. This may work with Haskell 98, but it's not possible to avoid non-trivial typechecking decisions affecting program behavior once you introduce, e.g., type families. Once you admit even a few type system extensions it seems you quickly buy yourself much of the current typechecking complexity.
When does the “next generation” of the cardano platform launch? Are there any initiatives to compile plutus to the EVM?
[yes, here; you need to scroll down](https://leanpub.com/thinking-with-types/)
Okay, this seemed to work. I had the `resolver` set to `8.6.2`, is there a reason why it does work with `lts-12.20` and not with `8.6.2`?
Because an old GHC is likely much smaller. Core is hard to read, so minimizing the amount that you have to inspect could make it easier to audit.
Nice post. The approach described there is essentially the same as the one used by the built-in `brick` editor, which supports single- and multi-line editing and uses the `text-zipper` library.
In the comments of the article, a commentator suggests using Hugs, ending with following quote: &gt; WRT bootstrapping from scratch. I recently came across (sorry I can't remember where) somebody bootstrapping GHC, starting from a very old version and using that to build later releases. How had they started? With Hugs of course: it's the only viable Haskell for bootstrapping.
That's why (in retrospective) I'm very careful to define existential types used in proofs so their equivalence is equality/identity. We can define `l :: Lens s a` and `l' :: Lens s a` which would have different existential type, but otherwise equivalent (their `view` and `set` are the same). For example `Lens (a, b, c) c`, the existential can be `(a, b)` or `(b, a)` or ... **but** I'm not comparing different lenses. More concretely, it's not arbitrary `r`. Given PreLens s a := Exists r. (s -&gt; (r * a)) * ((r * a) -&gt; s) the laws are: fst (snd l) . snd (snd l) = id :: s -&gt; s snd (snd l) . fst (snd l) = id :: (fst l, a) -&gt; (fst l, a) or in GHC-speak, the laws should be checked inside the `case l of`. --- I haven't written down the laws for `Setter`, but if you check the mechanization (https://github.com/phadej/lens-laws/blob/176bb0aa0555a5423e78a6c64880a702e3643515/theories/Setter.v, it's a little messy, sorry for that) then you see that existential functor I use is based on W A B X := (X -&gt; B) -&gt; A I first tried with more (obvious?) choice Q A B X := A * (B -&gt; X) but then you indeed end up into the situation where `(over f a, id)` and `(a, f)` (`over : (B -&gt; B) -&gt; A -&gt; A`) should be equal. **TL:DR** requiring equality makes proofs easier, though I need to use refinement types, or different representations. You can consider refinement types cheating, as I use proof irrelevance. But these refinement proofs are all `eq_refl`, that's why I'd like to encode proofs in cubical theory, then I won't need to rely on any axiom! (refinements would be truncated to be in `Prop` as they are in Coq. but proof irrelevance would be a theorem too, as well as funExt). --- BTW it's fun to see the extracted code from these proofs: __ :: any __ = Prelude.error "Logical or arity value used" type Sig a = a -- singleton inductive, whose constructor was exist data SigT a p = ExistT a p data SigT2 a p q = ExistT2 a p q type IsFunctor f = () -&gt; () -&gt; (Any -&gt; Any) -&gt; f -&gt; f -- singleton inductive, whose constructor was MkFunctor data Iso a b = MkIso (a -&gt; b) (b -&gt; a) type Setter a b = SigT2 () (IsFunctor Any) (Iso a Any) setter_complete :: ((a2 -&gt; a2) -&gt; a1 -&gt; a1) -&gt; Setter a1 a2 setter_complete over = ExistT2 __ (let {fmap = \xy wx yb -&gt; wx (\x -&gt; yb (xy x))} in \_ _ xy -&gt; unsafeCoerce fmap xy) (MkIso (\a -&gt; unsafeCoerce (\bb -&gt; over (\b -&gt; bb b) a)) (\w -&gt; unsafeCoerce w (\b -&gt; b))) setter_over :: (Setter a1 a2) -&gt; (a2 -&gt; a2) -&gt; a1 -&gt; a1 setter_over setter bb a = case setter of { ExistT2 _ fF iso -&gt; case iso of { MkIso f g -&gt; g (unsafeCoerce fF __ __ bb (f a))}} __ :: any __ = Prelude.error "Logical or arity value used" type Sig a = a -- singleton inductive, whose constructor was exist data SigT a p = ExistT a p data SigT2 a p q = ExistT2 a p q data Prod a b = Pair a b type IsFunctor f = () -&gt; () -&gt; (Any -&gt; Any) -&gt; f -&gt; f -- singleton inductive, whose constructor was MkFunctor data Iso a b = MkIso (a -&gt; b) (b -&gt; a) type Setter a b = SigT2 () (IsFunctor Any) (Iso a Any) setter_complete :: ((a2 -&gt; a2) -&gt; a1 -&gt; a1) -&gt; Setter a1 a2 setter_complete over = ExistT2 __ (let {fmap = \xy wx yb -&gt; wx (\x -&gt; yb (xy x))} in \_ _ xy -&gt; unsafeCoerce fmap xy) (MkIso (\a -&gt; unsafeCoerce (\bb -&gt; over (\b -&gt; bb b) a)) (\w -&gt; unsafeCoerce w (\b -&gt; b))) setter_over :: (Setter a1 a2) -&gt; (a2 -&gt; a2) -&gt; a1 -&gt; a1 setter_over setter bb a = case setter of { ExistT2 _ fF iso -&gt; case iso of { MkIso f g -&gt; g (unsafeCoerce fF __ __ bb (f a))}} type Grate a b = SigT () (Iso a (Any -&gt; b)) grate_completeP :: (((a1 -&gt; a2) -&gt; a2) -&gt; a1) -&gt; Grate a1 a2 grate_completeP grate = ExistT __ (let {fWD = \a x -&gt; x a} in MkIso (unsafeCoerce fWD) (unsafeCoerce grate)) grate_grate :: (Grate a1 a2) -&gt; ((a1 -&gt; a2) -&gt; a2) -&gt; a1 grate_grate grate f = case grate of { ExistT _ i -&gt; case i of { MkIso axb xba -&gt; xba (\x -&gt; f (\a -&gt; axb a x))}} type Lens a b = SigT () (Iso a (Prod Any b)) lens_complete :: (a1 -&gt; a2) -&gt; (a1 -&gt; a2 -&gt; a1) -&gt; Lens a1 a2 lens_complete view set = ExistT __ (let {fWD = \a -&gt; Pair (set a) (view a)} in let { bWD = \p -&gt; case p of { Pair s b -&gt; s b}} in MkIso (unsafeCoerce fWD) (unsafeCoerce bWD))
&gt;Type any chance of that happening? I'd love a print version!
@jtdaugherty. The main reason that I wouldn't use \`text-zipper\` \*for smos\* is because a text zipper in that library: 1. Cannot (by construction) be generated by \`GenUnchecked\`, which is important for \*testing\*. 2. Is not granular enough when it comes to failing to perform a manipulation. For example: \`moveDown\` is not subdivided into the different things it tries. That being said. I certainly used this library as inspiration, so thanks! And Thanks again for Brick! 
Oh Great! Thanks! This is actually the first positive response I've read so it's nice to know that someone cares.
I honestly think that my law is easier to understand (and test). The restriction of index `(s -&gt; a)`, could in polymorphic `Grate s t a b` case include the requirement that, every index value `s -&gt; a` used by `grater` can/should be "naturally" convertible into `t -&gt; b`. I intuitively understand that having a restriction on `S -&gt; A` type used in my law, is somehow the same as "wrapping" both sides with additional `grater`; but it's not clear to me. *Similarly* `set a . set b = set a` could be stated as `f . set b = f` for any `f`, such that `f = set a` for some `a`; that's a roundabout way for `Lens`, directly stating is better. But for `Grate` "the roundabout" way, is IMHO simpler to understand. And use in mechanization.
That's good to know. For what it's worth, I'd be open to discussing changes to `text-zipper` to make it suitable for your purposes if at some point you decide not to maintain an alternative implementation. But I mentioned that stuff here just to call attention to the fact that the functionality already exists in `brick`, in case people get the impression that your post seems to suggest this is missing there. I think it would be helpful to call that out in your post and provide some insight into why you are using a different approach.
I agree you are looking at the right place. Percentage allocs there is bad. I think it's the `sum` call (in a tuple) being too lazy. Try swapping it out for a strict sum, and forcing it outside the tuple.
I think your example is a strawman. `Num`, where addition is declared, doesn't mention monoids, associativity, or identities. Perhaps it is you that is not being balanced.
Ah, it sounds like I lost the plot of this thread, sorry about that. My reply represents my feelings about the usual critique of things like American affirmative action and similar mechanisms, based on my own experience (and maybe isn't relevant to the point you were making). I think there are good arguments on the other side (some I've heard e.g. women on this board make).
Who's talking about sibboleths, I know a lot about sibboleths, but I'd love to hear more. ;)
I'm trying to make a .dll file that I could call from Excel (VBA). After some looking I found this and for a moment I felt great relief (since I've never managed to make this work neither with Rust nor Nim, only C with cl compiler): [https://downloads.haskell.org/\~ghc/7.6.1/docs/html/users\_guide/win32-dlls.html](https://downloads.haskell.org/~ghc/7.6.1/docs/html/users_guide/win32-dlls.html) &amp;#x200B; The steps I took: 1) Created Adder.hs: [https://pastebin.com/qCpyLWpm](https://pastebin.com/qCpyLWpm) 2) Created StartEnd.c: [https://pastebin.com/hzUaKsg1](https://pastebin.com/hzUaKsg1) 3) Compiled with following commands: \`\`\` ghc -c Adder.hs ghc -c StartEnd.c ghc -shared -o Adder.dll Adder.o Adder\_stub.h StartEnd.o \`\`\` The first two commands went through fine, but the third one gave me this error: Then I got this error: [https://pastebin.com/EdUTLd26](https://pastebin.com/EdUTLd26) Ok, I managed to fix this by copying HsFFI.h into build directory manually. It gave several similar errors and I did the same thing. Finally I got this: [https://pastebin.com/wUs1J90a](https://pastebin.com/wUs1J90a) It seems that gcc is having some severe linking problems. I'm running Windows 10, fresh install of Haskell Platform 10 minutes ago (first install). Running commands from command prompt. &amp;#x200B; I promise to write step by step guide for other people after I get this working (I saw someone else struggling with the same problem in stackoverflow for example).
So let’s say we had a theoretical function called `newtype` that could in a child type and returned a newtype wrapping it. Using dependent types and such to make this work as nice as possible. This function cannot be a regular function as substituting equals for equals gives a different result: ``` foo = newtype Int bar :: (foo, foo) ``` In the above just one newtype is generated and both elements of the tuple need to have it. We now substitute the left hand side of `foo` for the right hand side. ``` bar :: (newtype Int, newtype Int) ``` In this example bar contains two different types that are both newtypes over Int Thus `newtype` is not a pure function. Currently this is handled by just making `newtype` a special top level piece of built in syntax. Another way would be keying it on some sort of Monad that you can run at the top level (essentially what TH does). 
"Print on demand" services (I think that's the term) let you avoid paying a big up-front cost for selling a print version. My mom uses something called Lightning Source that works through Amazon; I think she gets around 50% of the $20 each buyer pays for a book.
What's the reason that it uses "variant of GRIN" instead of upstream GRIN?
I guess they don't affect every language, but silent overflow (and underflow) of fixed-size signed (or unsigned) values seems to be a problem that is much more widely spread than just Haskell.
I can guess that John Meacham (the author of JHC) wanted to experiment with various things. e.g. - mutable variable at GRIN level - region based memory management And these experimental features were added as language extensions to GRIN. My opinion is that JHC's GRIN is overcomplicated. I prefer the original GRIN language design.
When I started learning Haskell (2005) there was not the slightest mention to category theory in the conversations. Only one link pointed in the Haskell official pages and it learning about that was discouraged. It was like so for many years until the last years.
No GC? It says so in the about page, not sure if that's still accurate.
What are the tradeoffs of having a compiler heavily split up like this? It's definitely easier to understand, but is there a cost in compile speed / something else? I don't know much about GHC, but I know that each step is heavily, heavily entangled with the last. Is that a historical accident, or a concession to unavoidable complexity when making an industrial compiler?
This is more on the advanced side, typically you won't need to dive this deep as libraries have been built *around* the data structure (e.g. Data.Sequence); but I think finger trees are pretty awesome :) https://chrispenner.ca/posts/intro-to-finger-trees
Let me see if I can recrate Boisseau's argument here. Consider the trivial lens Lens a a := Exists r. (a -&gt; (r * a)) * ((r * a) -&gt; a) Let `α0 := \x -&gt; ((), x) :: a -&gt; (unit * a)` and let `β0 := snd :: (unit * a) -&gt; a`. Then the value (with explict type witneses) `(unit, (α0, β0)) :: Lens a a` satisfies your two laws. Now consider `α1 := \x -&gt; (0, x) :: a -&gt; (Integer * a)` and let `β1 := snd :: (Integer * a) -&gt; a` and let `f := const () :: Integer -&gt; unit`. We see that `(unit, (α0, β0))` = (unit, (first f . α1, β0))` by equational reasoning. By existential parametricity we require that `(unit, (first f . α1, β0)) = (Integer, (α1, β0 . first f))`. Lastly `(Integer, (first f . α1, β0)) = (Integer, (α1, β1))` by equational reasoning. Our conclusion is that `(unit, (α0, β0))` should equal `(Integer, (α1, β1))`. Observe that 1. `(unit, (α0, β0))` satisfies your two laws. 1. `(Integer, (α1, β1))` doesn't satisfy your two laws. 1. The two values are supposed to be equal. 1. And this is the most important bit, if you try to use `(Integer, (α1, β1))` as a lens, everything works out fine! So this is why I agree with Boisseau that the laws you give are incorrect. Now, those laws may not be very incorrect, in the sense that within the equivalence relation given by existential parametricity there may always be a representative that satisfies your/Van Laarhoven's two laws, especially given a rich enough type theory. I find that to be very believable, though I have no proof. However, I still think those two laws are wrong by virtue of the fact that there exist values of lens type that 1. don't satisfy the laws, 1. are equal to ones that do satisfy the laws, 1. work perfectly fine as lenses.
JHC code base is much smaller than GHC one. Having separated compiler parts might be useful for random language experiments. I think that LLVM had proved that component based compiler design is useful. Clang, Rust, Swift all use LLVM as a library and many IDE and static analyzer tool use Clang as a parser.
`(Integer, (α1, β1))` doesn't, but `( { i: Integer | i == 0 } , (α1, β1))` does. And all definitions you used are still valid. If I try to use `( { i: Integer | i == 0 } , (α1, β1))` as a lens, it will still work out fine. And from abstract interface there is no way to create not `i == 0` residual. Your third point, *The two values are supposed to be equal.*, there should be an inverse of `f := const () :: Integer -&gt; Unit`, and its existence: `(const 0)` highlights that. Or if we are pedantic, shouldn't *all* `Unit -&gt; Integer` the equality should work out. It isn't so, the third point (*the two values are supposed to be equal*) doesn't hold. Equality/Equivalence are symmetric relations. --- Let's settle on, *there may always be a representative that satisfies your/Van Laarhoven's two laws*. I think I showed how to construct them. --- &gt; It would be nice to know what the real laws for isomorphism lenses should be. I haven't worked it out. Hand-wavy, If you know that `({ r : R | P r } * A) &lt;-&gt; S` is a bijection, for some `P`, then you can set `P r := exists s, r = fst (bwd s)`; can't you? In the example above we can reason that it should be `P i := i = 0`.
What I've found most useful in identifying which data structure to use is to analyze the big O performance I desire for certain operations and compare that to something simpler, which most often would be a list. Maybe you want to have fast insertions at the end of your list. In that case, `Seq` would be better since it is O(1) instead of O(n). Maybe you want to know if an element is in your list quickly. In that case, `Set` would be better since it is O(log n) instead of O(n). I've also found that doing [Advent of Code](https://adventofcode.com) has helped me get a better familiarity for performant data structures in Haskell, since I learn best by doing. Your code won't run in a reasonable amount of time if you don't identify the bottlenecks of your program and use the right data structures or algorithms.
Cool! I'd love to know more. If you prefer not to share your results here, then perhaps we could at least have a chat? Please drop me an email: andrey.mokhov@ncl.ac.uk.
For data structure libraries you're going to likely want [containers](https://hackage.haskell.org/package/containers) and/or [unordered-containers](https://hackage.haskell.org/package/unordered-containers), a beginner friendly introduction can be found at [https://haskell-containers.readthedocs.io](https://haskell-containers.readthedocs.io). You should also check out [https://guide.aelve.com/haskell](https://guide.aelve.com/haskell), its still a work in progress (contributions encouraged!) but has some pretty good resources so far for comparing libraries. For example, here's [the page on "strings"](https://guide.aelve.com/haskell/strings-o62hqc69). &amp;#x200B; \&gt; what situations really call for Seq over list The [Seq section of the containers walkthrough](https://haskell-containers.readthedocs.io/en/latest/sequence.html) has at the top what the Seq type gives you, specifically "a finite number of sequential elements, providing fast access to both ends of the sequence as well as efficient concatenation."
It's happening. I'm currently waiting to approve a proof copy. Hopefully they'll be available by Christmas!
h-hot
For haskell development environment, I recommend VS code with the following extensions: \- Haskero \- haskell-linter \- stylish-haskell For the whole setup, it may need a blog however, you can refer to the haskero installation for quick-start. 
Thanks, will give it a try tomorrow.
The direction and priorities of Haskell.org have caused a bit of friction in the past, for precisely the reason and experience you're sharing. An alternative, [haskell-lang.org](https://www.haskell-lang.org) was created, but people were really mad about this being a "community splitting" effort, and so work on it has slowed down. You might find that website to be more helpful. For getting an editor setup, I recommend ghcid, [which I wrote a blog post on](https://www.parsonsmatt.org/2018/05/19/ghcid_for_the_win.html). I know it's 2018, but no one is getting paid to write Haskell IDEs yet.
I am in the similar situation like yours. I dabbled a little bit with Haskell a few months ago, and I gave up because of lack of understanding in cabal and stack. Though Alexis King’s post https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/, seems quite helpful, but I still have some issue with building and installing Haskell apps on Windows with a slow internet connection. Now I need to really understand and have some time to understand the whole packaging system. It is mostly fine with a good internet connection, but people like me with a slow internet have to rely on offline installing. I tried with Haskell platform, but someone (don’t remember who) recommend against using HP because it will mess up settings and environment variables. 
I enjoyed using Haskell for Mac, because it let me play around without figuring out installers for all kind of stuff. At the time `brew install stack` wasn't working for me. Happy hacking
Text is when you want Unicode code points. It stores them, but it doesn't do normalization; I can't recall how it deals with ophaned surrogates / unmatched surrogate pairs. If you want to store human text outside of the ASCII range, you probably want this. ByteString stores bytes. Is has a few convenience features when you want to use an ASCII character in the source code instead of the (numeric) byte value. Other than that, it doesn't have anything to do with human text, it's just a sequence of 8-bit values. Both file and network I/O is done in bytes. In addition, if you are interacting with a C/C++ library, the `char` type in C is 8-bits on both MS Windows and Linux, so ByteString might be used for `char *` or `char []` interop with C; if the C function treats the bytes are some form of text, ByteString doesn't care either way. To convert Text to ByteString and vice-versa, you have to choose/know an encoding, and if converting from Text it should cover all of Unicode. The most common full-Unicode encoding is UTF-8, but Punycode is used for hostnames. --- For data structures in general I recommend reading the parts of Okasaki's Purely Functional Data Structures that are relevant to your task and available on line. In addition the `Edison` libraries on Hackage have a lot of details about the run-time complexity of various operations across multiple implementations of the same interface. That said, it's often best to delay the choice of data structure until you hit the first performance hurdle. Just use the simplest / most available one until you see the hurdle, then profile and make an informed decision then. You may also be able to write the majority of the code against an abstract interface (usually a type class in Haskell), and only make the decision in/near `main`. It's not essential, but it can make changing your mind based on future profiling data easier, but it can come at a performance cost of it's own.
This blog post has helped me a lot. https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/ It goes over much of what you're asking,a while also touching on some of the language level aspects
That's correct. In Haskell the language generally itself at least forbids it and us silly programmers destroy it by overly-liberal use of overly-liberal functions like `fromIntegral` everywhere. Indeed in many mainstream languages one doesn't even even have to `fromIntegral` to shoot oneself into the foot, one can even write `int a = b` where `b`'s range doesn't fit in `a`.
https://ro-che.info/ccc/26
Oh, nice! Thanks for the info :D
No, we need a "Thompsone" language for encoding the blockchain in financial contracts about which Simon Marlow blogs.
1. Don't need to keep switching to the commandline. Use `stack build --fast --file-watch` (or `ghcid` as ephrion suggests). 2. Yeah the IDE story is not good. If you look at the recent Haskell survey results, most people use editors like Emacs, Vim or VS Code. IME, Intero for Emacs works well (if it goofs up, usually an update/reinstall fixes things), I've heard people having a good experience with Dante for Emacs as well. 3. For your question on why the Haskell site doesn't have links to good resources, I don't know... the thing is, it is hard to provide an unbiased "getting started guide" (e.g. which build tool?), and granting one an "official blessing" without burdening the user with build system complexity is going to be tricky... It is not just "pick your IDE and let's go". Stack has its own docs. Cabal has its own docs.
Tip: you can process your profiles with [profiterole](http://hackage.haskell.org/package/profiterole) to get a much more readable report.
Most people develop in haskell by using `stack build` / `cabal build` in a terminal + their editor of choice in a different window. Some people use little scripts so they don't have to press ALT-TAB, etc. I'm sure that's disappointing to hear, but I think you'll find it's much less frustrating to just start using this simple workflow and get on to the interesting work of learning the language. It might be worth backing up and starting over again here, from the `stack` section: https://www.haskell.org/downloads 
&gt; In Haskell the language generally itself at least forbids it Well, we do have `(+) :: Word8 -&gt; Word8 -&gt; Word8` (e.g.), and it's implemented to wrap instead of (e.g.) error on overflow. I wonder, would *anyone* **actually** want a language where `(+) :: Word 8 -&gt; Word 8 -&gt; Word 9`?
screen / tmux might be better than Ctrl+Z'ing between bash and vi, particularly if you upgrade to zsh and vim. That's honestly how I prefer to develop *Java*.
Holy how did I never know about haskell-lang, what a great initiative!
Is this just a split of the old code base, or has JHC been updated recently? Would really love to be able to use Haskell to get back on my old [Haskell kernel](https://github.com/tathougies/hos). Despite my various attempts over the years, GHC is just too complicated to port to bare metal (for a hobby anyway). JHC is so much easier to use. Glad to see there's some interest in non-GHC compilers.
This is the [video](https://www.youtube.com/watch?v=sRonIB8ZStw) that got me actually working with a flow. It's a couple of years old now, but I think it probably still holds up. A few points: - It's `stack` based - It's Linux based - It's emacs based. - It's 2 hours long, and is a little slow at points, but it really does walk you through. The emacs bit probably isn't as important, but emacs + intero + stack works **REALLY** well. In editor type checking, completion, and documentation lookup, and that's just for starters. Chris has got a lot of other video's on that channel which are him coding on various projects, just showing how he works. 
Do you have issues with installing Haskell? If you're on Linux, I think using Stack is the way to go. I'm not sure how the experience would be for Windows/OSX. If have trouble learning Haskell, there are now lots of good resources to learn Haskell. There are few good books to learn Haskell now compared to the past few years. Anyway, I totally understand the frustration and I am more than happy to help if you need it.
&gt; Your third point, The two values are supposed to be equal., there should be an inverse of f := const () :: Integer -&gt; Unit, and its existence: (const 0) highlights that. They are supposed to be equal, but they are not in your construction because you are erroneously using a (strong) Sigma type instead of an existential type. The existential type could be constructed as a quotent of the Sigma type you are using, or you can define an equivalence relation on your Sigma type and consider operations that respect the equivalence relation. 
The problem the op is hung up on is really “I don’t like the guides because they aren’t for an ide.” Emacs modes don’t suit the op’s criteria. So this doesn’t address the problem. 
The problem is that the standard non-experimental way to get started necessitates being comfortable with the command line. No guide to a common workflow will tell you otherwise though some may recommend emacs or vim modes. If you want more ide support than that you’re in experimental territory, full stop. Thus, you won’t find any tutorial that tells you the standard ide story, sorry. That said, you don’t need to ctrl-z. Just use two terminals, and ideally one with a ghci session! Haskell has a great repl, and comfortability with it is the key to a productive experience.
I also recommend vscode + haskell-ide-engine
Great! Thankyou
If you're interested there is the applied FP course (https://github.com/qfpl/applied-fp-course) that is a series of exercises in developing a haskell application. With configuration, database integration, and json handling. Developed by the Queensland Functional Programming Lab, we're happy to help where we can. :) 
...from the same crew that uses their authority to spread FUD about Stack such as [this](https://github.com/data61/fp-course/commit/2747b55dc232f3270953803859bb749b700f3cff) or [that](https://github.com/tonymorris/do-not-use-stack/issues/1). 
The haskell-lang.org site does look better - haskell.org seems more focussed on language advocacy.
I have read that blog before, but I don't think it's aimed at complete beginners. It assumes you already know what stack and cabal are, how to createa project from scratch etc.
&gt; the thing is, it is hard to provide an unbiased "getting started guide" (e.g. which build tool?) I understand but why does Haskell suffer from this, when other languages don't? With Java, you create a standard project structure and use Maven to build the project. If you want to use an IDE you can point it at the Maven build file and it just works. With Scala you have a similar story with SBT. I don't need three different ways to build a project, I just need one. 99% of the time I don't care how the build process works, as long as it works, I just want to write code.
Unfortunately it's a pattern you can find in other ecosystems as well. 1. A identifies flaws in popular part of the ecosystem controlled by B. 2. A comes up with a plan to fix the problem. 3. A fails to convince B to allow A to fix the problem. 4. A decides to invest in implementing working proof of concept in the hopes to convince B. 5. B accuses A of splitting the community and having an ulterior political agenda. 6. A gets demotivated and gives up cooperating with B. A starts building their own ecosystem to workaround B's obstructionism. Go to 5. until B eventually goes extinct. 
&gt; I understand but why does Haskell suffer from this, when other languages don't? [I tried explaining in a previous comment](https://www.reddit.com/r/haskell/comments/9mm05d/2018_haskell_survey_results/e7ful2s/)
&gt; I also recommend vscode + haskell-ide-engine Generally curious, why has this comment been downvoted?
What about Ant? Java has multiple package management systems too. So, the way things have gone is roughly that there was once just the Cabal library, which is a Haskell library for describing packages, in terms of which people would write a Setup.hs program that would be able to do various package-management-related things then -- configuring and building the package, running tests, building documentation, etc. Since the library was easy enough to use that everyone's Setup.hs was pretty much always the same single line of code, we quickly moved away from having custom Setup.hs files (though they're still supported), and instead, a tool called cabal-install was written to parse the package description format of packages directly, with additional features like obtaining dependencies automatically from a package repository called Hackage. For over a decade, that was the main tool everyone used to install packages. There were some problems that one would run into using cabal-install from time to time, especially when ghc only supported a single installation of any given version number of a given package -- same package-version combinations built against different dependencies couldn't be distinguished, and you could end up with broken packages if you installed things in an incompatible way between projects. Some users frustrated with this situation decided to make their own tool called Stack, which had better support for sandboxing sets of packages and managing versions based on hashes in a Nix-style. Stack still uses the Cabal library under the hood, so all the same packages work with it, it's just another tool like cabal-install for managing their installation. In addition, Stack took on managing multiple versions of GHC as well. In the meantime, cabal-install and the underlying Cabal library also kept progressing, and now have expanded to include the sandboxing and hash-based version management features that originally motivated Stack. So we have a bit of a divide as to which tools people prefer now. It's been a bit unfortunately political since the outset, and that hasn't really stopped being the case. (I could get into describing that, but it's a little outside the scope of this post.) To some extent, competition is good, and it certainly invigorated the development of these tools. Some of the surrounding effects haven't been so great though. It's also worth mentioning that there's actually yet another popular mechanism for building Haskell software, which is to use Nix (and again, Nix makes use of Cabal under the hood when building Haskell software, but it's far more general of a build tool / package manager). While you certainly can use it for small projects, Nix is more heavily geared to larger scale software development, where you want to pin down the versions of all the dependencies used by your team, all the way down to external C libraries. One of the nice things Nix gives you is a way to drop into a shell with any particular software installed that you want. Both Cabal-install and Stack have some integration with Nix in this regard, and can optionally use it to obtain a shell before doing their normal build procedures. Where I work, we use Nix quite a lot, and while any time I need to write .nix scripts, it drives me a bit crazy, the effect of using it in the end is quite nice. For example, it's nice that what's deployed to production is the same exact binary that I'm running on my machine while testing, which is the same as on any other developer's machine. Also, we have caching servers set up with it to reduce the workload of recompiling all those dependencies across all our projects. It also makes deployment easier, in that you can describe an entire machine configuration with Nix, and then all you need to do is spin up an AWS machine with Nix installed and nix-copy-closure your whole machine config to it (a process which is automatable by a shell script with just a few lines).
"The Peytone" sounds like a great name for a fuzz guitar pedal
&gt; What about Ant? Java has multiple package management systems too. Sure, but in practice the tools don't compete. For the vast majority of cases Maven will do what you want and will work out of the box. If, however your use case falls into the 1% where you don't want Maven, for example where you want complete control over the buold process then use Ant. &gt; So, the way things have gone is roughly that [...] Thanks for the recap. I was vaguely aware of the Cabal vs Stack situation, but, having just spent the last hour reading some of the discussions, I hadn't realised how toxic it had become.
When I tried it a couple of months ago the setup was awful.
let us know when it's available! It's a sure buy for me!
I asked for a source. Likely it is https://elephly.net/posts/2017-01-09-bootstrapping-haskell-part-1.html, where he used hugs to build a (non-working) version of nhc98. I have a setup where hugs starts loading all of GHC band base … but I am also not very optimistic that that will go far.
&gt; Some users frustrated with this situation decided to make their own tool called Stack, which had better support for sandboxing sets of packages and managing versions based on hashes in a Nix-style (not only the code you're building is hashed, but the hashes of the dependencies are included as well, so that if anything changes, you have a distinguishable version). I believe you're mixing things up here. Stack was merely pushing cabal's sandbox concept (i.e. *not* hash-based Nix-style system), while Cabal had already started work on moving away from sandboxing towards the Nix-style based paradigm. Moreover, in hindsight and contrary to common belief, Stack's distracting emergence did certainly *not* benefit the development of the Hackage/Cabal ecosystem, but I don't want to get into this here. Instead I'd just like to give you an (incomplete) timeline to put important events into perspective and show that work on Cabal to address the started *long* time before fpcomplete decided to push their competing ecosystem tooling. - Sep 2012: [HIW2012: Making cabal-install non-destructive](https://wiki.haskell.org/HaskellImplementorsWorkshop/2012/Schuster) - Aug 2013: Cabal 1.18 [released](http://coldwa.st/e/blog/2013-08-21-Cabal-1-18.html), featuring sandboxing support which represent an intermediate stopgap solution on the way towards a proper Nix-style approach - Jun 2014: Very first Hackage meta-data revision [performed](https://hackage.haskell.org/package/HTTP-4000.2.4/revisions/) - Sep 2014: Blogpost [How we might abolish Cabal Hell, part 1](http://www.well-typed.com/blog/2014/09/how-we-might-abolish-cabal-hell-part-1/) - Dec 2014: Work on first generation prototype of http://matrix.hackage.haskell.org starts ([archived historic first-gen version](https://ghc.haskell.org/~hvr/buildreports/0INDEX.html)) - Jan 2015: Blogpost [How we might abolish Cabal Hell, part 2](https://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/) - Apr 2015: Vishal's GSOC 2015 project "Implement nix-like package management features in cabal" is accepted for GSOC 2015 - May 2015: Work starts and during email discussions even SPJ chimes in to emphasize the importance of this project: &gt; **I see this as mission-critical to relieving Cabal hell.** (Am I alone?) So it's great that Vishal is going to work on it. Go Vishal! &gt; &gt; Can I take it, then, that the Cabal developers will be sufficiently involved that, assuming there are no show-stopping problems, they'll accept the patch? &gt; &gt; It is tantalising to me that something so critical has been so long delayed. It'd be fantastic if it was done this summer. &gt; &gt; Simon - June 2015: Out of the blue, Stack is [publicly announced for the first time](https://www.fpcomplete.com/blog/2015/06/announcing-first-public-beta-stack) (ironically after having been developed behind closed doors for "around a year" at fpcomplete) - July 2015: As short-term measure in the interest of [Improving the "Get Haskell Experience"](https://mail.haskell.org/pipermail/ghc-devs/2015-July/009379.html), a more minimal HP which avoids cluttering the global package db is decided and **Michael Snoyman and Mark Lentczner jointly announce the plan to include Stack in the HP distribution and that the "Haskell Platform becomes the standard way to get *GHC* and related tools"** - Aug 2015: Blogpost [Help us beta test “no-reinstall Cabal”](http://blog.ezyang.com/2015/08/help-us-beta-test-no-reinstall-cabal/) - work was underway testing and polishing `cabal new-build` enough for inclusion in the next upcoming major version of cabal, i.e. cabal 1.24; ultimately it was decided to include it as a tech-preview rather than make it default, as it turned out to be more ambitious than initially antipicated - Sep 2015: Stack [adopts the non-Nix-style legacy sandbox-based sharing technique](https://www.fpcomplete.com/blog/2015/09/stack-more-binary-package-sharing) originally used by http://matrix.hackage.hackage.org/ (while planning was already underway to redesign and move matrix-CI over to the new more efficient Nix-style Cabal store framework) - May 2016: GHC 8.0.1 gets released (one of the most-delayed GHC releases); and comes with Cabal 1.24 which ships with a [tech-preview of the new "Nix-style local builds" feature](http://blog.ezyang.com/2016/05/announcing-cabal-new-build-nix-style-local-builds/) - Sep 2017: Blogpost [Haskell Summer of Code 2017: Last Mile for `cabal new-build`: first and last status update](http://fgaz.me/posts/2017-09-13-hsoc-cabal-new-build-status-update-1/) - Aug 2018: Blogpost [Haskell GSOC 2018: What I did on my summer vacation](https://typedr.at/posts/what-i-did-on-my-summer-vacation/) - Sep 2018: Cabal 2.4 released to Hackage, the last major release before Nix-style local-builds (i.e. `cabal new-build`) becomes the default UI of cabal This isn't necessarily an exhaustive timeline; it's primarily for illustrating that there's been a lot of work going on at improving the Hackage/Cabal ecosystem, and there's still a lot on the roadmap to help reach the full potential of Hackage/Cabal's constraint-solver centric paradigm! But given our work is not commercially funded and we rely almost exclusively on volunteer contributors (&amp; the occasional summer of code), we generally operate on a slower timescale... 
Not sure if recommended yet, vacation labs has a series Haskell with out the theory that might help. Also noticed that the book for it is showing up as 404 on the vacation labs site. A shame if true, because vacation labs offers Haskell internships and other positions Haskell needs to succeed.
This is a split of the latest JHC release (0.8.2). It's the old code base with some aesthetic modifications, i.e. tabs are removed from the source code. IMO JHC has very smart ideas implemented in poor and unmaintainable source code. I did the split to allow to do some compiler experiments in the future. JHC handles type classes at the type level while GHC implements them as dictionaries. It would be interesting to compare the two in whole program optimization setting. I work on a [GRIN](https://github.com/grin-tech/grin) implementation and an accompanying [GHC/GRIN](https://github.com/grin-tech/ghc-grin) frontend. GRIN is a whole program optimizer and code generator using LLVM. Generally I'm interested in all Haskell compiler/code optimizer related projects. i.e. GHC, JHC, [Intel Haskell Research Compiler](https://github.com/IntelLabs/flrc) and [MLton](http://www.mlton.org/). 
If it's the language that you're struggling with, and you know Scala already, a route that others are using is to first become familiar with Scalaz via https://leanpub.com/fpmortals and then the final chapter (on Haskell) will reveal all much like Daniel Larusso realised that we wasn't just learning how to clean a car. Ironically, the Scala community looks at many features on Haskell.org with envy! In particular, there is a strong desire for a https://hoogle.haskell.org/ With regards to downloading... perhaps you just missed the "Downloads" button on the home page? The simple fact of the matter is you don't need an IDE to write Haskell, just open up any text editor and get started. Use the one you're most familiar with.
&gt; I hadn't realised how toxic it had become This is the saddest fact about Haskell. I tune in every few months hoping that peace has broken out, and so far I am always disappointed.
It still is (you have to build it yourself), however it works really well.
The easiest possible starting position in my experience is 1) [Install stack](https://docs.haskellstack.org/en/stable/README/#how-to-install) 2) Use any editor that has syntax highlighting for Haskell (I use Sublime) 3) Have a command line open where you run `stack build --fast --file-watch` (or alternatively use [ghcid](https://www.parsonsmatt.org/2018/05/19/ghcid_for_the_win.html) which can be configured to use stack) 4) [Make a new project](https://docs.haskellstack.org/en/stable/README/#quick-start-guide) This seems to be about as easy as the [Scala](https://docs.scala-lang.org/getting-started-sbt-track/getting-started-with-scala-and-sbt-on-the-command-line.html) equivalent.
&gt; when other languages don't Have you ever tried setting up a JS development environment? The story is gradually improving I think, but at one point there were not only multiple package managers, but multiple sub-languages, multiple competing module definition standards, and basically the whole thing was an unholy mess that makes Haskell look clean and straightforward by comparison. Similarly, I've just setup a C++ development environment (in Windows) for the first time. It took me a couple of days and I still don't have any idea how dependency management is usually handled. I wanted someone else to be able to compile my code, but he decided (as someone who had developed in C and C++ before) that he just simply couldn't be bothered. On the other hand, I got someone who had never heard of Haskell before to the position where they could compile and run a Haskell project in an hour or so, and half of that time was spent by him not being very competent at the command line (cding into the wrong directories etc). 
No it is not, indeed
wtf? Why did this get two downvotes?
Your link to Makefile on github https://github.com/input-output-hk/cardano-sl/blob/develop/wallet-new/Makefile#L is dead
Chris Okasaki’s book [Purely Functional Data Structures](https://www.cambridge.org/gb/academic/subjects/computer-science/programming-languages-and-applied-logic/purely-functional-data-structures?format=PB&amp;isbn=9780521663502) is a great resource, but might be on the advanced side.
great diversion, your response simply doesn't address the original point: that is using far too much formalisation when it's not strictly not needed when starting out perhaps. Instead of reflecting on what others are observing and reporting as being problematic or an obstacle, you decide to go on the defensive. What is your ultimate goal? do you want to see higher adoption of Haskell? if yes perhaps start by listening to the feedback from newcomers? which reminds me of something that I dislike about Haskell in general: &gt; "Any intelligent fool can make things bigger and more complex... It takes a touch of genius - and a lot of courage to move in the opposite direction." Haskell is that intelligent fool sadly.
&gt; Have you ever tried setting up a JS development environment? Yes, and yes, it was dreadful, but that's a fairly low bar. &gt; Similarly, I've just setup a C++ development environment (in Windows) for the first time. It took me a couple of days [...] Not sure why, Visual Studio is very intuitive and once you've installed it, it takes minutes to create a sample project and run it.
Stack's GHC management works on some platforms that ghcup doesn't; notably, Windows. So stack can't simply delegate GHC management/installation to ghcup, though it'd probably be useful for them to work together.
Yep, the setup is the worst part. Well, to be fair, it's easy to do, you just clone the repo and run `make build-all` but you wait a lot and it uses up a lot of disk space. After you set it up, it works great though. It's way faster and way less fragile than Haskero. Also, the last time I tried, as far as I remember, Haskero didn't have Brittany integration and HIE has it, so moving your cursor over an undefined symbol error, pressing `ctrl+.` and choosing "add import" is not a dream anymore.
Egads, you took jhc And cabalized it! Awesome!
Will you please knock it off? Whether or not you intend to, you're absolutely turning into a single purpose account (https://www.reddit.com/r/haskell/comments/9nk5co/cabalversusstack_single_purpose_accounts/).
Will definitely start using it once when setup process is improved. Right now I'm using ghcid and vscode which is pretty painless, but it doesn't provide any IDE features obviously.
Shameless self promotion since it might help you: https://www.ahri.net/practical-haskell-programs-from-scratch/ - I'd be happy to hear any problems you have and fix it up to make it work for everyone! Feel free to pm or reply here.
You are right Haskell Platform 8.6.3 is now up. Unfortunately the documentation doesn't say much about where it gets installed and whether it might conflict with an existing (vanilla from GHCHQ) binary install. Is the main advantage that it comes with a slightly more extensive set of common libraries than the binary GHC release?
&gt; but I'd really like to know when to use Text vs ByteString, and what the differences are, or what situations really call for Seq over list. IMHO it's unlikely to be the right recommendation for this particular person.
Interesting idea. I'd be very much in favor of factoring Stack's GHC management out into ghcup, and having stack just automatically delegate to it. Having the same consistent behavior of GHC installation for stack users and cabal-install users alike would be great.
Using auto generated code doesn't generally count as bootstrapping unless the generator was bootstrapped as well
Interesting. Found [this](https://github.com/haskell-implementations/hbc/tree/master/src/lmlc). Is that just a frontend? Can it not be used to run lml code?
I'm not that familiar with the Java ecosystem, but I was giving a reason for why there isn't a "blessed way" -- it is because of the (strong) differing opinions in the community. E.g. if you pick one and start going through its docs, then you'll probably end up successful so long as you read the docs carefully. Earlier I used to have troubles with cabal but with the new versions, those have gone away. Similarly stack also works great out of the box, at least on Linux (I use Linux). W.r.t tooling, since tooling is mostly developed on a volunteer basis with little commercial support, it is best-effort and not best-in-class. Some tools like hlint and hoogle are unique and not available for other languages but other IDE-like tooling is not there. Sorry, I don't have a better answer for you than that.
For anyone with deep understanding of low level Haskell insides would be awesome to get an opinion of: [https://stackoverflow.com/questions/53770035/is-it-safe-to-interleave-manual-realworld-state-passing-with-an-arbitrary-monad](https://stackoverflow.com/questions/53770035/is-it-safe-to-interleave-manual-realworld-state-passing-with-an-arbitrary-monad)
I've [posted previously](https://old.reddit.com/r/haskell/comments/a2pseu/how_do_i_get_started_using_haskell_with/ebdmlrd/) on the subject of how to get started with the basics. If you'd like help getting yourself from 0 to working on a project, I'd be happy to link you to a Discord chat that will provide build-tool agnostic help to you and walk you through the process! Myself and many others are happy to walk you through the steps. It is difficult to get started, but very worth it.
Sorry for the confusion, I was talking about `fromIntegral`, so conversions of one integer type into another in my last post, not about operations inside the same type.
Seriously... dude... you know we're all friends despite our differences, and that the people not involved are the ones that think there's a war going on, right? Tony and Michael literally were just at the same conference hanging out just this week. Chill.
The Haskell Platform is my preferred method to install GHC on Windows. Thanks for all the effort! I have a problem in Windows 10 when installing `wai-app-static`: cabal v2-install wai-app-static All the dependencies are installed correctly, but the compilation of `wai-app-static` itself hangs, without any error message. I have installed other packages having executables (like `ghcid`) without problems.
&gt; using far too much formalisation when it's not strictly needed when starting out perhaps I don't agree, and have seen no evidence in this thread. &gt; What is your ultimate goal? Better programs. I believe that requires both better programming languages *and* better programmers. 
As a long time IntelliJ user I second VS Code with the haskell-ide-engine. It’s a surprisingly good IDE imho. Give it a spin.
It’s not too hard to write a non-performant implementation. But it’s plenty of work. 
[repl.it](https://repl.it/languages/haskell) is by far the easiest way to get started on Haskell. (It does not support hackage modules though)
Ah, thanks for the more detailed timeline! I didn't want to suggest that work on Cabal-install had stopped in any way, just tried to provide some kind of motivation for why Stack began existing. I do actually agree that it would probably have been more fruitful for interested developers to have contributed to a single effort here. It's unfortunate that's not always the way it goes.
Tried following the Stack instructions: MacBook-Pro:my-project jon$ stack setup Preparing to install GHC to an isolated location. This will not interfere with any system-level installation. Downloaded ghc-8.4.4. Configuring GHC ... Received ExitFailure 77 when running Raw command: /Users/jon/.stack/programs/x86_64-osx/ghc-8.4.4.temp/ghc-8.4.4/configure --prefix=/Users/jon/.stack/programs/x86_64-osx/ghc-8.4.4/ Run from: /Users/jon/.stack/programs/x86_64-osx/ghc-8.4.4.temp/ghc-8.4.4/ "ExitFailure 77" doesn't sound promising...
&gt; I don't agree, and have seen no evidence in this thread. Your disagreement is irrelevant, you are talking about the small picture I'm talking about the big picture: Haskell and category theory are synonymous, and when a programming language is decorated from a branch of abstract mathematics something is clearly not right. But carrying on the defensive and down-votes I'm sure that's truly helping with selling the "positive" image of the Haskell community (sarcasm). &gt; Better programs. I believe that requires both better programming languages and better programmers. strongly disagree a great author can write a better masterpiece with chalk and blackboard then an average writer with the most expensive text editor. Languages are just tools, and programmers that are too obsessed with the language (and not the problem domain) are in my view most likely poor programmers.
&gt; Assuming these are largely mtfs We do not have to assume. I lost one Transgender answer, because Libreoffice apparently had trouble parsing the CSV (Some answers ended up in the wrong columns), but of the remaining 34 trans answers we had 19 females, 14 non-binary and one male. So we do have slightly more cis-, than trans-women (about a 60/40 split). 
Oh I see now! Thanks a lot. So it seems that the only thing to work around these issues could be to have first-class definitions, could it not?
I feel that at least one party toned things down quite a bit - and that's the start of things, always one person has to make the first step.
I wonder what the actual runtime footprints of type ambiguity look like? If they don't explode, one might combine dynamic dispatch with nondeterministic programming. Searching the space of viable typings during computation. Sort of like GLR parsing. Especially since bootstrap only needs to run "once", so an HPC-style long-duration run of many cores would be tolerable.
Can you elaborate a bit on what you mean by that? The key thing we need is top level compile-time generativity. We have plenty of runtime generativity in the IO monad (newIORef and friends), and we have special top level statements like `data` for top level compile-time generativity. Unfortunately since these top level statements are so special and not first class, the only way to tap into them is via TemplateHaskell. One solution I quite like is to allow for top level monadic (specifically `MonadFix`) expressions, these expressions would have to be of type `Generate a`, just like how `main` must have type `IO a`. This `Generate` monad would have a `generate :: Generate Unique` function (perhaps use a typeclass to allow `IO`/`ST` to also share this function for `IORef`/`STRef` and similar). Then our `newtype` function would have type `Unique -&gt; Type -&gt; Type` or `Type -&gt; Generate Type`. Then you would be able to create library functions of type `Configuration -&gt; Generate Bundle` where `Bundle` contains a mixture of types / functions. Rather than the usual TH approach.
&gt; Haskell and category theory are synonymous I disagree, and have seen no evidence supporting that. &gt; A great author can write a better masterpiece with chalk and blackboard then an average writer with the most expensive text editor. Pithy. But, data from collected observations strongly indicates that this is simply incorrect when it comes to computer programming. Just asking people, or even teaching then how to write better C code doesn't decrease defect rate significantly. Automatically rejecting any C code that doesn't follow the MISRA limitations -- in effect writing MISRA C -- does. We need better tools *and* better programmers. Haskell is one of the former that made me one of the later.
&gt; I disagree, and have seen no evidence supporting that. You have seen NO evidence, absolutely zero, zilch, nothing of the common association of Haskell and category theory? How about you provide me with evidence that Haskell does **not** have any association with category theory at all? Can you provide me with evidence that shows that a group of people who know Haskell that has never heard of category theory or has has zero idea that it has any association or link with category theory? The association between Haskell and category theory is _common knowledge_, I don't need to "provide you with evidence" of something that is plain and obvious, can I ask you please stop your "debate only" mode and have a _real discussion?_, there is no "points" to win or anyone to impress here, otherwise this is a rather pointless back and forth and useless random argument. &gt; Pithy. But, data from collected observations strongly indicates that this is simply incorrect when it comes to computer programming Please do show me these studies, if such "silver bullets" existed don't you think the industry would have been using this by now? Or perhaps there are far more attributes involved and at play then just naively thinking a single attribute is conclusively the deciding factor in terms of defect rates. &gt; We need better tools and better programmers. This line right here, I agree 100%, I do think we need better tools in general yes. &gt; Haskell is one of the former that made me one of the later. well that's good for you and I don't doubt it, I'm glad that Haskell helped improved you as a programmer. My experience of Haskell didn't change me as a programmer, maybe I'm just an odd individual? but I'm not going to project my experiences on to you.
My idea was to have a type whose values are declarations. Unfortunately it is not completely clear to me how to implement this kind of type. I would guess that one could try to implement _data type_ declarations as contexts, i.e. list of typed identifiers (the type name and its constructors), and function definitions as list of reductions. The declaration type should provide some eliminators to get these identifiers and reductions out of its values, in order to allow other code to use them, but clearly there are &lt;irony&gt;"some"&lt;/irony&gt; details to be filled. The one could build compilers in such a way that values of these _declaration types_ are expanded in real definitions at a preprocessing stage. That's pretty much my idea, I hope it was clear, or at least clear enough.
It may be intuitive if you are used to IDEs. I am not. It was not intuitively obvious to me: - what a solution was as opposed to a project - that all of the compiler options would be hidden in a sub menu properties panel that has a terrible UI - that there was no obvious dependency management or package system, and that the recommended solution would be to download zips of projects like boost and then run ten different commands on them to get them to build, just so I can link against one tiny part of boost - that compiler versions were somehow tied in to specific VS editions - that files had to be manually added to a project for VS to use them in certain ways etc etc. My point is not that VS is terrible (I have grown to appreciate some of what it does), but that it seemed to me that VS is a lot more difficult to get productive in than Haskell with stack. I think I'd have had a better onboarding experience with C++ if I'd used a linux setup though, from what I could judge.
Glad that you got it working! I do remember when I very first set up a Haskell environment (pre stack) that once you had installed certain things it was not trivial to reverse the changes, and definitely different options don't play nicely with each other. I agree with your general points, btw, that it would be better if instructions like this were prominent across the Haskell ecosystem, but as others have pointed out there are various reasons why this is the case. Also, I should say, I am not particularly attached to stack - the new cabal seems to do a very good job as well, and I use both. But stack seems to be the easiest/safest beginner route at the moment. Hope you have fun, and feel free to message me if you run into any difficulties!
That is good to hear, hopefully things will improve.
GHC not optimized for multiple cores?
I thought commercial haskell was a language fork. Research Haskell Commercial Haskell I got really excited there for a minute.
&gt; You have seen NO evidence, absolutely zero, zilch, nothing of the common association of Haskell and category theory? That's NOT what I claimed. I claimed that I hasn't seem any evidence the "Haskell and category theory are synonymous". I recognized the connections with maths in my first post in this thread, when I stated that "Haskell doesn't obscure concepts by creating new names for things that already have mathematical names" -- &gt; the industry would have been using this by now? MISRA C is an industry standard.
&gt; intuitive There are no intuitive interfaces, just ones that match your past experience. It used to be common to claim the mammalian nipple was an intuitive interface, but then people were reminded of [latching](https://www.medela.com/breastfeeding/mums-journey/breastfeeding-latch) problems. I grew up (almost literally) on DOS manuals, which had a format that were inspired by manpages, so I want my programs to have manpages, and I can find them via apropos, and I prefer they work with stdin/stdout or files named on the command-line. I'm not averse to reading, searching, or referencing a large body of mostly text to find the information I need for my next task. I dislike having *anything* "buried" in a dialog box opened from a second-level menu. I don't mind good mouse control, but I'd like composable (memorable is overrated, practice will drill it in) keyboard commands, too. I wouldn't know where to start designing or honestly even reviewing a Haskell IDE. So, it's not my priority. I hope it becomes available to those that want it and I hope IDE users and myself can collaborate on code. But, any IDE that I helped design/create would probably not be something anyone "spoiled" on IntelliJ would even want to use.
That would be really unfortunate more than exciting. It was already done once at a bank, no need for more of that.
If you like Elm and miss Haskell on the front end, you should definitely check out miso. https://haskell-miso.org, it was meant to fill that gap, among other things.
I'm not sure how much the setup process for HIE could be improved really. The reason it takes so long is that it builds everything you might need and I'm glad it does that. For instance, thanks to that, I get the docs for all the library functions in my code when I hover my cursor over them, because it's already built all the haddocks for everything in the relevant stackage lts. If you have the disk space to spare I'd recommend you to just give it a go.
Pedantic node - Text is for Unicode scalars, not code points. Surrogates are code points but not scalars. Text will perform replacements on surrogate characters. &gt; A Text value is a sequence of Unicode scalar values, as defined in §3.9, definition D76 of the Unicode 5.2 standard. As such, a Text cannot contain values in the range U+D800 to U+DFFF inclusive. Haskell implementations admit all Unicode code points (§3.4, definition D10) as Char values, including code points from this invalid range. This means that there are some Char values that are not valid Unicode scalar values, and the functions in this module must handle those cases.
Pre-compiled binaries would improve it a lot. 
&gt; Put together marketing-style material for Haskell in industry Please don't hurt Haskell's no-bullshit reputation for the rest of us. Repeating [my previous appeal](https://www.reddit.com/r/haskell/comments/8jn95d/state_of_haskell_2018_report/dz2ulw7/) from when the last marketing-style survey was published: &gt;&gt; We already failed at avoiding success at all costs but please let us at least try to avoid dishonest marketing bullshit at all costs for Haskell to limit the damage.
What do (did) you mean failed at "avoid success at all costs"? To me it sounds like that phrase is exactly what your advocating.
/u/snoyberg, please give the code of conduct stuff a rest. You raised it as a proposal for Stack, and the corresponding reddit thread turned into a flame-war. The anxieties and concerns expressed about codes of conduct and their enforcement on the anti- side were _thoroughly_ validated by the behaviour of the people on the pro- side. People on the anti- side who asked questions, or raised their concerns in good faith, were accused by the pro- side of being various forms of -ist or -phobe; that's called kafkatrapping, and is a nasty and underhanded way of arguing. I trust your intentions are good, but at this point, it is clear that codes of conduct does not create mechanisms that nourish healthy communities; they generate flame wars, vitriol, fear and division. We've trodden over that ground before, let's not do it again. Please.
All marketing is not dishonest. Marketing is a necessary evil and Haskell needs to get better at it. Your "appeal" would be much better if you _define_ what _you_ mean by "marketing" (and same goes for Snoyman). Here's my (incomplete) definition: - Highlighting Haskell success stories in production - Solving the new-user on-boarding problem by better documentation, tooling, error messages, etc. - Blog posts &amp; talks about the pragmatic benefits of core Haskell philosophies and features (eg. immutability, strong typing, purity, etc.) 
I wrote words about this after my own struggles: https://jappieklooster.nl/pragmatic-haskell-simple-servant-web-server.html It get's you trough setting up a simple webservice. The posts after that also talk about databases etc.
Has anyone had any luck getting HaskellR to work? I've tried the instructions here: [https://tweag.github.io/HaskellR/docs/build-and-install.html](https://tweag.github.io/HaskellR/docs/build-and-install.html) with Nix, Docker, and globally. I also tried working through tweag's Docker image, but couldn't get that to work either. It seems with both Nix and Docker there is always something it can't find that it needs. &amp;#x200B; I got furthest with Docker, but IHaskell can't find a Jupyter install to register itself with. I've tried installing Jupyter through pip, stack, and apt-get, and by making symlinks to everywhere I can think of. I'm guessing I need to add some kind of dependency requirement somewhere, but I don't know how to do that. Any ideas?
&gt; Increase the set of documentation provided by Commercial Haskell, and probably host on commercialhaskell.com I'm confused. More and better documentation (even commercially-geared) benefits everyone, even non-commercial Haskell users. I'm not sure why this should be hosted on something different from haskell.org or the other official channels. And there are personal/company blogs for opinionated posts.
&gt; my DSL needs to support user-defined functions What type system does your DSL have? If types of user-defined functions are monomorphic (e.g. no `forall`) then it's possible to do this with GADTs, but if you have polymorphism then it's going to be tricky. 
I don't see how Haskell community could handle substantially more "popularity". I am aware that talks I watch live or on Youtube are self-selected, but seems that **everything** is compared to Haskell. - Golang has GC (for low latency yes, but there are work on GHC's GC) - Rust has more low level control, more performance... - Clojure has no types (which limit developer expressivity) - Idris, the pacman-complete language, OTOH, has fancier types (this is sarcastic comment, to be clear) - C++ has concepts which are not type-classes (https://www.youtube.com/watch?v=HddFGPTAmtU&amp;feature=youtu.be&amp;t=2660, to me they looks very like though, even value argument is not a problem with `singletons`) - Python has superb machine learning library support (...) - Scala runs on JVM (but how about Eta?) - ... or in browser? (but how about GHCJS, and *two* Webassembly backends...) - ... It's funny, that even C++ has to "fight" to be used in embedded space (compared to C). Haskell has too much such "fronts", that's is both fascinating but also scary. --- That' said, it would be interesting to experience "no one was ever fired for picking Haskell" -era. But trying to get there in one or two steps, isn't it too forced? I.e. isn't Increase the commercial adoption of Haskell simply too broad goal?
I was meant to answer here, but (accidentally) made a post reply, https://www.reddit.com/r/haskell/comments/a6fzga/improving_commercial_haskell/ebwnuw4/
Thanks for this post! I've never had a great intuition about finger trees but this is a very easy to access introduction.
Hi and thanks for the interest! Unfortunately, I don't feel too comfortable talking about my thesis yet and not sure if my university would be happy with me talking about it. However, I would be willing to post about it once it's done. It's not Haskell related, but I think it could be interesting in a similar way United Monoids are.
You're right, this seems to be a compiler bug, can you report it upstream? It seems to be blocking in the interpreter `#0 0x0000000003d53506 in interpretBCO (cap=0x4604380 &lt;MainCapability&gt;) at rts\Interpreter.c:395` `#1 0x0000000003d588b7 in schedule (initialCapability=initialCapability@entry=0x4604380 &lt;MainCapability&gt;, task=&lt;optimized out&gt;, task@entry=0x4d6ef00) at rts\Schedule.c:458` `#2 0x0000000003d5914b in scheduleWaitThread (tso=0x7505ba8, ret=ret@entry=0x0, pcap=pcap@entry=0x4cdfd18) at rts\Schedule.c:2533` `#3 0x0000000003d50968 in rts_evalLazyIO (cap=cap@entry=0x4cdfd18, p=p@entry=0x3d9f0d8 &lt;ZCMain_main_closure&gt;, ret=ret@entry=0x0) at rts\RtsAPI.c:530` `#4 0x0000000003d4fc95 in hs_main (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, main_closure=0x3d9f0d8 &lt;ZCMain_main_closure&gt;, rts_config=...) at rts\RtsMain.c:72` The line it's blocking on is a bit suspect, a debug build of GHC should be able to spot some light on this pretty quickly. I'm not entirely convinced this is Windows only, it seems to be a loop in the scheduler. I won't have time to look into this till next week, but maybe someone else upstream does.
A lot of the free guides aren't particularly good. Most of these things are only necessary for commercial development, and they're also pretty easy to learn in the context of commercial development. 
&gt; You raised it as a proposal for Stack, and the corresponding reddit thread turned into a flame-war. Surely, though, trial by Reddit is a flawed method of deliberation. One of the problems with it, I'd note, is that it lends itself easily to casually offering a tiny handful of comments as purported evidence for sweeping claims (for instance, about a "side" in a discussion arguing in "nasty and underhanded" ways).
I may be reading too much into the proposal but it seems more about FP Complete without mentioning it than anything else assuming that snoyman.com is of the same Snoyman of said corporation
No problem, and best wishes with your thesis! Please share it when you graduate. 
It's because you can't take human psychology out of the equation no matter how much you might wish. Saying "these opinions are my own and in no way reflect..." Is the corporate equivalent of someone saying "now, I'm not sexist but...". Even if you *aren't* sexist, if you spend the majority of your time saying sexist things, well, people will believe what they want regardless of what disclaimer you put. Similarly, any high ranking employee will need to be careful because, since their associations with that company are strong enough, people will unconsciously blend their personal opinions with the company image no matter how you try to prevent that.
Thanks for this. I actually have a pdf. But it seems to focus much more on standard ML than Haskell specifically. Is the version you linked revised for Haskell? 
I don’t believe it is. This is a recommended text, and often referenced, by a functional programming module that I am taking at university, which focuses on Haskell.
AoC is actually the exact reason I have questions. Thanks for your help! 
&gt; haskell.org seems more focussed on language advocacy It's a website on a programming language. What did you expect?
Thanks for the info! I was going to open a GHC bug but, after trying to write a minimal example, I encountered that any TH at all causes problems! For example, this hangs for me: {-# LANGUAGE TemplateHaskell #-} module Main where import Data.ByteString import FileEmbed foo :: ByteString foo = $(embedFile) main :: IO () main = Prelude.putStrLn "Hello, Haskell!" with the main file module FileEmbed (embedFile) where import Language.Haskell.TH.Syntax (Q,Exp) embedFile :: Q Exp embedFile = error "doesn't get here" This is too basic to be a bug in GHC, some other thing must be amiss.
Documentation on how to get started on developing with the language.
&gt;module FileEmbed (embedFile) where import Language.Haskell.TH.Syntax (Q,Exp) embedFile :: Q Exp embedFile = error "doesn't get here" No.. You're right it's very basic.. but I can reproduce it as well with 8.6.3 but not 8.6.2. I've started a debug build to bisect. Looking at the commits that got into 8.6.3, none of the ones I would have suspected show any large scale TH failures on the bots.
I also can't reproduce it with my nightlies from last night `&gt; ghc-8.7.20181216.exe --make .\Main.hs` `[1 of 2] Compiling FileEmbed ( FileEmbed.hs, FileEmbed.o )` `[2 of 2] Compiling Main ( Main.hs, Main.o )` `Main.hs:8:7: error:` `* Exception when trying to run compile-time code:` `doesn't get here` `CallStack (from HasCallStack):` `error, called at .\FileEmbed.hs:6:13 in main:FileEmbed` `Code: embedFile` `* In the untyped splice: $(embedFile)` `|` `8 | foo = $(embedFile)` `| ^^^^^^^^^^^`\^ &amp;#x200B; no hang. So I really wonder what's going on with that 8.6.3 binary..
&gt;I don't see how Haskell community could handle substantially more "popularity". &gt; &gt;I am aware that talks I watch live or on Youtube are self-selected, but seems that &gt;**everything** is compared to Haskell. If everyone is comparing how X &gt; Haskell, then it only makes sense to raise the counter-points. But I see your point -- perhaps focusing on some areas might help instead of spreading ourselves too thin. E.g. the Rust page talks about 4 things like CLI apps, embedded use, networking and wasm. Perhaps it would be useful for the CH group to focus on some key areas first. &gt;But trying to get there in one or two steps, isn't it too forced? I.e. isn't &gt; &gt; Increase the commercial adoption of Haskell &gt; &gt;simply too broad, non-actionable on goal? My understanding is that that phrasing is a bit ambitious so perhaps "vision" or "long-term goal" might be more appropriate for it instead of "goal". The individual bullet points of better marketing etc. can be thought of as short-term goals.
&gt; Clojure has no types Clojure does have types. It's just dynamically typed. This is a meaningful distinction because there are languages out there that actually don't have types, like forth.
&gt; but at this point, it is clear that codes of conduct do not create mechanisms that nourish healthy communities; they generate flame wars, vitriol, fear and division. If you look at the diversity for [nteractio](https://twitter.com/captainsafia/status/1064601420895264768?s=19), it is a result of conscious effort at being inclusive, and a code of conduct is one part of it. I would rather look at numbers from large open source projects than the reactions of a handful of vocal redditors.
Thank you for the correction. I didn't actually realize there was a standard term for codepoints minus surrogates.
Be cautious about what you produce and generous about what you accept. It isn't impossible to make an effort to avoid such feelings, and similarly we should try to offer people a reasonable benefit of the doubt if the way they said something might offend us. 
The prose is language-agnostic. The inline examples are SML+polymorphic recursion. The appendix (which is only available in the print edition) is Haskell. [My Idris "port"](https://gitlab.com/boyd.stephen.smith.jr/idris-okasaki-pfds) might also help, since Idris is closer syntactically to Haskell than SML.
Fair enough, although there's quite a bit of stuff in https://www.haskell.org/documentation to get you started.
&gt; Increase the set of documentation provided by Commercial Haskell, and probably host on commercialhaskell.com &gt; One easy, concrete, and hopefully beneficial to the ecosystem: I propose to remove haskell-lang.org, and instead put its documentation on commercialhaskell.com &gt; Concretely, this will hopefully help rectify a mistake I made, and provide a clear vision for why this site exists and is separate from haskell.org (providing opinionated, commercially-geared documentation, without needing to distance from haskell.org) I think this is a very good step :) In hindsight, haskell-lang ended up causing more trouble than it helped, so I think it’s good to see reflections on this, especially since a lot of people are trying to mend the wounds of the community—it’s good to see these little things also!
Essentially the same question was asked a few days ago. Many helpful responses here: https://www.reddit.com/r/haskell/comments/a69ww2/struggling_to_get_started_with_developing_with/?utm_source=reddit-android
Is it an issue that FP Complete wants Haskell to have wider commercial adoption and acceptance? I would say our goals align pretty well there :)
And as I said there, [repl.it](https://repl.it/languages/haskell) is by far the easiest way to get started on Haskell. (It does not support hackage modules though)
So wouldn't you say this isn't really getting started? The question seems primarily about the developer environment and didn't mention the language at all. repl.it sounds to be the other way around.
There is this app HaskellforMac. http://haskellformac.com/ I have not tried it actually and it isn't free but there is a free trial.
Any given function will expect a particular combination of `Int` and `Bool` as input, no polymorphism here.
The other replies in this thread are good. But also, I have my own Haskell tutorial called [Wise Man's Haskell](https://andre.tips/wmh/writing-real-haskell-programs/) which contains a section discussing how to setup a Haskell environment with VScode and Stack. It assumes you have already downloaded [the Haskell Platform](https://www.haskell.org/platform/), so be sure to do that. 
VSCode + haskell-ide-engine + stack works quite ok on Windows.
This is still an interesting tool though. I think after looking at links from the other comments here I'm starting to get an understanding of whats going on. I was able to install stack the use the integrated terminal on VSCode to run a few files, but I will definitely keep looking
I think you're talking about the closed source compiler at standard chartered. Why was that unfortunate?
Hmm this is all a bit confusing. What would you expect (+) (fmap . fmap) (eval env texp1) (liftA . liftA) (eval env texp2) to do? It looks like it'll be hard to make it even type check. And do you know that `liftA` *is* `fmap`?
One option would be to use the term "bimonoid" as in the paper "Concurrent Kleene Algebra and its Foundations", which I linked from the blog post. If you, like me, prefer the term "united monoid", then you can wait for the paper I am currently working on, which will have a section about united monoids. I will put the first version on arxiv.org soon, so you will be able to reference it.
It is unfortunate in so far as there was duplicate effort (consider Lennart's ICFP appeal for DSL friendly error reporting) and work/packages that didn't translate due to divergence of the underlying language or features supported by the tools. I'm in favor of exploration but am concerned a language fork would fragment rather than enhance the community.
I [posted yesterday](https://www.reddit.com/r/haskell/comments/a69ww2/struggling_to_get_started_with_developing_with/ebuizqp) with a link to my own guide, and as with that post I'd very much like to improve that guide should anything be non-obvious - I think there are a lot of people like yourself who don't know how to get started and I'd like to help fix that. I tested my guide on Windows too, so no problems there I hope! I don't assume any IDE, instead offering tools that are surprisingly helpful without being a full-blown IDE (I come from Java and excellent IDE support!)
Then it's in the realm of possible, you may want to start by studying https://github.com/goldfirere/glambda which implements STLC in this style, and then extend it with top-level bindings.
&gt; (+) (fmap . fmap) (eval env texp1) (liftA . liftA) (eval env texp2) I can't manage to make sense of this expression but that aside, I don't really get why you'd want a Functor or Applicative structure for this. A great deal of the usefulness of these is the fact that they give you tools that work with generic types whereas you have precise types set in your Result definitions. If you really want to have a quick and easy way to access and modify values in your Result type, I think what you want is to have lenses and prisms for it ([if you don't know what they are here's a tutorial](https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/a-little-lens-starter-tutorial)). But that could be overkill in this case I guess that's for you to decide.
I think you can get closer to what you want if you make a small modification to Result: rather than using closed recursion (referring to Result again in one of the constructors), add a type variable to Result (making it kind *-&gt;*) and refer to that type variable in its stead. Now, rather than using Result throughout your code, use a fixed-point of Result (I.e. Fix Result). See the data-fix package for an example of what I'm talking about: http://hackage.haskell.org/package/data-fix-0.2.0/docs/Data-Fix.html Sorry I can be more illustrative; typing from a phone before I catch a flight. That probably doesn't get you exactly what you want, but I think that gets you closer.
The problem being that the `pantry` package is not released on Hackage so it isn't packaged in nixpkgs. You should ask the `pantry` maintainers nicely to release it on hackage, then the script will probably work again.
Generally, if your type is morally a container of something (the `Element` type), I'd recommend [mono-traversable](http://hackage.haskell.org/package/mono-traversable). But in this case it's not clear what your element type is: you could set it to `Integer` and work on those, but then it wouldn't work on strings.
Yes. I dedicated 3 paragraphs to my experiences with the so-called documentation page above.
I have no experience with writing GUIs in Haskell, but I’ve heard of people using these packages: - https://hackage.haskell.org/package/gi-gtk - https://hackage.haskell.org/package/gi-gtk-declarative - https://hackage.haskell.org/package/gloss - https://hackage.haskell.org/package/threepenny-gui
I'm pretty sure what you're looking for is a [catamorphism](https://www.stackage.org/haddock/lts-12.20/recursion-schemes-5.0.3/Data-Functor-Foldable.html#v:cata). The idea is that you have a recursive structure, and want to evaluate it by folding it leaf-first. Look up "recursion schemes" or ask for more info :)
There are two ways of tackling this in Haskell: * You could use a binding to the well-known graphics libraries. Your program structure will then resemble C/C++ code, but it will probably be reasonably fast. * You could go the FRP way with Helm and program in a more idiomatic way. Performance will be tricky though. Also, graphics programming skills will be harder to carry over from other libraries and frameworks.
&gt; I tried downloading the Haskell Platform. Once downloaded I noticed there was no actual IDE included with the download. It would probably be helpful to the Haskell Platform folks if you can say why you thought an IDE would be included. Then they can fix whatever was confusing on their site and help future people in your position.
I think this is an important distinction. I wouldn't want to see things like the "you should rewrite everything in Rust" articles, but the perception of Haskell as esoteric and untested in production, and the lack of onboarding materials have been the biggest ongoing hurdles for adoption at my company. I say "ongoing" because, even though we use it in production in places, I still regularly have to justify our use of Haskell (to people outside the company) in situations where no one would bat an eye if it were much of anything else, and we still have engineers that are squeemish about touching the Haskell code; running into hurdles getting set up and deploying doesn't help. In both cases, these are people who are fine with Scala, Rust, or Erlang, but Haskell is for some reason seen as impractical. &amp;#x200B; I'd love it if there were some non-profit or something where I could confidently throw some money with the expected result of better documentation, libraries to interface with the esoteric C-library/service of the month, or just some instructions on how to orchestrate deployment to whichever cloud.
`first` and `second` are useful when you only want to transform part of your input, not all of it. For example, suppose you are implementing a program which rotates images 90 degrees. You already have functions which do the bulk of the work: readImage :: FilePath -&gt; IO (Metadata, Pixels) rotatePixels90 :: Pixels -&gt; Pixels writeImage :: FilePath -&gt; (Metadata, Pixels) Now you just need to tie those pieces together. You can do it using do notation, as usual: rotatePicture90 :: FilePath -&gt; IO () rotatePicture90 filePath = do (metadata, pixels) &lt;- readImage filePath writeImage filePath (metadata, rotatePixels90 pixels) But let's try to write this in a more pointfree style. If we don't give a name to the `Pixels` portion of the `(Metadata, Pixels)`, how can we specify on which part of that tuple we want to apply `rotatePixels90`? We can use `second` to specify that we want to apply it to the second component of the tuple! rotatePicture90' :: FilePath -&gt; IO () rotatePicture90' filePath = do image &lt;- readImage filePath writeImage filePath (second rotatePixels90 image) Now that each step feeds directly into the next, we can write our code in a pointfree style: rotatePicture90'' :: FilePath -&gt; IO () rotatePicture90'' filePath = readImage filePath &gt;&gt;= (writeImage filePath . second rotatePixels90) Huh, that's not much shorter than the original code, and certainly not clearer. Oh well. Pointfree code isn't always clearer than pointful code, but sometimes it is, and so it is useful to know about tools like `first`and `second` which allows you to write code involving tuples in the pointfree style. And it's not just tuples either! Suppose `readImage` was returning a record with multiple fields: data Image = Image { timestamp :: UTCTime , author :: String , pixels :: Pixels } readImage :: FilePath -&gt; IO Image writeImage :: FilePath -&gt; Image You can write helper functions which convert that record to and from a tuple-based representation: fromImage :: Image -&gt; (UTCTime, (String, Pixels)) toImage :: (UTCTime, (String, Pixels)) -&gt; Image This time `Pixels` is in the second component of the second component, so we'll have to use `second` twice. Let's ignore the IO part and focus on lifting our pure `rotatePixels90` function to a pure `rotateImage90` function: rotateImage90 :: Image -&gt; Image rotateImage90 = toImage . second (second rotatePixels90) . fromImage There you go! Isn't that nicer than the pointful version? rotateImage90' :: Image -&gt; Image rotateImage90' image = image { pixels = rotatePixels90 (pixels image) } 
\[There is a trick\]([https://kseo.github.io/posts/2017-02-05-avoid-overlapping-instances-with-closed-type-families.html](https://kseo.github.io/posts/2017-02-05-avoid-overlapping-instances-with-closed-type-families.html)) that allows to get rid of overlapping patterns.
It's pretty awesome by the way. Has a repl built-in with graphics re-render
You appear to have substituted "diversity" for "healthy community" when framing your response. As Richard Stallman said when he announced the [GNU Kind Communication Guidelines](https://lwn.net/Articles/769167/): &gt; If the developers in a specific free software project do not include demographic D, I don't think that the lack of them as a problem that requires action; there is no need to scramble desperately to recruit some Ds. Rather, the problem is that if we make demographic D feel unwelcome, we lose out on possible contributors. And very likely also others that are not in demographic D. &gt; &gt; There is a kind of diversity that would benefit many free software projects: diversity of users in regard to skill levels and kinds of usage. However, that is not what people usually mean by "diversity". Indeed, improving the reach of Haskell commercially will require welcoming people of a wider range of technical backgrounds and helping them skill up. "Diversity" is often invoked to mean a very US-centric perspective on race, gender and sexuality.
I guess now that I think about it, I haven't seen any language come with its own IDE except python maybe. I guess just because Haskells website had snippets of cool looking code, it made me think that they had their own IDE
I'd agree with you about Reddit dynamics if I had been referring to a thread from the wider site, but the example I alluded to was from \/r\/haskell. Its participants are members of the Haskell community. Arguing over it created division, fear and vitriol in the Haskell community. I had the very unpleasant experience of watching "advocates for inclusion", members of the Haskell community, bully someone who was probably not neurotypical, call him or her a "fucking nitwit", accuse him or her of harboring "-ist/-phobic thoughts" and ultimately say that they would exclude him or her if they had the power to do so. That is not behaviour I want to see in the Haskell community.
My pleasure! Your "24 days of..." series has been indispensable to me on my Haskell journey! Let me know if you ever need a guest blogger 😉
I'm currently working through [Haskell Programming from first principles](haskellbook.com), just coding in vim, and usign `stack ghci` to run the exercises. It's pretty painless.
I know which thread you are talking about. My comment was meant to address it (or, more specifically, your description of it).
Before, to get Haskell running on Lambda, you had to wrap the Haskell binary in a call from the node runtime. Now with Lambda Layers, and the runtime provided by [Theam](https://github.com/theam/aws-lambda-haskell-runtime), you can write your handler and code in Haskell. &amp;#x200B; This is just a small example of doing exactly that.
In my view, having a healthy community is a pre-requisite for having a diverse one, so the fact that there are communities out there which are significantly more diverse than the Haskell community means that you can have a healthy and diverse community while having a code of conduct at the same time, contrary to what you said. Please don't take my comment as a point to start a debate whether "diversity is a good goal to strive towards or not" (you can already guess which side I'm on).
Very cool. Thanks for sharing.
Just to reiterate in this thread: https://github.com/haskell/ghcup is a one-stop shop for unix environments: $ # install ghcup per repo instructions $ ghcup --help $ ghcup install-cabal $ ghcup install # installs recommended GHC version (currently 8.4.4) $ ghcup set &lt;any installed version, e.g. "8.4.4"&gt; $ cabal install ghcid # https://github.com/ndmitchell/ghcid $ mkdir foo $ cd foo $ cabal init -p "myfoo" $ ghcid ... and start editing your project!
I disagree. I think you can have unhealthy diverse groups. Last time we had a code of conduct thread, good points were made about the negative effects of codes of conduct in the Rust, Node and Elm communities. &gt; Please don't take my comment as a point to start a debate whether "diversity is a good goal to strive towards or not" (you can already guess which side I'm on). Agreed. You can see my perspective in the post you replied to, so let's not retread that here.
I struggled with this for a while - there are many old GUI packages, and many of them don't work or can't be installed on Windows (which I use). However, the major packages tend to work (although it tends to be easiest to install them using `stack`, not `cabal`): * The [`threepenny-gui`](http://hackage.haskell.org/package/threepenny-gui) library is best if you just want a simple way to make GUIs. However, it doesn't really do *desktop* GUIs as such: it displays your GUI as a webpage running on `localhost`. (However, it does have [Electron integration](https://github.com/HeinrichApfelmus/threepenny-gui/blob/master/doc/electron.md), which can be used to make a desktop application.) * If you want a simple way to make a desktop GUI, [`fltkhs`](http://hackage.haskell.org/package/fltkhs) \- which binds to the FLTK library - is a good bet, as it's the easiest to install of all the Haskell desktop GUI libraries (even on Windows!). Unfortunately, the FLTK library doesn't create the prettiest GUIs in the world (e.g. see the [GitHub page](https://github.com/deech/fltkhs/tree/master/images)). If you care about such things, the author has also released the [`fltkhs-themes`](https://hackage.haskell.org/package/fltkhs-themes-0.1.0.1/docs/Graphics-UI-FLTK-Theme-Light.html) library, which provides a set of widgets with a much nicer style - see [the GitHub page](https://github.com/deech/fltkhs-themes-demo) for a showcase. * The standard way to make GUIs in Haskell today seems to be using the GTK library. (This is what Leksah uses.) There are two widely-used libraries available: [`gtk3`](http://hackage.haskell.org/package/gtk3) and [`gi-gtk`](http://hackage.haskell.org/package/gi-gtk]). Although the latter seems to be recommended these days, I have found it to be almost impossible to install on Windows (at least, I haven't succeeded yet), and I haven't found any problems with `gtk3` yet. And by the way - if you're on Windows, don't be fooled by the dense [Haskell Wiki installation page](https://wiki.haskell.org/Gtk2Hs/Installation)! If you're using `stack`, then you can install GTK on Windows by going through the [GTK+ Download page](https://www.gtk.org/download/windows.php), but ignoring the instructions to install MSYS2 and prepending `stack exec --` to each command, so e.g. `stack exec -- pacman -S mingw-w64-x86_64-gtk3`; after this, you should be able to use `gtk3`! (It should be easier on other platforms - simply follow the [appropriate download page for the OS](https://www.gtk.org/download/index.php), and then you should be able to use `gtk3`. - If you just want to draw stuff on a window, then have a look at [`gloss`](http://gloss.ouroborus.net/) (a very simple yet useful interface to OpenGL) and [`sdl2`](http://hackage.haskell.org/package/sdl2) (which gives bindings to the SDL library).
I don't think that the duplicate effort was unfortunate! The experimentation on that compiler did make it to GHC in many cases (consider the result of Lennart's ICFP appeal for error reporting -- we have it!).
Thanks for highlighting this! I think that would be a very nice step. People that had been recalcitrant about linking to (very nice) documentation on h-lang.org would be much more likely to do so if it were hosted in the way suggested!
There was (well, there still is) a language called Smalltalk which was notable for coming with a highly integrated IDE.
Make it a `Functor`. data ResultF r = TigerInt Integer | TigerStr String | Record [(Atom, r)] | Array [r] | Unit deriving (Eq, Functor, Foldable, Traversable) I'm not sure what an `Applicative` structure would be here... Your original type is the "fixed point" of this type type Result = Fix ResultF You can find `Fix` in [recursion-schemes](http://hackage.haskell.org/package/recursion-schemes-5.1/docs/Data-Functor-Foldable.html#g:2) as pointed out by u/isovector.
Interestingly enough, this is actually (a part of) the exact motivation for using *recursion-schemes* :) There's [a nice tutorial here](https://blog.sumtypeofway.com/an-introduction-to-recursion-schemes/) that goes over this exact example. Essentially, you parameterize your type over your recursed type: data ResultF a = TigerInt Integer | TigerStr String | Record [(Atom, a)] | Array [a] | Unit This type itself is already useful. But then you can give it to a fixed-point combinator, like `Fix :: (* -&gt; *) -&gt; *`, and you get `Fix ResultF :: *`, your original result type! Using this, you get a lot of use out of `ResultF`, and also general combinators for lifting non-recursive functions on `ResultF` to become recursive functions on `Fix ResultF`.
I think my problem was I created this project with \`stack new myproject simple\`. This is solved by getting rid of the \`executable\` section in \`myproject.cabal\` and replacing it with a \`library\` section.
Yeah, sorry. I realized this afternoon that `liftA` is wrong -- what I actually wanted is a composition of `&lt;*&gt;` to apply the function inside `Either Result (function)` into the other `Result`. But that's not possible because `Result` can't contain a function.
Thanks everyone; I'll look into recursion schemes and give that a shot.
FWIW I've seen a report that ExitFailure 77 comes from a broken gcc due to missing "xcrun", and can be fixed by xcode-select --install.
Dang, thanks man! I'm on my phone right now but when I get home I'm definitely gonna play around with these!
&gt; For the record, I grew up on cmd-line development [...] but ... it's 2018. Bruh, the terminal is the greatest invention of mankind. It's certainly 2018, and the terminal is still king.
I use VSCode with Haskero and ghcid. I have videos describing my setup: https://usethetypes.com/.
[A friendly reminder](https://r09nmzkmg7.execute-api.eu-west-1.amazonaws.com/default/my-haskell-lambda?name=%3Cscript%3Ewindow.location%3D%27http%3A%2F%2Flmgtfy.com%2F%3Fq%3Dcross%2Bsite%2Bscripting%27%3C%2Fscript%3E).
Continuing the conversation in the other thread.
I'm not sure I understand the gist of your comment. Are you saying that Haskell doesn't have a "sweet spot" compared to other examples that you mention? Or are you saying that Haskell _needs_ to pick a sweet spot and market around it, so that initial adoption happens around said sweet spot?
I personally found Vim + GHCI to be the best combo, (I used to use VSCode with haskell-ide-engine, but GHCI is able to do all of that with no configuration) I wrote a (very short) [blog post](https://kndrck.co/posts/my_simple_haskell_development_environment/) on the tools I use to develop on haskell if you're interested.
Interestingly, Firefox is happy to load the page and execute the JS, whereas chrome stops the page loading with an `ERR_BLOCKED_BY_XSS_AUDITOR` error.
Yes, this is the issue. Pantry is currently included inside the Stack codebase itself, since it's still being actively iterated on together with Stack itself. When master is gearing up for an official release, we'll split pantry into its own repo and release to Hackage. Right now, the API is still unstable. The stack.yaml file within the stack codebase refers to the pantry subdirectory. I would guess you would do something similar with nix, or use something like stack2nix to generate the appropriate Nix derivations from that stack.yaml.
Thanks for the supportive comment, I appreciate it.
That's wonderful. A functional language that uses lineal types fully eliminates a lot of burden and make it as fast as the best non-pure languages, and you demonstate that this is possible. Iguess also that latency is also very good. It's time to create a Haskell second generation with no garbage collection, with dependent and lineal types. Please make the syntax as close to Haskell as possible.
Yep exactly, that's why I said "intuitive if you are used to IDEs". I do think there are reasonably objective measures for measuring UI usability though (which is not quite the same thing as intuitiveness). For example, the number of mouse clicks / navigational steps to take important actions, consistency between different UI components, the relationship between visual prioritisation and usefulness, accessibility of closely related information sets etc. VS is not great in some of these respects. 
This mostly just sounds entitled, which is probably why people are angry.
&gt; That's NOT what I claimed. I claimed that I hasn't seem any evidence the "Haskell and category theory are synonymous". Fine I see you you would rather focus on the pedantic details of "synonymous vs association" great lets move further and further away from the original point. _(sarcasm)_ &gt; I recognized the connections with maths in my first post in this thread, when I stated that "Haskell doesn't obscure concepts by creating new names for things that already have mathematical names" And that's the "meat and bones" of what I'm trying to get about, even OOP could be presented in a mathematical and overly formalised way _(e.g. Object Calculi)_ , but to someone learning OOP is that formality useful or even required? And that's the central question, you are saying: * Haskell doesn't hide the mathematics that underlies it I'm saying: * Is it strictly necessary for a **programming** language? [emphasis mine] and more importantly in learning a programming language? In answering my own question, I would say no _(as evidenced by other languages which operate fine without needing to "expose" the underlying mathematics)_ Given that Haskell _(named after a mathematician)_ and created by computer science professors who's primary role is in academia and entrenched in Mathematics as part of research into the exploration of a purely lazy language created a language who's language is very formal is absolutely not surprising. But the goals of an industrial language "for the masses" are strictly not the same as those of mathematics professors, one is language exploration and academic research the other is engineering and implementation. &gt; MISRA C is an industry standard. Great, but that's just a standard for C, what about all the other languages. 
ehh you're expressing it wrongly: ```haskell data Primitive = ```
We are not alone wrestling with these problems.
Please format your post properly. 😊 Also, is it not literally **right there** telling you to install `autoreconf`?? 
GHC's error messages are quite good...
That URL seems odd, here is the gitlab blog - https://boats.gitlab.io/blog/post/rust-2019/
I'm really not sure why you would use `runghc` when you're already using a project manager. As you said yourself, the solution is to use `stack exec`. It will do what's necessary to bring alll necessary libraries into scope, including `Lib`. `ghc` and `runghc` command lines can soon become harrowing, and that's one reason why `cabal-install` and `stack` were developed. 
The word wrap makes it almost unreadable :/
FYI: [A bug report has been created for this.](https://ghc.haskell.org/trac/ghc/ticket/16057#ticket) Check it out if you want to track the process.
Thanks. Words are not split across lines anymore!
You're very welcome! I've spent a long time trying out all the various options for Haskell GUIs, and I'm just happy that I can help everyone else by writing down my experiences.
Right click -&gt; Inspect -&gt; In the styles section, untick \`word-break\` :D
&gt; My Question is why is it so hard to make reasonable Errors in Haskell as in every other Language? Well, at one level you're right: GHC's error messages *are* quite dense, and it would be nice to have better ones. However, you do get better at understanding and correcting the errors as you write more Haskell: for instance, I remember being stuck on one type error for an hour (I think) when I wrote my first larger Haskell program, but now I can interpret most errors fairly quickly. If you give us specific error messages you are struggling with, then it would be easier to help. &gt; I tried Cabal and Stack for my project, but now went to ghc —make. I wouldn't recommend this. While this approach may be simpler for smaller projects with no external dependencies, Cabal and Stack quickly become useful for larger projects. I use Stack, and I find that making a new project is as simple as running `stack new my-project-name simple`, and then using `stack build` to build it. &gt; I think that a project manager should be minimal and shouldn’t interfere with my project structure and other tools (git). I use git with Stack and so far I haven't run into any problems. What problems are you having?
What is the blocker issue for your Haskell kernel project?
Thanks. And I don't seem to be able to edit the original post, or am I missing something?
I think for myself that as long as I’m still learning give —make is simple enough. And one of my problems is, for what do you need “mismatched type at line X in Y in equation Z”, shouldn’t just the “mismatched type” and the line number enough?
Yeah I'll try that. Sorry for the formatting. 
I think Rust does a better job, but Haskell does way better than other languages such as C++
You should be depending on the library name not the modules. So whatever you called that package you should list it in the test suit stanza
But there is a fundamental difference. Rust is half as old as Haskell. We should be the grown ups by now :P
Only enough in the sense returning "Error" or "No error" is enough. I can work through and figure out what line the error is on by applying the rules myself. I can work through what part of that line is the actual issue by applying the rules myself. I can work out what other pieces of code are in conflict to cause there to be an error on that line myself. But why should I? The compiler already did all that work to figure out there was an error in the first place. It can just tell me. Then I don't have to figure out its the 3rd Int on that line is mismatched with 3 different functions in the module calling it with different parameters but is local without issue or whatever. Type errors are inherently because they don't match something. What something? What several somethings?
You can't change the link in a post (it's to prevent abuse)
There are other fundamental differences, though. Rust is the brain child of a large corporate company, they have a bucket load more finding than Haskell does, and it shows. Haskell would need a full team of full time developers (paid), and massive community/commercial funding to be able to achieve a lot of it's lofty goals in a timely manner. The fact that it's made almost all of it's progress due to 1-2 companies generously donating some employees, and PhDs implementing their thesis's in Haskell is a bit ridiculous. Like watching a supercar race down the speedway only to learn it was built out of duct tape and JB weld by a few engineers in a basement in their spare time.
&gt; After searching the rest of the dependency tree exhaustively, these were the goals I've had most trouble fulfilling: TBBlockCipher, TBBlockCipher:test, Octal This means that those dependencies are missing. If you have a recent enough cabal you can run cabal new-test --enable-tests which will automatically try to download and compile any dependencies you've listed in your `build-depends`
This comment presupposes that an ideal form of organization exists that works for all circumstances. As with any community the Haskell community is in flux, always. What works today may not work tomorrow. Process should be malleable and require continuous reflection and refinement it to meet the changing needs of the day.
It does a better job than C, Java, and Python hands-down. I think Rust tries too hard and sometimes the error messages suggest the opposite of you want - that is debatable of course, but Haskell is still ahead of many mainstream languages.
I don't believe `stack exec` will automatically bring `Lib` into scope unless you `stack build` it first.
What does [port JHC tests](https://github.com/csabahruska/jhc-components/blob/master/Ideas.md) mean? Are the original JHC tests not runnable any more? 
One alternate fast &amp; convenient workflow you might consider, to suit your iterative development style, is `stack repl src/Main.hs`, and then just `:reload` and `:main` from within ghci. There may be a trick to shortening this to a single ghci command, but I'm not familiar enough with it to know what that might be.
[JHC test system](https://github.com/johnmeacham/jhc/tree/master/regress) uses a perl script. It is not ported yet. 
I understand that this is not the most complex application, but it's so nice to read code as clean and clear as this. Thanks for sharing.!
Perhaps one of the potential improvements to Commercial Haskell could be to move such blog posts to commercialhaskell.com
I think the main reason for type error messages in particular being a bit cryptic at times is that Haskell’s type system is so powerful. For example, Haskell has type inference, which allows you to make your code more readable by leaving out a lot of intermediate type information. The downside is that you don’t get error messages of the form “This expression should have type *τ*” but instead “The type of this expression doesn’t match *τ*”. Another examples is type classes: You can define your own numeric types, for example, and use them almost like the predefined ones, but if you accidentally use a string in place of a number the error message you get won’t say that a string is wrong here but that `String` is not an instance of `Num`.
If you only write a simple program that doesn’t depend on any libraries beside the basic ones, `ghc --make` might indeed be the built mechanism of choice. As soon as you use additional libraries, you should use some more advanced build tool. I use the version-2 commands of `cabal-install` (things like `cabal build-v2`). This is a entirely declarative solution: you just specify the components of your project and their dependencies, and a simple `cabal build-v2 --all` will download all necessary libraries and build everything that has to be built, taking into account what has been changed.
There was a time when MS was really interested in Haskell, I think then F# or something happened and Haskell fell off their radar.
&gt; Haskell doesn't hide the mathematics that underlies it I agree and I think that's a *good* thing. &gt; Is it strictly necessary for a mainstream industrial programming language? No, but it's not strictly necessary to create "friendly" names for things that already have names and obscure the connection between the two, either.
&gt; I see you you would rather focus I would rather we each address each other's points as they are, rather than building strawmen and knocking them down.
&gt; I do think there are reasonably objective measures for measuring UI usability though I don't know, people are weird, and tasks vary even within fairly specialized fields. I'm sure you could optimize to a fixed distribution of tasks, but what happens when that distribution changes? I'm not sure UI / UX will ever be "solved", and it definitely don't think the current crop of IDEs is significantly better than a well-customized vim.
I think the GitHub issues thing about discussion by text not being enough is key. VC every now and then to discuss issues.
I used trifecta this year :)
The problems are getting tough as hell these days :( And I don't have so much time!
Something like this: data ResultF a = TigerInt Integer | TigerStr String | Record [(Atom, a)] | Array [a] | Unit deriving Eq, Functor type Result = Fix ResultF You then get access to all the fold / unfold machinery from recursive-schemes library. You also get to `fmap` on `ResultF`s.
Although the language capabilities are really awesome, the IDE and tooling is far from perfect. Even with so sophisticated type system, without the IDE support the wide adoption won't happen and this future will not come.
I'm not sure I'd put the "Correctness by construction" checkbox on Haskell 2010, the current state of GHC (whatever extensions you want), or Dependent Haskell. Agda, sure. Idris, if you turn on the totality checker. Coq, definitely. But, no current or future version of Haskell seems to be planned where the type system corresponds to a consistent logic, which is generally what I think about when trying to formalize the term "correctness". I do think the work on Dependent Haskell is cool and exciting. We have a lot more libraries and much better tooling for Haskell than we do for Agda or Idris, and it's nice to start with "batteries included". Plus, we can always use testing (rather than *just* types) w/ Depdenent Haskell if the type system doesn't give us any guarantee we want. But, I think the Future of Software Development is Blodwen / Idris 2.
I'm a dependent types pessimist.
Writing a database interface at the moment and realising how useful dependent types would be. To read a value and return a type based on the value.
Can you elaborate?
I really enjoyed the tutorial. Thanks for sharing it!
I suppose this is all true but personally..seeing "-XDependentTypes will be in GHC 8.XX" would blow out any possible IDE/tooling announcement. And it would probably make me more productive as well. But I write Haskell with emacs (just `haskell-mode` for indentation help) &amp; `ghci` (using all its features which get pretty IDE-like). Takes you a long way!
I think "correctness by construction" is meant to be fast and loose. And Haskell can definitely be correct by construction modulo bottom.
I agree about the totality checker (also the Type:Type axiom) - not quite correct by construction, but a pragmatic compromise. This is why I think an Agda -&gt; DH translator is an interesting direction to explore. I don't believe in strict-by-default languages such as Idris/Blodwen, though they're fascinating pieces of tech as well and there are features I'd like to steal from them.
Luckily for you .. * They are opt-in * We have abstraction * You were possible a &lt;insert Haskell feature/idiom&gt; pessimist at some point in the past as well :)
I cannot wait! I've used `singletons` in production before. It worked (I got some guarantees and powerful types) but it wasn't pretty :)
Ada is efficient, has a strong type system and has been relied upon in the aerospace industry for years. I wouldn't say it's the future though.
&gt;Rust is half as old as Haskell. I guess you were using "half as old" informally. For anyone interested, Rust is 3.5 times as old as Haskell * [https://en.wikipedia.org/wiki/Haskell\_%28programming\_language%29](https://en.wikipedia.org/wiki/Haskell_%28programming_language%29) * [https://en.wikipedia.org/wiki/Rust\_(programming\_language)](https://en.wikipedia.org/wiki/Rust_(programming_language))
I think you're right, I had to do something with brew and xcode as well.
Can you give a code example? How hard is it to get the same behaviour using ADTs + type classes + type families?
Agda and Ada ar two different languages, and only Agda has dependent types.
Neat pun. One concern I have is in terms of composing things across libraries - right now, Haskell type signatures sit in somewhat of a sweet spot where with a little bit of effort everything "clicks together". If the types get significantly more complex then (a) they're harder to understand and (b) functions have very precise requirements for arguments. E.g. if someone in an external library implements a function on a data structure but doesn't provide proofs for all the properties you'd expect to hold for the results, then you copy most of their implementation and try to thread along the proofs, only to realize that the problematic function calls more functions from the same library which are also problematic...
&gt; They are opt-in Yeah... kinda. The extent to which they will be opt-in depends on what libraries will use them, and how.
I really enjoyed the article. Glad to see so many details and explanations. Those are exciting plans! The only thing I was disappointed to not find was mention of Liquid Haskell, and other such in the "proofs separate from code" paradigm. Does the author have a write up addressing that? Thanks!
All libraries are opt-in. The existence of servant doesn't stop anyone from writing an HTTP server with WAI directly, for instance. I've always felt the "dependent types will infect libraries and make me write proofs" to be a very FUD-based argument.
&gt; They are opt-in Elsewhere on this submission you mentioned that you used singletons in production. I think that makes it very clear that while someone may opt in to using dependent types, they can't opt out if they have any coding relationship whatsoever with someone who uses dependent types in their work. Since our industry be that way, I estimate that it'll be at most 2 years from now before you have changed companies and your singletons code is now legacy bequeathed to someone else. Luckily for them, dependent types are opt-in. At a larger scale, if Haskell were to be really dependently typed, how will that manifest in base? It is not reasonable to say that one can opt out of base. How base should support dependent types is, by the way, a huge open question that will not have a satisfactory answer any time soon. So there will be many dependently typed bases; people who find the proliferation of alternative preludes troublesome should duck and cover. Being able to opt in in many mutually incompatible ways is even worse than being forced to opt in in one way. &gt; We have abstraction I don't know what this means in this context. &gt; You were possible a &lt;insert Haskell feature/idiom&gt; pessimist at some point in the past as well :) Yes? So? I was pessimistic about roles, but everyone else was too. Pessimism doesn't mean "This will never work". It means I think this stuff is way harder than anyone else thinks. It's not around the corner, and when it lands it will be an extremely painful period of time before it shakes out how to use it properly.
&gt; All libraries are opt-in. base &gt; I've always thought the "dependent types will infect libraries and make me write proofs" to be a very FUD-based argument. How fortunate for you that you have always been able to decline to work with someone else's code.
Ada SPARK has sophisticated contracts, which could be argued to be a part of the type system. I think what they're implying is that "strong type system + efficiency" does not imply "this is the future" because Ada does not enjoy mainstream success. They're not confusing Ada and Agda.
There are many issues, and I know there's a lot of different approaches to them being developed. I think the plurality of possible approaches is an argument in my favor. One major practical issue is how to gracefully mix proof obligations with code. Strictly speaking, they're not different things with dependent types but actually they are. The most promising approach I've heard, and it's promising in large part because I believe in his ability very much, is u/edwardkmett 's coda project. I'd like to see how that shakes out and I am watching the space in general. At a more philosophical level, types are for reasoning about terms. If my types are as complicated as my terms, I'm in for the same world of pain as having a convoluted, ill-conceived, buggy program at the term level. I don't trust myself to use dependent types responsibly to create proof obligations that are commensurate with logic they are trying to secure and I don't trust others to do so either for a long time. The fundamental tension is that a type of a term is also a typing of its environment and *no one can predict how their code will be used*. I expect the foot-gun market to boom when dependent types land in Haskell.
&gt;The syntax is Haskell-like, but, unlike Haskell’s lazy evaluation, values are calculated immediately in Plutus.
&gt; base base is full of low abstraction modules. I have seen 0 indication that they'll be replaced wholesale by dependently-typed implementations. Like I said, FUD. &gt; How fortunate for you that you have always been able to decline to work with someone else's code. I've worked professionally in Haskell with other's code. Some has been bad due to low-abstraction issues partial functions and overuse of tuples and Either. Some has been bad due to high-abstraction issues such as singletons. I just don't complain. Call out mistakes in code review, refactor with value propositions for your decisions in hand, and sometimes just live in the bad code. I don't see how dependent types are going to ruin everything when working with others. Another textbook FUD argument.
iirc Rich Hickey makes a very similar argument re: library composition against static types &amp; Haskell as a whole. And the typical counterargument is better type information *helps* with composition. 
When (or if) the ecosystem of Idris catches up it will leave Haskell behind in no time and will become our much-sought-after Haskell 2.0, I predict
I think the strictness aspect will hamper it significantly. 
When calling fancy typed code from more conventional code, perhaps it would be possible to gather the required evidence at runtime. Like this signature from [`fin`](http://hackage.haskell.org/package/fin-0.0.1/docs/Data-Type-Nat.html#v:reify): &gt; reify :: forall r. Nat -&gt; (forall n. SNatI n =&gt; Proxy n -&gt; r) -&gt; r 
1. I would be glad to be proven wrong. In my admittedly little experience with dependent types, that has not been the case. 2. I am not making an argument (I do not have a package ecosystem as big as Haskell's with dependent types which I can point to), merely voicing a concern. 3. Hickey's point has a certain element of truth to it. That doesn't mean that "Haskell is bad" -- it is just that we have decided that we here on r/haskell feel that the costs are worth the benefits. With dependent types, some people are on the fence about "are the tradeoffs are worth it?" and some people have already picked sides, and that's ok.
I really agree with this -- Haskellers are typically *very* bad about hiding difficult or fancy uses of features behind a more friendly abstraction/indirection.
&gt;Since our industry be that way, I estimate that it'll be at most 2 years from now before you have changed companies and your singletons code is now legacy bequeathed to someone else. Luckily for them, dependent types are opt-in. I've already left that company, and from what I've heard, people aren't having trouble with it. Because it's possible to write maintainable `singletons`\-based code if you know what you're doing **and** encapsulate `singletons` well. It also helps when `singletons` is used to provide tangible value (in my case, removing the need for boilerplate &amp; otherwise error-prone value-level manipulations)
We need a hackage+cabal / PyPi+pip / cpan / cran / ctan. Even then though there's a *lot* of work required. Would be nice to have a Haskell -&gt; Idris transpiler, even if it left the totality checker off.
"Morally correct by construction"? :)
You seem to use the term "FUD" as a more pompous synonym for "I disagree". :^)
&gt; I've already left that company, and from what I've heard, people aren't having trouble with it. That's cool. I believe you. Now, of course, that won't always be the case yea? There is a sociology to this sort of thing.
Yeah, I don't think it would be "Haskell 2.0" because of that. [That's Agda. ;)] But, pervasive laziness is not something that all programmers want to reason about. The changes coming in Blodwen make it easier get the compiler to check erasure, so maybe once we are freed up from that maybe we can spend the cycles thinking about laziness. (And maybe the "linearity" annotations will help there, too.) For the people that want a strict Haskell, Idris could be a good ecosystem to grow. (We've already got dependent types; so we are slightly ahead of Haskell there.)
No. I use it to refer to arguments built on Fear, Uncertainty, and Doubt. And acting like `-XDependentTypes` will "infect" `base` and force dependent types &amp; undue proof-writing upon the masses fits the bill. Unless you have sources or something to substantiate that that's at all being considered. Because without that, the only reason to use it as a fact is Fear. I'm not saying your concerns aren't valid (I agree that refactoring `base` to be as dependently typed as possible would be silly). Just way too strongly worded and "jumping the gun."
Yes, that’s true but that wasn’t the point of my Question, I asked why Haskell can’t have simple easy to comprehend Error Messages!
&gt; Uncertainty Yes, exactly. Uncertainty. That is what makes me a pessimist. Along with my current experiences with singletons. As I've said elsewhere, being a pessimist doesn't mean I don't think it will work out eventually. Just take your estimates and multiply them by 10, and that's my timeline for dependent types becoming useful. By the way, one of the most common examples in papers about dependent types is securing low-level interfaces, like file handling. You know, that thing base does?
These aren’t my problem either, these Errors only require the basic understanding of Haskell. But why do I also need a description of the place at wich my error occurred instead of just underlining the part the made problems in the line + the line number and then a more thorough description of what might caused the problem. If I build my program in the Terminal and the first thing ( from the bottom ) is a in the equation ... instead of Error: Name, Description, Position.
&gt; But, pervasive laziness is not something that all programmers want to reason about. This is why there are roughly 1000 strict languages for every lazy one. I'm generally a fan of continuing to explore the other side of the bet when the ratios are that lopsided. Usually something is being overlooked.
&gt; One major practical issue is how to gracefully mix proof obligations with code. I think all of the intentional approaches I've seen support doing this however you'd like, and even mixing the styles if multiple approaches are used across the libraries you are pulling into a single program. If, later, there's "One True Way", we can optimize toward that later. My experience is still limited, but I tend toward "external" proofs (same file, but not having the proof term decorate the return value) for function behavior just because it makes it easier to talk about 2+ functions in the same proof term. Also, from a software maintenance perspective, I prefer NOT exporting the bodies of any functions, but rather exporting some proof terms that serve as a function specification (and again, not their bodies). It allows me to change the function body later as long as I can still derive the specification proofs. I focus on intentional theories, because I want `let fibs = Data.Function.fix $ scanl (+) 0 . (1 :) in \n -&gt; fibs !! n` and `Data.Function.fix $ \f n -&gt; case n of { 0 -&gt; 0; 1 -&gt; 1; n -&gt; f (n - 1) + f (n - 2); }` to not be equal, even though (when applied to any natural number) they reduce the the same value.
As much as laziness is beneficial for code modularity, it also makes inductive reasoning broken. Is the future a language where inductive reasoning is hard?
&gt; Fear, uncertainty and doubt (often shortened to FUD) is a disinformation strategy used in sales, marketing, public relations, politics, cults, and propaganda. FUD is generally a strategy to influence perception by disseminating negative and dubious or false information and a manifestation of the appeal to fear. Making a comment on Reddit voicing concern in a reasonable manner, without asking everyone else to "stop asking for/using dependent types, they will lead to bad things", can hardly be classified as a "disinformation strategy".
&gt; if someone in an external library implements a function on a data structure but doesn't provide proofs for all the properties you'd expect to hold for the results, then you copy most of their implementation I don't really maintain anything that anyone uses right now, but I assure you I'd prefer something like this be contributed. I absolutely *know* I'm not necessarily going to prove everything anyone might need, but I do want to expose enough that either your proof is contained or you can build your proof in a separate module. You don't even have to come with a implementation (though it's greatly appreciated); open an issue and let us know what you need that's missing. I do feel like there's quite a bit of experimentation for what people even *want* from a library as far as carrying around proof-relevant terms. And, also, is there is cost for choosing something that maintains those relationships. I also feel like this is a very "first world problem" in that we are asking what is the *best way* to preserve information in DT language where in a non-DT language there's *no way* to have the compiler preserve that information.
I would guess, read a table column specification from the schema table, and return a table type based on that. I'm still having trouble seeing how that would work out in practice though.
&gt; intentional Isn't it "intensional"?
&gt; If my types are as complicated as my terms Are they? They are concise specifications. They don't have much performance considerations, unlike the runtime code they check -- which simplifies them. Even when they are complex, they are *another* formulation - allowing at least an equivalent check, requiring that a mistake be made twice rather than once.
&gt; I've always thought the "dependent types will infect libraries and make me write proofs" to be a very FUD-based argument. Especially since Idris (beleive_me / really_believe_me / idris_crash), Agda, and Coq all have some sort of way to assume any proof you need. There are cases where this might lead to a run-time crash, but that doesn't make it *worse* than a non-DT language.
&gt;I don't really maintain anything that anyone uses right now, but I assure you I'd prefer something like this be contributed. This means you need to have some form of subtyping (like in liquid types) for you to not break your old callers. I don't know if DH is going to have subtyping of that form, and how that might interact with type inference. &gt;I also feel like this is a very "first world problem" in that we are asking what is the *best way* to preserve information in DT language where in a non-DT language there's *no way* to have the compiler preserve that information. Sure. Having lots of different data types brings the problem of writing boring functions for them, so we have deriving. There are a variety of proof search/automation techniques out there, but it isn't clear to me (largely because I'm a noob in the space) if there are one or more which would play nicely with Haskell.
Almost certainly. I'm a poor speller, and it doesn't help when the spell-checker in my browser thinks intensional is an spelling error.
Totally. I'd actually be *more* stoked about Idris if it were non-strict. I don't like interacting with the Agda stdlib (too much Unicode) and the implicit bindings in Idris really do cut down on the type noise in a lot of places. But, I play around in it, too.
What about that new quantitative type theory in Idris 2? Doesn’t this solve the irrelevance problem while also bringing in other nice features like linearity all in one nice package?
&gt; This means you need to have some form of subtyping (like in liquid types) for you to not break your old callers. Maaaybe. Or perhaps the new version of `xxx` becomes `generalized_xxx` or (ala Win32) `xxx_Ex` and we re-implement `xxx` in terms of new stuff. Maybe a type changes from a data type with no exported constructors to a type alias or newtype. There's plenty of ways to improve a package without disturbing the API. Plus if there's a compelling reason to change the API, the sooner I realize it, the sooner I can start the migration process.
It would be fun to see what a Lazy Idris looked like. You'd probably need to rewrite much of the Prelude, but one of the differences in Idris 2 will be "as small a prelude as possible" so it ought to be something you could try. I'm not especially interested in making Idris lazy myself (I don't dislike laziness, it's just not a thing I want to have to worry about) but I know a lot of people are. So I'm trying to make it as easy as possible to try for anyone who wants to give it a go!
&gt; Posted on December 16, 2019 
I have some problems with the current `Lazy` implementation that I can't explain -- but it's definitely not behaving like call-by-need Haskell. In particular [this](https://gitlab.com/boyd.stephen.smith.jr/idris-okasaki-pfds/blob/master/src/ImplicitQueue.idr) queue implementation ala Okasaki has disastrous performance in general and very odd behavior in the REPL. But, I'm probably just doing something wrong.
Agreed, this is the classic tagged union (no subtyping) vs union (yes subtyping) "debate", but at the proof level :P. Whichever you pick, there is a tradeoff in terms of less flexibility vs possibly weaker inference, and in either case you can have workarounds for the problematic bits.
I wouldn't expect it to behave like Haskell even with the Lazy annotations, given that once you leave 'Lazy' things are just going to run to completion. The REPL is different again, because you're relying on the evaluator as used by the type checker, which computes normal forms (in Idris 1) even when it doesn't really need to. This is one thing that is changing significantly in Idris 2, though hopefully the only thing that anyone will notice in practice is that it works much better :).
Can't prevent off-by-one errors with Rust's type system (at least not yet)'
Don't worry about it. "intensional" is a scam word.
FUD originally referred to a deliberate strategy of *sowing* fear, uncertainty, and doubt as an intentional attack. I would hesitate to apply it to presumably well-intentioned arguments that happen to be *motivated by* fear, uncertainty, and doubt.
I'm pretty sure the MS Research centre in London is where most of the work on GHC is still done
Could you elaborate a bit on how you encapsulate usage of singletons or point me to a resource about this?
&gt; Especially since Idris (beleive_me / really_believe_me / idris_crash), Agda, and Coq all have some sort of way to assume any proof you need. That only works for proofs of propositions, i.e. types that do not have computational relevance, and in general for types whose properties are enforced in an extrinsic way. For example, compare the morally equivalent functions `ix : Σ (l : [a]) (isSorted l) -&gt; b` and `ix : SortedList a -&gt; b`. You can cheat and fake `isSorted l` in the first case, but in the second you need to provide a legitimate sorted list, because the function needs it for computation. &gt; There are cases where this might lead to a run-time crash, but that doesn't make it worse than a non-DT language. I would argue that a liberal (ab)use of believe_me &amp; co. (where "liberal" = beyond very rare cases in low level/FFI code) *does* make it worse, in the sense that a dependent types that lies to you is worse than a simple type that is honest in not making advanced claims of correctness.
I suppose it's still too early to ask whether any dependent Haskell has arrived in some pre-release version of 8.8?
&gt; You can cheat and fake isSorted l in the first case, but in the second you need to provide a legitimate sorted list, because the function needs it for computation. Yeah, but you can cheat and fake out the constructors of `SortedList`, giving a `unsafeIsSorted : (l : [a]) -&gt; SortedList l`. In fact, I would expect someone to have provided an `DPair [a] isSorted -&gt; SortedList a` that I can cheat with `(my_list, cheat_isSorted my_list)` to prove my `unsafeIsSorted`. I don't recommend broad use of `undefined` or `unsafeCoerce` in Haskell, either, but if the type system is getting in your way and you know it is safe, you stick it in and come back to it later. Same thing with Adga / Idris / Coq, just for proof terms.
So it's the difference between "I need a function operating on some `a` which is an instance of `Floating`" (what ghc thinks you're saying), and "I need a function that will work for *any* `a` that is an instance of `Floating`" (what you're probably trying to say). In your code, the type signature `takesFloating :: Floating a =&gt; ([a]-&gt;[a]-&gt;[a]) -&gt; Int` promises that I can call `takesFloating` with a function of type `[Float] -&gt; [Float] -&gt; [Float]`, for example - obviously, that won't work when we then call `needsDouble` on it. The answer is to enable the `RankNTypes` extension, and then be a little more precise about what we want: {-# LANGUAGE RankNTypes #-} takesFloating :: (forall a . Floating a =&gt; [a]-&gt;[a]-&gt;[a]) -&gt; Int takesFloating f = needsDouble f needsDouble :: ([Double]-&gt;[Double]-&gt;[Double]) -&gt; Int needsDouble f = 1 
`unsafeIsSorted : (l : [a]) -&gt; SortedList l` is not what I'm talking about. In my example, `SortedList a` is a type whose elements are sorted lists, much like `Fin n` is the type of natural numbers below `n`. It is not a predicate `SortedList l` on lists `l : [a]`, which would probably be a proposition, hence something you can fake. Having said that, how would you construct a function `unsafeIsSorted : [a] -&gt; SortedList a` that fakes sorted lists? Postulating it or putting `f = undefined` won't do, because the result can't be computed upon.
the underlining colored bits are only a thing since ghc 8.2 so that's why there is some ugly redundancy. The reason that not every error message is exactly self explanatory is that it'd be a lot of work to make it so. And after a while people just learn to parse the existing format. This isn't exactly ideal, especially for people learning haskell, so it will hopefully improve further over time.
Thank you!
The fact that you weren’t happy with the documentation doesn’t mean Haskell.org is more focused on language advocacy than documentation on the language. And you were hardly unhappy with the documentation, you were unhappy with the reality of what was being documented, which is that there is no mature IDE for Haskell.
I'd need to know more about the constructors, and other functions returning a `SortedList a`, in scope. For something like: data SortedList a where Nil :: SortedList a Single :: a -&gt; SortedList a Cons :: (x : a) -&gt; {lt : x &lt; (head l) = True} -&gt; SortedList a I'd do: unsafeIsSorted : [a] -&gt; SortedList a unsafeIsSorted [] = Nil unsafeIsSorted [x] = Single x unsafeIsSorted (x:y:xs) = Cons x {lt = idris_crash "unsafeIsSorted: lt assert"} (unsafeIsSorted y:xs) -- or: Cons x {lt = believe_me Refl} (unsafeIsSorted y:xs) It would be no more or less dangerous than asserting any other `x &lt; y = True` value.
&gt; better type information helps with composition. Not sure what you mean by "better", but in the case of dependent types, most of the times you get *more precise* types. And more precise types means that it is less likely two pieces of data will have the same type, which means less composition out of the box and more need for explicit coercions, in general. I don't want to be negative, I really love dependent types and fancy type systems. It's just that the more I use them, the more I become convinced that current Haskell really hits a sweet spot between type-based correctness and ergonomics. I think one of the best things about Haskell is that it showed the world that just a bunch of simple, well-designed tools like ADTs, typeclasses and advanced parametric polymorphism get you a *long* way. I wouldn't be sad if Haskell continued exploring this part of the design space, and left dependent types to its successors.
Yes, but this only works because you can encode the property of being "sorted" as a proposition inside the definition of `SortedList`, which you can then fake. But fair enough, this is a perfectly reasonable way to write it, so I guess it was a bad example for the point I'm trying to convey. Consider instead `Fin n`, defined as usual, and a safe indexing function `ix : Fin n -&gt; Vector n a -&gt; a`. You want to use a natural number to index, but you don't want to prove that it is actually a `Fin n`. However, you can't really turn a natural number into a fake `Fin n` without `unsafeCoerce`, which is not pretty.
&gt; important note: I cannot change the signature or the body of needsDouble as I'm importing it from a library. But can you change the type of `takesFloating`? If your code uses `needsDouble`, then you really need a `Double`, you can't pretend like your code will also work with other types. &gt; Would this work? Not with that type signature! `takesFloating` is still claiming that it works with any type with a `Floating` instance. You have made your code work with more types, namely any type which has a `Convertible` instance, and that could allow you to support more types than just `Double`, but it's still not powerful enough to work with any type with a `Floating` instance. So your type signature should instead be `takesFloating :: (Floating a, Convertible ([a]-&gt;[a]-&gt;[a])) =&gt; ...`. &gt; instance Convertible ([Double]-&gt;[Double]-&gt;[Double]) where &gt; needsDoubleConverted f = needsDouble f That typeclass has a very narrow purpose! I think it would make more sense to create a typeclass for converting between `a` and `Double` in both directions: class DoubleConvertible a where toDouble :: a -&gt; Double fromDouble :: Double -&gt; a fromListOperator :: DoubleConvertible a =&gt; ([a] -&gt; [a] -&gt; [a]) -&gt; Double -&gt; Double -&gt; Double fromListOperator f xs ys = fmap toDouble (f (fmap fromDouble xs) (fmap fromDouble ys)) takesFloating :: (Floating a, DoubleConvertible a) =&gt; ([a]-&gt;[a]-&gt;[a]) -&gt; Int takesFloating f = needsDouble (fromListOperator f) In fact, such a typeclass already exists! Well, kind of: [`realToFrac`](http://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#v:realToFrac) can convert any type with a `Real` instance to a type with a `Fractional` instance: realToFrac :: (Real b, Fractional c) =&gt; b -&gt; c `Double` has instances for both, and since you want to be able to convert between `a`s and `Double`s in both directions, you'll have to ask for `a` to have both a `Real` and a `Fractional` instance: fromListOperator :: (Real a, Fractional a) =&gt; ([a] -&gt; [a] -&gt; [a]) -&gt; Double -&gt; Double -&gt; Double fromListOperator f xs ys = fmap toDouble (f (fmap fromDouble xs) (fmap fromDouble ys)) takesFloating :: (Floating a, Real a, Fractional a) =&gt; ([a]-&gt;[a]-&gt;[a]) -&gt; Int takesFloating f = needsDouble (fromListOperator f) And we can even drop the `Fractional a` because `Floating a` already implies `Fractional a`.
If two types don't match up due to more precise types, then that's usually a good thing! It means there's a little bit more code you need to write (and potentially decisions to be made) so that things work.
You'd [define a new command](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#ghci-cmd-:def) to do that via ghci&gt; :def go (const $ return ":reload\n:main")
I confess I don't know too much about dependent types, but it seems there is a fair amount of work telling the type checker that your code is correct. I would like to minimize the amount that I'm required to tell the type checker, while maximizing the amount the type checker tells me (in terms of incorrect code). In this regard, it seems that refinement types are a superior solution to dependent types.
Not a question but a fun thing I noticed just now. Recently, I've been writing a bunch of code like: data Point = { px , py :: Int, ... } ... sortBy (comparing py &lt;&gt; comparing px) points or something like it without thinking too much. I thought it should work because Ordering has a Monoid instance and it does work. However, right now, I just paused for a bit and realized HOW it works. There is a `Monoid b =&gt; Monoid (a -&gt; b)` instance which works as you'd expect - feed the input argument to both functions and combine the outputs. This derivation is being called TWICE, first to derive Monoid (Int -&gt; Ordering) and then Monoid (Int -&gt; Int -&gt; Ordering). Type class instances for `(-&gt;)` are magic :D. Also a good example to show how we take non-linearity for granted as a part of having such flexibility.
&gt; The answer is to enable the `RankNTypes` extension That's not the answer here -- with the given implementation of `takesFloating`, the only type that makes sense is `([Double] -&gt; [Double] -&gt; [Double]) -&gt; Int`. With your signature, you're making `takesFloating` less general than it could be, as you're requiring a polymorphic function when a concrete one would suffice. (Also, in general, `RankNTypes` is rarely the answer to a beginner question. ;))
You can produce something that takes a natural number and produces a `Fin n` if it is less than `n` and produces a non-sensical value otherwise. believe_fin_me : Fin n -&gt; Fin m believe_fin_me fn = replace (believe_me Refl) fn natFin : (n : Nat} -&gt; Fin (S n) natFin Z = FZ natFin (S n) = FS (natFin n) Combine to get an unsafe index. If your `n` is actually small enough, you'll be fine. If your `n` is too large, you'll witness what happens when `Fin Z` is inhabited. :) Just like sometimes `unsafeCoerce` is used in Haskell when you know you are right, and it is safe.
Type Driven Development in da house! :) ;)
What I disagree with is the claim that "better" (where to me "better" = "more precise", unless you have another definition) types help with composition, since it is, as a matter of fact, not the case. However I'm not making value judgments, and in fact I agree that it is a good thing if increased precision is what you want.
A lot of it was just `data`/`newtype` wrappers around types from `exinst`. The tutorial is pretty good: http://hackage.haskell.org/package/exinst-0.7/docs/Exinst.html More concretely: I represented a couple of related ADTs as 1) a `singleton` `Enum` where each value represents a constructor in the ADT and 2) a `data family` indexed on (1), where its definition is the fields of each constructor. This allows you to write relations between constructors of different ADTs using type families, for instance. `exinst` solves a lot of common problems you run into when you represent ADTs this way. And once you have `exinst`, it's not too far to provide normal-looking Haskell types &amp; functions that operate on these dependently-typed structures.
Pervasive laziness is one of the best and least appreciate features of Haskell, it is sad to see another language neglect it. 
You can do the same sort of thing you'd do with a list you "know" is `NonEmpty` or a key that you "know" is in a `Map`. Just use `fromJust`! unsafeNatToFin :: Nat -&gt; (n :: Nat) -&gt; Fin n unsafeNatToFin nat bound = fromJust (natToFin nat bound) [Idris `natToFin` for reference](https://github.com/idris-lang/Idris-dev/blob/f76dac0affe019c2b1dac14fadb1e689f2063711/libs/base/Data/Fin.idr#L107)
Thanks, I'm new to singletons and this is the kind of stuff I've been looking for!
Chrome blocks code that is also located in the URL, I guess Firefox doesn’t 
beam?
Shameless plug for my preferred setup (which, crucially for me, supports navigate to library source code inside the IDE) https://gist.github.com/androidfred/a2bef54310c847f263343c529d32acd8 
Shameless plug for my preferred setup (which, crucially for me, supports navigate to library source code inside the IDE) https://gist.github.com/androidfred/a2bef54310c847f263343c529d32acd8 
Be nice
Yes that's true. It's still software development. So it's trade-offs all the way down and rarely strictly better. But at least with Dependent Haskell it'll be harder to bump your head on the ceiling :)
Do you mean like `Frames` does when constructing a row type?
`ghcup` is something like [2000 lines of bash script](https://github.com/haskell/ghcup/blob/master/ghcup) while `stack`'s GHC management is pure Haskell -- I know which I'd rather rely on. Why not take the GHC management code from `stack`, factor it into a library, and then `stack` and `ghcup` can both depend on that?
Thanks for the detailed roadmap with the rationale. I am a fan of using Haskell but an admitted shortcoming of my knowledge is knowing exactly what goes into each of these language features and it’s nice to see a reasonably straight forward description of the moving parts that go into making this come together. It makes me excited to keep using the language to know these active developments are still taking place.
&gt; We have abstraction My experience so far with elaborate type systems has been that they tend to make abstraction more difficult. It's been necessary to dig into the details of a dependency's types to get fairly simple things done, and this has led to weaker encapsulation.
I will absolutely buy it in print version. Today I read through the freely available sample and I have the feeling I understand a bit more about GADTs and some GHC extensions! Thank you!
The use case for https://lens-by-example.chrispenner.ca/articles/prisms/review is not clear. Why would I prefer... _ErrorDB . _TransactionFailed # "Contention on User Record" ...over... ErrorDB $ TransactionFailed "Contention on User Record"
What you seem to want is something like: data ProgramType = IntegerType | StringType | a :-&gt; b -- ... data family ProgramResultType (a :: ProgramType) where ProgramResultType IntegerType = Integer ProgramResultType StringType = String ProgramResultType (a :-&gt; b) = a -&gt; b -- ... data Program (a :: ProgramType) where LiteralInteger :: Integer -&gt; Program IntegerType LiteralString :: String -&gt; Program StringType -- Your fmap ModusPonens ::Program (a :-&gt; b) -&gt; Program a -&gt; Program b --- ... evaluate :: Program a -&gt; ProgramResultType a and you can sort of do that with lots of extensions but at some point writing ProgramResultType is going to get really annoying and hairy. So I'd advise you instead not to write an fmap instance and just have: ModusPonens :: Program -&gt; Program -&gt; Program
existentials?
I mention this very briefly, but basically agree with you that there's no reason to use it over the constructor in MOST cases: &gt; Well for one we can still construct NoMatch using review with this prism, which may be helpful just for consistency reasons (although I'd recommend the actual constructor in most cases), it's also useful when used with checking functions such as has, hasn't or isn't, we talk about this in the article on preview There are cases where using review makes sense, but many of them require introducing Classy Prisms, which were out of scope for this article. I hope to get to it soon though! 
Second this, Haskell for Mac is *surprisingly* under-appreciated!
I highly doubt that his actual `takesFloating` function looks like that... 
A very lowlevel example: In C you can declare an array of static size and you can also allocate an array of dynamic size. But you can not type the array of dynamic size, you must decay it to a pointer. The fact that it's C doesn't change much. It's dynamic information that we want to use to reason about static guarantees. char arrayStatic[4]; size_t size = 4; char arrayDynamic[size] = malloc(size); // Doesn't work The above would work with dependant types of course. In haskell, there are some approaches to [faking dependant types](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.2636): {-# LANGUAGE MultiParamTypeClasses #-} data Zero data Suc n = Suc n class Nat n where instance Nat Zero instance Nat n =&gt; Nat (Suc n) {-# LANGUAGE FlexibleInstances #-} class Nat n =&gt; Vect n a v where instance Vect Zero a () instance Vect n a v =&gt; Vect (Suc n) a (a, v) pretty unwieldy and you have to encode a lot with classes and instances so here is the same with data kinds: {-# LANGUAGE DataKinds #-} {-# LANGUAGE KindSignatures #-} {-# LANGUAGE GADTs #-} data Nat = S Nat | Z data Vect :: Nat -&gt; * -&gt; * where Nil :: Vect 'Z a Cons :: a -&gt; Vect n a -&gt; Vect ('S n) a zipV :: Vect n a -&gt; Vect n b -&gt; (a -&gt; b -&gt; c) -&gt; Vect n c zipV Nil Nil _ = Nil zipV (Cons a as) (Cons b bs) f = Cons (f a b) (zipV as bs f) -- rejected because the lengths are different r = zipV Nil (Cons 1 Nil) (+) 
As a newbie, I can't thank you enough for providing such an easily understandable step-by-step tutorial, this is like the best Christmas gift ever :D Of all the narratives this single one line "first and second which allows you to write code involving tuples in the pointfree style" clicked for me most! btw, I've filled in all the bindings for the functions below, mind having a look and see if I got those right? Thanks. type UTCTime = String data Image = Image { timestamp :: UTCTime , author :: String , pixels :: Pixels Int } readImage :: FilePath -&gt; IO Image readImage path = return $ Image "Jan 1" "John" (P 1) writeImage :: FilePath -&gt; Image -&gt; IO () writeImage path img = return () fromImage :: Image -&gt; (UTCTime, (String, Pixels Int)) fromImage (Image t a p) = (t, (a, p)) toImage :: (UTCTime, (String, Pixels Int)) -&gt; Image toImage (t, (a, p)) = Image t a p rotateImage90 :: Image -&gt; Image rotateImage90 = toImage . second' (second' rPixels) . fromImage rPic :: Filepath -&gt; IO () rPic path = readImage path &gt;&gt;= writeImage "fakepath" . rotateImage90 
For length, I can understand to some extent why you might want it (after all length indexed vectors are _the_ canonical example in literally every introductory tutorial :P). I was specifically looking for the example with database tables, because I hadn't heard of that before.
Think column names which are the (static) indices into the table.
Okay, fair enough. Seems overkill but maybe my mind will change after seeing them in use. :)
+100
No, ghcup is POSIX sh.
On the right-hand-side of `build-depends:` you have to give a list of packages. But you have put some modules there. To get access to the modules exposed by the local library, add the library as a dependency. In other words, let `&lt;NAME&gt;.cabal` be the name of your cabal file. Then your test suite stanza has to look like: Test-Suite test-bs type: exitcode-stdio-1.0 hs-source-dirs: test main-is: Suite.hs build-depends: base &gt;=4.11 &amp;&amp; &lt;4.12 , QuickCheck &gt;= 2.11 , &lt;NAME&gt; Moreover, if you want to import for example `BitString` in a module file, you have to do it as `import Util.BitString`.
Y'all have some good points. I rest my case until I come up with a situation that's not as easy to handle. Hopefully I won't find one ;) 
Hello :) How's the binding of "Category Theory for Programmers"? Is it good quality? I'd hate to deal with loose pages...
it should have dependent types but also linear types (no garbage collection) and lazy. Like this https://github.com/MaiaVictor/Formality
Of course, the concern isn't that anyone will forbid the writing of libraries with dependent types, but rather than social factors will lead to the loss of the community around these libraries. It's fair to say that the smaller the set of people who can productively use libraries built by the Haskell community, the less useful the language is. Saying that one could just write their own libraries is an extreme kind of reductionism. One could do so, of course, but it's hardly the way to accomplish anything interesting. It seems to some of us that nothing we've seen of Dependent Haskell so far looks like a reasonable way to write the software we're interested in writing. There's something here quite different from the past; it seems like 10 years ago, an interested young Haskell programmer could read the ICFP proceedings, and find lots of cool ideas that had immediate feasible applications. Even lens and the like, as cryptic as it can be, is applied in pragmatic ways and solves everyday problems. This doesn't seem to have that at all. So sure, it's absolutely theoretically interesting, if you're exploring the intersection of computer science and lambda calculi with mathematical logic. But being *more* practical than Coq or Agda or the singletons library is not the same thing as being practical as a general-purpose tool for non-specialists. So the concern, and I think a reasonable one, is that interoperability of this dependently typed fragment of Haskell libraries is likely to be pretty bad, and has the potential to splinter the Haskell research community too far from most of its users. Haskell has lived in a sweet spot for rather a long time. Perhaps it was inevitable that wouldn't last forever. Or perhaps that sweet spot will survive, and the loss will be minimal. We can hope. None of this is to say that the cost isn't worth paying. In the end, none of us has the power to forbid people from following their interests, anyway, and if the current generation of programming language research is there, so be it. But the cost exists.
Please consider whether you are communicating in a respectful way.
Practically speaking, the code I write, at least in Haskell, very often looks like the specification already. Sure, sometimes one must do things in a more complex way to accomplish better performance, but this is some Haskell is actually very good at. Yes, I suspect that if types are to express much more of the information about the specification of a function, then they will quickly become as complex as code, a lot of the time, and duplicate the content. Writing everything twice to check the two against each other might indeed be good for some high-assurance software. It's not appealing to me. Most of the time, if I have the option to write half the code, in a way that clearly expresses its purpose once, and test it to ensure correctness, I'll take that. This is especially true in exploratory programming, where I often want to just try something quickly and see what happens. If it doesn't do what I was expecting, I'm going to notice anyway.
Sure, I don't think highly-specified types will be great for every use case, especially exploratory programming. One more point I did not mention is that when the types are complex, they may guide a theorem prover into writing the correct "equivalent" code for you, so you do only have to write it once.
Correct by optimism :)
Oh, actually I am following Formalty *very* keenly!
I'm not sure if this is the best way to highlight the power of cartesian. Since the only cartesian used in the whole example is `-&gt;`. I would say that if anything what you wrote would be more relevant for lenses: `Cartesian` ``` rotateImage90 :: Image -&gt; Image rotateImage90 = toImage . second (second rotatePixels90) . fromImage ``` `Simple` ``` rotateImage90' :: Image -&gt; Image rotateImage90' image = image { pixels = rotatePixels90 (pixels image) } ``` `Lenses` ``` rotateImage90'' :: Image -&gt; Image rotateImage90'' = pixels %~ rotatePixels90 ``` I don't have a great `Cartesian` example of my own, as I have not really used it much myself, I have typically used `first` and `second` from `Bifunctor`. Proc notation involves it so that could be a place to start.
It doesn't seem to have held back PureScript at all. I haven't done *a lot* with PureScript, but it feels like programming Haskell +/- a couple of warts (better records, worse composition operator).
creating an incremented list by doing: [5,5..100] this does an infinite 'loop'/list on ghci, and doesn't run on repl.it (or runs but not shows the creation of the infinite list, and has to be stopped manually) any idea why? I tried to create other lists such as [2,2..20] [3,5..1000] all worked. doing [5,10..100] also works as expected. 
That's a cool insight. Btw I think you meant (Point -&gt; Ordering) and (Point -&gt; Point -&gt; Ordering). I suppose the deriving of the instances actually goes backwards (counter to the =&gt;), so the compiler sees &lt;&gt; being used with (Point -&gt; ...) arguments, knows by that it has to use the Monoid instance for functions, thereby infers that the ... has to be a Monoid. Then substitutes the &lt;&gt; definition for functions, and discovers another nested (Point -&gt; ...) layer, before it arrives at the Monoid instance for Ordering.
Is it possible to derive [TypeEquality](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Type-Equality.html#t:TestEquality)? Instances of `TypeEquality` are almost always GADTs. 
How about this naive solution: always create a `Data.Map.Lazy.Map String Value` with the full set of fields, then filter by field name. Hopefully laziness would save you from unnecessary work.
An article very well written, for all levels of Haskell experience that summarizes the problem very well. If Haskell documentation would be written at this level, Haskell would be very popular.
For sure. But on stuff like "It is impossible to have a specific dialogue open, that has contextually important information in it, while also having a separate dialogue open where you use that contextually important information" I think it is fairly clear. And yes this is one of the marvellous design decisions in VS...
How about: data User = User { ... } -- contains all the fields instance ToJSON User -- default conversion to JSON, including all fields userToJSONshort :: User -&gt; Value -- abbreviated conversion to JSON, excluding som fields instance ToJSON (User,OrderValue) -- abbreviated conversion to JSON, including OrderValue &amp;#x200B;
Are you suggesting that one should not convert API responses into Haskell records? The type that you are suggesting is "isomorphic" to `Aeson.Value`, btw.
I think there is a solution that you missed which is to use extensible records as implemented by, eg, [Vinyl](http://hackage.haskell.org/package/vinyl) or [generics-sop](http://hackage.haskell.org/package/sop-core-0.4.0.0/docs/Data-SOP-NP.html). Using this approach, if you get fancy enough, I think you would be able to tie the api arguments to the return type, such that you will get compile-time guarantees that your implementation of a specific api call does indeed generate the correct output, but that will take some fairly intricate structures to achieve. If you look at the solutions you have, I don't think there is much difference between concrete records and polymorphic records. Both of them have the problem that you will need as many record types / type synonyms as you have different responses. So as you say you would probably have to have a field that is just some untyped `Value`. As soon as you do this, you are throwing away type information. Given that, you may as well have a single data type with a bunch of maybes, because at least then it is clear what all the possibilities are, including the name of each field and the type of each field. If you want more structure than that, then I think the including sum type would be the best approach, so you would acknowledge some restriction on return types. So, for example ``` data User = User { id :: Int , name :: Text , username :: Text , more :: UserMore } data UserMore = UserMoreNone | UserMoreList { ``` So really I think it is a choice between a single record with maybes, and a fully typed solution using extensible records. 
Seems fine.
I would add as an attempt to clarify that the problem you are addressing is, specifically, that of representing the commonalities between types that are of **variable length** in terms of the number of types they reference. Without using type level lists (the extensible records approach), you are then of necessity forced to choose between either a fixed length typed structure (record or polymorphic with maybes / units) or a variable length structure that strips away types (`Value` or `HashMap`). 
&gt; the deriving of the instances actually goes backwards Not yet. A paper at ICFP this year showed it's possible for all non-overlapping instances though. --- In GP's example: * `px :: Point -&gt; Int` and `py :: Point -&gt; Int` * therefore `comparing :: (Piont -&gt; Int) -&gt; Point -&gt; Point -&gt; Ordering` * combing with the derivation `Monoid Ordering` =&gt; `Monoid (Point -&gt; Ordering)` =&gt; `Monoid (Point -&gt; (Point -&gt; Ordering))`, we get `&lt;&gt; :: (Point -&gt; Point -&gt; Ordering) -&gt; (Point -&gt; Point -&gt; Ordering) -&gt; Point -&gt; Point -&gt; Ordering` * ... which is exactly what sortBy needs. :)
How are the types which inhabit that class? From what I understand, something like \`\[\]\` won't, because its shape does not given enough information about the types of the contents. So you should only apply this to singleton types, which I guess are those whose constructors return disjoint indices. Right?
I gathered from the post that the `User` type would be separate from the underlying database type, and was actually more like a `UserApiResponse` type, but I may be mistaken. Your solution effectively skips the problem of having an intermediate type representing a response (which is definitely a viable option!) but does have the drawback that the definition of a response is scattered around various instances rather than being clearly defined anywhere. 
Something I've enjoyed at very small scales, and which I'd like to know how it scales, is `generic-lens-labels`. Just enable `DuplicateRecordFields`, and you can use `#name` as a lens for the field `name` on any record with a `Generic` instance; no template Haskell required.
Small nitpick: it is "isomorphic" only to the `Object` constructor of `Aeson.Value`.
Interesting. How does the compiler know which derivations to apply if it doesn't reason backwards? Does it just search a decision tree to some limited depth? 
Thanks, fixed.
Is using `generic-lens-labels` a good idea for a public library? Isn't it too experimental? 
I would either wait for -XExtensibleRecords or use classy lenses. Looking at vinyl blew my mind if I were to use it, it would require many or big enough project such that the cognitive overhead is worth the effort, also this puts a burden on team work. But with classy lenses and -XDuplicateRecordFields I would feel comfortable. 
Automation is indeed the major selling point of refinement types. They also have their problems though: - Expressivity: If your SMT solver du jour can't prove it, you're out of luck. With DT, you can prove most things (if you put in lots of effort). - Error messages: If your SMT solver du jour can't prove it, the error message you get is more or less, 'I couldn't prove it.' With DT, you have much better insight into exactly why the proof is failing. One interesting approach is [F*'s](https://www.fstar-lang.org/), which integrates both dependent and refinement types. In principle, that may yield a 'best of both worlds' language, but I'm not sure how the practicalities will work out.
It does *reason* backwards, it just doesn't *derive* backwards. It already knows it's looking for a `Monoid (Point -&gt; Point -&gt; Ordering)`, and it reasons backwards to find the evidence to derive that instance. Instances are required to be unique, so it matches the instance head, then if there's a context it searches for those next. For overlapping / overlappable instances (an extension), it matches the instance head, and if there are multiple matches it sees if there are any marked overlapping, if so it discards all the ones marked overlappable, then it chooses the "most specifc" instance head (although if everything is going right there's only one at this point), and uses that instance, including trying to satisfy it's context. IIRC, of course. So, it reasons that it can produce a `Monoid (Point -&gt; Point -&gt; Ordering)` if there's a `Monoid (Point -&gt; Ordering)` (instance head: `Monoid (a -&gt; b)`; context `Monoid b`), and then reasons it can produce that if there's a `Monoid Ordering` (same instance, different bindings for type variables `a` and `b`). That instance exists (with no context), so it uses it to derive the required instance. --- The ICFP paper shows that if there's a context+head like `(Ord a, Ord b) =&gt; Ord (a, b)` then and you want a `Ord a` you can get it from an `Ord (a, b)`. So, you get bi-directional derivation (and also bi-directional reasoning). The motivating example being a GADT with tuples in that wants to show parts.
&gt; to some limited depth No depth limit, IIRC. https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#instance-termination is used to prove the search will terminate. If you remove it, it is possible (though not easy, still) to send instance resolution into a loop.
The `stack` installer is 755 lines of POSIX shell script. ``` curl [https://get.haskellstack.org/](https://get.haskellstack.org/) | wc -l 755 ``` I'd rather rely on pure Haskell too, but when it comes to deployment on UNIX-like operating systems, you have two choices: a big POSIX shell script or support every package manager. 
Idris has some support for linear types, though it still does use a GC.
How do classy lenses solve the problem? Also, is `-XExtensibleRecords` a real thing?
how do SMT solvers fail, other than by taking too long to solve systems of constraints?
&gt; Writing everything twice This is certainly not the goal. Both Agda and Idris support term inference and have editor modes (and other tools) to synthesize your terms from your types.
I use approach 1 ("Create complete/concrete records"). The trick is to use `RecordWildCards` and `DuplicateRecordFields` so that you can easily slurp fields of the same name from one record into another, like this: {-# LANGUAGE DuplicateRecordFields #-} {-# LANGUAGE RecordWildCards #-} data Point2D = Point2D { x :: Double, y :: Double } data Point3D = Point3D { x :: Double, y :: Double, z :: Double } project :: Point3D -&gt; Point2D project (Point3D {..}) = Point2D {..}
I wouldn't say it's experimental; it's not likely to be changing its behavior or API in any way at any point. The only issue with it is the orphan instance, which creates incoherent instances with any other code trying to use OverloadedLabels. Luckily it's not like `Ord`; it doesn't matter if IsLabel is used inconsistently since nothing like `Data.Map` can be broken by it. It just creates a conflict with other IsLabel instances. But if that troubles you, you can use `field @"name"` instead from `generic-lens`
Hi, Thanks a lot for taking the time to answer. &gt; But can you change the type of takesFloating? I cannot, unfortunately. I want to use the `ad` package, that requires the argument's type to only be bound by a typeclass. But I'll definitely try the rest, thanks a lot!
Hi! Thanks a lot, I'll definitely try that it does look like a very neat solution. You'll also right in your other comment, I absolutely need to keep the typeclass in `takeFloating`'s signature, and cannot make it more concrete.
How does inferring code from types improve upon inferring types from code?
&gt; Not sure what you mean by "better", but in the case of dependent types, most of the times you get more precise types. And more precise types means that it is less likely two pieces of data will have the same type, which means less composition out of the box and more need for explicit coercions, in general. My experience is that Liquid Haskell avoids this tension. You write essentially the same code that you would have written in ordinary Haskell but now you are checking more properties at the type level. Specifically, you don't have the coercion/newtype clutter that most other approaches suffer from.
RankNTypes are cool, and I think the other dude was wrong to call this a beginner question - you do not seem like a beginner. :) 
You sir are a gentleman and a scholar!
There are a few possibilities: - You've provided too little information to the SMT solver, so it can't deduce the conclusion from the given premises. - The proof is beyond the capabilities of the SMT solver's logical framework. SMT solvers have become considerably more powerful, but are still limited (and have to be, as they deal only with decidable theories). - The theorem you're trying to prove doesn't actually hold. This happens more often than you'd think when developing formal proofs. ;) I participated in a workshop on F* recently. When it works, it's magic: Even moderately complex lemmas can be proven by essentially saying, 'by induction'. But when it doesn't work, you get no information and have to reverse-engineer what the SMT solver might be trying to do. Of course, F* is research-grade software, so shonkiness is to be expected, but I've yet to see a good solution to the error message problem.
No, `proc` notation (aka Arrow notation) uses the Arrow typeclass, not Cartesian. The typeclasses `Arrow`, `Bifunctor` and `Cartesian` all provide a method named `first`, and in all cases, their purpose is to transform a portion of the input. The difference between the three typeclasses is where each library puts the typeclass in a hierarchy of other typeclasses. `Arrow` specializes `Category`, `Bifunctor` specializes nothing, and `Cartesian` specializes `Profunctor`. At least, it does in the OP's snippet, where does that code come from? I couldn't find a package with that definition.
I think [that's a Hugs thing](http://goto.ucsd.edu/~rjhala/Annot/Cabal/Language-Haskell-Extension.html#v:ExtensibleRecords), so not a real answer :P And yea, I don't think classy lenses help with duplicate field names; they're just supposed to make it easy to encode how to get a given type out of other types.
FWIW, from version [1.1.0.0](https://1.1.0.0) \`generic-lens\` packages offers \`Data.Generics.Label\` module so you don't need \`generic-lens-label\` anymore. It also provides IsLabel instance for Prism's too.
And in more tricky situations, generic-lens work amazingly well.
Oh neat!
you refer to the article "Compiling Agda to Haskell with fewer coercions" as a source of the statement (unsafeCoerce destroys the performance); but that article in 5.4.3 states that "Removing most coercions, as is done in the second stage, has barely any impact on the runtime. This has not resulted in the performance improvements that were a goal of this thesis. Coercions are designed to have little impact on the runtime, by using rewrite rules that attempt to erase coercions during compilation." &amp;#x200B; i am a noob in the topic; did i misunderstand something? &amp;#x200B; &amp;#x200B;
I can't quite conceptualise how this solves the problem, would you mind expanding a little on it?
When you infer code from types you're telling it what the result should be and having it figure out what to do, rather than telling it what to do and having it figure out what the result will be. Usually people are better at knowing what result they want than knowing what steps to take.
If the problem is "having and accessing many data types with identical field names", then DuplicateRecordFields will allow such data types to exist, and `generic-lens` will allow you to access the fields with a lens.
Thanks, your statement of the problem you were solving clarified things greatly! I was struggling to work out how lenses allowed you to get around the issue of a data structure with a variable number of fields. 
I realize that profunctor and arrow are currently separate hierarchies, it’s just that morally they should be merged, as all Arrow’s are Profunctors and Strong. I just meant that for example usage of why `first` and `second` are powerful you may want to look at proc notation even if it’s technically a slightly different (but morally equivalent) function. Bifunctor is a whole different deal, polymorphic over a different part of the function. 
I thought dependent types makes type inference undecideable? Does the lack of type type inference apply to dependent types only or to the entire language? With that being said, dependently typed ORMs would be more useful as the compiler would be able to recognize a list of size 6 versus a list of size 7, which is useful when distinguishing between VARCHAR(6) and VARCHAR(7).
Perhaps the problem is not with dependent types but rather with the fact that they are still not fully supported and so we have to jump though hoops by for example using singletons in order to get the same results?
Could you elaborate on that?
I'm pretty sure I've been completely respectful. I haven't attacked anyone - only their arguments. Even my FUD comments are explained pretty thoroughly IMO. [Please go look at the comment I was ultimately responding to:](https://www.reddit.com/r/haskell/comments/a710ix/comment/ebzse5d) * It 
&gt; Agreed, this is the classic tagged union (no subtyping) vs union (yes subtyping) "debate" I'm firmly on the tagged union side. The only thing I've ever had untagged unions increase is the defect count.
I've been writing a servant-like library for command line interfaces and I'm having a strange problem where it seems the context is missing a constraint which I did put in there. &amp;#x200B; `{-# LANGUAGE OverloadedStrings, TypeInType, DataKinds, TypeOperators, TypeFamilies, ScopedTypeVariables, FlexibleInstances, FlexibleContexts #-}` `module Commander` `(` `) where` `import GHC.TypeLits` `import Data.Kind` `import Data.Proxy` `data (a :: k) :&gt; b` `infixr 9 :&gt;` `class HasProgram prog where` `type Program prog :: Type` `program :: Proxy prog -&gt; Program prog -&gt; [String] -&gt; Maybe (IO ())` `instance (KnownSymbol sym, HasProgram prog) =&gt; HasProgram (sym :&gt; prog) where` `type Program (sym :&gt; prog) = Program prog` `program _ p (x : xs) = if (x == symbolVal (Proxy :: Proxy sym))` `then program (Proxy :: Proxy prog) p xs` `else Nothing` `instance HasProgram (HasProgram p, Read t) =&gt; HasProgram (Arg t :&gt; p) where` `type Program (Arg t :&gt; p) = t -&gt; Program p` `program _ p (x : xs) = program (Proxy :: Proxy p) (p (read x)) xs` &amp;#x200B; I'm getting an error telling me to add Read t to the context of the final HasProgram instance, as its saying that it can't deduce it, but its right there... Anything I'm missing here? I'm leaving out some other code which isn't necessary for understanding this particular issue.
From the hackage: &gt; however orphan instances are inherently problematic. Please do not depend on this package from any hackage libraries. That's a no.
True, refinement types are a great way to have more static guaratees without sacrificing ergonomics. In a sense, they are morally equivalent to those dependent types for which the compiler can figure out what coercions to put and where by itself.
I agree; you were and are still being respectful. I think it was primarily a friendly reminder before you stepped over the line. Your tone was getting very accusatory on very little evidence, and was close to a claim of mind-reading.
I agree; you can have lazy evaluation paired with totality/productivity so I don't think we really have to give up inductive reasoning. In the current state of Haskell, inductive reasoning isn't correct because recursive data types are necessarily well-founded. data Nat = Z | S Nat aleph_0 : Nat aleph_0 = S aleph_0
Good catch. I suppose I would've noticed this if I read the paper past 2.4 which states that: &gt; By adding a coercion at nearly every AST node, GHC struggles to optimize the program. Even though the backend tries to inline coercions by using an optimized version of unsafeCoerce, GHC has to be very pessimistic because it has very little type information. I've heard about the `unsafeCoerce` issue some years ago, and this was the first decent PDF I found that confirmed my prior knowledge. * Maybe GHC got better at optimization since then. * Maybe the code examples that the author uses to test his results fail at exposing the issue (and it would show up in larger programs). * Maybe merely removing `unsafeCoerce` is necessary but insufficient to see a significant performance improvement. * Maybe I am operating under false assumptions?
Yes, instances will be something like this: {-# LANGUAGE GADTs, DataKinds, KindSignatures #-} data Nat = Z | S Nat data SNat (n :: Nat) where SZ :: SNat Z SS :: SNat n -&gt; SNat (S n) instance TestEquality SNat where testEquality SZ SZ = Just Refl testEquality (SS m') (SS n') = case testEquality m' n' of Just Refl -&gt; Just Refl Nothing -&gt; Nothing testEquality _ _ = Nothing
It starts with the fact that Haskell does not have proper inductive types. Every type is, additionally to what you put in, inhabited by the bottom (ie non-termination). In a strict language non-termination would be an effect, but in a lazy language it is a value. The fact that it can pop-up everywhere and anywhere as a valid value makes inductive reasoning broken. If you then say, OK let's skip computer verifiable inductive proofs ala Coq, Agda, etc., and focus on the real world usage where you do the reasoning in your head (and pen &amp; paper) you are hit with complicated cost dynamic, both in time and space. See https://existentialtype.wordpress.com/2011/04/24/the-real-point-of-laziness/ for more details. Next, in theory, Haskell as a lazy language gains on the side of equational reasoning, alas lazy IO breaks that one as well: http://okmij.org/ftp/Haskell/#lazyIO-not-True Please note that I think Haskell is a fine language, doing far better than others. I'm only answering your question.
Thank you.
Just prefix the fields with the name of the data type and you're done. You should not expect field names to be as short as they are in OO languages because they don't have the same scope. In Java, field names are scoped to a particular class. In Haskell, field names have global scope. Name length should reflect this. It's exactly analogous to how when you make a local phone call in the U.S. you use a 7-digit number and when making a long-distance call you use a 10-digit number. It is a fundamental law of nature that name length must be roughly proportional to the log of the the size of the scope or you will start to have collisions.
&gt; I thought dependent types makes type inference undecideable? Does the lack of type type inference apply to dependent types only or to the entire language? One of the necessary conditions for Dependent Haskell to be accepted into GHC is that all (known) expressions that can currently have their type inferred without it can still have their type inferred with no (additional) type annotations. The current work on the DT inference strategy will never *generate* a Pi type, but it will *propagate* Pi types from your annotations both inward and outward. So, there will be some Dependent Haskell expressions that require type annotations, at the very least some explicit Pi typing. But, it shouldn't break any non-DT code, at least for now. It is unknown whether this is the limit of the DT type inference (along the "decidable frontier" (in analogy to the "Pareto frontier"); that is unlikely though), but it seems to be an acceptable sweet spot for all involved. But, yes, full DT inference in general is undecidable. In fact, even Rank-3 (parametrically polymorphic) types can't always be decided unless you add a rank-reduction pass somewhere (if you can get everything down to Rank-2, it is decidable, albeit slow). Also, H-M has some small examples that nonetheless illustrate exponential behavior, so type inference is sticky all of the place.
&gt; `instance HasProgram (HasProgram p, Read t) =&gt; HasProgram (Arg t :&gt; p)` I think you have an extra `HasProgram ` there at the beginning (before the first paren). I'm surprised the type/kind checker didn't complain about it, though I can see why it passed parsing.
Thank you! 
I'm not sure that the primary problem here is the naming issue, though I seem to be in the minority. The issue runs deeper, because OP wants a way to represent, at the type level, an api response that could be any run-time-specified collection of fields.
I like this in a way, but it doesn't address the issue of having an unmanageably large number of record types if you want to have a separate concrete type for every possible api response.
Vinyl isn't that bad! It might help to think of Vinyl recs as just typed hashmaps.
One of the MS Research offices is in Cambridge, UK. And it's where Simon Peyton Jones is, but I wouldn't say that's where most GHC work is done. Most GHC work is done by a small handful of people, globally distributed, only two of which are in Cambridge.
Sounds almost like dependent typing. ;)
&gt; In Haskell, Functors are Monads I guess you mean the other way around?
You can probably fix this at the aeson level. You have a Maybe UserExtra field in User, but serialization merges the two into one json object. That should be straight forward to generalize.
I mostly go with trees-that-grow. This is basically the same as polymorphic records except that you parametrize over a single phase type and use type families to map that to actual types. Gives more friendly type errors and can be hidden with type synonyms but a huge scale it gives bad compile times.
&gt; a way to represent, at the type level, an api response that could be any run-time-specified collection of fields. I think that desire will result in something way more complex than is likely necessary.
All done in ghci, using `GHC.Generics`. :set -XDeriveGeneric -XAllowAmbiguousTypes -XFlexibleInstances -XFlexibleContexts -XUndecidableInstances -XScopedTypeVariables -XTypeApplications -XDataKinds -XTypeFamilies -XTypeOperators import GHC.Generics For a concrete example, let's take a simple class that counts the `Int` fields in a datatype. class CountInt a where countInt :: Int Some test cases: data T = T Int Int deriving Generic -- two Int fields data S = C T | D Int deriving Generic -- one Int field In particular, `deriving Generic` defines instances for the `Rep` type family. `Rep T` looks like this, hiding all the `MetaFoo` inside `_` for sanity: :kind! Rep T Rep T :: * -&gt; * = D1 _ (C1 _ (S1 _ (Rec0 Int) :*: S1 _ (Rec0 Int))) ^^^^^^^^^^^^^^^^ second field ^^^^^^^^^^^^^^^^ first field We have a type-level representation of the definition of the type `T`. Similarly for `S`: :kind! Rep S Rep S :: * -&gt; * = D1 _ (C1 _ (S1 _ (Rec0 Int)) :+: C1 _ (S1 (Rec0 Int))) To work on those representations we can define another class: class GCountInt (f :: * -&gt; *) where gcountInt :: Int It has instances for the various building blocks of `Rep` type instances. `D1`, `C1`, `S1` are synonyms for `M1 D`, `M1 C`, `M1 S`, which is just a newtype to carry metadata at various levels (type names, constructor names, field names, strictness annotations, etc.). We're counting fields here, so we only need to unwrap the newtype, ignoring that extra information. instance GCountInt f =&gt; GCountInt (M1 _i _c f) where gcountInt = gcountInt @f Constructors are separated by a sum operator `(:+:)`, and we add up the counts. Similarly, fields are separated by a product operator `(:*:)`. instance (GCountInt f, GCountInt g) =&gt; GCountInt (f :+: g) where gcountInt = gcountInt @f + gcountInt @g instance (GCountInt f, GCountInt g) =&gt; GCountInt (f :*: g) where gcountInt = gcountInt @f + gcountInt @g Finally, fields are wrapped in `Rec0`, which is a synonym for `K1 R`. We'll let `CountInt` handle the field directly. instance CountInt a =&gt; GCountInt (K1 r a) where gcountInt = countInt @a Now to tie things up, we need to declare instances for `CountInt`. One for the "base case" `Int`, and one for generic types. We need an annotation to confirm to GHC that the overlap is intentional, and that the second instance doesn't apply to `Int`. instance {-# OVERLAPPING #-} CountInt Int where countInt = 1 instance GCountInt (Rep a) =&gt; CountInt a where countInt = gcountInt @(Rep a) And here we are. countInt @T -- 2 countInt @S -- 3 
SMT solvers are not limited to decidable theories. It's often precisely when you veer into undecidable territory that SMT solvers become unpredictable and unhelpful when they fail.
Or, as I find endlessly fun to point out, older than Java - 1990, vs. 1995. Amusingly, a great deal of Haskell's syntax has been around even longer, since it borrows heavily from Miranda, released in 1985.
Fair warning to newer users listening in : Recursion schemes are a hell of a rabbit hole. Neat as hell, but mind bending, frustrating, and artificially limiting. Also a source of surprise strictness for the uninitiated, and the resulting code is weird as hell and unmaintanable to those not in the know. Verdict - Do not engage with this concept unless you have disposable time (probably measured in days) to throw at digesting it, or unless you're already unusually comfortable with Haskell's type system and the concept of fix points.
Agreed, but it would be super nice and type safe! Personally I would probably go for a whole bunch of Maybes in a single adt unless I had the time to put a package together with a fully typed solution.
Stack does more than ghcup. So a bootstrap procedure for the actual binary makes sense. For ghcup, it doesn't. The actual logic to get the right GHC version is encoded in our meta files. It is very little code in fact. Switching language has pretty much no benefit, but now puts the burden of doing that portably on us. In addition, the stack install script does things we will never do, such as: try_install_pkgs() { if has_apt_get ; then apt_get_install_pkgs "$@" elif has_dnf ; then dnf_install_pkgs "$@" elif has_yum ; then yum_install_pkgs "$@" elif has_apk ; then apk_install_pkgs "$@" else return 1 fi } So ghcup is not about doing everything for you or being a complete solution. It follows the unix philosophy and tries to be non-interactive. POSIX sh is the go-to language for that, unless your program logic is actually complicated.
groupBy! Thank you
All Arrows are Profunctors? _Composing on the left with `arr`... composing on the right with `arr`... checking the laws..._ Huh, you're right! In that case, I feel less bad for using `(.)` instead of `dimap` in `rotateImage90` :) 
Wait, I didn't understand what you said when I wrote this. Constructors do not need to return disjoint indices. This can be an instance of `TestEquality` but not disjoint data NotSingleton (n :: Nat) where ZeroA :: NotSingleton Z ZeroB :: NotSingleton Z Succ :: NotSingeleton n -&gt; NotSingleton (S n) This has constructors with disjoint return types but can't be an instance of `TestEquality` data ListOrSet (a :: Type) where ItWasList :: [a] -&gt; ListOrSet [a] ItWasSet :: Set a -&gt; ListOrSet (Set a)
`FromJSON` is the real problem - not `ToJSON`
&gt; artificially limiting How so? Haskell's recursion-schemes doesn't enforce any of the totality / productivity that might be required in a more *well-founded* library. There are definitely nested (or non-uniformly recursive) types that it simply doesn't handle, but that's because their universal folds are significantly more complex. &gt; surprise strictness Maybe it's changed since I contributed, but I don't remember *any* calls to `seq` or strictness annotations, so it's only as strict as the types indicate. --- All that said, recursion-schemes *isn't* as easy to get a handle on as things like `Foldable` and `Traversable`, and so it does require some time investment to get the full value out of, and I certainly wouldn't encourage it's use by anyone that isn't comfortable with basic type families (the error messages get weird).
'Surprise' is a loaded term, I'll grant. Mostly it's just that the traversal is different enough to think about that your instincts and intuitions, especially about infinite structures, end up getting 'betrayed.' And by 'limiting' I mean you get 'trapped.' If you get 2/3rds of the way through modeling a problem as a recursion scheme and realize you made a bad assumption, you might as well start over from scratch, because nothing you've written will apply to any other paradigm. This is not a normal experience in Haskell, I'm used to getting huge changes in functionality for little effort, and highly reusable code that's easy to build an abstraction on top of with minimal adjustment. I don't mean to trash talk recursion schemes. It's a legitimate tool, and there are awesome advantages to separating the concern of traversal. But it's not easier to work with, or think about, it's far from beginner friendly, and there is very nearly always a way to get what you need in a way that is easier to think about, less alienating, and more portable. It's the sort of solution you might try applying to a problem you already solved, or mostly solved, with another method, and then realized it would be a good fit for. I would recommend it as a tool to add to one's tool belt when they're feeling more comfortable with Haskell, and have the time to play around a bit. But it's newbie hostile and a terrible tool for prototyping or exploratory modeling.
Looking forward to see your library in action after it's releaseed! I have minor request, if you don't mind :) Feel free to ignore. One difficult thing I've found hard to with `optparse-applicative` is mixing of commands and arguments. For example, I'm working on [`summoner`](https://github.com/kowainik/summoner) which allows to scaffold Haskell projects easily and its CLI looks like this (couple examples): summon new my-project summon new my-project --ignore-config with --test --benchmmark without --github You can see that `new` is kinda _main_ command with its own options and it has several subcommands: `with` and `without`. This is implemented in a such way to provide more convenient interface. But it was quite hard to implement and it doesn't work smoothly (`--help` option doesn't work for `with/without` command). Also, because of this API I can't make project name argument optional... It would be great if your library could somehow allow more explicit control over commands, subcommands, arguments and subarguments :)
If Aeson had an easy way to do "mixins" (like Beam does) this would be much easier. You'd still have the same number of record but without duplication and your JSON would still be flat.
Even better, just derive \`Generic\` on your records and then let the client library use labels if they want to. Just be sure not to use the orphan internally.
Yea that's nice but the internal code will often want to use accessors itself, I assume
What are some approachable resources for writing performant Haskell? Particularly when mutability is necessary? 
Don't tell anyone else or use this in production code but... {-# LANGUAGE ImplicitParams #-} {-# LANGUAGE ConstraintKinds #-} {-# LANGUAGE RankNTypes #-} type Record c = (forall a. (c =&gt; a) -&gt; a) type Person = Record (?name :: String, ?id :: Int) bob :: Person bob = (\x -&gt; let { ?name = "Bob Testerson" ; ?id = 3 } in x) displayPerson :: Person -&gt; IO () displayPerson person = do putStrLn ("Name: " ++ person ?name) putStrLn ("Id: " ++ (show (person ?id))) main :: IO () main = displayPerson bob 
&gt; I think there is a solution that you missed which is to use extensible records as implemented by, eg, Vinyl or generics-sop. (Edit: this may be analagous to what Beam is doing in some places (Edit2: Actually I don't think it is)) I look at Vinyl and generics-sop and am mortally scared of the type-level magic that is going on there. Having to resort to such stuff to write a REST API client doesn't seem like it's worth the cost-benefit ratio. Btw, what happened to [`superrecord`](http://hackage.haskell.org/package/superrecord) ? Are people adopting that for production projects? &gt; But I think this only makes sense if the consumer of the api is also using Haskell or something with a similar type system. Otherwise it is just another way of nesting a bunch of if/case statements. The context is writing REST API **clients** for public/popular APIs like Quickbooks, Intercom, Sendgrid, etc. &gt; As for the trade-off between these - the single record is simple, relatively maintainable, and gives some compiler reassurances, but is not modular and outsources the question of which calls generate which outputs purely to human written documentation. Yes, I think given the options, using a record with a super-set of fields, with a bunch of `Maybe`s is the most pragmatic solution. Anyways, the source of truth is the remote API, which we have no control over. Users of the Haskell API will anyways need to deal with errors thrown by the remote API. It would be best to not introduce any more json serialising / de-serialising errors because we got the JSON shape wrong.
IIRC it has never had a formal specification. 
I don't understand the bottom distinction, application can still bottom in a strict language. What do you mean by effect? That example does not break referential transparency as far as I know, even though it's operation is ... subtle.
You can enable multiline input in GHCi with: `:set +m`
This will make my `Migrate` instances (`safecopy`) so much nicer!
Oh, I must have misunderstood that. Sorry, and thanks for the clarification.
I like to add an option I haven't seen mentioned, but which I have used myself in similar situations. data User f = User { id :: UserID , name :: Text , username :: Text , last_signin_at :: UTCTime , created_at :: f UTCTime , updated_at :: f UTCTime , default_shipping_address :: f Text , default_billing_address :: f Text } type FullUser = User Identity type AbbrevUser = User Proxy data UserOrderValue = UserOrderValue { user :: AbbrevUser , orderValue :: OrderValue } But as the number of different `User`\-variants increase, this would become less and less comfortable.
Folder structure is a lot more straight forward in functional programming. Modules should be grouped around the type they operate on. So a module should contain a type and the transformations of that type to another type. Say you have a module User, it should define record `data User = makeUser {..}`. Then every exposed function should contain functions that operate on the User type. For example it could contain a database operation `get :: Key User -&gt; IO User`. 
That's a great explanation, thanks a lot! And preserving sanity is important, thanks for those underscores, too =) I have a further question - is there a mechanism that would achieve the same but for recursive types? Say, I have \`data Rec = R Int Rec | Stop deriving Generic\`, the computation \`countInt @Rec\` obviously does not terminate. In this case, however, we encounter \`Int\` only once (from the point of view "how many times we see Int as type parameter"). If there were some sort of state for this recursion over types, I could imagine a solution "mark current type as being processed =&gt; iterate recursively over type parameters; if we encounter a type marked as processed, return a placeholder value for that type =&gt; return what we got; mark current type as already processed". E.g. if we have types \`data Rec1 = Rec1 Int Rec2 | Stop1 deriving Generic\` and \`data Rec2 = Rec2 Int Rec1 | Stop2 deriving Generic\`, it follows as 1. \`countInt @Rec1\` 2. mark Rec1 as being processed 3. accumulate 1 from \`Int\` in \`Rec1 Int Rec2\` 4. mark Rec2 as being processed 5. accumulate 1 from \`Int\` in \`Rec2 Int Rec1\` 6. Rec1 is being processed, accumulate 0 7. Stop2 has no ints, accumulate 0 8. mark Rec2 as processed 9. accumulate 0 from Stop1 10. mark Rec1 as processed 11. output 1+1+0+0+0=2 Is this recursion feasible? &amp;#x200B;
While tend to agree, but I don't think it is specific to functional programming. I usually try to group things by feature first, it leads do better organisation of code base, even for OOP.
You are right. For me the revelation just came from working with functional code but I did figure it would translate well in OOP too.
I wonder if anybody could make comments like this for [project-m36](https://github.com/agentm/project-m36)? A Haskell-native purely relational database.
Link to Meyer's paper: http://se.ethz.ch/~meyer/publications/functional/functional.pdf
What is the problem?
One way to do this is to start by typing `:{` on one line. This starts an 'area' (for lack of a better word) in which you can type multiline functions; you can finish it by typing `:}` (on a separate line). The other approach that I use is to simply write it on one line; in this case, that would be `doubleSmallNumber x = if x &gt; 100 then x else x*2`.
It seems like the use-cases are a bit different. In a multi-node setup, as project-m36 seems to cater to, being resilient to Ctrl-C for example is really optional. I have no idea about the other issues though.
Various bits and pieces of GHC are formally specified (in papers, there's also a formalism for Core in the source), but we don't have any formal verification or checking against the specs. Also no defect list.
Do the canonical database systems like `Persistent` fail to meet some ACID guarantees? If they are suitable for production then I fail to see an upside to using anything else.
The people describing these things as "bugs" are being a little hyperbolic. They are *contentious points* in the design and implementation of the language. Orphaned instances, as most of the pragmatic StackOverflow answers suggested, are *sometimes* the best way to achieve something. Given that there's no way to outlaw instances from imports (yet), this means that you sometimes get lumbered with instances you didn't want if some library author decided to write them "on your behalf". Other people were complaining about the idea that "people could change their instances and your program could start crashing or doing other strange things at runtime". This is a bit of a silly argument, since they could just as easily replace their library function implementations with `undefined` and get the same outcome. It's just an arguable point in the language design and people like to paint it as a "bug" in the hopes of turning unknowing people against it. There is a large amount of specification of how the language should behave, which is given in the [Haskell Report](https://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1270011). It describes, for instance, the semantics of how Float and Double work for enumFromTo. Unfortunately the machinery of floating-point numbers means that it's hard to keep the spec simple *and* have it do precisely what you'd expect in all situations. *** Regardless of my answers above, the answer to the question "if it is formally specified, then why does it have bugs?" is that people are human and make mistakes. Big mistakes get corrected but small ones are often not worth the effort to fix. This applies to all things everywhere, not only Haskell.
My hot tip: Functions should do the *bare minimum* that they need to do. This means your functions will be one line most of the time. More complex functions are harder to read, and harder to compose. The functional paradigm revolves around the composition of functions. If your functions are hard to compose then it will only become harder to think functionally. A good litmus test is: "can I name this function with exactly what it does?" For example, you can probably find a name for this function: ``` _____ :: Int -&gt; Int_____ x = x * 2 ```
Quick note: hPutStrLn is not defined in those places. It is defined in GHC.IO.Handle.Text, and re-exported from both of those modules (GHC.IO.Handle, System.IO) My understanding of how base is organised is rough, but as follows: GHC.IO.*: IO Event Manager and a bunch of IO-related functions. A lot is re-exported from System.IO.*. Depends on some of the concurrency stuff in Control.Concurrent.*. Control.Concurrent.*: concurrency stuff. Depends on the IO manager from GHC.IO.*. (base has many cyclic dependencies) Foreign.*: a bunch of stuff dealing with pointers/memory addressable content/ffi Text.Read.*, Text.ParserCombinators.*, Text.Show.*: some extra utilities for Show/Read Text.Printf: C-style printf Unsafe.Coerce: unsafeCoerce (type system backdoor) System.*: a bunch of stuff programmers might need for dealing with the system externally, such as doing basic IO, getting environment variables, cpu time, a timeout function, and StableNames/Weak Pointers. Type.Reflection.*: GHC's Typeable machinery In addition, note that a lot of modules in base are just re-exports of other things. From a note at the top of GHC.Base we can see how most of the Prelude is built up (though this note is missing a few things i think, hasn't been updated in a while): The overall structure of the GHC Prelude is a bit tricky. a) We want to avoid "orphan modules", i.e. ones with instance decls that don't belong either to a tycon or a class defined in the same module b) We want to avoid giant modules So the rough structure is as follows, in (linearised) dependency order GHC.Prim - Has no implementation. It defines built-in things, and by importing it you bring them into scope. The source file is GHC.Prim.hi-boot, which is just copied to make GHC.Prim.hi GHC.Base - Classes: Eq, Ord, Functor, Monad Types: list, (), Int, Bool, Ordering, Char, String Data.Tuple - Types: tuples, plus instances for GHC.Base classes GHC.Show - Class: Show, plus instances for GHC.Base/GHC.Tup types GHC.Enum - Class: Enum, plus instances for GHC.Base/GHC.Tup types Data.Maybe - Type: Maybe, plus instances for GHC.Base classes GHC.List - List functions GHC.Num - Class: Num, plus instances for Int Type: Integer, plus instances for all classes so far (Eq, Ord, Num, Show) Integer is needed here because it is mentioned in the signature of 'fromInteger' in class Num GHC.Real - Classes: Real, Integral, Fractional, RealFrac plus instances for Int, Integer Types: Ratio, Rational plus instances for classes so far Rational is needed here because it is mentioned in the signature of 'toRational' in class Real GHC.ST - The ST monad, instances and a few helper functions Ix Classes: Ix, plus instances for Int, Bool, Char, Integer, Ordering, tuples GHC.Arr - Types: Array, MutableArray, MutableVar Arrays are used by a function in GHC.Float GHC.Float - Classes: Floating, RealFloat Types: Float, Double, plus instances of all classes so far This module contains everything to do with floating point. It is a big module (900 lines) With a bit of luck, many modules can be compiled without ever reading GHC.Float.hi u/andrewthad and i have discussed the possibility that one could have a sort of "base's base", called pico, that is essentially rts/ghc-prim/ghc.base, and then split a lot of stuff out, building on top of it. Decoupling things in base is hard, and has been tackled before (i think u/nomeata tried, but i rememberwhen looking at his attempti thought everything was too modular and perhaps overly ambitious because of it). Lots of grossly entangled parts! My intuition is that it might be possible to split off the Event Manager (GHC.IO.*) and the concurrency modules into their own packages, and have base re-export them for a window of time. Same with a lot of the one-offs like Text.Printf. these could use the newer re-export feature of cabal that allows you to re-export a module from a package where it did not originate, as its same name.
&gt;Thanks for your accurate and pretty fast reading. &gt; &gt;True, in Haskell, some Functors are not Monads... Of course... In my paper, I wanted to give the feeling that when we understand the use of \[\] and Maybe, the work is almost done for Monads. So let me change the title by : "In Haskell, most Functors are also Monads" and give the clue with :info \[\] and :info Maybe asserting the fact that they are also Monads. &amp;#x200B;
I believe the upside is that you get to preserve your algebra without converting the datatypes to a more relational-database friendly representation.
Then you're going to be constantly unwrapping / rewrapping the Maybes for all those fields. Yes, Haskell gives you good tools to do this, but it's still a significant cost. Having separate data types for the results of the different kinds of queries is a much more straightforward and simple solution in my experience. If you really need a bunch of different combinations of fields being absent and present, then perhaps row types would benefit you, but again, in my experience that's usually not the case.
In a strict language, non-termination is an effect which can be likened to IO gone badly wrong :-) You simply get stuck in runtime, for one reason or another. With bottom in lazy language, you are additionally infecting the logic of the language. It is not consistent. Bottom becomes a normal value very much like 3::Nat.
Ah, makes sense. I suspect that's a price that must be paid for "production ready" software at the moment.
&gt; most Functors are also Monads That will cause more confusion than it will eliminate.
First thing I read after opening is this: &gt; A combinator describes a specific way of building a new mechanism &gt; from existing ones. The combination is defined in a rigid way: a &gt; take combination (as in take 3 apple) associates one quantity &gt; element and one food element. As noted earlier, this is the math- &gt; ematical equivalent of defining a structure by its exact &gt; implementation. Forgive my naïvety, but this sounds like such a complete misreading of polymorphism that it's hard to believe it came from the person who damn near coined the term.
 They are both concrete universalities, there is no neutral stand point, the totality of programming appears to you differently if you're an OOP guy or a FP guy, so, of course, the battle is a false one. A proper thing would be a new form, a better one, and I think we already have this new way. The origin can be found in how Swierstra adressed Wadler's "expression problem" in "Data types a la carte" and in Oleg Kiselyov, Hiromi Ishii "Freer Monads, More Extensible Effects". But this new particular universality opened the space for a superior approach, one that overcomes the limitations of FP and OOP through what John De Goes calls "algebraic programming" in his criticism of FP 
&gt; I agree about the totality checker […] What, if anything, prevents us from adding a new kind of inhabited types to Haskell, whose inhabitants do not include bottom, and which require a totality proof to compile?
You can already do that using `ExistentialQuantification`.
`ghc-8.6.2` is a *tiny* resolver that only includes a few things necessary to work with GHC. The `lts` and `nightly` have significantly more stuff in them. `nightly` resolves are using GHC 8.6.3, so you can just use one of those instead if you want the newest GHC version.
&gt; He addresses two functional programming papers in pretty much detail and comes to the conclusion that the features that set functional programming apart can be subsumed in object oriented designs. Subsumption is not necessarily a feature. For example, structured programming (i.e. `while` and `for` loops) is subsumed by `goto`, but most modern languages disallow `goto` because it leads to unmaintainable code. Sometimes the absence of a feature is itself a feature. You can think of functional programming as forbidding shared mutable state for the same reason that programming languages disallow `goto`: shared mutable state leads to unmaintainable code and the absence of shared mutable state is a feature.
&gt; stack does more than ghcup. So a bootstrap procedure for the actual binary makes sense. For ghcup, it doesn't. I don't see how this follows. It makes sense for both stack installer and ghcup to be in POSIX. gchup is just a binary installer, nothing more. It is fairer to compare it to the stack installer than stack. Indeed, I'm sure it could install stack too, removing the need for a standalone stack installer.
I understood from the post that having variable fields was a crucial aspect of the api. So, say, there are 30 possible fields, and each one of them could be requested or not. And it turns out that OP was actually talking about a client for existing apis, which do offer that functionality. So the question is how to expose that in Haskell, not whether such an api is a good idea in the first place (personally I agree with you about that!). There are only three alternatives for something that is variable (but constrained) in both number and type (I think). Generally typed structures like `Value`, fully typed structures with absence (Maybes) or some kind of row type / heterogenous list / dependent types.
Apparently, the entire treatment of referential transparency is "all serious programs written in Eiffel follow a command-query separation model". Frankly, this is the kind of argument I expect from kids who just learned PHP, not one of the supposedly leading lights of the software engineering field. What in this paper actually merits response?
&gt; "In Haskell, most Functors are also Monads" Which is fine, but save it for introducing Monad. You don't introduce functions by showing how they're Profunctors after all.
That seems feasible, though quite more involved. You can simulate state in type class resolution by adding extra arguments. Here, `s0` is the initial state, `s1` is the final state, that will get instantiated during the instance resolution process. class CountInt (s0 :: [*]) (s1 :: [*]) a where countInt :: Int Information from `s0` flows into `s1` at the leaves of the search, for example when encountering primitive types: instance {-# OVERLAPPING #-} (s0 ~ s1) =&gt; CountInt s0 s1 Int where countInt = 1 Otherwise the first thing is to check whether `a` is in `s0`, and you can use another type class to dispatch on the result of that check: instance CountIntIf (IsIn a s0) s0 s1 a =&gt; CountInt s0 s1 a where countInt = countIntIf @(IsIn a s0) @s0 @s1 @a class CountIntIf aInS0 s0 s1 a where countIntIf :: Int When the type `a` is already in `s0`, then the search returns, and the final state is `s0`. We want unification of `s0` and `s1` to happen *after* we've decided whether `a` is in `s0`, so the equality constraint is in the context of the instance (instead of `CountIntIf 'True s0 s0 a`, which requires the equality between `s0` and `s1` to be known beforehand). instance (s0 ~ s1) =&gt; CountIntIf 'True s0 s1 a where countIntIf = 0 Otherwise we add `a` to the list of seen types, and start exploring its representation. instance (GCountInt (a ': s0) s1 a) =&gt; CountIntIf 'False s0 s1 a where countIntIf = gcountIntIf @s0 @s1 @(Rep a)
&gt; The context is writing REST API clients for public/popular APIs like Quickbooks, Intercom, Sendgrid, etc. You might look at what [Amazonka](http://hackage.haskell.org/package/amazonka-core) and the [Gogol](http://hackage.haskell.org/package/gogol) are doing.
... a new Core language/logic.
&gt; The context is writing REST API **clients** Haha sorry! I read through the post very carefully and more than once, but somehow missed that crucial word! I will console myself with the fact that, judging from the other responses, I am not the only one to have made that mistake. So I get why people are hesitant to dive into type level programming, there is definitely a pretty steep learning curve, and the documentation around a lot of these things doesn't help enough when getting started. I only began to feel at all comfortable with it after some very frustrating time spent playing around and looking at source code - it is definitely not a 'pick up and use' solution. Personally I like it though, and I think it is the right direction to go in longer term. I think saying things like "Having to resort to" is a bit like how I felt when first learning Haskell and realising that I had to define a bunch of data types to do much. I was used to generic structures (like JS objects and lists), and when I couldn't do what I wanted with those I felt like "this is ridiculous, why do I have to resort to making this complicated algebraic data type (whatever that means) just to do something that is so simple in JS". And yet... that is actually the great power and attraction of Haskell. I would say the same thing about something like Vinyl or generics-sop, this is not a "having to resort to", it is a "taking fullest advantage of the awesome capabilities of our language and compiler to make something that is robust in a way that few other languages could ever match". I don't mean to say that I think you should use Vinyl or whatever. In fact, given that you are writing an api **client** you almost certainly should not if you want many people to use it! The ecosystem around stuff like this just isn't there yet. And finally, as a potential future user, I think you are right - I wouldn't introduce any more structure than the underlying api itself uses without a good reason for it. I wouldn't look on having a big record with a bunch of Maybes as anything like a weakness of Haskell though, it is merely reflecting the reality of the api. I think it sometimes seems like these things are more awkward in Haskell because we are often forced to acknowledge reality explicitly in our types, whereas in fact all that is happening is that we are handling uncertainty clearly and upfront rather than hiding it everywhere in the program.
I'm not sure lazyIO is the best example, since it's not possible to do with a truly opaque / native `IO`.
On superrecord, hopefully other people know more, but I have used Vinyl in the past simply because it provides some functionality (like curry and uncurry) that I didn't find elsewhere. And since Vinyl introduced `ARec` the main issue that the author of superrecords [had with it](https://www.athiemann.net/2017/07/02/superrecord.html) (that accessing fields was O(n) rather than O(1)) has been resolved. 
From what I can tell, Amazonka is using the Maybe approach with lenses, for example: ``` data ListSkills = ListSkills' { _lsSkillGroupARN :: !(Maybe Text) , _lsNextToken :: !(Maybe Text) , _lsMaxResults :: !(Maybe Nat) } deriving (Eq, Read, Show, Data, Typeable, Generic) ```
I think this is similar to what [Beam does](https://tathougies.github.io/beam/tutorials/tutorial1/). Can definitely be a useful pattern!
I like the idea of project M36 a lot, and I believe it deserves serious attention, but when I reviewed it a few months ago it seemed quite far from production ready. The code used `error` liberally (rather than using something like `Either`), which I didn't like. But more importantly, the performance was... not suitable for my intended use case, let me say. You can see my issue [on the repo](https://github.com/agentm/project-m36/issues/210), and I'll quote a little here: &gt; My test first inserts 100k records in one big batch and then tries inserting records one by one. The simplest case defines no constraints on the data at all. The second case defines just a single unique key (inclusion dependency) for the relvar. &gt; No constraints single insert: ~1s &gt; Single unique key inclusion dependency: ~7s This was when running without any persistence, so no disk IO. At that time the persistence layer was also very poor, but I believe that may have been fixed now. 
No. Some bits have papers about them. Disregarding GHC extensions, we have the report, which documents a lot of the reductions and desugaring, but nothing operational. If you want something formal specified from top to bottom, best to go with CakeML (or possibly CompCert).
&gt; And by 'limiting' I mean you get 'trapped.' If you get 2/3rds of the way through modeling a problem as a recursion scheme and realize you made a bad assumption, you might as well start over from scratch, because nothing you've written will apply to any other paradigm. This is not a normal experience in Haskell, I'm used to getting huge changes in functionality for little effort, and highly reusable code that's easy to build an abstraction on top of with minimal adjustment. I don't think this is any more true of recursion-schemes than parser combinators or lenses. You've not given a convincing example, and recursion-schemes really are universal in more than one sense. If you can provide a base functor for a type `r`, and a function `r -&gt; a` then you can use `gcata`, a well-chosen algebra, and correct distributive action to define an extensionally equivalent function. That is, there's no function on a recursive type you can't write using `gcata`. (Similarly there's no function returning a corecursive type that you can't write with `gana`.)
Some of these issues are what motivated me to start working on [acid-world](https://github.com/matchwood/acid-world), but at present that is still in an early phase of development. Two of them, though, are not really fair On keeping everything in memory - yes, acid state is limited to either high memory environments or smallish databases. But for many projects this might be entirely acceptable, and the performance benefits might be worth the cost of an extra 8GB of RAM. It is very situational is my point, and not an argument against the approach in general. On querying efficiency. A similar objection could be made to any relational database if your data isn't structured in such a way that the database can easily construct queries against it. In practice what people end up doing is rewriting their model to fit their database, rather than their database doing an amazing job of efficiently querying any arbitrary structure. I think the criticisms generally elide the motivating factors behind using something like 'acid-state', which are 1) to provide a way of persisting the models and structures that you actually want to use without forcing you to write your entire application around SQL and 2) to remove the need for an interface, that will of necessity be bug prone and awkward, between the richly typed internal world of Haskell and the sparsely typed world of an external persistence layer. 
Pretty much, and I think the cost of the conversion is a lot higher than people realise, because we are all already so used to paying it.
`...many common Functors are also Monads.` - succinct, clear, accurate. Building an intuition that stuff you encounter in base may have a monad instance is a good thing. Saying 'many common' instead of 'most', and using 'also', helps build intuition that someone somewhere did work to make a functor an instance of monad, without leaving them with nagging questions about how that was accomplished.
&gt; I agree and I think that's a good thing. I don't think it is: for example does a car mechanic need to study theoretical physics and chemistry? even though that's what car engines rely upon ultimately. &gt; No, but it's not strictly necessary to create "friendly" names for things that already have names and obscure the connection between the two, either. So you have accepted that its not strictly necessary. Very good, we're making some progress. &gt; but it's not strictly necessary to create "friendly" names It depends on what you are trying to do, you said your aim are better programs, and for that no we don't need to know category theory, and no we don't need a programming language entrenched in formalisation. And no software defects happen for multiple reasons _(not just type errors)_.
\&gt; there is no way to create a thing of type "a" from thin air No, but you have more than thin air. You have a thing of type "a" at hand. You could have capability to construct new values of type "a" given some Copyable/Mockable/PropertyGenerator/Whatever typeclass which "a" belongs to. Or am I missing something?
Since we know that functional and object-oriented programming are equivalent, it isn't even right to say that functional programming is subsumed by OOP. One can also use functional programming to write with OOP features such as message passing.
Just as in OOP it is (often) better to think about the messages sent between the objects than the objects themselves, in FP it is better to think about the composition of functions rather than the functions themselves. (Both are special cases of the category theoretical observation that the arrows are more interesting than the objects.)
&gt; In practice what people end up doing is rewriting their model to fit their database, rather than their database doing an amazing job of efficiently querying any arbitrary structure. This is why I use Redis so much. It doesn't give proper ACID guarantees or anything, but for my purposes, it's good enough, and the design flexibility it provides with its large supply of interesting primitives is profound. I have no idea how I would translate 90%+ of my designs to SQL databases.
Agree with "In Haskell, many common Functors are also Monads". Thanks.
&gt; when we understand the use of \[\] and Maybe, the work is almost done for Monads. I definitely agree that it essential to see some examples of monads to build intuition. But it's also important not to forget that there are many monads that do not quite match the intuition you get from "container" monads like [] and Maybe (for example, IO or Cont). Keep up with the good work! 
Aseembler subsumes them all
You can't evaluate the cost without also evaluating the payoffs. And the payoffs are obvious: people like their existing datastores, and they have a lot of existing tools to integrate and plug them together and people have extensive knowledge of maintaining, testing, deploying, and using them efficiently. Almost none of these apply to Haskell-native datastores in any meaningful way and they all require a lot of shared experience to understand -- they are not the byproduct of writing good code and implementing it well. "Well implemented, good code" is only a prerequisite for that, and that's already hard enough. You can of course write all of those tools and things yourself with your magical Haskell-only datastore to close the gap -- but now who's the one dealing sunk costs they weren't aware of? Concerns like this are ultimately far more important than any individual Haskell programmers' concerns about relational algebra or whether they like running SQL databases or whatever.
Interesting! I recently had the same problem for [my project](https://futhark-lang.org) and bribed some students with a bachelor's degree to produce a [similar benchmark result visualisation tool](https://futhark-lang.org/benchmark-dashboard/index.html#/visualize?selected=%5B%5B%22230%2C25%2C75%22%2C%22opencl%22%2C%22GTX780%22%2C%22futhark-benchmarks%2Ffinpar%2FLocVolCalib.fut%22%2C%22LocVolCalib-data%2Fsmall.in%22%5D%2C%5B%2260%2C180%2C75%22%2C%22opencl%22%2C%22GTX780%22%2C%22futhark-benchmarks%2Ffinpar%2FLocVolCalib.fut%22%2C%22LocVolCalib-data%2Fmedium.in%22%5D%2C%5B%22255%2C225%2C25%22%2C%22opencl%22%2C%22GTX780%22%2C%22futhark-benchmarks%2Ffinpar%2FLocVolCalib.fut%22%2C%22LocVolCalib-data%2Flarge.in%22%5D%5D&amp;graphType=slowdown&amp;slowdownMax=2&amp;xLeft=0&amp;xRight=100). It was built from scratch, and as students are wont to do, in Javascript ([source code](https://github.com/diku-dk/futhark-benchmark-dashboard)). However, it does have the very nice property of generating purely static files (with embedded Javascript for dynamic zooming/toggling etc), which means deployment is trivial, as you just need an HTTP server capable of serving static files. Benchgraph looks like it requires a custom server. Is that a correct assessment?
&gt; I don't think this is any more true of recursion-schemes than parser combinators or lenses. You can take a functioning application, and slap lenses in, try it out, and see if it's a good fit. You don't generally end up writing new instances or new representations of data to support the use of lenses. You can build a set of functions around a data structure, make a serialized representation of it, and then build a parser using parser combinators, without ever touching the set of functions or the data structure. You can also take parser combinators or lenses out for a spin, build a small program that looks familiar, and decide whether or not they're a good fit for a particular problem. You cannot do that with recursion-schemes. They're probably going to inform the design and implementation of the core types that your code is operating on, so that's an immediate re-write of most functions, and the code you end up writing to support them is non-idiomatic and difficult to understand, so it's hard to build a quick prototype. Proper use of recursion schemes generally requires completely re-thinking how you frame a domain problem as a starting point. That is an awesome exercise and it's worth doing - But if you do it while you're on a deadline, or before you have a firm grasp of the language, you're going to have a bad time.
Talks by Bryan O'Sullivan (https://skillsmatter.com/skillscasts/5466-bryan-o-sullivan) and Johann Tibell (https://skillsmatter.com/skillscasts/6547-high-performance-programming-in-haskell). Some notes by Mark Karpov (author of Megaparsec): https://www.stackbuilders.com/tutorials/haskell/ghc-optimization-and-fusion/ Not exactly easy to jump in but reading the source for unordered-containers, containers, vector and text (and bytestring/attoparsec I guess). The megaparsec source code is also quite readable.
Wanted to get some feedback on this strategy for language term representation: I have used Bound in the past to get cheap and easy substitution, but it comes at a cost: you have to write Monad/Eq1/Show1 instances (or autoderive them, which is iffy with complex datatypes). This is further complicated if you have a sequence of transformations between expression types, requiring you to generate all that boilerplate for each type. For example: data Exp a = Var a | App (Exp a) (Exp a) | Abs (Scope () Exp a) deriving (Functor) $(deriveEq1 ''Exp) -- If it works... $(makeBound ''Exp) -- If it works... As an alternative, I have implemented a substitution type like this data Scope n f a = ScopeBound Int -- debruijn index | ScopeFree a -- free variable | ScopeBinder Int n (Scope n f a) -- binder with arity, annotation, and body | ScopeEmbed (f (Scope n f a)) -- functor layer for your expression type -- instance Functor, Applicative, Monad, MonadTrans, etc. Like Bound, monad bind (implemented carefully) becomes free variable substitution. You can implement `abstract` and `instantiate` combinators, but you don't get the same kind of safety that Bound gives you. The key advantage that I see is that your functor definition is much simpler: data ExpF a = App a a deriving (Eq, Show, Functor, Foldable, Traversable) type Exp a = Scope () ExpF a I was hoping to use this strategy because I would let me use functor sums and products to annotate parts of the expression tree, keeping the concerns of substitution and annotation separate from the underlying expressions themselves. Surely this has been implemented before - do you know of any good examples? Have you tried this yourself?