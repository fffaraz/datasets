Thanks!
I can't speak to your actual question but would [file-embed](http://hackage.haskell.org/package/file-embed) potentially work for your use case?
This is horrible. I like it. :D But in all seriousness: what would you use global implicit parameters for?
The last example allows you to write strings with 1 less character per string. In a file with a lot of strings this can significantly reduce the file size.
This is amazing. Surprisingly, I even have a use case. I've been working on some `diagrams` code for doing fashion design a la [silky](https://silky.github.io/posts/2019-01-27-Designing-Functional-Cloths-with-Haskell.html). Lots of times I want to hard-code a subdiagram's color scheme for the project I'm created it with, while at the same time, keeping everything reusable library code. This would let me hardcode my colors as global `IP`s, but still let them be redefinable in other contexts!
 ?cmp = compare -- global implicit isort = sortBy ?cmp isort [1,3,2] let ?cmp = flip compare in isort [1,3,2]
Also this https://gist.github.com/llelf/6c01ded225877914f38a
oh wow!
If the only improvement is the documentation, why not just document one of the existing libraries?
I wouldn't be surprised if there isn't some real unification of language in the far future where the system/proof/data things share a single term (e.g. "reifs") and the process/proposition/type things share a single term (e.g. "ideats"). There seems to be a deep, cross-cutting ontology here.
&gt; the `data Tuple a = Tuple a a` functor That's a horrible name for that functor. `Two` or `Pair` or `HomogenousPair` would all be better.
See my comment [here](https://www.reddit.com/r/haskell/comments/cbsh8o/ann_worldpeace1000_a_library_providing_open_sum/etigmt0?utm_source=share&amp;utm_medium=web2x) where I answered this, but in short, I should have done that! Of course, creating my own library allowed me to add as much documentation as I wanted, without having to get approval from an existing maintainer.
The article mentions that ad-hoc polymorphism is problematic with generics. That isn't quite true - subtyping plus generics are the problem. I actually quite like java's approach there but it is a lot more complicated than hindley milner. Like, you could do guarded mpredicative polymorphism in Java long before ghc implemented it.
Neat discovery. I opened a discussion on reddit a while back asking why people don't use `ImplicitParams` [here](https://www.reddit.com/r/haskell/comments/6gz4w5/whats_wrong_with_implicitparams/), encountering [this rather horrifying inconsistency](https://www.reddit.com/r/haskell/comments/6gz4w5/whats_wrong_with_implicitparams/diugtwu/), which is actually documented in the GHC manual [here](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#implicit-parameters-and-polymorphic-recursion), but IMHO it should all be in red in a `&lt;marquee&gt;`, with an under construction animated GIF on it.
Hm, perhaps I should have marked this NSFW.
The functional dependency in `IP` is an unfortunate lie. You can easily wind up with `IP "x" Int` and `IP "x" Double` both in scope, despite violating the fundep by merging contexts.
Writing to the record is problematic though. You'd ideally like to be able to talk about a "lens" into the larger record type, but GHC will happily ignore your fresh `?x` in `(p, ?x :: Int) =&gt; p` when it can't see into `p`.
It would be a truly odd maintainer that didn't accept properly-licensed, high-quality documentation.
Opened pull request with custom type errors: * https://github.com/cdepillabout/world-peace/pull/5 The good thing is that it was added externally, without changing any types or functions from the `union` package. So, if in future you want to use `union` as a library, you still can have friendly type errors without any effort.
If your implementation is similar to that of union (or any of the other packages out there), I would imagine that it should not be that complicated to still adapt your examples to those packages. To be honest: If I'd want to use an open union type I'd rather spend a few more minutes trying to figure out how to use one of the faster implementations rather than use your slow implementation just because you have examples.
I'm so glad you're documenting this, "We all float down here" but not in kinds!
As a counterpoint to the comments in this thread, not everything needs to be valued nor optimized in terms of "usefulness" or "direct contribution to the community" or whatever. A learning project doesn't need to be defended like an NSF grant proposal. Some things can be taken in a lighthearted manner. "Fun" and "new experiences" are easily underrated. Furthermore if the only difference is really documentation (which might not be obvious a priori, especially not for yourself before beginning such a learning project), the amount of effort to port it elsewhere is arguably trivial compared to writing it in the first place.
Please no
Recent version of stack generates `stack.yaml.lock` file. Is it safe to add this file under VCS?
Interesting! Thanks, I didn't know about it. It does seem to do what I want, though I'm more interested in *how* it does that. As it turns out, `$(embedFile …)` expands, roughly, to [`unsafePerformIO $ unsafePackAddressLen …`](https://github.com/snoyberg/file-embed/blob/47b499c3c58ca465c56ee0295d0a76782a66751d/Data/FileEmbed.hs#L151), without any extra `NOINLINE` annotations or `-fno-cse -fno-full-laziness`, which are recommended in `unsafePerformIO` docs. Now, being a template haskell splice, it can't really add those anyway, but that's another point in design space. Hm. So I guess the answer to ‘does `file-embed` work here’ is that that's what I'd like to know, in a sense. But thanks for the suggestion, it's interesting to see how other people approach the same problem.
Many suggestions here of memoization, I would add a reference to the State monad.
I'm pretty sure this is extremely important for Homotopy type theory, which key is the topological/homotopical view.
Awesome! Trying this out right now! If your PR gets merged, it will be safe to delete my local copy of `Stack` and return to the official release, correct?
Glad to hear it!
&gt; I didn't find something like this implemented so I'm thinking of doing it. But why? &gt; just doing what ever the hell I want Carry on then.
Are there any paper(s) which give a theoretical background for numeric computing which would suit functional programming?
Why bring Aeson into the game here? At this point, all you're interested is extending whatever parser you have from, morally, `URL -&gt; [(Name, Value)]` to `URL -&gt; [(Name, Either Value [Value])]`. So what you need is really just something like a post-processing step that takes you `[(Name, Value)]` list, extracts everything that looks like array syntax, parses the `Name`s into `(Name, Index)` pairs, massages that into a list, and recombines it with the entries that do not use array syntax.
Couldn't you do a Set Monad (the Ord constraint makes it not possible to use the default Monad class but it could still be a mathematical Monad.)
For God's sake Csongor, stop it.
&gt; But why? Because I want to send structured form data to servant endpoint and have it converted to my custom deeply nested data type. It is relatively simple when you are working with ajax, then you can just convert your form input into `JSON` but even then you can't really send binary data like images and files. You basically need two steps (first create entity from `JSON`, give response back to the frontend that this entity is created and then upload file in the second step and attach it to the entity, because `aeson` doesn't support binary data so you can't send files in a `JSON` field). Or, you can have hidden form which has one field for files and second field for text encoded `JSON` which is absolutely dirty in my opinion. --- Most importantly I want to be able to create complex forms which require no `JavaScript` meaning I need to have simple way of converting "flat" `URL` encoded form fields into structured / nested data types. Since there is no standard for syntax of those fields `PHP`'s way dealing with structured form data seems like the way to go.
So semantically it’s more a TDNR despite ghci reporting that funded?
It's not just for arrays / lists. You can encode deeply nested records as well, and I want to be able to reuse `FromJSON` instance so that I can convert this "parsed" type into my record easily. I mean, yes I'm thinking of whipping up my own `Value` type to cut the dependency on `aeson` but I'm wondering if it's worth it (so far it seems like it, because I need a way to store a `ByteString`).
Ah, weird constraints, of course. Putting files in a URL sounds horrifying. But why not use a simple `POST` with `multipart/form-data` for files and `application/json` for anything else? E.g. [servant-multipart](https://hackage.haskell.org/package/servant-multipart-0.10.0.1/docs/Servant-Multipart.html).
Sorry, I was actually talking about `multipart/form-data` (which is essentially same format as url encoded but different delivery). And I've tried `servant-multipart` but it suffers from the problem I've described before. I'd have to have some `JS` which would look at my form, convert it to `JSON`, store that `JSON` into the text field in a hidden form which will actually be sent to the server, and this `JS` would also have to put files into that hidden form fields. Then on the server side I'd have some record like: ``` data Form = Form { json :: Value, files :: [ File ] } ``` And I'd have to further process this `Form` type to convert it to my actual record which actually represents my form.
This is very inspiring. Thanks!
I saw isovector at a grocery store in Los Angeles yesterday. I told him how cool it was to meet him in person but I didn’t want to be a douche and bother him and ask him for code reviews or anything. He said, “Oh, like you’re doing now?” I was taken aback, and all I could say was “Huh?” but he kept cutting me off and going “huh? huh? huh?” and closing his hand shut in front of my face. I walked away and continued with my shopping, and I heard him chuckle as I walked off. When I came to pay for my stuff up front I saw him trying to walk out the doors with like fifteen effect systems in his hands without paying. The girl at the counter was very nice about it and professional, and was like “Sir, you need to pay for those first.” At first he kept pretending to be tired and not hear her, but eventually turned back around and brought them to the counter. When she took one of the algebraic effects and started scanning it multiple times, he stopped her and told her to scan them each individually “to prevent any type infetterence,” and then turned around and winked at me. I don’t even think that’s a word. After she scanned each effect and put them in a bag and started to say the performance overhead, he kept interrupting her by yawning really loudly.
&gt; I hate when I use a typeclass-based function and the type I use it on turns out to implement it in an unsafe or unreasonable way I believe this is one of the cases for using `newtype`; you can then re-implement the typeclass the way you expect/prefer. If it's actually *unsafe* I think that would be a good issue to file against the library itself! But in general, `newtype` is there precisely because there is often more than one way to make a data structure an instance of a typeclass (for instance, integers are monoids under both addition and multiplication). If you find the original instance doesn't fit your use case, just newtype it!
classic isovector
This one?: [https://www.youtube.com/watch?v=dEh2Yt-YkAQ](https://www.youtube.com/watch?v=dEh2Yt-YkAQ)
I’m specifically referring to instances that I think should not exist at all. There aren’t too many of them in major libraries as people rightfully complain, but you can see some examples in `inj-base` that throw various exceptions. This is a fairly small side note compared to the main focus of the post. I’m just explaining why “just publicly export all instances no matter how unsafe” is (admittedly fairly obviously) not a good option.
While I do think there is some connection between (at least the culture of the way people typically write) Java and the bureaucratic business context, I'm not sure this is legit Lacanian theory.
Has anyone really been far even as decided to use even go want to do look more like?
Hi isovector! Glad to see you're always with one thing up your sleeve :) I want to say congratulations and I hope many good things come soon! Lastly I'd like to suggest something: I've been reading about your work on algebraic-effects and all the ecosystem around it and find it difficult to wrap my head around it. Like, I know how to build an application in some imperative language, it's very straightforward. But in Haskell I see all these new and weird ways of composing events that are so theoryfull and different from the 'pleb' way to do it. That's just how I really see it. Maybe you could do some blog post trying to put yourself in the position of the pleb that tries to wrap their head around these concepts and maybe provide some recipes or systematic way to think about them!
&gt; If your PR gets merged, it will be safe to delete my local copy of Stack and return to the official release, correct? I presume so. Of course, this assumes that it _does_ get merged. The maintainers of `terminal-size` don’t look terribly active, and the last commit on that project was from 2016. So it may well never get merged. (On the other hand, I’ve just thought of a simpler solution which would apply to `stack` directly, so if the PR isn’t merged in a couple of days I’ll go on to try that solution.)
&gt;GHC on Windows is buggier and de facto less supported; that's a fact. &gt; it isn't POSIX-compatible, the usual POSIX utilities aren't available, the process and threading models are quite different, signal handling is different, encodings are different (Windows generally uses UTF-16, whereas most Unix-likes have settled on UTF-8), system APIs are completely different, etc. etc What, specifically, that you mentioned there, results in a GHC bug? Not an implementation detail that you have to handle for cross platform development, but an actual, real bug with GHC? What are the actual bugs that you personally have experienced with GHC while working on Windows? I've been using GHC on windows almost exclusively for quite some time now, and I have encountered precisely 0 GHC bugs, and exactly 1 unanticipated difference in semantic execution with low level APIs. They are certainly out there - There are a few high-profile bugs with GHC on Windows, but they aren't something that you're extremely likely to run into on a day-to-day basis out of the blue, and they certainly shouldn't be the first suspected culprit on the first vague mention of a problem with 'some issues with gcc.' The individual didn't even specify whether or not they'd had success with the same build/code on another OS - You jumped to the outlandish conclusion of GHC bug with literally 0 cause to do so. Can you really not see why that might be considered harmful?
I don't know where the madness ends and the hilarity begins
&gt; The Haskell community is by far the smartest, kindest, most incredible group of people I’ve ever had the good fortune to be a part of. I wonder, how can you be firm behind this opinion? There are indeed incredibly smart and kind haskellers, but you can have an abysmal experience interacting with the Haskell community: people trashing your library just because it's not perfect, telling you to ask questions in `#haskell-beginners` channel because the current channel is for smart people, continuing dramas around build tools and other technologies. Even your proposal regarding adding _markdown template_ to `ghc-proposals` has some BS comments in there. I notice that Haskellers (and programmers, in general, to be fair) tend to focus too much on technologies completely disregarding human feelings. It's just not worth it to have one feature in GHC or library implemented in a slightly better way if someone gets depression while spending their free time to contribute to the Haskell ecosystem. Please, try to be more kind when interacting with other people. Next time, when somebody implements something, like a library, instead of writing the following under comments: _One problem with your library is X._ (which sounds like _you've spent 2 weeks of your free time to implement the library and contribute to the language development, but I'm going to spent next 2 hours to find the only thing which is not perfect and point you to this thing telling how bad it is_) you can write: _Great library! I appreciate your time and effort you've spent on this work. I can see how it can be beneficial to some people and I'm glad to see improvements in this field. I find your idea interesting and I would like to know more about how you implemented it. Would you mind some feedback from my side or discussion about your solution if I have more time to read the code?_ You're already using Haskell to save a great amount of time in your life, you can afford to spend one more minute writing a kinder comment :)
Yes, it would! Except, that Unicode is complicated...
It's worth noting that "\\x2795" in Haskell is equivalent to "\\u2795" in Python 3. There's probably a way to specify decimal code points in Python, but I can't remember it.
I also would like to know whether it's okay to put it in`.gitignore`.
Is this a joke? That would explain the likes.
Thanks for posting this. I'm sorry it's being downvoted, but I think it's an important thing to be said. Admittedly, the community isn't perfect. But I have yet to interact *in person* with a Haskeller who turned out to be not very nice. I think sometimes the internet can get in the way of civility, but that's more of an opportunity for us to improve than a fundamental flaw. I mean, I've posted a few unkind blog posts in my day. I don't take back the technical criticism, but if I were to write them again, I would be more tactful about how I worded things. As you rightfully point out, it's possible to make constructive feedback while also acknowledging work well done. &gt; Even your proposal regarding adding markdown template to ghc-proposals has some BS comments in there. Only one, and several people explicitly called out that it was inappropriate. Let's not let the negative few overshadow the excellent majority! As for the rest of it, I'm going to beg of you --- and everyone who reads this --- for a call to action. Whenever you see negative behavior, gently and politely call it out, and ask the responsible party to stop. Let's replace the unsavory parts of our culture with a new one, where we demand not only amazing technology from another, but amazing civility.
`stack test` runs all your test sites, so follow [the instructions](https://github.com/sol/doctest/blob/master/README.markdown#cabal-integration) for creating a test suite. Alternatively, you can use [doctest-discover](https://github.com/karun012/doctest-discover) to create that test suite even more easily.
What exact issues are you having /u/Heapwalker? There's nothing specifically in the compiler that targets pro features of Windows. GHC currently supports as a minimum Windows Vista SP1, and we'll be moving it to Windows 7 soon.
I hope it is. I hope nobody seriously decides to optimize for source code file size in 2019, where hard disk space is so much cheaper than the other constraints (most notably developer efficiency) your code has to adhere to.
Hi! Author here. Tom is right that the ideas the article is about are older -- I tried to give references to a variety of stuff. That said, I do think the relationship of topology to programming language theory does tend to be often underappreciated, which is why I wanted to write the article. It has been spectacularly successful in domain theory, but there are many promising aspects of topology whose connections have not been sufficiently pursued, and I tend to think that even those who use toposes to think about semantics will do so by paying attention _much more_ to the logical than to the topological aspects of toposes. This is not to say that people aren't doing it at all though -- witness Vickers' wonderful book "Topology Via Logic" or Shulman's use of the Chu construction to study linear logic, and much else. I would of course not call it a "Bazerman Program" but I do think there is a room for a renewed "Tarski Program" that places a greater emphasis on stone duality and related topics in programming language semantics, and more importantly I think that there is a _ton_ of room for importing modern (post-Tarski) algebraic topological methods into studying the topological structures that programming language semantics and logics give rise to. This is all quite vague though -- more a hunch than a program. I think the way to get to the latter is to do some actual work that paves the way, and maybe shows that this approach can go somewhere. But I do have some work underway that translates that hunch into some maybe interesting results, and hopefully it'll be presented in an accessible way soon enough. And if anyone wants a preview, and will be in the Boston area on August 8, they should swing by the MIT category theory seminar :-) http://brendanfong.com/seminar.html
Thank you for your response full of optimism! I appreciate the effort you put into improving the processes around GHC and Haskell community in general. I wish you great success in your journey for enhancing the state of the Haskell, and I sincerely hope you won't burn out during this journey. It warms my heart to see such positive people full of energy like you in the community :)
[removed]
You cannot have SimpleBeanFactoryAwareAspectInstanceFactory, AbstractInterceptorDrivenBeanDefinitionDecorator, TransactionAwarePersistenceManagerFactoryProxy or RequestProcessorFactoryFactory outside of what Lacan calls "The Big Other". It must be a surplus enjoyment in writing code for enterprise Java way
This is a modification of a copypasta if I recall correctly.
/u/Phyx I had problems with the gcc compiler and the user rights. I installed Haskell Platform and I was not able to run ghci in a standard cmd or powershell only as admin but still was not able to compile additional packages and its dependencies since it didnt find bash and gcc, missed a /tmp folder etc. It was fixed (a bit) after I removed everything and installed haskell stack. I assumed that it might be related with the OS .
No that seems like an issue with haskell platform. It's nothing to do with the OS. I recommend chocolatey https://chocolatey.org/packages/ghc/ to install things on Windows.
 [/u/Phyx](https://www.reddit.com/u/Phyx/) Many thanks, I will check it out.
I wrote https://hub.zhox.com/posts/chocolatey-introduction/ a while back on how to use it. I'll be updating it soon too with the next release of the compiler.
Also, it's not a weird constraint. It is a normal feature existing in other languages but it seems like everyone is working on SPA-s these days so no one is interested in implementing this.
Especially these days
While we're talking about `ImplicitParams` don't forget that they can be useful too :-) https://discourse.haskell.org/t/records-of-functions-and-implicit-parameters/747
&gt; One problem with your library is X. &gt; &gt; which sounds like "you've spent 2 weeks of your free time to implement the library and contribute to the language development, but I'm going to spent next 2 hours to find the only thing which is not perfect and point you to this thing telling how bad it is" To be fair, this is exactly the sort of feedback I would find useful. It's short and to the point. Now I can fix X or document the shortcoming. On the other hand, if I had to face a comment like this &gt; Great library! I appreciate your time and effort you've spent on this work. I can see how it can be beneficial to some people, and I'm glad to see improvements in this field. I find your idea interesting, and I would like to know more about how you implemented it. Would you mind some feedback from my side or discussion about your solution if I have more time to read the code? Then I'd be taken aback. "Would you mind some feedback"?? Just go ahead and give me the feedback! Don't make me say "Yes please, go ahead and give your feedback", it's an inefficient mode of communication.
Works great with msys2 as well. I've installed GHC and cabal into msys2 root/opt and added to path in msys2 bash. I switch computers often so I've put the msys2 install on an external HDD with no problems. Needed extra-libs in the cabal.project.local for postgresql libs
But think about it. It is a lot more efficient to just type in a single question mark than it is to type in a pair of quotes. Not to mention that when you open a quote, most text editors simply re-parse the entire file for syntax highlighting, which, in the case of large files, can take a long time - only for this to be undone when you close the quote! So we have to options: use question marks, or stop using syntax highlighting.
&gt; To be fair, this is exactly the sort of feedback I would find useful. We are all different people with diverse background and various expectations. You expect one type of feedback. Others expect the other type. I think that being nice by default is a good default. I've provided some examples, they might not illustrate the root of the problem perfectly, but I hope that the idea is more or less clear. Regarding asking about feedback: sure, go ahead and provide it right away, no need to ask. Just think more about wording.
Sure, everybody is different. But it is easy to accept some inefficiency if that means a lot more people will feel welcome and start contributing to the community. Turning them away would be much more inefficient for the community as a whole!
I’ve generally heard this more commonly referred to as extensible variants/sums, as opposed to open sums. Also usually extensible variants libraries seem to have extensible records/products available alongside them.
I've found a way to make REPL work for my SDL/Haskell project: $ ghcid --command "stack ghci --ghci-options '-fno-ghci-sandbox'" --test ":main"
I think one way to do that would be to use custom `Setup.hs` in a cabal package, in which use [`cpuid`](https://hackage.haskell.org/package/cpuid) (which looks thoroughly abandoned, but seems to work just fine) to detect supported SIMD flavours and use something like [`defaultMainWithHooks`](https://hackage.haskell.org/package/Cabal-2.4.1.0/docs/Distribution-Simple.html#v:defaultMainWithHooks) to set GHC flags accordingly ([user guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/using.html#platform-specific-flags) claims that llvm backend detects them automatically, but you can set something like [`-DAVX`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/phases.html#ghc-flag--D%E2%9F%A8symbol%E2%9F%A9[=%E2%9F%A8value%E2%9F%A9]) for use in your own `CPP`). Disclaimer: I have no experience doing any of that, so take any advice I give with a grain of salt.
Would `reflection`\-trick be safe here for creating needed helper?: [https://gitlab.com/snippets/1874588](https://gitlab.com/snippets/1874588)
&gt; I've provided some examples, they might not illustrate the root of the problem perfectly, but I hope that the idea is more or less clear. The problem is not clear to me, and I'd be happy if you could explain it to me in more detail. My default assumption is that people are well-meaning, so I fail to see the issues that you point to. Specifically, &gt; telling you to ask questions in #haskell-beginners channel because the current channel is for smart people If I decided to learn a new language, and someone would tell me to ask questions in "#lang-beginners", I would assume that they * care about structured communication, such that information goes through the appropriate channels * want to direct me to a venue where more people have the intention of mentoring me, therefore increasing the chance that I get the help that I need The person telling me that is likely trying to be helpful in a way accessible to them. On the other hand, your assumption is that they mean "the current channel is for smart people". Well, if someone doesn't know a language, it doesn't mean they aren't smart, it means they lack the necessary background, so this wouldn't even make sense. So, why would you expect that the other person is malicious and spewing nonsense, rather than being well-meaning and giving sensible advice? Next, your other example: &gt; continuing dramas around build tools and other technologies I think the drama is escalating precisely because of the attitude such as yours, where you assume people to be malevolent by default and expect a paragraph of praise before the actual comments. Even my last sentence you may interpret either as a personal attack, or just a neutral observation in an attempt to establish a more efficient mode of communication that avoids drama instead of bolstering it. That said, I have not actively participated in the build tool drama, and I use both Cabal and Stack in my day-to-day work, so I cannot exclude the possibility of malicious actors influencing the conversation. And I'm not trying to defend all the things some people may have said. My point is that when there's ambiguity, assume the best intentions. &gt; Even your proposal regarding adding markdown template to ghc-proposals has some BS comments in there. And what would that be? I have re-read the thread and have seen only one comment that spurred controversy: &gt; AFAICT, the barrier to entry is deliberate. If you really want some change to GHC, learning ReST is the least of the effort. Toughen up! Is this what you were referring to? Well, first of all, it's prefixed by "AFAICT", so the rest of the comment is basically the conclusions that this person has made based on their personal experience of interacting with the GHC Proposals process. And if you spend some time looking at the discussions that they participated in, you will see that they have participated in a great deal of complicated, exhausting arguments. Therefore, "toughen up" is a reasonable attitude for this person, they merely decided to share this feeling/impression with others. Yes, it was swiftly clarified that there are no *intentional* barriers to GHC contributions (and that's great!), but I can see why this person would assume otherwise. ---- To conclude, I just want to reiterate that if you assume people are well-meaning by default, you will have a much better time in any community, not only in the Haskell community. We are all united by a shared goal, after all, that's what makes us a community :)
I'm like 99% sure Servanrt *used* to support query param arrays like this, but they're pretty gross imo; why not just use \`myarray={Foo,bar,baz}\`, or, if you need to handle more complex syntax than that, make it Base64 encoded or something?
You can use [`mkTextEncoding "CP1251"`](https://hackage.haskell.org/package/base-4.12.0.0/docs/System-IO.html#v:mkTextEncoding) to create the encoding, and then [`hSetEncoding`](https://hackage.haskell.org/package/base-4.12.0.0/docs/System-IO.html#v:hSetEncoding) to set the encoding of the file [`Handle`](https://hackage.haskell.org/package/base-4.12.0.0/docs/System-IO.html#t:Handle). In order to acquire a file handle, use [`withFile`](https://hackage.haskell.org/package/base-4.12.0.0/docs/System-IO.html#v:withFile), and then [`hPutStr`](https://hackage.haskell.org/package/base-4.12.0.0/docs/System-IO.html#v:hPutStr) to write to it.
In my projects, I usually prepare a .hs file that's only meant to be loaded in ghci for supporting development, debugging and one-time hacks to fix stuff in production. *Anything* is game in those files, as long as improves the REPL experience
I'd love to see a decent blog post about how to go about finding optimal values for these, I know they can make a big difference but some time about how to tweak them would be very useful.
I *strongly* agree with the conclusion of your post. Though I feel perhaps the bit above the line might be missing the point. Nothing you have said here is false, but at the same time, I doubt /u/chshersh was looking for a point-by-point refutation of their feelings. I interpreted this as a well-meaning person putting in effort to describe the off-putting aspects of the community. Based on the upvotes, /u/chshersh is not the only person who feels this way. Let's not be too hasty to dismiss that. Thankfully there's an easy solution. By default we can try to be gentler and less quick to reach for criticism. And for people like /u/int_index who are explicitly looking for hard-hitting feedback, they can just ask for it explicitly.
It feels to me that the combination of compact regions and segregated memory regions would allow this; forkSegregated :: (IO a -&gt; IO b) -&gt; IO (ThreadId, Send a, Async b) sendTo :: Compactible a =&gt; Send a -&gt; a -&gt; IO () would copy (or move?) `a` into a compact region which is only accessible in the forked thread and whose memory is managed by that thread's heap.
Thank you very much! It helps me)
Don't if you can help it. 8 bit encodings must die.
Such a great attitude.
I've used servant for a long time and I don't remember such feature. I've expanded on the problems I've encountered in the newly created repo readme if you are interested. Main problem is that all "solutions" are heavily dependent on JavaScript and there is no way to edit complex data through plain HTML forms in a nice and streamlined way. Also, the file upload is a pain with `JSON`. https://github.com/mastarija/formalin
https://docs.haskellstack.org/en/stable/lock_files/ suggests storing these files in source control.
&gt; just publicly export all instances no matter how unsafe is actually the only option for globally coherent type classes. Coherence is necessary for things like Set / HashMap to work reasonably, in particular in order to have fast union operations.
&gt; I intend to preemptively shut down discussions around bikeshedding That's my favorite goal right there. There quite a bit of bikeshedding going on from community members, and I've been a part of this problem before. At the end of the day though, all of us non-steering-committee-members don't really need to be doing this. It is distracting and wastes time and energy of the proposal author. Don't get me wrong, there are a lot of good insights that come from ordinary community members too on these proposals, but the useful insights concern things like soundness, not syntax. I fully trust that the people on the committee to paint the bikeshed a decent color.
&gt;I've used servant for a long time and I don't remember such feature. Maybe not as long as me then =) [https://hackage.haskell.org/package/servant-0.4.2/docs/Servant-API-QueryParam.html](https://hackage.haskell.org/package/servant-0.4.2/docs/Servant-API-QueryParam.html)
Sandy, you're doing so much excellent work - I would love it if you started up a patreon or something similar to help fund these adventures, and the rest of your work. I'd happily send you $5/mo (AUD,USD, who cares). I've had this thought every time you've written something; I'm pretty stoked about Polysemy, I was very pleased to see you added to the GHC committee, and very impressed with your attitude towards making shit better. I'm sure I'm not alone in the feelings of "I wish I had the time to do amazing things, but at least Identity like two fund someone else who actually is". u/kcsongor is another person I'd put into this list, it's great seeing someone young bringing so many fascinating topics to the community (though with great power [comes great responsibility](https://www.reddit.com/r/haskell/comments/cbvjlg/global_implicit_parameters/)) - Csongor if you're reading, generic-lens is just amazing, you had me at "performs as well as hand written lenses". &amp;#x200B; So basically, *shut up and take my money*.
I have seen myself getting offending when receiving certain feedback from people who were not so tactful at wording it, so I agree with this advice - but only as long as those people *continue* to provide such feedback on my work. Feelings aside (and--this is offtopic to Haskell--I'd rather be free of them), feedback from people more knowledgeable than me is only going to further my own knowledge. I also worry that some people may even refrain from giving critical feedback in the fear of being seen as unkind, which ends up negative affecting me. I started out writing professional Haskell in the beginning of 2018, and despite by affective challenges I'm actually thankful for the feedback received on my code even if they are not upto whatever moral standards being established.
This isn't constructive phadej.
How long are you going to accept submissions to your form? I'd love to share my place with you, but I'm going to move out soon from my current home, and I don't know yet where I'd be in 2 months...
&gt;Let's not be too hasty to dismiss that. With the disclaimer that I'm in full support of kindly wording any critical feedback or suggestion, may I ask? If I tell someone to ask their question in #haskell-beginners and if they think of me to be too smart, driving them away from a channel reserved for smart people (and, by implication, thinking of themselves to not be smart enough), then wouldn't they too be hastily dismissing my suggestion and my well-meaning intention? &gt;Thankfully there's an easy solution. By default we can try to be gentler and less quick to reach for criticism. While being gentler and less quick to reach for criticism are laudable modes of communication generally speaking I fail to see how it is a solution to preventing misperceptions like the above.
Hehe, you are right "older brother" :)
&gt; if you assume people are well-meaning by default, you will have a much better time in any community I think maybe that assumption is the problem. It's not easy for everyone to be in such a benevolent state of mind. On a bad day, if you look at a wall of issues, anyone can feel like "I'm not adequate" rather than "these other people care about my project". It's no simple task to rationalize feelings away. Impostor syndrome, for example, wouldn't be a thing if all it took was to say "assume you'll make it, you'll feel much better". Perhaps I'm stretching it, but "toughen up" seems self-contradictory: it assumes that people are fragile (to *become* tough, one must not be tough in the first place), while dismissing that fragility at the same time. Haskell is a technical topic, but if we could easily separate the technical from the emotional, therapists would be out of a job. Maybe spending a whole paragraph of encouragements on every comment is too blatant, but I also really think there is a problem worth solving, and that everyone can be a part of the solution by providing moral support once in a while. I don't mean to treat everyone like a snowflake, but surely there's also a reason why we smile at each other, and that's too easily lost in text. Much like our documentation could use work to make the language and libraries easier to use, our communication could use work to make criticism easier to take.
&gt; Problem is, aeson / json doesn't support binary data (unless we encode them as text) I don't see why this is a problem, you can `base64` encode/decode whatever binary data you need to send over the wire. IME that's how other people solve this problem.
Thanks it's exactly what I was looking for, I'll give it a go!
u/ocramz
Congratulations, Sandy!
I'll bring it up with the committee and see if we can address that for you. Thanks for letting us know
&gt;Time spent using shake + pandoc is time spent learning general tools, which will be useful on all kinds of projects. Nothing wrong with Hakyll, but sometimes you don't need specialization. Exactly the reason for me not going with Hakyll. &gt;PS If you're making a static site with shake check out Development.Shake.Forward. Thanks for the suggestion! It does look [so much simpler](https://github.com/srid/rib/pull/17) that I've decided to make my static site generator to use it.
We don't use a blacklist and I think that email that our spam filter plonks does *not* generate bounces. The latter email address is not correct -- it is `admin` with no plural. I don't know what's happening with the former issue. Searching the logs doesn't reveal much. The address (which should be advertised better) for infra administration is admin at h.org, and folks are also pingable on the #haskell-infrastructure irc channel on freenode. This page documents that stuff, but it is not terribly discoverable: https://wiki.haskell.org/Haskell.org_infrastructure Perhaps the committee could add a bit more info to the homepage to help direct people. (However it would have to make very clear in the homepage that this was *only* server infrastructure else I imagine lots of other stuff would get directed that way too).
This post from u/dons is old but gold https://donsbot.wordpress.com/2010/07/05/ghc-gc-tune-tuning-haskell-gc-settings-for-fun-and-profit/ . The plots in that post convey the intuition nicely. Simon Marlow’s answer to this SO question explains the difference between -A and -H https://stackoverflow.com/questions/3171922/ghcs-rts-options-for-garbage-collection Unfortunately it’s all empirical at the end of the day, so benchmarking is required to get good results.
I don't understand. Being gentler and kinder is exactly how one would work to avoid misunderstandings like this. It's not one or the other. They are the same goal.
Seems like there is no "standard" way of doing distributed transactions. I am thinking about writing a haskell implementation of the open xa standard. What do you think? Whoever wants to join me, just contact me.
This is a very bad solution and is definitelly not how most people solve this (it is how some peiple do it though). First your client has to encode the file, and then your server has to decode it. Not only is this wasting CPU but Base64 encoding also increases the overall size of the file being transfered, and you have to depend on JS to do the encoding. Also it causes performance issues when uploading la
How does this relate to [Logic Programming](https://en.wikipedia.org/wiki/Logic_programming)? The blog post only covers LP-type problems, is symbolic execution equivalent?
&gt; Coherence is necessary for things like Set / HashMap to work reasonably, **in particular in order to have fast union operations.** I don't understand how the bold part is related to coherence. &gt;&gt; just publicly export all instances no matter how unsafe &gt; is actually the only option for globally coherent type classes. What if you could also export a "marker" which prevents a certain instance from being used? As far as I can tell the problem coherence solves is to avoid accidentally using one instance instead of another. What I outlined still enforces that only at most one instance for a given type can ever be in scope globally. It can't be used if your scope is too large, maybe it breaks coherence as defined currently, but is it merely a superficial conflict with the formal definition, or is it a deep incompatibility with the principles behind coherence as a concept?
I've written more about approaches I've tried and their drawbacks in the readme. Here's a link if you are interested: https://github.com/mastarija/formalin/blob/master/README.md
&gt;Being gentler and kinder is exactly how one would work to **avoid** misunderstandings like this. If this is absolutely so why would u/int_index, just for one example, be "taken aback" at a comment phrased in a gentler and kinder manner?
I've only had positive interactions in the haskell community, and never gotten criticism that dragged on my soul. (If I ever did, I'd wish I had a more rugged soul, not that people who took the time to interact with me, or improve my code or thought-process were kinder). So, my 2c, I vote against dramatizing or politicizing or virtue-signaling or emotional pandering or reforming others, or spending effort on non-creation. I vote in favor of merit and technical competence, even if the package it comes in is "too harsh". Be kind. Be tactful. And learn to let shit roll off your back yo :D (not you isovector, I'm just shouting into the abyss).
I have a copy of Pierce that I would be willing to sell for something like £20 + shipping. Or no shipping fee if you’re willing to collect in person in London or Oxford.
The comment you are referring to isn't a misunderstanding about the intent of the communication. It's just about needing to tell people that their feedback and suggestions are welcome. I'm not sure what to say, except that it may occasionally just be necessary to communicate what you're looking for, since people want different things.
Sandy, you're so full of energy, – contributing to GHC, doing research into effect systems, writing blog posts both technical and non-technical, and now serving as a GHC Steering committee member! That's remarkable, and I do believe you have the potential to change things for the better as you intend. Good luck!
This is a hard nut to crack. Actually, I think it's worse: this is an impossible nut to crack. My starting assumption is *different people are different*. This is exemplified by the different views a close friend of mine and I have of other people: We were out recently together, and another person was mildly rude to us. We both agreed that the other person's actions were rude, but I immediately assumed there were reasons for the rudeness: perhaps that other person was in a rush or had just badly stubbed their toe. My friend thought the other person (whom we do not know otherwise) was just an inconsiderate person. This friend and I have joked about our different attitudes a lot, and neither of us will change, I'm sure. Upshot: different people are different, and there is nothing to do about that. In an online community such as ours, this means that we have to accept the fact that others may not take our comments in the spirit they were intended. Once we know an individual, we can tailor our communication to fit their needs. (Why should *we* tailor to fit *their* needs? Because we want to be nice people.) But what do we do before we know an individual? That is hard. Clearly, /u/chshersh and /u/int_index would like different styles of communication. (I'm using these two as representatives of a spectrum, because they have put themselves forward and stated how they like communication.) I'm not sure there's a way of appeasing them both, at least in the start of a conversation. If we can't appease them both, that means that, before I know my conversation partner better, I'm likely going to get this wrong some of the time. We have to be accepting of that mistake in ourselves and in others. Several times, I've responded to new contributors more in the manner that /u/chshersh has advocated for and less in the manner that /u/int_index has. More often than not (it seems -- I didn't count), I've been told not to be so gentle and just get to the point. But I don't think I will stop doing this, as its seems being too gentle is less likely to scare off new contributors than being too harsh. I have also gotten feedback that I have, at times, been brusque/dismissive/intimidating/&lt;other adjective that induces bad feelings&gt;. So I keep trying, knowing I'm never going to get this right. Bottom line: I want the Haskell community to welcome everyone who wants to be here, regardless of how they like to receive feedback (among many other differences!).
&gt; &gt; in particular in order to have fast union operations. &gt; &gt; How is the [above] related to coherence? One oft-proposed alternative to coherence is for Set / HashMap constructors to remember the instance the were created with and carry that around. However, when doing a union, you end up in a case where you have multiple (potentially inconsistent) "remembered" instances. So, to provide a fast path at all you have to some the problem of instance dictionary equality (which has function equality as a sub-problem). Failing that, you effectively have to process things as a list because the internal branching structures may be inconsistent. Indexing / parameterizing the Set / HashMap on the instance dictionary is possible in some dependently typed languages, but it doesn't make for the nicest API, IME. It's also not easy to do in GHC even if it is possible. (It is not possible in the Haskell documented in the 2010 report.) &gt; What if you could also export a "marker" which prevents a certain instance from being used? I agree that this approach would not *obviously* violate coherence, but I don't think it would work as well as you'd like. If module A uses the instance, module C forbids the instance, and module B imports both A and C, then calls a Rank-2 function from C, passing in a function from A that uses the instance -- well, C has (indirectly) used the instance while forbidding it. Module A doesn't necessarily know anything about module C.
GHC's testsuite driver was written nearly 20 years ago and, for its age, works quite well. As far as priorities go, rewriting it in another language and port our &gt;8000 tests to the new framework is quite low on the list.
From the very beginning of the OP &gt; Right off the bat I will say this is not related to orphans or coherence, these hidden instances should still cause compilation failure if they overlap.
&gt;There was a typo in my post here, I did mail the singular: \`admin@hackage.haskell.org\`.
Would you argue that GHC should allow implicit parameters on the top level? (After all you demonstrated that it essentially already does)
That sounds exactly how I would want things to work with such a feature; if C has reason to believe the instance is unsafe, then the safe thing to do is to not use it. I may be missing your point.
Congratulations Sandy! As someone who follows Haskell and Rust development, I noticed a few differences. While both communities are friendly and focused on correctness, Haskell has more focus on academic research more than mundane work tools and processes. So Haskell has more awesome things explored, but fewer of them get past the proof of concept stage. My hope is that as the processes get faster and more transparent, the gap between research and industrial grade implementation in GHC will get easier to cross. I want to thank the people who do the grunt work on the Haskell ecosystem, be it build system, test suites, bug triage or steering the org.
Well said, and I think this is extremely important. I think it's worth adding that civility can be a *shield* for people who are expert at playing political games, and I think there are certain people in the community who are just that... and *do* that. I won't elaborate here, but suffice it to say that I have definitely had exchanges with prominent members of the community where I got the distinct feeling that that person was really just sandbagging while being *very* polite and making every effort to appear impartial. This is why I'm particularly pleased to see your mention of not letting perfect be the enemy of good. The idea of 'perfect' has been a huge blight on the Haskell community for a long time. It worked while it was small, but these days I feel Haskell really requires more **enginerring** type work.
This is along the lines of Attribution Bias.
I was trying to leave it as that (a trivial example), but what I meant to say was that - no, it does not necessarily avoid conflicts and misunderstandings. An another commenter wrote this: &gt;I think it's worth adding that civility can be a *shield* for people who are expert at playing political games, and I think there are certain people in the community who are just that... and *do* that. And that's basically it (not just political games, though). Civility is a veneer; and doesn't necessarily reflect the truer intentions, which come through the veneer anyway as we are emotional beings.
&gt; If I decided to learn a new language, and someone would tell me to ask questions in "#lang-beginners", I would assume ... How would you *feel* if you'd tried *really* hard to actually learn $LANG for X months and you were brushed off with a RTFM, or "go to \#lang-beginners", or... Personally, I deal with this type of thing pretty pragmatically, but I can certainly imagine how this type of response would be a huge turn-off for anyone earnestly trying to learn $LANG. Of course there are downsides to being too friendly: You leave yourself open to being trolled/abused/wasting time, etc., but here I would invoke the same principle that you do: Assume good intent of the person asking and try to reject/redirect them as gently as possible.
&gt;I think it's worth adding that civility can be a *shield* for people who are expert at playing political games, and I think there are certain people in the community who are just that... and *do* that. Well-put. This is exactly my thought but I wasn't feeling comfortable to express it here for the fear of being seen ... umm ... not gentle. &gt;I won't elaborate here, but suffice it to say that I have definitely had exchanges with prominent members of the community where I got the distinct feeling that that person was really just sandbagging while being *very* polite and making every effort to appear impartial. It is not limited to the Haskell community, as this behaviour happens in any society where politeness is the norm.
Thanks for the kind words!
[removed]
Good question. I will admit I haven’t looked much into why it isn’t officially supported, so there might be a good reason I’m unaware of...
If an instance should never be used (i.e. is just wrong, e.g. \`Num \[a\]\`), you can use the [type-errors](https://hackage.haskell.org/package/type-errors) library to make sure that never happens. You can also add a constraint to your \`insert\` function to make sure it only works/fails on certain types.
That’s not really what I’m talking about. The instance can be used, just only in internal modules, outside those modules it should not be usable.
First of all, this has nothing to do with molecular mechanics, but water is too wet, can someone provide some less wet water? --- You can say it has nothing to do with coherence, but that doesn't change the fact that almost *everything* related to Haskell type classes is related to coherence; it was a cornerstone of the theory, design, and implementation, and what distinguishes them from Scala implicits and the like.
C would be (indirectly) using the instance, even though it considers the instance "unsafe".
I recommend using an internal-use-only newtype, and have the instance on that non-exported newtype, and not on whatever exported types should not have the instance. With coercions and roles, newtypes really are free at runtime. :)
It turns out that *exactly* the described API exists in the [`futures`](http://hackage.haskell.org/package/futures-0.1) package, with literally nothing else. It's so short and simple though that I'm not sure how battle tested it is. What are peoples' experiences with this package?
??
cheers!
it worked. thanks.
__&lt;off-topic&gt;__ Please never use `unsafePerformIO`, ever. Maybe if you have 10 years of Haskell experience and working on some wild low-level behind-the-scenes stuff. --- Example in `AppMock.hs`: `mockId :: Id a` can easily just be ``` mkMockId :: MonadIO m =&gt; m (Id a) mkMockId = Id &lt;$&gt; liftIO genObjectId ``` and just have `insertPlaylist = mkMockId` --- And if you need a dummy `UTCTime`, just construct one: ``` mockTime :: UTCTime mockTime = UTCTime (fromGregorian 2019 1 1) 0 ``` --- Can I ask why you have all those classes, btw? Seems like non-FP habits. The only thing you seem to be doing is bypassing the DB and any authentication in your mock app. You can ditch all those type classes and just use different routes if you want to test the endpoints, like: ``` post "/login" $ json $ LoginResponse "jwt token lol" ``` When starting with Haskell, if you think you need to make a type class, you probably don't. [Here's some solid advice](http://www.haskellforall.com/2017/10/advice-for-haskell-beginners.html) when starting out. __&lt;/off-topic&gt;__
Is the argument that this: ``` data Monoid a = Monoid { identity :: a , op :: a -&gt; a -&gt; a } class TheMonoid a where theMonoid :: Monoid a ``` Is easier to grasp/understand than the traditional way, including for newcomers? I'd tend to disagree. The old method boils down to "you make your type part of this typeclass by providing the required functions which define its instance". While the new way this more convoluted: "to define an instance you need an intermediary data type for a record that will hold the functions defining the typeclass", which makes building and using a typeclass more cumbersome.
Nice trick! I sat down with [Soares Chen](https://github.com/maybevoid) at Monadic Party and started writing something like this, but we'd somehow missed the `IP` connection, and were trying to fake it by converting &lt;-&gt; our own class. This is way cleaner than what we had.
I just started using syntax highlighting and now I have to stop? Brutal.
This isn’t about forbidding instances. This is about hiding instances the same way you hide constructors. So the example doesn’t make much sense. Let’s say module A is internal and has access to the instance, module C is external and does not, module B imports them both and uses a function in C with the module A instance. Nothing bad happened here, if module A didn’t want it’s function that uses that instance exported it shouldn’t have exported it. Basically I’m just saying if you desugared all the implicit instance passing and made it explicit. Then you wouldn’t be able to explicitly apply an instance that wasn’t exported to you.
That's not the argument as I see it. You can keep the current syntax and think of it the current way, that doesn't change the fact that classes can *also* be thought of as records, as evidenced by this entirely mechanical translation, as well as the way GHC actually compiles classes. So `class` conflates a `data` declaration plus a mechanism for implicits, except that the `data` part is currently hidden from sight. The point is that making that `data` component explicit, however we manage to do it, enables new idioms that subsume and extend a certain number of features/other idioms, mainly deriving (all bullets except the last one from OP) and "non-canonical instances" (the last bullet). &gt; There is no a priori reason why there should be one way to express structure. The way I would say this instead is that we *can* express the structure of a `class` in the same way as a `data`. It doesn't mean everyone *should* do it that way, and in fact we can keep `class` as it is, it would then be syntactic sugar. No one is forced to look under the sugar, but the point is that the surface language still becomes simpler while allowing those who care to express more.
That API is unsafe with respect to exceptions.
I can see a lot of ways this could go wrong. I still think you are better off sticking the "unsafe" or "forbidden" instance on your own internal-use-only newtype and just not exporting that newtype (at all). But, yes, I can also see ways that you could add a "hidden" / "forbidden" flag to an instance and prevent (or warn) on it's use, but still have the compiler aware of the instance for overlap / coherence checks. You'd have to define what happens when you get the instance from multiple imports and at least one has the "hidden" flag and at least one does not. But, I guess you'd just default to the old behavior. If you want to use the Because all instances are always imported and (re-)exported, you'd need to be careful to apply the "hidden" mark in some "closed surface" around the definition, if not exactly on the definition. If you can only apply the flag at the definition site, you you'd be able to share the implementation with other modules in the same package. If you don't close the surface, then you've exposed a way to get your instance without the "hidden" flag.
Unsafe meaning what exactly? The exception behavior I would want is that exceptions thrown in the future stop computation on that thread, and the exception is re-thrown whenever another future or the main thread joins on that computation. Are you saying it would do something other than this?
It was hard for me to follow because your code is mangled in this Reddit app, but it sounds to me like you want some kind of thread pool and a work queue. Have you looked at the async package? It's pretty much the go-to concurrency package.
Ok I don’t think you get what I mean. Maybe I wasn’t clear enough in the op. I can say with 100% certain this will not effect coherence or the old Ord Map dict shenanigans. This change will cause strictly less programs to compile. It would not allow you to put a new instance down on top of the old hidden one, that would not compile.
I have; the examples in the post are all using the `async` package. The ergonomics are great but I get the sense I'm not using it right, because worker threads are not started until the value is demanded, which kind of defeats the purpose.
He is referring the question to someone else who might have a good answer for you.
Thanks, Gershom, for the panoply of interesting facts and references, both here and in your original essay. &amp;#x200B; (By the way, I second Gershom's recommendation of Vickers' "Topology Via Logic" and also, really, anything by Shulman. )
When using the RIO library, there is a pattern of startup that is exemplified by the standard rio stack template. You define your App type in Types.hs. In Main.hs, it calls runRIO with the App, and that gets stored in your Reader. The App contains things like a log func, process context, and other configuration data. But what if determining that initial config is really complex? You have to load some config files, and do some calculations on those. There might be an error during that phase - say, a config file is missing, or incorrectly formatted. You'd like to report that error using RIO's logging functions, but... you can't. They aren't available until you're in the RIO monad. Would it be considered a reasonable practice to have two stages of runRIO, with two different App structures? One just for the initialization phase, where everything would get logged to, say, the console. Then another one for once your app has been bootstrapped.
I've been [streaming](https://www.twitch.tv/jappiejappie/) my reflex adventures the last few days. Also on [youtube](https://www.youtube.com/channel/UCQxmXSQEYyCeBC6urMWRPVw?view_as=subscriber).
Yeah that should work ok most of the time. Although I foresee it being very annoying when extensible records and such come around. One smell test I often use is how it would compare to an OOP language. I consider Haskell to be much better than any OOP language so for any given feature I want to show why it’s unnecessarily entirely or show why Haskell does it better. Once we have extensible records having private fields will be more ergonomic in OOP than in Haskell. As assuming you define a type to be a newtype over an extensible record, then you will need to not autoderive instanced for the fields that are private, and will instead have to unwrap the newtype every time you use them.
So I still don’t hate my original suggestion of being able to name an instance to make it private by default. Then you have to export/import the name explicitly to use it. Note that this would not allow explicitly passing that name into a function or anything like that, it’s not a true named instance, it’s just for export control. If you ever get overlapping instances compilation should fail, no matter which ones are hidden vs not. I don’t want to mess with coherence.
I'm a newbie, and you guys with your "monads", "monoids", "endofunctors", and "what's the problem?"s can be a bit intimidating sometimes. Aware of my tyro status, I qualified a lot of my claims above. I probably hedged too much, though. Here I'll ask a few follow-up questions, and I'll be more direct this time. &amp;#x200B; \- Does anyone know how many natural transformations there are between 'T &lt;&amp;&gt; T' and 'T', where 'T' is lax? I'm assuming the number is '|M\^(C\*C)|', but I'm not sure. &amp;#x200B; \- Do any of you have any quibbles about using 'ap' to define '&lt;\*&gt;' in the 'Applicative' typeclass? You do it but wrinkle your nose? I ask because, from this novice's perspective, it seems off somehow, too arbitrary. But that viewpoint could just be a result of my inexperience. So, those of you (maybe all you haskellers) who support the use of 'ap', what justifies its use? (Actually, I've read comments made on Reddit by people on both sides of this question, so I'd be interested to see where the mean opinion lies.) &amp;#x200B; \- Would a second 'Applicative' typeclass, one where 'ap' is discouraged, have any utility? Or does it already exist? (Of course, anyone can just write up a second 'Applicative' whenever, so I mean, specifically, a compromise typeclass with community support.)
I can see that. Named instances that you can't explicitly pass is probably going to strike people as weird, but I can see at least some utility in naming things even we still have coherence. Especially if we introduce any way to modify how an instance is exported, then it's good to have a name to use for the entry in the export list.
High technical skill or achievement should not excuse bad behavior. That said, using medical techniques developed by Nazis doesn't endorse National Socialism or whatever corrupted version of such was practiced by the Third Reich. Similarly, accepting the PR with the best code or using the fasted/safest/best-documented library doesn't endorse the author's behavior. Sufficiently bad behavior can / should / will get you removed from where code is discussed / shared, though. That doesn't invalidate any technical masterpiece you make; it just makes it harder to "advertise". If your work is sufficiently grand, it will be found.
&gt; How would you feel if you'd tried really hard to actually learn $LANG for X months and you were brushed off with a RTFM, or "go to #lang-beginners", or... I'd be surprised and disappointed that I hadn't already been in #lang-beginners several times in those months.
`unsafePerformIO` is almost never what you **want**, even in the rare case when it is what you *need*.
Yeah I’m trying to think of any dual purpose we can give these instance names. But any form of direct passing would necessitate losing coherence, so I’d probably stick with my earlier suggestion of just letting functions choose between `Monoid_ a -&gt; ...` and `Monoid a =&gt;`.
This is awesome, I’ll have to check it out.
This was inspired by https://neilmitchell.blogspot.com/2018/11/downloading-all-of-hackage.html, but the solutions presented in there were unbearably slow, so I've parallelised it a bit. Also, if somebody can figure out why `cabal get` is 80x slower (100% CPU-bound), I'd appreciate it.
Thanks!
Link gives a 404
Can static-haskell-nix build haskell-ide-engine?
oh man i've been wanting something like this ! thanks
`ap` as a default implementation for `&lt;*&gt;` for an `Applicative` that happens to be a `Monad` is well justified by the existence of a canonical distributive law for `(-&gt;) a` over all functors from Hask to Hask. Given the lack of a standard name, let's call that `dist :: Functor f =&gt; f (a -&gt; b) -&gt; a -&gt; f b`. We can build `[Hask,Hask]` into a monoidal category in several ways. One way is by taking the tensor product to be Day convolution, with its unit being the identity functor. Monoid objects with respect to that category are Applicatives. This symmetric monoidal category is `([Hask,Hask],Day,Identity)`. Another way is taking the tensor product to be functor composition, with its unit being the identify functor. Monoid objects with respect to that category are Monads. This yields a monoidal category `([Hask,Hask],Compose,Identity)`. There is a monoidal functor from `([Hask,Hask],Compose,Identity)` to `([Hask,Hask],Day,Identity)`. It is derived from the aforementioned mapping from `f (a -&gt; b)` to `a -&gt; f b` which can be constructed using the strength of every functor in "`Hask`", from the fact that we have an internal hom as a CCC, etc. (You can come up with lots of ways to think about this. e.g. exploiting that (a -&gt;) is a right adjoint so it preserves limits, or operationally by thinking of capturing 'a' in the environment, but regardless, there is a mapping `dist :: Functor f =&gt; f (a -&gt; b) -&gt; a -&gt; f b`, which induces this functor). This is basically the thinking behind theorem 8.1 in [Notions of Computation as Monoids](https://arxiv.org/pdf/1406.4823.pdf), which probably provides much clearer exposition than I do here. Ultimately the "power to handle `(a -&gt; f b)` is more than enough to handle `f (a -&gt; b)` through the mere existence of `foo` above. That mapping isn't quite enough to lock down the meaning of `(&lt;*&gt;)` in terms of `(&gt;&gt;=)`, Day convolution is a _symmetric_ monoidal category, so you could also use this to implement the "effects" in the opposite order, but convention makes that choice as with `Traversable`. We made `Applicative` a superclass of `Monad` for a reason. The reason is the existence of the canonical mapping above. By the same token `Traversable` is strong enough to imply `Functor` and `Traversable`, `Comonad` implies `Functor`, etc. Removing the law that `(&lt;*&gt;) = ap` from `Monad` would mean that we'd need to forever care about distinctions between things like `traverse` vs. `mapM` and could never be able to silently erase the former from history. You could never safely refactor `foo = do bar; baz` to `foo = bar *&gt; baz` as that might have some subtle invisible distinction because the two classes would be divorced from each other in meaning. We'd forever be accumulating code that hinged on a distinction that adds no power. The code duplication to deal with the distinction we had before is already bad enough, and we've yet to fully heal from it, as evidenced by the fact that both `mapM` and `traverse` _still exist_. Consciously reintroducing it sounds like an absolute fucking nightmare to me (offense not intended, though the strength of feeling is _quite_ intended.) Haskell is generally quite bad at adding operations and laws in separate steps, so having a "ApplicativeButNotExactly" as a superclass of "Applicative" and then having Applicative which acts as a guarantee that "no I really meant it to be compatible with any Monad that might come along" or something like that just doesn't work with the language we have. Type inference won't pick up on the fact that you meant the compatible one, and you'll always have to worry about which you meant. On the other hand, having a separate copy of the entire 'Applicative' ecosystem off to the side means rampant cut and paste coding. Having `Monad` and `Applicative` be related means that `ZipList` can't dip over and have `[]` like `Monad` instance, despite the fact that the `Applicative` does something completely unrelated. This is a good thing!
Thanks!
Nice write-up!
Just chiming in to say I don’t like ImplicitParams and hope people don’t use them. Neat idea but too magical and risky.
GHC shouldn’t allow implicit parameters at all.
Same here, perhaps a private repo?
I disagree with that! I think the rough edges are fixable, and they could make a really useful feature.
Better download the entire Internet to make sure to have all the foreign dependencies included.
You *could* add another OVERLAPPABLE instance for HasNat (a, b) with the type error?
&gt; for example right now to figure out how many packages depend on integer-gmp Fwiw, you don't need to download all of Hackage for that; there's companion tool for cabal I'm working which can accomplish queries such as those in under a second and without downloading all of Hackage: $ haquery rdepends integer-gmp --vstyle=cabal3 | grep -v '^$' | cat -n 1 DSA ^&gt;= { 1 } 2 HsOpenSSL ^&gt;= { 0.7, 0.8, 0.9, 0.10, 0.11 } 3 acme-everything ^&gt;= { 2015.4.15.1 } 4 aern2-mp (^&gt;= 0.1.0.0 &amp;&amp; &lt; 0.1.3) 5 aeson (^&gt;= 0.3.2.0 &amp;&amp; &lt; 0.3.2.5) 6 altfloat ^&gt;= { 0.2.1, 0.3 } 7 arithmoi ^&gt;= { 0.1.0.0, 0.2.0.0, 0.3.0.0, 0.4.0.0, 0.5.0.0, 0.6.0.0, 0.7.0.0, 0.8.0.0, 0.9.0.0 } 8 asn1-codec ^&gt;= { 0.1.0, 0.2.0 } 9 base ^&gt;= { 4.2.0.0, 4.3.0.0, 4.4.0.0, 4.5.0.0, 4.6.0.0, 4.7.0.0, 4.9.1.0 } 10 beamable ^&gt;= { 0.1.0.0 } 11 bencoding ^&gt;= { 0.4.4.0 } 12 bitset ^&gt;= { 1.3.0, 1.4.0 } 13 blaze-textual ^&gt;= { 0.1.0.0, 0.2.0.0 } 14 blaze-textual-native ^&gt;= { 0.2.1 } 15 buffer-builder-aeson ^&gt;= { 0.1.0.1, 0.2.0.0 } 16 bv ^&gt;= { 0.4.0, 0.5 } 17 bv-little ^&gt;= { 0.1.0.0, 1.0.0 } 18 bytestring ^&gt;= { 0.10.4.0 } 19 bytestring-show ^&gt;= { 0.3.3 } 20 cantor-pairing ^&gt;= { 0.1.0.0 } 21 cborg ^&gt;= { 0.1.1.0, 0.2.0.0 } 22 clash-ghc ^&gt;= { 0.99 } 23 clash-lib ^&gt;= { 0.6.19, 0.7, 0.99 } 24 clash-prelude ^&gt;= { 0.6, 0.7, 0.8, 0.9, 0.10, 0.11, 0.99 } 25 couch-simple ^&gt;= { 0.0.1.0 } 26 crypto-numbers ^&gt;= { 0.2.2 } 27 cryptonite ^&gt;= { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26 } 28 discrimination ^&gt;= { 0.4 } 29 double-conversion ^&gt;= { 0.1.0.0, 0.2.0.0 } || (^&gt;= 2.0.1.0 &amp;&amp; &lt; 2.0.2) 30 eccrypto ^&gt;= { 0.1.0, 0.2.0 } 31 exact-real ^&gt;= { 0.2.0.0, 0.3.0.0, 0.4.0.0, 0.5.0.0, 0.7.1.0, 0.8.0.0, 0.9.0, 0.10.0, 0.11.0, 0.12.0 } 32 fast-arithmetic ^&gt;= { 0.2.3.0 } 33 fast-digits ^&gt;= { 0.1.0.0, 0.2.0.0 } 34 fast-mult ^&gt;= { 0.1.0.0 } 35 fib ^&gt;= { 0.1 } 36 fixed-precision ^&gt;= { 0.2.0, 0.3.0, 0.4.0 } 37 floatshow ^&gt;= { 0.1, 0.2.0 } 38 formatting ^&gt;= { 6.3.0 } 39 funflow ^&gt;= { 1.0.0, 1.1.0, 1.3.0, 1.4.0 } 40 galois-field ^&gt;= { 0.1.0, 0.2.0 } 41 ghc-instances ^&gt;= { 0.1.0.0 } 42 ghc-typelits-extra ^&gt;= { 0.1.3, 0.2, 0.3 } 43 ghc-typelits-natnormalise ^&gt;= { 0.4.4, 0.5, 0.6 } 44 ghcjs-base ^&gt;= { 0.2.0.0 } 45 gore-and-ash-network ^&gt;= { 1.1.0.0, 1.2.0.0, 1.3.2.0, 1.4.0.0 } 46 hashable ^&gt;= { 1.1.2.4, 1.2.0.0, 1.3.0.0 } 47 hashabler ^&gt;= { 0.1.0.0, 1.0, 1.1, 1.2, 1.3.0, 2.0.0 } 48 haskell-mpfr ^&gt;= { 0.1 } 49 haste-compiler ^&gt;= { 0.2.99, 0.3 } 50 haste-lib ^&gt;= { 0.6.0.0 } 51 haste-prim ^&gt;= { 0.6.0.0 } 52 hgmp ^&gt;= { 0.1.0.0 } 53 hmpfr ^&gt;= { 0.3.3, 0.4.0 } || (^&gt;= 0.3.1 &amp;&amp; &lt; 0.3.2) 54 hstox ^&gt;= { 0.0.1 } 55 integer-logarithms ^&gt;= { 1 } 56 jose ^&gt;= { 0.3.38.0 } 57 long-double ^&gt;= { 0.1 } 58 mcl ^&gt;= { 1.0.0 } 59 numerals ^&gt;= { 0.4 } 60 pantry-tmp ^&gt;= { 0.1.0.0 } 61 pregame ^&gt;= { 1.0.0.0 } 62 pvss ^&gt;= { 0.1, 0.2.0 } 63 ron ^&gt;= { 0.5, 0.6 } 64 ron-rdt ^&gt;= { 0.5, 0.6 } 65 ron-schema ^&gt;= { 0.5, 0.6 } 66 ron-storage ^&gt;= { 0.5, 0.6, 0.7 } 67 scientific ^&gt;= { 0.3.1.0 } 68 semirings ^&gt;= { 0.1.0, 0.2.0.0, 0.3.0.0, 0.4 } 69 simple-enumeration ^&gt;= { 0.2 } 70 ssh ^&gt;= { 0.3 } 71 stdio ^&gt;= { 0.1.0.0, 0.2.0.0 } 72 store ^&gt;= { 0.1.0.0, 0.2.0.0, 0.3, 0.4.0, 0.5.0 } 73 text ^&gt;= { 0.11.1.0, 1.0.0.0, 1.1.0.0, 1.2.0.0 } 74 text-format ^&gt;= { 0.1.0.0, 0.2.0.0, 0.3.0.0 } 75 text-show ^&gt;= { 0.4, 0.5, 0.6, 0.7, 0.8, 1, 2, 2.1, 3, 3.1, 3.2, 3.3, 3.4, 3.6, 3.7, 3.8 } 76 text-utf8 ^&gt;= { 1.2.3.0 } 77 urn-random ^&gt;= { 0.1.0.0 } 78 variable-precision ^&gt;= { 0.2, 0.3.1, 0.4 } &gt; but the solutions presented in there were unbearably slow I have something in the works for that too... stay tuned
Sadly, GHC complains about duplicate instance declarations. With: instance {-# OVERLAPPABLE #-} TypeError NoHasNatFound =&gt; HasNat (a, b) I get: HasNat.hs:36:31: error: Duplicate instance declarations: instance [overlappable] (TypeError ...) =&gt; HasNat (a, b) -- Defined at HasNat.hs:36:31 instance (GetNat a ~ GetNat b) =&gt; HasNat (a, b) -- Defined at HasNat.hs:37:10 | 36 | instance {-# OVERLAPPABLE #-} TypeError NoHasNatFound =&gt; HasNat (a, b) | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Failed, no modules loaded.
Could you fix the formatting of your code? It needs to be indented by four spaces rather than fenced off with ```
I realized that later, and checked the person's account. He's been working on this problem for a long time. I didn't realize it was such a difficult problem.
:( I don't know, then. Sorry!
It seems like you can check equality of type-level natural numbers using an additional custom type family: instance NatEq a b =&gt; HasNat (a, b) where type GetNat (a, b) = GetNat a type family NatEq (n :: Type) (m :: Type) :: Constraint where NatEq n m = If (GetNat n == GetNat m) (GetNat n ~~ GetNat m) (TypeError NoHasNatFound) This worked for me: ghci&gt; getNat (ContainsNat1 @5, ContainsNat2 @5) (ContainsNat1,ContainsNat2) ghci&gt; getNat (ContainsNat1 @3, ContainsNat2 @5) &lt;interactive&gt;:18:1: error: • You dun goofed • In the expression: getNat (ContainsNat1 @3, ContainsNat2 @5) In an equation for ‘it’: it = getNat (ContainsNat1 @3, ContainsNat2 @5)
(as seen in the latest Haskell Weekly)
But why? For automatically choosing functionality typeclasses are fantastic because things like global coherence make them far more predictable. When coherence is too strict and you want choice in what you pass in, just use explicit parameters. If there are lots of explicit parameters being passed around everywhere just group them together into record. If even that is too annoying then wrap it up in a `Reader`. I think implicit parameters are way too magical and implicit (duh), and they make debugging and understanding code much harder. For similar reasons I also don't like RecordWildCards.
Sadly, GHC doesn't take constraints into account when matching instances - this can be used to help inference sometimes (using type equality trick) but it makes your instances look the same to the compiler. But, we don't really need separate instances - we just need to create constraint that fails on invalid case: type family SameOrError (a :: k) (b :: k) (msg :: ErrorMessage) :: Constraint where SameOrError a a _ = () SameOrError _ _ msg = TypeError msg instance SameOrError (GetNat a) (GetNat b) NoHasNatFound =&gt; HasNat (a, b) where type GetNat (a, b) = GetNat a
Awesome! That's exactly what I was looking for :). I'm really excited to get these error messages in. They should make things so much more accessible.
I think it should be very easy to define a `Monad` instance for `Async` (or more likely, a `newtype` wrapper around it), but I'm afraid it might be very easy to mess up exceptions and leak threads all around the place while doing so. FWIW, here's a way to achieve what you want using the `async` API: import Control.Concurrent import Control.Concurrent.Async worker :: Int -&gt; IO Int worker n = do threadDelay 1000000 return (2 * n) nextValue :: Int -&gt; IO Int nextValue n = do threadDelay 100000 return (n + 1) mainLoop :: Int -&gt; [Async Int] -&gt; IO [Int] mainLoop 10 l = reverse &lt;$&gt; mapM wait l mainLoop n l = do n' &lt;- nextValue n withAsync (worker n') $ \resultH -&gt; mainLoop n' $ resultH : l main = mainLoop 0 [] &gt;&gt;= putStrLn . show
This is a great summary. I'd be interested to know which packages broke their APIs. Most that I'm familiar with are meticulous about their deprecation cycles and easing the user experience of upgrading.
Does this do what you want? asyncResults :: Int -&gt; IO [Async Int] asyncResults 10 = pure [] asyncResults !n = do n' &lt;- nextValue n result &lt;- async (worker n') results &lt;- asyncResults n' pure (result : results) main = do asyncs &lt;- (asyncResults 0) nums &lt;- traverse wait asyncs print nums
One very painful example is `proto-lens` library. It's used to generate Haskell data types and decoders/encoders for them from Protobuf message definitions. In earlier library versions, constructors of those data types were exported from the generated modules. So you could create values like this: pure UserResponse { _UserResponse'name = userName , _UserResponse'age = userAge , ... } It is a clean, understandable and safe Haskell. This wasn't perfect (due to long field names), but this was okay. However, in newer versions, library authors decided not to export constructors anymore. Instead, now you need to use overloaded variable `defMessage` and lenses to create values like this: pure $ defMessage &amp; name .~ userName &amp; age .~ userAge Migration to this way of defining values was painful and resulted in a massive refactoring of the whole codebase. Because of this change, the current code is less typesafe and less maintainable despite our best efforts to make it clean (enforcing explicit type applications for `defMessage`, moving defined data types to boundaries of the application, etc.).
My bad, I had indeed clicked the `Private` button accidentally. Fixed!
Could you elaborate more on your experience debugging code with implicit parameters? In my experience, they are easier to reason about than Reader. The semantics are very similar, but implicit params are a lot more compositional, because individual functions can be more precise in the arguments they take. I too prefer not to use RecordWildCards, but I don't immediately see the connection - RWC bring into scope variables that are not syntactically visible, but implicit params are declared in the function signature, which makes them similar to explicit arguments in this regard.
According to my understanding the author misunderstands quite some things about OOP, FP, Erlang, ... . But [i also dislike OOP](https://libeako.github.io/c/ID_205033498.html).
&gt; worker threads are not started until the value is demanded That reminds me of the "Tasks vs Future" debate in Scala. [Here is a presentation I gave on the topic](https://www.youtube.com/watch?v=8n4A0qXb15Q&amp;feature=youtu.be&amp;t=848)). Here is a summary. A Future starts executing as soon as you define it, whereas a Task does not begin executing until you run the computation, that is, after you have already constructed the whole computation. Furthermore, each Future runs in a separate thread, whereas with Tasks, you need to be explicit about which Tasks should run in parallel with which other Tasks. As a result, using Future takes very little effort in order to get a lot of threads running in parallel, but that means you get very little control over the number of threads. This can be bad if your tasks are very short, because in that case, adding more threads makes the computation slower, because of the overhead of forking and waiting on threads. In your case, your tasks are intentionally very slow, so a Future API does seem more appropriate. `async` provides both a Future-like API, via `async`, and a Task-like API, via combinators like `race` and `concurrently`.
Reddit is inconsistent; I see the backticks and poor indentation on mobile but I see properly-formatted code on desktop!
&gt; except that `Async` is not a monad You're not using `Future` as a Monad though, you're only using it as an Applicative. Async is not an Applicative either, but Concurrently is! I know you've already tried using Concurrently and that it didn't work for you, but that's because you're building up a large Concurrently computation and then only starting it once the computation is fully constructed, after all the `nextValue` calls have completed. You need to use each primitive for its strength: `async` for starting threads early, and `Concurrently` for combining their results. Here is my solution: mainLoop :: Int -&gt; IO [Int] mainLoop 10 = return [] mainLoop n = do n' &lt;- nextValue n fut &lt;- async (worker n') runConcurrently $ (:) &lt;$&gt; Concurrently (wait fut) &lt;*&gt; Concurrently (mainLoop n')
&gt; which is bizarre since it's the number one monad in javascript so I would have expected the Haskellers to be all over it. Is it really a monad though? In a typical Javascript program, you would call e.g. `httpPost("tiny.com/shorten?path=reddit.com/r/haskell")`, and that function returns a Promise. I can chain that Promise, by calling its `then` method on a function which takes the result of that Promise and computes another Promise. Maybe that POST request returns the string `tiny.com/rrh`, and then we want to make a GET request to that url in order to make sure it redirects to `reddit.com/r/haskell`: httpPost("tiny.com/shorten?path=reddit.com/r/haskell").then(httpGet) If this was Haskell, then it seems like the types of the relevant functions should be as follows, right? httpPost :: String -&gt; Async String httpGet :: String -&gt; Async String then_ :: Async a -&gt; (a -&gt; Async b) -&gt; Async b testShorten :: Async String testShorten = httpPost "tiny.com/shorten?path=reddit.com/r/haskell" `then_` httpGet Well, not quite! Async is reference to a computation which is already executing in another thread. The function which launches that thread, `async :: IO a -&gt; IO (Async a)`, runs in IO, not in Async. Async is a reference, not a type of computation, so it doesn't make much sense to be running "in" Async. In Javascript, any function can perform side-effects, but in Haskell, we like to use types like IO to track side-effects more precisely. So we would write this instead: httpPost :: String -&gt; IO (Async String) httpGet :: String -&gt; IO (Async String) then_ :: Async a -&gt; (a -&gt; IO (Async b)) -&gt; IO (Async b) testShorten :: IO (Async String) testShorten = httpPost "tiny.com/shorten?path=reddit.com/r/haskell" `then_` httpGet And now we can see that the type of `then_` isn't quite right for a Monad instance. Unlike Javascript, Haskell supports blocking calls, so we would probably write this instead: httpPost :: String -&gt; IO String httpGet :: String -&gt; IO String testShorten :: IO String testShorten = httpPost "www.shorturl.com/shorten?path=www.reddit.com/r/haskell" &gt;&gt;= httpGet No Async needed! A monadic API is intrinsically sequential, so it makes sense that neither Async nor Concurrently has a Monad instance. Instead, the way to use `async` is to use Async, Concurrently, and IO in different places: IO to define sequential segments, Concurrently to run different sequential segments concurrently, and `Async` to start computations at the point where we have all the information we need to run run them rather than at the point where their value is needed. Both of those points are in some IO segment, possibly nested inside some larger Concurrently computation, possibly nested inside some yet larger IO segment.
Why is the pure version just the `IO` version, but with `unsafePerformIO`!?
&gt; It turns out that exactly the described API exists in the futures package Not quite: Future has a Monad instance, but `(&gt;&gt;=) :: Future a -&gt; (a -&gt; Future b) -&gt; Future b` does _not_ mean that `Future` has a Javascript-like API. In Javascript, we can extend a Promise with a post-computation: once the `a` is available, that post-computation can use this `a` to launch another asynchronous computation, and then the extended Promise will wait until that second asynchronous computation is complete. But with `futures`, we need to call `fork :: IO a -&gt; IO (Future a)` in order to launch another asynchronous computation! We cannot do that inside a pure function of type `a -&gt; Future b`, we can only return a `Future b` which is already in scope. So you can, say, launch three asynchronous computations, and then extend the first computation so that depending on its result, the extended computation will join the second or the third computation. But you cannot launch a single computation, and then extend it so that depending on its result, the extended computation will either launch the second or the third computation.
The documentation for `futures` specifically says "The IO action must not throw exceptions!". Looking at the code, it's pretty clear that if an exception is thrown inside the thread, the `putMVar` will never be executed, and so when the main thread joins, it will hang forever on the `readMVar` instead of rethrowing the exception.
Im late but the other solution is to use the record state type and use `gets age` and `gets name`. You could then roll your own getters. ``` getAge = gets age newName = gets name ``` Last time I wrote a type inferencer I did essentially this, but I seperated the name generation into its own transformer layer with a getter called `supply` and then didnt give it a MonadState instance.
Ok, if you have an email _not_ on the recursion.ninja domain, can you use it to forward the bounce notices to admin@haskell.org, and we can proceed from there?
I would think that only \`packages\` and \`store\` should be shared. They should both only contain user-independent information. The "leakage" would be information leakage in that users would be able to know which packages other users had built and downloaded, and potentially in what configuration they were built. &amp;#x200B; User customizations are in \`config\` and it should not be shared, and there is no special reason to share \`logs\`.
The key to herbert's companion tool is that all `.cabal` files, including revisions, are already in the index-01.tgz tarball. So using a tarball processing library, one can read out all that data in a streaming fashion and walk through dependency information, descriptions, etc. -- basically everything but the code itself. So many ecosystem investigations can be conducted on that basis alone. Vis a vis `cabal get` perhaps it isn't set up to use an external http-transport?
Nice solution and you don't even need the explicit `async` or the `wait`. You can just say `Concurrently (worker n')`
To me, a comment like "One problem with your library is X." sounds like a rather normal, to-the-point comment that you would make in a technical discussion on the internet. If you see bad intentions in this, or get discouraged by such a comment, it's your problem. The second style on the other hand sounds so patronizing it's almost insulting. People communicate in different ways. Assuming good faith on the other side is key to communication on the internet, especially on technical topics. If you come across comments in a style you don't like, just focus on the technical feedback and move on.
Thank you!
Interesting. That's the kind of thing where a fork sounds warranted!
There was an interesting discussion about this exact breakage [here](https://github.com/google/proto-lens/issues/314). I do think that /u/chshersh is being a little unfair here with respect to `proto-lens`. The package authors appear to have extensive experience working with protocol buffers and these changes were made to bring the Haskell library more in line with how they are meant to be used/are used in other applications. In particular, I found this comment very helpful in reconciling the perceived lack of type safety in the change: &gt; For Haskell code this means we start with the notion of defMessage. So, yes, we sacrifice the ability to enforce Haskell level invariants to be align with protocol buffers goals.
I share your knee jerk reaction, but... I've also come to find that there are a number of limited circumstances under which implicit parameters lead to significantly cleaner (and faster) code than a large reader environment. GHC can plumb them down to just the parts of your code where they are used.
Yes! I also think that the the confusing behaviour is fixable: GHC should simply never implicitly generalise over an implicit parameter. That way, if you want a function with an implicit param, you must declare it explicitly.
Performance is not particularly compelling to me, as that seems like something that GHC could be improved to deal with, similar to my desire for pure performance backpack uses to be replaced with a smarter GHC. Cleaner code is an interesting argument though, so I'm curious if you have any examples on hand. It seems like with the `Has` pattern, plus if needed things like `local` and `lens`, you should be able to do anything you can with implicit params with `Reader`. I'm kind of guessing at this point what your example is going to be, but perhaps there are some situations where anonymous extensible records/rows/variants etc. are needed to match the ergonomics of ImplicitParams.
[This thread](https://www.reddit.com/r/haskell/comments/6gz4w5/whats_wrong_with_implicitparams/) from a while is probably a good read. But in general you can't easily tell at the call site where implicit params are being used, as absolutely any function that has an implicit param in scope could be using it. Also type inference further complicates things quite significantly. I should have been more clear about RWC, i'm talking about covariant use of it (although I dislike both usages of it fairly equally): ``` foo x y z = Foo {..} &lt;..&gt; Bar {..} &lt;..&gt; Baz {..} where bar = x + y baz = bar + z qux = bar + baz ``` I have no idea which of `x`, `y`, `z`, `bar`, `baz` and `qux` are used in which of `Foo`, `Bar` and `Qux`. This is very similar to the situation with ImplicitParams: ``` foo x y z = foobarbaz x y z where ?bar = quxqux (x + y) (z + z) ?baz = barbaz y z + bazqux x ?qux = foofoo + barbar + quxqux x (z * y) ``` Now the advantage compared to last time is I know only `bar` `baz` and `qux` can be sent around automagically, but the disadvantage is that there are even more places they can be sent off to, hell even `+` could have been redefined to take them in. Compare this to `Reader` which is strictly better than both (but not perfect): ``` foo x y z = do bar &lt;- quxqux (x + y) (z + z) let baz = barbaz y z + bazqux x qux = foofoo + barbar + quxqux x (z * y) local (\x -&gt; x { xBaz = qux }) (foobarbaz x y z bar baz) ``` This code is a lot more clear. We know that the implicit stuff is only going in to `quxqux` and `foobarbaz`, and the only modification we are doing to the environment is adding in `qux` when we call `foobarbaz`. With proper extensible records you could do something like `local (subset [#foo, #bar]) foobarbaz` to make it clear at the call site that only the `foo` and `bar` fields are being sent off instead of all of it, without having to look at the definition of `foobarbaz`. Also I wish `let` was optional and record syntax was clear, but that's kind of just nitpicking. In general I don't like implicitness but I also do like conciseness and ergonomics. So typeclasses are an acceptable tradeoff as the conciseness and ergonomics are fantastic and coherence makes them feel much less magical and hard to reason about. But ImplicitParams and RecordWildCards are too far into magic-land IMO.
Note that presumably you would make the location of the store and packages directories writable to all users, and thus a malicious user could overwrite the package data of a package installed by an other user. So from a security perspective that is something to consider.
Would this allow one to replace part of the contents of a package a user observed some other use must be using with a version that did something different? It seems it might as for this to be useful all users would presumably have write access. &amp;#x200B; Of course systems are not very secure locally in general.
I would also argue that the new way at teaching it would really get at the fundamentals better, which is that typeclasses are just a way to have extensible functions from types to values. So `Show` is just a function that converts a type to a printer for that type, and `Eq` is just a function that converts a type to an equality checker for that type. It's also worth mentioning that with extensible rows/records/variants you can easily end up with something like: ``` class Monoid a = monoid :: Record { identity = a , op = a -&gt; a -&gt; a } instance Monoid [a] = { identity = [] , op = (++) } ``` So you don't necessarily need a separate data-type if you don't want, you can just use an anonymous extensible record. And as mentioned syntax sugar can always be kept around, so that the above would be expressed as before, although that would of course not expose `monoid` so you wouldn't have access to the underlying structure if you use the old-style definition. &gt; I think this argument is really poor. There is no a priori reason why there should be one way to express structure. I guess I can more concretely defend this point, but honestly I'm surprised it's considered an arguable point. Having two ways to express structure is redundant and adds unneeded complexity for the compiler and the user. With two forms of structure, every useful existing concept that deals with and manipulates one form of structure needs to be copied over to manipulate the other form of structure as well. You can see this precisely in the proliferation of `Deriving*` extensions, whereas without this duplication the natural expressiveness and standard evolution of Haskell would be more than enough to keep up.
In [this “Futue of Coding” podcast episode](https://futureofcoding.org/episodes/011) one of ReactJS’s creators Pete Hunt tells extensively about his experiences with negative and sometimes borderline “hateful” feedback in the Internet. I find his approach about it which he tells about, which I can only summarize as “stay positive and keep communicating positively” as very inspiring.
Good questions! I _think_ that hackage security would prevent abuse of the `packages` dir, but it would be good to check. In particular, downloaded packages are validated against their hash in index-01.tgz I believe. However, I don't know if that check is also performed when using an already downloaded package in the `packages` dir. If not, we should probably file a ticket so that it is :-) Similarly, it would be good to see what happens if a package with different source is compiled and built into the `store`. It may be that the hashing is done only on name/version/options so that you could trick someone into using something maliciously patched. However, if there is a hash against the hash of the source, then this would be much less possible.
&gt;**I intend to inspire the committee to** ***move faster.*** &amp;#x200B; Cool. This may be the wrong thread but since I believe it came up in the thread on typeclasses I would love to see more willingness to make some breaking changes in the future in terms of parts of the language that pretty must everyone agrees are far from ideal (i.e. records) &amp;#x200B; &gt;**I intend to remove as many barriers to entry as humanly possible** &amp;#x200B; This would be great. IME, open source communities of an older vintage often have a more club-like atomsphere and feel less amenable to the contribution of some newcomer who just decided to help out on a particular issue. &amp;#x200B; I'm not sure I have the time/knowledge to really help out on ghc related issues at this point but your post does inspire me to invest that time. &amp;#x200B; I'd also like to stress the emphasis on tools etc and, potentially the contrast between Rust and Haskell in this respect. It must be said though that having dabbled with Haskell over the years the difference between how things are now with tools like stack, the haskell-ide-engine etc and how they used to be is immense and is more or less the difference between Haskell being something you sell to non-Haskellers as something you could viably use in your day job and something you only really used as a hobby. &amp;#x200B; OTOH part of the value of the Haskell community is its a community which is usually more interested in exploring the best way of doing something (where best often means "most correct"). One doesnt want to lose that either. Nevertheless I dont think there is an essential conflict between the goals of exploring new ideas, phasing out inferior ones, making the language itself easier to work with when you are trying to "get s\*\*\* done" and bringing more people into the fold.
This looks amazing — I often want to be able to find reverse dependencies. Do you have any idea when this will be released?
I think you can already find reverse dependencies at http://packdeps.haskellers.com/reverse.
I don't know, have not tried yet. The chances of that working would certainly be much higher if it was on Hackage ([it isn't](https://github.com/haskell/haskell-ide-engine/issues/328)) and Stackage, and thus nixpkgs. If you want to give it a shot, I can probably help with the fallout if you encounter error messages!
&gt; Vis a vis `cabal get` perhaps it isn't set up to use an external http-transport? What does that mean?
I have a Haskell plan for that too, but that is not ready yet.
Cool! How does it interact with Cabal flags though? That is, if the `build-depends: integer-gmp` is behind a flag? The query that I was particularly interested in was "find me all packages that depend on `integer-gmp` and that don't have a flag which enables building with `integer-simple` instead". &gt; I have something in the works for that too That suggests you know why it's slow! What's the reason?
https://github.com/ekmett/codex/blob/a8268a650ec6f2634610fcb9a07f82da0198955c/engine/src/Engine.hs has an awful type signature but the user experience is pretty tame. https://github.com/ekmett/codex/blob/a8268a650ec6f2634610fcb9a07f82da0198955c/engine/example.hs keeps the user in their own monad and just equips them with super powers. This pattern scales in the sense that if I need to give you a dozen different such sets of capabilities, I don't need to name some gigantic product environment/state. I use this only when I do not intend to ever call `local` so the distinction between `e =&gt; m (m a)` and `e -&gt; m (e -&gt; m a)` isn't visible to users, otherwise you can get capture problems where the semantics differ from a more traditional ReaderT based monad stack. The key for me is the portions of the environment you use get plumbed to you, and through the "ReaderT design pattern" you can emulate state and through state writer effects as well, so for the "big 3" effects, this serves as a way to meet all those needs. Leaving only figuring out which portions of state/writer logs to explicitly backtrack on error as a degree of freedom to consider, while simultaneously not lying about async exceptions. I use implicit parameters like this mostly for things that will be one-and-done setup/configuration. I use reflection-like tricks when I need to use this pattern but may need to recursively instantiate the same monad/effect at different types. The key for me is that it can offer _huge_ performance gains over working in an abstract monad transformer stack, and is less painful than working in the wrong concrete monad transformer stack, and the type checker does all the work to make it efficient, even before I lean on some worker/wrapper thing like the usual ReaderT pattern.
Even if the sources are fully protected, you could just replace the binaries, couldn't you? I assume that these aren't checked at link time. /u/kindaro, in general I would assume that any system not explicitly designed with this use case in mind, like Cabal, will be insecure. So if your users are technically competent and don't trust each other, a few extra GB of space would be the better investment.
You've kind of proved my 'intimidating' point, I think :) &amp;#x200B; But, seriously, I have great respect for you and other leaders of the Haskell community, and because I've read various remarks by you over the years regarding this issue, I recognize and appreciate the degree of goodwill shown by you in answering yet another impertinent newcomer's clone of past questions. &amp;#x200B; I still have a few hazy questions about this matter, but I think I'll ponder your response for a while and, if it's ok with you, perhaps in a few days reply to ask you if you could clear up any of my lingering misconceptions -- or even just to says thanks. Your answer's depth of detail (as well as feeling) deserve due measure of consideration. &amp;#x200B; Thanks much.
Here's a project of mine! It's going to be a website for hosting online fiction. I'm actually also a bit of a beginner, maybe approaching intermediate in Haskell, and it's very much work in progress, but it's probably illustrative if you don't know Haskell and Elm well. Feel free to ask me questions about it! https://github.com/sullyj3/Campfire
To be fair your question was couched in such a way that figuring out what the right technical level of reply you were looking for was basically impossible =) I figured best to just err on the side of including everything I could think of on the spot, given you'd already gone a day or so with hearing nothing but crickets.
The root problem is that proto3 only allows awful types due to everything being optional among other things.
I am already aware of that website, but often I find it to be a bit unreliable. Besides, it would be easier to have a command-line tool.
Well, thank you very much. You've certainly given me a lot to think about for a while. I enjoy rigorous math, but sometimes I require a span of processing time to fully appreciate it. &amp;#x200B; And, yeah, the crickets surprised me. I know category theory isn't everyone's cup of tea, but I expected a bit more interest. That's mostly why I changed up my tone so dramatically. &amp;#x200B; But, again, my thanks for your response. (Also, by the way, I've quite enjoyed your recent propagators lectures.)
&gt; --http-transport=HttpTransport Set a transport for http(s) requests. &gt; Accepts 'curl', 'wget', 'powershell', and &gt; 'plain-http'. (default: 'curl') You can pass in what tool to use for downloads, which could probably have a significant impact on download speed?
My predecessor at a previous job thought that the best thing to do with JSON was to parse it into a data structure that was originally designed for XML. That ended up being the direct cause of 3/4 of the bugs in the entire system. There’s significant overlap in functionality between JSON, URLEncoded parameters, and multipart/form-data but IMO using the same data type for any two of them is just asking for trouble.
My experience is the opposite. I barely see any deprecation warnings and we have more than 100 direct dependencies. Often times API is just gone (if we update dependencies) and the project fails to compile. \&gt; Most Haskell projects don’t really have a concept of backwards compatibility. They regularly release breaking changes because they tend to be either hobby projects or projects that simply have ambitious technical goals which don’t necessarily align with your company’s goals of having stable interfaces. I totally agree with that, but I think another reason is that PVP encourages "fire and forget" wrt API. If I compare that with my experience of e.g. the Go ecosystem, it is fundamentally different. People would rather create a new package than doing large changes to existing API and they would lose their sleep over the idea of breaking someone elses code.
Fair point. I can make a case for checking all the bits I detailed, but extending that to check the binary checksums does seem maybe a bridge too far for normal usage, and perhaps its better to just say that there must be some degree of trust for "local" files for a sufficiently narrow definition of local.
Pvp?
Are there any advantages over `memoize`?
I think you have two options: 1. Add the instance to a hidden newtype wrapper that you use but don't export. 2. Persuade the database library author to add an insert function that doesn't require an instance but takes a dictionary instead. (I've had success with this approach with postgresql-simple).
https://pvp.haskell.org/
I'm having some troubles writing a custom \`Setup.hs\`. I want to check a property of the user's machine and then potentially add a flag to the build process. Unfortunately, I couldn't find any tutorial, and dealing with \`defaultMainWithHooks\` seems a bit cumbersome.
Yes, that's why I'm making a custom data type for parsing multipart / url encoded data with my custom field name syntax. It kind of hurts me to know I'm basically re-implementing \`aeson\`'s \`Value\` with addition of \`ByteString\` support. But what the hell :D
That's very unfortunate. I guess I mostly use very general Kmettish libraries where the amount of actual "choice" in the API is quite small. &gt;PVP encourages "fire and forget" wrt API. What do you mean by that? I don't see how PVP could be different to SemVer in that regard.
It is not much different to semver. Both encourage fire and forget and are a pain to deal with.
It is cumbersome, but you also have a pretty specific use case that requires the most flexibility. In this SO answer you can find a number of examples of custom Setup.hs files that override the cabal configuration hooks : https://stackoverflow.com/questions/32676638/how-can-i-pin-a-version-of-a-haskell-dependency-to-a-version-of-an-underlying-na/32679607#32679607
Hey. I am trying to access the Ptr value in a GLContext provided by SDL2 library. Here is the function to get a GLContext: http://hackage.haskell.org/package/sdl2-2.5.0.0/docs/SDL-Video-OpenGL.html#v:glCreateContext How do I get access to the Ptr inside? import qualified SDL as SDL import qualified SDL.Internal.Types as SDL main :: IO () main = do SDL.initializeAll window @ (SDL.Window wp) &lt;- SDL.createWindow "My SDL Application" SDL.defaultWindow context @ (SDL.GLContext gl) &lt;- SDL.glCreateContext window putStrLn $ show wp Not in scope: data constructor ‘SDL.GLContext’ Neither ‘SDL’ nor ‘SDL.Internal.Types’ exports ‘ | 15 | context @ (SDL.GLContext gl) &lt;- SDL.glCreateContext window
https://www.ibm.com/support/knowledgecenter/en/SS9H2Y_7.7.0/com.ibm.dp.doc/json_jsonx.html
500 mb is peanuts. Why bother?
FYI for intero/emacs users. I’m not sure what change it was, but intero 1.40 seems to work with this version of stack. Stack 2.x previously did not for me.
&gt; Decisions: [...] Threading: &gt; * TChan &gt; * TMVar &gt; * TVar I like how STM is so devastatingly better than the alternatives that the question isn't whether to choose STM or MVars or IORefs or Chans, but which STM primitives to include :)
AppMock is entirely used for dev purposes so that I can write my front-end app without running the real back-end, I don't see what the issue with unsafePerformIO is. The typeclasses have nothing to do with FP or any other paradigm, it's just free decoupling and dependency injection, in fact easier to write and maintain than other languages like C# and Java. Your description is a bit of an oversimplification, I would argue that it allows for a lot of flexibility down the road.
The `defaultMainWithHooksArgs` thing looks ideal. I had started to see what manually modifying `simpleUserHooks` would give, but it gets ugly, and I still wasn't of what I was doing.
What would be a better way?
The first that comes to my mind is this, "Towards a functional run-time for dense NLA domain": https://www.fing.edu.uy/~mviera/papers/fhpc13.pdf . It focuses directly on implementing some BLAS and LAPACK primitives in Haskell. However there have been a few lines of work on using ideas from functional languages (non-strict evaluation, polymorphism) to represent numerical algorithms in new ways, such as automatic differentiation, function approximation and so on. What applications or problem areas are you interested in?
So it might sound naive and wrong. But I saw Tensorflow.js was written in typescript. And I wanted it to convert to purescript. But I was having difficulty with finding types, I used a lot of ways like using foreign and options but it was a tedious task. So I was looking whether there are any theoretical papers that can help me write tensorflow from scratch (it's impossible I know that, but I wanted to try). So I wanted to try, using actual research and not bindings. Since research in building ML libraries using functional is extremely intriguing. TL;DR I wanted to do some actual research on this topic where we can use type theory/HoTT to write Tensor computation and representation of data. And numeric computation is the foundation for it. So.. am I being unrealistic I'm not sure, sorry if I'm wasting your time.
The \*type\* GLContext is exported, but the \*constructor\* is not. It doesn't look like the library is written in a way that you need access to the pointer itself, and most functions accept a value of type GLContext, not the contained pointer. Are you trying to pass the context to a separate library or something along those lines? Let us know what exactly you're trying to accomplish or use the context for.
For those of us that don't follow GHC development closely when can we expect a non alpha 8.8?
[removed]
I have a suggestion that isn't exactly what you are asking for, but if your primary concern is disk usage it might be useful. If you use btrfs¹ as your filesystem, you can use its deduplication² support to save space by periodically —perhaps via cron— running one of the [various deduplication tools](https://btrfs.wiki.kernel.org/index.php/Deduplication) on the relevant directories. After deduplication, the data belonging to any identical files³ will only be stored once — any changes to a deduplicated file will trigger copy-on-write⁴ behavior. This would not result in shared directories, nor would it save you any compile time or download bandwidth, but any downloaded source files or compiled binaries⁵ that were identical would —after each deduplication run— be stored on disk only once. ¹ Or ZFS, or newer versions of XFS, I believe. ² aka 'reflink', 'shared extents' ³ Blocks actually, so partially similar files will also be partially deduplicated. ⁴ Though I'm not sure if GHC compilation is reproducible/deterministic, so this whole suggestion might be completely useless.
Thanks. I ended up figuring it out. I can use the [Raw.Video](http://hackage.haskell.org/package/sdl2-2.5.0.0/docs/SDL-Raw-Video.html#v:glCreateContext) module which has a glCreateContext function that returns the type I need. I am writing bindings to the [Dear Imgui](https://github.com/ocornut/imgui) library. If anyone wants to help out, let me know. I'm using c2hs, but there's a steep learning curve.
How do I combine tuples based on their 1st element? And sum the 2nd? For example I have a list of tuples (&lt;item name string&gt;, &lt;quantity&gt;) \[ ("itemA", 2) ("itemA", 1) ("itemB", 2) ("itemC", 0) ("itemC", 2) ("itemB", 2) \] and I want the output \[ ("itemA", 3) ("itemB", 4) ("itemC", 2) \]
I don't know what it means but it sounds cool
I'm shy about applying for three reasons: - I am an "advanced beginner" in functional programming, and a Haskell neophyte (I do more Coq, Scala, F#, and a bit of OCaml). I feel like you would have a lot more to bring me than I would have to bring you. - I would strongly prefer to offer you a closed room, which I can't promise right now. - I'd need at least a month of advance warning of your arrival in order to take days off from work (and maximize my chances of figuring out a private bedroom for you). I welcome your thoughts on any or all of the above.
As far as I can tell, it’s just a way to generate a Haskell module from a syntax tree. You can then pretty-print the generated module, or you can run it through GHC during runtime.
Nope, still doesn’t work for me. Luckily I did already figure out a (workaround)[https://github.com/commercialhaskell/stack/issues/4901#issuecomment-510390753], which still works.
&gt;I attempted to verify what I was saying above about the definitions of f, g, g' in Core, using the -ddump-simpl compiler flag of GHCi, but it didn’t fulfil my expectations. &gt; &gt;I attempted to install [ghc-core](http://hackage.haskell.org/package/ghc-core) but the build failed, so further analysis is put on the shelf. This is an interesting blog post, but I think I would have preferred for it to be postponed until these issues were resolved.
Sign up anyway! I'll get more in touch when I get to that part of the world, and we can re-evaluate then! In the meantime learn lots of FP :) I'm excited to meet everyone!
 import Data.Map as M output = M.toList . M.fromListWith (+) $ input
Ah, sorry to hear that. I probably should have mentioned I’m on a Mac. Those issues seem windows specific.
I would prefer automatically generate API/ABI hashes/signatures paired with automatic "human-readable" SemVer-ish versions. It doesn't solve the problem of some projects deciding (or just forgetting) to maintain compatibility with older versions, but I don't believe that has a technological solution. It does solve the problem of human mistakes or forgetfulness with respect to increasing the "right" version number. It also eliminates the urge to "lie" in the version number because only dependents you are not aware of would be broken by the final removal of a deprecated symbol. I don't know if any of that would alleviate the problems that /u/maerwald has had to deal with.
Dynamic linking is impossible on windows which leads to huge headaches with licensing. Half of the libraries on Hackage straight up don't work on windows because they depend on Unix stuff (I recently tried to make a cli app in brick only to find out it had no support for windows). Even libraries that do technically have windows support are a lot of times a huge hassle to get setup (a lot of gui libs). Windows is definitely a second class citizen in the Haskell world, I've looked through the repo for GHC because of issues I've had in the past and there are Windows related issues that have been open for over a decade now. Haskell developers simply don't care about Windows, which sucks, because ideally Haskell could be a great cross platform language.
Interesting to hear about your experience with Gloss. It's definitely has limitations, but I like how simple it is. I kind of hope someone makes a version of Gloss with the browser as the display layer instead of OpenGL (similar to threepenny-gui, but not FRP). By the way, I think adding a screenshot to the README would be a good idea. It's an easy way to show off your work, which in turn will make people way more likely to compile the game and try it out.
If I understand the goal of that change correctly (which I might not), their change makes a lot of sense. Someone at Formation was telling me they ran into a bunch of problems communicating with a Scala server that used standard protobuf 3 functionality (defaulting values), since the protobufs wouldn't roundtrip from Haskell -&gt; Scala -&gt; Haskell. (That said, the protobuf 3 defaulting change sounds terrible to me)
I haven't used c2hs before, but recently used hsc2hs for a C binding I needed to wrap. I only had to use a small subset of the library's functionality and the wrapper reflects that, but it felt pretty straightforward to get it going. The biggest time sink was writing peek/poke instances where necessary
I wonder, was it discussed previously somewhere the possibility of adding strictness annotations to types in type signatures for functions? Just for consistency with record data types. So for data types you can write: data User = User { userName :: !Text , userAge :: !Int } It would be nice to be able to write functions in a similar way with similar semantics: sumAcc :: !Int -&gt; [Int] -&gt; Int sumAcc acc [] = acc sumAcc acc (x:xs) = sumAcc (acc + x) xs
&gt; It also eliminates the urge to "lie" in the version number because only dependents you are not aware of would be broken by the final removal of a deprecated symbol. Why are maintainers trying to "save" bumping the major version? Do they think it's like an endangered species?!
I'm not sure I agree that your library has a "much better documentation" than mine ;) [https://docs.haskus.org/variant.html](https://docs.haskus.org/variant.html)
You can just do: \`stack install tidal\` You then have to update your atom plugin settings to start ghci with \`stack ghci\`, as detailed at the end of this page: [https://tidalcycles.org/index.php/Troubleshooting\_a\_Tidal\_install](https://tidalcycles.org/index.php/Troubleshooting_a_Tidal_install)
No it's a good answer. In my experience the different ways of installing Haskell fail in different ways, and installing it is mostly a case of going through them all to see which one works on a given system.
I've not used nix - how would you recommend using it to install a Haskell library? I see the tidal library is there, but not the latest version. So is it a case of doing \`nix-env -i cabal-install\` and then the usual \`cabal update; cabal install tidal\`?
&gt;I kind of hope someone makes a version of Gloss with the browser as the display layer instead of OpenGL [https://code.world/haskell](https://code.world/haskell) (or the begginer-friendly [https://code.world](https://code.world/)) does almost this: it exposes an API very close to that of gloss and compiles haskell to javascript using GHCJS.
Consider the following: import GHC.TypeLits data S f a = S (f "blah" a) instance ((forall (s :: Symbol) w . Eq w =&gt; Eq (f s w)) , Eq a) =&gt; Eq (S f a) where (==) (S x) (S y) = x == y -- instance ((forall (s :: Symbol) w . Ord w =&gt; Ord (f s w)) , Ord a) =&gt; Ord (S f a) where -- compare (S x) (S y) = undefined GHC accepts the equality instance and everything seems fine. But try uncommenting the next lines: when I do virtually the same thing with `Ord` instead of `Eq`, suddenly GHC complains that it can't find an `Ord` instance for `w`. Fwiw, this pattern works for all other classes I've tried (both mine and builtins), but for some reason `Ord` is rejected, and I have no idea why. Can anyone see what's going on?
TL;DR The library allows to write the text of custom type errors like in the code below: type MessageText (e1 :: k) (e2 :: k) (es :: [k]) = "You require the following two effects from your computation:" % "" % " '" &lt;&gt; e1 &lt;&gt; "' and '" &lt;&gt; e2 &lt;&gt; "'" % "" % "However, your monad is capable of performing only the following effects:" % "" % " " &lt;&gt; es
I see two core principles in oop. Separating interface from implementation and using late binding. Seperating interfaces from implementations is a great default for libraries. Ml modules do a great job with this and maybe backpack can make it feasible for haskell. Late binding as default is debatable. Ml modules have structural subtyping without dynamic binding. But late binding is great for certain classes of problems where you need homogenous collections of interfaces.
&gt;One thing he was 100% right about is **limiting** programmers in the choices they can make. In fact, the **fewer choices** programmers have, the more **resilient** their code becomes. By this criteria I am sorry but modern Haskell/GHC fails almost as badly as C++, the choices are in different dimensions to C++ - but choices there are plenty!
How do i get the position of an element while iterating over a list? If I iterate over a list recursively, how do i get the position of the selected element?
Don't. Instead you can iterate over the list `zip [1..] origList`. The position will be the first element of the tuple.
Nice! It's making me pretty excited to see so much effort on errors happening recently. Maybe one day we'll hit rust level quality of error messages throughout the ecosystem :) (Hmm. Would that make a good 2020 GSOC project?)
Is it possible to auto-convert a whole project from old-timey records, to lenses? I have a medium size, just for fun, Haskell project (it's a WIP roguelike). As you might expect, this uses a whole load of records. I've recently learnt about lenses, and am very tempted to refactor much of the project to use them. Obviously, I could just rename the fields, add `makeLenses` and then fix all of the resulting errors by hand, but this _feels_ like something that could be done automatically. I suspect such a tool doesn't exist, does it?
Well, it is in git, so no use in deleting the docs now.
More like they want to get the new code out to as many people as possible as quickly as possible and if they don't bump the major version, more people will get it automatically.
That would be awesome! I'd love to see wider adoption of type errors and overall improvements to error messages :)
You mean i zip the list first, so that i have tuples with the position in fst and the value in snd place?
You mean i zip the list first, so i have a list of tuples and get the position with fst?
Your code doesn't take asynchronous exceptions into account. [See here for an article on the matter](https://www.fpcomplete.com/blog/2018/04/async-exception-handling-haskell). As an example, [`closeConnection`](https://github.com/ahri/eventdb/blob/master/src/Database/EventDB.hs#L66-L70) is susceptible to asynchronous exceptions - I can do `throwTo yourThreadId` and it will kill the thread with one file closed and the other still open. `closeConnection` is called in a `bracket` (which [does `mask` the finalizer](https://www.stackage.org/haddock/lts-13.28/base-4.12.0.0/src/Control-Exception-Base.html#bracket)) - however, in this case, `takeMVar` is an "interruptible" exception, and so even in the `bracket`, a deadlock on the `MVar` will send an exception to the thread, resulting in your program not finalizing the exception. If you do `uninterruptibleMask`, though, then you're suspectible to a deadlock freezing your program. This stuff is really hard to get right and I'd probably use a `TVar` and the `Control.Exception.Safe` module from `safe-exceptions` instead. Have you benchmarked that this is faster than SQLite or Postgresql?
I'm not sure if it has been discussed in the contect of Haskell, but that is a feature of the Clean language.
I was able to get in touch with Michael Snoyman about this. He said I am taking the right approach. I should be able to use the built-in SimpleApp for the bootstrap phase, rather than constructing a new App structure for it.
Thanks a lot for reading my code and guiding me in a useful direction! I'll read those links thoroughly - to be honest I have read the async exception one before but it felt a little more academic then and having a real problem to work on helps me cement this stuff. TVar sounds useful, I'll probably swap over after having read up. I have not benchmarked my database against existing options; my use-case is fairly light on performance, and I'd guess that mine may write as quickly (since you want to block until a transaction has gone through so there's not much leeway there) but for reads mine will be a lot worse as I'm not caching at all - I see this as an optimisation for later. It's largely a learning experience, but I like to implement something I can use as then "I've got skin the game" so to speak!
CodeWorld is awesome! I should play with it more.
I wasn't writing at my clearest when I made that comment. I definitely liked the docs. We'll try out Water-Wars at Boston Weekly FP sometime and let you know how it goes.
We would be very happy to hear your thoughts and if you enjoyed the game for some minutes :) You can also try to create custom maps, although that feature isn't really documented :/ Note, that scaling of the UI is not implemented, so creating too large maps would break the game.
Thank you very much
That's strange. What version of \`cabal\` are you using and where did it come from?
You need your ghc to be in your path. GHC installed via stack is not placed in your system path. For build commands you can also configure passing in the --with-compiler flag to point to a ghc not in path as the one to use. Unfortunately, I don't think that you can do so for `init`.
I'm really interested in playing this but just haven't found the time yet. Good job and thanks for the detailed report!
I'm looking to parse some JSON from an API as part of a simple side-project I am doing. Parsing using Data.Aeson looks really nice, but I've also seen api-builder (https://hackage.haskell.org/package/api-builder), which looks like it takes a way some of the boilerplate associated with calling the API etc. Is there any reason why I wouldn't just use api-builder? Has anyone had experience with it that they could share? If it's of any relevance, I am looking to store the parsed data in a graph database (Neo4j).
It makes me so happy that someone else is interested in this same problem domain. There's a similar project a coworker and I have worked on called [rotera](https://github.com/andrewthad/rotera/). We use memory maps instead of file handles, and we use one file instead of two, but result is pretty similar. My reading of your code suggests that the file described by `fdIdx` has all the offsets in it. One thing I'm curious about is how can you delete old events? I don't see any use of `mod` or `rem` for wrapping around after you've stored some amount of bytes. Also, the use of synchronous writes probably degrades performance in some cases. It is nice as the end user to be able to push events in as inefficiently as I want (possibly one-at-a-time) and then call some kind of flush function at the very end. Of course, maintaining the atomicity guarantee that you're going for can be kind of difficult if you don't use synchronous writes, so I don't really know what the best option is here. But, good work! If it works for whatever you're using it for, it's probably good enough.
Rotera looks very interesting, I had looked into Kafka a little but in more of a speculative way to transmit events around if I ever need more than one node. I'll have a read through your sourcecode as I could definitely do with learning from similar projects! Your reading is correct; the index holds pointers into the log file, at calculable points so that random access is possible despite allowing the data in the log to be of varying sizes. I don't intend to support removing events; the event sourcing paradigm I'm working to is that the log contains all events ever contributing to a state and that I'd prefer to keep everything. I have thought a little about compressing this data; e.g. I could build a state from the log and then re-generate the minimal set of events needed to construct that state, which I would use to re-write logs in case I really want to compress the data at the expense of losing information, but as I say my primary intention is to hoard this data till the end of time, buying storage if necessary! The synchronous writes are a bit of a blunt instrument due to my ignorance of systems at this level - I wanted to be sure I could force that data through to the physical drive and perhaps optimise performance as I gained more familiarity with this. Now that I have some tests I feel a little more comfortable so perhaps I should look again at this. Your observation that inefficient writes make more sense from a user perspective is very sensible - I hadn't really thought about it, and only just move away from writing a since event at a time, towards a transaction containing multiple events. Thanks very much for your tips, `pread` looks awesome! I somehow missed that when reading the docs so I really appreciate you pointing that out. Truncation is of less interest for my purposes, but that's a neat bit of info to store away :) I'm a little sceptical that my approach of having unlocked reads of the index/log is actually gaining me any performance benefit, I wonder if you have any intuition about this - I went down the POSIX route so that I could open two FDs in parallel thinking it would give me some awesome benefits but to be honest it seems like in my multi-threaded tests the reads are very slow, though I don't have evidence for that yet.
&gt; I kind of hope someone makes a version of Gloss with the browser as the display layer My time to _shine_! https://github.com/fgaz/shine https://hackage.haskell.org/package/shine No FRP, just an interface similar to gloss. It's also different from threepenny in that it's all compiled to js. There's no native executable server (which may or may not be a plus).
Api-builder uses Aeson. The very first example even shows manually written Aeson instances. In light of that fact, could you clarify your question? At first it sounds like you are wanting to use api-builder \_instead\_ of aeson when it is actually in addition to.
Thanks for doing this. I'm new to Haskell and crawling through the book. Your notes are very helpful for review and to see if I understood everything.
While I am not entirely sure about the details of why this happens, I have had problems similar to this, and managed to solve it by changing the constraint instance ((forall (s :: Symbol) w . Ord w =&gt; Ord (f s w)) , Ord a) =&gt; Ord (S f a) such that it has an added `Eq` constraint instance ( forall (s :: Symbol) w . Eq w =&gt; Eq (f s w) -- Duplicate the `Eq` , forall (s :: Symbol) w . Ord w =&gt; Ord (f s w) , Ord a ) =&gt; Ord (S f a)
Thanks, these are all good points! I haven't come across library issues as much, because e.g. cross-platform UI libraries can be a hassle to set up even when programming in C, so I tend to shy away. But even when the underlying ecosystem is to blame, rather than Haskell's Windows support, you're still stuck without a build, so I concede it's a bit of a moot point.
If it doesn't exist, this should be an interesting project!
I want to write a program that uses libmpd to display local synchronized lyrics for files, printing each line as it is sung. I thought the best (least resource-intensive) way of doing that would be to forever `idle [PlayerS]`, and whenever the player state changes I'd use `forkIO` to start a thread with pre-calculated timings that sleeps, prints a line and so on. I'd then kill it when the player state changes again. However, I'm having issues with that second part. Could anyone hint me a little toward the right path? Perhaps there's a completely different way I should be doing this?
Keep at it! Don’t get discouraged. Ask for help when you need it, most of the community is very friendly and willing to help!
Thanks for the information! I haven't heard about Clean. It's nice to see that other people thought that this would be a nice feature :)
As /u/sclv has mentioned, GHC installed via `stack` is not in your default path. You should be able to use `stack exec --no-ghc-package-path -- cabal init` to run `cabal init`. In general, if you want to run a `cabal` command using `stack`, you can use `stack exec --no-ghc-package-path -- &lt;your_cabal_command&gt;`.
I'm a new Haskeller too but was put off by how damn verbose the book is. It takes so long to even get into anything at all, I'm not sure how the later stages of the book are though. I've been reading Get Programming in Haskell and enjoying it.
Very cool, thanks for posting!
I've almost finished the book myself - it's really been quite excellent. Of course it doesn't cover everything perfectly, but well enough that it's actually pretty fun to read.
Don't hesitate to ask for help!
this is my fear of buying HFFP. As someone who has done casual reading of core haskell concepts over a year or 2, it would be kind of draining prospect having to speed read through chapters of boilerplate to get to something that seems a newish concept. I am eyeing off GPIH due to there having a higher probability of getting into the meat of it quicker
Thank you very much! This worked for me!
Beginner here learning Haskell. I am going through Haskell Book, and the question concerns Monoids and Semigroups (Chapter 15). Specifically, Problem 9 of Semigroup Exercises. In the exercise, we have to write Semigroup instance for the following type, and test it with `QuickCheck` newtype Combine a b = Combine { unCombine :: (a -&gt; b) } This is how it is supposed to work Prelude&gt; f = Combine $ \n -&gt; Sum (n + 1) Prelude&gt; g = Combine $ \n -&gt; Sum (n - 1) Prelude&gt; unCombine (f &lt;&gt; g) $ 0 Sum {getSum = 0} Prelude&gt; unCombine (f &lt;&gt; g) $ 1 Sum {getSum = 2} Prelude&gt; unCombine (f &lt;&gt; f) $ 1 Sum {getSum = 4} Prelude&gt; unCombine (g &lt;&gt; f) $ 1 Sum {getSum = 2 I was able to derive `Semigroup` instance for `Combine a b`, but could not for the life of me figure out how to test it using `QuickCheck`, how to derive `Arbitrary` instance for `Combine`. I also cannot comprehend how the above examples are computing the results they are computing. Here's my solution newtype Combine a b = Combine { unCombine :: (a -&gt; b) } -- instance (Eq a, Eq b) =&gt; Eq (Combine a b) where -- (==) (Combine f) (Combine g) = f == g -- desperation! instance Semigroup b =&gt; Semigroup (Combine a b) where (Combine f) &lt;&gt; (Combine g) = Combine (f &lt;&gt; g) -- instance (CoArbitrary a, CoArbitrary b, Arbitrary a, Arbitrary b) =&gt; Arbitrary (Combine a b) where -- arbitrary = do -- x &lt;- (arbitrary :: Gen (a -&gt; b)) -- return $ Combine x -- type CombineAssoc = -- Combine Int (Sum Int) -- -&gt; Combine Int (Sum Int) -- -&gt; Combine Int (Sum Int) -- -&gt; Bool -- main :: IO () -- main = -- quickCheck (semigroupAssoc :: CombineAssoc) The code in comments is what I tried to do to make it compile, but it gives errors, which I think may be because of lack of `Eq` and `Show` instance for `Combine`. So here are my questions: 1. What does it mean for two functions to be combined with `&lt;&gt;`? How come `unCombine (f &lt;&gt; g) $ 0` equals `0`, but `unCombine (f &lt;&gt; g) $ 1` equals `1`. 2. How can I test this using `QuickCheck`, how can I derive `Eq` and `Show` for `Combine`.
FP Complete did a webcast on exceptions that may help: [https://www.youtube.com/watch?v=T5y8sFmCFnA](https://www.youtube.com/watch?v=T5y8sFmCFnA) &amp;#x200B; And +1 for [Control.Exception.Safe](https://Control.Exception.Safe), its very useful
Yeah, I'm in a similar sitch myself. I've got a basic knowledge of the big concepts, but still don't feel like I grok the language quite well enough. That's actually why I wanted to start this project. As for the verbosity and pacing of the book, I've not gotten further than the first five or so chapters, and while the first three chapters are pretty slow if you know how this stuff works (especially chaps 2 and 3), it suddenly got a lot better when moving into types, and I expect it to keep getting more interesting the further along I get.
I'm glad you find them helpful! Now that I know there's at least one person who's interested, I'm all the more inclined to keep on going. And as the other reply said: keep going; don't give up!
Neat. I often just copy/paste the resource as a local file, for the reasons stated in the post. But I'd much rather do this static embedding because it's really my actual intent. For build reproducibility you could link to a specific GitHub commit blob, too. As it's only downloaded once and cached, you're not hotlinking GitHub.
&gt;(Hmm. Would that make a good 2020 GSOC project?) Yes!!
If yourself, or anybody going through the book, need help there's an FP Slack https://fpchat-invite.herokuapp.com/ with lots of helpful individuals. I'm always willing to help newcomers too if possible so feel free to DM me :)
Whaaat. Well I can't deny that that does indeed work, but I don't see how that makes any sense, lol. This looks like a GHC bug. Do you know if there's an issue open for this? I checked the issue tracker myself, and while there's several open issues for quantified constraints, to me none of them looked quite like this. Fwiw, this still happens on `8.8.1-rc1`.
You’re welcome! If you’re interested in how this works, here’s an overview: * `stack` installs GHC and other utilities in its own directory, which is not on the default search path. When you run a `stack` command, it modifies the search path to include its own directory. * The `stack exec -- &lt;command&gt;` subcommand executes `&lt;command&gt;` using Stack’s own search path. So if you want to run (say) `ghc`, you can use `stack exec -- ghc &lt;ghc_arguments&gt;`, and `stack` will then find the path to the appropriate version of `ghc`. (Although note that there are easier ways to run `ghc` via `stack`.) * When you run `cabal` normally, it requires `ghc`; however, if `ghc` is in a `stack`\-specific directory, `cabal` will not be able to find it. Running `stack exec -- cabal &lt;cabal_command&gt;` allows `stack` to modify the search path so `cabal` can find `ghc` and any other utilities it may need. * However, by default, `stack` also modifies several environment variables, including `GHC_PACKAGE_PATH`; it does this in a way which is incompatible with `cabal`. Thus the `--no-ghc-package-path` argument must be added to stop `stack` from modifying this variable. You don’t really *have* to know any of this information, but I personally find it useful to understand what’s going on behind the scenes.
&gt; We both agreed that the other person's actions were rude, but I immediately assumed there were reasons for the rudeness Going on a side-track here, but this is the highly fascinating phenomenon of attribution: Where are the reasons for a person's behavior, is it within themselves, is it the environment? Interestingly, we (as in the majority of people) attribute good behavior by ourselves to reasons that lie within us, while bad behavior by ourselves tends to be attributed to outer reasons (loud environment, rude waiter 5 minutes before etc.). For others, this tends to be reversed: Good behavior is then often attributed to the situation and bad behavior to the person itself. That's the very simple 101 of attribution (i.e. depression messes heavily with how we attribute behaviors and emotions), I find it a really interesting topic! I have observed that it makes communication less likely to be aggressive or misunderstood when attribution of other people also first assumes outside reasons. (Which is implied by the old advice for communication "Always assume your correspondence partner wrote/spoke in good faith").
1. The starting point is the value passed in is passed to both functions first, and then their results are added via Sum. So for zero, you substitute it in for n in both and get `(0+1) &lt;&gt; (0-1)` which reduces to `1-1`. For 1, you get `1+1 &lt;&gt; 1-1` which reduces to `2 &lt;&gt; 0`. It is confusing and not easy to understand at a glance. But when lost in Haskell, a good starting point is to just perform substitutions over and over since that's all Haskell can do at the end of the day. 2. You can't write reasonable Eq or Show instances. So even though you have Arbtrsry Combine, what can QC do to inform you of the failed case? I actually don't know how people handle this situation with property tests in general.
For (1), here's the function Semigroup instance: http://hackage.haskell.org/package/base-4.12.0.0/docs/src/GHC.Base.html#line-328
Have you considered using Nix? Nix supports this kind of multi-user environment perfectly, where the users share common artifacts but are still totally isolated.
[`Data.Function.fix`](https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Function.html#v:fix) has that type and unusual definition fix :: (a -&gt; a) -&gt; a fix f = f (fix f) which is *actually defined* ([`fix` source](https://hackage.haskell.org/package/base-4.12.0.0/docs/src/Data.Function.html#fix)) as fix f = let x = f x in x for sharing. See [Wikipedia *Fixed-point combinator*](https://en.wikipedia.org/wiki/Fixed-point_combinator), the ability to define `fix` is theoretically very interesting but others will explain it better.
And here are the slides: * [`co-log` slides](https://docs.google.com/presentation/d/1PaOmMqAiHzo3ZzUwKsALDf5bjzRQoaGIaHnxK7resvE/edit?usp=sharing)
MonadResource is a superclass of yesod's MonadHandler, the resources are freed after the response is sent or after we fail to send one due to an exception. So you can use `mkAcquire` to define a resource, namely the stdout and stderr Handles, which you acquire using `streamingProcess` and free by closing the handles. Then you can give the stdout handle to `respondSource`. Note that if your process outputs to stderr a lot (more than the OS's buffer size), it might hang waiting for you to read it, meaning you won't get the full output from stdout. So if you don't plan to read stderr, it's better to redirect it to `/dev/null` or to spin up a thread which does the equivalent, by continuously reading lines from stderr as they become available, and doing nothing with them.
All of the lets in your main function are unidiomatic. It would be nicer to make them top level variables. E.g. instead of: main :: IO () main = do let x = 10 :: Int let y = 20 :: Int print (x + y) do this: x, y :: Int x = 10 y = 20 main :: IO () main = print (x + y)
Will do, thanks :D
I think it’s also worthwhile to point out that fix is not total, as it doesn’t always terminate. However, including the fix point combinator as a primitive in typed lambda calculi enables us to do general recursion which gives us Turing completeness. The inconsistency introduced by ‘fix’ is simply an inevitable cost in exchange for more powerful computability.
There are built in functions for `foldr1 (+)` and `foldr1 (++)`: `sum` and `concat`. And I would implement `circlePerms` as: circlePerms xs = take n (map (take n) (tails (cycle xs))) where n = length xs Also the `circlePerms [] = []` case is redundant in you definition.
Ok .I can see how I can indeed create and close the handles using \`mkAcquire\`, but what I'm not sure is how to interleave it with \`respondSource\`. \`respondSource\` expect a \`Conduit\` so do I need to "decorate" somehow my conduit to it release the resource acquired with \`mkAcquire\`. Also, I think why I need to use \`stderr\` is for the reason you mentioned. If I don't consume \`stderr\` then I don't get an empty output, unless is the opposite and the program stops when I close \`stderr\` (thus the finalizer to close \`stderr\`. I'll on the command line how glabels behave exactly.
\`co-log\` is on my list of things to investigate this week, perfect timing. &amp;#x200B; Thanks for posting!
No problems! Feel free to ping me with any questions about the library if you have any :)
Or you could use `where`: main = print (x+y) where x = 10 y = 20 Personally I prefer using `where`, but others may differ.
Sum didn't work here because it calls fromIntegral internally (at least I got an error that my undefined fromIntegral was called with sum). I'll have to see about concat though. &amp;#x200B; Oops yeah - I remember testing that and probably forgot to remove it or hit ctrl-Z one too many times when debugging other code. Thanks :D
Yes, if you don't implement all functions of a type class then things start breaking. You can look at the [linear](https://hackage.haskell.org/package/linear-1.20.9/docs/Linear-V2.html#t:V2) package for a very similar vector data type that does implement all type class functions for Num (and also of Fractional and Floating). You can click on the word source next to each instance in haddock to find out how they are implemented.
Awesome, thank you!
 &gt; This is not an endorsement of drinking or jokes
Pithy summary. I hope your learning goes well. I learned from this book also and really enjoyed it.
Doing god's work
"Jokes"/riddles like this are an instance of something called Dynamic Epistemic Logic, and oleg has some cute stuff for encoding these sorts of problems more generally: http://okmij.org/ftp/Algorithms.html#dyn-epistemology
What's `haskeller5` up to these days?
No. In the Curry-Howard correspondence, `a -&gt; a` means that anything implies itself, and the proof is the identity function. According to `a -&gt; a`, false propositions also imply themselves (e.g. `Void -&gt; Void` by instantiating the `a` type variable to `Void`). `(a -&gt; a) -&gt; a` means that if anything implies itself, it is provable, and because anything implies itself, anything is provable. Another person pointed out that `(a -&gt; a) -&gt; a` is inhabited by `fix`. `fix` is a way to introduce nontermination in a language.
It's bleak
Really nice and neat slides!
*Co*ol and *co*nvinient looking!
Thank you for your kind words 😊 I indeed spent a lot of time making them...
Thank you for your **co**mment ;)
Someone's gotta do it
Yes, I am exploring that way too. Nix is more complicated than Stack or Cabal, so I am not there yet.
This is a really great effort, I myself am having much trouble understanding lambda calculus. Wish I had a reference or something to understand what exactly is lambda calculus first.
I want to use Servant for interfacing with a json api, however it operates in a slightly weird manner. For example to get the users, you need to POST domain.com/api/ with data `cmd=listUsers`. How can I modify/extend servant for this? Also, in most requests the API would require a key. Is there a way to eliminate the code duplication that comes with this?
I am not sure I can follow your Comonad implementations, the types just look incorrect to me. extract :: w a -&gt; a extract :: Monoid msg =&gt; LogAction m msg -&gt; m () duplicate :: w a -&gt; w (w a) multiplicate :: (Foldable f, Semigroup msg) =&gt; LogAction m msg -&gt; LogAction m (f msg)
I manage to make it work using \`mkAcquire\` and \`withAcquire\` to create and delete a temporary directory. However, it doesn't work if I use \`withTempFile\` instead which seems to close the handle before the response has been send.
You can think of `LogAction` as a specialised version of the arrow. `w a` for the functions arrow stands for `m -&gt; a` where `w ~ (-&gt;) m`. `LogAction` stores a function of type `msg -&gt; m ()`. So x -&gt; y ~ msg -&gt; m () x ~ msg y ~ m () If you replace `LogAction` in type signature (in your head) with the underlying function, you can notice similarities with the original functions from `Comonad`: extract :: w a -&gt; a extract :: (m -&gt; a) -&gt; a extract :: (msg -&gt; m ()) -&gt; m () Same for `duplicate`. With the only difference that for the function `duplicate` looks like this: duplicate :: (m -&gt; a) -&gt; (m -&gt; m -&gt; a) But for `LogAction` it looks like this: duplicate :: (msg -&gt; m ()) -&gt; ((msg, msg) -&gt; m ()) Functions of types `a -&gt; b -&gt; c` and `(a, b) -&gt; c` are isomorphic and basically the same (because we have `curry` and `uncurry` functions). So in `co-log` the more convenient version of a pair is used instead of closer to original `m -&gt; m -&gt; a`. Motivation behind this implementation is described in detail in Haddock for `duplicate`: * http://hackage.haskell.org/package/co-log-core-0.2.0.0/docs/Colog-Core-Action.html#v:duplicate
I can see the similarity now, but compared to the other instances, this is far from exact. For example simply choosing `a ~ m ()` is not something I would have thought you are allowed to do. The functions in Comonad have to be natural transformations, otherwise the laws make no sense and a lot of the beauty is lost.
That's why in my talk and the blog post I explicitly mention that it's not possible to implement the `Comonad` instance for `LogAction`. However, it's possible to introduce comonadic behaviour. You might not be able to chain multiple `LogAction` with the `=&gt;&gt;` operator, unfortunately. But you can call comonad-like functions over `LogAction`. Those functions are useful, similarities with `Comonad` are straightforward, and I came up with those functions thinking about comonads. I never said that `LogAction` is a comonad, but it has a structure similar to comonads.
Suppose you did have `totalFunction :: forall a. (a -&gt; a) -&gt; a`. Then, I can produce a `problem = totalFunction (id :: Void -&gt; Void) :: Void`. From there defeating the type system and causing all manner of bad behavior is `absurd`ly easy.
Triple-backtick works on new reddit. It does not work on old reddit. Support mobile apps is mixed, at best. 4-space indentation works on new, old, and mobile reddit.
&gt; so do I need to "decorate" somehow my conduit to it release the resource acquired with `mkAcquire` Like you said, `addCleanUp` is no longer supported, so you can't attach it to the conduit. You need to attach it to the MonadResource constraint, using `allocateAcquire`.
I'm taking a first crack at using existential types in a side project, and I've pretty quickly reached the bounds of my type system knowledge : ) The idea is to have a tree of data, and allow each node in the tree to be decomposed further into sub-nodes and explored by the user. The reason for using existential types is that each node in the tree should be able to determine how it can be decomposed, and I'd like to decouple the individual instances of `Decomposition` from the rest of the code. Forgive the abundance of code, I'm not sure how much is relevant: type Label = B.ByteString type Decomposition a = (Label, a -&gt; [a]) class Decomposable a where decompositions :: [Decomposition a] render :: a -&gt; B.ByteString decompose :: Decomposable a =&gt; Decomposition a -&gt; a -&gt; Tree a decompose (l, f) a = Branch a l $ map Leaf $ f a type Cursor = [Int] data Tree a = forall a. (Decomposable a) =&gt; Branch a Label [Tree a] | Leaf a The point where I got stuck is when I tried to write a function that applies a function at an arbitrary position in the tree: applyAtCursor :: Decomposable a =&gt; (Tree a -&gt; Tree a) -&gt; Cursor -&gt; Tree a -&gt; Tree a applyAtCursor f [] t = f t applyAtCursor f (c:cs) (Branch x l xs) = Branch x l ((take c xs) ++ [applyAtCursor f cs (xs !! c)] ++ (drop (c+1) xs)) where xs' = (take c xs) ++ [applyAtCursor f cs (xs !! c)] ++ (drop (c+1) xs) Often times I'll have an idea for a module and during the implementation I'll type myself into a corner and learn something new, but I'm at a loss with this one: /home/blu/Desktop/explorer/app/Main.hs:90:53: error: • Couldn't match type ‘a1’ with ‘a’ ‘a1’ is a rigid type variable bound by a pattern with constructor: Branch :: forall a1 a2. Decomposable a2 =&gt; a2 -&gt; Label -&gt; [Tree a2] -&gt; Tree a1, in an equation for ‘applyAtCursor’ at app/Main.hs:89:25-37 ‘a’ is a rigid type variable bound by the type signature for: applyAtCursor :: forall a. Decomposable a =&gt; (Tree a -&gt; Tree a) -&gt; Cursor -&gt; Tree a -&gt; Tree a at app/Main.hs:87:1-83 Expected type: Tree a Actual type: Tree a1 • In the third argument of ‘applyAtCursor’, namely ‘(xs !! c)’ In the expression: applyAtCursor f cs (xs !! c) In the first argument of ‘(++)’, namely ‘[applyAtCursor f cs (xs !! c)]’ • Relevant bindings include xs' :: [Tree a1] (bound at app/Main.hs:90:11) xs :: [Tree a1] (bound at app/Main.hs:89:36) x :: a1 (bound at app/Main.hs:89:32) f :: Tree a -&gt; Tree a (bound at app/Main.hs:89:15) applyAtCursor :: (Tree a -&gt; Tree a) -&gt; Cursor -&gt; Tree a -&gt; Tree a (bound at app/Main.hs:88:1) | 90 | where xs' = (take c xs) ++ [applyAtCursor f cs (xs !! c)] ++ (drop (c+1) xs) | I'd like to understand what I'm missing here, but I'm also interested to know if there is a better or more idiomatic way to implement what I've described while keeping it externally extensible.
Actually it doesn't quit work. It only works because I forgot to remove the temporary file. When I do, the file is deleted before the sendResponse is finished ...
Oh I get it now, each function is of type \`a -&gt; b\`, hence when we pass \`1\` to \`unCombine ( f &lt;&gt; g)\` , \`f\` and \`g\` get applied to 1 separately, resulting in \`Sum { getSum = 2}\` and \`Sum { getSum = 0 }\`, these two are then combined using their \`&lt;&gt;\` , resulting in \`Sum {getSum = 2}\`. Similar combination occurs if \`f\` and \`g\` are defined as \` f = Combine $ \\n -&gt; Product (n + 1)\` and \`g = Combine $ \\n -&gt; Product (n - 1) . &amp;#x200B; I'll pass on testing all this with \`QuickCheck\`, it's hard for me at this stage to figure that library out for these complex cases. &amp;#x200B; Thanks for the response.
Oh I get it now, each function is of type `a -&gt; b`, hence when we pass `1` to `unCombine ( f &lt;&gt; g)` , `f` and `g` get applied to 1 separately, resulting in `Sum { getSum = 2}` and `Sum { getSum = 0 }`, these two are then combined using their `&lt;&gt;` , resulting in `Sum {getSum = 2}`. Similar combination occurs if `f` and `g` are defined as ` f = Combine $ \n -&gt; Product (n + 1)` and `g = Combine $ \n -&gt; Product (n - 1) . I'll pass on testing all this with `QuickCheck`, it's hard for me at this stage to figure that library out for these complex cases. Thanks for the response.
I wrote a blog post on the [`fix :: (a -&gt; a) -&gt; a`](https://www.parsonsmatt.org/2016/10/26/grokking_fix.html) function. It's neat.
- `forall r. r -&gt; r` is isomorphic to `Unit` - `forall r. r -&gt; r -&gt; r` is isomorphic to `Bool` - you may be familiar with [Böhm-Berarducci encoding](http://okmij.org/ftp/tagless-final/course/Boehm-Berarducci.html) (incorrectly called Church-encoding often). What type has `forall r. (r -&gt; r) -&gt; r` encoding? newtype Void = Void Void which is how you write `Void` in Haskell98 (see e.g. https://github.com/ekmett/void/blob/e79cc9e5e8a36158943fa0f1dbd559e41f5d3268/src-old/Data/Void.hs#L63). Try to construct value of that :)
Yeah sorry, that wasn't clear. My question is more about whether using api-builder is worth it, given once you've built the parser with Aeson, calling the API seems like it should be relatively simple. As a beginner I'm wondering if writing some of those steps myself could be worthwhile, too.
I think this SO Q&amp;A has the answer to 2: https://stackoverflow.com/questions/41350192/how-to-test-semigroup-law-for-this-data-type
This type looks fishy, note that the `a` on the left of `=` is different from the `a` on the right under the `forall`. In other words this is equivalent (note that `Leaf` does have `a`): data Tree a = forall x. (Decomposable x) =&gt; Branch x Label [Tree x] | Leaf a It might be clearer in `GADTSyntax`: data Tree a where Branch :: forall x. Decomposable x =&gt; x -&gt; Label -&gt; [Tree x] -&gt; Tree a -- a, not x! Leaf :: a -&gt; Tree a &gt; The reason for using existential types is that each node in the tree should be able to determine how it can be decomposed, and I'd like to decouple the individual instances of Decomposition from the rest of the code. I'm not entirely convinced by this. That sounds like the classic antipattern of shoehorning existential types to do OO-style encapsulation, where a simple tree would do a much better job. &gt; applyAtCursor :: Decomposable a =&gt; (Tree a -&gt; Tree a) -&gt; Cursor -&gt; Tree a -&gt; Tree a You, as the implementor of `applyAtCursor`, are given a function `Tree a -&gt; Tree a` specialized to that one `a` the caller knows about. However, in the tree you only have `Tree a1` for various existentially quantified `a1`, so you can't apply that function. Instead you can take `forall a. Tree a -&gt; Tree a` that ensures the caller can't specialize it to any particular `a` when passing you such a function, but that may also not be what you want...
Love it! Please keep it up, there's a niche market here you're 100% cornering.
⊥s up!
After putting some more thought into it, I believe this behavior is correct and simply an inherent part of the theory behind quantified constraints. That said, the error message fails to get at the essence of the problem. Whenever an instance is defined for a subclass, the constraints for the subclass need to satisfy the constraints for the superclass. For instance, `Eq (Identity a)` requires `Eq a` and `Ord (Identity a)` requires `Ord a`. However, the `Ord (Identity a)` also needs `Eq (Identity a)`, which further requires `Eq a`. Consequently, `Ord (Identity a)` requires `Eq a`. Fortunately, `Ord a` implies `Eq a`, so everything adds up. When looking at the instances for `S`, we can conclude one critical fact about this: The constraints of `Ord (S f a)` need to imply the constraints of `Eq (S f a)`. In other words, the constraint forall s w. Ord w =&gt; Ord (f s w) needs to imply forall s w. Eq w =&gt; Eq (f s w) However, this implication is not necessarily true, since I can create an instance data Foo a = ... instance Ord a =&gt; Eq (Foo a) instance Ord a =&gt; Ord (Foo a) such that this instance forall a. Ord a =&gt; Ord (Foo a) only implies that forall a. Ord a =&gt; Eq (Foo a) Consequentally, GHC Haskell is correct to not accept the initial instance provided in the question. Incidentally, if you change the original `Eq (S f a)` instance to require `forall s w. Ord w =&gt; Eq (f s w)` and `Ord a` instead, there is no need to add an additional constraint to `Ord (S f a)`. Theoretically, everything I have stated is a natural consequence of the fact that functions (which quantified constraints are presumably equivalent to) only subtypes (i.e., be covariant) when their argument supertypes (i.e., be contravariant.)
Thanks for the reply! I was able to get it (partially) working using the GADT syntax. I'm rewriting it now using plain algebraic data types, which is already more straight-forward but seems like it will dump a good amount of boilerplate on the library user. I appreciate your input, I'll likely post again after I'm happier with it and see if there is a good way to simplify the usage and remove boilerplate without resorting to TH. Thanks!
What is that symbol called?
 let void = Void void in void
I'd just like to ask for some clarification on what functor and applicative (and, if you like, monads) "buy" you *relative* to each other. Mainly in terms of developing an intuition around the signatures of their defining functions (**fmap** / **&lt;\*&gt;**, **pure**). One thing I hadn't considered which seems obvious now, is that you cannot change the shape of a functor. Fmap (which is what defines a functor) merely allows you to apply a function to the value/s in the functor. Therefore, you can't modify the shape of the functor in any way; you're agreeing to a contract which says "I can do what I like with the values inside, but I will not change the underlying context/container". What other intuitions can we gather from the signature of **fmap** that are most important? What intuitions can we get from the signature functions of applicative by comparison/contrast? The main thing I could gather was that because we have **&lt;\*&gt;** we can apply functions greater than arity 1. For example, we can multiply the values contained in two **Just Int**s. Furthermore, once you can apply functions of two arguments, you can apply functions of any numbers of arguments by chaining them. This means we can "look inside" multiple contexts at once, and combine them in some way into one context. However, this means we can now change the shape of the applicative, right? If we: (*) &lt;$&gt; (Just 5) &lt;*&gt; Nothing then we started with a **Just Int** and wound up with a **Nothing**. This means we're no longer agreeing to the previous contract - the context may change after applying our function/s to the value inside the context. Does this mean we can say that, with applicatives, the context in sensitive to the values when **&lt;\*&gt;** is used? Also, because we can lift arbitrary things into the context using **pure** does that mean we can change the shape of the applicative "freely"? I think if this gets any longer, nobody is going to want to read it and so I haven't a chance in hell of getting an answer! I'd best stop ;) Thank you.
I've heard bottom
https://en.wikipedia.org/wiki/Up_tack
It's called bottom in the case of type theory 😁
Nice ⊥ you got there.
I'm blushing 😳
I always read it as if it's a middle finger pointed my way.
You're welcome! I'm looking forward to hearing more about it.
When I was starting out, this book was a life saver. I had only dabbled in programming, so Haskell is my first "serious" programming language, but this book took me through everything at a pace that I knew wth was going on. I re-read it later on, to solidify my understanding and pick up what was fuzzy at the time. A second read-through can be very cursory/just skimming, but even that does help. I'm someone who loves to figure out how things work, so maybe other people don't have the patience to go through everything, even though they know it already. But I certainly advise anyone who skips/skims through the book to go back a chapter or two if it's not clear wth is going on, since everything is very well explained and built on top of prior information.
https://wiki.haskell.org/Bottom
It's marked as `unsafe*` for a reason. These couple of uses _might_ be fine, but please never rely on this in actual code. Given how Haskell compiles and runs, there is no way to know how `unsafePerformIO` will influence your code. It will change depending on if you optimize aggressively (or not), it might not be run at the moment that you think it would run, etc. It is very unwise to ever use it. There is always a way to either get the value at runtime, or just insert a dummy-value without needing `IO`. Free decoupling and dependency injection is stuff I've NEVER heard of programming Haskell professionally for 5+ years. It sounds like OOP/imperative-inspired behaviour (since I know the least about those paradigms, and your use of type classes look like interfaces) Maybe a good idea to get rid of the notion of DI by reading this: [https://blog.ploeh.dk/2017/01/27/from-dependency-injection-to-dependency-rejection/](https://blog.ploeh.dk/2017/01/27/from-dependency-injection-to-dependency-rejection/)
&gt; This pattern scales in the sense that if I need to give you a dozen different such sets of capabilities, I don't need to name some gigantic product environment/state. So would extensible anonymous records/rows/variants etc. potentially solve this problem? Rather than accepting `a :: Int, b :: Bool, c :: String` as implicit params and possibly passing forward `a :: Int, b :: Bool, d :: Char` to a child, you would bundle the first 3 into a record behind `ReaderT`, and then call `local (insert @"d" 'c' . subset)`. &gt; The key for me is that it can offer huge performance gains over working in an abstract monad transformer stack, and is less painful than working in the wrong concrete monad transformer stack Is this something that GHC can learn to optimize out, through more aggressive specialization and inlining? I can understand some situations in which changing code for perf reasons is understandable, such as changing the algorithm itself, but when two chunks of code seem to fundamentally accomplish the same goal in the same general way, it seems like a smart compiler should be able to deal with it.
&gt; Calculations which are undefined are denoted by the ⊥ symbol, pronounced Bottom, which the documentation explains as the compiler giving you the finger. From the "official" [Haskell wiki page](https://en.uncyclopedia.co/wiki/Haskell).
I find this information very helpful as well. Thank you!
&gt; So would extensible anonymous records/rows/variants etc. potentially solve this problem? I don't believe so, because I wind up doing some kind of linear or at best logarithmic search to find the needle in the haystack. &gt; Is this something that GHC can learn to optimize out, through more aggressive specialization and inlining? I don't think so. Here we get to use runInIO to flatten any transformer stack out to just an IO, and to use the small subset of constraints to plumb just the parts that are used. Anything abstract in the row type is going to have to plumb a bunch of unused fields around through the call. Here they are weeded out before I call you. You _might_ be able to write a ghc plugin that transforms the call sites by thinning the portion of the reader environment that isn't used, but this is basically duplicating exactly what the implicit parameter logic is doing for you here.
&gt; I don't believe so, because I wind up doing some kind of linear or at best logarithmic search to find the needle in the haystack. I was under the impression that all indexing into the default extensible records would be O(1) (at the cost of modification being O(n)). Either because the exact type is known at compile time and thus you are basically just dealing with a C struct, or if it is in constraint form then the parent function would pass in the offsets as typeclass dicts and you would hope for GHC to inline things. So when calling a child function with some differing set of parameters you would pay O(n), but that's what you pay anyway with implicit params since you need to actually pass in said parameters, but then you shouldn't have to pay any further for reading from those parameters. Ideally GHC would optimize out any intermediate steps of boxing up and then retrieving those values. &gt; Anything abstract in the row type is going to have to plumb a bunch of unused fields around through the call. Here they are weeded out before I call you. Ah ok makes sense. Yeah for your use case it seems like you would want to avoid polymorphism in the row type, so you would just use a concrete record, and use a function like `subset :: (b &lt;: a) =&gt; Record a -&gt; Record b` combined with `local` when calling child functions that only need a subset of the environment. The perf should be the same as far as I can tell, and ergonomics wise the only extra verbosity is having to call `local subset` or `local (insert @"k" v)` instead of `&lt;nothing&gt;` and `let ?k = v` respectively.
One different intuition: `Functor` allows you to transform the value inside a single `f` with some pure function. `Applicative` allows you to apply an arbitrary arity pure function to arguments that are all within separate `f`s.
The analogy to a struct doesn't work because everybody winds up implementing these as a big open record type, which is a tree of leaf level records. If you carefully balance that tree you get log time access. You'd need to switch things over a `SmallArray#` based implementation to flatten it and I haven't seen anybody pull that off yet in Haskell, as it requires some kind of across-constraint reasoning, which I suppose could be done with a plugin to get Ohori-style records. That would give you something that is dual to the story offered by GHC out of the box, but I've yet to see that plugin written.
&gt;I want to use Servant for interfacing with a json api, however it operates in a slightly weird manner. For example to get the users, you need to POST domain.com/api/ with data cmd=listUsers. How can I modify/extend servant for this? &amp;#x200B; Modify? You just define the API using Post and use ReqBody for the body: type API = "api" :&gt; ReqBody [PlainText] Text :&gt; Post [PlainText] Text &gt;Also, in most requests the API would require a key. Is there a way to eliminate the code duplication that comes with this? To what code duplication do you refer? The natural way to pass API keys is either a `Capture` or a `QueryParam`: type OtherAPI = QueryParam "key" Text :&gt; Get [PlainText] Text Perhaps a complete example would help? Consider the website that has routes of * api/ POST "cmd=listUsers" gets a text return of a couple users * other/?theKey=&lt;key&gt; GET gets a text return of a secret if you use the right key. We can encode the server and client in the same program: {-# LANGUAGE DataKinds #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE OverloadedStrings #-} module Main where import Data.Proxy import Data.Text import Servant -- For demonstration (client) import Servant.Client -- servant-client import Control.Concurrent -- base import Network.HTTP.Client (defaultManagerSettings,newManager) -- http-client -- For demonstration (server) import Servant.Server -- servant-server import Network.Wai.Handler.Warp ( run ) -- warp -- Shared code between client and server type API = "api" :&gt; ReqBody '[PlainText] Text :&gt; Post '[PlainText] Text :&lt;|&gt; "other" :&gt; QueryParam "theKey" Text :&gt; Get '[PlainText] Text theWebAPI :: Proxy API theWebAPI = Proxy -- Client stuff apiEndpoint :: Text -&gt; ClientM Text otherEndpoint :: Maybe Text -&gt; ClientM Text apiEndpoint :&lt;|&gt; otherEndpoint = client theWebAPI doClientStuff :: IO () doClientStuff = do mgr &lt;- newManager defaultManagerSettings let env = mkClientEnv mgr (maybe (error "impossible") id (parseBaseUrl "http://localhost:9999")) let run :: ClientM a -&gt; IO (Either ClientError a) run x = runClientM x env run (apiEndpoint "cmd=listUsers") &gt;&gt;= print run (otherEndpoint Nothing) &gt;&gt;= print --server stuff server :: IO () server = run 9999 (serve theWebAPI serveRoutes) serveRoutes :: Server API serveRoutes = serveOne :&lt;|&gt; serveOther where serveOne :: Text -&gt; Handler Text serveOne body = if body == "cmd=listUsers" then pure "TomMD,3l_Di4bl0" else throwError err500 serveOther :: Maybe Text -&gt; Handler Text serveOther key | Just "secret" == key = pure "I shave all men who do not shave themselves." | otherwise = pure (Data.Text.pack $ show key) main :: IO () main = do _ &lt;- forkIO server doClientStuff Which yields: Right "TomMD,3l_Di4bl0" Right "Nothing"
If you're looking for the abstract `filter` function, you can have a look at the `Filterable` typeclass from the [witherable](http://hackage.haskell.org/package/witherable) package: class Functor f =&gt; Filterable f where mapMaybe :: (a -&gt; Maybe b) -&gt; f a -&gt; f b This is a polymorphic version of `mapMaybe` function from `base` package. Basically, it combines `Functor` and `filter` capabilities in a single function. And you can easily implement ordinary `filter :: (a -&gt; Bool) -&gt; f a -&gt; f a` using `mapMaybe` only. Functor indeed cannot change the shape of the object. Another way to look at this is one of the `Functor` laws: `fmap id ≡ id`. Function inside `Applicative` cannot change the shape of the structure. However, function in monad can't. Compare: (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b (=&lt;&lt;) :: Monad f =&gt; (a -&gt; f b) -&gt; f a -&gt; f b In `Monad` function is allowed to change the context depending on the current value in the context. In `Applicative` function is _pure_ and how context is changed depends only on other contexts in the function arguments, the function cannot change it.
&gt; The analogy to a struct doesn't work because everybody winds up implementing these as a big open record type, which is a tree of leaf level records. If you carefully balance that tree you get log time access, but almost everybody implements it as a flat list and pays linear time for indexing. &gt; You'd need to switch things over a SmallArray# based implementation to flatten it and I haven't seen anybody pull that off yet in Haskell, as it requires some kind of across-constraint open-world-incompatible reasoning, which I suppose could be done with a GHC typechecker plugin to get Ohori-style records. Yeah sorry I should have been clear that I wasn't really thinking about any library-level solutions. I'm thinking GHC/Haskell-the-language building in Row/Record/Variant (and probably Array/Product/Sum as well as positional equivalents) and implementing it the same way that `data` currently works plus some additional typeclass logic for when the type is abstract and a primitive set of combinators for efficiently combining them. I suppose you may also need ST/IO mutable variants as primitives too, as I can't think of a way to build them on top of immutable variants (IORef would involve extra indirection). I could also see libraries then implementing more tree-like or list-like versions on top of the above that are more efficient for immutable update. But I think the contiguous ones are the ones you want as GHC/Haskell primitives. &gt; so you're doing twice the work... Since Records would be known by GHC, it seems like it would often be possible to just compile a function of the form `Record {a = Int, b = Bool} -&gt; ...` internally to `Int -&gt; Bool -&gt; ...` and have the respective call-sites call it as such. Now of course if you directly do various operations on the record itself, instead of just grabbing the fields, then this optimization might not make sense. Hell even a type signature of `Record {a = Int, b = Bool | r} -&gt; ...` could be compiled to the same thing as long as the record is only used to grab and use fields out of it. &gt; My point is more that I want the typechecker to figure all that crap out for me as I know from experience folks, myself included, will just take the easy path and pass along all the extra parts of that record if it'll save them some typing. You could write a helper function `call :: Monad m =&gt; (b &lt;: a) =&gt; ReaderT b m c -&gt; ReaderT a m c` and thus the only boilerplate is to write `call` whenever you would normally implicitly have ImplicitParams fly in. I don't think I can do any better than that without implicit subtyping or keeping things polymorphic and relying on the optimizations I mentioned above. &gt; All of that taken into consideration, in the end you could get a solution that does more work, requires a manual compiler plugin, requires a manual implementation of an open product type as a SmallArray# which is unsafe all the way down to the core -- though, safe by surface convention at least -- but at least you never used `{-# language ImplicitParams #-}`. =) So ideally the more work stuff could maybe be addressed by the above, but I could be wrong so don't quote me on that. The manual compiler plugin would be a deep fundamental change to Haskell the language and would ideally come with completely removing `data` (except for GADTs) and stock deriving and current records/variants and gets very close to obviating Generic entirely. So yes it would be a deep change, but it would not be for the sole purpose of getting rid of ImplicitParams, it would be for all the other benefits (including making my job a lot more fun, not that it's all that bad right now, Haskell is quite enjoyable). The unsafety is true when thought of as a library, but if thought of as builtin to Haskell/GHC itself, then it's really just as unsafe as `data` and `newtype` and everything else in Haskell is right now. But yes I wouldn't have to type `LANGUAGE ImplicitParams` which does bring me great joy :)
Why do people no longer use the standard hierarchy for module names? I've been starting to see it a lot. I would have expected this stuff to appear under `Data.Type.Error.Pretty` or something.
Is it written somewhere that every module should start with `Data`? To me, adding `Data.` in front of `Type.Errors.Pretty` doesn't bring any value in terms of understanding the meaning of the module. Moreover, if I or any Haskell beginner see `Data` in the imports, she (or I) is more likely to assume that this module comes from `base` (or some other boot library). Also, `Type.Errors.Pretty` module name mimics package name quite nice — fewer things to remember. I understand why new modules in `base` should have this prefix: for consistency with the rest of the `base`. But I don't think that these rules should apply to every other package in the world. It could make sense to keep compatibility with `base` and name module similar to the module where `TypeError` and `ErrorMessage` are, but this module is `GHC.TypeLits`. And the name of this module gives zero clues about its content and the fact that it contains custom type errors. So I decided to choose similarities with the `type-errors` package instead.
&gt; The constraints of Ord (S f a) need to imply the constraints of Eq (S f a). In other words, the constraint forall s w. Ord w =&gt; Ord (f s w) &gt; needs to imply forall s w. Eq w =&gt; Eq (f s w) But see this is where I'm getting stuck. Why is that the case? Why instead can't it use the original (larger) context with the new target head: i.e., why can't `forall s w. Ord w =&gt; Ord (f s w)` induce a search for `Eq (f s w)` in the context of `forall s w. Ord w` (note that this is *not* a search for `forall s w. Ord w =&gt; Eq (f s w)`!)? It seems like it should be able to search that larger context we already know we need and simply toss the unneeded pieces.
What ever happened to accessible titles? Does the talk have a tldr for those of us without a category theory background?
&gt; Is it written somewhere that every module should start with `Data`? It is not. There are other roots, like `Codec`, `Control`, `Foreign`, `Graphics`, etc. It is also a misconception to think that `Data.` is owned by `base` or boot libraries; there are a bunch of type-related things under `Data.Type` from non-boot packages: * `Data.Type.List` from `type-list` * `Data.Type.Set` from `type-level-sets` * `Data.Type.Ordinal` from `type-natural` And those were just the ones I found in my browser history. In this case, I think consistency with `type-errors` is probably a good idea, but I consider it similarly ill-named and commented to that effect on its announcement post. See also: https://wiki.haskell.org/Hierarchical_module_names By the way, congratulations on the release. Gripes about the module name aside, I think it's great to make good type errors easier to write.
It's a talk about a logging library built around this type: ``` newtype LogAction m msg = LogAction (msg -&gt; m ()) ```
Yeah, I'm aware of other module roots and other libraries following this naming scheme. Module names in Haskell can be discussed for a very long time, and different people have different views on how modules in the package should be named :) Thanks for your congratulations! I also would like to see type errors used more often and I'm happy to help other people to write type errors easier! :)
You don't need to have a category theory background to understand the talk. In this talk and my blog post, I'm trying to explain common and fundamental Haskell abstractions on real-world examples to make them even more accessible. For example, `Functor` is a fundamental typeclass in the Haskell ecosystem: class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b Would you say that the talk requires to know about category theory in advance only because of the title mentions `Functor`? Well, `Contravariant` is not that different from `Functor`: class Contravariant f where fmap :: (a -&gt; b) -&gt; f b -&gt; f a So, if you would like to understand these family of typeclasses better, I recommend to watch the talk and read the blog post, because there I'm explaining these concepts in simple words :)
I went with a combination of both - where for the values that might change (like the initial positions of the bodies) and top level variables for the calculated ones (like the initial velocities of the bodies)
I looked at their implementation and find it a bit weird and don't really feel like this is something that should be relied upon. They've defined fromInteger for vectors in a way that a vector of some integer is a vector where each element is that integer. That just feels mathematically wrong. Aside from that I feel like this could break down the expected behaviour inside sum depending on the implementation. I've also looked at the sum source and can't find the actual place where fromInteger is used (probably inside getSum somewhere) - why isn't sum simply defined as a fold / what advantages does the other implementation bring?
Not sure if I am a fan of this. In my experience seperating type signatures from definitions leads to pain for anything but trivial functions. Having to search for the definition to figure out type classes or higher rank functions is a huge pain and it feels like this could encourage that type of code. My first impression was that foo x y z :: m a = bar @m x y z could be useful for local bindings but - pattern-bound type variables don't generalize - if some type class relies on the result type it isn't ambiguous anyway.
It's nice that term-level and type-level syntaxes are becoming more similar.
I don't like this at all. Quoting myself from another thread: &gt; I think it's very useful to have a clear visual separation between the type of a function and the names of the formal parameters. Most of the times I only care about the type of a function, and not at all about the parameters names. The visual separation of current Haskell allows me to spot the signature and read it instantly, without having to mentally filter out what's not a type.
I'd like to `traverse` a Map in parallel using https://hackage.haskell.org/package/parallel-3.2.2.0/docs/Control-Parallel-Strategies.html#v:parTraversable but this resulting `parTraverse` is not a drop-in replacement for `traverse` due to the additional `Traversable` constraint on `f`: parTraverse :: (Traversable t, Applicative f, Traversable f) =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) parTraverse f = (`using` parTraversable rseq) . traverse f What causes this discrepancy?
&gt; Scala also has objects, implicits, type unsoundness, etc. Should we accommodate users coming from Scala by ruining Haskell? The idea is to borrow the bits that aren't harmful and fit well with the language, rather than to copy everything blindly. I see that you disagree that this particular change is harmful, but I don't see how it implies that the goal is to ruin Haskell.
Could you clarify how adding support for inline signatures would encourage separating type signatures from definitions? I think you have a point, but I can't quite make the connection.
&gt; Most of the times I only care about the type of a function, and not at all about the parameters names. Really? Row polymorphic records *a la* Purescript is usually a popular idea around here.
Yes, I'm struggling with modules in Haskell, I tried a lot of different configurations and I'm not happy with everything being it's own file as well. The way Haskell handles records and typeclasses make it hard to avoid naming conflicts, also Elm allows you to [combine named and qualified imports](https://github.com/gigobyte/Listeo/blob/master/client/src/Main.elm#L9) in the same statement which is super nice but unfortunately not available in Haskell which results in a lot of unnecessary imports.
The "ruining" part was more referred to the examples after "also has", apologies if it wasn't clear. In this case, I see the proposal making Haskell less opinionated and unique, which is not good if the "opinions" were the right ones in the first place. I understand it's not a main point, but it was a point that got my attention since I don't see it as a very good argument.
It was implied that we are talking about Haskell as currently defined/implemented, so I'm not sure how Purescript records are relevant to this discussion. I'm also not very familiar with Purescript. I'm open to changing my mind whenever Haskell gets PS-like records, but until then...
&gt; uf :: a -&gt; a = id -- OK &gt; uf :: a -&gt; a = not -- OK, a ~ Bool &gt; uf :: a -&gt; a = ('x':) -- OK, a ~ String &gt; We expect programmers to make use of this power. When would it be useful? It seems very misleading to me to annotate a function as generic when it actually isn't. In F# if you do something like that, you get a warning: let uf : 'a -&gt; 'a = not ^^^ This construct causes code to be less generic than indicated by the type annotations. The type variable 'a has been constrained to be type 'bool'.
Yeah, I think this is one of the bigger things to like about this proposal.
\&gt;In my experience seperating type signatures from definitions leads to pain for anything but trivial functions. Isn't it the case currently that separating type signatures from definitions is the only option? This proposal is to allow combining type signatures and definitions, e.g. \`myFun (x::Int) (y::Int)\`.
With seperating I meant having type signature and implementation on non-adjacent lines: foo :: X -&gt; Y bar :: (C m) =&gt; X -&gt; m ... bunch of other stuff ... foo = bar This is sometimes useful for stuff like smart constructors of an ast - a bunch of very similar functions with the same type can be more readable this way. Most of the time it is unreadable and very difficult to follow.
Rarely you want type signatures on non-adjacent lines, usually for multiple trivial and similar functions add, sub, mult :: Exp -&gt; Exp -&gt; Exp add = BinOp Add sub = BinOp Sub mult = BinOp Mult Enabling this for more complex functions is an explicit goal under Motivation 1. In my opinion if you need the result type to understand the function body you should just keep the type signature next to the function body. The second motivation mentions that it expects users to make use of the extra power of non-rigid type signatures. Not sure where the confusing syntax would be preferable over partial type signatures, though The third point under motivation is fair but I am not sure if it justifies the extra learning and maintainance burden of a new syntax.
&gt; However, this means we can now change the shape of the applicative, right?... This means we're no longer agreeing to the previous contract I wouldn't say that -- keep in mind, the function you choose isn't what effects this change of shape. The change of shape is determined by the Applicative instance itself, *not* your choice of function to apply.
`parTraversable` is traversing `f`, not `t`. One problem is that it might not make sense to traverse `t` in parallel in the context `f`: what if the context `f` is nondeterministic, e.g., lists? If indeed all you want is to evaluate the application `f x` to WHNF for every `x` of your traversable structure, then you can `map` instead of `traverse` first: parTraverse f = sequence . (`using` parTraversable rseq) . map f
I don't know whether this will be helpful, but your question inspired me to write this fun thing: https://blog.poisson.chat/posts/2019-07-17-functor-play.html
I think it is remarkably rare that I give a name to all my parameters and don't pattern-match on at least one of them. How does this proposal deal with multiple clauses for a function? Also, I think the proposed implementation is bad. It introduces an grammar ambiguity and then gives a rule for resolving it. I really think Haskell's grammar is ambiguous enough, and we should be able to implement the proposal with introducing additional ambiguities by keying off the relatively unique `::` token.
I initially try using `withAcquire` instead of `allocateAcquire` thinking `allocateAcquire` needed an explicit release. It didn't work with `withAcquire` but does work with `allocateAcquire` indeed. Thanks
`all` = [`foldMap @_ @(via All)`](https://github.com/ghc-proposals/ghc-proposals/pull/218)
&gt; For one thing, it is what Scala, F#, OCaml, and some other languages do, so we would better accommodate users coming from these languages by allowing this style 1) That's a terrible reason 2) I happen to hate this style, it mixes information together and makes it harder to read 3) Haskell function declarations are too lightweight visually for this to work, it's easy to miss the point that it changes from declaration to implementation when scanning the code.
Oleg lurks behind every riddle :Ð
I really like the way this looks. Even if we don't get everything with an extension in Haskell, it's at least worth pondering to have this sort of thing in mind for future programming language design, where perhaps with a blank slate this could be done well.
Ugh, this is the exact opposite of what I would want
I do clojure for a living. We are also hiring :)
I explored a similar idea to this `layout` keyword, with different goals, called `infix`: https://gist.github.com/chrisdone/d9d33e4770a2fef19ad1 Today, I'd prefer work to go into a structured/projectional editing direction and then things like this won't matter. It would just be a stylesheet.
Agreed, I don't like any of the herald options, and even using the "most acceptable" herald (`with`), I still don't like the look of any of the resulting code.
 instance Num a =&gt; Monoid (Sum a) where mempty = 0 mappend (Sum x) (Sum y) = Sum $ x + y You might not see it but this code uses `fromInteger`; see the rules for polymorphic numeric literals.
Most of the length is due to code snippets. Second place is examples and explanations triggered by multiple people saying something was unclear. It's not as verbose as the page count implies. &amp;#x200B; Believe me, I'd rather have a shorter book in editing and printing. I'm eating the cost and pain for y'alls benefit, not mine.
I came across this class `ShiftMap` when reading `concur-core`s code. ```haskell -- | Mapping between Natural Transformations class ShiftMap s t where shiftMap :: (s ~&gt; s) -&gt; (t ~&gt; t) instance ShiftMap m m where shiftMap = id instance ShiftMap m (IdentityT m) where shiftMap = mapIdentityT instance ShiftMap m (StateT s m) where shiftMap = mapStateT ``` There isn't any other documentation. Does anyone know what is this class means, where can be used?
What are the existing problems that this is attempting to fix which are referenced but not explained or linked in the introduction?
I have been wondering about using \`|\` to indicate layout. It's an entirely half-baked idea, but the idea is: foo = putStrLn | "Hello" Would desugar to foo = putStrLn "Hello" Not exactly compelling, but also not the kind of thing you'd use it for. A bigger example is intersectionMaybe :: Ord k =&gt; Map k a -&gt; Map k b -&gt; Map k ( Maybe a, b ) intersectionMaybe a = Map.merge | Map.dropMissing | Map.mapMissing | | \_k b -&gt; ( Nothing, b ) ) | Map.zipWithMatched | | ( \_k -&gt; (,) ) | Just &lt;$&gt; a Which means intersectionMaybe :: Ord k =&gt; Map k a -&gt; Map k b -&gt; Map k ( Maybe a, b ) intersectionMaybe a = Map.merge Map.dropMissing ( Map.mapMissing ( \_k b -&gt; ( Nothing, b ) ) ) ( Map.zipWithMatched ( \_k -&gt; (,) ) ) ( Just &lt;$&gt; a ) The layout keyword seems to be doing a similar thing, but maybe is less noisy.
Two immediate syntactic annoyances immediately come to mind when working with haskell. 1. No nice way to do multi-line strings, or "string with zero escaping required for literally anything inside of it", or strings with nice interpolation 2. Record syntax sucks, and working with most data structures syntactically sucks too (you end up abusing lists or tuples as a syntax and that doesn't really work well for fancier things like heterogeneous lists). Curly brackets (`{}`) and semicolons being reserved for nearly useless reasons exasperates the situation (except you can use curly brackets with data and newtype too) This proposal looks like something could help with both, if done right.
Wow, thanks for such a detailed response! I'll take a look at this later when I get home.
I would like that to happen by removing the in-line kind signatures and forcing those to be separate personally.
I am not sure if I understand what you're saying, so if I am speaking past you, please clarify. That said, note that my entire point was that the aforementioned "larger" context is not actually larger (i.e., supertypes or subtypes) in any way, shape, or form. Specifically, you can prove that the context is not larger by contradictory examples (e.g., `Foo`.)
&gt; That said, note that my entire point was that the aforementioned "larger" context is not actually larger Right, I understand your Foo example (I think) - the point is that the `Eq (Foo a)` instance requires an `Ord a` instance, not merely an `Eq a`. What I don't understand is why `Ord a =&gt; Ord (Foo a)` need imply `Eq a =&gt; Eq (Foo a)`. Why can't it simply imply `(any context smaller than or equal to Ord a) =&gt; Eq (Foo a)`? Then it would do what we want: it would find your `Ord a =&gt; Eq (Foo a)` and it would find my `forall s w . Eq w =&gt; Eq (f s w)`.
I wouldn't be opposed to allowing something like `(x :: Int)` in type/kind signatures for "named parameters". No semantic changes at this point, but might be used for Dependent Haskell, and could be used by Haddock as a way to talk about a specific parameter.
[`compactable`](http://hackage.haskell.org/package/compactable) is an alternative to `witherable` &gt;Compactable fully subsumes Data.Witherable, offers more laws, and is more general.
Caveat: I'm not a category theorist and I've never used it, but... It looks like it's a way to describe how to convert a natural transformation from a functor to itself to a natural transformation between some other functor and itself. To make things concrete, if we take this `duplicate` function as our natural transformation from a functor (`[]` in this case) (Nto itself: duplicate :: [a] -&gt; [a] duplicate [] = [] duplicate (x:xs) = x:x:duplicate xs Then the three instances that are defined in that module give us the following conversions for free: duplicateList :: [a] -&gt; [a] duplicateList = shiftMap duplicate duplicateIdentity :: IdentityT [] a -&gt; IdentityT [] a duplicateIdentity = shiftMap duplicate duplicateState :: StateT s [] a -&gt; StateT s [] a duplicateState = shiftMap duplicate (Note `duplicateList` is just the trivial case where we get back the same transformation we provided).
Yeah I'm probably fine with that as it currently seems like the only way dependent types are dealt with. It is a little annoying how `(x :: Int)` is sometimes equivalent-ish to `x` modulo extra type inference (in regular expression context) and sometimes equivalent to `Int` modulo `x` being in scope (type signature of dependent type). I get why `:`/`::` is used in both of these situations, but it's a little annoying. f x = (x :: Int) ~= f x = x f :: (x :: Int) -&gt; Vec x () ~= f :: Int -&gt; Vec 5 ()
Maybe it is easier to understand if I approach this from the theory that backs both normal and quantified constraints: Quantified constraints are, by definition, essentially functions over constraints. Most of their theoretical simplicity, utility, and well-behavedness are derived from that fact. If you represent your constraints as explicit dictionaries, then we get eqS :: (forall s w. Eq w -&gt; Eq (f s w)) -&gt; Eq a -&gt; Eq (S f a) ordS :: (forall s w. Ord w -&gt; Ord (f s w)) -&gt; Ord a -&gt; Ord (S f a) By definition, whenever a value can be yielded from `ordS`, a value can also be yielded from `eqS`. In other words, whenever there exists arguments that can be passed into `ordS`, there must also exist arguments that can be passed into `eqS`. If this property does not hold, then an instance `Ord a` could exist without an instance `Eq a` exist. Consequently, the arguments in `ordS` must be able to be turned into the arguments in `eqS`. That is, there must exist a function reduceEqS :: (forall s w. Ord w -&gt; Ord (f s w), Ord a) -&gt; (forall s w. Eq w -&gt; Eq (f s w), Eq a) Consider trying to implement this function. After a while, it should become clear that it is literally impossible to implement this function, since it relies on a `Ord w` existing when only a `Eq w` is available. And since GHC's type checker is built on exactly this theory, the same rules that apply to explicit dictionaries must also apply to implicit dictionaries (e.g., constraints.) Perhaps a valid type theory could be built on your points. I honestly do not know. However, that eventual type theory would not be able to inherit all the nice properties of standard type theory that constraints build upon.
Looks interesting, I will play with it. A comparison of co-log to existing logging libraries in use would be appreciated.
Does reddit support language code blocks using \`\`\`haskell ... \`\`\` in the four-space indent style? For that matter does it support highlighting at all? I'm not seeing any colors regardless, but I'm making assumptions about how the markdown engine works which are apparently unsubstantiated.
If a Haskell user already understands `Semigroup` and/or `Monoid`, perhaps it would be best to explain `Category` as a generalisation of these concepts? Specifically, it allows composition for data types where not every single data type can be composed, but rather where certain "rules" (e.g., output goes into input) needs to be met. Consequently, `Category` can be used to reuse and generalise code when the structure of whatever is being used is completely static. Perhaps also draw in how all monoids form a single category, and how a single category can form a finite or potentially infinite amount of monoids.
I felt it was a bit verbose as well but I stuck through it. I didn't want to rush ahead to the good stuff. I read each chapter, did each exercise, and followed through. It was slow going. However I was in the situation where I was an experienced enough programmer and had dabbled in Haskell on and off. I read enough material and wrote enough code to be dangerous but I still felt like I wasn't \_getting it\_. I could hit a wall with Haskell where my programs couldn't get any more complex and my code was getting frustratingly messy. I think Haskell has different fundamentals one needs to learn and internalize. It was slow going and frustrating to feel like a complete beginner again... but it was worth it. Having a guide and resource to dig into more than the superficial features of Functor, Applicative, etc has been invaluable.
ML/OCaml's modules are similar to what you suggest. However explicitly passing them around can be a huge pain.
There is one major benefit that using type classes with no overlapping instances gives you over just passing instance dictionaries, and that is coherence. Say you defined a `Set a` data type using a binary tree, which requires `Ord a`. You have two options: - You could take an `Ord a` instance as a parameter in every function. However, if the set's elements are sorted by the `Ord a` instance that was passed at construction, passing a different `Ord a` instance will be a logic error that you can't statically prevent. - You could have `Set a` carry an instance of `Ord a` that the set functions use. The problem: how would a `union` function work if the two sets it was passed used different instances?
Thank you.
I'd be most interested in motivating examples, instead of theory.
Thank you for the response. I appreciate your explanation and am interested in the talk. I just plead that we try to gain the interest of the newbie haskeller. Covariant anything sounds really theoretical to me.
The module lacks a high-level introduction. To be consistent with other Haskell documentation, I recommend something along these lines: Categories are 'multi-sorted monoids'. For an introduction, see *Saunders Mac Lane, Categories for the Working Mathematician, Springer 1971*.
I think part of the problem is that there aren't any interesting combinators (none in `Control.Category` at least) that do anything interesting with a `Category`. Even worse, there aren't very many instances of it either. Contrast this with typeclasses like `Functor`, `Applicative`, `Monad`, `Traverseable`, `Monoid`, `Eq`, `Ord`. There are wealth of functions that make use of these typeclasses. So in my mind, the real question is: what are the useful general-purpose combinators we can build with `Category` (possibly involving other typeclasses as well)?
&gt;The problem: how would a `union` function work if the two sets it was passed used different instances? Naturally, one would have to kick this issue upstream to typecheker by encoding the instances in the type parameter of the `Set` datatype. That could make for a cool paper!
Hmm, I'm glad you wrote it out this way - it's clearer to talk in this vocabulary. Ok, I can see how `reduceEqS` is constructed given those definitions of `eqS` and `ordS` - that all makes sense. Here's my qualm: so in my original comment, I only gave `ordS`, and the compiler (with a very misleading error message) bailed on finding `Eq (S f a)`. I know it's failing on that `Eq (f s a)` instance head because the following compiles: instance ((forall s w . Eq (f s w)) , (forall s w . Ord w =&gt; Ord (f s w)) , Ord a) =&gt; Ord (S f a) where compare (S x) (S y) = undefined So here's my question: GHC only needs to find an instance with head `Eq (s f w)` and it will be happy, but why does it need to use the tail you gave in `eqS`? Why must it specifically be that tail and not some other tail, like, say, `myeqS :: (forall s w. Ord w -&gt; Eq (f s w)) -&gt; Eq a -&gt; Eq (S f a)`! I say we should choose this tail because we already know we need an `Ord w`, so we're not putting any additional burden on ourselves by requiring it here, and with this inferred definition, `reduceEqS` is now solvable. But wait, doesn't this mean we'll only find instances of the form `Ord a =&gt; Eq (f s a)`, when in fact we would expect that most of the instances we have available will instead be `Eq a =&gt; Eq (f s a)`, and thus we'll incorrectly fail to find them if there is no `Ord a`? Yes, but that's actually fine, since we would have been dead in the water without that `Ord a` anyway when we bubble up to our overall goal of `Ord a =&gt; Ord (f s a)`!
&gt; isn't using type classes and instance derivation akin to the hated methods of dynamic typing? Not really. It is ad-hoc overloading, which feels a bit like duck typing, but it's principled primarily through coherence which makes for a carefree syntax with with too much lingering suspicion. Now, there are certainly problems with the approach. Global properties are inherently anti-modular. Uniqueness of instances is very often a useful fiction, not an absolute truth and that doesn't always go away even if we did have laws expressed in a way that static analysis could check them. Practical concerns also abound, like how to insert a class into a hierarchy in a backward-compatible way, and how best to represent intersections of classes that introduce new laws, but no new operations. Haskell would be a very different language without type classes. And, I think that language would not be as nice. --- That said, something that looked more like directed term inference with a fallback to Agda's instance lookup or Scala's implicit resolution might even be better. Though we'd still be stuck determining some acceptable decision procedure for equality of instances/dictionaries, for things like `unions` and the like with good complexity classes.
I've attempted it in Idris before. It doesn't make for as nice an interface as you might think. It's certainly possible for simple code, but if the compiler can't unify the instance index/parameter, it gets a bit hairy. I think it is also harder to optimize because you end up carrying an existential or `Reader ord` wrapper. It may actually be the better solution; I can think of several cases where passing in an anonymous `Ord a`, `Eq a`, or `Group a` instance might have been more clear than introducing a `newtype` wrapper, and it would nearly always be fewer lines of code.
I don't think you can really use `type` like that and get good results. It would at least have to be `newtype`s. There's no way to differentiate `Subtraction` from `Addition` since both are just aliases for `a -&gt; a -&gt; a`. Also, what if you wanted to use features like functional dependencies or associated types that typeclasses currently support?
If we did this, how would we add two numbers? One of the key motivations for type classes is to be able to use the \`+\` operator on operands of many types and have the right thing happen.
Yeah, some books forego some explanation of minute details or things and instead teach them later on. I personally felt that HFFP was rather verbose and dry and to me that academic style isn't 'fun' so it's hard to keep reading. On the other hand some books that skip some details in favour of getting you up and running quicker and teaching you how to make things with shorter explanations are more fun to me and so I prefer books like that. I guess it's a difference in style. A book could be the most comprehensive and objectively best in the world but if it's ultimately boring a lot of people will not push through to the end.
I wrote previously that comparison with other libraries requires more time and compared `co-log` with `logging-effect`: * https://www.reddit.com/r/haskell/comments/9iskza/ann_blog_post_colog_composable_contravariant/e6psqwx/ There's also an overview of logging libraries in Haskell by /u/bravit: * https://drive.google.com/file/d/1AFuVWznFWDSsNeraz1HMPmCT9kgT7jUw/view
In `tomland` we have the following data type: data BiMap e a b = BiMap { forward :: a -&gt; Either e b , backward :: b -&gt; Either e b } This data type has a `Category` instance, and this makes values of this type extremely composable. See Haddock with nice illustrations: * http://hackage.haskell.org/package/tomland-1.1.0.1/docs/Toml-Bi-Map.html I can also recommend reading the whole blog post about `tomland` :) * https://kowainik.github.io/posts/2019-01-14-tomland
I hate to be negative, but I already understand the types discussed in the article and I still couldn't figure out what the article was trying to say.
Thanks for telling me about the library! I never heard about `compactable`. My first impression is that `Compactable` looks more complicated: the typeclass has more methods, and it's not that easy to grasp everything at once. I would like to see something like `Filterable` or `Compactable` in `base` though.
That's okay. I don't know what I was really trying to say either. If experiments always succeeded, life would be boring.
I don't think it supports any highlighting, unfortunately.
 main = allWantDrink [True, True, True] allWantDrink (False:_) = putStrLn "No." allWantDrink (True:[]) = putStrLn "Yes." allWantDrink (True:xs) = putStrLn "I don't know." *&gt; allWantDrink xs allWantDrink [] = putStrLn "Quiet day..."
Idk about agda, but Scala's implicits are definitely a step down from typeclasses, at least the way they're implemented in Scala.
Oh sure, multi-sorted monoids and Mac Lane, that will make everything clearer for beginners. /jk
Surprised noone's linked [Type Classes vs. The World](https://www.youtube.com/watch?v=hIZxTQP1ifo). The main thing coherence buys you is not needing to know which modules were in scope to know which instance a function is talking about. Contrast this to some other languages (apparently Scala) which try to resolve which instance in scope is the Best One via a set of prioritization rules. Personally, I'm okay with Agda's solution, which says instances don't have to be globally unique, but at any particular use site, the program will refuse to compile if the resolution does not turn out to be unique (where "unique" means all paths to a valid instance resulted in the same term up to definitional equality). This does mean that unlike (ideal) type classes, scope does matter, but at least unlike Scala, there's no esoteric rules for choosing an instance from multiple options within a scope. Regarding things like structures which require an order to talk about them, I would probably take the position that the required order should be a type index, rather than saying "you can only talk about one global ordering." For example, in my world we'd have `union :: Set o a -&gt; Set o a -&gt; Set o a`.
[removed]
You are right when you are saying that you cannot change the shape of a functor, but the contract only apply to \`fmap\`, or any function working on functor. It doesn't mean you can never change the shape of a functor, only that you can't change if the only thing you know is a functor. If you have a list (and know it's a list) you can modify it using \`filter\`, but \`filter\` is a function dealing with list not with any functor. &amp;#x200B; When you say \`&lt;\*&gt;\` doesn't aggree with the contract, it doesn't have too: the contract only applies to \`fmap\`. In the same way that you can modify the shape of a list (using \`filter\`) you can modify the shape of an applicative using \`&lt;\*&gt;\` even thought it's a functor. You can see that as modifying the shape of a functor, but you are not modifying the shape of ANY functors, just the one which are applicative too, i.e, the one which you've been given an interface to modify the shape (using \`&lt;\*&gt;\`). Fundementaly, Applicative is a Functor where you can \`zip\` two , so you can tranform \`f a\` and \`f b\` to \`f (a,b)\` (using \`liftA2 (,)\`). A both arguments can have a different shape, you can't preserve the shape as the result can't match the shape of both arguments.
(It helped me!)
This expression has Void type, but still does not construct a value of that type because it'll bottom ;-) Try `main = fix Void `seq` return ()` : the program will hang. This comes from the fact that Void definition is a newtype and not data. There is no actual value constructor to build values, hence the fact that all expressions of that type *must* be bottoms.
I still maintain that something like [this](https://www.reddit.com/r/haskell/comments/c9kmvd/simplifying_typeclasses/) would be very close to giving us the best of both worlds, you can explicitly pass structures like `Eq` and `Ord` and `Group`, but you still get implicit and coherent typeclass dispatch when you want it.
As someone who is a huge proponent of polymorphic records I can say I agree with /u/fsestini and don't think there is any inconsistency here. To be more specific I want polymorphic records to store large amount of data in flexible ways, such as only passing a subset of the fields over a network, or only requiring a subset of the fields for creating the object initially, or fattening certain fields to follow a ƒoreign key. And in the cases where I do actually want them just to make function arguments more named, having them solely in the documentation/source would not benefit me. I want them at the call-site too. `run 5 800` with the docs saying the first arg is named `threads` and the second is named `port` is not helpful, but `run { threads = 5, port = 800 }` is great.
Should it mention `type Cat obj = obj -&gt; obj -&gt; Type`? Category :: Cat obj -&gt; Constraint (-&gt;) :: Cat Type Kleisli m :: Cat Type Op :: Cat Type (:~:) :: Cat obj Coercion :: Cat obj (:-) :: Cat Constraint
Trippy. It's like DeepDream imagining Haskell.
Personally, I would argue that while Rust and Haskell have very capable and sound type systems, with each their own form of polymorphism mechanisms, that's about all they have in common, other than being programming languages. I have used Haskell only for a few months, and I've attempted to use Rust multiple times only to come back to more FP-like languages in the end. As such, I am in no way well versed to actually answer this properly. But imma share my observations anyways: - Haskell is very strict about expressing side-effects, Rust is not. Rust is only strict about expressing error messages, something Haskell sorta isn't since it has exceptions in monads that are not described explicitly by a given function's signature. - Rust will protect you from most cases of race conditions in multi-threaded cases, while Haskell can only protect you when using specific functions (e. g. IORef's atomicModifyRef and aromicWriteRef) or through specific structures (TVar and MVar come to mind, although I have yet to use them a whole lot so I might be wrong here). - Haskell is proper functional programming, while Rust borrows concepts from FP (functors, pattern matching etc.) as well as OOP (methods, encapsulation etc.). - Rust mostly picks features, syntax and functionality that works best given its intended use case (a replacement for C and C++, with broader versatility), as well as what works most sensibly with the borrow checker. Haskell mostly picks what conforms closest to our understanding of category theory. - Rust has a whole standardized build system, in the form of Cargo, bundled with the rustc compiler, as well as a proper toolchain management suite with rustup. Haskell (from what I've gathered) technically doesn't have one standard way of doing things, but most consider the stack toolchain to be the defacto way of handling dependencies and builds. So rustup = stack, cargo = cabal (but cabal is most often called upon through 'stack build'), and rustc = ghc (Glasgow Haskell Compiler). Rust and Haskell think about structuring programs very differently from each other as a result of these (and many more) differences. Type soundness is really the only big thing they have in common in my eyes.
I've done a comparatively big amount of rust compared to haskell (currently reading real world haskell, it's available for free online(non pirated as far as I can tell!) and can only recommend it) and I'd say they're languages that share a lot of basic features like pattern matching and algebraic data types, typclasses are essentially traits etc. but the code you write is very different, as are the mindset you have with each language. I think already knowing rust will help you in learning haskell but you still need to put in quite some effort to actually learn the language. regarding rust and fp I've recently read * [https://www.fpcomplete.com/blog/2018/10/is-rust-functional](https://www.fpcomplete.com/blog/2018/10/is-rust-functional) * [https://github.com/JasonShin/functional-programming-jargon.rs/blob/master/README.md](https://github.com/JasonShin/functional-programming-jargon.rs/blob/master/README.md) * something else I can't remember right now that compared rust haskell and ocaml
Over co-enginnering at his co-best
&gt; Haskell mostly picks what conforms closest to our understanding of category theory. facepalm.gif &gt; most consider the stack toolchain to be the defacto way of handling dependencies and builds [citation needed] Also the closest to rustup is probably (ghcup)[https://www.haskell.org/ghcup/]
&gt; facepalm.gif Snarkiness aside, I would love to hear your opinion on how the GHC steering committee picks what's a part of Haskell and what's not.
[facepalm.gif](http://i.imgur.com/rOhUhfS.gifv) --- ^(*Feedback welcome at /r/image_linker_bot* | )[^(Disable)](https://www.reddit.com/message/compose/?to=image_linker_bot&amp;subject=Ignore%20request&amp;message=ignore%20me)^( with "ignore me" via reply or PM)
Good bot? Technically...
&gt; it's available for free online In fact, it is linked from the sidebar of this very subreddit as "[Real World Haskell](https://web.archive.org/web/20190509070638/http://book.realworldhaskell.org/read/)".
Oops, didn't know that. Maybe I should really start to look at the sidebars of the subreddit that I frequent in greater detail :D
typeclassopedia will get you started on applicatives, functors, and monads, foldables, etc. Then there's "what I wish I knew when learning Haskell"
data Stream a = a :| Stream a deriving (Show) How to understand the :| operator?
I guess you should ask the members. But anyway, the GHC proposals are published online and discussed publicly on github. Maybe you could point out which ones are based on, or even mention category theory, since I haven't seen any.
It's an infix data constructor (I'd wanna say you see them less frequently than prefix); if you write the above in GHCI and type: :t (:|) (:|) :: a -&gt; Stream a -&gt; Stream a which is the way to construct a stream given an a and a stream.
Not so tiny URL: &lt;https://www.cs.nott.ac.uk/~pszgmh/mpc19.html&gt;
Plenty of haskell language concepts mention category theory (monoid, functor, monad) but they arent exactly identical to their category theory counterparts, just analogous. Suggesting that haskell is an incarnation of category theory is pretty farfetched imo. It's much more accurately an extended system-F.
Monoids are not a concept from category theory. I don't know how you get from a handful of CT-inspired core Haskell concepts (most of them introduced 20 years ago) to "Haskell *mostly* picks what conforms closest to our understanding of category theory."
Not a guide for Rustaceans, but let my mention [the guide I wrote](https://github.com/soupi/haskell-study-plan/) which might come close to what you are looking for (a slightly denser study plan for Haskell).
I'm coming from the other way around. Now learning Rust, I see a lot of connections between their type systems. Turns out, they are similar for a reason. They both come from a plethora of type theory and programming language literature. It's interesting to find out how a certain type calculus is presented in different languages. A one-to-one table may be helpful, but it can also be overwhelming and ignores a lot of nuances. I suggest to follow the normal method and find out the links by yourself along the way. Another benefit is that Haskell mostly calls the construct "by its name". For example, "sum type" instead of "enum". This makes it easier to read those literature. If you want to dig deeper, I recommend *Types and Programming Languages*.
The book isn't academic or dry at all. If anything I find the humor and goofy nature of some of the examples a bit off putting, but at least it's not downright rude like some of LYAH. If you think some part of the book is too detailed or too verbosely explained, then you already get it, and the explanation isn't "for" you. It's for the folks that don't get it. The book makes it pretty clear that you can skip ahead if you already get stuff.
Yeah... I wish we could all agree that leaning on external material for documentation is a code smell - academia vs. industry aside, it's a bad practice, and it's one of the most off-putting things about the Haskell ecosystem. Mind, referencing a paper - good. Referencing a paper as a stand in for a proper explanation - bad.
I have seen this problem as well on archlinux and its caused (to my knowledge) by different distros putting their ncurses libs in different places and naming the file differently. I could never get the ncurses package for haskell fully working. Something else you might want to try if you are not fully committed to ncurses is [http://hackage.haskell.org/package/brick](http://hackage.haskell.org/package/brick) an ncurses alternative based on FRP principles.
This could be useful. Does it have any unusual C dependencies? Although I've gotten ncurses working now, it's a bit of a pain, and is welcome something better.
Dunno if it helps but there’s a Category instance for Definitions in Squeal. https://hackage.haskell.org/package/squeal-postgresql-0.5.1.0/docs/Squeal-PostgreSQL-Definition.html#t:Definition
If the first documentation was good enough for me, I don't see why I need to write more, probably worse documentation just to provide it in situ.
Also AlignedList and Terminally https://hackage.haskell.org/package/squeal-postgresql-0.5.1.0/docs/Squeal-PostgreSQL-List.html#t:AlignedList https://hackage.haskell.org/package/squeal-postgresql-0.5.1.0/docs/Squeal-PostgreSQL-Migration.html#t:Terminally
Just to clarify: Brick is not based on anything related to FRP. But it will definitely be nicer to use than ncurses, in my humble opinion as author of Brick. :) Also, if Brick is too high-level for you, you can always just use [Vty](http://hackage.haskell.org/package/vty). (Brick is built on top of Vty and interoperates with it.)
Could someone please suggest a resource for unfolding with coproducts and free monads? For example, the type of content if one were to extend Data Types A La Carte to include unfolding. Thanks!
Actually, I'm reading through the docs now. It's going to take some time to grok, but I really like what I've read so far. 👍
Looking at the actual proposals, I now see what you mean. Correct me if I'm wrong, but the vast majority of proposals aim to enrich the type system, from what I can see. Corrected my statement above.
I mostly see type system additions and syntax enhancements.
Great! Please get in touch if you need anything. I'm happy to help.
Here's a more serious attempt at an intro. Feel free to use it however you like. --- A category is a collection of types `cat a b`. We call `f :: cat a b` a morphism with source `a` and target `b`. Morphisms with matching sources and targets can be composed: If we have `f :: cat a b` and `g :: cat b c`, then the composition of `g` and `f` is `g . f :: cat a c`. We also have identity morphisms `cat a a` for any `a`. A useful analogy for morphisms in a category is paths: If we can go from a place `a` to a place `b` and we can go from `b` to `c`, then we can go from `a` to `c`; and we can always go from `a` to `a` (i.e. stay where we are). The prototypical example of a category is the type of functions: `(-&gt;)` is a category where the morphisms are functions `a -&gt; b`. Composition is function composition `(.)`. The identity morphism for a type `a` is the identity function `id :: a -&gt; a`. See the instance for `(-&gt;)` below. Monadic functions `a -&gt; m b` (where `m` is a monad) also form a category. If we have `f :: a -&gt; m b` and `g :: b -&gt; m c`, then the composition is `g &lt;=&lt; f :: a -&gt; m c`. (`&lt;=&lt;` is from `Control.Monad`.) The identity morphism is `return :: a -&gt; m a`. See the instance for `Kleisli` below. To form a valid category, composition and identity must satisfy some additional laws (which our examples do): - Composition must be associative: `(f . g) . h = f . (g . h)`. This means that the order in which we compose morphisms doesn't matter. - Identity arrows are neutral elements for composition: `id . f = f . id = f`. Our notion of a category is a special case of the [mathematical concept](https://en.wikipedia.org/wiki/Category_(mathematics)). --- I would also add the following to the function documentation: For `&lt;&lt;&lt;`: `&lt;&lt;&lt; = (.)`. This alias is convenient because `(.)` overlaps with `Prelude.(.)`. For `&gt;&gt;&gt;`: `&gt;&gt;&gt; = flip (.)`. Composition is usually written 'backwards' for historical reasons: If we have `f :: a -&gt; b` and `g :: b -&gt; c`, we write `g . f :: a -&gt; c`. With `&gt;&gt;&gt;`, you can compose 'forwards' instead: `f &gt;&gt;&gt; g :: a -&gt; c`.
[Rust for functional programmers](https://science.raphael.poss.name/rust-for-functional-programmers.html) has a lot of side-by-side comparisons of Rust vs OCaml vs Haskell; it's intended to teach Rust, not to teach Haskell, but sometimes "X for people who already know Y" can also work as a "Y for people who already know X".
https://youtu.be/uR_VzYxvbxg
GHC Core is not a subset of Haskell – it is a different language with its own type system. GHC front-end compiles Haskell to Core, and then the GHC backend compiles Core to LLVM or native code. &gt; Is there a spec or something? There is! https://github.com/ghc/ghc/blob/master/docs/core-spec/core-spec.pdf
Very interesting! Thanks for sharing!
Oh very interesting, so a sort of IR-ish thing?. Thanks, will check out the link!
Really cool, thanks!
OK, I am going to have to admit that that question does require more expertise to answer then I have, as I am mostly acquainted with the type theory of type classes and instance subtyping, rather than the intricacies of how GHC solves instance heads. For the record, I would guess it is either due to a) some subtle theoretical property that makes your proposal unsustainable, or b) GHC's theory for quantified constraints does not necessarily work with instance heads the same way. However, these are indeed just guesses, and could be completely wrong.
Let's separate GHC implementation bugs from the Haskell spec here. Haskell says that instances must be unique. GHC has bugs allowing different scopes to have different instances for the same type sometimes due to the compilation model and the ghc developers deciding it's not a big deal in practice.
&gt; Naturally, one would have to kick this issue upstream to typecheker by encoding the instances in a new type parameter of the `Set` datatype. This needs dependent types.
The title says fast but I didn't see readme or docs mention speed. Saw some benchmarks, though.
I agree it's not clear from what I wrote, but I was actually talking about the uniqueness of potential, total, type-correct, instances, not about the uniqueness of declared instances. There's really only one Functor instance (if function extensionality is one of your axioms), by parametricity; I think the same thing is true for Foldable. We like to extend this and say there's only one Traversable instance, too, but for most types that's only a useful fiction since there a question of traversal order for all "shapes" with a more than two "`a`"s. --- I totally agree that we can ignore GHC bugs. I also think the approach of allowing multiple instances to exist and only erroring if more than one is imported into the same scope is fine, sane, and can basically be ignored for coherence arguments.
Speed is a natural consequence of using vectors to represent coefficients. Most other implementations (e. g., [`polynomial`](http://hackage.haskell.org/package/polynomial)) use lists.
Ah, that's what you meant? Sorry, then. I don't think Foldable is uniquely defined by its laws though. `data Two a = Two a a` has two potential instances, the same as Traversable. But that's just a minor quibble, I'm in general agreement with you.
The HP uses \*both\* cabal-install and stack, and provides little insight into what \*most people\* consider to be the defacto tool.
During this one month I have bounced between several resources. HPFFP seemed like a too long of a path. However, after this one month, I believe that is the best choice for someone learning from scratch. Now I am sitting here thinking how I can read a thousand-page book.
I notice that the current version of your package is 0.47.1, but the second number seems to update frequently. Is it safe to assume backward compatibility for all future 0.* versions?
I've long removed the statement in question that you're complaining about by now, but Stack was also [the most common way of installing GHC in 2018](https://taylor.fausak.me/2018/11/18/2018-state-of-haskell-survey-results/?source=post_page---------------------------#question-030).
Ah right. If we required a CommutativeMonoid / AbelianMonoid for foldMap then we'd be down to one, but as is we still have two.
Yep, the survey does seem to suggest a preference for stack, I agree.
This is "true" but without comparative benchmarks you can't really say it's true. Performance is complicated to predict, especially on small data.
GADTs allow to reflect structure of value terms into type space. They are not as powerful as full blown dependent type, but are good enough to allow a lot of tricks.
There are at least two ways: 1. One page at a time, if you're methodical and willing to burn through the whole thing. 2. Start at the front, read the table of contents, looking for things you don't know well. Then skim through the book, slowing down and reading deeply when you don't understand something but consciously not reading every word on ever page.
Are there any suggestions/guidelines for using lazy versus strict bytestrings to represent small string data - ie urls, system / cli arguments, addresses, configuration settings etc. I have defaulted to using strict bytestrings, since they somehow seem simpler/ cleaner without unevaluated thunks, and when there is no material performance benefit from laziness (cf streaming). And when I need to do de/serialization with Aeson, I use lazy text and bytestring conversions. Is this approach reasonable and consisent with what other's do?
One of my favourite `Category` instances is the one for [`ExchangeRate`s in `safe-money`](https://hackage.haskell.org/package/safe-money-0.9/docs/Money.html#t:ExchangeRate). You can exchange anything for itself, and if you can convert from USD to BTC, and BTC to EUR, then you can convert from USD to EUR (it might not be the only way to do so, but it exists).
You can still use bubble sort (or random permutations until order is correct, or even worse) and other inefficient stuff with vectors.
A bit old, but still should be valid: https://stackoverflow.com/questions/6121146/reading-ghc-core
With only two methods and six instances of various usefulness most interesting examples can be implemented without using Control.Category. I have one that allows you to create valid Haskell functions that at the same time can be rendered into valid JavaScript producing the same computations. I had to add a bit more classes thought. Cartesian, Closed, etc.
Does this implement, e.g., Karatsuba or FFT multiplication? An asymptotically better algorithm beats a small constant factor for large enough inputs.
Wow, this is so cool! Is this the way forward to building in-browser IDEs, dashboards etc?
Thanks. It's hard to say - there are so many good web dev. frameworks and languages out there (React, TypeScript, Elm, PureScript, ReasonML, etc.), and many are more popular than Reflex (or Haskell). But maybe you're right - our use case for Reflex is exactly that - when we develop in-house proprietary UX, 100% browser based, demos, proof-of-concepts, or on-premise DevOps-less solutions (essentially - the browser is our OS in these tools). (The rest of the time we use other cloud solutions for BI/Dashboard like Microsoft PowerBI) As for Reflex itself - I think the style of "Functional Reactive" combined with the powers of Haskell and Nix - gives unprecedented flexibility to compose and refactor your work, which is what we need as a startup (that is focused on analytics not UI). We have a very big production Reflex code base, that more than once, we've refactored aggressively in a couple of days.
Other people using reflex, I'm so happy!
 (&lt;=&lt;) = (&lt;&lt;&lt;) @(via Kleisli) (&gt;=&gt;) = (&gt;&gt;&gt;) @(via Kleisli)
Thanks for this. I think program generators like this one are crucial to make libraries like recursion-schemes really worth the use. However, I'm usually not a fan of microlibraries. Have you considered publishing it as an addition to recursion-schemes rather than as a separate package? On a related note, for the future, I would advise publishing libraries as a [package candidate](http://hackage.haskell.org/upload#candidates) before asking for feedback.
Thanks for your feedback! My first approach was to open an issue asking if this library would be a good addition to the recursion-shemes library and the maintainers said that it would be best if this was released as a separate package!
Alright then!
&gt; We need this proof because at each iteration except the last one, we call this function again on (i -1), but we proved only that i &lt;= x1, not (i - 1) &lt;= x1. You might be interested in [`ghc-typelits-natnormalise`](http://hackage.haskell.org/package/ghc-typelits-natnormalise).
It seems like it might not be a proper category, since ((f . g) . (h . k)) and (((f . g) . h) . k) might have different fees.
If Brick is following the Hackage PvP, that would not be true. The first two components together make use a "major" version. If Brick is following SemVer, that would not be true. SemVer point 4 is "Major version zero (0.y.z) is for initial development. Anything may change at any time. The public API should not be considered stable.".
I know that's the policy, but most of the versions are in the format 0.y, not 0.y.z. According to the policy, it is perfectly allowable to increment the y value (or any other value) without introducing breaking changes. I just don't want to make the assumption that this is what the author is doing.
I change the second number whenever I change (add/remote/alter) the API. I change the third number when I fix bugs or make other minor changes (e.g. documentation). For best results, constrain your dependency to X.Y.\* and relax the constraint to test with new versions when they come out. The first number will only ever change if I decide to rewrite the library or overhaul the API in a significant way, and that is unlikely to happen.
Good to know. Thanks!
https://richarde.dev/pubs.html for more
As it was mentioned above, there are benchmarks in source code, they are just not mentioned in `README`. Polynomial arithmetic is an area, where vectors absolutely and obviously shine. For arithmetic operations `poly` is roughtly 30-50x faster than `polynomial`.
Not yet, but neither known competitors implement them. It is unclear, how to define a threshold for Karatsuba multiplication for an unknown type of coefficients.
Sure, but there are benchmarks to ensure that asymptotics is still expected.
How do I handle C dependencies in a Haskell library? I am writing bindings to a C library. How do I actually include the files? Can i use a git submodule to a specific tag? How do I get Cabal to call make on the library, or do I need to add the sources into the cabal file under c-sources and compile myself?
The modules that are used for your project, should be in your project folder. And it should be placed inside (one of) the folder(s) that is assigned in `hs-source-dirs`. (In your case, `./src`) So if your `.cabal` file is in `/home/user/mylib/mylib.cabal`, your modules should go in `/home/user/mylib/src/...`
That sounds reasonable. I think lazy bytestrings are a lazy (in the pejorative sense) way to do streaming, as opposed to streaming libraries. There's barely a point to it for aeson since it allocates the whole object in memory anyway. They are certainly convenient when they are sufficient though.
Hi.. &amp;#x200B; Thank you for posting this in Reddit. &amp;#x200B; PS: Reddit was blocked in my country, so please forgive my slow response.
I've been looking for precisely this - only to go in the opposite direction: to learn Rust while already knowing Haskell. Specifically, to learn which concepts are similar - which would be Rust's equivalents of type classes / generics, etc.
In principle, you can do everything type classes do using just normal types, function records, and parametric polymorphism - as shown [here](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) \- if you don't mind having to pass an extra parameter to all the functions that (with the usual design) have a type class constraint in the signature (so that "sort xs" becomes "sort ordInstance xs", "x + y" becomes "add numInstance x y", and so on. &amp;#x200B; Type classes avoid that (not small) inconvenience; as a downside, sometimes it can be tricky to figure out which instance are you executing (and where in the code is that instance declared). That used to be a nightmare for me in OOP languages (WHICH instance of this interface is being run here exactly?), but hasn't given me serious trouble in Haskell - probably because 1) Haskell doesn't rely so much in classes/instances as OOP languages do and 2) Haskell classes usually have laws - and you end up realizing that knowing the class's laws is the only information you need to work with them - no need to look at how the instances work internally, as was the case when I worked with OOP instances.
Ah thanks for the clarification, i've only read through the docs and some of the examples it seems FRPish to me so I labeled it as such.
is possible to use **absolute path** for **hs-source-dirs** so that I can point to my module which is NOT under the **stack root** folder? e.g. hs-source-dirs: /mylib/mymodle.hs
Just because it was good enough for you doesn't mean it works for other people. In particular if it is an academic paper then a motivating example or example usage of the library may suit some people better.
This is awesome, I'm saving it. Thank you!
You can use existentials &amp; higher-rank types to associate type variables with terms—then you can’t mix them up unless you have a proof in scope that they’re identical. Something like: -- Encapsulates a value with a type-level name newtype Named n a = Named a -- Imbue a value with a type-level name for a scope name :: a -&gt; (forall n. Named n a -&gt; b) -&gt; b name x k = k (coerce x) -- Extract a value from a named value class Coercible a b =&gt; Extract a b where extract :: a -&gt; b instance Extract (Named n a) a where extract (Named x) = x -- A value sorted by the given comparator newtype SortedBy c a = SortedBy a instance Extract (SortedBy c a) a where extract (SortedBy x) = x -- A comparator type Compare a = a -&gt; a -&gt; Ordering -- Construct a set with the given comparator fromList :: Named cmp (Compare a) -&gt; [a] -&gt; SortedBy cmp (Set a) fromList cmp xs = SortedBy (Set.fromList xs) -- Union sets with the same comparator union :: Named cmp (Compare a) -&gt; SortedBy cmp (Set a) -&gt; SortedBy cmp (Set a) -&gt; SortedBy cmp (Set a) union cmp x y = SortedBy (Set.union (extract x) (extract y)) -- Give a type-level name to the value-level comparator; -- the compiler ensures the comparators aren’t mixed up name (comparing fst) \ cmp -&gt; let x = fromList cmp [("a", 1), ("b", 2)] y = fromList cmp [("a", 2), ("c", 3)] in union cmp x y -- {("a", 1), ("b", 2), ("c", 3)} This approach offers many benefits (arbitrary proofs attached to values) that you’d think would require dependent/refinement types.
Looks interesting, will definetly use it to look up stuff when I'm stuck, thanks :D
The researcher who wrote it, Mike Izbicki, aka, u/PokerPirate, also known for [hlearn](https://github.com/mikeizbicki/hlearn) moved away from Haskell a couple years ago, and as far as I know there is no plan to come back to it in the foreseeable future
Just a technicality but if I'm [reading this correctly](https://doc.rust-lang.org/1.30.0/book/first-edition/closures.html#syntax), even if `name` were a mutable reference the closure would still be pure in your scenario of "outside actors mutating part of the closure's state". Because you can't have multiple mutable borrows. So only the closure could break its own referential transparency.
Help with Selda: I am having a problem finding the last row inserted into a Postgres database, I lack understanding of the library and couldn't find examples I could use. I tried the following: getMaxId :: Query s (Aggr s (Maybe (ID Article))) getMaxId = do article &lt;- select articles return $ max\_ (article ! #articleId) getArticleById :: Col s (ID Article) -&gt; Query s () getArticleById id = do article &lt;- select articles return $ restrict (article ! #articleId .== id) But couldn't connect these too in order to create a query which finds the article with the max id. I've struggled with the compiler for the previous hour and a half peeked at the hackage docs for something that might help but didn't reach any result. Can some one help me ?
Yeah I looked into it, but he was working on subhask what happened there?
&gt; The only piece missing is the word counting. This is a bit tricky as it involves dealing with a state monad and wrapping it as an Applicative Functor: Couldn't you do something like: data SepCount = SC Bool Bool Int deriving Show mkSepCount :: (a -&gt; Bool) -&gt; a -&gt; SepCount mkSepCount pred x = SC p p (if p then 0 else 1) where p = pred x instance Semigroup SepCount where (SC l0 r0 n) &lt;&gt; (SC l1 r1 m) = SC l0 r1 x where x | not r0 &amp;&amp; not l1 = n + m - 1 | otherwise = n + m Then you can: import Data.Char (isSpace) main = print (traverse (Const . Just . mkSepCount isSpace) "hello world")
You do not have to copy the module to each new project. You can turn your module into a package which all of your other projects depend on
About the weird `fromInteger` definition: giving a counter-intuitive implementation such as in the `linear` package is still better than giving a partial implementation. It might be counter-intuitive, but it still satisfies all the laws (https://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#t:Num) so it is completely stable (no programmer should expect anything more than the typeclass laws). Instead of using `sum` you could also use `mconcat` which doesn't require a Num instance and just requires a weaker `Monoid` instance (https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Monoid.html#v:mconcat). For the second part of your reply: think about what happens when you call `sum []`, it has to return something but it only knows that the return value is a `Num` so it returns `mempty :: Num a =&gt; Sum a` which is defined as `0` (https://hackage.haskell.org/package/base-4.12.0.0/docs/src/Data.Semigroup.Internal.html#line-231) (bare numerals in the source code are automatically converted using the `fromInteger` function in Haskell).
Ahh yes, makes sense and I didn't think of that, thanks :D I'll have a look at the links you sent, thanks :)
&gt; We can also compare our results to a [previous analysis of Haskell source on GitHub](https://gist.github.com/atondwal/ee869b951b5cf9b6653f7deda0b7dbd8), which, too, finds that `OverloadedStrings` is the most popular extension. The ten most popular extensions listed in the figure above also feature in that analysis' list of the 20 most frequently used language extensions, although not necessarily in the same order. The reason for that is not immediately clear—it might be that our Haskell data set is not representative of all Haskell code on GitHub; after all, at the time of writing, there are around 45,000 Haskell projects on GitHub, while our data set contains only 2,312 packages. I would also expect more applications to be up on github than on stackage, which is mostly libraries.
There was a discussion about doing this for the new GHC build system Hadrian: https://github.com/snowleopard/hadrian/issues/227 A quick summary: It's possible but not easy and someone needs to do it.
This is actually more a SQL question than a library question. The SQL you want is select * from articles order by id desc limit 1 ^ this fetches the most-recently inserted article
It's not super unusual, at least in my experience. I have found the installation of the `haskell-ide-engine` to be tedious in the extreme. If you are using `stack` for projects, I can highly recommend Haskero, which runs out of each project folder and therefore doesn't complain about needing recompilation if you are using two different versions of GHC in different projects.
Have you considered using the `OverloadedLists` extension, or possibly quasiquotation, to provide a nicer way of writing polynomials? For example, you could say `[2 ,-1, 3] :: UPoly Int` which would give you 2 X^2 - X + 3.
&gt;Couldn't you do something like: &amp;#x200B; Yes, I think that's quite possible and would also make the example a lot easier to digest. I'll definitely have a closer look at your proposal and will also update my example. Thanks for bringing this up!
Awesome write-up! It’s nice to see more stuff on architecting programs and how to go beyond \_some\_ pattern to actual usage of it.
Oops, that's just typo. Will change it in a second! Thanks for pointing this out!
It took me almost 2h to install and then when you're finally done you find out it only works with a single version... Tooling for this language is truly horrible.
What a rad idea. Just signed up.
Not normal from my POV. HIE itself takes around 15 minutes at most to build, given that you have ghc already installed and deps built. Fx8350/16G
It is really simple if you just have some C files that need compiled and linked (no Makefile). See ed25519, ed25519-donna, or [hashable](https://github.com/tibbe/hashable) for some examples. If you need to incorporate a Makefile then you can't use cabal's `simple` build system and instead must do... something else. The [intel-aes](https://github.com/rrnewton/intel-aes/blob/master/Setup.hs) package is the only one I remember off-hand for this sort of work. It is more common to assume a system library and link to that than to bundle large C libraries and build systems.
Thanks. I think I have figured out the Custom build-type. Does this seem to be on the right track? https://github.com/dbousamra/imgui-haskell/blob/master/Setup.hs
As per my other comment, I have had no issue with intero. It can be installed globally (though you end up with the same issues as hie), but more commonly you install it into your local stack sandbox and it just behaves itself.
That’s a pretty interesting idea, thanks for suggestion!
No problem :) I have used this technique a lot. It makes dealing with some types really nice.
28 gig .stack folder size is normal?
Can't say right now, sorry. I'll check it in a couple of days
It works only with natural numbers from \`GHC.TypeLits\`. We use Peano naturals from \`type-natural\` because it is impossible to use induction with the first one.
One of the authors here. See [the supplement](https://richarde.dev/papers/2019/kind-inference/kind-inference-supplement.pdf), Section B, for some nice GHC-centric nuggets that are easier to digest than the main paper, which is rather technical.
I like this attitude
What do you plan to use shake for?
We played it, it was great!
I disagree. A lot of people can talk the talk, but cant necessarily cut it. I dont think it's a strong indicator personally
That seems quite literally impossible to me. I'm disappointed in that in that whole discussion thread no one ever mentioned the fact that all that work that configure does is to make sure you have a compatible environment. Hadrian is a haskell program. Which means it needs a working GHC, which means it needs a compatible platform linker and c compiler. It also needs compatible versions of shared libs such as pthreads and other things configure checks for. Without configure you have no way of knowing if Hadrian will even run, let alone if GHC will even run, given the various architectures and distros have different default flags for the C Compilers and different versions of shared libs. Sometimes the shared libs aren't even compatible but carry the same name (e.g. Common practice on macos where Apple tends to reinvent the wheel). No, i quite literally do not see how you can replace a script with no external dependencies, or rather a highly portable script, with a something with a dependency from here to the moon. And personally I find Shake/Hadrian to be more complex than any other build system I've ever used because of this type safety thing. It forced me to jump through hoops to do even the most simple stuff, and I have yet to see a return on this complexity other than academic. Want to know why configure is still around even though everyone (including me) hates it? Because it works and is dependable and very portable.
I think unless you specify it tries to compile hie for all the supported versions of ghc which might take longer. 40 minutes is not too crazy if that's the case.
Ah, right. &gt; because it is impossible to use induction with the first one. Well, you can define a function of type ``` elimNat :: forall n p. KnownNat n =&gt; (forall m. p m -&gt; p (m + 1)) -&gt; p 0 -&gt; proxy n -&gt; p n ``` using `unsafeCoerce`, right? And then use this function as a primitive.
Gosh...
[Gosh...](https://imgur.com/a/Tk5FECo)
https://github.com/snowleopard/hadrian says hadrian is a shake-based build system.
As a substitute for GNU autotools in C projects? I don't actually write in C, though. It's just a fun consideration at this point.
And Shake is written in Haskell. Others Scons is written in python, but core python (has minimal dependencies) which can be built quite easily when bringing up a distro. There's a reason configure is a separate entity from make.
This is something i tried to do at the beginning but could make the types work. I though about giving up and use postgresql-simple which uses strings as sql. I tried using selda because it stated theres prepared statements.
I stopped the installation after all the 8.x versions installed. However, I would suggest that they package the binaries for us to use... My .stack folder was a mess as well.
Short answer is that I found that the Haskell type system wasn't powerful enough for the things I wanted to do. As a simple example, there is currently no good way to do something like Einstein notation for multilinear algebra in Haskell, and there won't be until dependent types come. And even once dependent types come, they will probably be far too awkward for actual use.
I'm just curious, did you try to achieve your goals in another language?
Do you know of any palatable alternative to gnu autotools other than cmake?
Yes indeed. Thank you!
Perhaps you mean "tooling for a very complex, rapidly-evolving language, maintained by a handful of volunteers in their spare time", truly sucks. Please return to GHC HQ with your receipt and you will be entitled to a full refund.
I want to contribute to dataHaskell but I'm confused how? So I'm trying to understand what it uses so that I can make sense of the code and contribute
There's `meson` but like `cmake` it's not really an autotools replacement. They're both build system generators. They generate files for use with a build system such as `make`, `ninja` etc. There's a lot of make alternatives, but I don't know many autoconf etc alternatives. It would help if you state what you actually want to do. If your project isn't very big then you likely don't need a lot of features and the defaults would be good for you.
Now that I think about this: `getConst . traverse (Const . f) = foldMap f`. So the `Const` and `traversable` become unnecessary for this example. Maybe there should be a more complicated example where there is really a need for the traversable.
I've not had particularly good experiences with HIE myself, but Intero has pretty much always worked for me without issue.
What about this ? https://www.cambridge.org/core/journals/journal-of-functional-programming/listing?q=educational+pearls&amp;_csrf=WlYZ2W1z-IkbmfXtuG7G21he5_RICoAQJsXI&amp;searchWithinIds=49AD4731AAB0E94D8EF98BBB4EE56A7F
There aren't many though: KRISHNAMURTHI, S. (2006). EDUCATIONAL PEARL: Automata via macros. Journal of Functional Programming, 16(3), 253-267. doi:10.1017/S0956796805005733 SARKAR, D., WADDELL, O., &amp; DYBVIG, R. (2005). EDUCATIONAL PEARL: A Nanopass framework for compiler education. Journal of Functional Programming, 15(5), 653-667. doi:10.1017/S0956796805005605 WAKELING, D. (2006). EDUCATIONAL PEARL: Biological sequence similarity. Journal of Functional Programming, 16(1), 1-12. doi:10.1017/S095679680500571X YI, K. (2006). EDUCATIONAL PEARL: ‘Proof-directed debugging’ revisited for a first-order version. Journal of Functional Programming, 16(6), 663-670. doi:10.1017/S0956796806006149
Thank you, I've copied your comment to the GitHub discussion: https://github.com/snowleopard/hadrian/issues/227#issuecomment-513544927 &gt; Without configure you have no way of knowing if Hadrian will even run, let alone if GHC will even run, given the various architectures and distros If I understand you correctly, the benefit of finding out that GHC won't run before, well, trying to just run the GHC, is a better error message that you could get from `configure`. Is that right? Or am I missing something deeper? If we limit our focus only to architectures/distros where GHC does work, which seems to be implied if one tries to replace `configure` with Shake, then presumably the goal is no longer impossible? Of course, I take your point about complexity etc. but as this thread demonstrates some people find GNU autotools more complex than Shake.
That's very simple. (+) :: Num a -&gt; a -&gt; a -&gt; a (+) Num {add} = add
I must admit, functional dependencies aren't really applicable here
A possibility is to hide the underlying type of `Set a` and instead offer this: newtype Set a = Ord a -&gt; Data.Set.Internal.Set a Then, union is of the following type: union :: Ord a -&gt; Set a -&gt; Set a -&gt; Set a This would, of course, require sets to be lazy.
Yes, but this does bring about new possibilities, such as parametric instances: functionEqInstance :: Foldable f -&gt; Functor f -&gt; Eq b -&gt; f a -&gt; Eq (a -&gt; b) functionEqInstance {foldr} {fmap} {(==)} xs = Eq {eq} where eq f g = fmap (\x -&gt; f x == f x) xs
If you're using [Nix](https://nixos.org/nix/) (which works on all distros and macOS in a non-conflicting way), you can use my https://github.com/infinisil/all-hies to get precompiled binaries for the GHC versions you need.
This would solve many problems, while, to my untrained eye, introducing none (except the use of `#`). My previous post, [Why type classes?](https://www.reddit.com/r/haskell/comments/cei4q1/why_type_classes/), would be easily solved. Additionally, this brings about the exciting possibility of *parametric type classes*, allowing (for example) `Eq` instances for functions. In terms of the paper: instance funEq (xs :: [a]) :: Eq b =&gt; Eq (a -&gt; b) where f == g = and $ fmap (\x -&gt; f x == g x) xs
Its hard for me to understand doing something like this. You've now bumped a discussion that was over more than three years, with *someone else's comments* taken from a thread that is a perfectly useful place to have the discussion. The comments support the conclusion in the issue, so what is the next thing that you expect to happen?
What exactly is fun about speculating in topics you have no experience in? "I don't want it" seems like an irrelevant assertion from someone who would never have cause to use it. Typically people looking for alternatives to a tool, have used that tool and understand why it is unfit for their purposes. You do not even have a purpose for using the tool.
Is it? I was unable to get this to work. {-# LANGUAGE NoImplicitPrelude #-} {-# LANGUAGE NamedFieldPuns #-} import qualified Prelude as P data Add a = Add {add :: a -&gt; a -&gt; a} addInts = Add ((P.+) :: P.Int -&gt; P.Int -&gt; P.Int) addFloats = Add ((P.+) :: P.Float -&gt; P.Float -&gt; P.Float) (+.) :: Add a -&gt; a -&gt; a -&gt; a (+.) Add{add} = add a = (5 :: P.Int) +. (5 :: P.Int) The error I get is /home/implicit/src/hs_add.hs:17:5: error: • Couldn't match expected type ‘Add P.Int’ with actual type ‘P.Int’ • In the first argument of ‘(+.)’, namely ‘five’ In the expression: five +. five In an equation for ‘a’: a = five +. five | 17 | a = five +. five | ^^^^
You need to type a = (+.) addInts (5 :: P.Int) (5 :: P.Int) Every instantiation here needs to be explicitly communicated.
Right. One of the primary motivations of typeclasses is very specifically to spare you this.
This is a useful and important work, thanks. I personally think we need more materials about design and design patterns in FP. Having these OOP patterns translated to the FP concepts is a good start, but in fact we have more our own, inherently functional, design pattern and approaches that have even high abstraction than OOP patterns had. This allows us to build applications in a pure FP languages with more guarantees, with less accidental complexity, with more clearness and so on. And I can't keep myself from leaving my work unmentioned here. I'm working on the book that is called "Functional Design and Architecture", and I'm researching the new approaches and patterns with building a comprehensive guide to software design in FP. There is a lot of materials about FP patterns scattered out there, and also I've discovered several new ones. Not saying that I used many of these approaches in the real world projects. I thought you might be interested in this, so here are the links: ["Functional Design and Architecture" book](https://graninas.com/functional-design-and-architecture-book/) [Links and references to materials about Design in FP](https://docs.google.com/spreadsheets/d/19nMC6zU0DBmX0JgiKecYziHO51TSOB1pgqvVbG0yf1Q/edit?usp=sharing)
Yes, and I questioned whether that's necessary.
Me too, is purescript a viable language, does it have the type system that haskell doesn't
&gt; a fairly vanilla monadic API Vanilla 🤨🤪 PS. I worked with much of this tech a few years ago and it was great fun
Always good to see companies hiring haskellers! Could you give a quick description of how Standard Chartered gives back to the Haskell community? I'm sure it's doing so (in ways besides employing a bunch of haskellers, which is a big help in itself), but I'm not sure what they are.
I'm pretty sure this is not comprehensive. For example
&gt; It would help if you state what you actually want to do. I just wanted to know whether there are better alternatives to autotools for building C projects. Is cmake alone not enough for many C projects?
&gt; what is the next thing that you expect to happen? I expect current and future participants/readers of both threads (including the future me!) to benefit from seeing relevant and interesting arguments, such as the one by /u/Phyx. In case of doubt: I don't expect that anyone would attempt to reimplement GHC's `configure` in Shake any time soon.
I suppose you could do that, but it would mean that you would be giving up infix syntax for almost every common binary operator. Most programmers and even most academics seem not to think that this is a good place to end up. Rather than live in the world you advocate, the designers of of ocaml opted for == to have type 'a -&gt; 'a -&gt; 'a and to perform a runtime check on the types of the arguments.
I've never heard of that language before! Can you add a link to a reference or something like that to your README?
Is there any validation/convertion library built around type like `newtype V e i o = V (i -&gt; Either e o)`?. I'm couldn't find any so I wrote some for myself but I'm pretty sure there is something similar out there.
Assuming a vanilla monadic API is as good as vanilla ice cream it does sound quite tasty ;)
Not exactly answer to your question, but one of the fundamental pieces in the [tomland](https://github.com/kowainik/tomland) library is a similar type with the conversion in both directions: data BiMap e a b = BiMap { forward :: a -&gt; Either e b , backward :: b -&gt; Either e a }
Sure. It's a data mashup language (DSL) by Microsoft, that powers several services including PowerBI connections to data sources. It's also known as "M Language". https://docs.microsoft.com/en-us/powerquery-m/power-query-m-reference
Both `cmake` and `meson` are (IMO) better alternatives to Autotools. I prefer `meson` since it looks simpler and cleaner to me. What validated `meson` for me as viable general replacement of autotools is that systemd migrated from autotools to meson. Regarding CMake and Meson just being build system generators, so is Autotools. It's purpose is to generate a portable "configure" shell script whose job is to ultimately generate a Makefile if some requirements are met (dependencies and other checks). All this is done by both CMake and Meson as well; both have the "configure" and "build" phase. Perhaps the only difference in terms of functionality is that with Autotools, once you generate the `configure` script (and distribute it with your sources), the only "build system" requirement is Make (i.e. you don't need Autotools anymore) where as with CMake/Meson these need to be installed in to be able to build. In the general case I don't think this would be an issue since CMake/Meson are supported/available in the major platforms and support cross-compilation.
Not unusual or unexpected in my experience. When I've tried installing haskell-ide-engine in the past (about a year ago, most recently), it took roughly 40 mins (not sure the exact times, but it was in that ballpark; might have been longer) to install each version. I thought it would be a good idea to install all the available versions incase I wanted/needed to have support for different versions of GHC, and it did take many hours to finish, during which it was not really practical to do anything else on that machine. And it did end up taking up a couple gigs of space on disk as well. I do think that hie is a nice tool, and a very impressive body of work. But I have been frustrated by the experience of using it in the past, in part because of the bad experience of installing it. Nowadays I have given up on setting up an IDE-like environment for Haskell, as it seems impractical at the present time - instead I just use more lightweight tools like ghci and ghcid and accept the limitations that comes with.
The same behaviour exists in OCamland does not issue warnings. Frankly I find this a serious misfeature of OCaml. Admittedly, there is syntax to explicitly indicate polymorphism, but coming from a Haskell, polymorphism-by-default background, it irks me to need to spell it out all the time.
That is right, but I prefer to not use `unsafeCoerce` to prove anything. In my case the property is obvious, but what if the property is more complex? When you create proof as it is described in the article compiler would check your proof and say if something is wrong with the property.
&gt; The official posting is https://www.linkedin.com/jobs/view/1306121598/ That link shows a two-month old position that says applications are no longer being accepted.
Excellent. I really like the structure of the way this written up; you don't assume the reader knows what a Monad Transformer/Monad is, and then you show why it is useful, how it used in your project, what you are trying to solve and wrap everything up. I'm really enjoying the range of input from people all levels of experience in Haskell, and write-ups like yours are super-valuable. Thanks :)
Where I should keep my package if I create my own package so that other project can use my own package. I'm thinking I keep the package on Github or Bitbucket so that other host or mv can use the same package without any modification on .cabal file.
I think you could get a lot of haskellers behind your goal, and eventually come up with wonderful things!
No, I've moved on to more standard machine learning research because that's what I needed to do to get a tenure track job. My sense is that there's no languages that are ready for this, but that Haskell or Idris would be the closest. Julia's making a lot of great strides in this area, and since it's the language I'd be my money on right now. But Julia's type system is still no where near as good as Haskell's.
Github or Bitbucket should work just fine. Stack has syntax to have packages in git repositories be used as `extra-deps` (in the stack.yaml) by any other library.
Stack supports referring to other packages on the same filesystem, so you don't even have to host them on GitHub. You can do: packages: - . - /path/to/local/package
Please help me understand what I'm doing wrong with these STArrays!! I've been at it for hours, I am getting a type error which I think I understand but it seems out of context. It's like my index i is thought to be a different incompatible type (i vs i2) in my do binding 😵. What I'm trying to do is implement a DFS backed Dijkstra's shortest path algorithm using mutable arrays just for practice working with arrays. Problems at line 65 - 66 [here](https://pastebin.com/3fZBiHhN)
I concur with the other commenters: this really is a great tutorial on how design patterns can be applied in real-world code. I was particularly impressed by the `Has` typeclass: I saw the original technique of making lots of `HasField1`,`HasField2`,… classes, with one for each field, but never thought of giving it two parameters to reduce that boilerplate. A question: in several code samples you refer to three typeclasses `WithError`, `WithLog`, `WithDb`, but you don’t actually define them anywhere. How are these classes defined?
&gt; personally I find Shake/Hadrian to be more complex than any other build system I've ever used because of this type safety thing. It forced me to jump through hoops to do even the most simple stuff How so? I've had some frustrations depending on build flags, for instance, but I am curious what precisely you found frustrating.
I'll give you a concrete example. I wanted to do something very simple. I wanted to `||` the value of two optional arguments together. To my surprise however the result of the `args` function is an `Action Bool`. Ok shouldn't be a problem, but then `Action` isn't an instance of `Applicative`. Ok sure. Then I grep around trying to find other examples where this is done. No luck, then I try to find any documentation on this `Action` monad. Nothing, then I try to find a wiki or something shortly describing this. Also fail. Then I find a PhD thesis, but all I want to do is fix a bootstrap failure. I don't want to read a PhD thesis to know how to combine two options. I mean I could get it work, I just couldn't find out how it should work and why. In the end I just duplicate the line and change the value it checks for. I appreciate that documentation will get better over time, but at this time I find it would take a longer time for me to get comfortable with it than it took with `autotools`. I also couldn't test it because it the local build kept failing for me and I don't know why. It just does every so often. I have large changes pending to the Make build system, but I don't even know where to get started with Hadrian.
&gt; If I understand you correctly, the benefit of finding out that GHC won't run before, well, trying to just run the GHC, is a better error message that you could get from configure. Is that right? Or am I missing something deeper? That's part of it. But with configure we catch things such as "oh you're using version X of library A but Version Y of library B, X and Y are not compatible" whereas by just running the things that use A and B you may or may not get a failure, but if it does it'll be a cryptic one. As a concrete example, you can detect things like the default flags the C compiler for that distro have been compiled for, to make sure you know if the C compiler is usable for your case. Where again if you call ghc all you'll get is a cryptic error. You also have cases where the header files are not usable or available. Configure would tell you which ones you can use in order to compile Hadrian at all (and it's dependencies). &gt; If we limit our focus only to architectures/distros where GHC does work, which seems to be implied if one tries to replace configure with Shake, then presumably the goal is no longer impossible? Maybe, but that's a reasonable restriction to the project. If configure is replaced with Shake and only runs on known platforms then it makes porting ghc much harder. Especially if Hadrian uses TH or the runtime linker in any capacity.
Use \`{-# LANGUAGE ScopedTypeVariables #-}\` and \`forall i w. (...) =&gt; ...\` for your type signature of dijkstra. The issue is the type variables you are using are not meaning the same thing as the type variables in the function's type signature. Those \`i\` type variables are different.
But they aren't! That's why I'm so confused, they should be the same the i produced is of the same type the i that was constrained in the signature.
MAG &amp; Strats are embedded in this dinosaur of a huge old-school company, so it is easier to get it to throw money around than code. So Standard Chartered sponsors conferences like ICFP and IFL, and summer schools in Utrecht and the UK, but you will not see much open source stuff coming out.
&gt;test :: (Ix i, Num i) =&gt; Array i e -&gt; i test arr = let b = bounds arr :: (i, i) in fst b + snd b &amp;#x200B; Yep, we're talking about the same problem but you didn't understand my explanation - and that's ok, it isn't a simple thing to understand. In Haskell when you say `f :: Num a =&gt; a ; f = 1 :: a` those `a` type variables are not the same. This means the same as `f :: Num a =&gt; a ; f = 1 :: b` why? Because... reasons. If you want the `a` in the function declaration type signature to match the `a` in extra signatures then you must use scoped type variables and `forall` such as: ``` {-# LANGUAGE ScopedTypeVariables #-} import Data.Array test :: forall i e. (Ix i, Num i) =&gt; Array i e -&gt; i test arr = let b = bounds arr :: (i, i) in fst b + snd b ```
I urgently want to know how long is "soon" :D
Thanks a million! I had a fundemental misunderstanding about how the type system works, this was the nudge in the right direction I needed!!
Likewise. Here I'm working on a whole bunch of infrastructure around a language project, and thinking that I could integrate all of this fairly easily now, but not so easily in 6 months.
Great question. I forgot to fix this in the post, but \`WithError\`, \`WithLog\`, and \`WithDb\` are constraints defined like so: \`\`\` type **WithDb** env m = (MonadReader env m, Has DbPool env, MonadIO m) type **WithError** m = (MonadError AppError m, HasCallStack) type **WithLog** env msg m = (MonadReader env m, HasLog env msg m, HasCallStack) \`\`\` A type synonym is used to combine multiple constraints. This helps with readability. For example: \`\`\` prepareDb :: (MonadReader env m, Has DbPool env, MonadIO m) =&gt; m () prepareDb = teardownDb &gt;&gt; setupDb \`\`\` can be written as: \`\`\` prepareDb :: WithDb env m =&gt; m () prepareDb = teardownDb &gt;&gt; setupDb \`\`\` All functions that interact with the database need to be able to: 1. read from the environment (\`MonadReader env m\`) 2. grab the \`DbPool\` (\` Has DbPool env\`) 3. and perform \`IO\` (\`MonadIO m\`) so we just roll these three constraints into one.
Thank you so much :) I'm glad you got something from it!
What book should i read ? I'm halfway into Category Theory by Bartosz Milewski (at part 2). Until i realize that i wasted time engaging in pure mathematics while what i actually want is learning monad. I think i've achieved that, but what book should i read so that what i've learned so far get application for it. It would be better if there are interesting problem to deal with. I don't mind mathematical maturity requirement.
Thank you :) I'll take a look.
Thanks /u/rashadg1030! This also gives me another question: how come `WithDb` requires a `MonadIO` constraint? And what does that constraint do to purity and testing, since it means you have to run it in `IO`?
And I worked with /u/ndmitchell a few years ago and THAT was great fun too.
So, TL;DL Glean is a new immutable fact database and its schema definition language. It's intended application is to be a foundation on which we can define abstract interfaces so that independent pieces of software that process source code can store and query data about the code and work together.
&gt;\-fno-ghci-sandbox This indeed runs. But it disables a bunch of the features I hoped to use, like breakpoints.
&gt; `Action` isn't an instance of `Applicative`. The docs says it is an instance of `Applicative` though: https://hackage.haskell.org/package/shake-0.18.3/docs/Development-Shake.html#t:Action
We need `MonadIO` in order to actually connect to the database. There's no way to connect to the database without `IO`. We have this function `withPool` that all of our database functions rely on: ``` -- | Perform action that needs database connection. withPool :: WithDb env m =&gt; (Sql.Connection -&gt; IO b) -&gt; m b withPool f = do pool &lt;- grab @DbPool liftIO $ Pool.withResource pool f {-# INLINE withPool #-} ``` As you can see, we use the `liftIO` method from `MonadIO` to "lift" the action from the `IO` context to the `m` context (whatever that may be. In our case it would be `App`.). Here's an example of how we use `withPool` in one of our `postgresql-simple` function wrappers. We're wrapping the `postgresql-simple` function `query_` which has the type `FromRow r =&gt; Connection -&gt; Query -&gt; IO [r]`. The result is in `IO` (as with all other `postgresql-simple` functions) which isn't what we want. We want to "lift" it to our context, hence the need for `MonadIO` in the definition of the `WithDb` constraint: ``` -- | Performs a query without arguments and returns the resulting rows. queryRaw :: forall r env m . (WithDb env m, FromRow r) =&gt; Sql.Query -&gt; m [r] queryRaw q = withPool $ \conn -&gt; query_ conn q {-# INLINE queryRaw #-} ``` To answer your second question, we test these functions like any other. We give the function a set of inputs and we test for expected outputs. There's really no way to avoid `IO` in this situation, and that's fine. Of course, you want to avoid `IO` as much as possible, but we have to connect with the database somehow. That's how I see it at least. My mentors or someone more experienced with testing in Haskell might have something better to say about this. Our programs have to interact with the outside world, but the cool thing about Haskell is that it is explicit. If something crazy happens or I get some unexpected behavior, I know exactly where to look. We have an entire module for testing our database functions so check that out in case you're curious: https://github.com/kowainik/issue-wanted/blob/master/test/Test/Db.hs
Haven't had a chance to watch yet, but based on the title, thumbnail, and comments here: this sounds rather similar to what Rust is doing too? [Responsive compilers - Nicholas Matsakis - PLISS 2019 (1:27:40)](https://www.youtube.com/watch?v=N6b44kMS6OM) ([Slides](https://nikomatsakis.github.io/pliss-2019/responsive-compilers.html#1) / [Source code](https://github.com/salsa-rs/salsa))
Is this particularly related to Haskell in any interesting way?
Their search system doesn't support exact match (i.e. "Education Pearls") but it is better than nothing. Thank you anyway. Let me see if I can scrap and filter the result.
Simon Marlow is one of principal authors of GHC. Both of his collaborators named on the title slide are also well known within the Haskell community. More directly, somewhere around the mid-point of the video he switches to showcasing a couple of examples of Glean in Haskell.
How would that interact with local or temporary facts? Language servers can receive incremental updates for unsaved files and it makes a huge difference for time sensitive tasks like completion. Could glean add a layer for temporary facts or is working offline the price you have to pay for scaling to huge codebases? Afaik all language servers have to reimplement caching and incremental parsing from scratch. If glean could handle that part it could be significant boon. But refactoring still requires deserialization of the schema which arguably requires too much detail like whitespace to be stored. Alternatively refactoring could produce an opinionated format, I guess? But the fact base isn't meant to be authoritive so I am not sure if you could build an online pretty printer off it.
That’s really helpful, thanks! I don’t know much about these areas of programming, so it’s really helpful to get some examples of how larger applications handle these problems.
I use stack mostly on windows or when I need something that will easily install ghc for me. I use cabal when I want to use backpack at all, or need to easily flip between lots of configurations and don't want 50 working directories Unfortunately, cabal 3 with its multiple library support is likely to become critical to my workflow very soon, as I need a bunch of little backpacked libraries to keep my sanity, so I'm losing the ability to call out to stack for anything very rapidly. =(
Glean is written in a combination of Haskell and C++ (mostly Haskell). I somehow forgot to mention that :)
Oh yes, I've seen a demo of it and thought about implementing it in haskell... for a while (=
I install GHC with `stack` and then add the path to `cabal.project.local`: ``` with-compiler: /Users/int-index/.stack/programs/x86_64-osx/ghc-8.4.4/bin/ghc ``` Then it's just `cabal v2-build` from there.
What advantages does Glean have over OWL schemas for code plus storage in a triplestore? Doesn't the w3c stack effectively do datalog?
So far Glean looks to be a pretty standard datalog behind the scenes, almost identical to the design of [logicblox](https://developer.logicblox.com/). The main advantage I can see is the backing store serialization is type aware, and it has some things like array(int) types that are not in the usual datalog vocabulary. One one hand, triple stores tend to be incredibly verbose in storage usage, on the other, sparql, and other rdf query languages exist. At the end of the segment he shows an example of a supplemental index that could either index off of `file` to `variable name` or `file` to `array(variable name)`. You'd lose that degree of freedom with an rdf stiore, which of course RDF ontology developers would try to pass off as a feature rather than a bug. YMMV. On the flip side, logicblox and bddbddb offer largely complementary views of how to handle datalog, based on row-at-a-time and relation-at-a-time models respectively and datalog already exists, too.
This idea of building a database of facts about your code and then running datalog queries over it feels quite reminiscent of Semmle. I’d love to see a comparison!
Why not both? 🙂
Sorry for delay. Just checked: `~/.stack` is 8.5G and `.stack-work` in hie folder is 1.2G
I think it's a great idea to help people coming from an OOP background to see how to translate common idioms from OOP to FP. However, I think your document analyses the problem in mechanical way and sometimes ignore the motivation behind the use of a pattern : what problem are they actually trying to solve and the way patterns actually work together ( you could have for example a singleton of strategy factory ...) The GoF patterns have been mainly designed for C++, to work around C++ weaknesses and translate them into things where C++ is good at : objects, so every pattern end-up being an object or a way of structuring object, even when an object might not be needed at all. Let's look at the strategy pattern. Ok, you replace a strategy by a function and you could do it in C++, and that's what most people would do before being aware of the strategy pattern. So what strategy gives you in C++ that a plain functions don't ? Is it really a problem in FP etc ? One of the main reason for strategy, is even though you can have function pointer in C++, at the time of the GoF book, you didn't have closure. You could have as in your example a `square` , `double` functions . It's easy enough in C++ to pass around a function which take and return a double. What you can't do easily is for example partial application or function composition. You can't (or couldn't in 90s) take `square`, `double` and return a new function composing them : you'll have to have an object. In the same way, given an `add` function, you can't create a `+1` function : you need a closure for that. Object and what is called Functor in OOP is the way to way to do a closure (BTW you can wrap a function into an object but not the either around, therefore it's better in your interface to ask for an object than a function). Given that partial application and function application is easy in Haskell, then you can conclude that there is no need for the strategy pattern : just pass a function. You could also argue that passing a strategy function and using it is actuall a form of continuation which is indeed a FP pattern. (I still don't see what Functor have to do with Strategy). However using a Strategy object vs a function as other benefit in a OOP world. As it's an first class object, you can combine with al the other pattern. You can have builder or factory to chose/create the correct strategy. You might realize that your strategy is actually a composite, or use the decorator pattern to decorate the strategy etc ... Also, one the advantage of a Strategy object is inheritance. Let's say you need a strategy to render a table. you can split the render method into `renderHeader`, `renderBody`, `renderFooter`. You can then, if all of those method are virtual, create a subclass which redefine `render` to swap the header and the footer (it might be a Decorator), or just redefine a new Header etc ... All of this mechanism of partially overriding some method, is actually not that straight forward in Haskell. Another example is the singleton. Singleton has nothing to do with expensive computation but just to make sure you only have one instance of an object, often because it uses (and acquires) a resource. A good example is a log manager or sound device etc. You only want one logger, because you want to be sure that all log message go to the same logger. You don't want some log to go to one file and some other to go to another because different bits of code don't use the same logger. It is solved in Haskell either by using a Reader or the unsafeIO global referecn trick. In C++ it's just working around the fact that a static object might not work because you need read a configuration file before creating for example. Last example, the Visitor pattern is basically a clever way of doing multiple dispatching using C++ inheritance mechanism. In Haskell it can be achieve using multi parameter typeclass.
I did that for a while. Now I use `ghcup` for managing ghc itself and I'm equally happy with that.
I'm well aware of Simon's involvement in GHC. That doesn't mean Haskell is involved in every project he touches, although it might be likely. Fortunately he's confirmed that it is!
Thanks for pointing this out. I can't wait to see the code when this is open sourced!
His involvement at FB seems to have been exclusively with Haskell. I think he went to FB to popularize Haskell in production. I think it's safe to say that if Simon Marlow has posted a link to r/haskell, then it's related to Haskell in interesting ways.
Except it wasn't Simon to post the link. Not that this matters, since the author of the post doesn't really say much on \*how\* Haskell is involved.
My mistake. But clearly, it's a talk by Simon Marlow.
Be aware that you can use (:) to pattern match also, ``` splitAndPutBackTogether (h:t) = (h:t) ```
So, there would be a haskell-based language to prevent AI-pocalypse?
I use Power BI all the time at my day job, and I enjoyed reading through your Github code for parsing M. The 8 `NULL` character block that Microsoft prefixed the `DataMashup` file to obscure that it's a ZIP was particularly funny. Have you seen the `DataModelSchema` file that `.PBIT` (Power BI template) files contain? It seems to encode all the DAX and data model parts in a JSON format. Sometimes, it has `NULL` characters interspersed between the legitimate characters. Microsoft's obfuscation efforts are so lazy...
It's not intended to be obfuscation; it's the length of the following data block.
With DTC (Data Types A La Carte) we can compose an algebra using a free monad and evaluate/interpret said algebra in some semantic context using fold. What are you trying to unfold?
I'll attempt to summarize my project in this post lol. This work is from my master's thesis, which i hope to intend to get into publishable form sometime soon. &amp;#x200B; I have managed to demonstrate the potential for neural networks to be modelled by recursion schemes (more specifically, forward propagation as catamorphisms and back propagation anamorphisms). I wanted to then extend this to have some free monadic API on top so that I could construct neural networks without all the ugly nesting that occurs due to 'Fix f'. &amp;#x200B; I've managed to re-implement forward propagation (a catamorphism) as a fold over free monads; this is essentially evaluating a neural net with an input to get an output. However, I'm unable to model back propagation (an anamorphism) with this new free monad environment, however. &amp;#x200B; Sorry if this information was too vague, please shoot any questions you might have!
Now I've watched this: * There's a lot of conceptual overlap here - Glean is multi-language, where the above is just Rust (that said, there's no reason it couldn't be extended to other languages) * The focus is different in each - Glean focusses on batch updates and arbitrary queries, where Salsa focusses on streaming updates with pre-defined queries * Salsa focusses on incremental computation - you define a schema, and a bunch of queries, then incrementally compute (or recompute) facts/query results based on some input * Glean focusses on arbitrary queries - you define a schema and then compute facts based on some input The queries in Salsa seem to overlap with the derived facts which get indexed in Glean.
I've been really happy with `alias stack-shell=stack exec --no-ghc-package-path zsh` and then running $ stack-shell $ cabal new-build It pulls the right GHC and Cabal versions for the stack resolvers, or it uses the global resolver if nothing is present, *or* I can specify `stack-shell --resolver lts-9` or `stack-shell --resolver ghc-8.0.1` to pick a specific GHC that a given project might need. Super convenient, flexible, etc way to use both tools for developing tools/libraries.
I started out with stack. I think it was because I had it installed from trying hakyll. I didn't know what I was doing. Now I know a little more and recently I stack stopped showing errors. I would have a type Error and it would just say there is an error on line X but not say what the error was. I saw this an excuse to give cabal a try. I converted the project I am learning with to cabal, got a zlib error from cabal so I installed cabal2nix and been using that for a bit now and it is okay. With the added advantage that I am learning both Elm and Haskell at the same time so I can use nix to build both now. It isn't quiet there yet but I think it is just cause I don't know how to write my own nix expressions yet. So basically for me "Cabal, Or Stack?" I say nix. caveat: I did spend about 3 months trying to understand that there is nix, nixos and nixops.
It means I have to create package with my module. I'm looking at the tutorial how to create package with Cabal. It seems to me I need to install all the dependencies that my own module needs with "Cabal install package_name" I use stack because I try to avoid use "Cabal install ..." on my system. My Questions: how to create a package with Stack? or is possible to create package with Stack?.
Sorry, I don't understand.
Hi, that would be great! I'm around in central London near BT tower tomorrow (23rd) but I guess that would be too short notice. Will be back from holiday mid-August and could meet then. Thanks very much!
I'm referring to "the 8 null character block" which isn't actually all nulls. Okay, it looks like I misremembered: the first four bytes are a version number and then the next four bytes are the length of the subsequent data. This is actually documented for Excel for compliance reasons. See https://docs.microsoft.com/en-us/openspecs/office_file_formats/ms-qdeff/22557f6d-7c29-4554-8fe4-7b7a54ac7a2b .
Thanks for the explanation and link. Is there documentation on the rest of the internal structure of PBIX and PBIT files?
Is it safe to assume that you want to express backpropagation (gradient descent) as a coalgebra?
This is for the new buster release, right?
Doesn't retain coherence, which is required for the current implementations of `Set` and `HashMap`, and seems necessary for a fast `unions` implementation. --- It's almost like you didn't read any one of the top 3 replies to your last post. but, I *know* you did since you even replied to one. --- Losing coherence is not being considered for Haskell Prime.
&gt; opted for == to have type 'a -&gt; 'a -&gt; 'a I'm not an OCaML expert at all. But, I think you meant `'a -&gt; 'b -&gt; Bool`.
There is not; only on the part that overlaps with Excel. The documentation requirements for Excel are different than that of Power BI.
whoops! Correct.
FWIW, Nix can be installed on any distro (and macOS) and it allows you to use my prebuilt binaries from https://github.com/infinisil/all-hies. I should probably mention this in the Readme.
OK, sorry, if that was mentioned, I seem to have skipped it while reading. Regarding coherence, doesn't GHC have `IncoherentTypeclasses`? I know that isn't Haskell Prime, but AFAIK this hasn't been proposed in ghc-proposals. Regardless of that, and I apologize for being so ill-informed, what exactly is coherence? Since this proposal wouldn't affect instance derivation at all, that (my previous gut definition of coherence) isn't applicable. Or haven't I read the paper thoroughly enough?
That's correct!
You might be interested in [this GHC proposal](https://github.com/ghc-proposals/ghc-proposals/pull/218) aiming at similar target.
Coherence is roughly that only one instance exists -- or rather than all instances that do exist are indistinguishable ("coherent"). This is useful so `Set` / `HashMap` know that the `Ord` / `Hashable` they used to build an existing structure is "compatible" with the instance they are using to query or augment said structure. It also means the category where morphisms are entailments is "thin": there's only one meaningful way to get from `Ord Int` to `Eq [[Int]]` e.g.; `Ord Int :-&gt; Ord [Int] :-&gt; Ord [[Int]] :-&gt; Eq [[Int]]` gives "the same" instance as `Ord Int :-&gt; Eq Int -&gt; Eq [Int] -&gt; Eq [[Int]]`. This lets the compiler solve type class constraints by making arbitrary choices rather than relying on the some sort of input from the programmer, and is particularly important for how polymorphic recursion with constraints works (since we actually have to pick a way at compile time to build the dictionary used for the recursive call at runtime from the "current" dictionary.) --- Your previous suggestion about `newtype Set a = Ord_ a -&gt; Internal.Set a` has some merit, but it has problems around asymptotics for even something simple like `elem :: Ord a =&gt; a -&gt; Set a -&gt; Bool`; it stops being O(lg n) and starts being O(n). (Although, I'd welcome a refutation of this assertion, maybe there's some recursive/implict/lazy way to do things without spine strictness.)
Also worth mentioning that "everything you can do with lists, you can do with strings" is a double edged sword. It also means (barring extensions like \`OverlappingInstances\`) that everything you define for lists *must* work identically for strings. This means that if you define a pretty-printing function for lists that prints square brackets, you can't define a separate pretty-printing function that also works only on strings and prints nice double quotes. &amp;#x200B; In fact the \`Show\` class (which you should get to in this series soon) contains an extra method, \`showList :: \[a\] -&gt; ShowS\` which can be used to change the behavior of the \`Show\` instance for \`\[a\]\`. This method is hardly ever encountered so it's one of those things that seems magical when learning.
You may have to do some work to tease out unfold from a gradient descent. I don't believe the coalgebra side of things has been neatly packaged as DTC handles the algebra side.
That is the reason why I do not trust `cabal`. With `stack` basic things work just well. If things get more complicated, they can break unexpectedly (although that is not my normal experience and `stack` developers have always been responsive). But even installing `cabal` + `GHC` was a huge problem for me each time I attempted that. How am I supposed to trust that `cabal` won't drive me nuts, if I can't even install it easily? Although I do mostly write apps rather than libraries. But I've used `stack sdist --pvp-bounds` a few times and it worked well for me.
You still have to *write* the `newtype`, you just don't have to do the wrapping / unwrapping when a coersion is available.
Yes it is
ISTR Idris hackers specifically added good indentation-semsitive parsing to Trifecta. Given that language's sensitivity to indentation I would have expected them to have re-created those combinators with MegaParsec before they moved to it. It may or may not be broken out into an easy to import module, but it should be easier in MegaParsec than in Parsec.
Thanks, good to know it!
I thought as much. I was hoping that this thread post would allow others to give me some insight on what concepts I should be looking into.
Wouldn't `'a -&gt; 'a -&gt; bool` force the type checker to prove both arguments are the same type, obviating the need for a runtime check? You said there was a runtime check so I expected `'a -&gt; 'b -&gt; bool`.
OCaml does indeed require the arguments to have exactly the same type. The problem is that the type checker will allow you to compare *anything*. Even things for which no sane equality test can be written, like functions. If you try to compare two functions with ==, the compile succeeds, but the program crashes at runtime.
Well, there is this: http://www.cs.ru.nl/B.Jacobs/CLG/JacobsCoalgebraIntro.pdf.
I'm a bit rusty on my OCaml too, so bear with me here. :) First, a bit of a correction on my part: `==` is the physical comparison operator in OCaml. It has type `'a -&gt; 'a -&gt; bool`. It will tell you whether two values are in fact the same value. let a = [5];; let b = [5];; a == b;; (* yields false *) `=` is the value equality operator, and it also has the signature `'a -&gt; 'a -&gt; bool`. This is actually the scary one. It is type-correct to apply `=` to *any two values of the same type*, even if no sane value comparison can be performed between them: let incr = fun x -&gt; x + 1;; let decr = fun x -&gt; x - 1;; incr = decr;; (* runtime exception! *)
I'm plenty fine with (and in fact, probably should, since it's in line with the project goals) writing some combinators myself. I'll take a look at Idris's parser, thanks!
That's too bad... There's a great opportunity here to improve Power BI's version control story, if something like a Github hook could be used to pierce the PBIX veil and look at the substantive changes (m queries, DAX measures, data model relationships) between versions.
Thanks, that clarifies things for me.
I know they do some sort of indentation sensitivity, since `mutual` blocks are bounded based on indentation, and function comments have to align with the function that follows them. But, yeah, maybe they are doing their own hacky thing there.
&gt; systemd migrated from autotools to meson. I migrated from systemd to openrc because it often took many minutes for systemd to shut down a system. But, systemd is a big project. Being used in a big project is certainly a positive evidence.
Haskell Article I'm going to need more information details on Haskell Computer Programming. Where do I look.
Stack uses Cabal under the covers. Also Cabal's new-* commands work very well.
You May be interested in `composing and decomposing datatypes`, which goes into more recursion schemes- like content such as unfolding, building upon DTALC
As far as I know, WPF can’t be used with Haskell. Since WPF is a .NET-specific library, you have to use some sort of interoperation between Haskell and the CLR to access it. There’s really two options for that: 1. Create the GUI in C# or another CLR language, then write the implementation behind the GUI in Haskell, then call the Haskell code from C# by writing a C library which uses the Haskell code and can be used in C#. 2. Use a library to access CLR methods from within Haskell. Options include [`clr-haskell`](https://gitlab.com/tim-m89/clr-haskell/tree/master) and [Salsa](https://github.com/tim-m89/Salsa). Warning: I have tried both of these libraries, and have gotten neither to work. In general, if you want to create a GUI in Haskell, there are *much* better ways to do it than using WPF. I wrote a good summary [here](https://www.reddit.com/r/haskell/comments/bnoxy5/what_the_better_library_for_create_guis/en9esrb/?context=3); personally I use GTK, as do most Haskellers, but you may differ.
It looks like no-one has mentioned this yet, but megaparsec has combinators specifically for indentation. It’s also pretty similar to parsec, so it should be fairly easy to move from one to the other. &amp;#x200B; &gt;Is there a standard way to ask watchers what they want to see / if they have thoughts on how something should be done? I don’t believe there’s any _standard_ way as such. But adding an issue could work well: all watchers can see it and respond.
Stack It. Gitter Done!!! Leave the rest of Operations options to Experts!!! WiltelReddit AAS WTI MICHIGAN
It doesn't actually look that hacky. It seems like a very solid foundation for indentation-sensitive combinators. I just don't see the one thing - I have to imagine that Idris can parse this (excuse me for not knowing much Idris): ```Idris foo : a -&gt; a foo a = a ```
&gt; has combinators specifically for indentation This is why I was looking at it, I'm just not sure how powerful they are and if it's worth the transition. Thankfully I'm not too far into this part of the project, so this is the time to transition!
WFM % idris --check test.idr % cat test.idr test : a -&gt; a test x = x % idris --version 1.3.1
Will the release contain [HIE Files feature](https://www.haskell.org/ghc/blog/20190626-HIEFiles.html)? The blog posts says so, but there is no reference to it in the release post.
It seems that GHC itself uses some clever trick to handle indentation with `Alex` and `Happy`: * https://gitlab.haskell.org/ghc/ghc/wikis/commentary/compiler/parser#indentation Though documentation is not that detailed and I would like to know more how mature projects handle layout-sensitive languages. In [tomland](https://github.com/kowainik/tomland) we use `megaparsec` but the current implementation doesn't handle layout properly. Parser should accept `foo = 3` but not `foo = \n 3` or `foo = 3 bar = true`. Here is one of our core parsers: -- | Space and comment consumer. Currently also consumes newlines. sc :: Parser () sc = L.space space1 lineComment blockComment where lineComment = skipLineComment "#" blockComment = empty It looks like if you want to handle line-breaks and indentation manually, you need to do a lot of tedious and manual work... So I would love to know what people do usually.
... I understand what GHC is doing now. Last time I read that wiki page was before reading the algorithm in the (98) Report. The relevant section is section 9.3. The 10th pattern for `L` says to behave differently if the next token causes a parse error. Initially I was trying to implement that between lexing and parsing and gave up because of that. But I guess GHC is doing this layout insensitivity transformation _online with the parsing_. That actually sounds approachable. I suppose the correct way to do this with `MegaParsec` is to make whatever the equivalent of `satisfy` is for working with custom token streams also perform this transformation as it goes. For the 10th pattern, there should be a way to catch the parse error and backtrack, then push a `}` and go again. I think I may switch to `MegaParsec` anyway for the purpose of extensible error messages (and it also just seems nicer to use, but I'll find out :) ). I'll play with this approach tomorrow!
The project has a number of similarities with Google's Kythe. Was this considered, and if so, what made you decide to write Glean instead?
I am currently working a on a number of large projects which do in fact build with both stack and cabal. However to be honest maintaining that was such a huge pain in the neck that I had to build a tool to help maintain this. Having this community split on two build tools continues to be a huge pain in the neck.
Just a short question because I'm not familiar well with nix. If I install just hie binaries from the cachix then I wont find a lot of extra cost because of the nix ? I think so because the hie build all dependecies inside to itself ... Do I think it well ? Sometimes it is interesting for me because I have a 256 Gb ssd in my laptop with virtual images and I found that the nix directory can grow up to 10+ Gb easily.
I use megaparsec for most of my parsing needs, so can confirm it’s a pretty good library. I’ve never used the indentation combinators, but judging from the rest of the library they’re probably fairly powerful. &gt; their documentation/tutorial doesn't mix offline lexing with their indentation combinators What’s ‘offline lexing’? I’ve never encountered that term before. &gt; The indentation combinators seem at least partially dependent on a parser that consumes whitespace. I would say that this makes sense — you need a whitespace-consuming parser so it knows which part of the line to interpret as indentation. More broadly, the whole of megaparsec is designed around whitespace: the idea is that a parser for a lexeme will also consume the whitespace immediately following it, so the next parser doesn’t have to worry about whitespace at the beginning. All these things are detailed in the [tutorials](https://markkarpov.com/learn-haskell.html#megaparsec-tutorials).
For now, I suggest using [Visual Studio Code](https://code.visualstudio.com/) and setup Haskell related extensions like syntax highlighter, linter, [Haskell Language Server](https://marketplace.visualstudio.com/items?itemName=alanz.vscode-hie-server&amp;source=post_page---------------------------) and debugger. You can get help from [this article](https://medium.com/@dogwith1eye/setting-up-haskell-in-vs-code-on-macos-d2cc1ce9f60a).
&gt; if you define a pretty-printing function for lists that prints square brackets, you can't define a separate pretty-printing function that also works only on strings and prints nice double quotes. Well, of course you *can* define a separate pretty-printing function that only works on strings. What you can't do is have your general pretty-printer treat strings specially.
Thanks for the heads-up, we're contacting HR to bump it back to live.
does SC also sponsor airfare ticket? I got accepted last year in Utrecht summer school but my airfare funding canceled by one of my sponsor (which does not use Haskell at all). I thought if it came from a company who really use Haskell in their production, the chance not to be canceled is small.
&gt; If I install just hie binaries from the cachix then I wont find a lot of extra cost because of the nix ? Yes indeed. Nix only needs space for exactly what you tell it to install, there's no additional cost. However Nix does keep a list of past generations of your installs, so if you install some package, then uninstall it, it will still be there, just not installed anymore. This allows you to roll back the change easily, which is very useful when something got messed up, but it can take up more space. If you want to delete old generations, therefore reclaiming the space they're using, you should call e.g. `nix-collect-garbage --delete-older-than 3d` to delete generations that are older than 3 days.
Thanks ! I'll take a look to the nix system because it seems very convenient and safty :) What do you advice I need to use nixos or debian with nix is enough good ?
I have absolutely no idea.
It mentioned in release notes's "Runtime system" section
Nix is very convenient on its own, so using Nix on Debian will be a great first step :)
Tangent: This and similar "showcase" talks that are quite popular in Haskell leave me mixed feelings. Besides the barrage of type signatures which you can I guess unpack in your own time with some patience, the presenters constantly use expressions like "you can smuggle this in.."/"you can do weird stuff with this.."/"you can further generalize this..", OK but why _should_ you? I mean, there's certainly something in there about extensible validators/reusable deserializers etc. but what's the actual cognitive/refactoring/didactic/performance price for these abstractions? What led to them, in the sense of the evolution of ideas, and _under what circumstances_ should they be actually put in practice?
I'm working on a toy language that has a Haskell-like syntax to better understand compilers (and because it's a lot of fun!). So far the parser is mostly done, feel free to take a look and get some inspiration. Most interesting files are probably: - [https://github.com/luc-tielen/besra-lang/blob/master/lib/Besra/Parser/Expr.hs](https://github.com/luc-tielen/besra-lang/blob/master/lib/Besra/Parser/Expr.hs) (parser for my expression type) - [https://github.com/luc-tielen/besra-lang/blob/master/lib/Besra/Parser/Helpers.hs](https://github.com/luc-tielen/besra-lang/blob/master/lib/Besra/Parser/Helpers.hs) (helper combinators) (Note that I wrote some combinators of my own for more easily creating linefolds and dealing with aligned indents, force stuff to be on same line, etc...). I will probably revisit and polish up the parser a lot later since it can probably be refactored into something simpler, but first I want to dive deeper into the upcoming phases of the compiler :) (but will probably be a long time before I get there..) Hope this helps!
Too fast. I'm not sure why all those slides were presented if I haven't got the time to read them before the next slide is shown. It's good to be complete for lookup later but since I had to pause the video several times, I'm sure the live audience got a bit lost.
Why the post was edited to remove the tech stack? It was really an impressive one...
What's the tool you use to keep them in line? I try to keep the projects I maintain Cabal-buildable and support at least a few Stack LTS releases (though there's enough churn between major versions that even that stretches my sanity a little here and there), and I'd love to make it easier.
Disclaimer: opinions are my employer's and not necessarily my views.
I came away from this article wondering \_why\_ storing the multiplier as a phantom type is a good idea. Isn't it simpler and more natural to just use an abstract data type with a normalised representation? \`newtype Memory = Memory { getBits :: Int }\`, with a suitable collection of smart constructors (\`fromBytes, fromGigabytes, etc :: Int -&gt; Memory\`), seems to achieve all of the library's goals with much less machinery.