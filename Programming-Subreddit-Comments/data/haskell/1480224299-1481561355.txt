I notice `Rep` is a list type, suggesting aggregation of types of different kinds. It would be nice to have some kind of type mechanism to do that aggregation, but I don't think it's possible without type-level `(++)`: data Pair (a :: TYPE ra) (b :: TYPE rb) :: TYPE (ra ++ rb) pair :: forall (ra :: Rep) (a :: TYPE ra) (rb :: Rep) (b :: TYPE rb). a -&gt; b -&gt; Pair a b fstp :: forall (ra :: Rep) (a :: TYPE ra) (rb :: Rep) (b :: TYPE rb). Pair a b -&gt; a sndp :: forall (ra :: Rep) (a :: TYPE ra) (rb :: Rep) (b :: TYPE rb). Pair a b -&gt; b 
It's not hard to learn *per se*, if you would believe me. It's just because imperative programming dominates the world so you might learn about a few imperative languages like Java, Ruby, etc. , think that you can learn any one in a few days, only to discover that there's the Haskell thing that matches non of what you expect. It's like learning programming from scratch. It's like learning to write Chinese characters after sinking in latin letters.
&gt; It seems, to my untrained eye, that the withXXX set of APIs allow us to process IO in constant space and guarantee resource cleanup. Why are they not acceptable? The APIs you propose are not as composable. They solve the memory usage but let me deal with each bytestring chunk like a (functional) caveman. With conduit or pipe, if I have some source of `a`, and can plug any conduit (or pipe) that takes an `a` and produces a `b`, and that will produce a new source of `b`. All of the building blocks are swappable. For example, I recently worked on a tar parser for conduit. I can seamlessly plug [this conduit](https://hackage.haskell.org/package/conduit-extra-1.1.15/docs/Data-Conduit-Zlib.html#v:ungzip) to handle tar.gz files, and could use any source too (file on the disk, a ByteString in memory, network input ...).
[removed]
&gt; Take your mapFile and replace the FilePath arg with a Handle. Now this function is more usable. You can use it on a file or socket that's already open. Why would I do that? wouldn't that allow open file handles (or resources) to exist independently, thus bringing us back to the resource deallocation problem? I'm trying to work out why can't the resource manager be passed a resource consumption function, which is called as and when the resource is being consumed. doesn't this solve the streaming problem as well?
actually I'm back to my original question even in the context of streaming. why can't streaming be solved with the `withXXX` pattern?
Does anyone know *why* Apple made this (seemingly arbitrary) change in Sierra?
An offtopic question: &gt; Finally you make an API out of this and name it pipes or conduit or duct or funnels or whatever pun you like and package it up. Do you know if backpack can help with this "madness"? :-)
Would like to know this too, but haven't found anything. Some projects just broke after the update. You can always compile and run in a VM with little pain, so atleast you could continue developing.. Major pain if you need to run it on Mac too. Could this have an effect outside of the Haskell ecosystem too? 
 withStream :: MonadIO m =&gt; StreamIdentifier -&gt; (InputFromStream -&gt; m (StreamEvent)) -&gt; m () data StreamEvent = WaitForMore | CloseStream 
&gt; Notice how there's no way to write a function like next for your proposed abstraction because you have to process the entire stream in one go. How about https://www.reddit.com/r/haskell/comments/5f47br/trying_to_understand_what_pipesconduits_solve/dahoyba/ ? &gt; you have to materialize the entire set of decoded values in memory instead of streaming over them one-at-a-time If we have an list of `A` which is potentially very large, the question that I'll ask is, where is this list coming from? Through which interface to the external world? Shouldn't this problem of streaming be solved at that interface rather than complicating everything across the entire program?
Applicatives are not monoids in the category of endofunctors, they're monoids in the category of copresheaves. In the former the objects are functors C -&gt; C and the monoidal product is horizontal composition and in the latter objects are functors C -&gt; Set and the monoidal product is Day convolution. They just happen to have the same objects when C = Set.
Hmmm, what do you get for monoid objects with the other 2 monoidal products? I'd guess `(:*:)` gives `Alternative` and `(:+:)` gives something trivial?
Thanks! This is really useful.
I believe it also addresses the very important ld.gold and -fPIE issues.
Alternative has more structure than that, though, good luck getting folks to agree as to what it is. (:+:) is kind of a mess, but data Void a is such a monoid. (The identity object for a monoidal category gives you a trivial monoid object, seen easily through the laws for lambda, and rho.)
&gt; Unfortunately, GHC bugs prevented me from finishing any runner functions for effects. I could start a tumblr account from how much this triggers me. Having to stop exploring some complicated problem because of a compiler bug is IMO the most unpleasant experience in all of programming.
&gt; though, good luck getting folks to agree as to what it is. I personally really enjoyed the idea that [Alternative and MonadPlus are both the additive operators of near-semirings with Applicative and Monad respectively](https://lirias.kuleuven.be/bitstream/123456789/499951/1/main.pdf). I feel like that would be a good set of laws.
It was my fault after all, my apologies. Now it works. To my credit, I've reported real type checker bugs before. [Here's](http://lpaste.net/347242) the end result. I find it really amusing; here's some example code from the end of the file: -- We have a labelled indexed state and two labelled file handles -- The input indices are in the first line after the effects, outputs in the second. test :: Eff (State "x" ::: File "foo" ::: File "bar" ::: INil) (Int :&gt; FClosed :&gt; FOpen :&gt; Nil ) (String :&gt; FOpen :&gt; FClosed :&gt; Nil ) () test = do x &lt;- get -- no need for label here since State is unambiguous put $ x + 100 open @"foo" close @"bar" put "I have to put here some string" return ()
Nope, `const`'s type signature is more general. 
Of course it is. But the claim was that parametricity could be used to understand the implementation of these functions by only looking at the type signature. That `const` can be specialized to each of those types, and is not equal to the original functions, demonstrates that parametricity doesn't guarantee the implementation here. Rather, there are simpler implementations that also satisfy the type.
Can you give an example of how you expect this to be used? For a non-trivial example -- say, summing the squares of numbers from the input stream. I suspect that in writing such an example here, and comparing with code to solve the same problem with `pipes` or `conduit`, you will discover what problem `pipes` and `conduit` solve.
I'm going to try to hijack this thread to get an answer to [this question on indentation-aware parsing](https://stackoverflow.com/questions/37279101/indentation-aware-parsing-of-expression-trees) which Megaparsec [seems to support](https://hackage.haskell.org/package/megaparsec-5.1.2/docs/Text-Megaparsec-Lexer.html#v:indentBlock). For my original problem, I ended up postprocessing `haskell-src-ext`'s parse result instead of doing my own parsing... but I'm still hoping to learn the correct Megaparsec way. 
That's extremely disappointing. I presume the distributivity law is the culprit? I'm guessing the multiply by zero law holds in pretty much all instances.
Doesn't this fail if someone else who has a copy of `ref` and puts a `Right` something in it?
distributivity vs. left catch is the major issue, yeah.
`&lt;+&gt;` would be `&lt;&gt;`
RemindMe! 12 hours "update StackOverflow answer on levity polymorphism"
I will be messaging you on [**2016-11-28 20:14:12 UTC**](http://www.wolframalpha.com/input/?i=2016-11-28 20:14:12 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/5erk9u/levity_polymorphism_new_paper_from_richard_and/dairqls) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/5erk9u/levity_polymorphism_new_paper_from_richard_and/dairqls]%0A%0ARemindMe! 12 hours ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! dairr6b) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Me too ;)
In the code above, the input arrays have to be pre-sorted. If you told me that fusion might be used to interleave sorting and merging, thus improving the complexity class of the overall algorithm, _then_ I'd be much more motivated to use it.
I think that was his point - the result of that function is a list of process ids, and you have to collect the results manually and then assemble the final list, iirc (has been a few years since I used Erlang).
Yes, of course. Don't use `withLeft`, it's only an illustration :)
Thanks for your replies. And sorry for the very rigid request. I have little programming skills. Yes, I believe Faucelme understood my inquiry, I am trying to write a program that can stop and start a cycle. For example, if I press the key 'S' the cycle will begin and if I press 'S' again the program will stop. 
You should do it the same way you are already storing your `Scientific` values. A `Scientific` value doesn't know from what exponent it was created.
&gt; Jean-Christophe Mincke will present a Haskell library for doing distributed computing, inspired from Apache Spark. Is this a re-implementation of Apache Spark? Or is it a similar effort to the [*sparkle* library](https://github.com/tweag/sparkle), which uses Spark under the hood? Or, is this talk **about** Tweag IO's sparkle library?
That's what the `package` options are in the stack command there (turtle, markdown-unlit). Stack handles the installation of those packages for the given lts release. In this case I believe Stack will use the globally defined lts release (but you could just as easily pin the release in the stack invocation) and attempt to resolve dependent packages from that release.
If this is intended to be "best practices" demo then I think the invocation should pin down the lts version.
But it knows the internal exponent. So it should be possible to convert to a different exponent, somehow. `base10Exponent (fromFloatDigits 2.8987623864723)` -&gt; `-13`
The abstract makes it pretty clear: - It is implemented in simple Haskell. - It is based on five simple primitives. So it is neither a reimplementation of Spark (it is only inspired by it) and does *not* use Spark under the hood.
How would you know whether f is to be applied to the container type or the inner type?
Composition of functions is only a special case of categorical composition. There's a class called `Category` in base, defined like so: class Category (~&gt;) where id :: a ~&gt; a (.) :: (b ~&gt; c) -&gt; (a ~&gt; b) -&gt; (a ~&gt; c) Obviously, the function arrow `(-&gt;)` is an instance of `Category` with `id = \a -&gt; a` and `(.) = \f g x -&gt; f (g x)`. However, consider another instance, `Kleisli` arrows: newtype Kleisli m a b = Kleisli (a -&gt; m b) instance Functor m =&gt; Functor (Kleisli m a) where fmap f (Kleisli k) = Kleisli (\a -&gt; fmap f (k a)) instance Monad m =&gt; Category (Kleisli m) where id = Kleisli return Kleisli k . Kleisli t = Kleisli (k &lt;=&lt; t) Although `(.)` and `fmap` coincide for functions, they obviously differ for `Kleisli`.
Someone needs to mention Hayoo and Hoogle, which allow a person to enter a type signature, and return library functions which match that sig: - [http://hayoo.fh-wedel.de/](http://hayoo.fh-wedel.de/) - [https://www.haskell.org/hoogle/](https://www.haskell.org/hoogle/)
Nice! Another thing I like along the same lines is having literate Haskell documentation/tutorials included in the test suite so it never goes out of date. Here's an example: https://github.com/snapframework/heist/blob/master/test/suite/Heist/Tutorial/CompiledSplices.lhs and the generated docs are here: http://snapframework.com/docs/tutorials/compiled-splices
The efficiency here is not some "a bit better / a bit worse" thing as inlining is normally, it's about whether the GHC rewrite system can be programmed to perform the various transformations that are needed to make stream fusion work. In this case I've not tried this example and what happens with inlining things at different stages but here's what I'd guess. I'm assuming that all the stream functions systematically use `INLINE [1]` on the top level function and `INLINE [0]` on the stepper functions. This is important: you have to consider the rewrite strategy as a whole, not functions like this in isolation. We can have the top level of the functions inlined relatively early since it doesn't depend on anything having been done first, and all other things being equal it's better to inline early. Note that the top levels of these functions are all pretty simple. The complexity is all in the stepper functions. So the hope is that by the end of phase 1 we've got all fusible functions combined, each carrying around their original unoptimised step functions. At this stage the code size shouldn't have blown up too much which is handy if you're trying to read the intermediate core to debug this stuff. Then in phase 0 we want to optimise all the stepper functions by allowing them to be inlined into each other. &gt; Most importantly, how does one measure the effectiveness of this fine-grained inlining (apart from comparing runtimes, that is), and where do I have to look to understand how to optimize these things? Ultimately you can only tell by looking at the generated core code. For example make a definition in a standalone module like: foo merge f g h = map h (mergeStreamsWith merge (map f vec1) (map f vec2)) and then look at the generated core with -O2. Specifically you're looking for all the `Step` constructors to be eliminated and for it to look generally sane (you need to be able to grok core in general). Why the `map`s? That's to check it's fusing on its two fusible inputs and fusible result. &gt; * why and how the stream fusion magic works &gt; * what are the limitations of the "deforestation" approach; does it only apply to array-like data types, and in cases where the inputs are used exactly once? These and more related questions are answered in detail in my DPhil thesis http://code.haskell.org/~duncan/thesis.pdf
Ok but in the prelude, `.` is not defined for category but only for function composition. There is also already a operator for Kleisli composition `&gt;=&gt;` so we end up with 3 operators when only 2 are really needed.
But why? Just to avoid a different operator? Most of the time, when I compose functions, I am not thinking about functors at all. That tells me that making the only available operator to do so mean something about functors is not what I want. Particularly when you are working with abstractions where is common to get the details wrong, overloading too many things that are likely to be used side by side just makes errors easier to make. And even if the compiler catches them, the context lost because of the overloading obscures the cause.
What did they impact that made them important?
Those docs look neat! Any idea on how they were styled?
It's an internal representation thing. Scientific values are just an Integer and an Exponent, for example `2.8987623864723` is just `28987623864723` and `-13`. You'll have to make your own storing function. For example with some math: n*10^e = x*10^-2 &lt;=&gt; x = n*10^(e+2) So you could do: let n = fromFloatDigits 2.8987623864723 let (c, e) = (coefficient n, base10Exponent n) let x = fromInteger c * 10^^(e+2) and `x` will hold `289.87623864722997`, which is `2.8987623864723` base-10 `-2`.
But neither the F# nor C# want to avoid success at all costs, while the academic Haskell community does. C# and F# are not research languages and their research is applied, not theoretical. Don't misinterpret me. I consider the desire to avoid the "capture" of Haskell by industry (avoid success) as a logical consequence of the interests of the academic community. If I would be in that side, I would do so. Simply the academic community want to keep Haskell as the language for the purpose for which it was created. They are legitimated to do so. But we must make that motivation explicit. Not agreeing in that obvious divergence of interests benefit the current situation that is bad IMHO for the interests of the ones, like me, that want the success of Haskell in industry.
I'm using scala for android right now, and it's way better than java, but using a haskell-like language would be a dream. Android could be a great niche for frege
A monoid is, put simple, a triple consisting of a set, an operation, and an identity element. In Haskell however, we attach the information that says "this is a monoid" to the set (the type) alone, and proceed to define which operation and identity element go with it. So when we write `instance Monoid [a] where ...`, we are not saying "`[a]` is a monoid", but "there exists a monoid for `[a]`; its operation and identity are ...". And because `Monoid` is a typeclass, and each type can only have zero or one instances of a typeclass, this means we cannot model the situation where multiple monoids exist for the same set without introducing `newtype`s for disambiguation. This is unfortunate, but it does mean we can use monoidal operations (`&lt;&gt;`, `mconcat`, etc.) without explicitly specifying the monoid in question all the time, which is really really useful in practice.
Programming
(&gt;=&gt;) is there because it avoids newtyping the Kleisli arrow, it's a different case...
&gt;My answer is: - We have only single object in this category and it corresponds to type of category in this case it is type of the number (lets say Int). - Arrows corresponds to number instances (...,1, 2, 3, ...) - Id arrow corresponds to unit. For our case this is number 1. - Notion of composing two arrows corresponds to multiplication. That is perfectly right. &gt;But my answer makes me uneasy because arrows suppose to correspond to functions and object to types. That is only the case for some categories, including the very familiar example of **Hask** (the category with Haskell functions as arrows and Haskell types as objects, with `id` as identity and `(.)` as composition). The `Category` class is more general than your description (its instances need not be function types), and the mathematical concept of category is far more general than both. &gt;According to video, arrows corresponds to multiplying with certain element. so that means id element is (*1) . Not really. As you said before, the identity arrow is `1`. Applying `(*1)` to a number amounts to composing it with `1` in the category you are describing.
If human-readability isn't a property you need for the over-the-wire format, [protocol](https://developers.google.com/protocol-buffers/) [buffers](https://hackage.haskell.org/package/protobuf) were designed for this purpose.
It's just using the snapframework.com styles.
I think it would be easier to learn Haskell first and then follow-up with Python. That said, the important thing is learning your first "Pac-man complete" programming language. Once you get past the first one, the others are much easier to pick up. I originally learned TI-BASIC, followed by some other BASICs, then picked up C/C++ in college. Java, Effiel, C#, Perl, PHP, Ruby, Python, etc. I just picked up as needed/desired, with minimal effort. I was also exposed to, and wrote trivial programs in Prolog and Scheme in college. Haskell was interesting (and I think my Prolog and Scheme experience helped), but probably most enlightening. I still want to pick up Rust (Pijul HYPE!) and get a little more polished with Javascript. (I know enough about it to write GHCJS imports/exports, and I really think that should be enough.) Each language just makes the next one easier. Programming is about a way of organizing your thoughts, not about syntax, libraries, evaluation order, or even type systems; unless you find these useful in organizing your thoughts. Sometimes Perl is the best tool. Sometimes Idris is. PHP is almost never the right tool. But, if you don't have the tool in your toolbox you can't use it, and your works might suffer for it. Also, remember that your tools can make other tools, too. :) Good luck on being a programmer, whether you make a career or hobby out of it.
`(.)` means composition. Most of the time we use `(.)` to compose e.g. functions we do not think about what we are doing in terms of the `(-&gt;) r` instance of `Functor`. (See also: cdsmith's reply.)
Nice work! I really liked this feature when it came to Rails and Yesod.
The single-object category is all about the morphisms, because the object isn't interesting. I find it helps to think of the morphisms as (\*1), (\*2), etc. Obviously, (\*1) is the identity morphism. Composition works as you'd expect, such that (\*2) . (\*3) is equivalent to (\*6): for every pair of morphisms there exists a morphism representing their composition such that their triangle commutes. You don't even need to think about the object to generalise this to any monoid: there's some identity morphism, and a bunch of morphisms which all compose associatively, because they all go to and from the same object. When you bring back in the dirty details, all the information about the values being composed is carried by the morphisms.
That temporary bit of insanity in lambdabot has been gone for a while now. `(.)` is back to its correct definition.
Is this what people mean when they stress a monoid *object*, as in "A monad is a monoid *object* in the category of endofunctors". Well, that makes a lot of sense, since it doesn't make much sense to say the single object category is in the category of endofunctors.
One way is using [non-empty lists](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List-NonEmpty.html) foo' :: NonEmpty Int -&gt; IO String foo' (n:|ns) = ... **Edit**: /u/richardthepeace Now you have separated the failure (having no elements) nonEmpty :: [a] -&gt; Maybe (NonEmpty a) from the action `foo' :: NonEmpty Int -&gt; IO String` but I haven't discussed how to combine them again! Using [`Data.Functor.fmap`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor.html#v:fmap) just leaves us with the same solution you found clumsy fmap foo' . nonEmpty :: [Int] -&gt; Maybe (IO String) We need some other way of dealing with with nonEmpty [1,2,3] :: Maybe (NonEmpty Int)` our next attempt is [`Data.Traversable.traverse`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Traversable.html#v:traverse) which gives us your original `foo` traverse foo' . nonEmpty :: [Int] -&gt; IO (Maybe String) Let's assume you want to print the string foo'Print :: NonEmpty Int -&gt; IO () foo'Print ns = do str &lt;- foo' ns putStrLn str then you can use [`Data.Foldable.traverse_`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Foldable.html#v:traverse_) (note that `traverse_` is **not** in `Data.Traversable`) to discard the output traverse_ :: (Applicative f, Foldable t) =&gt; (a -&gt; f b) -&gt; (t a -&gt; f ()) traverse_ @Maybe :: (Applicative f) =&gt; (a -&gt; f b) -&gt; (Maybe a -&gt; f ()) traverse_ @Maybe @IO :: (a -&gt; IO b) -&gt; (Maybe a -&gt; IO ()) traverse_ @Maybe @IO @(NonEmpty _) :: (NonEmpty a -&gt; IO b) -&gt; (Maybe (NonEmpty a) -&gt; IO ()) traverse_ @Maybe @IO @(NonEmpty Int) :: (NonEmpty Int -&gt; IO b) -&gt; (Maybe (NonEmpty Int) -&gt; IO ()) where `traverse_ foo'Print :: Maybe (NonEmpty Int) -&gt; IO ()` performs **no** `IO`-action in the case of `Nothing` which signifies an empty list, does the same as \case Nothing -&gt; pure () Just ns -&gt; foo'Print ns expanding `foo'Print` \case Nothing -&gt; pure () Just ns -&gt; do str &lt;- foo' ns putStrLn str ---- Note that `foo'Print = foo' &gt;=&gt; putStrLn` so this can be written tersely: traverse_ (foo' &gt;=&gt; putStrLn) . nonEmpty :: [Int] -&gt; IO () In that line of code `nonEmpty` **creates** a possible failure that `traverse_` **branches** on.
What exactly was the change? I've been having trouble understanding the issue.
Thank you /u/dcoutts; I have started with your more bite-sized article ["Stream Fusion. From Lists to Streams to Nothing at All"](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.104.7401), which also discusses in some detail the case of nested computation I'm interested in (the `concatMap` example), and I'll tinker with the example you provided.
Unfortunately, - I think the Haskell ecosystem is [very immature for Game Programming](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#game-programming) so you will have to implement a lot of stuff you probably already get in other languages' engines. - AFAIK anything that is not x86-64 on OS X/Linux/Windows [is not very well supported](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#mobile-apps). This also means that Accelerate or Repa will probably not work for you. I'm afraid there are too many risk factors to use Haskell for your project imo :(
&gt; My default plan would be to use openFrameworks or Unity3D if I can find a good way to model the fluids in an object-oriented way, but my instinct is that this might all be easier if I embrace the functional nature of the design and take advantage of Haskell's libraries for parallelization, such as Accelerate or Repa. I can't quite follow how you can think that the parallelization features of Haskell might have a bigger impact for the game development than a whole game engine? You have to ask yourself if you want to develop a game or a game engine and a game? I know that game engine programming can be really interesting and great, but it can also be quite time consuming and you don't have professional experience in Haskell and have 18 months time. 
Personal anecdotes that may help. Background: I work professionally at a Japanese social game company (mostly server infrastructure, but help with Unity3d programming on occasion). Roughly two years ago I decided to learn Haskell by writing a networked game server on my own. It took quite a bit of time to wrap my then-imperative brain around how to use functional programming concepts, there were a lot of times I just wanted to give up and quit, but I stuck with it and in the end am absolutely glad that I did. My latest side-side project has me using the SDL2 Haskell libraries to write a tiled roguelike. You can definitely write games in Haskell, and it can truly be a joy! It wasn't until a couple months ago that I bought Haskell Programming from first principles, although in my case all it really did was give me some definitive proof about the direction my self teaching went. If you and your team are new to Haskell I could not recommend it enough as a starting point if you want to jump start learning, it'll definitely get your feet wet faster than the alternatives. If your team can get their head wrapped around Haskell in the first 3 months, that still gives you 15 to properly implement, optimize, and build your game. I can't speak much about graphical performance (roguelikes are just not graphically intensive) although running at a fixed 1/60s time step on a server has been quite performant without too many weird optimizations. If the game you plan on building isn't incredibly visual you should be fine (you may not need Accelerate or Repa even). I do know that GHC [cross compiles to iOS](https://ghc.haskell.org/trac/ghc/wiki/Building/CrossCompiling/iOS) although I've never given it a shot myself. If you'd like to talk a little further in person feel free to drop me an email at brian.jones at uncannyworks dot com.
Yeah, my feeling is that I'd be willing to spend a significant portion of the time writing the game engine if it's at all feasible to finish the game in 18 months. Since the requirements aren't incredibly high otherwise, I figured this might be a good time to do it. I should explain that my last experience working on game was basically a minecraft clone in Unity3D. We used a combination of cubiquity and a proprietary voxel texturing framework to make a decent Minecraft-like demo for researches to experiment with, but the experience made me release that Unity3D isn't well suited to working with voxels or non-game-entity-based games.
Hah! I've nicknamed my experimental project just like yours (if my search for uncannyworks on github was right): hackslash;)
Indeed! Hack-/ for short :)
Great, thanks! I'll look over it more detail once I'm more familiar with the language. SDL I've used before though, so that would definitely be something I can use.
Did not know that, thanks for the heads-up!
yes are right. And how about in haskell?
/r/haskellgamedev isn't super active, but they might have some resources for you.
You could look into PureScript, although I'm not sure about iPad support.
Random tip: a commonly defined operator is `&lt;$$&gt; = fmap . fmap`. Or even `&lt;$$$&gt; = fmap . fmap . fmap`. Then you can do something like `toUpper &lt;$$$&gt; foo [1,2,3]` which works with either of your definitions.
&gt; The way I work it out is, instead of try to match morphisms and composition, I matched monoid laws. Both paradigms has same laws and it is alot easier to match. Indeed; the category laws for the (one-object) monoid category are the monoid laws.
Pipes and conduit were both initially created when nobody knew how exactly a good API for this should work so I doubt backpack will be any help with similar issues in the future.
Developing a game engine in Haskell sounds like a very interesting challenge. Haskell's main drawback, as far as game development goes, I think would be it's unpredictable performance characteristics. Personally, if I were doing this professionally and/or writing a game that has some CPU-intensive physics or rendering, I'd look into using Rust. It has the performance advantages of C++, but also has a strict typing system that prevents most runtime errors. The libraries for game programming, like Piston, are a bit immature, but the community seems friendly and helpful.
You may want to look at [Falling Turnip](https://github.com/tranma/falling-turnip), which is a falling sand game written in Repa. Fully parallel and decently fast, from the looks of it (although it runs entirely on the CPU, not the GPU).
On such a small code obviously most suggested changes will be of small impact, or preferences. For example, for your: runQuery $ \conn -&gt; P.runSqlPersistM (getUserKey username) conn Since `conn` always is passed around, partial application is enough to drop the `conn` argument: runQuery (P.runSqlPersistM (getUserKey username)) Now If you want to reduce duplication, might as well make a function fromDB :: -- infered type fromDB query = runQuery (P.runSqlPersistM query) ... muser &lt;- fromDB (getUserKey username) And also for "patterns" like the following, you could do some lifting for smaller simplification: getUser :: T.Text -&gt; P.SqlPersistM (Maybe User) getUser username = do mUser &lt;- P.getBy (UniqueUsername username) return $ P.entityVal &lt;$&gt; mUser -- can be rewritten as getUser :: T.Text -&gt; P.SqlPersistM (Maybe User) getUser username = liftM (fmap P.entityVal) $ P.getBy (UniqueUsername username) Recently I myself switched to the practice of importing types unqualified, so for your example usage of text. It would be: import Data.Text (Text) import qualified Data.Text as Text Then in type signatures you'd just use `Text`, not `T.Text`, and when using functions exposed by that module via `Text`, like `Text.unpack` That's all I can think of glancing over your code
I find exactly the opposite.
Thanks! Also, I can convert the ``` P.SqlPersistM (Maybe x) -&gt; MaybeT P.SqlPersistM x ``` by using ``` MaybeT :: m (Maybe a) -&gt; MaybeT m a ``` correct?
But you could always handle the backend with Haskell!
Great idea. Sounds like `caaaar` and `cddddr` in Lisp.
Yes you can. But I didn't want to suggest that because I'm not familiar with `SpockM` (and Spock in general) and you'll need to sprinkle `runMaybeT` for each route (since doing so just for queries would be pointless). But I guess if you run each route "handler" inside MaybeT, should work without an issue as long as you lift the `text` functions (I assume those are from Spock)
Yes, `text` is from Spock ``` text :: MonadIO m =&gt; T.Text -&gt; ActionCtxT ctx m a ```
I can't find it in document.
Nice! Will look into this tonight!
Thanks, yeah I've heard of Rust but never looked into it.
Yeah, I checked that out already. Looks like it's been abandoned, so I reverted back to the last working version. Still doesn't build. Any experience building it?
Yeah I guess ideally I'd be looking to use a game engine that's more procedural. OpenFrameworks came to mind from my experience with Processing, but I'd imagine there'd be more modern engines nowadays. Don't think I'll go functional reactive style, but we'll see how everything shakes out once I get more used to the language.
I'll definitely check it out. Thanks!
thanks! If you have any questions, message me here, or on twitter, @wespiser.
Shots? It is completely legitimate for people to have different coding styles. What's the problem?
This metaphor was not meant to be that deep. Just an instance of the meme where function names have a repeated character that indicates how many times an action is repeated. I don't think I've ever seen that other than in those famous Lisp functions.
Python is a great beginner language. It is very easy to learn. But starting with Haskell will definitely be a strength. Learning python coming from haskell will be easy, whereas the reverse might be painful. So my suggestion is that you should stick to haskell. It will be the best choice for the long run, even if you end up using some other language in the future.
By using a category enriched on the monoidal category, with the hom-set of the only object being the monoid.
Your question is already answered but I wanted to point something out. cycle does not start or end anything. it creates an infinite list. since haskell is lazily evaluated it does not try to create the infinite list as soon as you declare it. instead, it gives you its elements when you ask for them. if you ask for the complete list it returns an infinite list, because it is what it is. if you ask for its first 10 elements it gives you those I don't know, I think this is important to know. 
If you're interested you may want to check out /r/rust and /r/rust_gamedev.
Hm, I still need to read about enrichment, but the latter half of that statement still sounds wrong. How can the hom-set of the object be the monoid? Doesn't that imply the monoid is a set, which ties you to monoids in the category of sets?
If only [Haskell for Mac](http://haskellformac.com) worked on the ipad. Maybe it's coming ? I'd ask them.
Where are you getting the `b ~&gt; c` you need for the semigroup compose? The argument you are given for the functor map is a `b -&gt; c`. On mobile, if this is terse and confusing.
You are correct. You must've written that while I was writing my edit.
I think that's a questionable thing to assert, it only looks associative at that particular type, the two (&lt;$&gt;) could be totally different functions.
I find most of the operators in this library to be terrifying, but the `&lt;$...&gt;` family of operators is defined there. https://www.stackage.org/lts-7.11/package/composition-extra-2.0.0
Yes, that's what makes rules like this useful! It means we can rewrite one expression to the other, which might be useful if one is more efficient than the other. &lt;$&gt; might not be the same function in each case, but it represents a class of functions that interact with each other in a certain way.
Also wanted to say you're right, due to the type of the 2nd &lt;$&gt;, g must be a function in order for the first &lt;$&gt; to produce a function (or else the expression is ill typed). So it is a bit gratuitous of me to make a statement like "&lt;$&gt; is associative". However, I still think it's interesting and potentially valuable as an observation.
I really don't think it works in many other circumstances than the one you named, but I totally am willing to be proven wrong.
I feel like it actually might be misleading in this case, I do like the sentiment though.
Does Intero work for you? My project builds with this stack.yaml and the lts-7.10 resolver line, but Intero won't start with some weird `GHC.PackageDb.readPackageDb: inappropriate type (not enough bytes)` error.
Perhaps sort the list first?
I hear a lot of times, that haskell is for academic purpose but not production. What does it mean?
As a side note, you say that `foldr`/`build` fusion can't really handle `zip`, [but it can, using hyperfunctions](https://arxiv.org/pdf/1309.5135.pdf). Edward Kmett has the [`hyperfunctions`](http://hackage.haskell.org/package/hyperfunctions) package for anyone wanting to experiment with this.
Thanks.
Nested angle brackets give me a headache though.
It seems like the `debugIsOn` thing being useful is predicated on GHC being smart enough to optimize away cases where there's a conditional on a constant expression. If that's true, why not: trace :: String -&gt; a -&gt; a trace msg f | debugIsOn = DT.trace ("TRACE: " ++ msg) f | otherwise = f &gt; That should compile, when -DDEBUG is not defined, to: I'm not quite sure where you're getting that. `#ifdef` is a preprocessor directive. If you have `CPP` enabled, ghc runs your source through the preprocessor and the preprocessor filters it. So for your code, if DEBUG is one: trace msg f = DT.trace ("TRACE: " ++ msg) f and if it's off: trace msg f = f There won't be any conditional in the Haskell code the preprocessor returns - evaluation of preprocessor directives occurs purely at compile time.
You might be interested in the [NoTrace](https://hackage.haskell.org/package/NoTrace) module. Then instead of importing either `Debug.Trace` or `Debug.NoTrace`, you could import `Debug.MyTrace` and in that module you could re-export one module or the other using an `#ifdef DEBUG`.
Thank you for the answer. &gt; It seems like the debugIsOn thing being useful is predicated on GHC being smart enough to optimize away cases where there's a conditional on a constant expression. Yes, this is better, thank you! &gt; I'm not quite sure where you're getting that. I was thinking of a later step, when trace msg f = f get substituted with trace "TRACE: whatever" False = False and so in the original function all this, in another step, will be replaced with a simple False. Or am I missing something?
Excellent news! I've been living dangerously using 8.0.1 and macOS Sierra - you got this out before I blew up! That's a great effort. Thanks to all the contributors. I can breathe again :) 
&gt; and so in the original function all this, in another step, will be replaced with a simple False. Well, the trace function takes an argument of any type (`f`), so `f` *could* be a `Bool` set to `False` or anything else. If you're imagining what happens if you call `trace` with debugging disabled: trace "Blah blah" False and wondering if it's the same as if we simply had: False then answer is yes. If debugging was on, then it's the same as: DT.trace ("TRACE: " ++ "Blah blah") False Does that help?
&gt; could this lead to a performance decrease if I put that code in a function that get called many times? GHC tries hard to inline your code in order to eliminate useless functions like `id`, but the only way to know for sure that it did perform an optimization you are hoping for is to observe the simplified output using [`-ddump-simpl`](https://downloads.haskell.org/~ghc/master/users-guide/debugging.html). Unfortunately there are so many other simplifications in addition to the one you are looking for that the output is almost unrecognizable, but I can confirm that GHC does optimize `id` away because the output for main = print 42 and main = print (id 42) and even main = print ((repeat id !! 100) 42) was exactly the same up to alpha-renaming.
The original version is aimed more for novices, and does a fine job. However, there are a couple things, like the use of the 'read' function, and 'IORef' that should probably be avoided. Plus its nearly a decade old. Please let me know if you have any suggestions!
I don't use Intero. Looks like it depends on a precise version of the compiler. See &lt;https://github.com/commercialhaskell/intero/issues/324&gt;. 
Here comes a lot of unlearning. Python, Java, etc procedural background. Sure, I use functional / declarative features, like generators and list comps, but Haskell is whole other mindset. Thanks for the comparison.
&gt; ~~spend your weekends contributing to ghc to make it less slow~~ Say I did want to learn Haskell perf tuning by doing and wanted to speed up GHC. Where would I begin? Are there clear performance-related tickets? Do I just try to make/find examples of slow compiling code and profile GHC compiling it? Would I need to make smaller GHC contributions first to ramp up? 
It could also hit programs compiled by ghc. Anything using enough different library packages (by direct dependencies and transitive dependencies) risks hittting that limit.
I bought Haskell for Mac, and really like it even though I almost use Emacs+Intero. For the iPad, Raskell is fun, I also recommend it. For writing bits of Haskell code, trying book examples, etc. while on an iPad, Raskell is very cool.
I'd get in contact with the people at [Keera Games](http://keera.co.uk/blog/category/products/games-products/) as they've put out the most Haskell games from what I've seen. The goal of the conversation would be communicating your requirements and intimately understanding the tradeoffs involved in choosing Haskell.
Independent of the question of whether Haskell is a good choice or not, I would never in your position be trying to learn a language I've never used before. I'd go so far as to question its professionalism. You're being paid to produce said game, not learn a new language, unless you worked that in quite explicitly. I'd strongly suggest going with whatever stack you're currently the most comfortable with. I do not expect that Haskell's advantages would be so large as to overpower the disadvantage of not knowing it yet. I mean, yes, by all means, add it to the toolbelt, but I'd suggest this is not the time or place.
I sometimes also define utility operators for things like that: (&lt;:&gt;) :: Applicative f =&gt; f [a] -&gt; f a -&gt; f [a] fList &lt;:&gt; fElem = (\a b -&gt; a ++ [b]) &lt;$&gt; fList &lt;*&gt; fElem So you can write `pure &lt;$&gt; digit &lt;:&gt; char &lt;:&gt; digit &lt;:&gt; char`
Why would this be better? Wouldn't factoring out similar code just be undoing gains from inlining?
Think of it like this. You have a list with a finite number of elements, lets say 100, and a very old, very slow computer. You write a program to print out the list. It starts printing the list, element by element, but it is so slow it takes 5 seconds for a single element to be printed out. You decide "ain't no body got time for that" and want to abort the process. What you do is interrupt the printing process, not stop the list. List is not something that moves. So what you have to do is listen to key events while the program prints out the list. Stop the printing process on key event. I don't see any way other than /u/Faucelme 's answer, but then again I'm no Haskell expert.
I've seen worse grading syllabuses. It's a start. 
Do you have others? Very interested. EDIT: Found this by /u/snoyberg (initial version): https://www.fpcomplete.com/haskell-syllabus
/r/haskell only allows **excellent** jokes
A bit surprised to see "Category Theory" as a concept in expert-level haskell programming? It feels like every other item in the list of concepts are concrete, useful, pragmatic tools and techniques. Category Theory kind of jumps out as being nothing like the others, heh
_sulks away_...
Isn't some knowledge of Category Theory at least required to "create novel abstraction with laws" ?
I believe tdammer's definition of monoid was meant on the level of abstraction usual in abstract algebra, though the associative property was missing.
*Hspec/doctest with Cabal as test framework* Hah, that must be cabal hell freezing over...!
Is there a textual version of this? It would be nice to make it into a checklist.
https://wiki.haskell.org/Monad_laws
How is this meant to be used? As a learner, I think I'd prefer graded exercises.
So, just a monad?
Expert-level skill "write distributed systems with garantees" doesn't seem to be covered by any of the related concepts. Reasoning about distributed systems is a weak point of denotational semantics, or so I've heard. You have to get down and dirty with state machines and stuff like Leslie Lamport's TLA.
This is very Haskell-oriented. Nothing about e.g. modules/functors (in the parametric module sense).
If you define Functional programming as being Haskell its a great checklist. Of course if you prefer Erlang, a Lisp, Prolog, or any other language that is not a member of the ML Family it makes no sense what so ever. 
Sort of? https://github.com/puffnfresh/daggy I think it's re-implemented in some of the fantasy-land spec libraries, but that's probably the simplest stand-alone js library I'm aware of for sum types/tagged unions.
But as long as you "undo" the inlining as suggested in the parent post, don't most of the drawbacks go away?
Not from the perspective that the main gain of inlining is allowing for further optimizations/rewrites to kick in.
I get the feeling the "undo" mechanism would not be so simple.
Hmm, so I'm a bit of novince, a bit of advanced beginer, a bit of competent and a bit of proficient.
Thank you, a change has been made and the site updated. 
Do people usually abstain from advanced type-level programming in Haskell production code?
Advanced Beginner here. Good news! You can write useful apps at this level. :-)
I abstain from it in a general sense. Type families are kinda gross. Error messages are bad. Type inference is bad. Power level is low. I pretty much stick to Servant style stuff at the most if possible. 
It was named Haskell instead of Curry to avoid accidentally reminding people of an effeminate man when they used it with a certain abstract machine. "Tim Curry? Oh dear, that wouldn't be good."
How is this different than Epigram? Is epigram still in production?
Cool. I'm not completely convinced of its advantage over other approaches, but I'm glad it exists. I'm a little confused by the documentation's use of "mixfix", though; as I understand it, "mixfix" only applies to operators with more than two arguments and more than one lexical symbol (like C's ternary operator `? :`).
Try the lamb pasanda, it's incredibly rich and creamy.
Off the top of my head: Like Haskell, Idris is a statically typed purely functional language. Unlike Haskell, Idris is strict by default, has implicits instead of type classes, and, of course, has dependent types. There are other experimental features not found in Haskell (e.g. uniqueness types, implicit casts, totality checking).
I dislike this rubric a lot but I can't put my finger on why.
That would be confusing, indeed, if that were what was said. But I doubt the concern was that Tim Curry was too effeminate, but rather that he is most known for Rocky Horror Picture Show.
Custom type errors help when your type families explicitly know there's an error in the code. What still sucks is that there's no way to provide custom errors when the compiler decides something is ambiguous or things like that. It would be nice to have a general mechanism for pattern matching on errors so that library makers could provide more useful messages for common mistakes when using their combinators.
FWIW, it's less gross in Idris, because most of it is just CoIC, it's orthogonal and it makes sense. I abhor all the many extensions covering overlapping and ill-shaped parts of serious type-level wizardry in Haskell. DH is at least a step in the right direction, although it's not a very big step.
I'd still recommend parsing into as much structure as possible, even if you need a `renderZipCode :: ZipCode -&gt; String` function.
~~`Category`, definitely. A semigroupoid is a category with all morphisms invertible, after all, and I consider that too strict of a restriction to impose on the useful and visually unimposing `.`, not to mention one that most uses of it today don't satisfy.~~ EDIT: That's a groupoid, not a *semi*groupoid. My bad! My vote is on semigroupoid composition for `.`now too.
~~Why `Semigroupoid` and not `Category`? Many useful categories have non-invertible morphisms.~~ EDIT: That's wrong. I was thinking of groupoids, not semigroupoids. `.` meaning semigroupoid composition would be fine IMO.
I advocate for not using any advanced type features in industrial Haskell code. One point I'd like to add is that many of the advanced type-level programming features are research concepts, not robust industry features. For this reason, they are time consuming to learn and a risk in terms of technical debt. 
https://en.wikipedia.org/wiki/Lambda_cube
http://img.pandawhale.com/72017-bill-and-Ted-excellent-gif-0heF.gif
I have a really hard time believing this. can you offer any evidence? 
This also looks like a motivating example for an extension, e.g., list sections (`[,,,,]`) or inline bindings (`[&lt;- letter, &lt;- digit, ]`).
in what ways has it been proven a fertile source of abstractions? and in what ways has category theory made libraries accessible? anything more substantial than something superficial like "this is a category so composition is associative and there's an identity"? that's not category theory.... that's the first page out of a 300 page book on category theory. 
Can anyone talk about what is most likely meant by "efficient persistent data structures"? 
Ah, yeah and Emacs (haskell-layer) fails to build it because Intero limits the dependency of GHC to 8.0.1. In case anyone is reading, I got it working by cloning the Intero source locally, increasing the GHC dep to `&lt;= 8.0.2`, and adding a reference to it in my own project's `stack.yaml`. Now it builds in macOS and everything is working again.
Yes, that's what I meant by "implicits". The terminology is due [Type Classes vs. the World](https://www.youtube.com/watch?v=hIZxTQP1ifo).
Ah I see. Thanks for the explanation.
"[Persistent](https://en.wikipedia.org/wiki/Persistent_data_structure)" is an older term for "immutable". So this means you could implement, say, a balanced `Map` by allocating a new spine and keeping most of the old subtrees untouched, as opposed to doing the rebalancing by mutating the tree.
I don't really think graph reduction as an intuition is so hard to develop that it would be worthy of "expert" level. Rather, it's just one of those things that few Haskellers tend to talk about (likely because people are more fond of denotational semantics) and there is not a lot of literature on it. I personally thrive on operational semantics and being able to understand how Haskell's evaluation works at a concrete level was immensely useful.
Cygwin is the right direction, as vty's readme says that is the only way to install it on Windows. You might find more help with this in vty's issue tracker.
General self-application won't type check in Haskell (or indeed in a simply typed lambda calculus), `x` needs to be both a function and its own argument \x -&gt; x x even though it seems to work in specific cases id id :: a -&gt; a const const :: x -&gt; a -&gt; y -&gt; a map map :: [a -&gt; b] -&gt; [[a] -&gt; [b]] but if we are explicit about our types, we see they are in fact instantiated at different types idid :: forall a. a -&gt; a idid = id @(a -&gt; a) $ id @a constconst :: forall x a y. x -&gt; a -&gt; y -&gt; a constconst = const @(a -&gt; y -&gt; a) @x $ const @a @y mapmap :: forall a b. [a -&gt; b] -&gt; [[a] -&gt; [b]] mapmap = map @(a -&gt; b) @([a] -&gt; [b]) $ map @a @b We can type, etc. selfId :: (forall xx. xx -&gt; xx) -&gt; (forall a. a -&gt; a) selfId id = id id ---- The inferred type in the [**MLsub** type system](http://www.cl.cam.ac.uk/~sd601/papers/mlsub-preprint.pdf) (which has type inference with subtyping + ^\(*parametric*\) polymorphism) is ^\([video](https://www.youtube.com/watch?v=E3PIKlsXOQo), ^[interactive](http://www.cl.cam.ac.uk/~sd601/mlsub/)\) self :: (a -&gt; b) &amp; a -&gt; b self x = x x and applying `self` to itself gives a bottom type omega :: Bot omega = self self and lets us type the Y-combinator y :: (a -&gt; a) -&gt; a y t = (\x -&gt; t (x x)) (\x -&gt; t (x x))
Is this related to something like knowing when to make data strict ("lazy in the spine, strict in the leaves"), etc.?
&gt; i thinkyou need dependent types to rule them out at type checking You need proof checking, which doesn't need dependent types but does typically need proven termination: no bottom, and no general recursion. Isabelle/HOL, eg, can prove laws like the monad laws, at the cost of more restricted expressiveness.
I was working on list sections years back, can't remember what happened to them. Using Idris' [`!`-notation](http://docs.idris-lang.org/en/latest/tutorial/interfaces.html#notation) that I haven't used much granted, the following pure [!letter, !digit, ] should do what you want.
Written by the same guy who did a lot of work on the Epigram compiler (indeed, as I recall that work comprised part of Edwin's thesis). Epigram was an exploration of one way to use dependant types to write programmes. It focused on using them to prove totality in functions; and is not Turing Complete. Idris is exploring quite a different direction in the use of dependant types. It is focused on them as a library writers tool, as part of a general purpose computing language. Totality is optional, and it is Turing complete. Hopefully that makes the different approaches clear; assuming I've not mangled something along the way
On mobile so I can't write code, but imagine this: You've got an if then else expression that returns 3 in one branch and False in the other. You then pass that expression as the argument to a function that ignores its argument. This program would never crash due to a type error, but Haskell will reject it because all branches must return the same type. 
Idris feels very much like Haskell in its syntax and style, but trades type inference for fully dependent types. 
Well, for one, monads came of of category theory.
&gt; Say I did want to learn Haskell perf tuning by doing and wanted to speed up GHC. Great, we would love to have your help! I generally find it is easiest to dig in to compiler performance if you have a concrete program which exhibits poor compilation performance. The easiest performance issues that I have solved have been those where I had a program which revealed some obviously super-linear asymptotic behavior in the compiler; with such an example in hand you can easily just increase the input size until the problem becomes apparent in the profile. There is a [compiler performance](https://ghc.haskell.org/trac/ghc/wiki/Performance/Compiler) page on the GHC Wiki which describes some of the themes which I've found while looking at compiler performance issues. Of course, you will likely find it helpful to spend a bit of time reading about GHC's structure before diving into performance. There are two principle sources for knowledge on GHC's internals, * the [commentary](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler) on the Wiki. These typically offer higher-level descriptions of the general structure of the compiler. * The plentiful Notes scattered throughout the source code. These are often more detail-oriented, discussing corner cases and design considerations. Let me know if you want to discuss next steps. Either Reddit, IRC (`#ghc` on `irc.freenode.net`), or email are fine options.
Speaking from a position of having fought Coq for a little while now: - Conceptually, dependent types are often significantly simpler than Haskell's hodgepodge of features (which, I should stress to offset the negative connotation, have their own significant advantages). - Everything to do with syntactic termination/productivity checking is bullshit, riddled with annoying corner cases and hacky workarounds. For termination (i.e. fixpoints over inductive types), well-founded recursion is mostly workable; for productivity (i.e. cofixpoints) I know of no general solution, and not for lack of trying to find one. - Dependent programming in general has a tendency to become unpleasant quickly due to (a) a lot of explicit manipulation of equality proofs; (b) the simplification (i.e. partial reduction) heuristic not dealing well with typically large proof terms; (c) having to unfold cofixpoints manually; (d) having to take great care wrt symbol opacity in order not to block computation; and probably some other stuff I'm forgetting right now. My general experience is that I often start out with a concept that sounds pretty good on paper, then give up halfway through the implementation due to overwhelming incidental complexity. - The above issues are sometimes mitigated by tactics which do a lot of heavy lifting for you but tend to fail on more complex tasks. Not sure how many of these pain points Idris addresses, though from what I've seen, the termination/productivity story isn't any better than in Coq (except of course the checker can at least be disabled). Agda supposedly has nicer dependent pattern matching, which I've been meaning to take a look at for some time.
Ah. Thanks for that explanation. I have read parts of Okasaki's Functional Data Structures and I know about some of those implementations in Haskell, but I got a bit hung up on the *persistent* part of the "efficient persistent data structure." At first I assumed it was related to something like the State monad being inefficient but then realized it would probably be more involved than that. Anyway, this gives me some foundation to explore further. Thank you.
Provocative title, interested in what people have to say
For a tour of frp libs you should check out https://github.com/gelisam/frp-zoo. 
Here is the relevant part: &gt; [...] I mentioned the "[situations, actions, and causal laws](http://www.dtic.mil/dtic/tr/fulltext/u2/785031.pdf)" thing because I think McCarthy was getting at one of the prime cruxes in computing, which is that you want to be able to advance state and you need to do it absolutely safely. So you'd like to be able to reason perfectly about how you're going to advance the state, and you'd like to organize the system so you get the best of both worlds. Monads is too much of a kludge. That's trying to save functions, and missing what McCarthy had already gotten to more than 50 years ago. I haven't yet read the linked document so I have no idea what he's talking about. *edit*: the paper seems to be about a Prolog-like system.
Yeah because OOP was so great. /jk Monads are cool but hey, let's talk about this again in 10 years.
I absolutely love that the Tsuru Capital post is just titled "Employment" :P.
I'm putting some blog posts together on reactive-banana, reflex, and associated techniques/patterns/etc - send me a PM if you want to have a look at the drafts for the reactive-banana stuff (which is nearly ready). 
You can implement it as (I'll use `y` and `fix` interchangeably) y :: (a -&gt; a) -&gt; a y f = f (y f) (or [`fix f = let x = f x in x`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Function.html#v:fix) for better sharing), the trick is doing it without recursion. [Implementing `fix`](https://www.reddit.com/r/haskell/comments/5fsvks/what_are_valid_programs_in_the_lambda_calculus/dan7lmd/) without recursion (in this thread). [The paper](https://www.seas.upenn.edu/~sweirich/papers/wadlerfest2016.pdf) that talks about `Dynamic` discusses this in **section 7**.
My number one problem with my code in production is people using in ways I haven't thought of or tested for. i.e. they give it inputs which sort of work but ultimately fail in some deep part of the code. If I could release a program where I encoded every input to every function as a dependent type then bugs deep in my software would be reduced massively. All errors would presumably be caught right at the start where a user tried to input something they aren't allowed to. At that point we'd stop talk about why they need to put that new value in, what they expect to come out of the other side and I'd update the code accordingly. Then it would all work again. 
Eh, I think if you treat monads as nothing more than an abstraction for making side effects safer, you're not really getting the whole picture. `Monad` is as vast and applicable an abstraction as OOP.
Note that Idris aims to make dependant types easier to use in application code, so your experience with Coq may or may not reflect the experience one would have with Idris. Just because it was hard once doesn't mean it will have to be that way forever!
Yea I'm very familiar with Idris's advantages. I'm wondering more about the drawbacks.
Agreed, although it's good that he at least gives what he thinks is a better way to solve the problem we solve with monads in Haskell. (McCarthy's idea, I haven't read the paper yet)
It's sad that this post is getting downvoted, probably because the title is provocative. I'd like to see more discussion here.
The answer is [Arrow](https://www.stackage.org/haddock/lts-7.9/base-4.9.0.0/Control-Arrow.html#t:Arrow). In simple cases Applicative or Alternative also can be used.
I replied to the issue on GitHub.
Here's the code formatted: expmod :: Integer -&gt; Integer -&gt; Integer -&gt; Integer expmod a x m | x == 0 = 1 | x == 1 = a `mod` m | even x = let p = (expmod a (x div 2) m) `mod` m in (p^2) `mod` m | otherwise = (a * expmod a (x-1) m) `mod` m I think it's just an efficient implementation of: a^x (mod m) Why it works is more due to mathematics than to Haskell.
That is an eloquent way of putting it. I tried Coq as well in the past few weeks, and not knowing much about proofs either formal or informal decided start from the first chapter of the Software Foundations book. If I merely took doing those simple exercises as a logic puzzle game it would not be too bad, but with an eye of using dependent types for general purpose programming, I was struck not just with how difficult the problems were - I'd often miss a rewrite or something stupidly obvious that brought me back to my school days math classes - but both how simple and hard they were. In Coq, proving simple things is actually quite hard. I do not want to even imagine how hard would it be to prove properties of nontrivial programs. As the underlying foundation for dependent types are formal proofs, I do not think they have much of a future unless they can be made much easier than they are now
The idea is to have `filter p s` take a proof that there are infinite elements satisfying `p` in `s`. Disregarding whether or not such a definition is particularly useful, it's a way to investigate how functions which are not syntactically productive can be defined. Bertot has an [implementation](https://hal.archives-ouvertes.fr/inria-00070658/document) in Coq. I'm interested in this problem because I have a cofixpoint (in an unrelated formalisation) which I can't for the life of me get Coq to accept.
The answer is not arrow because of the arr function which leads to the same problem.
To me it seems what we want to do is to analyze the pipeline without feeding it data. Arrow seems like a nice abstraction because it has all the pipelining tools you need to construct big pipes from small pipes, and it also allows you to have your own type of pipes. This type can be as open or as opaque as you want. The problem is that `arr` still lets you put arbitrary functions as parts of your pipe so you lose the ability to do a ton of things you could do otherwise. For example, without `arr` you could make instances of `Arrow` that can be printed or serialized. With `arr` you can't really do that.
Yeah, I guess you're right about that. It would still be nice if `arr` was instead put in a subclass of `Arrow`.
I think this is trying to get at one of Kay's recurring themes, which is treating time as a *logical construct* rather than an external, physical effect, i.e. time is a protocol which advances one step when all of the relevant components agree that it has advanced a step. How that corresponds to a wall clock is irrelevant; this tries to sidestep issues like race conditions and inconsistency by making them inexpressable. Two components can't be inconsistent, seeing different state values at the same time, since the notion of time is *derived from* the state's values: rather, the two component are at different times. Similarly, there can be no race conditions, where the output differs based on which component finished first, since "finishing first" requires a notion of time, and time is derived from the values being calculated: there is no time 'in between' the logical ticks, hence there's no point at which one component was finished and the other one wasn't. A couple of projects in this area would be the TeaTime protocol used by OpenCobalt, and the "Worlds" idea of VPRI (e.g. "Experiments with Worlds" at http://vpri.org/html/writings.php ) where new "worlds" can be spawned, which encapsulate mutation; this lets the world be "rolled back", going 'back in time' as far as the application's concerned. Of course, the elephant in the room is IO, since IO actions can't always be 'rolled back' (as anyone hitting "reply all" knows!); we can probably do everything else using ST. From an imperative and OOP perspective, these are powerful ideas to think about. From a pure functional perspective, our programs are essentially 'unchanging', but we *model* time-varying processes/transformations/etc. using immutable values. We may not have the difficult problems of concurrency and interference that imperative programmers face, but we also have to build up from an impoverished starting point (basically, lambda calculus + sugar). I also agree that the emphasis on monads for the past ~20 years has been a bit limiting. Thankfully we're breaking out of this mindset as alternative perspectives like algebraic effects and FRP are becoming more popular. Personally I make a conscious effort to avoid phrases like "the Maybe monad", "the IO monad", etc. because I really mean "Maybe values" and "IO values"; "Monad" is just one (particularly general) interface for manipulating them. It's at lest interesting to think about. I think the closest thing to "time" in a pure functional setting is a tail call: a 'point of no return' (literally!), which induces an 'arrow of time' on an otherwise static expression. I suppose relativistic programming is related, where we abandon the artificial notion of a universal clock, and accept that some events are causally related, and the rest may appear in a different order for different observers.
&gt; where you don't even know the language, you can't even provide estimates. There are several languages I've never written in before where I would be comfortable giving estimates: Dart, Rust, Elm, COBOL, Fortran, a few others. That said, I wouldn't have professionally provided a Haskell estimate before I wrote my first program in *it*. I can estimate how long it will take me to skill up in most languages, and I just add that to a mildly inflated estimate how how I would do it in a language I know but without a particularly helpful library.
Oops, I think you are right. So perhaps all "mapping" operations should be restricted as well?
`sequenceA` should only be an artifact of history, because it was not always the case that `Monad` had `Applicative` as superclass. the default implementation for `sequence` is `sequence = sequenceA`, and there should be no reason for instances override it.
Do you mean that "sequence" should only be an artifact of history? Or do you mean that sequence should be given only an applicative requirement and sequenceA should be deprecated? Because using sequenceA in its current state is absolutely a good idea, it sometimes improves the performance of your code (situations where the weaker power of applicative allows less overhead or even some parallelism). Also there are types that sequence straight up does not work for. Try: "sequence [ZipList [1, 2, 3], ZipList [4, 5, 6]]". 
Hrm, did you mean dearth, or wealth ?
I don't understand why you say that you can statically analyze an Arrow computation. Analysis requires examining some information about the computation without running it, and Arrow's methods do not provide any means of doing that. We can use them to *construct* pipelines, but there is no guarantee that we can examine the result: you cannot pattern-match on the result to see if it was constructed using `(&gt;&gt;&gt;)` or `(|||)`, for example. Even `(-&gt;)` and `Kleisli IO` have Arrow instances, and you cannot examine either without running the entire computation. Computations which can be analyzed use a data-based representation. That is, you can pattern-match on those data constructors, either to perform some static analysis or to interpret those constructors as actions and to run them. You don't need Monad, Category, nor Arrow in order to do that. Instead, those type classes provide a common language in which to express common forms of compositions, such as monadic composition, morphism composition, and the composition of wire diagrams. It just so happens that it is not possible to implement some of those compositions with a purely data-based representation. In order to implement `(&gt;&gt;=)` and `arr`, we must compromise and allow part of the representation to be an opaque function, on which we cannot pattern-match. It is still possible to do great things by analyzing the parts of the code on which we can pattern-match, and in this sense `arr` is less limiting than `(&gt;&gt;=)` because it only forces the part it contains to be opaque, and not the entirety of the rest of the computation. But because of the way arrow notation desugars, that opaque part includes all of the connections between the pieces of the graph, which is quite an important piece of information for analyses. Anyway, to summarize: while Arrow is less crippling than Monad in that it forces less of the computation to be opaque, data types which support Arrow need part of their computations to be opaque functions, so Arrow is not the solution to the opaqueness of functions. Representing actions as data is the solution.
Lenses and traversals and a few other things came into being in a more ad-hoc way iirc. Prisms I think did arise from looking at lenses in some categorical sense and taking a dual -- but it was the second or third try at it. And I might be wrong on the details, but I think that earlier categorical setting wasn't then current preferred profunctor notion. So yeah, category theory can serve as a guiding framework for posing very general thorny problems, but its no substitute for a lot of concrete insight (only a compliment), and even when it serves as a guide well, there are lots of dead-ends to chase down...
I very deliberately said "you can statically analyse an arrow computation without running any *actions*". That is, you can know which actions will be run by an arrow computation without actually having to run any of them. 
Woops! Thanks. Comment edited!
I mixed up groupoids and semigroupoids, sorry! In light of that, I'm now totally on board with having `.` mean semigroupoid composition. EDIT: Also, awesome with naming that makes sense like that, and thanks for explaining it to me.
That is a really nice presentation of how Haskell's types interact with a non recursive y combinator. Insisting on the isomorphism newtype Rec a = In (Rec a -&gt; a) Is awesome, I didn't know that would work. Further, you gave an implementation of the only important lambda term I thought Haskell didn't cover. So my question is essentially answered I think. A deep answer to my question probably requires I understand Hindley-Milner. As an aside, to help explain what motivated my question. When trying to solve a problem, like finding the diagonal of a room, if one isn't given enough information (height and width, but not depth) it can feel like there are multiple answers to the same question. Likewise, there are lots of type systems to choose between. I'm trying to make sure I understand what the variables are, and that all of them are accounted for. Your answer amounts to saying, that the Hindley-Milner "sweet spot" locks the type system into a very narrow band. Making the type system essentially unique. I can live with this. I'll follow up with your linked references. Thanks again for the very nice answers. 
&gt; Have somebody tried using other semantics? I don't see why. Denotational semantics are a perfect fit for functional programming languages. &gt; Second question, could someone comment how is FRP related to Control theory? As far as I understand, control theory is a mathematical theory on how to best control physical systems, e.g. "how to design knobs so that good things happen when you turn them". The main difficulty lies in that the systems reacts non-linearly or in an otherwise complicated way to your input, and getting the behavior you want is not easy. On the other hand, FRP is a way to better write interactive programs. The only commonality with Control Theory is that it uses the same notion of time-varying values, but other than that, the goals and methods of these disciplines are quite different. &gt; Or point me towards resources I can use to write about them. I am collecting some introductory material here: https://github.com/HeinrichApfelmus/frp-guides
the difference between `sequence` and `sequenceA` is determined not by the effect type, but by the traversable type; they should *literally be the same*. other than the fact that they have different constraints (ie, you have to be aware that you need `sequenceA` to use `ZipList` as your effect to type check at all), they will do the same things. 
wuestions? :) Fix the title please.
Och indeed, you can give up sorting the tail after you've sorted the prefix. You still have to force the whole list though. "Absolutely crucial" and "disastrous" are over selling things though - I'd settle for calling it "a nice trick".
Have you compared with buffer-builder? This library looks like a subset with perhaps identical performance characteristics. https://github.com/chadaustin/buffer-builder
What about take 5 . quicksort ?
I believe Arrows are analyzable in the following modest sense: if you have a bunch of arrows where each one has a monoidal value attached to it (perhaps indicating the resources the arrow will consume or something like that), when you build a complex circuit you can get a "monoidal summary" of the whole circuit without having to execute it. I think this is what the [Static](http://hackage.haskell.org/package/arrows-0.4.4.1/docs/Control-Arrow-Transformer-Static.html) arrow transformer does. I'm [playing a bit](https://github.com/danidiaz/plan-applicative) with the idea myself.
For those who are new to the community: the title "efficiently build a bytestring from smaller chunks" is a bit provocative. Because that was exactly the goal of Jasper's classic [blaze-builder](http://hackage.haskell.org/package/blaze-builder) library. The goal was achieved so well that blaze builders have now been incorporated into the [bytestring](http://hackage.haskell.org/package/bytestring) library itself as just "builders". It's one of the classic Haskell success stories. So I think what the author is really trying to say is: "even more efficiently than builders, but at a cost". Given that context, it would be great to know more about the costs and trade-offs. One cost is thread safety; that is a very large cost indeed. Also, builders come with a lot of knobs to tweak to improve performance and memory usage for specific use cases, in `Data.ByteString.Builder.Extra` and `Data.ByteString.Builder.Prim`. Since superbuffer requires a "smart choice" of buffer size, it seems that meaningful benchmarking against builders should also include some smart tweaking on the builders side. Thanks for sharing this. Bytestrings are often critical to the performance of Haskell programs, so exploring this space is always valuable.
Yup!
Exactly. It's somewhat explained [here](http://www.mathcelebrity.com/modexp.php) under successive squaring. 
&gt; Haskellers take the benefits of laziness for granted but aren't shy of maligning it. I like this way of looking at it. It's generally *hugely* beneficial for compositionality for pure code (stream fusion, etc.), but in *some very important cases* (mostly involving side effects) it's absolutely disastrous. I think the win-win scenario here would perhaps be (lazy/strict)-polymorphism.
Having a `Lazy a` family of types doesn't really get you everything you want. If `[a]` is strict, `[Lazy a]` is still spine-strict, e.g. You also need to have both least-fixed-point (strict / Mu) and greatest-fixed-point (lazy / Nu) versions of all recursive (and nested) types. If `[a]` is strict, then `[]` is `Mu ListF`, and Haskell-style lazy lists of lazy values are `Nu ListF (Lazy a)`, not `[Lazy a]` or `Lazy [a]` or even `Lazy [Lazy a]`.
I have to say: I tried working through the original version when I was (more) novice, and I ended up hopelessly lost. Maybe it's just that I'm a bit more comfortable with intermediate Haskell concepts (e.g., transformers), but I felt far more comfortable with what I saw in your version. I'm excited to sit down and work through it.
Aren't they originally from universal algebra?
I believe they are using "persistent" in contrast to "ephemeral", so it has to do with non-linear usage patterns for structures: retaining access to unmodified copies of a structure, or even modifying the same copy in multiple ways and getting multiple copies. Immutability is sufficient but not necessary for persistence. Technically, a queue implemented as a pair of lists is persistent, but using it persistently means the time bound for dequeuing is linear, instead of amortized constant time.
Also, direct use of diagrams is not a good fit for the fast and cheap real-time rendering needed for a screen saver. But diagrams is useful for drawing pre-rendered things which you then hard-code into the screen saver.
Note that the pragma does *not* make all function calls strict, rather, it makes all *bindings* strict. This means that new functions you define will be strict, but existing lazy functions from the prelude will be unaffected. To make a strict call (the argument is evaluated before the function is called), use `$!`. In both cases "strict" means that the argument is evaluated to weak head normal form, which is still not quite the same thing as in a strict language unless the argument is itself using `$!` everywhere.
Polarized sequent caculi are the future.
I understood about 50% of the words of that sentence :). Well, I *think* I've had a *very* superficial introduction to the basics of sequent calculus per Qi/Shen, but I don't have any practical experience. What's the connection to (lazy/strict)-polymorphism?
True. But that isn't the reason for the word "old" in the name. The old-locale library was written as a quick nearly-brain-dead hack, just enough to satisfy the minimal needs of the `old-time` library, with the idea that someone would write a real locale library, real soon now. That was... uh... how many years ago?
&gt; mostly involving side effects Which problem are you referring to? &gt; I think the win-win scenario here would perhaps be (lazy/strict)-polymorphism. Like [this](http://h2.jaguarpaw.co.uk/posts/strictness-in-types/)?
Interesting. I think the example in Section 2.1 can be implemented in Haskell with [indexed](https://hackage.haskell.org/package/ixmonad) or [graded](https://hackage.haskell.org/package/effect-monad) monads. I wonder if the example in Section 2.2 can be implemented with GADTs.
I asked /r/Haskell about this a while back and there was a lot of discussion: https://www.reddit.com/r/haskell/comments/36s0ii/how_do_we_all_feel_about_laziness/ I remain largely uncertain that laziness is worth it.
Do you have any practical examples? I'm finding it harder and harder to motivate laziness to non-Haskellers over time.
(From my other comment, this is only relevant if GHC could do per-thread heaps, but...) ideally you would register a handler when you disable GC, and the runtime would notify you and throw `ThreadKilled` (if it had to do a gc to finish cleaning up, that's fine, but it might be possible to configure it so it killed the thread at `heapSize - 150kB` or something like that). 
Laziness is the hair shirt that we wear to remain pure.
My gawd. I'm going to try to work through that paper, especially since the introductory paragraph is pretty... well, let's just call it "an ambitious project" as my friend Stephen Fry would say. Thanks!
Incredibly. So, we all know about space leaks, right? Well, there's a dual to space leaks- time leaks. Time leaks are when our programs spend time computing things that don't need to be computed- or the complexity introduced into our programs to avoid doing unnecessary computations. Let me give you an example. Let's say you need code to evaluate a function, and the derivative of the function. Now, these two computations entail a lot of common computation- so if you need both, it's definitely worthwhile to compute both simultaneously. But sometimes you only need one, and sometimes you only need the other. With lazy evaluation, the API is simple- it returns a tuple of (value, derivative), and the caller forces whichever values they need. With strict evaluation, we either pay a performance cost- computing values we don't need- or we pay a penalty in code and API complexity. Now we need to pass flags in to say which values we need, and the code itself needs to be checking all over the place as to whether this or that computation needs to be done, etc. In the extreme, we end up with 3 complete copies of the code, all of which need to be written, debugged, and maintained. And the caller code is more complicated as well- it needs to decide up front what parts of the result it needs. It's hard for to me to overstate how big of a deal this is. I know- I spent decades programming classic imperative-strict languages like Java and C, and you don't notice how much of a penalty you're paying for eager evaluation. A fish doesn't notice water- that's just how things are. Bust trust me- the cost is enormous, and much larger than you (and probably I) are thinking it is.
Purity is the silk robe that we wear to remain lazy.
How does xscreensaver interact with whatever renders the actual graphics? Is this just an exec call?
&gt; Here's Where?
The difference is: "time leaks" are obvious and easy to diagnose whereas space leaks are the sneaky silent killer. Given the choice, I'd take "time leaks" any day of the week.
You're may already be aware of this, but there's a [proposal for UnliftedDataTypes](https://ghc.haskell.org/trac/ghc/wiki/UnliftedDataTypes). If you skip down to the [Fully Alternate Proposal](https://ghc.haskell.org/trac/ghc/wiki/UnliftedDataTypes#Fullyalternateproposal) section, it's almost exactly what you describe. Unfortunately, there is an unaddressed problem involving a huge increase in generated object code. The proposal sums this up with an open question: &gt; Is there a way to reduce duplication in object code? If we can't find a "yes" answer, this problem may kill the idea. But maybe someone can figure it out.
Only if `ArrowChoice` isn't used. Once `ArrowChoice` is in the mix, you still know which arrow actions *may* be used, but you don't know which ones *will* be used.
Assuming you're using the latest platform, make sure to follow the instructions in step 3 of the windows platform page: https://www.haskell.org/platform/windows.html &gt; Modify your cabal config file (you can verify the location by running "cabal user-config init") to contain the following lines: extra-prog-path: C:\Program Files\Haskell Platform\8.0.1\msys\usr\bin extra-lib-dirs: C:\Program Files\Haskell Platform\8.0.1\mingw\lib extra-include-dirs: C:\Program Files\Haskell Platform\8.0.1\mingw\include 
That doesn't sound right, `0 / 0` should be `NaN`, not zero.
If you mean to control effects `arr` should be no harm to you because it does not have any effects. It's an zero-effect wrapper around pure function. Same, actually, can be said to `pure`. Even with `Monad` you already have type-level limitation on effects. But if you defined some script in monadic embedded interpreter, you cannot make much more to it than merely running it because it basically evaluates during running. With `Arrow`, you can inspect the script without running.
A simpler practical example is basically any form of dynamic programming where there isn't a nicely regular structure to the order you hit the subproblems in (e.g., because you don't hit all the subproblems and you only know which ones you hit by the time you're done).
Same thing: is O(n + (min n 5) * log (min n 5)) if lazy, but O(n * log n) if eager. (for the average case. For worst-case, replace x*log(x) with x^2 in both expressions.)
Not at all. I would have preferred Haskell to be optionally-lazy instead of lazy by default. More advanced/modern languages (Agda and Idris) are not lazy. Most excellent.
`mapM` ~~with `MaybeT`~~.
Do you have any resources where I could read about these `Mu` and `Nu` things? Your comment is quite foreign to me.
Laziness lets you always have worst case O(n) runtime for that expression. It would not take O(n\^2) comparisons even in the worst case since you're only taking a constant number of elements.
obligatory: http://augustss.blogspot.com.tr/2011/05/more-points-for-lazy-evaluation-in.html 
I find that very hard to believe. Discovering where your program is wasting time takes incredible amounts of effort and sophisticated tooling. It just so happens that both of those have been invested in this problem for conventional programming, so they have an advantage. Time leaks can be very sneaky too: you don't realize your code is too slow until long after it's in production and not running fast enough.
https://en.wikibooks.org/wiki/Haskell/Fix_and_recursion mentions Mu and Nu, and their definitions are on hackage in `recursion-schemes` (and possibly other packages). Have you done much with limits? I tend to think of a least fixed point as a lim inf and a greatest fixed point as a lim sup. In Haskell (due to laziness?), the least fixed point and greatest fixed point coincide, so we generally just talk about *the* fixed point (`fix` for functions, `Fix` for functors), but when you are start paying attention to inductive vs. coinductive and data vs. codata, it helps to separate them. Mu is a somewhat common name for the least fixed point and Nu is a somewhat common name for the greatest fixed point.
Are there resources for learning anything "competent" and up? A lot of the beginner stuff feels like "idiomatic" programming.
Yea the tardis monad is not a particularly compelling argument. It's just cool that it's possible.
Surely in a strict language emulating the mechanics of laziness is easier than duplicating the code 3 times?
Using `perf` with [flame graphs](http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html), discovering where you're spending time is trivial. Orders of magnitude easier than debugging space leaks.
This doesn't have anything to do with laziness, does it? Bind's rhs is a function, so would not be evaluated once `Nothing` is encountered in either a strict or lazy language?
Oleg gives an interesting insight about the potential of a native MetaOCaml and that there are some benefits of using ocamlopt in the [What about the native MetaOCaml](http://okmij.org/ftp/ML/MetaOCaml.html#native) section. And yes, in strymonas we use `ocamlopt` as currently this is the only way to compile the generated code into more efficient native code (btw, interestingly, with `ocamlopt` you can play around with the CLI options).
Gloss is pretty simple, and there are bunch of really cool (some are interactive / animated) demo projects here: https://hackage.haskell.org/package/gloss-examples. It also uses opengl under the hood, so I'd imagine that makes it performance friendly.
I've looked at the library but I did not go for it because the 'BufferBuilder' doesn't allow any contained IO which I really wanted to make sure the Haskell ByteString can get Garbage-Collected as soon as possible. I think you could also add a MonadIO instance? I also added some benchmarks that indicate that superbuffer is usually faster.
"next to useless" sounds a bit of overstatement. you can't quantify how much exactly, yes, but you can certainly see that there's some improvement (or that it's promising). Also just comparing the bounds and providing roughly the same turboboost behavior happening between the 2 different tests there's 4ms between the worst case of one and the best case of another; (it is was 4ns, I would certainly not assume the same thing)
Sort of. But no one uses Purescript or Idris solely because they are strict. I myself never used these languages in a real project and I wonder how it feels (in terms of strictness) to program in such languages.
You can't edit titles.
I tried Coq too, slowly working my way through Software Foundations, and I also was a bit shocked by complexity of proofs for "simple" things. On the other hand, those things are fundamental, so some attention to details is justified for me. Later in the book you get to use ready proofs and library theorems to prove quite interesting things, so I think the pain of the first chapters pays off.
I wish they would revive supercompilation, as this could handle many cases without using manual rewrite rules. EDIT: or an external program that could generate rewrite rules/fusion by running a supercompilator on your library. 
I wonder how hard it is to actually hit these cases while using the Strict pragma though. The pragma is no way around understanding how laziness in Haskell works, but for someone not taking advantage of laziness I assume it would reduce the chance of hitting unexpected behaviour caused by laziness a lot. 
OMG! you're so right! I missed that last step, it compiled right away! I could run the samples and all that! Thank you so much! I can continue my app then ;)
Well, same can be said about memory leaks, but that's not what we're talking about. There are plenty instruments for detecting memory leaks due to lazy evaluation, but seems there are none to find excessive computation which is not actually needed.
I wonder if we could push for this and/or improve the patch. Can you link to the patch?
You see top time consumers, and dig inside, see what work they do at a glance via seeing what functions they call. If something unnecessary is done it is often visible at that level already. If not, examining the annotated assembly and seeing cache misses - can help determine whether work is needed or not. 
&gt; thorough analyses of how much time is spent I don't think that evidence will prove laziness is very good. Let's have a reality check... Reading a value from memory is generally *much* slower than simply recomputing it, at least for simple computations. That's not even accounting for the cost of allocating a thunk (which requires a whole bunch of computations in itself), waiting for your parameters to go stale and get evicted from cache, then gathering all those back from memory so you can do the calculation after all, then writing that data into the thunk (who said pure function computations don't do memory mutation?). In fact CPUs have chosen the strategy of being stricter than strict. They perform computations speculatively as part of the pipeline, to try to ensure the result is available immediately when needed, even if that means many of those results are discarded unused. As the Pentium 4 proved, this can go too far, but still - certainly at very low levels, the performance trade-offs are heavily in favor of doing computations just-in-case, and not in favor of deferring them in hopes of not needing to do them. So in a direct comparison of pervasive laziness against pervasive strictness, reality is that laziness is going to lose. It leaks time all over the place, in huge (though constant) factors. For the most part, the fact that lazy evaluation is asymptotically at least as good as strict evaluation is irrelevant next to that - the constant factor costs are so huge, you'll never have a big enough instance of the problem for asymptotics to win out. Luckily, we don't have pervasive lazy evaluation. We have a compiler that does strictness analysis, and where it can prove strict and lazy evaluation equivalent, it uses strict evaluation. We have pervasive lazy semantics, but without (most of) the performance price. This works well, AFAICT, in large part because a key default hypothesis generally works - if the compiler can neither prove nor disprove equivalence of strict and lazy semantics, odds are that the computation is large enough that using lazy evaluation is a sane choice. What we really have is a hierarchy of levels of complexity. At the lowest levels, the optimizer *mostly* protects you from the costs of the computation model provided by your language. You just work with variables, the compiler figures out what to put in registers, etc etc. With laziness in the mix, GHC has to work a bit harder at this level, but dumping the grunt work on the compiler is the right thing anyway. A bit higher than that, you reap the rewards of that computation model. A bit higher than that, common idioms protect you from some more issues with that model, though they're so basic they're usually considered a part of the model anyway, at least until some unaware newbie gets some "what did you expect when you represented that video as a list of lists of lists of tuples of colour values" abuse. And higher still, you need that one person who really understands what's going on at all levels, nasty edge cases included, and how to handle those issues - but luckily you don't need (to be) that person very often. And that applies whatever language you're using. **EDIT** I should also say for sufficiently small computations, you don't need laziness. If the optimizer inlines the call of the function, it will then neatly discard all the unneeded parts of the computation anyway. These days, "sufficiently small" is actually pretty big - I have no idea how big/small the overlap with scales where laziness pays off is, though. I imagine it's very good for strictness analysis, allowing the equivalence to be proven in more cases. Actually, I suspect some of the advantages seen by bhurt42 and others are psychological. In many cases, they might get the same advantage in C if only they'd allow themselves to get those rewards, but the fear of doing unnecessary work is too strong. The optimizer can eliminate a lot of that work anyway. And even when it can't, often doing "unnecessary" work is the best option. 
No, it isn't, it depends on a particular sort in question IIRC (TBQH, I can't remember all the details about this since I first learned of it long ago :) And yes, it will have worse constant factors than a manually unrolled version, almost surely. The solution can also be written as the composition of other functions, such as the sibling comment implies, with varying degrees of the same "laziness" principle being exploited. In any case -- this is part of the reason why I said it's mostly a trivial example; a trick, a red herring. It's not actually a meaningful use case to be honest, but it's simple and gets the point across in a few seconds, so I can see why people use it. The example perfectly illustrates what is allowed; it's just not a very good example of what you'd actually benefit from in practice. What I'm saying is: yes, `minimum = head . sort` isn't the optimal definition, and it's not even a great example, I think, of what people would benefit most from. But it does illustrate the style and principle of what's possible ("compositional programming"), fairly concisely, and why that would be more difficult to approximate in a strict language.
Sorry. It was really late and I decided to open my mouth right before I fell asleep. What I was actually trying to get at was that `mapM f == sequence . map f`, and `map` is lazy. Here is a case where we take two very useful combinators and we can compose them into another good one with no worries.
How do you check which Haskell function is using the most memory? Heck, how do you even define that?
He's probably referring to this patch, which is from this paper: https://github.com/ghc/ghc/compare/local-gc http://community.haskell.org/~simonmar/local-gc.pdf It wasn't merged because it was immensely complex as a feature, when compared to its gains. It wasn't good enough. And it also does not solve the problem of the old generation being stop-the-world: long lived data still has the capability to stop your program and grind it to a halt. The feature only does concurrent collection of the young generation. It's also unlikely the patch will do you any good in isolation, as it has almost surely bitrotted horribly by now, considering GHC's runtime has added numerous new features in the past 5 years. The real solution to this is to equip GHC with a true, fully incremental and concurrent garbage collector. That's a hard problem, but it isn't unsolvable -- This patch didn't really solve it, though. Lobbying for that would be a good thing! 
While we're airing our unpopular opinions. * Making computer systems more like biological systems is a surefire way to understand them _less_, not _more_ * Object oriented programming is either syntactic sugar over half-assed abstract data types (in the Java sense) or a well-meaning but entirely stupid attempt to manage state by adding _concurrency_ (in the Alan Kay, message-passing sense), which as we all know makes state _harder_ to manage not easier. * Alan Kay hasn't kept up to date with our understanding of programming languages, and his initial contributions are of questionable benefit. I don't think he's much of an authority on software engineering or programming language design. Regarding his opinion on monads, I agree that monolithic state passing is a kludge, but monads themselves are valuable even outside the state monad. And, the benefits of programming in a model without mutable state can be seen the moment you try to prove (or even just specify) anything about a nontrivial program.
This all makes good sense. Any ideas where such a volunteer might come from?
That is if you look at `if` as a function not as a language construct. For instance, you wouldn't say that Java's `while` is in any way related to laziness, yet it as well only executes some code under a condition.
The example in 2.2 can be implemented with McBride's indexed monad. EDIT: Here's how. {-# language DataKinds #-} {-# language LambdaCase #-} {-# language GADTs #-} {-# language KindSignatures #-} {-# language PolyKinds #-} {-# language RankNTypes #-} {-# language RebindableSyntax #-} {-# language TypeOperators #-} import Data.Kind (Type) import Prelude hiding ((&gt;&gt;=), return) return = Pure (&gt;&gt;=) = Bind data (a := i) j where V :: a -&gt; (a := i) i data Is a b where Refl :: Is a a data DoorState = DoorOpen | DoorClosed data DoorResult :: DoorState -&gt; Type where Jammed :: DoorResult DoorClosed OK :: DoorResult DoorOpen data DoorCmd :: DoorState -&gt; (DoorState -&gt; Type) -&gt; Type where Open :: DoorCmd DoorClosed DoorResult Close :: DoorCmd DoorOpen (Is DoorClosed) RingBell :: DoorCmd DoorClosed (Is DoorClosed) Pure :: a -&gt; DoorCmd s (a := s) Bind :: DoorCmd i t -&gt; (forall j. t j -&gt; DoorCmd j u) -&gt; DoorCmd i u doorProg :: DoorCmd DoorClosed (Bool := DoorClosed) doorProg = do Refl &lt;- RingBell Open &gt;&gt;= \case OK -&gt; do Refl &lt;- Close return True Jammed -&gt; return False 
A small example: recipN :: Nat -&gt; Nat recipN n = 1 `div` n This function has "two" fixed points. It is strict, so `undefined` is a fixed point. But, `recipN 1 = 1` so 1 is also a fixed point. `fix recipN` is `undefined` because `fix` actually calculates the least-defined fixed point. I like to think of `fix` repeatedly applying the given function starting with an inital value of `error "Loop detected."` as if `fix f = last . iterate f $ error "Loop detected."` That syntax doesn't work, but the internals are sort of like that. `1` would be the most-defined fixed point of `recipN`. I don't think there's a good way to ask for that. It's a "different metric", but you can think of least fixed points and greatest fixed points of functors in the same way.
I like the mirroring in the two comments, but I think it reflects something of a "new norm". Originally, purity was the hair shirt we wore so we could write useful programs in a lazy language.
No problem. One less thing to learn :-)
Exactly! Thanks for adding the link :)
I've read the whole book and done all the exercises, it was a great learning experience. Now, I am still fairly new to programming and coming from a non-CS background this book has been a huge help for me not only with learning Haskell, but with getting more exposed to programming in general. I highly recommend it. Before the book I've tried CIS194 and LYAH, didn't get too far though. Haskellbook is my go-to for learning Haskell or getting familiarized with Functional Programming. I've read several programming books before it and nothing really comes close. The exercises, explanations, everything just clicks together really well. IMO, Haskellbook will be (already is?) the best way to learn Haskell.
You can get most of the upsides of laziness without the downsides by making a few data types lazy (e.g. lazy lists).
&gt; I mean, concurrency is hard Uhhh about that... did you get [that memo](https://www.fpcomplete.com/blog/2016/11/mastering-time-to-market-haskell) on how *Haskell's speed and __easy concurrency__ reduce the cost of development substantially*?
Because I've used GHC profiling and neil's trick, and I used perf with flame graphs. A flame graph gives you great insight about your program in minutes. Every single piece of information there has a clear context (where its costs are accounted) and what its part in the overall execution is. Zooming out or drilling down is easy, all the way down to single assembly instructions or all the way up to "main". GHC memory profiling takes hours and you gain less useful insight. The output is much more opaque. The resulting graphs are often mysterious - why did this piece grow at this time? What are "BYTES"? Who allocated these thunks? Neil Mitchell's methods are great because they finally expose more than zero information that we had earlier about stack use. It's infinitely more information than we had earlier, which is why it's so great. But compared with the profiling tooling we have for imperative languages, it is absurdly weak. 
It will be difficult in any language, though some languages will make it easier (such as Erlang, which can do such things 'trivially' almost). I don't *think* Haskell makes it much more difficult than any other language, inherently. Like usual, the devil is in the details, which is going to manifest in ways like the design of the runtime system and language features and how these all interact.
There are reasons besides space usage to generate code from a meta language like Haskell -- the most important probably being better tooling for things like verification, staging, or compilation, all tuned to the domain. Even if I could run general purpose Haskell on my microcontroller, it may very well just be a better option to generate code or use a DSL, if it specifically embodies my domain. This in turn allows many more assumptions, and tools, to become viable, like optimization or formal verification methods. &gt; I find laziness is actually limiting the use of Haskell to a niche where the time and space performance are not critical, but the correctness and readability are. I think this sort of complaint is in some ways overblown, and frankly I think it's just not that important at a certain point and fundamentally is an attempt to change Haskell into something it isn't. Just use something else. We don't need to turn Haskell into "The perfect programming language for every possible scenario, by literally trying to do everything" and we shouldn't try to. We also don't need to pervert Haskell, changing its fundamental design decisions, in order to appeal to people -- we should happily suggest to people that, sometimes, Haskell is not a good choice. We're already almost the C++ of FP languages, with the intense surface area the language has reached. You have to pick your battles and sometimes just say: this isn't a direction or goal the language should care about. Some people seem to think that the only measure of "is it efficient" is "can it run my 60fps game perfectly all the time with no hitches ever and it always does what I expect and have all the exact runtime features I specifically need and make my coffee" and are invariably disappointed when it doesn't, and conclude well, clearly, they just don't care. I don't know why people have these expectations honestly. Or like, when someone asks "can I run it on my microcontroller" and people are like "no", and conclude well clearly, Haskell developers are just hugely limiting themselves and this is a giant problem. Because... why? Should we even *care* about the use case of running on a microcontroller? We could better focus on making Haskell better at what it does. But frankly, having seen a lot of clients, few, if any, of them are worried about this sort of stuff. None of them ever thought they were going to use Haskell for that scenario, and most have been happily pleased with the performance for the most part, for systems like webapps, API endpoints, data processing systems (think ETL/batch jobs), with perf. having met or exceeded their expectations. Yes, I've spent my time debugging performance issues in Haskell. I've also done it in every other language I've ever written code in. It's maybe more frequent, and it has its downsides. But I'd hardly say its extremely limiting for the kinds of things we're aiming for. Laziness does make all these things harder. Personally, I like it. But aside from my likes, it's also one of the core components of what Haskell is. If you change that, you are fundamentally changing the language in an intense way. Frankly, if Haskell ever stopped being lazy, I'd probably just stop using it or fork it or something, because it's one of the things I like the most about it. I don't abstain from strict languages. I just like Haskell the way it is. I think it would be interesting to talk about functional languages for scenarios like that. I think you could certainly design an interesting language for these sorts of resource-intense, time critical scenarios, and such a language would probably be strict and have a lot of other interesting design decisions. But I don't think that language is Haskell, and attempting to shoehorn Haskell into it, I think, isn't really a valuable use of time or effort.
You can try disabling frequency scaling for the benchmarks which should result in less variance.
I think that `in` after the `let parsed =` line is throwing everything off. You don't need it. The `let ... in` syntax isn't used in `do` notation, it's used in pure expressions. I'm not too sure what your intention is here, but is this what you want? readSlot :: [Int] -&gt; Int -&gt; IO (Int) -- bd: board, p: player -- Asks user for a slot input, "loops" through and asks again if input is invalid readSlot bd p = do putStrLn "Enter a slot position (1-7): " userInput &lt;- getLine let parsed = reads userInput :: [(Int, String)] -- parse input as int if (length parsed == 0) then readSlot' else do let (x, _) = head parsed userInput -- only take first value if multiple int digits passed in if x &gt; 0 &amp;&amp; x &lt; ((numSlot bd) + 1) -- 0 &lt; x &lt; 8 then return x else readSlot' where readSlot' = do putStrLn "Invalid input!" readSlot bd p
 readSlot :: [Int] -&gt; Int -&gt; IO (Int) -- bd: board, p: player -- Asks user for a slot input, "loops" through and asks again if input is invalid readSlot bd p = do putStrLn "Enter a slot position (1-7): " userInput &lt;- getLine let parsed = reads userInput :: [(Int, String)] in -- parse input as int if length parsed == 0 then readSlot' else let (x, _) = head parsed input -- only take first value if multiple int digits passed in in if x &gt; 0 &amp;&amp; x &lt; ((numSlot bd) + 1) -- 0 &lt; x &lt; 8 then return x else readSlot' where readSlot' = do putStrLn "Invalid input!" readSlot bd p 
I dont know why people always speak of replacing IO with really specific patterns, this may be complete in some sense but that doesn't make it always convenient
Thank you for the help. I am still trying to debug, but I'll update the post if I can solve it.
The threshold for lazyness to pay off can be surprisingly low. I had things where decoding a value out of a word like f foo = ... let x = bitshift foo and xor with const ... in case something of ... where x was used in more than halve of the alternatives be more efficient when x was lazy (to my own surprise). I'm still skeptical about lazyness but it doesn't seem to matter much performance wise. I guess ghc created a thunk for x and maybe avoiding that would have made it more performant. But when doing that kind of heavy optimisation one can just check what's faster anyway.
I made a [minesweeper game](https://github.com/basile-henry/haskellsweeper) a while ago in Haskell using ncurses, it might interest you. It's basically just a remake of minesweeper but with a twist, it has an infinite grid! It's probably not the best code to learn about ncurses but if you have any questions about it I'll try my best to help. :-)
There are a few issues with this code that are causing the parse error. I don't know that simply posting 'fixed' code will be helpful in the long-run, though, so I'm going to explain a few more basic things about Haskell syntax. In Haskell, an if-then-else expression isn't exactly like that found in other languages. The form in Haskell is as follows: if condition then expression1 else expression2 The `else` is NOT optional. The whole if-then-else MUST evaluate to a value, and all possible values must be of the same type. This is illegal, and won't typecheck: if condition then "string" else 123 This is also illegal, but this time for syntax reasons: if condition then "string" -- missing else If `condition` evaluates to `False`, then the if-then-else won't evaluate to any value, which is never allowed in Haskell. As a side-note, Haskell is whitespace-sensitive, and indentation is very important at times. The convention in Haskell , when using an if-then-else, is to align the `then` and `else`. This is enforced by the compiler, and should be followed at all times. `let` is a peculiar area of Haskell syntax, made more complicated by `do` notation, because of the way that `do` notation is [desugared](https://en.wikipedia.org/wiki/Syntactic_sugar). When we're outside of a `do` block, we use `let ... in ...`, but inside a `do` block, it's best to omit the `in`, and simply continue on in the `do` block. For example: main = do let x = "foo" putStrLn x instead of: main = do let x = "foo" in putStrLn x Alright, on to the code. On the 10th line: `else let (x, _) = head parsed input`, we find an intersection of these two peculiarities of Haskell. Here, `let` is used as if it's in a `do` block, that is, without the `in`. However, the `else` expects an expression in this position, that is, an entire, whole, unbroken expression. Examples of such whole expressions are let-in expressions, `do` blocks, and simple values or function calls. The type of the `else` branch MUST be the same as the type of the `then` branch. The type of the `then` branch is `IO Int`. (If this is confusing, try putting `readSlot'` into it's own top-level function, and using ghci's `:t` command to see for yourself.) The type of the `else` branch, as it is, is... ? `let (x, _) = head parsed input`, by itself, is not valid syntax. `let`, without a matching `in`, must be used in a `do` block. So the answer is, start a new `do` block in the `else` branch: if length parsed == 0 then readSlot' else do let (x, _) = head parsed input if x &gt; 0 &amp;&amp; x &lt; ((numSlot bd) + 1) then return x else readSlot' where readSlot' = do putStrLn "Invalid input!" readSlot bd p If this is too long-winded, or you have any further questions, please don't hesitate to let me know.
duplicate: [https://www.reddit.com/r/haskell/comments/5fq5dn/functional_programming_and_haskell_computerphile/](https://www.reddit.com/r/haskell/comments/5fq5dn/functional_programming_and_haskell_computerphile/)
&gt;If the game you plan on building isn't incredibly visual you should be fine (you may not need Accelerate or Repa even). Does Repa compile for ARM though? 
But they didn't say it was for the reason, it would just be weird to have a serious research language associated with a delightfully weird piece of art unless that was entirely purposeful and reflected something of the nature of the project and it's community. . 
just to be clear for OP the library doesn't -- just the tech demo app :) but I'd be happy if it was useful! 
Makes me sad, but this stuff is important. I hope GHC takes note.
A benefit is code organization. You put your state transitions on one place, and the code with effects in another. This could make things easier to understand. Another benefit is that, if you specified the state transitions well, you do not need to write any test (against the specification I mean).
[Link](https://np.reddit.com/r/scala/comments/5fmyep/do_you_like_scala_give_haskell_a_try_repost_from/) to the /r/scala discussion.
GHC's garbage collector is high latency. In other breaking news, the sky is blue.
Go has a team of full-time core developers paid by Google to improve go (the garbage collector was not concurrent in 1.0) and GHC? 
Absolutely. Go probably has more people working (paid) full time on the garbage collector alone than there are people working full time on GHC. May I add, that a significant amount of the innovative work that goes into GHC is done my PhD students. For an improved GC for GHC PhD students are probably not a good fit (they need to do innovative research). What you need is a small team (probably no more than 2 or 3) of full time paid professional engineers.
Why switch from one niche language to another and risk stumbling into other problems (how good is OCaml's concurrency?)? Taking their requirements into consideration, I think they made a good choice by going with Go.
On the other hand, there are companies that use Haskell (e.g. certain banks). Some of them could be interested in using Haskell in low-latency applications. Some of them are probably willing to contribute financially if it helps.
Hey, I'm the author of the blog post. I spoke in more detail about our move from Haskell to Go in [this talk at CUFP](https://www.youtube.com/watch?v=ycf3LG1A8Hg&amp;t=1060s). I go into our general experience with Haskell in a commercial setting, explain why our use case was the worst case for GHC's GC and compare it to Go's approach. I'd like to mention that I still love Haskell, and think it can be an excellent choice for many use-cases. Just not when you have low latency requirements and a large working set. At least not at the moment, or you don't mind writing C/using the FFI.
&gt; people working full time on GHC Is this number zero?
Would you consider giving it another go with the go 1.8 beta? Latency is suppose to be even lower in that version.
Why not just a simple function? addTuple :: (Num a, Num b) =&gt; (a, b) -&gt; (a, b) -&gt; (a, b) addTuple (x1, y1) (x2, y2) = (x1 + x2, y1 + y2)
&gt; thus I need a library which can draw to window which it did not created itself. Which is a no-brainer with X11. Remember that X11 is a networked protocol and any X connection may operate on any XID. This also works seamlessly with OpenGL. One thing to be aware of when using OpenGL is, that you have to set the visual/FBConfig. The usual approach is to simply create a subwindow that's a child to the XSCREENSAVER_WINDOW and operate on that one. A desireable sideeffect of that is, that when your screensaver effect process dies, it takes its connection with it which also effects all XIDs created on that connection. Of course the nice thing is to do a proper window cleanup / connection teardown upon receiving SIGTERM.
Why didn't you go the C/FFI path? You could move the large working set to manual memory management and have the best of both worlds?
Remember that in Haskell, the `class` keyword does not mean the same thing as in object-oriented programming. If you want to say that the type of `f x y = x + y` is `f :: Int -&gt; Int -&gt; Int`, the Haskell syntax for saying so is *not* to write a class containing the type signature and an interface containing the implementation. Instead, simply write the type signature and the implementation next to each other, on the top level, outside of any class or instance: f :: Int -&gt; Int -&gt; Int f x y = x + y 
Apparently Go is more attractive to Haskellers than I had judged. To me it's like chalk and cheese.
Since these are well-known tradeoffs between small/large heap and short/long living objects, is it possible that RTS could determine these characteristics at run-time and adjust GC strategy on the fly? Since there is no universal solution, I think GHC should implement various strategies and switch to the right one for a given program automatically. 
...unless it's a bottomless pit!
Kindly redirecting to https://www.reddit.com/r/haskell/comments/5g8nd0/golangs_realtime_gc_in_theory_and_practice_from/daqfqvs/
I plan on running it with the HEAD of the compiler ASAP. Apparently. One of the bugs that I thought was causing the long pauses was fixed: https://github.com/golang/go/issues/16528. However, one of the Go developers ran the benchmark on the latest version of the compiler, and said there was still an issue: https://github.com/golang/go/issues/18155
The first twenty or so minutes is an intro to Calculus. If you've already taken lots of Calculus, you might want to start the video at 19:12. (https://www.youtube.com/watch?v=q1DUKEOUoxA&amp;feature=youtu.be&amp;t=1153)
This is what languages like Java do. The runtime has a number of GC algorithms available, and it will switch to whatever it decides is "most effective" given the runtime characteristics of the program. See this thread for more discussion: https://www.reddit.com/r/programming/comments/5fyhjb/golangs_realtime_gc_in_theory_and_practice_pusher/daof47v/
Hey thanks for that note and for making the talk, which was at the intersection of two things I want to learn more about (AD and Haskell!).
An open mind, and the ability to "simplify" lines of code on paper. In Haskell, you step through execution by simplifying, line after line, as in high school algebra. This is called "equational reasoning" in the Haskell community. Gabriel Gonzalez gives an intro to equational reasoning, but it assumes some Haskell familiarity, but read just the firt section on equality and substitution in Haskell: (http://www.haskellforall.com/2013/12/equational-reasoning.html) Finally, don't be too proud to read through and follow along with Learn You a Haskell for Great Good: (http://learnyouahaskell.com/). It might seem cutesy and elementary, but Miran has a gift for explaining difficult concepts in a natural, easy-to-grasp way.
GHC already does this, it has like 3 garbage collection algorithms. e.g. I believe Mark-Sweep is used in low-heap-usage scenarios (less than 1/3rd of the heap in use). My only concern with this approach, IMO, is that it has, maybe partially, lead to a proliferation of knobs which, in turn, are complex to understand and lead to "over fitting". Changes in your program can introduce very different runtime characteristics when core behavior like this just changes on the fly. Suddenly your specific tweaks/tuning becomes worthless (the compiler did/did not do something it should have) or even actively hurts performance. If you over fit, you have to constantly re-fit for every change. It's not necessarily a bad thing, though. But it's complex and introduces a lot of variables. But it's worthy to note that Go only has 1 GC algorithm, and even more than that, Go literally only features 1 tunable setting for its entire GC (which has to do with the ratio of allocated memory to starting heap, IIRC?) That's it. There's no fixed heap sizes or promotion options or anything like that. They've aimed to make this GC scale "up and down" easily and predictably. That's good for users, and it's good for the implementation because it tries to stay very focused at doing only one thing. Yes, they've done a throughput trade off by aiming for latency. Some people would probably really dislike that. But because it's predictable, and it scales in a much more obvious manner -- if you want to recover throughput, and decrease your number of collections: just add some more memory and cores. You can recover that 10% or 20% or whatever throughput deficiency by just spending a bit of money. That technique -- spend some money -- will continue to work as you want *more* throughput later on (more customers, more data). You won't bottleneck as hard from just doubling your specs. That's a lot nicer than knowing "At some point, no amount of memory and cores will help because the latency becomes untenable".
Re Raft impl: you could integrate a C/C++/Rust/... Raft implementation too via ffi :-)
I'm not the OP, but it sounds they weren't willing to pay the maintenance overhead for two languages.
I suggest you don't worry with that right now. As others have said, it is not a prerequisite for a beginner.
The other install options listed in that page are no less "official". You can, for instance, download [the tarball with the Linux binary](https://docs.haskellstack.org/en/stable/install_and_upgrade/#linux), extract it anywhere in your home directory and begin to use Stack.
It sounds like in an ideal world for you ghc would have chosen a similar path towards lower latency?
I realize now that the link I gave you is indeed about profiling the behavior of memory allocation and usage, rather than a way to deduce how much space a particular function is occupying. Thanks for pointing that out. 
I know too little about Kay's theoretical contributions to comment about your opinion of them, but: &gt; Making computer systems more like biological systems is a surefire way to understand them less, not more I agree, but I think you're taking him too literally. He's not talking about (I think) building airplanes with flapping wings, but about looking at biology to see how it deals with some problems so that we can get inspiration. &gt; either syntactic sugar over half-assed abstract data types (in the Java sense) You're forgetting the modularity, which is similar to modules in ML. Modularity is a much more important tenant of OOP than modeling data, and that part, at least, has proven to work very well (we've been able to write much larger, more elaborate software with OOP than with procedural programming). &gt; entirely stupid attempt to manage state by adding concurrency I don't think concurrency was added to help manage state in general. Concurrency was added to the programming model either because it exists in the domain (robot AI: Hewitt's actors, GUI: Kay's Smalltalk) and/or is necessitated by it (fault tolerant distribution). &gt; Regarding his opinion on monads, I agree that monolithic state passing is a kludge, but monads themselves are valuable even outside the state monad. I am almost certain he was *only* referring to the use of monads to manage state because managing state is absolutely necessary in pretty much every program. Anything else monads (as a programming abstraction) are useful for are either not nearly as important or pervasive and/or fairly well contained. &gt; the benefits of programming in a model without mutable state can be seen the moment you try to prove (or even just specify) anything about a nontrivial program. But Kay is talking about a logical time model, as he says, "don't use your CPU as your clock". Proving nontrivial properties of *logical* clock programs -- even concurrent and distributed ones -- with mutable state is commonly done, and done well (as well as anything else, at least). Formally verifying such systems is far more pervasive than formally verifying pure functional programs (although, TBF, that's because synchronous programs themselves are more common than pure functional ones, they are used in domains where formal verification is crucial, and the verification itself is rarely deductive but usually fully automated).
Did you notice Java was even worse on their benchmark? Does that mean Java is not ready for production? No. It just means that there are use cases where Haskell or Java will not be a good fit. [Here is another such case](https://www.reddit.com/r/haskell/comments/5fhm3i/should_i_use_haskell_for_small_game_project/).
What every one else said, and also accepting that Haskell is not going to have the tool/IDE support of, say, Java.
Thanks for the note and the vid, very informative
The throughput you could solve by falling back to the stop-all collector if the heap hits a size threshold.
Don't. You don't need that to learn programming in Haskell. Haskell is using some terminology from mathematics. But you don't need to understand theory behind it to use it in practice. 
A solid knowledge of the qwerty keyboard
Atom is pretty easy to set up [1] and has enough IDE-like features to keep me happy. That + `:r` in ghci has been enough! [1] https://github.com/simonmichael/haskell-atom-setup
You're probably right actually, and this definitely has the benefit of not having to "stack" effects, which we usually end up doing with Monadic effects
Laziness is perhaps the wrong default (at least for data structures), but when its useful, its very useful. Off the top of my head: * You can express complex iteration as simple list transformation. * You can give a name to any expression without changing its semantics. * You can express multi-pass algorithms as a single pass. * Your code is never asymptotically more expensive than the equivalent strict code, and often its cheaper. * You can use equational reasoning for refactoring. * You can write generic algorithms and compose them, without generating intermediate data structures, with no hand-specialisation. 
 &gt; :t getNext getNext :: (Floating a1, Integral a1, RealFrac a1) =&gt; a1 -&gt; a1 So `getNext` takes and returns a type which is both `Floating` and `Integral`. I suspect such a type is unlikely to exist. How about getNext n = n - fromIntegral (getDigit n) * fromIntegral (getPlace n) &gt; getNext 99 9.0 
Yeah, that works beautifully. So what happened there?
Roughly, `getDigit` and `getPlace` take floats and return integers. That means you need to convert their return values to floats before you subtract them from `n` (a float).
Do equivalent functions exist in that type class?
Because `getDigit` and `getPlace` take floats[1] [1] `Floating` and `RealFrac` to be technical about it.
Not really. They made it quite clear that it's just their use case that doesn't jive with the GC. They made no claims as to the production viability of GHC
This is a very interesting conversation but i think a lot of the issue is terminological separation induced by a common language :-) In particular, you seem to be using "state" to refer to "state share by coordinating processes". Obviously when we talk about "state" in the sense of e.g. the ST monad that's not what is being tackled. So there's some sort of slight-of-hand in which the complaint is that an orthogonal research domain is not widely known among practitioners (not researchers!) using a different tool to solve different problems. Like, let's not denounce a duck for not being an airplane. Sure monads don't tackle the issue of shared state in a situation with coordinating processes. But... nobody claimed they did! And of course TL tackled problems before pure FP, in the sense that TL was invented before pure FP languages even existed. And let's not narrow the scope of FP as to writing "compiler-like programs" -- sure that was a key motivation for the ML lineage, but from the start Haskell and its precursors had other issues to tackle as well, and even in the ML world directly, modern development has strayed quite far from this original focus. Vis a vis LICS I meant more that it is a place where people involved in FP research interact with people involved in all other manners of logical formalism and verification researchers in the field of computer science. So to me, the issue is you tend to see (or perhaps are just defending Kay seeing) competing schools, while I just see a big field of ideas and results that are the product of many approaches, and where researchers are constantly trying to draw out shared structure, translate results, and use this pool of knowledge as something to variously draw from in creative ways to tackle particular problems, be they practical or theoretical. (And I see this in particular in your attempt to cast synchronous programming and FP as competing paradigms in some way -- I just honestly don't see a tension between the sorts of tools they bring to the table).
It is lazy throughout so it's not surprising. One place it's discussed is here https://mail.haskell.org/pipermail/libraries/2013-March/019528.html
Irrelevant, Haskell is already used in production, and ...I was working 4 years with golang, and believe me, they will end up hating it just like me. Sometimes I think that golang will be remembered in the future as a sort of PHP but for concurrency. 
That is true, the way I was thinking of it is that since those heaps can be garbage collected in parallel and while other processes are running that it amounts to the same thing but yes, they didn't actually have to write a concurrent garbage collector. 
let me hoogle that for you :-) http://hackage.haskell.org/packages/search?terms=opencv So three packages, two not claiming to be complete, but perhaps good enough for your purposes (and if not, you can always patch and extend them) -- of these HOpenCV being the more recently updated and the one I've heard more recommend. Also there is one that looks like a more complete attempt but a bit older and perhaps bitrotted (but again, you could try to knock it into shape if you run into issues). A google also reveals an opencv3 binding that's still being developed, but may be useful to you (and a prior reddit discussion on it here: https://www.reddit.com/r/haskell/comments/4n392j/is_there_a_bindings_for_opencv_3/ )
What's in practice?
For your `gI` mapping, I have the following in my `haskell.vim`: " If the first two lines are imports, "gg/^import\ " will jump to the second " line instead of the first. To get around this, we use \zs to start the match " late, then move back to the beginning of the import. nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;il gg:execute "keepjumps normal! /^i\\zsmport\\ \rh"&lt;cr&gt;:nohlsearch&lt;cr&gt; This has two features compared to yours: the one mentioned in the comment (it will correctly jump to the first line if the first line is an import), and also it doesn't clobber the last-jump marks, so after using this one `''` and the like will still work. I like the [vim-surround](https://github.com/tpope/vim-surround) plugin, and have the following setting to add multiline comment markers with the `-` surrounder: let b:surround_45 = "{- \r -}" Unfortunately vim-surround doesn't really support deletion of these markers directly; however you can delete surrounding `-` and then surrounding `{` to get a similar effect most of the time. I wrote a function to insert a `LANGUAGE` pragma while keeping the cursor roughly where it was. I use this a lot: GHC goes "Hey, maybe you meant to turn on `ScopedTypeVariables`" and I'm like, "yup, `\lstv`" and recompile. It's awesome. Each extension has an abbreviation given by using just the upper-case letters of the extension name; if this is ambiguous, then you also add the next lower-case letter after the last upper-case letter. For example, `BangPatterns` is `bp`, while `DeriveFoldable` and `DeriveFunctor` are `dfo` and `dfu`, respectively. In addition to `\l&lt;abbrev&gt;` for popping a pragma in at the top of the file, I also have `\L&lt;abbrev&gt;` for popping one in above the current line and `l&lt;abbrev&gt;/` in insert-mode for popping in just the extension name, but I use those last two much less often. I'll included the code for this in a reply, because it's kind of long (there's a lot of extensions!). Depending on the code style of the project I'm working on, the [tabular](https://github.com/godlygeek/tabular) plugin is sometimes handy. I have a bunch of quick commands for aligning the current block of things: `\t:` to align type declarations at the `::`, `\t&gt;` to align function arguments at the `-&gt;`, `\t&lt;` to align do-block bindings at the `&lt;-`, and so on: `=`/`=`, `$`/`$`, `#`/`#-}`, `a`/`as`, and `i`/the module name of an import. nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;t: :Tabularize /\V\(\w\\|\s\\|^\\|)\)\zs::\ze\(\w\\|\s\\|\$\\|(\)&lt;CR&gt; nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;t&gt; :Tabularize /\V\(\w\\|\s\\|^\\|)\)\zs-&gt;\ze\(\w\\|\s\\|\$\\|(\)&lt;CR&gt; nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;t&lt; :Tabularize /\V\(\w\\|\s\\|^\\|)\)\zs&lt;-\ze\(\w\\|\s\\|\$\\|(\)&lt;CR&gt; nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;t= :Tabularize /\V\(\w\\|\s\\|^\\|)\)\zs=\ze\(\w\\|\s\\|\$\\|(\)&lt;CR&gt; nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;t$ :Tabularize /\V\(\w\\|\s\\|^\\|)\)\zs$\ze\(\w\\|\s\\|\$\\|(\)&lt;CR&gt; nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;t# :Tabularize /\V\(\w\\|\s\\|^\\|)\)\zs#-}\ze\(\w\\|\s\\|\$\\|(\)&lt;CR&gt; nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;ta :Tabularize /\&lt;as\&gt;&lt;CR&gt; nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;ti :Tabularize /\v\C^\s*import\s+(qualified\s+)?\zs[A-Z]/l1l0&lt;CR&gt; A handy snippet for taking a line like `import Data.ByteString.Strict (ByteString)` and adding a line like `import qualified Data.ByteString.Strict as BS` below it: nnoremap &lt;silent&gt; &lt;buffer&gt; &lt;leader&gt;q :co .&lt;CR&gt;:s/\V\^import/import qualified/&lt;CR&gt;:s/\v^[^(]*\S\zs\s*\(.*$//e&lt;CR&gt;:s/\v\.([^.]*)(\.(Strict\|Lazy))?$/.\1\2 as \1/&lt;CR&gt;:s/\C[a-z]\ze[^ ]*$//eg&lt;CR&gt;:nohlsearch&lt;CR&gt; I've also come to rely on a handful of (insert-mode) abbreviations for common Haskell code chunks like data declarations and common classes/module names. iabbrev &lt;buffer&gt; H# {-# #-}&lt;Left&gt;&lt;Left&gt;&lt;Left&gt;&lt;Left&gt; iabbrev &lt;buffer&gt; HA Applicative iabbrev &lt;buffer&gt; HB Bounded, Enum, Eq, Ord, Read, Show iabbrev &lt;buffer&gt; HBS ByteString iabbrev &lt;buffer&gt; HC Control iabbrev &lt;buffer&gt; HD Data iabbrev &lt;buffer&gt; Hd deriving ()&lt;Left&gt; iabbrev &lt;buffer&gt; Hda data =&lt;CR&gt;deriving (Eq, Ord, Read, Show)&lt;Up&gt;&lt;End&gt;&lt;Left&gt;&lt;Left&gt; iabbrev &lt;buffer&gt; Hdb deriving (Bounded, Enum, Eq, Ord, Read, Show) iabbrev &lt;buffer&gt; Hde deriving (Eq, Ord, Read, Show) iabbrev &lt;buffer&gt; Hne newtype =&lt;CR&gt;deriving (Eq, Ord, Read, Show)&lt;Up&gt;&lt;End&gt;&lt;Left&gt;&lt;Left&gt; iabbrev &lt;buffer&gt; Hnt newtype =&lt;CR&gt;deriving (Eq, Ord, Read, Show)&lt;Up&gt;&lt;End&gt;&lt;Left&gt;&lt;Left&gt; iabbrev &lt;buffer&gt; HE Eq, Ord, Read, Show iabbrev &lt;buffer&gt; HF Functor iabbrev &lt;buffer&gt; HH hiding iabbrev &lt;buffer&gt; HI import iabbrev &lt;buffer&gt; HL LANGUAGE iabbrev &lt;buffer&gt; Hl Language iabbrev &lt;buffer&gt; HM Monad iabbrev &lt;buffer&gt; HS System iabbrev &lt;buffer&gt; HT Text iabbrev &lt;buffer&gt; HQ qualified iabbrev &lt;buffer&gt; HW where
Code for inserting language pragmas: function! AddLanguagePragma(pragma) let s:orig_pos = getcurpos() let s:insertion_command = "O" " step 1: try to find a suitable place to add it " current heuristic is to try these in order: " * under the last existing LANGUAGE pragma " * over the module declaration line " * at the top of the file try keepjumps normal G$?\C\m{-# LANGUAGE\&gt; let s:insertion_command = "o" catch /E486:/ try keepjumps normal gg/\C\m^&gt;\?\s*module\&gt; catch /E486:/ keepjumps normal gg endtry endtry " step 2: add it keepjumps execute "normal" s:insertion_command . "{-# LANGUAGE" a:pragma "#-}" " step 3: format it, if the appropriate plugin is installed try if b:align_language_pragma try Tabularize /#-} catch /E492:/ endtry endif catch /E121:/ endtry " TODO: the position may have changed due to editing... keepjumps call setpos('.', s:orig_pos) endfunction " g:haskell_extension_abbreviations is auto-generated " TODO: give mappings for the ambiguous ones to tell the user they're " ambiguous (e.g. map &lt;leader&gt;ldf to a message telling them to either use " &lt;leader&gt;ldfu or &lt;leader&gt;ldfo); no need to muck with the abbreviations, " though, since nothing bad happens if you don't finish them properly let g:haskell_extension_abbreviations = { \ 'a': 'Arrows', \ 'aat': 'AllowAmbiguousTypes', \ 'adt': 'AutoDeriveTypeable', \ 'bl': 'BinaryLiterals', \ 'bp': 'BangPatterns', \ 'caffi': 'CApiFFI', \ 'ccm': 'ConstrainedClassMethods', \ 'ck': 'ConstraintKinds', \ 'cpp': 'CPP', \ 'dac': 'DeriveAnyClass', \ 'daite': 'DoAndIfThenElse', \ 'dc': 'DatatypeContexts', \ 'ddt': 'DeriveDataTypeable', \ 'dfo': 'DeriveFoldable', \ 'dfu': 'DeriveFunctor', \ 'dg': 'DeriveGeneric', \ 'dk': 'DataKinds', \ 'dr': 'DoRec', \ 'drf': 'DisambiguateRecordFields', \ 'ds': 'DefaultSignatures', \ 'dt': 'DeriveTraversable', \ 'ec': 'EmptyCase', \ 'edd': 'EmptyDataDecls', \ 'edr': 'ExtendedDefaultRules', \ 'efa': 'ExplicitForAll', \ 'en': 'ExplicitNamespaces', \ 'eq': 'ExistentialQuantification', \ 'er': 'ExtensibleRecords', \ 'fc': 'FlexibleContexts', \ 'fd': 'FunctionalDependencies', \ 'ffi': 'ForeignFunctionInterface', \ 'fi': 'FlexibleInstances', \ 'g': 'Generics', \ 'gadt': 'GADTs', \ 'gadts': 'GADTSyntax', \ 'ghcfip': 'GHCForeignImportPrim', \ 'gnd': 'GeneralizedNewtypeDeriving', \ 'hd': 'HereDocuments', \ 'iffi': 'InterruptibleFFI', \ 'ii': 'IncoherentInstances', \ 'ipa': 'ImplicitParams', \ 'ipr': 'ImplicitPrelude', \ 'is': 'InstanceSigs', \ 'it': 'ImpredicativeTypes', \ 'jsffi': 'JavaScriptFFI', \ 'ks': 'KindSignatures', \ 'lc': 'LambdaCase', \ 'lts': 'LiberalTypeSynonyms', \ 'mc': 'MonadComprehensions', \ 'mh': 'MagicHash', \ 'mlb': 'MonoLocalBinds', \ 'mpb': 'MonoPatBinds', \ 'mptc': 'MultiParamTypeClasses', \ 'mr': 'MonomorphismRestriction', \ 'mwi': 'MultiWayIf', \ 'na': 'NoArrows', \ 'naat': 'NoAllowAmbiguousTypes', \ 'nadt': 'NoAutoDeriveTypeable', \ 'nbl': 'NoBinaryLiterals', \ 'nbp': 'NoBangPatterns', \ 'ncaffi': 'NoCApiFFI', \ 'nccm': 'NoConstrainedClassMethods', \ 'nck': 'NoConstraintKinds', \ 'ncpp': 'NoCPP', \ 'nd': 'NumDecimals', \ 'ndac': 'NoDeriveAnyClass', \ 'ndaite': 'NoDoAndIfThenElse', \ 'ndc': 'NoDatatypeContexts', \ 'nddt': 'NoDeriveDataTypeable', \ 'ndfo': 'NoDeriveFoldable', \ 'ndfu': 'NoDeriveFunctor', \ 'ndg': 'NoDeriveGeneric', \ 'ndk': 'NoDataKinds', \ 'ndr': 'NoDoRec', \ 'ndrf': 'NoDisambiguateRecordFields', \ 'nds': 'NoDefaultSignatures', \ 'ndt': 'NoDeriveTraversable', \ 'nec': 'NoEmptyCase', \ 'nedd': 'NoEmptyDataDecls', \ 'nedr': 'NoExtendedDefaultRules', \ 'nefa': 'NoExplicitForAll', \ 'nen': 'NoExplicitNamespaces', \ 'neq': 'NoExistentialQuantification', \ 'ner': 'NoExtensibleRecords', \ 'nfc': 'NoFlexibleContexts', \ 'nfd': 'NoFunctionalDependencies', \ 'nffi': 'NoForeignFunctionInterface', \ 'nfi': 'NoFlexibleInstances', \ 'nfp': 'NamedFieldPuns', \ 'ng': 'NoGenerics', \ 'ngadt': 'NoGADTs', \ 'ngadts': 'NoGADTSyntax', \ 'nghcfip': 'NoGHCForeignImportPrim', \ 'ngnd': 'NoGeneralizedNewtypeDeriving', \ 'nhd': 'NoHereDocuments', \ 'ni': 'NondecreasingIndentation', \ 'niffi': 'NoInterruptibleFFI', \ 'nii': 'NoIncoherentInstances', \ 'nipa': 'NoImplicitParams', \ 'nipr': 'NoImplicitPrelude', \ 'nis': 'NoInstanceSigs', \ 'nit': 'NoImpredicativeTypes', \ 'njsffi': 'NoJavaScriptFFI', \ 'nks': 'NoKindSignatures', \ 'nl': 'NegativeLiterals', \ 'nlc': 'NoLambdaCase', \ 'nlts': 'NoLiberalTypeSynonyms', \ 'nmc': 'NoMonadComprehensions', \ 'nmh': 'NoMagicHash', \ 'nmlb': 'NoMonoLocalBinds', \ 'nmpb': 'NoMonoPatBinds', \ 'nmptc': 'NoMultiParamTypeClasses', \ 'nmr': 'NoMonomorphismRestriction', \ 'nmwi': 'NoMultiWayIf', \ 'nnd': 'NoNumDecimals', \ 'nnfp': 'NoNamedFieldPuns', \ 'nni': 'NoNondecreasingIndentation', \ 'nnl': 'NoNegativeLiterals', \ 'nnpkp': 'NoNPlusKPatterns', \ 'nnqo': 'NoNewQualifiedOperators', \ 'nntc': 'NoNullaryTypeClasses', \ 'nnwc': 'NoNamedWildCards', \ 'noi': 'NoOverlappingInstances', \ 'nol': 'NoOverloadedLists', \ 'nos': 'NoOverloadedStrings', \ 'npa': 'NoParallelArrays', \ 'npc': 'NoPolymorphicComponents', \ 'npg': 'NoPatternGuards', \ 'npi': 'NoPackageImports', \ 'npk': 'NoPolyKinds', \ 'npkp': 'NPlusKPatterns', \ 'nplc': 'NoParallelListComp', \ 'npo': 'NoPostfixOperators', \ 'npsi': 'NoPatternSignatures', \ 'npsy': 'NoPatternSynonyms', \ 'npts': 'NoPartialTypeSignatures', \ 'nqo': 'NewQualifiedOperators', \ 'nqq': 'NoQuasiQuotes', \ 'nr2t': 'NoRank2Types', \ 'nra': 'NoRoleAnnotations', \ 'nrd': 'NoRecursiveDo', \ 'nrnt': 'NoRankNTypes', \ 'nrpa': 'NoRegularPatterns', \ 'nrpr': 'NoRelaxedPolyRec', \ 'nrpu': 'NoRecordPuns', \ 'nrs': 'NoRebindableSyntax', \ 'nrts': 'NoRestrictedTypeSynonyms', \ 'nrwc': 'NoRecordWildCards', \ 'ns': 'NoSafe', \ 'nsd': 'NoStandaloneDeriving', \ 'nsi': 'NoSafeImports', \ 'nstv': 'NoScopedTypeVariables', \ 'nt': 'NoTrustworthy', \ 'ntc': 'NullaryTypeClasses', \ 'ntf': 'NoTypeFamilies', \ 'nth': 'NoTemplateHaskell', \ 'ntlc': 'NoTransformListComp', \ 'nto': 'NoTypeOperators', \ 'ntrs': 'NoTraditionalRecordSyntax', \ 'nts': 'NoTupleSections', \ 'ntsi': 'NoTypeSynonymInstances', \ 'nu': 'NoUnsafe', \ 'nuffit': 'NoUnliftedFFITypes', \ 'nui': 'NoUndecidableInstances', \ 'nus': 'NoUnicodeSyntax', \ 'nut': 'NoUnboxedTuples', \ 'nvp': 'NoViewPatterns', \ 'nwc': 'NamedWildCards', \ 'nxs': 'NoXmlSyntax', \ 'oi': 'OverlappingInstances', \ 'ol': 'OverloadedLists', \ 'os': 'OverloadedStrings', \ 'pa': 'ParallelArrays', \ 'pc': 'PolymorphicComponents', \ 'pg': 'PatternGuards', \ 'pi': 'PackageImports', \ 'pk': 'PolyKinds', \ 'plc': 'ParallelListComp', \ 'po': 'PostfixOperators', \ 'psi': 'PatternSignatures', \ 'psy': 'PatternSynonyms', \ 'pts': 'PartialTypeSignatures', \ 'qq': 'QuasiQuotes', \ 'r2t': 'Rank2Types', \ 'ra': 'RoleAnnotations', \ 'rd': 'RecursiveDo', \ 'rnt': 'RankNTypes', \ 'rpa': 'RegularPatterns', \ 'rpr': 'RelaxedPolyRec', \ 'rpu': 'RecordPuns', \ 'rs': 'RebindableSyntax', \ 'rts': 'RestrictedTypeSynonyms', \ 'rwc': 'RecordWildCards', \ 's': 'Safe', \ 'sd': 'StandaloneDeriving', \ 'si': 'SafeImports', \ 'stv': 'ScopedTypeVariables', \ 't': 'Trustworthy', \ 'tf': 'TypeFamilies', \ 'th': 'TemplateHaskell', \ 'tlc': 'TransformListComp', \ 'to': 'TypeOperators', \ 'trs': 'TraditionalRecordSyntax', \ 'ts': 'TupleSections', \ 'tsi': 'TypeSynonymInstances', \ 'u': 'Unsafe', \ 'uffit': 'UnliftedFFITypes', \ 'ui': 'UndecidableInstances', \ 'us': 'UnicodeSyntax', \ 'ut': 'UnboxedTuples', \ 'vp': 'ViewPatterns', \ 'xs': 'XmlSyntax', \} for [s:abbr, s:ext] in items(g:haskell_extension_abbreviations) exec "iabbrev &lt;buffer&gt; l" . s:abbr . "/ " . s:ext exec "nnoremap &lt;buffer&gt; &lt;leader&gt;L" . s:abbr . " O{-# LANGUAGE " . s:ext . " #-}&lt;ESC&gt;" exec "nnoremap &lt;buffer&gt; &lt;leader&gt;l" . s:abbr . " :silent call AddLanguagePragma('" . s:ext . "')&lt;CR&gt;" endfor
Gabriel has a post which details the CV situation in Haskell: https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#computer-vision In particular, the bindings from LumiGuide right now are the most up to date. ~~Unfortunately the documentation is scarce, and it's been a while since I've done anything CV related in Haskell.~~ Here are some links which might help you: - http://stackoverflow.com/a/39515797/3970496 - https://github.com/LumiGuide/haskell-opencv/tree/master/examples - https://www.reddit.com/r/haskell/comments/5gdmz7/opencv2_in_haskell/data6jf/
&gt; Your answer amounts to saying, that the Hindley-Milner "sweet spot" locks the type system into a very narrow band. Making the type system essentially unique. GHC Haskell's type system is much, much richer than just HM. `\(x :: forall a. a -&gt; a) -&gt; x x` is an example of a [System F](https://en.wikipedia.org/wiki/System_F)-like thing that is an extension to HM. And then there's [TypeInType](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#kind-polymorphism-and-type-in-type).
in python we have"import cv2". As for Haskell, is it still the 1st version or already 2nd one?
&gt; Haskell mostly draws on category theory anyway Just popping in to say: Don't let this comment scare you! You can do quite well in Haskell without actually learning any depth of category theory. Knowing category theory will, in fact, help only with some on of many directions in which your skill as a functional programmer can develop. Most Haskellers learned words like "monad" and "functor" as part of Haskell, not from previous graduate work in mathematics. You can understand them within Haskell quite easily, without much mathematical sophistication. We're pretty much in the same boat here, and you aren't going to be at a substantial disadvantage for having not studied category theory. (Indeed, many mathematicians I know, who do know a fair bit about categories, still express amusement at functional programming enthusiasts and their obsession with category theory. From a mathematical perspective, we tend to only use the shallowest of results, and to be mostly interested only in a single category and few constructions within it.)
&gt; looking at biology to see how it deals with some problems so that we can get inspiration. Why would biological organisational structures necessarily be best? Biological systems are the result of evolutionary processes, and therefore could be a local optimum for life, let alone for computer systems that are fundamentally different anyway. Biological systems are so complex that we have spent centuries trying to figure them out. I don't think that sort of problem solving approach is what we want for computer systems. &gt; You're forgetting the modularity, which is similar to modules in ML. Modularity is a much more important tenant of OOP than modeling data, and that part, at least, has proven to work very well ML modules are just elaborate static machinery on existential types, which are abstract types. That's why I was saying that OOP is usually about half-assed abstract data types. OOP has no monopoly on modular programming. Computer science had long been pursuing a programme of principled modularity and abstraction when OOP became the new fad and screwed up a lot of people's perception of what is important. &gt; (we've been able to write much larger, more elaborate software with OOP than with procedural programming). I will gladly agree that OOP has made software much larger and more elaborate. I disagree about the value of such things.
The tooling doesn't find the leaks but it aids a human in finding them. There are no tools to find space or time leaks in either, just tools to aid. 
I'm enjoying seeing stack factor into Vim + Haskell efforts finally.
On the other hand, familiarity with another more "mainstream" language makes Haskell itself *harder* to learn for some people. Haskell is different, so if you have deeply ingrained previous conceptions of what programming languages are like, you need to unlearn those as part of learning Haskell.
Well, do, sure, but not as a prerequisite for Haskell. It's great stuff, and if you also know some Haskell, playing with the two of them together is lots of fun! But if you are not planning on learning it now, that's fine, you don't need it for learning Haskell.
Just pointing out that the author of the parent post has successfully taught Haskell to classrooms full of children. So he knows what he's talking about. :)
This is a very nice option to have, I have done this with success on an HPC platform where I didn't have root, but still wanted to have GHC etc. available.
Do you think linear types would solve the problem in your case? If so, since linear type are coming to Haskell, do you think in the future people will not stumble on the same problem in Haskell?
I second that. Learning math was one of the most valuable things I learned during college. And it is clear that Haskell is influenced by math. It's just that many people are scared of by math or they will give up before they even start with Haskell. I would rather say that Haskell (community) can expose you to applied mathematics, if you want.
Yes. It's been highlighted in "State of the Haskell Ecosystem" as an area for improvement. https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#distributed-programming
 &gt; One cost is thread safety; that is a very large cost indeed. Not being thread safe seems like a good default. If course this should be in the types. 
Yeah, I Guess I have quite a lot of stuff in my vimrc to be extracted out to ftplugins and such, thanks for the sugestion, Will give that a try
It should work as long as you configure the vimrc that your editor on the host uses
Thats fair, but (in vim at least) the syntax highlight is the main reason of lag most of the time, you could add hundreds of mappings without Any overload, but then do ':syntax off' and boom! Turbo Mode! Vim's syntax highlighting has some problems, Hope they work more on that issue 
Haskell's STM semantics seem very loose to me. The fact that short transactions can starve long ones makes me feel uneasy about STM. I don't think there's a way to say "I want transactions to commit in the order they were started" or anything like that.
&gt; What I do instead is I use haskell-vim except syntax highlighter is customized and indentation script is removed. This adds no features other than good highlighting. Dammit, I've been struggling with the auto indentation in `haskell-vim`, and not once did it occur to me to just delete that part of the plugin. Edit: and right after writing this I go look at the plugin repo and see that a PR _from this post's OP_ adding an option to disable indenting was merged last week.
I hope they don't stop until we are able to fully import any Haskell package into Idris!
So you can already do that without even STM by just serializing all transactions through one worker thread, but then you lose parallelism. Any approach that guarantees commit order has to disable parallelism because otherwise other transactions could invalidate the currently preferred transaction. In practice most transactions are short so this has never caused issues for me
It wouldn't have to disable parallelism but maybe it would result in very bad performance. Like, if you start 4 transactions at once, and all take roughly the same amount of time, you could still have them run in parallel, and wait for the preferred transaction to finish at commit time.
The stack project is Api-Collection and thats the name of the folder but Auth-Api is the name of the haskell file and its a module called Auth-Api.
I wonder as well! I've been a programmer for years and I'm a math major (so, I guess I have the prerequisite skills); but I still can't wrap my head around Idris. Does anyone have any solid advice?
Cbjngecbi Ekrpatv
Not to be taken to seriously &gt; "for systems like webapps, API endpoints, data processing systems (think ETL/batch jobs)" &gt; but on the other hand, I don't think you really need laziness for those kinds of systems. ... and you don't need a (sophisticated) type-system, or even implicit memory management. --- I have been writing full-time the mentioned three for the last year using Haskell. From that experience, I can mention few things: - Sometimes we "consciously" rely on laziness, e.g. having some kind of `World`, where some parts are lazily populated. For example see: https://github.com/haskell/cabal/blob/master/Cabal/Distribution/Compat/Graph.hs You can do this kind of thing in strict language by having `Lazy a` or `Memo a` type. However it's implementation would need to use some kind of `unsafePerformIO`. - Often we use data as control structures, e.g. `maybe (fail "Unexpected") pure maybeFoo`. In strict language, you'd forced to use `either fail pure eitherFoo`. In this example it's even a good idea. But generally I don't like that one need to think about operational semantics when writing very high level and general code (and notice the very small but vital difference between `Maybe` and `Either` version!) - Another "higher level example" is e.g. using parsing combinators, e.g. in `purescript-parsing` you'd need to use `defer` &gt; the parser in `purescript-parsing` has a `Lazy` instance, which mean you can use `defer` to let you recursively reference a parser like [this](https://gist.github.com/felixSchl/c290ff9b710a587b51a3) - I'm having hard time finding "real world" examples, where I'd need to change code in current projects, if the Haskell were strict. On small scale difference wouldn't be big. Yet, my the lazy by default frees me from thinking about operational stuff when I don't want to, I can abstract things in much more ways (i.e. 99% of the time!) - AFAIK some libraries like [`bound`](http://hackage.haskell.org/package/bound) won't work, or would have impractical api to be used in a strict language. I.e. there are some abstractions, which aren't expressable in a strict language. You don't use them in every webapp or data processing system, but sometimes you do, and that might save a lot of engineering time!
If you use emacs (which IMO is the best editor to use for haskell code) you can take a look at haskell my [config](https://github.com/CSRaghunandan/.emacs.d/blob/master/setup-files/setup-haskell.el)
Edwin Brady, the principal developer of Idris, has a book called "Type Driven Development with Idris" in MEAP status right now. It's pretty comprehensive and I'm enjoying it, although I'm also going through a book about Coq at the same time so I'm going rather slow.
Is the "sweet spot" much wider then Iceland_jack suggested, or am I broadly misunderstanding something? Either way, I'm getting the sense that characterizing valid lambda terms is not an easy question. 
If you want to learn haskell, you should read a haskell book, or a haskell tutorial, etc. You can learn math later.
My understanding is it is bound to opencv 3, not 2?
Try it and find out.
`haskell-vim`'s indenting is not very good with scope, and it tries to re-indent lines when you make changes to them, which gets super irritating really fast.
[Spacemacs](https://github.com/syl20bnr/spacemacs/) with haskell-mode and stack.
There are so many more pragmas than I thought. I have a similar list in my ftplugins/haskell.vim, but it's about 1/4 the size. I thought I'd found them all.
Ah, I don't use haskell-vim (didn't know about it). I use 3 neco-ghc, vim2hs, and ghc-mod.vim. I think the editing is coming from vim2hs. It never tries to re-indent lines as I edit.
https://ghc.haskell.org/trac/ghc/wiki/LinearTypes From IRC: &gt; aspiwack[m]: Let me clarify a few things about linear types. The wiki page has indeed been dormant, that's because we have been working on a larger article (not public yet) discussing the details of the proposal and exploring (some of) the motivations and consequences, in cooperation with GHC HQ. (which reminds me that the design has evolved since then, and I need to update the wiki page accordingly). We've started with the implementation, but it's still in a very early stage. We really want it to happen, so we're dedicating as much time as we can. If you (or anyone) have a question about or want to discuss the linear types proposal, I'm lurking about: don't hesitate to highlight me.
Vim, tmux, stack, hoogle, git. I used to use the Vim haskell-mode and ghcmod plugins a lot, but don't so much any more. Most of my Vim 'power use' comes from ctrlp.vim for file / tag navigation and vim-fugitive for integration with git.
Sublime Text, ssh, nixpkgs, ghci, ghcid, git, hayoo, hoogle, hackage, Phabricator.
The first time I used fugitive to stage individual lines instead of chunks, I felt like a wizard.
This is a beautiful interface. It's nice seeing hamiltonians in a functional context. 
Emacs haskell-mode with a bunch of yasnippets and macros, magit, smex, zsh, nix-shell, nixpkgs, the silver searcher, cabal-repl / ghci and the Glorious Haskell Compiler Javscript Interpeter (ghcjsi)
I haven't had free time to try it yet, though I think it may have all the features I need. My only issue is flycheck doesn't seem to work with the stack package directories and gives me false import errors. Does intero fix that? I imagine I just need to instruct flycheck to use stack.
[Structure and Interpretation of Classical Mechanics](https://mitpress.mit.edu/sites/default/files/titles/content/sicm/book-Z-H-4.html) is wonderful, not sure about entry level though
I've been using a lot of Nix lately instead of Stack. It was quite conflicting for a while. Ultimately, here are the advantages I've come to understand about each of them: Stack: - Much easier configuration language (EDIT: To clarify, it's hard to overstate how much simpler it is to work with Stack) - Upgrading package set is significantly more lightweight, especially in terms of version bumping a particular package - Stackage appears to come with more build safety guarantees than nixpkgs, but the difference is negligible - Stackage hosts documentation (though Nix does give you this locally) - Works on Windows Nix: - Much more powerful build tool. You can incorporate all sorts of build tools however you like - Gives you much more power of configuration and system package dependencies - Nixpkgs seems to be a little ahead of the curve on keeping packages up to date while making sure most things build together. In general, it just needs help less than Stack need `extra-deps` - Remote binary caching is awesome. You can use it to have an internal server build internally developed libraries so that devs don't have to wait for it to build when a library is updated - `nix-copy-closure` is a truly wonderful way to deploy. Much nicer than Docker --- The rest of my stack is pretty minimal. Just Emacs and intero (with hacks to make it work with my nix builds) usually. Stack is nice for incremental building on top of the nix stuff, though Cabal is actually somewhat nicer in this paradigm, since Stack really isn't built for this.
glad that option is being useful to someone else
I've tried Emacs + evil mode a couple times, but it didn't work for me, maybe I'm just way to used to vim or something; Anyways, check out the 'asyncrun.vim' plugin, it makes it really simple to use Vim 8 async job control.
nice animation! :D
Emacs (sometimes Haskell mode, if not editing on a server), stack, stack docker for prod builds, use hspec instead of repl for exploration.
Atom + Stack really works well for me: 1. Compilation error markers on save 2. Insert type 3. Hints (ranging from redundant brackets, simplifying expressions, unused imports, unused variables/parameters etc.) 4. Type information over mouse hovers including which module defines them, showing both the original type and the specialization at the context. 5. Prettifying 6. Case splitting (I really like this!)
&gt; advanced type features Examples?
Can you elaborate on that alternative approach / your original attempt? 
&gt; I've been able to do everything I could with them with Atom There is almost no way that is true, there are simply way too many combinations of key presses that you can combine in far too many ways in Vim for Atom to even have a chance of competing in that regard.
I don't know what I am doing wrong but when the run the program runs in GHCI and I press the enter key the output is nothing. Here is the output: GHCi, version 7.10.3: http://www.haskell.org/ghc/ :? for help Prelude&gt; :l marika [1 of 1] Compiling Main ( marika.hs, interpreted ) marika.hs:11:1: Warning: Tab character marika.hs:12:1: Warning: Tab character marika.hs:13:1: Warning: Tab character marika.hs:17:1: Warning: Tab character marika.hs:18:1: Warning: Tab character marika.hs:22:1: Warning: Tab character Ok, modules loaded: Main. *Main&gt; *Main&gt; *Main&gt; 
LumiGuide uses it in production. They even got featured on TNW: http://thenextweb.com/insider/2015/10/19/meet-the-internet-connected-dutch-system-making-it-easier-to-find-a-spot-to-park-your-bike/ But that being said, you should check if it supports all the opencv bindings you need. They do support `CascadeClassifier`, `imread`, `cvtColor`, and `detectMultiScale`.
how do you use `hspec` for ~~testing~~ exploration, especially instead of the `repl`? EDIT: typo
Maybe because Free Monads are not free from incurring additional runtime cost?
[Edward Kmett describes a similar free monad-style formulation for IO, used in a Haskell-like language used at his job.](http://comonad.com/reader/2011/free-monads-for-less-3/)
When you load a file in ghci, it doesn't run automatically. You need to invoke `:main` and it will run the `main` function defined in the file. Also GHC doen't like tabs instead of spaces in source files, my editor must have slipped some somehow.
&gt; For spacemacs user, I just discovex `SPC s j` to navigate to definition using helm. Wow, thank you!
Don't understand first part of this question...I use hspec for testing at a variety of levels. I use hspec for exploration by writing throwaway test suites against the libraries I am getting to know and frequently rerunning test suite. Eventually I would like to switch back to repl with some combination of loading script files, and invoking loaded functions from repl
For some of this tooling, the time and effort to make it work within a particular setting make them absolutely not worth it : if you are going to spend a certain fixed amount of time, spend it on haskell, not on some secondary thing for a brittle result. That's especially true for beginners. Now the situation might have evolved since I checked, but I remain highly cautious of this..
anything isomorphic in the Set category will do
would be nice to have a small video of this setup showing them in action
What is the difference between make Stack + nix and Cabal + nix ? You don't use Stack's nix facility ?
what do you mean by insert type ?
Stack's Nix integration is a very loose form of integration. You can really only use it for non-haskell dependencies. You're still meant to use Stack's snapshots and its methods for building dependencies. This means no remote binary caching (though of course it preserves local caching), and no Nix style configuration of packages. I don't think Nix actually uses Cabal on its own. I believe it just uses `ghc --make`. But Cabal is friendly to Nix's method of acquiring GHC and Haskell dependencies, so you can use Cabal inside the Nix shell just fine. This is important because of incremental local builds, which Nix does not do. Stack also works fine this way, but it's not designed for it, so there's some buggy quirks, and I have no confidence that changes to Stack won't break anything. Also, you can't use Stack's Nix integration with GHCJS. Considering Stack doesn't work on NixOS unless you *always* use Nix integration, this is obviously a problem.
From my vague high-level understanding, I think it would solve our problem. To me, it a lot like reference counting, where the only "owner" of the data would be the thread holding the reference to the root of the data structure. There is only one reference to each message, because the data structure we used is acyclic. So when a message is expired, it can be freed immediately. On a related note: reference counting is generally a very good fit for this problem. This HN user managed to get extremely low pause (effectively non-existent) times with Python: https://news.ycombinator.com/item?id=13092837
Ah, ok. Still, they're not magic in the [GHC.Prim](https://hackage.haskell.org/package/ghc-prim-0.4.0.0/candidate/docs/src/GHC-Prim.html) sense, given that they are still implemented in Haskell and you can see and possibly duplicate the implementation given the right imports of GHC modules.
interesting points
A few pure answers: * There is https://hackage.haskell.org/package/containers-0.5.8.1/docs/Data-Map-Strict.html * But also check out https://hackage.haskell.org/package/unordered-containers-0.2.7.1/docs/Data-HashMap-Strict.html * and https://hackage.haskell.org/package/containers-0.5.8.1/docs/Data-IntMap.html I don't have speed benchmarks, but their memory usage is a bit different: https://www.fpcomplete.com/blog/2016/05/weigh-package#containers-vs-unordered-containers
It all has to do with managing capacities. There *are* monads, some of them are just implicit/packed, and we want to manage *explicitely* what is provided *implicitely* at some other level
Vim, konsole, stack, git, and ghci. 
git-annex, dars
Very detailed write-up that was also a fun adventure. Hopefully we can make further improvements in this area.
&gt; Unfortunately the documentation is scarce Really? I think we've been doing a good job with documentation - * https://lumiguide.github.io/haskell-opencv/doc/index.html * https://lumiguide.github.io/haskell-opencv/doc/OpenCV-ImgProc-Drawing.html
Did you mean [darcs](http://darcs.net/)? If so, indeed, being a a distributed version control system, it can be a part of a daily workflow of non-Haskeller; though I wonder if it is actually used to large extent by non-Haskellers to manage their source code. 
I think this is the most plausible reason. They don't have the solutions at hand for most of the exercises, and therefore generating the solutions and putting them somewhere would be a lot of work. They probably hoped that others would pick up the slack by generating solutions and sharing them in GitHub repositories. However, in order to save time and make people who like having solutions happy, why not have a curated list of the solutions that are available in the repositories? That way the authors won't have to spend the time necessary generating the answers themselves for all of the exercises, and will only have to curate the available solutions and generate solutions themselves whenever solutions for certain exercises in the repositories are not available (and the authors do not have those solutions at hand). 
Thanks for sharing this (I didn't know about it)! Hopefully it has all of the solutions and they are of good quality. If anyone has any other solution sets, please post / share them!
I didn't put Intero, because like ghc-mod it seems to be working only until I need it. I usuall care of the type of something when I have some syntax errors to fix, but then the file can't be parsed (unless I'm missing something ...)
I have been using Emacs for over 30 years, so the decision is based on what I am used to using.
What about [Detexify](http://detexify.kirelabs.org/classify.html) ?
hledger is another candidate. I don't know how popular command line accounting usage is, but hledger is a haskell reimplementation of ledger (written in C++) that is more user friendly.
I think you can find a fair number of non-Haskell projects among the [1678 public repos](http://hub.darcs.net/explore) on hub.darcs.net.
My opinion is that `&lt;&gt;` should always be preferred. I think of it the same as `+` from the Num class, which is obviously preferred over using specialized addition functions for each numeric type. However, there is a downside of less clear type errors. For very new beginners, I might make an exception and teach them with only `++` for a little while.
So it's a similar case as `map` vs `fmap` then?
That makes sense. I can understand why this would be done, but it would be nice to see some documentation of it. At it is now, there's no easy way for an end-user to know that something is different (or missing) when they build `text` with ghcjs. From a documentation standpoint, it seems like the best long-term goal would be to get the ghcjs optimizations into into `text` itself. Then users would just be able to browse the source code through hackage. It does not look like anyone has [ever tried to do this](https://github.com/bos/text/issues?utf8=%E2%9C%93&amp;q=ghcjs). I don't know if the blocker is just that people have been busy with more pressing issues or if it's that either Bryan or Luite doesn't want this merged into `text` (there are several legitimate concerns that I can think of offhand that either of them may share).
There are no fundamental reasons that GHC couldn't have incremental or concurrent GC, it's just a question of engineering effort (but a fairly significant amount of that).
Ah that clears it up a bit. I knew it wasn't exactly conventional.
custom preludes ftw
Yea I've been meaning to look into some of those. Any suggestions? My main gripes are that functions aren't defaulted to `Text`, and they don't use `MonadIO` or `MonadBase` by default.
The down side is I've been using vim for a decade and it has crashed maybe once. I used spacemacs for a week and it crashed at least every day. YMMV.
Bashing vanilla emacs into something worthwhile is pretty annoying, but spacemacs pretty much does it for you. It's good to know that the job control story is better in vim now. I think I've sunk too much into emacs to try switching back to using vim as an IDE. Right now I use vim for lightweight editing stuff, and emacs for more serious development.
Similar, but `map` is parameterized over Functor, a class of types of kind * -&gt; *, while `&lt;&gt;` is parameterized over Monoid, a class of types of kind *. This means error messages for Functors are potentially more confusing because they talk about part of the type of a value (e.g. Maybe), rather than the full type of a value (e.g. Maybe Int). I don't recall how much time it took for me to get used to dealing with these sorts of type errors. I think experienced haskellers often underestimate how hard it is to get accustomed to reading such type errors.
is it on a website?
python doesn't use version 3 at all?
&gt; You isn't exclusively garethrowlands by the way I'm not? Who else Am I?
Somewhat relevant: https://www.reddit.com/r/haskell/comments/4p82jy/what_haskell_technologies_should_i_probably_be/d4izh4h
Thx for the detailed note. Agree with the comparison. Note that Jasper indeed developed the first version of that library, but a large part of what was integrated was developed by myself and Leon P. Smith took over maintenance in 2015. Not that Jasper doesn't earn credit. He's an awesome contributor to the Haskell ecosystem. 
I'd definitely categorize this as one little nifty utility that can serve many non-Haskellers. :) Thanks.
That's why I switched to neovim. Async type checking helps me keep my sanity and focus.
I really like `protolude`, specifically `lifted-protolude` (my fork) that uses lifted functions and exceptions.
Normally `Data.Text` really is the same as in the original `text` package. There is `Data.JSString` in the `ghcjs-base` package, which provides exactly the same API as `Data.Text`, except for accessing the internals (they may have diverged a little if your `text` is newer than the version that `Data.JSString` is based on). `Data.JSString` has the same fusion behaviour as `Data.Text`. The point was that eventually we'd be able to offer a text api which would cover all functionality from `text`, except accessing the internals, that could have either `Data.Text` or `Data.JSString` as its underlying implementation. Unfortunately, we're not quite there yet in terms of machinery (backpack?) to do this properly, but since `Data.JSString` is so much faster, some users have already replaced the whole text package. In this case looks like it's an unofficial optimization by reflex-platform.
Such as http://instantwatcher.com (written in haskell). Or the incredible proof machine. http://incredible.pm/
Not the OP, but my 2 is that they do as long as the user-facing parts make essential use of Haskell.
Oh man, that one was a lifesaver in college!
there is VIM mode in Atom ;D
Great write up! The DynFlags fix should be regarded as legend! 
How powerful is dhall's type system? Does it use morte underneath? Edit: dhall is a fork of morte that doesn't allow functions from terms to types. It is just system F. That is a real shame. 
I didn't know about this -- thanks!
`vimus` is written in haskell, it's an `mpd` client like `ncmpc`
For the extra paranoid, is there an easy way of checking from Haskell that a Dhall program is already in normal form?
I heard this is being scrapped. Can anyone (dis)confirm?
I sort of doubt propellor is widely used, because even for Haskellers the documentation leaves a lot to be desired.
Yeah, it is system F-omega under the hood.
Probably the most direct way is: normalize e == e `normalize` should be fast if the expression is already in normal form
From my boss, who also said Simon Marlowe and the rest of the Haxl team are no longer there.
It wouldn't make a difference on the Haskell side since there is no expectation that you can decode all Dhall expressions into Haskell. You can't even decode Dhall functions into Haskell anyway.
Oops! You're right, I forgot to add one
So why did you make the choice to go system F? 
Adding dependent types is more complicated than just implementing the calculus of constructions (which is what `morte` did). You'd minimally need inductive constructions, dependent elimination, or universe polymorphism in order to do useful things with it (like length-indexed vectors with type-safe head). I'd also have to teach new users how to use that machinery, too, which increases the entry barrier Edit: You also need a really strong equality system built into the type checker (i.e. the built in type equalities that require no programmer assistance) if you want a dependent type system to be pleasant to use. Liquid Haskell is a great example of when this works well but it's a lot of work to implement.
Oh dear.
I've got a list of open-source projects here: https://guide.aelve.com/haskell/open-source-projects-itg7nf8p. Lots of projects there have 1001000 stars on Github and are generally useful for non-Haskellers. (I tried to sort the list roughly by stuff that nobody heard about is at the bottom, but may have failed  feel free to edit.)
This is cool. It's funny I was thinking just the other day to make a simply typed lambda calculus as a limited extension language, and more or less here it is. Hurrah! &gt; Dhall is sandboxed, meaning that the only permitted side effect is retrieving other Dhall expressions by their filesystem path or URL Presumably this doesn't permit infinite loops by retrieving itself? 
I was thinking about compile-time rather than runtime optimization. But it sounds like it isn't possible in any case. I don't have the Agda/theory chops to fully understand your linked comment, but thanks for the reference.
&gt; IPFS (a distributed and immutable filesystem) &gt; You're not limited to IPFS for hosting Dhall expressions. Any pastebin, web server, or Github repository that can serve raw UTF8 text can host a Dhall expression for others to use in nix, there's a function `fetchurl` that takes both a url and a hash, hashing the file against it once downloaded. like dhall paths, it's "effectful but deterministic". I assume that "immutable filesystem" means that you can't change a file's contents without changing its name; but, for arbitrary locations (like github), does dhall support pairing a path with a hash?
Timer-based interrupts are ugly. My ideal world would be open CPU designs coupled with a terminating (non-Turing complete) language where compiling an expression would give the clock cycles necessary to execute it. Stick that in an FRP framework, and you can poll for input rather than relying on asynchronous interrupts. I'm kinda working on the language part of this idea right now.
I usually use Stackage as a Hoogle search engine for this reason: https://www.stackage.org/lts-7.1/hoogle?q=%3C%3E
No. I am looking for an associative array, like a hash based dictionary. Data.Array is a regular array.
While it is true that good type classes have laws attached to them, I'm not sure I quite subscribe that extreme perspective. I think the point is just that one should not just use type classes as a way of overloading symbols. A typeclass is supposed to be an abstraction where the functions commanded by the type-class actually have additional guarantees that cannot be expressed in code: laws. That said, `Data.Default` from `data-default` does have a class exactly like the one they use - and I find it handy sometimes. What are the laws for `Typeable`, `IsString`, `IsList`, `KnownNat`, or `NFData`? These are all in `base` and I use them regularly. Even classes like `Read` and `Show` have no laws unless you implement both of them. I am being the devil's advocate here just to point out that while type classes ideally _should_ encapsulate abstractions that are sufficiently interesting to have laws, that isn't always the case.
https://wiki.haskell.org/Taking_over_a_package
I meant to post this last time and completely spaced it: http://phaazon.blogspot.com/2015/07/dont-use-default.html
The two most important papers that helped were: * [A tutorial implementation of a dependently typed lambda calculus](https://www.andres-loeh.de/LambdaPi/LambdaPi.pdf) - This one was basically my Rosetta Stone for understanding how to translate typing judgements to Haskell code * [Henk: A typed intermediate language](https://www.microsoft.com/en-us/research/publication/henk-a-typed-intermediate-language/) - `morte` is almost exactly the Henk language described in this paper. The paper is also a fantastic introduction to pure type systems which really helped clarify a lot of introductory type theory for me After that, a lot of what I learned was from having fun playing with `morte` and seeing what I could (and could not) implement within the language. Finally having a real System F engine to experiment with helped a lot. One paper that really helped here was: * [Automatic synthesis of typed -programs on term algebras](http://www.sciencedirect.com/science/article/pii/0304397585901355) This basically explained how to mechanically translate most Haskell data types to System F Then most of what I learned from that was people asking me really difficult questions whether or not something was possible to implement in `morte` on the issue tracker and me doing my best to try to answer their questions to the best of my ability
The link given by /u/gelisam is the right path to take. I'd also like to point out that you can see a history of some packages takeovers on the [hackage trustees github issue page](https://github.com/haskell-infra/hackage-trustees/issues). Herbert does an excellent job taking care of these.
This is really neat use of System F. It would be cool if someone made an html templating language that was like this (although I'm not sure what kind of syntactic sugar you would need to integrate it with an an XML-style DOM).
I think you can just say "it contains the lambda calculus as a subset" and be done with it :)
How recently did you try it? It has been pretty stable for me, but I might not be giving it a workout.
Whoo! `magnet:` URIs are, tragically, are not very well known, in part, I think, because they got sort of bundled up with bittorrent (and some smaller file sharing systems) as their sole user, to the extent that web browsers considered the URI scheme itself a reason to go launch an external program, rather than something worthy of deeper inspection. They really are a genius design, far ahead of their time. (Both the W3C "SubResource Integrity" standard and hilarity like https://www.w3.org/blog/systeam/2008/02/08/w3c_s_excessive_dtd_traffic/ would not have happened if they were more widely applied.)
Woah. Any more details? Were they considered ineffective? 
Seriously. I don't like Facebook, but it was good having a big company utilizing haskell. 
Fair enough. I think the expectation is for finite domains you should get a roughly fair distribution, and for infinite ones you should get something that is is roughly fair for any given size, and scales no more than polynomially in domain as size increases. But I agree that's pretty rough-hewn and not universal...
Nu-hoogle is still in alpha. It doesn't have type-based search implemented yet, which is the defining feature of hoogle.
I don't agree, I'd rather have pragmatism than mathematical purity any day. Sure, the standard library should probably avoid lawless typeclasses but if it makes sense to use a typeclass for practical reasons then go crazy. Sure, you can argue that having a "zero value" is what `Maybe` is supposed to be for, but `mzero` is totally useful to synthesise values in the case of, for example, `fold`. If anything, the issue is that a generic `Default` class that puports to apply to all situations is a red herring, you'll never come up with something that can apply to any situation. This is why universal zero values are a problem in Java, JavaScript, C#, Python, C++, C, etc. etc. etc. (not to mention the complaints about Rust's stdlib `Default` trait), and one of the reasons Haskell is so much nicer than any of those. If you have a domain-specific reason to allow value synthesis though, I'd say there's no problem there.
Fortunately NaNs (except possibly for signaling vs quiet) are indistinguishable in behavior without looking at the exact bit pattern. I suppose I should clarify that I want signaling NaNs, which can, on some platforms, trap. 
Thanks for correcting me on that. I only glanced at the documentation earlier. It does look thorough.
I actually had this exact discussion today. A number of people argue that type classes must have laws. I definitely share the general sentiment that it is better for type classes to have laws. But the extreme view that ALL type classes should have laws is just that...extreme. Type classes like `Default` are useful because they make life easier. They reduce cognitive overload by providing a standardized name to use when you encounter a concept. Good uniformly applied names have a host of benefits (see [Domain-Driven Design](https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215) for more on this topic). They save you the time and effort of thinking up a name to use when you're creating a new instance and also avoids the need to hunt for the name when you want to use an instance. It also lets you build generic operations that can work across multiple data types with less overhead. The example of this that I was discussing today was a similar type class we ended up calling `Humanizable`. The semantics here are that we frequently need to get a domain specific representation of things for human consumption. This is different from `Default`, `Show`, `Pretty`, `Formattable`, etc. The existence of the type class immediately solves a problem that developers on this project will encounter over and over again, so I think it's a perfectly reasonable application of a useful tool that we have at our disposal. EDIT: People love to demonize `Default` for being lawless, but I have heard one idea (not originally mine) for a law we might use for `Default`: `def` will not change its meaning between releases. This is actually a useful technique for making an API more stable. Instead of exporting field accessors and a data constructor, export a `Default` instance and lenses. This way you can add a field to your data type without any backwards-incompatible changes.
I wrote an entire blog post on exactly this subject: http://www.haskellforall.com/2013/04/defaults.html
Just found/watched [this very nice, accessible talk](https://www.youtube.com/watch?v=Cy7jBYr3Zvc) about pointfree, if you're interested.
I thought the `Show` law was that you got legal haskell code that could be copy pasted into an interpreter with the right imports and get that same value back. And similarly I thought the `Read` law was that some subset of the valid source code you could use to generate the object could be put in a string and read to get that same object.
Keep in mind that Github by default has the typical millennial hipsterplatform problem of sending you notifications about literally everything, resulting in people simply ignoring all notifications since they're largely noise. This is of course *entirely* hypothetical, and if you checked my Github account you'd find no evidence of me being so aloof.
Yeah, apparently the simplest lambda calculus exploit in the book breaks Nix: $ nix-repl nix-repl&gt; (x : x x) (x : x x)&lt;Enter&gt; Segmentation fault: 11 
I've found this to be an excellent book: https://www.amazon.com/Type-Theory-Formal-Proof-Introduction/dp/110703650X It's geared toward the core theory as opposed to implementation, like TaPL is, but it will get you up to the CoC quite quickly.
&gt; Reading a value from memory is generally much slower than simply recomputing it, Except what if to recompute it you have to read two or more other values from memory? :-P
[A strongly related discussion] (https://www.reddit.com/r/haskell/comments/5g8nd0/golangs_realtime_gc_in_theory_and_practice_from/)
&gt; Our experiments have shown that closure code specialisation &gt; can buy performance when it is used to remove dynamic &gt; space overheads. A 25% code bloat over stop-and-copy (an &gt; additional 15% over our previous Non-stop Haskell collector) &gt; buys us an incremental generational collector that runs &gt; only 4.5% slower than stop-and-copy when averaged over &gt; our chosen benchmarks and around 3.5% faster than our &gt; previous Non-stop Haskell collector (of course the bene- &gt; fits are greater when operating with limited memory, i.e. in &gt; the confines of a fixed-size heap). &gt; Although specialisation opens up a number of additional &gt; optimisation opportunities, for example write barrier elimination, &gt; they appear to buy very little in practice. With respect &gt; to building a production garbage collector for GHC &gt; our preferred option is therefore to use specialisation to remove &gt; the dynamic space overheads and provide a fast implementation &gt; of Bakers algorithm that avoids the read barrier, &gt; and to pay the small additional cost of the write barrier to &gt; limit the code bloat. A 4.5% decrease in speed for more consistent GC latencies in worst-case scenario's like that of OP's seems to be acceptable.
What about FP Complete? It is a heavy data processing company. They already find a ways to good use of money for Haskell improvement.
And is the engineering effort a question of financials? If so, perhaps the Haskell community could set up a Patreon or the like? I wouldn't mind paying 50 / year to allow people to work on it.
Hi Chuck, Simple answer: The return type of `getNext` needs to be constrained to something in order to have meaning (it's too generic). So `getNext 99 :: Double` would work. Long answer: So, from looking at the other answers, you can probably see that this occurs because the inferred type of the functions given involve floating-point numbers. So why does `getNext` fail where the others succeed? You'll notice that the type of integers `:t 5` is `Num a =&gt; a`. This is a generic type that could resolve to anything that implements `Num`. The "genericness" is propagated through functions, so the type of `id 5` is also `Num a =&gt; a`. The type of `getPlace` requires a `Num` output (`getPlace :: (RealFrac inp, Floating inp, Num out) =&gt; inp -&gt; out`), the type of `getDigit` requires an `Integral` output (`getDigit :: (RealFrac inp, Floating inp, Integral out) =&gt; inp -&gt; out`) but the type of `getNext` requires a `Floating`, `RealFrac` _and_ `Integral` output. If you check `:t getNext 5` you can see that the type of the applied `getNext` is `(RealFrac a, Integral a, Floating a) =&gt; a`, i.e. it could be anything that satisfies those constraints. `:t getDigit 5` and `:t getPlace 5` also show generic bounds, but they are only constrained by `Integral` and `Num`, respectively. GHC (maybe it's built into the Haskell spec, I'm not sure) has a special typing rule where if it finds a value of type `Num a =&gt; a` or `Integral a =&gt; a` (the latter is a subtype of the former) in a place where it needs a concrete type, it will resolve it to `Integral`, the infinite-precision int type. Hence, when you call `getDigit 99` and `getPlace 99` you see something, because Haskell calls the `show` function from `Integral`'s `Show` instance. With `getNext`, it requires `RealFrac`, which `Integer` doesn't satisfy, and as there is no default type for `Floating`, there's no concrete type to choose for `getNext 99`. Therefore, GHCi has no idea what version of `show` to call, and so it asks you to resolve it to a concrete type in order to make it obvious. This is why it says `In a stmt of an interactive GHCi command: print it`. GHCi converts anything that isn't an IO command into `do { let it = &lt;&lt;the line of input&gt;&gt;; print it }`. It's `print` that requires a `Show` instance. Haskell will try to keep things as generic as possible until it _absolutely needs_ a concrete type. I hope this helps you out in the future. Lots of love, /u/stumpychubbins
On Windows this combination is great! I would like to add those Atom packages: * language-haskell (atom-haskell) * [ide-haskell](https://atom.io/packages/ide-haskell) (atom-haskell) * ide-haskell-stack (atom-haskell) - for Cabal/Stack based projects support * [haskell-ghc-mod](https://atom.io/packages/haskell-ghc-mod) (atom-haskell) + ghc-mod + hlint + stylish-haskell - everything you'll need about types hints, go-to-definition, visualization, etc. * autocomplete-haskell (atom-haskell) * ide-haskell-hasktags (atom-haskell) - for fast search of symbols around projects * ide-haskell-profiteurjs (atom-haskell) + profiteur - for visualizing profilings in Atom * ide-haskell-repl (atom-haskell) + GHCi (immature still, but improving often) * "fonts" + FiraCore font - Great visualization of Haskell operators math-like * haskell-unicode-snippets * [Hayoo](http://hayoo.fh-wedel.de/) for search. It gives a lot more type based information than [Hoogle](https://www.haskell.org/hoogle/) at this moment. [atom-haskell](https://atom.io/users/atom-haskell) is a great selection of Haskell packages for Atom. The author is very-very responsive, positive person and willing to help! Atom is working great on high-DPI displays, too. For me under Windows this is best combination currently! 
There are at least informal laws for `NFData`, i.e. data must be evaluated to normal form and must not be changed otherwise. On a sidenote, I think it's in `deepseq`, not in `base`. I think you might be able to come up with informal laws for `IsString` and `IsList` as well. The problem I see with `Default` isn't that it's essentially lawless but that the concept of a default value is not well defined. With a monoid, the "default" value `mempty` has meaning in that it is the identity element with regards to the monoid operation. But in `Default` it can really be anything. Then there's the question what happens with `(Default t, Default a) =&gt; t a`. Say lists of integers. Should it be `[]` (default for list) or `[0]` (a singleton list containing the default of its element type), etc.
stack is actually using cabal library inside
How is `&lt;&gt;` super ugly?
A project being completely inactive and an unresponsive author can possibly be two separate things. For the former the link https://wiki.haskell.org/Taking_over_a_package posted by /u/gelisam/ is definitely the way to go. For the latter then forking with a clear manifesto and notifying dependents about why they would want to switch, may be your only option.
Great title haha, thanks! 
Thanks, but I'm not alone; there's other Trustees too doing an excellent job! :-) I'd also like to point out that you can also ask us to do an NMU (non-maintainer upload) in urgent cases for obvious/trivial fixes if it's an important package blocking the rest of the ecosystem, see [here for details](https://github.com/haskell-infra/hackage-trustees/blob/master/policy.md#3-source-changes-simple-patches). 
Not at all. That is only the case for _derived_ instances of `Show` and `Read`. The only law I know of for `Show` and `Read` is the one that binds them to each other: `read . show = id`. There is also `showsPrec d x r ++ s == showsPrec d x (r ++ s)`, but IMO that doesn't convey much of the essence of `Show`.
&gt; A front-end library for Selenium Isn't [webdriver](http://hackage.haskell.org/package/webdriver) such a library?
[Protocol buffers](http://hackage.haskell.org/package/protocol-buffers) which I fail to maintain needs love. Need to migrate to latest haskell-src-exts, for example. Would be nice to support protobufs version 3. Need to add documentation with nice examples. Help would be highly appreciated.
Cool, but please label your axes. 'jobs' doesn't tell me where you started counting. From 0? From 1?, ...
&gt; I think the point is just that one should not just use type classes as a way of overloading symbols. Ironic, since overloading symbols is why Haskell classes were introduced in the first place.
Each individual heap is still unbounded in size, and collected with a stop-the-world collector, so pauses are still unbounded. An individual thread with a small heap would be immune to the long pauses experienced by threads with large heaps, though. Also, having no sharing brings other tradeoffs, namely the overhead of replicating data or managing other methods for explicit sharing.
There are no tickets. I have some vague ideas about how I would approach it, but nothing concrete.
I totally agree that classes with laws are better but I don't get the "we can't reason if there is no law". We might indeed can't , but we probably don't need to reason about it. In the default example, what is the advantage of using `initialThing` vs `def`. I can ask the reverse, what do you gain using `initialThing` vs `def`? People claim you can't reason with `def` but what can reason better with `initialThing` ? Default is very handy when there is a sensible default which is not a Monoid, example sql configuration, all things, which have more fields that you care about. Making it an instance of Monoid would imply to have to maybe all the fields, (which means giving a implicit meaning to each "missing" value). so I have a basic MySQL configuration,` {host = 127.0.01, port=3306, user="root", password="password"}` or maybe I should use a socket instead and not use a port. The thing is I don't care I just want to be able to write `myDef {user=..., password...}`. Now calling `myDef`, `def` or `mySqlDefaultConnection` doesn't really change anything so I'm not sure why the Haskell Police should waste it's precious with this. At least with `def`, I know that I expect a sensible value and I don't have to hoogle anything to figure out which name I should use. Ok, I can't "reason" about def, but I can't either with mySqlDefaultConnection. It's not like I was planning to write plenty of polymorphic functions with a Default constraints. In fact it's the opposite, I'm using default only when I need to write non polymorphic code, i.e. when I need to instance out of thin air. 
use `structured-haskell-mode`. It takes care of the indentation better than `haskell-mode` default indentation rules
I don't agree with this in general. Maybe for Haskell it's ok, since Haskell allows silly things like inhabiting every type, but for a less logically loose language, I disagree. In this world, this typeclass is called `Decidable`, and it makes perfect sense for looking for definitional equality proofs, or really any proof where the means of constructing the proof is not important (I won't say "irrelevant", since for example, `&lt; : (a b : N) -&gt; Type` may entail a `c : N` such that `a + c = b`, and that `c` may indeed be used at runtime). Using a type class here allows leveraging instance search as an automated theorem prover: for example, `Decidable a, Decidable b =&gt; Decidable (a , b)`, and if you can express proof irrelevance in your language (i.e., when ambiguous instance resolution is unimportant), even `Decidable a =&gt; Decidable (a || b)` and `Decidable b =&gt; (a || b)`.
I see, I probably don't understand the differences in use-cases between the libraries well enough yet. By the madness I meant the situation when you have a library supported by conduit or pipes and not the other. Then, you're not "free" to choose whichever you prefer because you might not have the library you need. (Based on what you say, this isn't a problem at the end.)
Looks impressive! How about changing the syntax a little bit. Instead of lengthy let A = ... in let B = ... ... in { } I would also allow: let A = ..., B = ..., ... in { } (note the comma at the end of definition of A and B)
`cabal install structured-haskell-mode`
Yes and if this does not work, build from source :)
Thanks for the thoughts all, it's really helped me clarify my understanding. Why not typeclasses with defaults? Because: * defaults are more specific to how an input is used than to the type of the input * typeclasses are something of a blunt instrument for managing this (not first class, orphan instances problems, etc.) Disadvantages of lawless typeclasses [still figuring this one out]: * laws function like a kind of documentation to determine how to use a type classes, without them you're still figuring it out * sometimes (always? often?) lawless typeclasses privilege a certain implementation for a type when many are valid (e.g. `Arbitrary`)not sure this is valid because `Monoid` is lawful &amp; has the same problems. Reasons to consider lawless typeclasses despite these disadvantages: * Sometimes it's useful to have a single name for an operation across many data types
Looks interesting! The layout is wonky on mobile, though. Code blocks overflow to the right and you can't scroll the screen sideways without triggering Blogger's "swipe to change article". 
Yes, at least for most of my purposes, good documentation and laws are exactly interchangable. :)
Pauses are unbounded in theory but in practice not much of a problem because collection can run while the process is waiting to receive a message or IO result. But yes, that is why it is only soft real-time.
I mean infrastructure software, such as Docker, Kubernetes, consul, etcd, prometheus, grafana, etc.
Neat! What's the reason for not allowing arithmetic on integers?
Really interesting stuff! But code in variable-width and text in monospace font? :)
This sounds like [Lustre](https://en.wikipedia.org/wiki/Lustre_\(programming_language\)).
That's fantastic!
/u/dxld did a great job explaining the recommended solution (it's called a 'gap' in a string). As for the error in your code, however, **you need to put parens around `"all" ++ "those" ++ "strings"`**, otherwise your code is interpreted as `(myFunction var1 var2 "first part") ++ "more" ++ "long" ++ "parts"`, because in Haskell function application (`f x`) binds tighter than all infix operators (`++`, `+`, `-`, etc.)
Alternatively `myFunction var1 var2 $ "first part" ++ "more" ++ "long"` because parens are gross ;)
Other people have already given the right answer, but here's what I prefer to do: myFunction var1 var2 $ concat [ "uhnoheutnuhtoenhunt" , "uhtoehutnhotnheutnhotnjbtou" , "uhontbjknt uhoetnhtnbjt" ]
I really like the idea of pulling expressions from IPFS. It would be great to have this feature in regular Haskell as a better alternative to imports.
I've been working on something similar for Google Cloudstore, which is basically Google's DynamoDB. But I'm not nearly as far as you. So far it just stores any JSON `Object`. Unfortunately the documentation isn't available for your package, but I'm wondering whether it would make sense to separate out the DynamoDB-specific parts, so we can also plug in `gogol-datastore` into the same machinery. 
[Corrode](https://github.com/jameysharp/corrode), a C-to-Rust converter.
I've got [a GitHub repo](https://github.com/ElvishJerricco/nix-cabal-stack-skeleton) with a brain-dump of all the relevant stuff. For some reason, this setup causes the Emacs plugin to reinstall the `intero` Haskell package somewhat often; but you can fix that by adding `haskellPackages.intero` to the `buildDepends` in `shell.nix` (though I might recommend keeping a `stack.nix` file for `stack.yaml` to point at that imports `shell.nix` so that your ordinary `nix-shell` isn't polluted).
Sorry, wrong link https://github.com/google/proto-lens
A few things I wish were un-crufted and updated: Maxima CAS bindings https://github.com/danbst/hsmaxima Libraries for biology research: https://github.com/ingolia/SamTools https://github.com/ingolia/SeqLoc I also wish there was a comprehensive unsafe version of gnu science library (GSL) bindings. The ones through hmatrix are pretty slow due to the safe FFI overhead.
I think it works fine here because of structural typing ("anonymous" products and sums). I'm not sure it would work as well with nominal typing (data, newtype) in Haskell.
I'm intrigued, but that looks proprietary and it seems hard to find any actual code examples.
numpy-competitive numeric library with scikit on top of it.
There is a raft implementation in use in one of the block chain like projects, can't remember it's name, but the project built on top is called masala.
If I understand correctly, Lustre is a theoretical language that has been published about (and maybe has some open-source implementation? I don't know). Scade is its user-facing, proprietary version. You might be interested in the [Lustre research paper](http://www6.in.tum.de/pub/Main/TeachingWs2009Echtzeitsysteme/procieee1991-3.pdf) wikipedia links to.
&gt; Keep in mind that I've already: &gt; &gt; * Raised an issue. &gt; &gt; * Submitted a pull request with a failing test-case or given a minimal reproducible failure. &gt; &gt; * Pinged the author after months of inactivity. This is fine, but it seems like you also need "Submitted a PR which fixes the bug" on the end of there before taking any further action.
I'm grinning at the teasing, but I'm also going to pretend this is serious - for once, it actually occurred to me I should say so up front, so I only seem as much of a crazy person as I actually am. Anyway... Direct answer - if they happen to be in the exact same cache line the second is effectively free, otherwise it's obviously two slow reads plus the original computation, so the single slow read for the precomputed result obviously wins. In the end it's about the numbers - if you run realistic code compiled by two compilers, one that uses pervasive lazy evaluation, one that doesn't, which runs faster. I don't need to do that experiment. It was done back in the 90s, a time when the hardware gave strictness far fewer advantages (in the early 90s, there were still machines that didn't even have cache and with operating systems that didn't implement virtual memory), and lazy lost. Luckily, the religious loonies who followed that cult didn't lose faith (though some things SPJ has said in some talk videos suggest maybe he wasn't that far from it). They did the hard work to develop and prove the compiler technologies needed to cope with laziness, so it's now a solved problem. Nevertheless, a test of pervasive strict vs. lazy evaluation order won't go well. You need the compiler to optimize most of the laziness away, you need the understanding and idioms to deal with some more issues, and you need to explain that to skeptics without seeming to be a religious loonie in denial of the facts. It might help to actually not be a religious loonie in denial of the facts but, so far as I can tell, all programmers (actually, all people) are religious loonies in denial of some bunch of facts or other, so that particular experiment has probably never been done. Anyway, the point is you need the real-world performance comparisons that have already been done, and you need to be realistic about what they mean. GHC can compete with and even beat C for performance. I've not looked at specific benchmarks. Some occasionally get pointed out for being super-optimized non-idiomatic Haskell, as is usual for benchmarks of course, even for C benchmarks. More practically, the normal Haskell goal is to be fast on its own terms (performance of real-world Haskell projects is more important that keep comparing back to C), but still reasonably competitive with C, yet with less development time, fewer bugs, and less maintenance hassles. It's not that clear how important lazy-semantics-by-default are to that, if at all. It's not even clear that Haskell is the one true perfect language, mainly because it's not, though IMO it's certainly a very good one. One thing that surprised me about it when I first started seriously learning it in order to better pursue my crusade against it was that despite some impressions I got from the enemy religious lunatics, actually Haskell is entirely consistent with my relatively sane religion of engineering trade-offs and the absence of silver bullets, and even grudgingly allows my insane religion of architecture astronaut over-design (which is the right thing really, particularly the "grudgingly" bit). 
&gt; how to get it running in a production environment, not just on a single laptop I managed to do it by using the TCP transport and manually constructing the EndPointAddress as "$IP:$PORT:0". That is, each node has a [list](https://github.com/gelisam/chopt-test-task/blob/master/node_list.txt) of the IP and port number of all of its peers, obtained however you like. In this example I'm using a static file listing all the nodes, each node is given their own IP and port as a command-line argument, and they remove their own entry from the list. For easy testing all the peers in this example run locally and use 127.0.0.1, but I've also tried it with multiple machines on my local network and it works fine, it's not local only like SimpleLocalnet. Anyway, each node begins by [creating a TCP Transport](https://github.com/gelisam/chopt-test-task/blob/master/src/Network/Transport/MyExtra.hs#L39) using their own IP and port number. The resulting address always seems to be "$IP:$PORT:0", I don't know under which circumstances the last number could be non-zero. Then, each node creates a corresponding [Endpoint](https://github.com/gelisam/chopt-test-task/blob/master/src/Network/Transport/MyExtra.hs#L61) and attempts to [connect](https://github.com/gelisam/chopt-test-task/blob/master/src/Network/Transport/MyExtra.hs#L77) with all the other nodes. This is likely to fail since some nodes will be ready faster than others, so we need to try repeatedly until we connect to every node. Hope this helps!
I haven't found a program that generates Haskell client-side code from a Swagger specification, which would save me a fair bit of time in consuming Swagger APIs.
The code looked very similar, but after looking at the API as well, I agree that they indeed are too different. Probably better to release two separate "simple"-libraries.
[duplicate](https://www.reddit.com/r/haskell/comments/5go3sb/dhall_a_nonturingcomplete_configuration_language/)
Thanks for sharing. How is your dynamodb lib different from amazonka-dynamodb: https://hackage.haskell.org/package/amazonka-dynamodb?
Maybe. Haven't given it too much thought. You might be interested in the semi-standard [pretty package](https://hackage.haskell.org/package/pretty-1.1.3.4/docs/Text-PrettyPrint-HughesPJClass.html).
&gt; A mature GraphQL library (there's one started on Hackage, but it looks like development has slowed) I've always wanted a lens based api for GraphQL. GraphQL almost seems like a protocol description of lenses ie: ``` product { id } ``` would return the `id`s for all available products... I'm not sure how to go about implementing that though, I haven't dived deep enough in the lens library code.
So the way I originally had it was this: let a = ... let b = ... in ... However, I removed that because it didn't buy the user much and it just provided another way to express the same thing (and it also complicates the code for avoiding variable capture and rendering nice error messages for `let` expressions). For either proposal you're only saving a few characters and there's no semantic difference. For the Dhall language specifically, I err on the side of being simpler instead of being prettier.
Windows support for vty (and brick) so that we can have easy cross platform text UIs. One true command line options lib providing all the best features of cmdargs/optparse-applicative/docopts/turtle/etc. with optimal usability, learnability and docs. (Or, at least an actively maintained and improving docopts). Cleanup and packaging (or base merging) of Hledger.Utils.Debug or similar providing easy and standard runtime-selectable debug output. An actively maintained fully cross platform easy to install feature complete games lib/ecosystem using sdl2 or whatever. 
What do you propose? From my perspective, `Arbitrary` is a great argument *for* lawless typeclasses. It's useful to me (and I dare say a number of other people). And I don't immediately see how adding laws would make it better. I think there are a *number* of other points that speak against taking "classes should have laws" as an absolute rule. What's the law for `KnownSymbol`? Or `IsLabel`? Are classes like Read, Show, ToJSON, FromJSON, etc really only useful when paired together (otherwise how would you specify any laws for them?). Should `servant` be scrapped because most of its typeclasses have no laws? Should we never traverse a type-level list? If I represent a Monoid as a datatype rather than a class: data Monoid a = Monoid { mempty :: a, mappend :: a -&gt; a -&gt; a} Does that mean Monoid laws are no longer relevant? Why not? There's an important message - be sure you have a clear semantics for your classes - that's somehow been replaced by "have laws". Having laws is only one solution, and sometimes a useful metric. It's very useful to clarify your thoughts sometimes, especially when your thoughts were about an abstract structure like a Monoid, which is *defined* by laws. But sometimes you're thinking about how to map Haskell data structures into JSON. An Int, say: there's really only one right way to do it, it's obvious what it is, but we'd be pushing it to say that because there's a law undergirding the instance. And sometimes you're just doing type-level programming. I dislike `Default`, but I don't think it's because it doesn't have laws, but because most of it's instances are unintuitive and unhelpful. If it were just a way of not having to lookup what `defaultParametersForLibraryX` is actually called, I don't think I'd particularly object to it. *EDIT*: I should have read all the comments before posting. /u/youknownothing236 made pretty much the same point. 
That's the other way around :). There is a swagger-to-servant-types option in `swagger` - from there you're just one function application away from a client.
So this choice (and several other decisions) are kind of an experiment in API design to test the idea that code is easier to maintain if you always "add" things and never "modify" or "subtract". The rationale is that subtraction and mutation promote "patch-oriented programming" where you start from something imperfect and try to correct the mistakes to make it perfect. This in turn promotes a software engineering anti-pattern where any time something is wrong you try to paper over the problem downstream instead of pushing the fix upstream. Dhall's built-ins basically force to you to fix your upstream inputs. Dhall consistently omits utilities that you let you introspect or selectively modify portions of your input. This is, for example, why you can't (easily) delete or modify fields of existing records and it's also why you can't parse text or even compare two strings or numbers for equality. This is also why you can't take or drop elements from a list. You can do some limited projections (like `head`/`last` or getting the field of a record), but those always project to a different smaller type, and not a modified form of the original type. Going back to your original question: In the case of `Integer`s I'm following the "no subtraction" rule literally by removing addition specifically from `Integer`s. If you had addition for `Integer`s you could subtract by adding a negative `Integer`. Also, even if you could add `Integer`s, what would you use `Integer`-based logic for? None of the other utilities work on `Integer`s because they don't make sense for negative `Integer`s anyway. This is why I make the `Integer`/`Natural` distinction because I feel that `Natural` is more appropriate for programming logic, whereas `Integer` is not and should just be an opaque value.
I often get bitten by this when writing SQL queries, by the fact it doesn't insert separators (which makes sense).
BitTorrent the company actually attempted to build a browser based on magnet links: http://blog.bittorrent.com/2014/12/10/project-maelstrom-the-internet-we-build-next/. Unfortunately it fizzled out, but I definitely agree that magnet links are underused. Does anyone know if there are any Haskell libs that make downloading magnet links easy? Also, here's the Magnet Link specification for those who like their primary sources: http://magnet-uri.sourceforge.net/magnet-draft-overview.txt
You're welcome!
For things that are not `Monoid`s that you would like to provide defaults for, I would just provide an ordinary value for that (i.e. `defaultMySQLOptions`) without any type class instance.
This is absolutely essential. We use protobuf to communicate between Java and Haskell and I feel the pain of few missing features every day. The error messages are terrible! and `oneof` is not supported. I'd really appreciate it if someone can resolve those issues.
That works pretty well for me, but has some rough edges (not surprising since this isn't what hpack was built to do). For one thing it lists build-depends in not-quite-alphabetical order.
&gt; Windows support for vty (and brick) so that we can have easy cross platform text UIs. This is a good one.
Yesod comes closest to what you ask. Although you may find it does not provide nearly as many prebuilt out of the box auxiliary services as Spring does. 
&gt; People claim you can't reason with def but what can reason better with initialThing ? Well, to me the problem with `def` is its an example of typeclasses purely for namespace overloading. Given `initialThing` I can go look for the definition it of it. Given a `def` I need to first figure out what type it is supposed to be, which could involve doing some type-inference in my head, and then I need to go figure out where the instance for the `def` typeclass is, and it might even be a compound instance such as `instance (Def a) =&gt; Def (Thing a)...` where I need to trace back some other defaults before seeing the whole thing spelled out, etc. I'm not suggesting the "Haskell Police" waste time with this. Write your code how you want. I'm telling you why I, personally, find code that uses typeclasses for pure namespace overloading, harder to read.
Fair enough, but either you are in a polymorphic function and then you don't need to infer the type. Or (99% of the case), you are in nonpolymorphic function, and the context is enough to guess what `def`, especially if used in conjunction with field update example do connect &lt;- mysqlConnect def -- connection parameters or do connect &lt;- mysqlConnect def {connectInfoUser = "me" } -- ^ connectInfoUser ===&gt; ConnectInfo :-) Having said that, I don't use `def`, because I've been told it's bad. Maybe if I was using it would realize how bad and unreadable it is. &gt; initialThing I can go look for the definition it of it So it's sort of hungarian notation ... (which I use ...)
the irc channel is #haskell-beginners on freenode.net
https://gitter.im/haskell-chat/study I just bumped into this :)
I'm aware of #haskell-beginners on irc.freenode. That's my goto place whenever I've a doubt regarding haskell concepts
So you probably hear frequently that "type classes should have laws" and I will explain why In Haskell, we don't just want to write code but we also want to be able to reason about code. For example, consider the following code: example :: Monoid m =&gt; Bool -&gt; m -&gt; m -&gt; m example b x y = if b then x &lt;&gt; y else x Without knowing anything about which `Monoid` I pick, I know that the following code transformation is safe and does not change the behavior of the `example` function: example b x y = x &lt;&gt; (if b then y else mempty) The reason I can safely perform this transformation is because of the `Monoid` laws, specifically the one that says that `x &lt;&gt; mempty = x`. In a sense, the laws that a class obeys provide an interface for reasoning about what code transformations are behavior-preserving when programming generically over that interface. A type class with laws is a middle-ground between two extremes: * A completely opaque interface that leaks no details about its implementation * No interface at all A type class with laws gives you some notion of abstraction, generality, and reuse while still preserving a few useful details (i.e. the laws) that let you know *something* about how the interface should behave When a type class doesn't have laws, you can't really reason about code written generically over that type class. Without the laws you're essentially at the whim of the person writing the instance so you can't say much of anything about the code. When writing type classes that are consumed by the original implementer or somebody on their team this is not much of an issue. But when writing type classes that are designed to be consumed by the broader Haskell ecosystem this is very much an issue. This is why you see a lot of Haskell companies use things like `HasDatabaseConnection` internally because they can rely on internal communication to enforce meaning, but you get more push back if you try to expose an abstraction like that as a shared library on Hackage. You don't really have a way to clearly state which instances are semantically valid without laws. So, going back to `def`, the main issue with a `Default` class is that you can't really say much of anything about code written with `def`. Is there a useful example of a reusable function that uses `def` that actually works for more than one type or is it always being monomorphically instantiated to a single type as a convenient syntactic shorthand?
are you using this "https://github.com/LumiGuide/haskell-opencv" ?
e.g. data Basket = Basket { items :: [Product], created :: UTCTime } would not work?
No thanks . IRC works *much* better than Gitter and has an existing community. 
There are IRC bridges out there for Gitter. This came up in a google search https://irc.gitter.im/
How do you deal with created when writing `mappend`. You are given two dates (one for each data to append, which one do you get ? the first one, the second on the sum of the two ? How do you write `mempty` ? Is created null date '0000-00-00' ? Then you'll be better having a `Maybe UTCTime`.
It looks like I am the only one using IntelliJ (community edition) with `HaskForce` plugin. I think IntelliJ is a much better environment than Atom or any javascripty IDE.
Yeah, lags and so on, so no. Gitter is quite annoying.
Personally I prefer IRC. But I need to use a hosted service to really get the most out of it (including offline scrollback, etc). In my case this is irccloud. And I can see why irc is off-putting to many folks, especially of newer vintage, who are less used to its conventions. So having a web-based chat alternative for them seems reasonable enough. We have plenty enough Haskell users these days that I don't worry so much about a fragmentation of discussion channels -- there's too much to keep up on regardless :-)
Some ideas: - when you want to minimise development and maintenance cost, allowing more time for feature development - when you want to minimise packaging cost, encouraging wider availability - when you want installation to be easy, independent of platform or C libs - when you want instant startup and responsiveness, including on older machines - when a text UI fits better into your workflow, eg because you want to run it inside emacs or an IDE or iTerm2's drop-down hot-key console for easier window layout or text resizing or copy-paste or accessibility, or in dtach/screen/tmux for suspendable sessions..
I miss a common flat record description class to be able to write instance similar to this, only once parse m = MyClass &lt;$&gt; m .: "column1" &lt;*&gt; m .: "column2" &lt;*&gt; m .: "column3" Basically, to be able to read/write CSV, MySQL, PostGRES, JSON , Html table etc without having to write the exact same code 10 times. I know some of this can be (has been solved) using Generic or TH but that only works when the column name can be deduced from the field name, which is not always possible.
Although I agree with u/sjakobi (use hpack), isn't this what `cabal format` does? 
Could you elaborate ?
Yep I care about the latter one (make it impossible for foo' to inspect its arguments). And the reason I care is to be able to deduce what function does, without reading its implementation or documentation. It must be clearly visible in its type, that the function doesn't make any inspection on its arguments.
Any implementation of the 'grammar of graphs` in Haskell would be great. There are some abandoned projects in GitHub that tried but did not get to completion... But I appreciate `chart`, it does what I want most of the time.
Have a look at https://github.com/transient-haskell/transient and https://github.com/transient-haskell/transient-universe as well. You could probably add resource scheduling quite easily. I was pretty astounded at how easy it was to use.
It always seemed did odd to me that `hashtables` doesn't support "freezing" the entire table.
Interesting, would check it out for riak package. Btw, riak package needs some love as well :)
Do you mean a new package? Anyways, exciting news :)
For the service layer, what I mean by transaction management is: When you're building a typical Spring/Hibernate app, you can have Hibernate manage DB transactions for you at a very high level. The idea is that in 3-tier business apps, a service-layer method encapsulates a single "business operation", and all DB changes that result from that method invocation should be considered to be part of the same transaction. Now under the hood, your service method could, and probably is, making multiple separate DB calls. Which means that you need a way to keep track of all these separate calls so that in the event of an error, you can roll back all the changes from all these separate calls. And of course, you could have a service method call another service method, and you need to have a way to specify whether this second "inner" call is part of the same transaction of the first "outer" call or not (transaction propagation). This would all be an absolute nightmare to handle manually, but luckily Spring provides some annotations that basically make it all automatic. Quick example: public class SomeService { @Transaction public serviceMethod1(){ someDao.updateSomeTable(); someOtherDao.updateSomeOtherTable(); } } So as a simple example, if someOtherDao.updateSomeOtherTable() fails, then you need to rollback any changes that someDao.updateSomeTable() made in order to maintain integrity. The @Transaction annotation does this for us. Another example of how this gets used, is for testing. So you have some integration tests that run against your DB. But you can't let your tests leave a bunch of cruft in the DB after every test run. If you have a test that insert a customer with ID 1, well then obviously you can't run your test suite a second time, because the insert will fail the second time. Now you could setup your integration tests to create a new clean test DB every time, and that's probably a good idea in general. However, @Transaction saves us from even that level of effort, because we can use the annotation to automatically rollback all DB changes after each and every test method! It's incredibly useful.
Hi! If you want distributed task scheduling with built in load balancing, you might want to take a look at the HdpH library, as described in our paper: **"The HdpH DSLs for Scalable Reliable Computation"** http://www.macs.hw.ac.uk/~rs46/papers/haskell2014/HdpH_DSLs-haskell14.pdf And if you want fault tolerance, check out the DSL extension HdpH-RS (my PhD): **"Transparent Fault Tolerance for Scalable Functional Computation"** http://www.macs.hw.ac.uk/~rs46/papers/jfp2015/JFP2015-Stewart.pdf These DSLs are written in 100% Haskell, and share the network-transport layer from CloudHaskell in their software stacks.
Great thanks! But since you seemed interested when I posted previously, FLTKHS deploys pretty easily to Windows if you ever need that capability.
I like the OCaml pattern: ``` let A = ... in let B = ... in ... ``` no multi-let necessary. Especially if there is no let rec.
Thanks, and that points out some basic flaws in the current nix implementation. However, I do not think general recursion has been used in any capacity through out the Nix eco-system. It could be just a matter of tightening up the type aspect in the nix language specification to base it on simply typed lambda calculus, or CoC. Nix looks like the only prominent application that demonstrates the power of functional programming as far as a configuration language goes. Dhall could be a good playground to experiment with language design choices, and I hope some of your ideas can eventually go into contributing and improving existing systems like Nix. 
Yeah, at first I thought something like that wouldn't work because Nix actually does some basic type-checking. If Nix type inference and checking could improve it would fix a lot of these problems. There are a couple of issues with built-ins, some of which are difficult to type (like `builtins.listToAttrs`) There is one place where general recursion is used pretty heavily in Nix, which is the `callPackages` idiom in `nixpkgs`. Another place where I've ran into a few infinite loops by mistake is the `NixOS` module system under some circumstances.
The complex (which I'm hoping to simplify) setup is only for developers. Once the app is built you should just be able to email/Dropbox/Github release it.
Yes, he knows the date presented on the page is incorrect (this is a current posting and not from last spring). It's a bug that's being worked on in every moment Adam thinks web development is more interesting than HaLVM.
Isn't this just the `SPECIALIZE` pragma?
Inspect as in, this function can't be implemented in the fully polymorphic type f :: (Int -&gt; b) -&gt; Int -&gt; b f g i = g (i * 2) OP wants parametricity but with overspecialized types.
I took a brief look at it some time ago. Does it have the dependency semantics? The use case is that a task will compute some result that is persisted to a file. A downstream task that depends on the first task does the next stage of processing taking the content of that file as an input in some computation and writes its own result somewhere. The general impression I get from the distributed- packages is that it they mainly allow remotely executing code. I didn't see anything related to the dependency semantics I was talking about. I'm using distributed-closure for the closure but implement the dependency semantics myself.
That's just the server. I need a client as well.
Just a stupid question: Is RTS reported time the actual elapsed time? or is it the total time of all cores for GC? 
We would need ~4k donors to finance a single engineer at that rate.
There is a slack channel that you can join. http://www.fpchat.com/ There is a haskell-beginners channel, where many people are working through haskellbook. Even otherwise, the community is very helpful (not just to beginners).
Would a service that could find and cluster/group similar libraries across different languages &amp; identify lib gaps in lang X be useful?
.. instead you run your HaLVM app inside a Xen hypervisor which is a 1M-line runtime system written in C. Pure win.
~200K, but yeah if your point is about TCB size then yikes.
I'm pretty sure we've added `oneof` support some time ago in [PR 19](https://github.com/k-bx/protocol-buffers/pull/19) and [PR 31](https://github.com/k-bx/protocol-buffers/pull/31)
This doesn't work on large values. It gives the right answer for 2^100, and nonsense for 2^1000, I think due to some sort of overflow...
&gt; How does warp-tls perform relative to HAProxy/Nginx which use OpenSSL? Can't say much about warp-tls in this regard, but at $work we have a two versions of a http-client based tool, and the OpenSSL based version is (anecdoatally) significantly faster. &gt; How does warp-tls fail? I have never seen a warp-tls based server fall over. I had one at my last job that ran flawlessly (but wasn't heavily loaded) month after month after month. I also have one on my home server that is also lightly loaded and again, I've never seen it fail. However, I'm pretty sure of a warp-tls based server runs out of memory or the GC gets bogged dow, it would not be pretty. 
You never know when a message will arrive, so assuming you can GC in the background to reduce pauses just isn't going to work.
I think the question is essentailly 'is there a transaction monitor/manager for haskell'. Because if you have one, it's usually reasonably easy to roll your own free/operational monad and you have it ready (do we really need a framework for this?) The question of transaction monitors - it effectively boils down to data sources supporting 2-phase commit. Which is effectively SQL databases. I don't think there is anything like this currently available, however it doesn't seem to be that hard to create. I think nobody quite needed it yet, even big enterprises have many application talk to just one DB; it's much easier from all points of view...
my god... get of your high horse
Yup!
I assume these modules are part of a regular Cabal package, right? `stack haddock` will by default also generate the haddocks for the code in the current project. So, if you have a stack project (which will contain a `stack.yaml` file) and one of the local packages in that project is called `my-pkg` you can run `stack haddock --open my-pkg` to open the haddocks for that package in the browser. If you are very unfamiliar with stack I suggest you take a look at [a tutorial](https://haskell-lang.org/tutorial/stack-build) or [stack's documentation](https://docs.haskellstack.org/en/stable/GUIDE/).
Very cool. Is there a library on Hackage for this kind of stuff?
Well, it's an additional option for networking! Don't just reject it.
https://wiki.haskell.org/Taking_over_a_package/Ruling_with_an_iron_fist
Got any more?
Could there be a security hole coming from the possibility to include arbitrary files? For example, could it be possible to somehow read `/etc/passwd` by using it as some crazy expression? I am thinking about a situation, where configuration is coming from an untrusted source. If so, I think it would make sense to provide another `input` function with more control about how external value can be fetched. Woud that be reasonable?
Those dependency semantics sound like something [shake](https://hackage.haskell.org/package/shake) might support.
Shake is a build system. Can one use it just for its dependency semantics in a non-build context?
[removed]
&gt; I dislike Default, but I don't think it's because it doesn't have laws, but because most of it's instances are unintuitive and unhelpful. This is my exact complaint about `Arbitrary`. Take `Arbitrary Char` for example. It only generates the first 256 characters of `Char`, leaving the vast body of Unicode completely untested. Decisions about other types, such as `Int`, are similarly...well, arbitrary. So I usually don't use `Arbitrary` because I'd rather not look up what it is doing. Unfortunately there are modules like `Test.QuickCheck.Function` that assume you are using `Arbitrary`, so to use those modules, you need to do a lot of ugly `newtype` wrapping. Or...I just don't use those modules. In the end, though, I pretty much agree with you. I think it is most useful to view typeclasses as a lawless ad-hoc overloading mechanism. That's how they were used in the first place; there's a lot of lawlessness in the Prelude with arithmetic, for example. I get frustrated when libraries like QuickCheck have lawless typeclasses, create a bunch of instances of those typeclasses without documenting what those instances do, and then build other abstractions on top of those typeclasses when I have a good reason to avoid the typeclass altogether. If it's a lawless ad-hoc typeclass, don't heap other useful things on top of that shaky foundation. &gt; If I represent a Monoid as a datatype rather than a class: &gt; data Monoid a = Monoid { mempty :: a, mappend :: a -&gt; a -&gt; a} &gt; Does that mean Monoid laws are no longer relevant? Why not? Nope. You can have typeclasses without laws; you can have laws without typeclasses. I understand the occasional utility of a lawless typeclass, just like I understand the utility of making repairs with duct tape. Something of temporary or expedient utility should not serve as the foundation for something else. Unfortunately that is what QuickCheck does with `Arbitrary`.
no, sorry :(
I don't see how your example imposes any difference in type checking behavior.
I would expect the graphs from `containers` and from `fgl` to obey the laws, they are not really restrictive.
https://hackage.haskell.org/package/plots appeared the other day; it's based on `diagrams`, so it's very principled by construction. Very good documentation too. And `plot` (https://hackage.haskell.org/package/plot) has been around for some time now
2-phase commit just isn't as popular these days as it used to be.
 I wrote a toy graph library a while ago for a graph theory course, and my professor told me that he was convinced that "haskell is the right language for expressing graph theoretic notions" so this is pretty neato. Fun to see an algebra forming!
what made you not use emacs before and prefer atome then ?
Except that Xen isn't 1M lines, and it is a microkernel so there is better separation and a bug in your network driver can't corrupt your filesystem. And when you're already running under Xen anyways, Xen+linux+haskell is much worse than just Xen+HalVM+haskell.
&gt; Alternatively, instead of using the complement operation, one can use the join operation, which consists of forming the disjoint union G\cup H and then adding an edge between every pair of a vertex from G and a vertex from H. So that's their `connect`. 
`connect` doesn't require the vertices to be disjoint though. *edit*: Let's see, so why is P_4 = (a-b-c-d) impossible in a cograph? Suppose it was: then it must be possible to divide {a,b,c,d} into two partitions ps and qs such that the corresponding subgraphs P and Q can be combined into P_4, either via a union or a connect operation. But it can't be the union, or P_4 would have two disconnected subcomponents. So in P_4, every vertex in ps is connected to every vertex in qs. WLOG, let a be in ps. Since in P_4, a is only connected to b, qs must only contain b. But then ps must be {a,c,d}, the connect operation would cause b to be connected to d, contradiction. If the arguments to `(+)` are allowed to overlap, then we can trivially construct P_4 as `(a * b) + (b * c) + (c * d)`. I pointed out that `connect` doesn't require the vertices to be disjoint, but it's the fact that union doesn't require so which matters: even if ps and qs were allowed to overlap, then ps could be {a,b,c,d} instead and we would still have a contradiction.
Awesome! I needed grpc for the Cloud Speech API. 
[removed]
No harm in asking. 
I highly recommend reading up on [bidirectional typechecking](https://people.mpi-sws.org/~neelk/bidir.pdf); it really simplifies writing a type inference algorithm IMO. [This talk](https://www.youtube.com/watch?v=9v4_FQm-b4I) by Conor McBride is also a pretty great educational resource for writing a dependent type inference algorithm.
Ooh, nice! 
I am in the process of reading [this paper](http://web.engr.oregonstate.edu/~erwig/papers/InductiveGraphs_JFP01.pdf) about inductive graph representation, I wonder how the representation from this post will compare in terms of performance to the structure proposed in that paper. Perhaps I could check that myself after reading is finished :)
The LDAP libraries available are pretty basic and mostly just low-level bindings to the C libraries. Features like SASL External auth (using certificates to authenticate the connection) are missing completely, there is no real modelling of LDAP Objectclass invariants on the Haskell side even for a known schema,...
Thanks!
Some ideas for proposals [here](https://storify.com/lambda_conf/ideas-for-submitting-a-proposal-to-lambdaconf-2017), based on some survey.
Don't be a jerk.
An algebra of graphs, but the post has a co-algebraic feel to me, since graphs are being built up rather than collapsed. `Graph (Basic a)` seems like it's a quotient of the fixed point of the functor `Basic`. Is there a name for this kind of construction?
&gt; The global uniqueness of instances (which was part of the design, but is not implemented; we get local uniqueness of instances, instead) My recollection is that this is a quirk of GHC, which was not generally replicated in other compilers or interpreters. In any case, I'm generally in favor of having laws for class methods, but I don't think they're absolutely necessary as long as you can describe the abstraction the class represents. More generally, a universal `Default` class is questionable, but I think a project-specific one is fine.
I wonder, how do you know that any graph which doesn't contain a square is possible to construct with your rules (the union function always creating a disjoint union)? As (I think) other people pointed out, the article does not insist that the vertices sets are disjoint in the 'union' function, so neither of our comments applies to it. Thus confused me greatly as well for a while.
LambdaConf didn't ban a speaker (with neoreactionary views) whose anonymized proposal on Urbit (functional language / operating system / VM) had been accepted by a blind committee. The FIOL organization [has more context](http://fantasyland.institute/initiatives/FATE.html). Whether people agree or disagree with that stance is largely irrelevant to the question of civility. Calling people you don't agree with "asshats who covered themselves in glory" does not help anyone in this community.
Pijul (http://pijul.org) tries to address the inherent performance problems in darcs from what I understood. See http://pijul.org/documentation/model/#efficient-algorithms
You may be interested in this paper: ftp://ftp.math.ucla.edu/pub/camreport/cam09-83.pdf
Is it as straightforward as adding a type family `Edges g` and changing the type of `connect` to `(Vertex g -&gt; Vertex g -&gt; Edges g) -&gt; g -&gt; g -&gt; g` to capture the algebra of say, quivers and other fancier graphs? Hm, you probably want to stipulate that `Edges g` is a idempotent commutative monoid too. Although that's probably necessitated by the laws for `overlay`.
Thanks!
Non-strictness doesn't change asymptotic complexity. Exponentials don't care about puny constant factors.
From the section in the FAQ the OP linked to: &gt; Fixing the underlying patch theory problems will potentially take us a very long time, but we are working on it. It is clear that this is an issue with the asymptotic complexity of the algorithm itself, not an issue with a particular implementation of it.
Thanks. Let's keep that bridge burned.
A reminder that Bryan's insulting denigration is targeted at the organizers of LambdaConf, not "Moldy Bread". And another reminder that insulting and denigrating people over disagreements is generally what 3 year olds do. Mature adults are capable of civil approaches with intellectual rigor.
The organizers of LambdaConf made their stand. They decided that including terrible people like Yarvin was more important than making a commitment to being an open and respectful community. And again, this isn't *just* a disagreement. This is a disagreement about what is and isn't acceptable behavior. Saying that certain people are subhuman and deserve to be subjugated is *unacceptable*. The fact that the LambdaConf organizers think it's totally OK to associate with someone who loves doing that means they're, as /u/bos put it, asshats. Stop conflating *politeness* with maturity and intellectual rigor. I can rigorously tell the LambdaConf organizers to go fuck themselves. Just because you have no investment because you're not the targets of Yarvin's violent rhetoric does not make you intellectually or emotionally superior to the rest of us who know an asshole when they see one. You can make a logical argument that is simultaneously contemptful and angry. In fact, it is a sign of maturity that one can articulate the connection between their emotions and their intellect. Pretending that intellectual rigor necessitates politeness is downright pretentious and faulty. Don't tell me to be civil when I oppose people who write at length about how uncivilly and dehumanizingly they want to treat me. Don't tell me this is a paltry squabble that has no consequence for the community. Don't tell me that a person or organization can't be held accountable for the people they associate with.
There was a [Netty talk at Devoxx mentioning OpenSSL vs JSSE performance](http://cfp.devoxx.be/2016/talk/XHW-5562/Netty_-_One_Framework_to_rule_them_all). The speakers' microbenchmark (cringeworthy possibly) indicates a significant speedup using OpenSSL rather than JSSE in data intensive realtime use. Seems like it was GC pressure mainly causing bad latency spikes for that purpose. Otherwise the advantage of OpenSSL may be outweighed by its security risks.
Heh.
+1 on SSH client library.
Phooey, so are mine. At least now I got it in a test so I can catch regressions, but the last time I had to find one of these it took weeks.
What about `Edges g = Set Int`?
I'm not sure what you mean. What I mean: * Normal graphs ~ (Set v, Set (v, v)) * Quivers ~ (Set v, Map (v,v) Nat) * Hypergraphs ~ (Set v, Set (Set v))
I think the right solution in this case is to not run the compiler with elevated privileges
I was thinking of a Graph implementation that had independent terms for edges as well as vertexes and the incidence relation is an auxiliary structure in the implementation of `connect`. Like my example was the type of edges labeled with `Int`.
`transient`'s presentation could and should be improved _greatly_. For example, the first `transient` function that appears in the [tutorial](https://github.com/transient-haskell/transient/wiki/Transient-tutorial), [`keep`](https://hackage.haskell.org/package/transient-0.4.4.1/docs/Transient-Base.html#v:keep), doesn't even have one word of documentation. The rest of the haddocks are also sloppy and hard to understand. Why is there a typeclass called [`AdditionalOperators`](https://hackage.haskell.org/package/transient-0.4.4.1/docs/Transient-Internals.html#t:AdditionalOperators)? Many functions are undocumented, or have confusing behavior that's described in paragraphs of prose. For example, the [`logged`](https://hackage.haskell.org/package/transient-0.4.4.1/docs/Transient-Logged.html#v:logged) docs read: &gt; write the result of the computation in the log and return it. but if there is data in the internal log, it read the data from the log and do not execute the computation. &gt; &gt; It accept nested step's. The effect is that if the outer step is executed completely the log of the inner steps are erased. If it is not the case, the inner steps are logged this reduce the log of large computations to the minimum. That is a feature not present in the package Workflow. This library is very intriguing, but ultimately too confusing for me to bother digging into. That's my first impression, as a not-total-newbie to Haskell. (And I hope I haven't offended the authors by this post.)
What is the relationship between GHC Haskell and Liquid Haskell? Obviously they share the "Haskell" name. Is Liquid Haskell a GHC extension? If not, is it a fork of GHC or what?
The biggest issue with warp-tls has nothing to do with performance, but that it uses an obscure TLS implementation that has received fairly little review, and almost certainly has bugs. Native Haskell TLS is a cool idea. It's not a production ready idea. 
Liquid Haskell is a tool that you run outside of ghc to check if a file is valid.
It's already past time that conference is boycotted to oblivion.
It certainly has the burden of proof on its side. What would make it production ready? I'm mostly worried about x509 quirks and protocol attacks myself. There was recently a thread on the user of RDRAND in cryptography which I see as inconclusive. A while back, I did a cursory review of some x509 issues myself and to me the code looks decent. I would like a list of openssl issues and how `tls` is affected though. OTOH, if you look at some the later attacks on openssl, a lot of them are caused by unsafe C, so I'm expecting that a comparison show that `tls` is mostly unaffected.
Do you have any results you could publish?
&gt; Can't say much about warp-tls in this regard, but at $work we have a two versions of a http-client based tool, and the OpenSSL based version is (anecdoatally) significantly faster. What's the difference like? 10x slower? 2x slower? 1000x slower?
Maybe you should inform yourself before jumping to tone policing? We are here talking about someone who claims races exist, that some people are genetically inferior and that they are particularly disposed to being slaves. If anything, madelinja's answers are very tame.
[removed]
The point is that there is no point to doing that, because you get the most general type from HM, so what's the point?
You should look into [F*](https://www.fstar-lang.org/#introduction)!
I and I'm sure others would be interested to hear of any war reports about using Liquid Haskell in anger? Has anybody successfully (or not) used it for non-trivial codebases, commercially or open source?
Were they removed for being full of the above rhetoric? Because I don't remember them, and Googling `site:reddit.com/r/haskell lambdaconf` doesn't yield anything like that.
Btw, if you are into gitter and speak russian, there's https://gitter.im/ruHaskell/forall
***EDIT*** This following is wrong. I have a theory that you may be a victim of lazy IO (plus the list-of-characters representation of strings), but I don't remember enough to be sure. Basically, a naive reading of your `write` suggests all of your `str` strings are kept in memory until the recursion exits. That shouldn't be true because the function is tail recursive - unless there's something that needs all those `str` strings, a lazy computation that hasn't been "forced" yet. IIRC `hPutStr` doesn't necessarily do anything right away. The documentation says [hPutChar](http://hackage.haskell.org/package/base-4.9.0.0/docs/System-IO.html#v:hPutChar) can buffer characters. `hPutStr` no doubt uses `hPutChar`. But very likely the buffering of characters is a lazy computation too, so neither the buffering nor the output will be forced until the file handles are closed (in this case I assume by the garbage collector). If that sounds bad, imagine what happens when a file you're reading gets explicitly closed when there's still outstanding lazily-deferred reads. Yes, reads that already seemingly completed successfully can fail because the file is now closed. That seems pretty crazy and broken, but that's pretty much the standard opinion of lazy IO, so in itself that crazy broken-ness doesn't mean I'm wrong (though I may well be wrong because this is a weak reconstruction from half-rememberings). Anyway, the simple answer is don't use lazy IO - in fact if you're doing lots of string stuff, don't use `String`, use an alternative library for strings and I/O. Strings as lists of characters are never going to go away, but shouldn't be over-used. Unfortunately I don't know which is the default recommendation, so I'll leave that for someone else to state. For simple learning examples, another valid option is of course to live with the limitations. 
Haven't checked it, and can't right now, but maybe add hFlush outh after writing the string?
My original statement was far too strong. That's a fair point on hardened internal networks and your guidance around offloading the externally-visible TLS is particularly on point. The primary reason I've encountered for not doing TLS on internal links is enabling network IPS without the overhead/complexity of TLS decryption. The need for TLS on every link depends on data sensitivity, regulatory requirements, and trust-level of the network used (AWS recommends encryption of all messages sent over their VPC/EC2 networks internally). I think TLS is safe default, but in retrospect it is not a requirement for most use cases.
Xen and HaLVM are not microkernels. Xen is just an hypervisor that almost delegate everything to a dom0 (usually linux) with very wide privileges that are almost impossible to un-grant (if dom0 crashes then xen can't do anything and crash as well), and HaLVM a library operating system (also known as unikernel, cloud OS) for Xen.
For example, `take 5 . sort` is O(n) in a lazy language but O(n log n) in a strict one.
No that's not the problem. `hPutStr` writes immediately.
I've used [dbmigrations](http://hackage.haskell.org/package/dbmigrations) before. It worked for me, but if I were going to do it again, I would just roll my own solution. Basically, dbmigrations allows you to have a non-linear migration history. If you're familiar with darcs, it's sort of like that in that it's patch oriented. You can specify explicit dependencies between the patch if you want (just like in darcs). The problem is, I didn't really need a non-linear set of migrations. And for that, you could basically just have a directory with a bunch of sql files with some kind of special naming scheme: 001_apply.sql 001_revert.sql 002_apply.sql 002_revert.sql At least, that's what I would do if I needed better migration support. 
This suggestion is incorrect. The IO functions in `base` only use lazy IO for reads, not writes.
OK - so the writes are forcing any lazily-deferred reads. And I assume that means although the writes may be buffered, the buffer-filling is forced immediately, so the amount of delayed writing is strictly bounded by the buffer size. But in that case, why would `hFlush` (as per davids answer - admittedly one of the upvotes is mine) make any significant difference? And what *is* keeping the whole file in memory? Damn, I thought I knew all this stuff, even if I'm definitely getting very rusty again. 
They're not lazily deferred reads. `hGetLine` is not a lazy IO function.
&gt; hGetContents, getContents, and readFile, according to https://hackage.haskell.org/package/base-4.9.0.0/docs/System-IO.html#g:23 ~~http://hackage.haskell.org/package/base-4.9.0.0/docs/System-IO.html#v:hGetLine~~ https://www.reddit.com/r/haskell/comments/5h6emf/haskell_run_out_of_memory/daxuikp/
Looks lost to the Internet. Here: https://statement-on-lambdaconf.github.io/ Perhaps someone more famous having the same opinion I do makes it more valid? There are plenty on that list. I really should spend more time cultivating my Internet presence because that is the basis on which my argument should be judged, apparently.
I think that `getLine` is not an efficient function either. https://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.IO.Handle.Text.html#hGetLineBufferedLoop Aside from returning a String (bad enough), it's a loop. And that loop conses each chunk that it receives until it hits end-of-line. Then it runs `concat (reverse xs))` on that accumulated list of lists. Then you're `read`'ing that pile of mess, and then `hPutStr`'ing it. Combine that with really long lines and time.
I think the default flushing mode is newline, and you never write a new line. 
I don't think this is the issue. You can see [in the source](https://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.IO.Handle.Text.html#hPutStr%27) that `hPutStr` is written in terms of `writeBlocks`, which flushes when `n+1&gt;bufSize` _or_ `c=='\n'` (when in `LineBuffering` mode). You can test it out: $ cat demo.hs import System.IO main = do hSetBuffering stdout LineBuffering hPutStr stdout (repeat '.') $ stack ghc -- demo.hs -o demo -O $ ./demo ............................................................ ^C It writes without any newlines.
It does not work :)
You may find this of use: https://wiki.haskell.org/Smart_constructors
The lines are about 1KB each.
The facts also deserve to be heard, so party-line propaganda doesn't overwhelm the truth in a cloud of foggy mist. LambdaConf, with significant support from the community (and with smaller but more vocal opposition), intentionally denied giving a platform to the political views of a neoreactionary (who was only permitted to speak on his relevant project). This neoreactionary, who is a pacifist and a Jew, has written against fascism. He believes that different races have different average levels of IQ in standardized tests (with Ashkenazi Jews measuring higher than Asians, Asians higher than whites, etc.). He has maintained that individual variations trump variations in average, and has stated it is foolish to attempt to guess someone's IQ from their race. He has not said smarter people are superior (an idea he insultingly calls IQism), but instead has said intelligence has no relationship to a person's worth. He has never said anyone deserved slaveryrather, he has redefined slavery to include government/citizen, parent/child, and other relationships, and has made the controversial suggestion that slave traders preferred taking slaves from some regions because of their physical and behavioral characteristics (i.e. "some people make better slaves"). He is a leader of no one, just someone who wrote his views on an anonymous blog years ago (subsequently doxxed). He has denounced violence of any kind, repeatedly. His beliefs may indeed be threatening and offensive to people, but that is precisely why LambdaConf refused to allow him to bring them to the conference. The logical conclusion to this witch hunt that many seem intent on pursuing is to purge from conferences (and professions) anyone who engages in any "sufficiently" immoral thought or behavior (where "sufficiently" is defined by a mob of people who quote second, third, and nth-removed sources without even basic fact checking). Whether that includes "Moldy Bread", people who voted for Trump, people who have made sexist or misogynist jokes on some unrelated online form, people who oppose LGBT rights (including trans bathrooms), people who have ever engaged in causal racism (yes, private jokes about asian tourists are racist), people who work on drone software that murders innocent people, etc. When this is done, only the "pure" people will be left, who apparently get free reign to insult and denigrate their opponents and make outlandishly false claims without any repercussions, because they're on the "right" side. Is it surprising at all why some people want to keep this outside the Haskell community?
If my neighbor (Only 10% of my county voted for Trump, by the way) wants to treat me like subhuman scum, the last thing I should do is be civil with them. Conflating *politeness* with *civility*. As if having the political opinion that certain people are inferior and should be subjugated is something to politely disagree about.
If you do roll your own, might as well publish it as a package. Then others won't have to roll their own.
I might actually do that some time soon.
I don't care about the nuances of his odious political opinions. I will say, however, that I believe he has a right to speak at unrelated conferences despite his odious opinions, and that allowing him to do so does not constitute an endorsement. It is possible to disagree with people with every fiber of your being on many issues and still have communication with them in other areas, and in this regard I support the LambdaConf stance. ("He can speak here about a technical topic, but cannot discuss politics.") Apparently he adhered to this requirement as well. I hope that people can see the importance of this principle and will make talk proposals on interesting subjects in order to support one of the most interesting American tech conferences.
A few months ago, I went through hell to figure out how to load 100M integers into an array in Haskell and by the end of it, I lost the desire for what I wanted to do originally as I had been banging my head against the wall so much by that point. Hopefully what I did then might be of some use to you. Here is the parser I [made for that task](https://github.com/mrakgr/futhark/blob/parser_attempts/unfoldr_v8.hs). As it turns out, if you use the Text package (or God forbid the standard String,) expect IO to drain your memory rapidly. Bytestring is what you should be using. The above example uses a custom implementation of a mutable Vector with the ST monad to load the data into it. As the standard unfold has the flaw of not returning state, I also made a new unfold function that does. A kind person on this sub also made a more [generic version of the code](https://github.com/mrakgr/futhark/blob/parser_attempts/monadic_foldr.hs) above. For simply loading stuff into an array, the example is 10% faster than the version I did in F#, though in F#'s favor I did it in 5 minutes rather than &gt;10 days it took me to do it in Haskell.
A silly example of a sort that doesn't is bubblesort, where `take 5 . bubblesort` is still O(n^2) - you have to pretty much complete the sort even to know the head item. It could be adapted to get the O(n) back, but it wouldn't be clear it even was a bubblesort any more as opposed to an insertion sort. I'm not sure with the common pure functional quicksort - I think the degenerate cases mean you get bubblesort-like behavior - each degenerate "partition" either picks out the first item and the rest (best-of-worst case - getting the first few items doesn't require the whole sort to complete) or the last item and the rest (worst-of-worst case - sort must pretty much complete to even take the head item). Actually, writing various sort algorithms that work for this is probably a nice exercise. Even handling the basic sorts, irrespective of laziness, is an excuse for some zippers and difference-lists practice. 
A good database migration framework 
http://sqitch.org/ Not Haskell, but better than anything that exists in most languages. Comparable to ActiveRecord.
I tried Hakyll. I think that's all I want to say. :)
You've to close `inh` and `outh` at the end of main. I really don't think that should be happening. I tried using the technique mentioned here: http://neilmitchell.blogspot.com/2015/09/detecting-space-leaks.html The program runs fine with a stack size of 1K. I don't see the memory consumption increasing at all. My output file is about 400MB. GHC version 8.0.1
Yes maybe sure probably kinda? My main goal with that work is to make HaLVMs 1000% easier to debug, by running them on normal-ish Linux infrastructure. My secondary goal is to do this without reference to the system libc, which seems like a potentially neat way to write early-boot / non-libc-based tools for Linux. If we can get them to run under Docker, that's also cool. I'd assume it wouldn't be too hard: just create a disk with your Linuxified HaLVM on it and run it, but we'll see!
This isn't an issue with the compiler. Its an issue with winghci -- the windows wrapper around the GHC repl. The ghc compiler itself installed just where you'd want it. This can happen if for some reason your system path wasn't modified to point to the ghc or ghci executable (this should be straightforward to check) or if that somehow doesn't have proper permission bits. Note that the platform installer also installs stack for you -- so if you want to use it, you won't need to install it separately. But the key thing is that winghci is just a nice way to access the repl -- but from the terminal you can just run ghci directly, as well as ghc to actually compiler programs, etc.
If you were a grocer, I would expect you to sell Moldbug an apple. That's being civil. Then, if you were politically reasonable, you would go and support the DNC, you would organize, and you would win elections in 2020, so that people like the person you just sold the apple to wouldn't *maintain* power. Not selling him the apple is childish behavior, not "activism". 
Maybe "about 1KB" is enough. Maybe there's an exceptionally long line, and that's just about enough. &gt; According to Sherlock Holmes, once one eliminates the impossible, whatever remains must be the truth. I, on the other hand, prefer not to rule out the impossible. I think that's approximately a quote from a Dirk Gently book. Anyway, could you try factoring these three lines into a separate function... str &lt;- hGetLine inh let arr = read str :: [String] hPutStr outh $ arr!!5 Reason - that way, `str` and `arr` are clearly out of scope when `write` recurses, so no matter what weird transforms the optimizer might apply (or fail to apply) to `write`, there's no way they can be forced to stay in memory. It shouldn't make a difference, but... 
We ended up sticking to https://github.com/thuss/standalone-migrations and writing raw SQL. The downside is that you have a ruby dependency, but on the upside it has the occasional benefit of being able to script certain complex migrations in ruby looping through raw sql templates. Great for avoiding code duplication in larger changes. You can fairly quickly write your own migration manager with a bit of bash and pg_dump (if you're on postgres), just keep the whole thing as simple as you can.
Pretty interesting, I've worked on something in the past that has a part very similar to tyou question, I'd also have to look over the data,set symDiff funtionality
It's not that the theory prescribes that there should not be any difference, it's an intentional choice. The idea is that we want to use equations to reason about the behaviour of our programs, but while Haskell functions are much closer to mathematical functions than the imperative procedures of most other languages, Haskell functions can still throw exceptions, diverge, and there is also the thorny issue of lazy evaluation. Those features make Haskell functions more complicated than purely-mathematical functions, and prevent us from applying mathematical theorems about mathematical functions to Haskell functions as-is. One solution is "denotational semantics", in which we assign a mathematical expression, such as a set, a number or a function, to each program, and we call it the "meaning" of the program. We can then reason and prove things about the meanings, and thereby derive conclusions about the corresponding programs. When we choose which meaning to assign each program, we can choose to abstract away from some of the complexities. For example, we could choose that the meaning of `f x = x + 1` should be the function `f(x) = x + 1`, thereby ignoring the fact that `x` could diverge and cause `f x` to diverge as well. If we conclude something about this family of meanings and we want to apply the result to our programs, we will have to qualify our conclusion with "provided that no computation diverges" or something along those lines. The more common choice is to represent `f` as f(x) =  if x =  x + 1 otherwise Where  is a distinguished value, which isn't used as the meaning of any terminating expression, and is only used as the meaning of diverging expressions and of expressions such as `undefined` which throw exceptions. Now we can use our mathematical conclusions to say something about whether the corresponding program will terminate with a value or not, but we cannot distinguish between programs which fail to do so because they diverge or because they throw an exception, as our model doesn't distinguish between those two cases. There is indeed "no meaningful difference between a nonterminating expression and erroneously calling a partial function", because in order to simplify our model, we chose to give those two programs the same meaning, a distinguished value named "bottom". A single bottom is sufficient to describe whether a function is strict in a given argument or not, and that's why this is the most commonly-used denotational semantics for Haskell. But if you want to prove something about exception handlers, you would choose a different denotational semantics, probably one with a distinguished set `E` of exceptions, plus a special value `d` for an expression which diverges: f(x) = d if x = d x if x  E x + 1 otherwise 
DB migration seems to be a simple task and you may think that you can do it even with a bunch of sql scripts. But, in fact, it's extremely tricky specially once you have a production db. Try to choose a good one and get yourself familiar with it's ups and downs before your application is production ready. We ended up using [liquibase](http://www.liquibase.org/) that has been around for a long time and seems to be stable and loaded with features. Adds dependency to Java.
I think OP was saying `fun x = (x + 5) / x` and assuming that division had the type `Double -&gt; NonZero Double -&gt; Double`
As a library author who's had a PR sitting in my inbox for a while now... thanks for the reminder. Finally managed to find time to do the merge and push the update to hackage.
Whoops. I messed up the type of `div`. Despite the fact that the type I gave for `div` is totally wrong, I think that everything that follows is still right if you just assume that `div` means something other than division that does have that type.
It does seem the software he is writing is linked to his ideologies, and doesn't have much merit on its own. That could be an argument against letting him speak.
Keep in mind that this post is more about the interface than the representation, although it does also make some claims about the two representations it describes. 
The argument on programming style is a bit of straw-man (not completely). There is a significant portion of the Node.js/JavaScript community that utilizes promises, coroutines or es2017 async/await for their concurrency needs. Async/await especially makes concurrent execution more readable, but likely comes with a significant cost as it leverage promises. Any argument you are trying to make will be rejected by the community at large without exploring these alternate styles. There is also an additional argument to make about async/await vs Haskell's async library. Async/await is syntax in JavaScript...
These mixed-language examples will surely win mindshare over. Very much needed and appreciated !
As someone who does actual real-world organizing, I find your suggestions condescending and pathetic. Keep your milquetoast respectability politics to yourself.
Try adding an *hFlush outh* immediately after your *hPutStr*.
Concurred. I find that the syntax argument did not win me over, as async syntax is already being implemented as we speak. I did very much appreciate the benchmark comparisons of the route stuff.
There's the http://fpchat.com Slack for folks that want something persistent/mobile-friendly, already has 1,200 people in the Haskell channel.
&gt; And another reminder that insulting and denigrating people over disagreements is generally what 3 year olds do. being mad is illegitimate. your anger is invalid. don't bring it up, go away. it proves you're a child, when you fight back and are angry against people who thinks you are genetically predisposed to slavery, and against people who ignored that fact... can't you see that you are really the problematic one, here, when you get angry about your own dehumanization? why not think about me. i don't like being called a poopy head. standing up and saying you don't want to give quarter to internet asshats, and don't approve of people who do give them a leash? childish, the epitome of being a 3 year old. you should know better. get with the program. but, giving legitimacy to the idea "we will accept fascists if they're really nice and non-political", well, it's the epitome of Freedom and true, Progressive thought. it's very good. it owns, in fact, to be totally and completely politically impotent. i'm definitely smarter and have a much more sophisticated political palate than a 3 year old, as you can see. i learned from the school of Buzzfeed articles. &gt; Mature adults are capable of civil approaches with intellectual rigor. beep boop. adults are robots. emotions are bad and only for children. i bend girders for a living, and don't know what nuance or dichotomies are.
I won't downgrade either so it's not a problem. I have a quick look at the code and I don't understand how it figures out what's been run already.
At least you showed your true colors. You aren't arguing in good faith and your sanitizing description of Yarvin's writings has the logical solidity of Purell. This is the last post I'm going to make in response to your weak rhetoric because it's obvious you're not trying to listen or engage with anyone who disagrees. This is purely for the benefit of others reading your post. &gt; The facts also deserve to be heard, so party-line propaganda doesn't overwhelm the truth in a cloud of foggy mist. Considering your last response to my arguments elsewhere in this thread were snide deflection, I find it completely laughable that you're claiming that the people who condemned LambdaConf last year were engaging in propaganda in order to obfuscate the truth. I can't believe you're trying to make this argument and think it should be accepted as civil because you're maintaining this paper-thin veneer of cool detachment. The irony runs deep. There were **several** viewpoints expressed directly and indirectly on this topic demonstrating that multiple people with multiple perspectives used their own reasoning from their own values to come to similar conclusions that still have their own personal nuance. I think it is absolutely insulting to everybody in this community that you would make such an assertion which wholesale dismisses very thoughtful independent members of this community who made honest, cogent statements [like here](http://www.haskellforall.com/2016/04/lambdaconf-should-reconsider-its-policy.html) and [here](https://medium.com/@jleitgeb/lambdaconf-and-the-question-of-diversity-307feb807c91#.j78rxhsaz). &gt; LambdaConf, with significant support from the community (and with smaller but more vocal opposition), intentionally denied giving a platform to the political views of a neoreactionary (who was only permitted to speak on his relevant project). This neoreactionary, who is a pacifist and a Jew, has written against fascism. He believes that different races have different average levels of IQ in standardized tests (with Ashkenazi Jews measuring higher than Asians, Asians higher than whites, etc.). He has maintained that individual variations trump variations in average, and has stated it is foolish to attempt to guess someone's IQ from their race. He has not said smarter people are superior (an idea he insultingly calls IQism), but instead has said intelligence has no relationship to a person's worth. I assume you mentioned he's jewish because you're trying to subtly slide in the idea that a jew cannot hold deplorable political views. Well, he can, and he does. The fact that Breitbart was jewish doesn't mean Breitbart the "news" source doesn't give platform to reactionary and even anti-semitic ideology. I think you're either a fool, or now, since you started off this particular comment with such a corrosive insinuation against those who disagree with you, that you're deliberately being misleading about his views. Yarvin's writing is dense, long-winded, incoherent, full of confusing allusions, references to obscure texts written by slavery apologists, deliberately vague, and an attempt to rebrand fascist and white supremacist ideas to make them more palatable to mainstream discussion. Every time he's called out on his antidemocratic, antiegalitarian ideas, he doubles back and makes further vague statements to distance himself from criticism. When Yarvin says he's anti-fascist, even though he's trying to legitimate the ideas that prefigure fascism, he is saying "don't reject my ideas". I do not believe in this nominal judgment of political ideologies. The substance and structure of an ideology is what determines what it is. All of the dissimulation on Yarvin's part does not exempt him from accountability for his beliefs or the communities of violent reactionaries that have sprung up around them. Frankly, the last bit is what makes me think you're a flat-out apologist for his viciously deplorable ideas: &gt; He has never said anyone deserved slaveryrather, he has redefined slavery to include government/citizen, parent/child, and other relationships, and has made the controversial suggestion that slave traders preferred taking slaves from some regions because of their physical and behavioral characteristics (i.e. "some people make better slaves"). You got it backwards. Yarvin doesn't redefine those other relationships to be slavery. Plenty of people who don't have deplorable, reactionary politics draw the comparison between the authoritarian aspects of government/citizen, parent/child, boss/laborer and slavery. The term "wage slavery", while politically charged, is not considered appalling by most. What Yarvin did, and what people find deplorable is that he goes to great lengths to try to **normalize slavery** as a natural human relationship. This is a direct quote from his blog: "The relationship of master and slave is a natural human relationship: that of patron and client. " The redefinition is in the exact opposite direction you claim it is. Because... *drum roll* YARVIN WANTS TO NORMALIZE THE IDEA THAT SLAVERY IS OK. Either you never tried reading his writing (I can't blame you) or you're being deliberately misleading. &gt; The logical conclusion to this witch hunt that many seem intent on pursuing is to purge from conferences (and professions) anyone who engages in any "sufficiently" immoral thought or behavior (where "sufficiently" is defined by a mob of people who quote second, third, and nth-removed sources without even basic fact checking). Whether that includes "Moldy Bread", people who voted for Trump, people who have made sexist or misogynist jokes on some unrelated online form, people who oppose LGBT rights (including trans bathrooms), people who have ever engaged in causal racism (yes, private jokes about asian tourists are racist), people who work on drone software that murders innocent people, etc. When this is done, only the "pure" people will be left, who apparently get free reign to insult and denigrate their opponents and make outlandishly false claims without any repercussions, because they're on the "right" side. Ok, so, first of all, this isn't the logical conclusion of anything. This is a slippery slope argument which everyone knows is a fallacy. Second, a witch hunt implies that the condemnation is *baseless*, and well, it's not. The fact that you refuse to acknowledge or engage with the substance behind the condemnation of Yarvin and LambdaConf's decision to give him a speakership does not make it a witch hunt. This ties into your first line, where you claim we're all a totalitarian party that's trying to force propaganda down everyone else's throat. You *refuse* to engage people based on the logical merits of their arguments, or their community values, or their personal experiences. So, the only conclusion for you, the stooge willingly defending the indefensible, is that we must all be illogical. This point is so laughable I can't believe an adult actually wrote it in good faith. &gt; Is it surprising at all why some people want to keep this outside the Haskell community? Take away all the people who signed the [statement on lambda conf](https://statement-on-lambdaconf.github.io/) and it's not entirely clear you even *have* a community left to defend. So many crucial members of the community who have contributed so much are part of this discussion you call a witch hunt. Who the fuck do you think you are claiming to be arguing in the interests of the community? Miss me with that bullshit. P.S. I encourage anyone who reads this to think twice about calling me childish for being full of contempt, annoyance, and disdain for such flawed and toxic rhetoric. *Maturity is being able to reconcile your ability to feel and ability to reason; people who pretend they have are coldly logical all the time are deluding themselves into thinking that a polite, detached aesthetic is what makes rational thought rational.* A child acts on their emotion but does so without understanding why they feel that way. An adult acts on their emotion and understands why they feel that way.
oh boy :) I know it's easy to confuse microkernel with a small and minimal hypervisor, lots of people are doing this mistake. It also sounds more secure and more trendy (fitting the owner's marketing). You'll see no reference to microkernel (for good reason) in the xen paper [xensops](http://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf), and you'll have a hard time to match the definition of microkernel to Xen. Most of what make the whole thing work is provided in the monolithic linux kernel and in the linux userspace. Maybe it's easier to consider Xen as a flavor of x86 architecture (heh that's exactly how it's represented in linux source code) where you do hypercalls to organise the memory. maybe if you squint hard enough and looks at the overall host, it does appear to look like a microkernel since services are not running in Xen directly, and this is what confusing people a lot, but ultimately if dom0 crash or misbehave your host is kaput. this isn't very microkernely in my view :) Driver domains don't change anything to this, just make it less likely to happen, by moving further service from the linux monolithic kernel dom0 running the show (so maybe you could say that xen microkernelise your linux dom0). Having spend too many years of my life trying to make them work, they are also a major pain to setup.
That's handled by the PostgreSQL backend. It creates a table and records which migrations have been run (based on the ChangeName). When you run the migration again it will skip any that have been done already.
Ok, you can think that. I don't know *how* you can think that if you've read his shit, or read anything that's sprung up around his shit, or contextualized it in the broader white nationalist alt-right resurgence we're going through right now (that is, a rebranding of white supremacy), but you can think that. And yes, it is fair to group him with that ilk because those are the people he associated with before he started getting called out for it and responded by obfuscating his ideology in innuendo and dissimulation. Marxists and critical theorists make the arguments you described all the time. Their arguments, their rhetoric, their epistemology are all completely distinct from Yarvin's. Yarvin, meanwhile, uses euphemisms like human biodiversity in order to legitimize race as a biological category in a pseudoscientific way. Yarvin wants to make the idea that power and domination are inevitable so that he can justify governments that are based on maximizing domination by the most powerful over the weak and marginalized. [The purpose of a system is what it does.](https://en.wikipedia.org/wiki/The_purpose_of_a_system_is_what_it_does) I learned that statement from a fellow haskeller. The purpose of Yarvin's rhetoric is to create a veneer of respectability for white supremacy, anti-democratic forms of government, and racialized pseudobiology. That's what you get when you contextualize his politics within the communities he developed those politics. His technical project is firmly rooted in these principles even which, to me, is kind of weird but whatever.
They will only win mindshare if they use most popular practice vs most popular or best/latest with best/latest. In this case the whole is that the newer way of concurrency in node might be more popular.
[removed]
How would you approach this without `unsafePerformIO`?
&gt; and you would win elections in 2020 I have to say, there's an odd bit of irony to someone here trying to lecture people about being organized in legitimate ways and shit, and what the right method is... but they seemingly don't remember midterm elections exist or something? Politics is so silly with things like that.
The latest Windows 10 Insider build implements the Linux syscalls necessary to run GHC programs compiled for *nix on Windows in the "Windows Subsystem for Linux". This addresses my [uservoice issue](https://wpdev.uservoice.com/forums/266908-command-prompt-console-bash-on-ubuntu-on-windo/suggestions/13339443-the-glorious-glasgow-haskell-compilation-system-a) and [one](https://github.com/Microsoft/BashOnWindows/issues/112) or [two](https://github.com/Microsoft/BashOnWindows/issues/307) issues reported to Microsoft on GitHub. If you do not know what the Windows Subsystem for Linux is, they have a [blog post describing it](https://blogs.msdn.microsoft.com/wsl/2016/04/22/windows-subsystem-for-linux-overview/). In short: it's a personality layer on top of the NT Kernel that allows native execution of software compiled against the Linux kernel, on Windows. Think WINE, but the other way around. 
There are a lot of folks that strongly disagree with Yarvin *and also* think that completely ostracizing people is wrong. It's not just straight white men either; people of color, queer people, women, etc are all in this camp. When another speaker the previous year advocated for literally executing capitalists/startup C-levels/etc, there was no uproar or controversy. The speaker gave a great talk and did nothing on premises to make anyone uncomfortable. In principle, I think it's wrong to ostracize *even if you think it's good*. I grew up in a marginalized group in an oppressive culture, and the oppressors *think that they're doing the right thing*, and it was still wrong to do. In practice, I think it's ineffective to ostracize, because the *point* of debate -- as you've noted -- is to convince the audience and bystanders that your opponent is wrong. If you refuse to discuss or argue a point, and your opponent makes an argument, then they win by default. That sucks, and I want to *defeat* fascism, not pretend it doesn't exist. Sunlight is the best disinfectant, and the practice of ignoring and belittling opponents has completely and utterly failed.
Ha, watch how fast a bunch of liberals can stab each other in the back! Do you sincerely believe A) that I was using a rhetorical flourish but it was a good opportunity to get a punch in, or B) that I was very unaware of the existence of midterm elections and you are educating me about the next opportunity to have an impact?
&gt; oppressors think that they're doing the right thing This is important. Almost no one has an internal narrative of anything other than, "I'm the protagonist good-guy."
Yes and no, I think. WINE allows running unmodified Windows PE format executables on *nix, and WSL allows running unmodified Linux ELF format executables on Windows. Cygwin requires recompiling from source, so you cannot simply copy-paste a binary from a Linux machine to Windows and run it with Cygwin. This does work with WSL. 
This is very nice. From what I see this is for Insiders. When will this reach stable? 
Well Persistent already create migration which create tables and add new column from the model which is pretty handy. I just need to add indexes on top of that. I don't want then to have to writr all of this manually if I switch to another system. I was thinking I could use the diff command of liquidbase to keep both system in sync ...
(probably) shipping with the Creators update early next year. 
So the servers written in Haskell tend to take some function which usually has the form or something similar serve :: x -&gt; IO () I have to use the GHC Monad to access the internals of GHC during the time that my server is running, however I can't pass my function interpret :: x -&gt; Ghc a inside because this is actually a reader Monad internally and it doesn't have anything to read inside of the server. What one could have is a server with callbacks like this: so_free :: MonadIO m =&gt; x -&gt; m () Alas, this doesn't work in my case, and I tried to go and change the server I was using to do so but I eventually ended up at a point where I was messing with stuff in the internals of the library and I couldn't make it work past that really unless I rewrote the whole server. 
Hm, it's not immediately clear why you are unable to use the GhcMonad as intended, but our own project includes a [module](https://github.com/agentm/project-m36/blob/master/src/lib/ProjectM36/AtomFunctionBody.hs#L30) which sounds very similar to your own. You will likely wish to the create the session once at start and then pass it to the module executor. GHC.Types won't be found if your package search paths are incorrect (extraPkgConfs in GHC 7.10), so make sure they are non-empty.
Windows 8.1
I did download it to an external hard-drive due to my computer having no memory. Could that be the issue of why it can find the GHC?
So minorities are killing themselves because a few tech conferences have refused to politicize speaking engagements and attendance? Sorry, but I'm going to have to ask for a citation on that one. If you have evidence that allowing people you (or a Twitter mob) judge "sufficiently immoral" to participate in professional events under a strictly enforced code of conduct results in mass suicides, physical violence, or anything else at all, then please present it. Otherwise, why say things like this? It undermines your credibility (at least with some) and hyperbole in general gives further ammunition to hate groups.
Yes, that's very likely. At the moment I can't test the install of the platform to a custom location to see where things end up getting installed. Your path will typically have something(s) with the prefix `C:\Program Files\Haskell Platform\8.0.1\` and suffixes like `bin`, lib\extralibs\bin`, and `mingw\bin`. You may need to set that appropriately for where your install actually ended up. Bear in mind that the first step is to test if you can launch ghci or ghc directly from a terminal, leaving winghci out of the picture entirely. Also, as I recall, there are paths hardcoded into the install of GHC. So if you just literally moved the folder from `C:\Program Files\Haskell Platform\8.0.1\` to a different location using e.g. windows explorer, any number of things may have broken. If this is the case, its possible to install "raw ghc" directly from the ghc homepage: https://www.haskell.org/ghc/download_ghc_8_0_1 I can't tell you offhand the instructions to follow on download of that .tar.xz, but the install process from there should have a flag that lets you set an install location with paths configured, so you could pick a different install location directly to your external drive that way too, if all else fails. (Note that the binaries besides the ghc ones that came with the platform are themselves relocatable however, so the `cabal` that you got should work from whatever location, and ditto `stack`, `haddock`, etc.)
There is an issue on the github page that talks about how this could work: https://github.com/Gabriel439/Haskell-Dhall-Library/issues/4
 getThirdItem (x:y:z:xs) = [z] -- ?
[removed]
Comical note: I had never heard of Yarvin and never would have known anything about him or his views if it weren't for people giving him a platform *by trying to prevent him from having a platform that he was never going to actually have*. So, congratulations on becoming Yarvin's PR team in an attempt to show everyone how amazingly virtuous you are.
&gt; against significant opposition from **part of** the community FTFY 
Your code works fine for me, if you want the character alone and not return it like a string (so, return `'c'` instead of `"'c'"`) you can do `getThirdItem (x:y:z:xs) = z` or `getThirdItem (x:y:z:xs) = [z]` to return a singleton string (a string containing a single char).
&gt; support for creating processes in one subsystem from the other Haven't kept up since the first insider preview, I didn't realize this was planned at all. This could imply that they're working on a way around IMO the biggest shortcoming: if you work in the Ubuntu subsystem, building Windows binaries is a cross-compile. If they can allow simple PE wrappers around ELF binaries which then launch Ubuntu-side, and provide a scriptable package manager interface from Windows-side, you could just dev for Ubuntu and distribute "native" Windows installers that are, at worst, no more annoying than needing the VB runtime.
[removed]
Lol'd at the ending. Okay, so I watched all of it and I think I understood everything. Now what use is this? Honest question
I never used the word endorsed. My exact reasoning was that LambdaConf found his views *acceptable*. I did not conflate acceptability with endorsement. That's you reading what you want into my comments. The core issue here is that some people think that all political views are acceptable. Others disagree, and their grounds for disagreement have never been directly engaged by LambdaConf's organizers who would rather just pretend like they were definitely obviously right all along and no one else has any valid consideration in their decision. If we're playing a game, you're not very good at it.
Objects being adjoint is apparently very useful in logic. Hence they should be useful for types. One of the isomorphisms mentioned is essentially currying. Edit: Currying is okay because Hom(X x Y, Z) =~ Hom(X, Hom(Y, Z)) in Categories with the necessary properties. The point is, the language to specify that relation is from Category Theory. Category Theory is useful because it gives a framework to study functions, or function like things.
LambdaConf didn't find his views to be acceptable. They found his technical presentation to be acceptable. You are being dishonest to state otherwise.
Unfortunately, you are still playing games. LambdaConf found Yarvin's views _unacceptable_, and went so far as to ban him from discussing his views at the conference. But LambdaConf also found Yarvin's views _irrelevant_ to his submitted topic and irrelevant to a professional event focused on functional programming. One can simultaneously find someone's views _unacceptable_ but still engage with them in a professional context (whether that's letting them participate at a conference, selling them goods or services, giving them a job, or buying goods or services from them). This admittedly mind-bending concept is a relatively modern invention that most term _civility_ or sometimes, _professionalism_. Reject it if you like, but word games don't change the facts of what happened.
yes exactly : `using a != is a`
That would be very unfortunate. WSL at the moment is an opt-in developer feature for Windows 10 only that requires a Ubuntu install. Many people and some companies need to develop native Windows software and Windows DLLs.
How OSX (essentially pure development OS for Haskell -- no decent server version, relatively small share of desktop market) is more important to support than Windows?
Firstly, no one here has been able to reproduce the problem. Secondly, *if* the problem is a line that is too long then kqr's version has the same problem, since it uses `linesC`.
Yeah you're right. After looking a WSL more carefully, I see that it provides a Linux kernel interface, rather than a Linux ELF interface, so it requires running an entire Linux kernel, rather than just binaries, so its usefulness is more limited than I anticipated.
Let's ask /u/ndmitchell.
This is true however it does not change JavaScript's early binding of concurrency. This is informed by the exposure of the event loop in language semantics. Continuation passing style forces the programmer to worry about event loop plumbing. The plumbing is rigid and forces early binding. Non blocking IO cannot be expressed synchronously, so it cannot be left as an abstract runtime implementation choice. Every abstraction to improve this situation has required transpilation (promises, generators, async/await), so the semantic model does not evolve, it becomes obscured and early binding persists. Haskell on the other hand allows room for the runtime to decide how IO blocking, green threads, etc. are handled. All expressions can be expressed synchronously with async behavior as an additional expression. The runtime can decide to express this in an event loop or as threads, to block or interleave IO from separate threads because the plumbing is less prescriptive. The prime example is that I can write a synchronous function in Haskell that utilizes concurrency, but in JavaScript the enclosing function will leak that concurrency. -- foo is sync to its caller foo = x &lt;- await asyncFoo return x // foo must be async to return x async function foo () { let x = await asyncFoo(); return x; } There are many more details of course, but early binding is a mistake and conflates execution patterns with logic where it is not always necessary. 
JS async/await primary purpose is not to model concurrency, but asynchronicity. Cooperative/synchronous concurrency is not supported by the JS runtime. And JS is single-threaded, so you have no other choice than to use an event loop. Like I said, async/await should be compared to IO in Haskell. The fact that a function uses IO also leaks to the caller.
As a High School Student, I'm really looking forward to attempting a Summer of Haskell project in a few years :) 
Since Haskell doesn't have side effects, your pure function cannot print anything. Try main = putStrLn (getThirdItem "abcd")
The longest line is 1.2KB.
Thanks for the help. :)..you are correct, i was assuming show z would print but nope..thanks for the help, i got it figured out :)
thanks :)
yes, i ended up doing it a bit differently, but thanks for the help.:)
At least HaLVM should work when they run on top of the Linux syscalls :-)
In my experience, `-O2` can improve performance by a few orders of magnitude.
But since apple is neglecting power users some people (like me) are switching to other OS
I don't think the servant API is complicated. Definitely shorter (and thus better readable) than strelka.
&gt; Web-routing is not rocket science. It doesn't necessarily require any advanced programming concepts like type-level programming with higher-kinded types. A router can be implemented using the familiar simple concepts. Assuming this is referring to `servant` and there isn't some other library also using funky type-level stuff I haven't heard of. Defining your api at the type level lets other things consume it, once you've written your api in `servant` you get things like client libraries and mocks almost for free.
I can see why someone might want a value level specification instead of a type level specification, the code to interpret it and turn it into an actual web server is much simpler at the value level than the type level. If you get something wrong in servant the error messages are very confusing for those who aren't masters at Haskell type wrangling. Here I would assume that the type errors would be much more in line with what most people would expect compared to other libraries in the Haskell ecosystem. Servant provides an amazing API, servant's implementation is black magic.
This particular article was savaged on HN. Much of it was "oh, haskell can't possibly be better" thinking, some of it was legitimate. I doubt many minds were changed.
&gt; Servant provides an amazing API, servant's implementation is black magic. The best way to put it I'd say :) Regardless. This library isn't here to compete with Servant. The Servant ecosystem is probably much more featureful, but it also requires a way more advanced knowledge from the users. I simply cannot imagine explaining what's going on there to a non-seasoned Haskeller. This is why I expect the libraries not to intersect in the audiences. Strelka is here to compete with the rest: Yesod, Scotty, Spock, Snap, Happstack, Apiary and etc.
&gt;Xen is not a kernel Yes it is. I have no idea why you think a kernel and a hypervisor are somehow mutually exclusive, but they are not. I showed you evidence that you are incorrect. Simply repeating "no I am right because nothing" over and over is not compelling.
Servant is undeniably complicated. Just taking one of the simplest possible routes from the documentation, type MyApi = "books" :&gt; Capture "isbn" Text :&gt; Get '[JSON] Book By convention, enforced by the typeclass instances, the verb must go last, because (using `servant-client` as an example), there are only (BuildHeadersTo ls, ReflectMethod k1 method) =&gt; HasClient * (Verb k1 * method status cts (Headers ls NoContent)) (MimeUnrender * ct a, BuildHeadersTo ls, ReflectMethod k1 method, (~) [*] cts' ((:) * ct cts)) =&gt; HasClient * (Verb k1 * method status cts' (Headers ls a)) ReflectMethod k1 method =&gt; HasClient * (Verb k1 * method status cts NoContent) (MimeUnrender * ct a, ReflectMethod k1 method, (~) [*] cts' ((:) * ct cts)) =&gt; HasClient * (Verb k1 * method status cts' a) instances, none of which contain `(:&gt;)`. There's obviously a good reason for this (endpoints have a single verb defined at a time), but this is also only one of the dozens of little tricks employed to make fewer nonsense endpoints representable. I love servant, and I've stolen many ideas from its design for personal stuff, I just cannot fathom how anyone, no matter how experienced, can say the servant API isn't complicated.
I've been filling out my [bazqux](https://bazqux.com/) (written in Haskell!) subscriptions recently... do you have an RSS feed for your blog?
&gt; It would be really crazy if it could find a specific function definition You mean tags? There are several programs that generate tags for haskell, I think [hasktags](http://hackage.haskell.org/package/hasktags) is the most popular.
The DSL is simple enough to fit into two [slides](https://www.andres-loeh.de/Servant/servant-wgp-talk.pdf#39). But the haddocks... haddocks for instances definitely don't tell the story as nicely! 
I wouldn't say that the implementation techniques used in Servant are unique to it. It's essentially implementing the universe pattern in Haskell. (That's not to say that servant isn't impressive, it's just that the techniques are not outright novel.)
I finally use conduit as suggested by kqr, and it works great. Thank you for the information.
Ditto. In the same vein, I bet that `ensureThatAccepts`, for example, could be written in a way such that ``` whenM (accepts HTML) $ do -- ... ``` works. Or ``` whenM (methodIs GET) $ do ```
If I can replace Ubuntu with NixOS, that would be perfect.
YEEHAW. The other bug I'm waiting for: [Typing text in the bash: existing text is overwritten, new text is not inserted](https://github.com/Microsoft/BashOnWindows/issues/1307) [Demo](https://www.youtube.com/watch?v=eDd9A8YJCH8)
I'm also interested in finding efficient algorithms for "sufficiently simple" graphs. As I allude to in the end of the blog post, some of the standard graph algorithms that take O(|V|+|E|) time can be implemented in O(|expr|) time instead using a more compact representation, such as `Basic`, which doesn't explicitly store each edge. For example, it is possible to do a depth-first search of `clique [1..n] :: Basic a` in O(n) steps instead of O(n^(2)) using a conventional edgelist representation. Graphs that can be described with algebraic expressions of small size are in some sense "sufficiently simple", just like graphs with small treewidth.
&gt;Does this effectively make Windows support inside GHC obsolete? LXSS is basically a toy VM running Ubuntu Server 16.04. It's great for a lot of reasons, but it does NOT obsolete Windows support for any application.
I haven't yet figured out how to handle arbitrary edge labels in a nice way without breaking the algebra. However, I know how to nicely model one particular type of edge labels -- Boolean functions (and semirings in general). This can be done by introducing an additional unary labelling operator to the algebra `[x]expr`, which applies label `x` to all vertices and edges in expression `expr`. You can read about this in [the paper I linked from the blog post](https://www.staff.ncl.ac.uk/andrey.mokhov/algebra.pdf).
Servant is great for CRUD-type REST APIs. However, if your api is not very REST-like (because you have to interface with third-parties), then every non-REST feature forces a decision point whether or not it should be encoded in the type of the API (Usually having lots of decision points means that we'll choose the wrong one, very often). e.g. do I put my super-special authentication mechanism in the type? do I put my check on the `origin` header into the type? do I put my CORS information in the type? etc..etc.. Moral of the story is, sometimes we don't know (or can enforce) that we'll be building these nice, vanilla REST API's up front, and having the capability to *easily* accommodate the crazy (w/ a value-level api) that working with businesses throw at us is sometimes necessary.
and you haven't demonstrated *anything* apart from a one liner coming from wikipedia which doesn't say what you think it says. I never said kernel and hypervisor are mutually exclusive, I'm saying Xen is not a kernel; compare the ESX wikipedia page which mention explicitly kernel and microkernel (as a "is" not a "using a design of") 
Are the notes avaiable?
Why would that be better than the `Alternative` branching?
You know, it's funny. Most of the times I see Category theory used in Haskell, the theory is used recursively. So, you would have endofunctors from the category of types to the category of types. Turns out Functors form a category. So these endofunctors form a category. Then we can define monoids purely in terms of functions, which means we can define them purely in terms of categories. We then give these weird monoids funny names to signal which category they are a monoid over. For example, monoids in the category of endofunctors are Monads! So they succeed in having a subject with is both concrete and abstract. But mapping categories against the type system isn't something I see programmers do very frequently, but I'm new at this too, so that may well speak to my lack of experience... That was kind of a ramble...
I kinda glossed over it and it looked like it was written with `do` notation anyway. I like the alternative branching, actually.
&gt; If they can allow simple PE wrappers around ELF binaries which then launch Ubuntu-side, and provide a scriptable package manager interface from Windows-side, you could just dev for Ubuntu and distribute "native" Windows installers that are, at worst, no more annoying than needing the VB runtime. Until you realize that you need to talk to a native Windows API for any number of reasons, and you're totally shit out of luck because LXSS doesn't allow you to truly call APIs across subsystems -- it's an entirely different kernel mode subsystem than the NT one. For example, `fork()` makes little sense in the NT kernel, as it does not have COW/clone semantics for process memory, so `fork()` on cygwin has always been a hack -- but is completely doable in LXSS because it doesn't use the NT memory management subsystem at all. Likewise, Linux has absolutely no equivalent to IO Control Ports, which is the mechanism Windows uses for asynchronous I/O. I'm frankly surprised they're even allowing cross subsystem process spawning, but I imagine this is something people have been begging for since day 1, so it's sensible. (I wouldn't put much stock in it going substantially further, but who knows.) It also requires that your application can deal with the simplistic subsystems LXSS provides, though this will improve with time -- e.g. semaphore support and epoll support were not fully there for a while (e.g. no `epoll_wait`). I don't think they've added SysV IPC, either, which means multiprocess systems like Postgres aren't going to work until then. Many programs will work without this stuff, but not all of them. LXSS is probably never going to be a real alternative to shipping actual native Windows applications -- unless your needs are extremely simplistic and generic, IMO, in which case, you could have gotten by with like a billion other alternatives that do not require end-users to install LXSS at all (and it's not even available in Windows Server, either, so LXSS is a complete no-go for anything you'd want to ship there).
I'm not really sure I get this - this is the same kind of routing that snap and happstack have been doing.... forever (and what HAppS did before happstack existed). Is the point just to factor that out into a library? Or to bring that style of routing to WAI? I'm also curious what the author thinks of http://hackage.haskell.org/package/fn (disclaimer - I'm one of the primary authors) - which does this style of routing, but in (I think) a novel way - it is not monadic so handlers take parameters as normal arguments...
What's the misbehavior? It looks sane to me.
"Black magic" is a little bit much. I'd argue it's FUD. The servant paper explains things very well. I read it before I knew what a type family was and I understood it through and through.
Those are 2 of the 10 books I'm going through right now, so I'm actually going backwards a little bit, I think.
The entire point of webmachine is that it abstracts away the low level details. This doesn't do that at all, it is another way to write those low level details. There's already two webmachine clones in haskell if you want that.
Oh God, those poor students. All their colleagues will know Python, R, C and C++. And they'll know some category theory XD
You can use a .ghci file in your repo, and have stuff like `:set -isrc` in there. Note that if ghcid fails to start it could be because of what you have in that file though.
That's what the Linux Subsystem does on Windows, too. It's not running a VM or using the Linux kernel, just translating system calls.
That's why you should never use implicit type signature.
An explicit one wouldn't have helped, unless you knew what it was you were caring about.
But it is not misbehaviour of type checking, is it? Type inference will infer too small integer, because factorial is defined for any instance of Num and Int will overflow. If you actually give factorial type signature `Integer -&gt; Integer` then it will not type check. Nice example though.
Thank you for clarifying. In that case, I think it does sound like a viable alternative for producing Windows executables using GHC, at least in some cases.
Good thing `-Wall` warns about defaults. Prelude&gt; ((lenDigits . factorial) 199) &lt;= 199 &lt;interactive&gt;:3:15: warning: [-Wtype-defaults]  Defaulting the following constraints to type Integer (Enum a0) arising from a use of factorial at &lt;interactive&gt;:3:15-23 (Num a0) arising from a use of factorial at &lt;interactive&gt;:3:15-23 (Show a0) arising from a use of lenDigits at &lt;interactive&gt;:3:3-11  In the second argument of (.), namely factorial In the expression: lenDigits . factorial In the first argument of (&lt;=), namely ((lenDigits . factorial) 199) &lt;interactive&gt;:3:15: warning: [-Wtype-defaults]  Defaulting the following constraints to type Integer (Enum a0) arising from a use of factorial at &lt;interactive&gt;:3:15-23 (Num a0) arising from a use of factorial at &lt;interactive&gt;:3:15-23 (Show a0) arising from a use of lenDigits at &lt;interactive&gt;:3:3-11  In the second argument of (.), namely factorial In the expression: lenDigits . factorial In the first argument of (&lt;=), namely ((lenDigits . factorial) 199) False Prelude&gt; (\i -&gt; ((lenDigits . factorial) i) &lt;= i) 199 True (Not sure why it warns twice though, probably a bug)
I put the mis between bracket. Even if there is a reason for this behaviour, it's still at least for me not obvious and I understand the OP surprise.
In short, it's all about the design. IMO, Strelka is simpler and more intuitive. It's way better isolated both on the package and the API levels: it's not an opinionated framework packed with solutions to unrelated problems and the abstractions introduced by the library have more specific purposes and are clearly isolated. There's no custom operators or typeclasses for composition - everything is either Monad, Alternative or Monoid, which everyone is used to - this provides for being intuitive. It doesn't bind you to any ecosystem or anything. As for your "fn" package, I've answered to [your question on the other site](https://lobste.rs/c/yrkkd2).
a link to a 17 pages article is not a definition.
Yes, but I'm not sure how many people have used it that way. Would be interesting to see how it works in practice. 
how is fpcompletely related to reddit_is_gayest ?, or someone forgot to switch his anonymous (hear: troll) account to the right one when replying ?
It's a good place to start.They'll undoubtedly have to learn C++ or Python or Fortran or some combination later.
/u/joeyh I'll go take a look at the HN commentary! Curious about informed comparisons
Because macOS support when you already support Linux is pretty easy, whereas Windows does absolutely everything about as differently as it possibly could. It's not more important in a vacuum, but since Linux support is a given then the activation energy for supporting macOS on top of that is far lower.
Windows has neglected power users for something like 20 years and a lot of developers still use it, macOS is still the better choice for developers out of the "corporate" OSs. The problem with the latest round of their laptops is that it neglects the video editors, the photoshop users, the 3D modellers, and the music producers that need higher amounts of RAM - and whereas at one time Windows was the main OS for developers, those users have _never_ had a better choice than macOS, and the ecosystem reflects that.
I feel like the real problem is that it's way too easy to accidentally use `Int`. Much like how it's way too easy to accidentally use `String`.
And this is why I've put so much efforts to keep GHC on Windows alive and well.
Explainable, but sane? I'd expect ~~eta expansion~~ lambda abstraction to preserve (inferred) types... that's not easily possible without inference supporting higher rank types, which is impossible iirc, right?
[removed]
To be fair, it's written in Ur/Web **and** Haskell.
So, is there going to be another one? I would love to join.
I got an error "Error communicating with the remote browser. It may have died." Can you help me fix it? Here is the code: runSession defaultConfig $ do openPage $ "http://www.javascript-coder.com/files/window-popup/javascript-window-open-example1.html" searchInput &lt;- findElem (ByCSS "a[href='javascript: openwindow()']") click searchInput -- open a new window here sessions &lt;- sessions session &lt;- getSession let newSessionId = fst $ sessions!!0 -- get the new session id newSession = session {wdSessId = Just newSessionId} putSession newSession -- set the new session state for the monad title &lt;- getTitle liftIO $ print title
This is only my opinion, but I think the existing example would be much easier to read if it were flatter and used better import aliases: module Main where import Prelude import qualified Strelka.RequestParser as Req import qualified Strelka.ResponseBuilder as Resp import qualified Strelka.WAI as Wai main = Wai.strelkaServer 3000 (return . Right . runIdentity) route route, hi, bye, notFound :: Req.RequestParser Identity Resp.ResponseBuilder route = Req.consumeSegmentIfIs "hi" *&gt; hi &lt;|&gt; Req.consumeSegmentIfIs "bye" *&gt; bye &lt;|&gt; notFound hi = Req.ensureThatAcceptsHTML *&gt; html &lt;|&gt; text where html = pure (Resp.html "&lt;h1&gt;Hello world&lt;/h1&gt;") text = pure (Resp.text "Hello world") bye = Req.ensureThatAcceptsHTML *&gt; html &lt;|&gt; text where html = pure (Resp.html "&lt;h1&gt;Goodbye world&lt;/h1&gt;") text = pure (Resp.text "Goodbye world") notFound = pure (Resp.notFoundStatus &lt;&gt; Resp.text "Nothing's found") 
Well, this coming year we hope to get funding via google summer of code as we have in the past. Failing that, we will certainly attempt to do a SOH again instead.
I agree, which is why the Drawbacks section exists. :-)
This is removed so I'm not sure if you changed your mind and I certainly can't see what you typed, but there are two points here: 1. It's not hard! It's done slightly differently to other languages, which may not be something you're used to. The thing that really sticks out is that in Haskell, we basically only use one single regex operator regardless of what kind of results we're interested in. Typeclasses help here. Some examples at the bottom of this comment. 2. We don't need regexes so much in Haskell. We have [parser combinators](https://two-wrongs.com/parser-combinators-parsing-for-haskell-beginners) instead! Examples: &gt; import Text.Regex.TDFA Let's create a string to search through! &gt; let path = "joe@example.com:/etc/nginx/nginx.conf" We also need a regex. This regex represents all sequences of three letters with leading or following non-letters. &gt; let threeLetters = "(^|[^a-z])([a-z]{3})([^a-z]|$)" The "universal matching operator" is `=~`. Haskell automatically figures out what sort of result we want based on the type. For example, we may want to know how many three-letter sequences there are in our path. &gt; path =~ threeLetters :: Int 3 Or perhaps, we just want to know whether there are *any* three-letter sequences. &gt; path =~ threeLetters :: Bool True We can also ask for all the three-latter sequences, first the full match, then the individual parts of them. &gt; path =~ threeLetters :: [[String]] [ ["joe@" , "" , "joe" , "@"] , [".com:" , "." , "com" , ":"] , ["/etc/" , "/" , "etc" , "/"] ] If we want to do efficient processing, we can also ask for an Array containing just the indices and lengths of the matches, to be able to do splicing. &gt; path =~ threeLetters :: MatchArray array (0,3) [(0,(0,4)),(1,(0,0)),(2,(0,3)),(3,(3,1))] Maybe we just want to know the index and length of the first match. &gt; path =~ threeLetters :: (MatchOffset, MatchLength) (0,4) Or perhaps we want a triple containing (what's before the first match, the first match, what's after the first match). This is useful when you want to split on a regex! &gt; path =~ threeLetters :: (String, String, String) ("","joe@","example.com:/etc/nginx/nginx.conf") That said, use parser combinators. 
I see often to use Text instead of String but which flavour of Text? So far I really never have any problems using String but time I had to use Text I spend my time converting between lazy and strict. It seems like half of the libraries use lazy text and the other strict. I even don't know what they have the same name.
For simple enough types that is not much of an issue. Using the example in the article, if the choice is between `Maybe Bool` and... data CommandResult = NoMoreInput -- ^ Haddock might go here! | Success | Failure ... there are hardly any reasonable uses of the `Maybe` API that you will miss out on by making the change. Pretty much anything you might want to do with `CommandResult` boils down to pattern matching.
There are only ghc-7.10.*.
Going by the rule of thumb that says "strict data, lazy functions" I prefer strict text. You can view lazy text sort of like a "StringBuilder" in other languages. Of course lazy text works for streaming too, but I'd lean toward recommending conduit or pipes if that's what you really need.
It contains the definition right at the top. What is your malfunction?
&gt; I'd expect lambda abstraction to preserve (inferred) types... In this case, though, the lambda introduces an interdependency that wasn't there originally. In the second expression... (\i -&gt; ((lenDigits . factorial) i) &lt;= i) 199 ... both occurrences of `i` must boil down to the same type. On the other hand, in the first expression... ((lenDigits . factorial) 199) &lt;= 199 ... the polymorphic type of `199` can be, and ends up being, specialised differently in each occurrence. I have added [a complementary answer](http://stackoverflow.com/a/41079130/2751851) that drills down on this issue. 
I understand Lazy.Text to be the equivalent of C++ ropes, which seems a good idea. What are the use-cases when Strict.Text is actually a good choice. Let's say for example I have a user with a `firstname` and a `surname` and function `name user = firstname user &lt;&gt; " " &lt;&gt; surnname user`. It's surely not efficient to build a new "string" each you want to display it.
Thanks for clearing that up! Which kind of proves my point about the subtleness of this... Depends on the definition of 'sane', I guess.
`These` is such a fantastic little type, I'm glad others are using it and talking about it. http://hackage.haskell.org/package/these
When I benchmarked my Yesod app, reverse proxy Nginx was much faster than warp-tls. From memory, somewhere between 50% and 100% faster.
First one. Ran apachebench against the homepage with warp-tls and with nginx-&gt;warp. Profiling the app showed something in tls using a lot of CPU. I think it was DH handshaking? Not sure.
&gt; Unit is not very blind, since even synonyms of it mostly mean the same thing: we dont really care about the result. On a related note, I find nonce "I did the thing" types useful when I have a bunch of branches and I want to make sure that "I did the thing" in each case. Otherwise, so many things have type `m ()` that my types can't tell the difference between a complete and incomplete branch.
[removed]
I think the subtlety here comes from the fact that it's easy to forget that `199` is `Num` and not just a literal integer as in pretty much every other languages. It is therefore easy to not realize that the two `199` are actually not the same "object".
I see you're the kind of person spouting nonsense about the "regressive left". I'm sure you won't mind if I decide not to waste my time on you.
With dynamic linking I have no problem. But on Windows GHC link all statically. If LGPL code linked static that mean I should provide .o files or source code of my program. http://www.gnu.org/licenses/gpl-faq.html#LGPLStaticVsDynamic
No, because there are folks who don't run Windows 10.
There was recently a "what's happening with HaLVM" where running on top of Linux was on the roadmap at least.
I think GHC should have a language freeze release that prioritizes compile-time performance. It seems like experimental language features and type system enhancements are getting most of the attention while every released compiler is slower than the preceding one. In particular, I'd like to see a tremendous speed up in the compiling of large hard-coded data structures. In C, you can near-instantly compile a large hard-coded lookup table. It's understandable for Haskell to do this slower, but currently it is extremely slow and if the table is large enough, it can crash the compiler. Surely we can do better than that. EDIT: Clarification: I'm suggesting a point-release that allows only bug fixes and performance enhancements. Despite my use of the words "language freeze" I'm not suggesting GHC maintainers somehow force contributers to stop working on the language and instead target performance. (I think the very idea of that is absurd and was surprised people would assume that's what I intended.)
I see disproportionate hand-wringing over "boolean blindness" or somesuch. If something has two possible values the harm from doing something like type PowerSwitch = Bool on :: PowerSwitch on = True off :: PowerSwitch off = False is remote. This saves having to write a raft of helper functions for your own type--`Bool` comes with these for free. `PowerSwitch` has the same shape as `Bool` so why reinvent the wheel? I see much less hand-wringing over the misuse of `Int` for things that have many fewer values than `Int` does, or the use of `Double` for things that aren't intended to be approximate floating-point values. Those things cause a lot more harm than "boolean blindness". 
Those are more like `filterMap` and `partitionMap`. The original signature `filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a]` has an advantage: the functional argument can inspect the input, but not change it. Each element of the output is guaranteed to be an element of the input, because `filter` is parametric in `a`. You dont get that from `filter :: (a -&gt; Maybe b) -&gt; [a] -&gt; [b]` or even `filter :: (a -&gt; Maybe a) -&gt; [a] -&gt; [a]`. You do get it with this type: filter :: forall b. (forall a. b -&gt; a -&gt; Maybe a) -&gt; [b] -&gt; [b] filter f = catMaybes . map (join f) The caller receives an inspectable value and an opaque token, and returns the token if the value should be included in the output: filter (\ x t -&gt; if even x then Just t else Nothing) [1..10] But`forall a. Maybe a` has as much information as a `Bool` here, so this just overcomplicates things for nothing. 
&gt; In particular, I'd like to see a tremendous speed up in the compiling of large hard-coded data structures. Can you give an example of any Haskell code using this? It seems like a very uncommon example that would likely yield no improvements in the vast majority of Haskell compiles.
Well, the fact that any fixed length integer type is too small for large factorials is pretty much common knowledge though, isn't it?
Nearly every open source project has release branches and have rules on what can/should be merged in releases including GHC with their bug-fix point releases. I was merely suggesting a special kind of point-release that specifically targets compile-time performance. I'm not meaning to judge people for spending time on things that interest them. I'm letting it be known what I'd appreciate. Sometimes even open-source coders like to please their users after all. EDIT: I'd also like to point out that I was sharing an idea in the spirit of making GHC more awesome. You're the one who decided to assume bad faith. I don't think it's fair at all.
Because, part of the reason it's not common is because GHC simply can't handle it. If we just profile average existing code, people wanting to write perfectly valid Haskell might keep on finding out the hard way that GHC can't do it because the profiling was looking at existing typical published code rather than the code that novices write before their rude awakening. It's a self-fulfilling cycle. The amount of pain involved in large lookup tables is so bad that I think it deserves attention even if the profiler shows lower hanging fruit for typical builds.
Ah, I see. 'twould be a grand thing, if it happens.
That doesn't really refute the point that forbidding certain features is unlikely to enhance others. We could definitely use a good performance centric effort, but blocking other ideas isn't the way to do that.
I suppose if you could go back in time and impose the restriction, you might see a performance increase. But as it stands, imposing the restriction today will not have an effect on performance. GHC will continue to be as slow as it is currently. Instead, we should ask that new features be benchmarked, and make sure they have zero/minimal performance regressions; and we should ask that people spend time on performance enhancement. You just won't get improvement without people actually working to improve it, which you won't get by turning certain people away from GHC development.
There's another dimension to it: do you use those features? In fact, at one point a conclusion we came to regarding GHC performance was that *GHC is probably no slower than it ever was on "classic" Haskell code*. Think Haskell98. GHC hasn't really gotten like, substantially worse in these cases when just compiling very boring Haskell code, even after several major compiler releases. But the reality is people are not writing Haskell98 anymore, but instead GHC Haskell with many other features. So there is a shift in the kind of things people "expect" to do and the compiler to deal with, and this changes the goalposts, so to speak, about where you want to optimize. In any case, I'm not really disagreeing with you, just pointing out is not *only* the addition of features that make things slower, but their *uses* and the interactions between them, that can dictate these things. It's likely "Modern Haskell" has very different cost centres than "Classic Haskell", which probably is part of it all.
Does a bug-fix point release turn away GHC development? I was talking about a single one-time release, not some permanent rule for GHC. I think you and gasche are seriously over reacting.
I would like this. I can never remember if `filter (const True)` means "keeps everything" or "keep nothing". With your definition, those would be `filter Just` and `filter (const Nothing)` respectively. 
Is there a way to go from route definition =&gt; list of routes? That's ond of the advantages the other DSLs (like Yesod or Servant) have. For the purpose of debugging or documentation I can get a fairly flat list of easily-readable routes. 
BTW, does anyone know what the author, u/camccann, has been up to? He hasn't been active on Reddit, Github, and the Stack Exchange sites for a while.
&gt; I think main problem here isn't blindness of Bool but the idea that type signature should be enough and documentation should not be necessary - which is false. I did **not** claim that documentation should not be necessary. I also didn't mean to claim that `filter` should be replaced with this variant. I merely claimed that this variant of filter gives the user more expressive power. It's more capable, but certain less succinct in many cases.
I should clarify; I don't think `filter` should be replaced with this variant. I just think this is a variant that is more powerful and avoids blindness. I guess `filterMap` is a better name.
And I did not claim that you did... but you said that it should be replaced with maybe. Just saying that all it adds is code clutter.
There's something adorable about the names of the functions in that package
It only adds code clutter in the cases where it adds code clutter =P There are plenty of cases where that isn't true. Any time where you care about the details that make an element pass a predicate which aren't present when an element fails, you want to change the type to include those details. The simplest example is a list of maybes filterMap :: (a -&gt; Maybe b) -&gt; [a] -&gt; [b] f :: [Maybe a] -&gt; [a] f = filterMap id It's much cleaner in this case than it would be to use the `Bool` oriented filter, since in that case you'd have to use `fromJust`. There's a pretty wide variety of functions that can be implemented with this `filterMap` that cannot be implemented with `filter`. But all `filter` functions can be implemented with `filterMap` filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a] filter f = filterMap (\a -&gt; if f a then Just a else Nothing) So `filterMap` is strictly more powerful. Not just clutter.
Sorry &gt;:
I'll admit I only skimmed the post, but if you need to parse JSON whose structure is known only at runtime, your best bet is probably to parse the Value and use lens-aeson to simplify reading it.
FYI, your version of `filter` already exists as [`mapMaybe`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Maybe.html#v:mapMaybe). It's a very useful function, but I'd rather keep `filter` as is and have the option to use both.
Right. I actually use this function a lot and somehow completely forgot that it's in base =P
The hard part though, is I know how to manipulate Maybe Int and understand what states it can be in immediately, but I don't know anything about CommandResult without looking up the definition and it's helper functiions. Worse, breaking out the domain types hinders common abstractitons and becomes obsolete once the domain shifts. There are certainly use cases for breaking out new types, but it has a real cost and doesn't always improve understanding. 
Yes, but I am not talking about useful applications of `mapMaybe` eg. `fmap head . filter ([] /= ) . fmap (take 1)`. Original article is about replacing binary decision with custom enumerations. 
My whole point was just that there are a lot of cases where blindness is a problem, and it is often solved with things like `mapMaybe`. When the blindness isn't a problem, then sure, `filter` is fine.
oh oh oh, do you think you were running into http://stackoverflow.com/q/40879983/176841 ? 
General code review is appreciated. 
This is Frege though.. but I'd very much like to learn a principled answer as well.
I guess this is one of those "do you prefer to accidentally lose correctness or performance?" questions and I'm on the correctness side. 
Nope. I wasn't doing patterns. Just hard coding a top level lookup table.
The code looks something like this: ia32_tables :: Array Word8 X86TableDesc ia32_tables = listArray (0,56) [ .... ] tbl_Main :: Array Word8 X86Instr tbl_Main = listArray (0,255) [ ... ] X86TableDesc is just a record. It takes 2 whole minutes to compile that.
In F# it's called `choose`.
I've looked at your tutorial, works great overall. Thank you. Only issue it is a bit verbose. Maybe you have some recommendations on how to make it a bit more compact?
Haskell has a large amount of type-level features that probably cover the Rust type macro use cases even without the involvement of Template Haskell. Lots of things can be written as plain classes or type families.
What is verbose? Parser combinators? When you see something like metar :: ReadP Report metar = do code &lt;- airport time &lt;- timestamp wind &lt;- windInfo return (Report code time wind) you can of course use the fact that all monads are applicatives and turn it into metar = Report &lt;$&gt; airport &lt;*&gt; timestamp &lt;*&gt; windInfo but I'd consider that an advanced techniques and perhaps leave it for the time being. Verbosity is good in terms of maintenance! Sure, it takes a little longer to write, but it is also much, much easier to read after a couple of weeks have passed.
If you give an example in Rust of where type-level macros would be helpful, we can probably translate it into idiomatic Haskell for you.
I strongly disagree, for the reasons I have given elsewhere. It's like suggesting we drop Mac support because Parallels runs Linux so seamlessly. Sure WSL works differently to a VM, but it still requires a Ubuntu install. 
Here is the [original GHC issue](https://ghc.haskell.org/trac/ghc/ticket/12708). Here is the [Levity Polymorphism paper](https://www.microsoft.com/en-us/research/publication/levity-polymorphism/). This looks like a really cool idea. But it is a fairly deep change There is a consensus that we should first implement it in a library and get some experience with it before wiring it into GHC.
The compromise is: do what /u/kqr says, but every time you are about to use `Integer`, stop and think about whether this value could ever go beyond 2^63, and if not, use `Int`. Which means almost always use `Int`.
If your control flow is mostly linear ("do this, then do that, then do this ...") but it's difficult to write it this way, because the API insists on callbacks, then a free/operational monad or /u/tomejaguar 's suggestion will help. On the other hand, if your task makes essential use of events that occur in arbitrary order, then functional reactive programming (FRP) would be the way to go. For more on FRP, see the [frp zoo][2] or my [frp-guides][1]. [1]: https://github.com/HeinrichApfelmus/frp-guides [2]: https://github.com/gelisam/frp-zoo
Corollary: every time you are about to use `Int`, stop and think about whether this value is in a CPU-bound performance critical code path and if not, use `Integer`. Which means almost always use `Integer`.
Your question does not provide all details, but if you do not distribute your code nor binaries built out of that code (e.g. use it only in-house), LGPL and GPL doesn't limit you in any way.
That's like saying Linux support in GHC is obsolete because you can run Windows executables in Linux with WINE.
The article version would be something like `filter :: (a -&gt; FilterResult) -&gt; [a] -&gt; [a]` with `data FilterResult = Keep | Discard`, to make it clear that `Keep` keeps the value and `Discard` discards it instead of remembering if `True` means to filter in or filter out.
Do you know naturally what the Bool means in `filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a]`? Do you know if `True` means filter in or filter out? Because to me it's not naturally clear. Having it a data like `Keep | Discard` would be natural, albeit annoying, possibly.
Well, no, it's not a corollary, it's the negation. I'm generally staunchly in the correctness first camp. First get the semantics right, make the code as readable and maintainable as possible, and avoid premature optimization. But with `Integer` I am wary. Perhaps it's because of my background in other languages; perhaps it's because of seeing how we were burnt with the `time` library, which is wonderful in its overall semantic correctness, but all too often becomes a performance problem only because of its use of `Integer`. Maybe someday we'll have some good annotated integral types that will allow us to specify range limits that are enforced at compile time, while emitting fast code that uses native integers.
The linked article uses HList as an example, using macros to make it less syntax-heavy
Serious answer: Template Haskell can already expand in type contexts out of the box, and there really isn't anything special about it. Not sure if this would fit your particular use cases though. Tongue-in-cheek answer: Use Haskell to implement a compiler for a dependently-typed language, and write your code in that. People [have actually](http://www.idris-lang.org/) [done this](http://wiki.portal.chalmers.se/agda/pmwiki.php)...
Thanks! These instructions are actually not stack-specific. Once you have a GHC bindist using integer-simple, you can install it in the usual way and use cabal normally.
Wish there was a widely-used/standard solution to this myself. Any reason for 2.7 vs. 3+? This guy seems to have done some work on an oddly-obscure library https://john-millikin.com/articles/ride-the-snake/ I pinged him a while back to ask it be put on stackage but no dice. If you want to hire someone, you might try http://www.tweag.io/ since they developed https://tweag.github.io/HaskellR/ for probably a similar need.
2.7 had better library support when we started our Python code base. Also, in the stuff we do we almost almost never need to use UTF-8 instead of latin-1 so the explicit distinction between text and byte strings had little benefit and only increases the complexity of the code.
Wait, quick question: `filter (&gt;7)` throws/filters out all elements greater than 7, right? or do i misremember this one again?
What if you could go the other way? Would that be helpful? https://github.com/nh2/haskell-from-python/blob/master/README.md Also, you've probably seen this? https://wiki.haskell.org/Applications_and_libraries/Interfacing_other_languages#Python
I agree that the type signature doesn't clearly communicate the behavior here, but I think that renaming the function may alleviate this issue. The issue with using a data type like FilterAction, is I can't assume anything about what filter might do to the list, or what the passed in function may do, without knowing the definition of FilterAction. In this case, that's simple, but it doesn't scale as the number of functions in use grows, and it introduces the concern that FilterAction may be extended and innocent looking calls to filter may start doing completely unexpected things. I'm not saying don't make domain specific types, i'm just saying (as the author briefly mentioned at the end) that domain specific types come at a cost, and don't always make things better. 
How about a server that just evals python code you send it? Obviously you would have to consider security, but anyway you would have to consider security.
Could you point me to an example if the first instance - a free/operational monad being used to deal with callbacks.
More helpfully: there is [a connection](http://duplode.github.io/posts/what-does-fmap-preserve.html) between free theorems and some manifestations of naturality in Haskell; that, however, is a case of two independent theories converging, rather than one of one presupposing the other.
This isn't equal in instance declarations, you can write: class Foo a where data A data B instance Foo A where instance Foo B where but you can't write class Foo a where data A data B instance a ~ A =&gt; Foo a where instance a ~ B =&gt; Foo a where without `OverlappingInstances` (and it might not even work with it, I'm not sure). Essentially the instance resolver doesn't know that `a ~ A` and `a ~ B` are non-overlapping, in the same way that it can't treat `Bar a` and `Baz a` (where `Bar` and `Baz` are constraints) as non-overlapping, because there could be some type that satisfies both `Bar` and `Baz`. It works in function signatures, of course, and it's super useful, but you're best off overall just using type synonyms.
As far as I know such an optimization would not be possible with Stack1. stack1new must be able to create a stack with any type inside it. Which would mean you would have to do something like "if a is not a bool then store it like x, if a is a bool then store it like y" which I am pretty sure violates the whole parametric thing Haskell has going on. 
Thank you for the response. I've never actually used the libraries listed on the wiki and I have been wondering if they are viable.
And instead of `(/= [])`, which has type `Eq a =&gt; [a] -&gt; Bool`, you should use `not . null`, which does not require the `Ord` constraint. :-)
Maybe this: https://hackage.haskell.org/package/berp
So the problem is that cpython proper calls python 3? A quick skim on hackage found some packages, perhaps not very up-to-date or bitrotted that may work: http://hackage.haskell.org/package/HaPy http://hackage.haskell.org/package/pyfi http://hackage.haskell.org/package/json-python And here's a semi-updated fork of the classic MissingPy library which was the solution years ago: http://hackage.haskell.org/package/missing-py2 This claims to run with python 2.7.3, but may need a little updating to work with a modern ghc...
and between a new specialized type and the generic old type, you can name values like: kept = True discarded = False for readability. and put then in the docs for filter. you can then capitalize them (or redefine them) later, as constructors. 
&gt; I suspect the package authors would gladly accept a patch fixing this -- and perhaps even before. https://github.com/isomorphism/these/issues/69
Great examples. Thank you. 
Basically to be LGPL compatible you don't need to reveal your own source code but you still need to provide a way to inspect and swap out the LGPL parts included with your project. In practical terms, this means either your project is open source _or_ you link to the LGPL code dynamically _or_ you link to the LGPL code statically but provide object code for your project so end users can still link to another version of the LGPL library.
Yes. TemplateHaskell lets you write macros for expressions, types, and patterns. I'm not even sure Rust has the lats of these. As it turns out, this particular example doesn't even need macros in Haskell. You could define it more nicely using GADTs, or (if you want to stick with the definitions you already have) with a type family: type family HList' [*] :: * where HList' [] = Nil HList' (x:xs) = Cons x (HList' xs) Now, you can write `whatever :: HList' [Int, [Char], Int]`
Could you provide an example of how would you use GADTs for this example? Would it just be like the above example of using data kinds with GADTs, or would it not require data kinds?
Looks like I've got some reading up to do on GHC's type-related extensions... Thanks for the reply!
Which time package do you use now (still `time`)? 
Sorry, I can't help you. Perhaps open up an issue over at the [Github repo](https://github.com/kallisti-dev/hs-webdriver/issues) ? Or ask on StackOverflow and inform the author
Pattern synonyms allow you to do this kind of renaming of constructors, though at the end of the day you don't actually have a newtype.
If you compile with -Wall then GHC will warn you whenever it specialises a polymorphic literal without a specific type annotation. So it'd warn "199 defaulting to Int" during compilation (I generally try to avoid defaulting behaviour, as it avoids these sorts of pitfalls)
&gt; So, in conclusion, just say no to type based shrinking. Nah, use proper types instead. The problem here is that the type allows too many states. In Haskell you'd make a newtype and corresponding Arbitrary instance which only generates even numbers. And shrinking will then just work. Basically, instead of integrating shrinking into generation, you integrate it into the type.
Is there was a general way to convert isomorphic types both ways? Imagine I add another type `data Haha = Lol | Rofl | Lmao` and I too map it like this: Lol &lt;-&gt; Nothing Success &lt;-&gt; Just True Failure &lt;-&gt; Just False So instead of writing lolToMaybe and lolToCr (in general case if I have *n* types, I'd need *n(n-1)/2* conversion functions between them), I have one polymorphic?
So did it just claim that type based shrinking is worse because shrinking is not tied to generation? In QuickCheck generation is tied to types, and I think this indirect 'tying' works pretty well. newtype EvenInt = EvenInt Int instance blah blah -- Could even use a fmap just like Hypothesis quickCheck (\(EvenInt x) -&gt; x % 2 == 0) This simply can't fail like described in the post. Saying generation and shrinking are not tied together is like saying `mempty` and `mappend` are not tied together. **However**, it's still a nice feature that one can construct a generator and a shrinker together, much like writing pretty-printers/parsers together as one bidirectional thing. But it's not okay to blame the parser combinator library that you called the wrong pretty printer, and this doesn't make parsec 'wrong' and bidirectional parsing libs 'right'. Similarly, integrated shrinking doesn't make type-based shrinking 'wrong'.
That's an interesting question. I don't have an implementation at hand, but I imagine it would be fairly easy to implement something like the following: data CommandResult = NoMoreInput | Success | Failure (crToMaybe, maybeToCr) :: (CommandResult -&gt; Maybe Bool, Maybe Bool -&gt; CommandResult) (crToMaybe, maybeToCr) = createIso [ (NoMoreInput , Nothing) , (Success , Just True) , (Failure , Just False) ] createIso :: [(a, b)] -&gt; (a -&gt; b, b -&gt; a) createIso = error "Unimplemented"
It's committees all the way down.
That uses a List though. I imagine storing that amount of Data in a linked List will suck no matter which way you do it.