in lens, it seems functions' operator aliases are often flipped, for that reason. e.g.: &gt; (\^..) = flip toListOf so: &gt; f xs = xs ^.. (y . z) &gt; f = toListOf (y . z) as well as for left-to-right composition. 
The function `subtract 5` already does that, and indeed the Haskell definition of infix makes it weird ghci&gt; 5 `subtract` 3 -2
Here's a tutorial on how to setup emacs for haskell. https://github.com/serras/emacs-haskell-tutorial/blob/master/tutorial.md 
Thank you!
This thought occurred to me as well, but I didn't think it was important enough to bring up. My workaround has been to do this: map (`mod` 2)
Thanks. I don't think that merging is a good idea. This library falls in the TH problematic area, which is not much of the concern of "monad-control". It also bears a more general title than something like "monad-control-derive" for a reason, since it already provides features not only for "monad-control" and is intended to evolve in that direction.
Seems very insightful! My vote goes for that unless presented with a convincing contrary case. All the current APIs with verbal functions intended for infix usage seem extremely inconsistent and offputting to me. Your proposal could solve this. PS. I recommend to update the title of the post to be more descriptive. I for one at first interpreted it incorrectly and almost downvoted.
A quick meta-comment addressed at /r/haskell readers (and voters): I suggest you hesitate before downvoting comments that are already in the negative but that have constructive contents and, especially, constructive *replies*. For some readers, comments with deep negative scores will automatically collapse, hiding (in this case) valuable discussion. I only pile on downvotes for trolls, memes, bullshit, and that sort of business.
&gt; Why would one want to limit his / her audience Exactly! I highly doubt that there exist any experienced Haskellers of Russian or whatever other descent who don't speak English. It is the de facto language of the community and of the absolute majority of learning materials.
I am afraid it is too late to actually change this in Haskell. But I think it worthwhile to bring up anyway just to hear what others think of it and if there would be any major problems with flipped infixed funs. About the title: Thanks for bringing it to my attention. I'll see if I can come up with a better one. EDIT: Apparently titles cannot be changed.
+1, to the point that I would actually like to see it happen if someone could construct a path there that wasn't batshit insane. I'm skeptical that such a path exists, though. Edited to add: On second thought... what about adding a new "make this operator a function, but flipped" syntax as an extension? Edited to add, the second: Oh, apparently no syntax extension necessary? https://www.reddit.com/r/haskell/comments/2sk390/i_wish_that_f_flip_f/cnquut8 Maybe we should start using that (and designing for it)? 
Notepadd++ is an easy install. The syntax highlighting is built in. Configuring tab stops and all that isn't hard.
Duck! Incoming idiot non-programmer question(s): Why is it too late to change this in Haskell? How difficult would it be to change the language to have the behavior you mentioned?
I disagree but I do want a good way of using postfix notation. f $ g $ h $ x means you take x then apply h then apply g then apply f. Cognitively it makes much more sense to me to think in "chronological" order than reverse chronological order: to write things in the order they happen. (~) = flip ($) x ~ h ~ g ~ f 
Every library that uses backticks to use functions in infix form would break, many silently for function that take two arguments of the same type. EDIT: I suppose with a pragma it could be optionally enabled as desired, though it still could be a pretty disruptive change, when it comes to being able to easily understand other's code.
That would be pretty awesome. Or perhaps very confusing with sometimes overlapping imports. -XImportAnywhere would also let literate haskell posts not start off with the traditional "here's a pie of imports to ignore" section; the imports could be put at EOF. :) I have to admit I've sometimes thought about #include imports.hs; it gets tiring importing S and M and B and T into every other module.
A few more counterexamples: a `zip` b -- zip a b a `mappend` b -- mappend a b
I have a lot of experience with both. Simply put React is better in every aspect. React has significantly better performance, cleaner design, better community, doesn't have a bunch of useless baggage like Angular modules, better quality overall - Angular is constantly introducing backwards-incompatible changes and is riddled with bugs - the documentation is very incomplete and downright incorrect in many places. Also did I mention the Angular community is mostly illiterate children that have been indoctrinated to just spout 'Angular Best' over and over?
You don't need to learn `lens` for this--/u/htebalaka is just pointing out that `lens` exports a convenience function, `(&amp;) = flip ($)`, just like your `(~)`. I think some other libraries do the same thing--I think `threepenny-ui` calls it `(#)`.
Indeed those are good examples of functions for which the current (order preserving) behaviour is a bit better.
Virtually everything written in Haskell would break and need to be changed. I would call that a technical issue.
Actually you can't do that because (-2) is lexed as a number, but I get your point. `map`&amp;nbsp;`(/2)`&amp;nbsp;`[7,`&amp;nbsp;`5]` will be `[3.5,2.5]`. But that is because `(/`&amp;nbsp;`2)` is not the same as `(/)`&amp;nbsp;`2`. The first is a section, the second is regular function application. If you want them to be the same, you would need the infix syntax to flip the arguments, and then of course fix all the current operator functions to take their arguments in the correct order. oh and it would also eliminate having to wrap them in parentheses to use as normal functions - they would be normal functions at that point.
How about a `f` b &lt;=&gt; f a b &lt;=&gt; b ´f´ a ?
I'd prefer it too. IMHO the nicest solution of all. And the only argument against is the clashing with Data.Sequence's `(&lt;|)` and `(|&gt;)`.
I quite like Agda's mixfix function application syntax: _and_ : Bool → Bool → Bool if_then_else_ : {A : Set} → Bool → A → A → A So, that `sepBy`would be: separate p by (symbol ",")
I suspect a "SUPERCOMPILE" pragma is inadequate. You don't want to supercompile a function, you might want to supercompile map in it's first argument, when that first argument is one of a handful of closures. It's probably a stop-gap measure, but I can't see it working in the long term. Compiling from GHC Core to GHC Core leaves the things you can express constant, and you then need to lower to ASM afterwards and do things like register allocation. The GRIN work showed if you think about everything together you can do better, by having custom calling conventions per function based on how things work together. Supercompilation could potentially do the same thing.
As an example, supercompilation statically discovers that map foo can be fused together to eliminate some overhead. JIT might discover that dynamically, by observing foo is the common argument to map, and by tracing discover how that conjunction usually works together. JIT has to include a dynamic check, but as long as its outside the inner loop, they may roughly add up to the same thing. Note that this is an idea I think people should explore, not a conclusion :)
&gt; Why do people assume I just crawled out from under a rock here? Because you behaved like it? Your arguments are good (I think I fall somewhere in between you and Johan on this issue) but you undermined them by accusing him of complaining for the sake of complaining, and people called you on it.
I'm not sure I agree with your proposal, but on a related note I have wished that I could make *any arbitrary* expression infix, provided it has the right type. I can often partially apply a function to reduce it to a function of two arguments, but I can't then use infix notation to use that - which seems a shame.
Yeah, sorry, that's not how arguments work. They either stand or they don't. People are literally telling me to watch my tone because Johan is someone "important". Let's assume for the sake of argument that my post really was so insulting: in that case, why should who he is matter? Shouldn't my "insult" be a problem no matter what? No, the reality is that what I *literally* said was that this was all "griping for griping's sake". It wasn't even directed at him, but the *entire thread*. He didn't even submit this thread; when I commented on it, he wasn't even here. No *reasonable* person could assume that I had it out for him personally in any way. When he *did* decide to comment, the first line of his post was "This kind of statements are unnecessary in civilized discourse." Honestly, which is more rude? "This is all griping for griping's sake" or "This kind of statements are unnecessary for civilized discourse"? This is massive escalation. Instead of being a rational discussion, it descended into a flamewar. My post was sitting at +15 when he commented on it; obviously lots of people here agree with my points. No one thought to tell me to watch my tone until he came along and made the discussion about that instead of CPP. He took a statement obviously not directed at him personally and made it personal. Frankly, I found it exceptionally childish.
I'm leaning towards the idea that instances should be named, and in case of ambiguity, explicitly passed. That would make Haskell a somewhat different language, though.
CPP is ugly, but there is some much breaking changes in both GHC and libraries, all the time, that if you want to be able compile things on environments other than a single computer at a single point of time, you need some form of conditional compilation. (I'm personally also guilty of abusing CPP for fun and profit, but that's much rarer) We could have a better conditional compilation system and/or macro system though (Template Haskell is overkill for many purposes, not talking about breaking changes in TH itself... Also it has a single implementation - I'm still hoping for an ecosystem with more than one Haskell compilers).
Yes I realized while writing that comment the reason was probably the infix syntax. Indeed often there are "conflicts of interest" when deciding argument order.
Has 7.10 changed the type of `elem` in the `Prelude`? If so, this is exactly an example of a bad change.
I think the proper way to solve all this would be to a) not have backtick syntax and b) have mixfix syntax: sep p by (symbol ",") Actually... mixfix is still limited. Language-level support for [fluent interfaces](http://en.wikipedia.org/wiki/Fluent_interface) would be nice. Or, well, just do it with GADTs and other type-level hackery: It is *rarely* the two-argument functions you're confused about.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Fluent interface**](https://en.wikipedia.org/wiki/Fluent%20interface): [](#sfw) --- &gt; &gt;In [software engineering](https://en.wikipedia.org/wiki/Software_engineering), a __fluent interface__ (as first coined by Eric Evans and [Martin Fowler](https://en.wikipedia.org/wiki/Martin_Fowler)) is an implementation of an [object oriented](https://en.wikipedia.org/wiki/Object_oriented_design) API that aims to provide more readable code. Fluent interfaces are examples of [monads](https://en.wikipedia.org/wiki/Monad_(functional_programming\)). &gt;A fluent interface is normally implemented by using [method cascading](https://en.wikipedia.org/wiki/Method_cascading) (concretely [method chaining](https://en.wikipedia.org/wiki/Method_chaining)) to relay the instruction context of a subsequent call (but a fluent interface entails more than just method chaining ). Generally, the context is &gt; &gt;* defined through the return value of a called method &gt;* self-referential, where the new context is equivalent to the last context &gt;* terminated through the return of a void context. &gt;The first two points correspond to &gt; &gt;* the unit of a monad &gt;* the fact that a monad is an [endofunctor](https://en.wikipedia.org/wiki/Functor) &gt;while the method chaining is the binding operation. &gt; --- ^Interesting: [^Microsoft ^Office ^2007](https://en.wikipedia.org/wiki/Microsoft_Office_2007) ^| [^SmartDraw](https://en.wikipedia.org/wiki/SmartDraw) ^| [^Builder ^pattern](https://en.wikipedia.org/wiki/Builder_pattern) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cnqsuzx) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cnqsuzx)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
What if the person in question has knowledge equivalent to a CS degree without having officially studied for that degree at a university? Would that not demonstrate equivalent or greater dedication? Which options does such a person have?
I've renamed it to [AutoStack](https://github.com/jrk-/autostack).
IMO it's a gratuitous change. But you're right that it does not require CPP since the type signature you may have to add is backwards compatible.
I think this is exactly what I need! :) thanks! 
Using (&amp;) = flip ($) you can: [1, 2] &amp;(zipWith (+)) [3, 4] == [4, 6] 
It's how Haskell programmers do things in order while maintaining all the other guarantees of the language (related to state, laziness, etc) You basically need monads with all the other guarantees of the language. http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html is a good article on this opinion on them. Monads, while not needed in most other languages, are neat and clean in a way many other control structures in those languages aren't. You are going to understand monads if you do anything big in Haskell. You just can't really get that far without them. But the real joy is using them in other programming languages to get side effect free implementations of a lot of complex initializations and transformations. See [Ruby](http://codon.com/refactoring-ruby-with-monads), [JavaScript](http://sean.voisen.org/blog/2013/10/intro-monads-maybe/), [Objective C (This implementation of maybe is particularly instructive as to what it's replacing, as it's a implementation of a particular monad)](https://github.com/svoisen/SVMaybe), and [C#](http://blogs.claritycon.com/blog/2013/08/functional-concepts-c/) Haskell, in general, is willing to use what I'd call "Poorly defined in English concepts" if they have strong functional (in the typical english sense of the word functional) definitions. All that's really required is a strong symbolic/structural description of how something works and a name. I like to say Haskell is willing to use "named and sketched, but unexplained" concepts a lot. My English definition would be: **Monads are a set of side effect isolating ways to chain operations.** They are probably the simplest way to do many things in Haskell, but a pretty good way to do things in imperative and other functional languages much of the time to isolate failure concerns and improve the likelihood of correct results You've probably written much of a monad if not its entirety if you ever tried to make some sort of logging, item creation system, etc very chainable in another language.
I occasionally wish this was how things worked, especially when I try to partially apply things like `member` or `isSubsetOf`. But (as has been pointed out) it's not a straightforward improvement. Cases like `zip` and `mappend`, as well as the broken symmetry between backticked identifiers and parenthesized operators, make me uncomfortable with this being the default. I think ideally we'd have `&amp;` (or something like it) in the prelude and encourage libraries to order arguments for partial rather than infix application.
I recommend composing functions with (.) or (&gt;&gt;&gt;) first and then applying them with ($) or (&amp;). Composed functions are way more friendly to refactoring, by being... well... composable.
Sometimes the only option is to define the instance as an orphan in a library. For example, say there is a type class that is not in base (`ToJSON`, for example) and a data type that is defined in a different package that is not base (e.g. something like [`These`](http://hackage.haskell.org/package/these)). Where should a `ToJSON These` instance live? Neither of the packages depend on the other. The only relatively sane solution is to create a package `these-json` that depends on both and contains the instance. But you want this instance to be *the* instance, so it has to be a library that everybody imports.
This feels cleaner to me - no new type needed: infixl 0 -|, |- (-|) = flip ($) (|-) = ($) ... [1, 2, 3] -|zipWith (+)|- [4,5,6] Or break out the unicode, and use, say... `◃` and `▹`: [1, 2, 3] ◃zipWith (+)▹ [4,5,6]
But (\`minus\` 5) can be used. 
This just seems like bike shedding, have we reached at that point as a community now?
Hah! I had the same thought...
I don't claim that I have thought through all issues. But I feel there should be solutions (say the Set type is parametrized by the instance?). How different the language ends up, how knows :)
Don't get me wrong. I wish all the best to this project. It's very frustrating to realise that all the Haskellers from Moscow could probably fit in a single bus. It certainly is an important investment into changing that. However I reason from a position of a publisher of the content, and from such a perspective the point about limiting the audience is a killer. I for one, being a Russian who lives in Russia, maintain my blog on Haskell in English for that exact reason. I imagine I could make a few publications on your site and Habr, but it certainly wouldn't be often. Beginners certainly can generate the content but what's the value of it? I wouldn't bet on them. You need to attract professionals.
Somebody waited long for this bug number.
That makes a lot of sense, thanks.
Academia is a different planet entirely. There's a lot of previously written material out there explaining the culture, but it's not for the faint of heart. You'd better have a proven record in an academic setting, strong recommendations, and an enormous dedication to your work **just to get in**. There's already a lot of competition from very qualified people seeking the same roles as you. The no degree route may work in industry, but there is no way it would work in an academic setting, *barring exceptional cases*[1]. [1] http://www.newworldencyclopedia.org/entry/George_Green#Late_undergraduate_education
almost as evil as evil :p
That sounds like a HLint warning, not a GHC warning.
Its a matter of taste. I also prefer to disable that particular warning.
This ! Thank you.
For the lazy, here is a proper comparison of the two: mapM_ (printEmailAddress connection) (reverse messageList) mapM_ (\uid -&gt; printEmailAddress connection uid) (reverse messageList)
Hmmm, I think the key point to take away is that the list that I called "messageList" should perhaps be called uidList in which case the meaning/intent is clear. On the other hand, your own example of mapping the length function over "xs" seems similarly opaque because (to me, anyhow) it is unclear that xs represents a list of strings, so using a parameter called 'str' is helpful. Now granted there may be times where in fact you don't care what is the actual type of the list and I get that but I wonder if it's as easy to understand an algorithm when you don't know what kind of variables are being manipulated. While I recognize the value of conciseness in principle, I've seen too many examples where programmers write really cute one-liners that are almost incomprehensible (and so hard to maintain) but would be really clear if 3 or 4 lines were used instead. I'm wondering whether the Haskell idiom contributes to this.
Something like `map (\str -&gt; length str) xs` would be rather inaccurate since `length` can take a list of any type. It's not clear that xs contains a list of strings in `map length xs` because it doesn't have to.
I find programs I write in Haskell are very significantly shorter than ones in other programming languages, and that's because it's clear and consise. I disapprove strongly of writing incomprehensible code, but whilst you can be incomprehensible by being stupidly terse you can also be verbose to the extent that the meaning of your code is obscured. Certainly there's no harm in omitting the lambda, nor in calling a string xs - a traditional haskell name for a list. for example. once you're more used to haskell and thinking in terms of functions, you'll start to find f = function3 . function2 aconst . function1 clearer than f x = answer where inter1 = function1 x inter2 = function2 answer = function3 inter2 It's not cute, it's mathematical.
Ed tells me you're the fella filming all this - and you did a great job! I haven't watched all the lightning talks, but the points in Encapsulation vs. Code Reuse are very interesting. On the one hand, I can see how frustrating it is to be locked out of an API without good reason, but on the other it "feels right" to keep some things private. I wonder if with stronger types there is progressively less of a reason to put up abstraction boundaries. Either way /u/ChangeTip $1 (private) :)
Link to slides somewhere?
For cases where you need coherence, names are unnecessary because uniqueness is a requirement. For cases where you don't need coherence, data structures bound to names but normally passed as implicit arguments is the way to go, but not incredibly popular in Haskell. (This is the way Scala does everything.) It would be great to get all the advantages of both with a uniform syntax, but that's impossible.
hi, I'm the author. I just added a customize interface for the executable and the flags, so you're proposition might work now. Greets
Thanks so much for all the feedback, it's much appreciated.
You can definitely eliminate lots of different types of runtime costs using partial evaluation and supercompilation. In your case, since your program doesn't have any dynamic input, I think partial evaluation and supercompilation would do more-or-less similar optimizations. The definition of supercompilation is not clear to me. I'd like to think it as a specialized use case of partial evaluation. I'll give an example using partial evaluation but the point definitely applies to supercompilation as well since it also based on evaluating open terms based on known arguments/values. One amazing example is "Futamura projections", as described in [Futamura's 1971 paper](https://cs.au.dk/~hosc/local/HOSC-12-4-pp381-391.pdf). It basically shows that if you partially apply(partial evaluate the application) an interpreter to a program, you basically compile the program, meaning that you eliminate the some(ideally all) of the costs like parsing and type checking. (since those only depend on input program, not input of the input program) Of course this is an idealized case, in practice you probably couldn't eliminate 100% of the parsing/type checking costs, but could definitely eliminate a lot. (Futamura goes further, and applies partial evaluators to themselves, and gets amazing results. You can read his paper(linked above) and my blog post about it: http://osa1.net/posts/2015-01-11-understanding-futamura-projections.html) But again, this is probably never used in practice. (because of lots of reasons that I won't dive into right now) Instead I think one practical tool which does something close to this "idealized" partial evaluator is the multi-staged programming language MetaOCaml. In MetaOCaml, you can write an interpreter that does all the parsing and type checking stuff in stage 0, and interpretation(execution) in stage 1. You can then generate a specialized version of your program, compiled with standard OCaml compiler with all the optimizations, that doesn't do parsing or type checking.(this specialization is done on given input program, of course) This is equivalent of compiling your program to native code and you don't need the interpreter anymore. If this doesn't sound like insanely cool, let me sum up what happened: You wrote an interpreter then specialized it on a source program. This gave you a compiled version of source program and you eliminate all the parsing + type checking costs. You got a compiler from your interpreter. This is of course, not a partial evaluation. In multi-staged programming you manually guide the program to generate specialized program, so in a sense you do the job of partial evaluator. (or at least guide the language to do the job of a partial evaluation) --- In your _particular_ example I think given "sufficiently smart" supercompiler, it's possible to eliminate the costs.
You would be *amazed* at how frequently doubles (and floats!!) are used in finance. I have worked at a large investment bank and currently for for a large hedge fund, and can confirm that use of floating point types is common in both of them.
General comment: I get the sense that lots of folks use the term supercompilation to refer to computing under binders/full beta reduction/any compile time partial evaluation. Supercompilation does involve computing under binders but it specifically involves introducing "speculative" case analysis on free variables. Picking good places to introduce these "splitters" on free variables involves heuristics and search and cleverness (unlike many partial evaluation policies like constant folding).
Incidentally, does the Haskell Plugin for Sublime Text 3 support ligature replacement?
and don't forget printEmailAddress connection `mapM_` reverse messageList
hungarian notation was supposed to identify type in a broader sense. ie a poor man's newtype. I can't recall the creator, but I remember reading that he hated what hungarian notation has become. for example: String domainFred = emailaddressFred.getDomain; 
Better to use a 10-word phrase that 80% of your audience will understand than one word that &lt;8% of your audience will understand, IMO. I'm not completely opposed to having to learn more to understand the error message, but it does need to be understandable to a programmer that didn't implement the part of the code that has the error in it. Skolem was/is probably the worst offender I've run into.
Why not just use `Integer` and make it even multiples of whatever the smallest currency unit is? Example: newtype Money = Cents { getCents :: Integer }
The thing is, problems with skolem variables are a result of the typechecking algorithm trying to do a unification with a unification variable that occurs outside of the special existential constraint used in the OutsideIn inference mechanism GHC uses. There's no 80%-understandable explanation -- you have to know about these type-checking algorithms either way, in which case you'll know what skolems are.
Ah, obvious! Thank you! I'll use that. I still need a quick way to set individual pixels, though, for depth buffer manipulation :( 
I didn't use slides, but the code is all up on https://github.com/kwf/data-kiln.
Except better, because of typeclasses. When other languages have a function like Data.Traversable.sequence it is always specialised to e.g. lists or sequences. Rarely does it work for all monads.
The key word is "demonstrate". Someone may have enough knowledge to start a PhD (which is not enough to mean that they are the kind of person who could deliver a PhD thesis more or less on time, but I digress), but their possession of that knowledge is not in itself a demonstration. Similarly, a supervisor will have to demonstrate competence in risk management when hiring. Options include: (1) find a supervisor who isn't scared, working at an institution which can afford the risk; (2) get a degree, possibly on a more tightly timed masters programme; (3) continue outside academia, drawing status from the interest one generates in one's ideas, rather than one's collection of parchments.
Nonstandard career trajectories would certainly be more flexibly accommodated if (as is often the case with summative assessment at school level) the dissertation/degree exam process were administered separately from the preparatory education, so that anyone capable of passing the assessment could have the qualification. Of course, that would necessitate a bit more standardization of assessment than most teachers of undergraduates would enjoy. And dissertation supervision (no way we're hiring you to write a thesis if you haven't worked with a supervisor to produce some chunky academic writing already) is harder to unlink from the degree teaching process. It is, of course, possible to have fun at university as a mature student with a lot of experience of the subject as deployed in practice. You'd certainly get plenty of opportunities practising the communication of ideas to other students. It does take time, of course, but some masters courses might be less risk averse in their recruitment policies than PhDs tend to be. Remember that a PhD is a professional writing and teaching job. Having and sharing ideas still gets to be your hobby, of course, and you might even get a little more time for it. Compatibility with academic processes is an important attribute for a PhD studentship candidate to demonstrate. It's true that not everyone with a degree actually has that compatibility, but it's a good indicator.
Oh, these would also be good examples to put on the new Haskell homepage (when it lands) as they are up to date and addressing a wide variety of topics. 
At least the *Hasklig* project does not yet support Sublime. On their [github page](https://github.com/i-tu/Hasklig/) Sublime is listed under *No Support* as &gt; - Sublime Text (Vote for the enhancement *here*)
Looks very easy to use. I see that Jeff is on the maintainers list for diagrams-svg, so I'm curious about the motivation for this; is it for people who already know SVG and/or want to generate precise output? I'm a bit of a stranger to SVG generation and the diagrams package, but if the use-case ever strikes me I will have to choose between these options. Any comparison would be appreciated.
Nice stuff! Could you share your dotfiles you used to set this up?
As it happens, I did a PhD in CS without an undergraduate CS degree. (I did have a BS in physics, though.) To get into grad school, I needed to demonstrate that (a) I knew enough CS, (b) I was able to grapple effectively with the academic literature, and (c) was capable of integrating into CS academic culture. I did this by (a) doing well on the CS GRE exam, (b) building a compiler for a strongly typed object-functional language whose type system was built on recent academic research, and (c) befriending some CS academics well enough that they were willing to write me recommendation letters. It is likely that if you did something similar you could swing admission to a good program. Expect high variance in the process, though, because you are trying to convince the admissions committees to take a chance on you, and open slots for new students are quite few. &gt; If you think about it - what is the university losing by accepting a student that pays for everything himself? You can't pay for everything yourself, even if you're Bill Gates. Building a really effective student-advisor relationship takes serious amounts of time and emotional energy, which are not fungible with money. Because the PhD process is a preindustrial apprenticeship process, most universities do have some kind of procedure for handling applicants who are very good but don't meet the forimal criteria. This means that if you have a champion on the inside (i.e., a potential advisor) they have a way of getting you in --- but note that this involves developing relationships.
I achieved xmonad in a sandbox simply by putting a wrapper script into my path which checked its parent PID's cwd to see if it was called by xmonad and added a cabal exec with the matching sandbox option around the ghc call in that case (with cabal-install 1.22). I am not on that system right now but if there is interest I could post the wrapper script here. Of course the xmonad binary from that sandbox is also in my PATH.
IMO, "invalid type unification outside of existential constraint imposed by inference" would be a **lot** more useful than "Skolem". I'll admit that "unification" is still problematic, but my understanding is that this can't occur when unifying a type variable with a concrete type, so using something like "type equality testing" is not the lossy simplification it could be in other contexts, but it just plain incorrect. Other than that, most programmers will know those words, even if they haven't seen them in that order.
But don't you have to specialize when you implement a typeclass for a particular type?
That's cool! Wonderful to have this kind of instant feedback/interaction on reddit :)
1) I just found https://github.com/i-tu/Hasklig which might do this 2) How did you do that autowikibot?
That clarifies things, thanks!
oh, look, it's that time of the week again
Reading it now. It's pretty good!
&gt;&lt;&gt;
Just realized that link is just for the font --- still need sublime support. Bummer
I've written http://hackage.haskell.org/package/gloss-juicy to go from juicy pixel images to the gloss format.
If lookup ref xs' returns a Maybe (Maybe Int), wouldn't join be the canonical thing to use in place of fromMaybe Nothing? Why does solve' concern itself with name in the first place? Shouldn't you run a lens over snd, or something like that? Edit: Now I have an urge to implement it. solve :: [(String, Either String Int)] -&gt; [(String, Maybe Int)] solve xs = fix (\xs' -&gt; map (second $ solve' xs') xs) where solve' xs' Left ref = join $ lookup ref xs' solve' _ Right x = Just x
C++ templates are not quite the same thing as generics in the sense that's understood in Haskell, ML, and even the Java family. Templates are parameterized code specialization (where some parameters may represent types), while generics are type-ignorant code that always does the same thing regardless of the types it's polymorphic in. A generic (polymorphic) function *can't* behave any differently based on the actual value of its type variable. Type classes introduce a *limited* way to introduce some type-varying behavior to otherwise polymorphic functions. It's possible to do this sort of generic code in Haskell, etc. because of the uniform representation of references to objects. C++ templates must deal with varying-size/shape instances directly due to the much heavier use of value types in C++. Templates have other uses aside from allowing you to write similar code to that which an implementation of generics allows, and some of that extra stuff includes a bit of overlap with what you can do with Haskell type classes, but again they're also rather different. It's also possible for GHC to do some code duplication and type-specific specialization for type class instances, but this is purely an optimization and not fundamental to how it's system of polymorphism and type classes works. 
Sounds cool! I didn't realize ~/.xmonad/xmonad-xxx-xxx don't have to be a binary - a shell script does make things easier. Can you show me that wrapper scipt!
Did you mean my xmonad config files?
This is stored on my system as /home/taladar/haskell/x86_64/bin/ghc which is in the PATH before the actual ghc binary in /opt/haskell/x86_64/bin/ghc. xmonad, xmonad-contrib and xmonad-extra (as well as potential other dependencies of the xmonad.hs) are installed in teh sandbox in /opt/haskell/x86_64/sandboxes/xmonad (or rather the one I got when calling cabal sandbox init in that directory). #!/bin/bash PATH="$(echo "${PATH}" | sed -e 's#/home/taladar/haskell/x86_64/bin:##')" if grep -q "xmonad" "/proc/${PPID}/cmdline"; then /opt/haskell/x86_64/bin/cabal --require-sandbox --sandbox-config-file=/opt/haskell/x86_64/sandboxes/xmonad/cabal.sandbox.config exec -- /opt/haskell/x86_64/bin/ghc "$@" else /opt/haskell/x86_64/bin/ghc "$@" fi It is not a shell script replacing anything in ~/.xmonad/xmonad-xxx-xxx, it is a shell script wrapping ghc so when xmonad calls it it is called with the matching sandbox.
I'm struggling to figure out what you mean.
ah, I see! I got it wrong, you are wrapping ghc when xmonad wants to recompiling it, Nice approach!
Hi! First of all, there is a tool named "hlint" which scans your code and gives suggestions. This is what it gave: Found: (show $ take y xs) ++ "\n" ++ lineToStr (drop y xs) Why not: show (take y xs) ++ "\n" ++ lineToStr (drop y xs) Parity.hs:39:43: Error: Use all Found: and $ map (== head xs) (tail xs) Why not: all (== head xs) (tail xs) Parity.hs:78:1: Error: Eta reduce Found: findCompletionPath gs = aStar (S.fromList . findNeighbours) (const . const 1) (const 0) hasGameEnded gs Why not: findCompletionPath = aStar (S.fromList . findNeighbours) (const . const 1) (const 0) hasGameEnded 3 suggestions Second: use a module header. It'll allow you to import your code in other modules module ParitySolver where You'll have to rename your file slightly. Further: good comments, good usage of new types (Board, Direction, etc) You can use "unlines" for the show method. unlines $ map showOneLine lines To update values in tuples, you can use first and second from Control.Arrow (\(a,b) -&gt; (f a, b)) == first f (\(a,b) -&gt; (a, g b) == second g (\(a,b) -&gt; (f a, g b) == f *** g Edit: also checkout "unwords", "intercalate", "intersperse" and "nub". There also exists a search engine, https://www.haskell.org/hoogle/ where you give a type, and hoogle returns a function for it. If you use duckduckgo, the bang !h works for it.
* What is this Color type for ? :P * Passing (const . const 1) and (const 0) to astar looks like a smell to me : maybe astar is an overkill algorithm for your problem. I don't know if it could be a cause of your program to be slow.
What are the functional differences between hdevtools and ghc-mod?
I think they are implying that it's time for another "Approximation of Monad while missing the point entirely" blog post. Not sure if this one actually fits that bill, but I'd wager that's the complaint. For example &gt;The analogy is only valid for a few monads. The ones I’ve seen that this applies to are IO, State, and Get from Data.Binary.
Indeed, though the execution time, even when compiled, is still very high, taking into account that the provided example is one of the first few (simpler) levels. Any idea how I might improve the performance?
but... apparently ghci-modi is going to be deprecated soon. https://github.com/eagletmt/ghcmod-vim/issues/48
Indeed, my bad.
Thanks for your tip! I've implemented the heuristic you suggested and I've gained a lot of speed. All levels I've tested so far were solved in &lt;&lt;1s, whereas before it was more like 5 - 30s
You could speed it up even more by calculating the maximum of the fields outside of the map f fields. This way it'll only be calculated once, and not every time your doing the subtract for each field.
Shouldn't that be optimised away by GHC? Anyway I tried that in the most recent commit, with a little speed test, though there is no noticeable/measurable difference in exec time. 
In SML, the linked list type doesn't include bottom as a value. If you're writing a function that accepts a list as an argument, you know that either the list will be finite or your function won't get called at all. Not all well-typed expressions denote values, though. Some may fail to terminate. That's a little bit of anarchy, but still less anarchy than in Haskell.
I used it for some geometry code. Using floating point I could not even compare two triangles for exact area equality. Using floating point for even slopes of a line leads to only an approximation of the real slope. 
You might be interested in reading up on loeb: * https://github.com/quchen/articles/blob/master/loeb-moeb.md * http://blog.sigfpe.com/2006/11/from-l-theorem-to-spreadsheet.html
`&lt;self-advertising type="blatant"&gt;` You might also be interested in another take on Löb's-theorem-as-fixed-point, which forms the heart of my current thesis project: https://github.com/kwf/ComonadSheet. I gave a talk about it at Boston Haskell: https://www.youtube.com/watch?v=F7F-BzOB670. `&lt;/self-advertising&gt;`
This is a really clear explanation of what the "expression problem" actually is.
Here's my effort: import Control.Monad (liftM2) import Lens.Family ((%~)) import Lens.Family.Stock (_2) xs :: [(String, Either String Int)] xs = [("a", Left "b") ,("b", Right 42) ,("c", Left "a") ,("d", Left "e")] solve :: [(String, Either String Int)] -&gt; [(String, Maybe Int)] solve = (_2 %~) . flip partialSolve &gt;&gt;= map partialSolve :: Either String Int -&gt; [(String, Either String Int)] -&gt; Maybe Int partialSolve = either partialSolve' (const . Just) partialSolve' :: String -&gt; [(String, Either String Int)] -&gt; Maybe Int partialSolve' s = liftM2 (&gt;&gt;=) (lookup s) (flip partialSolve)
&gt; because now we must distinguish two semantics for types -- their "well typed inhabitants" and their "value" inhabitants I don't think I understand your objection. Isn't marking specific terms as "values" the first thing we learn to do in, say, the early chapters of TAPL when defining operational semantics? Isn't this the same story all the way up to fancy type systems and canonical forms etc? Are you talking about something else?
Cool! Looking forward to seeing the final project and thesis.
This reminds me of the way polymorphism works for objects in PureScript
Is no one on reddit today? This is literally the most exciting thing that's ever happened on /r/haskell. Nikita has secured his place in heaven. Why aren't we all high-fiving right now?
I think it's cautiousness and a bit of shock :) that said, this looks like the silver bullet we've been looking for!
I may be wrong, but it seems you can reuse `FreeT` just factoring the difference out: data X x f b = X (x -&gt; b) (f x) instance Functor (X x f) where fmap f (X g w) = X (f . g) w type ReplayT x f = FreeT (X x f) I don't know if this `X` type is used/declared somewhere else.
I think "head" can be seen as a "record access" too. Consider this: data List a = Cons { head :: a, tail :: List a } | Nil 
The main disadvantage is the use of Template Haskell, which can slow down compile times and cause issues with cross-compiling for different architectures. That said, there's no reason this couldn't eventually be merged into the Haskell compiler as an extension if it stands the test of time and then you wouldn't need Template Haskell. It would also probably be syntactically sweeter, too.
Since GHC 7.6
I'm in a hell of a shock myself! If you'll look into the commit history, you'll see that I've started this project just 2 days ago. Not a single project ever went so smoothly for me. The whole idea just appeared out of the blue and so clearly. Weird I tell you!
Because it was posted a few hours ago, a lot of us have probably been doing other things (in the U.S., it's the middle of a long weekend, and in Europe it's midnight), and (speaking for myself alone) it's not immediately obvious why this is better than any of the numerous extensible/anonymous records systems that already exist (e.g. `vinyl` or `HList`), other than being pervaded with TH, which (again, speaking for myself) immediately obfuscates what's actually going on. Not to be a downer--I'm not passing judgment on the library by any means. Just answering the question.
Does this work with fields that use `RankNTypes`?
You wrote that, right, I was wondering about the date though.
I didn't even know that you could use records with sum types—I could have sworn it was invalid Haskell. Someone kind must have lied to me. But now—this is one of the most disgusting things I've seen on Reddit. ... Is there room for one more in your shower?
Not yet. It's not like there's something tough about this, we just need to extend [the parser](https://github.com/nikita-volkov/record/blob/master/library/Record/Parser.hs#L44-L71).
A strict language makes two guarantees about the behaviour of evaluation. The first is that we only evaluate closed terms, we don't evaluate under lambdas. The second is that we only evaluate the body of a function if it's arguments are in normal form. Since all closed normal forms of type `ListInt` are finite this allows us to reason about `f` by assuming it's inputs are finite. From this perspective `f notFinite` is a well typed expression. There is no need for multiple senses of "belonging". The only difference between `f notFinite` and `f finite` is that one evaluates to a normal form and the other doesn't. The simplification a strict language provides is in the guarantee that `f` is only evaluated on a subset of well typed arguments (namely, those that are in normal form). The rest of the theory for a strict language is more or less the same as a lazy language, there's no additional complexity introduced elsewhere.
Yes, your type is equivalent to mine. That makes the implementation much simpler! The question is now: `X` is not `Coyoneda`, but it's something close. What is its name?
Sorry, that's not quite what I meant. The `OverloadedRecordFields` extension apparently doesn't support fields with higher rank types because GHC doesn't allow higher rank types in type class/family parameters. Your `FieldOwner` class looks very similar to the `Has` class described [here](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Design), so I was wondering if/how you got around the issue.
Amazing! I would be very happy if this is what Haskell records were. I don't see ADTs in the demo even though it seemed implied that this has the functionality: what is the status with that (can you add it to the demo)? 
I agree, TH is a burden, but if this idea is good enough I hope it gets merged into GHC 7.12 or something :)
&gt; It does not obligate the user to predeclare field names and/or import them before being able to use them. `vinyl` does not require that. Fields have always been identified using type-level strings. Since 0.4 named fields have been factored out of the core record type. There's some jiggering necessary to stuff type-level strings into value-level expressions; e.g. the old pre-0.4 notion of `fieldX = Field :: "X" ::: String`. It's not really predeclared or necessarily imported. &gt; * It is as lightweight as tuples or ADTs are in terms of memory consumption, because it doesn't store constructors per each field. &gt; * It introduces no performance overhead compared to normal ADTs, because it does not require to traverse the HList datastructure to reach the field. As regards `vinyl`, both of these boil down to the difference between `(x, y, z)` and `(x, (y, z))`. There's a difference, but it's not that significant, particularly if we expect records to have few fields. (I'm not sure exactly what the runtime representation of `Rec` looks like. It is equivalent to a strict pair, and could be implemented as such using type families--I did it a while ago, but there are some downsides--but it's actually implemented with GADTs, which might lead to some superfluous baggage.) If I understand the current version of `vinyl` correctly, with optimization on the lenses in [`RElem`](http://hackage.haskell.org/package/vinyl-0.5.1/docs/Data-Vinyl-Lens.html) have performance characteristics equivalent to nested tuples. It has to "traverse the datastructure", yes, but that's just `n` case analyses. If you're using a record with 24 fields--`record`'s maximum--that really shouldn't be significant. My bigger concern with `vinyl` has always been that it's built on GADTs, and I don't really understand whether that gets in the way of other optimizations.
I second this. it seems very promising
&gt; P.S. It's 5:30 am here, so there might be some mistakes and typos. Thanks for all the material. We're on opposite sides of the globe; my day is winding down. I'll look over this tomorrow and let you know how it lines up with my thinking. I will say that `record` is really not a good name, exactly as you point out--you'd think `record` and `replay` would have much closer types than they do.
It's a shame that you had to reimplement Haskell syntax parsing tho, taking away a bit of its elegance. However, the first thing I'm going to do when i get home is give this project a spin! This solves a huge pain I had (and i reckon a lot of people have), and now I'm finally able to un-split my Types.hs file!
Yes but if you're using lenses you've already accepted your fate. I would love to see both things become part of the standard.
It's [MLK](http://www.opm.gov/policy-data-oversight/snow-dismissal-procedures/federal-holidays/#url=2015). (I like that OPM has that under "Snow &amp; Dismissal.")
This is huge! Why aren't there any posts about this on the mailing lists? 
I think OverloadedRecordFields doesn't elevate records to the type level.
Good idea! I'll cover this right now.
1. How does this compare to Vinyl? 2. How does this compare to OverloadedRecordFields? 3. How about a version of this for Variants?
Mathematics is a *huge* subject. So whilst learning Haskell might help you better understand some areas of maths (e.g. category theory), it won't necessarily *inherently* help you in other areas (e.g. PDEs, statistics) outside of utilising/developing Haskell libraries for those areas. i would say that learning Haskell might help you better understand programming and computing in general, in areas such as denotational semantics vs. operational semantics, algorithmic complexity (e.g. time vs. space tradeoffs), and types-as-hardware-constraints vs. types-as-abstractions in the type theory sense.
&gt;I really wish I had learned Lambda Calculus and Category Theory before I had learned Calculus Calculus :) Interesting, why is that?
yeah the wiki is kinda lacking
Confirmed, I don't get it.
I think you might have reinvented OCaml, but I'm not 100% sure.
Does this mandate fully qualified record names, or can you still just pass bare arguments to the constructor?
[Enjoy.](http://en.wikibooks.org/wiki/Haskell/Category_theory).
Well that's an overstatement I think. Math is a huge subject and a big chunk is really about the Reals as such and another big chunk is combinatorial in outlook and is about counting things in a sense, and another chunk is topological in outlook and is about equating and relating things in a sense, etc. All these pieces fit together in surprising and mysterious ways. We each carve out different chunks of the mathematical world that suit us most, and find enough there for our needs, and often we can attack the same problem with tools from disparate domains.
Indeed, the issue with `RankNTypes` is not specific to `OverloadedRecordFields` and will also apply to this extension (and any extension that represents record field polymorphism using constraints like this). GHC's type inference algorithm for higher-rank functions depends on knowing the full type of the function immediately (by looking it up in the context, not by solving constraints).
Right, ORF works with existing Haskell record datatypes and selectors rather than adding anonymous record types and new syntax for field access. There's something to be said for adding them though...
I think it would be very helpful if you wrote down the type of a lens (e.g. `[l|birthday|]`).
Also it's fst and snd, not "zero" and fst
Nice work! As a bit of a newbie, I wonder what the advantage is over `OverloadedRecordFields` (resp. what the disadvantage is of the latter). I am not fond of littering the code with quasi-quoters, is there a way to avoid this? If this was an extension, could one provide a native syntax?
It seems that `OverloadedRecordFields` introduced a new constraint of the form `r { x :: t }` which Nikita has encoded as `FieldOwner "x" t r`. The `OverloadedRecordsField` design doc says that `r { x :: t }` is just sugar for `Has r "x" t`, so this part of the interface seems similar. It would be interesting if Nikita could write down a comparison. https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Design
You can see GHC release dates here: https://www.haskell.org/ghc/
`zth` and `fst`? ;) edit: as a native german speaker, even thinking about attempting to pronounce `zth` is horrifying.
&gt; define lenses for all the fields Surely you need one lens that gets you inside the newtype and then you just use the lenses provided by `record`?
&gt; Also it's fst and snd, not "zero" and fst If we start with index 0, the elements at index 0 and 1 are still the first and second elements, respectively, so `fst` and `snd` still make sense.
&gt; It introduces no performance overhead compared to normal ADTs, because it does not require to traverse the HList datastructure to reach the field. Does it also allow for controlling the strictness of fields and unpacking their contents?
Yes, if I'm willing to reveal that much about the implementation. But if I want it to be abstract, I guess this is no worse than the current records.
Nice experiment! Some feedback: * While compiler error messages are comprehensible, I wouldn't say that they are good. Compare: Could not deduce (Record.Types.FieldOwner "ageasdf" Int (Record.Types.Record2 "age" Int "name" String)) arising from a use of ‘Record.Types.lens’ ... to something like this Not in scope: 'ageasdf' * You always get overloaded functions, even when you don't want it: it's no longer possible to say "this function takes a Foo" (unless you want to fall back on the old record system), you must always say "this function take a record with fields X, Y, and Z". Ideally should be able to smoothly move between non-overloaded and overloaded functions of record types. * As mentioned elsewhere, having to use TH is annoying, but perhaps this could be solved by compiler support. * Does the representation still allow for strictness control and unpacking?
In TH tuples are just lists... you just drop its constructor
&gt; Where are haskell/functional programming/new language research being published to? ICFP is probably the most relevant conference - just going through the archives is treasure trove of information. &gt; Where do you guys find these papers and articles? I would like to know about any resource where I can be exposed to great material. For the most part, word of mouth, combined with a insatiable desire to know more - the latter results in active work on my half doing searches for related terms, following paper references, asking focussed questions and getting *more* papers to read, etc. Another good hub is the Haskell wiki article on [research papers](https://www.haskell.org/haskellwiki/Research_papers), if you can get the wiki to load for you... Edit: Just found [dohaskell](http://www.dohaskell.com/) - wow! &gt; Feel free to recommend me older papers on the topics also. I haven't read any about this yet, so most anything should be knew to me. The "functional pearls" series is worth its weight in gold - there is some verrry interesting stuff there! Those fancy academic paper databases might be useful here, because if you don't know what you want to read, you could at least choose the pearls that have the most citations. If you want to just fill your hard-drive with papers, you might be interested in my [`papers`](http://github.com/ocharles/papers) repository. In just a few commands, you'll have several hundred papers to dig through :)
The definition notFinite = ListInt 1 notFinite is not valid in a language like SML. You have to mark a definition as being recursive to be able to use recursive bindings, and only (syntactic) functions can be recursive (if memory serves). So your example is not a valid SML fragment. 
If I define type Teacher = [r| name :: Text, language :: Lang |] type Student = [r| name :: Text, language :: Lang |] how can the compiler detect the type error if I use a `Student` where a `Teacher` is required?
Think of iit as pronouncing `zsh` with a lisp :)
Thanks for replying! Awesome! I will be looking through the resources you hooked me up to and figuring out where to start. It'll be awesome, I'm sure! About IFCP - what is that? Google doesn't give me any relevant results. Can you hook me up?
I believe it's a typo and s/he meant the ICFP - International Conference on Functional Programming. That should yield better search results. :)
Yes, `[r| name :: Text, language :: Lang |]` really *is* a pure record type whose identity is determined by its field names and field types.
Yes, ICFP - oops :)
As far as I can tell, something like [r| {name :: String, language :: String} |] just means Record2 "language" String "name" String and the definition of `Record2` is data Record2 n1 v1 n2 v2 = Record2 v1 v2 By the `FieldOwner` typeclass we get lenses [l| name |] :: Lens (Record2 "language" String "name" String) String [l| language |] :: Lens (Record2 "language" String "name" String) String (`Lens` is equivalent to `Control.Lens.SimpleLens`) Thus we see that type Country = [r| {name :: String, language :: String} |] means type Country = Record2 "language" String "name" String and we can use the lenses to access it. [EDIT: Corrected field order, thanks to /u/Jameshfisher]
The way to think about `[r|{ name :: String }|]` is that it really *is* a record type with exactly one field called `"name"` whose value is of type `String`. `Complex` doesn't satisfy that condition so you need to use a `FieldOwner` constraint as Nikita says. 
When I did my degree in maths I found some things came up and were very handy having already learned them through Haskell but otherwise there wasn't much I could bring to the table. On the flip side, having learnt maths improved my Haskell a lot. And now I'm more of an Idris fan. Ymmv
&gt; It is if `Foo` is a newtype wrapping the record type. Doesn't that break the overloaded accessors, then?
It seems you introduced AddExp and MulExp. In the post, you only have functions taking AddAlg and MulAlg (the expressions), which are in the "right" subtype relation. In many languages with function types (at least Scala, maybe Java 8), the function types are in the right subtyping relation without you declaring anything. That is, functions from AddAlg&lt;T&gt; to T are a subtype of functions from MulAlg&lt;T&gt; to T — it sounds backwards, but it's correct (and is just as backwards as what you explain; this is called "contravariance"). &gt; You want to add a case to an existing datatype? Just make a new product type that includes the old one as a case! That doesn't really work, because your existing code base won't accept your new datatype — you can map from old to new, not viceversa. And the conversion function is no different — it only allows code for the extension to use values of the base type In fact, it seems you aren't really supposed to write standard Java functions like that, because they aren't extensible. Either you add those functions in the same way as eval and view, or you give them a more parametric type (I won't try reproducing it on the spot).
Awesome! I'm looking forward to seeing them.
I like programming in Haskell, but I see a point to the objection, and I'm not sure how to counter it. From what I gathered, the difference is not about what's well-typed, but about correctness (and correctness of which code). In ML, evaluation of `f notFinite` will first evaluate `notFinite` and only then pass the resulting *value* to `f`. Accordingly, ML programmers declare `f` correct if it gives the wanted output whenever it gets a valid input *value*; so, to prove correctness of `f`, you only need to deal with bottomless inputs. In Haskell, instead, `f` can get inputs containing bottom, either at the top or anywhere, so each time you look at a value you have one more case to deal with in the induction. ML people think you have to consider this in your correctness proof, and dislike that. Haskell people disagree. Who's right? Some functions that are only supposed to work on bottomless inputs, and for those it seems we can ignore the complication (in fact, things are trickier). But there are functions that *are* supposed to work on inputs containing bottoms (and to be fair, that includes many list functions)! Worse, in circular programming sometimes a function terminates iff it does not force part of the input, since to evaluate that input in the first place, that function call must terminate. There you need to be careful. What's worse, since Haskell is lazy by default (I know there are benefits), there is no separation between types with and without bottoms (while languages with some form of that separation do exist) — so you need to move that info from types to docs, which does not seem very Haskell-ish. I don't think that's such an indictment of Haskell, but I think it's an interesting objection. Luckily, there are people using linear logic to better deal with effects (including nontermination).
Please reply with a link to your PR when it's done! 
Thanks for giving me a hearty laugh first thing in the morning. :)
Frankly with that math background you don't have enough for even procedural programming, let alone functional...
The first of these can be solved by a very simple type checker hack that allows custom error messages. I've got a ghc trac ticket for it.
Yes, recursive values don't "make sense", but you can circumvent that. (Tested in the SML/NJ REPL). As expected, the last line won't terminate. ``` datatype listInt = Nil | ListInt of int * listInt; fun notFiniteF() = ListInt (1, notFiniteF()); val notFinite = notFiniteF(); ```
But that's something entirely different (I have my Bob Harper hat on now). You've made a partial function of type `() -&gt; listInt`. The partiality is not in the `listInt` value, it's in the function. If the function returns, it will give you a total `listInt` value.
Actually I think a problem like that will be easily solved with a TH macro, which will derive appropriate `FieldOwner` instances for the newtype wrapper. Pinging /u/spindakin /u/tibbe.
http://www.reddit.com/r/haskell/comments/2svayz/i_think_ive_nailed_it_ive_solved_the_records/cntxd4d
That sounds like it would solve it perfectly!
The primary difference from my point of view is that unlike in `OverloadedRecordFields` in "record" the records are both first class values and first class types. I find it much more flexible, since it opens up a lot of features, which we previously didn't even think of, like, for instance, named parameters for a function: connect :: [r| {host :: ByteString, port :: Int, user :: ByteStrin, password :: ByteString} ] -&gt; IO Connection instead of the opaque connect :: ByteString -&gt; Int -&gt; ByteString -&gt; ByteString -&gt; IO Connection At the same time you can always restrict the first class record type to something in the spirit of `OverloadedRecordFields` simply by wrapping it in `newtype`. Here is a nice perspective of the implementor of `OverloadedRecordFields` on the matter: http://www.reddit.com/r/haskell/comments/2svayz/i_think_ive_nailed_it_ive_solved_the_records/cntphlb 
What do you mean by this?
Thanks for the support!
On the other hand, Rust and (I believe) Swift use base 0.
I love you haskell people!
Yes, I was thinking of adding another implementation (namely the BWBoard type, which reflects levels &gt;50). Though in the fromLevel function in LevelParser.hs I'm having a problem with the types. I'm trying to return a GameState with a generic type var bound by the Board class, but it's not letting me. Any ideas? Edit: After reading [this](http://www.reddit.com/r/haskell/comments/1j0awq/definitive_guide_on_when_to_use_typeclasses/) thread on when to use type-classes vs additional ADT constructors. I was convinced by one of the answers, removed the Board class and refactored the extra BWBoard to be a new constructor in Board.
&gt; You always get overloaded functions, even when you don't want it: it's no longer possible to say "this function takes a Foo" (unless you want to fall back on the old record system), you must always say "this function take a record with fields X, Y, and Z". This is an interesting point. I agree with your general point that sometimes you want non-polymorphic functions. But the non-polymorphic functions that are currently auto-generated actually can be a significant obstacle to achieving backwards-compatibility. I discussed this in a [blog post](http://softwaresimply.blogspot.com/2014/08/field-accessors-considered-harmful.html) awhile back. I think that post, your recent post on too much CPP, and this new record post are very related. Anyway, it may actually not be a bad thing that these functions are always polymorphic. If you want non-polymorphic ones, maybe you should write them yourself.
Is this a common convention? It makes sense the more I look at it, but I feel like if I came across this version IRL I would just undo the infix notation in my head.
http://lambda-the-ultimate.org/ discusses a lot of papers at length. It's hard work following a lot of what goes on there though, I find that keeping an eye on /r/haskell still teaches me plenty. Also /r/dependent_types/ and /r/Idris etc
Good point! I'm so infected by the Haskell way of thought I forget about such restrictions :-) For the point I was trying to make, about the embedability of strict dsls in a non-strict language, I _think_ any other example would suffice nonetheless. For example /u/Blaisorblade's `notFiniteF()` is nonetheless a well-typed legitimate expression in a strict language. We still need a semantics to consider if it maps to a value or not.
After reading the thread on the mailing list on type synonyms, it occurred to me that if you are willing to give up random argument order in exchange for currying, you could even do: connect :: { host :: ByteString } -&gt; { port :: Int } -&gt; { user :: ByteString } -&gt; { password :: ByteString } -&gt; IO Connection to achieve a more typesafe version of: type (:-) a b = b connect :: (host :- ByteString) -&gt; (port :- Int) -&gt; (user :- ByteString) -&gt; (password :- ByteString) -&gt; IO Connection Assuming some extension to avoid needing the quasiquoter, and that Record1 is a newtype. EDIT: On second thought actually applying such a function would probably be pretty tedious, needing to wrap each argument in a singleton record.
If the accessors are defined in terms of the FieldOwner typeclass, and you're just newtyping an anonymous record like `newtype Foo = Foo [r| { x :: Int , y :: Int } ]`, couldn't GeneralizedNewtypeDeriving already accomplish that? Or maybe it doesn't play well with multi-parameter typeclasses?
I also thought it was a done deal and was eagerly awaiting the release. This is a must have feature for me and the software that I write, so if there are additional ways to help show that there's demand for the changes, please let us know.
It would be nice to see it with a native syntax, but I think this is really a fantastic way to test out this kind of feature -- it's much easier for the average Haskeller to try out a new library than a GHC fork, and that experience can only make a possible future language extension even better.
That's very interesting; I didn't notice that `MFlow` was built on a generalized package. I think that would be very useful in many situations. &gt; it uses a more conventional state monad. But it requires `MonadIO` from the underlying monad, because of the side-effect. I do like the name `step`; it's more expressive of what's going on.
How would this work with Applicative notation, like parsePerson = Person &lt;$&gt; parseName &lt;*&gt; parseAge Would we need to use `do` notation instead (together with `ApplicativeDo` if no `Monad` instance is available)? Or should the compiler automatically generate curried multi-argument constructors whenever a record is `newtype`-ed?
Actually it can, but you'll need to derive an instance for each field: newtype A = A [r| {x :: Int, y :: Int} |] deriving (FieldOwner "x" Int, FieldOwner "y" Int) To my taste it's too much boilerplate and I'd rather have it automated with a macro. It's actually quite trivial, so I think I'll implement it soon.
Good question. For now you can solve this by explicitly referring to the record constructor of the appropriate arity: Record.Types.Record2 &lt;$&gt; parseName &lt;*&gt; parseAge If it's `newtype`-wrapped, then Person &lt;$&gt; (Record.Types.Record2 &lt;$&gt; parseName &lt;*&gt; parseAge) However it should be explored how a better transparency could be achieved. EDIT: Of course, you'll have to be careful to reorder the fields alphabetically: Record.Types.Record2 &lt;$&gt; parseAge &lt;*&gt; parseName
not overly. It makes more sense if you're used to the infix use of `fmap` (or `&lt;$&gt;`) I think, since `mapM_` is just another specialized form of application.
&gt; There's something off about record/replay: the record function uses f a (which is kind of action, like in record ask) while the replay function uses a list of x (which are values, supposedly returned by those actions). Isn't this related to the "free"-ness of the monad, though? `FreeT` raises a functor to a monad (which is just a monoid in the category of endofunctors, meme meme), and `[]` is the free monoid, so moving `f x` to `f [x]` makes sense.
https://www.google.com/search?q=haskell+filetype:pdf&amp;tbs=qdr:y Looks for PDF files mentioning haskell for the past year. If you use Firefox you can make a so called smart keyword for this, see: http://johnbokma.com/firefox/keymarks-explained.html
Slightly off-topic, but including this ticket a little bit... every interaction I've seen involving Simon Peyton Jones and the community, whether it be mails in the mailing list, interviews, or just comments like this one, makes me think that he's just a wonderfully kind old guy that just found something that he loves doing. Because most of his personality seems like he does brilliant work mixed in with fun, almost whimsical goofing around.
I think I should revise my ad hoc statement to say that in most of the exercises in high school math the student is just performing reduction. Having an understanding of what is a function and what are variables in say an expression in calculus is a lot easier if you can desugar it to a term applied to a term. But I'm a programmer so maybe I'm biased.
Mmm, well the other programming languages ones I follow are /r/MathTheory /r/agda /r/functionalprogramming /r/lisp /r/ProgrammingLanguages /r/elm /r/purescript /r/rust /r/scheme /r/DataflowProgramming /r/types Most of them are a bit on the slow side to be honest. This subreddit is extremely active.
Huh?
@jonsterling author of vinyl is down below in the comments. https://www.reddit.com/r/haskell/comments/2svayz/i_think_ive_nailed_it_ive_solved_the_records/cntlkbb
Considering `RecordWildCards` it would make sense to do: do name &lt;- parseName age &lt;- parseAge return $ Person [r| {..} |] I think for something better you would need something like idiom brackets for records, so you could do `Person &lt;$&gt; {| name = parseName , age = parseAge |}`. I dunno if that would really add anything though; frankly, aside from the superfluous `return` they read almost identical: do { name &lt;- parseName ; age &lt;- parseAge ; return $ Person {..} } Person &lt;$&gt; {| name = parseName , age = parseAge |} EDIT: Even without the newtyping use-case, a section-ing syntax could be handy in general (and for applicative use), like: { a = , b = } :: t -&gt; t1 -&gt; { a :: t , b :: t1 } { a = , b = } = \x y -&gt; { a = x , b = y } It would make the argument order easier to control (`{ b = , a = }` would have the arguments in the opposite order), especially when the actual data constructor order is non-obvious.
Well that gets us closer, but lots of high school math is rewriting too! And there we start needing to ask bigger questions...
Out of curiosity, how did you get those numbers? I don't know very much about profiling in Haskell.
I did it using "printf debugging" via Debug.Trace. Here's the code: http://lpaste.net/118793
This is now the all-time top post of r/haskell
Thank you!
Congrats! This is the [top /r/haskell post of all time](https://www.reddit.com/r/haskell/top/?sort=top&amp;t=all)!
Wow ) Thank you.
May it reign until we storm the gates of cabal hell.
You can put minimum base version to 4.6 There are small things like [this](https://github.com/fanjam/paypal-adaptive-hoops/blob/master/src/Web/PayPal/Adaptive/Core.hs#L397) where you don't need the function toUSD, just mappend (USD c1) (USD c2) = USD $ c1 + c2 Also that instance doesn't violate the monoid laws, I'd remove that comment :) I'm sure someone else will be able to give you a simpler variant for m2PayPal but I can't suggest one of the top of my head. Since you working with currency and you only have USD, I would have liked to see how you'd handle multiple currencies, or leave ways to specify other currencies. 
One question occured to me while reading the post that I forgot to mention. Why no quasi quoter for pattern matching? E.g. foo [r|{x=Just x,y =[123]}|] = … could be rewritten into view patterns. Is it just because you didn't get round to it? Having to deal with parsing and such? Or is there a deeper issue I don't see yet?
I mean when you write a record data Person = Person {name :: String} `name` is just a function `name :: Person -&gt; String` that is automatically defined for your convenience. [LYAH](http://learnyouahaskell.com/making-our-own-types-and-typeclasses#record-syntax) gives a great explanation in these terms.
Lovely. You may have all my internetz, kind sir :3 Would you like to comment on what happend to the left operand of `=&lt;&lt;` in your thought process? Has it been woo'ed?
The best way is to compile your program with profiling support: $ ghc -O2 -prof -fprof-auto Primes.hs Then when you run it you can turn on profiling $ ./Primes +RTS -p This will save the profiling information in a file [Primes.prof](http://lpaste.net/5404614690112274432)
I've been playing with the idea and came up with some code ([here's a gist](https://gist.github.com/fizruk/ceb0731cde1b59c7f8a8)). The idea is that for each functor `f` (which kind of encodes a "command") we can provide a functor `g` that will encode a "recorded command" (or rather it's "recorded parameters"). The function `replay` uses recorded parameters from `g` to instantiate `f`. While `f` and `g` correspond to single commands/records, `Free f` corresponds to a computation tree and `Free g` corresponds to a computation log (*not necessarily linear!*). The example used in that code snippet uses 3 commands: `ask`, `halt` and `fork`. The last one enables non-linear computation structure. The `test` program demonstrates how such a structure can be recorded/replayed.
Hmm, that doesn't read very well. I changed it to say "defines" instead of "contains." 1,114,112 is the maximum number of characters (abstract characters, not glyphs, to use Wikipedia's terminology) that Unicode *could* define. That is, there are 1,114,112 unique codepoints that Unicode could give meaning to, but they've (so far) only given meaning to 113,021 of them. The section "Fun UTF-8 facts" talks a little more about this.
I think this is what's going on with regards to efficiency: Your version will check for possible factors up to sqrtInt 600851475143 = 775146. Their version doesn't do that. It stops after finding the last factor, so their amount of "mod" uses is only in the thousands. For each factor that fits, they divide the number by that factor and then repeat checking that same factor until it doesn't fit anymore. That's what shortens their search.
Is for me, but the benefits are more substantial when you have to execute the thing you've written.
You could even do something like `{&lt;expr&gt;|a,b} = \x y -&gt; &lt;expr&gt; { a = x , b = y}`, which would be useful for newtypes, as well as functions which take named arguments, though I'm not really sure if that's a good idea or not. At least the fact that the record syntax isn't that expressive gives opportunity to experiment.
Interesting, will definitely give this a spin. I was trying to get Flow working and it was a complete and total pain in the ass and in the end, it just kept crashing. Also, would it be doable to use GHCJS to make this run in the browser? That would be quite nice
1. Yes, haskell is quick to code. But long to get there. You can get to comfortable development speed in other languages much faster than in haskell. The deciding factor for me was not the speed of development. I have more than 25 years of experience in software development. And one thing i learned is that i can adapt to any language and become productive eventually. So this was never a concern for me when i started learning haskell. 2. The most important thing that i personally got from haskell is greatly reduced upkeep efforts (maintenance). Complex system are practically never finished. So it is much more important how difficult it is to maintain and continue to build a growing system than how quickly you can jumpstart a working prototype. And the problem with many languages is that the difficulty of maintaining a growing system raises not linearly but quadratically. In other words twice complex system is 4 times harder to maintain. Haskell helps to control that growing complexity. 
Here is a lovely link providing access to all the preprints of icfp papers from 2014: https://github.com/yallop/icfp2014-papers It provides links to similar efforts for 2013 as well as some other things, including Haskell symposium papers. I actually think reading cutting-edge stuff is only sometimes useful. The papers are good, to be sure, but what you really want is often the material that has been "sifted through" one generation prior -- and you'll find that by seeing which papers get cited in the current crop. Those ones tend to be the ones introducing and explaining the ideas that turn out to be "big" in the sense of useful for others to then pursue. Depending on what grabs your interest specifically, often the place to go is further back and more narrow on particular topics to get a sense of the motivations and general outlook of more recent stuff -- and often you'll find the approaches in the older work more than suitable for your needs. The other thing I do when I find a paper that I think is very good is to go to that academic's webpage, and assess their past publication history, either to trace through a particular research programme of theirs, or on the basis that if I like their approach to one thing, then I will find their work elsewhere equally interesting. This is especially good for finding overlooked gems -- approaches that are very good and useful, but for whatever reason (perhaps, sometimes, because they _just work_) haven't given rise to much in the way of follow-on work. Along with ICFP, and the related language workshops and symposia on Haskell, ML, etc., POPL (Principles of Programming Languages), PADL (Practical Aspects of Declarative Languages), and IFL (Symposia on Implementation and Application of Functional Languages) are good venues to look for interesting work, among others. You can hit their webpages and view their accepted papers for various years and often google up the relevant preprints yourself.
How does this tool plan to support external libraries?
Is that not [`Mealy`](http://hackage.haskell.org/package/machines-0.4.1/docs/Data-Machine-Mealy.html)? (Edit: it *is* the non-recursive form which is nice, though!)
This seems great! Though I can't seem to understand most of it (for example the proofs, or the intuition behind the paper). I think coming up with the specific applications for the optimization theorems would greatly help understand this more!
Hah. Same problem we've always had with "strong lax monoidal (endo)functors".
If anyone has more examples, I would like to see other interesting operators that provide dinatural transformations besides `fix`?
Relatedly, I don't see the advantage in keeping the initial state existentially quantified out but distinct, as opposed to "packing" it into the step function.
Out of curiosity what practical libraries do you see missing?
What do you mean?
So it is! I didn't realize that
Definitely compared to imperative languages. When doing java for example, I have to struggle really hard with getting things done and it almost always ends in despair because the code doesn't get reusable or composable to the level Haskell code would have been. Almost every time you solve a problem in Haskell, you end up writing a bunch of reusable components in the process, without added effort. This I'd say is not the case with other langs. It simplifies testing as well.
I mean that if you write it as Mealy, you don't need to quantify over an `s` but you get a type that is just as good.
My understanding is that when composing the non-existential form GHC has a harder time optimizing. This is just hearsay though.
Your understanding is correct. More generally, GHC has a harder time optimizing recursive data types and recursive functions. The rule of thumb is that each iteration of a recursive function takes 10s of nanoseconds minimum. To break the 10ns barrier you need to start using rewrite rules and shortcut fusion or non-recursive representations.
I agree about the state of libraries being worse than it could be. One should not have no read a paper just to be able to use a library that is supposed to solve a common task. &gt; GUI - libraries are largely terrible especially cross platform (tried: WX, GTK, Gloss and ThreePenny) Could you elaborate on this, please? Is not the situation the same for other programming langauges? 
I think you're focusing too much on how easy it is to write code and not enough on how easy it is to read and maintain code. Functional languages optimize for the latter. I do think that Haskell is just as easy to write as other languages, but the main thing that differentiates Haskell is how easy it is to refactor, evolve, and maintain existing code.
"Combining examples with trial and error" can be dangerous because it can lead to stuff accidentally working; i.e. local maxima.
zsh is pronounced "zee-shell". Maybe think of it like pronouncing "zenith" but being rendered dumb by a hiccup in the middle.
That's a good question. Many libraries use JS dynamic idioms in their APIs. However, due to Google Closure's and TypeScript prevalence, I've found that a lot of libraries' APIs can be described with reasonable types. For the few cases where dynamicity is hard to deal with, one could wrap the problematic API with something more static.
I installed ghc 7.8 on fedora by building it from source. you need the fedora 7.6 ghc and the fedora cabal. then you build ghc from source, then the latest cabal from source. worked for me. another alternative are ghc docker images.
First, I agree that certain dynamic-style idioms are hard to express in a reasonably simple type system. The design space of allowing truely dynamic code is already being covered by Closure and Flow. I'm aiming at programs that are either easy to port to a more restricted form, or haven't been written yet, and the goal is to increase safety. However, I believe we can find a reasonable balance between type system complexity and "dynamic-like" programming idioms. For example, supporting ML-style polymorphism plus row-type polymorphism already solves a lot of the issues that a type checker like Google's Closure can only solve with the magic "any" type. Further, as the Flow people are doing, there is a lot to be gleaned off of data flow analysis, such as type guards (something I plan to implement as well). The goal is not to create another functional language, (or to use one for the web) but to improve the development environment for native JS when that's what you require or want to use. Thanks to pointing at Dialyzer, will take a look. 
It can, but that's not necessarily a bad thing. Not everyone learns the same way. I tend to have a better grasp of the theory once I've played with an implementation. I made use of Applicatives in the form of `Constructor &lt;$&gt; a &lt;*&gt; b &lt;*&gt; c` for a good month before I actually understood what it was doing. It's easy to assume that "I got it working" is the end point of the learning process. But it's just the first step.
How does the type inference compare to JSNice?
Web development: I spent a couple of days doing some work with Snap, and two things I immediately missed were: * DRY database mapper with a migration solution (something like Django's [solution](https://docs.djangoproject.com/en/dev/topics/migrations/)). * A good multi-purpose template library, something like [jinja](http://jinja.pocoo.org/) or [mako](http://www.makotemplates.org/). Heist is cool, but is only HTML/XML. Yesod is also geared towards HTML only. There also isn't a solid authentication library. The auth snaplet is way too inflexible, for example. In general, it seems one has to code much of the low-level stuff oneself, which is "battery included" in other ecosystems. It has gotten a lot better in the recent years, but there is still long way until one can be as productive in Haskell in webdev as with Python. 
I'm not talking about the polymorphic accessor functions. I'm talking about all functions written against any record type! You cannot write a function that only accepts `Foo` records anymore. Everything gets duck-typed. I do think we should program more about interfaces though (~type classes). Just not in the duck-typed kind of way.
**Databases** Have you tried [persistent](https://www.fpcomplete.com/school/starting-with-haskell/libraries-and-frameworks/persistent-db) IIRC, it has migrations and other features you would expect.
Notably, it's also `ArrowChoice`, `ArrowApply`, `Strong`, and a very slow `Monad` :)
I strongly agree. You should think of the tuple indices as projections out of an n-ary product. 0-indexing would imply the existence of a projection out of the empty record, which would not make much sense.
The most obvious benefit to using Haskell is maintenance of your programs. They're easy to change and update. The language supports you. Measuring how long it takes to get a minimum viable product is measuring only the first &lt;10% of the software development process.
I support this effort! While I'd rather not touch JavaScript with a spatula, it exists anyway, so sometimes one must deal with it. The reasons I like this project: * It's plain old predictable and formal HM. ML/OCaml/Haskell programmers will immediately get it. * It doesn't add anything syntactical to JavaScript. Doesn't require additional tooling. You just run the type checker on the code. * Anything that doesn't reasonably fall within proper type checking (no statistical or "any" stuff) is simply excluded. A bold move. Most of the other checkers buckled on that. 
How about "Judge"?
First meet-up is a [pub trip](http://www.meetup.com/York-Haskell/events/219747804/), which is always a safe bet. We're not sure yet what the meeting schedule will be, or what sort of events we'll do (personally, I'd like talks), but that can be figured out at the meeting. As far as I know, there aren't any other programming meet-ups in York, although there are in Leeds, so hopefully we'll get people who travel over there coming to our things.
Row polymorphism is nice! Have you thought about treating all strings [as first-class labels](http://lambda-the-ultimate.org/node/174)? This might enable you to type any sort of dynamic member lookup by string, ie. foo["bar"], giving values like foo a meaningful static type.
Can you provide a command line log of your actions/outputs?
Keep in mind that JS has mutable variables (and record fields and array cells), which add quite a few complications. It's not even enough to do the simple value restriction used by the ML family, because variables in those languages are still immutable - they hold mutable reference cells. So yes, it's based on HM, but but but....
Yeah it was quite problematic even just to get a recent enough version of ocaml running on the box for some reason :)
You probably write fewer bugs but write more syntax errors and syntax errors (in Haskell) might be slower to fixing the equivalent bug (if it wasn't catch by the type system). What I mean is Haskell doesn't fix the bug for you, it just catch some and transform them into syntax error. You still have to understand the error and fix it. And the loop, write -&gt; compile -&gt; fix errors is not necessarily faster than, write -&gt; run/test -&gt; fix bugs. For example, let's say you are trying to add 1 to a string. In haskell you'll get an error like, at compile time No instance for (Num [Char]) arising from a use of ‘+’ In the expression: "a" + 1 In an equation for ‘it’: it = "a" + 1 In Ruby, you'll get TypeError: can't convert Fixnum into String from (irb):1:in `+' from (irb):1 from /Users/mb14/.rvm/rubies/ruby-1.9.3-p194/bin/irb:16:in `&lt;main&gt;' when you run your tests (manual or automatic). I let you decide which error message you'll find easier to understand ... That's for the obvious trivial mistakes that you'll find out anyway (as a bug or a syntax error) quickly because their are part of the feature you are writing right now. Usually people tests somehow, the feature they are writting. Haskell doesn't necessarily helps for that type of errors and when you consider your work done and deliver it, I don't think you have more or less bug in Haskell than in other languages. Then you have the type of bugs which are not convertible to a syntax error. For example you want the min of a list, but get the max, or use `&lt;` instead of `&lt;=` or `&gt;` etc ... These errors are the most common ones (once the previous one have been corrected) and hard to find in Haskell or not. The haskell debugger makes it a real pain to debug. However, the fact that functions are pure , and so easy to test in isolation and test suite like quickcheck make it a breeze make sure that every functions work correctly. Where Haskell shines, however is to find and prevent regressions : bugs that you create by modifying something which is used somewhere else. As I said, you usually test your current features but not the old ones, which can be broken easily. With Haskell, if you change the name or the signatures of one function, you are guaranted, that the compilers will find everywhere where i't used and won't let go until you fixed everything. This is sometime can be annoying, because you have to update codes you are not using now, without being sure that what you are writing works. With other languages, you'll have on people to write tests correctly (and have thought of everything which could go wrong). I prefer to rely on the compiler :-) 
To throw an idea on top of my head, a bot could test whether code really solves the given problem or not. I.e there is an expected output defined in the challenge and you want to write a solution that would pass those unit tests are the type system. It could automate a lot of things and perhaps measure performance of a given solution. For example: &gt; **Problem:** Find out whether a list is a palindrome. A palindrome can be read forward or backward; e.g. (x a m a x). &gt; **Test:** expect True [1,2,4,8,16,8,4,2,1] expect False [1, 2, 3] expect True ['m', 'a', 'd', 'a', 'm', 'i', 'm', 'a', 'd', 'a', 'm'] To make it language agnostic, the bot can compile the program in a given language inside a sandbox. Then just check results with "./isPalidrome tests" So a solution could look like this #HASKELL isPalindrome :: (Eq a) =&gt; [a] -&gt; Bool isPalindrome xs = xs == (reverse xs) main = test isPalidrome #PYTHON def isPalindrome(num): return num == num[::-1] test isPalindrome where test is just a small language specific function that gets automatically imported and just feeds the tests to a given function.
Ah, Cunningham's law. Thanks!
I just did a quick test to see if it works, and it looks like it does: $ git clone git@github.com:sinelaw/sjs.git $ cd sjs $ cabal sandbox init $ cabal install $ cabal install --ghcjs Now native and JS can be compared: $ node .cabal-sandbox/bin/sjs.jsexe/all.js test/valid/record.js $ .cabal-sandbox/bin/sjs test/valid/record.js So far they seem to be giving the same results. Hooking it up to an editor in the browser should be doable in a way similar to the try-purescript example: https://github.com/ghcjs/ghcjs-examples/tree/master/try-purescript
If your main priority is "hitting the ground running" then maybe you should check out OCaml instead of Haskell. Its a very similar language that also features a powerful type system but it trades away the lazyness and purity of Haskell for strict evaluation with side effects, which makes it easier to write imperative-style code. Real World Ocaml is a great book to get things started with.
I just want to add that in a non-trivial application you can't exercise all code paths easily, even with extensive test coverage. One of the benefits of static typing is the completeness of code coverage for the things that static typing does check.
&gt; do you notice an increase in your ability to write haskell programs over those other programs? Depends. Other languages tend to have more (or more well-known to me) libraries for some tasks and that can speed development. For me, the biggest win in Haskell is the reduction in rework and that it is easy to manually refactor. Some of this comes because I write Haskell code differently. I'm more aware of the totality (or partiality) of what I'm writing and so, by the time I've ready to do a first run, the code/changes is/are better; the type system also helps avoid places where I wrote a TODO comment but never came back and TODID. &gt; Can you make a case for me to learn the language? No. I think you'll have to make that case yourself. I learned the language specifically because it was a pure functional language with a written specification that was freely available. I wanted a change from from C/C++/Java/C#/sh I had been writing. Initially I didn't even have something to write, but I used it for the next fun, competitive programming I looked at. I still don't write that much Haskell. I've only got one (languishing) package on Hackage and a few other incomplete projects at Gitorious, and I haven't found a niche for it at work and none of our school assignments are language-flexible, yet. I do enjoy writing in Haskell more than other languages; the syntax and semantics come together to make it a lot easier to abstract many tasks that I just end up repeating (copy-paste-modify) in other languages -- it feels like it gets out of my way better than other languages while providing better static guarantees for the final program. &gt; How long does it take to start to be useful in this language? Initially, I had no project, so I spent a lot of time reading (language specification mostly, but also RWH) and experimenting in the REPL before I ever tried to write my first whole program; it was probably several months. That first program [looks pretty bad](https://github.com/stephen-smith/ai-contest-2010/blob/master/MyBot.hs), but I didn't have that many problems implementing what I wanted; deciding what I wanted -- design -- overshadowed all the time I spend both coding and learning more Haskell.
ugh, a duplicate. what a shame.
This is very interesting and I've been playing around with it - but is there a way to do deep pattern matching using this? Say I have the datatype: class ExpAlg a where num :: Int -&gt; a neg :: a -&gt; a I would like to make an operation `simplify` that will make expressions of the kind `neg (neg x)` to just `x`. Is this possible? Something about the 'tagless' part or the way it looks like a catamorphism using f-algebras makes me thing not?
I have ghc 7.6 installed. 
http://www.fpaste.org/172006/14217721/
http://www.fpaste.org/171969/42176971/ Linking dist/build/cabal/cabal ... Installing executable(s) in /home/mr-fool/.cabal/bin Warning: The directory /home/mr-fool/.cabal/bin is not in the system search path. Installed cabal-install-1.22.0.0 whereis cabal cabal: /usr/bin/cabal /usr/local/bin/cabal [mr-fool@localhost ~]$ 
What other repos do you have loaded? It might clash with the ghc-7.8 repo which provides GHC 7.8.3. 
I can personally attest to many late nights where I shouldn't have been coding, refactored 2000'ish LOC Haskell code bases touching most files, it compiled and just worked.
&gt; DRY database mapper with a migration solution (something like Django's solution[1] ). This is what I was using Persistent for, but I believe Groundhog also does this. &gt; Yesod is also geared towards HTML only. What about Julius, Cassius, and Lucius: http://www.yesodweb.com/book/shakespearean-templates
&gt; The lens documentation wasn't useful but a whole bunch of really useful posts online Checkout the wreq tutorial as it includes practical examples of using lenses: http://www.serpentine.com/wreq/tutorial.html#a-quick-lens-backgrounder &gt; I recently tried to decipher Conduit and Pipes. In the end I decided that it was just easier to implement what I needed from scratch. It always seems this way, but it's a ruse. The generality of Pipes and Conduit as well as the provided functions and idioms should not be underestimated. What tutorials did you go through for Pipes and Conduit? I'm hoping you at least went through Tekmo's very thorough docs on Pipes (http://hackage.haskell.org/package/pipes-4.1.4/docs/Pipes-Tutorial.html). As for Conduit: https://www.fpcomplete.com/search?search=conduit 
 $ where cabal /home/your_user/.cabal/bin/cabal /usr/bin/cabal /bin/cabal /home/your_user/.cabal/bin/cabal If you have multiple run each with the fullpath with --version. Maybe from one path it's latest while another is not. Also you don't install ghc with cabal, you either build from the source you download or from the custom repo. Seeing you're trying a custom repo I suggest you `yum remove` from your local machine ghc, haskell-platform and do an autoremove; and after that install from the custom repo
The Haskell error is way better in this scenario, at least it shows me the expression and it makes it blatantly obvious. For ruby, I assume you get only a line number. However I'm not saying haskell can't have bad error messages, like 'Unexpected semi-colons in conditional' when you're not careful with the `if then else` indentation 
Nice work. How do you deal with conditionals? if(obj.a) { return obj.b; } else return obj.c; } I presume you insist the object must have all of the `a`, `b`, and `c` fields? I wonder if this is restraining in real code. What about arrays? `for .. in` iteration? Implicit conversion? Prototypes? Any formal discussion (paper?) of the type system planned?
yum remove ghc output http://www.fpaste.org/172026/17731651/ sudo yum autoremove output http://www.fpaste.org/172027/14217731/ sudo yum-config-manager --add-repo https://copr.fedoraproject.org/coprs/petersen/ghc-7.8.4/repo/fedora-21/petersen-ghc-7.8.4-fedora-21.repo output Loaded plugins: fastestmirror, langpacks adding repo from: https://copr.fedoraproject.org/coprs/petersen/ghc-7.8.4/repo/fedora-21/petersen-ghc-7.8.4-fedora-21.repo grabbing file https://copr.fedoraproject.org/coprs/petersen/ghc-7.8.4/repo/fedora-21/petersen-ghc-7.8.4-fedora-21.repo to /etc/yum.repos.d/petersen-ghc-7.8.4-fedora-21.repo repo saved to /etc/yum.repos.d/petersen-ghc-7.8.4-fedora-21.repo sudo yum install ghc-7.8.4 And then I could not figure out how to pipe the entire output of it but it is basically complaining my dependency
What output do you get when running `yum install ghc-7.8.4` or whatever the package is called?
got it please refer to my op post
Cool! Looks good. Very minor comment though – [here](https://github.com/fanjam/paypal-adaptive-hoops/blob/master/src/Web/PayPal/Adaptive/Core.hs#L91) you note "Text is used for error messages". This is a good example of where `type` is appropriate I think, I would have skipped that comment and written type ErrorMsg = Text and later used `ErrorMsg` in place of `Text`. This makes it clearer and also easy to change, and you would not be confused as to whether `_rlEmail` is an error message type or just a text. Alternatively, go all the way and write newtype ErrorMsg = ErrorMsg Text deriving (Show, IsString) using `GeneralizedNewtypeDeriving` or something.
I love this series. Please keep on going !!
Which is what I say about "regressions" (in the last paragraph).
Ehh... not woo'ed yet :) Soon, though. Here's more explicit comparison: `($) :: (a -&gt; b) -&gt; a -&gt; b` `(&lt;$&gt;) :: (a -&gt; b) -&gt; F a -&gt; F b` `(=&lt;&lt;) :: (a -&gt; F b) -&gt; F a -&gt; F b` Bind gives us a non-linear `fmap`, imo ;) *EDIT:* Erm... non-linear meaning the function you map can go from normal land (green) to a context land (red). This is just another look at join though, which takes a doublly-wrapped context-land and merges it into one.
Have you seen [this paper](http://code.haskell.org/~dons/papers/icfp088-coutts.pdf)? It's similar to this, but with extra fluff to allow eliminating even more recursions (e.g: in a filter-scan) and with REWRITE rules to eliminate redundant conversions from lists to streams back and forth.
The `Stream` type there is different in that it just "steps" over itself. This captures one notion of a "transducer" or whatever -- a morphism `Stream a -&gt; Stream b` such that a fixed sequence in the input yields a fixed sequence in the output. We can actually capture such notions of "continuity" by a Baire topology! http://arxiv.org/abs/0905.4813
`InputT` instantiates MonadIO m =&gt; MonadIO (InputT m) which means that it supports the `MonadIO` typeclass when its `m` argument does. `MonadIO` looks like class MonadIO m where liftIO :: IO a -&gt; m a which essentially allows you to embed `IO` into the monad by "converting" it to the right type. Obviously `IO` instantiates `MonadIO` instance MonadIO IO where liftIO = id so `InputT IO` does as well. loop :: InputT IO () loop = do line &lt;- getInputLine "&gt; " case line of Nothing -&gt; return () Just input -&gt; do liftIO (putStrLn "asd") x &lt;- liftIO (runExceptT $ foo input) case x of Left err -&gt; outputStrLn err Right res -&gt; outputStrLn (show res) loop
Ok, I tried something like this, as you can see from: `return $ putStrLn "asd"`. But this is not displayed on the stdout...
`return` and `liftIO` are different operations. In fact, for any monad, `InputT`, `IO`, `Maybe`, `State`, etc, the following fragments are identical do ...{first}... return x ...{second}... do ...{first}... ...{second}... In other words, `return`ing something in the middle of a `do` block without capturing the value using a left arrow `(&lt;-)` is the same as merely discarding it. That's why you don't see that action being performed. On the other hand, `liftIO (putStrLn "asd")` injects the `IO` operation into the `InputT` operation in such a way that the `IO` bits are executed as desired.
Thanks! I didn't know about r/types and r/maththeory so that is great!
&gt; Furthermore, this is assuming it takes zero time to create the test suite, and we all know that's not true I find that, when I'm writing in an ecosystem I'm comfortable with, the test-writing overhead is _very_ low, mostly because I rarely write code without a REPL open where I can experiment with it, and writing tests tends to replace a large part of the REPL interaction.
In `(Category(..))` example you have to consider the members (but not `Category` itself) as imported implicitly. Otherwise adding a member to Category which conflicts with an implicitly imported one will actually change which one is imported without a compilation error! So you still have two levels, though a bit more complicated.
&gt; Do you know of any success stories or case studies [Here's one](https://www.youtube.com/watch?v=ZR3Jirqk6W8) from /u/bos. It doesn't have a whole lot of examples, but ~~it was a successful transition from another language to Haskell that~~ it illustrates some of Haskell's advantages. EDIT: I was actually thinking of the skedge.me talk, which is already linked [elsewhere](http://www.reddit.com/r/haskell/comments/2t0709/speed_of_writing_programs_in_haskell_vs_language/cnun2lg) in the thread.
Try: PATH=/home/mr-fool/.cabal/bin:$PATH cabal install cabal-install
 good migration system has support for upgrading and downgrading, generating those automatically for adding/deleting fields, with the option of providing custom migration code for more complex migrations, and more. Good point, I only just started using Haskell for web development. It's been a while since I used what was South that you refer to as Django's migration solution. I agree that it's a hole that needs to be filled. I've noticed wanting it a couple of times in my side webdev projects and will try to keep it on top of my mind when I start new projects.
Does the UoY teach functional programming? When I spoke to an admission tutor, last year, the department swapped from teaching Scheme to Python.
You can only do Show, Eq, etc. on tuples up to size 15 and virtually nobody complains.
Haskelline's `InputT` unfortunately is missing instances for many standard classes, and currently does not export enough of the internals (under the rallying cry of "encapsulation!") for you to write them yourself. =( That said, this one is there. `InputT m` has a `MonadIO` instance as long as `m` has a `MonadIO` instance, which lets you use `liftIO`.
&gt;I see easier maintenance mentioned a lot, but I can't quite picture it It's hard to explain. Once you really grok Haskell's type system and corresponding error messages you begin to think of your programs like jigsaw puzzles. The compiler (along with tools like Hoogle and ghc's TypedHoles extension) help you find and sort through all the pieces that might or might not fit in a particular location. This is a *tremendous aid* to maintenance and completely unavailable in dynamic languages (to my knowledge).
Here's another nice repo https://github.com/steshaw/plt-study
I haven't spent the time to get that where I want it yet. Right now it's just a monadic fold transformer with early exit. It's been a little messy to get the desired parallelism the way I wanted it.
It really depends on your definition of real-world. Maybe you could reimplement the last project you did in haskell ? Otherwise, you could code a small project to fetch numbers in parallel from random.org api, and collect some statistics. This would introduce you to different libraries. Also, read this : [What I wish I knew when learning Haskell](http://www.stephendiehl.com/what) It goes through many pitfalls and idioms. 
There are a few places where it's used, the compilers part of the *Systems Software and Compilers* module uses Haskell, and part of (or maybe most of now? it seems to have changed) the *Principles of Programming Languages* module uses Scheme. But Java is used for pretty much everything else, which is a little unfortunate.
The first "big" project I did in Haskell, which really helped me to make the jump from small exercises to whole programs was a website. Haskell has a number of web libraries, from the really minimal (eg, Scotty) to the big and complex (eg, Yesod), so you should be able to find one to suit your style.
I personally love playing around with Gloss, and I think it could be a nice way to make fun little self-contained applications. Check these [examples](https://www.youtube.com/results?search_query=haskell+gloss) out.
I'll come along. The was a York Geek night for a while. Not sure if that's still going, but it was developer based
What would be the point of having it then?
We should have a functional programming course again soon. Prof. Runciman (who used to teach it) had to put the course on hold while he served as Deputy Head of Department. He is now back to his normal role and would like to start teaching it again. Sitting in on his FP course is the reason I'm doing a PhD. If you ever get the chance to take his FP course, do it.
I meant, would that be the sort of derived instance you'd want for a newtype wrapper? I was thinking of trying to code up one in TH.
I got it already please see the op post
Yeah I get your intent, but the sole purpose of the `newtype` wrapper is to provide an identity to the wrapped type. According to your `FieldOwner` instance, there is a mistake in your declaration of `Person`. It should be not ``` newtype Person = Person {- some record -} ``` but ``` newtype Person a = Person a ``` Which renders it to be just a pointless wrapper. 
&gt; and completely unavailable in dynamic languages (to my knowledge). The dynamic equivalent of something like Haskell's type system would be higher order runtime contracts. A good contract system will let you enforce abstractions (like you can do in Haskell) instead of letting errors through and only manifest later, when you try to call a method on null or something like that.
For typed holes the closest I can think of is how in Smalltalk you can run an incomplete program inside the debugger and pause it at your "hole" and write code while being able to inspect the value of everything in scope. As for Hoogle, its a though thing to write when you don't have a standard type system that every library uses or if your type system is simpler (hoogle would be much less useful if Haskell didn' thave parametric polymorphism). tbh, Hoogle is also on the top of my list when people ask me why I like Haskell :)
Say I wanted write arduino code in haskell. One way is to develop an EDSL, which produces 'C' code that can then be passed to the arduino 'C' compiler. So is this what Eaton Corp did, if I am to understand you correctly?
Ahhh, that's pretty cool. Amazing how many great things Alan Kay and co. came up with back in the day!
Ah, right, I typed it into reddit without checking - oops. I did indeed mean the former version of person without the type parameter. I agree completely that the other is pointless
Yes, the clarity is great! I do gloss over the C++ parts but it's good to have them because they illustrate how things are so much cleaner in Haskell.
It's *really* slow. It has to accumulate values from the entire history of the scan, e.g. fold over the entire lower diagonal. You can see the impl in the link to Mealy above.
I swear this is a month late for a good reason and not because I forgot about it. Edit: I'm thinking about moving from jekyll to hakyll to generate the site, anyone know of any resources for speeding the transition?
ah okay i see, thx.
Do you have any distilled down examples that are simple enough they might turn into a good bug ticket for ghc?
What we want is a failure at _typecheck_ time. I don't think TH is sufficient in this regard. I imagine we'd actually need to write a framework using the GHC API for tests like this (or just using the `ghc` binary itself to drive compilation of files :-) ). They have tests like this in the `ghc` test suite itself. I don't know of any general purpose frameworks for this though. Neat idea!
Its just a very special case of it!
Excellent post. Just the other day I was thinking that this kind of information should be compiled somewhere. Some kind of "Tips from the Trenches" set of best practices. People who use Haskell industrially certainly have a decent amount of experiential knowledge and it doesn't seem like we have a good way of communicating this to newcomers. Another one that comes to mind immediately is a Types module. For small applications this can just be Foo/Types.hs. But for larger projects you eventually need Foo/Types/ and each type needs its own standalone module inside the Types directory. Then you also need to be quite aggressive about keeping code out of those modules. Stick to just the data type, smart constructors if necessary, accessors, lenses, and type class instances. Experience has shown me that if you don't do this, you will eventually run into cyclic dependency issues. But the bigger benefit is that you always know exactly where to go to find certain things. In fact, the talk I gave a few months ago at the NY Haskell Meetup probably falls into this design pattern category. Unfortunately we had audio problems, so were not able to post the video. For those interested, the repo for my slides is [here](https://github.com/mightybyte/modules-and-types).
I don't understand why it is a value in Haskell and not in ML. You can't work with them like values in Haskell either, the best you can do is not evaluate them. Perhaps the distinction intended to be drawn is that things like (5,⊥) can be treated as a kind of value, but it still seems better to call it a "non-terminating computation" than a value. You can't serialize that or put make your tuple unboxed.
I would say that if one's going to go for an analogy, "producer" is a better one here than "container." 
If you are willing to admit the existence of partial values, why not apply it to an applied partial function, too? The difference is that Haskell functions translate unevaluated values to unevaluated values, they don't impose the side effect of evaluating anything. When you do your `let foo = bar` you are invoking a kind of bind function on the right hand side that is strict in the entire value. In Haskell, it is fully lazy. However you could override the meaning of = by putting things in a strict context like StrictIdentity.
Great idea. Will try it. I have noticed many libraries have some of their cleverest code in the 'Util' module. Good idea to get rid of those modules. 
&gt; Control.Lens.Loupe The `Control.Lens.Loupe` module has been deprecated for a while and will not be in lens-4.8. `Control.Lens.Lens` has the various operators mentioned.
But is `(a -&gt; r) -&gt; r` an "`a` producer" in any sense? Or `Const m a` likewise? I'm not saying its wrong either -- like all analogies it works until it doesn't :-)
I completely agree. `machines` is an experiment that got out of hand, and really could use more documentation and tutorials all around to help people get the hang of how to use it! We have more than enough monad tutorials, and arguably are getting to the state of more than enough lens tutorials. What brave souls will take up the cudgel of machines tutorials? Get in quick, before the market is flooded! By the way, to be fair, the idea of a mealy machine as a stream transformer _way_ anticipates this library -- I started thinking about them in this fashion from a post by Conal circa 2008 (http://conal.net/blog/posts/functional-reactive-chatter-bots) which in turn points out that they were in the `arrows` library as an `Automaton` far earlier even.
It would be interesting to see what GHC does with a partially unrolled fold vs a regular fold. edit: Just benchmarked this with criterion and saw a 15% decrease in time when summing a long list of numbers with a fold that had been inlined 4 levels deep vs regular.
Sounds like a perfect task for [tasty-golden](https://hackage.haskell.org/package/tasty-golden). Specifically, the [next version](https://github.com/feuerbach/tasty-golden/pull/12) (which I'll reease within a week or two) will have a convenient [goldenVsProg](https://github.com/phile314/tasty-golden/blob/dev/Test/Tasty/Golden/Simple.hs#L87-L93) function which you can use to run ghc on your input file.
I don't think this is what you are looking for, but your comment inspired me to make to make this: It uses a GADT to create positive and negative types, and has a type-safe simplify function to flatten nested values. {-# LANGUAGE GADTs, MultiParamTypeClasses, FunctionalDependencies, FlexibleInstances #-} data Pos data Neg data Signed a b where MkPos :: b -&gt; Signed Pos b MkNeg :: b -&gt; Signed Neg b instance (Show a) =&gt; Show (Signed b a) where show (MkPos x) = "Pos " ++ show x show (MkNeg x) = "Neg " ++ show x class Biconditional a b c |a b -&gt; c instance Biconditional a a Pos instance Biconditional Pos Neg Neg instance Biconditional Neg Pos Neg class Simplify s c where simplify :: (Biconditional a b c) =&gt; s a (s b d) -&gt; s c d instance Simplify Signed Pos where simplify (MkPos (MkPos x)) = MkPos x simplify (MkNeg (MkNeg x)) = MkPos x instance Simplify Signed Neg where simplify (MkNeg (MkPos x)) = MkNeg x simplify (MkPos (MkNeg x)) = MkNeg x posNeg :: a -&gt; Signed Pos (Signed Neg a) posNeg = MkPos . MkNeg negPos :: a -&gt; Signed Neg (Signed Pos a) negPos = MkNeg . MkPos posPos :: a -&gt; Signed Pos (Signed Pos a) posPos = MkPos . MkPos negNeg :: a -&gt; Signed Neg (Signed Neg a) negNeg = MkNeg . MkNeg test = simplify . negNeg $ 4 
This is great. I've been doing this in a few cases but hadn't really recognized the pattern - having it pointed out like this and given a name is really helpful.
Existing implementations of this process are known as .Extra: * http://hackage.haskell.org/package/data-extra-2.5.4 * http://hackage.haskell.org/package/extra * http://hackage.haskell.org/package/monad-extras * https://github.com/chrisdone/ircbrowse/tree/master/src/Data * https://github.com/haskell-infra/hl/blob/master/src/Text/Blaze/Extra.hs * etc. Everyone who's been developing apps in Haskell for a while ends up with these. Although personally I don't feel particularly happy about this pattern because it's just categorizing one's util modules. You end up with packages like my deprecated `data-extra` and Neil's `extra` and eventually you end up with, *cue dramatic music*, **this**! * http://hackage.haskell.org/package/MissingH Replace "Utils" with "Extra" or "Extended" and it's the same thing. This *is* a pattern. But it's not a desirable one. It's a step above one big utils module or package, definitely, but not much higher. We're lacking in alternatives, though. Our module system is pretty limited in this regard. --- While we're on the subject, if you have a generic task that isn't related to your project, don't put it under `MyProject.Foo`, put it under the hierarchy that makes sense, so if it's some networking stuff, like a socket-based heartbeat or whatever, put it under `Network.Heartbeat`, not `MyProject.Heartbeat` and don't let it import your project-specific types. Then you can re-use it later. Use it in enough projects and you can put it in its own meaningful package. * E.g. here https://github.com/chrisdone/present/tree/master/src/Data/Data * Or here: https://github.com/haskell/hackage-server/tree/master/Data
Yes, you're right, but I don't think my "extra" package (or necessarily any of the others) are going in the direction of MissingH. The MissingH package is best categories as "stuff John wrote" (there's a Debian dependency parser in there), while I've deliberately gone with "stuff I would like to see in base" - the focus is one of the hardest things once you have a "my generic stuff" package.
Far from all built-in lenses start with _. Only ones that would otherwise be invalid identifiers or conflict with existing identifiers do.
~~Note: `_1` and `_2` are not `left` and `right`.~~ ~~They are the first and second elements in an N-element tuple-like structure. So `_3`, `_4`, `_5`, .. also exist.~~ ~~Also, I have a `Vector2` type:~~ ~~data Vector2 a = Vector2 !a !a~~ ~~And it also uses `_1` and `_2` to access the elements. `left` and `right` would be confusing names for x &amp; y axes!~~ 
&gt; (You cannot start an identifier with a number though, so `1l` or `1_` won't work), and (`left` and `right` would conflict with those in `ArrowChoice`). I should have phrased it better. :S
People often say that Python is good for really quick prototyping. You can do that in Haskell, just delegate the type errors to runtime and you can instantly run your incorrect programs. Very quickly you will discover that you'll want to fix all the stupid bugs you run into, so you'll just go and fix the type errors... I found Haskell to be amazing for long-term maintenance. It helps a lot with refactoring. Regarding the difficulty, I think it is overstated. I personally trained a 1st year undergrad co-op student and he was productive within a month. In fact, some of his contributions went into the release.
Please read the question more carefully. The assert in the sample code is not checking whether the expression `mass == 15 * meter` evaluates to True, but whether the assignment `mass = 15 * meter` typechecks or not.
Yeah, I was aiming for understandable, but moving to bifunctors might be an option.
Right, I am familiar with the `.Extra` process – the `.Extended` idea is obviously based on it – but I think the subtle differences make the exact scheme I described a much nicer approach. For example, a problem with `.Extra` and `.Utils` packages and modules is that it doesn't solve the orphan instances problem quite as well. Whether we include `Data.Vector.Binary` or e.g. `Data.Vector.Extra`, might give us a subtly different `Binary` instance, which is of course, the original problem there is with orphan instances. That is why, strictly speaking, the instance should always be placed in either `Data.Binary` or `Data.Vector`. That way, if `Data.Binary` declares an instance, you will get a compiler error when you try to declare a conflicting one in `Data.Vector`, since you *have* to import `Data.Binary` to do that, bringing the instance in scope. Now, the way these `.Extended` modules solve the problem is very similar. You "have" (well, this isn't compiler-enforced, but scheme-enforced) to declare this instance in either `Data.Binary.Extended` or `Data.Vector.Extended`. Since you will be importing `Data.Binary.Extended` in `Data.Vector.Extended` in order to declare the instance (as a drop-in replacement for `Data.Binary`), you will get a compiler error if you try to declare another instance. Another very important distinction is that you should not upload these `.Extended` modules to Hackage, thereby claiming the global name of e.g. `vector-extended`. After all, there is no way that you are going to be able to cater this package to everyone's needs without depending on half of Hackage and including tons and tons of different-but-slightly similar utilities. Aside from that, it would also slow down development speed, of course. A third distinction is clearly the idea of re-exporting the original module, so it can be used as a drop-in replacement, and it doesn't cause import noise (also mentioned above). All in all, I think having a specific scheme like this for a project vastly improves over depending on a bunch of util modules on Hackage. I was also (and actually, I still am) quite hesitant to include `-extra` packages in my dependencies for projects, but I think this is something slightly different.
Ah, I see :)
While others have helpfully explained how to use typing rules properly, I'm still trying to understand what is causing you to ask such a strange question in the first place. I think understanding that is important, otherwise we might not be addressing the root of your misunderstanding! So, why would you even think of putting more than one thing on the right of the turnstile (⊢)? Is it because you're studying systems like the sequent calculus, which do have more than one term on the right? Is it because you think that type variables go on the left, and concrete types go on the right? Is it because you're not trying to put a second term on the right, but rather, to separate assumptions with a mix of commas and turnstiles?
Hmmm, I see. So I guess you assume that objects are static? function f(obj) { f.foo = 1; } you require that `f` must _already_ have the `foo` field? In that case it seems absolutely imperative to me that you support prototype inheritance in types. Whatever about disallowing implicit conversions etc (all the nasty parts of JavaScript), prototype inheritance is a key part of the language I would say. As for arrays, I guess I saw your comment on the repo that "treat arrays and functions as objects with properties" which sort of answers my question. Interesting. I wonder how far you can push this and how much actual JavaScript code will be accepted by the type checker.
In the extra package I reexport the original module, it's a technique that works very nicely. It's also great for maintaining version compatibility, so you don't get redundant imports in newer GHC's where you don't have to rely on the Extra module. Putting these on hackage is just a question of sharing. I have lots of them in the extra package, and then each project adds a few more that I don't consider polished or general enough to deserve being in the extra package. For a long time I didn't have a package, and then I ended up copy/pasting and finding bugs/optimisations, and lacking test material - centralising was a very good idea.
tl;dr `(*)` produces an integral so it can take in integrals. `(/)` could produce a fraction, so it takes in fractionals.
Sorry, yes, was too quick, I meant `obj` instead of `f` in both places. I mean that it seems to me you will want to accept something like function Person(name, age) { this.name = name; this.age = age; } Person.prototype = { "address": "Unknown" } var john = new Person("John", 45); john.address = "New York" or something like this, no? Even more important of course when definition functions on objects, rather than other attributes.
Thanks, those answers are quite helpful. I think perhaps my example of putting both on the right side of the turnstyle is errant. Mainly I'm trying to understand why you would need to say Gamma, y : S ⊢ E : T, rather than just Gamma ⊢ E : T . It seems the answer is that the rule needs to add y : S to Gamma before it can proceed?
The existing answers hint at this but just to be completely clear, the environment is all of `Gamma, y : S` and not just `Gamma`. In this context `','` may be viewed as an operator which creates a new environment by adding an assumption to an existing environment. Some authors make this explicit by including judgements: Gamma ctx ; t type ----------- ------------------ epsilon ctx Gamma, x : t ctx This can be read as "`epsilon` is a context[/environment]" and "`Gamma, x : t` is a context if `Gamma` is a context and `t` is a type" where `epsilon` denotes the empty context/environment.
I'm not sure either :) Can you give a counterexample of the sort of thing that you probably do _not_ want to support?
machines is the one with Mealy iirc: http://hackage.haskell.org/package/machines-0.4.1/docs/Data-Machine-Mealy.html
Yep. The EDSL they developed is open source, general purpose for embedded systems, and available for you to use.
You're right - sometimes. For example: When I build a monad stack for some specialized purpose, I put the definition of the monad itself, plus all of the surrounding monadic operations, lifts, run functions, etc., that go with it, together in the same module, *not* in the `Types` module. But for types that represent some fundamental concept in the semantics of the application, they go into `Types`. When I don't do that, cyclic dependency hell is my punishment.
This is so simple yet so brilliant! You can also point out that this method makes it easier to submit pull requests to the upstream hackage package when that is appropriate. Both to keep things better organized and for the above reason, I recommend putting the `.Extended` modules in the `hackage-extended` package into separate `source-dirs` named for the hackage packages they extend.
Thank you! Apparently hiding the `haskeline-0.7.1.2` package worked (or reinstalling it via `cabal install haskeline --reinstall`, I did try that before hiding it). Anyway the problem is solved now, I'm not sure though how I wound up in this situation.
Outstanding post. I'd also love to understand how you organize your home directory. Like, you have all these `OneOff.hs` scripts and `MyLib-WIP` you started but then got distracted from and libraries that you're maintaining on hackage but also developing further in the context of some medium-sized applications. There's this nest of sandboxes referencing some other WIP in other sandboxes. But then how do you version-control the sandbox or the sandbox of sandboxes? Sorry I'm getting ranty. It's just that after only six-months of haskelling on a fresh machine, I'm about to declare home-directory bankrupcy and I just wonder how I could better next time.
Thank you! I'll try out Sublime Text first since my basic notepad is such a hassle when it comes to formatting correctly, but defiantly will check out the haskell-vim-now when I'm further along!
It also seems to me that lens has solved several of these problems, too: 1. namespacing can be solved by declaring records in separate modules and using typeclass based lenses. Honestly, I don't think that having record + lens declarations going in their own module is that big of a problem. 2. Partiality is solved, I think, by prisms. So we can have foo :: (HasFoo t, Functor f) =&gt; (Foo -&gt; f Foo) -&gt; t -&gt; f t bar ^. foo -- Bar has a foo member baz ^. foo -- Baz has a foo member and (Right 3) ^? _Left -- evaluates to Nothing
But `Data.Fold.M` is also a Mealy machine. Aww. I'm always confusing these two packages...
&gt; It seems the answer is that the rule needs to add y : S to Gamma before it can proceed? Kind of. The rules --------------- Γ ⊢ unit : Unit and ------------------------- Γ , x : Int ⊢ unit : Unit are different rules; the first says that `unit` has type `Unit` regardless of the context, whereas the second says that `unit` only has type `Unit` when there is a variable of type `Int` in scope. Of course, for `Unit`, the first rule makes a lot more sense! A situation in which a rule with an extended context would make sense would be a rule for a construct which binds a new variable, such as a lambda or a let statement. Let's look at the `let` statement. For simplicity, let's say that in our language, the type of `42` is `Int`, not `forall a. Num a =&gt; a` as in Haskell. We would like to have a rule which expresses the fact that `let x = 42 in x + 1` and `let x = "foo" in x ++ "bar"` are well-typed, while `let x = 42 in x ++ "bar"` isn't. Clearly, the reason why the later is not well-typed is because `x` is bound to an `Int`, but it is used in an expression which requires `x` to be a `String`. We'd like to say that an expression of the form `let x = e1 in e2` is well-typed iff `e1` and `e2` are both well-typed. But as we've seen, the expression `x ++ "bar"` is only considered well-typed if `x` has type `String`. So if an expression `e2` contains variables, we cannot answer the question "is `e2` well-typed?", as we don't have enough information. The correct question is: "assuming that `x` has type `Int`, and that `y` has type `String`, and so on for all the variables which may appear inside `e2`, is `e2` well-typed?". The context Γ lists all the variables which are currently in scope, and says what their types are. With it, we can simplify our question to "Given the context Γ, is `e2` well-typed?". The typing rules are used to answer that question. For a `let` statement, to answer the question "Given the context Γ, is `let x = e1 in e2` well-typed?", we need to check that `e1` and `e2` are themselves well-typed. And at least in the case of `e2`, this question must be asked with a slightly larger context, specifying the type of `x`. Γ ⊢ e1 : S Γ , x : S ⊢ e2 : T --------------------------------- Γ ⊢ let x = e1 in e2 : T Interestingly, the following rule would also be a valid way to describe how to type-check a `let` expression: Γ , x : S ⊢ e1 : S Γ , x : S ⊢ e2 : T ----------------------------------------- Γ ⊢ let x = e1 in e2 : T The difference between the two versions of the rules is that the `Γ ⊢ e1 : S` expresses the fact that `x` is not yet in scope inside `e1`, whereas the `Γ , x : S ⊢ e1 : S` expresses the fact that in this language, `let` is a recursive let, meaning that `x` can be defined in terms of itself. let zeroes = 0 : zeroes in zeroes I hope this helped to clarify what those Gamma contexts were all about!
If it helps you to think in terms of an implementation (as I have known some people who build their intuition this way): the rule Γ, x: a ⊢ e: b ---------------------- Γ ⊢ (λx:a.e) : a → b would correspond to an implementation roughly like typeCheck :: Map Var Typ -&gt; Expr -&gt; Typ {- other rules -} typeCheck gamma (Lam x a e) = Arrow a (typeCheck (insert x a gamma) e) where the variable being added to the left of the turnstile corresponds to the call to `insert` in the implementation.
In my defense, things got really awkward. It's not so much that lenses are unavoidable but that when working with nested types you'll eventually reach a point where you have so much noise in your code that lenses will become the lesser evil. A good trick for finding generalizations is to try to work out what the type signature of such a generalization would be, and then search for it on Hoogle. This was how I got the hang of Applicatives. Lenses definitely won't make it easier to grok monads, the two things are unrelated. Everyone seems to come to an understanding of monads differently, but for me it helped to actually read the [definition](http://hackage.haskell.org/package/base-4.7.0.2/docs/Control-Monad.html#t:Monad) and [source](http://hackage.haskell.org/package/base-4.7.0.2/docs/src/GHC-Base.html#Monad) for the `Monad` typeclass.
The statement Γ, y : S ⊢ x : T can be read as: &gt; Whenever I'm given a context `Γ` *and* `y : S`, I can have `x : T`. Remember that a context such as `Γ` can be thought of as some *list* of assumptions of the form: a : A, b : B, c : C, d : D, … But wait, if `Γ` represents a list of assumptions, why can't `y : S` be "absorbed" into `Γ` like this? Γ′ ⊢ x : T where we conveniently define `Γ′` to be equivalent to `Γ, y : S`. But you see, the problem is that you're missing the other half of the picture. A full typing rule would look like this: Γ, y : S ⊢ x : T --------------------- Γ ⊢ λ y . x : S → T This can be read as &gt; Suppose: whenever I'm given a context `Γ` *and* a variable `y : S`, I can have the expression `x : T`. &gt; &gt; Then: whenever I'm given the same context `Γ`, I can have the function `λ y . x : S → T`. Now the problem is more evident: if I were to "absorb" the assumption `y : S` into `Γ`, where would I get the variable `y` underneath the horizontal line?? How would I know the type of `S`? You can't pull the variable out of thin air, because that would imply that you can use *any* variable `y : S` you want! Altering a rule like that could lead to to contradictions and inconsistency, although in this case I think it just makes the rule useless rather than breaking the type system. Someone correct me if I'm wrong, but I think removing `y : S` from the top just turns this into a form of Weakening.
We've mostly attempted to deal with the "lost in the forest" problem through tutorials, and the fact that there are "little" lens libraries out there like `lens-family` that can be used to get folks acclimated to the ideas. &gt; Something like `Control.Lens.Basic` with only the commonly used functions might be helpful. The question that always arises around such minimal modules is then what goes in and what stays out? e.g. Is "zoom" basic enough to go in `Basic`? It has a ridiculously complicated type. In `lens-family` this is worked around by only letting `zoom` work when the outside type in the monad transformer stack is actually `StateT`. Is that crippled version more "basic"? Is it more basic to make the combinators less general so they work with fewer cases, but have simpler types? Languages like racket have a whole hierarchy of languages they expose users to adding feature after feature. That might be a lovely tutorial, but I'm not so sure it makes a good library design, and the upgrade path for users is fairly complicated. How to organize (and teach) a package the size of `lens` is a bit of a case study waiting to happen.
This is especially helpful.
This is actually very insightful! Didn't pay enough attention to your post at first, but the same idea has just visited me and I remembered. __Yes! It is possible!__ And it's just as cool as anonymous records! First of all, let's refer to this concept as anonymous tagged union, stressing on "tagged", because we assign names to its components (like in ADT). Having a predefined set of polymorphic types like the following: data Sum2 (n1 :: Symbol) v1 (n2 :: Symbol) v2 = Sum2_1 v1 | Sum2_2 v2 data Sum3 (n1 :: Symbol) v1 (n2 :: Symbol) v2 (n3 :: Symbol) v3 = Sum3_1 v1 | Sum3_2 v2 | Sum3_3 v3 -- and so on And their instances for the following class: class ConstructorOwner (n :: Symbol) v a where deconstruct :: Proxy n -&gt; a -&gt; Maybe v construct :: Proxy n -&gt; v -&gt; a We'll be able to both construct and deconstruct their values using functions: type Either' a b = Sum2 (Proxy "Left") a (Proxy "Right") b getLeft :: Either' a b -&gt; Maybe a getLeft = deconstruct (Proxy :: Proxy "Left") consLeft :: a -&gt; Either' a b consLeft = construct (Proxy :: Proxy "Left") The features we get with this are: 1. Same as with anonymous records, we move constructors away from the namespace, allowing ourselves to define constructors with same names in a single module! 2. We become able to write more polymorphic functions, which leave the possible constructors open: getLeft :: ConstructorOwner "Left" v a =&gt; a -&gt; Maybe v getLeft = deconstruct (Proxy :: Proxy "Left") Thus, the `getLeft` function will work for any type having a constructor named `"Left"`. Wow! This is amazing! 
Not sure, what you're implying by "handled".. ?
Are you using Windows? While Haskell tries its best to be as platform-agnostic as possible and support every OS, there is some amount of friction when it comes to using Haskell* on Windows due to the OS's quirks. In particular, some packages are known to not work well on Windows (e.g. `network`) without an POSIX emulation layer like mingw or cygwin installed. [*] This applies to nearly every platform-agnostic programming language that isn't .NET-related or Java.
Just speaking about Yesod specifically, but install instructions are available here: http://www.yesodweb.com/page/quickstart. I also just made a screencast making a small site with Yesod that'd be a good overview if you're interested—the main thing that's not covered is `cabal install yesod-bin` https://www.youtube.com/watch?v=SadfV-qbVg8&amp;feature=youtu.be
Yes! The hom-set of a category is the set of maps between A -&gt; B. In Haskell, this is just the type `A -&gt; B`. So the co[ntra]variant functors fall out from `Reader` and something like `newtype FlippedFun b a = FlippedFun a b`
Well, as far as parametrizing the container goes, you can do this: data QuadTree f a = Node { a,b,c,d,e,f,g,h :: Quadtree a } | Leaf (f a) type ArrayQuadTree = QuadTree Vector type ListQuadTree = QuadTree [] I'm fairly confident that you can achieve your other goals too with some cleverness. [Accelerate](http://hackage.haskell.org/package/accelerate-0.15.0.0/docs/Data-Array-Accelerate.html) is likely a good example for enforcing dimensionality at the type level. 
Also, for generic cons'ing, you might look at [SemiSequence](http://hackage.haskell.org/package/mono-traversable-0.6.2/docs/Data-Sequences.html)
Especially if it could filter out the line number, which is a little nasty. Though I guess this should come from a copy/paste anyway. Would just be annoying if you had to edit the line number when you added more doctests.
Haskell, like all cartesian closed categories, is a category "enriched in itself". That means that instead of saying we have morphisms between A and B given by a "set" of functions `a -&gt; b`, we can instead say that the object corresponding to the type `a -&gt; b` is exactly a hom-object corresponding to morphisms between A and B. As such, the co/contravariant functor on the function arrow are precisely hom functors, as you observe :-)
`Γ` does not include `x : T`, but `Γ′` does.
Take a look at `uniplate` and the various plate bits in `lens`.
Alternately you could restrict the role of the octree to holding 'something per leaf'. data Octree a = Node (Octree a) (Octree a) (Octree a) (Octree a) (Octree a) (Octree a) (Octree a) (Octree a) | Leaf a Now, Octree can be a Functor, Foldable, Traversable, Applicative, Monad... Want to deal with grafting at the leaves as a list? Then work with Octree [a], etc. This lets you separate the concerns of "how to get down to the leaves" from "what to do when you get there". A lot of the time I use an octree it isn't to store point data, but properties of the space itself.
He pretty much got it up and running from zero prior Haskell knowledge within a month of fiddling with it in his spare time. I was duly impressed.
Seems a really neat idea. However, as I understand the extended module has to stay "private", so why not calling it the other way round ? like `Private.Snap.Core` or `Local.Snap.Core` for example. So it's clear that's it a private module. That way you can also have just one `Private` directory with an `Snap.Core.hs` file (with the appropriate extension). Also, things like `Prelude.Extended` will probably end up being shared between projects of the same developper (you always want to import `Control.Applicate` don't you ?). So do you manage this without copy/pasting ? What I was thinking, would be to use something like `Users.&lt;hackage-account&gt;.Prelude`. This way you can upload your package on hackage and share it between different projects and say at the same time : "this is a personal package, you are not supposed to used directly". 
And if you don't want to commit to Oct, you can do data NTree n a = Node (n (NTree a)) | Leaf a data Bi a = Bi a a data Quad a = Quad a a a a data Oct a = Oct a a a a a a a a type OctTree = NTree Oct You'll need suitable `Functor` etc instances for `Bi` etc. And probably some type class for indexing into the data as well. 
&gt; People who use Haskell industrially certainly have a decent amount of experiential knowledge and it doesn't seem like we have a good way of communicating this to newcomers. That's *exactly* what a pattern language does, and it would be a triumph if the haskell community could produce one, and it would be an artefact of enormous utility beyond the haskell community--but too many programmers in the functional world have decided that there aren't, or even can't be, any patterns for haskell programming (having over-interpreted an article which misunderstands a bad example of a pattern language). So it goes.
so am I, kudos to him.
Oh, I see. Now I understand your earlier terminology. You are thinking of the prototype of the "constructor" to be the objects "class", and this single prototype link is the only one you support; any further links in the prototype chain would be "inheritance". Ok, I get you now. Not really JavaScript terminology :) I think this is ultimately too limiting and not very true to the JavaScript language, but you'd have to experiment I guess. Incidentally, I recently came across http://research.microsoft.com/en-us/projects/koka/ and I sort of reminded me of this. You may or may not find it interesting :)
I can see how it is a functor but how would an Applicative or Monad instance look like?
I am very hopeful about supercompilation as a truly powerful optimisation technique. But I have to report that, after three years of Max's PhD work (and Max is a seriously intelligent and productive guy), we didn't get it working to the point where it could be considered a routine optimisation. Research is like that. On the plus side, we learned a LOT. I found the supercompilation literature to be inspiring, but really hard to understand in detail. (This may be my shortcoming, but it may also be a shortcoming that others share.) I feel quite proud of the work that Max did on understanding, modularising, and explaining supercompilation; our "[Supercompilation by evaluation](http://research.microsoft.com/~simonpj/papers/supercompilation)" paper is one example. But that was just the beginning. Max's thesis (same link) is an amazing piece of work, and describes a series of innovations that we explored. So why isn't it in GHC? Read Max's thesis! I think the tl;dr is this: we couldn't find a *robust* way of (a) getting the performance gains the supercompilation offers without (b) getting vast code bloat, often in cases where it is entirely unnecessary. Max has moved on to other things now. To me the field seems wide open. Before wading in, do read the thesis. But perhaps a bit more persistence and ingenuity will succeed where we failed. Simon
 join :: Octree (Octree a) -&gt; Octree a join (Node a b c d e f g h) = Node (join a) (join b) (join c) (join d) (join e) (join f) (join g) (join h) join (Leaf n) = n instance Monad Octree where return = Leaf x &gt;&gt;= f = join (f x) 
Hi Nikita! Nice job! I have a question: You say that there is no performance penalty for using your records at runtime. I wonder, then, how is the following function compiled? getPersonBirthdayYear = view [lens|birthday.year|] In his paper [1], Daan Leijen mentioned that a fast implementation could pass field indices along with record parameters (in the same way as dictionaries are passed along with typeclassed parameter). Is this what you're doing? [1] *Extensible records with scoped labels* http://research.microsoft.com/pubs/65409/scopedlabels.pdf
There is an underlying typeclass `FieldOwner` whose instances record when a record type has a particular field, so you end up passing the appropriate selector and update function in a typeclass parameter. Note that the records here aren't extensible: each record type has a statically-known number of fields, and there is no way to generic extension or restriction operations, so the selector/update functions are basically just dealing with tuples (and hence can be efficient).
&gt; I want a general purpose templating library, like jinja/mako, where I can write whatever kind of text-based files I want. FYI, Shakespeare is that. It's the general purpose templating tool that powers Julius, Cassius, and Lucius. You can use it on whatever sort of text file you want.
Oldie but goodie. I'd like to see the stuff they talk about updated to use the newest features (i.e. type families).
I declare one instance based on previous and one for the last field. I.e. instance (FieldOwner n v (Record_k n1 v1 ... nk vk)) =&gt; FieldOwner n v (Record_&lt;k+1&gt; n1 v1 ... nk vk n&lt;k+1&gt; v&lt;k+1&gt;) and instance FieldOwner n&lt;k+1&gt; v&lt;k+1&gt; (Record_&lt;k+1&gt; n1 v1 ... nk vk n&lt;k+1&gt; v&lt;k+1&gt;) 
I was surprised to see that you're using `hoistScope` for such a mundane task! It was added to the library (at least, the version from Bound.Scope.Simple was) after I showed Kmett how I was [using bound on non-monadic terms](https://gist.github.com/gelisam/9561388). He said it was a "clever abuse" of his library, and then proceeded to show me how I could make my implementation much more generic by using something like `hoistScope`. Which he then added to his library so that others could abuse it too :) I was also surprised to see how much you hate to switch to the DeBruijn representation, since in my opinion, this is the brilliant part of the library's API! Whenever you want to go under a binder, you change the representation of the free variables from a generic `a` to a representation which precisely describe the distinctions you care about: is this the variable which was bound, or is it one of those random other variables from the generic context? I guess in your case, you can get away with using `hoistScope` instead of changing your representation of the free variables because you're not examining the variables anyway.
Okay. I got it. Not sure, whether there won't be problems with compilation. And I highly doubt that the conversion lambda will get inlined. Hence this most likely will reduce performance. Concerning the limit of 24, it's just a off the top of my head. It's not a problem to generate more records. 
Now I feel even more depressed ;) Skimmed through the slides and didn't understand anything - the slides are too high level for me or missging details so that I understand what it is about, maybe they have been used during a talk. Then I went back to reread your comment. Decided to search internet for the term "unfixed data types". Funny, ddg links just to one page - the linkedin profile of Ivan Veselov. Seems like pretty new stuff. Maybe it will help to make one day Haskell easier to understand for newbs like, but at the moment I am overwhelmed and feel, like I should not go into yet another new solutions. I feel a bit depressed now. Will go and continue writing my boilerplate until I see how to generalize ;) But don't get depressed by my comment. Thanks for trying to help.
I'm not sure that I follow. I'm wanting to understand if the Γ on top of the bar is the same as the Γ below the bar. (i.e. does it contant y : S) 
Where do you perform the command? It says the package is not available or found.
Link bait warning, it's "'IO Monad' considered harmful", i.e. about the term, not the concept.
Refreshing article. Every time someone says 'IO Monad' a puppy cringes in fear.
I think you meant data NTree n a = Node (n (NTree n a)) | Leaf a But whatever you call it, it's just the Free Monad :)
IMHO Elm does it wrong: succeed : a -&gt; Decoder a A decoder that always succeeds. Useful when paired with andThen or oneOf but everything is supposed to work out at the end. For example, maybe you have an optional field that can have a default value when it is missing. If I didn't know this is just the "return" of the "Decoder" monad I'd be thoroughly confused about what this does.
For HList we can have a "base" instance and instance for "iterate". Right? I can't imagine how to make it with tree. How to declare instances? How to divide "left" and "right" case? I get a problem because instances couldn't be different by constraint only. 
Yes, `Γ` on top of the bar is the same as the `Γ` below the bar. But no, it does not contain `y : S`. Only `Γ′` contains `y : S`.
People keep trying to mock out `IO` into smaller types that contain less possible effects. Personally I'd rather use this with extensible effects because the thought of ~20 small monad transformers each introducing 1 possible effects brings me to near tears. It's a bit tricky though since all of those effects are noncommutative and that makes the actual package `extensible-effects` get rather unwieldy :/
Changing 24 -&gt; 40 for creation Records and instances for them (no changes fo Tuples) increase time for 'cabal install' from 50s to 3m on my notebook. Anyway record is cool for many application and as prototype for "anonymous record" feature!
&gt; It is always possible to turn any code that uses lazy evaluation into code that does not use it. What is the invariant here? This could turn terminating code into non-terminating code, right? So what is he saying?
It definitely isn't well documented, but you can use [Text.Shakespeare](https://hackage.haskell.org/package/shakespeare-2.0.4/docs/Text-Shakespeare.html) directly via `shakespeare`, `shakespeareFile`, and etc. In terms of something with similar power to jinja or mako? Probably not. This is definitely an area where Haskell could do better.
Well it seems you can generate 2*k instances with logarithmic access complexity. You can generate small arity as is. And then represent higher arity as tuple. Instead of **Record&lt;2k&gt; n1 .. v&lt;2k&gt;** just use **(Record&lt;k&gt; n1 .. vk, Record&lt;k&gt; n&lt;k+1&gt;...v&lt;2k&gt;)** and generate just two instances: instance (FieldOwner n v (Record_k n1 v1 ... nk vk)) =&gt; FieldOwner n v (Record_k n1 v1 ... nk vk, a) where setField n v = first $ setField n v getField n = getField n . fst instance (FieldOwner n v (Record_k n&lt;k+1&gt; v&lt;k+1&gt; ... n&lt;2k&gt; v&lt;2k&gt;)) =&gt; FieldOwner n v (a, Record_k n&lt;k+1&gt; v&lt;k+1&gt; ... n&lt;2k&gt; v&lt;2k&gt;) where setField n v = second $ setField n v getField n = getField n . snd And then again - Tuple of Tuples. (You don't need generate many types with many params also) It seems working! 
Between the haddocks and the examples linked from the haddocks I found the documentation for Bound to be pretty good. I think I might have read through [this](https://www.fpcomplete.com/user/edwardk/bound) a month or two before reading the haddocks, and played around with writing some code with bound after reading the haddocks (and looking at the examples), so it's possible that might be messing with my memory / evaluation of the documentation. Maybe I've lowered the bar after coming across so few libraries with the level of "example coverage" as bound :) Edit: I can totally see how that introductory paragraph would be opaque to quite a few people, I just wanted to mention my experience with the docs as another perspective.
You could start by factorising those 8 elements into a function. Something along the lines of (a quick sketch): data Partition = Negative | Positive deriving (...) data Octant = Octant { xPartition, yPartition, zPartition :: Partition } deriving (...) data Octree a = Octree (Octant -&gt; Octree a) | Leaf [a] It also looks rather like `Free ((-&gt;) Octant) [a]` which gives you some useful utility functions. 
What I meant is that you may have to rewrite the code, like in the `eq` example. (In the worst case, you end up simulating lazy evaluation in an eager language.) It's indeed not a good idea to take a fixed expression and switch from lazy to eager evaluation, that doesn't work.
`eq xs ys` (for the `zipWith` definition of `eq`) does not equal `xs == ys`, because `eq [] ys` is `True` for all `ys`, but `[] == ys` isn't.
&gt; It’s actually literally the worst and everyone in the world is worse off every time someone says it.
Here are some of the reasons why I consider the IO *type* "harmful": 1) It's supposed to signify effectful computations... except when it doesn't: ex1 :: IO String ex1 = do let result = "A big huge string that I'm not going to output anywhere" return result 2) Its absence is supposed to signify pure computations... except when it doesn't: ex2 :: Int ex2 = unsafePerformIO $ do putStrLn "All of my fusion optimizations are surely gonna be correct thanks to this purity that the type system enforces" return 5 3) It's an ungranular sin-bin of all kinds of effects: ex3 :: IO Purr ex3 = do launchBallisticMissiles print "Gotta ensure a nuclear winter before you pet a kittie" petAKittie 4) For all of Haskell's "safety guarantees and strong typing" talk it actually doesn't guarantee anything: ex4 :: IO () ex4 = do h &lt;- openFile "out.txt" WriteMode hClose h hPutChar h 'z' -- Nice resource handling checks here, IO! 5) Since it doesn't do much, you'd at least expect it to behave as nice as possible... except that it forces awkward syntax onto you: ex5 :: IO ex5 = do --let (weatherTomorrow, weatherTheDayAfter) = getWeatherForecast {- Won't compile, because it's IO -} (weatherTomorrow, weatherTheDayAfter) &lt;- getWeatherForecast -- Forces you to use special "&lt;-" syntax putStrLn $ "Tomorrow's forecast is " ++ (show weatherTomorrow) putStrLn $ "And the day after that " ++ (show weatherTheDayAfter)
I disagree entirely. The fact that `IO` is a monad is the key point that you usually want to pass across: there's pure code and there's impure "trapped" code. Without explaining the "trapped" part, `IO` is useless and unappealing. By considering the word "Monad" dangerous, *you* are the one further propagating the dangerous myth that Monads are something extremely hard (or even impossible) to understand. PS: And yes, it's not really trapped because of `unsafeIO` and company, but those functions are usually considered dangerous and frown upon in the Haskell community if used unfairly, so the point still stands. You can misuse anything, but that doesn't make it broken.
&gt; PS: And yes, it's not really trapped because of unsafeIO and company, but those functions are usually considered dangerous and frown upon in the Haskell community if used unfairly, so the point still stands. You can misuse anything, but that doesn't make it broken. unsafeIO is one of those non-conformist but important features that a Haskell compiler employs to afford library writers unobstructed access to to the outside world. This feature was never intended for capricious application, rather any software author using it is in a sense contractually obligated to ensure that the resultant code will not interfere with the expected behaviour of what is advertised as pure/non-effectful by the type signature of the library's entry point. 
&gt; 1) It's supposed to signify effectful computations... except when it &gt; doesn't: The IO Monad signifies that the programmer intends to use effectful computations. If the programmer says he intends to use effects, but doesn't, how is that a problem with the IO Monad? &gt; 2) Its absence is supposed to signify pure computations... except &gt; when it doesn't: It's a backdoor. You really shouldn't use it, but if you know what you're doing doesn't have side effects, it's a way of telling Haskell I know what I'm doing. The nice thing is the "unsafe" in the name. When reading the code, it's a giant red flag. &gt; 3) It's an ungranular sin-bin of all kinds of effects: All kinds of IO effects. Other kinds of effects live in other monads. What's wrong with this? &gt; 4) For all of Haskell's "safety guarantees and strong typing" talk it &gt; actually doesn't guarantee anything: Purity and strong typing can't prevent you from doing stupid things. Why pick on IO here? You can screw up the implementation an algorithm that doesn't involve IO at all, in totally pure code, where there is no way for the type system to prove you screwed up. There's only so much that Haskell can do to protect you from yourself. At least Haskell lets you do this: bracket (openFile "out.txt" WriteMode) (hClose) $ \h -&gt; do hPutChar h 'z' &gt; 5) Since it doesn't do much, you'd at least expect it to behave as &gt; nice as possible... except that it forces awkward syntax onto you: It's a monad. That &lt;- syntax has a very specific meaning, that you are pulling a value out of a monad. You want to pull a value of a Reader? Same syntax. Let bindings are only meant for pure values. The `do` notation is syntactic sugar anyways for the following: getWeatherForecast &gt;&gt;= \(weatherTomorrow, weatherTheDayAfter) -&gt; putStrLn $ "Tomorrow's forecast is " ++ (show weatherTomorrow) &gt;&gt; putStrLn $ "And the day after that " ++ (show weatherTheDayAfter) How would the let binding make any sense in this context?
I think the term is responsible for about half of the [Monadic Myths](http://www.stephendiehl.com/what/#monadic-myths). I think I would have grasped Monads much more quickly if I hadn't encountered them first in the context of IO.
Except that the IO type is an instance of the Monad type class... That is actually useful information telling you what you can do with it, since there are a number of functions that work on any monads, such as liftM or mapM, which can be applied to IO as well. IO has everything to do with monads. That's why you can use the `do` notation and why `&gt;&gt;`, `&gt;&gt;=` and `return` work for IO. Lists are a basic type, so basic that many list functions have been duplicated in the Prelude. `map` and `mapM` do the same thing for lists. Understanding that lists are also monads unlocks a lot of interesting possibilities. If you are working with lists monadically, it makes sense to say the list monad. You are never working with IO not monadically, just like you are not working with Reader, Writer or State not monadically.
I would also accept "IO [MacGuffin](http://en.wikipedia.org/wiki/MacGuffin)".
The fix is easy: eq xs ys = (length xs) == (length ys) &amp;&amp; and (zipWith (==) xs ys)
Thank you, I've read "Real World Haskell" and know all this. Doesn't mean that the IO type is the best way to go about making effects explicit or separating them from pure code. 1) IO is a type for separating computations with IO effects from computations without them. Letting pure values live in IO just makes that separation sloppy and unsatisfactory. Ideally, a function flagged with, say, `UsesHTTP` and `UsesScreen` should use both of those things, and omitting any one should force at least a warning from the compiler. "Hey buddy, why'd you declare that this function uses the screen when it doesn't? You're misleading the clients of your library and perhaps preventing me from doing some optimizations!" 2) It's not much better than imperative languages, really. It makes IO like C except with a "please wrap side effects in this function" sign. It's just a programmer convention. Oh and I can rename `unsafePerformIO` to `purify` or `thisIsSafe`, but that won't make it better. 3) With IO and `IORef`s one can do anything, even spawn a separate Haskell process that will run a computation in an arbitrary monad. So IO type is really "use any effects you want" type. And defining "restricted IO" types is no better than what imperative languages do. 4) The Haskell type system is fine, it's just this IO type that I can't figure out the point of. Is it meant to separate pure code from impure? Obviously not, since IO code can be pure and non-IO code can be impure. Is it meant to modularize effects and make them fine-grained? Again, no - monad transformers and extensible effects try to do that, IO doesn't. Can it at least help make using effects safer and saner? Again, no, that's something Idris does but not the Haskell IO. So what's it for? 5) IO isn't just a monad. The concept and typeclass of `Monad` has nothing to do with IO, and people would do good to teach Haskell monads without mentioning IO until the student is familiar with at least 2-3 non-IO monads. IO has a special place in Haskell as a way to segregate impurity from purity, but it's a very simplistic way to go about it. "Oh let's tag everything that has rights to communicate with the outer world with this huge monolithic type constructor that you can't escape from, except that you can with this unsafe function that you don't even need any language extensions to use!" It's one of those better than nothing things but it's not ideal and there can be other ways.
&gt; Monads have nothing to with trappedness. Yes, they do. The monad axioms give you a way to lift values (`unit`), but no way to extract them. The monadic interface gives you a way to manipulate values and computational contexts without allowing values to escape. That is precisely why `IO` is a monad.
&gt; Monads have nothing to with trappedness You're very emphatic, but I have to disagree. If I give you foo :: Monad m=&gt; Int -&gt; m Int bar :: Monad m=&gt; m Int baz :: Monad m=&gt; Int -&gt; Int -&gt; m Int I think you'd find "trappedness" might describe the distinctive quality of the code you could write from that. I think you answer "how does haskell do IO" by talking about three equally important things: - values of type `IO`, and how its implementation is hidden - `Monad` for composing types of `IO`, and the qualities of "trappedness", and sequencing that characterize it - `main` as the entry point to your program, where your `IO ()` type is handed over to the haskell fairies who unlock it and "run" things **Edit**: in general I agree that saying "the IO type" is usually what one should say instead of "IO monad". 
Yes, a monad *can* provide a way out. But the point is that the interface is sufficiently powerful to allow arbitrary computations *without* giving you a way out.
&gt; 1) IO is a type for separating computations with IO effects from &gt; computations without them. Letting pure values live in IO just makes &gt; that separation sloppy and unsatisfactory. This is true for all monads. The monad is a context from which a value was derived, and not the result of a pure context-free calculation. This separation is essential to the Haskell type system and is very useful. &gt; It's not much better than imperative languages, really. That's like saying that `Nothing` is not much better than `null`. You could call dealing with `null` just a programming convention, too. `unsafePerformIO` exists only because Haskell is a pragmatic programming language. `fromJust` also exists for the Maybe type, but you'll get an exception if what you thought was `Just` was in fact `Nothing`. Because Maybe is a monad, there are better ways of dealing with values wrapped in Maybe (that also work for IO!). What makes `null` insidious is that it is a silent killer, while Haskell forces you to make your intentions obvious, which is what separates Haskell from C. &gt; So IO type is really "use any effects you want" type. This is true for any monad. You can nest monads inside other monads. But IO represents a particular kind of environment, an imperative environment like C. It's the stuff you can do outside of the IO monad that differentiates C from Haskell. &gt; Is it meant to separate pure code from impure? Yes, it is. Just like any other monad. Which is why IO is a monad. &gt; Is it meant to modularize effects and make them fine-grained? Yes. The IO monad represents is a type of monad which encapsulates IO actions. &gt; monad transformers Monad transformers allow you to combine effects. For example, if you want a Reader to provide a read-only environment where the read-only values come from a file. You can combine these effects in a `ReaderT a IO` &gt; IO isn't just a monad IO is a monad, by virtue of it being an instance of the Monad typeclass. IO is implemented in Haskell using monads, which is why IO has everything to do with monads. You don't have to care about IO being implemented using monads, but it becomes very useful when combined with combinators that work on monads. &gt; Oh let's tag everything that has rights to communicate with the &gt; outer world with this huge monolithic type constructor world that &gt; you can't escape from. That type constructor indicates that the value came from the outside. Which is how monads work. Monads keep values chained to their environment, restricting how those values can be unchained. &gt; except that you can with this unsafe function Which you have to explicitly call. Just like `fromJust`. &gt; It's one of those better than nothing things but it's not ideal and &gt; there can be other ways. Better than nothing is way better than `null` or hidden side-effects. Are there better ways, sure why not. But IO being a monad makes perfect sense.
&gt; 2) It's not much better than imperative languages, really. It makes IO like C except with a "please wrap side effects in this function" sign. It's just a programmer convention. Oh and I can rename unsafePerformIO to purify or thisIsSafe, but that won't make it better. This is precisely what SafeHaskell is for. It allows us to track these things and make sure that they don't show up in critical code.
There's something relating the two points that you maybe miss. If I happen to know that Decoder is an instance of `Monad` then I know that there is such a thing as "the Decoder monad" -- it is the type decoder and the collection of operations that constitute the monad instance. So now if I know what a monad is, knowing that something is "return in the Decoder monad" is a sensical statement -- it is, conceptually, the projection of the "return" function from the tuple of values that makes up "the Decoder monad". And then I know immediately a bunch of laws and relationships on it :-) So yes we can explain `Decoder` as though we had no concept of monads, and we can give its names specific operations. But if it happens that I already know what a Monad is, then documentation that shows how Decoder is one (whether or not one can equip such an instance in `elm`) is going to help me understand it much faster.
Here's a nice mock of it: https://hackage.haskell.org/package/IOSpec I can't imagine wanting to use such a thing "in anger". We have as many effects as we have potential FFI bindings :-P
Ah yes, you're right. Generally, you're stuck with the monad, which is why I used the word "trap". But, there are plenty of functions outside of the monad laws which let you strip away the monad, which is generally unwise.
I've read the the haddock, the source, the examples, and the FPComplete article and it still doesn't make much sense, tbh.
Actually, you are using IO monadically all the time. It's just that you don't know or care. Do you compose IO actions in a `do` block? Congratulations, you are using IO monadically. You are using `&gt;&gt;=` and `&gt;&gt;` under the covers, straight from the three monad laws. It's just that the implications of that are irrelevant to you. You can also use lists in a `do` block, and that's when things get interesting.
An implementation of a type class is an assertion that the structure has _at least_ that much functionality. It says nothing about whether it may have more. If it did, then nothing could actually implement the monad type class. Proof: All monads are applicatives. But an applicative implementation implies that the type does not implement anything more powerful than the applicative interface. The monad type class requires more power than the applicative type class. Therefore anything with a monad implementation must not be more powerful than an applicative and therefore can not have a monad implementation. Contradiction, and only one statement eligible to be false.
Of course, it is all a simulation of "not pure", and in some monads like State, the simulation is very convincing. But Haskell attempts to prevent the simulation from spilling out into the "pure" code, so you never forget if you are awake or still dreaming :-)
There is impure code in Haskell. There are plenty of side effects. Here are a few examples, some more practical than others: * ⊥ * exceptions * memory usage changes * time passes * CPU gets hotter
Not really something specific to monads or IO.
What were his thoughts about the experience? Did he ever write about it?
He'd mentioned at the time to me that he was thinking about writing up his experience in a blog, but I don't think that ever happened. Life intervenes. ;) Digging back through old emails, it seems the main thing he noted was that optimizing Haskell is a heck of a lot different to optimizing code in other languages and that it was hard to find resources on how to do so.
&gt; If I happen to know that Decoder is an instance of Monad then I know that there is such a thing as "the Decoder monad" -- it is the type decoder and the collection of operations that constitute the monad instance. Yup. &gt; So now if I know what a monad is, knowing that something is "return in the Decoder monad" is a sensical statement It's sensical but that's not what's up for debate here. It's the strange favouritism of calling a data type by one of the interfaces it happens to implement. An Int is an int. It's an orderable, an enumerable, a number, a boundable, etc. But those are just interfaces. So always referring to Int as "the Int orderable" is just weird and confuses people. So is using "the X monad" instead of just X. &gt; So yes we can explain Decoder as though we had no concept of monads, That's not the point Evan was making or saying was something anyone should do. Did you read the link? https://groups.google.com/forum/m/#!topic/elm-discuss/1acyOfxvasA
The same is true for Applicative or Functor. I think you have some bias towards the Monad class.
I think you misunderstand what is "up for debate". I agree with the OP's article. I _also_ agree that documentation should discuss when types have associated monad instances, etc., and that this can be very useful. I know what the point Evan was making was, but I also am given to understand that in elm the idea that we shouldn't say "the X monad" has been extended to trying to talk about monadic _aspects_ of things even when that understanding _is_ an important shorthand to people that already know what's up. Now that might not have been exactly the intention of Evan's argument, but it is what actually occurs in the elm documentation at places, as /u/_lostman pointed out.
Your concern about `IO` being used to wrap a whole bunch of semantically-unrelated effects is fairly common among Haskellers -- people often speak disparagingly of the "`IO` sin bin." I tend to agree that you should use the most specific monad you need, but that's a matter of program design. There is no *formal* inconsistency in `IO`. Your other concerns are misguided. &gt; "Hey buddy, why'd you declare that this function uses the screen when it doesn't? You're misleading the clients of your library and perhaps preventing me from doing some optimizations!" This makes absolutely no sense. I mean, let's take a really simple example: in the `IO` monad, the `return` function does not end up performing *any* IO. But without `return`, `IO` would not even be a monad! The thing is, types which implement `Monad` are still types. `IO ()` is a *real type* with *actual inhabitants.* It would be an absolutely terrible idea to go around restricting what people could do with their data. &gt; It's not much better than imperative languages, really. It makes IO like C except with a "please wrap side effects in this function" sign. Would you rather be completely unable to read files or react to user input? That would make Haskell a fairly useless language. &gt; It's just a programmer convention. It really isn't. It (or, at least, *some* principled way of doing IO) is an integral part of the language design. Look: if you went into a large Haskell program and replaced every instance of `IO` with an "equivalent" sequence of `unsafePerformIO` calls, you would almost certainly break the program. From the compiler's perspective, `unsafePerformIO` is just another function, and is thus subject to all the usual optimizations. This is where the "unsafe" part comes from: in order to use it correctly, you need to *prove* that the Haskell compiler won't be able to optimize it away or anything like that. This is the fundamental reason why `IO` (or something like it) is absolutely necessary in Haskell. Separating `IO` out from pure computation does encourage good programming, but the more important reason is that it prevents the compiler from optimizing away function calls that would otherwise appear to be unnecessary.
noob here. can you give an example of each one?
I don't know how you can get that from the word Monad. I'll talk to you about the Ring on Q. Does that mean that Q has no multiplicative inverse? Does saying that Bool is a Monoid mean that it has no additive inverse? The IO type is something that is sufficiently powerful to allow such computations. But it doesn't just gain that from Monad. It gains that from Functor, Applicative, Alternative, etc. None of those have an "escape route" either. And furthermore, saying "IO Monad" still *doesn't* say *anything* about whether or not IO has an "escape route" or not. It also doesn't tell you anything about whether or not you "only need Monad" to work with it :)
I guess that's a criticism of my second bullet point? If so, then I again disagree; as far as I can see, the `Monad` is just and exactly powerful enough to be the IO story for haskell. You can't even write `echo` with `Applicative` (please correct me if I'm wrong).
see reply: http://www.reddit.com/r/haskell/comments/2tbbxh/io_monad_considered_harmful/cnxx33t :)
Are you talking about evaluation or execution?
Meta-point, is starting out a comment with "you are wrong" a positive way to engage conversation?
As the blog post suggests, you are not "using" monad unless you use bind (&gt;&gt;=) or do notation (which uses bind), or perhaps Control.Monad.join. So if you write main :: IO () main = putStrLn "Hello World!" you didn't use bind, you didn't use do, you didn't use monad.
ah, i see. i've been working with the option monad in F#, and it seems to confine me at times. sometimes i want to "break out" of the monad, but then i know i'm doing something unnatural, and i stay trapped until the end when i can pattern match and deconstruct. the IO monad in haskell seems more rigid (as you alluded to). 
This won't terminate if infinite lists are involved, while `(==)` would correctly say `False` in `repeat 1 == repeat 2`. I think switching the arguments to `(&amp;&amp;)` will fix that particular thing, though.
It isn't favouritism to advertise the datatype's most useful feature. What good to me is the Decoder type without the Monad instance? Elm has built-in functions for objects up to 8 fields. If you want more you have to catch up on Monads. But the docs don't mention Monads at all. So if you're a newcomer to Elm or Haskell and are trying things out, you don't even know what you don't know. If I told you "Decoder is a Monad" then you at least have a breadcrumb to follow. You can open up Google and search for Monad and maybe you'll get somewhere.
&gt; There is nothing that has to do with monads at all in printing a string. The idea that putStrLn "hello world" is monadic is as absurd How about calling 'putStrLn "hello world"' twice?
Unfortunately, "IO type constructor" does not roll off the tongue as well.
I've just read that article about Conal, and other ones such as these: http://blog.jle.im/entries/series/+intro-to-machines-and-arrows Those are very good to show the intuition on how to use Mealy Machines in a "Haskell" setting (finally understanding why it is an instance of Arrow for example :P ) 
Ah, life intervenes, indeed. But despite that, it's very interesting to hear thoughts from game dev since it's a field which, in a classical sense, should feel as though it has little to gain from Haskell... whatever the truth may be ;)
Well, except that instance Applicative IO where pure = return (&lt;*&gt;) = ap But I do get your point; in spirit there's nothing inherently monadic about calling putStrLn twice. I think once Monad is tarred and feathered the pitchforks will be up for Applicative. Paraphrasing the blog post: "How do you call putStrLn twice? The answer is definitely not 'with IO Applicative'" IO is abstract so we can only talk about what it does. And what it does (among other things) is that it behaves like a Monad. As an aside, how about ST monad? Another abstract type. I don't have a solution to explaining Monads to newcomers but I believe one can write Haskell without worrying about it to much. Similar example that comes to mind are Clojure's transducers. Nobody seems to agree on what they actually are (judging from the myriad of 'transducers explained' blog posts and associated discussions). They are however quite easy to use.
Intellectuals, use Haskell to advance the glorious Juche idea given to us by the Supreme Leader!
I would definitely consider this impure, because it can yield different results on different runs of the program. Theoretical implications aside, this destroys your ability to rely on the results of any testing you do. Consider this: let x = consistentMapStore 0 y = consistentMapStore 1 in x `par` y `pseq` x &lt; y If the 0 is stored first, this will be true; if it's stored second, it will be false. Since x and y are run in parallel, the result is nondeterministic. This doesn't happen when working with pure values, which is one of the reasons it's so easy to introduce parallelism into many Haskell programs. A related concept is function memoization. Conal Elliott's excellent [MemoTrie](https://hackage.haskell.org/package/MemoTrie) library provides a function `memo :: HasTrie t =&gt; (t -&gt; a) -&gt; (t -&gt; a)`. This takes a function and uses a very clever lazy evaluation approach to avoid recomputing results when the function is given the same input multiple times.
I like to use "pure" from Control.Applicative, instead of "return" whenever it is possible, unless I am using "do" notation and the "return" comes at the end.
"IO type" isn't so wrong, especially when people are almost never honestly referring to `IO` but instead `IO a` anyway.
Is it actually a thing for North Korean propaganda to speak to "intellectuals"? ~~Soviet~~ Communist states have traditionally had a spotty record with intellectuals...
Afaik, /u/dcoutts is working on incremental updates for the index-tarball
I like to think of it as a different usage of type. When we talk about list, maybe, or Map, we talk about the (sometimes abstract) data type, not about the type constructor.
I actually meant "communist states" and not Soviet states. That's what I get for trying to communicate before coffee. But thanks for the interesting trivia. I've never looked very deeply into that kind of stuff.
&gt; x &lt; y Who said `Value` was `Ord`? Isn't the only function on `Value` `consistentMapGet`? &gt; it can yield different results on different runs of the program How?
Oooh, "execution environment". I like that.
I confess I didn't expect Haskell to work all that well in interactive data analysis, but this is really cool. I especially like the optional streaming of rows using pipes (how does Pandas do that?) and the possibility of custom universes of column types thanks to Vinyl. 
With the recent additions to IHaskell, I believe Frames actually works completely interactively! So you can type all of this in to a notebook (instead of using a file) and skip the compile-load step entirely (I think that you also get around the stage restriction that way). You can then use `Chart` to plot your data and get pretty graphics! Hypothetically it may be possible to make it so that the Frames-generated data tables are also displayed nicely in IHaskell a la [the Pandas IPython display](http://i.stack.imgur.com/x8QWA.png). I'm very exited by this :)
I'm not really versed in Haskell, and I read through the article thinking "Does he mean 'the IO type *constructor*?'" When you talk about the IO type or the Map type, are you referring to an actual type-in-Haskell's-type-system which consists of the union of all IO a/Map a, or just a conceptual type?
 module Trap (Trap, trap) where newtype Trap a = Trap a trap :: a -&gt; Trap a trap = Trap The only ways you can get an `a` from a `Trap a` are by pattern matching or if I provide a function `Trap a -&gt; a` (which would use pattern matching internally). I don't export the constructor itself so you can't do pattern matching. I also don't provide such a function, so you can only put things into the type, but can never get them back out. I don't have to provide a monad instance for that.
&gt; If it gets data from the disk I don't think it does though? &gt; What if the key hasn't been stored? How do you construct such a key?
That's super easy to avoid by making `Key` opaque though.
Oh, certainly. Still amazing that it takes only linear time *O(n)* as opposed to *O(n log n)* time, though. Actually, if you use a sorting algorithm that works in an on-line fashion, like heapsort, then you get something very close to the single pass algorithm, as long as you keep the heap sufficiently lazy. The code would be something along the lines of sort = Set.toAscList . foldl' (flip Set.insert) Set.empty except that `Data.Set.Set` does not work, because it is probably too strict. 
This is a correct implementation of equality, but unfortunately doesn't have the "return early" behavior that I wanted to highlight anymore.
AFAIU this is essentially a reimplementation of HList. It will perform worse and will occupy more memory. We need a flat datastructure. Recursive ones won't do.
Being a Haskell newbie I was intrigued by the sentence at the end of the article which says: "Already, many real-world Haskell code that “does IO” doesn’t ever directly work with the IO type itself." Do you know what he's talking about exactly?
The IO type is useless without a bind function which is part of the monad interface. So teaching someone about the IO type without teaching them about bind is pointless. And when you teach them about bind the obvious next step is to teach them about monads.
Yeah, I was typing this comment on my phone last night as mental note to expand on this today, but it came out harsher than I intended.
&gt; Does this concept have a name? I would call it “hand-rolled pointers”.
IO type is far from useless without the bind function; that's the point of the article. IO type is very, very, very useful, even without bind. Just like how list is useful without concatMap. (And even more so if you consider libraries that allow you to never directly use it too.)
This is a common misconception of beginners. IO, Reader, and State are not pure, that's the entire reason for having them in the language in the first place. Haskell is a "pure functional language" precisely because the IO abstraction is pure, allowing us to remain pure in the presence of IO. This as opposed to, for example, OCaml where IO is indeed an impure. The fact that you can never escape IO is exactly what makes IO pure. Purity is not related to "context dependence" as you seem to imply with: &gt; Impure code, or code that works on values pulled from a monad, is context dependent The SOLE definition of purity is that "given the same input value(s), a function will always return the same result". None of the types you mention violate this principle. For example, `putStrLn` is a pure function, because calling it with the same input always results in the same output (i.e., an IO action that prints that input). Reader and State have a direct desugaring into plain haskell functions, making it even clearer that they cannot possibly be "impure", unless you try to argue that functions are impure. The trick behind IO is that thinking of it as "pulling values from it" is the wrong mindset. An IO value is an action that can be run. You can combine actions into more complex actions using a function like `withResult :: IO a -&gt; (a -&gt; IO b) -&gt; IO b`, but none of the parts of this function are impure. We're taking a value `IO a` and a plain function `a -&gt; IO b` and returning a new value `IO b`. At no point in this do any side effects happen. The only thing you can do is use these pure tools to construct a single large `IO a` datatype called `main`. This IO value can then be executed by the runtime, but this is not observable by the haskell code.
Please tell me I don't need to use Lens to use this.
Not echo. 
I feel you. I think your point about writing echo is spot on. And that monads do trap values. I liked the article too. But the author's comments here seem like they're just assertions, which isn't an argument. It's just an assertion. 
A type can provide a way out, a monad cannot.
AFAIK HList is linear by arity. Here we have log overhead by access and memory. Great ideas by my opinion of record are: * Using DataKind * QQ and parser. * Using Lens. Internal representation is problematic. Personally I prefer general O(log(n)) implementation then restricted O(1) 
I don't get what you're trying to say. Monads never trap or not trap they just supply a context. It's the type that "traps" or does not trap. E.g. with the list type I can use `head`, `!!` or whatever to get my value out, I can use `maybe` to get values out of the Maybe type, with StateT I can get the value, the state or both out. It's mainly the IO type that has no way of extracting things out of it again (which makes sense when you think about what the IO type actually represents).
Personally, it would have helped me start out quicker if everyone just stopped referring to "monads" or "functors" as *nouns*, as *things*, rather than as structures of operations that *actual data-types* can implement. Like, referring to the points within spaces, the spaces, the paths between points, the paths between spaces, and the paths between paths as if they were all just the same kind of thing was really confusing.
They only use `view`, I think. They'd need something like that anyways, so why not use Lens. 
Paging /u/duairc. [Quote, 11 months ago](https://www.reddit.com/r/haskell/comments/1xnmiv/monad_layers_an_alternative_to_transformers/cfd8nlb): &gt; Thanks, I appreciate that! Your comment, and just the surprise of seeing this at the top of the Haskell reddit without me putting it there, has given me a good bit of motivation. It's great to see that people appreciate my work. I'm focusing at the moment on doing some web design work for a local social centre where I hang out, but when I'm finished that I think I'll try to finish the next release of layers. It's great knowing that other people are actually interested in that! Any update on this?
That makes me wonder: If a function is a value, why isn't a type constructor a type?
What everyone is trying to explain to you is that your view of Monads appears to be wrong. You absolutely can write echo with Applicative. You could write it with Functor even (you just couldn't compose two echos and have both run). EDIT: Never mind, I misunderstood what was meant by echo here.
And how exactly is this different from a functor or applicative? It's not. Trapped or not trapped is dependant on the type.
&gt; Thus, once a computation has entered the world of I/O, it can never leave. This has nothing to do with being a Monad and everything to do with the type `IO a`. This requirement of anything done in IO must stay in IO is part of how the *type* is defined and would remain this way even if it had no Monad interface and everything was implemented via arrows. By contrast, StateT gains most of it's power from its Monad instance yet I can get the final value *and state* out of the StateT monad without problem. Because this makes sense *for the type*. If the module were changed not to export runState or the other related functions than values would also be "trapped" in StateT.
You understand correctly, but it's simpler than that: There will (very likely) be overhead related to sorting that does not exist in a naïve single pass algorithm.
Yeah, it seems `hoistScope` wasn't really needed. I've updated the post to reflect this. &gt; I was also surprised to see how much you hate to switch to the DeBruijn representation I wouldn't put it that way :-) It's just that I've never actually used deBruijn for real, and I'm not sure what problems it comes with, so I wanted to be cautios. I also thought that one purpose of Bound is to hide the indices as much as possible, and I wanted to see how well it manages to do that. I wasn't quite sure what to expect, but now I think I better understand the benefits of the representation.
Some of those requirements remind me of [haxl](https://github.com/facebook/Haxl). I've never used haxl, but it sounds like you could define E(i, X) as a data source, and haxl would batch those calls together.
&gt; Who said Value was Ord? I was basing that on this: &gt; it generates a previously-unused key k by incrementing a counter Of course, the keys could be made completely opaque, so that no comparison is possible. However, in that case, I'd be interested to see how the implementation differs from: newtype Key = Key { unKey :: Value } consistentMapStore = Key consistentMapGet = unKey And that, of course, is perfectly pure, but not terribly useful. In the case of the hash-based implementation, we still have nondeterminism in the case of a collision. We can resolve this either by guaranteeing no collisions (by making the keyspace at least as large as the space of possible Values - in which case why not just put the Value inside the Key?) or assuming no collisions, in which case the function isn't really pure, but might be "pure enough" for a given application, I suppose. However, if we let the user do practically anything interesting with the key, e.g. serialize and deserialize it, we're back to nondeterministic behavior: let k0 = deserialize someSavedKey v0 = consistentMapGet k0 k1 = consistentMapStore v1 in v0 `par` k1 `pseq` v0 If k0 and k1 are equal, this will either succeed or fail, depending on whether the `consistentMapGet` or the `consistentMapStore` runs first. The only situation I've been able to concoct in which this would do something potentially useful and still be pure is as a way of deduplicating the heap - run a bunch of values through `consistentMapGet . consistentMapStore`, and they'll be sharing memory instead of potentially having multiple copies of values with the same hash (which would still need to assume that collisions are "rare enough" to make your function "pure enough"). However, you could also do this with `memo`, without using unsafePerformIO. Perhaps someone else can come up with a more interesting use case than I can.
How could you write echo without Monad? I thought the whole point of Monad was to have computations that were dependant upon previous computations.
Ah, there it is! Thanks.
Ok, I get what you're saying now. But there are still a lot of things you can do with IO without using Monad and Monad or not has nothing to do with trapping values. fmap (+5) (readLn :: IO Int) The value will be just as trapped because *the IO type is trapping it*.
Well, don't give up! Those concepts are not that complex actually. They're just hard to *explain* in their full generality. Also, screw the `Monad`, delay it until much later. Instead, focus on: * `Functor` * `Applicative` Once you understand these two typeclasses, you'll probably laugh at how actually simple everything else is. I'd say that `Functor` and `Applicative` have a great portfolio or kicking people's minds in the right direction (in contrast with `Monad`) to understand some Haskell beyond the basics. In particular... Do you find yourself frequently pattern-matching over the AST type in unrelated code? Try and see how much of these can you replace with an `fmap`, together with appropriate `Functor` instance. I bet a lot. ---- For unfixed data types, the concepts of [fixed point](http://en.wikipedia.org/wiki/Fixed_point_%28mathematics%29) and the fixpoint operator are absolutely essential. They're required! Go read the definition and examples carefully. Now realize that not only the fixpoint operator is conceptually a higher-order function (as it *takes another function* `f` and returns a value `x` such that `f (x) = x`), but the returned fixpoint value `x` itself *might be a function*. As an excersize, try to write `fibonacci` without using the name `fibonacci` directly in the same definition. In science, it's pathetic to give self-referencing definitions, right? Hint: you can add another argument `r` and call `r` instead of wherever you would've called `fibonacci` recursively before. Now all you need is another function (and it's often named `fix`, for a reason) which supplies the freshly defined `fibonacci` function... to itself as the `r` argument! In this peculiar way we can define functions *as fixpoints of another functions*, and it is now your final `fib`. → spoiler: `fib = let { fib' r i = if i &lt; 2 then 1 else r (i-1) + r (i-2); fix f = f (fix f); } in fix fib'` ---- The next step would be applying the same trick (called *unfixing*) to your AST datatype; instead of data Expr = Const Int | Var String | Plus Expr Expr you write data ExprF e = Const Int | Var String | Plus e e type Expr = Fix ExprF with a suitable definition of `Fix`. It should be type-level fixpoint operator. data Fix f = Deep { undeep :: f (Fix f) } Notice how similar unfixing `Expr`-the-type is to unfixing `fibonacci`-the-function. ---- And finally, this gives us a chance to do wild stuff with `ExprF`. Do you need an annotated AST, like for example `TypedExpr` where *the type* of the expression is attached to each AST node? Easy: import Data.Functor.Compose data TyEnum = TInt | TString data TyAnnF e = TyAnnF TyEnum e type TypedExpr = Fix (Compose TyAnnF ExprF) Notice that we didn't do any modifications of the core `ExprF` datatype! It's preserved out there as it was before; further yet: whenever you extend the `ExprF` with new alternatives, `TypedExpr` gains them automatically. By the way, `TypedExpr` above won't compile: you need to write the appropriate `Functor` instances first. Do that, it's simple! Once you understand the structure of this `TypedExpr` type, you'll be ready to go and read that *Datatypes à la Carte* presentation again :) Among other, the slides will show you some examples of *functions* on `Expr`, `TypedExpr` and alike (which I'm too lazy to reinvent here). How about a function `typecheck :: Expr -&gt; Maybe TypedExpr`, can you write it? :) Remember: your own typesystem need not be mindbogglingly complex; start with just two types, `TInt` and `TString`. Having implemented that you will have the necessary detail to understand the slides (here I didn't explain the core idea) and much more other literature. And there is some good literature appearing out there. 
I think you meant `x &gt;&gt;= f = join (fmap f x)`
Pretty much the very point of lenses is that they compose, so I don't see how you can get what you want there.
I consider it to be very useful to be explicit about imports from non-mainstream libraries and not merging everything into the current namespace: import MyRarelyUsedLibrary (symbol1, symbol2, (&lt;!!&gt;)) This way grepping the current file already gives a hint where the symbol originates from. 
I start up ghci: ghci&gt; :info Num class Num a where (+) :: a -&gt; a -&gt; a (*) :: a -&gt; a -&gt; a (-) :: a -&gt; a -&gt; a negate :: a -&gt; a abs :: a -&gt; a signum :: a -&gt; a fromInteger :: Integer -&gt; a -- Defined in ‘GHC.Num’ Note the last line.
Yes, after seeing this written out I see that my update isn't actually a composition at all but more like, perhaps, a join. Compose is like (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) but I need (a -&gt; b) -&gt; (a -&gt; b) -&gt; (a -&gt; b) for things passed to update/modify. I'm not sure how practical it would be to enforce that only functions joined this way are permitted. I guess I would have to tag every lens-like thing with the record it operates on somehow and that must surely break compatibility with lens. 
SBV seems like a really awesome library, and I've played with it a little bit. Does anyone have any examples of using it for something serious, in "production"? (I don't doubt that it has been used or anything, I just would love to see examples of what people do with SMT solvers in Haskell!)
It depends on your objections to lenses. If recompiling all of the `lens` package has got you down, you're in luck! `Frames` doesn't depend on `lens`, and even various demo programs use `lens-family-core` which is much smaller than `lens`. If that doesn't help, you're still in luck! You can use `rget` (defined in `Frames`) instead of `view`. If you want to modify a subset of the columns of a row... you'll probably want to use a lens. But I'd still preface that with, "You're in luck!" because it's on the lighter-weight side of lenses and yet is tremendously powerful. So if you ever choose to explore that kind of thing, it's there.
I'm sure you can invent an alternate implementation yourself, but anyway I thought I'd suggest [alignWith](http://hackage.haskell.org/package/these-0.4.2/docs/Data-Align.html#v:alignWith), or the one-off type-specific version that I'm sure we've all written at least once, `zipWithExtra :: (a -&gt; b -&gt; c) -&gt; (a -&gt; c) -&gt; (b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c]`.
This is such a cool idea I thought it needed highlighting. Sort of a "bestof" kind of post for the Haskell subreddit.
Math can help see the bigger picture, but you can learn and use the language just fine without.
Yes, sbv looks very cool. I have a question: is the intent to support the common part of the supported solvers or are there any plans for allowing to use many extensions from Z3 or other solvers? The point is that you cannot do much with plain SMT-LIB. 
None
To get good at writing programs, here's about all the math you need it's stuff you learned in high school: http://www.vex.net/~trebla/haskell/prerequisite.xhtml In other words, you need to know how to "plug things in" Haskell is used to examine some of those more complex abstractions you mentioned. But you don't need any of that stuff. In the process of learning Haskell, you may become interested in and distracted by the math stuff that other people are talking about (and that you mentioned above), because it's actually pretty interesting. But know that the math stuff is really a different conversation than learning how to write applications. You might need to know what the simple "interface" of a functor is, but not it's mathematical or category theoretic definition. Certain libraries have dizzying-high levels of abstraction internally; the abstractions might rely on certain mathematical concepts to achieve their level of re-use and composability or get other nice properties. Typically you don't need to understand all the internals in order to be able to use the library in your application. Just know that there is a mix of people who are interested in abstract programming language theory stuff and people who are interested in leveraging a powerful language to write cool programs. It's a bit confusing because sometimes the conversations don't make it clear when they are talking about theory or practice. 
Can you explain exactly how that works? I can't make sense of it at all. Is there any performance overhead if you access most of the list?
Can someone explain what this is doing?
Oh I see. It's allowing `(!!)` to walk over recursive calls to `sort` without having to fully evaluate them. Genius!
http://www.reddit.com/r/haskell/comments/2tfapq/the_blueprint_technique/cnyk06s Let me know if you need any more details.
You don't need to know any math. All of those fancy articles are not necessary to understand anything. They are the result of people having fun and thinking about things. But in the real world, you don't need any more math than any other language. The reason you see those articles isn't because they are necessary for learning haskell. it is because some people who have learned haskell have had a lot of fun writing and reading them. if that is not you, then don't worry :) Imagine a programming language where the community loves to talk about bunnies. Then you try googling the anatomy, taxonomy, physiology, breeding of bunnies and find it hard to understand. then you come back to the community and ask, "how much about bunnies do I *really* need to know to learn this programming language?" The answer is, of course, none. 
A good book describing similar techniques for designing and implementing efficient data structures with lazy evaluation is [Purely Functional Data Structures](http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504/ref=sr_1_sc_1?ie=UTF8&amp;qid=1422039661&amp;sr=8-1-spell&amp;keywords=purelly+functional+datastructures). 
`ghc -ddump-minimal-imports` helps to some extent...
If you just write `sort ls ++ [x] ++ sort rs`, to access any element of the list you have to compute enough of `sort ls` to know how many elements it has (for example `x` is the 0-th element exactly when `sort ls` is empty), and more generally its spine (it's structure as a list, without evaluating the elements). That requires going into the recursive computation of `sort ls` -- and actually sorting this sublist entirely. On the contrary, `withShape (sort ls) ls` has the same spine as `ls`. It is only when you force one of its elements that the spine of `sort ls` is computed until this element. This is done through the judicious use of a [lazy pattern](https://www.haskell.org/haskellwiki/Lazy_pattern_match) `~(x:xs)` for the first argument of `withShape`. This means that when accessing an arbitrary element of the result list, the spines of `ls` and `rs` have to be computed (this is a linear filtering cost), but the elements of `ls` and `rs` do not both need to be recursively sorted -- only the one sublist in which the element you're looking for belongs, or none if it is exactly the pivot element you asked. This is exactly the [Quickselect](http://en.wikipedia.org/wiki/Quickselect) algorithm, a variant of Quicksort designed to retrieved single elements rather than the whole sorted list. This implementation is thus a lazy quicksort version that "degrades", when you only access a few elements, to a quickselect. &gt; Is there any performance overhead if you access most of the list? Yes, `withShape` has the side-effect of copying the spine of its left parameter. It will allocate a bit more, but should be a small overhead (and only a constant multiplicative factor). In the general case (not in this implementation) you may lose some sharing properties of the returned data.
Very neat, this is exactly what I was looking for! Thanks.
&gt; Experimenting has revealed that withShape will always return a empty list if the second pattern is also irrefutable. ~~This sounds highly implausible!~~
This question came up just the other day! Or a related one at least. (http://www.reddit.com/r/haskell/comments/2swlt1/haskell_and_math_math_and_haskell/) I guess we should add this to more FAQs?
Ha
`wrap`?
On this note, I think there should be the following laws relating `Alternative` and `Applicative`: -- x * (y + z) = (x * y) + (x * z) x &lt;*&gt; (y &lt;|&gt; z) = (x &lt;*&gt; y) &lt;|&gt; (x &lt;*&gt; z) -- x * 0 = 0 x &lt;*&gt; empty = empty ... and also the right-distributive versions as well.
I would have quit haskell a long time ago if not for Dash. Edit: I notice you're using nix. [You will need Zeal](http://zealdocs.org). I got it to build eventually after some battles in Arch. Hopefully nix has a better formula.
Yes, I can think of a production use or two... https://github.com/GaloisInc/cryptol
&gt; The point is that you cannot do much with plain SMT-LIB. What gives you that impression? While people have used Z3 as a library to make impressive domain-specific solvers I don't feel like SMT-LIB limits you to a horribly small problem space.
Cool. What can I accomplish using this information? Edit: Nevermind, Tekmo explained it.
Oh I see! Thanks.
Frames isn't on hackage yet, so I doubt anyone's used it for real work as of now. This is just the first step, but it's an important one to get the ball rolling. As to your last point. you are absolutely right! I'll need help to figure out what's needed, and help adding what's missing. You can do quite a lot of manipulation with the pieces that are available, e.g. adding rows and columns should work out of the box. Pivoting would need a bit of thought, but I'd be surprised if there wasn't a way of doing it. A question there is if we want to synthesize data types for finite variants discovered in the data, or make the programmer write them down themselves. The answer will depend on the people who really give the library a go. 
I just love it when somebody answers somebody elses rhetorical questions.
How are you persisting the records? Is the issue that you're backend can't update two tables atomically?
That's what happens when I don't run the type checker. :(
Oh now I wish there was a programming language with a community that loves to talk about bunnies. 
Looks like Tekmo figured them out: http://www.reddit.com/r/haskell/comments/2tfgol/alternatives_convert_products_to_sums/cnynnyt
I know that feeling :)
I'm not convinced Rust got it right either. It seems like `throwError` should be the appropriate way to implement [`break`-like functionality](http://www.haskellforall.com/2012/07/breaking-from-loop.html). The reason I say this is that then you implement it within the language and there's no ambiguity about what the enclosing scope is. As a bonus, if you use `throwError` then you can catch the break attempt and handle it with `catchError`.
&gt; That thread was before my time, I think. Nonsense, you've been here for much longer than I have! I think it's more likely that I'm misremembering things and that the thread doesn't exist.
I remember reading about the `MonadPlus` law controversy when I was first learning Haskell.
Your comment.
I basically think of Applicative/Alternative as being */+ in an indexed semiring/seminearring. For that reason you might not want to bolt down the laws intended. I'd be interested to see if there are any ring-like models of Alternative—e.g. Alternative instances which add `inv` such that `x &lt;|&gt; inv x = empty`.
Have you tried [uniplate](http://hackage.haskell.org/package/uniplate)? (Not sure it's the solution. I'm curious myself.)
Ya. I'm pretty sure that's because it hadn't been invented yet, by you.
&gt; But we are not going to say "use the List monad!" when someone asks for a container in haskell But, if someone asks how to implement non-determinism (like for a NFA), I will definitely tell them to use *the List monad*. Similarly, when someone asks for how to do error handling, I always tell them to use *the Maybe or Either monad*.
On a related note, for a long while there was this strange obsession with unicycling in the Haskell community. I think shapr has a ton of pictures lying around of famous functional programming types trying to ride his mountain unicycle.
Addition and multiplication are sufficient. You can defer your mind==blown moment for as long as you want, until you find out how to add and multiply *types*. 
I fully agree. You don't perform IO with IO monad. You perform it using functions that return values wrapped in IO. The fact IO wrapper is a monad only allows you compose these functions nicely. That's it. The phrase "IO monad" is even incorrect English. Should be "the monad IO", same as "the class Person" or "the feature login".
Dishonest, I agree. On the other hand, it's up to you whether you value honesty. I used to be scared of `unsafePerformIO`, then I noticed [everyone else is doing it](https://github.com/search?q=unsafePerformIO&amp;type=Code&amp;utf8=✓), so I decided to become a moral degenerate.
We are working on translating a relational language (like Alloy) into SMT. Getting any performance out of Z3 requires using :pat or strategies, for example. I know that Z3 also has declare-rel, but it's specific to one of Z3 backends. These are lesser known ones.
Free / operational encodings next to an evaluator in IO? I'm not quite sure either, though.
As a related question, are there any resources that will teach you haskell along with the mathematical concepts they're based on? Most I've seen seem to focus on just the language.
This made me smile, thanks :)
Wow, its like someone discovered a new programming language hiding in IO. It reads surprisingly well too.
Just use a form of `Const` that carries a value in a ring. Done.
For what it is worth, this same kind of reasoning can be used to reason about a form of "contravariant" applicative functor and its alternative analogue: http://hackage.haskell.org/package/contravariant-1.2.0.1/docs/Data-Functor-Contravariant-Divisible.html There `Divisible` takes products to products contravariantly, and `Decidable` takes coproducts to products contravariantly.
lens' zippers are now in http://hackage.haskell.org/package/zippers , they work with everything that has a `Traversal` (everything that is `Traversable` and more).
Yeah it doesn't use the same name nor solve the same exact problem but it uses the same technique.
Especially in tutorials/articles. I see people often do it the other way, probably in the name of having clear/simple code without a massive scary import block at the top, but I think in the long run explicit imports really help when you're learning.
&gt; However when I look at Haskell online I see terms like the Howard-Curry Equivalence, functors, Kleisli Arrows, beta reduction, Church numerals, Category theory, etc etc, terms that I honestly haven't ever heard of before and whenever I google them I get to something that seems like it's very complicated and needs a lot of math. A lot of these ideas are either pretty trivial, pretty trivial when restricted to Haskell (instead of their full generality) or you really don't need to sweat them. For example, Church numerals and Curry-Howard are theoretically interesting but not terribly useful in day-to-day Haskell development, and you need to understand category theory to program in Haskell about as much as you need to understand [fields](https://en.wikipedia.org/wiki/Field_(mathematics\)) to do basic arithmetic. On the other hand, a Functor in Haskell is a fancy way of saying "some type that has a 'map' function": ghci&gt; map (+ 1) [1,2,3,4] [2,3,4,5] ghci&gt; map (+ 1) (Just 1) Just 2 ghci&gt; map (+ 1) (True, 1) (True, 2) Most things sound much scarier than they actually are.
That's not how that works.
I'll never understand how people can't live without semicolons and curly-braces. It's just more for you to type.
About Yesod. Scratch my last comment. I was looking at the wrong documentation. I am going to give it another go using: http://yannesposito.com/Scratch/en/blog/Yesod-tutorial-for-newbies/ and the examples in the book:http://www.yesodweb.com/book I'm impressed with what I see so far. I stand by my comment that it needs to attract some other types of developers. Great web platforms aren't built just around strong back-end code, but have engagement from front-end web devs too. That said, this is looking pretty promising.
How to declare an instance of a newtype using Recrd? The following compiles: {-# LANGUAGE NoImplicitPrelude, QuasiQuotes, DataKinds #-} import BasePrelude import Record import Record.Lens newtype MyInt = MyInt { myInt :: Int } instance Eq MyInt where a == b = (myInt a) == (myInt b) But if I replace the newtype statement with newtype MyInt = MyInt [r| { myInt :: Int } |] then the last line gives me two errors of the form "Not in scope: `myInt'.
You can only access members of a record using lenses. So instead of &gt; a == b = (myInt a) == (myInt b) you need &gt; MyInt a == MyInt b = view [l|myInt|] a == view [l|myInt|] b
I absolutely love that idea. Watching. Thank you!
Haskell is a lovely concise language and you can use it just like any other language for most tasks. Its abstractions are amazingly powerful when you want them, but can be understood in their own right (i.e. without recourse to maths books). Clearly, if you ever enjoyed pure maths, you'll experience more endorphins from Haskell than any other language.
Thanks!
Actually you're interpretation is correct. The reason I want to prevent updating joined records is because I'm fairly certain that some of the backends I will work with won't be able to support it. It seems that most of my question was about my own interface and somewhat less to do with lens at all. I should have thought more before posting. But I thank you for your prompt answers.
That's great news. I am really looking forward to trying it.
I would like to convince my colleagues (applied mathematicians / statisticians) to use Haskell. I can do without an additional cognitive barrier (and I am inclined to think that Lens the package is such a barrier).
To make it accessible to beginners, I'd like to stick to functions from the Prelude, so I simply to decided to change the problem and call the function `prefix`. :-)
I think a 10-15 minute session can get across the ideas required to *use* lens in basic form and read lens code (after basic Haskell proficiency is reached). At least this is my experience with one person I've taught to :) 
As an aside - once you get a few more functions in the pipeline, the second form will be shorter, since there will only be one pair of parentheses rather than one per function. You can also use ($) - which is just function application with low precedence to change it to h x = putStrLn . show $ x and save another character. IIRC it's considered good style to limit yourself to one ($) at the end of a line / expression. Although I'd eta reduce in that circumstance, because hlint has trained me to conform :) Edit: whoops, /u/Soul-Burn already referred to ($)
But much easier for an editor to manipulate automatically for indentation. 
structured-haskell-mode would've been much easier to write (and would be more reliable) had the Haskell world favoured explicit braces, semi-colons and parens. The current favored "state pretty much nothing useful without a full parser" is a PITA.
*Now* my appreciation for Haskell makes sense. Unicycling is one of those weird things I think is really fun!
This has been a time-saver so many times when I've been working with Python. Either the symbol is prefixed with the library name, or they have been explicitly imported at the top, so either way, it's just a grep away. You *can* import \* in Python which just dumps everything in the current namespace, but that's discouraged for good reason. I wish the same was true of Haskell imports.
&gt; In hindsite1, what I was trying to say in my last post The link is dead
``-XEmPortuguês`` ;-)
&gt; Tesser transducer I read in the last post that you didn't get the parallel benefits you were hoping for. Did you try making the accumulator a monoid instance? If you can do that then a mempty can kick off a fold on the chunks (I'm following the tesser explanation) and the serial reduction of the chunk fold output is then also a fold I think. And it doesn't even have to be serial if the fold is commutative. My gut feel is that mapReduce in haskell would be called foldFold! I think this is a Moore machine still, but one with early termination which is cool.
Very concise, just like Haskell ;)
Should also be "hindsight", unless I've missed some joke ...
This is a great way to learn some Portuguese; the comments are practically isomorphic to the Jav.. err Haskell code.
That's not my point. Taking value of first element of (..(a ++ b) ++ c ) ++ ...) takes time proportional to the number of appends and that's how most likely a random element of the list will look like.
They didn't even stick to SPJ form: main = do { xs &lt;- getLine ; let ys = map toUpper xs ; putStrLn ys }
You certainly have to through *O(log n)* appends to find a particular element of the list with `(!!)`, but in a way, the `withShape` function "hides" the other nested appends when applying `(!!)`. I may need to write a blog post about this. :-)
You missed an opportunity to use "How I Learned to Stop Worrying and Love the Monads" as the title...
I’m not sure it’s mostly a habit. I’ve met several people who fervently expressed their utter dislike of anything using whitespace instead of braces, so much that they stated they would not consider using a language for that sole reason. While it’s likely irrational, it still suprises me how often this comes up to be an issue.
The primary goal of Haskell's syntax should be to make it as effortless as possible for Humans to read and write, and avoid as much distracting visual noise (which I count curly-braces-for-indentation in) as possible rather than make it trivial to write parsers for... I guess LISP is an example for the latter end of the spectrum.
Definitely. Fully explicit Haskell would still be pretty hard to work with. Whereas in their simplicity s-expressions are predictable to read, and trivial to both write and manipulate. Haskell syntax results in code structure which is comparatively all over the place, not predictable, its syntax hard to write effectively and harder to manipulate. I can expand with examples if you're interested.
I've been playing around with a somewhat similar idea: treating a producer of values as a "fold consumer". You can see an [example of this here](https://github.com/Gabriel439/Haskell-Turtle-Library/blob/master/src/Turtle/Shell.hs#L83): newtype Shell a = Shell { foldIO :: forall r . FoldM IO a r -&gt; IO r } That has certain advantages and disadvantages. It makes it very easy to implement resource acquisition with deterministic and prompt resource disposal, but you have to fold the entire stream and you can't skip any values.
I have been lucky enough to see and work on a few large "functional" codebases over the years. Running major production systems, doing real work -- and they all ended up being mostly imperative code on top of some minimal 'core team' v1 functional leftovers. It feels like developers start procedural then learn to be more functional, but companies seem to go the opposite direction -- a core group builds something elegant -- then they need a bunch of features, hire a bunch of devs, who write massive 1000+ line blocks of imperative code -- and overtime those overwhelm the codebase, but get the job done. 
&gt; These are always equivalent. Keep in mind that GHC can only inline fully applied functions, so η-reduction may affect your program's operational semantics. (I'm not even sure whether η-reduction is allowed denotationally, but I can't think of a counterexample.)
It is very impressive that Haskell can do this. However, since I am much more familiar with R, equivalent code in R with dplyr package looks a lot simpler and intuitive for me. For instance, except for "3. Better Types" section, equivalent code in R with dplyr will look as follows. # using 'dplyr' package library('dplyr') # 1. data import u_col_names &lt;- c('user_id', 'age', 'sex', 'occupation', 'zip_code') users &lt;- read.csv('data/ml-100k/u.user', sep='|', col.names=u_col_names, header=FALSE) %&gt;% tbl_df() # to prevent printing too much information # 1.2 sanity check (same as the post) class(users) str(users) summary(users) # lapply(users, summary) # 2 subsetting ## 2.0 head, tail users %&gt;% head() users %&gt;% tail() users %&gt;% head(3) ## 2.1 row subset users %&gt;% slice(50:55) ## 2.2 column subset users %&gt;% select(occupation) users %&gt;% select(occupation, sex, age) ## 2.3 query / conditional subset users %&gt;% filter(occupation == "writer") ## 2.4 int_doubler &lt;- function(df1){ df1$user_id &lt;- 2 * df1$user_id df1$age &lt;- 2 * df1$age df1 } users %&gt;% slice(1:3) %&gt;% rowwise() %&gt;% int_doubler() # or users %&gt;% slice(1:3) %&gt;% rowwise() %&gt;% { .$user_id &lt;- 2* .$user_id .$age &lt;- 2* .$age . } 
&gt; I may need to write a blog post about this. :-) Probably. :) Considering that the classical algorithm for geting k-th element of sorted set is a little more complicated than this (median of medians). 
In a way, yes, but that's not exclusive to programming. Math will hone your problem solving skills, it will make you better at recognising patterns, and most importantly it will give you a huge toolset to model and solve problems with.
What if you need polymorphism on the other side? Ex: class SillyParse a where sillyParse :: ByteString -&gt; Maybe (String, a) instance SillyParse Integer where sillyParse bs = (,) "I'm an int" &lt;$&gt; parseInteger bs instance SillyParse Bool where sillyParse bs = (,) "I'm a bool" &lt;$&gt; parseBool bs 
It is a *thing*. It's not first-class in Haskell; confluence makes it unnecessary to pass them explicitly and incorrect to build them dynamically. Things similar to type class instances are first-class in Idris and Scala, but confluence isn't a goal there -- rather it is "just" principled ad-hoc overloading.
Folding the entire stream is an acceptable compromise when building a "shell" interface to external processes, as it helps you avoid deadlocks caused by full output buffers.
what is a universal algebra? I thought just an "algebra" meant some stuff with functions on them and laws between the functions, but that's informal. what's the "universal" part? (http://en.m.wikipedia.org/wiki/Universal_algebra seems to agree, maybe) thanks
&gt; state pretty much nothing useful without a full parser IMO, trying to reformat / auto-indent / etc. code without a full parser is like trying to extract values from HTML with regular expressions. Sure, it might work some of the time, but it's also a good way to [summon](http://blog.codinghorror.com/content/images/2014/Apr/stack-overflow-regex-zalgo.png) [Cthulhu](http://stilldrinking.org/programming-sucks).
In general, I don't like whitespace-sensitive languages, but Haskell's layout rules have not failed me, yet.
In math an algebra is roughly a vector space and a product. It you generalize the definition by accepting lot of other operators and functions then you get an universal algebra. Here universal just mean general. 
Hey, at least it isn't full of IORefs.
I used to call call DSLs Dog-Shit Languages. They were the bane of my existence. I worked on this large system which appart from a massive C++ API includes about 19 different little (incomplete, buggy, constantly-changing) languages for configuring, passing messages and scripting the system. In fact, the reason I came to Haskell was hoping to write little compilers to turn all those dog shits into Lua. A fresh start. In the process I learned about these EDSLs which I really quite like. They allow you to make a purpose-built vocabulary to describe your domain while retaining the full power of the host language (if you want) and not having to reinvent any wheels. The tragedy of Haskell, for me, is that as a mighty sword it is heavy to lug around. If I wanted to use it as an embedded configuration, messaging or scripting language then I would be the only person trying to do any of this. Haskell does not want to replace Lua.
That's a lot of code just to write something at the type level. Hopefully there would be a way to let template haskell handle much of this if it is a useful enough design pattern?
Which part is a lot of code? The *library* or *client* code? The *library* code is significantly shorter than the previous post. The *client* code is mostly just instances of the `InputData` and `ReturnData` type families, which need to do a type-level pattern match across each `CRUD` verb and each type in the universe `k`. I'm not sure if template haskell could help here. It may seem like boilerplate but those types will be defined *somewhere*, explicitly or implicitly, whether you're using the library or not.
Is it just me or is all the terminology confusing? :P At times there is a clearly defined and standard terminology used, like Moore/Mealy machines, and of course Haskell terminology (monad transformers, etc). But then this terminology is intertwined with other terms like "fold", "transducer", "transformer", "reducer", "step", "producer", "consumer", etc. It feels confusing at times. For instance, in this context the term "fold" is equivalent to "Moore machine", but in any other context it's actually equivalent to "catamorphism". Does that mean catamorphisms are Moore machines (somehow)? Another example I find is "transducer". Are they actually FSTs ("Finite State Transducers")? Just by looking at the types and applications it's hard to figure out. Is there a clear guide on how this terminology works (and relates to other more standard terminology and concepts)?
If you see the symbol in a blog post then I don't know of a better search than old-fashioned Hoogle, but often it can't find things by name. That happens to me all the time. :( However if you're editing code in a Haskell project you can use the tools codex and hasktags to let you jump to definitions of symbols within your cabal sandbox. In my dotfiles I bind the &lt;C-]&gt; key to jump to the definition of the symbol under the cursor. https://github.com/begriffs/haskell-vim-now Finally you can build hoogle to run locally on the command line. There's an option to build an "extended" index which may be bigger than the one online. (it sure takes longer to build!) https://www.haskell.org/haskellwiki/Hoogle#Database_Creation
Thanks for sharing! (This subreddit is relatively friendly to self-submission, it seems; I know I've gotten away with it before.) IMO, this combinator-style API is one of the great treasures of functional programming, and I think it's worth drawing attention to. /u/Tekmo has a great post on this kind of thing as well.[0] [0] http://www.haskellforall.com/2014/04/scalable-program-architectures.html
There's Hugs. Hasn't been updated in a while, but it's running Haskell98 just perfectly. What would also be nice would be a Haskell interpreter written in Haskell: Forego all those extensions, just have the standard, plus an automagic FFI to compiled Haskell, and *minus* the standard library. In particular, rip out `IO` and `unsafe*` to make it safely embeddable. Applications could then ship with that and their own Prelude, to do scripting-type stuff, just like you'd include lua: Parser, type inference and checking, maybe compile to bytecode, interpret. 
Composition is the way to chain functions. Monads are a way to chain functions were the output value is wrapped in something.
There's an LLVM-based one in the works, but still WIP: https://github.com/Lemmih/lhc
&gt; type instance InputData 'Read ProductCRUD = Product &gt; type instance InputData 'Read OrderCRUD = Order Not really important for your post but are you sure about these? Shouldn't read get some kind of key as the input (similar to delete) instead of a full value?
Good call. Updated. Unfortunately, even type-level programming won't save you from bad design :)
There are two things I hate about this style: the trailing character to close, and the different character to open. main = do { blah1 ; blah2 } xs = [ 1 , 2 ] module Foo ( a , b ) where I wish that the separator could be placed at the beginning, and a OTBS sort of brace style used instead: main = do { ; foo ; bar } xs = [ , 1 , 2 ] module Foo ( , a , b ) where 
I may be missing something, but isn't this basically the use-case for [lens](https://hackage.haskell.org/package/lens)?
Não mesmo... alguém postou na A.P.D.A. Mas concordo contigo.
EDIT: No. AFAIK lenses don't provide extensible tuples and records, and they don't autogenerate for all data without template hacks. Maybe? But this is more about rounding out the expressiveness of the type system. We encode the numbers with underscores when we pattern match, why not just allow us to use a literal? 
&gt; In math an algebra is roughly a vector space and a product. That's only in one area of mathematics. "Algebra" is a far more general concept and it's a terrible shame that linear algebra decided to name one of it's objects that. 
The composition operator becomes useful when you want to define a function in order to use it on its own. mapM_ (putStrLn . show) someList If you're using the function straight away it is clearer to just write your first version putStrLn (show x)
Initially I tried to use GHC's parser, but the AST generated by the GHC API contains ⊥ for things that are filled in later in the process. I was unable to get around this issue at the time, it prevented my attempts at a generic traversal to generate source locations for use in Emacs. So I chose haskell-src-exts. In practice, after using it for 2~ years, it can handle 95% of the code out there. When I hit the 5% I put up with it. Alan Zimmerman has been working on fixing this landmine issue in the GHC AST, which I think should be fixed in GHC 7.10. So eventually my tooling will probably switch over to using GHC's parser, once 7.10 is established.
That's an interesting point. Defending myself, though, sometimes the type signatures really are redundant - they specify the same most-general type that the compiler would infer anyway. Whereas potentially explicit delimiters can express something that indentation can't - alternative solutions of the what-does-this-indentation-mean problem, such as... { { { some_c_code_in_pointlessly_nested_blocks (); } } } As "pointlessly nested" suggests I can think of no sane reason you'd want that, but it's still a valid interpretation of an indented block (possibly assuming it's indented by at least three spaces). Maybe there's some language where it might even be meaningful. Anyway, the argument I'd use to make your bad-analogy point is that an explicit signature is a more direct representation of information that's already potentially present, but hidden in what could be a complex type-level computation. That has no parallel for block delimiting - indentation is already a particularly visually direct way to represent block structuring. Really, the point was only that redundancy can be potentially good. The analogy wasn't meant to be taken very far. 
It's not even the most funny thing from that code, actually 
Thanks! That Vinyl link was insightful. I wasn't familiar with Type Families. My "r @ x" was what the [documentation](https://www.haskell.org/haskellwiki/GHC/Type_families#What_are_type_families.3F) calls "type indices yielding a type".
You just proved what I've said.
Those questions are easily answerable, but that's not the problem. The problem is, that they come up all at the same time, thereby causing confusion and perpetuating the stereotype that Haskell is a language that's hard to get into. The latter may be true regardless, but it is at least making things harder for no good reason.
Oh, goodness, I suppose I really didn't make things clear enough! There are many indexing schemes possible in the Haskell version. We could use numeric indexing, or even a Vinyl record of getters that we then apply en masse to a Frame row by yanking the reader context out of the record of getters for something very like your multiple column selection. It would look something like `users &amp; select (occupation :&amp; sex :&amp; age :&amp; Nil)` where `select` is some combination of `rtraverse` and `rget`. I wrote those examples the way I did to address my biggest issue when using R: that when something's not working I can't just write down the types I think things have. My next biggest issue is that when I select a particular column, I feel like a piece of software should resolve how that indexing should work. In `Frames`, column selection and subsetting are `O(1)` operations. When you've got data in memory, everything is as densely packed in memory as possible, and indexing doesn't involve any lookups. I appreciate your feedback on these things a ton! Earlier feedback from Ben Gamari spurred the `pipePreview` helper which I think is a step in the right direction to offer shorter syntax for common operations. We have some statistics and charting support that you can see in the demos, but they're not as nice as what's available in R. The problem in writing this library is that different folks have different pain points, so contributions aren't just welcome, they're essential!
There are some exercises for LYAH [here](https://github.com/noelmarkham/learn-you-a-haskell-exercises).
Thanks I'll check it out! I also found [The Haskell Road to Logic, Math and Programming](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.9312&amp;rep=rep1&amp;type=pdf). I'm working through it now and it's exactly what I was looking for.
As I recommended [in the thread](http://www.reddit.com/r/programming/comments/2tf17i/io_monad_considered_harmful/cnz9mwc?context=3) in /r/programming, I think 'action' is a nice pedagogical stepping stone from using `do` notation for `IO`, to using it for other types' monads, without calling them monads. For example, you can quite naturally talk about `IO` actions, `State` actions, `STM` actions, `Handler` actions, and all these things are expressed elegantly using `do` notation. At no point do you have to say that `do` notation is for monads - you can say it lets you write all different types of actions.
The GADT is there to add extra safety. The `HasInfo` and `NoInfo` tags exist only to prevent the invalid case from being type checked. Basically, the GADT is defining a closed set of ways by which you can construct a `Property a`, and `a` can take only one of two values: `HasInfo` and `NoInfo`. Compare this with just `data Property a = Property (Maybe Info) a` where there's nothing stopping me from using the constructor to create a `Property HasInfo` while not actually providing any `Info`, or even something like `Property Int`.
&gt; the GADT is defining a closed set of ways by which you can construct a `Property a` Yeah, I've never found this to be particularly useful in general. I prefer to limit the ways you can *use* a datatype rather than limit the ways you can construct it. The former seems easier to encode with simpler functionality. Note that under my suggestion `Property Info` corresponds to Joey's `Property HasInfo` and `Property ()` corresponds to Joey's `Property NoInfo`. &gt; Compare this with just `data Property a = Property (Maybe Info) a` where there's nothing stopping me from using the constructor to create a `Property HasInfo` I'm not sure what this is in response to. My suggestion was nothing like that. In my suggestion the `a` is where you store (or don't store) the `Info`.
&gt; I can expand with examples if you're interested. Sounds like a great topic for a blog post!
This is a great use case for GADTs and type families. Very nice! Any reason you are not using *closed* type families here? I.e.: type family CInfo x y where CInfo HasInfo HasInfo = HasInfo CInfo HasInfo NoInfo = HasInfo CInfo NoInfo HasInfo = HasInfo CInfo NoInfo NoInfo = NoInfo type family CombinedType x y where CombinedType (Property x) (Property y) = Property (CInfo x y) 
Plotting charts you should be able to do now. I'm always plotting charts in the repl and when I have something that needs a lot of cpu, I just compile the code I am working on. That's not the same as ggplot2 of course but you can some of the things that ggplot2 can e.g. https://idontgetoutmuch.files.wordpress.com/2014/06/044408c77f048f73.png?w=1560.
I am using it now to do a fun (not work) analysis. I'll let you know how I go. I do agree with @repoptrac that currently the R is easier to read.
I've used R and Pandas for work but not for anything very advanced. I am probably going to use this in future. BTW I'm disappointed about your experience with Julia but I am also sad that the inventors of Julia didn't try see if they could use Haskell a bit more aggressively (clearly they looked at it; I recall seeing a video where one one of the inventors said they rejected it because the type system would put off too many applied mathematicians something I have witnessed first hand).
Of course it can be use with GHCI. The error message is quite clear. Since the `person` declaration in the demo lacks a type declaration for demonstration purposes, the compiler infers its type using `Integer` for integer literals. At the same time, `getPersonBirthdayYear` expects a specific type `Person`, which uses `Int`. To fix your issue I suggest to specify the type signature manually: `person :: Person`. However I must note, that on 7.8 I was unable to even reproduce your issue.
&gt; But often we work in the ideolized haskell Yes, that's the world I live in ;-)
Good save! Supplementary question: given that statically typed pure functional programming is the mathematically demonstrable correct answer to “programming”, so I'm told, and anyone who chooses not to use it is therefore an idiot, and even though haskell isn't actually the most capable implementation of that paradigm but never mind that, why *isn't* the haskell site implemented in haskell? /sarcasm Why doesn't the haskell community eat its own dogfood on its own most prominent site? Why not show the rest of us idiots how its done? /really not sarcastic at all. Really.
https://www.youtube.com/watch?v=L3LHAlcrTRA#t=45
I'm quite sure that I tried that and it didn't work. I forget the details of that attempt, and it could well be that I didn't need the GADT, but did still need the type families to make property combinators work. (I assume type families could be used over Property Info and Property () ) In what way is a GADT more heavy-weight though?
YMMV, but I did exactly this and got bitten hard because I put constraints on my instances, which worked just fine in 7.4. 7.8 disallowed this because it violates a coverage condition and I was left with a massive refactor on my hands. TLDR: Only do this if you're sure that no `Combine` instance will ever need type-class constraints.
You know, /sarcasm doesn't magically cure strawman arguments.
I'm curious what Haskell has done to you to make you troll its subreddit looking for down votes and justification to hate people who use it.
The amount of side arguments you inject in our pure conversation is horrifying. Please only use logical argumentation methods that are not fallacies. Perhaps you should read some more papers on it. (Yeah, this response is tongue-in-cheek, too, but at least my point is *valid*.)
But is it *sound?*
Do this instead: cabal install -j hakyll That will run on multiple cores. Also, make sure that your default is set to `documentation: false` in your `.cabal/config` file, or set that in the local `cabal.config` file in your sandbox. Aside from that, you are right (i.e. what you imply by posting your complaint in this particular reddit thread) that it could be just a transient problem with Hackage being slow, or a network problem. Check that by going to the human-readable [front end of Hackage](http://hackage.haskell.org/packages). In the end, a fresh sandbox does take some time to build when you're building something heavy like a web app. This is open source, so you're compiling everything from scratch. Be consoled that you rarely have to build a sandbox from scratch - usually only when you start a new project. And with time, you'll learn tricks like sharing sandboxes between projects with similar sets of dependencies that will speed things up for you.
Oh, come on, you know that a minority of over-excited, overly-loud, over-represented fanboys exists in the haskell community as in every other.
Hmm. Both of those type-level functions look very familiar. Type-level boolean algebra or type-level applicatives, anyone?
GHC is also LLVM-based if you want it to be :)
I don't care about, for example, C++ (these days) but I'd quite like haskell to be more influential than it currently is.
Fyi, the new site we're intending to cut over to is in fact written in Haskell.
See here: https://www.reddit.com/r/haskell/comments/2tlska/haskell_web_site_is_really_really_slow/co0ftll tl;dr: https://wiki.haskell.org/Haskell almost works, and we'll cut over soon. **update tl;dr:** We've now cut over to wiki.h.o. Please report any problems :-)
That's parsing, not typing.
There's a difference between your implementation and mine! Mine differentiates between a scalar and a vector of size 1, whereas yours does not. Neither is better than the other, but I personally prefer maintaining the distinction.
When I tried haskell ~3 years ago (before even cabal sandbox) I had such a bad time getting anything installed that I gave up. Haskell was super fun to use on ghci but I really didn't understand how to get something larger working because no library would be compatible, and nothing would build out of the box. This time round I'm using nix [1] and its new haskellng [2] infrastructure and it is really amazing. I can focus on learning, almost everything installs, and once you build a cabal package the binary output gets re-used in other nix-shell sandboxed directories. [1] http://nixos.org/ [2] http://lists.science.uu.nl/pipermail/nix-dev/2015-January/015591.html
The link for `yarr` doesn't include Haddocks. [Here's a link to hackage to save some keystrokes.](http://hackage.haskell.org/package/yarr) I don't think familiarity with `yarr` is necessary for this post, thankfully. *Edit*: Well, that doesn't work either. All the module doc links are broken. The most recent version with working docs is [1.3.2, which is a year and a half old](http://hackage.haskell.org/package/yarr-1.3.2). Weird.
The only person who has been over-excited and overly-loud in this conversation is you
How do you run the examples? There are a lot of modules that can't be found when I try to load them in cabal repl (e.g. ListT and Lens.Family). Are you using a sandbox where you've installed some more libs?
/r/iamverysmart
Yes, used it a lot on high school! Some of those systems are not linear, though... only the plane case, afaik. Correct me if I'm wrong.
In the light of [2] above, do these [setup instructions](http://www.reddit.com/r/haskell/comments/2mv3vo/nix_the_cabal_purgatory_howto/) still apply? I'm asking without deep understanding as someone who just wants to try one day.
Is there any chance you could replace the fibonacci code with factorial? I'm not keen that the first Haskell code someone sees is an unnecessarily badly non-linear implementation.
There's also jhc; not sure how that measures up
Source and issue tracker: https://github.com/haskell-infra/hl
The only functional dependency-related change in 7.8 that I'm aware of was to disallow instances that violated functional dependencies by themselves (https://ghc.haskell.org/trac/ghc/ticket/1241). That should not be affected by constraints on the instance though. If you were getting a coverage condition violation from instance constraints then all you needed to do was turn on UndecidableInstances.
Something like this perhaps? http://www.reddit.com/r/haskell/comments/2szzu0/composable_streaming_folds/cnupity
 http://rationalwiki.org/wiki/Straw_man &gt; A straw man is an intentional misrepresentation of an opponent's position, often used in debates with unsophisticated audiences to make it appear that the opponent's arguments are more easily defeated than they are. [1] Unintentional misrepresentations are also possible, but in this case, the individual is guilty only of simple ignorance. While their argument would still be fallacious, they can be at least excused of malice. Since your posts seem to be completely void of any reflection or thought, the above may help. At best, given said above, you're a simply an ignoramus.
runs on haskell
&gt; It's commonly used to solve sets of linear equations Only in the sense that some modern algorithms are based on it.
I agree, for the most part, with the article. It makes me ask the question, though, what is it that would make a good dependently-typed API or interface. Seems to be an open question, but I would be interested in ideas. 
&gt;Or is there a name for the problem? Expression problem? http://kievfprog.net/talks/oleksandr-manzyuk.pdf https://oleksandrmanzyuk.wordpress.com/2014/06/18/from-object-algebras-to-finally-tagless-interpreters-2/ http://en.wikipedia.org/wiki/Expression_problem http://hackage.haskell.org/package/HList http://okmij.org/ftp/tagless-final/JFP.pdf http://okmij.org/ftp/tagless-final/ http://lambda-the-ultimate.org/node/2438 
Then contribute a useful application written in Haskell
I provide an example here: https://groups.google.com/d/msg/haskell-cafe/5B4DCN3lqi4/pQw_O7Y_rfoJ
You can use the `Data.Typeable` module along with `Data.Type.Equality` to check arbitrary (typeable) user-provided types at runtime. {-# LANGUAGE GADTs #-} import Data.Typeable (eqT) import Data.Type.Equality ((:~:) (Refl)) data Object a where Object :: Typeable a =&gt; { objID :: ObjectID } -&gt; { objDataType :: VariantID } -&gt; { objData :: a } -&gt; { objPredicate :: PredicateCondition -&gt; Object a } -&gt; { objJoin :: Object a -&gt; Object a }, -&gt; { objGenerate :: (IR parameters and result) } -&gt; Object a data SomeObject where SomeObject :: Object a -&gt; SomeObject eqObjectTypes :: Object a -&gt; Object b -&gt; Maybe (a :~: b) eqObjectTypes (Object _...) (Object _...) = eqT joinSome :: SomeObject -&gt; SomeObject -&gt; Maybe SomeObject joinSome (SomeObject a) (SomeObject b) = case a `eqObjectTypes` b of Just Refl -&gt; SomeObject (objJoin a b) Nothing -&gt; Nothing
Ah, the English robot trapped in a German's body. It's also worth checking out [twittner's](http://hackage.haskell.org/user/ToralfWittner) other libraries, such as: * [redis-io](https://github.com/twittner/redis-io) * [wai-predicates](https://github.com/twittner/wai-predicates) * [wai-routing](https://github.com/twittner/wai-routing) * [swagger](https://github.com/twittner/swagger) Alot of his code (including the aforementioned cql-io) powers the backend of wire.com, top notch stuff! 
Noob here. What does ANN stand for?
Announcement
Wow, the composition-of-functions representation in the "Finally Tagless" paper is really cool. This is good food for thought. Thanks!
Announce. It's a new library and he wishes to announce its existence to the world (or at least to the Haskell community here on Reddit).
Thanks! I'm not sure why I missed Data.Type.Equality before.
I had to install lens, foldl and lens-family ymmv.
Have you had a look at the dependencies in Hakyll? It's a fairly large and complex piece of kit, with all sorts of pieces to render, including on the fly in development mode with the snap framework. That said, I do recall it taking a long, long time to build. 
Great. I was intrigued by this when running across it previous, with some "watch this space" documentation. One thing that's not clear to me is whether or not private storage is a requirement for getting an app with a large sandbox off the ground.
You might be interested in the ongoing discussion regarding the proposed OverloadedRecordFields extension: see [this ghc-devs thread](https://www.haskell.org/pipermail/ghc-devs/2015-January/008049.html) and the [relevant GHC wiki page](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Redesign).
Wow! Many thanks for all your effort and encouragement! I will follow your advice and will start re-read the sections about Functors (in Learn you a Haskell) and implementing fmap. Then I will see if I manage to follow-up with the next steps that you outlined - but your example of anotating an untyped Expr with types is very intriguing, and at some point I will need annotations for parts of my AST (though not for types). Haskell community rocks. And while learning Haskell is really hard for me, it gives this feeling of living on the edge and discovering techinques I never imagined.
A monad is like a soap bubble :) however there are may different kinds of them. Each time you cross the wall you might change its meaning. Bubbles at the same level of the same kind join together, thus there is no wall between them. In Haskell all lives in one big bubble IO. When stuff leaves through IO you can actually see it; and there is only one IO bubble per program... Would people still say "Bubbles" are "scary"? or can we do without them? 
There are a lot of times when you shouldn't use a typeclass the same way you would use an OO interface. I made the mistake pretty early on, only to realise that `data Foo a` was usually better than creating a type. But I disagree that it's always the case. Foldable/Traversable ring true to me as typeclasses implemented in an OO fashion. If we look at Java they have OO classes for say a LinkedList, but it implements Collection, List, etc. That's no different than Data.List implementing Foldable/Traversable. And when we have transformers things are way easier because there are classes. Your custom Monad can be an instance of MonadState, so that it can use `get` or `put`. I know it's not identical to why you create interfaces in OO, but the principal is the same. When your abstraction is generalisable to multiple concrete structures, you create an interface that can be used by an implementer without having to worry about which instance they use.
Should have gone for a Buzzfeed style clickbait title, like "10 things I learned about Monads that will change the way you look at Haskell forever"
Only for the actual nix system part. The "old" Haskell way still works but I'd really recommend haskellng. For haskellng [1] is interesting. The only downside is that this really is bleeding-edge. I.e. you'll have to use the latest tools from HEAD instead of the stable 14.12 release. If you have a checkout of a new-ish nixpkgs you can install a new cabal2nix which you'll need for [1] like this: nix-env -f . -iA haskellngPackages.cabal2nix [1] http://stackoverflow.com/questions/27968909/how-to-get-cabal-and-nix-work-together
Thanks for the link. That's what I asked for - some kind of instruction for beginners.
Someone submitted a link to this comment in the following subreddit: * /r/SubredditDrama: [Progammers can get catty too - some nerd flavoured popcorn from /r/haskell](https://np.reddit.com/r/SubredditDrama/comments/2tpmxf/progammers_can_get_catty_too_some_nerd_flavoured/) ---- This comment was posted by a bot, see /r/Meta_Bot for more info. Please respect rediquette, and do not vote or comment on the linked submissions. Thank you.
Actually, it's an enormous performance win -- see the [MLton compiler for SML](http://mlton.org) for an example of what it can do. However, there are two big issues with it you have to take into account: 1. First, taking real advantage of whole-program compilation tends to require some serious changes to the architecture of the compiler. Many of the algorithms you want to use are naturally superlinear, and so just running them blindly on multi-MLOC program loses even on modern machines. So doing this without killing performance requires a lot of thought -- MLton, for example, does a very early dead-code elimination pass before trying to run its expensive optimization algorithms. 2. Second, the whole-program assumption makes dynamic loading incredibly hard to implement, because dynamic loading basically gives you a bunch of code at runtime, and a whole-program compiler wants your whole program at compile time. You can live without dynamic loading for small-to-medium-sized programs, but essentially all large programs have a plugin architecture that requires dynamic loading. In theory, it's possible to work around this by compiling static whole-program analysis with JIT compilation, but AFAIK this is hairy enough that no one has done it. (Not even for languages like Java or C#, which have *vastly* more resources aimed at their compilers than Haskell or other academic research languages.) 
The only problem with nix is it is really not beginner friendly. I find it hard to write nix expressions even though I've done a good number already. Really wish the nix expression language was a haskell edsl we could experiment with in a repl. 
&gt; Actually, it's an enormous performance win Is it? I haven't been following Mlton closely, but my impression was that the performances were mostly in line with OCaml, which does not do whole-program optimization (except for a rather minimal bit of cross-module inlining) but capitalizes on clever data-representation choices and a relatively good runtime. Maybe whole-program optimization is better at some kind of code than others. In which application area would you guess it could bring improvements to GHC (which has pretty sophisticated optimizations already)? My guess (out of the thin air of course) would be that GHC (or OCaml) are already pretty decent at symbolic code (compilers, etc.), with a lot of allocation and pointer-following, and that the area in which to invest would be numerical code and tight loops, where a good support for numerical optimizations, SIMD etc. could make a large difference -- and indeed that's where GHC LLVM backend really shines. Or maybe the advantage of whole-program optimization is not really on speed per se, but on the fact that it is good at making abstractions cost-free, or avoiding some runtime-helping techniques that make interoperability harder? I find it impressive for example that MLton is able to perform well while keeping its integers unboxed and untagged for example, which is definitely a plus for easy foreign-function interfaces. Rather than a performance boost, would whole-program be a good choice for being a "good citizen" or going towards system programming?
Could you please link to some benchmarks showing the resulting code size and performance comparisons?
If you write a first-order program (whether functional data destructuring or imperative array-bashing), my expectation is that GHC, Ocaml and MLton will all perform about the same, since the program's behavior is pretty much locally determined anyway. If you write higher-order programs, I expect MLton to dominate in the performance game, since using HOFs doesn't cost *any* performance. In contrast, Ocaml and other ML compilers don't really do very much optimization of them, and so every higher-order function costs you. IIRC, MLton typically generates code roughly 3x faster than SML/NJ, which I expect to be competitive with Ocaml. Now, GHC does do some optimization of HOFs (especially via fusion), but it generally won't optimize when you pass closures to higher-order functions. So there should still be a delta, but I'd expect it to be smaller in this case.
I got a sense of its weight when I was installing it. While I am impressed by the discipline and the code-fu of the author, I just did not have the patience to wait to simply set up a toy project to play with. I will be checking out nix with /u/realteh's earlier [post](http://stackoverflow.com/questions/27968909/how-to-get-cabal-and-nix-work-together) to see if that can make this a 5-minute ordeal.
Yeah, that would be great. However, it seems to me that nix has some abstractions that are ideally suited to its domain. I'm thinking of the way arguments are treated (defaulting, variadic arguments), or the patterns that make easy to write expression (the callPackage pattern, the override pattern etc [1]). It's not that these couldn't be replicated, but wouldn't we end with an almost literal translation, complicating what is rendered simple by nix syntax? By the way, for the repl, check out `nix-repl`. On the other hand, I absolutely agree that experimenting with the repl should be the way to learn nix. Luca Bruno does a bit of that in the series below, but I'm still dreaming of the equivalent of lyah for nix. [1] I recommend Luca Bruno's series on nix here: http://lethalman.blogspot.it/2015/01/nix-pill-18-nix-store-paths.html
This works for any traversable which is an explicit fixed point: http://hackage.haskell.org/package/fixplate-0.1.5/docs/Data-Generics-Fixplate-Zipper.html 
&gt; a fresh sandbox does take some time to build when you're building something heavy like a web app Really? As an outsider looking in at Haskell as a possible replacement for a Scala webapp, if I have to wait more than 2 minutes to do a clean build (what the current 25kloc app takes in the slowest case), then my interest has seriously waned. I mean, I get that Haskell beats the pants off of Scala in a number of areas, but in the end as developers we have to produce, not wait to produce. At the least incremental compilation is an absolute must (Yesod has that out of the box apparently).
Someone, pay this man a full time salary and remove his quota on tools development. :) 
&gt; Mathematica's system doesn't work in general. Put any system like x5 - x+1 and the symbolic solver will choke. It justs uses a bunch of ad-hoc solvers and simplifications for different systems of equations, there is no general algorithm that underlies it. If you can refine your problem space then the there likely exists some algorithm that works in a small amount of cases, but computer algebra is a very ad-hoc and imprecise field. Categories of equations where solvers exist for some equations are: &gt; I find it funny/interesting that there is nothing general that works. I guess there are many researchers trying to come up with such a system, right? Do you know some papers I can see what they are doing?
Currently, you should be able to use Stackage as a replacement for Hackage, by setting the [`HALCYON_CABAL_REPO`](https://halcyon.sh/reference/#halcyon_cabal_repo) option. However, LTS Haskell is a `cabal.config` file which includes version constraints for every package in a particular Stackage snapshot. Additionally, the file includes `installed` constraints for packages provided with GHC, such as `base` — and not literal version numbers. This is completely different from `cabal.config` files generated by `cabal freeze`, which is what Halcyon is prepared to handle right now. Halcyon expects a `cabal.config` or a `.halcyon/constraints` file which includes version constraints only for the packages which actually are dependencies of your app. A hash of these constraints is used to locate the right sandbox directory for your app. Additionally, Halcyon uses the `base` package version number to determine the right GHC version for your app. If you use a particular Stackage snapshot as your Cabal repo, and generate a constraints file using either `cabal freeze` or `halcyon constraints`, you will effectively be using LTS Haskell. However, if you just drop the LTS Haskell `cabal.config` file in your app’s directory and try to install the app with Halcyon, you could run into unexpected problems. It’s not an intended use case, and I haven’t tested it yet.
[Previously…](http://www.reddit.com/r/haskell/comments/2ogoot/deploy_any_haskell_application_instantly/)
I've mentioned this here before, but i have a pretty decent (IMO) [monad library for Clojure](https://github.com/bwo/monads), with return-type polymorphism (demonstrated in a [super dinky expression evaluator](https://github.com/bwo/monads/wiki/An-expression-evaluator)).
Hey! The reason is that PureScript took the "opposite approach" when it comes to generate JS: whereas fay, haste or ghcj tries to give you a subset or the full Haskell experience, PS went by creating a language _inspired_ to Haskell, which makes full use of the modern js stack. This is why the need for npm. As far as the serialisation goes, at the moment is not supported, although I suspect it might not be trivial to achieve for the reason I have explained in the first paragraph ;)
&gt; A substitution stepper I've thought about this idea from the perspective of education, and I felt it would be really cool if it did it as a graphical animation (in a way akin to [gource](https://code.google.com/p/gource)), because that's how I visualize Haskell evaluation in my head. However, I've never actually tried to implement it and don't know how big of an undertaking it would be.
I can expect it not tp be hard to go use an approach that fits completely in the haskell toolchain. Consodering serialisation: maybe some extra code will be needed (that can be shared with purescript adaptors for other haskell web FWs), it will be totally awesome though. Maybe json is a good 'common ground'.
Is there any chance you could show how you'd use singletons to shrink the resulting library code?
I may do another post in this series that does this. I haven't used the lib too much, but I think it would go something like: ``` $(singletons [d| data CRUD = Create | Read | Update | Delete; data Crudable = ProductCRUD | OrderCRUD |]) ``` This should create all the data family instances (e.g. `SCrudable`), leaving only the Type Family Instances. There may also be a way to promote a function to a type family instance, but I'm not sure. 
&gt; The trick is to make return produce a kind of superposition which doesn’t collapse to a specific monad type until it touches a monad that already knows what type it is. Ah, late binding. Looks like this was a bit harder than it could have been. Even C++ has dynamic dispatch built in for you. Any "dynamic" language should have a good story for dealing with problems like this or you just end up in the position of needing static type information but not having any. By the look of the quasi monad, Scheme shunted this work on the user.
It's not just late binding: this is a problem of needing to "call a method" right now, on an object whose class you do not know, and that you will not learn until later. Return-type polymorphism is not the same thing as ad-hoc polymorphism. The solution in TFA uses both kinds of polymorphism: ad-hoc is built-in to Racket, in its generic-interfaces library, and isn't very interesting in this context; a technique for implementing return-type polymorphism in dynamic languages is the focus of the article. [Edit: although I suppose you could call that "late binding", since it's a very broad term. I initially interpreted what you wrote as meaning just dynamic dispatch, which on its own doesn't solve the problem. You also need to *defer* dispatch in some cases. My apologies if I misinterpreted your comment.]
Like this? http://chrisuehlinger.com/LambdaBubblePop/
Yeah, that's basically it.
Is gource actually useful for understanding code in practice? This is the first time I've heard of it, but it seems sort of like an art project that happens to be based on code. I mean it looks really cool but I can't tell at all what's going on in that example video.
I think there is: [Data.Promotion.TH](https://hackage.haskell.org/package/singletons-1.0/docs/Data-Promotion-TH.html), which is described in [this](http://www.cis.upenn.edu/~eir/papers/2014/promotion/promotion.pdf) draft paper. I've never used it, so I don't know how effective it is.
Thanks for this elaborate answer. I'll give it a go now.. :)
My phone has 4gb of ram. 
We are not talking about a clean build of a single package, we are talking about a clean build plus a clean build of all dependencies in the entire Haskell ecosystem other than the compiler. If you have dozens of dependencies of course this takes more than 2 minutes which is an unrealistic expectation even for a large (dozens of modules) individual package.
I looked into it and it's possible to cobble together a haskell interpreter using `hint` and `FFI` so you can call a haskell script from C and that script can call C functions. The library weighs 48MB which is not winning any awards. (Lua is &lt; 400KB.) But for large systems this is no big deal. Could be my new favorite thing.
Could you put it up on Github? I think many people would be interested in that
This is strangely satisfying
For differentiation `d f`, the *chain rule* will get you pretty far: d (f (g x)) = d f (g x) * d g x Of course, the two mentions of `g` on the right hand side can easily make the resulting term explode in size. That's where you need simplification rules.
I think hearing someone make the comparison of monad binding to pipes is what finally made it click for me when I was first learning Haskell. Obviously later down the line I was able to gain a proper understanding of them and their underlying concepts, but I really agree with the post from a few days ago about the term "IO Monad" throwing newbies for an unnecessary loop. Coming from a weaker math background, I hated Haskell and didn't learn it for years because I thought it was good for nothing more than fancy math and stroking one's ego. Any time I searched for an explanation of why you need this "monad" thing to print a string it was almost as if people tried to make them sound as complex as possible to keep me out of some sort of *special club*, so it's nice to be seeing so many posts like this lately. tl;dr: good tutorial
I don't feel a big need for a debugger--equational reasoning, debug statements, and ghci are enough for pretty much everything I do--but an IDE with lots of refactoring tools would be fantastic. I think Haskell has huge potential to allow IDEs to do really cool things. The type system gives us so much more information about our programs than other programming languages. I think there is tremendous untapped potential here. For instance, think about code changes / refactorings that currently require a lot of manual labor. Even things as straightforward as adding or removing a type parameter can be difficult. You can do a lot with regular expression substitutions, macros, etc, but even the simplest things like needing an additional type class instance can make this a dicey proposition at best. We should be able to do this in a type-aware way that should make it much easier and less error prone. I can imagine refactoring something from a monad to a monad transformer and having the IDE automatically look for all functions in that monad and update their type signatures in the specified way, adding constraints if necessary, etc.
LiquidHaskell supports Catch-style checking, via the flag "--total", essentially using GHC's core. See, for example: http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1422312145.hs (or this if you want to see all the inferred types directly in the source) http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1422312286.hs
You're absolutely right that dynamic dispatch is not a complete solution to the problem. I didn't think of that at the time, constructors cannot be inherited and there's no dynamic dispatch on them. So you'd have to implement that part manually in C++ too.
Dozens is pretty optimistic. Our web apps typically have over 150 indirect dependencies. Building all that will take time, in any programming language.
I certainly wouldn't want that in my `.cabal/config`. Parallel builds hide a lot of the information about what happened during the build in a log file. That's fine for full sandbox reconstructions; it's worth the speedup, and anyway if it fails you can run `cabal install` again without the `-j` to see the full errors out front. But most of my builds are just re-builds of at most a handful of modules. I'd rather watch the action live than save a second or two with `-j`.
I really think you are misunderstanding what is happening here. If something like 2 minutes per month makes that much of a difference to you, then I wish you the best of luck. And I simply don't believe you that compiling most of the Scala ecosystem from scratch, including things like all of the core Java libraries, can be done in a few seconds. Because that is the equivalent of what you are trying to do in Haskell. The GHC/Cabal toolset does have incremental compilation. It's not specific to Yesod. (Yesod devel mode automatically detects changes to source files, recompiles them, and redeploys them in your browser, which is nice for web development.) What we are talking about is not that. We are talking about starting from scratch with a brand new sandbox, which means recompiling all direct and indirect dependencies. It's a scenario that doesn't come up too often. But when it does, then yes, it takes a few minutes.
This is not a *[M-word]* tutorial. It is a tutorial about how to *avoid* using the word *[M-word]*.
honestly who has 32 gigs of ram
those two ideas are mutually inclusive.
I agree, but that would be a great separate article. I thought that this article was nice and concise. 
Really? Are you absurdly pedantic or pedantically absurd?
x currently isn't defined in either of the quadruple examples.
This has actually been definable for a long time. It is not currently defined in base or anything standard. Before igloo removed Eq and Show as superclasses of Num folks used to have to lie and supply those instances, too, however. 
I see. Still don't know what it means though. My brain keeps wanting to parse it as a needing to be a function from some numeric value a to another numeric value a, but that doesn't make sense. I understand: Num a =&gt; (a -&gt; a) -&gt; a And: (Num a, Show a) =&gt; a -&gt; a For example, but not: (Num (a -&gt; a), Num a) =&gt; a -&gt; a In particular, I don't understand how I'm to interpret the function signature when its in the class constraints (left of the =&gt;) part of the signature.
semantically, yes, they are equal. we can show this by expanding the definitions: f $ x = f x f . g = \x -&gt; f (g x) starting with the expression `double $ double x`: double $ double x = double (double x) starting with `(double . double) x`: (double . double) x = (\y -&gt; double (double y)) x -- changed the parameter name to avoid confusion = double (double x) note that they reduce to the same expression. (we didn't even need the definition of `double` to show this!) feel free to ask if you need clarification on how to do this. i don't have any resources for it at hand but can explain further as needed. a possible further question, which i don't have the knowledge to go into, but someone else may be able to answer: for ghc, do these get compiled the same way? edit: as a further note, people will often say they prefer the version that uses composition for various reasons. the one i hear cited most and that i think is most important is that, when you have a chain of composition, you can do a straight text substitution to factor out part of that chain. e.g., foo = f . g . h . i . j can be refactored as foo = f . g . bar . j bar = h . i whereas it takes slightly more work to do the same with the version using `$`. another is that you are free not to mention the parameter at all in many cases, which, to view it one way, may remove undue emphasis on the thing being acted on and put more on the action itself.
If you follow the type checker through, we have (*2) $ (+2) The first connective is `($)` which has type `(a -&gt; b) -&gt; a -&gt; b`, so we know that `(*2)`'s type must unify with `a -&gt; b` and `(+2)`'s type must unify with `a`. Thus since both `(*2)` and `(+2)` have types `forall c . Num c =&gt; (c -&gt; c)` we can unify a -&gt; b ~ Num c =&gt; c -&gt; c a ~ Num d =&gt; d -&gt; d and solve a ~ (Num (d -&gt; d), Num d) =&gt; d -&gt; d c ~ a c ~ b c ~ (Num (d -&gt; d), Num d) =&gt; d -&gt; d a -&gt; b ~ (Num (d -&gt; d), Num d) =&gt; (d -&gt; d) -&gt; (d -&gt; d) Thus the final type is `b`, e.g. `(Num (d -&gt; d), Num d) =&gt; d -&gt; d`.
I don't think this is the only valid Num instance for functions. A thing I've kind of half-wanted sometimes is a Num instance for continuations. Since any applicative can be given a Num instance, and `(a -&gt; r) -&gt; r` is isomorphic to `Cont r a`, you should be able to give a Num instance to `Num a =&gt; (a -&gt; r) -&gt; r` in a sensible way, except defining `fromInteger = flip ($) . fromInteger`. Then you could do something like: newtype Seconds = Seconds { getSeconds :: Int } deriving (Ord) seconds :: Int -&gt; Seconds seconds n = Seconds n minute :: Int -&gt; Seconds minute n = Seconds (60 * n) test :: Bool test = 1 minute &gt; 59 seconds Granted, it means a non-intuitive non-unique orphan instance, doesn't really support equational reasoning (`let { x :: Int ; x = 10 } in x seconds` doesn't work), and I imagine all of the other methods being defined would be undesirable most all of the time. Still it's kind of a fun way to fake postfix functions. EDIT: Also, you need to define singular and plural versions of each function if you want it to read like English.
You're being too helpful to the newbies! This website must be taken down and replaced with satire, ridicule, and smug remarks!
Thanks for pointing my attention to that! My idea was that the proxy for `k` would be statically known; for instance, maybe the `Vect` we are `take`ing from was created by `concatMap`; then we could take `k` elements and know there are `(n-1) * k` left over. I originally didn't even have the `Proxy k` as an argument, but then GHC complains that `k` is not determined, and thus requires using `-XAllowAmbiguousTypes` to defer the ambiguity check to use sites. The much simpler and better option is just to add the constraint that the number of elements to take is no longer than the list. That way, you don't need to know exactly what that difference is; only that it is non-negative. I will make that change; thanks!
The fact that you get this type doesn't mean there are instances that'll satisfy these constraints. Well, you could write them, but they are not provided in base. ghci doesn't yell at you because numeric literals are overloaded, i.e by default, the type of `2` is `forall a. Num a =&gt; a`. If you are more specific about `2`'s type, you get the "expected" behavior: λ&gt; let x = 2 :: Int λ&gt; :t (*x) $ (+x) &lt;interactive&gt;:1:9: Couldn't match expected type ‘Int’ with actual type ‘Int -&gt; Int’ In the second argument of ‘($)’, namely ‘(+ x)’ In the expression: (* x) $ (+ x) So your initial code uses 2 as a value and a function, it could work, but not when you stick to a particular `a`, at least not without the instances discussed in all the answers to your message.
Thanks! Wow, I'm going to have to read that *slowly*... 
In fact, there is nothing special about `(e -&gt;)` here; one can define this for any applicative functor (with identical code to what's in the link).
I know lines of code is not a reliable indicator of build time, but in a say, 25kloc app, what are we talking on average to do a clean build? 5 minutes, 10 minutes, 20+ minutes? Just curious to see what production deployment looks like in Haskell land. Obviously for daily dev if incremental build support can isolate code changes to dependent source file(s) alone, then there's likely little friction in terms of the compiler getting in one's way -- that's what I'm most interested in, daily dev, as deployment usually occurs on remote build server, so kick off the build and go do something else (vs. local builds where one's CPU is cooked and the laptop fans attempt to achieve liftoff ;-))
I didn't look that closely, but as far as I can tell it doesn't address the problem that this post addresses. Monads in dynamically typed languages are often simple enough, the big problem is writing functions that a) Work for any monad automatically, and b) Either use or are functions that don't take a monadic value as input.
Actually, you wrote an IO tutorial.
Well, hundreds seemed a bit large so I figured dozens would be about right for most applications. 
&gt; name I changed the name to Inferno
Slightly off-topic, but might be interesting: I've gradually moved over to a new style of composition that uses the nice set of operators: `&amp;`, `&gt;&gt;=`, `&lt;&amp;&gt;`. They are all infixl 1 and in all of them data flows left-to-right, which means you can compose them linearly without parenthesis, and they read like a unix pipe. I find it very nice: get &lt;&amp;&gt; someField &amp; lift &gt;&gt;= useFieldInLiftedMonad &amp; someLens %~ someFunction &amp; furtherProcessing You just read it top-to-bottom, the precedence is lower than almost all other operators, so you can throw in various computations in each line that way. 
&gt; when you have a chain of composition, you can do a straight text substitution to factor out part of that chain This is my key reason to prefer associative operations.
well this is new :) 
Why not lift your functions and literals via `liftA2`, `fmap` and `const` explicitly instead of implicitly?
A great story. Mostly unrelated to monads, though. :)
I asked the same question years ago: http://stackoverflow.com/questions/3030675/haskell-function-composition-and-function-application-idioms-correct-us
Isn't `*&gt;` now exported as part of `Applicative` due to AMP and not this change?
As someone who wishes the BBP went even further I would be fine with (and even supportive of) a language pragma to control which prelude for a few versions. In the long run having one is probably better from a testing/maintainability/usage standpoint but given the unrest around these changes it seems like a fine compromise.
As it has been pointed before, having two preludes is a bad idea, because it would be the worst thing to inflict to newcomers. The point of prelude is to be common for everybody. **edit**: I fully agree with this comment http://np.reddit.com/r/haskell/comments/2icjmf/how_to_rewrite_the_prelude/cl0xdyr
Yes, that one is due to the AMP thing, not this one.
I was under the impression that you couldn't clone a specific branch with git. That is, I can either pull the whole thing then checkout the hoogle4 branch, or I can do some git-fu to pull a bare repo then checkout the full branch, but I suspect having a separate repo will be less confusing for users unfamiliar with git. Also, for github at least, [only the master branch is considered when doing searches[(https://help.github.com/articles/searching-code/#considerations-for-code-search).
Oooooh, hoogle instant **and** it searches outside of base. I *love* FPComplete, but once this is out, their hoogle is getting relegated to the back burner!
Right, that makes sense. Tuni is an anagram of `unit`. I guess that would make the shrimp `bind`.
On a vaguely related note, [servant-purescript](https://github.com/anchor/servant-purescript) was apparently just released as well.
True. For me though, the only reason to use FPComplete's was the additional sources that were included, which seems to have been addresed by this version. That isn't to say that there aren't other tools that FPComplete provides that I'll still take advantage of ;-) Thanks to you both (Neil and FPComplete devs).
(&lt;3) the drawings, but surely Kleisli composition, otherwise known as left-fish (&lt;=&lt;) and right fish (&gt;=&gt;) deserves a pictorial mention?
I'm sure it's interesting but could you a give an example of what is for (input -&gt; output)?
I thought that about the shrimp at first, but I don't get how the last sentence supports that haha. Also could the Tuni bit be about the monad laws?
Alternative preludes can be enabled without a new pragma just be import Prelude (); import OtherPrelude or (in extreme cases) the existing pragma NoImplicitPrelude and an import. I agree prelude should not be changed, but also no need at add anything to support an alternate.
:-) "Crocodile Monad Transformer Tutorial"
@katieandjohn is asking whether doing whole program optimization is a big win. So, if that's already implemented in GHC then if would be great to see how "big" of a win that was by comparing the code size and performance before and after having the optimization. Say, GHC 7.2 did not have it but 7.8 has a lot, then building and profiling the same code with the two versions would give a good indication of the improvements.
Nowadays laptops have.... It's got so cheap that it doesn't make any sense not to upgrade the RAM given the performance boost and time saved you get. I have Lenovo W540 with 32Gb of RAM.
The purpose of the Haskell Report is to describe Haskell. Changes kind of have to exist before you can describe them, unless you want to risk writing an entirely meaningless document. Things happen in the implementations first, and then people figure out what part of that should be "standard".
Try e.g. http://hackage.haskell.org/package/foldl which defined its own `foldMap` I confess thinking that `Foldable` is just not that great a type class really; maybe I'm wrong. The idea of what can be 'folded' with an operation `a -&gt; b -&gt; b` or `b -&gt; a -&gt; b` is basically the idea of a somehow-sequential thing. This idea is not particularly bound up with things of kind `* -&gt; *`. Thus we have things like [`monotraversable`](http://hackage.haskell.org/package/mono-traversable-0.7.0/docs/Data-MonoTraversable.html) which is actually useful and gives you nice, abstract, generalized functions that work like lightning with really good types we should be talking learners into using, like `Data.Vector.Unboxed.Vector`. I would import it everywhere if it weren't so ugly ... mainly because namespace has been swallowed up. It seems like the reason something like `Foldable` didn't go in that direction is historical: it requires a few type extensions like TypeFamilies that didn't exist yet, so it couldn't yet capture the idea it was looking for.
The base library already has IsList, which sounds like what you are thinking of - I also feel that Foldable doesn't seem the right thing to base Haskell on. 
More importantly(?) than the ghc version number, changing the Prelude would have to be a new Haskell version. The Prelude is very much part of the language spec.
Yes, I was thinking I was chiming in with what Lennart A. said about `IsList`. `IsList` wouldn't be so bad if it weren't defined in terms of `[]`; something better would involve something like the reasoning that went into `monotraversable` to make it usable with really rock solid sequential types. It manages to get `Text` in, as far as can be done, and also `Vector`, as far as can be done -- and then also things like `[a]` and `Seq a`. In general, I would never use the `Foldable` operations with anything but `[]`, since that's the only type of kind `* -&gt; *` where really deep thought about optimization has happened. Inviting learners to use the 'generalized' version is basically inviting optimization disasters that send people fleeing from the language.
Prelude needs an update. If you ever go out and try one of the prelude replacements, like say basic-prelude, it makes diving into the language a lot easier. It'll be a shame that many of the tutorials will be out of date, but there's no workaround for that. At the very least I'd like the pragma to choose a prelude so I could just put it in a cabal file instead of manually importing every project.
I did try incremental modifications from the existing code base first (that got somewhere, but not to the point where I could actually see anything working), then on a branch (https://github.com/ndmitchell/hoogle/commits/hoogle5), and finally in a new repo. The new repo went far better than the other two approaches. I think the reason is: * I can break all the tests. * I can completely change the API (or in fact, delete the API). * Enough has changed that the similarities are mainly the user-facing HTML (which I copied). It doesn't reuse all that much. * I was able to think "what do I want", rather than "how would I modify what I have to do what I think what". By getting to what I thought I wanted (and changing) quicker, by having less code, I covered more of the design space much faster. So I think the rewrite gave me more freedom to think bolder and more half-baked thoughts, and not worrying about how to get there. I know Joel On Software suggests you should never rewrite, but I've done it 5 times now with Hoogle, so I don't think it's always a terrible thing to do..
It's got a massive pile of other operations in, that's the complaint here.
The AlternativePrelude language pragma is designed to do exactly that. I think the idea here is we should play around with Prelude replacements, then pick one that already exists, rather than writing a new one from scratch. Personally, I like the existing Prelude :)
... but language pragmas should *never* be in the .cabal file -- the should be in the source code, where people can actually find them and not be confused by why the language is magically different in this file.
I think it's even more people finding out about how deep it goes and being horrified.
Since GHC is breaking Haskell2010 compatibility (due to AMP at least, if not also BBP), then new versions of GHC will actually *not* be valid Haskell compilers *until* a new language spec comes out. I will definitely be hanging back on my versions pre-breakage for quite some time.
Well this has been the chicken and egg problem for all these years! We can't change the spec until a compiler implements the new feature, we can't change the libraries until they are changed in the spec... I suppose a flag "sort of" works around this, but I don't think it does so in a worthwhile way. In any case, to get around this problem, the Haskell-Prime process, such as it is, deliberately announced some years ago that libraries and prelude changes would follow a different process. So I'm all for whatever discussion we need, and making the "right" changes -- but at this point I think the language spec issue as a roadblock has been discussed and decided.
`git clone --branch hoogle4 --single-branch` If you omit the `--single-branch`, it is equivalent to `git clone; git checkout hoogle4`.
Maybe `IsList` wouldn't be an invitation to optimization disaster if the basic default operations were something like toChurchEncodedList :: l -&gt; (Item l -&gt; a -&gt; a) -&gt; a -&gt; a toFoldl' :: l -&gt; ( a -&gt; Item l -&gt; a) -&gt; a -&gt; a (pardon revolting names), and whatever would be suitable `build` in the other direction. This works for `Text` and `ByteString` and unboxed `Vector`. Basically `[]` should be out of scope when a beginner is working with those types. But maybe `OverloadedList` is fated to use the `[]` type. 
Isnt Foldable been around for like forever? Also its being taught in several blogposts, tutorials and printed books. Have we been teaching the wrong abstraction all along? Or are you suggesting to modify Foldable?
BBP is, from what I understand, just moving Traversable and Foldable into Prelude and generalizing prelude functions to use them, which seems pretty reasonable to me. What is the horrifing part of BBP?
 f = go where go = ... go ...
nobody cares about modified preludes unless they start being provided by default with ghc as this discussion seems to prove ;-)
same goes with AMP...
Foldable has been around forever, but the version that is suggested for 7.10 has gotten a lot of new methods.
why?
&gt; I know Joel On Software suggests you should never rewrite, but I've done it 5 times now with Hoogle, so I don't think it's always a terrible thing to do.. I think that really depends on the codebase. For clean code written mostly by a single person, it makes sense. Also, Joel's advice doesn't hold as much for Haskell code as it's often loosely coupled.
&gt;Absolutely. IMO, there should be a new Haskell report with the AMP changes. ...and all the other changes base has accumulated in its report-listed modules since haskell2010? &gt; The big difference with the AMP changes compared to the BBP changes is that I don't think anyone could have missed that AMP was coming since ghc warned about it all the time. how should ghc have warned about bbp specifically if the main objection seems to be rather that Foldable is not the right abstraction to begin with? 
Sure. I just don't want the idea of conforming to a standard or not dragged into this :-)
I really like the first proposal. For my own (especially private) code, it would be great to just set it up to use the classy prelude at the cabal level. I feel like one of my biggest gripes with Haskell is that I have to spend a lot of time tending to imports and dealing with conflicts. I understand that there is the concern that it's less beginner-friendly, but I'd probably not use it in public libraries as much as I would for executables and private codebases.
&gt; Well... IsList clearly isn't the right abstraction, as lots of things are Foldable but not IsList (Map, for example). &gt; :i Map type role Map nominal representational data Map k a ... instance Ord k =&gt; IsList (Map k v) -- Defined in ‘containers-0.5.6.2:Data.Map.Base’ I don't think anyone is objecting to the module `Data.Foldable`, or suggesting the `GHC.Exts.IsList` should be included in `Prelude`. `Enumerable` and the like are coming up because the merits of putting `Foldable` in the `Prelude` are being considered. 
Can we not change the implementation somehow so it doesn't lead to these undesirable consequences? In mathematics, one naturally writes f + g to mean the point wise addition of functions (usually but not always to the reals or complex numbers). It seems that the implementation tail is wagging the requirements dog here.
I know I can do this and I occasionally use this or write an instance of Num but it is irritating; I'd like to be able to write the way I write mathematics.
Ah, brain-slip on my part! I forgot how nice that associated type is. Well, any sort of tree with a choice of branching is a good example then :-P. In any case, I'm wondering what the abstraction should be if _not_ `Foldable` -- `IsList` doesn't fit the bill and `Enumerable` is the same abstraction, just slightly less powerful (because no `foldMap`). As I've pointed out, I think the `Foldable` changes are the ones causing all the angst, not the ones generalizing to use `Traversable` or `Functor`. And at the same point, I think the `Foldable` changes matter the least in terms of what would make programming Haskell more pleasant, and most importantly what would make important changes easy to discover. So I tend to think that BBP sans foldable generalizations is the way to go, and that lets us kick the can on this whole "right" abstraction thing anyway :-)
I took the crucial point about `Enumerable` to be in the comment class Enumerable t where toList :: t a -&gt; [a] -- Implementations should use 'build' but it seems like the whole discussion is raising difficulties, not making a definite new proposal.
While we are burning bridges can we merge map and fmap? A man can dream?
Sweet!
Sweet!
&gt; We can't change the spec until a compiler implements the new feature Why not? Seriously. Why not have a forward-looking specification that doesn't have an implementation when released or certainly doesn't have an implementation *when the change is added to the draft specifcation*? We certainly don't want to pull a CSS 2, but there were plenty of C++11 changes that were literally research projects performed while the committee was pulling together the draft. There's no reason that Haskell2016 Prelude couldn't be vastly different from the Haskell2010 Prelude even if no current Haskell compiler had made similar changes. While backwards compatibility could be an issue, I've fine with "burning bridges" when a new language standard is released. (There's some aspects of the C++ STL that barely survived the transition from C++98 to C++11.) Other than that, just saying there is this symbol with this type and these semantics isn't a complicated task for any of the changes proposed for this new, generalized, prelude. I think it would be good for Haskell compilers to be in serious competition from which one will implement the whole specification. Both GCC and Clang got better when they kept trading off the crown of "most C++14 features implementated". I also think it's past time for some of the GHC extensions that are supported by UHC or JHC to be rolled into the standard, even if they aren't on by default. In those cases we already have (at least) 2 implementations!
I agree that it feels like a nicer design. But my point is this is just (almost) the same semantics as `Foldable`! So it is in my mind a variation on the "same" abstraction, not a different abstraction. It does let us separate more methods out, but those methods will have the same signature generalized over a typeclass anyway, so it seems like it is a relatively minor question overall?
Note also that you can't have `Traversable` in `Prelude` unless you also have `Foldable`, as things stand.
One thing that I never quite cared for with Haskell is the fact that GHC seems to just do whatever it wants, and everybody seems to just accept it. Haskell may be a standardized language, but that doesn't mean much if the vast majority of programs use GHC-only extensions.
&gt; Note also that you can't have Traversable in Prelude unless you also have Foldable, as things stand. Well, you can expose `Foldable` without bringing its methods into scope in such a way that they step on the prelude ones.
Because "just" making those changes is a major, fundamental change to Haskell itself. Even if there is a consensus that the change should be made - which is far from clear - it needs to be done in a more gradual way, with a chance to try it out in the libraries, tweak it if we didn't get it quite right, and at least a short deprecation cycle of some kind to reduce breakage.
My point is that it has always been the rule of the Haskell process that the spec must be written to describe something that has been implemented in at least one place. All prior experience points to this being a good practice, as otherwise you get a spec that seems great, and then you try to implement it, and whoops!
&gt; how should ghc have warned about bbp specifically Well for starters, both AMP and BBP introduced several new exports from Prelude, and one of the AMP warnings was specifically about name conflicts that would arise as a result. So we could have had a parallel warning for BBP. It's particularly galling that someone who got that AMP warning in 7.8 and updated their program so that it would be compatible with the AMP Prelude additions in 7.10 now has to deal with another pile of Prelude additions *in the very same GHC version*!
Of course that's right. I also don't have any confidence in `Data.Traversable`, especially as a tool for beginners, but this is another subject that is less clear to me. It seems to me that an amazing percentage of beginner space leaks and optimization catastrophes arise from naive use of `mapM` `sequence` and the other tools for using a monadic bind to extract a complicated lazy Haskell data structure from inside `IO` in particular. Tekmo made some such point a couple of years ago, and my experience since then has borne this out massively. That is, many many typical beginner disasters arise from *just the methods* that `Traversable` 'generalizes'. Data.Foldable by contrast supports the anodyne underscored functions `mapM_`, `sequence_` which don't have any of the fool's traps waiting in them. That is, again, the only generalization of beginner-worthy functions that goes beyond `[]` will be monomorphic and use an associated type. But as soon as we see this, I think, the thought that they belong in `Prelude` of course seems doubtful for another reason. All of these apparantly 'forward looking', avant garde changes are secretly binding the language to a past in which certain type possibilities like associated types were not yet available. 
The process you describe is basically how improvements to GHC _do_ happen -- they just aren't tied to the "draft" being considered part of the prime process.
what's the alternative?
Why didn't we just move the existing implementations in `Foldable` to be in terms of `foldl` to match those in the prelude? This might have been a less noisy solution?
I think the main objection is due to numeric literals being overloaded and juxtaposition meaning function application, neither of which is likely to change. Adding a `Num` instance for functions makes it much more likely that a non-obvious typo will result in well-typed nonsense: an easy example is forgetting a comma in a list of numbers, e.g. `[1, 2, 3 4, 5]`. If literal overloading and arithmetic were separated instead of combined in one type class I suspect people would be more willing to consider allowing the latter to be defined for functions. There's also the objection that other instances are possible (e.g. Church numeral style, where `3 f` = `f . f . f`) but the pointwise instance seems to be the generally agreed-upon "obvious" choice.
Consider a kickstarter /u/chrisdone?
On the other hand, as of 3 weeks ago, 569 packages in stackage built clean with the changes, including pretty much all of the core of it, and of the 50-100 that remained, most were awaiting author feedback on already-filed issues / fixes or are broken by a small change downstream. I haven't checked in on progress since, but it appears to be proceeding apace.
The first of those is a completely legitimate breakage caused by this change. The second is AMP. The third, on the other hand, is definitely transitional pain. -Werror is a terribly fickle target and I for one have never been able to go from one GHC release to another without something tripping it. Note: adding `import Prelude` at the bottom of your import list will usually trick the usage checker into happiness. It checks if module provide anything new that you use top-down, and you're usually using _something_ from Prelude.
Yaxu! Love your work :)
Many of the other changes you suggest are pretty good! One day we should do them... but as even getting this far suggests, such progress is slooow.
Adding new expressions doesn't seem to work for me. Do you have to hit ctrl+enter or something?
Remember that we used to have ghc ship with both base 3 and 4 and we made that work fine. The crucial thing was that they exported the same types (in face one re-exported the other), rather than defining new separate types, which meant that the consequence of picking one version or the other didn't propagate to other packages. That meant that we could have both co-exist and have an incremental transition. For this stuff, I don't think it involves incompatible types, so it may also work fine to have both and mix.
For what it's worth, when we started using Haskell at IMVU, the _number one_ issue with teaching people was that the Prelude itself is not usable in production code. In production code, you generally want to use Vector and Text and ByteString, not lists and strings, and few of the Prelude functions work in a general way. I fully support burning bridges and getting to a world where Haskell is as nice out of the box as Python. :)
It makes sense for closed source organizations, where there is a well known conventions that is meant to be applied uniformly. A Prelude extensions works in this case.
This is because there are no serious competitors to GHC as actual industrial-strength real compilers.
&gt; Get rid of return since we've got pure. Make monad comprehensions the default--or perhaps strike list comprehensions; we have do. Get rid of mapM since we've got traverse and rename sequenceA to sequence. Remove the fail abomination. And of course good grief, get rid of map for fmap. The original BBP email from Wren included many of these things. We're adopting a much more moderated approach by trying to focus on what changes we can make towards these ends without immediately breaking all the things. Switching the world to `pure` or moving return to the top level isn't even a thought we could think until the AMP had gone through. I'm not advocating for or against it, but it took us almost a decade to get AMP and it is still breaking code. Once AMP is in place and the world has adapted, folks can put forth serious proposals about how and if we should remove or deprecate now-redundant members from classes so that they can be moved to the top level outside of the class or removed outright. I'm actually fairly neutral on if this is a good idea or not. Eliminating `mapM` for `traverse` is a cultural shift, but there turns out to be a technical hurdle as well! There are cases where `mapM` can actually avoid blowing the stack on a large traversal where `traverse` necessarily will. You need a very large but not infinite heavily-right-biased structure, then there is an implementation of `mapM` that doesn't blow the stack, but every implementation of `traverse` will. Removing `fail` by moving it to another class is something that Lennart here has been campaigning for tirelessly. Somewhat ironically it breaks more code than everything he's complaining about here. We're open to looking at options for how we might lay warnings for that in 7.12 and how we might carry it out in 7.14. It turns out it is actually a fairly messy business. For `map` vs. `fmap`, we weren't brave enough to make that change. ;)
I would encourage you to try to resurrect the haskell-prime process then. There is a committee, there is a mailing list. Breathe life into them.
&gt; It really doesn't have to be that way. You can have a forward-looking specification / standard that doesn't have a full implementation when it is released. Scheme tried this with R6RS. It didn't end well.
As did the C++ committee with template exports. Didn't work out so well either.
&gt; The whole type-level computation/quasi-dependent-typing stuff is yet another can of worms that has accreted over time and should really be refactored, at least from a pure syntax POV. Writing programs in Haskell that make use of such advanced features shouldn't be any harder syntactically than in, say, Agda or Idris. That work _has been_ happening with type literals, work on dependent types, etc :-)
&gt; Why not? Seriously. Because we do not want to program in C++. Seriously though. I'm not ragging on imperative languages here, I'm talking about the more important issues of aesthetics and community. C++ has been and always will be a hodgepodge language, an octopus formed by nailing planks to a dog. That approach has its ups and its downs. On the up side, rolling out new features is extremely easy. On the down side, fully understanding —*and fully specifying*— the semantics of such a language is a nightmare. Which style a given developer prefers is entirely up to them, but any language itself has to make a stand somewhere. And languages build their communities based on where they've made their stand. &gt; There's no reason that Haskell2016 Prelude couldn't be vastly different from the Haskell2010 The problem here isn't "things have changed". Change is necessary, and often desirable. The problem is looking at the ramifications of the changes and asking whether they compromise the aesthetic principles we —as a community— have come to expect. That is, do the changes violate "the spirit of Haskell"? The AMP change was a major breaking change. Back in the day, I was the one who started this whole discussion of "burning bridges" because proposals like the AMP had been circulating for years and years, and everyone agreed they were right, but noone was willing to commit to the fact that this obviously correct change required breaking old code. Or, more particularly, everyone was getting bogged down in novel and ingenious ways to try to make it so that the change did not in fact require breaking old code, but none of these approaches was ingenious enough to be accepted as the obviously right approach. My modest proposal was that we needed to just accept the breakage and move on; that backwards compatibility must be sacrificed on the altar of making progress. Shortly afterwards the AMP-as-such was formalized. And when it was put into practice, everything broke and noone complained, because the AMP is very obviously in keeping with the spirit and aesthetics of Haskell: it's all about keeping the language clean and staying true to our theoretical roots, in the exact same way we've been doing all along. The BBP obviously got its name from my modest proposal, but there's an important difference. See, Haskell has for a long time held a very specific position with regards to how polymorphic to make things. On the one hand, yes, we love and embrace things like `Traversable`; on the other hand, we've long kept the prelude as monomorphic as feasible. You can see this in the fact that we have the function `map` even though it's just the list instance for `fmap`. Similarly, we have `(++)` even though it's just the list instance for `(&lt;&gt;)` or `mappend`. And so on. These aren't historical warts, [they were intentional decisions](http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/). The issue with the BBP is not that it (potentially) breaks old programs, the issue is that it marks a significant change in this aesthetic philosophy about how polymorphic things should be by default. There are decent arguments for both positions, and this isn't the first time this issue of how polymorphic to be has come up. The real question is not whether the BBP is "good" or "bad", it's how do we guide the community along a path of change so that we maintain a consensus about what the spirit of "Haskell" is and should be.
&gt; What is the horrifing part of BBP? Perhaps [this comment](http://www.reddit.com/r/haskell/comments/2ttvsj/major_prelude_changes_proposed/co2ofii) will shed some light on things
You're not bound to a piece of paper like early mathematicians :) Pay with an explicit lift here, and get a safer type-check there.
You can't evaluate _only_ that subexpression, but it's fine to reduce it one step, or any finite number of steps, and you still get an expression which will evaluate (under lazy evaluation) to the same result. Chris Done's proposed substitution stepper only takes single steps at a time; I'm saying that it would be even better to let the programmer select which step to take interactively.
We have 4 sources of members in Foldable Minimal Definition -------------------- `Foldable` exists to provide `foldMap`. `toList` isn't sufficient, as it forces you to re-associate everything to the right. This comes at best as an asymptotic hit for some cases and affects termination in others. So, once we accept Foldable as really being about `foldMap` we run into a problem. `foldr` based on `foldMap` alone can have worse stack behavior, so we add that to the class. `foldr` and `foldMap` are both {-# MINIMAL #-} definitions for the class. What about the rest? Historical Members --------------------- Before 7.10 there were a number of members that users could overload for efficiency, `foldl`, etc. which we've left untouched. These can be more efficient in terms of stack usage for some containers as well, so moving them to the top level outside of the class is problematic on both that front, and the 'breaking of existing code' front. Avoiding Semantic Changes ------------------------------- But we ran into an unexpected hurdle when we went to adopt `Foldable`: When you go to export `Foldable` it turns out that many operations that were defined in the `Prelude` and the operations that folks were using that were defined in `Data.Foldable` had different semantics. Usually it was whether it folded left to right or right to left. `sum` folds one way in the `Prelude` and the other in `Data.Foldable`, etc. This accounts for most of the meat of the expansion of `Foldable` in 7.10. It provides sensible defaults for everything, and allows these operations to retain their existing semantics because we wanted to avoid silent changes to semantics. Generalizing types was one thing, but yielding different answers seemed to be beyond the pale. Asymptotic Improvements ------------------------------ Now, there were some other additions to `Data.Foldable` this time around that are more gratuitous, which I actually initially pushed back on: rwbarton and dfeuer raised objections that some `Foldable` operations could be implemented in an asymptotically more efficient manner for many containers, so the class was enlarged to support those usecases. Those are probably the least defensible members of `Foldable` from the `Foldable` as generalized `foldMap` perspective, but they do allow a number of generic operations to be implemented in `O(log n)` or `O(1)` on many containers rather than `O(n)` and most users never need consider them, so it was deemed the lesser of evils to include them since the class was already quite big, and it can let more container types share the common combinator, reducing import conflicts. If anything had to wind up on the cutting room floor out of all of this, these would be the things I'd be the least sad to see go. However, that said, that last set of additions actually went through the usual `libraries@` proposal process.
I was quite disappointed when this wasn't part of the BBP.
The committee figured we had enough things that had to generalize for consistency that we'd avoid picking fights on stuff like `map` where we had options.
Rather than dream, why not pretend? * Only use `fmap` in your own programs * Whenever you see `map` in someone else's program, pretend it says `fmap` Mission accomplished!
The consequence of that design is that users then can't define instances without dragging in a bunch of imports. We considered and rejected that option with the rubric that where possible if we give you access to a class you should be able to define it without further imports. AMP could have been implemented without exporting `Applicative` from `Prelude`, too. If we'd done that to define a `Monad` users would have to import `Control.Applicative` by hand. `Traversable` sans `Foldable` invites the same comparison.
&gt; Because we do not want to program in C++. &gt; [...] I'm not ragging on imperative languages here[...]. C++ has been and always will be a hodgepodge language, an octopus formed by nailing planks to a dog. That approach has its ups and its downs. On the up side, rolling out new features is extremely easy. On the down side, fully understanding —and fully specifying— the semantics of such a language is a nightmare. I don't think that's so much the process, as it is the participants. C++ is a very popular and widely-deployed language, so everyone wants their pet feature. The Haskell community is much more focused, and we have several voices that do not hesitate to call out poorly or incompletely specified features. We don't have to "nail on" every new feature, but we also don't have to wait for every feature to be implemented before specifying it. In fact, waiting for an implementation could be distracting to efforts to specifying a feature, because of the desire to answer hand questions like "What should X mean?" with "Whatever X-impl does, today." &gt; The problem here isn't "things have changed". I do think this is a problem, too. Things have changed, and the Haskell report hasn't moved forward. &gt; Change is necessary, and often desirable. The problem is looking at the ramifications of the changes and asking whether they compromise the aesthetic principles we —as a community— have come to expect. That is, do the changes violate "the spirit of Haskell"? I don't, yet, have a dog in that fight. I've used `Traversable` and `Foldable` before, and I think that `Prelude` would be better for non-beginners if it were more polymorphic, but I really haven't through too deeply about whether `Foldable` and `Traversable` are the "right" abstractions. Maybe I just haven't worked on a large enough project, I'm also fine with the existing `Prelude`.
The fundamental problem with the BPP (and excessive overloading in general) is that whilst it arguably makes code easier to write, it also makes code harder to read. That is the wrong tradeoff.
&gt; Scheme tried this with R6RS. It didn't end well. Nearly a dozen implementations of R6RS since 2007, and a new standard (R7RS) semi-finalized in 2013. Looks a heck of a lot better than the Haskell status, at least to outsiders. HTML 5 and CSS 3 have both added features that appeared in the drafts before any implementation was available. (Although, the reverse happens much more often.)
Can you provide examples of code (which weren't the result of pathological code-golfing) which are harder to read due to BBP?
&gt; I fully support burning bridges and getting to a world where Haskell is as nice out of the box as Python. :) I think this is very important to the adoption of Haskell (which itself is important because Haskell gets a lot right and it moves us towards a world where we get paid to write it).
Scheme was designed to be easily implemented at the expense of almost everything else. It isn't surprising that there are a lot of implementations. R5RS can be implemented in a couple of days. R6RS compliance can be built up gradually and is quite approachable. That said, my recollection was that folks found quite a few corners of R6RS unimplementable _after_ it had been ratified. (Note: it was ratified under far far more divisive voting lines than anything we've ever considered here in the Haskell community.) One of the things that has changed in the scheme community that is particularly ironic for the purposes of this discussion is that the SRFI process actually requires a reference implementation to proceed. ;)
Okay, so substitute "using qualified imports" for "using hiding clauses" (indeed, qualified imports is the sensible way to use Data.Foldable/Traversable currently). What exactly was wrong with needing qualified imports for those modules? We have a lot of other modules that are intended to be imported qualified already like Data.ByteString.
All your posts do an excellent job of explaining why the bbp is well designed. At a personal level and from a technical point of view, I am really happy with those changes. Thank you for your awesome explanations (and all your work in haskell-land of course :-))! However it is a bit sad that no one took the time to write something in the ghc blog about this subject. When I search about this topic, I find technical information like [this](https://ghc.haskell.org/trac/ghc/ticket/9586) or blog posts/ reddit discussions opened by people against it. Again I really support this change, but it seems that either the ghc team deliberately avoided the discussion with a broader audience or that no one cares about the objections that people like nmitchell may have (eg the fact that it is harder for newcomers to learn about more abstract types). People who contributed come here and post replies to defend their work. They do a fine job. But it would be way more effective if it were posted on the ghc blog. It must be annoying to listen to everyone complaining all the time about any kind of change, but it is a price to pay when the community is growing. I believe they would have an easier life if they took a bit more time to explain the non-technical decisions they make in the form of "official" posts. 
The issue is just that you have to do more work in your head when reading the code - essentially doing the type unification in your head. Right now - in cases where I have the choice - I prefer to write `map` because it will make life easier for the reader than `fmap`. ... at least I have that choice. With BPP reading something like `mapM` is going to become much harder.
Monads can be quite awkward to use if your language also doesn't add type classes and passes around all those method dictionaries automatically for you. For example, this perfectly nice haskell code: foo :: Monad m =&gt; Int -&gt; Int -&gt; m Int foo x y = do a &lt;- f x b &lt;- g y return (x+y) would in a more traditional language desugar into something really ugly, along the lines of function foo(m, x, y){ return ( m.bind(f(m, x), function(a){ return m.bind(g(m, y), function(b){ return m.lift(a + b); }); }); ); } This is specially the case if you want to write code that is abstract over the choice of monad, like the functions in Control.Monad. For example, [Javascript promises](https://www.promisejs.org/) have a monad-like interface with the "then" method standing for "&gt;&gt;=" but there is no static guarantee that "then" returns a value in the same monad as its input (the method calls are all dynamically dispatched) and there is no way to write a polymorphic function that uses "return" aka "pure"
I see, I thought I misunderstood what you said. Still you would, in essence, be making up your own evaluator (ie. one that isn't GHC) so it might not be helpful in debugging a crash in a GHC compiled program. It would be a fun thing to play with though. Why do you say that a debugger that evaluates in the same order as GHC is terrible ?
I don't think the core libraries team avoided discussion. I do think that they are _very good_ at thinking libraries proposals through, and _really bad_ at coordinating and sharing discussions in an organized rather than haphazard way, and this is quite unfortunate :-) It would be good for someone to go and pull together /u/edwardkmett and other's answers here and elsewhere on this into a FAQ that summarizes the changes, and the answers to the various "why not just..." that frequently get raised.
I love the fact that we have the level of confidence in Haskell that allows us to conclude "It compiled. It's probably working 100% then."
&gt; SRFI process actually requires a reference implementation to proceed. I'm not saying we don't need to require a reference implementation before the feature appears in a rubber-stamped, official standard. I think that is actually invaluable. If you don't require that, you get the mess that was CSS 2 (great idea; unimplementable). I'm saying that the public drafts need to be foward-looking and (one of) the target of existing implementations. Implementation issues and innovations feed the draft revision process and vice-versa. When there is a complete and correct implementation, then the current draft gets rubber-stamped and becomes the new standard. (And the complete and correct implementation becomes the "flagship" implementation until the next standard comes out.)
What the hell are BBP and APM? I'm guessing APM is related to Applicative and Monad?
Even the AMP warnings weren't perfect, they missed rather critical stuff like `(&lt;*&gt;)`.
:: Maybe Pun 
C / C++ like -- One standard, at least 2 competing implementations (gcc, clang) that are both "production quality", wide adoption, growing awareness of the differences between the specification and the implementation, new standards (of varying impact) every 3 - 13 years.
This is great! Though, when I was trying to understand Haskell for the first time, the difficult thing to understand was not what the 'IO a' type is, but what the 'do notation' does with it. Any program I looked at used the do notation in a way that I could not reproduce until I figured out how general monads work (since 'do' is usually explained via &gt;&gt;=). This was very frustrating. So, I wonder if it is possible to create a guide for beginners about 'do notation'. It should explain how to use it with IO first, then what it does with Maybe or List, and finally how it relates to &gt;&gt;= in an optional sections at the end (one about how to translate other's code written with &gt;&gt;= into do notation, and then one saying how to create a type that works with do notation). Basically, it would be the linked article written in the opposite order and expanded. The challenge, of course, is to have it still make sense. P.S. My imperfect attempt to explain this is at the bottom of section 8 of http://learnxinyminutes.com/docs/haskell/, but it needs expansion to be useful. 
Terrible is probably an exaggeration. I was thinking about the GHCi debugger, which I find confusing to the point where it's not useful to me at all---it seems it just skips around to random places in the program. But probably that's also because it doesn't display a call stack. A "substitution stepper" would show you what context you are in, as part of the term, so even if it only ever evaluated in CBN order, it would still be useful. You are right that being a separate evaluator, not GHC, is a big problem. My thinking before was that this is unavoidable, i.e. that if you just worked from the x86 compiled binary that GHC made, you couldn't possibly line up execution traces with substitutions in the original source Haskell program. But reading some more, it seems like Bernie Pope's [buddha](http://www.berniepope.id.au/docs/BerniePope.PhD.Thesis.pdf) did something pretty close to that, so maybe it's not as impossible as I though.
Does the pseudo-`Num a =&gt; Num (a -&gt; a)` instance suit your concerns? It and /u/htebalaka's [instance from the other comment](https://www.reddit.com/r/haskell/comments/2tsdwi/num_instance_for_functions/co1ybpm), which I haven't worked through, are the only ones I've seen with sensible `fromInteger` definitions. Or if you want to hop over to the Burning Bridges thread, we could talk about dropping `fromInteger` from `Num`. (I'm kidding about jumping to the other thread.)
NB: The new members of `Foldable` are largely a separate concern, they just happen to affect the same class, but were the subject of an independent `libraries@` proposal that spun out of an issue raised... by you.
Oh, that would explain why I've never seen an `Arrow` parser out in the wild despite seeing some papers on it.
There is pretty much only one major parser out there that still uses arrows. https://hackage.haskell.org/package/hxt
This is probably a great time to mention the libraries committee and the libraries mailing list - if you want to have input into these kind of things in the future, why not sign up to the libraries@haskell.org mailing list? I understand that some people might be too busy to sign up to yet another mailing list, or that some people may have missed the formation of the committee or otherwise be unaware of the role it plays. There's been public discussions about this from May 2013 (at least) that anyone can contribute to. There's a lot of discussion on the mailing list by people who seem to really know their stuff - which equates to a lot of volunteered time. I'm not saying that's a reason to ignore any huge red flags that people might find. Just that it'd be great if some of the people bringing up counterpoints to the BBP were around on the list for the next big effort so that things go more smoothly / cause less surprise in the future. I kind of figured (at least for me) that not volunteering my time means that I'm implicitly trusting the output of the libraries committee. But that's just my own view of the relationship between myself and the libraries committee, based on how I think about time management and open source volunteering. I'm sure others will have different views :) (I'm also assuming that being too busy is a variable which is independent of being pro- or anti- some particular decision being made on the libraries list, and hence that the decisions made by the libraries people probably broadly reflects what would have happened if all of the busy folks were involved as well.)
I'm curious about what those JS promise proposals were about. I always found it a bit funny that `then` is this weird mix of `fmap` and `&gt;&gt;=`.
Just yesterday there was a post on implementing monads in dynamically typed languages that shows pretty clearly that they don't need to be cumbersome. You can define your functions in a monad-agnostic way, then decide which one to use at the point of running a computation, and let the library do your dictionary passing.
In a conversation with co-workers about these changes, I fired up a ghci session to show the difference between `mapM` and `traverse` and what it would mean to generalize the functions in Prelude, and how traverse was just a more general mapM. Here is how that went down: &gt; :i traverse Top level: Not in scope: ‘traverse’ Perhaps you meant ‘reverse’ (imported from Prelude) Ah, of course. I forgot the import. &gt; import Data.Traversable &gt; :i traverse class (Functor t, Data.Foldable.Foldable t) =&gt; Traversable (t :: * -&gt; *) where traverse :: Control.Applicative.Applicative f =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) ... -- Defined in ‘Data.Traversable’ Great. Now let's look at mapM: &gt; :i mapM Top level: Ambiguous occurrence ‘mapM’ It could refer to either ‘Data.Traversable.mapM’, imported from ‘Data.Traversable’ or ‘Prelude.mapM’, imported from ‘Prelude’ (and originally defined in ‘Control.Monad’) Oh. Okay we'll check out the one in Prelude by prefixing it. &gt; :i Prelude.mapM Prelude.mapM :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b] -- Defined in ‘Control.Monad’ &gt; :t traverse traverse :: (Traversable t, Control.Applicative.Applicative f) =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) Now let's specialise `traverse` for Monad and List, to show that it's equivalent. &gt; :t (traverse :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b]) &lt;interactive&gt;:1:2: Could not deduce (Control.Applicative.Applicative m1) arising from a use of ‘traverse’ from the context (Monad m) bound by the inferred type of it :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b] at Top level or from (Monad m1) bound by an expression type signature: Monad m1 =&gt; (a1 -&gt; m1 b1) -&gt; [a1] -&gt; m1 [b1] at &lt;interactive&gt;:1:2-50 Possible fix: add (Control.Applicative.Applicative m1) to the context of an expression type signature: Monad m1 =&gt; (a1 -&gt; m1 b1) -&gt; [a1] -&gt; m1 [b1] or the inferred type of it :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b] In the expression: (traverse :: Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b]) Curses! Looking forward to AMP. Insert explanation about how Applicative should be a superclass of Monad. Pretend that it really is. Wave hands about. &gt; :t (traverse :: Applicative m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b]) &lt;interactive&gt;:1:14: Not in scope: type constructor or class ‘Applicative’ Okay this is getting silly. &gt; import Control.Applicative &gt; :t (traverse :: Applicative m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b]) (traverse :: Applicative m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b]) :: Applicative m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b] This is *really* *really* common teaching/learning scenario using the current state of the Prelude. 
Well where are the other 2 implementations going to come from!? Who is proposing to write them?
Isn't this (the re-export thing) the kind of problem that that fancy Backpack dependency language is meant to solve?
I'm not sure I can see how to apply Backpack to this problem.
C# is in the same situation. For fun I tried writing a [Maybe type](https://github.com/jb55/Data.Maybe.cs) for use at work a couple years ago, but it required re-implementing the entire monad stdlib. You have to write an implementation for sequence, mapM, guard etc. for every monad you define. This is ridiculous of course, but you can do it.
All of `Data.List` also changed.
Well, that's heartbreaking. I live in Boulder, but I'll still be in school in Seattle then. Bummer.
JHC and UHC already exist. I think JHC is clasest to being "production ready", and it's already a solution is you want to target a foreign architecture that has C99 support, but not GHC. Clang didn't exist for quite a while, then it did. (I think because Apple didn't like the gcc license?) Those same reasons might cause a new Haskell compiler to come about. But, that's *can't* happen in an environment where everyone says "Haskell" but means "GHC" and the report is effectively left to languish. Finally, a standards-based language makes it easier (vs. implementation-defined) to do experimental things like Haskell on the JVM / CLR or even mess with the other side like generating Haskell. It's not necessary, but it does make things easier, especially if there's some reason you can't / won't start with the "flagship" compiler's source code.
I agree, there are many imports such as applicative we should consider being imported by default. (Not sure if part of BBP)
It seems that what has emerged most clearly from this tempest-in-a-teapot is that the 'usual libraries proposal process' and whatever is being called the 'core libraries committee' are totally opaque to the community and deeply irrational. I follow typical haskell venues, but only realized this was happening when stuff broke with ghc-7.1-rc1; it seems to have been the same with people like Lennart A and a bunch of other notables. Surely the very first thing to do when there is a non-esoteric proposal is to link the discussion *here* (not that I care for this site!); only then is there likely to be any genuine discussion.
Actually when I saw the fish eating other fish, I was wishing for an avian monad tutorial, so that there would be a turducken.
`Vector`, `Bytestring` and `Text` all support `foldMap :: Monoid m =&gt; (Elem t -&gt; m) -&gt; t -&gt; m` -- the 'essence of Foldable' -- and a pile of stuff that can be defined by it or is interdefinable with it.
`Applicative` comes into `Prelude` as part of the AMP.
Strange, I never use Vector or Text, and only rarely ByteString. 
The report was left to languish after the competing Haskell implementations considered production ready started to dwindle, not vice versa. Restarting the prime process requires parties interested in forcing a common standard. It will happen as a consequence of more, more serious implementations, and its lack is not an impediment to them. You also neglect the fact that we still have a standards based language. That standard just happens to be the relatively conservative Haskell 2010.
The subtle issue with generalizing an argument that was monomorphic to one that is polymorphic is that you might have had code that depended on `f = []` holding, that built and consumed with just generic combinators and map. e.g. some cases like Foldable.sum $ map (+1) (pure 1 &lt;|&gt; pure 2) could break. a few similar cases where you build up a structure using only `MonadPlus` or `Alternative` and consume them with `Foldable` combinators suffer now. Not a terribly strong argument, but since we actually had a fairly large task on hand with making AMP work, FTP, etc. we actively decided to back off on it. Making `map = fmap` doesn't change the existence of `fmap` as the member of `Functor` that you define, so we decided not to needlessly change this one combinator. It was a fight we didn't have to fight, so we didn't.
Indeed, there are a lot of similarities if you squint a bit.
Rust is a good case for why most languages don't have monad/functor/etc. based libraries. If you have a strong type system, then HKT is pretty much required. With that in mind, it's important to realize that higher-kinded type systems aren't trivial. Here in Haskell-land it's easy to forget that Haskell is one of the only languages to *have* higher-kinded types.
That was just an expression of gushing fandom for Lennart A's sake; I certainly recommend giving it a try though! Everything I write is 20 times as fast since I learned about it ... though maybe I just got better at writing Haskell in the meantime... Its [649 dependencies](http://packdeps.haskellers.com/reverse/vector) might signal that it it's worth a look.
&gt; It's a pretty small and dark room. Could we use more active libraries@ mailing list participation? Absolutely! Casey was referring to the General Principles mentioned in the first section here: https://wiki.haskell.org/Library_submissions &gt; but thought it was supposed to be restricted to maintainers of ghc boot libraries and the like; I'm not sure who is supposed to be using it or voting on it. The libraries list is open for participation. The +1/-1 process on proposals is mostly to figure out where folks stand on a particular issue. From the library submissions guidelines: &gt; The more effort the proposer invests (for example, by constructing a patch rather than making an off-the-cuff suggestion) the more consideration s/he can reasonably expect. &gt; Proposals that have widespread support, and are accompanied by patches (preferably with tests and documentation), should normally be accepted by the maintainer. So, as a rule of thumb the maintainer for a package is advised to take community opinion as strong guidance, but we as a community tend to avoid running things along straight up/down votes. &gt; The maintainer is trusted to decide what changes to make to the package, and when. They are strongly encouraged to follow the guidance below, but the general principle is: the community offers opinions, but the maintainers decide. The notion of having a maintainer of a package arose for the most part organically as part of the Haskell Platform, folks looked around and gathered a bunch of packages that were considered the 'best of breed' packages for stuff we wanted to ensure worked together. Those packages were written independently by a rather wide array of folks. Over time, some folks stopped working on packages, some stuff had been maintained by GHC HQ and was largely let go. Nobody was actively maintaining a large chunk of the actual code that makes the platform go. After that reached a head around the time of the "Burning Bridges" proposal and when it seemed like the Applicative/Monad proposal was going to stall yet again after years of trying, a committee was formed to have a collective "maintainer" for this stuff that otherwise would only change when someone at GHC HQ saw a patch and merged it in. The job basically consists of reviewing hundreds of patches in phabricator submissions, working to resolve trac issues, trying to achieve consensus on libraries proposals, trying to make obvious progress where we can without breaking stuff, while avoiding thrashing back and forth, and generally moving things along.
It's not just about bottoms. There are cases where both `toList` and `foldMap` are defined, but `foldMap` is more efficient. For example, if we define a (total) `last` in terms of `foldMap`, then the we can use it to get the greatest element of a `Set` in *O*(log *n*) time, but via `toList` only in *O*(*n*) time. (Apologies if this has already been pointed out).
We could do that. It would however, necessarily break more code. 1.) `foldr` implemented just in terms of `foldMap` can use more space, and leak stack. 2.) Ripping out the pre-existing members of `Foldable` breaks about 80% of all of the instances that were already there. 3.) Most of the new members, such as `sum` and `product` were added to avoid changing semantics on existing code. The Prelude version and the Foldable version folded in different directions. This would require us to silently change the semantics on code that existed in the wild. Given the already large nature of the class, we shot a thread out to the libraries@ mailing list about adding the new members and went down that road instead. 4.) As a knock-on effect to #3, rwbarton raised the issue that they could improve the asymptotics for many Foldable combinators, which dfeuer pushed out to the libraries list: http://osdir.com/ml/libraries@haskell.org/2014-09/msg00067.html My objections on that front were overcome and those were added as well. Ultimately Foldable "is" `foldMap` with a few uncomfortable compromises added. Removing the historical members is pretty much a non-starter in terms of the sheer amount of code that would break. So that leaves #3 and #4 above as parts of the bikeshed that we might want to compare color swatches on. One could definitely argue that the wrong decision was reached on items #3 and #4 when they came up on the `libraries@` mailing list and that changing the output of user code would have been better. Honestly, it is a pretty near call as to which is the lesser of evils, but we as a rule try to avoid changes in semantics that would stealthily change code that worked into code that compiles but now yields a different answer.
They're MonoFoldable though! I'd argue the new Prelude changes don't go far enough, and they should use MonoFoldable too. ;) But I'll take what I can get. 
I still believe lenses are the most appropriate answer to this. You can parametrize `sumOf`/`forMOf` (and similar operations) with a lens pointing to the element you want to iterate over, so it works with both `Foldable`/`Traversable` containers and things like `ByteString`/`Text`. You also get `MonoFoldable`/`MonoTraversable` as a special case where you use the `each` lens.
I agree. Something like what happens with C/C++. The problem is that Nothing is to GHC as Clang is to GCC. Sure, other Haskell compilers exist, but the more people that only consider GHC worthy, the more people that will use GHC-only extensions. The more libraries that only support GHC, the more people that will be forced to use GHC. The worse off all the other compilers will be. The vicious spiral continues... I personally try to avoid ghc-only extensions whenever possible, but I often feel rediculous doing so. Like I've got an arm tied behind my back. At least GHC is an open source project, so if things get too out of hand, it can be forked. Maybe a fork could be the basis of the new compiler that keeps GHC in line. Then again, if the standard is stagnant, who can blame GHC for going their own way?
&gt; Everything I write is 20 times as fast... Ah, speed, gotcha. I'll have to check it out. &gt; Its 649 dependencies... [...](http://fadwebsite.com/wp-content/uploads/Scared-Cat-Wallpaper.jpg)
Those Promises/A+ threads were very painful. As someone who writes both Haskell and JavaScript, I really wanted them to rectify the `then`/`chain`/whatever-you-want-to-call-it situation.
But that's 649 _reverse_ dependencies, ie. packages that depend on `vector`.
How does `return` work in a language without a type system? If it doesn't work, how do you build up the reusable blocks like `replicateM` that make monads worthwhile as a concept?
It is great if you already understand monads, I thought it was entertaining! The only problem is it implies that only bigger fish can swallow smaller fish, so at first I thought this was explaining monad transformers, not monads. But monads can only swallow other monads of the exact same type, so that would be like saying a fish can only swallow other fish of the same size. Also the "bind" operator is not explained, which I think is pretty essential. Maybe "bind" happens when a fish lets the shrimp make a copy of the rock when he swaps it out so other fish can swallow the copies before being swallowed by other fish, and this is a very good metaphor for procedural programming.
While doing my internship at Galois, one of the things I got to do was put together a mini-course on programming in Idris. I hope it's interesting and/or useful. * [First session](http://vimeo.com/117221082) * [Second session](http://vimeo.com/117973383) * [Third session](http://vimeo.com/117979741) * [Notes and exercises](https://github.com/david-christiansen/IdrisAtGalois2015)
The problem with C# and with the new Optional type in Java 8 is that all objects are optional by design so the new class might helpful if you're very strict on never using the standard null, but it doesn't give you the guarantee a value won't be null.
I watched the first session a few days ago, Thank you for doing this. It was very interesting.
I looked through the history, and the only type-inference fix I could find from you was [this one](http://git.haskell.org/packages/Cabal.git/commitdiff/782ba67a9a83c10fd4a5998cfe700aa5d0c96d18). Were there more fixes needed?
Come on... not *all* of `Data.List` changed, please take a look at https://git.haskell.org/ghc.git/commitdiff/05cf18f883bf2d49b53a1d25cb57eff3333eb0c9 (or alternatively: https://phabricator.haskell.org/D229) as well as https://git.haskell.org/ghc.git/commitdiff/1f7f46f94a95ab7fc6f3101da7c02529e1964f24 (or alternatively: https://phabricator.haskell.org/D235)
Can you point to examples where being able to tell which particular `Foldable`-instance is being mapped over is essential for the reader? Personally, if I see a `fmap` I know it's something that can be mapped over while preserving its structure. And most of the time it's a container-ish data structure (like a tree, Map, Seq, DList, list, vector, ...). And I don't think it gives me essential information to know which particular data structure was mapped over. I mostly benefit knowing that the `Functor` laws are holding.
I'm actually curious what kind of programs you write. Cause if I don't use `Text`/`ByteString` instead of `[Char]` I waste at least an order of magnitude in terms of heap-allocation and time-dimension. As on 64bit, `[Char]` requires 24-bytes per character for the `(:)`-spine, as well as 16-byte per `Char` (unless it's in a lower code point, in which case there are pre-allocated shared singletons instances). And in addition to the heap-space overhead, you also get pointer-chasing overhead when scanning a `[Char]`. Moreover, operations in the ~~drop*~~ `take*` family have to copy the `(:)` spine. I really try to stay away from `String` unless for short and short-lived strings where it doesn't hurt much.
Are you really suggesting to break the majority of packages on Hackage with GHC 7.12 (and some even with GHC 7.10 due to `-Werror`) just for the sake to keep the operations in `Data.List` list-specialised? (Instead of the current BBP incarnation which provides a smooth upgrade path breaking as little as possible) Moreover, I already mentioned that [the AMP warnings were not acted upon](https://www.haskell.org/pipermail/libraries/2015-January/024820.html). So what makes you think enough ppl will act on BBP warnings in a timely matter?
&gt;higher-kinded type systems aren't trivial Implementing them well may not be trivial, but just implementing them doesn't seem that hard, considering that C++ accidentally implemented higher-kinded polymorphism via template templates. D similarly has it via its version of template templates. These language just lack other features that would make working with higher-kinded polymorphism easier, like type classes.
I don't think this is really comparable to py2to3. This is a matter of changing one line of code in some modules.
The problem is that the `static`/`unstatic` intro/elim pair doesn't preserve typability under eta-expansion. That is, if `e` has the type `Static a`, there's no way to show that `e` is equal to `static (unstatic e)`, because `static (unstatic e)` will not in general be well-typed (for instance, if `e` is an ordinary variable). With a let-style eliminator, the elimination form `let Static x = e in e'` takes a term `e` of type `Static a` and binds it to a variable `x` which is known to be static. Then you can write the eta-expansion e --&gt; let Static x = e in (static x) which will be well-typed. The loss of eta isn't fatal, but it will hinder equational reasoning and optimization quite a bit.
I wasn't so much referring to the `py2to3`-tool (where I don't think it matters much if it automates changing only one `import` or more than that as in the case of Python) as rather to the schism the breaking API change lead by having all existing Python2 code broken under Python3.
If the breaking change is too much for people to stomach, we could make a modest change to ghc (in 7.12) to allow some symbols to be preferred over others when multiple functions with the same name are in scope.
As another note about Rust, effort is being put in to make sure the stdlib is backward compatible with HKT in future if/when it's added. There are quite a few things that HKT makes much more elegant.
I'd go for something simple, only to be used temporarily as we transition. So in `Data.List` you'd say something like {-# WEAK foldr #-} foldr = ... If there is several symbols available on lookup, and all but one are weak then you pick the non-weak one. Otherwise, error as usual. This is a pretty ugly mechanism, and I'd only encourage it for the situation we'll have with `Prelude` and `Data.List`.
No one opposes `Data.Foldable` and `Data.Traversable`. It's obvious that since they attach to things of kind `* -&gt; *` they will have numerous pleasing properties. The question is what they are doing in the Prelude.
Also interesting (for comparison purposes imho) might be - [lmod](https://github.com/TACC/Lmod): a modern implementation of environment modules, allowing you to only see certain 'relevant' packages in your path, depending on what sort of lower-level tools (e.g., a compiler, MPI libs, ...) you are using. - [spack](https://github.com/scalability-llnl/spack): a software build and installation system similar to [easybuild](https://hpcugent.github.io/easybuild/). Both allow multiple versions of libraries to co-exist, e.g., by making use of environment modules to expose what should be used.
For context, the bit about the maintainer having the final call was mostly about deciding *not* to do something--even with overwhelming support for making a change, the maintainer wouldn't be obligated to make the change if they thought it was a bad idea. There's certainly no expectation that a maintainer would make their own changes unilaterally, especially on critical libraries like `base`. Note that, in this case, "democratic common sense" was firmly on the side of making the kind of changes being discussed now.
I think *you* are joking, but if most people using GHC call it Haskell, then Haskell well be a implementation-defined language. If that happens, there are people that will avoid it. For people coming from PHP, a specification is clearly not a priority; since PHP doesn't have a specification either. It does, at least, have multiple implementations, but they have multiple incompatibilities and there is not guideline to avoiding those incompatibilities. For people coming from Fortran, C, C++, or COBOL, it's may be more valuable. They've probably had to migrate between compilers at least once, and they know the value of portability and writing to a specification instead of to an implementation. But yes, when GHC 7.8 stops being maintained, there will no longer be a maintained Haskell2010 compiler, unless JHC or UHC add support before then. Simply making the AMP non-optional means that valid Haskell2010 code no longer compilers in GHC 7.10.
What are you, a constructivist or something? :) We don't need concrete examples in order to reason about what will happen. Less free type information =&gt; more work for the reader to do to reconstruct what instance is being used =&gt; code becomes harder to read.
Can Haskell steal Elm's solution? Because Elm's solution is really, really good.
Awesome libraries :D
It isn't a "kind of change", but the quite particular apotheosis of `Foldable` that I take to be Lennart A's topic. This is what has taken us all by surprise; maybe it's a good idea; anyway it's going to happen. The overwhelming majority of the discussion of this point was due to ten -- fully half of it to *six* -- people, though thirty nine seem to have been informed at the time. 
Currently, the approach Cabal takes is the packages can describe version ranges of other packages they depend on. In general, many different compositions are possible. It is then someone else's task to determine which versions you actually use together. This can be done by employing a dependency solver such as in cabal-install, it can also be (partially or completely) done by publishing known compatible sets of packages (this is somewhat what Stackage does, and will to my knowledge be further discussed in part 3 of this blog series). Nix itself does not have any facility for resolving dependencies. It is required that you tell it in advance exactly what versions of packages you want to use. For NixOS, this is done mainly by Peter Simons and a few other developers who maintain package sets for several compiler versions.
That's just a Java problem. In C# you can make value-typed classes (struct keyword) which can't be null. Without that feature the lib would be useless.
Very well-written post, thanks. One small thing that I would suggest is to put author's name either on top or bottom of it, since currently there's only link to username `duncan`, I think that would be nicer.
`algo.monads` is probably the worst of the existing Clojure monad implementations, sadly.
IIRC, we're currently just exporting the class. To do otherwise violates the expectation that you can define an instance with the bits and pieces given to you.
I'm not heavily invested in the current "practical compromise" design of `Foldable`. I'm happy to be let folks shout it out, I'd even personally +1 the proposal to go with a 7.8-style design. I happen to like simplicity, too. If we're going to change the semantics of all the code that folks have out there that use `Foldable`, we should at least propose it through the libraries list, so if folks have a reasonable objection it can be heard. I'd like to point out though that the "other side" has arguments that aren't completely vacuous, though. &gt; (where "right" is the semantics prescribed by the Prelude) The is a contingent of folks who believe that `sum` should be built with the `sum = getSum . foldMap Sum`, which has non-obvious benefits that there exist `Foldable` containers that can use that to get sub-linear folding times or fold in parallel. There is also yet another fairly vocal contingent who believes that `sum` should be defined via `foldl'` to avoid space leaks. The current "messy compromise" enables both of these groups to fix their containers on a one-off basis, but this is an incidental benefit at best, not one essential to the nature of "Foldable". Finally, `Foldable` is unique in the classes on * -&gt; * that we offer in that because `f a` only occurs in negative position we can know properties of `a`. This means that there can exist quite a few foldable containers for which `sum`, `product`, `minimum`, `length`, `elem`, etc. can be `O(1)` or `O(log n)`. This was the substance of Reid's proposal that led to much of the expansion of an already rather rotund class. So with all that in mind, we can see that we have four major viewpoints about the correct definition of `sum`. The minimalist in me would prefer to avoid the current design, but three of those four views are served by having it in the class, and offer pragmatic benefits in terms of asymptotic performance or space utilization.
Thanks!
I didn't do the video recording and processing myself, so I'm not really sure.
They've been in active use by the community for several years, provide deep insight into the nature of containers, have laws that have proven to have remarkably deep roots, are intuitively easy to use, neatly sum up operations that long predate them, showing connections between all sorts of seemingly unrelated 'common sense' combinators, and the notion of putting them in the Prelude proved to be overwhelmingly popular, and they finally let Haskell's use of several terms out of the box be almost as usable as Python's. The question is what are they doing exiled to a rather undiscoverable corner of `base`, while Prelude is making them harder to use. ;)
This is the best description of the cabal problems I've seen so far. Very good pictures too.
How do you know the readership? I know for a fact a number of people read the thread that aren't listed there.
I'm not opposed to having sum in a class, but I think it should be a special class. And I think it's up for debate what else should be in that class. I'm surprised Foldable had a definition that deviated from the Prelude in the first place. 
Good to know. Was this feature always in the language? Worked with C# in the past but haven't noticed it. Anyways, if it's there, it's at least a step in the right direction!
Neat. The listing isn't clear, but is relocation provided for people outside the EU?