Ah, you’re right, I misinterpreted. Apologies for the unnecessary vitriol. That said, I would personally prefer that all text functions “do the right thing” with Unicode or simply don’t exist at all, so I might actually prefer not exposing a `reverse` function if it’s going to operate on code points, since that feels like an easy way for people to accidentally shoot themselves in the foot. I would be fine with making `pack . reverse . unpack` the only way to do a codepoint-level reverse. There could even be a rewrite rule from that to a more efficient `reverse` function, but by not exposing it, you at least force people to think about what reversing a list of `Char`s means.
I would give it a shot. Come up with a title, and a short abstract and see if the reviewers consider it worthy. Last year I've given a talk on the Cmm to LLVM IR backend via a plugin and a potential extension to plugins that allows to add backends via plugins.
Hi, sorry for the trouble you run in, and sorry for the late reply. I'd also like to encourage you to open these kinds of problems as issues on the issue tracker on https://github.com/zw3rk/ghc in the future. 1) hmm that is indeed strange. However I believe what you did should work properly as well, as those repositories are mirrored. 2) yes, that should work as well. 3) also right, those "errors" can be ignored. 4) ensure that the directory you provided as `--prefix` when building libffi, contains the `lib` and `include` subdirectories, with the libffi.a / libffi.so and the headers presents. those are the directories ghc expects for the `--with-libffi-libraries` and `--with-libffi-includes`.
&gt; What i have in mind is essentailly the game loop situation e.g. 'a load of permanent data that persists between frames, If the data is truly permanent and doesn't need updating[1], you can store it in a [compact region](https://downloads.haskell.org/~ghc/master/libraries/html/compact/Data-Compact.html). Then, it is not checked by GC. [1]: although, compacts do allow adding data
i think many of us feel similarly, perhaps you could stay engaged in the open source and meetup communities?
It should be coming soon.
I'm afraid even `pack . reverse . unpack` won't work for composition chars since a composition char will occupy multiply `Char`s in a list, so `pack . reverse . unpack` will be identical to current `reverse`, even doing composite normalization is not enough since normalization won't combine all the composition char. The only proper way to handle it is base on a lookup table and introduce a state into reversing process, that will be definitely slower, but like you said, it worth that. 
About Twitter, it's disgusting that you need hundreds of megabytes of memory to display tweets that are 140 characters. That gives you 1 million bytes per character, talk about wasteful. 
&gt; `pack . reverse . unpack` will be identical to current `reverse` Yes, but that’s sort of my point: since it’s too hard to fix it to just do the right thing, unambiguously, I think it’s better to make it hard than to make it easy. Having to roundtrip through `[Char]` makes it explicit that it’s just doing a codepoint reversal, and it means the `Text` API doesn’t have any footguns.
[removed]
&gt; If the data is truly permanent and doesn't need updating yes thats the scenario ,more or less. another variation is the 'streaming engine' with some background task updating dynamically loaded textures etc as you move around the scene. on a timeline, the area of communication is immutable whilst the 'game' is using it, but there could be a window of 'assets-update' where vica versa, the game state is frozen and it's view of the currently available assets are updated. all the referential transparency stuff (expressing the updates with pure functions) should make it quite clear, but what might be a bit hazy with laziness is the mapping to time; so some clear dividing lines at which eager-eval is done would help.
Okay so does anyone else find the comments here defending him weirdly formal? Or is that just me being .. judgemental? 
&gt;does anyone else Probably
It might be easier to combine brick with [haskeline](https://hackage.haskell.org/package/haskeline). Haskeline also supports some basic but useful user configuration, like vim mode. It's also what GHCi is built on, and since GHCi supports multiline text via `:{` and `:}` commands, you might not even need brick, depending on how sophisticated you want the editor to be.
How cool would it be if we could write an alternative to the GHCi REPL with `brick`? IHaskell in the terminal.
I will still definitely be around somewhat and attend meetups sometimes, but I just wrote this to acknowledge to myself that I need to move on professionally for a while. When I was in the middle of ramping up on Haskell at work, besides curiosity, I felt an ongoing pressure to keep understanding more GHC extensions, more complex libraries, more nuances of Stack/Cabal/Nix, GHC profiling, to keep making the convincing case that I could transfer to another Haskell shop (if things went bad culturally, or Haskell lost momentum in the current shop, or I was ready to move up). If there were dozens of Haskell companies, instead of a few, locally, I might not have felt that pressure. But since Plan B usually meant trying to land a remote position, it felt like you had to be a rock star to stay in Haskell. I may chip in some bug fixes to open source libraries as well. I enjoy reading and doing exercises in Algorithm and Complexity texts the most, but you can only focus on that for so much time, whereas hacking together some Haskell or reading a Reddit thread is easier to jump in and out of.
How does this compare to [styx](https://github.com/jyp/styx)?
Anything else would be hypocritical for a community praising the glory of formal methods 
Scotty. Because it's very simple.
Hi and thanks for the reply. I've opened an issue as requested at https://github.com/zw3rk/ghc/issues/2. 
Yesod, because batteries included.
As /u/ublubu surmised, for both `amazonka` and `gogol` fragments of the Haskell AST (individual record labels/fields, types, etc) are generated using `haskell-src-exts` and rendered to text, with anything related to layout then controlled via templates. I'll link some of the related code - but that said, I'm not proud of any of this and I wouldn't recommend copying the implementation from either of the aforementioned projects as it's frankly quite horrible. As an example, a Haskell record (product/struct in schema parlance) is rendered via the following steps (post-parsing/transformations): * The initial `StructF` type parsed from the service schema which corresponds directly to a Haskell record: https://github.com/brendanhay/amazonka/blob/97552c16faabccbc2a2e95b3cd5c0b2b2c19062d/gen/src/Gen/Types/Service.hs#L276 * The output `Prod` type, corresponding to an input `StructF`, post AST transformations and rendering of the `haskell-src-exts` AST for this particular type: https://github.com/brendanhay/amazonka/blob/97552c16faabccbc2a2e95b3cd5c0b2b2c19062d/gen/src/Gen/Types/Data.hs#L53 * The transform from `StructF` to `Prod` + `[Field]` happens here: https://github.com/brendanhay/amazonka/blob/97552c16faabccbc2a2e95b3cd5c0b2b2c19062d/gen/src/Gen/AST/Data.hs#L157 * Various ancillary functions and combinators relating to the Haskell AST fragments: https://github.com/brendanhay/amazonka/blob/97552c16faabccbc2a2e95b3cd5c0b2b2c19062d/gen/src/Gen/AST/Data/Syntax.hs * The larger `Service` AST with `StructF` (and others) replaced with `Prod` and their corresponding rendered Haskell AST fragments are passed to an [ede](https://hackage.haskell.org/package/ede) template as serialised JSON variables: https://github.com/brendanhay/amazonka/blob/97552c16faabccbc2a2e95b3cd5c0b2b2c19062d/gen/src/Gen/Tree.hs#L91 https://github.com/brendanhay/amazonka/blob/97552c16faabccbc2a2e95b3cd5c0b2b2c19062d/gen/template/types/product.ede https://github.com/brendanhay/amazonka/blob/97552c16faabccbc2a2e95b3cd5c0b2b2c19062d/gen/template/_include/product.ede Having gone down the AST fragments + templates route for amazonka/gogol, I've recently implemented a more expansive project using only `ghc-exact-print`. The end result is arguably more robust but it feels considerably more troublesome to get any kind of pretty / human authored layout compared to the use of textual templates. The template approach is definitely more straight forward to manipulate layout and perhaps using a decent compile-time checked template approach such as dhall or quasiquotation you could get the best of both worlds.
Twitter isn't a static page displaying 140 characters of text. In any case, a browser tabs RAM usage isn't usually dominated by the size of the website it displays anyway.
Both `haskell-tools-ast` and `ghc-exact-print` are built atop the `ghc` API - same-same but different.
`haskell-src-exts` still offers a better API - but typically has problems keeping pace with `ghc` proper. If you're not planning on rendering syntax relating to bleeding edge extensions then it's fine, otherwise I'd recommend using the `ghc` API if you can stomach it. There are a few packages designed to offer a nicer interface, such as the previously mentioned `ghc-exact-print`
More related to the other comments about the lack of 'nice' layers atop the `ghc` API, one problem is the source code formatting/positioning libraries all tend to assume you're roundtripping code. That is, reading in and then modifying code (such as a formatter or maybe linter) before outputting it losslessly. Most of the time when rendering or traversing programatically generated code you don't care about location information and simply want to write everything out relatively positioned. I'm going to assume this already exists and I've missed it, or it is somehow more trivial than I thought to build this atop `ghc-exact-print` or similar. If anyone has encountered this before or solved it, please chime in.
I'm always happy to see Haskell screen casts; just being able to watch the thought process and workflow of other people (especially professionals) is really helpful. Btw. it would have been even better if the title advertised it was a screencast. I expected a long-form article ;)
And it was a typo, sorry about that :'( So the only things I (had to/did) do differently were 1. Use stack not cabal 2. Get ghc path by running stack exec -- which ghc 3. Ignore libffi build errors (-print-multi-os-directory and no input files) 4. Change hoopl submodule link to https://github.com/haskell/hoopl.git. 5. Make sure that the correct raspbian-sdk/prebuilt/bin is in the path :'( 6. On the pi, run: sudo ln -s /usr/lib/arm-linux-gnueabihf/libffi.so.5 /usr/lib/arm-linux-gnueabihf/libffi.so.7 Thanks again
It has its problems. 
Can also be used with modifiers like [`Reverse`](https://hackage.haskell.org/package/transformers-0.3.0.0/docs/Data-Functor-Reverse.html) and [`Backwards`](https://hackage.haskell.org/package/transformers-0.5.4.0/docs/Control-Applicative-Backwards.html) infixl 5 ::: data Snoc a = Lin | Snoc a ::: a deriving via Reverse (Foldable, Traversable) and other modifying newtypes like [`Down`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Ord.html#t:Down).
This should definitely be a thing! I've not used iHaskell before, how replicatable do you think it would be?
For some reason you have to install PostgreSQL if he's asking you for pgconfig. Not sure why but try that. Also, I'd recommend VS Code with Haskero extension. SublimeHaskell is kind of behind other solutions.
&gt;Haskell and other fringe languages have usually been my second passion, after some topic within CS like AI or Algorithms. It's exciting being able to use really interesting libraries and continually sharpen my type skills. Haskell's not *that* obscure, all things considered. &gt;Hopefully if/when I return, there will be so many companies using Haskell on a wide spectrum of domains and complexity of problems tackled, that there will be more Haskell jobs than there are people who want to work in Haskell. If you're looking for a Haskell job, I'd recommend taking on some independent projects to learn stuff like lenses and recursion schemes. A lot of hobbyists just know monad/monad transformer knowledge and less beyond that. It's not *entirely* fair to expect you to teach yourself in your free time, but it's one way to take control of your career beyond just waiting. 
Why would Gtk2HS be insufficient for a windows application?
&gt; I'd like to introduce all the major roguelike features. Just a heads up, you should create an email filter autodeleting everything containing all of the words "I actually think roguelike should mean" in any order.
&gt;In any case, a browser tabs RAM usage isn't usually dominated by the size of the website it displays anyway. No, it is dominated by the huge massive mess of spaghetti javascript twitter foists on you to display an incredibly simple tiny little page that requires only a few lines of javascript as an optional enhancement.
It appears to be `~/.config/yi/yi.hs` (according to [this](http://yi-editor.github.io/posts/2017-01-06-dyre/) and [this](https://github.com/yi-editor/yi/blob/44aca2ccbd8aa6c881fd69d46cb52a23aff7aa9e/yi-core/src/Yi.hs#L18)).
Thanks for the elaboration! I actually looked at haskeline and brick separately before I wrote my post and it hadn't occurred to me to combine them back then, I guess I'm still getting used to how composable Haskell libraries are! Just noticed you posted a brick tutorial recently - that'll be useful! 
Yet another JS framework?
How much information should I give in the abstract? Does a link to the full text help? Should I give links to examples/usages of the topic in the wild?
I'm not sure there are any sweeping conclusions that can be made, but I'm curious to hear people's thoughts on why this is the case. 
 data Order = Order { _orderData :: Blah , _customer :: CustomerId , _salesperson :: Maybe SalesPersonId } getCustomerOrder :: CustomerId -&gt; [Order] -&gt; [Order] getCustomerOrder cid = filter ( (==cid) . _customer ) The typeclass idea is probably bad (see [this](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/)). 
Well it's faster when number of entries is &lt;100 then rust begins to pull away.
Yes, but realistically, there shouldn't be *any* functions where Rust is slower, unless GHC does some crazy optimizations (which I guess it does). And, as I mention in the post, performance of a program is dependent not only on input size but also how often you call it, which could cause a program written in Rust to be suboptimal.
I used javascript for a while, including working on a few medium-sized programs in a team as part of a full time job. Now that I also know what it's like to have powerful static typing which helps guide you to design better programs rather than just getting in the way, as well as catching mistakes *before* your code goes to production, I'm just not convinced that any additional investment in writing nontrivial programs in javascript would be worth my time, really. I believe that a good framework could make a huge difference, certainly, but it can't fix the problems in the language.
I mean if you wrote Rust code that did the same thing, sure. But you're comparing a linked list data structure to presumably some sort of vector data structure. They're fundamentally going to operate with different performance characteristics, even if they were written in the same language. Try comparing the Rust code to strict Text in Haskell, or the Haskell code against linked lists in Rust. Also make sure they're doing the same work to account for unicode EDIT: You also need to try to reduce the use of `(:)` so you can get sharing to reduce GC pressure. suffix :: [a] -&gt; [[a]] suffix [] = [] suffix [_] = [] suffix (x:xs) = xs : suffix xs
[removed]
Sweeping conclusion: further research required. Do an analysis of the machine code output of the Rust vs Haskell version to see why Haskell is faster for short input strings.
&gt; I found a benchmark where Haskell is faster than Rust I can find benchmarks that will show literally anything I want. The [Benchmarks Game](http://benchmarksgame.alioth.debian.org/u64q/which-programs-are-fastest.html) is one of the most "fair" language benchmarks I've seen. Communities have had years to optimize their implementations with certain, shared restrictions on what is legal, and so it is pretty representative of things. Since Rust has only been stable for two years, its results are still improving. &gt; note that it must return a vector of owned Strings so as to be comparable to the Haskell. This is not strictly true. Returning a vector of slices would be more idiomatic in Rust, or a vector of Cow (clone on write) at a minimum. In the full GitHub benchmark, the author even chose to implement a version based on returning slices! (references into the existing string) Haskell may very well be doing something similar, since data is (usually) immutable in Haskell, there's no reason to create complete new copies of each chunk of data, when you could just have references. On my machine, the Rust benchmarks look like this: test test::bench_suffix ... bench: 296 ns/iter (+/- 44) test test::bench_suffix_extra_long ... bench: 87,924 ns/iter (+/- 27,474) test test::bench_suffix_long ... bench: 5,948 ns/iter (+/- 893) test test::bench_suffix_medium ... bench: 1,335 ns/iter (+/- 418) test test::bench_suffix_medium_unicode ... bench: 1,414 ns/iter (+/- 204) test test::bench_suffix_ref ... bench: 96 ns/iter (+/- 27) Returning a vector of slices is 3 times faster than returning a vector of owned strings, and that delta would only grow as the size of the benchmark increases, since owned strings would require increasingly large O(n) copies. Using the _ref implementation instead of the owned one, the benchmark changes to these numbers: test test::bench_suffix ... bench: 61 ns/iter (+/- 9) test test::bench_suffix_extra_long ... bench: 2,417 ns/iter (+/- 425) test test::bench_suffix_long ... bench: 281 ns/iter (+/- 37) test test::bench_suffix_medium ... bench: 97 ns/iter (+/- 32) test test::bench_suffix_unicode ... bench: 116 ns/iter (+/- 16) I left out the _ref benchmark at the end of the list because it was redundant here, being the same as the first benchmark. Using Cow&lt;str&gt; in place of &amp;str references in the returned vec, we get these numbers: test test::bench_suffix ... bench: 85 ns/iter (+/- 17) test test::bench_suffix_extra_long ... bench: 8,445 ns/iter (+/- 844) test test::bench_suffix_long ... bench: 832 ns/iter (+/- 61) test test::bench_suffix_medium ... bench: 198 ns/iter (+/- 16) test test::bench_suffix_unicode ... bench: 227 ns/iter (+/- 30) So, there is definitely some overhead from using Cow, but it's much less than using purely-owned Strings for each suffix. Running `stack bench`, I got this: benchmarking suffixes-short/suffix (hylomorphism) time 44.39 ns (44.10 ns .. 44.69 ns) 0.999 R² (0.999 R² .. 1.000 R²) mean 44.48 ns (44.23 ns .. 44.91 ns) std dev 1.046 ns (794.7 ps .. 1.403 ns) variance introduced by outliers: 36% (moderately inflated) benchmarking suffixes-medium/suffix (hylomorphism) time 248.3 ns (247.4 ns .. 249.4 ns) 1.000 R² (1.000 R² .. 1.000 R²) mean 250.0 ns (248.7 ns .. 251.5 ns) std dev 4.895 ns (3.977 ns .. 5.960 ns) variance introduced by outliers: 25% (moderately inflated) benchmarking suffixes-unicode/suffix (hylomorphism) time 285.5 ns (283.9 ns .. 287.1 ns) 1.000 R² (0.999 R² .. 1.000 R²) mean 285.3 ns (283.9 ns .. 286.8 ns) std dev 4.930 ns (4.093 ns .. 6.223 ns) variance introduced by outliers: 20% (moderately inflated) benchmarking suffixes-long/suffix (hylomorphism) time 7.011 μs (6.981 μs .. 7.042 μs) 1.000 R² (1.000 R² .. 1.000 R²) mean 6.999 μs (6.968 μs .. 7.031 μs) std dev 107.6 ns (89.40 ns .. 134.6 ns) variance introduced by outliers: 13% (moderately inflated) benchmarking suffixes-extra-long/suffix (hylomorphism) time 635.2 μs (632.3 μs .. 638.8 μs) 1.000 R² (0.999 R² .. 1.000 R²) mean 639.5 μs (635.8 μs .. 644.1 μs) std dev 13.52 μs (10.70 μs .. 17.39 μs) variance introduced by outliers: 12% (moderately inflated) 
&gt; suffix = hylo algebra coalgebra . drop 1 wat Why not just: suffix = tail . tails
&gt; Returning a vector of slices would be more idiomatic in Rust, Slices aren't sized, so a vector of slices wouldn't compile. &gt;I can find benchmarks that will show literally anything I want. I think you misread the intention of the post. Rust programs I write are consistently faster than their Haskell equivalents, but the fact remains that examples like this will dent Rust performance unnecessarily. I'm also interested in how abstractions can improve performance, though there's nothing quite conclusive here. I addressed comments on references [here](https://www.reddit.com/r/rust/comments/6le7i1/i_found_a_benchmark_where_haskell_is_faster_than/djt4ywj/?context=3). &gt;Using Cow&lt;str&gt; in place of &amp;str references in the returned vec, we get these numbers: The results are less dramatic, but the Haskell is actually still faster on small/medium strings when using a `Cow`. The Rust stops being obnoxiously slow, but it's still not what I would want from a systems language (and I trust the compiler will improve as time goes on). Further, it's *not* equivalent to a `String`; things like appending do not have the same efficiency. Whether or not I made the wrong choice picking `String` over `Cow`, I will have to hear more, but the fact remains that this is either a sign of a weak spot in Rust that we should be aware of, or a sign that GHC's optimizations are quite advanced when you tailor to them. 
A String slice is &amp;str, which is sized, because it is a reference. A bare slice (not behind a reference) would be an unsized type, but that's not what I'm suggesting, and if it were what I'm suggesting, I would not have benchmarks to show, because it would be impossible. &gt; I addressed comments on references here and you're *wrong*, as shown by the responses there. I gave you a full response, if you want to just dismiss it, then there's no point in other people responding. They will tell you the same things.
The config file is a proper haskell project that imports the yi libraries. No config files need to be saved in any particular place. Note: this applies to the recommended static configurations. See [here](http://yi-editor.github.io/posts/2017-01-06-dyre/) for the difference between static and dynamic configurations. TL;DR static configurations are proper haskell projects (i.e. compiled with stack or cabal) and dynamic configurations are xmonad like configurations. So if you just installed yi with `stack install yi` you cannot configure yi. To be able to configure yi you can download one of the example **static** configurations (for example yi-all-static) from [the github page](https://github.com/yi-editor/yi) and remove the `location ../..` field and the subdirs in the stack.yaml (they are there only for the automated travis testing and the people who want to use the github version). Then finally just compile with `stack install`. Please don't be afraid to open an issue on github if you have further issues.
This only applies to dynamic configurations, which are not recommended (by me) anymore.
I sometimes view JS's design as an incentive to keep my scripts small and simple, which mostly ends up benefitting the users too.
i have done repl-like interfaces on top of brick and reflex, see [this example project](https://github.com/lspitzner/frpcli). Note that we use a fork of brick for the reflex-integration. Also note that it is possible to temporarily suspend brick to execute some arbitrary other process, so it should be straight-forward to drop into any commandline editor (like e.g. git does for commit messages etc.).
you edited your comment since I last replied, so here's another reply. &gt; The results are less dramatic, but the Haskell is actually still faster on small/medium strings when using a Cow. The Rust stops being obnoxiously slow The Haskell code is using code that is more equivalent to straight-up String slices (&amp;str) than Cow, but you can still speed the Cow Vec up by using pre-allocation like this: use std::borrow::Cow; pub fn suffix_vec(s: &amp;str) -&gt; Vec&lt;Cow&lt;str&gt;&gt; { let mut vec = Vec::with_capacity(s.len()); for (j, _) in s.char_indices().skip(1) { vec.push(Cow::from(&amp;s[j..])); } vec } It boosts the benchmark results by 30% to 50% faster, thanks to pre-allocation. I would still argue that returning a Vec&lt;&amp;str&gt; is actually the desired result, not Cow, and definitely not String.
Reddit bots get more bizarre by the minute.
This seems like odd advice for a few reasons. The OP already had a job using Haskell for one. &gt; learn stuff like lenses and recursion schemes What is the rationale for choosing these concepts? `lens` is a nice utility library that has some tradeoffs in terms of complexity. Recursion are not on the top of my list for professional Haskeller knowledge. Neither of them seem essential. I suppose they both operate as signals though ...
Thanks a lot, I've been wondering whether or not the two could work together well. Was it difficult to modify brick to integrate it with reflex?
This seems like it smashes boilerplate for a vast increase in complexity. The beauty of something like `redux` is that is creates clear demarcations between state, mutation and observation. Commands flow in one direction, state changed flows in the other, all state change has a single source of truth in the reducer. This technique stratifies state and provides unbounded opportunity for mutation and observation. Count me out.
In PureScript, we use haskeline for the REPL, and we have a special `:paste` command for multiline input, terminated by `^D`. It works quite nicely, I think. The relevant bit of code is [here](https://github.com/purescript/purescript/blob/b3e470deb302f8f400bbe140e600eba5c9e2c2b5/app/Command/REPL.hs#L108-L113).
I have the following in my `~/.stack/global/stack.yaml` to support `stack install hsdev` λ cat ~/.stack/global/stack.yaml flags: {} packages: [] extra-deps: - hdocs-0.5.1.0 - hsdev-0.2.2.1 packages: - location: git: https://github.com/haskell/haddock commit: 240bc38b94ed2d0af27333b23392d03eeb615e82 subdirs: - haddock-api extra-dep: true resolver: nightly-2017-04-07 
It leverages the Jupyter project which has had a lot of work put into the console and the notebook interface. I think it would be difficult.
&gt; Given that Rust is a systems programming language, I hope to see its performance to catch up to Haskell as things progress. ^(^^Jesus ^^Christ.)
I agree with and you are completely right, IMO to embrace static typing and see what difference does it make, its not bad idea to implement a side project in JS and see how things run ... regarding the framework(architecture), do you think that this is a good framework or other architecture as `redux` or `cycle` worth more
what about composition? it claim that every component is compassable and decomposable, to be more specific [example](https://github.com/calmm-js/karet-shopping-cart#summary)
[This is a "haven't used it but here's my opinion anyway" post.] I recently wrote my first Haskell program that made substantial use of lenses. I had loads of type errors, which was great because I had few bugs left after sorting out those errors, but the errors are pretty complicated due to the amount of polymorphism in the combinators. I'm afraid to go deep into a lens-based project without a type checker.
Thank you, this'll be handy in lots of future REPLs!
In theory the event-based design of brick translates in a straight-forward manner into the frp model. But I had to look into brick internals to do this - one cannot implement the reflex interface on top of the public brick interface without adding a completely useless layer of indirection. I ended up reimplementing parts of `Brick.Main`. (and this is essentially why it currently is a fork and not merged upstream.) One might see the current type signature of the interface as a downside - it uses a continuation-style indirection to pass input/output events: brickWrapper :: forall n t . (Ord n, R.ReflexHost t, MonadIO (R.PushM t), MonadIO (R.HostFrame t)) =&gt; ( R.Event t (Maybe Event) -- inputs -&gt; R.Event t () -&gt; (forall a . R.Event t (IO a) -&gt; RH.AppHost t (R.Event t a)) -&gt; RH.AppHost t ( R.Event t () -- outputs , R.Dynamic t [Widget n] , R.Dynamic t ([CursorLocation n] -&gt; Maybe (CursorLocation n)) , R.Dynamic t AttrMap ) ) -- ^ one line :/ -&gt; RH.AppHost t () Removing the continuation would simplify this, but in turn force the user to use `mfix`. By now I think that this would be preferable indeed, it is just so much easier to understand: brickWrapper :: forall n t . (Ord n, R.ReflexHost t, MonadIO (R.PushM t), MonadIO (R.HostFrame t)) =&gt; R.Event t () -- outputs -&gt; R.Dynamic t [Widget n] -&gt; R.Dynamic t ([CursorLocation n] -&gt; Maybe (CursorLocation n)) -&gt; R.Dynamic t AttrMap -&gt; RH.AppHost ( R.Event t (Maybe Event) -- inputs , R.Event t () , ( forall a . R.Event t (IO a) -&gt; RH.AppHost t (R.Event t a) ) ) (but I have not implemented this yet - the current interface works well enough and I am not even aware of other users of this fork) In most cases you want to implement the output events/dynamics (which you pass into the wrapper) in terms of the input events (which you get from the wrapper), so you need `mfix` with the latter interface. (And yes, there now are reflex "Events" and brick (input) "Events", but that is not too confusing in practice.) But really this is not specific to brick - it seems more like a (somewhat open, to me at least) question on how to design frp-based interfaces. And in general the frp-interface that you get has important benefits over an event-based interface: Note how you have to modify the "application event" type whenever you want to add some new custom event to your application in the current brick interface, while you get much better separation with reflex-brick.
This seems plausible. I'm still a bit unclear on some of the details - if you ever write this up into a GHC proposal, please address these concerns. It should be noted that this proposed deriving strategy is a lot more fragile than `GeneralizedNewtypeDeriving`. All of your examples appear to be making some implicit assumptions that the type you're passing to `via`: 1. The kinds happen to match up. For instance, in this example: data V3 a = V3 a a a deriving via WrappedApplicative (Num) There's quite a lot of kind-checking that must happen here behind the scenes. My guess (please correct me if I'm wrong) is that you're checking that the kind of the first type variable to `WrappedApplicative` (`f :: * -&gt; *`) matches the kind of `V3` (`V3 :: * -&gt; *`), and moreover, `WrappedApplicative` has an equal number of remaining type variables as `V3` has type variables (in this case, `WrappedApplicative` has one remaining type variable, `a`, and `V3` has one type variable, `a`), and all of those type variables have corresponding kinds (in this case, `*`). It would be good to write up how this algorithm works. This might work, although it imposes some onerous restrictions on what newtypes you can use for certain data types. For instance, you wouldn't be able to write `data V3 a = V3 a a a deriving via WrappedMonoid (Semigroup)` (using `WrappedMonoid` from [`Data.Semigroup`](https://www.stackage.org/haddock/lts-8.21/base-4.9.1.0/Data-Semigroup.html#t:WrappedMonoid)), since the kinds of `WrappedMonoid` wouldn't line up with those of `V3`. 2. Here's the important bit - it appears that you can't pass just any old newtype to `via`. After all, what if you tried this? newtype Wat a = Wat Int instance Num (Wat a) where ... data NotInt = NotInt Bool deriving via Wat (Num) You'd attempt to derive a `Num` instance for `NotInt` by coercing `Wat NotInt` to its underlying representation type. But here, it's underlying representation type is `Int`! I'm guessing this isn't what you intended `via` to be used for. Am I correct in saying that any `newtype` that is passed to `via` should satisfy this property? newtype N f a_1 ... a_n = MkN (f a_1 ... a_n) That is, the type variables of the newtype appear exactly in the order in which they would be if one were to uncurry the application of the first type variable to the remaining type variables? Perhaps you should come up with a term to describe this property. Again, these are my impressions after skimming the proposal. I have not attempted to read the Template Haskell implementation, so correct me if there are details that I am misconstruing.
Most of this composition is OOP object composition and duck typing at work. This type of design does a great job removing boilerplate, but is less effective in "taming" state. As your product grows you'll undoubtedly begin to accrue more and more complicated interactions between your myriad of stateful objects and unraveling what happened when something goes wrong becomes increasingly complex. The idea behind observables is that they stay in sync via the virtue of the producer subscriber graph, but I've only seen this fall apart once you get past toy examples because of the inherent complexity of the web they weave.
I understand your concern, do you think that [flow](https://flow.org) might work?
It could be summarized in the following way: - Your program start in the browser. - `atRemote` executes its argument in the server and return it back to the browser. - The widgets are made with a rendering DSL similar to blaze-html that generate DOM nodes instead of strings. - `form-element ```fire ``` On&lt;event&gt;` return the value of the element when the event is fired. - The rendering of a widget can be located withing any DOM element with `at`. if not specified, it is aggregated in sequence. - Once the formElement/widget return a value, all that is after it (its continuation) is executed and redrawn. - Widgets may have browser and server elements (thanks to atRemote) and compose with Applicative, alternative and monad combinators
Thanks for the detailed explanation :) I agree with you that this is an interesting open question, and that was another reason why I wanted to hear how you implemented it. The second type is definitely way more readable. What was the reason for why you wanted to avoid using mfix? 
I might simply have not considered mfix/mdo powers at the time; more likely i was following some advice to keep mfix-blocks small (or avoid them entirely) in order to reduce the risk of creating cyclic dependencies between events. iirc these are generally not detected but make the frp-network livelock. (And it also tends to help with readability if you force some order on the bindings).
&gt;Also make sure they're doing the same work to account for unicode They've both been tested with unicode, as can be seen with the test suites. 
Suppose the compiler supports this somehow natively. At which phase would we switch to CCC? I'm a bit worried that this will lose convenient properties of GHC Core, so I suppose this should be a replacement for STG. It looks really similar to evaluation by graph reduction, also to SKI calculus and friends. Do CCCs have a notion of recursion? I wonder if it breaks down for more realistic examples. Don't get me wrong, the approach in itself is eye-opening, as is the insight it brings on overloading syntax, but I don't quite see where it fits. In an ideal world, static analyses (as in IDE) could just interpret programs in a certain CCC, like in the interval analysis example, sharing language frontends. On the other hand, this should already be possible right now on GHC Core. 
This makes sense! I added a note to [my post](https://www.reddit.com/r/haskell/comments/6jv15h/haskell_infrastructure_swift_navigation/djjfyrd/) on this thread to clarify that you definitely want to keep lower version bounds.
/r/haskedev: I don't think you're actually helping your side in this argument. /u/tomejaguar: &gt; version bounds can be ignored and therefore having them is never worse than not having them. I actually think there is a slight problem here: right now authors don't have a way to distinguish "I'm disallowing this version because it causes terrible perf/security problems" from "I'm disallowing this version because I haven't compiled with it yet, if it compiles it's fine." My dream way out of this situation would be: 1. `cabal.config`: (optional) Provides an example of working versions to get the solver started. 2. version bounds in `.cabal` file: (optional, but required if `cabal.config` is missing) authorial version bounds. Can indicate semantic issues, so should never be relaxed (or perhaps relaxed with extreme care). 3. Not-in-source-code bounds (for instance stored in infra dedicated to `cabal-install`): A restriction on top of source bounds, generally for things like disallowing versions that don't compiling/test. Very rarely or never a loosening of source bounds. I don't think we need to go with this exact solution, but I do like how cleanly it allows the meaning of each component to be defined.
Impressive work. I didn't know about `SmallArray#` before, and this looks like a neat use of it. Another notable package is [record](http://hackage.haskell.org/package/record) (it was a pretty big announcement here when it came out). I don't remember how it compares to other approaches, only that it uses a whole lot of custom product types, somewhat like `rawr` and `labels` that you mentioned. [data-diverse](https://hackage.haskell.org/package/data-diverse) is also a recent package in this area, with quite a similar approach; there was a post by its author about [slow compile times](https://www.reddit.com/r/haskell/comments/6jvujf/why_is_compiling_slow_for_datadiverse/). Have you encountered similar issues? It might be interesting to see a comparison of compile times for their test suite. As more and more computations are happening at compile time (\*cough\* dependent \*cough\* haskell \*cough\*) I wonder whether that necessarily comes with slow compile times; a lot of type-level computations seem just small enough that compiling them would still be more expensive than simply interpreting them. All this low-level hackery with unboxed values and `unsafePerformIO` also makes me worry that it gets in the way of optimizations. If you set two fields successively, `set f2 v2 . set f1 v1 $ r`, does the intermediate record `set f1 v1 r` get optimized away?
&gt; NOTE These features have not currently been released, so don't try using them in a stable Stack executable. If you'd like to test them out (and I'd certainly appreciate the extra testing), you can run `stack upgrade --git` to build a Stack executable from the master branch. Nevertheless I am extremely excited about this new feature. Custom snapshots with HTTP(S) and Git dependencies are a game changer for large projects with many shared dependencies. I currently do something similar with a combination of a custom snapshot and a lengthy `stack.yaml`. Being able to put everything in the snapshot will rock! Edited to add: It's not in master yet. Keep an eye on this PR! https://github.com/commercialhaskell/stack/pull/3249
&gt; Custom snapshots with HTTP(S) and Git dependencies are a game changer Most def! I'm very excited. This crucial feature finally allows for the emancipation of Stackage from Hackage's oppressive monopoly. With Snapshots supporting Git dependencies it becomes technically feasible to cut out the middle-man and move to a state-of-the-art way of decentralized package hosting via Git.
I still don't get why it's supposed to be the Stackage maintainers problem if a package works perfectly with Stack but cabal fails to be useful. That sounds more like a problem of cabal the cabal devs should fix, not Stackage.
This unfortunately suffers from the fact that type level lists are in fact lists and not sets, so foo = #name := "hello" &amp; #age := 22 &amp; rnil bar = #age := 22 &amp; #name := "hello" &amp; rnil are neither considered equal nor are they considered the same type. This can be fixed by doing type level insertion sort with the `rcons` function. My own (incomplete) attempt at the record problem ([`hash-rekt`](https://github.com/parsonsmatt/hash-rekt)) takes a similar approach, in that it's a bunch of phantom types and type families that's guarding a `HashMap String Dynamic`. Since key/value pairs are inserted in a sorted manner, the equivalent foo = Rec.insert @"name" "hello" $ Rec.insert @"age" 22 Rec.empty bar = Rec.insert @"age" 22 $ Rec.insert @"name" "hello" Rec.empty do evaluate to be both equal (via `==`) and the same type.
&gt; version bounds ... never worse than not having them. That's trivially false, as there's countless example where version bounds have caused problems for Stackage maintainers and Stackage users. One recent example is [here](https://github.com/fpco/stackage/issues/2469) and Stackage is stuck with them &gt; I wish we could opt out of them, they were a terrible idea hoisted on the community against our will for bad technical reasons. Now Stackage is stuck with them, like all other bad decisions that are made upstream of us. 
&gt; I don't think you're actually helping your side in this argument. Why not?
If it really were better for packages to have no bounds then Stack could simply ignore them. Really, my proof seems watertight. In what way do you think it fails?
This seems pretty neat. Though I don't understand the JSON examples at all. The whole benefit of aeson is that you can write parsers that turn external JSON data into business logic types. There's no need for intermediate request/response types, in fact, aeson provides and uses an intermedite type already (Value) so if you use an intermediate type yourself you're just adding another layer...
It's obviously not that simple *technically* to ignore bounds as otherwise Stack would have already ignored them long time ago. They *are* useless to Stackage but /u/snoyman can explain the technical details way better than me.
This looks like the row polymorphism that PureScript has
Being tested with unicode doesn't mean they are doing the same amount of work to support it. One of them might be doing a bunch of work the other is not, despite the fact that that work has no impact on the output of the code in this particular benchmark. (I don't have any particular reason to believe that's happening here, just pointing out the distinction).
Can you also use the gl package with this?
Try using `.to_owned` instead of Rust's `.to_string`, that should be faster.
I have switched to stack, and I definitely like it. Have any of the devs mentioned if they'll eventually switch to not having a cabal file anymore? It's a bit weird having two configs, but it works and is easy to make work, so not really a big deal.
Not sure. I'll looked at some [example code](https://github.com/lukexi/halive/blob/master/demo/Cube.hs) and it *seems* like it shouldn't be too hard. I'm willing to help you if you want to give it a shot.
I hope more people share more about the solution of the problem mentioned in the article.
Very busy lately, but i will try to get a demo going and ask if things get stuck
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [louispan/data-diverse/.../**ManySpec.hs** (master → 627b272)](https://github.com/louispan/data-diverse/blob/627b272acffb161ea30624552daf66fa764ca3a1/test/Data/Diverse/ManySpec.hs) * [louispan/data-diverse/.../**WhichSpec.hs** (master → 627b272)](https://github.com/louispan/data-diverse/blob/627b272acffb161ea30624552daf66fa764ca3a1/test/Data/Diverse/WhichSpec.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dju4cmc.)^.
A note about backwards-incompatible changes to the `stack.yaml` file format: One of the great promises of `stack` in general is that you can essentially put your projects into a time capsule and not fear that they will stop being buildable in a year (or 10 years). If the `stack.yaml` format changes, this promise gets broken from a different angle. Would it be possible to regain it by using the resolver (or some other signal) to tell stack to download and run a previous version of itself? 
I think this is a good concern, but I also think the best place to solve this is at a different level than Stack. If you're thinking in terms of 10+ year compatibility you probably want a reproducible system package manager. I complain a lot about Nixpkgs getting promoted on this subreddit too much, but this is a perfect use case for it.
Hi, I'm the author of [data-diverse](http://hackage.haskell.org/package/data-diverse). I think part of the slow compile time was due to GHC using a lot of memory trying to inline polymorphic recursive functions where the types change on each recursion. I managed to reduce this with NOINLINE and INLINEABLE pragmas. Also, the slow compile times may also be due to excessive Show and Eq instances being used in the hspec tests. I stopped worrying about the slow compile times after I realized I was getting similar compile times with ordinary tuples! I also used a similar encoding for [Many](http://hackage.haskell.org/package/data-diverse-0.5.0.0/docs/Data-Diverse-Many-Internal.html), where it's encoded using Data.Map instead of SmallArray#. You can think of 'Many' as a typesafe cons-able and tail-able tuple. The main difference with data-diverse is the api where I don't bother with labels (although you can use them if you want), because I realised that you can just use the field types as labels themselves (eg with TypeApplications or type annotations). Data.Diverse.Many is pretty fully featured, with getters/setters for single/multiple fields (which allows reordering of fields), and "folding". Check out [ManySpec.hs](https://github.com/louispan/data-diverse/blob/master/test/Data/Diverse/ManySpec.hs) for all the things you can do with Many. You might also be interested in the dual 'Data.Diverse.Which' Check out [WhichSpec.hs](https://github.com/louispan/data-diverse/blob/master/test/Data/Diverse/WhichSpec.hs) for all the things you can do with Which. Caveat: I'm thinking of making one last breaking change, which is to move all the lens to a separate package.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [louispan/data-diverse/.../**ManySpec.hs** (master → 627b272)](https://github.com/louispan/data-diverse/blob/627b272acffb161ea30624552daf66fa764ca3a1/test/Data/Diverse/ManySpec.hs) * [louispan/data-diverse/.../**WhichSpec.hs** (master → 627b272)](https://github.com/louispan/data-diverse/blob/627b272acffb161ea30624552daf66fa764ca3a1/test/Data/Diverse/WhichSpec.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dju5b2e.)^.
Thanks for the enthusiasm here, but just to set the record straight: - This PR still requires a lot of testing before it's ready to be merged - While custom snapshots will have the ability to refer to Git repositories, the PR makes no changes in how Stackage snapshots are defined, meaning they still must come from Hackage - And this blog post is just a preview, especially important for people reading this post in the future and wondering why they can't connect to the server
Look at hpack
haskal
Another approach would be to have stack have automatic migration for stack.yaml. This I believe would be easier for things like 10+ year support. 
I agree with you in principle. The only reason why I'd consider a breaking change here is to prevent people from using the (really bad) current default of treating Git repos and HTTP(S) URLs as non-dependencies. I'd be fine keeping syntactic backwards compatibility support indefinitely (probably with a warning), as long as we've made it difficult or impossible to get Stack to do the wrong thing. Also, u/seagreen_ is right: if you want full buildability long in the future, you better capture the Stack executable as well, and if you're paranoid (and you should be), grab the GHC tarballs and your system libraries too. There's a reason Stack has Docker support: one of the first projects we used it for had very strict reproducibility requirements, and as bad as Docker is, it's the best terrible option out there right now\*. I'd be very interested to hear if someone can point me to a use case where they treat a Git repo or an HTTP(S) tarball as a non-dependency (`extra-dep: false`) and believe this is good behavior, I haven't encountered it myself. \* I'm not saying _technically_ the best, but once you include discussions with corporate IT departments... you don't have many other options.
Is there any reason there aren't type-level sets?
My [data-diverse](http://hackage.haskell.org/package/data-diverse) tackles this problem in a slightly different way. 'foo' can have the fields [reordered](https://github.com/louispan/data-diverse/blob/627b272acffb161ea30624552daf66fa764ca3a1/test/Data/Diverse/ManySpec.hs#L267) to be the same as 'bar', so then you can 'Eq'ate them. I stole this idea from [labels Project class](https://github.com/chrisdone/labels/blob/a2a6967cd2dbd379281fcba6d57e9ad7648b5234/labels/src/Labels/Internal.hs#L102). This gives you the best of both worlds. it's relatively cheap (in usability) to reorder to identical types, and you can still have different types with the same fields in them when you need them.
Thanks. I ended up copying the "yi" folder and using that as a template instead of using any of the examples. I have opened 2 pull requests and an issue on github. I don't mind being assigned to fix the issue but I want at least one outside opinion on what "fixed" would even look like.
Now you're cooking with categories!
Actually, for the 5-10 year case the primary use-case I'm imagining is blog posts, tutorials, videos, etc. They are the least likely to be updated on a regular basis but the most likely to cause frustration to the person trying to use them (likely a newcomer). We've become spoiled (thankfully!) and just put our stack.yaml files in our blog posts.
Very true. Yet see my response to snoyberg.
Type level lists are sugar for the `':` and `'[]` list data kind type constructors. There are no data constructors which yield sets. Symbols offer a type level ordering, which allows you to write insertion sort and just maintain the ordered property by only exposing functions which preserve it.
It's so much nicer to just have order not matter by default, though. I personally wouldn't try to sell my coworkers on a record library that cared about ordering.
Where can I find more information about `':` and `'[]`? They are near impossible to do a search for.
Nice work! How fast will conversion between `Text` and your library be? That might be the most executed code-path unless you figure out how to refactor Stackage from one of those libraries to yours. And since conversion is pure overhead, there might not be a benefit in larger systems. Maybe backpack can solve this?
The colon is called the "Cons" operator.
That's a really good point, thank you for raising it.
I know the list data type, but what is "list data kind type constructors"? That looks like a random mashup of related but separate terms to me.
what to do you suggest for me to use in order to build loosely coupled application that scale?
I personally think that while from a theoretical point of view this is a problem, it does not really matter in practice. You never really argue about the concrete position of a label in a record, rather than that a record has a specific subset of labels. Maybe you have a good practical example when this becomes a problem? Technically, the library takes advantage of the order in the type-level list by using it to compute the physical location of the field value in memory. If you changed that to a set, then you can not rely on this anymore. Using `HashMap String Dynamic` is not really an option due to performance. What you could easily do though in superrecord would be adding a `sort`/`reorder` combinator that sorts the values by key and reorders them physically in memory. This could also be done when adding a field (the `&amp;`) operator, but then again that would mean (physically) resorting on each insert.
There is a bit in here about how lists are promoted to the type level: https://downloads.haskell.org/~ghc/7.4.1/docs/html/users_guide/kind-polymorphism-and-promotion.html
Why was the comment you replied to removed? Did the moderators remove the comment?
&gt; Recursion are not on the top of my list for professional Haskeller knowledge. Could you please share your list with us?
Can you make a separate module where `FldProxy l` *is* the lens? Optimally `v ~ ()` `rcons`es and `w ~ ()` deletes the field. See https://github.com/turingjump/bookkeeper/issues/10#issuecomment-242266117
Just look up DataKinds
I thought the [stack script interpreter](https://docs.haskellstack.org/en/stable/GUIDE/#script-interpreter) should be good enough?
This used to be true, nowadays it's the same. See https://github.com/rust-lang/rust/pull/32586
I found this very confusing when I was starting out with Haskell. I still find it odd, but I find I mostly alter the cabal file and only very occasionally mess with the stack config (usually when I want one dependency from nightly or something).
&gt; It's obviously not that simple technically to ignore bounds as otherwise Stack would have already ignored them long time ago. I find it very hard to reconcile the two claims that "Stackage proves that you can defeat cabal hell without any version bounds at all in your .cabal file" and "Now Stackage is stuck with [bounds]".
In the `cabal` file you have the "what", in the `stack` file you have the "how", they have different goals, and it is good that they are different.
Fwiw, in Cabal this separation of concerns is also expressed in the naming of the [`cabal.project` file](http://cabal.readthedocs.io/en/latest/nix-local-build.html#configuring-builds-with-cabal-project) whose intent is to specify your project configuration (non-modular; this can include pinning down exact releases of packages, setting the GHC compiler version to use, setting specific compile flags, set profiling levels, enable/disable tests/benchmarks, etc), whereas the `$pkgname.cabal` file (fun-fact: cabal also supports using a fixed name, e.g. `package.cabal`, regardless of the name of the package) describes the unit of distribution (i.e. individual packages - *not* a project) in a modular/declarative/composable way, which also implies including an accurate specification of the inter-package dependencies (which provably requires package names with *compatible* version-ranges).
More parsing? Hahahhaha
It seems that the main claim in the post is that performance improves if you add more methods to `Stream` so that for specific types such as `Text` users can provide manually specialised methods which are then faster. There seems to be quite a lot going on in the post though could you perhaps clarify where you think the performance improvement comes from? What I wonder is that how the additional error handling provided by Megaparsec affects the performance characteristics. I would appreciate some analysis of this impact. As an aside, I think the CPP in some of the Megaparsec benchmarks is unnecessary and could be replaced by a function which converts characters to integers which would be inlined. 
I agree. These kind of widgets do not compute in the functional sense since they do not return values. they channel results trough abstractions like observables, that are pipes that the programmer has to assemble using identifiers. A widget that compute in the functional sense should return his stream of values to the functional expression in which it is inserted. Then, the execution flow is explicit in the equations. No passing of identifiers are necessary . No internal espagueti of pipes to de-scramble in order to understand what the program does. All is explicit in the expressions. The combinators? widget `fire` OnEvent -- generates a new widget that return his value when -- the event fires inside widget1 &gt;&gt;= widget2 -- widget2 is "observer" of the results of widget1. -- A result of widget1 executes widget2 and redraw it widget1 &lt;|&gt; widget2 -- any of both widgets can return a result when they are fired widget1 &lt;&gt; widget2 -- a new widget that return the monoidal sum of the current values -- of both widgets when one of the widgets receive an event f &lt;$&gt; widget (,) widget1 &lt;*&gt; widget2 -- defined similarly. at &lt;DOM identifier&gt; widget -- render the widget within the DOM identifier The beauty of that model is that, because it is mathematically determined, all the frameworks that implement it should be isomorphic. Also, widgets are dropped in a single place and nobody should care about their internals, since for the programmer, all he want to know is determined by the types. Also, very important: *Behaviour and event management is implicit; They are put out of the equations* 
&gt; could you perhaps clarify where you think the performance improvement comes from I mention the changes that I did in the section "Back to Megaparsec vs Attoparsec". Minor things like replacing `noneOf "\""` with `notChar 34` all contribute to the result, but the main point is to try to use `takeWhileP`, `takeWhile1P`, and `string` as much as possible because their speed is incomparably better than `many` and `some`-based parsers. In the case of JSON parser the inlining that the original Attoparsec parser has and that I initially forgot to add may be responsible for part of the speed-up as well. About CPP: it came from the original Attoparsec parser stolen from https://github.com/bos/attoparsec/blob/master/benchmarks/Aeson.hs. I can see why it's done this way: you can have nicer pattern matching in `value`. Now that `Token ByteString = Word8` in Megaparsec, I needed human-readable names as well with the ability to use the names in pattern matching. While ugly, the code in benchmarks is mostly auxiliary, I keep it to play with it and guide me through design of Megaparsec 6, so I don't care much about its aesthetics. It may be a good idea to add a `char`-like function that accepts `Char`s but parses `Word8` though, for readability.
[removed]
Thanks to some awesome work by mistuke@github, we now has support for windows. See https://github.com/raaz-crypto/raaz/commit/cbf7b4403135bbebf0cbe18a50523d66cbb2eaff
Most people using Debian for a development system will be on testing or unstable though. (For context, Ubuntu is based on testing.)
The `stack.yaml` is for **the `stack` executable**, while the `.cabal` file is for **the `cabal` library**. Underlyingly, `stack` uses the `cabal` library for some essential features. **The `cabal-install` executable** also uses **the `cabal` library**, and no other settings file. This causes the most confusion, and is AFAICT one of the main reasons why `stack` does not include all dependency information in a new, non-compatible file format (let's pretend that `hpack` does not exist, and besides, `stack` converts this to a `.cabal` file just before it builds). Moreover, the Hackage package repo uses the `.cabal` file, and I would consider that the most compelling reason to keep things as is. Stackage snapshots will always be smaller than the full Haskell library ecosystem, and sometimes you just need a library which is not included in any snapshots. I'd rather refer to libraries by their Hackage name and version in `extra-deps` than have to deal with Github paths and commit hashes. Hackage deps are also cached by `stack` globally, whereas source control deps don't appear to be.
Watching this! It's indeed very interesting to see the thought flow of a professional Haskeller. For example, this: *(SPOILER!)* traverse_ insert $ sequenceA ingredients recipeId That looks like about the right amount of effort you spend expressing your good ol' needs in terms of ready-made functions! That's pretty interesting! I've never seen this done in any other language. (There *could* be someone doing this with scala, maybe). I would probably have used `TypeApplications` to specify this as `sequenceA @_ @((-&gt;) Key Recipe)`, but that's not a big deal.
This looks a lot like OCaml's structural types at first glance (modulo concerns from @ephrion). Is that a fair comparison to make?
Oh, maybe that's what /u/singpolyma meant.
Here's my own experiments with Alpine Linux, GHC and Docker. This generated a very small, approximately 8MB, hello world application. It may be out of date but it may be worth forking and adjusting to your needs. [https://github.com/mgreenly/dockerimages/tree/master/alpine-stack](https://github.com/mgreenly/dockerimages/tree/master/alpine-stack)
&gt; I still don't get why it's supposed to be the Stackage maintainers problem if a package works perfectly with Stack but cabal fails to be useful. That sounds more like a problem of cabal the cabal devs should fix, not Stackage. I'm pretty sure you're deliberately misunderstanding at this point but just in case. It's not a "problem" with `cabal` that people are creating packages that only build with `stack`, the two tools have different attitudes to dependency version management. The problem is that people are uploading those packages to Hackage, _which is for `cabal`_. The only "problem" the Hackage maintainers have is whether or not they should allow those packages to be uploaded to Hackage at all. As I said in my first post, I believe they should not. The "problem" the Stackage devs have is deciding if they want to diverge from Stackage containing a subset of Hackage, and instead allow people to upload packages there.
My sweeping conclusion (as a Rustacean) is: it's a very interesting comparison from the view of comparing semantics and convenience. Haskell as a higher-level language optimises very good with the info it has, while Rust is (due to its semantics) bound by doing explicitly what you tell it to. You return String, so there's no way to delay the allocation of that string too far. Haskell can do that. Your Rust implementation is a reasonable first implementation, but while it can be optimised, you _have to work towards it_.
I changed it, but I am still getting the same error.
This is a great write up! Not to mention potentially very useful for our devops work. Thank you!
Hi everyone, author here. If there is any interest, I'd be happy to answer all of your questions.
`gql` in vim
In addition to what /u/Noughtmare said: some people publish their yi configs on github: https://github.com/search?q=yi-config For example, here is mine (static, vim keymap for colemak keyboard, some snippets, some experimental stuff): https://github.com/ethercrow/yi-config
I just dropped upper version bounds for anything but base in my hpack files when I switched to stack.
The `stkb.rewrap` extension does this nicely in Visual Studio Code. It goes beyond `gq` in Vim in that it properly handles things like lists within Markdown-formatted comments.
Partway through the article, just a small request... It seems like maybe `json-**.json` are files where the `**` has something to do with the size? And the x-axis is... processing time in seconds? Some text details, and labeled axes would be very helpful. Thanks!
That machinery for converting between positions and chunks seems pretty complex. Did you consider using the methods from [monoid-subclasses](http://hackage.haskell.org/package/monoid-subclasses) instead like [picoparsec](http://hackage.haskell.org/package/picoparsec) does to the same effect? Its speed was closer to attoparsec, if I recall its benchmark results correctly. 
I really don't get how the Hexagonal Architecture is substantially different form the usual IoC / dependency injection frameworks, like those of Spring / Java EE. Your logic depends on interfaces not implementations, and it doesn't itself select the implementations. Is there anything more to it? 
Indeed, the Hexagonal Architecture is a pattern of dependency injection. It is however a bit more specific and gives some more piece of advice on how to structure the code and inject the dependencies into it. It also touches some architecture concerns. It takes a stance against layered architecture. So in my view, it is a bit more than just DI. 
&gt; takes a stance against layered architecture What makes an architecture "layered" or "non-layered"? What are the practical implications of "non-layered" architectures? Does it mean that the "logic" cedes a greater amount of control of the flow of execution to the "interpreter", for example in error handling and transaction control? (Argh, I answered in the wrong place.) 
Am I right in saying that this is because `Set` is not an algebraic data type?
Thanks for the write up. Suggestion: it would be great if your tool could also be used as [direnv](https://direnv.net/).
Some architectures are based on the idea that we have some layers of abstraction, that we can stack one of top of the other. A given level can talk directly to the level below it and that's it. For instance, we could make the business logic be on top of the layer giving access to the data-base. This is not necessarily bad, but this is not the only choice. The stance defended in the Hexagonal Architecture represents one such other choice. Instead of absolutely trying to define layers on top of the others, it tries to create some place in which you try to isolate some domain logic. In this view, there is no real upper or lower layer, but rather an inside and an outside. And the advice is to abstract this outside in terms of services needed by the domain (the point of view of the inside). Technically, it might just be considered a matter of cosmetic (after all, there will likely be DI in both cases), but in some cases, I find it helps reasoning differently: * Avoiding being bothered by absolutely trying to fit layers (which may or may not be appropriate to model a given problem). * Choosing names differently (names that are less focused on DI than representing services the domain logic needs). Overall, I think it fits the notion of Embedded DSL and Free Monad pretty good (my view as expressed in the post), in which we do not necessarily care about layers either. I am not sure if it answer your question, but this was my best shot in a few minutes :) 
There is a language extension called `DataKinds` which lifts data constructors to the type level. So the Haskell code to define a list data structure: data List a = Cons a (List a) | Nil ordinarily creates three things: 1. A type constructor `List` with kind `* -&gt; *`. 2. A data constructor `Cons` with type `a -&gt; List a -&gt; List a` 3. A data constructor `Nil` with type `List a` Enabling `DataKinds` *also* creates these things: 1. A *kind* constructor `List` with kind `k1 -&gt; k` -- that is, something taking a *kind* and returning another *kind*. 2. A *type* constructor `'Cons` with *kind* `k -&gt; List k -&gt; List k` 3. A *type* constructor `'Nil` with *kind* `List k`. So `':` is a lifted list constructor. There is also `'Just`, the lifted Maybe constructor, and also `'True` of kind `Bool`.
&gt; We make two HTTP requests if we want two keys from the same secret. This rarely induces extra overhead for our own use cases – we don’t often store multiple secrets in a single path. For general use cases, this data-fetching may be optimized. Sounds like you could benefit from looking at Haxl. It will automatically parallelize your IO operations (reading secrets in parallel like you're doing with mapConcurrently) but it will also solve the problem of fetching the same secret twice, because Haxl automatically caches the operations. :) 
Internally we use the following snippet for this in our bashrc: function with_secrets() { SECRETS_FILE=$1 # This shifts the positional arguments by 1. $1 is no longer available, # $2 becomes $1, $3 becomes $2 and so on. This way, we can easily pass # $@ to vaultenv. shift /usr/bin/vaultenv \ --host vault.service.consul \ --port 8501 \ --token XXxxXXxxxxx \ --secrets-file "${SECRETS_FILE}" \ -- "$@" } Then in bash: $ with_secrets mysecretsmappingfile.secrets bash $ env SUPER_SECRET=lol $ 
Author of `brittany` here, the formatter based on ghc-exactprint. Firstly, I have no idea about how HSE does exactprinting; I'll only talk about the ghc-exactprint route (although the differences seem small, judging from the other comments here). When you have no annotations returned from parsing (because you did no parsing) you could in theory call the relevant internal brittany function to do the pretty-printing with an empty set of annotations (or rather: with only a set of annotations that contain exactly the comments you want to have, and "default" values for all other stuff). To some degree I'd image this would almost work out of the box, because, as a formatter, `brittany` ignores/overrules a good amount of the information present in the annotations anyways. I can think of a couple of issues, e.g. the cases where annotations are used to define order (see the documentation on `annSortKey` in [in the ghc-exactprint docs](https://hackage.haskell.org/package/ghc-exactprint-0.5.4.0/docs/Language-Haskell-GHC-ExactPrint.html#t:Annotation)). Basically you'd lose order in a couple places where the AST uses unordered containers. But I think it would not be too cumbersome to hack around such issues; e.g. by using appropriately-ordered fake SrcSpanInfos. But then, I might be missing something here, and I have not really tried what happens when passing an empty annotation set to the formatter, so no promises :p
&gt; fun-fact: cabal also supports using a fixed name, e.g. `package.cabal`, regardless of the name of the package That is a fun fact! One idea - what about just allowing (for the package `foo`) `foo.cabal` and `project.cabal` and nothing else? That way we don't end up with some people using `project.cabal`, some `package.cabal`, some `lib.cabal`, etc.
I have a little performance tip: try using https://github.com/yesodweb/yesod/archive/7038ae6317cb3fe4853597633ba7a40804ca9a46.zip instead of git. (A branch or tag can be used as well, just put it where the commit hash was.) For large public repos, this really makes a difference in both performance and disk usage. Also, I think it leads to less confusion. When I first saw that stack supports git, I naturally expected that if I change the commit hash, it wouldn't clone the repo again. Took me a while to find all the issues and pull requests needed to understand why things behave the way they do. I think things would be much clearer if the documentation clearly said that git support in stack is nothing more than a more expensive transport protocol and offered an alternative in the form of https zip archive. I do understand that it's sometimes necessary, if you can't or don't want to use http(s), e.g. for private repos, but in many cases it's just better to avoid using stack's git support entirely. (And finally, since stack includes the commit hash in git repo directory name, the logic with clone/reset can be dropped and replaced with a --depth=1 clone instead, making the transport a bit less expensive. Not sure if worth the effort though.)
Great article. DDD is one of the buzzwords going around the developer circles in my company right now, and it's very handy to know how some of it translates to Haskell. 
As a curiosity, to what effect the `CommutativeMonoid` is used, since it has no methods and therefore cannot be inferred? Is it used by rewrite rules?
Writing a little parsing utility for some logfiles is a good way to go if you have easy access to raw data. Project Euler can be a fun way to flex your problem solving muscles also, although without a strong math background some of the problems can seem arbitrary and frustrating. I tend to use Haskell in ghci a lot at work as a faux scripting language, which I found was pretty great at tuning my ability to make good use of lazy IO.
No. The only role of the class is to serve as a superclass of ReductiveMonoid, CancellativeMonoid, and GCDMonoid which do have methods. 
I try to write everything in Haskell if possible, it works well when writing small scale tools even in the work space because I could ship prebuilt (win) binaries; Or even shell script for some automation task.
The classic suggestions are: + Format converters. These are nice because they can be pretty simple, they're immediately useful, and Haskell's a good fit for them. My first Haskell project converted Password Safe passwords into [pass](https://www.passwordstore.org/) passwords. + Games. These can be as simple as you want (eg a choose-your-own-adventure text based game) and also since they have a creative element you have the fun of knowing you're doing something no one's done before. + Unix utilities. Things like `wc` are easy to write, and if you're the kind of person who cares about this it might be fun to have your own memory safe word counter on your computer.
Sounds exactly like [Backpack](https://ghc.haskell.org/trac/ghc/wiki/Backpack)
You can try Nix and DockerTools : http://nixos.org/nixpkgs/manual/#sec-pkgs-dockerTools
It's definitely possible to use libraries besides GMP: https://ghc.haskell.org/trac/ghc/wiki/ReplacingGMPNotes
I would still encourage package authors to upload to hackage as part of their release process, though.
But hpack doesn't replace stack.yaml, does it? So it's either cabal file + stack file, or hpack file with stack file. I don't see how this addresses what OP was talking about.
bitemyapp* :)
&gt;That looks like about the right amount of effort you spend expressing your good ol' needs in terms of ready-made functions! I enjoyed writing it, I always forget `traverse_` has a weaker constraint than `Traversable` and is just `Foldable`. What did you mean here? I don't like `TypeApplications` all that much. It forces me to remember things about the type signature that I don't normally have to care about.
Is that new (cabal.project)? I've never seen any projects that use that. Got examples?
I'm really glad that `extra-deps` will begin supporting git urls. The fact that these had to show up in the package list instead of the dependency list was something I always found counterintuitive.
Thanks for the kind words and the shoutout on Twitter :) Should you end up evaluating vaultenv: feel free to ping us if something is unclear or if you have a use case that isn't covered 
that's it, my next Haskell side project is going to be a website for managing/sharing Haskell side project ideas and examples
My apologies good sir, thanks for doing what you do btw
Thank you! Did you see I did another (two parter) stream the other day? https://twitter.com/bitemyapp/status/882478431321149441
Yes! 
https://adventofcode.com/ It's full of small challenge (around 100 at the moment) that can be solved in any programming that language you want. They are varied enough to touch to a lot of problems like parsing and dynamic programming
There are many many different ways of writing DSLs where the declarations (the what) are decoupled from the interpretation (the how). Free monads are just one way to achieve something like this. They can be useful but always have a very imperative feel to them.
A `.cabal` file is for a package, while a `stack.yaml` file is for a project. It becomes clear when you have multiple packages in your project.
- Static blog generator - take a folder of markdown files and create a few html files out of it - Build a picture album website (and notify me when you do) - Web scrapper - get information from a website and display it in a nice way (maybe use [brick](http://hackage.haskell.org/package/brick)?) - Make an over the network chat program for two people or more - Build an interpreter - Whatever you have in mind
Wow! Thanks for the link, this is amazing work!
I solved problems on project Euler. 
Build a small Todo app in yesod with a database backend
That would mean there would be two configuration files cabal would consult: `project.cabal` and `cabal.project`. That’s *definitely* not confusing.
`scratch` works well for me. [Here](https://github.com/APNIC-net/crocker) is a recent project building in a single multi-stage `Dockerfile` producing a final image containing only the binary. It can also be built using `stack` directly, but that depends on the presence of a Docker image that can be created using the first five lines of the multi-stage `Dockerfile`.
whatever you're currently interested in, or some task you need for work. haskell has unique advantages in parsing (many libraries, like parsec or earley, the program can read like a bnf) and scripting (type inference, the turtle library), among many other domains. so munging/querying logs is always a good one. 
&gt; What did you mean here? &gt; I enjoyed writing it Yes that.
I wrote a Haskell compiler. The first one, in fact. 
Exercism.io has about 80 challenges. 
&gt; There are no layers below or above the Hexagon. Instead, there is a layer around the hexagon consisting in adapters Yep, typical software engineering meaningless bullshit right here. Layers are not above your core logic, they are around it! We understand so little about how to reliably build software that there is a lot of space for gurus to come up with fascinating theories that capture developers interest, encapsulate a couple actually good advice ("test your code" and "don't wait years before checking that your idea of the clients needs actually correspond to their need" is A+ objectively meaningful advice), and gain temporary popularity thanks to selection bias (if you build a team of people caring deeply about the process of software development, chances are it may work better). Which is just fine -- we had alchemy before most real chemistry. But it's quite fun to see a post about "Hexagonal Architecture and Free Monad" that says "don't worry, you don't need to know what free monads are!". Because Free Monad actually means something, has a precise definition, and Hexagonal Architecture does not.
First, get familiar with the syntax by doing coding exercise in CodeWars or something similar. Then learn about Typeclass &amp; mtl library. Then get practical Haskell experience by building a simple RESTful API (url shortener, for example) using Scotty with DB of your choice.
\^ Underrated comment of the thread :) For those interested, sources of versions 0.9999.3 and 0.9999.4 are available here: https://archive.org/details/haskell-b-compiler /u/augustss do you happen to have the source of 0.9999.5c around? I couldn't find it anywhere. Apparently it used to live at http://haskell.org/hbc/hbc-2004-06-27.src.tar.gz but that page doesn't exist anymore.
I reimplemented Git, which gave me an appreciation for things Haskell is exceptionally good at (e.g. parsing) and things it's not so great at (e.g. bit twiddling). My project is [here](https://github.com/vaibhavsagar/duffer).
Extra bonus: because everything is JavaScript, the back button doesn't work reliably (it will often nuke all the tweets you'd scrolled past and you have to start over) and it shows you a "sign up for twitter" splash page every time you hit back. 
We could definitely pick a better name than `project.cabal`, the part I care about is standardizing on something, I don't have an opinion on what.
Is the trick M-q? It's the `fill-paragraph` command in Emacs which wraps a line of text into a readable paragraph, respecting comment syntax (including things like leading `*`s). It's a super useful command for wrapping all sorts of text, not just comments. For reference, it's not Haskell-specific whatsoever.
I have some versions somewhere. 
No. If static overheads are higher. 
Excellent writeup, and thanks for sharing! Looks like a really useful piece of code. I took a look at the github and the code looks really clean too :). &gt; Try to do that without first-class functions! This line confused me. Are there any languages in use that don't have first-class functions? I mean, C has them! EDIT: [posted to /r/programming](https://www.reddit.com/r/programming/comments/6lre5u/channable_introducing_vaultenv_run_an_executable/) since it probably has some general interest!
Bit twiddling? Are you talking about doing it in SIMD or something? The plain old general purpose register bit twiddling seems fine in Haskell.
No, just bog-standard variable length integer encoding and decoding (see [here](https://github.com/vaibhavsagar/duffer/blob/master/duffer/src/Duffer/Pack/Bits.hs)). It was fine in Haskell, but not an improvement over doing the same thing in e.g. Python, which disappointed me.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [vaibhavsagar/duffer/.../**Bits.hs** (master → 39d5a9f)](https://github.com/vaibhavsagar/duffer/blob/39d5a9f5333a974b2c2a202c8bfdf5475f2764df/duffer/src/Duffer/Pack/Bits.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply djw52ps.)^.
I love writing programming language tools in Haskell. A fun option is to implement your own little language interpreter along with bounded verification or even program synthesis using Z3. Blowing my own horn a bit, I gave a tutorial talk that's a good starting point for writing that sort of thing in Haskell: [Analyzing Programs with Z3](https://www.youtube.com/watch?v=ruNFcH-KibY).
As someone who has spent a lot of time trying to understand Free monads can you share more about what specifically it is that gives you the imperative feel about them?
Think of a tool you could make use of, even if you already use other software that does the same thing. For example, I have have lots of old mp3s of mixes and radio broadcasts, most over an hour long. Thinking about writing a simple mp3 splitter of some kind, maybe with a brick interface. No need to open up a whole audio suite just to do that!
I've been teaching myself Haskell by building an HTTP server that will eventually list directories and file contents https://github.com/btipling/server/ My very first project was a simple programming puzzle where I attempt to validate parenthesis order https://gist.github.com/btipling/d14861da3d877543165659b490595a0c I'm still in the "I barely have any idea what I'm doing" but my code compiles and works.
How about a more general version: https://cs-syd.eu/projects/suggestions/ospip.html
Hi and thanks! I meant to express my happiness about most APIs I have encountered while writing Haskell. Passing in the specific to a general pattern is really lovely and a breath of fresh air compared to adding more control flow to the same for-loop. I'm not saying you cannot write code like this in other languages. Haskell definitely steers you in the right direction here, though. I've changed the post to clarify what I meant. Thanks for bringing this up :)
I am not sure whether this [thread](https://www.reddit.com/r/reflexfrp/comments/6l5ddn/how_to_structure_a_reflex_application/) might be useful, it was about how to structure `reflex` application and ofcourse modular approach is a thing mentioned 
I wrote a small [text generation language](http://polemic.vmchale.com/madlang) and a command-line client for [twitter](https://github.com/vmchale/command-line-tweeter). Haskell was great for both because of its facilities for parsers and also abstract data types. 
Writing parsers for formats is a very good idea, as it's useful and not too tricky (depending on the format of course :-P ). You can start really small, for example [dataurl](http://hackage.haskell.org/package/dataurl)
I second Yesod.
To give some perspective, I decided to make graphics / image processing with it, and it has been okay :/ Now, since Haskell is very good for parsing / code generation, you could probably make a transpiler of some sort, and I think that would be much better. 
There is a bit of context that is missing here, regarding "don't worry, you don't need to know what free monads are!". Of course it is interesting to know what Monads or Free Monads are. Of course it is interesting to learn Haskell and dive into the ecosystem it has sprung. Of course these words should not be taken as an advice not to go deeper. I think we can all agree that these concepts have a steep learning curve. And so I felt it was not needed to address the topic by telling the reader "you should learn Haskell, Free Monads, and forget all the OO bullshit you spent that much time learning". I do not think such an approach is interesting nor would work in practice. _The goal behind this post is explained in its introduction_: my day to day reality is that I hear a lot of developers that just looked past FP saying "there is not much that matches what I learned in OOP. FP is cute, but I cannot use it for real". I even did some 10 minute Haskell lessons in my company, for more than 80 days, just to hear in response "Yeah, looks cool, but it is not for me". This post was written to address that reality and try to reach part of these developers to explain them that: * FP is not OOP: OOP patterns not applying directly in FP does not mean that FP is crap * FP has its own pattern to deal with real world concerns (which are more declarative in general) * That these patterns are not frightening, just different (and to do so, I wanted something reassuring - hence the reassuring words) My hope is that some reader (even just a few of them would be great) will then go dive into the subject some more, or just be interested in learning some Haskell.
&gt; Write some secrets to the test server: &gt; &gt; # Tell the vault client to connect over HTTP &gt; $ export VAULT_ADDR='http://127.0.0.1:8200' &gt; $ vault write secret/hello foo=world bar=supersecret Security problems: * Anyone observing `ps` or `/proc` will see your command line. Your secrets are completely exposed to local users until `vault` has terminated. * It's easy to forget to disable shell history. Your secrets may be stored unencrypted.
`vault write` terminates quickly, it just makes a write request to the server. Hopefully you don't have random local users on your production servers :) Also, on FreeBSD, `/proc` isn't even mounted by default, and there's a sysctl that prevents users from seeing others' processes (in `ps`, etc.)
I feel like Backpack is a good indication that haskell's module system is/was too weak. I didn't realize how lacking it was until I'd worked with agda for a while. Coming back to haskell one of the things I missed the most was the module system. I'm not sure how similar Backpack is so it might be interesting to look into. I believe agda's was inspired or at least similar to ML's functors which you might find documentation on more readily.
Once you get really good at writing parsers in Haskell you can try writing one for an indentation sensitive format: it's possible but it's definitely trickier than parsing formats that use start and end tokens!
Thanks for your comment, this is interesting. I apologize for formulating my remark in a very unclear way (the fact that it came right after the snark paragraph did not help), but indeed I agree that the fact that Free Monad has a precise definition does not mean that one should force people to learn it before discussing the patterns of functional programming or comparing with the OO world. (I've done similar decompositions of the SOLID principle in the past, which is somewhat easier.)
Libraries to talk to existing REST APIs is relatively easy too, not sure if it is a good way to learn something though if you do not build anything on top of the API library afterwards.
For the type of functions in that module I'd probably have used [binary-bits](http://hackage.haskell.org/package/binary-bits) (either that or add some bit-twiddling related functions to my parser library)
You really ought to take a look at ML. Standard ML and OCaml both have different takes on module systems like this. It's one of the features that Haskell sorely lacks.
Best talk of ZuriHac!
would you mind sharing with us some articles about the subject please?
I'm of the opinion that a small CLI tool is the best way to get your feet wet. The first useful thing I built was called [dockmaster](https://github.com/samtay/dockmaster) which required constant context awareness (e.g. current directory, home directory, etc.) and hence really forced me to get use to juggling `IO` types around. Another added benefit is that you can see the power of the Applicative typeclass by leveraging optparse-applicative. Since parsing is a great way to get your feet wet with a monad that isn't `IO`, I think a small CLI tool that transforms configuration files between JSON and YAML would probably be a great exercise. Could add in XML, INI as well.
I'm using minideb with a custom Dockerfile to build a base image. It's much lighter basing deployables on `stack-build`. It's difficult to produce an image that would be both minimal and usable for any application, because the set of dynamically-linked libraries can differ from project to project (ICU, Postgres or OpenSSL libs come to mind). `stack-run` used to be built with a hand-written list of all libraries that could possibly be used by Haskell packages in a Stackage snapshot.
My youtube isn't working, but I typed this into vim to see what it did... Holy mother of the loch ness monster. I can't tell you how many keys I've pressed to split READMEs and cover letters and who knows what else into neat 80 character lines. Thank you internet stranger.
Referencing https://www.reddit.com/r/haskell/comments/6kr9tv/why_are_there_so_many_simons_in_the_haskell/djoe6m3/: &gt; It's actually a title, awarded for contributions to functional programming. Perhaps we should have our own altcoin to crowdfund ongoing developments in the Haskell community. Then, practicing simony, we could exchange Simoncoin in order to win the titles "Simon", and maybe even "Master Simon" based on our contributions.
Sorry to hear about the hard drive, that's one of the most sickening feelings.
It's probably domain dependent. If it's your standard web app I'd say whatever toolchain lets you safely and securely take stuff out of a database, display it in some usable way, and put it back in before the browser times out.
[Codewars](http://codewars.com/) also has a few katas in haskell.
Functional architecture - The pits of success by Mark Seemann talks (also) about Hexagonal Architecture (aka Ports and Adapters, or Onion Architecture) https://www.youtube.com/watch?v=US8QG9I1XW0 &gt; These books are so big ["Agile Software Development, Principles, Patterns, and Practices" or "Dependency Injection in .NET"] should be a telltale sign for us: If we need that many pages [700 and 500 respectively] to explain a concept, maybe we start to think there should be a better way. That relates to the fact that Hexagonal Architecture is hard to achieve in Classical OOP. --- It's not about Free Monad is the thing we use an FP. The key is that we use **pure functions**. --- In the first step towards better architecture it's enough to have controlled effect, in other words: **IO**. log :: String -&gt; IO () searchTrainAt :: DateTime -&gt; IO [TrainId] getTypology :: TrainId -&gt; IO (Maybe TrainTypology) reserve :: Reservation -&gt; IO (Maybe Reservation) This will already make the architecture "hexagonal". Especially if the developer tries to have as much stuff outside the `IO` as possible: reservation rules are pure function rules :: TrainTypology -&gt; Maybe Reservation --- Dealing with `IO` directly feels like using a hammer. Let's use simple abstraction, no GADTs! data Interface m = Interface { , log :: String -&gt; m () , searchTrainAt :: DateTime -&gt; m [TrainId] , getTypology :: TrainId -&gt; m (Maybe TrainTypology) , reserve :: Reservation -&gt; m (Maybe Reservation) } Now we can write generic handleReserve :: Monad m =&gt; Interface m -&gt; ReservationRequest -&gt; m ReservationResult And different interpretters are different instantiations of `m`: realInterface :: Interface IO realInterface = Interface { log = putStrLn , ... } fakeInterface :: Interface (State InMemoryDb) fakeInterface = Interface { log = \_ -&gt; return () , ... } --- It will become tedious to pass `Interface` object around, so you'll invent a mtl-like interface soon enough: class Monad m =&gt; TrainMonad m where log :: String -&gt; m () searchTrainAt :: DateTime -&gt; m [TrainId] getTypology :: TrainId -&gt; m (Maybe TrainTypology) reserve :: Reservation -&gt; m (Maybe Reservation) or maybe even split `log` out into `MonadLog`. Then we can write the business procedure with more elegant type-signature: handleReserve :: (MonadLog m, MonadTrain m) =&gt; ReservationRequest -&gt; m ReservationResult The drawback, is that we'll need to use `newtype` to stay sane newtype Real a = Real { runReal :: IO a } -- maybe we'll need to amend this later newtype Fake a = Fake { runFake :: State InMemoryDb a } --- So why we need a Free Monad? The selling point is that Free Monad turns computations into inspectable (to some degree) values. For free Applicatives (where the whole structure is inspectable) there are a lot of use-cases (e.g. optparse-applicative); for Free Monad there is less, as `&gt;&gt;=` hides a lot of structure. In the train reservation, example we are not interested in inspecting the structure, so using Free Monad doesn't really bring us anything. "Tagless encoding" using mtl-like interface gives as all flexibility. *Note:* Scala community feels to be crazy about Free Monads, but they **need** a monad with a trampoline, to get anything nontrivial done without blowing the stack. *Another note:* If you try to make your Free Monad composable, e.g. separate `log` operation into own data-structure, you'll run into *extensible effects*. Feel free to pick *mtl* or *extensible effects* (which works great when effects are "independent", i.e. just different `IO` stuff classified) approach, but IMHO mtl is still simpler. --- To conclude: - Separating IO and pure computations is a win already, forcing you in something close to the Hexagonal Architecture. - Free Monad is an overengineered way to do that. Record of functions or MTL-like type-classes for capabilities are way simpler concepts, no need to introduce scary Free Monad if you don't need to. - Programmming with mtl-like type-class is exactly "programming against the interface, not the implementation". - Free Monad is an abstract implementation, which can act as an interface - do you notice a roundabout here? --- Coincidentally, in his talk Mark uses restaurant reservation as an example, IMHO worth watching.
Construction *makes* ordering a problem. If I define type User = User ["name" =: String, "age" =: Int] then there's a single way to construct that correctly, without having to call `reorder` or `sort` or whatever. &gt; Using HashMap String Dynamic is not really an option due to performance. Which operations are you concerned with?
[slides](https://docs.google.com/presentation/d/1_I5mYXivG5NbOHnICld_Xo41uOyAT57Yade5qfz2toQ/edit#slide=id.p)
I use binary-bits in [Rattletrap](https://github.com/tfausak/rattletrap) for parsing Rocket League replays, which are not byte aligned. It's wonderful! 
[Here](http://agda.readthedocs.io/en/v2.5.2/language/module-system.html) are the Agda docs on the subject. From my limited experience with Agda, the module system really sits in a sweet spot between power and complexity.
I use Atom for a variety of reasons, but [Sublime Text](https://www.sublimetext.com) is a solid choice for a native editor. 
Yes I use TextMate 2. I've also used atom, but a few months ago an update made the performance suffer too much.
When was this?
July, August 1990.
Can be done with parsecT with int as state right? Or do you mean without libraries?
[hledger-ui](http://hackage.haskell.org/package/hledger-ui) is not large, just four screens, but it might be an interesting step up in complexity.
Are there benchmarks too? 
[slides](http://www.math.jhu.edu/~eriehl/compose.pdf)
Slides at http://ndmitchell.com/downloads/slides-drive-by_haskell_contributions-09_jun_2017.pdf
If you want to test this out with Stack, give [this `stack.yaml`](https://gist.github.com/tfausak/623d283cfe7a338bcbfca56ddfec0ecd) a try. 
Thank you for the link, and of course the detailed answer! I will have a look at it as soon as I can. I am not accustomed about the extensible effect library either so I will definitively have a look at it. Regarding your remarks, I completely agree that there are other implementations available (and considered a mtl based one too). But the Free Monad felt (strangly) a bit simpler to explain in this context: in particular, there is almost no type classes to deal with (there is just a small section in the article). But I think I got the "Free Monad" madness you are referring too as well :) 
I'm not sure what you were expecting. Bit twiddling is pretty similar across languages. The main exception is that languages with automatic promotion (C, C++, not sure what else) go out of their way to help you do it wrong. The main weakness in Haskell bit twiddling is that the `Bits` instances don't have enough `INLINE` pragmas. This bites us a bit in `containers`, which has to implement some of its own bit-fiddling functions (wrapping GHC primops) to make `IntMap` and `IntSet` as fast as they can be.
Interesting! I think for now I'll go with the piecemeal HSE + templates approach since my code generation needs are relatively simple. In the future, I'd like to use `ghc`'s API instead like you all suggest. It'd be cool for `brittany` to facilitate pretty code generation. (Any tips/examples or interesting tutorials/docs for `ghc`? I assume I can figure it out from the Haddocks, but I always welcome assistance.)
Not in the repository yet. Mainly because the moment after I write and benchmark something I have a tendency to change the API -- and Mitchell Charity went and templated out a ton of new math primitives based on the old intrinsics-based code that I previously had that change the way the whole API works yet again. I also haven't figured out how to fold the lightweight fiber code in in anything approaching a portable manner. In the meantime * https://ispc.github.io/ has a bunch of examples and benchmarks * https://github.com/nlguillemot/CppSPMD/tree/master/src has a bunch that use a related approach. The approach used there provides a disappointing API, but better performance for the varying machinery. The current way varying&lt;T&gt; works is suboptimal in my current approach.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [nlguillemot/CppSPMD/.../**src** (master → 9190d4e)](https://github.com/nlguillemot/CppSPMD/tree/9190d4efba48198c7dcf0a34e4ed1cb4230b6681/src) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply djx1g2q.)^.
This is really cool! How does it compare to [servant-named](https://hackage.haskell.org/package/servant-named)? Is it compatible with other things like [servant-js](https://hackage.haskell.org/package/servant-js) and [servant-docs](https://hackage.haskell.org/package/servant-docs)?
Yes, it should be compatible with anything that can work with a classic Servant API type - `ToServant (MyRecord AsApi)` gives you a vanilla Servant type you can use in any existing context, and `toServant myRecordImpl` does the same for implementations. Compared to `servant-named`, here you can use familiar record syntax at the type and value level, which has advantages such as better error messages and GHC telling you if you've forgotten something.
Thanks, and well done!
Is there a performance penalty for generics here? (this is a pretty silly question IMO: even if this somehow doubles the latency at $job's Servant app, we'd be clocking in 14ms latency...)
It's possible, but it should be infinitesimal compared to actually executing the request.
A great talk
Which library did you use for handling the images?
This is great but still it does not address the final problem: You have to choose between either extra type level guarantees or functional composability. Call it the metafunctor paradox if you like: Suppose that you have two components. Imagine that they are made by different people or even different companies. then either the two have exactly the same types and that includes the same kind of states -if you lift the state to the type level- or you can not combine them with whatever binary or monadic operator. What is a technique such is monad transformers, typed records etc that can not compose components? anything but functional. Rant follows: Game Over. You have to choose between either functional programming-composability or type level paranoia. The latter means to return to non-equational gluing of components with upstream patches, liftors, transformors, and other artifacts. Call it type safety if you like. I call it "a mess". The former is what real programming problems needs. The latter does not solve real world problems and introduce new ones. There is no technique more cumbersome in software than paranoid type safe haskell programming. so much that haskell is not being used significantly in the real world. It's time to end with the type paranoia and start doing functional programming.
I use TextMate 2 and made a [Haskell bundle][1] for it once, which I think improves upon the official bundle. I don't know whether it still works, though, didn't have time to install it, yet. [1]: https://github.com/HeinrichApfelmus/textmate-haskell
The only recently updated SSH-related packages on Hackage are [vado](https://hackage.haskell.org/package/vado) and [libssh2](https://hackage.haskell.org/package/libssh2) (plus its conduit package). Only the latter seems to support general use, but neither seem very mature. The latter isn't even included in Stackage. They're also not pure Haskell, if that's what you're looking for: vado spawns an ssh client process, libssh2 is a library binding.
Am I crazy, but is this not one of those things where it's actually much much simpler to just specify a server API using an IDL-like thing (e.g. Protobuf Service, Grpc Service, Thrift Service, etc.) and just generate the "holes" you need to fill in with actual code? (I'm obviously not talking about actually editing generated code -- maybe generate a record type where filling in the fields with functions will achieve an implementation of the HTTP API.) Sure, if you have to conform to some very idiosyncratic HTTP API, there might a big advantage to be able to specify *any* type of HTTP API, but if you're in control of both endpoints...? Asking for a friend. EDIT: I could *kind* of see using this kind of complexity if you *really* need static guarantees about what exactly you may ask servers (and when!), e.g. specifying a whole *session* of interactions (maybe TLS handshakes would deserve this kind of assurance), but for generic HTTP REST APIs? Really?!?
What advantage would that bring that would outweigh the cost of dealing with making sure your generated code is up to date and mapping between Haskell and IDL types? The big advantage with Servant etc is that it's all just Haskell, so you're sure it's all consistent. 
Even with ParsecT with Int as state it can be tricky.
It didn't actually fail, but the Windows 10 installer has trouble with the difference between "reformat one partition" and "reformat two partitions"
I'm not sure what the problem is with making sure your generated code is up to date? Cabal can handle that. (No idea about Stack, but one assumes it can also do it.) There's really no extra step. EDIT: Advantage: No obtuse type-level programming. (etc.). I'm not saying Servant (and the like) are *bad*. I'm just saying that there may be a much *much* simpler solution that solves 90%+ of the problem... and people may be missing it for the type forest. (Don't get me wrong -- I *love* types, but I really think that this is one of those situations where simple code generation is actually a much simpler and appropriate solution.) EDIT#2: I'll also add: This could also be done via Template Haskell. The mechanism doesn't matter, though I *do* like the ability to actually see the generated code. (Yes, yes, you can inspect TH-generated code, but it's a bit harder than "less xxx".)
VSCode takes the best out of both worlds: it is easily extensible and has great addons for Haskell development (syntax highlighting, Haskero, Hoogle) and yet doesn't suffer a drop in performance (as big as Atom does, anyway). If I were you, I'd give it a try.
What's simpler about implementing an API with eg. Protobuf compared to Servant? I think making a basic JSON API with Servant is much simpler than working with proto-lens for example. You don't need to deal with a different language or code generation, and the API you make can be consumed by any client. I think the type level stuff in Servant can be made much simpler (like in my solga library), but it's not like it's more difficult to use.
Thanks for the response. Pure haskell isn't a requirement for me. I was considering spawning a shell myself, but I'd be happy to let some library do it for me.
I'm sorry, are you shifting the goalposts? ;) More seriously, I have no experience with Protobuf in Haskell-land, but in JVM-land it's literally just a question of implementing an interface (that was generated for you). If you update the service definition you'll get compilation errors until you bring your implementation up to date. (I'm not sure what hprotoc does generate for service definitions, but I imagine it's a type class with similar properties. Regardless, my point is independent of any particular implementation. My point is that **for services** code generation may actually be superior to advanced type-level programming.) There's no type magic, no worring about `:&lt;|&gt;` (etc.), etc. I'm not sure what else to say. Maybe we're just talking past each other.
We're talking about Haskell land here. The magic generated RPC system you're talking about doesn't exist, but if you made it it would be great. I know what you're talking about and I've implemented my fair share of Protobuf-based services in Java but I can't say my development experience was as good as with Servant. If you can get past the tricky type level stuff it's fantastic to have your service description in the same language because nothing gets lost in translation. It's like parser combinators vs generated parsers. 
The built-in bundle seems to be more richer in color highlight than your bundle. However, the built-in bundle has these problems that have been bothering me: https://imgur.com/a/wFqLs Have you noticed these?
It would be interesting to hear from /u/duijf about how they get production secrets into Vault. It has an HTTP API - is that used, or do people actually SSH into prod servers and add keys over the CLI?
&gt; We're talking about Haskell land here. The magic generated RPC system you're talking about doesn't exist, but if you made it it would be great. It's so simple that I wouldn't even think of sharing it. Just the idea "generate code" should be enough. (Plus, licensing and copyright law, etc. The obstacles aren't technical. If only.) &gt; I know what you're talking about and I've implemented my fair share of Protobuf-based services in Java but I can't say my development experience was as good as with Servant. If you can get past the tricky type level stuff it's fantastic to have your service description in the same language because nothing gets lost in translation. It's like parser combinators vs generated parsers. Oh, sure I understand that the (for lack of a better phrase) "Servant way" is sometimes marginally *better* for some ends, but that's usually not where my main point of friction is. My usual point of friction is humans agreeing to a contract -- and I find that a 'simple'[1] IDL is more amenable to that... and as long as we can generate code in the implementors favorite language we're good. [1] Well, they don't have to know all the details, do they? EDIT: So, I guess the anwer to my original question would be: No, I'm not crazy, but neither are you?!? :p EDIT#2: "Event thing" -&gt; "even think". Wtf?
GAAAHHH. I experienced that on a personal computer with Windows 7, back in the day. Not sure about 10/current state of things, but every earlier version I've installed, including 8.x has had ZERO respect for pre-existing partition layouts.
* GHC HQ now also provides FreeBSD and OpenBSD distributions for amd64; this will allow us to more quickly ship distributions to users by eliminating the need for a long lag time between source release availability and having all binary distributions available. Does this mean apt-get will give 8.2 over 6.4 (or whatever)?
OpenGL basics and a Roguelike https://lokathor.gitbooks.io/using-haskell/content/
Shameless plug for my server-generic library which provides a simpler but less featureful implementation of this idea: https://hackage.haskell.org/package/server-generic-1.0.0/docs/Server-Generic.html
&gt; So you'd start writing your program by defining the type of the top-level module, which would be a higher level module consisting of other sub-modules. And when you want to swap a sub-module because the old one is deprecated you could do so as easily as changing a few lines of code, providing that their types match. &gt; &gt; In the web-browser example what this would look like is that your submodules would be wrappers around the external libraries that you use. So if you wanted to switch from Webkit to Webkit2 or QT or anything else, you'd no longer need to rewrite all of your code, but simply write a new wrapper that has the same type as the old one, and plug it back in. Holy inner-platform effect, Batman! This is a *terrible* idea in the general case. If you have really good reasons to want to be library-independent in a specific domain, e.g. UI, then take the time to spec out a proper abstraction layer, and write facades that implement that abstraction layer in terms of, e.g. Qt or Webkit or whatever. Don't just dive in like some freakin' rockstar ninja coder; that's a recipe for disaster.
I had some difficulty at the beginning becuase "computation" was used as a noun representing what I would call the "result of a computation", where the verb version of computation is represented by f. Keep that in mind when watching, f is the actual computation that is executed that will heat up the CPU, and T is extra structure in the result type --- it is the monad.
I discovered your blog a while ago and just wanted to say I love the post about mazes as well as the post about lazy dynamic programming. Great work indeed!
&gt; Am I crazy, but is this not one of those things where it's actually much much simpler to just specify a server API using an IDL-like thing (e.g. Protobuf Service, Grpc Service, Thrift Service, etc.) and just generate the "holes" you need to fill in with actual code? You should ask the people who use Swagger this. Or QT. It's not bad the first time. Going back and redoing it? Code generation has limitations. Look at how everyone else is arriving at it in the web world. The most successful frameworks use scaffolding out of the gate, but don't revisit it. I've used servant so far because I like the swagger generation so other languages can quickly generate clients. I'm not so concerned about this notion of a "type safe client-server relationship" (that's a fantasy anyways).
&gt; Does this mean apt-get will give 8.2 over 6.4 (or whatever)? No. `apt-get` will get you whatever the package manager for your distribution provides. For the recently released Debian Stable that is [8.0.1](https://packages.debian.org/stretch/ghc). Not idea what Ubuntu might be providing but I do know there is an Ubuntu PPA with a wider range of options. 
 &gt;Does this mean apt-get will give 8.2 over 6.4 (or whatever)? No. That's determined by the distribution. Debian freezes versions in stable releases. Contact your distributions maintainer or read their documentation for more information.
I thought so, thats why for my first try at a compiler i went for one that doesnt care about whitespace
Sure, if you're TRYING TO PARSE FUCKING C++ (MOC), it's going to be tricky. This is *not* controversial. Everyone knows this by now. Most languages are not quite as awful, and for most purposes code generation is actually pretty simple and pleasant. (You're absolutely right about tooling being... interesting, but that the exact point I was trying to make: If you generate *reasonable* code then *all* the tooling just works. Out of the box. No further work needed.
I've written a couple of interpreters for languages that don't care about whitespace, but for my first try at a compiler I'm doing something that not only cares about whitespace but does so differently from any language I know about.
Twitter thread: https://twitter.com/BartoszMilewski/status/883433307119157248
I'm used to things in Haskell being unequivocally better than any other language I've used before, and this is the first thing I found that wasn't :). Another thing that irked me was the way I had to stick `fromIntegral` everywhere, but that might be due to the absence of automatic promotion that you mentioned. I do wonder what a better approach to this would involve. For example, I'd like to be able to state "the output of this function is a list of numbers that are all less than 128" or "The output of this function is a list of numbers that all have the MSB set except for the last one" in my type signature. Is that a dependently typed thing?
Thanks for sharing! After a quick skim I can't see much that would have vastly improved my life while writing this, but I did write [a `parseVarInt` function](https://github.com/vaibhavsagar/duffer/blob/39d5a9f5333a974b2c2a202c8bfdf5475f2764df/duffer/src/Duffer/Pack/Parser.hs#L83) that came in very handy.
`NamedFieldPuns` is quite nice to improve the readability of code that relies a lot on record data types. Some people like to use `RecordWildCards` for this, but I really dislike how it complicates parsing -- not just for the compiler, but for me as well.
Cool, you should share it. Would love to see your ideas
Only a small proportion of my ideas have made it from my brain to my computer so far, but it's [here](https://github.com/quickdudley/carry). Since you were specifically interested in the layout rule I'll add some comments to the function that does most of it.
This is great! Would it be possible to generate typesafe links a la `safeLink` based on the record field name? To me that would be the killer feature, as I find `safeLink` to quickly become unwieldy for complicated paths.
It isn't silly. If we don't pay careful attention to performance then we end up with dozens of layers of unnecessary slowness piled on top of each other and the result is the constant "why is haskell so shitty on this benchmark?" threads.
Check out [the "language features" section in the GHC user guide]( https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/lang.html). Toplevel function definitions get signatures, but `instance` methods don't. It can be quite annoying when you encounter a new type class to have to do mental gymnastics to make sense of the types. `InstanceSigs` fixes that: you can now annotate `instance` methods with their specialized types. 
It's called the [monomorphism restriction](https://stackoverflow.com/questions/32496864/what-is-the-monomorphism-restriction). [Here's a recent post here](https://www.reddit.com/r/haskell/comments/6jivj1/monomorphism_restriction_why/) where I gave another explanation 
[removed]
SPJ's talk on join points from their paper back in November. The change is present in the upcoming GHC 8.2.1 release. Paper: https://www.microsoft.com/en-us/research/publication/compiling-without-continuations/ PLDI 2017 proceedings: http://sigplan.org/OpenTOC/pldi17.html Join point's page on trac: https://ghc.haskell.org/trac/ghc/wiki/SequentCore Ticket tracking the progress: https://ghc.haskell.org/trac/ghc/ticket/12988
Obligatory plug for [`freer-effects`](https://hackage.haskell.org/package/freer-effects), since `freer` isn't active anymore.
Usually the reason for using something like protobuf/gRPC is to ensure that the interface specification is language neutral and not controlled by one side
Fantastic talk!
Great talk ! If only the audience hadn't derailed it every minute or so with nitpicks ..
Thanks for that, I've been wanting this for ages!
I wrote an implementation of this in PureScript (https://github.com/natefaubion/purescript-run) using the row system, which is identical to the named effects in the post. I also wrote a full Pipes implementation (https://github.com/natefaubion/purescript-run-streaming) using this approach, which I think is really nice, and _greatly_ simplifies the types. To reiterate something mentioned in the post, it's often cited as an advantage that freer effects lets you use multiple types for a given effect, but this is somewhat disingenuous. This is exactly as true as it is using MTL effects _without_ functional dependencies, which almost no one wants to do because inference is awful. Using labels on effects is equivalent to adding the functional dependencies back in, and you get back full inference. You can still have multiple types for a given effect, but you must concretely target the label, such that the label determines the type of effect. In PureScript, we use a compiler-solved class `RowCons` to do this.
It might help if you told us what your current favorite languages and programming interests are.
One other thing that's neat about the implementation of `run-streaming`, is that it specifically relies on _duplicate labels_ for many operations. Ie., that multiple types inhabit a given label and are ordered. This keeps the effects truly compositional, in a way that's not possible even in Pipes proper.
`InstanceSigs` is a life saver, especially when dealing with type families
Have you noticed incorrect/inconsistent colors such as these? https://imgur.com/a/wFqLs (TextMate 2)
I can't help but feel slightly responsible for this =)
Keep it up Bollu, I'm really excited to see where this goes. (also, if as a byproduct of this SIMD support in GHC improves greatly, including in the NCG, I would be super happy - and of course you're going to have to do this work so you can make fair comparisons between the LLVM and NCG! =)
`MultiParamTypeClasses` is one of those "How come this isn't on by default?" extensions. That, and `FlexibleInstances` and `FlexibleContexts`. 
I looked into this and it's certainly possible to do data Site route = Site { about :: route :- "about" :&gt; Get '[PlainText] Text , faq :: route :- "faq" :&gt; Get '[PlainText] Text } deriving Generic and then fieldLink faq == "faq/" Is this what you had in mind, or getting a link for a nested route?
probably. Im not picky
I turn on `LambdaCase` and `MultiWayIf` without hesitation if it looks like they will be useful. I enable `TypeApplications` if there is ever an ambiguity. I honestly think that the `show . read` and `Overloaded*` controversy is completely ignorable with `TypeApplications`. `show . readInt` and `Set.fromList` are IMO worse than `show . read @Int` and `fromList @Set`. Since with the latter you only pay the extra syntax when there truly is an ambiguity, whereas with the former you pay every time. Once I start dealing with more complex situations and want to put more information into the types. I tend to very quickly turn on `TypeFamilies`, `MultiParamTypeClasses`, `GADTs` , `DataKinds` and if necessary `Flexible*`. When dealing with external libraries and frameworks I often also see usage of `Derive*` and `OverloadedStrings`, as well as some of the stuff above, depending on how heavily the library uses the type system.
https://github.com/haskell-servant/servant/blob/master/cabal.project or https://github.com/haskell/hackage-security/blob/master/cabal.project
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [haskell-servant/servant/.../**cabal.project** (master → 265b427)](https://github.com/haskell-servant/servant/blob/265b4276db908d01130532ace098a881794afc69/cabal.project) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply djyazte.)^.
Here is an implementation that doesn't require any `Dynamic`. It's just a minimal example and doesn't even have lookups or anything, but implementing such functions is pretty easy. module Dict (CanInsert, Dict(Nil), Insert, Item(I), (&lt;:)) where import Data.Proxy (Proxy(Proxy)) import Data.Monoid ((&lt;&gt;)) import GHC.TypeLits (CmpSymbol, KnownSymbol, symbolVal) import GHC.Types (Symbol, Type) data Dict :: [(Symbol, Type)] -&gt; Type where (:-:) :: Item k v -&gt; Dict as -&gt; Dict ('(k, v) : as) Nil :: Dict '[] infixr 5 :-: instance (Show (Item k a), Show (Dict as)) =&gt; Show (Dict ('(k, a) : as)) where show (a :-: Nil) = show a &lt;&gt; " &lt;: Nil" show (a :-: as) = show a &lt;&gt; " &lt;: " &lt;&gt; show as instance Show (Dict '[]) where show Nil = "Nil" data Item :: Symbol -&gt; Type -&gt; Type where I :: forall s a. a -&gt; Item s a instance (KnownSymbol k, Show a) =&gt; Show (Item k a) where show (I a) = "I @" &lt;&gt; show (symbolVal $ Proxy @k) &lt;&gt; " " &lt;&gt; show a class CanInsert (k :: Symbol) (v :: Type) (as :: [(Symbol, Type)]) where type Insert k v as :: [(Symbol, Type)] (&lt;:) :: Item k v -&gt; Dict as -&gt; Dict (Insert k v as) infixr 5 &lt;: instance CanInsert' nk nv '(k, v) as (CmpSymbol nk k) =&gt; CanInsert nk nv ('(k, v) : as) where type Insert nk nv ('(k, v) : as) = Insert' nk nv '(k, v) as (CmpSymbol nk k) na &lt;: (a :-: as) = insert' na a as $ Proxy @(CmpSymbol nk k) instance CanInsert nk nv '[] where type Insert nk nv '[] = '[ '(nk, nv) ] I nv &lt;: Nil = I nv :-: Nil class CanInsert' (nk :: Symbol) (nv :: Type) (a :: (Symbol, Type)) (as :: [(Symbol, Type)]) (c :: Ordering) where type Insert' nk nv a as c :: [(Symbol, Type)] insert' :: (a ~ '(k, v)) =&gt; Item nk nv -&gt; Item k v -&gt; Dict as -&gt; Proxy c -&gt; Dict (Insert' nk nv a as c) instance CanInsert' nk nv '(k, v) as LT where type Insert' nk nv '(k, v) as LT = '(nk, nv) : '(k, v) : as insert' na a as _ = na :-: a :-: as instance CanInsert' nk nv '(k, v) as EQ where type Insert' nk nv '(k, v) as EQ = '(nk, nv) : as insert' na a as _ = na :-: as instance CanInsert nk nv as =&gt; CanInsert' nk nv '(k, v) as GT where type Insert' nk nv '(k, v) as GT = '(k, v) : Insert nk nv as insert' na a as _ = a :-: na &lt;: as 
Afraid not, this merely means that the binaries for these operating systems will from now on be produced by GHC HQ instead of the generous volunteers that produced them previously. However, if you are a Debian/Ubuntu user looking for more modern GHC packaging, have a look at [hvr's PPA](https://launchpad.net/~hvr/+archive/ubuntu/ghc). It generally provides the newest GHC release (including release candidates) for the last several Ubuntu (and, indirectly, Debian) releases.
IMO the downsides of the restriction outweigh the risk of this happening. As it seems like something that is never really going to happen. And things like renaming imports become a massive pain due to the restriction. 
Code generation is where abstraction goes to die, and this includes TH. Code generation is saying, "This is the highest level of abstraction you're ever going to get in this domain, so deal with it, and hope you won't need reusability in what you're doing here".
So react-hs is actually in the process of decoupling the state paradigm from the rest of the library. So that way you can import `React.Flux` for a flux based state handling system, or you can just handle state yourself or import some other system. Doesn't really directly answer your question but thought it might be a relevant thing to note. I haven't actually used redux or flux in pure JS, just vanilla React and then react-hs. From what I heard you could mimic redux by just only having one flux store. 
Thanks, I'm going to try this because of intero support.
You don't need to use `HashMap String Dynamic`. You can instead use the strategy I gave an example of in my comment: https://www.reddit.com/r/haskell/comments/6lgmkr/comment/djybfbq?st=J4VTXXVQ&amp;sh=68663514
Overloaded Strings, always and forever. The 'dervivable x' class of extensions is really nice when you're doing a bunch of stuff with custom types, but I might hold off on those if you're not at the level where you'd be comfortable with writing the instances yourself. RankNTypes is also super useful. As an intermediate haskeller who is usually just trying to get some work done, I don't often find much use for making my own typeclasses, and hence, I don't get much use out of some of the fancier stuff. Honestly I advise staying away from that for awhile until you get your feet under you with building simple programs. You can burn a lot of time building reinventing a square wheel, and it won't be a useful skill to have until you're doing more advanced stuff.
`freer-effects` also seems to be much faster than `freer`, likely due to it's much more efficient (but "unsafe") union type
Thanks, I'll give it a try. Yeah, Atom was slow as hell. Someone seriously needs to make a video to show off their Haskell workflow in VS Code.
That's what makes code generation so reasonable for `lens`, generic deriving, etc., and so lousy for so many other things. If you stick it on something inherently concrete, there's no abstraction to lose.
I don't understand why code complexity enters into it. I feel like we're having two separate conversations.
I think on the contrary that is quite likely to happen. &gt; And things like renaming imports become a massive pain due to the restriction. What are you referring to?
How often do people actually define expensive math computations on the top level and then don't put a type signature and then use it multiple times and GHC's optimizer fails to combine then? A lot of things have to happen at the same time for it to cause problems. &gt; What are you referring to? import qualified Foo fBar = Foo.bar fBaz = Foo.baz fQux = Foo.qux The above doesn't work, because of the dreaded monomorphism restriction, so if you want to do something like the above you are probably going to want 3 lines per rename, since lines alternating between type sigs and function names without spacing looks really ugly IMO. And honestly the above is one of very few situations where you can justify not having a top level type sig (IMO), so it's funny that it's the situation that the monomorphism restriction affects the most. Almost always you should have those top level sigs at which point the restriction doesn't even get activated.
&gt; How often do people actually define expensive math computations on the top level and then don't put a type signature and then use it multiple times and GHC's optimizer fails to combine then? Okay, I was thinking of a different situation, where you define a polymorphic function not at the toplevel, but in a `where` clause in (polymorphic in a non-obvious way) and not inlineable, e.g., too big or recursive. That renaming thing is a good point though.
That'd be great, yeah. 
Ooh I forgot about that. I would be ok with the monomorphism restriction preventing you from using a where bound variable polymorphically (as in multiple times with different types in the same single function call, obviously polymorphism that mirrors the polymorphism in the input / output types is fine), unless you have an explicit type signature. Currently MonoLocalBinds (which gets turned on automatically when you use GADTs / TypeFamilies, and should get turned on if you use OverlappingInstances to avoid unintuitive type checking failures but didn't last time I checked) almost does the above. Although if the where bound variable doesn't depend on any of the input parameters (and essentially could be made a global without any consequences besides naming conflicts) then it won't be monomorphised. On a side note I would be fine with MonoLocalBinds being enabled by default. Since that way turning on GADTs or similar won't ever stop your code from compiling. And everyone loves those extensions. 
It's quite unsatisfying that we cannot compile polymorphic functions with levity polymorphic arguments. Monomorphization (as in C++/Rust) is really the right default for compiling polymorphism, as it's the option that generates the fastest code. It's the default that lets the programmer avoid asking himself/herself "should I hand-monomorphize this code for performance or can I afford to use a polymorphic function?". This is the "zero-cost abstraction" approach. I wonder if we could allow a limited for of such polymorphism, where we'd compile one version of the function (e.g. `choose` in the talk) for boxed values and one per unboxed type seen at a call site. If there's a call site we cannot compile (e.g. because of polymorphic recursion or whatnot) the compiler would reject the program. This would still allow for all type system features as long as the user uses boxed values and useful polymorphism in the case he/she uses unboxed values.
`LambdaCase` and `ScopedTypeVariables` are pretty useful
Well, my bundle was originally created for TextMate 1. There, I made a few custom syntax categories, and had to manually add them to my color scheme as well. But this probably also means that there will be a relative dearth of colors when these custom categories are not in the color scheme (which is the default). I haven't looked into this issue on TextMate 2, it looks like you can't edit syntax highlighting anymore. EDIT: To answer your question: I haven't noticed these, but that's most likely because I wasn't looking.
&gt; 2017 &gt; 360p Really? This is barely watchable :(
Well for it to really be useful it would obviously need to support nested routes as well. But yes, that is a huge improvement. The `safeLink` version of that would be something like: let faq = Proxy :: Proxy ("faq" :&gt; Get '[PlainText] Text) toUrlPiece (safeLink (Proxy :: Proxy SiteAPI) faq so fieldLink is enormously less verbose, even in this simple example with no captures. This would be really great. Unfortunately I am no good with the type magic side of Haskell myself, so I cannot offer a pull request. But if `servant-generic` gets that, I will probably use it for everything I can.
I'm positively surprised there are any videos from PLDI at all. afaik there are just few sessions from POPL e.g. *Luckily* there are papers, and PACMPL is free access journal! (Hopefully PLDI will join it)
Besides elegance, are there other advantages for the profunctor approach? The article says: &gt; This representation (van Laarhoven) shares many properties with the profunctor representation we describe, but is slightly less elegant (it requires instantiation of the functorFeven when it is not needed—for example, for simple adapters). Does this have performance implications? Conversely, does the representation used in `lens` has advantages besides the historical one? 
I think the main advantage for `lens` is compatibility with the Prelude (`Functor`, but especially `traverse`). [This issue] (https://github.com/purescript-contrib/purescript-lens/issues/26) by /u/edwardkmett is relevant, if a bit outdated
Or if we could hear the nitpicks. It was intentionally a talk encouraging input from the audience, as far as I can tell.
One big factor is that indexed optics don't work in the pure profunctor encoding. Indices really matter to a bunch of use-cases, e.g. it was a large chunk of why `wreq` switched to `lens` from all the little `micro-lens`, `reasonable-lens`, `lens-family` variants -- as bos needed the indices to make it concise to talk about things like HTTP headers and the like. The big factor is the "historical one" you mention, which is that the `lens` approach lets folks supply lenses and traversals without incurring a dependency on lens or any third-party package at all. But there is also a level of concision in the notation compared to the profunctor version: both f (a,b) = (,) &lt;$&gt; f a &lt;*&gt; f b is a perfectly legitimate traversal and writes very easily. It is much more concise than the equivalent in profunctor form. Ultimately any encoding of a "pure profunctor" traversal using "Wander" or "ProductProfunctor" or "MonoidalProfunctor" is jumping through hoops to encode this same thing. Getters and Folds just need `contravariant`. Isos and Prisms need a dependency on `profunctors`, of course, once you incur a dependency on `profunctors` you can write most of the profunctor optics. (IIRC, it is still missing a couple of classes you'd need, though.)
&gt; Monomorphization (as in C++/Rust) is really the right default for compiling polymorphism, as it's the option that generates the fastest code. It also means a lot of compile time. Each time a dependency changes you have to compile the whole graph again. &gt; It's the default that lets the programmer avoid asking himself/herself "should I hand-monomorphize this code for performance or can I afford to use a polymorphic function?". This is the "zero-cost abstraction" approach. That's certainly true. However, you should also consider how often unboxed (in the sense of non-pointer) values are used. &gt; I wonder if we could allow a limited for of such polymorphism, where we'd compile one version of the function (e.g. `choose` in the talk) for boxed values and one per unboxed type seen at a call site. It's not that easy, you will need some kind of JIT specialization (whenever a package changes, but not necessarily at runtime): The call site may not be in the same package. Thus you would have to **a)** ship some intermediate unspecialized representation of the code and **b)** respecialize each package and all of its reverse dependencies recursively in the graph. One could reduce a lot of unnecessary work by keeping track of which definitions actually changed. Also, you would have to do this for every polymorphic definition (function) that is exported.
&gt; Functionality allowing GHC to automatically use the ld.gold linker when available (see #13541). Not only has there recently been significant user demand for this feature, but this also serves to work around a performance bug in BFD ld affecting GHC 8.2 (#13739) Does this mean linking got slower in GHC and is hidden by `ld.gold` speedup, or does it mean a newer `ld.bfd` made it slower and this is fixed by using `ld.gold`?
&gt; Functionality allowing GHC to automatically use the ld.gold linker Where are we with `ld.lld` compatibility? It didn't work when I last tried but that was 8.0.2.
Nice to see the promotion of OpenBSD and FreeBSD bindists. A fully static build of GHC, maybe with integer-simple, linked against musl libc would be useful as a portable Linux bindist that works everywhere.
Is AArch64 using LLVM for codegen?
Famously this is what .Net does. It's pretty neat. 
&gt; If there's a call site we cannot compile (e.g. because of polymorphic recursion or whatnot) the compiler would reject the program. That would mean that the app writer gets an error referring to the implementation of a library function, no? The type-correct usage wouldn't be visible in the type?
[Here you go](http://learnyouahaskell.com/chapters). That should get you started :)
There is `haskell-learning-group` where you can find yourself a mentor in learning Haskell: https://github.com/haskell-learning-group/haskell-learning-group
And much faster than the original `extensible-effects`. The `freer-effects` benchmarks do a decent job of demonstrating this.
[Haskell Programming from first principles](http://haskellbook.com/) is by far the best resource on learning to program in Haskell. LYAH lacks exercises, and as a result only teaches you how to *read* Haskell, not write it. Real World Haskell is outdated, though still useful as a second textbook. HPFPP has a ton of exercies, is well written, and teaches you everything from basic types, to monad transformers, testing, packaging, and project management. My job uses Haskell to develop high performance web applications and worker processes (eg pop a message off a message queue, do a job; grab a row from a database and do a job). I find Haskell to be a suitable tool for any application where programmer productivity, performance, or correctness matter, which is nearly all of t hem.
It was recently decided that PLDI will not join PAC PLAY. 
Exactly one. ((a b) b') ^! c Function application associates to the left and has highest precedence. In the presence of a single operator, this is unambiguous.
i'm sorry but how do you know `a` is a function?
As you used the function application operator, ` `
If it's not, then you're going to get a type error. Syntactically, `a` is a function taking two arguments.
couldn't `b` be an operator?
Not without backticks, since it's an alphanumeric identifier. a `b` b' ^! c This expression does have two possible parses: (a `b` b') ^! c a `b` (b' ^! c) If their fixities are the same, then GHC will complain that it can't unambiguously parse the expression.
`fromIntegral` is indeed needed because there isn't auto-promotion. It makes it *explicit* where a type is being widened or narrowed, rather than leaving those critical points implicit. I can't think of a non-trivial situation where I wouldn't care *a lot* about where that happens. Your typey stuff indeed sounds like the sort of thing dependent types can handle well, but some of those things may also be doable (now or soon) with Liquid Haskell.
Thanks! I took ISPC for a spin. I'm impressed, even on my circa 2013 Intel Core i5 with AVX256. I'll send a PR to cook it on AWS CodeBuild for on demand Xenon timings. Coming from HPC I was indoctrinated that OpenMP and OpenCL were state of the art. I've got a pet problem that involves dense matrix-matrix multiplication over the integers modulo 2; and another that involves composing permutations/transformations on 8 elements. I'm interested to see how it does on smaller packed types. 
What is wrong with `INLINABLE` + `SPECIALISE`? I think this gives you the best of both worlds.
I see: reference http://pldi-sc.blogspot.fi/2017/07/pldi-and-pacm-decision.html?m=1
This seems like a troll downvoted 
I haven't used any other distro than Arch for many years and they enabled SSP and PIE by default in clang and gcc (to land soon in stable channel) https://git.archlinux.org/svntogit/packages.git/commit/trunk?h=packages/gcc&amp;id=5936710c764016ce306f9cb975056e5b7605a65b https://git.archlinux.org/svntogit/packages.git/commit/trunk?h=packages/llvm&amp;id=d8e9410057f7e4ac327f3ea64b8403d075e458cd https://git.archlinux.org/svntogit/packages.git/commit/trunk?h=packages/llvm&amp;id=1d5568f96e56efb2186b3ae081488e1e5a509397 Is that something that would break GHC? Maybe other distros did the same thing before (very likely) and the fallout has been fixed in GHC, if any. Just putting this out there before the final release if that's a problem one should be aware of. I mean, I don't use Arch's GHC package since it's packaged and built in a different way and doesn't match the versions I use. So any issue would be good to know before I waste valuable compile time bootstrapping a stage2 8.2.1.
I see. So that means that is the main interest of post like [this](http://oleg.fi/gists/posts/2017-04-26-indexed-poptics.html) by /u/phadej?
this game ended quickly :D
Unfortunately the approach used in that article means that indexed optics don't automatically degrade to their unindexed form. This means users always have to choose if they are going to supply an indexed version of an optic or an unindexed one, as indexed optics can't be used directly as normal optics. This leads to twice the pressure on the user's namespace and a ton of manual conversion. Most of the time you don't care about the index. This makes you pay to think about it in that overwhelmingly more common case. Similar issues arise if you were to say that all optics had to be wrapped in newtypes making them incompatible with one another. Then you couldn't just use a lens as a traversal, but would have to exploit explicit casts, or one-off tricks to compose things. (Though, admittedly in that case, you wind up with a much much larger explosion in the use of the namespace.) Now to play devil's advocate some more, if the culture shifted to using such ad hoc newtypes, or even to just using a well-stratified hierarchy of profunctor typeclasses, one saving grace if you're willing to embrace that style of approach is that there are some thoughts that you can't really think with lens-style optics in Haskell today. Notably, we're bad at dealing with "relevant traversals" or "affine traversals" because of missing things in our class hierarchy. Also the notion of what I call a "coindexed prism" would let you return an error about why the parse or match failed, but it isn't compatible with the unification trick that we use to make it so indexed optics devolve to normal optics. (You more or less have to choose one or the other.) So, perhaps one could make the benefits outweigh that doubling (or tripling) cost of using an explicitly different representation for "indexed" optics. (There is also another notion of indexing that is currently incompatible with the Haskell encoding that is needed for dealing with walking typed ADTs. Using a less cluttered foundation makes that potentially easier to explore as well.)
We're now using split sections by default (except on Windows which will have it on 8.4) which generates a lot of smaller sections. The problem is that bfd linker is just slow. One of the trade-offs of supporting so many platforms on such an old design is its slow. So we take a hit there. Gold and lld do much much better in that regard. It's a replacement for spit-objs which should help a lot more with reducing file sizes. 
Dead simple benchmark using dynamic libraries so you don't have to recompile driver code. https://github.com/chadbrewbaker/BenchPress/tree/master/vectorAdd
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [chadbrewbaker/BenchPress/.../**vectorAdd** (master → 811aaf5)](https://github.com/chadbrewbaker/BenchPress/tree/811aaf56205c4d715912b98eb9b26d6c9384b3a8/vectorAdd) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply djzvdp3.)^.
Yes. And in turn it made Matthew to play with the ideas further, so who knows what happens next. Maybe Bartosz will come with categorical view of indexed optics :)
I've been trying VS Code today. Mind-blowing. Thanks.
How do I sign on/join?
All information is on that github link: https://github.com/haskell-learning-group/haskell-learning-group#how-can-i-join
Great, thanks for the insight. I appreciate it. Can't wait to dive in with haskell
Thank you. Mentorship will definitely speed up my learning.
Thanks :)
Almost all ways of adding parenthesis change the meaning, but I count 14 ways total. ### `(((a b) b') ^!) c` = `((a b b') ^!) c` = `(a b b') ^! c` = `a b b' ^! c` * `a :: t -&gt; u -&gt; v` * `b :: t` * `b' :: u` * `(^!) :: v -&gt; w -&gt; x` * `c :: w` * This is the unambiguous meaning when written without parenthesis. ### `((a (b b')) ^!) c` = `(a (b b') ^!) c` = `a (b b') ^! c` * `a :: t -&gt; u` * `b :: v -&gt; t` * `b' :: v` * `(^!) :: u -&gt; w -&gt; x` * `c :: w` ### `((a b) (b' ^!)) c` = `(a b (b' ^!)) c` = `a b (b' ^!) c` * `a :: t -&gt; (u -&gt; v) -&gt; w -&gt; x` * `b :: t` * `b' :: y` * `(^!) :: y -&gt; u -&gt; v` * `c :: w` ### `(a ((b b') ^!)) c` = `(a (b b' ^!)) c` = `a (b b' ^!) c` * `a :: (t -&gt; u) -&gt; v -&gt; w` * `b :: x -&gt; y` * `b' :: x` * `(^!) :: y -&gt; t -&gt; u` * `c :: v` ### `(a (b (b' ^!))) c` = `a (b (b' ^!)) c` * `a :: t -&gt; u -&gt; v` * `b :: (w -&gt; x) -&gt; t` * `b' :: y` * `(^!) :: y -&gt; w -&gt; x` * `c :: u` ### `((a b) b') (^! c)` = `(a b b') (^! c)` = `a b b' (^! c)` * `a :: t -&gt; u -&gt; (v -&gt; w) -&gt; x` * `b :: t` * `b' :: u` * `(^!) :: v -&gt; y -&gt; w` * `c :: y` ### `(a (b b')) (^! c)` = `a (b b') (^! c)` * `a :: t -&gt; (u -&gt; v) -&gt; w` * `b :: x -&gt; t` * `b' :: x` * `(^!) :: u -&gt; y -&gt; v` * `c :: y` ### `(a b) ((b' ^!) c)` = `(a b) (b' ^! c)` = `a b (b' ^! c)` * `a :: t -&gt; u -&gt; v` * `b :: t` * `b' :: w` * `(^!) :: w -&gt; x -&gt; u` * `c :: x` ### `(a b) (b' (^! c))` = `a b (b' (^! c))` * `a :: t -&gt; u -&gt; v` * `b :: t` * `b' :: (w -&gt; x) -&gt; u` * `(^!) :: w -&gt; y -&gt; x` * `c :: y` ### `a (((b b') ^!) c)` = `a ((b b' ^!) c)` = `a (b b' ^! c)` * `a :: t -&gt; u` * `b :: v -&gt; w` * `b' :: v` * `(^!) :: w -&gt; x -&gt; u` * `c :: x` ### `a ((b (b' ^!)) c)`= `a (b (b' ^!) c)` * `a :: t -&gt; u` * `b :: (v -&gt; w) -&gt; x -&gt; t` * `b' :: y` * `(^!) :: y -&gt; v -&gt; w` * `c :: x` ### `a ((b b') (^! c))` = `a (b b' (^! c))` * `a :: t -&gt; u` * `b :: v -&gt; (w -&gt; x) -&gt; t` * `b' :: v` * `(^!) :: w -&gt; y -&gt; x` * `c :: y` ### `a (b ((b' ^!) c))` = `a (b (b' ^! c))` * `a :: t -&gt; u` * `b :: v -&gt; t` * `b' :: w` * `(^!) :: w -&gt; x -&gt; v` * `c :: x` ### `a (b (b' (^! c)))` * `a :: t -&gt; u` * `b :: v -&gt; t` * `b' :: (w -&gt; x) -&gt; v` * `(^!) :: w -&gt; y -&gt; x` * `c :: y` * Only form not simplified by having function application at highest precedence. Outside of Haskell (e.g. in academic papers) you might find the parentheses dropped or implied by positioning more aggressively. As you can tell from the associated type annotations I've provided, if you already know the types involved and that the expression is valid, then you may not even need the parentheses to distinguish between these 14 cases. There's another 14 where parentheses are used to turn an operator into an identifier. `^!` -&gt; `(^!)`. There's another 28 where backticks are used to turn `b` into an operator.
sweet!
For the record, the `ProductProfunctor` version is both :: ProductProfunctor p =&gt; p a b -&gt; p (a, a) (b, b) both p = (,) ***$ lmap fst p **** lmap snd p I'll let others decide whether they consider that "jumping through hoops" or merely a slightly modifed version of the van Laarhoven form!
Is `ProductProfunctor` the same as `Strong` / `Cartesian`? Oh, actually it looks like `Monoidal`. What's the significant here?
First of all, let's count how many binary trees with `n` leaves there are. count 1 = 1 count 2 = 1 count 3 = 2 count 4 = count 3 * count 1 -- 2*1 = 2 + count 2 * count 2 -- 1*1 = 1 + count 1 * count 3 -- 1*2 = 2 = 5 count 5 = count 4 * count 1 -- 5*1 = 5 + count 3 * count 2 -- 2*1 = 2 + count 2 * count 3 -- 1*2 = 2 + count 1 * count 4 -- 1*5 = 5 = 14 If the expression was `a b c d e`, there would thus be 14 ways to parenthesize it. Same if the expression was `a a a a a`; with a sufficiently-polymorphic type for `a`, every parenthesization type-checks: a = undefined example01 = a (a ((a a) a)) example02 = a (a (a (a a))) example03 = a ((a a) (a a)) example04 = a (((a a) a) a) example05 = a ((a (a a)) a) example06 = (a a) ((a a) a) example07 = (a a) (a (a a)) example08 = ((a a) a) (a a) example09 = (a (a a)) (a a) example10 = (a ((a a) a)) a example11 = (a (a (a a))) a example12 = ((a a) (a a)) a example13 = (((a a) a) a) a example14 = ((a (a a)) a) a Now, since we have a binary operator, we can use parentheses to turn it into a normal argument (`(^!)`), a left section (`(... ^!)`), or a right section (`(^! ...)`). It’s also possible to use it as a regular binary operator, of course, but since `(… ^!) …` is equivalent to `… ^! …`, let’s avoid over-counting. For the left section, let’s count how many expressions there are if `n` variables are included in the ellipsis of `(… ^!)`: leftSectionCount 1 = count 1 * count 4 — 1*5 = 5 leftSectionCount 2 = count 2 * count 3 — 1*2 = 2 leftSectionCount 3 = count 3 * count 2 — 2*1 = 2 For the right section, there is only one variable: rightSectionCount 1 = count 1 * count 4 — 1*5 = 5 So in total, I count 14 + 5+2+2 + 5 = 28 semantically-distinct parenthesizations. Here they are: solution01 = a (b ((b (^!)) c)) solution02 = a (b (b ((^!) c))) solution03 = a ((b b) ((^!) c)) solution04 = a (((b b) (^!)) c) solution05 = a ((b (b (^!))) c) solution06 = (a b) ((b (^!)) c) solution07 = (a b) (b ((^!) c)) solution08 = ((a b) b) ((^!) c) solution09 = (a (b b)) ((^!) c) solution10 = (a ((b b) (^!))) c solution11 = (a (b (b (^!)))) c solution12 = ((a b) (b (^!))) c solution13 = (((a b) b) (^!)) c solution14 = ((a (b b)) (^!)) c solution15 = a ((b (b ^!)) c) solution16 = a (b ((b ^!) c)) solution17 = (a b) ((b ^!) c) solution18 = ((a b) (b ^!)) c solution19 = (a (b (b ^!))) c solution20 = (a ((b b) ^!)) c solution21 = a (((b b) ^!) c) solution22 = (((a b) b) ^!) c solution23 = ((a (b b)) ^!) c solution24 = a ((b b) (^! c)) solution25 = a (b (b (^! c))) solution26 = (a b) (b (^! c)) solution27 = ((a b) b) (^! c) solution28 = (a (b b)) (^! c)
You are right, it is the same as `Monoidal` in the article.
My favorites are `IncoherentInstances` and `ImpredicativeTypes`. They have a solid theoretical grounding and make code much easier to reason about. Just messing with you. Those are pretty much the worst extensions. My actual favorites are `RankNTypes` and `GADTs` because they increase the expressiveness of haskell. Unlike most people, I don't really like `FlexibleInstances` or `FlexibleContexts`, and I avoid them where possible.
&gt; Unfortunately the approach used in that article means that indexed optics don't automatically degrade to their unindexed form. Interesting. So I wonder what would happen if one just used the same approach as in `lens`. In `lens` we have type IndexedTraversal i s t a b = forall p f. (Indexable i p, Applicative f) =&gt; p a (f b) -&gt; s -&gt; f t Could we do type IndexedTraversal i s t a b = forall p f. (Indexable i p q, Applicative f) =&gt; p a b -&gt; q s t where class ... =&gt; Indexable i p q | p -&gt; q What goes wrong with that approach? 
Compose two of them and the intermediate type is ambiguous.
But `p -&gt; q`, so isn't there some hope left? EDIT: Perhaps we have to make `q` the result of a type function rather than a parameter to the typeclass type IndexedTraversal i s t a b = forall p f. (Indexable i p, Applicative f) =&gt; p a b -&gt; (IndexedBy p) s t
Things get considerably messier when multiple constructors and more fields get involved.
It is the same as `Monoidal` in the article. I don't like that name though, since `Either` also gives rise to a monoidal structure, and what are you going to call profunctors which preserve sums?!
It's true. I tried to make a nice interface to such things for `SumProfunctor` and couldn't get anywhere. Perhaps you should change your go-to counterexample to one involving sums! EDIT: But for more fields, no, I disagree. It's not particularly messy.
Minor nitpicks: It'd be better to use `(&lt;/&gt;)` instead of `++` in `liftIO $ readFile $ config ++ x`; and the type of `veryAwesomeLoader` probably is meant to be `:: .. =&gt; String -&gt; m String`. More on-topic, let me first note that as the author of the `multistate` package I am somewhat biased. The "has" pattern together with `Reader` seems to have a lot of similarity in handling with any mtl-reimplementation that gets rid of the functional dependency, see e.g. the `mtl-unleashed` package. Getting rid of the FD hinders type-inference, but unless I am missing something, everything expressible with the "has"+Reader(+FD) pattern can be expressed in Reader(-FD) with at most one additional function definition that helps type inference along (but without the whole "has" type class (!)) (or even easier: a ScopedTypeVariables type signature before the ask: foo :: (MonadReaders Int m, MonadReaders FilePath m) =&gt; m (Int, FilePath) foo = (,) &lt;$&gt; askPoly &lt;*&gt; askPoly -- no problem with type inference here -- because type information here has -- a clear path inwards from the type sig to the -- askPolys. that was too easy, lets see an example where we need a type sig: bar :: (MonadReaders Int m, MonadReaders String m) =&gt; m (String) bar = do i :: Int &lt;- askPoly -- need sig. because `read`'s polymorphism blocks/muddies the -- inwards path; type here would be ambiguous otherwise. s &lt;- askPoly pure (show i ++ s) and if that type signature is ugly, you could just define `askInt :: m Int; askInt = askPoly`. In comparison to a full type class definition per type this is way less overhead. If you find the `mtl-unleashed` solution unsatisfying, because the `Readers` stack is ugly, then `multistate`'s `MultiReader`(`T`) improves on that, instead of ReaderT Int (ReaderT String m) a you'd have MultiReaderT '[Int, String] m a and you can add types to this without increasing the depth, because `MultiReaderT '[Int, String, Bool]` still has the same depth. This adds to composability (in comparison to Reader±FD solutions), because it remains compatible with any function polymorphic in `rs` for a stack containing `MultiReader rs m a`. With `multistate` your example looks like this: load :: (MonadMultiReader FilePath m, MonadIO m) =&gt; String -&gt; m String load x = do config :: FilePath &lt;- mAsk liftIO $ readFile $ config &lt;/&gt; x awesomeMessage :: (MonadMultiReader Int m, MonadIO m) =&gt; m () awesomeMessage = do repeatCount :: Int &lt;- mAsk replicateM_ repeatCount $ liftIO $ putStrLn "hello!" veryAwesomeLoader :: (MonadMultiReader Int m, MonadMultiReader FilePath m, MonadIO m) =&gt; String -&gt; m String veryAwesomeLoader filePath = do awesomeMessage x &lt;- load filePath liftIO $ do putStrLn "config contains:" putStr x pure x main = runMultiReaderTNil $ withMultiReader (3 :: Int) $ withMultiReader ("./" :: FilePath) $ veryAwesomeLoader "sampleconfig.txt" (and the type signatures there are not necessary I think.) I am not certain if `multistate` is as fast as "has"+Reader, but I'd think that the performance difference is negligible if it exists. Last I checked, `multistate` at least performed a good amount better still than anything in the extensible-effects direction, which is where all of this is heading eventually. (but I would not put too much trust on the very basic benchmarking I did; I think `multistate` makes it very easy for the compiler to inline lots of its mechanics and this might break down at certain complexities / if you throw around sufficiently large actions that are polymorphic in the exact monad (stack) etc..) There might be other downsides to `multistate` though, perhaps the type-level trickery involved (lists lifted to the type level etc.) or perhaps the error messages you get - I have not checked those. TLDR: I think that the solutions provided by the `mtl-unleashed` or `multistate` packages are as good as the pattern described in the post, at less overhead and with no additional downsides.
I've done something like this once in my life. I was using the CakePHP framework and did some of the application logic in haskell. It was a terrible experience, and I do not recommend it. After this, I learned how to use yesod, and started building apps with the entire backend in haskell. Those experiences were much better. Just to be clear, I'm not necessarily suggesting that you should rewrite the application in haskell. If you've already got 10,000 lines of python code that are correctly modeling the problem, just keep using python. On the other hand, if you've got 500 lines of python that are literally just wrapping postgres, I think switching to full-stack haskell on the backend seems reasonable. Also, I'm not suggesting that you should use yesod. Depending on your application, there are several web frameworks which vary in appropriateness.
Perhaps. When I first tried it I wasn't able to make it work, but I may have missed something. (Although, at the time it may have been that I was also wrestling with incorporating coindexed prisms, and those required q to determine p instead.) It has been a while, and I've lost the specifics. If it does work, `q` probably needs to be a class associated type family though as fundeps are a bit weaker. But, I think I remember part of the issue: You get p -&gt; q, and q -&gt; r but no ability to use the composition of two indexed traversals as an indexed traversal, because that doesn't directly tell you p -&gt; r. If you go to package two such IndexedTraversals defined according to your definition above into a new IndexedTraversal it'll fail to package up. Thinking about it out loud, you might be able to get there with an extra superclass constraint on Indexable i p saying that Q (Q p) ~ Q p or something. I don't think I ever tried that!
With that tweak and the use of UndecidableSuperClasses, it seems to work: {-# language FlexibleContexts #-} {-# language FlexibleInstances #-} {-# language KindSignatures #-} {-# language MultiParamTypeClasses #-} {-# language RankNTypes #-} {-# language TypeFamilies #-} {-# language UndecidableSuperClasses #-} import Data.Profunctor import Data.Profunctor.Traversing class ( Unindexed (Unindexed p) ~ Unindexed p , Indexable i (Unindexed p) -- add the kitchen sink here ) =&gt; Indexable i p where type Unindexed p :: * -&gt; * -&gt; * indexed :: p a b -&gt; i -&gt; Unindexed p a b instance Indexable i (-&gt;) where type Unindexed (-&gt;) = (-&gt;) indexed = const newtype Indexed i a b = Indexed { runIndexed :: i -&gt; a -&gt; b } instance (i ~ j) =&gt; Indexable i (Indexed j) where type Unindexed (Indexed j) = (-&gt;) indexed = runIndexed type IndexedTraversal i s t a b = forall p. (Indexable i p, Traversing p, Traversing (Unindexed p)) =&gt; p a b -&gt; Unindexed p s t foo :: IndexedTraversal Int a b [a] [b] foo = error "TODO" bar :: IndexedTraversal Int a b [[[a]]] [[[b]]] bar = foo.foo.foo I've yet to actually run any of the code, just type checked it. UndecidableSuperClasses didn't exist last time I gave this a shot. (The `indexed` combinator might need to be tweaked a bit as well. There are a couple of candidate definitions.)
Using `INLINABLE` + `SPECIALISE` is usually sufficient for what most people need. One thing I can think of that's mildly unsatisfying is that the way you end up working on unboxed primitives. This approach specializes to a concrete type and then (if the function is does anything that involves iterating) applies the worker-wrapper transformation to give you code an inner loop that's operating on unboxed primitives. Then you've got this code at boundaries that does the boxing and unboxing (ie- calling `I#`, the data constructor for `Int`). But maybe it would be nice to not have this boxing happen at the boundaries, especially if your passing the result into something that's just going to immediately unbox it again. Admittedly, the performance penalty for this is probably small, and GHC may end up eliminating the unneeded boxing, but it's not certain like it would be with monomorphization of levity-polymorphic functions. That's the issue I can really think of, but there may be use cases others have in mind that I haven't considered.
I [also](https://www.reddit.com/r/haskell/comments/6m83rn/lets_play_a_game/djzy5m1/) got 28, so I think that's correct. I also see no fault with your generation procedure.
&gt; If their fixities are the same, then GHC will complain that it can't unambiguously parse the expression. Unless they are both `infixr`, or both `infixl`, then it will work appropriately.
Maybe /u/snoyberg is aware of one, but I've never heard of a Yesod site that has been put in production with a load of 100,000+ users. Servant benchmarks are also very troubling. Django/DRF is battle tested with 3k active contributors. It comes with a batteries included admin interface. CSRF tokens and authentication is cake. I have a duty to give my client something solid that they can support without a superstar team like Facebook had when they ran into JSON serialization/database connection pooling issues under load. http://www.serpentine.com/blog/2015/05/13/sometimes-the-old-ways-are-the-best/
Sometimes `FlexibleContexts` is really useful though, particularly with `MultiParamTypeClasses`. For example is you use `ask` to get a value of type `Foo`, then you are going to need `MonadReader Foo m =&gt; m ()` or similar to get the function to work properly and be as general as you want. And `FlexibleInstances` can be pretty crucial when dealing with `DataKinds` and other type level programming. For example making a heterogeneous list you can show will most likely need `instance Show (HetList []) where` and `instance (Show x, Show (HetList xs)) =&gt; Show (HetList (x : xs)) where` which both require `FlexibleInstances`. I do agree to use caution with them, but they can be incredibly essential at times, and the workarounds are generally pretty gross and lack any real benefit.
Do you have an opinion of [Haskell: The craft of functional programming](http://www.haskellcraft.com)? It is the only Haskell book my library had, but it seems good to me so far (though, I haven't read any other Haskell books).
Circa 2011, but it underscores how much Yesod had to catch up on database performance issues at scale. http://www.yesodweb.com/blog/2011/03/improving-persistent-performance
Yeah, these were used throughout the Haskell Book and really help in complicated instances.. not sure why they are restricted by default.
Nice, thanks for the tip on NamedFieldPuns, just found a use for this a minute ago. And yeah, RecordWildCards are convenient but having implicit bound variables can just make things more complicated.
Cool, I did not know about `LambdaCase` or `MultiWayIf`, love the syntax! To be honest I'm not following at all the second paragraph about `TypeApplications`. Care to send me a link where I can read up on something about that? So yeah, I have seen a little bit of the bigger extensions like `TypeFamilies`, `GADTs`, etc., but I have no clue how to recognize a situation where I should start using them. Any chance you could shed light on that, if you have experience with those?
I'd really appreciate sources for in depth discussion and analysis of the `Flexible*`. I've read a few stackoverflow answers on it, but didn't really understand them. Up until now... I basically just try to compile, and obey GHC when it tells me to enable `Flexible*`. Always feel like that meme of the dog at the computer:)
yes.
See [this commit](https://git.haskell.org/ghc.git/commitdiff/2785ef0e31a123400da950ffafebe6cb1ce3f4eb). It does try gold, lld, and falls back to ld. *Note: This breaks on macOS, if you have ld.lld in path, as ld.lld (at least the one from the llvm+clang binary distribution) is an ELF only linker -- which should be fixed shortly, see [this differential](https://phabricator.haskell.org/D3713).* 
That could be. It wouldn't be the only compiler error that wouldn't be visible in types however. It's still a compile-time check.
&gt; It also means a lot of compile time. Each time a dependency changes you have to compile the whole graph again. It does, but we can always throw more resources at the problem. This part of compilation is also parallelizable. &gt; That's certainly true. However, you should also consider how often unboxed (in the sense of non-pointer) values are used. Very often, even for things you might think of as boxed. Consider `Map ByteString Int`. First unboxing the `Int` is a big win (2 fewer machine words per entry), but you might also want to unbox the `ByteString`. `ByteString` already contains an indirection to the payload in it so you might want to unpack the `ByteString` "wrapper" around it to avoid having to chase two pointers to get to that payload. &gt; It's not that easy, you will need some kind of JIT specialization... JIT specialization is another approach to the same problem, but you don't *need* it as shown by C++/Rust.
`INLINABLE` gets us a little bit towards the goal. `SPECIALIZE` doesn't really help much at all: `SPECIALIZE` doesn't work because it's not modular. The library author of a polymorphic function must upfront specialize for some set of types. This means that he/she can't specialize for yet-to-be-defined types. It then falls on the library user to know what to specialize and how (which is bad). Worst case it might not even be possible, because a polymorphic library function might call a polymorphic function internal to the library that the user can't specialize. It also means that the library can't practically specialize functions with more that one type variable. Consider trying to specialize this function: `lookup :: k -&gt; Map k v -&gt; Maybe v`. There are O(n^2 ) specializations to write and that number is prohibitive even if you only consider the scalar types (e.g. `IntX`, `WordX`, and so forth.) This problem is shared by `INLINABLE`. `INLINABLE`, while workable in some cases (that's why I use it in unordered-contains), has several problems: It's fragile, just like anything that relies on inlining. It's know to fail even in simple cases like this: f :: Eq a =&gt; a -&gt; ... {-# INLINABLE f #-} f' :: Eq a =&gt; a -&gt; ... f' x = f x {-# INLINE f' #-} The reason has to do with phase ordering from what I can tell. I have outstanding bug reports against unordered-containers which essentially say "things ought to specialize but in large programs but they don't". We need guarantees of specialization. It doesn't work on data types. The polymorphism in data types is really the most important polymorphism to specialize away, because it directly affects efficiency of data access in programs. The real test of our compilation of polymorphism is whether it can specialize `Map Int Int` and its functions (to not waste 4 machine words per entry and add two additional pointers to chase on access).
A blue ribbon definition of optics! type Optic p a b s t = p a b -&gt; p s t
So `FlexibleContexts` allows things like `MonadReader Foo m` as constraints, which is not typically allowed, instead of just `MonadReader r m`. Typically the thing inside the constraint must be a type variable. `FlexibleInstances` allows things like `instance Show (HetList '[])`, instead of just `instance Show (HetList a)`. Typically you must have exactly one non type variable on the top and only type variables inside. Both are perfectly safe. Although they are sometimes a code smell, and sometimes GHC telling you to turn them on really means you fucked up. I'd still feel free to use them whenever you feel like it though, they won't break anything for sure. 
TypeApplications allow you to specify the concrete value of type variables in polimorphic functions / values: id :: a -&gt; a id @Int :: Int -&gt; Int fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b fmap @[] :: (a -&gt; b) -&gt; [a] -&gt; [b] fmap @[] @Int :: (Int -&gt; b) -&gt; [Int] -&gt; [b] fmap @[] @Int @Bool :: (Int -&gt; Bool) -&gt; [Int] -&gt; [Bool] They can be useful for resolving ambiguities that the type inference algorithm can't resolve automatically. TypeFamilies essentially allow arbitrary functions on types, this can allow you to write functions you may have thought impossible, such as a function that if the input type is an Int returns a Bool, and vice versa. So if you ever want a function with a crazy type that doesn't fit within the usual rules, you probably need type families. Note that before you use them double check if it really is a good solution to your problem, they are often more power than you need. GADTs allow you to make your constructors much less trivial than simply "give me an `a` and a `Foo b` and I'll give you a `Baz a b`" which is all you can really do with the built in algebraic data types. They allow you to write more complex data structures, particularly when paired up with (potentially injective) type families, things like heterogeneous lists and statically length'd lists or reducible ASTs (ASTs where you can remove certain sub-parts and reflect that removal in the type system, say for desugaring). They allow you to encode a lot more invariants in the type system, and typically quite cleanly. So basically, function with weird type: TypeFamilies. Data structure with complex structure / invariants: GADTs (and maybe also TypeFamilies).
Isn't the point of WW to have this inlinable wrapper so that no repeated unboxing/reboxing happens? I don't get how this is special to what `SPECIALISE` does?
Very enlighenting, thanks! I'm not sure how Rust handles things, but the C++ way pretty much is to provide an inlinable implementation which is specialised at call sites. How is that superior to what Haskell does? How could we improve on what we have now?
I can't find presentation slide, it is hard to read from youtube, anyone?
Neat! I've been looking for something like this for my AWS credentials too. Cheers!
This sounds just like [GNU Stow](https://www.gnu.org/software/stow/).
Thanks! I'd read (though not quite taken in) that article; it was what inspired my question about restricting the kinds. If I understand you correctly: in defunctionalization one passes around symbols corresponding to higher-kinded types. In this approach, the symbols are the types themselves. That right? I'll give the type families you suggest a go, too.
Not sure about the slides but the full code can be found here: &lt;https://github.com/sdiehl/zurihac-crypto&gt;
Are you aware that you have a ton of HTML entities in the Haskell code block now?
I'm surprised no one else has posted it yet, but `NoImplicitPrelude`. Ever since I wrote something useful and then failed to eradicate the use of `String` and partial functions from it, I've used that (with [BasicPrelude](https://hackage.haskell.org/package/basic-prelude-0.6.1/docs/BasicPrelude.html)) as the starting point for everything I've done. Also, today I finally got the hang of `ScopedTypeVariables`, and that's pretty handy with heavily polymorphic code.
mixin packages: https://github.com/ezyang/thesis/releases/download/final/main.pdf already implemented (ghc 8.2) &gt; Monomorphization (as in C++/Rust) check &gt; "zero-cost abstraction" approach. check &gt; guarantees of specialization check &gt; The polymorphism in data types is really the most important polymorphism to specialize away check 
&gt; It does, but we can always throw more resources at the problem. This part of compilation is also parallelizable. I don't agree with your argumentation. This is also one thing C++ is notorious for. **;)** I think the more important question to ask is: Would it be a disadvantage to put it in such an intermediate representation (*unspecialized code*)? For example, could this lead to missing optimization opportunities? &gt; Very often, even for things you might think of as boxed. I'm probably not 100% sure what the definition of unboxed entails. In the video it meant kind `#`. Ad-hoc polymorphism isn't a problem with kind `#` but parametric polymorphism usually is. As far as I understand it, the question is how often do you work with kind `#` and parametric polymorphism. &gt; JIT specialization is another approach to the same problem, but you don't need it as shown by C++/Rust. But then you will have to add the source code to each package. As a free software enthusiast I would love to do that but I guess not everyone would be happy about it. (You might also save a lot of work by pre-processing considering that you need to respecialize at least at every adjacent dependency change.)
The [list of accepted papers for HS17](https://www.haskell.org/haskell-symposium/2017/) has been published. I have submitted a PR adding the list to [Gabriel's repo](https://github.com/gasche/icfp2017-papers), don't hesitate to contribute links to preprints. ;)
Isn't the equivalent of type TraversalL s t a b = forall f. Applicative f =&gt; (a -&gt; f b) -&gt; s -&gt; f t bothL :: TraversalL (a,a) (b,b) a b bothL f (a,b) = (,) &lt;$&gt; f a &lt;*&gt; f b just: type TraversalP a b s t = forall p. (Cartesian p, Cocartesian p, Monoidal p) =&gt; p a b -&gt; p s t bothP :: TraversalP a b (a,a) (b,b) bothP p = par p p ? If so I'm failing to see how `bothL` is more concise. Expressed in more familiar Haskell terminology, certainly.
Well yes, but how does that generalise? How do you write the equivalent for `all10` which works on a 10-tuple?
Nice. A simpler version of update-alternatives?
Thanks for the detailed explanation! I suppose all the little projects I've done so far have been too simple to see the need arise for those ones, but I'll have a better idea of when to start reaching for them now.
If you want to give your client something they'll be able to support without you, it seems like a python + haskell hybrid is entirely out of the question. Also, even for your own ease maintaining the project, I recommend not doing a hybrid approach. Yesod gives you out of the box support for csrf tokens and authentication. I don't really know what an "admin interface" in a general context, but yesod definitely doesn't come with that. I've never run into performance problems with yesod, but I've never had a high number of concurrent connections, so I cannot really offer any meaningful commentary on yesod's performance under a heavy load. However, a more useful metric to provide would be not the number of users but the number of requests per second you need to handle. It's easy to handle 100,000 users if they only log in once a week and skim a few pages. That might only end up translating to two or three requests per second, which is easily handled by all web frameworks that I know of.
Fixed?
Yes, thank you :)
Thanks for pointing it out!
For sure, if you have `MultiParamTypeClasses` on, the `FlexibleContexts` and `FlexibleInstances` are pretty much required. But, I don't really use `MultiParamTypeClasses`. I prefer to use associated type families and associated data families, but that's just a personal preference. The `HetList` example is also a good example of where `FlexibleInstances` is the only solution. Interestingly, if you are dealing with a heterogeneous list parameterized by a universe interpreter: data Rec :: (u -&gt; *) -&gt; [u] -&gt; * where RNil :: Rec f '[] (:&amp;) :: !(f r) -&gt; !(Rec f rs) -&gt; Rec f (r ': rs) You can actually write the Show instance another way. What vinyl does is similar to what you suggested: instance RecAll f rs Show =&gt; Show (Rec f rs) where But, we could instead have: class ShowForall f where showsPrecForall :: Int -&gt; f a -&gt; ShowS instance ShowForall f =&gt; Rec f rs where This useful when your universe isn't the kind `*`, and I've actually gotten some good mileage out of this in projects I've worked on.
I can definitely agree with associated type families / data families, I personally also much prefer them, and I pretty much always avoid `FunctionalDependencies` (although I sometimes need `MultiParamTypeClasses` anyway, but generally only when I have a `TypeFamily` that genuinely needs more than one input parameter). With that said I am currently using the FD version of `mtl` due to its popularity, so I'm kind of stuck with MPTC / FD. I think technically can't you make `HetList` work without `FlexibleInstances` with a helper class like the following: class ShowHetList (ts :: [Type]) where showHetList :: HetList ts -&gt; String Then you get: instance ShowHetList ts =&gt; Show (HetList ts) where show = showHetList But like I mentioned at the end of the previous comment, IMO this workaround isn't all that clean and has no measurable benefit. The `Rec` example is cool though, thanks!
God this would be the dream.
&gt; If I understand you correctly: in defunctionalization one passes around symbols corresponding to higher-kinded types. In this approach, the symbols are the types themselves. That right? Indeed! (I would call them "type functions" rather than "higher-kinded types" though.) Now that I look at it again though, maybe it is better to not quite think of it in terms of unifying type constructors and defunctionalized symbols. Instead we could say that the operator `($)` is overloaded to permit the application of both of these kinds of objects. This suggests the following generalization: type family ($) (x :: k) (y :: k1) :: k2 where (x :: k1 ~&gt; k2) $ y = Apply x y (x :: k1 -&gt; k2) $ y = x y where [`Apply`](http://hackage.haskell.org/package/singletons-2.2/docs/Data-Singletons.html#t:Apply) is the open type family that defines application of defunctionalized symbols in the *singletons* package. This should (I haven't actually tried it) make `($)` work with all the symbols that are in the package already, and it allows users to define their own. The identity function symbol seems to be called `IdSym0 l` where `l` is the type of the expected argument.
I like it! I used to have something similar written in ruby but that employer was super-possessive of any and all code I wrote. I never got around to re-implementing it. Would you be amenable to a PR to support Dhall for the configuration as opposed to yaml?
Yeah, technically you can do the `ShowHetList` thing, but there's no good way to write the instances for it without using `FlexibleContexts`. So, it doesn't end up letting you express anything you previously couldn't.
Wait really? instance ShowHetList '[] where instance (Show a, ShowHetList as) =&gt; ShowHetList (a ': as) where Doesn't the above work?
Oh dang. That totally works and doesn't require flexible anything. Thanks for pointing that out!
It's definitely very similar, the key (albeit small) difference is that Confetti is great for swapping out sets of config files with a single command. Definitely a fairly niche use case, but I honestly mostly wrote this for myself at work, where I do that frequently :)
Thanks! Bummer about your employer taking code like that though... I'd _love_ to be able to configure this with Dhall, I've been meaning to check that out for some time. A PR for that would be most welcome!
Ah I hadn't ever heard of update-alternatives actually, it is very similar! From what I can tell the differences are: * Confetti can swap out sets of of config files with a single command (the main reason I wrote it) * Manages those groups with a config file so actually invoking the swaps is as simple as possible * In it's current form, updates-alternatives is way more general and flexible than Confetti is. 
Glad to hear! Managing AWS creds + and application config files simultaneously was the reason for writing. And of course to write more Haskell
`both` is almost by definition the worst example here, as it is literally the one that is written into the definition of Monoidal. Things get a lot messier when you need to deal with multiple cases (and to a lesser extent with more fields). Now you're stuck inefficiently transforming from an arbitrary ADT into pairwise sums and products to shoehorn it through an API, then patiently undoing the damage on the flip side. At the very best you can hope that using a concrete optic with a concrete combinator that case of case analysis can undo the inefficiency of that implementation.
Ah good point, I wasn't thinking about that. For comparison, you've got all10L :: TraversalL (a,a,a,a,a,a,a,a,a,a) (b,b,b,b,b,b,b,b,b,b) a b all10L z (a,b,c,d,e,f,g,h,i,j) = (,,,,,,,,,) &lt;$&gt; z a &lt;*&gt; z b &lt;*&gt; z c &lt;*&gt; z d &lt;*&gt; z e &lt;*&gt; z f &lt;*&gt; z g &lt;*&gt; z h &lt;*&gt; z i &lt;*&gt; z j all10P :: TraversalP a b (a,a,a,a,a,a,a,a,a,a) (b,b,b,b,b,b,b,b,b,b) all10P = hlist10 . (\p -&gt; p `par` p `par` p `par` p `par` p `par` p `par` p `par` p `par` p `par` p) hlist10 :: AdapterP (((((((((a, b), c), d), e), f), g), h), i), j) (((((((((k, l), m), n), o), p), q), r), s), t) (a, b, c, d, e, f, g, h, i, j) (k, l, m, n, o, p, q, r, s, t) hlist10 = dimap (\(a, b, c, d, e, f, g, h, i, j) -&gt; (((((((((a, b), c), d), e), f), g), h), i), j)) (\(((((((((k, l), m), n), o), p), q), r), s), t) -&gt; (k, l, m, n, o, p, q, r, s, t)) `hlist10`'s signature can be inferred, so could be elided, but even so it's definitely additional work over `all10L`. You can drop some parens with `TypeOperators` and `PatternSynonyms`, but it's still more code: infixl 7 :*: type (:*:) = (,) pattern (:*:) a b = (a, b) hlist10 :: AdapterP (a :*: b :*: c :*: d :*: e :*: f :*: g :*: h :*: i :*: j) (k :*: l :*: m :*: n :*: o :*: p :*: q :*: r :*: s :*: t) (a, b, c, d, e, f, g, h, i, j) (k, l, m, n, o, p, q, r, s, t) hlist10 = dimap (\(a, b, c, d, e, f, g, h, i, j) -&gt; a :*: b :*: c :*: d :*: e :*: f :*: g :*: h :*: i :*: j) (\(a :*: b :*: c :*: d :*: e :*: f :*: g :*: h :*: i :*: j) -&gt; (a, b, c, d, e, f, g, h, i, j)) `hlist10` is definitely the kind of thing I'd want `Generics` or `TemplateHaskell` to generate for my record types. 
&gt; Kmett’s constraint library [20] provides generic infrastructure for reifying quantified constraints in terms of GADTs, not unlike in the MonadZipper solution above. While not impossible, encoding the Trans problem with this library is a daunting task indeed. type Trans t = (Lifting Monad t, MonadTrans t) Color me daunted. ;) That said you do have to ask for and manually open the dictionary when you need to prove `Monad (t m)` given `Monad m` and `MonadTrans t`. Quantified constraints should exist.
In Oxford, but no papers from Oxford academics?! 
maybe they should have used class Monoidal op z p where empty :: p z z par :: p a b -&gt; p c d -&gt; p (a `op` c) (b `op` d) type ProductP = Monoidal (,) () type SumP = Monoidal Either Void type TraversalP a b s t = forall p. (Cartesian p, Cocartesian p, ProductP p) =&gt; p a b -&gt; p s t :)
I'm unreasonably excited about this. I'm sure /u/Iceland_jack is, too, given all of the use cases he's found for this feature ;)
Yeah confetti fits that niche of simplicity. If you need something all singing then use update alternatives. I personally dislike UA as its CLI is a bit confusing.
In a [recent talk]( https://www.youtube.com/watch?v=LMTr8yw0Gk4) SPJ said that join points allowed fusion to kick in even in the abscence of `Skip` in the `Stream` type. Would that mean that the first stream version in this post would work well in the latest version on GHC?
The title "Elaboration on Functional Dependencies" sounds really intriguing, especially given the names of the authors. Does anyone happen to have a preprint of this available?
I think the key difference is that you specify what files to symlink in a config file, whereas stow does everything in a directory tree. Stow is then smart about using existing subdirectories or symlinking them.
I love these kinds of posts! First thing I noticed out of the gate: `isize` != `int`. `int` is gonna be a `u32` here, `isize` is gonna be a `u64`. I'd imagine it's this rather than &gt; It turns out that GCC it able to optimize this into a downward-counting loop, which drastically improves the performance. My gut says this has to do with UB in C, but C's UB is not Rust's UB, and that these sizes are actually the difference. Would need more investigation of course, but if similar Rust and C don't produce the same code here, that's a bug. &gt; This means that no namespacing is necessary, but on the other hand adding a new iterator adapter would mean the new function would not follow the same function call syntax as the others. (To me, this seems like a watered down version of the expression problem.) You can, actually, trait Foo { fn foo(&amp;self); } impl&lt;A, B&gt; Foo for A where A: std::iter::Iterator&lt;Item=B&gt; { fn foo(&amp;self) { println!("Foo!"); } } fn main() { let v = vec![1, 2, 3]; v.iter().foo(); } You'd need to do some more shenanigans if you wanted it to be fully chain-able than the terminal call, but I'm not gonna bother here. &gt; the Rust iterator implementation is noticeably slower than the low level loop. I'm finding it a bit harder to follow exactly what bits at this point, but sounds like you may have found a bug! &gt; Also, this behavior of Rust is in direct contradiction to the existential we used above to explicitly hide internal state from our type signature, whereas in Rust we're flaunting it. Rust is getting a form of existentials very soon in the `impl Trait` syntax. &gt; And that hunch is that the double-inner-loop problem is kicking in. Ehhh I'd be skeptical; due to all of the stuff mentioned above with regards to stack allocation, knowing type sizes, etc, usually stuff gets inlined, optimized, and ends up pretty fast. But more than that, there is only one loop here, and that's in `sum`, which is a `fold`, which is let mut accum = init; for x in self { accum = f(accum, x); } accum which is, as mentioned in the article, sugar for a loop. This is why iterators should be just as fast as loops in Rust; they literally compile to them. &gt; This is non-idiomatic Rust, and therefore (AFAICT) the compiler is not performing any such optimizations. Yup, this is going to inhibit _all kinds_ of optimizations.
Really nice post, a great way to learn a lot about both Haskell and Rust. Thanks Michael!
The best I know how to do is to use a compiled Haskell executable. I've done that before. I used elisp to pass the desired region of text to a Haskell executable and replace the region with stdout. It's not great, but you could definitely automate it more if you wanted.
It made my day! I couldn't help sharing the news with my coworkers
Ah I still haven't responded to your comment on the newtype idea, I'll get on that
Thanks for the feedback, I've posted a reply here: https://www.reddit.com/r/rust/comments/6mf2sz/iterators_and_streams_in_rust_and_haskell/dk16oz9/
Thank Yitz. Learning Rust _is_ quite interesting :)
This is news to me, and would be very exciting news. Anyone have a recent GHC build they want to try out my code on and report back? I'm unlikely to have a chance to tonight.
I tried this but added a few lines for FreeBSD, freebsd64: 8.2.0.20170704: url: https://downloads.haskell.org/~ghc/8.2.1-rc3/ghc-8.2.0.20170704-x86_64-portbld-freebsd.tar.xz content-length: 135597700 but I couldn't get it to work. The output are below and I could not get any more info by running it with either the verbose flag nor by running the failing tar command by hand. Any helping pointers? δ stack setup Preparing to install GHC to an isolated location. This will not interfere with any system-level installation. No sha1 found in metadata, download hash won't be checked. Downloaded ghc-8.2.0.20170704. Running /usr/bin/tar Jxf /usr/home/davl/.stack/programs/x86_64-freebsd/ghc-8.2.0.20170704.tar.xz in directory /usr/home/davl/.stack/programs/x86_64-freebsd/ghc-8.2.0.20170704.temp/ exited with ExitFailure 1 ghc-8.2.0.20170704/utils/hp2ps/dist/build/tmp/hp2ps: Can't create 'ghc-8.2.0.20170704/utils/hp2ps/dist/build/tmp/hp2ps' ghc-8.2.0.20170704/utils/haddock/dist/build/tmp/haddock: Can't create 'ghc-8.2.0.20170704/utils/haddock/dist/build/tmp/haddock' ghc-8.2.0.20170704/utils/hsc2hs/dist-install/build/tmp/hsc2hs: Can't create 'ghc-8.2.0.20170704/utils/hsc2hs/dist-install/build/tmp/hsc2hs' ghc-8.2.0.20170704/utils/ghc-pkg/dist-install/build/tmp/ghc-pkg: Can't create 'ghc-8.2.0.20170704/utils/ghc-pkg/dist-install/build/tmp/ghc-pkg' ghc-8.2.0.20170704/utils/hpc/dist-install/build/tmp/hpc: Can't create 'ghc-8.2.0.20170704/utils/hpc/dist-install/build/tmp/hpc' ghc-8.2.0.20170704/utils/runghc/dist-install/build/tmp/runghc: Can't create 'ghc-8.2.0.20170704/utils/runghc/dist-install/build/tmp/runghc' ghc-8.2.0.20170704/ghc/stage2/build/tmp/ghc-stage2: Can't create 'ghc-8.2.0.20170704/ghc/stage2/build/tmp/ghc-stage2' tar: Error exit delayed from previous errors. Error: Error encountered while unpacking GHC with tar Jxf /usr/home/davl/.stack/programs/x86_64-freebsd/ghc-8.2.0.20170704.tar.xz run in /usr/home/davl/.stack/programs/x86_64-freebsd/ghc-8.2.0.20170704.temp/ The following directories may now contain files, but won't be used by stack: - /usr/home/davl/.stack/programs/x86_64-freebsd/ghc-8.2.0.20170704.temp/ - /usr/home/davl/.stack/programs/x86_64-freebsd/ghc-8.2.0.20170704/ Unpacking GHC into /usr/home/davl/.stack/programs/x86_64-freebsd/ghc-8.2.0.20170704.temp/ ...% 
My understanding of this particular case is that join points allow GHC to treat some class of recursive tail calls as plain loops, which allows such functions to be inlined like a non-recursive one
I remember trying that a while back with ViM and thinking that I lost characters across the shell I/O (I think in particular `"` and `'` characters messed things up). I guess I could pass it as a JSON string? In any event, the problem with this approach is that you have to create a `main` parser for every string manipulation function you are interested in using. Terrible! But perhaps better than nothing.. :)
Can't you just have a command line argument to indicate which text manipulation you want?
The end user still needs to manually instantiate the module to make it work for all the types they need it to. But, other than that, backpack does seem like it solves most of the issues pretty well. Thinking of backpack in the context of levity-polymorphism inspired me to play around and open [this issue](https://ghc.haskell.org/trac/ghc/ticket/13955) on the GHC trac.
Hello Ed, do you have a snippet or know of a profunctor optic implementation that encodes your "coindexed prisms"? I was playing around with profunctor optics in Scala and ended up getting stuck on coindexed. Thanks. 
Admin is a view for account management. https://tutorial.djangogirls.org/en/django_admin/ It requires wrapping your DB tables in Django. I think I will start out 100% Django, then wrap DB tables again in Persistent as business logic solidifies; eventually splitting off the Yesod a separate server for a subset of routes.
It's a bit of a mess. The description I gave above for indexing is subtly wrong if you want coindexing. You'd probably need to build a similar class to what I described above, but might need to move the index 'i' into the parameter of the profunctor rather than outside. There are different choices for how it moves around. Does it live outside the profunctor like in the sketch above? Do you move it into 'a', into 'b'? etc. Each of these has different trade-offs in the space of profunctors that they admit. Similarly you get some issues when you talk about where to put the Either in coindexing. Here's a plausible sketch of how it might work. I just wrote it here and haven't typechecked it, etc. though: class ( Uncoindexed (Uncoindexed p) ~ Uncoindexed p , Coindexable i (Uncoindexed p) , Profunctor p ) =&gt; Coindexable i p where type Uncoindexed p :: * -&gt; * -&gt; * coindexed :: p a b -&gt; Uncoindexed p a (Either i b) instance Coindexable i (-&gt;) where coindexed f = Right . f etc. Now something like that i can tell you 'how it went wrong' if it fails, and you can derive combinators analogous to what we have in lens for &lt;. .&gt; and &lt;.&gt; to tell you how to merge together different failure types.
who needs units anyway? https://www.fpcomplete.com/static/iterators-benchmark.png 
Well, you could have a single `main` that accepts switches passed from Elisp. I remember back in the day there was a library for calling Python code from Emacs that worked on a similar principal; Emacs would spawn a Python server and make RPC calls into it. You could define functions for emacs to call, and I think call back into emacs from Python.
While it might be nice to have units on that graph, the point of the post is their performance relative to each other, not their absolute performance. 
&gt; Haven't been able to figure out how to import a function from a module that doesn't export it. You just can't. What are you actually trying to do? I also don't see `isFile` anywhere in [`System.Directory`](http://hackage.haskell.org/package/directory-1.3.1.1/docs/src/System-Directory.html).
Well, you can't! `System.Directory` doesn't know anything about a function `isFile`, so you can't import it from `SystemDirectory`. There is a function `doesFileExist :: Filepath -&gt; IO Bool` which checks if the given file path is a file and not a directory.
The problem is that you are representing your record as a linked list which will result in bad performance `O(n)` (vs `O(1)` in superrecord)
see other comment why this is not optimal (from a perf. point of view)
The cast from and to `Dynamic` will involve an additional pointer traversal to check that the type representations are equal at runtime, which is wasted as we already know that if our type families are correct =) I personally don't see a real problem with only one way to construct an entry. For the two use cases I illustrated we rarely construct these records by hand anyhow.
You could fix that by using a Trie instead. That would give you `O(k)` (where `k` is the key size) insert / modify / delete which is the best you could ever possibly get in any data structure. This works since the structure of a Trie is fully determined by the list of its keys, assuming that is you use some sort of fully determined structure for each piece, which is fine as a linked list of unique Char's is `O(1)` in size, technically.
A trie may help with the O notation, but with the `SmallArray#` implementation we only need to follow a single pointer. You can't reach that with a trie.
Ooh, that's a good point! I can just `Any` and `unsafeCoerce` instead :) The record ordering is something that has annoyed me, though it may not be a problem in larger applications.
Yes! But then you still don't get the single pointer access to an element that you would get if you used a `SmallArray#`, so you'd still be a bit slower.
Can I get a cute error cursor like trifecta does? This is the only reason I switched :)
What's the interaction with DataKinds? Is it possible to satisfy a `forall (a :: K). C a` if for every type in `K`, I have an instance? (I haven't read the paper yet)
Wait a minute, in that case you must be reconstructing the entire array for any modification? Trade offs man. I could do the exact same with mine using a type level linked list but using a SmallArray# under the covers and still be able to have keys that are always ordered. 
I don't really understand what you mean by "type level linked list using a SmallArray# under the covers" - that's exactly what superrecord does?
@edwardkmett why would someone use this over Eigen? Is the advantage that you get to write your own packet evaluation infrastructure?
My point is that having the type level linked list always be sorted and avoid this whole "ordering of keys matters" thing that people are complaining about is very doable. Using a combination of what I wrote and what you already have.
Yes, as you can compute the old positions and the new positions you can then copy the elements one by one. 
I agree that there really should be units in those graphs. The graph is produced by Criterion, so the same problem appears every time someone shows results from Criterion. Although relative performance might be the most important part in this case, I find the absolute numbers to be of some interest as well, mostly as an informative order of magnitude thing. In some other comparative benchmark I saw, that was using Criterion, the units would have been nice to have just to know if higher or shorter numbers were better. The same applies to that image in isolation ^. "Are the numbers time durations or rates/throughput?" /end rant.
I don't think this is quite sound, because you really have a loop. Specifically, the `Indexable i (-&gt;)` instance has an `Indexable i (Unindexed (-&gt;)) = Indexable i (-&gt;)` superclass constraint. So I suspect GHC is within its rights to try to pull `indexed` out of an infinite cycle of superclasses instead of doing the right thing. Edit: I don't know if there's a real risk of things going wrong, but it smells awfully fishy.
How does this affect Cabal's off by default split-objects. I mean, split sections turned on by default in `ghc foo.hs` means either the Cabal config option is opt-out or split objects got replaced by split sections (more granular than just objects), right?
Would the fix also apply if I set LD=lld (for whatever reason) and happen to be on Linux? I made that mistake until I noticed the official and correct way is `LD=ld.gold`, `LD=ld.bfd` and `LD=ld.lld`, not `LD=lld` even though `lld` is in PATH.
I mean mutations are already `O(n)` right? Since you are operating on an array and Haskell doesn't allow mutation. So that doesn't seem like any real penalty, particularly since the movements should be computed at compile time based on the types. 
Meta comment: this post was awesome, and I believe it's much more fruitful than negative comments about, say, certain build tools. This is what brings communities together, thanks! 
I just want to point out that Rust's `&amp;mut` is quite unlike Haskell's `IO`/`IORef`, and is pure in the ways that really matter. `fn foo(&amp;mut T)` is semantically equivalent to `fn foo(T) -&gt; T`. It's using uniqueness types in a similar way to how Clean does, if you want a reference from the functional programming world. (Rust as a whole *Is* impure, and its functions *can* have arbitrary side effects (well, almost) -- but if it did track purity in the type system, `&amp;mut` would be part of the pure fragment! And for that "well, almost" -- it does *not* include being able to mutate arbitrary data, you can only mutate the `mut`-declared local variables and `&amp;mut` references you have unique access to, and some special shared-mutable types like `Cell`, which is analogous to `IORef`. Rust controls the mutability and aliasing of its own types very strictly. But you can read or write files or launch some missiles, if you want.)
Split sections is replacing split objects, split-obj is still there for now (will eventually be deprecated) but the two are mutually exclusive. Split obj was sort of a hack and had various limitations where-as split sections is a bit more disciplined. it's inspired after the `-ffunction-sections` and `-fdata-sections` options in GCC, Clang and MSVC etc. The idea is you put each function and data chunks into their own individual section group. This will increase the size of the generated object file since you have a slight overhead here and increase the number of sections. later when linking, you can tell the linker using e.g. `-gc-sections` that it should remove any sections which is not used, after this groups are combined again in the final link. This results in about 30% smaller files than split-objs.
I've started implementing the idea [here](https://github.com/agrafix/superrecord/commit/41f3d7c296b5c162d9a3c35e0313c550d91a54a2), and it seems to work. Still need to implement `combine` though.
I got really interested in the paper "Algebraic Graphs with Class", I searched for it earlier, but to no success, does anyone have a link to this paper? Edit: Thanks!
Here is the accepted version: https://github.com/snowleopard/alga-paper.
&gt; Specifically, the `Indexable i (-&gt;)` instance has an `Indexable i (Unindexed (-&gt;))` = `Indexable i (-&gt;)` superclass constraint, That cycle is *exactly* what UndecidableSuperClasses is for. During construction elaborates out the superclasses until it reaches a finite fixed point, then it plugs in the bits. This is literally all UndecidableSuperClasses does. It makes it possible to write that line of code. Later on at the use site the elaboration order for how constraints get simplified using superclasses and instances keeps it from going wrong. If the fixed point wasn't bounded, then UndecidableSuperClasses would give you one of its classic unintelligible errors, but it is bounded. This wasn't possible until UndecidableSuperClasses, but the scenario is exactly the sort of thing Simon added them to the language for. Sadly, most of my other scenarios that I wanted them for require something more.
This paper is really nicely laid out. The references even line up perfectly with the bottom of the page!
I thought about doing a blog post, but really, it's just: - Install Haskero I use the terminal tab to run a GHCi for REPL experimentation. The only awkwardness is that if GHCi can trigger Setup.hs build steps such as code generation, I haven't found how. I use Stack for all projects now, and Haskero's Stack support is excellent.
This post is awesome. Thank you for writing it up.
For the most part, this is true. But this kind of mutability would prevent something from an STM implementation from working out nicely.
I believe that setting `LD=x` will honor the choice of `x` and not try any automagic. `ld.x` is a convention after all, and if you specify `LD=...` the assumption would have to be that you customized your toolchain, and want that specific linker to be used. I also do not believe that we should try to automatically correct this in the build system, and if we did, I would consider this a bug.
Just to add to this. This is an ELF feature, MachO has `-dead_strip` and `.subsections_via_symbols`. This however does *not* work with the LLVM backend (yet). It should hopefully work with LLVM5.
Nice post, i'd say it a benchmark between two systems: fusion and unboxed sum really. From my experience, writing fast numeric loops is all about unboxing. Which GHC does pretty well, under the assumption that the loops are fused, either by build/foldr or stream/unstream, so use list or vector doesn't matter here: they should produce identical core, otherwise there's a fusion bug. Another thing to note is that, `Option` in rust is kind like an unbox sum in GHC 8.2, which doesn't carry all the indirections like haskell `Maybe`. So it's OK to use them in iterating. While in haskell we have to rely on case-transformation and inlining, which is definitely not smart enough.
I should mention, I've spent about 45 minutes trying to find such a resource, including sending a join request to the /r/TidalCycles mods in the hope that more specific guidance can be found there. Usually within this time or less, I assume I do not know what I need to know in order to find the results I'm hoping for.
What I'm really looking for is a haskell book that really goes deep into using lenses and traversals. Tutorials explain the types, but don't go through enough practice for what I type to either (a) compile or (b) have compiler errors that I can make sense of.
&gt; I think the more important question to ask is: Would it be a disadvantage to put it in such an intermediate representation (unspecialized code)? For example, could this lead to missing optimization opportunities? I can't say for sure, but I don't think so. You can still specialize the yet-to-be-monomorphized code before exporting it from modules (so it can be specialized at the call site). GHC used to do that for code in interface (.hi) files but after a change to INLINE (for the benefit of rules) it stopped doing that. That does seem to cause problems sometimes (not enough "time" for unspecialized code to both be specialized and optimized given phase ordering in the compiler). &gt; I'm probably not 100% sure what the definition of unboxed entails. You can think of it as kind # or as the kind of types that can be stored in registers plus aggregates of such kinds. &gt; As far as I understand it, the question is how often do you work with kind `#` and parametric polymorphism. Indeed. What I'm trying to argue for here is whatever solution we have it ought to be able to "deal with it" by making it unboxed. :) 
I haven't thought about this in the context of Backpack. It would be interesting to see if one could write a containers library in that style and benchmark it. /u/ezyang perhaps? ;)
GHC already does the same for functions (by putting code in the .hi file). We don't have a story for data types. I think that's all doable but we 1) need to do it and 2) provide guarantees to users when it's done.
Looks like a good book. Less verbose than Haskell From First Principles, but covers less as well.
Odd, thanks for the function. I got the information from [here](https://hackage.haskell.org/package/system-fileio-0.1/docs/System-Directory.html).
I was trying to see if a file exists or not, I got the function from [here](https://hackage.haskell.org/package/system-fileio-0.1/docs/System-Directory.html).
No. Haskell's logic is constructive. Implementation-wise, you need to have the right dictionary in your hand, not just somewhere in the universe.
What are some of the use cases you had in mind?
Would this allow for profuntor and bifunctor to have functor as a superclass?
If you click the [Contents link](https://hackage.haskell.org/package/system-fileio-0.3.16.3) up in the right corner, you will see that the package "system-fileio" has been deprecated. The one everyone else is talking about is [directory](https://hackage.haskell.org/package/directory) [Here is the backstory](http://www.yesodweb.com/blog/2015/05/deprecating-system-filepath)
Rust's invariants are sufficient to implement STM in a library memory-safely, but you of course can't guarantee the absence of other kinds of side effects in transactions. https://github.com/Marthog/rust-stm
It installs the binary in the default install directory, which on Ubuntu is `$HOME/.local/bin` (IIRC). The fact that `stack` doesn't add this to your `$PATH` variable automatically -- and that `stack setup` has to be run manually when needed -- are probably the two biggest wrinkles in `stack`'s UX. Try adding this permanently to your `$PATH`: export PATH="$(stack path --compiler-bin):$(stack path --local-bin):$PATH" It should make `ghc` available from the command line, as well as any programs installed with `stack install`. I'm typing this on mobile without testing it, so I'm probably blowing smoke out my ass. I have zero experience with `tidal` or its workflow, so I can't comment on the second question.
Feels like Hackage should be better at showing that the module/package is deprecated. Is there any way to submit pull requests to Hackage?
With [this](https://github.com/agrafix/superrecord/tree/ed4896aba14db5787fd39e79774d44abd37083eb) commit it does pass the test suite now. The only downside is that when manually writing explicit type signatures for records we have to use `type Record r = Rec (Sort r)` now otherwise it will not work properly.
Wooh, the bug I opened when I was an (over-)eager undergraduate :)
I'm no stack expert, but I do have tidal installed via stack working on my system. Everything in stack is 'project specific' but there's a 'global project' into which you want to install the tidal package. This is what running `stack install tidal` in any directory that doesn't already have a stack project in it does. I have `$HOME/.local/bin` in my `$PATH` anyway so that I can run haskell binaries easily from the command line, but AFAIK that's not how tidal works, it's run as a package from within GHCI. Now, stack doesn't install a `ghci` binary anywhere useful but you can run the global GHCI with `stack exec ghci`. So to make tidal work with my editor of choice (atom) I just had to edit the settings of the tidalcycles plugin, so that it runs `stack exec ghci --` rather than the default `ghci`. Note the `--` is important, it allows some arguments to be passed to GHCI. From reading your post, I see that you're expecting to have a stack project that has `tidal` as a dependency and to sequence some music that way. While I'm keen to avoid suggesting this is a bad idea, simply because I don't know what you're trying to do, getting going with atom and the `tidalcycles` plugin is surely the quickest, easiest way to start making noises with tidal. No stack project necessary. Another note: the version of the `tidal` package on the stackage resolver might be a little old, but I managed to force it to install the latest version with a trick I've now forgotten. Google is your friend. In my case the latest version was nice to have because it overloads some literals, making the syntax a little easier on a beginner. Feel free to keep asking questions until you're making loopy noises!
This is (experimentally) fixed with [this](https://github.com/agrafix/superrecord/tree/abc7c12eb3a84d25fe90cd004d9e8ae770cff01a) commit. The final downside that remains is that you have to write `Record '[...]` instead of `Rec '[...]` when manually providing explicit type signatures. (`type Record r = Rec (Sort r)`). You can now write what you want: do let areEq = (#foo := True &amp; #bar := False &amp; rnil) == (#bar := False &amp; #foo := True &amp; rnil) areEq `shouldBe` True
And what are you now? :)
Don't hesitate to send a PR to [this repo](https://github.com/gasche/icfp2017-papers) to get the link listed!
The "Proof by Static Evaluation" reminds me of Boutillier's work on a [Simple `simpl`](https://hal.archives-ouvertes.fr/hal-00816918/) which AFAICR also tries to only unfold definitions when it'll yield a iota-reduction (i.e. a firable pattern-match). His principled solution is a very nice abstract machine-based approach.
[Official github repo](https://github.com/haskell/hackage-server) And there are already related open issues [#263](https://github.com/haskell/hackage-server/issues/263) and [#372](https://github.com/haskell/hackage-server/issues/372) (the latter is more about singular version deprecations); afaict nobody has started working on either yet.
I think `tidal` is meant to be used as a DSL and does not even contain an executable.
Done, thanks!
This book will have such tutorial: https://www.reddit.com/r/haskell/comments/6c07cu/were_writing_a_book_intermediate_haskell/
Good post, but the first thing that comes to my mind when reading the benchmark problem is "there is a closed-form expression for that."
Do you know any similar libraries that aim to do this? How does it compare to https://github.com/lykahb/groundhog ?
Just so I follow, `split-sections` already does the right thing when generating millions of sections for more effective LTO and also does the right cleanup/reduction at link time, right? I mean, I would expect GHC to do the right thing (`--gc-sections`) if `split-sections` is on. It's interesting that gcc's man page says this for `data-sections`,`function-sections`: &gt; Only use these options when there are significant benefits from doing so. When you specify these options, the assembler and linker create larger object and executable files and are also slower. You cannot use gprof on all systems if you specify this option, and you may have problems with debugging if you specify both this option and -g. Maybe it should mention that this doesn't apply if `gc-sections` is used and more so shouldn't `-flto=...` enable `data-sections`,`function-sections` implicitly? Maybe it can't because it doesn't know if `gc-sections` is used in `-Wl`, but if you're enabling custom flags, then you can be expected to give the required options to LD as well. The split of CC and LD and RANLIB and AS and AR complicates matters here it seems. D's dmd compiler or Go's compiler haven advantage here, but GHC probably couldn't use LD if it was bundled into CC. The compile driver/wrapper ought to do the right thing with a high level set of abstract switches and leave the tools separate as they are, I guess.
That would be a neat package indeed. And it would resolve the [Int64Map issue](https://github.com/haskell/containers/issues/36), which currently would require an enormous amount of code duplication.
How does it compare to https://github.com/lykahb/groundhog ?
Did you read this? It's basically all you need to know to get a quick start with `stack` in general: https://docs.haskellstack.org/en/stable/README/ What you should do: 1. Get a `stack` project going using `stack new` as per [these `stack` docs](https://docs.haskellstack.org/en/stable/README/#quick-start-guide) 2. Go add `tidal` as your dependency in `your-awesome-project.cabal` (as per documented in the 'workflow' section just below) 3. Run `stack build` If everything was right (I tried, it had been for me), you're done! You now have a 'new Stack project that uses Tidal'. To use a library there's no separate `install` step. Just depend on it and `stack build`. The `stack install` command is for installing executable packages so that, well, you get to use your executables from the local bin path, as others have mentioned. It's not for libraries. It's documented here: https://docs.haskellstack.org/en/stable/GUIDE/#install-and-copy-bins 
https://github.com/agentm/project-m36
I don't think this qualifies, never the less it may be helpful: https://github.com/codygman/concise-json-parsing-in-haskell
Try doing: mkdir /home/george/.emacs.d/haskell-fun/ And trying the haskell-emacs reinstall again.
I setup haskell-emacs on osx/emacs 25 and have done this.
Very well written and approachable. I liked the list vs stream fusion comparison at the end.
Very nice. For those who just want to check if a type does/doesn't have an instance, I've found [ifcxt](https://github.com/mikeizbicki/ifcxt) to be very useful.
Surely it can't reset `mut` variables on its own. I think this is what he was talking about.
Yeah, unfortunately it doesn't quite work atm ;) But it should...
It would be **extremely** cool if it did. When I first read the levity-polymorphism paper, I was a little disappointed when I realized that levity-polymorphic values were not possible, although the explanation of why they weren't allowed seemed compelling. Backpack seems like it could theoretically sidestep the whole "how big is this value" issue since there's no codegen going on, just type checking (to my understanding). It gives me some relief that you've confirm that this should be possible.
It'll make sure the `TVar`s are handled correctly but can't prevent you from e.g. mutating non-`TVar` global variables in a transaction or whatever, in which case it'll still be type/memory-safe but will have unpredictable behavior. Is that also what you meant, or something different?
it might be some unit of speed or time, which would flip the outcome
I'm still getting Opening input file: No such file or directory, /home/george/.emacs.d/haskell-fun/HaskellEmacs.hs Do you use stack? Would you be down to send me your stock `haskell-fun` folder?
While I know of groundhog, I have never used it and can not provide a great answer. There are a few obvious differences though. groundhog was designed to be a high level DSL that gets compiled down to SQL and the queries and updates are executed by some other database system. While you could use relatable that way, it is really designed to be used directly as part of a native haskell relational database engine. groundhog uses a bunch of template haskell code generation to make its magic happen. relatable just uses fancy type extensions (though in the example, template haskell is invoked for the `acid-state` backend). `acid-state` is neat because it allows you to use whatever datatype is best for your data set -- it does not force you to make everything relational, even when it does not make sense. For example, if you are dealing with a lot of tree structures -- a relational database is not a great fit. On the other hand, many times a relational database *is* a great fit -- so what do you do then? In theory, you just use `relatable+acid-state`. In practice, `relatable` is still just proof of concept. I have a project in mind that will use it, but it is currently on the back burner.
I might be interested in putting together a small book for lenses and traversals, focusing on practice and exploring some of the compile errors. I'd start with a few blog posts to gauge interest. Any other requests?
Ah! So the problem is not that `System.Directory` doesn't export `isFile`, the problem is that there are two separate packages which expose a module named `System.Directory`, only one of which exports `isFile`. You were looking at the documentation for one and compiling with the other. It's a confusing situation, but fortunately most packages pick a unique module prefix for their module names, so this confusing situation doesn't occur too often. I imagine that the system-fileio package chose to use the same module name as the [directory](http://hackage.haskell.org/package/directory-1.3.1.1/docs/System-Directory.html) package because they wanted to be a drop-in replacement for directory? Except this wouldn't really work, since the exported function names aren't the same. Weird. Anyway, we were confused by your question because there is also a way to write definitions which are private to a module, like this: module MyModule (MyType) where data MyType = MyPrivateConstructor By design, if you then `import MyModule`, you will only get `MyType`, not `MyPrivateConstructor`, and there is no way to force `MyPrivateConstructor` to be exported anyway. Although, now that I think about, [maybe there should](https://youtu.be/zV70nrsMEZg?t=34m36s)! So that's why /u/Syrak linked you to the code for directory's version of `System.Directory`, to show that the module did not define a private function named `isFile`. But as we now know, your problem isn't that `isFile` is private, but that it is defined in a different module with the same name; in fact, since you saw `isFile` in the documentation, it means that the documented module does export `isFile`, it can't be a private function. So, you have two choices. One is to make sure you are looking at the documentation for the correct package. And while you're at it, also make sure you are looking at the correct version; you linked us to version 0.1, the oldest version, but if you were using system-fileio, you would probably be using 0.3.16.3, the latest version. The other possibility is to modify your list of package dependencies to match the documentation you are looking at, by removing `directory` from your `&lt;project-name&gt;.cabal` file and adding `system-fileio == 0.3.16.3` instead. But, as others have pointed out, this package is deprecated, so you probably want the other option.
After pasting `HaskellEmacs.hs` from the GitHub repo into `haskell-fun/HaskellEmacs.hs`, I now get a bunch of intero errors when compiling. I only get this once, however, and if I re-run haskell-emacs it says it successfully compiles. Do you use intero with haskell-mode? Maybe that's the issue? In any event, the stock functions like `Matrix.identity` are now being populated into emacs. The problem now is that when I run, i.e., `(Matrix.identity 3)` (like in the README) I get: Debugger entered--Lisp error: (invalid-function (lambda t "identity :: Int -&gt; [[Int]] Returns an identity matrix of size n. " (cons (quote progn) (cons (list (quote process-send-string) (quote haskell-emacs--proc) (list (quote format) "%S" (list (quote haskell-emacs--optimize-ast) (list (quote quote) (cons (quote Matrix\.identity) (list . t)))))) (quote ((haskell-emacs--get 0))))))) (lambda t "identity :: Int -&gt; [[Int]]\n\nReturns an identity matrix of size n.\n" (cons (quote progn) (cons (list (quote process-send-string) (quote haskell-emacs--proc) (list (quote format) "%S" (list (quote haskell-emacs--optimize-ast) (list (quote quote) (cons ... ...))))) (quote ((haskell-emacs--get 0))))))(3) macroexpand((Matrix\.identity 3) nil) macroexp-macroexpand((Matrix\.identity 3) nil) macroexp--expand-all((Matrix\.identity 3)) macroexpand-all((Matrix\.identity 3)) eval-sexp-add-defvars((Matrix\.identity 3)) elisp--eval-last-sexp(nil) #[257 \20\303
Nice! Will it also cover bifunctors and arrows?
Bifunctors and arrows :)
That is actually the version I use in a more categorical setting, though z and op aren't freely selectable, but must be the unit and tensor of a mono ideal category with the associated natural isomorphisms.
Yea I *think* that's the sort of problem /u/snoyberg was talking about. Where actual purity would not have this problem, the "purity" emulated by borrow rules does.
Sure. I think we're all in violent agreement.
Some of the most common lenses in different packages, patterns for use, and special common cases. Like command line args, tree transformations, etc. Those would help! 
The [lens over tea](https://artyom.me/lens-over-tea-1) article seems to be fairly in depth, but I can't really comment on it as I haven't actually read the whole thing yet, just have it bookmarked.
Wow, thank you for the thorough response. I appreciate it, it makes a lot of sense. As I am relatively new to Haskell projects, I think I need to read up on .cabal files. Thanks.
You need a .cabal file (or a `package.yaml` file in more modern setups) in order to tell the compiler in which packages it should look for the modules you import. How were you compiling your code?
Killer :D
It will cover `contravariant` functors, `bifunctors` and `profunctors`. But not sure about `Arrows`. Though everything is possible :) 
I've recently used [`hnn`](http://hackage.haskell.org/package/hnn-0.3/docs/AI-HNN-FF-Network.html) with success. Maybe I can write more about how in case there is some interest.
In addition to what everyone else has said, to fill you in on where stack installs stuff: Besides `~/.local/bin` for executables, which others have mentioned, stack installs packages in two places (afaik): `~/.stack` for global packages and `${PROJ_DIR}/.stack-work` for project-specific versions. (Where `${PROJ_DIR}` is just the root of the project you created with `stack new ...`).
I do! You can find the current version of the draft "hidden" in my webpage (the page has no entry for it yet): https://people.cs.kuleuven.be/~george.karachalias/papers/fundeps.pdf Once we have the camera ready version ready, I will make an entry for it and send you an update. Thanks for the interest and I hope you like it! :-)
It's true! Our PSE is inspired both from Coq's and Isabellel's simplifications tactics. What we would like to do though is create a more predictable simplification procedure so that the user knows when the simplification is supposed to finish the proof. PSE (using SMT's) has the some basis for this predictability, still working on that... Not sure if Boutillier's work has any predictability/completeness guarantees/design-goals. 
When dealing with instructions that assume you have a global ghc/cabal installation, `stack exec` can be useful. Check this out: $ stack exec -- ghc-pkg list tidal /Users/drb226/.stack/programs/x86_64-osx/ghc-8.0.1/lib/ghc-8.0.1/package.conf.d (no packages) /Users/drb226/.stack/snapshots/x86_64-osx/lts-7.6/8.0.1/pkgdb tidal-0.8.2 /Users/drb226/.stack/global-project/.stack-work/install/x86_64-osx/lts-7.6/8.0.1/pkgdb (no packages) This tells me that `tidal` has been installed into my machine's `lts-7.6` snapshot. If I `cat ~/.stack/global-project/stack.yaml`, I can see a line that says `resolver: lts-7.6`, which is why stack selected that snapshot. Since I ran `stack install tidal` outside of any stack project, it defaulted to the "global project".
To expand on this. If you have installed `tidal` into your global project's snapshot package db (which you probably have), you can get a repl up and running with that package loaded like so: $ stack repl tidal Of course if you intend to write source files, then it is recommended that you make a project folder with a `stack.yaml` file. I'm just trying to illustrate how the default global project can be useful for exploring stuff without creating an explicit project for it.
Not to say that what you link is not helpful, but it does not seem to contain an actual, clear answer to the question in the headline. (if you spot it, the commandline help will mention "install" being a shortcut for "build --someflag", but there is no description of what "build" actually does. From the actual docs, ["What makes stack special?"](https://docs.haskellstack.org/en/stable/GUIDE/#what-makes-stack-special) might give very vague hints, but you'll spot those only once you already know the answer.) You could also say "it works in general rather similar to cabal's new-build" but that seems similarly undocumented on a glance.
Yeah, unfortunately a lot of documentation/tutorials seem to be of the form "if you want standard-usecase-x, press these buttons in this order" but never properly explain what the buttons do. I don't need/use stack much, so unfortunately leave you hanging here and leave it to the experts to actually answer your question. 
(&lt;rant&gt; if only we had one single, established build tool that was properly documented. if you split the group of "knowledgeable members" for particular topic into two or three (if you count cabal new-build) groups, it is no wonder such a trivial answer is not getting a succinct answer.&lt;/rant&gt;)
Just two weeks ago I needed monad transformer composition, and had to use `constraints`. Luckily, `MonadTransCompose` is only a nasty internal detail for now. Unfortunately this paper says &gt; • We provide a prototype implementation, which incorporates higher-kinded datatypes and accepts all examples in this paper and not e.g. &gt; We have implemented type inference for Qualified Class Constraints, using ..., in the Glasgow Haskell Compiler (GHC) (reference to http://www.seas.upenn.edu/~sweirich/papers/gadt.pdf)
Don't worry, this is just a transitional. Over time the group with the better tool will supersede the others.
&gt; Wait, stop the presses! I thought I accounted for this, but the difference might be explained by a rogue "--fast"
&gt; --avoid-bogo-sort
--no-sleep-sort
&gt; --enable-quantum-bogo-sort
I used the `Lifting Monad` approach for https://www.youtube.com/watch?v=YTaNkWjd-ac (In particular I needed a tensor for the category of monad transformers for the precursor to that talk from the week before.)
 They just aren't part of the specification. 
Thanks, that's a great explanation.
Arrows aren't currently planned. Bifunctors will most likely be there (we've been writing about them today, actually).
 Does it have compatibility rules? Or could you replace it with a triplet of type b constraints?
Anyone else having trouble building `cassava`?
 No? 
An (under-)eager overgraduate, presumably.
&gt; No? Good to know. On an entirely unrelated note, every time I see your comments on r/haskell, I wonder about the origin of your handle. Is it lojban?
You also have to write all the `Lifting` instances by hand, which is horrible. There doesn't seem to be any way, at the moment, for GHC to even help talk about implication constraints. The best current option I can see here is something like class Trans t where lift :: Monad m =&gt; m a -&gt; t m a mon :: Monad m :- Monad (t m)
&gt; GHC 8.2.1-rc3 is still about 30 seconds faster than 8.0.2 though.
In the course of writing a new web app, I was looking for some more flexibility in how logging was done though wai's middleware components and I wanted to be able to hide passwords, which was my primary use case for making this. Hopefully it's useful to some other people. Questions, comments, or contributions welcome!
Using the ghc --make name.hs command. 
I don't know. What are compatibility rules, and what are constraints of type B?
op's real question seems to be "how do I start a stack project that uses library X?" rather than the one in the title.
With some GHC patching, it works! See https://ghc.haskell.org/trac/ghc/ticket/13955#comment:5
Good. But remember Rank2Types is the deprecated alias of RankNTypes. https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#arbitrary-rank-polymorphism
This is interesting. Can this encoding be a choice when making bindings for libraries in OOP languages?
I'm glad to see you got further. I'll try and see if these functions are still working in Emacs 25 for me. What version of Emacs are you using?
I was primarily answering 'how on earth can I create a new Stack project that uses Tidal'. Actually I thought others have already explained this irrelevant `stack install` thing (apparently not quite), so I wrote: &gt; as others have mentioned And I thought the link explained the common confusion about 'install'.
The type signature of Mealy machines and the use of rank-2 polymorphism reminded me very much of the [ST](http://hackage.haskell.org/package/base-4.9.1.0/docs/src/GHC.ST.html#STRep) type.
I have a hard time seeing what any of this has to do with actual OOP. None of the essential OOP keywords are ever mentioned (subtyping, late binding, LSP) and it isn't clear how they map to the proposed encoding. Can we please encode objects from some kind of primitive textbook examples? Shapes, Circles and Rectangles, anyone?
I'm not sure if OOP is the most fruitful way to explore this approach, but mealy machines: newtype Auto a b = ACons { runAuto :: a -&gt; (b, Auto a b) } Can be seen as a stream transformer, which can be used for FRP like programming (both paradigms allows locally stateful programming, which I think is a more descriptive term than `OO` in this case). https://blog.jle.im/entries/series/+intro-to-machines-and-arrows.html https://blog.jle.im/entries/series/+all-about-auto.html https://hackage.haskell.org/package/auto EDIT: A monadic mealy machine is also at the heart of the [wires](https://github.com/esoeylemez/wires) library (successor of the more popular `netwire`), which implements proper FRP: newtype Wire m a b = Wire { stepWire :: a -&gt; m (b, Wire m a b) }
Shapes don't encapsulate state, so they don't need to be objects.
Auto is a cool library. You can even save and restore Autos, which I found surprising given that the state is distributed across each individual arrow.
Assuming this gets implemented in GHC some day, two things I absolutely have to try out are: * Quantified contexts involving `Coercible`. Currently, there are some classes that you can't derive via `GeneralizedNewtypeDeriving` due to the way roles for higher-kinded type variables are determined. With `QuantifiedContexts`, we could regain the ability to do this, which would finally allow for, e.g., putting `join` back in `Monad` in a way that works with `GeneralizedNewtypeDeriving`. (See [GHC Trac #9123](https://ghc.haskell.org/trac/ghc/ticket/9123) for a full discussion.) * I have a redesign of `deriving Generic1` in mind that infers `Coercible` contexts instead of `Functor` ones. In addition to producing faster `Generic1` instances, this would allow more things to be `Generic1` instances than before, since we'd be allowed to compose things which aren't legal `Functor`s in a generic way. I have a sketch of how this might look [here](https://gist.github.com/RyanGlScott/596fc1267c3e1195894e77d17ff68e69), with some comments explaining where `QuantifiedContexts` would come into play.
And 3 months later it seems to be broken again. Any chance of moving the content to the github repo?
Thanks if you can give a shot to grenade and write something then it would be helpful.
Hey, I found this article from the author of grenade http://www.huwcampbell.com/posts/2017-02-17-introduction-to-grenade.html it seems interesting and helpful.
I wrote some code recently to generate reports for my spending using the plaid banking API (free development version) and send text notifications by email using my Gmail account. The SMS email relay seems to be down or has blocked me though, so I might need to use twilio. 
I understand that those keywords are important for _class-based_ OOP, but they don't seem essential for OOP. Dr. Alan Kay says: &gt; OOP to me means only messaging, local retention and protection and &gt; hiding of state-process, and extreme late-binding of all things. It &gt; can be done in Smalltalk and in LISP. There are possibly other &gt; systems in which this is possible, but I'm not aware of them. http://userpage.fu-berlin.de/~ram/pub/pub_jf47ht81Ht/doc_kay_oop_en As for late binding, I think a first-class function (which is the representation of Object) can be thought of as "extreme late binding".
It was nice knowing you all!
This is beautiful. But every graph I've needed in industry has been labelled. Is there any hope for extending this to that?
&gt; Still, occasionally, we need objects. Haha, is that all the motivation you're going to give? Interesting post anyway.
Tested with ghc-8.2.0.20170704: https://gist.github.com/jmspiewak/87ccfe0e20c05cb0114c90d26ae23a7c Iterator 2 and 3 are now close to recursion, 4 is ~3 times slower. Some other differences may be caused by llvm versions - it's 3.7 for 8.0 and 3.9 for 8.2. Native Code Gen results are also included.
Are you trying to be funny on such a controversial topic? A bit risky.. Apart from that, this argument assumes that nobody would create yet another tool that is "even better" in the future. And the risk of that probably is higher the more fragmentation already exists.
No, I have specifically avoided words like class or inheritance on purpose. Subtyping and LSP are not class-based notions. Given the strongly typed nature of Haskell I don't see how you can talk about OOO and escape the problem of subtyping. An pbject with late binding is a bunch of closures conveniently packaged up . OK, we already have closures, and if we only ever use objects with one method, an object is just a first class function. Why do we need another encoding? 
It depends on what knowledge has the audience. If most attendees do not know Haskell, neither any "functional" language, neither any language with pattern matching / ADT, you will kill them with a complex example involving a DSL. Perhaps something simple which introduces the Haskell syntax and subtleties, which ends on some opening slides with examples of more complex tasks. Use GHCi output on most of your slides, show code. If your code is longer that 4 simple lines, find another example. For example: - Start with a simple recursive function, such as a `sum`, discuss how this involves pattern matching on the cons cell, a concept which can be generalized. - Quickly show how the type are inferred but static, but polymorphic / generic. For example, an `lerp (a,b) f = a + (b - a) * f`, this may have some benefit : - "static" people will like how it is static, safe, type checked, but generic through `Num` instance and simple to write thanks to inference - "dynamic" people will like how it is as simple to write as python, without clutter, and perhaps they will buy the safety argument - Troll a bit on partial functions (for example: `minimum`), ask the attendee how they solve the problem in there language of choice (you'll get many answers: exception, sentinel value, Boolean flag, we don't handle it). Discuss why you think theses solutions have flaws and show them the `Maybe` approach (with a `case` handling example) - Show them ADT, for example a really simple one such as `data Shape = Rectangle Float Float | Square Float`. Show them pattern matching on it. - Show them polymorphic ADT. For example a binary tree with `length` and `lookat` function. Insist on how much it is cool that this is polymorphic, that types are inferred, but how much it is secure. - Show them generic deriving. This is a killer feature. Show them that by simply deriving `Show`, `Serialize`, `Ord`, `Eq` on a simple type (such as a `data Square = Square Float`) GHC can write boring code for you. - Show them that side effect have a specific type `IO`, don't say the *M* word. Show a simple example of `IO` handling, such as: main = do putStrLn "What is the airspeed of a swalow in m/s?" answer &lt;- getLine putStrLn ("Thats exactly " ++ ((read answer) * 3.6) ++ " km/h") Discuss how there is nothing special or no reason to be afraid of `IO`, but that function which are doing side effects are "tagged" by the type system, and that is great for code understanding. Ask them if they can refactor this piece of C code : int a = computeA(); int b = computeA(); return a + b; in: int a = computeA(); return a + a; // or a * 2 Some will say "yes, that's obvious", some will say "no, perhaps `computeA` is doing some side effects". Tell them how it is easy to know about that in Haskell because that's written in the type of the function. - Show them a few example of library which are really great thanks to the static typing. Such as `QuickCheck`, `Optparse-Generic`, `Dimensional`. Find something which will talk to your audience. - Discuss laziness (a bit, depending on your audience, show that it helps writing more composable code), may reduce runtime or it may simplify the code. - Discuss tooling (mostly stack, that's a killer feature if you come from a community with no dependency management tool) That's enough for a 30 minutes talk ;) You can develop many of these points in one slide and in one minute I guess. I'll end with two slides: - One with some tips if they want to have a deeper look at Haskell. The MAIN tip is "Don't fear the Monad, forget about it, write code, that's just an abstraction, write code !". - Ends with a really long list of great point about Haskell that you cannot handle during this talk, but for which you'll be happy to give a few answers during the question session, such as: - compilation to javascript - concurrency - static analysis (liquid Haskell) - hoogle ! Killer feature I did something like that (in french) a few months ago, see: https://github.com/guibou/AtelierHaskellAramis/blob/master/slides/presentation.rst I had a good reception. Good luck. (edit: grammar / presentation. I'll kill for a statically typed English...) 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [guibou/AtelierHaskellAramis/.../**presentation.rst** (master → 9d3d9e2)](https://github.com/guibou/AtelierHaskellAramis/blob/9d3d9e2f09a2c9a426158b3ddf87d212a8d3757b/slides/presentation.rst) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dk41nu6.)^.
The paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.2237&amp;rep=rep1&amp;type=pdf
Edit distance is the number of editing steps needed to go from one term to another. There are several definitions of edit distances, differing mainly by what counts as one step. In other words, two types are considered more similar the fewer edits are required to change one into the other.
[This](https://www.youtube.com/watch?v=W_i_DrWic88) is an _amazing_ talk on giving technical talks. There are speaker notes for it [here](http://damian.conway.org/IBP.pdf).
GHC actually unconditionally passes `--gc-sections` to the linker so you get dead-stripping automatically for any libs built with `-split-sections` regardless of the flags used to compile/link the final executable. And since base is built with split sections, you can fairly safely assume that all built executables have dead code to strip :) As for LTO, I think function/data-sections is not all that relevant. If you use LTO, it makes more sense to not use sections at all but rather just generate machine code only for the live code in the first place.
Objects is the consequence of lack of composability like a dry river converted in a set of ponds is the result of the lack of rain
&gt; the expression `seq a b` does not guarantee that a will be evaluated before `b` I find this rather perturbing. It means that foldl' f z (x:xs) = let y = f z x in y `seq` foldl' f y xs is no guaranteed to be space-leak free! I can understand that the Haskell specification might not want to force implementors to choose an evaluation order, but why can GHC itself not make this guarantee? Are there good reasons not to?
Assuming your comment relates to the algebraic graphs paper, I created an issue to discuss this: https://github.com/snowleopard/alga/issues/17
I was just stating the basic effect of natural selection in order to provide you some comfort. Currently it appears like everyone's transitioning to Stack and ideally this will go fast enough so we won't have to endure this fragmentation much longer, in order for the risk of your hypothetical "even better" tool to be averted.
Is there a recording of the talk corresponding to these slides? I'm especially curious about the "isomorphism" and "unification" slides, as I think I would prefer to limit the results to those which are more general than anything isomorphic of my query. Yes, `undefined` would be a result for every query, but if we *then* sort the filtered results by edit distance, `undefined` will end up at the bottom of the list.
something something magic strictness optimizer 
tired, mostly
I think Neil talked a bit om how this worked here: http://www.haskellcast.com/episode/012-neil-mitchell-on-development-tools
Oh my! That is wonderful! Thanks for looking into this so fast.
It could be a good choice as long as subclasses never override a member function with a different type signature. eg. if `Increment` was overridden to take two `Int`'s in your library you would need a new type constructor.
I also fail to see how `seq` gives us that guarantee, which I also find a little troubling. My understanding (this could be wrong though) is that `seq` changes the semantics of it enough that forcing `y` to WHNF on each pass becomes a valid program transformation, but it does not guarentee that this transformation will happen. I would love to be wrong.
I agree with you on this. However, I see a benefit in using Confetti over GNU Stow. Let me give an example on why it could benefit someone. Right now, I'm managing my dotfiles with GNU Stow and versioning them with Git. In order to deal with my various setups (personal laptop, personal desktop and work laptop), I have a Git branch for each setup as I didn't find a better solution until now. Each branch is based on master, which contains anything used by all setups. Whenever I update master, I need to rebase all the branches. Anyway, it's not ideal. With Confetti, I wouldn't need to deal with all these branches and I could only have the master branch to keep track of my setups. I'm totally unrelated to the project, I just simply find it awesome.
I use basically this, but substitute lifting from `Lifting Monad` as `mon`. Same amount of work, just with sightly different factoring.
That what `Lifting` gives you, essentially. class Lifting p f where lifting :: p a :- p (f a)
Thank you! There is a link in the video description too :).
I wouldn't dare let beginners touch `IO`. It's very hard to think about what `IO` really means in an imperative mindset. There's no 'side effect' in `IO`. In Haskell when you call the function `getLine`, and even when you evaluate it, nothing's going to read from `stdin`. That whole 'read from `stdin`' thing doesn't happen in Haskell land at all. In contrast, in 'C' when you call `fgets` a read from `stdin` is included *in the evaluation process*. It may be helpful to think those similar to async IO in JavaScript (and Node.js in particular). You can't afford to wait for IO action to complete when you evaluate something because that'll take way too long and you don't want to block your browser/entire server. What do you do? Use a callback! What's happening is basically: - You call one of the special functions that takes a callback `f` (in JS land) - The runtime system does the IO (*out of* JS land) - When that's done, your callback is run (in JS land) It's a pretty similar story in Haskell, where the evaluation of `IO` things are 'in' Haskell, but the *execution* is *outside of* Haskell. Except much much more things are `IO`-ified because, well, purity dictates so :) Yes, don't mention the 'M' word. I had always thought throwing random technical words are fine as long as the audience doesn't really care. Turns out they do :(
Even if I agree totally with you on the nature of `IO`, I think it is too complex for a first look at the language. I rather prefer a beginner thinking that to do side effect in Haskell, you need to : - create a function which "returns `IO`" - use `do` - use `&lt;-` for affectation. That's true, yet incomplete, but it is enough to begin with (at least I think). Later they will understand that `do` notation works for any `Monad`, that `&lt;-` is not really affectation and that `IO` is not impure. But they will have this opportunity only if they are not scared by the language initially.
I have some ideas about examples to give. I would include something that's easy to understand without too much *programming* at all. Something like [diagrams](http://projects.haskell.org/diagrams/) would already be a pretty nice introduction to how combinators could be used to combine Haskell programs, or in this case, cute little shapes. Monadic parser combinators would also be a nice-to-have. [servant](https://hackage.haskell.org/package/servant) shall amaze your audience by its ability to generate client Haskell functions, client JS code, as well as scaffolding around your whole server handler code base, all staticly, within Haskell, so you only write the functions and servant fits them together. An example that's a bit cheaty is this [world most concise image conversion HTTP API](https://haskell-servant.github.io/posts/2015-08-05-content-types.html). I would say to include amazing equational reasoning, but I don't have anything good in mind.
It makes people think `IO` is there just to make programming harder :( At least it hasn't been easy for me to make people buy into purity
&gt; It makes people think IO is there just to make programming harder :( Writting code, yes, programming, no ;). It is harder at first, then it avoids bug. That the same thing with types: writing a piece of code which "runs" in Haskell is more difficult than in python, but is that bad thing? For an introduction, I don't want to insist on `IO`. But I'm sure there is someone in the audience which has eared that io are difficult in Haskell because of Monad. One slide which shows that, apart for the weird `&lt;-` affectation symbol, everything looks familiar, plus one slide which explain that purity helps writing better program, and that's all.
This is great, thank you! It's a little surprising that 2 and 3 are now so much faster than 4.
I agree with starting with very basics that guibou mentions. Near the end, start presenting examples with a library that deals a topic that is familiar in your context - sql, http, json, or whatever you all happen to use frequently.
That makes sense.
Simon Peyton Jones also gave a [great talk](https://www.youtube.com/watch?v=sT_-owjKIbA) on the subject. My favourite piece of advice from it is to "dig deep, not wide": instead of trying to cover all the aspects of the subject in a short amount of time, pick one single aspect and give a deeper explanation of this one facet.
For an audience who doesn't know Haskell, I like to [talk about combinator libraries](https://www.youtube.com/watch?v=85NwzB156Rg), because it's a concept which translates well to other languages and so you can give them a glimpse of what it's like to program in Haskell without having to explain Haskell's syntax.
and here i assumed that the "better tool" would be cabal 2.0. i must be truly blind.
Thanks. I'm running Emacs 25.1.1
You're going to have to cover `IO` somewhere, otherwise no one in the class would even be able to write Hello World. As for "The M-Word", I'd agree that covering monads (or even Functors, for that matter) would be a tall order for a 30 minute introduction, but by no means should they be avoided or tiptoed around in later talks as if they were anything scary or complicated -- they're only a couple of operators after all, and if they're coming from JS, they may already be familiar with the similar concept of "then-able".
You accidentally one of your sockpuppets, /u/metafunctor 
1 create a program everyone know how to do in a language that everyone know 2 do it in haskell. No functional ideology at this step. make sure that everyone understand more or less what it does 3 elaborate from that program: what is hidden behing the `map` and the `do` that you used for doing loops and IO respectively. Tell them that the first is functional, the second imperative, but it is more general because IO is a monad, desugar it and show that a monad is a chaining of function calls that generate a IO expression. 4 Leave open from them many doors to explore and then finish with some references.
I've written and seen this written this for `monad-logger` based loggers before, I need to do the same for Katip scribes soon: https://www.stackage.org/package/katip If you haven't used Katip before, I urge you to take a look at the library and the examples and see how it solves the problem of structured logging and multiple heterogenous logging backends. I am glad you published this, but I'd like to make a comment that is tied to the specific example you chose: passwords. Do _not_ solve this security problem with filter logging! Make separate API-exposable types that have `ToJSON` instances that don't have secure information in them. Just don't generate or write serialization instances for datatypes with information that shouldn't reach the outside world! Including `Show`! In the past I've handled filter logging by copying and pasting a snippet, so I'm looking forward to reading the code over my lunch today to see how you structured this.
Lol. If it was, everybody would be using it and you'd see people asking questions about it here. Instead the only threads we see here are about asking help with Stack, being recommended to use Stack, and getting helped to make best use of Stack. This should tell you something. Feel free to downvote if you agree.
&gt; The ability to die by oneself is quite useful. Metal.
Nope, but it managed to get back to GHC 7.8 speed: https://mail.haskell.org/pipermail/ghc-devs/2017-May/014200.html (I remember when 7.8 was the slow version of the compiler and people used 7.6 for fast validation)
I may be misinterpreting /u/jmspiewak's results, but it looks like it's only that much faster with LLVM. I guess this is because LLVM is probably way better at optimizing regular ole loops than GHC's native code generator?
Yes but please, answer the question. I supposed that my post was not listed because some automatic or manual censorship, so I submitted with other user. Really I don´t know what happens. Do you see the post? I don't. 
Will this lead to an ambiguous grammar in the presence of nested `s?
&gt; Which library binding can you recommend? Is there even more than one option? All the tensorflow packages on hackage have the same author and homepage, so they're clearly different parts of the same library. [tensorflow](https://hackage.haskell.org/package/tensorflow) [tensorflow-core-ops](https://hackage.haskell.org/package/tensorflow-core-ops) [tensorflow-logging](https://hackage.haskell.org/package/tensorflow-logging) [tensorflow-opgen](https://hackage.haskell.org/package/tensorflow-opgen) [tensorflow-ops](https://hackage.haskell.org/package/tensorflow-ops) [tensorflow-proto](https://hackage.haskell.org/package/tensorflow-proto) [tensorflow-records](https://hackage.haskell.org/package/tensorflow-records) [tensorflow-records-conduit](https://hackage.haskell.org/package/tensorflow-records-conduit) [tensorflow-test](https://hackage.haskell.org/package/tensorflow-test) 
[PureScript has this](https://github.com/purescript/purescript/issues/839) and it is pretty neat.
If you are moderator, You can see that this is a serious question. Please approve one or the other 
You can always use a let/where to name your function: let f = M.unionWith 0 (+) in M.fromList [("a", 1)] `f` M.fromList [("a", 3), ("b", 0)]
This is the way to go. The OP's code is a jumbled mess.
You can even give it an operator name: M.singleton "a" 1 +++ M.fromList [("a", 3), ("b", 0)] where (+++) = M.unionWith 0 (+)
Or use the [`&amp; .. $` trick](https://twitter.com/kmett/status/779189745888952320): &gt; import Data.Function &gt; [1,2,3::Int] &amp; zipWith (,) $ [10,20,30::Int] [(1,10),(2,20),(3,30)]
Reddit is a nice place for the community to discuss. ~~The GHC bug tracker has a "feature request" category~~. This feature looks appealing at first as an obvious generalization of the existing notation for variables only, but the actual gain seems very minor. Should nesting be allowed? How does it compare to the currently possible ways to write it? M.unionWith 0 (+) (M.fromList [("a", 1)]) (M.fromList [("a", 3) ("b", 0)]) M.fromList [("a", 1)] `union` M.fromList [("a", 3), ("b", 0)] where union = M.unionWith 0 (+)
Have Haskell code side-by-side with equivalent Ruby/Java/etc. (where possible). This is a trick I've used to give Haskell talks to non-Haskell audiences and it works really well. It's especially nice when you're demoing `data Foo = Bar Int | Baz Bool (Maybe Char)` and the equivalent Java is several slides long. I would focus on stuff like pattern matching and lack of `null` values. Something like case Map.lookup "Foo" someMap of Nothing -&gt; ??? Just theFooFromTheMap -&gt; ??? is fairly easy to understand, especially when juxtaposed with Foo foo = someMap.lookup("foo"); if (null != foo) { ???; } else { ???; } I would not shy away from IO. Haskell has a reputation for impracticality, so don't be afraid to write pageContents &lt;- forConcurrently webPagesToScrape downloadWebpage for pageContents $ \pageContent -&gt; do putStrLn pageContent You're not teaching Haskell, you're showing it off.
The GHC ticket system is no longer used for proposing language extensions. This should be done via the [GHC proposals process](https://github.com/ghc-proposals/ghc-proposals).
Oops. Thanks for the correction.
Somewhat off topic, but a fun thing about your example is that if you set up the context correctly, you can use the `&lt;&gt;` (`mappend`) operator: λ :set -XOverloadedLists λ fmap getSum . getUnionWith $ [("a", 1)] &lt;&gt; [("a", 3), ("b", 0)] fromList [("a",4),("b",0)] The default `Monoid` instance for `Data.Map.Map` is left-biased, rather than relying on the values being monoidal, but we can fix that with a newtype wrapper {-# LANGUAGE TypeFamilies #-} module Data.Monoid.UnionWith where import Data.Map import Data.Monoid import GHC.Exts newtype UnionWith k v = UnionWith { getUnionWith :: Map k v } instance (Ord k, Monoid v) =&gt; Monoid (UnionWith k v) where mempty = UnionWith empty mappend (UnionWith m) (UnionWith m') = UnionWith (unionWith mappend m m') instance Ord k =&gt; IsList (UnionWith k v) where type Item (UnionWith k v) = (k,v) fromList = UnionWith . Data.Map.fromList toList = Data.Map.assocs . getUnionWith 
I attached a fragment of the core to the gist. Looks like *2* gets compiled into a single loop, while *4* ends up as two loops with Int boxing and unboxing between them. EDIT: Actually *2* is a loop with two entry points: a joinpoint jump and a tail call. *Iterator 5* is almost the same, but uses two joinpoint jumps instead. Added to the core. EDIT: Additional observation: *iterator 5* uses cmov, which results in a sweet 8 instruction loop. *Iterator 2* branches. EDIT: In case anyone was wondering why *list* is slower: it's probably because it has the opposite comparison order - first checks if the number is even, then if high was reached in both branches.
diagrams also works with a [IHaskell](https://github.com/gibiansky/IHaskell) notebook, which would give a nice demo.
I guess the lack of heap allocations in *iterator 2*'s inner loop allows LLVM to aggressively shuffle the code, while Int boxing in *4* prohibits it. It seems NCG doesn't see this opportunity.
Largely. There are still some substantial regressions, but lots of things are indeed much faster. The early inlining patch helped a lot, I gather. So did the [patch](https://ghc.haskell.org/trac/ghc/changeset/2b74bd9d8b4c6b20f3e8d9ada12e7db645cc3c19/ghc) that stopped the specializer from generating code with infinite loops and invalid coercions. Reid Barton also did a lot of work plugging space leaks in the simplifier.
What should the fixety of the operator be? Normally, with back quotes, we can declare the fixety of the operator. This is a name thing (not a variable thing). Allowing any expression with free variables in back quotes would not mix well with fixety declarations. (Or what if the thing *is* a free variable...?) One option would be to disallow fixety declarations for things other than single names. This seems ad-hoc. Maybe one should be able to declare the fixety of any operator by means of the top-level function name? 
Oh sweet, I'm going to use this to torment contributors. Thank you!
GHC doesn't rearrange `seq`s unless it has a decent reason to. If it creates a space leak that way, you should probably report a bug. If you really want to control the timing of evaluation tightly, you can use `pseq`. But GHC probably knows more than you do about what order will be best for code generation.
Cool! I suppose you can emulate (interface) subtyping with functor coproducts &amp; injections, Datatypes A La Carte style. And you can override methods and/or call base by writing another object represented by a function which delegates to the base object.
Or just run hlint in CI! Neil has made it really easy to do so! https://github.com/ndmitchell/hlint#installing-and-running-hlint
/u/bitemyapp Greetings! Your comment does not meet our standard of **60** characters per line. Please consider revising it to conform to our [haskell subreddit format standards](http://example.com).
Not exactly what the post was about, but OOP has been possible in Haskell for a long time: https://arxiv.org/pdf/cs/0509027.pdf
Indeed. The use case is for people that want to point out lint problem in comments rather than failing the build. 
Yeah this is why I want this. A lot of people will stop at a red build and shrug, leave the PR as is.
That's awesome, so glad to hear this has helped you out! Please let me know if there's anything you need added or fixed while you're using it. It sounds like we have very similar use cases.
Does every audience member have programming experience? You might summarize Simon PJ's "Haskell is Useless" talklet. If that's all they take away from your talk, at least they have that. Can you work up a small program that's in some area your company works in? Or possibly choose a domain that's familiar to everyone, such as word wrapping in a word processor. From de Moor &amp; Gibbons' "Bridging the Algorithm Gap...". In the paragraph problem, the aim is to lay out a given text as a paragraph in a visually pleasing way. A text is given as a list of words, each of which is a string, that is, a sequence of characters: &gt;type Txt = [Word] &gt;type Word = String A paragraph is a sequence of lines, each of which is a sequence of words: &gt;type Paragraph = [Line] &gt;type Line = [Word] The problem can be speci ed as &gt;par0 :: Txt -&gt; Paragraph &gt;par0 = minWith cost . filter feasible : formats1 
Try running stack test in a terminal. This problem is because the test dependencies have not been installed I think.
I've been running "stack build" and "stack test" all the time, and successfully. The problem seems to be just with Haskero.
The early parts are very deep, but you need to read them pretty carefully. After that the section on traversals covered most of my"real world" use cases. I hope to go back and read it in full though.
I love how little discussion ensued for that feature...then again, I am also sort of terrified of how little discussion ensued for that feature.
Because of the complications with nested uses, fixity, etc., I much prefer the simplicity of just using `let`/`where` to define custom operators as needed.
Having recently read /u/thumphriees ["Notes On Fusion"](https://www.reddit.com/r/haskell/comments/6kfzzn/notes_on_fusion/), and followed on from there to read about stream fusion in ["From Lists to Streams to Nothing at All "](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.104.7401), I thought it was fascinating how join points obviate the need for `Skip` constructors (@ 14:43).
Approved. 😉
Wrong subreddit, /u/nish2575? 
I would imagine that this syntax could get very confusing very quickly. There is a standard syntax for function application that we should use by default except for a certain, already large (compared to other languages) set of standard operators...
For me, this really highlights how terrible of a choice the symbols $ and &amp; were. It's a shame we can't easily change them to something a bit more obvious. How is someone supposed to look at that and know what it's doing without memorizing and "just knowing" what $ and &amp; do? Using triangle pipes at least gives you some potential intuition.
&gt; It's a shame we can't easily change them to something a bit more obvious. Just write a custom prelude with your preferred names? It won't work for things like `runST $ do ...` in which `$`'s magic is important, but it will work just fine for infix expressions. In fact, the tweet I linked to refers to a [trick](https://wiki.haskell.org/Infix_expressions) in which the operators are named `-!` and `!-` instead.
Yea there's really only https://github.com/tensorflow/haskell. I've been poking around and going through some of the basic TF tutorials except in Haskell. I'll be writing more about it soon!
You can set breakpoints in GHCi with the `:break` command. Other than that, avoiding explicit recursion will usually prevent any loops.
Try stack build --test. Maybe in plugin settings you can replace all calls to stack build with stack build --test? 
This may not be appealing for most, but an indispensable tool I recently discovered is the trace function (in Debug.Trace). trace is technically a function with side effects, but it makes it super easy to inspect values in potentially large functions that you've already written. The signature is: trace :: String -&gt; a -&gt; a So you give it a string and a value and it will simply return that value, but it will also print the string that you gave it to stderr. So if you have an expression that you want to print, you can change: fn = expression to fn = let e = expression in trace (show e) e and it does the same thing, but also prints the expression.
I would be quite happy to have a Haskell version of the [`python classify_image.py`](https://www.tensorflow.org/tutorials/image_recognition) tutorial, have you gone through that one yet?
Do you have some examples of your code that's producing infinite loops? Different static analysis tools will find different things. Setting warnings `{-# OPTIONS_GHC -Wall #-}` is a useful first step. This will warn you of many potential problems (unreachable code, incomplete case matches, etc), some of which can cause infinite loops. For example sumOneTo :: Int -&gt; Int sumOneTo n = n + sumOneTo (n - 1) sumOneTo 1 = 1 will loop infinitely. `-Wall` won't report this directly, but will report that the second case is never used: Temp.hs:5:1: warning: [-Woverlapping-patterns] Pattern match is redundant In an equation for ‘sumOneTo’: sumOneTo 1 = ... Since this is your stopping condition, you should be able to infer that this could cause an infinite loop and that you need to reverse your case order. sumOneTo :: Int -&gt; Int sumOneTo 1 = 1 sumOneTo n = n + sumOneTo (n - 1) 
That's the first thing that came to `clang`'s mind too.
That's really sad that Haskell relies on "harmful printf debugging". (edit) From my experience, the debugging process in Haskell is more centered around extracting function and testing them, in GHCi or with quickcheck.
This looks like [graded monads](http://hackage.haskell.org/package/effect-monad). A similar concept is [indexed monads](https://stackoverflow.com/questions/28690448/what-is-indexed-monad).
So, I think this question is actually a very interesting one and gets somewhat at the heart of the difference between OOP/imperative programming vs Functional programming (especially in Haskell). I'll warn you right now that you're not going to get a very direct, "practical", hands-on solution to your question; mostly due to the fact that it's likely coming from having a different mindset. First off, I'm imagining that you're writing your code using: a) somewhat large functions; b) direct recursion; c) fairly liberal use of let, if/then/else, and where. (If I'm wrong, let me know) There are a few key insights that will really help with haskell. 1. Haskell *is* lambda calculus. There is nothing in Haskell that you can't rewrite in lambda calculus and 'compute' down to a final answer on paper. This is amazing and also means that every single function, line of code, etc., can all be reasoned about through a concept of 'reduction' (of which 'substitution' is particularly important) 2. Haskell is pure, lazy, and has referential transparency. This means you have perfect equational reasoning. So if you have a function `a = b c` and a function `b = c d` you can get rid of b and rewrite function a to be `a = c d c` and that's *always* the case. This is why I said substitution is particularly important. 3. Types tell you more than you think they do. Types are glorious. Long live types. It sounds like a cult, but there's a lot of truth behind it. Alrighty, now let's talk about recursion. * Here's a function: `ones = 1 : ones`. Can you tell what it does just by looking at it? If I give you the type `ones :: [Integer]` does that help you out? Let's evaluate that out mentally. 1. ones IS 1 appended to ones. So mentally I think of a list [1 : ones] and substitute that for the ones on the left hand side of the equal sign. My mental model is now vaguely something like `[1 : ones] = 1 : ones` 2. Oh hey, another `ones`! What does that do? Well `ones = 1 : ones` so now we have `[1 : 1 : ones]` 3. Oh hey, another `ones`! (repeat forever) Now, I lied a tiny bit when I said I was going to talk about recursion. `ones` is actually an example of something called corecursion. Well defined recursion will always terminate, well defined corecursion will always be "productive." What I mean by productive is that even though `ones` produces an infinite list, I can stop `ones` after some number of "recursive calls" and get an actual answer. If that flew right over your head, no worries, look up corecursion every couple of weeks for funsies and one day it'll just click like magic. * Here's another function (don't worry, it's "real recursion"): factorial 0 = 1 factorial n = n * factorial (n - 1) Does this function loop forever or does it eventually terminate? Most people would immediately say "it terminates" but the real answer is "it depends." What?! Okay, think in your head what 'factorial is doing'; I'm pretty sure you mentally imagined an integer, which is great, but if your type is `factorial :: Float -&gt; Float` then, well, hopefully you get lucky and you input an integer and then it'll work right. But wait, what *kind* of integer? What happens if you input `-2` into that factorial function? Let's mentally evaluate it and find out! 1. First, substitute `-2` into the function for n. Since -2 is not zero, we do not hit our base case and thus go to the next line. 2. Alrighty, we have `factorial (-2) = (-2) * factorial (-2 - 1)` now. 3. Oh hey! Another factorial. Now we have `(-2) * (-3) * factorial (-3 -1)`. 4. And, as you can *equationally reason*, since you started at a negative number and you always count down, you will never reach a base case and thus you will never terminate. For negative numbers, this is not well defined recursion *or* corecursion; it doesn't do anything productive as I can never stop it after some amount of steps, and it never reaches a base case. Bam, potential infinite loop detected. How do you fix it? Well, the infinite loop occurs when either a) the function is given a non-integer or b) the function is not given a positive number. Or, in other words, it only works for positive integers. How do we fix that? factorial :: Integer -&gt; Integer -- Makes sure we can ONLY use integers factorial n | n &lt; 0 = n * factorial (n + 1) | n == 0 = 1 -- This is our base case regardless of direction. | n &gt; 0 = n * factorial (n - 1) -------- So, to summarize a few basic concepts here: 1. Recursion must take off a slice each time in a way that will always reach the base case. For factorial, we did that by ensuring only integers were used and that we would count by 1 towards zero every time. 2. Corecursion must generate a useful value and *then* "recurse." So if you typed `ones = 1 : ones` into ghci and then ran `ones` you would see [1,1,1,1,1,1,1,1,....] pouring out into the terminal until you typed Ctrl+C. That's because you get a 1 added to the list and *then* it recurses, so you can type something like `take 15 ones` and get a list of 15 1's. * So, if you're writing a recursive function and your terminal just hangs forever, you're missing a base case somewhere and the function isn't doing anything productive. If you're writing a recursive function and it spews output forever, you accidentally wrote a corecursive function. * If your corecursive function just hangs forever, it means you're not doing productive work before you "recurse". Can you see why `ones = ones : 1` is what we call "non-productive"? (hint: expand it out and see when you add things to the list) 3. Haskell is lazy which means that debugging sucks ass. You have no guarantee of when things actually execute, so a stack trace, debugger, stepper, etc., are all somewhat cumbersome to use. Luckily, you can use equational reasoning to "solve" a lot of this stuff in your head. Let me know if that helped! Also, feel free to visit \#haskell on IRC.freenode. --------- Extra credit: What do these two functions have in common? or :: [Bool] -&gt; Bool or [] = False or (x:xs) = x || or xs any :: (a -&gt; Bool) -&gt; [a] -&gt; Bool any _ [] = False any f (x:xs) = f x || any f xs (If you give up, google 'foldr and foldl in haskell' and watch your mind be blown. Most Haskell programmers don't actually use primitive recursion; rather, they use recursive functions such as folds and zips in order to compose things together)
Are your slides up anywhere?
This is supposed to sort a list of numbres by their absolute value: absSort [ ] = [ ] absSort (x:y:[ ]) = if (abs x) &lt; (abs y) then [x,y] else [y,x] absSort (x:xs) = absSort small ++ (x : absSort large) where small = [y | y &lt;- xs, (abs y) &lt;= (abs x)] large = [y | y &lt;- xs, (abs y) &gt; (abs x)] However it does nothing.
Works fine when I load it into GHCi: &gt;&gt;&gt; absSort [3,-1,4,-1,5,-9] [-1,-1,3,4,5,-9] You know that `absSort` doesn't modify its input right? The output is a new list of the same data in sorted order: &gt;&gt;&gt; let xs = [3,-1,4,-1,5,-9] &gt;&gt;&gt; let ys = absSort xs &gt;&gt;&gt; ys [-1,-1,3,4,5,-9] &gt;&gt;&gt; xs [3,-1,4,-1,5,-9] 
How are you determining that it does nothing? Aside from the formatting, it looks correct to me.
(please indent your code by four spaces so that reddit formats it properly)
you may also be interested in `traceShow` traceShow :: Show a =&gt; a -&gt; b -&gt; b and if you want to skip the let binding when using `traceShow`, try: fn = traceShow =&lt;&lt; id $ expression -- same as fn = trace =&lt;&lt; show $ expression
&gt; `runST $ do ...` Frankly, the `$` should never have been required in that syntactic construct. There's long been talk around making `do {...}` and other similar things primary expressions.
"harmful printf debugging"? Do you have a particular link in mind?
Facepalm, I was doing this: f2 = [1,2,3,-1,-2,-3] main = do let a = f2 print a let a = absSort a print a Thank you, now it works.
OK. What's funny in that case is that when using LTO with clang it's been suggested to pass these in LDFLAGS -Wl,-plugin-opt,-function-sections -Wl,-plugin-opt,-data-sections in addition to the usual flags. I don't mean the normal -fdatasection/-ffunction-section in CFLAGS.
As an alternative, if you are using code climate: https://docs.codeclimate.com/v1.0/docs/hlint
I suggest disallowing nested backticks
I haven't gotten quite that far yet. Still doing Iris and MNIST stuff at the moment.
AHHH So here's your infinite loop: let a = absSort a This isn't updating `a` to be the result of running `absSort` on the previous value of `a`. This is defining a new `a` in terms of itself (corecursion). It's the same as: let a = f2 print a let b = absSort b print b 
Or simply `traceShowId expression`
Now I see it, suddenly other 'infinite' problems I was having are solved. Thank you.
Yep! I ran into this issue. After you change targets (I prefer "All Targets"), you must press/hit `enter` on "Validate". Then, the type-errors should go away in your test suite.
I second what /u/gelisam is saying. In fact I am usually using the [`compose-ltr`](https://hackage.haskell.org/package/compose-ltr-0.2.3/docs/Data-Function-Compose-LeftToRight.html) package to that end.
I'd be interested in trying this. Perhaps under a GHC language extension pragma? (What else?)
I don't remember, perhaps I read something interesting, perhaps I maid the title from the considered harmful "considered harmful" pattern. In most language, printf debugging is usually inferior compared to a debugger. In Haskell, that's different, trace can be useful, however its execution order is not guaranteed at all, so the result can be surprising. 
Graded monad is in the right track. It lacks the equivalent combinators for Applicative, Monoid, Alternative and even numeric algebra, the equivalent extension of the Num class. I want to call the attention of the Haskell community to get serious about composability that is IMHO critical. It is more genuinely a functional problem than type safety. It is also more necessary than ever for complex systems.
Thank you, this did the trick! Unbelievable that this simple trick was nowhere to be found on the internet - until now. Thanks again!
Btw, did you figure out a way to set "All Targets" automatically on VS Code launch - on a per project basis?
Nope. I probably should open a PR to set that as the default, but I've been lazy, ha.
Nothing *needs* to encapsulate state, but sometimes it is convenient. For example, shapes in a shape editor like Paint or Visio is convenient to view as having state (current size, position, colour and so on). If you don't like this example you are welcome to encode any other from a beginners' OOP tutorial. Edit: moreover, OOP practitioners sometimes do use objects that don't have any state (see e.g. [flyweight design pattern](https://en.wikipedia.org/wiki/Flyweight_pattern)).
Is there any good arguments not to make them primary expressions? I've always been annoyed by the `$`s that "open blocks of code", appear redundant and gives off a much worse initial impression than Haskell deserves.
Haskell's execution order is often not what an imperative programmer expects, and trace is very useful for revealing it.
Here's a notebook where I got the example code for MNIST working, in case it helps: https://nbviewer.jupyter.org/github/lgastako/public-notebooks/blob/master/TensorFlow%20MNIST%20Working.ipynb
I like this mindset of "showing it off" instead of trying to teach it. I'll probably go out with it
I'll check out Katip. I haven't heard of it before. Right, your suggestion makes sense. The password problem could be solved probably just by changing the `ToJSON` instance to not serialize the password. Of course, it's possible I suppose for the ToJSON instance to need all of that information for whatever reason, or something, so it might also make sense for the API to expose a `ToSecureLoggable` type class, perhaps. Thanks for the feedback! Please let me know if you have any other questions or comments!
You don't mean literal edit distance on the type names, do you? eg. where `String` to `Text` is 6 but `String` to `Int` is 5?
Ah, no, I found a slide in the deck that /u/spaceloop posted that clarifies: https://i.imgur.com/CtiaikL.png
I believe most will have programming experience, This event is destined for undergraduate students and IT workers. The company I work for has a some DSLs that we use daily, hence my interest in showing how one can write a parser for a given DSL without trouble. It is also my favorite part of Haskell so far. This example you showed (paragraph) seems really good to show some clean and concise code.
This is going to be really useful! 
Even printf debugging nseems to best way to debug in Haskell, there is something wrong in having to modify something to check how it works. A bit like your GP having to Dona surgery to realize than everything is fine. The two main drawback is you have the risk of introducing some bugs by removing the printfs and when debugging, you actually spend your time compiling (which a waste of time and battery on my laptop).
&gt; "a visually pleasing way" Well, at least it has better specs than I'm used to.
Thanks ;)
&gt; memory-safe Might not be, if the transaction calls free() [or equivalent] and has to be retried, resulting in a double-free.
I'm having trouble identifying with these sentiments. Really, what is so terrible about them? They work fine. Concise, to match their fundamental and common usage. Truthfully I have never heard of anyone having trouble with them before, all these years. What happened all of a sudden? &amp;, which I happen to have invented, was widely applauded at the time. It means "and then...". How much more mnemonic can you get? What was my mistake? It's true that $ isn't mnemonic relative to other uses outside of Haskell. But how hard is it to remember that one symbol, which is so well established, for so many years, and so ubiquitous? It means what it means, just like "rose" means what it means, even though it would smell as sweet by any other name. Without $ it wouldn't sound like Haskell.
I haven't looked closely at `rust-stm` in particular but it should be possible to rule that kind of thing out by restricting the transaction body to be `FnMut` or `Fn` (not sure offhand if `FnMut` is sufficient but I think it should be). Do you have a particular example/scenario in mind?
I do not remember any good ones. I think they went something like "we should also do if and lambda, but ambiguity; guess we should forget about this discussion inexplicably now"
You are probably talking about limits and colimits in a suitable category. They generalize or restrict sets of objects, as strictly as possible, taking into account of their interactions, which seems to fit your description. This imposes the definition of some kind of category, whose objects are types and/or functions and where these limits and colimits have the desired semantics. I would say that the problem is here, and is therefore strongly connected to type systems, whose "force" may be variable. I have no specific proposition, but this is a problem that troubles me too.
First off, I'd like to thank you for inventing `&amp;`, it's pretty great. That being said, for me personally, the difficulty comes from the difference between a linguistic mnemonic and a symbolic intuition. One of the reasons why 1 character types are often used is because it's more informative to look at the shape of the type signature than try to grok some "concrete" meaning from an inherently abstract concept. In that way, it seems to me that the more abstract something is, the more inherently symbolic in nature it should be; further, symmetries between notation help immensely. \+, x, and - have a nice similarity of shape, especially as you extend your symbols with further constructs such as ⨁ and ⨂. It's much clearer to me that ⨁ has something to do with a binary "addition-like" operation than saying `mappend`. Likewise, I would reasonably be able to guess that ⨞ was related to a "piping-like" operation (ie composition of some sort). The period makes for a natural function composition because it's supposed to be out of the way as well as similar to the mathematical composition operator; actually using the c-dot unicode equivalent is much better as it comes straight from math. But $... What the hell does that do? There's nothing in a symbolic language, nothing in the makeup and construction of that symbol which gives *any* hint to its purpose. After familiarizing oneself with a host of symbolic operators that have a symbolically-derived "intuitive" meaning, $ sticks out like a sore thumb. Even worse is to combine $ with &amp;; now there are two operators with absolutely zero indication that they are related to each other; both convey very abstract and symbolically-related concepts, and only one has any mnemonic at all (and it's purely an English linguistic one). The fact that &amp; is supposed to be a mnemonic for "and then..." (which I've never heard of before), to me, well... it entirely misses the point of creating symbolic notation. --- There's nothing *wrong* with $ and &amp;; they just seem to break a ton of 'unwritten rules' that separate well designed symbolic notation from perl-golfing syntax. `|&gt;` and `&lt;|`, on the other hand, are much clearer symbols in comparison; they are clearly related, have symbolic roots in other similar symbols (`|-&gt;`, unix-pipe, etc) that all have conceptually-similar meanings. It's unfortunate that keyboard and ascii limitations (not to mention the horrors of "properly" handling all of unicode) prevent languages from really taking advantage of symbolic notation.
For a different take in structured logging, take a look at `di`: https://hackage.haskell.org/package/di-0.2/docs/Di.html I've only added a simple backend that prints to stderr or any other file handle in a very stringy fashion, I'll be adding more structured out-of-the-box backends soon, but one could easily roll their own with what the library exports (which after all is one of the goals of the library).
No. And my rust experience is extremely limited; just the tutorial and some very small experiments.
Yea that's probably good enough. Actually, my biggest concern with this idea would be syntax highlighting. If this is meant to improve readability, it needs a readable way to highlight it, and it would be tricky to find a good way to both highlight the "operator's" syntax *and* give it a distinct coloring. Easiest way would probably be to just force the whole expression inside the back ticks to be one color (like it is now), since it will usually be short enough to be readable without coloring.
It would have worked with a &lt;- pure (absSort a) instead.
I've attempted to use the tensorflow bindings on the Windows Subsystem for Linux. There is currently a problem with how WSL interacts with GHC8, though, and the bindings library uses type applications, so I've stalled for the time being. I'm ridiculously excited about what this will allow us in the (near) future. 
How does Hoogle deal with type variables, then? In other words, how do it treat `a -&gt; a` and `b -&gt; b` as if they are the same?
I think it's more of the matter of *choosing the right tool for the right job*. If you can do it in python, then go for it. The exception to this is if there is anything to **gain** from using one language over the other, such as furthering your own knowledge 
Haskell is a strong choice for learning new things. Hit up Reddit and irc for help, people are very friendly. Written tutorials that are freely available are generally either pretty bad, or outdated. I recommend Real World Haskell - it suffers from being dated, but is a very strong, sensible guide through basic principles from a practical programmers perspective. Try to save deep reading about category theory for later. It's not directly relevant to your code, and although it's super fascinating, it's likely not going to help you understand anything for a long while. Imho : Coming from a JS / Python background, I bounced off of clojure at about a million miles an hour, but got good and stuck in with Haskell. Haskell seems far easier to learn and work with.
Haskell is a great language to learn because it is so different. It'll exercise your problem solving skills on things that you could probably write in your sleep in Python. I don't have much cause to use Haskell but recently I had to write a small CLI tool that required a parser, and I remembered Parsec so I wrote the tool in Haskell. It worked well, but, man, I felt like I was thinking _very hard_ for a problem that I could have easily overcome by throwing some impure state at it. But hey, I got to use some monads I hadn't before. 
I'm so glad you asked! They are at http://vaibhavsagar.com/presentations/lazy-functional-state-threads/
`Object` is a generalisation of monomorphic mealy machines. In the library implementation, there's something compatible with `Wire`: http://hackage.haskell.org/package/objective-1.1.1/docs/Data-Functor-Request.html 
&gt; no one in the class would even be able to write Hello World. Is that relevant? It would be a problem if the goal was to persuade people to use Haskell. Otherwise it could just be a simple intro.
Yes, with a great catch. &gt; easily be done with Python If you never learn Haskell, *everything* will be easier in Python.
Every language is *full* of symbols that mean nothing outside of the language. You even cite 'unix-pipe' as if that were somehow clear and indicative of what it does.
My favorite piece of advice was to treat the talk like a sales pitch. You can't teach them the language in 30 minutes, the best you can do is maybe convince them to learn more about it later. I crashed and burned in a lunch and learn talk at my company by trying to teach Haskell to a bunch of peers who weren't really interested in new languages. You can show code and cool examples, but try to focus on some things they can take to their own language, their own world. This is the most likely way you can give them a good impression of Haskell IMO.
I would say yes, but with the warning that if you learn Haskell well enough you may grow to find Python inadequate like I did. &gt; I considered Clojure to meet these goals If Python is your main language now, you can probably learn more from Haskell's static types that you would with Closure's dynamic types (Python is also dymanically/uni typed).
The current common solution seems to be typeclass constraints. Let's show the types first. m :: (e1 :&lt; effs, e2 :&lt; effs) =&gt; A effs u and f :: (e2 :&lt; effs', e3 :&lt; effs') =&gt; u -&gt; A effs' v You could combine them with a function bind :: A es p -&gt; (m -&gt; A es q) -&gt; A es q And you get m `bind` f :: (e1 :&lt; effs, e2 :&lt; effs, e3 :&lt; effs) -&gt; A effs v A bit of type families would easily allow you to write the less verbose. m `bind` f :: ([e1, e2, e3] :&lt;&lt; effs) -&gt; A effs v Now, if you implement `A` as a 'effect tracking whatever thingy', `effs` as a 'set of effects', and `:&lt;` the 'is member of' operator, you would get this composition. The code of `m` doesn't need to know about `e3` to be able to have `effs` containing `e3`. Then your `runX` functions would handle concrete instantiations of `effs`. This is the approach in use by at least [freer](https://hackage.haskell.org/package/freer) and [mtl](https://hackage.haskell.org/package/mtl) (sort of). It's also more or less how you get optics to downgrade themselves in [lens](https://hackage.haskell.org/package/lens). And even the numerical hierarchy is, in a way, done in this manner. In `1 + 2.5`, `+` isn't an operator with type that magically allows it to be used as `Int -&gt; Double -&gt; Double`. It's possible because `1` *knows*, by itself, how to be a `Double`. And it imposes its implementation burden is on `Double`, which needs to have `instance Num Double`, which contains `fromInteger :: Integer -&gt; Double`.
It's not like `IO` is particularly nice or anything. It's the other way around: *purity* is pretty nice.
Not every problem that can be solved with software can be solved with Haskell, but at least every problem that could be solved with Python/Ruby/Perl/Clojure can be solved with Haskell. Also, it will make you think about programs a little differently, and probably improve your Python in time, even if you do use Python more in the long run.
It is never wasteful to learn.
thanks for sharing! what's with the memory thing at the end? Does memory management not work properly in IHaskell?
What about `strace` (or the BSD equivalent if that's different)?
I don't think Python is unityped. Don't its runtime type checks ("incompatible types") count?
Oh, I never dug into that enough to figure out what was going on, but as it suggests, running that notebook used 4g, and manually invoking garbage collection did nothing to reduce it's memory usage.
And if one has no particular job, I'd be surprised if one found they would rather pass the time bikeshedding in python before Haskell. 
Well, extending the graded approach to the whole hierarchy of type classes seems pretty straightforward. I'm sure the author will welcome contributions in that direction. &gt; I want to call the attention of the Haskell community to get serious about composability that is IMHO critical. It is more genuinely a functional problem than type safety. It is also more necessary than ever for complex systems. What evidence is there that this is such an important problem? Don't people get a lot of work done with simple monads already? &gt; Suppose that you have two components. Imagine that they are made by different people or even different companies. then either the two have exactly the same types and that includes the same kind of states and effects to combine them if you lift the state and effects to the type level. Otherwise you can not combine them with binary operators How do your type signatures address this hypothetical problem? What are some concrete instances of this problem?
I would repeat the tasks you are most familiar with in Haskell, don't see it as wasteful. This will allow you to dive deeper faster. For example, using the Haskell webdriver library might allow you to focus on the Haskell as the webdriver concepts might already be internalized. Then you can expand into other uses of Haskell from there.
&gt; I considered Clojure to meet these goals, but I have to admit it's syntax can get a little ugly at times. I encourage you to overlook this. If you find the semantics of Clojure interesting (perhaps you find the 'code is data' philosophy really cool), then that will matter a lot more than syntax in the long run. That said, since you haven't learned a statically typed language yet you would get a ton from Haskell. I learned Haskell after Python (well, with Go in the middle) and it helped me mature a lot as a programmer.
Sorry, run time checks don't count :). At compile time the type of every value in Python is `Object`. See: https://existentialtype.wordpress.com/2011/03/19/dynamic-languages-are-static-languages/ 
Well it does look like a pipe and does something similar to what a pipe does in real life.
"things that could be easily done" and "expand my mind" are basically mutually exclusive. I've never met a thing that was easy (in the sense that I already knew how to do it) that was also mind-expanding. Clearly you have that sense as well. Here are some things you might learn from Haskell, coming from Python: * Python's simplicity is actually a facade: Python *hides* complexity but it does not *remove* complexity. * Static type systems can be surprisingly powerful tools for building and understanding programs. * Using very simple constructions (simple functions and simple data types) can get you much further than you may have imagined. * OOP really isn't the only way to write a program, and in fact it may well be a poor way. * It is possible to actually sleep at night without writing equal amounts of app code and tests.
why not? 
Learning a new language is rarely a bad idea. The only big exception is that if you don't know many languages yet, you should likely avoid learning lousy ones. I wouldn't recommend making Perl, BASIC, or Emacs Lisp one of your first languages, or even Pascal or Fortran 77.
Well, what do you want to get out of it? &gt; The goal of learning another language is to get used to different concepts/expand my mind as far as programming goes Python vs Haskell: - Dynamically typed vs statically typed - object oriented vs functional - interpreted vs compiled - single threaded vs concurrent - strictly evaluated vs lazily evaluated In terms of different languages, you can't get all that much different. I was in a pretty similar boat as you way back, and my choices were essentially Clojure, Scala, F#, and Haskell. Clojure is dynamically typed, which was more similar to the Ruby I was familiar with. Scala and F# both have significant ecosystem lock-in and complexity due to the hybrid OOP and FP models, and Haskell's type system is more advanced than Scala or F#'s anyway. I guessed that Haskell had the most to teach me, and I've learned a *ridiculous* amount since. 
Nice idea, although it might be simpler to require the backticked expression to be parenthesised, like so: `(M.unionWith 0 (+))` This also allows for nesting, and I suspect will be a simpler change to make to the parser. 
Exactly. The ASCII symbol is even called "pipe" as well; it's not like the Unix name came from nothing like $ did. Edit: I misspoke with this comment. I forgot the pipe name came from Unix, not the other way around. And, on further reflection, the Unix pipe symbol does seem to be arbitrary in nature as well. However, at least it's somewhat "standard" just did to ubiquity now, and appears in quite a few places with some variation of symbol and concepts but still with some overall similarity. I should've mentioned that the Unix pipe has a symbolic mnemonic rather than a linguistic one (ie, it makes a reference to the shape of a pipe and the function of a pipe, not to any English word or phrase), which is what I was trying to get at with my original comment, I just did so poorly.
Woah, cool! TIL!
You'd be surprised at how many people give up on a language because they can't do the trivial "output text." It makes the language seem like a useless toy for them; hello world should definitely be on a slide somewhere, if for no other reason than "look, we can do the stupid toy program without a PhD, see?"
Learn Haskell and you might see yourself doing things that you might have never thought yourself of being capable to do. Make a database, build a compiler, or write a language. You can definitely do web development with Haskell, but if you're evaluating the language only from the perspective of building tiny CRUD apps that doesn't do much computation (data transformation, decision trees, arithmetic etc.) you might not see the benefits the language can bring you. The thing to understand about Haskell (and any typed FP lang for that matter) is Equational Reasoning. We're not "programming" in the conventional sense - do something, store the results somewhere, do something else etc. Instead we're simply writing a bunch of function definitions. That's pretty much what Functional Programing is. Imagine Python, but the only thing you're allowed to write are functions that take parameters (including other functions), applies some other functions or mathematical operations on them, and return the result. That's it. A bunch of function definitions, something like the high-school algebra/trigonometry cheat sheet. You can assign values to bindings, so `a = 10` is fine, but you can't mutate it after defining it. So `a = a + 10` is not fine. Why is that? Because we're writing equations, and while we can assign values to variables in an equation, you don't change it after it is assigned. Thinking about FP as a set of mathematical equations than as a variation of 'programming' will give you the right kind of intuition about its constraints. Now how're you supposed to do stuff without any mutation? How would we write a loop for example? A loop is generally written using an iterator which we mutate on each iteration. But in Functional Programming all you have is just function definitions and their application. So we don't really have a `loop` in FP. Because a loop is a sequential set of instructions. "Initialize an iterator, do something, increment iterator, if iterator is still less than X, continue, otherwise break out of the loop". These are as imperative as it gets, and there is no semantic equivalent for this when you're writing a bunch of equations. We don't express anything as a set of "do this; then do that" in an equation. An equation is just a definition that takes a bunch of variables, and defines its result as an application of a bunch of functions. But how are we supposed to "express" computation that involve repetitive application of functions? Recursion! power x 0 = 1 power x y = x * power x (y-1) Here instead of writing down a set of imperative instructions (do this; then do that), we just expressed the power function as two equations (this is the whole FP describes the "what", while imperative describes the "how" of things argument). And we used recursion to express repetitive computation. This is why you see a lot of recursion in purely functional programming. So Functional Programming is based on "describing" computations as a set of mathematical functions. The core primitive is function application, expansion, and substitution. This is a fundamentally different model of computation from imperative programming that we're used to. But isn't imperative programming more flexible? Is it possible to express all the possible computations using just equations? Any practiced functional programmer will tell you that you can, but there is the Church-Turing thesis that proved that any computation that can be expressed in an imperative "do this; then do that" manner can also be expressed in a functional "fun a takes these parameters and is an application of these other functions" way. Also vice versa. I've been talking about just computations so far. What about side-effects? Like talking to a network, or writing to a file, or getting user input? Side-effects fit naturally into the imperative model - it is just a set of sequential instructions and we can easily instruct it to do any operation we need. But how do you express side-effects inside equations? Pure functions can only express computations, but not actions. That is where Monads come into play. It is a mathematical concept that elegantly allows us to express side-effects inside equations. You don't need to understand what it really is to use it. You don't go looking into how a loop is implemented in assembly when writing imperative code; you just use it. You can start there with FP as well, and as your curiosity takes you, you can dig deeper.
Or you could use ``` join traceShow :: Show a =&gt; a -&gt; a ```
That's good to know
There is an opportunity cost to learning, though.
_moral superiority level up_
Unfortunately this need ad hoc glue code in the form of type classes. It moves the glue code to the type classes. And that bind is not the bind of the standard monad, lIt seems that the graded monad does it. That means that we need to standardise new definitions of the common Haskell combinators or at least to make rebindable syntax for (&gt;&gt;=) &lt;&gt; etc as transparent as possible. I have no experience on that.
&gt; What evidence is there that this is such an important problem? Don't people get a lot of work done with simple monads already? I tried to make evident the importance of this problem in the post. It is a very important problem for real world composability IMHO. independently of the wording used to express it, It is a common complaint that Haskell libraries are hard to inter-operate, while in a functional language, if any, the advantage should be to make easy the composition of computations. &gt; How do your type signatures address this hypothetical problem? What are some concrete instances of this problem? Because if we can combine component A and B with different effects and states, then we can combine any number of haskell primitives that use them with mathematical guarantees and no glue code/rewriting/recompilation etc. Basically my type signatures are intended as an upgrade of binary the Haskell operators so that effects and states can be aggregated, but the laws stay unchanged. This allows to combine mathematically any two Haskell computations that return the right types, no matter the effects and states that they use. 
`stack test` is an alias for `stack build --test`.
This is a first modest step towards [supporting alt source in Stackage](https://twitter.com/snoyberg/status/857219130570756096) which would allow pulling releases from Git repos directly!
A python programmer could just as well say »Haskell is untyped, sorry, static checks don’t count, at runtime the type of every value is an int« and defend this similarly.
I don't know if graded monads need a runner for each effect like in the case of monad transformers and free monads. In this case they can not qualify for this strong notion of composability.
Ghc has a variant x `pseq` y = x `seq` lazy y Which does have ordering guarantees as long as y isn't inlined. The lazy is the difference between `x and y will be evaluated to whnf before returning` and `At least x will be evaluated to whnf before returning`.
Very instesting. I would like to hear more elaboration in this line of thinking. A solid base on CT should be desired. To define the operations at the category level rather than at the type level should be necessary since a category can include families of types. Maybe the Monoidal Category that seems to allow the definition of a monoid at the category level is an example. The haskell interpretation of monoids is not general enough.
No he could not because that makes no sense.
&gt; And that bind is not the bind of the standard monad The type I gave does not have this problem. bind :: A es p -&gt; (m -&gt; A es q) -&gt; A es q could easily be `(&gt;&gt;=)` for an `instance Monad (A es)` , or even `instance (ValidEffects es) =&gt; Monad (A es)`. Indeed [freer](http://hackage.haskell.org/package/freer-0.2.4.1/docs/Control-Monad-Freer.html#t:Eff) provides such an instance. &gt; need ad hoc glue code in the form of type classes I wasn't able to see what's so bad about glue code. Could you elaborate? --- It is indeed possible to do effect tracking without glue. See `Eff` in PureScript. However it's just purely tags, which means it's pretty powerless and I don't believe it has much benefit over plain `IO` actions
It's useful to distinguish between "types" (compile-time information) and "tags" (the equivalent run-time information). Some languages have this distinction: Ada actually calls its objects "tagged records", and when people invent their own object-oriented wheel in C they commonly use what's called "tagged unions" – they're explicitly run-time tagging their data. Haskell also supports tags: the regular sum type, where you provide alternative constructors for a type, is a tagged type. It's a shame modern popular languages are so bad at pointing this difference out.
oh ok thx. I'm pretty new to tensorflow and got overwhelmed with the number of packages xD 
&gt; I would say yes, but with the warning that if you learn Haskell well enough you may grow to find Python inadequate like I did. Strongly agree. I used to start up the Python interpreter for simple calculations/scripting, and that habit is still ingrained in my muscle memory. Every single time I come to a point where I ask myself, "Why didn't I just start GHCi instead? That would have been so much easier after all."
 bind :: A es p -&gt; (m -&gt; A es q) -&gt; A es q But to make composability not restricted by effects I need: bind :: A es1 p -&gt; (m -&gt; A es2 q) -&gt; A (es1 &lt;&gt; es2) q Where es1 and es2 are different, since we want composability across effects and states. Is that possible? If type class constraints are used to make this possible, the bind operation would need such class constraints added, so it is not the standard bind... &gt; I wasn't able to see what's so bad about glue code. Could you elaborate? Glue code is ad-hoc code that may destroy mathematical properties or may need constant maintenance in the form of additions here and there. The whole concept of making things interoperable by adding seam code here and there whenever a new piece of code is added is no better than Object Orientation. Funcional programming has the potential of making composability seamless and mathematical. If it is possible, do it. Any real world programmer would die for it.
That's fair. 
&gt; This allows to combine mathematically any two Haskell computations that return the right types, no matter the effects and states that they use. How do you know that you haven't just lifted that problem to the type level? How would you expect `+`, `effs` and `effs'` to be able to interoperate without knowing each other? If you think you can compose `A e1` and `A e2`, getting an `A (e1 + e2)`, but ultimately not able to relate these together, then it doesn't count as composition.
here `+` is a kind of monoid between effects at the type level. As /u/Syrak said Graded monads use this idea. The implementation should work effectively with any combination of effects so that it perform what the type signature suggest.
Dynamic configurations don't allow building with your configuration with stack or cabal to do proper dependency management. When something goes wrong it is harder to find the cause because the dynamic recompilation hides all the compilation work. It adds weird errors like https://github.com/yi-editor/yi/issues/885 (which in turn feeds my second point).
Haskell will be easier in a few years. I hope that there would be a process of convergence: For one side, haskellers would abandon bad practices which complicates the usage of the language and the ecosystem. (I will not enumerate them. Time will put each thing in its place). For the other side, programmers in other languages are adopting techniques and concepts of functional programming and more specifically Haskell, that add real value to their languages. The convergence in a form of simplified Haskell, but more powerful in some practical aspects is inevitable. It is not that some things like maps as arguments would be simplified. They can't, but that pattern would be common sense for beginners in the near future.
I'm not sure I agree with this. If the character was horizontal, I might be inclined to agree, but since it is vertical it is more like a separator than anything to me.
&gt; I've always been annoyed by the $s that "open blocks of code" I agree. There is some discussion about it here: https://ghc.haskell.org/trac/ghc/ticket/10843 I wish that ticket was more active or already accepted. 
&gt; It is possible to actually sleep at night without writing equal amounts of app code and tests. In my Haskell development, I'm still writing a lot of tests. The tests are maybe 1.5-2x in lines over my code. (I can imagine it being 4-5x in loosely typed languages though since you have to check against NULL and stuff like that.)
I like your suggestion. Although what if we make the parens optional for most use cases, and require them only for nesting? `union2` `M.unionWith 0 (+)` `(M.unionWith 0 `add`)` I suspect this can get rid of the ambiguity and the extra syntax at the same time.
You may find [this](https://ncatlab.org/nlab/show/relation+between+type+theory+and+category+theory) interesting. But it is a open field in general.
[`pseq`](https://hackage.haskell.org/package/parallel-3.2.1.1/docs/Control-Parallel.html) is linked from the Stack Overflow thread. However, [the docs](https://hackage.haskell.org/package/parallel-3.2.1.1/docs/src/Control.Parallel.html#pseq) do not give that definition, nor do they explain what `lazy` is. Could you provide more details?
Firstly: yes, this patch does technically make it much easier to modify Stack in the future to recognize alternative package sources (such as HTTP(S) URLs and Git repos) available in Stackage snapshots. However, such functionality has _not_ been added to Stack now, Stackage does not support such alternative sources, and there are no current plans to change that. I've personally been weakly opposed to making such a change for a while. I wish I could find the thread where I discussed the tradeoffs of this, but 5 minutes of searching didn't reveal it unfortunately. But basically, the tradeoffs I see in this discussion are: Pros (in favor of alternative sources): * It's an easier release system (many people want to just `git tag` and be done with it). Uploading to Hackage is an extra step. * For people in favor of enforcing PVP upper bounds on Hackage: it gives people not wanting PVP bounds on their packages somewhere else to upload their packages to. * For people who don't want PVP bounds on their packages, it allows them to bypass the Hackage Trustee review process and cabal file revision. Cons (against alternative sources): * There's no longer a single definition of `foo-1.2.3`. It could be what's on Hackage, but it could also be what `lts-9.32` includes from a random Git commit (depending on how we implemented things). In practice, I'd hope this never happens because people would synchronize their Git tags with Hackage releases, but it opens the possibility. * It makes it harder to find Haskell packages. Neither Hackage nor even Stackage could provide a single listing of all possible packages available. * It complicates the interop story with cabal significantly. When designing both Stackage and Stack, there was a lot of thought and care put into making things as compatible as possible. A big part of that was agreeing on using Hackage as the One True Source for package information.
Great work! Will this, by any chance, help towards fixing this issue where "stack test --coverage" randomly refuses to work (due to lack of tix files) unless you do "stack clean" first?
It looks like the lower-case letter L. Pipes are cylindrical objects...
One thing which might be missing is the possibility to have a file (or multiple files) common to all groups. Right now, I would need to duplicate the same file in all groups.
Can you touch type? If so, perhaps you recall a time just before you could comfortably touch type, a time when two or three fingers were sufficient to be accurate and fast enough. Perhaps you recall the discipline needed to get the hang of touch typing even though it was slowing you down. Then suddenly, you were free. You could type without looking! You are near that phase transition now. Persist learning Haskell. You may continue using Python, but your code will start to look different. How's that for a type system analogy? :) 
It is just a line. That is the only meaning it has naturally. Just a line. The $ is just a line with an S, originally it was a column with ribbon. You can always do the same with $, call it "Spanish pipe", or "Send through the pipe". Just ignore the "Dollar" definition. Every symbol is just a symbol, you are free to add any connotation :) &lt;- like this smile (2 dots and a curve) that can means happiness or a trilobite
Wow, this is really cool! So backticks are kind of redundant? Neat! EDIT: Oh, I just realised this depends critically on the relative precedence of `&amp;` and `$`. Not so nice! 
&gt; [`&amp;`] means "and then...". How much more mnemonic can you get? Are you claiming `&amp;` is *more* mnemonic than `|&gt;`? &gt; "rose" means what it means @}-,-`-
I fear we're hitting a level 3 on [Wadler's Scale](https://wiki.haskell.org/Wadler's_Law).
Haskell is never wasteful to learn. Even if you don't plan on programming in it on a regular basis, experimenting with its concepts will make you a better programmer. You will learn to distinguish pure computations from effetcs, to emphasis design over instructions and the importance of types, even in untyped languages like Python, because even in those language, your application domain **is** typed. Indeed, depending on your background, Haskell can be difficult to learn. There are many things you can learn in Haskell (monads, functors, lenses, etc) but for the main part you can, and certainly should, just ignore them first. Concentrate on simple functions and values. The real challenging part is not what you will have to learn but what you'll have to **un**learn : Haskell is pure and non-strict. That's a huge difference with almost all other programming languages.
Sure. You're implementing it not me. However I would you suggest you start with requiring the brackets first. Without them I imagine there's more "gotchas" in the parser. I actually think it's looks nicer with the brackets anyway, but you're right in that you could avoid forcing that convention if you really wanted too.
That seems ... surprising. Surely anyone willing to put in the effort to raise a PR would at least look at why the build was failing?
Im feeling a bit lazy, but is there an example of saving and restoring them?
Not that I'm aware of, but anything is possible. It would be wonderful if you could give master a test.
If you check GHC.Conc.pseq you find [this](http://hackage.haskell.org/package/base-4.9.1.0/docs/src/GHC.Conc.Sync.html#pseq) definition. Lazy is explained in [GHC.Magic](https://hackage.haskell.org/package/ghc-prim-0.5.0.0/docs/GHC-Magic.html#v:lazy) although the implementation is, well, magic.
And I agree with you. I was not advocating stow over confetti, just pointing out the difference. Confetti is awesome!
Python isn't *untyped* it is *unityped*
The name of the symbol came from its use in Unix, not the other way around. In fact, let's petition the Unicode Consortium to change the name of $ to APPLY. Who ever uses that old currency anymore, anyway?
Thanks!
What is the problem with mtl style typeclasses? For every effect you define an interface class MonadFoo m where kerfuffle :: m Int kabonk :: Text -&gt; m () You can then combine interfaces: packageOneFunc :: (MonadReader r m, HasFuffleCount r, MonadFoo m) =&gt; m Int packageTwoFunc :: (MonadReader r m, HasNodeConfig r, MonadBar m) :: Int -&gt; m Hash packageOneFunc &gt;&gt;= packageTwoFunc :: (MonadReader r m, HasNodeConfig r, HasFuffleCount r, MonadFoo m, MonadBar m) =&gt; m Hash Of course we still need a type that fulfills all those constraints but monad transformers do help there. If you want more fine grained effect types you could try indexed monads. They work basically the same, you just need rebindable syntax and some more type variables. 
Seconded, this looks much better than what we have currently.
Thanks to Alexey Zabelin for starting the [Call for participation](https://haskellweekly.news/issues/63.html#call-for-participation) section! Check it out if you're looking for a way to contribute to the Haskell ecosystem. 
I used to work as a Python developer. I have lots of love for the snake, things like `requests` are amazing, BUT some times I ended up solving a problem with Haskell to translate it to Python. We had a game where I would beat people writing faster (dev time, not exec which is obvious) and shorter code that would work better. I have written crawlers in Perl, Java, Python and Haskell. I prefered the Haskell solution to the Python one. BUT it will take you time, it might depend on how much time you have for it, how clever you are and I must say that in 6 7 years the amount of tutorials has grown in number and quality. Go for it! Clojure could come latter ;)
I wrote about [scraping websites with Haskell](http://taylor.fausak.me/2015/05/21/scraping-websites-with-haskell/) a while ago. That post is relatively low-level though. I think [wreq](https://www.stackage.org/lts-8.22/package/wreq-0.5.0.1) is the way to go in terms of HTTP clients for scraping. 
Yes, I have read your post.... but wanted to get current status !!
&gt; Python is my main language. (...) The goal of learning another language is to get used to different concepts/expand my mind as far as programming goes. &gt; Haskell's steeper learning curve makes me wonder if it could take quite a while. I was in your situation several months ago. I've started learning Haskell, and it was all fine - until one point when nothing was fine ;) I felt completely lost with too much new stuff suddenly appearing. I decided this was nice short excursion, but I'll stay (only) with Python. &amp;nbsp; Until one thread in /r/Python where OP asked what language should he learn after Python, and there were several people mentioning F#. I was quite skeptical about that recommendation, but decided I could maybe give FP another try. I've been learning F# last couple of months and I'm pleasantly surprised so far. For me it was much easier transition from Python to F#, than to Haskell. &amp;nbsp; The moral of the story? If you find out that Haskell might not be your cup of tea, maybe some other FP language will better suit you.
I'd like to note that there seems to be some OSS work supported by AlasConnect. One particular example that's benefited me is their Tisch tutorial: https://github.com/alasconnect/ac_haskell-example/blob/master/src/Tisch/Main.lhs I am very far from being in the target audience for this job opening, though. :)
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [alasconnect/ac_haskell-example/.../**Main.lhs** (master → 7906681)](https://github.com/alasconnect/ac_haskell-example/blob/7906681c48e2107c3043c27cc962c8120821e9db/src/Tisch/Main.lhs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dk60jso.)^.
No, you have to change the definition a little to be meaningful for Haskell types represented as ASTs. So edit operations are not 'insert char' or 'delete char', but more like 'wrap type var'. Iow, type tree operation, not character string ones.
Would wreq also be suitable for use in a wrapper library web based (https) API?
Thanks for thinking through this so well. While the politics involved make it tempting to make brash decisions, this is exactly the kind of clear-headed decision-making that the Haskell community needs.
I have done a lot of web scraping with Haskell recently. I've used [scalpel](https://hackage.haskell.org/package/scalpel) and find it to be very convenient for standard web pages. I haven't gotten into more complex scraping involving form POSTs but that would be easy to add. Full-blown JavaScript-aware scraping is something I have not entertained yet and I'm sure is much harder. For more heavy-duty usage, I recently released a rather crude scraping "engine" which helps you scrape thousands of pages using your own set of rules. For anonymity, you can fire up a bunch of tor proxies and tell the engine to run all its web requests through them (concurrently). It also supports things like caching, throttling, and User-Agent spoofing. https://github.com/grafted-in/web-scraping-engine
Hi, learn Haskell. You will get to a new mode of working where not-crashing is the default. You will be able to upgrade to newer versions of libraries and the language at ease without your functionality breaking. You will be able to learn and use other languages much faster because many things they do are just specialisations of approaches you will have encountered in Haskell.
I've also been using wreq for this.
I'm using scalpel-core and it works great for simple/regular HTML. I prefer using wreq to perform the http requests. I'm very interested in your package for the sake of anonymity. Care to expand how it works? I looked at your code, but my limited knowledge of Tor and being somewhat new to Haskell makes it hard for me to wrap my head around this.
Do you use `QuickCheck`? IIRC there was a home-work "challenge" in coursera Scala course [1] where you had to write ScalaCheck to catch errors in invalid heap implementation. It's not very *hard* to catch all problems with single property, by checking against the model, a bit like in one checks `HashMap` against `Map`, and `Expr` contains all interesting operations [2] [1]: https://github.com/lu-ko/scala-coursera-reactive/blob/8ff64ddfb494f3a455a268c04f2f4546550e8c56/progfun-quickcheck/README.md#the-assignment [2]: https://github.com/tibbe/unordered-containers/pull/157/files
&gt; I wish I could find the thread where I discussed the tradeoffs of this, but 5 minutes of searching didn't reveal it unfortunately. I think it was [this here](http://www.snoyman.com/blog/2017/01/stackage-design-choices) where you point out various problems and offer the idea of dropping the Hackage-requirement: &gt; I've had some private discussions around this, and thought I should share the idea here. Right now, Stackage requires that any package added must be available on Hackage. A number of newer build systems have been going the route of allowing packages to be present only in a Git repository. Stack has built-in support for specifying such locations, but snapshots do not support it. Should we add support to Stackage to allow packages to be pulled from places besides Hackage? &gt; &gt; Pros: knocks down another barrier to entry for publishing packages. &gt; &gt; Cons: Stackage snapshots will not automatically work with cabal-install anymore, extra work to be done to make this functional, and some issues around determining who owns a package name need to be worked out. &gt; It's an easier release system (many people want to just git tag and be done with it). Uploading to Hackage is an extra step. I never got why we have to go through the error-prone additional ceremonial step rather than simply publish directly from CI-vetted and PGP-signed Git tags. &gt; For people in favor of enforcing PVP upper bounds on Hackage: it gives people not wanting PVP bounds on their packages somewhere else to upload their packages to. &gt; For people who don't want PVP bounds on their packages, it allows them to bypass the Hackage Trustee review process and cabal file revision. This! Hackage Trustees messing with my packages is the most repulsive aspect of Hackage. I really don't care about busy work just to keep cabal users from running into cabal hell. I know this may be an unpopular opinion, but imo that serves them right for not migrating to Stack. &gt; There's no longer a single definition of foo-1.2.3. It could be what's on Hackage, but it could also be what lts-9.32 includes from a random Git commit (depending on how we implemented things). In practice, I'd hope this never happens because people would synchronize their Git tags with Hackage releases, but it opens the possibility. I don't think this is a big issue. Publishing from Git would be based on PGP-signed tags, and once you have such a Git-tagged release that's the authoritative release that overrides any other distribution. &gt; It makes it harder to find Haskell packages. Neither Hackage nor even Stackage could provide a single listing of all possible packages available. I generally only trust packages on Stackage as those are stable packages that have been actively vetted to work with their testsuites passing. If it's not on Stackage it's either not worth using or it needs to be added to Stackage. I can also highly recommend [stackgo](https://github.com/psibi/stackgo) to automatically redirect your browser from Hackage to Stackage and improve your Haskell experience significantly! &gt; It complicates the interop story with cabal significantly. When designing both Stackage and Stack, there was a lot of thought and care put into making things as compatible as possible. A big part of that was agreeing on using Hackage as the One True Source for package information. I don't see a big benefit. It may be nice to have for some people but I only use Stackage to browse for packages and their documentation. And as you point out above having to through Hackage to publish to Stackage is just another extra step complicating everyone's life. EDIT: Typos
Mainly because you often can't compose two functions that have differing `MonadError` constraints, for example. But this is solvable with the `ether` library, or by just writing some convenience functions for embedding such calls in a larger constraint (like a `MonadError` constraint using a sum type of the possible errors from the calls).
I mean lots of things depend on (relative) precedence of operators. Such as `x + y * z` and `x || y &amp;&amp; z`. The tradeoff you make with Haskell is that you have to remember some precedences but you get nicer, more readable and more concise code with way less parenthesis.
I agree with that.
[removed]
I'm actually using `scalpel-core` as well since I didn't want to use curl for web requests. I think I'm using `http-conduit` under the hood. The repo has an example package with a simple `bash` script that will generate a tor-rc file. If you install tor, you can simply run `tor -f &lt;torrc file&gt;` and it will use the generated configuration. I think the script, by default, tells tor to run 30 proxies. When you build your own scraper, my package gives you a configurable `main` function to use as your `main`. It will take care of argument parsing and whatnot. One of its arguments is an optional tor-rc file to use. If you pass that, it will create a bunch of threads that all connect to a separate tor proxy. Your scraping rules can then produce URLs that need to be scraped and they will be fed (via a queue) to the collection threads which will pick a random proxy to use for each URL. Rules can also produce records (result data) which get fed into a CSV file. My package is rather crude at this point and could really use more documentation and improvements, but it's worked for my needs so far and I haven't had time to invest more TLC into it. I'd be happy to answer any questions via GitHub issues or whatnot. 
I really do not like they have no characters in common. IMHO only binary operators that commute should be made of balanced characters. * (`$`, `&lt;$&gt;`) is balanced * (`|&gt;`, `$&gt;`) is not balanced
Thank you for your response and your work. Edit: *your response
Thanks for sharing I will look at it
How's this for a shapes example: data Shape x where Area :: Shape Double Perimiter :: Shape Double rectangle :: Monad m =&gt; Double -&gt; Double -&gt; Object Shape m rectangle l w = Object $ \case Area -&gt; return (l * w, rectangle l w) Perimiter -&gt; return (2 * (l + w), rectangle l w) square :: Monad m =&gt; Double -&gt; Object Shape m square w = rectangle w w circle :: Monad m =&gt; Double -&gt; Object Shape m circle radius = Object $ \case Area -&gt; return (pi * radius ** 2, circle radius) Perimiter -&gt; return (2 * pi * radius, circle radius) I can try to improve it if you suggest specific weaknesses. One thing that I find particularly annoying that I can't see a way around is the need to explicitly re-create the object with its new state.
Oh neat, hadn't seen that library before. How does that compare to classy lenses/prisms?
When you maintain a few libraries, you start seeing that mundane overheads, delays tend to accumulate so you do what you can to beat a nice smooth happy path for people to do the right thing.
No problem! But yeah I do think that it's not typically worth it to do stuff like that. In my opinion if you end up with a situation with class `Foo` and then also class `FooMyType` you probably would have been better off just having `Foo` and doing `instance Foo (MyType (SubType1 ...))` with `FlexibleInstances`. Off the top of my head I can't see any real disadvantages of the flexible instance approach (for this specific situation that is). And to me it seems more direct and models what you actually want more accurately, particularly since `Foo (MyType ..)` specifies unambiguously and unfalsifiably that you are interacting with `MyType`, but `FooMyType` is merely a convention.
That looks really cool, hadn't seen that library before. How does that approach compare to classy lenses/prisms?
implying that if one learns Haskell, other languages become more difficult? I could see that. Before I caught the vision of what Types meant, I remember trying for a couple weeks to port a Free / Cofree Monad DSL pattern into clojure. Bad idea.
Awesome! I like the practical approach and comparisons with Java. I've been pondering a few Haskell apps with Twilio and glad to see it has such good support. This article has some issues with how it presents package management in Haskell: 1. Cabal isn't necessary at all in this example. The sandbox isn't doing anything. 2. The "proper" way to initialize a new package is to do `stack init`, select your dependencies, and then add them to `stack.yaml` as `extra-deps` specifying the version you need. Starting off with the cabal file doesn't take advantage of all of `stack`'s goodness. 3. The `stack install twilio` line is misleading. `stack install` installs the *executables* associated with a package. To install a library dependency, it only needs to be part of the `cabal` file with `stack build`. 
I no longer care for the classy lenses / prisms stuff. It's kinda clumsy, and it's barely helping composability since you still have to ask others to implement every possible `HasFoo` class anyway. A simple example of one of the problems is when you call a function with a smaller constraint when you're using a bigger one. data Foo = Foo Bar instance HasFoo Foo ... instance HasBar Foo ... foo :: (HasFoo a, MonadReader a m) =&gt; m () foo = do bar -- Error! ... bar :: (HasBar a, MonadReader a m) =&gt; m () bar = ... Even though we know `Foo`s have `Bar`s, we can't call functions with `HasBar` when only given `HasFoo`, unless we do a little extra work. We can add a `HasBar` constraint to the `foo` function, but that means anyone who calls `foo` is now forced to also implement `HasBar`, unless we write an overlapping, undecidable instance (which has a whole host of problems). Alternatively, we can `asks` for the `Foo`'s `Bar`, and then use `runReaderT` on `bar`, but that defeats the whole purpose of the pattern!
Thanks for the feedback! Will go over again, but I do feel that both help and make it easier to get the dependencies to install together. As the post goes over both cabal and stack, I hope readers will be able to use both in the future for Haskell dev.
Whoops. I knew that at one point... My Unix friends would be disappointed in me (thanks for the correction)
I'm missing something. How is `&amp;` defined, and where? Hoogle didn't help and I haven't seen this before. Thanks!
&gt; implying that if one learns Haskell, other languages become more difficult? No, just noting that in a language you don't know, even simple programs become virtually impossible. Try saying "Hello, world!" in Swahili *without learning any Swahili*.
&gt; There's no longer a single definition of foo-1.2.3 But that's already the case today. You have to qualify 'foo-1.2.3' with the source where you pulled it from (public hackage or an internal hackage server perhaps), and which revision of the cabal file (hackage cabal file revisions...). &gt; It makes it harder to find Haskell packages. Not all people upload their packages to hackage. Hackage can no longer provide a 'single listing of all packages available'.
Haskell [makes you a better programmer](https://www.reddit.com/r/haskell/comments/31tsru/why_should_i_learn_haskell/cq4x3e6/), even if you don't write Haskell, so I don't think it's a waste to learn. However, depending on deployment environment, it may be easier continue to do things in Python. Compare the Android environment. While you certainly *can* deploy Haskell applications to the Google Play store and run them on millions of devices, it's probably easier to use a language with strong JVM interoperability (e.g. Eta, Frege, or Scala), at least, if not just to use Java.
Let people know if you have problems finding developers with potential to grow into Haskell. I am sure people here could come up with ideas for how to recruit good potential Haskellers in Alaska.
Glad to see Twilio giving some Haskell love. I don't think the cabal file needs all those dependencies. All you need is the one you're directly using, which in this case is `twilio`. Also no need to run `stack install` at all. Furthermore, if you want to avoid all that waiting for the sake of an article, just tell them to open up `stack ghci` and run `main`. No need to build the project. Also, since you bother to mention cabal, it's a bit misleading to give it such a hard time. It already has `cabal new-build` which essentially eliminates this problem and will become the default very soon. That might be a better direction to go (e.g. "Cabal has historically been a bit more troublesome, but with the latest version, you can use it without any issues using `new-build`...")
&gt; When I use a word it means just what I choose it to mean—neither more nor less. -- Humpty Dumpty, rather scornfully --- You can only say "static checks don't count" if you ignore most of the use of the word "type" in maths, including computer science.
It's defined in [`Data.Function`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Function.html#v:-38-) as `flip ($)`. It's a relatively new addition, which is why it's not in `haskell.org/hoogle`'s relatively old database. It's both in [`hoogle.haskell.org`](http://hoogle.haskell.org/?hoogle=%28%26%29&amp;scope=set%3Astackage) and in [stackage's hoogle](https://www.stackage.org/lts-6.2/hoogle?q=%28%26%29) though.
This is great. I think that at this point Haskell has definitely passed the immortality threshold.
Thanks! I just put hoogle in google and totally forgot the other places hoogle lived. I thought that might be how it was defined but wasn't sure.
&gt; Starting off with the cabal file doesn't take advantage of all of stack's goodness. I also tend to start with the cabal file and generate the `stack.yaml` from it, which goodness am I missing on?
I'm not sure they could. Type theory predates compilers by a while. Static type systems correspond closely with types a la type theory; dynamic types don't really make any sense in that setting. So there's a long-standing sense (which predates dynamically typed languages by decades) in which types are inherently static. I don't think there's a similar context in which types are inherently dynamic. Honestly, it's the kind of statement that gets most of its shock value from unfortunately overloaded terminology. "Dynamically tag-checked languages can be statically type checked with a static type system with a single type" is an interesting observation, but one unlikely to be controversial.
&gt; Note that, because a custom snapshot is intended to contain immutable package data, it does not support local filepaths as package location, as these are expected to change over time. I've been thinking a bit about this lately. What if we could select a specific git commit even for local paths? - location: git: /local/absolute/path/to/a/package commit: fb4e7f9cfd66f9ebc30f3394cfbdcaa58bd044db Because this way we could get high level reproducibility, couldn't we? Or is this already possible perhaps somehow? And I don't just mean custom snapshots, as I haven't had the occasion to try them yet, but also for plain old `extra-deps`. (I've also tried to submit this comment via disqus on the page some hours ago, and later noticed that for some reason it was filtered as spam.) 
Hi, just went through the article. I think the confusion with dependencies and building is because Stack uses curated package sets and the Twilio library isn't on any of them, and also because Twilio's dependency constraints aren't quite up to date. These steps should correctly set up a Stack project with the Twilio library: 1. Create your project: `stack new hello-twilio` 2. Edit the `stack.yaml` and change the `resolver` to `lts-6.35` then add `twilio-0.1.3.1` to the `extra-deps` array. 3. Add `transformers` and `twilio` to the `build-depends` key of the `exe` section of `hello-twilio.cabal` file; these are the only two dependencies needed for your example. 4. Run `stack setup` and `stack build`. Also, while signing up for Twilio I noticed Haskell wasn't in the languages, maybe time to add it :)
Cool, thanks for the feedback! I added an issue for this
I appreciate that, thank you very much.
Your points are valid, but a move like this will take things to a different level. Right now, `stack build foo-1.2.3` and `cabal install foo-1.2.3` on a fresh installation are always using the same source code (sans cabal file revisions of course). That would no longer be true. I happen to agree that finding Haskell packages on Hackage is not a real use case, it's all but impossible to find a quality library just by sifting through that listing.
Yea once `new-build` is the default, I think the only thing left that Cabal still does wrong by default is the lack of version pinning. You can easily do it, it's just not the default. It's hard to do this without having to check in a massive version lock file unless you use snapshots though, which Cabal also does not have by default.
Ah, so you mean times when it's useful to have object identity?
I noticed your comment come in and then couldn't find it. I don't know why Disqus marked it as spam. I just tested, and using a file path for a Git URL seems to work just fine, so you may already be good to go there :)
Do most people use [taggy-lens](https://hackage.haskell.org/package/taggy-lens) to get at the data in HTML or what?
Haven't Lenses and the zoo of Optics, Prisms, etc. subsumed SYB and Strafunski style traversals? I've not looked into it, but I would hope you could define Strafunski style traversal strategies with Lenses if you needed them. 
Thank you, I forgot I wrote that blog post. Your points are solid. And if we were starting over from scratch today, I would argue strongly for a decentralized package system. In my opinion, this all comes down to tradeoffs of cross-tool compatibility and backwards compatibility versus having the ideal solution. The extra complication of the social aspects of this (like public attacks on PVP non-conformers) is making this a more heated discussion than it should be. I fully understand why you (and many others) want this change to happen. I also understand why others are opposed to it. I just wish our community was better at having these kinds of discussions openly.
I'm glad to hear that it may already work! I shall give it a try sometime. Thanks for your response too!
[removed]
I managed to implement a higher-order function with this signature: gmapT :: (Generic a, GMapT' constraint (Rep a)) =&gt; proxy constraint -&gt; (forall x. constraint x =&gt; x -&gt; x) -&gt; a -&gt; a It checks at compile time that there is a `constraint` instance for all the fields of your datatype, and at runtime it applies this function on all the fields. Is that what you had in mind?
&gt; If you want more fine grained effect types you could try indexed monads. They work basically the same, you just need rebindable syntax and some more type variables. It not a question of what I can use than a question of making the haskell community being aware of the problem and create an standard of graded or indexed monad or whatever that address the problem as a replacement of monad transformers, extensible records and conventional extensible effect monads
Pretty sure that, at the moment, the "blessed" IDE setup is Emacs + Intero. Another alternative might be VS Code + Intero + some kind of integration plugin between the two.
In the meantime, Transient provides such kind of composability in the large for some high level effects. The "price" to pay ,as you said, is simple types. But I think that simpler types makes things simpler. Specially for beginners. I think that composability comes first in the list of priorities.
Interesting. So you are switching by message tag. I didn't quite catch that at first. Can you build interface hierarchies? I don't see how to do that easily. OOP would be rather crippled without it (though still technically OOP). As for recreating the object, maybe something like this: circle radius = let self = Object $ λcase Area -&gt; return (π * radius ** 2, self) Perimiter -&gt; return (2 * π * radius, self) in self
Hmm, not sure what identity has to do with it. 
If you're going to use stack, using intero makes a lot of sense. Emacs + Dante also works well with stack, cabal and cabal2nix projects, but missing some intero features
is it worth it? yes, yes it is. I came to Haskell from Python. with global type inference, you can write even scripts without annotations, which is pretty sweet. if you're familiar with Python, I don't think Clojure is worth learning. as a dynamically typed language, it shares most of the same conveniences and bugginess as Python. &gt; automating the browser with selenium, storing some information inside of files and a little web development. Python's good at those tasks, but Haskell makes them simpler and more reliable (obviously, I would I think it's better, that's why I'm here :-). I'd start with some simple shell scripts via the Turtle library: https://github.com/Gabriel439/Haskell-Turtle-Library https://hackage.haskell.org/package/turtle-1.3.6/docs/Turtle-Tutorial.html or, make a small web app with one of the more minimalist frameworks: https://www.spock.li/tutorials/getting-started http://snapframework.com/docs/tutorials/snap-api I've never used selenium from Haskell, but this package seems reasonable: https://github.com/kallisti-dev/hs-webdriver 
I'm hoping more people start using Haskell-IDE-Engine once the HSoC project is done. It's miles better than Intero if for no other reason than LSP
Could you please point me to a tutorial for Emacs+Intero on both linux&amp; Windows that just works? I'm not an Emacs guy, but I did hear good things about this combination and I'm willing to learn, I just don't want to fiddle too much with it. 
since GHC is an optimizing compiler, why can't we just compile GHC with itself over and over again to make it arbitrarily fast
I have tried Scalpel, and it is a decent parser (although it lacks documentation on regex use, e.g. for matching hrefs that link to a json). However, it's not a web scrapper. It lacks the ability to interact with e.g. loading delays, js, etc. I was going to try webdriver for that, but eventually I switched languages, so no feedback on this.
Actually, just make sure to split work into work-units under 30 seconds, as &gt; GHC 8.2.1-rc3 is still about 30 seconds faster than 8.0.2 though. But you have to be careful not to compile things below 30 secs or you'll go back in time.
[The official documentation](http://commercialhaskell.github.io/intero/) might do. Protip: the default Emacs keybindings are dangerously unergonomic. I don't know how most people deal with that, but personally I just use evil-mode for Vi-style keybindings.
&gt; Reid Barton also did a lot of work plugging space leaks in the simplifier. This is a hero's work. 
Try [hs-scrape](https://hackage.haskell.org/package/hs-scrape) which internally uses wreq and xml-conduit. [Here's an example](https://github.com/codygman/hs-scrape-paypal-login/blob/master/Main.hs) of logging into PayPal and displaying your balance with [hs-scrape](https://hackage.haskell.org/package/hs-scrape) which internally uses wreq and xml-conduit: import Control.Applicative import Control.Monad import Control.Monad.IO.Class import Data.Maybe import Data.Monoid import qualified Data.Text as T import Data.Text.IO (putStrLn) import Network.Scraper.State import Prelude hiding (putStrLn) import Text.XML.Cursor (attributeIs, content, element, ($//), (&amp;/)) -- At the bottom of this file you'll find a repl session[0] to help understand the getPaypalBalance function. -- Additionally there is a more verbose version of the getPaypalBalance function that makes the composition -- and order of operations more explicit. getPaypalBalance cursor = fromMaybe (error "Failed to get balance") $ listToMaybe $ cursor $// -- Create 'Axis' that matches element named "div" who has an attribute -- named "class" and attribute value named "balanceNumeral" -- This axis will apply to the descendants of cursor. element "div" &gt;=&gt; attributeIs "class" "balanceNumeral" &amp;/ -- The Axis following &amp;/ below matches the results of the previous Axis. -- In other words, the following Axis will match all descendants inside of -- &lt;div class="balanceNumeral"&gt;&lt;/div&gt; element "span" &gt;=&gt; attributeIs "class" "h2" &amp;/ -- The content Axis is applied to the results of the previous Axis. -- In other words, it gets the &lt;span class="h2"&gt;content&lt;/span&gt; out. content 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [codygman/hs-scrape-paypal-login/.../**Main.hs** (master → 9f67afb)](https://github.com/codygman/hs-scrape-paypal-login/blob/9f67afb183320a2372df0bb6b1e7ccc7029c6a72/Main.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dk6r5zj.)^.
Very cool! I've added a link to the HLint manual, https://github.com/ndmitchell/hlint#integrations
Thanks, glad you liked it. I was inspired by your talk at Zurihac. 
Nowadays (in 2017) instead of `syb` you should use `uniplate`. You can see its goals in the first paragraph of description: https://hackage.haskell.org/package/uniplate
Even better :)
- `stack` is build on a snapshot assumption - multiple versions of package won't really fit its model - to my understanding current GHC supports multiple versions of the same package as dependencies (https://downloads.haskell.org/~ghc/8.0.2/docs/html/users_guide/packages.html#package-thinning-and-renaming implies that) - yet Cabal (the file-format) doesn't support that: https://github.com/haskell/cabal/issues/4035 there is a ticket though
[Fairly relevant previous discussion here.](https://www.reddit.com/r/haskell/comments/4zc6y3/why_doesnt_cabal_use_a_model_like_that_of_npm/d6vymnt/)
Glad to see that was useful for someone outside our org :) We still have a ways to go, but hopefully by next year we'll be making some solid contributions back to the community.
Always happy to take in enthusiastic developers with experience in other technology, knowing Haskell is by no means a requirement. Hard to be too picky and choosey due to the small population size of the state.
Onsite where? ALASKA?! 
[I wrote this about two years ago.](https://github.com/tejon/Meeseeks) It's pretty simple, but one maybe interesting aspect is that I wound up using [hxt-css](https://hackage.haskell.org/package/hxt-css) instead of TagSoup, because that was the easiest way to support loading standard CSS selector strings at runtime. Scalpel didn't exist yet, though... I need to take a look at that!
&gt; eg. if `Increment` was overridden to take two `Int`'s If the signature is different, wouldn't that be overloading, not overriding? And so the original Increment which takes a single Int is still accessible, next to this new completely separate method which might as well not be called Increment?
Thanks for sharing it looks nice...!!
Nix allows this. https://nixos.org/nix/
Try adding this: type RGB = (Double, Double, Double) data ColoredShape x where Shape :: Shape x -&gt; ColoredShape x Color :: ColoredShape RGB coloredShape :: Monad m =&gt; RGB -&gt; Object Shape m -&gt; Object ColoredShape m coloredShape color shape = let self = Object $ \case Color -&gt; return (color, self) Shape msg -&gt; fmap (second $ const self) $ runObject shape msg in self class Subtype f g where upcast :: Monad m =&gt; Object f m -&gt; Object g m dispatch :: g x -&gt; f x instance Subtype ColoredShape Shape where upcast cs = Object $ \msg -&gt; fmap (second upcast) $ runObject cs (dispatch msg) dispatch = Shape 
[removed]
IMHO, the comparison in the beginning with Java feels forced and completely pointless.
Looks like Fairbanks, based on their website (so yes, in Alaska)
&gt; implying that if one learns Haskell, other languages become more difficult? Relatively difficult, yes.
IMHO, I have taken interest in Paul's ideas for Unison's fine grained, content based approach to dependencies. I am curious to see how that might work.
There are probably a few people who know Scala already scattered around Alaska, if you can just find the right meetup or forum to connect with them.
I get there are instances where it might be nice to have, but this flexibility doesn't seem to be worth the complexity it adds to the ecosystem + codebases + tooling.
I'm not sure sorry
Neat! Is this online anywhere?
Apologies for not cleaning up imports, this was rushed for a project I needed it for at the time.
For reference, the question was asked by [Alexey Radul](http://alexey.radul.name/).
On windows Emacs has been a bit annoying for me, and now I've become quite in love with VS Code (with Haskero to interact with Intero) (and Stylish Haskell and HLint)
It seems slightly weird that in the middle, after discussing build tools, there is a part about installing GHC and using it. As a new user I would then think this is required for the build tools, which is not the case with Stack! And like stated elsewhere, the stack install command is not needed in the running of your package in development. Thats slightly confusing.
[removed]
&gt; I fully understand why you (and many others) want this change to happen. I also understand why others are opposed to it. You wrote in another comment there are no *current* plans to implement such a change. Is this just a diplomatic way of avoiding to give us any assurance you won't pursue some plan to that effect in the future, while at the same time giving those who want that change hope it's not off the table yet?
See Section 5 in http://dreixel.net/research/pdf/ggp.pdf
I did not know about its existence... Nice What is LSP though? 
Might also be interesting to look at this (still pending) Rust/Cargo RFC: https://github.com/rust-lang/rfcs/pull/1977
Language-Server Protocol
This works, but overriding methods in subtypes is not entirely obvious. I managed to add a Print method to shape and override it in coloredShape this way: {-# LANGUAGE Rank2Types #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE LambdaCase #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE FlexibleInstances #-} import Control.Arrow (second) newtype Object f g = Object { runObjectU :: forall x. f x -&gt; g (x, Object f g) } runObject x = runObjectU (upcast x) instance Show (Object f g) where show o = "obj" data Shape x where Area :: Shape Double Perimeter :: Shape Double Print :: Shape String rectangle :: Monad m =&gt; Double -&gt; Double -&gt; Object Shape m rectangle l w = let self = Object $ \case Area -&gt; return (l * w, self) Perimeter -&gt; return (2 * (l + w), self) Print -&gt; return ("rect " ++ show l ++ " " ++ show w, self) in self square :: Monad m =&gt; Double -&gt; Object Shape m square w = rectangle w w circle :: Monad m =&gt; Double -&gt; Object Shape m circle radius = let self = Object $ \case Area -&gt; return (pi * radius ** 2, self) Perimeter -&gt; return (2 * pi * radius, self) Print -&gt; return ("circle " ++ show radius, self) in self type RGB = (Double, Double, Double) data ColoredShape x where Shape :: Shape x -&gt; ColoredShape x Color :: ColoredShape RGB coloredShape :: Monad m =&gt; RGB -&gt; Object Shape m -&gt; Object ColoredShape m coloredShape color shape = let self = Object $ \case Color -&gt; return (color, self) Shape Print -&gt; do (rep, _) &lt;- runObject shape Print return ("colored " ++ show color ++ " " ++ rep, self) Shape msg -&gt; fmap (second $ const self) $ runObject shape msg in self class Subtype f g where upcast :: Monad m =&gt; Object f m -&gt; Object g m dispatch :: g x -&gt; f x instance Subtype ColoredShape Shape where upcast cs = Object $ \msg -&gt; fmap (second upcast) $ runObjectU cs (dispatch msg) dispatch = Shape instance Subtype a a where upcast = id dispatch = id I added the trivial `Subtype` instance and `runObjectU` so that I don't need to `runObject (upcast obj)` all the time. Also changed the spelling of Perimeter ;) But I don't really like the way I had to override `Print`. I would add this stuff to the article.
Nix allows this, but the Haskell infrastructure in NixPkgs does not actively support this, I am pretty sure. It uses Cabal in a way similar to how Stack uses it. It's probably quite doable to achieve this with Nix as a supporting tool, because Nix is, well... programmable, and packages are first class immutable values instead of the mutable search-path-dependency-injection-tangled mess people are used to. So yeah, Nix definitely *allows* this.
No, this has nothing to do with nix. If Cabal/GHC don't allow it, nix won't either since it uses them to actually build the package.
I've heard the word "class" used instead of "tags" for the same distinction - "classes are not types". I like "tags" more for this distinction because you could argue that classes are elaborate tags in this context, like vtables and reflection info instead of just an int. Also, classes and types tend to get conflated by some OOP practitioners, so "class" instead of "tag" probably has a somewhat alienating effect for those people.
Well, maybe hackage isn't suited for finding packages, but it is very useful for figuring out what the latest version of a package is and it provides a single location where docs can be found (compared to for example rust, before docs.rs was a thing, where you had to find the link to the docs individually for each package (and hope that they also provide links for old versions)). Also it is nice for downstream packagers to have a single source (just think about what even made stackage possible.)
I use [tagsoup](http://hackage.haskell.org/package/tagsoup) and built the crawling infrastructure around that. If you are crawling many sites and only keeping small portions, it's really important to use the `copy` function of `ByteString`/`Text`/... to prevent massive amounts of memory to be wasted.
I'm not sure you're right that GHC cares. Cabal definitely cares, but Nixpkgs doesn't really use Cabal for all that much.
Wreq doesn't see much development lately and there are issues that are not addressed. Most importantly connection sharing in multithread environment. As a shameless plug, I have written Req: https://github.com/mrkkrp/req. Readme also compares the library with existing solutions and has an example of usage.
FWIW, if you're more Vim-inclined [that's supported too](https://github.com/parsonsmatt/intero-neovim). Disclaimer: I'm one of the maintainers