Hi! I would suggest you a project about how to design haskell applications using advanced design technologies and approaches. It's called Andromeda: https://github.com/graninas/Andromeda This is a SCADA software that should allow you to evaluate control over spaceship. The project will have such parts: - External Domain Specific Language to write control scripts. - External Domain Specific Language to design ship's critical systems (devices, sensors, etc.). - GUI application and working environment for SCADA engineer (devices monitoring and overall control, graphs, scripting, ship's systems modeling, etc.) - Simulator that simulates a ship to test control programs. The project now has a simple Hardware subsystem, a dummy GUI, a basic multithread simulator and some Logic Control elements. These things have been used: - Advanced functional declarative desing; - Free monads and Free languages - MVars, threads - QML, MVVM - Some type level programming - Embedded DSLs including basic DSLs for scripting - Primitive external DSL for scripting - MVar request response pattern - Service handlers - Abstract interpreter pattern - Tests - Functional interfaces - State, IO monads - State transformers - Lenses - Compilers - STM And many other interesting things I use to design application in a pure functional way. In fact, I'm working on this project to support my upcomming book. The book has working name 'Functional design and architecture'. If you decide to contribute into the project, I'll share materials with you that you'll be understanding the whole idea. Best, Alexander
Business casual is fine at most banks in London for a programming job: Trousers, shirt, shoes. You might want a suit and/or tie for the interview, but it really won't matter for tech jobs. * source: I work in a Haskell team at a bank. &gt; 50% chance it's the one you're applying to :-)
Scala, we're coming for you! You're not safe on the JVM any more!
`succ` and `pred` and `toEnum` (and sometimes `fromEnum`) in the Prelude, and there is no safe alternative. We may need to burn down the `Enum` typeclass and start over.
I am talking about the dress code for the typical interview. You also have a very large benefit of being very well respected in the Haskell community; a benefit the person applying might not have. This might mean you get treated preferentially. Your experience is not relevant imo.
RT systems analysis. Of course, no, you're nowhere near turing-complete, then, but promptness of answers can be guaranteed.
I do find that formatting is important to me. Nowadays, I write the example type as data Date = Date { dateYear :: Int , dateMonth :: Int , dateDay :: Int } Note the aligned double colon `::`.
At work we do this too - easiest to read imo
I think providing this function actually requires stronger laws than the cancellation property. 
Given that this is interop you can compile and hotload method changes. If the inline Java compiles to methods that are called by Haskell hotloading should be achievable without massive work. This can be done with HotSwap (which can replace method bodies at runtime). http://zeroturnaround.com/rebellabs/reloading_java_classes_401_hotswap_jrebel/ Using Haskell support for dynamic modules will likely mean having to restart the JVM (which is slow) or having to use a more general hot code loading product like JRebel.
I'm not quite sure what you're saying. Are you also saying that `(-)` doesn't belong in Cancellative?
I don't know that it requires inverses. Sets can form a monoid under union, and sets have a `(-)` operator (intersection) that doesn't represent an inverse.
Good catch ! I didn't see that.
Fixed ! Thanks.
Feedback: inclue more type signatures in your posts and documentation, not just examples of use. They will help explain your thoughts .
I agree that overtly no one would agree to using this as a criterion but a large portion of the decision is subconscious.
True. I might subconsciously hold it against a candidate if they dress up too much. 
This sounds interesting. I also used to fly planes. How can i learn more or get involved?
then what happens when i want to match on the arguments for the next elements in the list (crs)? 
 keepNCSymT (x:xs) ((CommRel a b c):crs) | x == a || x == b || x==crs(1) || x==crs(2)= ...
Thanks for the log! Yes, the only reason it works for fibUp is that you don't need the induction hypothesis for `fibUp` to obtain the result; the result follows from the fact that `fib n`is a nat and the properties of `fibUp`. For `fMono` you *do* need `thm x` and `thm (y-1)` and the induction hypothesis `fMono f thm x (y-1)`, which is why the code (proof) above is rejected.
This is the second question in a week you've posted about some basic aspect of writing Haskell code. I recommend you stop what you're trying to do and refresh your language knowledge by reading something like Learn You a Haskell for Great Good, or whatever else it was that you used to learn Haskell. Also that might also help you learn the correct terminology to use when asking questions. Since both this one and your last one are pretty confusing to deduce from the wording and terms you use.
Either recurse, or pattern match on the sublist `crs` someFunc ((SomeDatatype a b c):((SomeDatatype a' b' c'):rest)) = ... will deconstruct the first and second elements of the list. `(a:(b:rest))` is just as valid a pattern as `(a:rest)`.
Depends how well it is coiffured. If it looks like it could house migrating sea birds - negative If it has been and of trimmed, waxed, permed - you're good :-)
Partial functions do much worse than make type signatures harder to read, they make them into a false promise. Say you write a function `route911Call :: 911OperatorLocations -&gt; CallOriginLocation -&gt; 911Operator`. In idiomatic Haskell this is a promise that a `911Operator`can always be chosen by `route911Call`. I can use this function without knowing anything about the problem domain. If the function is partial then I can't reason about it using the type signature. Instead I have to deal with the guts of the code. If the function is complicated this will be a lot of work. Worse it's work that the type system won't be helping with. And doing this over and over for many functions isn't sustainable, you'll end up with runtime errors. EDIT: PS: The Prelude is full of partial functions like `head`. Don't use them. Haskell badly needs a new default Prelude.
Because you're using Windows. ;) I recall seeing stuff like this with some distros of Haskell on Windows; once I switched to stack I never had any issues.
There's a library for this, called [`spoon`](https://hackage.haskell.org/package/spoon)
What if your hair and beard makes you look like a crazy Russian?
Would it make sense to try and get the `Enum` instance for `Float` and `Double` removed from the Haskell specification, and/or from `base`?
data Positive a = Maybe a I think this will get you full tower instances with the manual tweak for group. 
[open "easy?" issues](https://github.com/simonmichael/hledger/issues?q=is%3Aissue+is%3Aopen+label%3Aeasy%3F), [developer guide](http://hledger.org/developer-guide.html), [2016/9 call for contributors](https://groups.google.com/d/msg/hledger/heqnWZaMiwk/srXNj5wjAgAJ) for hledger 
Thanks for the quick reaction. While you're here, there's another couple of things that look odd: * Your `Abelian` class requires a zero. Can't you have abelian semigroups? Maybe move `zero` to `Group` and make `Abelian` only require `Semigroup` (keeping `(+) = (&lt;&gt;)`)? * You've written "A semiring, also known as a rif is a Rg with multiplicative identity.", where I think you mean "rig"
Use the `PrtSc` key to take a screenshot or use a dedicated program. 
I don't understand why that proves that they don't have the right relationship?
In a monoid with subtraction always (A + B) − B = A. In the set union monoid, this is not true of set difference.
in addition to maxigit's example, another possible use case is to keep things unambiguous if you're deliberately colliding aliases. I'm not entirely convinced there's a good use case for *that* though.
if even say a simple division needs to return an Either, rather than just a number, due to division by zero, it can get just impractical. So Haskell allows both approaches to use them where appropriate; unchecked exceptions or either types for you to use, and there's a couple of libraries for dealing with them. Sadly that does make it rather difficult to guarantee the ecosystem is making a sane choice, but in actuality the situation isn't catastrophic either.
It is quite out the scope of web frameworks. As for whether it is possible in *Haskell* in general... Of course it is, but chances are you don't have the time and skills to do it (neither do I, for that matter). If I had to do this, I would probably write bindings to some pre-existing OCR library, like Tesseract, and use them from my application.
Could you go into more detail about "no errors"?
Looks promosing. the only thing I have to figure out is how to deal with finding the company data and the financial data. This api sends me all data at once.
[This is a cool demo on how good c++ is at it](https://www.youtube.com/watch?v=zBkNBP00wJE)
IMHO it's quite unique in Haskell that YOU can influence the optimizer through rewrite rules thanks to purity. 
There was a [DDOS attack][1] on a DNS resolver. As the error message indicates, there was a short time period when the hostname `github.com` could not be resolved. This also meant that you couldn't reach the GitHub website, for example. [1]: https://www.dynstatus.com/incidents/nlr4yrr162t8
I tried again, but it's still unable to resolve. What can I do?
Not unique (compare with https://www.pvk.ca/Blog/2014/08/16/how-to-define-new-intrinsics-in-sbcl/ for example), but quite unusual for sure.
https://www.reddit.com/r/programming/comments/58o5p6/github_is_down/d920qsm/ &gt; Everybody go home. No point working. Just kidding. Here's the entries you can put in your hosts file until dns is happy again: &gt; 192.30.253.113 github.com &gt; 151.101.44.133 assets-cdn.github.com &gt; 54.236.140.90 collector.githubapp.com &gt; 192.30.253.116 api.github.com &gt; 192.30.253.122 ssh.github.com &gt; 151.101.44.133 avatars0.githubusercontent.com &gt; 151.101.44.133 avatars1.githubusercontent.com &gt; 151.101.44.133 avatars2.githubusercontent.com &gt; 151.101.44.133 avatars3.githubusercontent.com EDIT: Alternatively, OpenDNS appears to be unaffected.
I'd say bold your last sentence, just to be sure it's not missed once someone sees the IP. For what it's worth, what I see on my screen does not appear to have been tampered with.
Did, thanks. The whole incident raises the question: What is the attackers motivation for DDOSing a DNS provider? The only good answer I can think of is: To pull off a Man In the Middle attack. 
Well, I think I remember reading something on a Bruce Schneier blog post about some powerful unknown entity systematically testing the capabilities of key infrastructure points in the North American Internet to see what their breaking point is. Hope this isn't related. Also this has already negatively impacted our family business.
Yeah it was pretty rough when I just reread it. I'm writing stuff up now that should explain.
Is anyone else really not comfortable with this style, even for C? Like, what's wrong with a fixed size unrolled while followed by a switch? Or even, gasp, a second while, rather than a pseudo-goto.
I added what I thought was necessary to get the feedback that I need at this point. I don't really think this would be good to use in its current form for anything other than experimentation, its just an instantiation of a design using some features of GHC that I haven't seen used in this way before. 
There are quite a few ways to call out to shell, the package below is one of them: https://hackage.haskell.org/package/process-1.4.2.0/docs/System-Process.html As for tesseract: tesseract &lt;image&gt; &lt;text&gt; It should be straight forward to call tesseract from Haskell using System-Process. 
Also see https://www.youtube.com/watch?v=8C8NnE1Dg4A&amp;&amp;t=7m28s
I don't think so, because you can't take NonNegatives very far and Positive is even more constrained. I think if you define a type like `newtype Positive a = P { unP :: a }`, you can define instances for `Semigroup (Positive Int)`, `Abelian (Positive Int)` (once you move it to only require `Semigroup`) and `Cancellative (Positive Int)` (and so on for other numeric types), and then you're done. If you do a NonNegative newtype instead, you can also get `Monoid`. I think if you make a new law for `Group` that requires `cancel` to never return `Nothing`, you can avoid propagating `Maybe` all through your hierarchy. Also, have you considered defining `LeftCancellative` and `RightCancellative` (with methods `lcancel` and `rcancel`)? They seem like they'd be useful to generalise the idea of stripping prefixes and suffixes from lists (when you have `Eq` on the elements, at least).
I love Rust, but its type system is not like Haskell's beyond the basics. Sure, they're both based on typeclasses (traits in rustspeak), but Haskell's is way more powerful for now.
I'm fairly sure I'm going to switch to https://github.com/SamuelSchlesinger/Gaia. The names are much better, and no Scalar so far. lcancel and rcancel will be useful for sure. I'd like to gather canonical uses as examples of each class
I believe the Rust community is coming around to proper higher kinded types, which is the biggest gap IMO. Yes, Haskell has *a lot* of other type system features, but none so important as type classes and HKTs.
And also type families/fundeps, which Rust already has.
There are sides for most of them in here: http://functionalconf.com/schedule.html , The video recordings will be out in a week or two.
I found this. https://gist.github.com/badboy/6954cc6ce1c71b921094 But what is it good for? Genuinely curious! 
C++ is really great at making horrible programmers look good and smart while earning them fat salaries.
&gt; This little trick was to basically flog the data bus as fast as the processor could. Do you have any info on that? Because I find it difficult to believe that Duff's was ever faster than the equivalent hand unrolled loop. 
&gt; Applications requiring very high performance and predictability might be one. Where else? Could be, but idiomatic C (and at some point Rust) will usually beat the idiomatic C++ implementation on perf and predictability.
Everything where you need really high performance. But if you don't have to use C++ (libraries, legacy code), take a look at Rust. Imo it's C++ from scratch and done right, also it has quite a few similarities with Haskell, might make it easier for you.
I guess he has laziness on his mind.
As far as I can tell, video game engines are more or less exclusively C++. I don't have any experience in that space, so correct me if I'm wrong, but on the face of it that makes a lot of sense to me: - Good performance is typically a major selling point since it directly determines how shiny your graphics can be. - You want predictable frame times to avoid visible stuttering, so garbage collection (at least using the general-purpose collection strategies) is out of the question. - Since large parts of the codebase are performance-sensitive, you can't get away with the usual 98% Python and 2% highly optimised C. - Projects are very complex and thus require some sort of organisation beyond the facilities of procedural languages, which OOP can provide.
What the hell are you trying to argue? Did you even understand what the OP wrote?
If you give up exception handling, virtual functions and RTTI, I don't see why C++ should be slower than C. You can at least use C++ as a "C with classes" and avoid features that may introduce runtime overhead to introduce some structure to make the code more manageable. Apart from classes (entailing class hierarchy and access control), you also get namespaces.
I know this doesn't answer the question, but you could also look into using an operating system that's more reliable across upgrades.
Higher kinded types allow the type class system to "overload" over type functions. Like list, maybe, or functions. This makes it possible to write against abstractions like monads. Monads make combing certain functions easier.
If GC is not acceptable for your performance, use C++, or even better, Rust. There's also a few problems that are better solved with OO, where in Haskell adding a variant breaks many functions and requires a bunch of spread out changes, but adding it in OO just requires adding a class and implementing the methods it inherits. Games seem to fit in this category, though I'm sure people have come up with nice solutions for this. 
It really isn't. System packages solve one out of about five problems you have when distributing software.
You have no control over the versions you're using. This is fine for toys, but you can't expect that to be acceptable if you care about your application continuing to work next month. You also limit yourself to only distributing to systems using the same set of packages. You also force every project in the environment to use the same versions of everything. You also can't develop against devel versions of popular libraries because half your system will be built and linked against the current release, etc. At some point, you have to start managing these things on your own. C and C++ tool chains do not help you at all. 
I don't think the OP is entirely correct: &gt; It is just a graph with 3 nodes and 2 local rewrite rules is not true of the "optimal" evaluator. Levy's optimal evaluator has an *infinite* number of nodes and rules, and you can see this in `optlam` as well. It adds a rule/node scheme to basic interaction combinators in order to achieve optimality. [This seems to be an encoding of arbitrary lambda terms in to interaction combinators](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.9699&amp;rep=rep1&amp;type=pdf), lacking the need for an infinite number of nodes as well as typability under EAL. It's not "optimal" but it outperforms call-by-need, as most interaction net encoding of the lambda calculus do. As an aside, does anyone know of any implementations of elementary affine logic? I've been looking at some papers on it but they seem a bit dense, and if I can't figure it out I'm just going with this non-optimal interaction combinator encoding instead so I don't need to worry about it.
Typically OS selection is based on the target system the users will be running, and not based on what the developer has the best time working under.
&gt; You have no control over the versions you're using. This is fine for toys, but you can't expect that to be acceptable if you care about your application continuing to work next month. APIs (and generally, ABIs too) shouldn't change between minor versions in such a way that version pinning is necessary. They do change between major versions, but the OS will probably keep distributing the older version as well as the new version until applications port over. &gt; You also limit yourself to only distributing to systems using the same set of packages. No, you can specify a set of dependencies that need to be installed, either automatically if your application is packaged, or manually if the user is manually installing it. &gt; You also force every project in the environment to use the same versions of everything. Yes. This means you don't need to ship a copy of every library your application uses. Think Stackage. &gt; You also can't develop against devel versions of popular libraries because half your system will be built and linked against the current release, etc. Again, think Stackage. Unless you're building a toy, you probably shouldn't be developing against the devel version of libraries. &gt; At some point, you have to start managing these things on your own. C and C++ tool chains do not help you at all. Yes they do. CMake and pkg-config are good examples.
Reading the comments of this thread I find it fascinating that so many Haskellers have gone down the Rust path, as I have myself. I tend to agree. Haskell is not suitable for situations where you want absolute speed or can't stand the overhead of a GC. For these applications I would also choose Rust over C++, as others have mentioned. I think I can see why Rust is so attractive to Haskell programmers. Rust's traits are basically the same type of abstraction mechanism as typeclasses, it has a strong type system, and focuses (IMO) a lot on pushing different classes of errors to compile time instead of runtime.
We had Well-Typed come and do a week's training for our team. It was a combination of their Introduction to Haskell and Advanced Haskell courses, along with some Time spent on implementing DSLs (which we had requested). Everyone on the team enjoyed the course. I thought it was excellent, and continue to refer to my notes and slides from the course. Edsko de Vries presented it, and his breadth and depth of Haskell knowledge was evident. I really enjoyed his training style; he struck a good balance between instruction and participation. The pace of the course was relentless. We had a fairly mixed group in terms of Haskell knowledge, and towards the end the beginners were a bit lost. This was a calculated decision on our part: being in South Africa made it prohibitively expensive to split the course up, and we decided we'd rather have the more advanced Haskellers take the most from the course, and then train up the others. Conversely, we had one member of our team who had done some of the work they covered in the Advanced Haskell part in previous training they'd done, and he probably got the most out of the training due to his greater familiarity with the work. If it's economically viable, I'd suggest having some time between an introductory level course and a course covering more advanced type system stuff. No matter how well presented the course is, you need time to internalize the basics before you can confidently apply more advanced type system constructs. As to your desire to use Elm as the introductory language... I'm not sure I see the advantage of that, but I guess you didn't really ask for my opinion on it :) Being very much a Haskell company, I'd be interested to see whether We'll-Typed are happy to do such. Tldr; we had Well-Typed come train us, and were very happy with the results, but would have benefitted if we'd had some time between the basics and the deeper material.
I agree. Rust's sum types (fat enums) are an amazingly useful feature that seem to be missing from every other high-performance language.
You do see OS blend C and C++ together, however. The lack of need of an FFI to go from C to C++ means you can write the low level stuff in C while doing the higher level work in C++. 
Most of your assumptions are made on others following good development practices which is great to do in theory but not in practice. &gt; It is if you have a good package manager. So when you are developing your project you base it on a specific distro? Besides projects specific to the distro this is never, ever the case. You shouldn't assume a good package manager when developing your independent code. This is why autoconf and CMake actually check for the existence of a library, and not just assume that the package manager installed what was necessary. &gt; Distributions are not just changes in configuration options. Same kernel, same upstream code base. The major difference between distros is 1) the package manager they use, 2) the patches they apply and 3) how fast they release/close to bleeding edge they are. But this is not the point. The point is that while they are all the same OS, you can't, with complete confidence, just unpack a deb file and toss it into an rpm based distro. You have no idea what each distro does to change their files. Really, the issue is not just distros. Everything has to be cross platforms these days. C++ can be very portable, but without a dependency manager, it is up to the developer to do all the work. As for releasing packages, you can't even do your own deb and rpm. Naming and versioning conventions are formalized but not standardized. Its gotten much better but for a long time there just about every rpm based distro had their own version numbers which made compiling by hand the more viable option. &gt; By and large, they just complain that you are missing a given library, which is the best way to do this. Yes. Aren't we talking about the fact that C++ doesn't have a dependency manager which makes C++ development a pain in the butt? &gt; because API breaking changes are so rare in the C++ world, you don't even need to stick to an LTS version And this is why no dependency manager exists currently, nor will probably ever become mainstream. Still doesn't make development any easier. I have a project using stack on Linux, has my source code and two extra files (cabal and yaml). In a single command I can download, compile and install dependencies for this project. I push that to github and when I get to work I pull the code to my Windows machine. Run the same command and automagically all the same stuff happens. The hours of time saved from having to do that same work in C++, I hate to think about it. &gt; You don't need to, the minor revisions are ABI compatible, True, but also an assumption. I really like the fact that stackage has LTS versions that are *verified* to work together. I've run into problems with python and pip upgrading a package and it breaks my system. stack does a much better job. C++ its all up to the OS package manager (if your OS has one) or the README/INSTALL document and hopefully a version checking autoconf/CMake file. Like you said, ABI shouldn't break and for the most part they don't. Its why no manager has become mainstream for C++. &gt; No they didn't, they just had to update their libopenssl package to the latest version supplied by their distro, which would be patched. It depends on who "they" are. Many companies base their products on distros and when something major like that happens, they will try to get the bleeding edge update from wherever they can to patch something like that. Its not a good reason to say C++ needs a dependency manager, as that would probably have slowed things down. I think the more common use of development versions of libraries are for situations where features are added to a slow release project. Minor revisions released maybe once or twice a year, code tested with a CI. Besides knowing the new feature's ABI is set in stone, there are lots of people who would use a development version of a library in non-toy situations. 
Because of Haskell's lack of ABI stability (this is why `cabal upgrade` doesn't exist, and why dependency resolution in Haskell is so difficult), `stack` is made neccessary (and in fact deals with Haskell's [or perhaps GHC's] deficiencies surprising well). If you've used old versions of `cabal` (without sandboxes), or `pip` (though in my experience, Python doesn't have many of these problems) or `npm` (I still haven't successfully installed a node project), you'll understand why language specific package mangers are not a good idea. With the exception of Haskell, which really needs stack, language-specific package managers cause more trouble than they are worth.
[I’ve very recently started working on something like this.](https://github.com/lexi-lambda/rascal)
With `pip` I've run into the issue but only when globally installing a package. I think the trouble comes when the language-specific package manager does things on a global level. Nothing worse than two apps requiring specific versions of a library rather than `&gt;= version`. I have `npm` running three different web apps on the same machine. Each one has different versions (some major version) of packages. But since they are all localized to the sandbox of the app there is no real issue. I think if you are doing something globally you need to use the OS's package manager. But I think PC-BSD has it right. Putting apps in their own little sandbox, with their on versions of libraries means the developer has extreme control over what versions of libraries are installed. I totally agree with you that language-specific dependency managers are typically more pain than they are worth. But the development time saved by just running `stack build` and all the deps are downloaded and built, so nice. Especially since I do development on a Windows machine when I'm in the office and a Linux machine when I'm at home. I cringe when I have to pull a C++ library source code to compile, only to find out I have to pull and compile another library it depends on. Wish I could restrict all my work to Linux.
Assuming almost zero Haskell knowledge! Haskell is modular and safe, because it is functional and strongly typed. Sometimes these requirements conflict, consider two functions: usualInverse :: Double -&gt; Double usualInverse x = 1 / x safeInverse :: Double -&gt; Maybe Double safeInverse 0 = Nothing safeInverse x = Just (1 / x) usualInverse is troublesome at 0, but safeInverse is annoying to use because it is difficult to compose. Here is a solution: maybeReturn :: a -&gt; Maybe a maybeReturn x = Just x maybeBind :: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b maybeBind Nothing _ = Nothing maybeBind (Just x) g = Just (g x) Now if you have a function inc inc :: Double -&gt; Double inc x = x + 1 You can convert inc into a function which returns a Maybe Double (that always succeeds) using maybeReturn. Now you can "compose" inc with our safeInverse using maybeBind. inc_after_inverse num = maybeBind (safeInverse num) \x -&gt; maybeReturn (inc x) If inverse fails, the entire calculation returns nothing. These functions can be composed, and are type safe. For my example the underlying type for all the outputs and inputs was double, but that isn't required. My example is kinda bad, because that's important. return and bind can be defined for more then Maybe. They are also meaningful for IO. But there is a problem. Maybe and IO aren't types, they are type functions. In an arbitrary polymorphic function, say swap :: (a, b) -&gt; (b, a) You're never going to see a = Maybe. But in our definition of maybeBind: maybeBind :: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b It's absolutely critical the function is polymorphic over the type Maybe operates on. That way when we define the interface for our generic bind: bind :: F a -&gt; (a -&gt; F b) -&gt; F b It's clear it can correctly abstract over the analogous interfaces: maybeBind :: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b IOBind :: IO a -&gt; (a -&gt; IO b) -&gt; IO b Higher Kinded types let you define an interface for type functions in a safe way. Type functions which give a (correct) definition for return and bind are called Monads. Monads are used all the time in Haskell, because they make the language modular, without sacrificing safety. For the real Monad interface, they use the operator (&gt;&gt;=) instead of the word bind. It's name is bind. So inc_after_inverse would be written like: inc_after_inverse num = safeInverse num &gt;&gt;= \x -&gt; return (inc x) Haskell's do notation is just syntax sugar for Monads, here is an example conversion: function value &gt;&gt;= \x -&gt; return x goes to do x &lt;- function value return x The list comprehension syntax is also syntax sugar for Monads. I won't put that down. That was a little long. I hope that made the use of all of the above a little more clear. Feel free to ask questions...Or just google Monads. 
&gt; And can avoid a lot of pointer indirection etc without giving up "polymorphism" Could you tell me how avoiding pointer indirection or lack of polymorphism improves "perf and predictability"?
Yes. You need KindSignatures for that though (and ExplicitForall or RankNTypes).
C++ has much better C++ interop than Haskell.
awesome
Because there isn't a unique definition of Applicative for lists. I was trying to figure out type signatures for functions over higher kinded types, so I could choose which functions to use. So you could do this (in principle): lift' :: (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; ( (a -&gt; b) -&gt; f a -&gt; f b ) -&gt; ( f (a -&gt; b) -&gt; f a -&gt; f b ) -&gt; f c lift' f a b &lt;$&gt; &lt;*&gt; = f &lt;$&gt; a &lt;*&gt; b Not sure I got that right, and it's kind of hairy. But that's the idea. Does that just work automatically? *Goes and checks* 
You need more parentheses around the symbols, but yes, this works. Any typeclass involving higher-kinded types can be translated, via dictionary conversion, into a non-typeclass version, where the methods of the type class are passed in as arguments (as seen above.) 
1) Some pragmas could be done with macros, but they'd be very involved macros as they often hook up to optimization phases. The easiest to macro would probably be INLINE; others scale up in difficulty. 2) For macro programs that want to compute over ASTs, the ability to only have to handle Core can be quite useful. However, developing a macro system that can transparently interoperate between the surface language and a (not directly representable) intermediate syntax is very unusual (you won't see it Racket) 3) Yes. But it's a question of magnitude. A lot of things can be made fast. 4) In the macro world, "referential transparency" is often used to refer to macro hygiene. A macro system can be hygenic or unhygenic; depends on the system. 5) Generic programming puts good use to types; in general, types are a powerful tool for guiding program synthesis. If you want to look more into this, I'd recommend looking at the interaction between Typed Racket and Racket's macro system. 6) Absolutely not!
Just checked it, and you are absolutely right. I don't think I have ever used an inferred type system as rich as Haskells. Thanks for being patient with a newb!
This might interest you: http://www.haskellforall.com/2012/05/scrap-your-type-classes.html?m=1
Any update?
Full disclosure, I'm not the author of that post. But you can use HKT plus record passing to fake classes. 
For what it’s worth, though Typed Racket is *implemented* as a typechecker embedded in the Racket macro system, user-defined macros do not have any access to type information at macro-expansion time. This is because Typed Racket is effectively “one big macro” that wraps every module. The whole module is expanded, including user-defined macros, then the typechecker runs. This means type information doesn’t really have meaning until after user-defined macros are expanded. Some new research in the Racket world might provide an alternative approach to typechecking that actually allows type-directed macros, and in fact it allows much more than that (such as the ability to write new forms that interact with the typechecker directly). The paper, [*Type Systems as Macros*](http://www.ccs.neu.edu/home/stchang/pubs/ckg-popl2017.pdf), is quite interesting if you’re into either type systems or macros, but especially interesting if you’re into both. :) My project I mentioned elsewhere in this thread, [Rascal](https://github.com/lexi-lambda/rascal), is an experiment based on this research.
Right! Forgot about Freer Next time I'll think harder before posting 
As far as I can see, the sentiment in this thread is "Pick Rust over C++, if you don't mind losing C++'s ecosystem". While I agree that Rust has a lot of nice features that make a C++ programmer envious, I don't think the language is strictly better than C++. While Rust's traits are a delight to use, they're much weaker than C++'s templates. You simply don't have higher order traits, so you can't express the equivalent of: template&lt; template &lt;class&gt; class Container&gt; Container&lt;double&gt; f(Container&lt;int&gt;); There's a lot more you can't express in Rust's type system, while you could hack templates to do what you want in C++ at compile time. Many people dismiss C++'s templates as a "glorified preprocessor", but the reality is that they're much more integrated into the "object language" than this statement implies. This is most visible with the recent `constexpr` functions, so you get template instantions depending on `constexpr` functions which in turn depend on other template instantiations. `Boost::Hana` is just one example of how you can reuse code between type-level programming at compile time and value-level programming at run-time. Like for instance if you have a complex type-level data structure and also a runtime representation of it.
That kind of stuff is useful in places, but breaks down as soon as you want to implement e.g. `Map`, which needs guarantees that it gets passed the same `Ord` instance for every `insert`/`delete` or things blow up. Of course, it'd be possible to construct a `Map` object that has an `Ord` instance passed in once and then exposes `insert`/`delete` methods specialised to those and in many languages that'd be a viable way, however, it doesn't really fit Haskell. Suddenly, you'd also want subtype polymorphism and whatnot. Rust is actually the only other (big) language that enforces coherence like that, and thus the only other language which has *real* typeclasses. (But, alas, no higher kinded types (yet)).
Particularly with the work on GHCVM =)
Experience has shown that good type class design always requires laws. What laws does `mul` satisfy for the `Magma` class? And what is the advantage of having a `Magma` class representing both addition and multiplication? I'm trying understand what you are gaining with this machinery.
`std::variant` is better than nothing, but it leaves a huge amount to be desired on the ergonomics front, especially when it comes to pattern matching.
I have upgraded my MacBook to Sierra (maybe too eagerly) roughly as soon as it was available. I had GHC-7.10.3 and GHC-8.0.1 on this machine before the upgrade and both kept working after the upgrade. Can someone tell me what exactly is broken?
there seems to be a proposals for c++20 on pattern matching
No it isn't. https://en.m.wikipedia.org/wiki/GNU_Compiler_Collection
C++ biggest problems are incompatibilities across compilers and areas of still undefined semantics, which the compilers can define their own way. These are being cleaned up. Just recently we wasted time trying to compile something using std=C++11 only to find out we had to use std=gnu++11... what if you have some dependencies which only compile with one and others with the other? Also, I highly recommend watching [CppCon 2016: Herb Sutter “Leak-Freedom in C++... By Default.”](https://m.youtube.com/watch?feature=youtu.be&amp;v=JfmTagWcqoE). These things you get in Haskell for free but I believe it is highly instructive for any Haskell developer to understand that. 
I hope LINE shares some information on writing services in Haskell. For example, what do they use for logging, metrics, service discovery, etc.
Ideally I'd like to see a way to quickly drop down to Rust to implement hot spots in my Haskell codebase that I can't get fast enough in Haskell for whatever reason (usually lack of knowledge), like I can with C now. I think it's technically possible with FFI but it would require some polishing in order to be practical.
Magma has no laws. "In the beginning there was darkness", to quote git. I go back and forth between unifying APIs for addition and multiplication, but I'm liking how all the major laws are connected to magmas. You could almost call it Binary - the underlying need to combine two things of the same type together to form a third thing. This binary operator requirement motivates us then to discover Neutral (and a Unary law!), for example. Without a binary operation (to add, compare or order, say), we have no need for a neutral, for a zero. As a roadmap for thinking about stuff that isn't traditionally thought of as a number (or is a number but lacks good representation in haskell like a matrix), the machinery tends to set you on the right path. (And reduces boiler-plate for free!) So I'd say you are gaining correctness; guided towards what is the most correct representation of the number class heirarchy, according to math. And this will pay off. 
Do whatever makes your code better. And, yes, there's certainly a reason why typeclasses are often explained by dictionary passing, it's nicely operational. OTOH qualified types (which typeclasses are) are worth investigating and understanding on their own, just as HKTs. It's not just about overloading, after all. For example, [the GTK bindings are full of method-less typeclasses to express subtyping relationships.](http://hackage.haskell.org/package/gtk-0.14.5/docs/Graphics-UI-Gtk-Buttons-Button.html) It's highly unidiomatic Haskell, OTOH it is completely idiomatic GTK and works perfectly. Not to mention Oleg-style hackery like implementing RSA on the type level.
To be a bit more precise, a magma (S, *) is closed under * (by definition). &gt; most correct representation of the number class heirarchy, according to math. And this will pay off. How will OP's (abstract) algebraic hierarchy pay off in the context of functional programming? 
Maybe that wasn't the best word to use, but the `Coercable` instances are given for types and their newtypes, so putting it as a constraint to `Distributive` basically means "A type is `Distributive` if it has two newtypes that satisfy some constraints".
Ok, perfectly clear thanks. 
It gets really close! The output here is `("bla-bla ", "6a:f5:37:52:5a:77")`, i.e. an extra space after "bla-bla". I tried replacing `anyChar` with `noneOf " "`, but that won't do. I have a lot to learn, also figuring out what this `Control.Applicative.liftA2` is doing.
Here's another version that's just a tad more efficient, but will have the same problem as the other one: manyTillKeep :: Parser a -&gt; Parser b -&gt; Parser ([a], b) manyTillKeep p end = (\(f, e) -&gt; (f [], e)) &lt;$&gt; go id where go f = do e &lt;- (Left &lt;$&gt; end) &lt;|&gt; (Right &lt;$&gt; p) case e of Left x -&gt; return (f, x) Right x -&gt; go (f . (x:))
You can try liftA2 (,) (manyTill anyChar (try (lookAhead (space *&gt; fingerprint)))) (space *&gt; fingerprint) But actually you will need to write a more precise specification of what can be included in the first substring. About `liftA2`, it takes a "normal" function of two arguments, here `(,)` which builds a tuple from two arguments, and "lift" it inside an applicative, here a parser. Roughly speaking, it transform two parsers to a parser of a tuple of two items which are parser by the previous two parsers. This is close to: do a &lt;- (manyTill anyChar (try (lookAhead fingerprint))) b &lt;- fingerprint return (a, b) (edit): `(*&gt;)` is an application infix operator, it takes two applicatives (here two parsers) and returns the result of the right parser, but only if the left parser then the right one parses. If you are using parsec, I recommend playing a bit with the `Functor` `(&lt;$&gt;)`, `(&lt;$)`, `($&gt;)` and `Applicative` `(&lt;*&gt;)`, `(&lt;*)`, `(*&gt;)` infix operators.
Very, very cool application.
&gt; This is most visible with the recent `constexpr` functions, so you get template instantions depending on `constexpr` functions which in turn depend on other template instantiations. So C++ is one of the first dependently-typed languages?
I mean it may not pay off for everybody, I just put the language in which I think about these things into the programming language I use to program. This isn't really a complete product or anything I'd use to base a real project on right now, it's just one other way of embedding this type of notation into Haskell.
Umm... Hard to say, since type-level programming is untyped in C++ (It does have a very rudimentary "kind system" since you can distinguish between a `class` and an `int`, but you can essentially use a `class` to carry anything). I imagine template metaprogramming as using a scripting language that's very deeply integrated into the compiler. With the arrival of `constexpr`, that scripting language can now also interpret pure C++ functions. Note BTW, even that is very loosely enforced; it's OK for a `constexpr` function to cause side effects, as long as it doesn't do that in one of its compile-time invocations. That's one of the reasons I like C++, it doesn't try to force an opinion, it tries to comply with your code whenever it makes sense in the particular context. It's no replacement for an awesome type system like that of Haskell, but it's still much better than an arbitrary set of opinions forced down your throat in the form of a crippled type system.
 &gt; Optimized idiomatic Haskell code is usually ~2x that of the equivalent C++ algorithm Maybe it's just me, but I find it absolutely amazing that I can write code at the high level of abstraction that Haskell offers, and *only* suffer a 50% performance decrease, compared to code where I have to worry about the underlying hardware implementation (register-based, ie. storing/loading stuff to/from variables). Maybe it's because I come from Python, where 5% the performance of C++ is actually pretty good, which had thoroughly succeeded in convincing me that high level and high performance were mutually exclusive. 
In my experience, the IntelliJ and Eclipse plugins for Haskell pretty much just don't work (though I haven't tested much). If you're willing to learn Emacs, [Intero](https://commercialhaskell.github.io/intero/) offers by far the best Haskell IDE experience I've had. Pretty seamless auto-completion, type-at-point, jump-to-def, etc.. But Emacs is a tough learning curve. It's an old-style terminal based IDE so it's largely based on memorizing key combos and commands, and has very little mouse/menu interaction. If you don't want to use Emacs, there is a mediocre backup option. Running `stack build --file-watch` will have stack build your project every time one of the files changes and show you the results. GHCID is another way of achieving this which should perform faster, though I don't know about its stack integration. It's no auto-complete or anything, but it will at least show you errors as you write, and can maybe offer `Perhaps you meant ...` suggestions. That said, you sound a little over-dependent on IDEs. I agree that they're incredibly nice, but you should be capable of writing code without one.
I actually used this and did a silly post-parsing: removedSpace x = AccessPoint {ssid = init $ ssid x, bssid = bssid x, signal = signal x} Actually, I just had to take `init` on this string while reconstructing the same data structure. This is probably very inefficient, but I'm happy that I now have a working parser!
Anything can be in the first substring, that's the tricky part! And no, I did not make that output :-) The only thing I know is that the fingerprint is trustworthy, so that's why I defined the problem like that. Your suggestion seems really good (thanks for the guidance), I feel like those might be the parts I've been lacking trying to solve the problem!
+1 for Atom. That's pretty reliable and fairly straightforward to set up. As for spacemacs, I'm sure that's great too but I haven't got it working yet.
Nice try, Putin.
And what is a good naming convention to split them off into non-partial selectors?
One I've seen in GHC is: data TyThing = AnId Id | ATyCon TyCon | ... with data Id = Id ... etc. I've also occasionally used a shortened version for the variant; e.g., data ElaboratedPackageOrComponent = ElabPackage ElaboratedPackage | ElabComponent ElaboratedComponent. Sometimes you can name the constructor the same as the type but that gets awkward when the type is non-abstract and you need to give its constructor a name.
And if you're writing multi platform code and can't just choose the compiler version on your own, then you might use it in 2030.
I think that `a -&gt; a -&gt; a` is closed under `a` by definition, right? You can't synthesise a value that is _not_ an `a`, so surely that law holds without the need for class laws?
I thought lambada was just a song that should stay buried in the 90s forever; this is definitely more interesting.
with explicit recursion: {-# LANGUAGE TupleSections #-} import Text.Parsec import Data.List import Data.Char import Text.Parsec.String fingerprint :: Parser String fingerprint = sequence $ intercalate [char ':'] $ replicate 6 [hexDigit , hexDigit] fingerprintLine :: Parser (String, String) fingerprintLine = try (spaces &gt;&gt; fmap ("",) fingerprint) &lt;|&gt; do a &lt;- many space b &lt;- many1 (satisfy (not.isSpace)) (c, d) &lt;- fingerprintLine return (a++b++c, d) This is more efficient than `manyTill anychar fingerprint` because that would try match a fingerprint after every character
You can remove the space without a parser: (liftA2 (,) (fmap (reverse.dropWhile isSpace.reverse) $ manyTill anyChar (try (lookAhead fingerprint))) fingerprint)
Possible to explain it without using too many mathematical terms? 
That doesn't explain why Duff's Device is faster than the equivalent hand unrolled loop at all (in fact it doesn't even claim that). 
I think it boils down to the fact that if you have a bunch of "fragments" of a record that agree on the intersection of their fields, then there is a unique record that they all "come from". This is something that is kind of obviously true about records, and I don't think my post did anything but wrap up this fact in obscure language.
&gt; along with some Time spent on implementing DSLs (which we had requested). It's interesting that you requested this. Why did you want to know how to build Haskell embedded DSLs, and what is your use case? What kind of embeddings were you thinking of? Shallow embeddings, i.e. libraries 100% compiled and executed with the GHC compiler and its RTS, or deep embeddings whose implementation escapes from GHC, e.g. the various accelerate backends?
It's inferred from the context. If loop unrolling was faster, why wouldn't Duff have done it? You don't think the guy knew what he was doing?
The following works fine for me (ghcid+stack): stack exec -- ghcid src/Test.hs --test main
I think I stumbled on this post a while ago but didn't understand a damn word and thus was quite impressed. It makes a lot more sense today and I agree with you that it does nothing but wrap simple things into obscure language to scare beginners off. Thanks for your refreshing honesty :-)
What is a powerset functor? Is it a collection of functors P = {F1 : A -&gt; F A, F2 : A -&gt; F (F A),...} or is does it actually map the category of sets onto their powersets P(X) = 2^X for every set X?
Indeed. If I recall correctly, at the time I was trying to understand sheaves in general by coming up with concrete examples that would be familiar to me as a functional programmer. Looking back, it would have been more effective to take a different perspective! But learning is messy. Now I think that a concrete example is not nearly so useful as a concrete *counterexample*.
&gt; Also, have you considered defining LeftCancellative and RightCancellative (with methods lcancel and rcancel)? They seem like they'd be useful to generalise the idea of stripping prefixes and suffixes from lists (when you have Eq on the elements, at least). https://hackage.haskell.org/package/monoid-subclasses-0.4.3/docs/Data-Monoid-Cancellative.html
Our primary software product is used by companies to track their compliance with various aspects of South African legislation. This requires them to upload a whole lot of data about their business, at which point we munge the data and then run calculations on it. One of the problems with our existing approach is the opacity of the calculations; things just come out the end as a score, without much insight into why the score is what it is. Our EDSL allows us to express the calculations in a very similar format to our previous codebase, but instead of just performing the calculation it builds an AST of the calculation and allows you to interpret it in various ways. Another requirement is to make it easier to throw up new data structures. To get data into &amp; out of the system, we need to support web based CRUD, importing from spreadsheets, exporting to spreadsheets, generating PDF reports, etc. We have (or rather will have, it's not yet complete) an EDSL that allows us to define these data structures such that we can generate all of these from the single definition. In terms of the actual implementation, it's currently a shallow embedding using GHC; more specifically it uses the tagless final approach.
There was a series of blog posts about running rust inside of haskell through the FFI. From haskell's perspective it is the same as dropping to C, so if you already do that it should be fairly familiar. In Rust you just want to declare nomangle and pub extern before your function definition. If the function is accepting a pointer you may have to wrap your initial binding to a variable in an unsafe block. here: http://mgattozzi.github.io/2016/10/01/haskell-rust.html and here is his library for easily exposing rust to haskell: https://github.com/mgattozzi/curryrs
The latter, since elements off P(X) are subsets of X.
The second option.
I've done some more work on this since then. Finishing up some last minute things but you'll also be able to also run Haskell in Rust much easier then what my other blog post had. Right now it's just the primitives that are easy to call across the languages. The next step is making it easy to pass data structures between the two.
I've been working on a Haskell implementation of this stuff. Hopefully this helps: https://github.com/bts/free-transformers
Bit unfair on emacs there, it's quite possible to do most things by pointing and clicking. 
Grep "template" in gcc source directory.
Awesome! This is exactly the kind of stuff I'm looking for. What would be the simplest way to enable persistent storage for this app, using an AWS service? Is there something simpler than PostgreSQL using [postgresql-typed] (https://hackage.haskell.org/package/postgresql-typed)/[opaleye](https://hackage.haskell.org/package/opaleye) to AWS RDS PostgreSQL?
We use RDS, and as for libraries my personal favorites are just plain old [postgresql-simple](https://hackage.haskell.org/package/postgresql-simple), [opaleye](https://hackage.haskell.org/package/opaleye) or [hasql](https://hackage.haskell.org/package/hasql). However, it could arguably (depending on what you need, really) be even simpler to use DynamoDB with something like [amazonka-dynamodb](https://hackage.haskell.org/package/amazonka-dynamodb).
This is spectacular self-awareness, a great TL;DR, *and* not a worthless observation! It's similar to the universal property of pairs that is often brought out for consideration without shame, so extending it in this way is, I think, worth being aware of.
Thought about what you said, it was probably glib to argue typeclasses were less fundamental. There are design spaces where they are a nice solution. I think the "Num" typeclass in particular rubs me the wrong way. But I suspect I'm alone in that :)
My application will be storing many (thousands/millions) of fairly small client state objects (&lt;500 bytes), for the purpose of making a Bitcoin-based payment system. Payments are made sequentially on a per-client basis, so multi-thread access to the same object isn't needed (I lock the object to begin a payment). In addition to fast access to single objects for the purpose of client payments, I also need to be able to - in one atomic operation preferably - scan over all the state objects in sorted order, and collect enough of them to match or exceed a specified value, while updating the fetched items as temporarily inactive. I currently use an [STM DiskMap](https://github.com/runeksvendsen/key-value-store), that I wrote to have something simple for testing, which has the above features. It's basically an STM map as an interface for storing the items in the map on disk as files, with a three-step atomic **mark as busy/disk write/mark as done** update process. It seems to work, but I would prefer to have something well-tested, scalable, and managed by someone else. **EDIT:** I should also note that, even though it's payment-related, no sensitive information is contained in the state objects, as I will have a service that caches the state objects in-memory (acting as a proxy in front of the DB), while on start-up checking that the information is legitimate. I don't want to trust the DB, so all data is checked before going into memory, and after that a discrepancy will be detected.
Maybe he didn't want to repeat any code, maybe he just thought it was an interesting trick. 
https://www.jstatsoft.org/index.php/jss/article/view/v040i08/v40i08.pdf I'm just skimming, but it looks like it relies on a shim/library inside C++ to handle a subset which can interface with other C++ code.
* Fast access to single objects. Using something like DynamoDB requires a HTTP GET per object you want to return. While this overhead can be mitigated somewhat by connection pooling it's not going to be as fast as using RDS with some like `hasql` (`libpq` and the binary postgres protocol). * Atomic operations operating over many previously stored records. For example, a `SELECT/UPDATE` within a SQL transaction. DynamoDB won't give you the same atomic guarantees unless you use some application level locking/versioning, which you have to roll on your own. [AWS Java Reference](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html) and [Haskell version](https://github.com/brendanhay/credentials/blob/8a968cc8df19f89dbf320a91f5866f9172b78ead/credentials/src/Credentials/DynamoDB.hs#L229-L248). * Scalable. It's worth noting that holding open a transaction (and database connection) to incrementally store state objects would be undesirable if these objects are written over a longer time period. So from the first two points at least; for a managed AWS service it seems at a cursory glance you probably want to use `RDS` with either the PostgreSQL or Aurora (MySQL) storage engines. 
Obviously the above is fairly hand wavy. There are possibly strategies such as storing many smaller objects as part of a single DynamoDB record etc that could fit your use case.
In the case of optimization you wouldn't for sure. Haskell in Rust is good if you need access to things Haskell can do better than Rust. It depends on the use case. It's more like, "If you need it here it is"
There are many idiomatic C++es, and they're fundamentally opposed to each other.
Your account made natural join from relational algebra much less unnatural and ad hoc to me, fwiw.
thanks neel! I appreciate it. The learning process involves a lot of messing around, and it is always nice when an onlooker is able to get something from it as well.
Wow, this is really cool! I feel the API doesn't use static typing as well as something like [dlib](http://blog.dlib.net/2016/06/a-clean-c11-deep-learning-api.html?m=1) though. It would be nice to see some higher-level abstractions for stacking layers.
Ah, this is great. Thank you.
Related: https://arxiv.org/abs/0904.2012
Incredible - nice work! Can't wait to try this out...
thanks. saw that today as well. 
I was working on a toy project similar to this. I was going for something like scheme with the type system of Haskell.
I am vaguely familiar with the creation of this. I believe that the focus was to provide a reasonably strong typing, while keeping readability for the first API. If you have an idea how to improve typing without making it an hard to read, PRs are most welcome!
One of the ways to look at `Foldable` is as the `toList` class (or [toFreeMonoid](http://comonad.com/reader/2015/free-monoids-in-haskell/)). In this sense, there is a natural mapping from `Nothing` to `[]` and `Just x` to `[x]`. The [instance for](http://hackage.haskell.org/package/base-4.9.0.0/docs/src/Data.Foldable.html#Foldable) `Either l` is strange, but goes with the spirit that `Left` semantically an error, and then `Foldable` then says "get the result, if any". In a perhaps more controversial way (as are all the proposed `Foldable` laws), since `foldMap f` is considered by many to be a monoid morphism (`foldMap f (a &lt;&gt; b) = foldMap f a &lt;&gt; foldMap f b`), and `Maybe` allows you to lift a `Semigroup` into a `Monoid` by adding `Nothing` as an identity, the instance allows you to map `Semigroups` into `Monoid`s.
Holy cow!
Did you write this?
As the author of [mysql-haskell](https://github.com/winterland1989/mysql-haskell), i'd like to make an Opaleye adapter, but i don't if that's make sense since Opaleye is designed with postgresql in mind. 
An advanced use would be with the `lens` library. Consider the following list of maybe lists: example :: [Maybe [Int]] example = [Just [4], Nothing, Just [], Just [3, 5]] The numbers are buried three levels deep. If we want to get their sum we can do: &gt; foldMapOf (folded . folded . folded) Sum example Sum {getSum = 12} This works because `Maybe` (as well as `[]`) has a `Foldable` instance.
You can also use [`sumOf`](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Fold.html#v:sumOf) &gt;&gt;&gt; sumOf (folded.folded.folded) example 12 which has the types sumOf @Int :: Fold s Int -&gt; (s -&gt; Int) sumOf @Int folded :: Foldable f =&gt; f Int -&gt; Int sumOf @Int (folded @[]) :: [Int] -&gt; Int sumOf @Int (folded @[] . folded @Maybe . folded @[]) :: [Maybe [Int]] -&gt; Int
DISCLAIMER: I haven't used Opaleyes because I HAVE to use mysql for legacy reason. So, what I know of Opaleye is just what I gathered by skimming through the tutorial. Features that I think are missing might just be the result of my ignorance. Having said that, my first impression looking at Opaleye, is the arrow syntax is ugly, as well as the generated SQL and the benefits are not immediately obvious. I know lots of people swear by Opaleye, so it must be worth it. I can't really tell what feature are missing from Opaleye but I can tell you what the [sql-fragment](http://maxigit.github.io/sql-fragment/Database-SQLFragment.html) approach offers which I think Opaleye doesn't. I'm the author of sql-fragment. I'm not trying to sell over Opaleye. sql-fragment is at the moment a proof of concept (even though I'm using in production), and would love seeing some of its idea taken into other more mature package. So please take the time to read the haddock [there](http://maxigit.github.io/sql-fragment/Database-SQLFragment.html). To summarize, the problems sql-fragment solves - reusability : fragment can be composed easily - auto-join : join can be deduced automatically from a static join graph - field (or column ) oriented: projection are not needed. Just select the fields that you need. The basic idea behind a fragment is to store a SQL query with a phantom type representing the type of the row returned by the query. Fragments can be then combined using the `!&amp;!` operator and that's it. It's type safe, generates clean SQL, is backend not dependent and can in theory write any possible sql. Let's take the examples from Opaleye turorial. &gt; personTable :: Table (Column PGText, Column PGInt4, Column PGText) &gt; (Column PGText, Column PGInt4, Column PGText) &gt; personTable = Table "personTable" (p3 ( required "name" &gt; , required "age" &gt; , required "address" )) becomes personName = "person.name: :: SQLFragment '[Text] personAge = "person.age" :: SQLFragment '[Int] '[] personAddress = "person.address :: SQLFragment '[Text] [] Note that sql-fragment is field oriented instead of table oriented. Projections are automatic ghci&gt; toSelect personName SELECT name FROM person ghci&gt; toSelect personAge SELECT name FROM person ghci&gt; toSelect personAddress SELECT name FROM person PersonQuery in Opaleye becomes personQuery :: SQLFragment '[Text, Int, Text] '[] personQuery = personName !&amp;! personAge !&amp;! personAddress ghci&gt; toSelect personQuery SELECT name, age, address FROM person The magic appears when doing `!&amp;!`, all the select clauses are concatenated and the from clause removes duplicate (If you need to join the same table twice in a query, it is possible using aliases). The projection example &gt; nameAge :: Query (Column PGText, Column PGInt4) &gt; nameAge = proc () -&gt; do &gt; (name, age, _) &lt;- personQuery -&lt; () &gt; returnA -&lt; (name, age) becomes nameAge :: SQLFragment '[Text, Int] '[] nameAge = personName !&amp;! personAge ghci&gt;nameAge SELECT name, age FROM person the product example : &gt; personBirthdayProduct :: &gt; Query ((Column PGText, Column PGInt4, Column PGText), BirthdayColumn) &gt; personBirthdayProduct = proc () -&gt; do &gt; personRow &lt;- personQuery -&lt; () &gt; birthdayRow &lt;- birthdayQuery -&lt; () &gt; &gt; returnA -&lt; (personRow, birthdayRow) becomes (if I understood well the example) personBirthdayProduct :: SQLFragment '[Text, Int, Text, Text, BirthdayColumn] '[] personBirthdayProduct = personQuery !&amp;! birthdayQuery ghci&gt; toSelect personBirthdayProduct SELECT person.name, person.age, person.address, birthday.name, birthday.birthday The restriction example &gt; youngPeople :: Query (Column PGText, Column PGInt4, Column PGText) &gt; youngPeople = proc () -&gt; do &gt; row@(_, age, _) &lt;- personQuery -&lt; () &gt; restrict -&lt; age .&lt;= 18 &gt; &gt; returnA -&lt; row becomes youngPeople :: SQLFragment '[Text, Int, Text] '[] youngPeople = personQuery !\! personAge !&lt;=! (18 :: SQLFragment '[Int] '[]) ghci&gt; toSelect youngPeople SELECT name, age, address FROM person WHERE age &lt;= 18 Note we are introducing here 2 new operatos `!&lt;=!` and `!\!. `!&lt;=!` takes two fragment and compare them ghcid&gt; toSelect $ personAge !&lt;=! ( 18:: SQLFragment '[Int] '[]) SELECT (age &lt;= 18) FROM person We can use 18 here because SQLFragment are an instance of Num. However we still need to help the type inference by typing 18 with `SQLFragment '[Int] '[]). This should be not needed now we have injective type family. The `!\!` operator promote a select clause to a where clause ghci&gt; toSelect mempty !\! (personAge &lt;= 18) FROM person WHERE age &lt;= 18 The good things here is we can define a new fragment young = personAge &lt;= 18 and reuse when ever we want ghci&gt; toSelect $ personAge!&amp;! young SELECT name, (age &lt;= 18) FROM person ghci&gt; toSelect $ personQuery !\! young SELECT name FROM person WHERE age &lt;= 18 I might be wrong but I'm not sure the arrow syntax allows this, define and reuse restriction. For those wondering what what the `'[]` in SQLFragment signatures, it's the parameter type. We could define youngerThan :: SQLFragment '[] '[Int] youngerThan = "($1 &lt; ?)" !%! personAge Now `personName !\! youngerThan` will have `SQLFragment `[String] '[Int]` type. It return a row made of one string and expect an Int as parameter. Now the intersesting bit, autojoin. &gt; personAndBirthday :: &gt; Query (Column PGText, Column PGInt4, Column PGText, Column PGDate) &gt; personAndBirthday = proc () -&gt; do &gt; (name, age, address) &lt;- personQuery -&lt; () &gt; birthday &lt;- birthdayQuery -&lt; () &gt; &gt; restrict -&lt; name .== bdName birthday &gt; &gt; returnA -&lt; (name, age, address, bdDay birthday) becomes universe = personName !&lt;-&gt;! birthdayName personAndBirthday :: SQLFragment '[Text, Int, Text, Day] personAndBirthday = personQuery !&amp;! birthdayBirthday At that point, no joined have been made ghcid&gt; toSelect $ personAndBirthday SELECT person.name, age, address, birthday FROM person, birthday However, I've defined a join graph (`universe`) which contains all the possible joins I'm intersted in. Using `autoJoin` will inject the necessary join (and only the necessary one). ghcid&gt; toSelect $ personAndBirthday `autoJoin` universe SELECT person.name, age, address, birthday FROM person JOIN birthday ON (person.name = birthday.name) If joins are not needed, they won't be injected ghcid&gt; toSelect $ personAge `autoJoin` universe SELECT age FROM person ghicd &gt; toSelect $ personAndBirthday `autoJoin` ( personName !&lt;-&gt;! birthdayName &lt;&gt; personName !&lt;-&gt;! bankDetailsName ) SELECT person.name, age, address, birthday FROM person JOIN birthday ON (person.name = birthday.name) You might think that it's dangerous, but in practice, you don't call directly `toSelect` but use a wrapper to execute the fragment. This wrapper expect a fragment AND a join graph. This join graph can be defined once for all even though in practice, to avoid cycles it's better to describe a few corresponding to different context. I'm not saying that sql-fragment is better than Opaleye , it's definitily not (and I don't really have time to work on it at the moment) but it looks like we have similar concepts, and I'm sure you could integrate some its concept (like combinator vs arrow) and autojoin to Opaleye. Any comments or question are welcome (sql-fragment do much more that that). 
Sharing what I've already tried documenting before: https://gist.github.com/saurabhnanda/c2dc9526f9f99e6c10f92120326ee8a6
EDIT: Basically, the main difference I can see between both is Opaleye encodes the row type as a tuple, whereas sql-fragment as a list of type (using DataKinds). The advantage of using a list, is you can do nice things, like appending them "a" :: SQLFragment '[String] '[] !&amp;! "b" SQLFragment '[Int] '[] =&gt; SQLFragment '[String, Int] '[] zip them "a" :: SQLFragment '[String, Int] '[] !==! "b" SQLFragment '[String, Int] '[] =&gt; SQLFragment '[Bool, Bool] '[] and transform them to tuple when needed (see [there](https://github.com/maxigit/sql-fragment-mysql-simple/blob/master/src/Database/SQLFragment/MySQL/Simple/Tuple.hs) for an ugly way to do it).
It's not from Google. It says that in the README.
&gt; The instance for `Either l` is strange, but goes with the spirit that `Left` semantically an error, and then `Foldable` then says "get the result, if any". It's strange just as the `Functor` instance for it is strange. It's a tacked on instance that confuses many beginners and annoys others. The question wouldn't have arisen if `Either` was just a sum type with no instances and some "Result and Error" type would have made sense both for `Functor` and for `Foldable`, like `Maybe`. Similarly, instances of the pair type `(,)` that only apply to the second type are confusing. In particular, taking the length of a pair returns 1 rather than 2 for the same reason, while it would make sense that length wasn't even applicable for this type.
There's really only one correct way to implement `Foldable` on these types (semantically; I'm sure there are countless ways to write effectively the same instances). So it kind of just makes sense that the instances should exist. There's no real reason for them not to. You can occasionally get some use out of them in cases like `fold :: Monoid m =&gt; Maybe m -&gt; m`, where the foldable functions just happen to do what you need without requiring you to write a boilerplate function. It's just kinda useful every now and then.
Wow. I hope I never have that problem. I haven't had it so far working on pretty big projects, but I've also not had to use Happy.
The title threw me off, but I noted that after posting
No, but I am familiar with the project and know people who wrote it.
Sorry, post should have been named a bit better.
How is it under the tensorflow namespace on github if it's not official Google?
I find `forM_` very useful, for example: forM_ option $ \val -&gt; do actions... It executes the do block when `option` is `Just val`
Seems like it might occasionally be useful, but I think it might be easier to just write verified programs in Agda and compile them to Haskell using MAlonzo.
Man if only there were some platform that allowed users to release their projects under their own usernames while still being able to contribute to company projects.
fwiw I use `mapM_` on `Maybe` all the time.
Will check it out later, thank you!
`Foldable` for `Maybe` is useful because it's like a 0 or 1 element list. I like to use `for_` to iterate through it and perform an action only when there's 1 element. for_ maybeThing $ \ thing -&gt; do action thing 
I've been meaning to write this up because I think Opaleye is smart and powerful, but using it is phenomenally difficult for a Haskell beginner. I think the baseline expectation for an API as useful as this is having a step-by-step guide that builds up in complexity. Ending in a simple API covering obvious CRUD operations for your typical schema (an employee one probably suffices). Whilst I can see there's `.lhs` stuff on the Github* project, it just really didn't seem to click for me. I eventually wrestled with the compiler enough and worked out what my problems were, but all I wanted to do was CRUD and filtering. I eventually got C &amp; R working, but I have since winced at working on this area of my codebase recently as working with Opaleye again feels painful and I've other priorities. My other issue with docs was the whole "be explicit" recommendations in the Haddock, but it wasn't at all obvious to me _where_ or _how_ I should be explicit. Please don't take this as a serious criticism of the framework as it's smart and has great potential, I am just venting what I've bottled up. If I can at all help please let me know and I will try and write up my notes in to something legible this week. ** These artefacts seem to give you all the pieces of the puzzle, but no running program: it'd be nice to have everything covered e.g. getting a connection, running a query, extracting results, etc. It's close, but no cigar!
Well if you reduce them enough (I'm aware Agda does not currently do this &lt;__&lt;), all the irrelevant proof stuff should vanish entirely, and it should even be able to do so fairly efficiently if you mark the terms as irrelevant. Also, going crazy with reduction is actually possible in Agda because you are not allowed to write any pathological (non-normalizable) terms. One example where I can clearly see Agda getting big wins here is with their free monad construction: all the terms I played around with normalized (C-c C-n) perfectly well: the interpretation layer vanished entirely and I was left with nothing but primitive IO commands. I honestly didn't expect it to work anywhere near as well as it did -- I hadn't even done anything special to try to trick it into reducing!
^ wrong video
This is good feedback. Ideally there would be a way for me to write literate markdown that includes Haskell examples that are not compiled. I think a better tradeoff is to include the examples, but just forgo the highlight vs having no examples at all.
&gt; Is that a good thing? Or would it be better if their were multiple compilers competing in this area? These aren't mutually exclusive. GHC is awesome, and I'm amazed it does the job it does as well as it does. Of course, having more compilers would surely induce some competition, but that isn't some global decision we can make as a community. The decision to make a new compiler would have to be made by the group that will work on the new compiler! We have limited resources already, and it's clear that the Haskell compiler community has rallied around GHC. I'm certainly not going to try and convince people what compiler they should work on in their free time. &gt; Especially if a key maintainer of GHC passes away (God forbid), for example, right? I'm not a GHC maintainer, but from what I know the [bus factor](https://en.wikipedia.org/wiki/Bus_factor) of GHC is well above 1. I wouldn't worry about this at all. In fact, if we had multiple compilers, then surely the bus factor would be even lower because we are spread out over more compilers! &gt; Do you think Purescript is giving good competition that is propelling both forward? Purescript is great, and it's nice that a Haskell-like language can explore design decisions unburdened from backwards compatibility concerns. So yes, I do think that having Purescript around provides some benefits for GHC. ------------------------ I realize that the word "monopoly" has a really bad connotation, but that needn't be so. A monopoly is not *a priori* a bad thing. Monopolies are bad when they keep out competition through immoral or unfair means. However, if someone has a monopoly simply because the are the best thing in town, that is a win-win for the monopoly and consumers. More specifically, pooling resources together into a single compiler lowers the total overhead versus having multiple compilers. With one compiler, we only need one issue tracker, code base, build system, website, etc. It's also much less burdensome to just add a language extension to GHC than to wait for some committee to agree on it, and then multiple compilers have to implement it. Now, that isn't to say GHC and GHC development is perfect, no improvements can be made, and everyone should stop trying to improve the process. I've seen great posts recently by /u/aseipp and others about how they plan on improving the system for first-time contributors, and about concrete improvements they want to make to GHC to make it faster, use less memory, etc. All I'm saying is having a single compiler is a local optimum we've arrived on as a community given our resources. If we had 10x as many people with the skills and willingness to work on Haskell compilers, and if there were huge flaws in the GHC community, then you bet there would be a new compiler. **However, if the only thing stopping someone from making a new compiler is the fact that GHC is awesome and getting feature parity would be a monumental task, that's hardly a bad thing!** 
An additional question, is there some way to crete a wrapper instance for (,) so that we could define a foldable instance on the first type? 
Easily: newtype Pair a b = Pair { unPair :: (b, a) } instance Functor (Pair a) where fmap f (Pair (a, x)) = Pair (f a, x)
`for_` has already been mentioned as a nice use for Maybe: for_ maybeThing $ \thing -&gt; do print thing The *real* utility is in only asking for what you need from a thing. I have a utility function for displaying account information: printAccountStatus :: Foldable f =&gt; f AccountId -&gt; IO () printAccountStatus accounts = do env &lt;- initializeEnvironment runApp env $ for_ accounts $ \account -&gt; do acctData &lt;- retrieveData account liftIO $ print (formatReport acctData account) Now, when I'm using this function at GHCi, I can call it like: &gt;&gt;&gt; printAccountStatus (Just 1234) Or I can do &gt;&gt;&gt;&gt; printAccountStatus [1,2,3,4] Or, supposing I need to do some set operations that aren't convenient in SQL, then I can pass in a `Set AccountId` and it'd work just great. Now, since I know that all I am doing with this function is iterating through a collection of things and executing an action for each thing, then the type class constraint *prevents* me from doing the Wrong Thing. So my code is more likely to be correct and safe, since I am asking for the minimal interface. Likewise, it's also maximally generic and reusable. tldr: `Maybe` *has* one valid instance of `Foldable`, so we define it, and you're free to use it.
Good question. /u/bgamari has stepped in to help with the docbuilder. He resolved a few but not all issues that could lead to flaky builds (https://github.com/haskell/hackage-server/pull/544) and redeployed sometime Sunday (perhaps after your package upload?). We have a number of tickets on the tracker, but I've been trying to redirect the general complaints that need investigation to https://github.com/haskell/hackage-server/issues/478 if you want to add a comment about your package there. He's also cleared the queue of "failed builds" to try to catch the ones that had issues prior, but given that includes legit failed builds as well as false ones, this will take a while to churn through. Anyway, you can add your package to the ticket for investigation. And indeed, in the meantime, with the latest cabal its _really_ easy to build and upload docs. There's a script for building your own docs that shows how to use this feature here: https://github.com/ekmett/lens/blob/master/scripts/hackage-docs.sh (I also have a PR in flight for hackage server that'll put the now-standard upload-your-docs instructions into the Upload help page so people don't have to ask about this on reddit or get it handed down as folklore :-) )
```any```, ```all```, and ```elem``` are all useful for Maybe. The ```Traversable``` instance for Maybe is also undeniably useful, so ```Foldable``` comes for free regardless of how useful it specifically is. 
Thanks; this generates `&lt;package&gt;-docs.tar.gz` which, when uploaded, gives "Invalid package id '&lt;package&gt;-docs'. The tarball must use the name of the package." . 
The more modular GHC becomes, the less it matters that it's a monopoly. because then there's a gradient of compilers, each of which uses as much of ghc as it can, not just "the best" and "ones that have one interesting feature but are worse along 99 others". ghcjs (and ghcvm?) already have their own backend and runtime, but I don't know how easy swapping out the STG stage (or whatever) was. type checker plug-ins already exist, and one plug-in provides typesafe units. but syntax plugins (which don't exist) would let people play around with different import syntaxes, for example. 
&gt; No. Purescript is not Haskell. I do think PureScript is showing us some good ideas (such as the syntax for extensible effects/records). I think its advantages over Haskell have some people using it instead. That's useful competition.
Does anything like this exist for stack yet?
I disagree. I think if you have haskell-like syntax and semantics (modulo laziness), purity, and typeclasses, you are "a haskell", just like many things can be called "a lisp" or "a scheme". Haskell is not just a language, but its also now a progenitor of a _group_ of languages, which is a testament to its success. Purescript is a good testing ground for ideas that can be ported easily back to GHC and vice-versa.
I don't see any practical use for that definition of "a haskell". French and Spanish are also both "a Latin", and knowing one helps learning the other to some extent, but that's all. What percentage of Hackage can PureScript compile and run correctly? In the present context of GHC monopoly, this "a Haskell" compiler is even less relevant than a conformant Haskell 2010 compiler, which stands no chance of being used. 
&gt; https://gcc.gnu.org/viewcvs/gcc/trunk/gcc/ This is some pretty sweet C code, then: https://gcc.gnu.org/viewcvs/gcc/trunk/gcc/config/i386/bdver1.md?revision=232055&amp;view=markup More seriously, C is a not-quite-proper subset of C++, so the C code in gcc is largely both C and C++ of some gcc-flavored variant. Both the C and C++ standards bodies update their languages to track relevant changes in each other so that this remains a viable way to develop. The Wikipedia page actually has references for the claims being made there; probably the most relevant being https://gcc.gnu.org/gcc-4.8/changes.html which itself links to https://gcc.gnu.org/wiki/cxx-conversion -- it's hard to argue with the project's own release notes and dev wiki. The codebase will most likely never look like what today's C++ developers would write, but it will build with a C++ compiler and will thus be C++ code even if it largely stays valid C as well.
&gt; It converts Haskell source code to the equivalent Agda representation, which maintains the semantics That is a very bold claim.
Isn't `mapM_` just `traverse_` but worse?
Ironic, given how one of the initial purposes of Haskell was to provide a single language standard so we didn't have to deal with a plethora of similar languages... 
If "worse" means "specialised to monad", then yes.
sure, but both are from Foldable, so it's the same point :)
I've discussed modularity with ghc devs and they seem to be very much against it. They prefer the benefits of monolithity over modularity. I disagree. Some examples: * extracting the rts's event loop to its own library, perhaps a fork of libuv: response: event loop needs ghc specific things * Using Haskell src exts: would be difficult to maintain compatibility iirc. * Making ghc a cabal package: would make circular boot deps impossible to support, would require much work And so on. IMO if ghc split into many such little projects it would have a much smaller barrier of entry and components would have saner rebuild times. It'd be worth losing circular build support, or whatever specific benefits ghcrts gets that no other libuv user could possibly want.
[Not yet](https://github.com/commercialhaskell/stack/issues/737).
Very interesting! But the links to the presentations of M Karacsony and D Berenyi are switched
I remember a question about `reflex-dom` library: how to attach some data for XhrRequest that'll be just passed along with a response? Author of the library pointed out there is this function: performRequestsAsync :: (Traversable f, MonadWidget t m) =&gt; Event t (f XhrRequest) -&gt; m (Event t (f XhrResponse)) and the `(,) a` has a `Traversable` instance...
Sure, but I don't find this problematic. You need to perform those reductions at runtime anyway, so is your runtime behavior also pathological? If not, then its pathological-ness depends on the order of reduction, in which case yes, that's something that's fine to break at compile time so you can properly annotate a no-inline to make sure you get the desired reduction order.
Thanks for the info. I'd love to hear more examples. Can you talk more about the issues with decoupling the runtime, and how you would? Can you explain your point about haskell-src-exts? The GHC API exposes a parser that, iirc, is pure, though the syntax tree is messier. I feel like the issue here is with the package, not the compiler. Also, architecture can be more modular without being explicitly packaged (though separate packages is a stronger modularity). Besides, never said it was worth it! Just that there's an alternative to "monopoly versus competition". 
rats i thought someone started a haskell gathering around cheap beer ...
I mean yeah it does kind of mean that. Monomorphic versions of functions can be nice as they can disambiguate which type you want. And some functions can be overly polymorphic and sacrifice parametricity in the process, such as if you call "coerce" after every functionfor some stupid reason, which is bad as then type inference is basically gone. But with that said going from generalized to application to specialized to monad has zero benefits IMO. 
I love `fold` on `Maybe`! It acts as a really good "dwim" function to remove `Maybe`ness, by using a `Monoid` to supply a sensible default when the value is `Nothing`. What on earth does this mean? Well, lets say you have a function that might fail to produce a list - its return type is `Maybe [Int]`. If we precompose a call to this function with `fold` we get a return type of `[Int]`. `Maybe` has been removed, and if the function fails then `Nothing` is turned into `[]`. If the function succeeds, we get back the list as it was. There are plenty of monoids out there, and more often than not when I want to strip off `Maybe`, the underlying type happens to be monoidal and using the neutral element is a sensible choice. For example, `Maybe Text` can be `fold`ed into the text itself, or the empty string. In a similar case, I use `for_`, `traverse_` and others on `Maybe` a lot. Here we either execute an action if we can, or we don't if we have a failure value.
Should I read your warning as "if you need to recurse, use pattern matching"? Or am I reading into this too much?
You read it exactly right. :) I once spent hours finding a speed bug that was due to this exact problem, and the solution was pattern matching.
[The activerecord migration guide](http://edgeguides.rubyonrails.org/active_record_migrations.html), for anyone who hasn't seen it before. I wouldn't necessarily say that it needs to be a DSL, though that's pretty nice. The use of datestamped migrations (as opposed to sequence numbers) means that people can create migrations on different branches without stepping on each others' toes. Most of the time, migrations commute. I know many haskell database libraries do automagic migrations, or fail noisily when that's not possible, but being able to control your migrations and rollbacks seems really valuable when it comes to handling deployments.
I don't think that's true at all: non-commercial projects definitely compete if only for users and contributors. Usually, they also compete less formally, sort of edging each other on (think about comparisons between different features, performance benchmarks and so on). More generally, think about the whole programming language ecosystem: most languages people actually use are not commercial projects at heart (even if they have commercial backing). And yet there is definitely a lot of competition between different languages, and this does lead to improvements—whether languages copying features from each other, or making changes to solve the same problems in different ways.
&gt; I've discussed modularity with ghc devs and they seem to be very much against it. They prefer the benefits of monolithity over modularity. I certainly wouldn't agree with such a blanket statement. Let's look at the examples you cite, &gt; * extracting the rts's event loop to its own library, perhaps a fork of libuv: response: event loop needs ghc specific things I'm not sure which event loop you are referring to. The runtime system itself doesn't have an event loop per se beyond the idle GC loop, which is very little code. Perhaps you mean the IO manager? If so, note that very little of this code can be found in the RTS; the majority of the event manager is [implemented](https://github.com/ghc/ghc/tree/master/libraries/base/GHC/Event) in Haskell in the `base` library. &gt; * Using Haskell src exts: would be difficult to maintain compatibility iirc. There is currently work afoot which originated from the native Template Haskell HSoC project and which at some point may allow us to split out GHC's various representations. However, this is a non-trivial task and it will take time. &gt; * Making ghc a cabal package: would make circular boot deps impossible to support, would require much work. To be clear, `ghc` already is a `cabal` package. It is not, however, on Hackage. Changing this is a possibility that occasionally comes up. /u/hvr has in the past examined the related problem of uploading `base` to Hackage. I think both are a very interesting possibilities, but they are non-trivial tasks and so far no one has put in the effort it would take to even know how much work they would require. Anyways, the point is that no one is opposed to further splitting up GHC. However, it is quite far from the highest priority we have. As long as there are people complaining of compiler bugs, compile-time performance, and runtime performance, I don't see me or any of the other regular contributors working on any of these problems. &gt; IMO if ghc split into many such little projects it would have a much smaller barrier of entry and components would have saner rebuild times. Few GHC contributors complain about rebuild times, which are often between seconds and a few minutes. You just need to be certain you have frozen your stage1 compiler; the build system already has quite a complete dependency graph so you really won't gain much in the area of build time by breaking things up. If anything GHC build times would likely increase since libraries need to linked before any dependencies can be linked.
Thanks for linking that! AR migrations are one of the most important things I think rails got right. I agree that the dsl is not necessary (maybe some user functions to easily specify things such as `drop_table`). It would be interesting if it were possible for to automatically derive the migration between two table states ie: ``` Table '[Col1, Col2] -&gt; Table '[Col1, Col2, Col3] ``` would produce the SQL required to add that missing column `Col3`. 
Maybe, but not nearly as nicely as other options. If I wanted to point and click, I would choose Atom over Emacs in a heartbeat. As a simple example of one thing Atom does better than Emacs with point and click: The file navigator.
Those events (BudHac, ZuriHac, MuniHac, BayHac, etc) are all organized by some kind of Haskell association or foundation? Do anyone knows if there are similar events in Brazil or elsewhere in South America?
It has the downside that many more people just use whatever extensions GHC has.
I would really happily use a Haskell 2010 compiler
You can pry `RankNTypes` and `MultiParamTypeClasses` from my cold dead heads.
Thanks! Don't really understand why this works yet, but you are right 
Is there a way to split a Source for consumption by a Tee? For example, if I want to do something like: filenames -----&gt; checksum --------------&gt; filter ---&gt; print name \ / \--&gt; log --&gt; getContents --/ is that possible through the composition of existing machines? 
FWIW, many of the functions with the `___M` suffix are technically worse practice now. | Old | New | |-|-| | `liftM f a` | `fmap f a` | | `liftM2 f a b` | `f &lt;$&gt; a &lt;*&gt; b` | | `ap f a` | `f &lt;*&gt; a` | | `forM a f` | `for a f` | | `forM_ a f` | `for_ a f` | | `sequence a` | `sequenceA a` | Those all abstract to more general typeclasses than `Monad`, letting you sometimes get more use out of the code you've written. Plus, `Applicative` instances are sometimes more efficient than `ap`. Scrap your `forM`s!
No. To get different behavior of an instance, you're just going to have to use a `newtype`. Or, if you're willing to plunge into lenses, it has all kinds of tools for doing that kind of thing.
If you don't need termination then I'd highly recommend Pipes
Agree, stuff like typed holes (borrowed from Agda I believe) is so useful.
Nice catch! It's fixed now. Thanks.
Thanks. I've used `pipes` and actually prefer `streaming`, but I wasn't asking for an alternative to `machines`; only whether I should be using it at all.
Gotcha!
Well you're ahead of me
Maybe I missed something, but it seems to be missing a README with either rationale or example?
But commented out, with the prefix: "The following are inferred:"
AFAIK, UHC supports Haskell2010 
True. But it is obviously very similar in concept to the other common builders we are familiar with (bytestring, text, blaze, etc.), and very simple.
Sorry, I assumed the title was self-sufficient. The rationale is the same as for the builders of the other array-based types like `Text` or `ByteString`: to let you efficiently and conveniently construct it using the `Monoid` interface.
I'm not convinced by the misspelling argument. If you have a typo and write a non-existent encoding it's a runtime error in any existing library, and if you write a different existing encoding by accident (iso-8859-1 -&gt; iso-8859-2) then this library won't help you either. When does the "spend hours debugging why is program in producing garbage" happen? In general I think typed libraries are great but you have to consider how much time you save in avoiding bugs and how much extra time you spend on satisfying the type checker (and taking longer to build this module). In this case, I think misspelling a hardcoded encoding is rare enough that the extra effort isn't worth it, especially when you want to write code that can operate on arbitrary encodings.
Is there anything that forces the use of boxed vectors, instead of the interface from `Data.Vector.Generic`? That would allow the construction of Unboxed and Storable vectors too.
What is self-evident for some might be very obscure for others.. yes, a readme (and inline comments) go a long way. Examples would be even better
Let's have an Erlang like (pure message passing, node restart-on-fail) language in Haskell as well! 
Thanks for the feedback, it's exactly why I have called this an experiment ;) I coded this mainly to scratch my own encoding itches, so I will soon find out if this library would prove to be useful at all or just a fun weekend exercise :P 
Since monads are applicative functors, I would rather say that monads are unnecessary here. In a way, monads are applicatives on steroids: they allow you to choose the next computation *based on the result of the previous one*. You do not need that extra power here: the call-graph is entirely static, and thus, the Applicative typeclass is sufficient. Plus, since GHC8, you can benefit from the Applicative-Do notation to get the same syntactic pleasantness :-)
It appears `haskell-platform-8.0.1` ships [directory-1.2.6.2](http://hackage.haskell.org/package/directory-1.2.6.2/docs/System-Directory.html), which does not have `getFileSize`. As a workaround you can define `getFileSize` as follows: import System.IO getFileSize :: FilePath -&gt; IO Integer getFileSize f = withBinaryFile f ReadMode hFileSize
Do you have an explicit example?
Thanks! I installed version 1.2.7 using cabal.
[removed]
Why the use of `unsafePerformIO`? The Q monad can perform IO actions, so if you're doing that for Template Haskell, then you can do it safely. 
Do you mean because there is no tail call optimization?
You are dead right, dumb me for not recalling that! Thanks!
Now [it has a Readme](https://github.com/nikita-volkov/vector-builder). Pinging /u/noMotif, /u/ocramz, /u/rainbyte.
Great! Thank you!
Perfect. How does this relate to the emerging Haskell for data science ecosystem? What use cases did you have in mind?
All kinds of use-cases, where you can qualify the task as the following: construct a vector by merging smaller pieces of data (vectors, lists, foldables, singletons) efficiently.
I don't like your personal prelude in the dependences.
It has an example too :) Thanks!
I've done some informal benchmarks of the builder, and by defining: fromListVB = VectorBuilder.build . foldMap VectorBuilder.singleton concatVB = VectorBuilder.build . foldMap VectorBuilder.vector and comparing with the standard `fromList` and `concat`, the standard ones are faster by a factor of 30% (when compiling with -O2, 2x without). Is this overhead expected, /u/nikita-volkov ? But if we try to define the same with `Vector` functions: fromList' = foldMap Vector.singleton concat' = foldMap id the builder-defined ones are around 10x faster. 
While that is often true, there are realistic development scenarios where having compile-time checking of encoding names would be very useful. Especially if the same type-level encoding names can be referenced elsewhere in the program, so that you could prevent wrong encoding errors based on application-specific business logic at compile time.
Isn't [transient](https://hackage.haskell.org/package/transient) also doing something like that on a library level?
Nice! Would have been great if you had touched on `MonadFix`. I thought you were getting there toward the end, but alas, it remains a bit of a mystery to me.
I haven't looked into that deeply, though a sequel might be necessary.
Nope! By nature of the `(&lt;*&gt;) = ap` law, it's illegal to construct a monad that's capable of the things in the OP. We've seen a couple of monads break this law ([I'll tout fraxl again](https://github.com/ElvishJerricco/fraxl)), but only when the breakage is not meant to be observed. In this case, the only reason to break the law is to make that breakage observable; thus, it wouldn't be forgivable. This is kind of why I love applicative parsing. Context sensitive grammars are not often more useful then CFGs, and applicative parsing can offer a wide variety of extra tools and optimizations. Earley parsing, for example, has to be implemented in Applicative, IIRC.
Thanks both of you. You makes posting the link on reddit useful :-)
+1 for the detailed answer and proving me legallly wrong ! PS: Fraxl looks very interesting, I definetely need to look into it.
My students have enough to chew with `Applicative`, I did not want to add the complexity of `Alternative`. Also, what should `empty :: Grammar a` be?
Here's a few more: - s/is a simle EBNF library/is a simple EBNF library - In the applicative redefinition of parseCSVP the type should be `Parser` not `Grammar`: parseCSVP :: Grammar [[String]] - s/a function hat lets us/a function that lets us - s/and a Pareser a/and a Parser a - s/This yiels this not/This yields this not Great post, I would have loved to get this assignment in one of my courses! :-)
https://github.com/Gabriel439/Haskell-Pipes-Library/issues/100 You basically "can't" make the mistake causing the quadratic slowdown with machines. &gt;If a program is properly streaming, the libraries all have the property that the time it takes a given program to run is directly proportional to the length of the input stream. It's a touch more complicated than that.
For my `Grammar a`, I believe there is no good candidate, i.e. there is no neutral element to `orElse` (which would be `(&lt;|&gt;)`).
And then there is https://github.com/lenary/idris-erlang :)
No, the blog post's Grammar type is like the Earley package's Prod type. The monad in Earley observes sharing, it's only a frontend for defining Prods. 
redefine choice as a list, not binary, and empty is `Choice []`. 
But this is changing the type to be less “tight”, as this does not represent any valid `BNF` grammar!
OK, so not lame after all ;-)
Thanks for writing this Saurabh! I definitely plan to address your points.
If the type family is closed, why not make an Enum? With `iconv -l` as its constructors. The value level is simpler than the type level.
This is an accumulating program not a streaming one. But if that's the kind of case you are thinking of then ok. I would think that by now this matter is much better understood, see e.g. these remarks of Tekmo's https://www.reddit.com/r/haskell/comments/2fao00/practical_machines_a_simple_tutorial_for_the/ck7lpcv/ , though even that was a couple of years ago. If you are accumulating a stream into a pure structure, there will of course be a question how you you are constructing it. This is always a question about the structure you are accumulating into, not about the streaming itself.
I don't see a way to give Grammar a Monad instance without violating the `(&lt;*&gt;) = ap` law.
The complaint is completely irrelevant today. Stack comes in the Haskell Platform now, so it's pretty hard for new users to have this problem. I called it lame because at this point in time, his rant is irrelevant, and it was posted on this sub as if it weren't.
Can anyone explain me why bother? From the examples i see that the recursion is still there, explicitly coded. The only difference is that now you refer to a function not by its own name but by f In other words you replace someRecurFunction x = ... with someRecurFunction x = fix (\f b -&gt; ...) x What advantage am i getting out of this? 
Nice. :) Followup question for the subreddit: Why should I ever *use* fix? Certainly not to make code more legible. Possibly for efficiency? But the efficiency appears to rely on laziness-as-magic, which I do not think is a very firm foundation. I could be wrong about those things, though (both whether it is why I should use fix, and whether it is trustworthy).
Just an FYI, you don't actually need `TypeInType` here. I created a PR to demonstrate: https://github.com/adinapoli/iconv-typed/pull/2. You should then be able to clean up all of that CPP stuff :) I've noticed that `TypeInType` tends to fix a lot of other compilation issues, but is often not strictly required.
I can do that tomorrow.
I thought about that, actually don't do PR. It's best to have the comparison benchmarks as independent projects. So can you just publish your project?
(1) `fix` applied to a function `f` of type `(a -&gt; b) -&gt; (a -&gt; b)` allows `f` to call itself recursively by calling its first argument `g`. It's just regular recursion, we're simply using a syntactic trick to hide the recursion inside of `fix`: instead of having `f` call `f` within its body, it calls its argument `g`. The recursion is hidden inside the definition of `fix`. -- | -- &gt;&gt;&gt; fib 4 -- 5 fib :: Int -&gt; Integer fib = fix $ \recur n -&gt; if n &lt; 2 then 1 else recur (n-1) + recur (n-2) (2) `fix` applied to a function `f` of type `(a -&gt; a)` allows `f` to construct a cyclic value of type `a`, one in which some pieces point to itself. For example, we can implement [`cycle`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#v:cycle): -- | -- &gt;&gt;&gt; take 10 (cycle "abc") -- "abcabcabca" cycle :: [a] -&gt; [a] cycle xs = fix $ \loop -&gt; xs ++ loop (3) `fix` applied to a function `f` of type`(a -&gt; m b) -&gt; (a -&gt; m b)` is just like form (1), except `f` and `g` can perform some side-effects. -- | -- &gt;&gt;&gt; fibM 4 -- recursive case -- recursive case -- recursive case -- base case -- base case -- base case -- recursive case -- base case -- base case -- 5 fibM :: Int -&gt; IO Integer fibM = fix $ \recur n -&gt; do if n &lt; 2 then do putStrLn "base case" return 1 else do putStrLn "recursive case" (+) &lt;$&gt; recur (n-1) &lt;*&gt; recur (n-2) (4) `fix` applied to a function `f` of type `m a -&gt; m a` is just like form (3) except the recursive call doesn't take any argument. For example, we can implement [`forever`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Monad.html#v:forever): -- | -- &gt;&gt;&gt; forever (putStrLn "hello!") -- hello! -- hello! -- hello! -- hello! -- hello! -- hello! -- ... forever :: Monad m =&gt; m () -&gt; m a forever act = fix $ \recur -&gt; act &gt;&gt; recur (5) `mfix` applied to a function `f` of type`a -&gt; m a` is just like form (2) except that you can use some monadic effects while constructing your cyclic value. -- | -- &gt;&gt;&gt; take 10 &lt;$&gt; cycleM "abc" -- hello! -- "abcabcabca" cycleM :: [a] -&gt; IO [a] cycleM xs = mfix $ \loop -&gt; do putStrLn "hello!" return (xs ++ loop)
Thanks a lot. This is getting embarrassing. To my defense, this started out as lecture notes with limited reach, not as a public blog post…
Because if you have a function in open recursive form you can mess with the recursion. For instance you can add memoization. http://stackoverflow.com/questions/3208258/memoization-in-haskell
This is very cool but the issue I have is there isn't a way to statically verify that Opaleye agrees with the final, migrated tables. To me that would be the _key_ feature of a haskell migration library, verification of schema (post migration ofc). However, rivet seems to beat other migration libraries completely when it comes to actual usability.
I'm still a beginner at Haskell, but I remember having trouble with this part of the course too. What helped me, was to enable the compiler extension that allows you to write the type of the instance and then following the type. For example, for the Functor instance, you would have the following type: ``` fmap :: (a -&gt; b) -&gt; Parser a -&gt; Parser b ``` You can then see that the function passed as a parameter must modify the element corresponding to the type `a` in the `Parser`. Hope it helps you.
Right, `Grammar` is just `Const BNF`. I did not introduce `Monoid` in my class yet, though.
Alright, I'm lost, still not up to date on all these concepts even if I somewhat managed to get through the article. Any reading material/keywords suggestions? 
To answer my own question, one of the advantages of fix is that it is much more performant than a direct recursion: http://stackoverflow.com/questions/37366222/why-is-this-version-of-fix-more-efficient-in-haskell With direct recursion every call to a function is evaluated again and again. With fix it is evaluated only once in a let statement. Plus fix version can be inlined by compiler, while direct recursion cannot. 
True. At first I had problems seeing how the super-friendly Haskell community could be competitive, but then I realised things like Yesod vs. Snap, Conduit vs. Pipes and so on. Definitely potential for competition!
I think PureScript may succeed in gaining popular adoption where Haskell has failed because it addresses what I consider the most substantial complaints about Haskell which are records and laziness (and some of the lesser complaints, like defaulting to arrays and string types instead of lists, and granular effects instead of IO, generally cleaning up the cruft that Haskell's collected over 25 years), while also retaining important things from Haskell like purity, type-classes and monads.
I think the typical use case was covered near the end, mainly for loops an alternative to a inner `go` function. while :: IO Bool -&gt; IO () -&gt; IO () while cond action = fix $ \next -&gt; do b &lt;- cond if b then action next else return () vs. while :: IO Bool -&gt; IO () -&gt; IO () while cond action = go where go = do b &lt;- cond if b then action go else return () I go back and forth whether it is worth using `fix` for this case but it is how I have seen it used in the wild and how I have used it myself. 
You need to use something like Anthony Cowley's async code for that. IIRC he has some code for broadcast sinks. Out of the box, machines only deals with splitting off more inputs, not multiple outputs.
I've been keeping `machines` running. While it isn't being actively developed, a bunch of folks have built things on top of it quite gainfully, and I've encouraged people to build new functionality in separate packages rather than directly into the `machines` library itself, so it will remain pretty stable. I've started some half-hearted attempts at converting the whole thing to a multicategory-based scheme, to allow gracefully for the use of more inputs, but I haven't quite found an encoding of it that I like.
Just use Atom and be done with it.
I would like to follow with the slides. Any pointers to them ? Thanks 
I gave a talks on fun stuff with `MonadFix`. If you understand German, you might enjoy http://bobkonf.de/2016/breitner-monaden.html (There is a transcript and a video recording available.)
First, we can start with the type of `fmap` : `Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b`. In our case, `f` is `Parser`, so we can write `fmap :: (a -&gt; b) -&gt; Parser a -&gt; Parser b`. So we need to transform the `a` in a `b` using the first argument of fmap. In this case, I highly suggest the use of typed holes : if you write `_`, GHC will tell you the type your are supposed to put on that place. For example: Prelude&gt; (fmap _ ["hello", "world"]) :: [Int] &lt;interactive&gt;:2:7: error: • Found hole: _ :: [Char] -&gt; Int So I will use this approach, first what are we supposed to put on the right side of the `fmap` function? : newtype Parser a = Parser { runParser :: String -&gt; Maybe (a, String) } instance Functor Parser where fmap f p = _ &lt;interactive&gt;:5:42: error: • Found hole: _ :: Parser b Ok, we need a `Parser b`. The only way I know to create a `Parser` is using its constructor `Parser`, which takes one argument: instance Functor Parser where fmap f p = Parser _ &lt;interactive&gt;:6:49: error: • Found hole: _ :: String -&gt; Maybe (b, String) Ok, we need a function `String -&gt; Maybe (b, String)`. This looks close to the function stored inside the `Parser`. However with a `b` instead of an `a`. Well, we will find a solution, and I'm sure we need to wrap the function inside another function. So let's do it: instance Functor Parser where fmap f p = Parser (\s -&gt; _) &lt;interactive&gt;:7:56: error: • Found hole: _ :: Maybe (b, String) We progress, we need a `Maybe (b, String)`. How am I supposed to find this? I can get a `Maybe (a, String)` from `runParser p`, so I will use that: instance Functor Parser where fmap f p = Parser (\s -&gt; let parseResult = runParser p s in _ parseResult) &lt;interactive&gt;:8:91: error: • Found hole: _ :: Maybe (a, String) -&gt; Maybe (b, String) We need a function `Maybe (a, String) -&gt; Maybe (b, String)`. However we know that we have a function `f :: a -&gt; b`, so it is easy to create the function we need: f' :: (a -&gt; b) -&gt; Maybe (a, String) -&gt; Maybe (b, String) f' f Nothing = Nothing f' f (Just (v, s)) = Just (f v, s) We are close to the solution. Lets apply `f'` on our problem: instance Functor Parser where fmap f p = Parser (\s -&gt; let parseResult = runParser p s in f' f parseResult) where f' :: (a -&gt; b) -&gt; Maybe (a, String) -&gt; Maybe (b, String) f' f Nothing = Nothing f' f (Just (v, s)) = Just (f v, s) If I'm not wrong, I found the solution, using only the type indications given by GHC. Now I have a more convoluted solution: instance Functor Parser where fmap f p = Parser ((fmap (onFirst f)) . (runParser p)) where onFirst f (a, b) = (f a, b) How does this work ? We know that `runParser p` is a `String -&gt; Maybe (a, String)`. We want a `String -&gt; Maybe (b, String)` function, so can we compose our function with another function `Maybe (a, String) -&gt; Maybe (b, String)`. `Maybe` is a `Functor`, so `fmap op :: Maybe t -&gt; Maybe t'` if `op :: t -&gt; t'`. So we are interested in a `op` function `:: (a, String) -&gt; (b, String)`. The `onFirst` function I wrote does exactly that.
Yep, that would be great. Though to be honest, since those properties are generally very easy to test (just insert any dummy data), I've not cared too much about statically verifying that opaleye agrees with the actual tables (whether or not migrations are involved).
Does it help if you think in terms of "what a parser is"? It is a function that takes a string and (potentially) extracts a value out of it, accompanied by the unconsumed part of the string. If you have a function **a -&gt; b**, what what is the simplest way to transform a parser that (potentially) returns an **a** to a parser that (potentially) returns a **b**? It shouldn't do anything fancy or too surprising. For example if the function is **id** the parser should remain unchanged. It can seem puzzling because, unlike with containers, the return value "still doesn't exist". But there is a common operation that is able to "transform" values that haven't been produced yet: function composition!
The slides for Gabriel’s talk are on [github](https://github.com/Gabriel439/slides/blob/master/munihac/foldmap.md).
1) `fix` lets you write your function non-recursively, and then you can modify its input/output in some way prior to tying the knot with `fix`. i.e: Exporting the un-fixed un-recursive function is more flexible. 2) In my experience, this same trick, lifted to the type-level is far more useful. For example, if you take the recursive list type and replace the recursion with a type variable and use `Fix` to tie it to recursion -- then you can instead tie the unrecursive list type using a different type constructor to generate a correct `ListT`. A more common example is that of Expression ASTs. Using non-recursive ASTs allow adding useful things like metavariables, annotations, and more instead of directly `Fix`ing the AST. 3) Fix is a useful way to reify recursion. Imagine a context-free grammar parser type constructor. If you make it a monad, it becomes turing complete, and you cannot compile the parser to an efficient parse table. If you make it an Applicative, then you get a context-free language but only if you assume infinite recursion is allowed. Due to referential transparency, an Applicative parser built with recursion will appear infinite and so cannot be compiled to an efficient regular or context-free parser table. If instead, you reify the recursion using an explicit parsing primitive visible to the Parser type/compiler, you can make the recursive parser appear finite. How do you reify the recursion? One way is to use a fix-like construction that lets the user tie the knot, but in a way that's visibly recursive to the parsing compiler.
Well it should be possible to generate the SQL schema from Opaleye, and compare that to the one on the DB server. What I'd like is a way say that applying the migrations makes us agree with Opaleye.
I think you may have misunderstood that StackOverflow post. The post says that one `fix` implementation is more efficient than another, not that `fix` is more efficient than recursion.
Ah, you are correct. Well then even less reason to use fix :) 
Have you looked at [purescript](http://www.purescript.org/)? I'm actually curious if anyone knows the *practical* differences between GHCJS and PureScript.
They each have one unique advantage: * The `Foldl` approach can encode stateful folds without space leaks, whereas the `Fold` approach can't * The `Fold` approach can be parallelized but the `Foldl` approach can't be parallelized So basically it's a stateful vs. parallel tradeoff between the two types
Yes indeed i was mistaken. Oh well, even less reason to use fix :) 
{-# LANGUAGE NegativeSyntax #-}
I have, it and elm look promising. The problem I have with both is the lack of gui-libraries that I could just pick up and use for a project, learning the language as I go. I don't do front-end so when I have to it's more of a hit and run tactic until I can get someone who does do front-end to help. GHCJS is an entirely different beast from either, to the extent that it's hard to grok its true nature. It is not a "js replacement" despite being able to be used as one. It's fairer to say that it makes javascript equivalent to x86_64 from ghc's perspective. It's an entirely different backend for GHC. It is seriously hard to grok how contains, transformers, mtl, bound, lens, Control.Concurrent, etc. can be compiled to JS and just work... but they can and it does. 
 {-# LANGUAGE AplNegate #-} x = ¯3
&gt; wheras having to write (\y -&gt; y-x) instead of just (-x) is 7 more characters and looks more clumsy. This probably doesn't address your issue fully but what about simply using ``` (-) x ```
Purescript is strict, ghcjs is Lazy 
Yeah that was way too hasty. Deleting and reposting in a sec. No, I'll leave it deleted. Sorry for the noise and confusion. The problem with the test as written is that the type of decode result is left as `Value a`, which means the Show instance throws off the expected test result. The test harness wants "[[]]", but gets "Value [Value []]". I'm not going to try to work around that right now.
Ah actually, that doesn't accomplish what I needed, since there's no easy way to have both processes in scope for rejoining, or output a product. splitProd gives me: ProcessT m a r -&gt; ProcessT m b r -&gt; ProcessT m (a, b) r But I need: ProcessT m a x -&gt; ProcessT m a y -&gt; ProcessT m a (x,y) The concurrent capabilities don't really seem required either. I guess I'll ask Anthony anyway :) 
If you look at [`Data.Functor.Compose`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor-Compose.html), you should note that there is this instance: (Foldable f, Foldable g) =&gt; Foldable (Compose f g) Which means that types like `Compose [] Maybe` or `Compose Maybe []` are `Foldable` "for free." This sort of thing can be an useful trick when you're dealing with nested structures—somebody gives you something nasty like a `[Map k (Maybe (Map k' v))]` and you want to fold the `v`s, so you `fold . Compose . Compose . Compose`.
[`InstanceSigs`](https://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html#ghc-flag--XInstanceSigs) is a great extension! Just like top-level type signatures it provides documentation but it also helps me with partially applied type operators instance Functor ((-&gt;) a) where fmap :: (b -&gt; b') -&gt; ((a -&gt; b) -&gt; (a -&gt; b')) fmap = (.) ---- type families[¹](http://www.cse.chalmers.se/~emax/documents/svenningsson2015combining.pdf) class Syntactic a where type Internal a toFunC :: a -&gt; FunC (Internal a) fromFunC :: FunC (Internal a) -&gt; a instance Syntactic (FunC a) where type Internal (FunC a) = a toFunC :: FunC a -&gt; FunC a toFunC = id @(FunC a) fromFunC :: FunC a -&gt; FunC a fromFunC = ... instance (Syntactic a, Syntactic b) =&gt; Syntactic (a, b) where type Internal (a, b) = (Internal a, Internal b) toFunC :: (a, b) -&gt; FunC (Internal a, Internal b) fromFunC :: FunC (Internal a, Internal b) -&gt; (a, b) instance Syntactic a =&gt; Syntactic (Option a) where type Internal (Option a) = (Bool, Internal a) toFunC :: Option a -&gt; FunC (Bool, Internal a) fromFunC :: FunC (Bool, Internal a) -&gt; Option a ---- Sometimes the type function is defined in a separate module or is associated with a superclass[²](https://hackage.haskell.org/package/hask-0/docs/Hask-Functor-Faithful.html) class Functor f =&gt; FullyFaithful f where unfmap :: Cod f (f a) (f b) -&gt; Dom f a b with `InstanceSigs` we don't need to hunt for the definitions of `Cod` and `Dom`, I cannot compute this in my head instance FullyFaithful (-&gt;) where unfmap :: Nat (-&gt;) (-&gt;) ((-&gt;) a) ((-&gt;) a') -&gt; Op (-&gt;) a a' instance FullyFaithful Dict where unfmap :: (Dict a -&gt; Dict a') -&gt; a :- a' instance FullyFaithful (:-&gt;) unfmap :: Nat (:-) (-&gt;) ((:-) a) ((:-) a') -&gt; Op (:-) a a' ---- Same with class Process p where type Co p run :: p -&gt; Co p -&gt; End instance Process p =&gt; Process (In v p) where type Co (In v p) = Out v (Co p) run :: In v p -&gt; Out v (Co p) -&gt; End run (In vp) (Out v p) = run (vp v) p instance Process p =&gt; Process (Out v p) where type Co (Out v p) = In v (Co p) run :: Out v p -&gt; In v (Co p) -&gt; End run (Out v p) (In vp) = run p (vp v) ---- **Edit**: The method type signature can be more polymorphic: &gt; The type signature in the instance declaration must be more polymorphic than (or the same as) the one in the class declaration, instantiated with the instance type. &gt; &gt; — [*Glasgow Haskell Compiler Users Guide*](https://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html#ghc-flag--XInstanceSigs) So in the case of the [`ComonadTrans`](https://hackage.haskell.org/package/comonad-5/docs/Control-Comonad-Trans-Class.html) instance class ComonadTrans t where lower :: Comonad w =&gt; (t w) a -&gt; w a for the [`Day` convolution](https://hackage.haskell.org/package/kan-extensions-5.0.1/docs/Data-Functor-Day.html#t:Day) (don't worry, you don't need to know these concepts), it only requires a `Functor` instance so it is more polymorphic than required instance Comonad f =&gt; ComonadTrans (Day f) where lower :: Functor w =&gt; (Day f w) a -&gt; w a lower (Day fb wc bca) = bca (extract fb) &lt;$&gt; wc so it tells us something extra about the method implementation (it can never be *used* at this type `lower :: (Comonad f, Functor w) =&gt; (Day f w) a -&gt; w a`). A simpler example is class Foo a where f :: a -&gt; Int instance Foo Int where f :: forall a. a -&gt; a f = id @a
&gt; A Parser for Things &gt; &gt; is a function from Strings &gt; &gt; to Lists of Pairs &gt; &gt; of Things and Strings! &gt; &gt; —[Dr. Seuss on Parser Monads](http://www.willamette.edu/~fruehr/haskell/seuss.html)
The.. um.. diseased, feverish corner of my overworked imagination? I've put down the compiler and turned off my brain for the night, hopefully no more mistakes will be made for the next couple hours...
ah i understand your issue now.
Neat! This is a good idea, and I hope it gets picked up and used across appropriate pages...
Typically people use the package `Criterion` for this, there are plenty of tutorials on how to do this. Typically you will package up your code as a library, and in your `.cabal` file add a new executable package that depends on your library and the `criterion` package. Here is a how-to on Criterion: http://www.serpentine.com/criterion/tutorial.html
This is the correct answer. Do not write your own benchmarking suite - by the time you have something stable you'll forget what you were even trying to benchmark!
Is that actually a thing? I can only see a prime.haskell.org page with that name and that's it.
&gt; We can defer the ambiguity resolving at runtime by using `AllowAmbiguousTypes` This is not what `AllowAmbiguousTypes` does. Nothing has moved to run-time. When you're thinking about things that are passed at run-time, here's a useful guide: * type variables bound via `forall` are passed at compile-time (`forall` is an *irrelevant* quantifier) * class dictionaries required via `=&gt;` are passed at run-time (`=&gt;` is a *relevant* quantifier). However, the inliner is tuned to, well, inline them, thus removing the overhead. * function arguments are passed via `-&gt;` at run-time (`-&gt;` is a *relevant* quantifier). Now, before `TypeApplications` and without `AllowAmbiguousTypes` we usually had to pass `Proxy` parameters to specify type variables that can't be inferred otherwise. Those proxies go to the left of `-&gt;`, making them *relevant* (present at run-time). It was really sad, as they are entirely unnecessary. Now we can pass type variables directly, eliminating proxies. Thus, with `AllowAmbiguousTypes` you have *less* things passed at run-time, not more. Dependent Haskell will give us a version of `forall` that requires you to explicitly pass the type variables at call-site, removing this use case for `AllowAmbiguousTypes` (there are still others).
I don't think such a thing exists. I'd also strongly recommend against adopting a style at odds with all other Haskell code, just to avoid a trivial syntax wart. It's not worth it. Bite the bullet and write `subtract x` instead. You'll get used to it.
The latest version of the proposal is at https://github.com/ghc-proposals/ghc-proposals/pull/6 and I really need to find time to push it forward...
1. This was once the case, but I really don't think that's true anymore. It's just as industrial strength as many other popular languages in industry. 2. Compile time issues aren't _that_ bad. The industry put up with far worse from C++ and Scala. 3. [This is a great guide to libraries for common tasks](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md). 4. You'd be surprised how often you're relying on lazy evaluation. It allows a form of compositional programming not possible with strict evaluation. I really like it, despite the fact that there are some theoretical issues with it and it's popular to hate on it. It's quite possible to avoid TH in your codebase (I almost always do). As for historical baggage, it's getting a lot better with the changes from the core libraries committee. There is a cadre of people who are pretty intransigent when it comes to changing stuff, so they have to move slowly here, but it is improving.
2 . If you think that Haskell compile times are bad be aware that the Swift compiler takes ~45m for around 13 total dependencies.
Source? That sounds like a bug
Posts like these always bring up non-strict semantics (laziness), despite that they're the entire point of Haskell existing. Perhaps that fact is too poorly advertised to beginners and "outsiders"? After all, nobody complains that Javascript runs in the web browser, or that C doesn't have a garbage collector. Most other languages have strict semantics, so can't we at least have ONE language that attempts to do something different instead of repeating past mistakes by default? Sorry if my comment is harsh, there's just a lot of straw on that particular camel's back. :)
Looks like you're on Swift 2. Swift 3 has fixed some number of the crippling performance bugs. It's weird because when Swift has a performance bug, it's often some absurd exponential problem that causes things to take multiple minutes that should take less than a second. They're generally trivial fixes though.
This is what happens when you try to guess what an extension does instead of _actually_ reading the proper documentation :P Thanks /u/int_index for the corrections, will amend the blog post accordingly!
Excellent motivating response! And thanks for the tips, duly noted. Just to clarify: I really meant "*a bit* hard to stay motivated", because Haskell just has so much to offer that I really highly value. Just didn't add all that to the post. Haskell *feels* much more right than Elixir for me personally. :)
It sounds like an exponential performance bug. Swift 3 fixed a lot of exponential performance bugs. I'd say your chances are 50/50. There's no reason to be so pessimistic and assume Swift's tooling is just perpetually horrible.
Sorry everyone, I haven't had a chance to look at mvc-update or oHm yet. But I here are the [slides](http://www.slideshare.net/LouisPan3/composable-widgets-with-reactive-pipes) of my talk at fpsyd meetup, that explains the libraries a little bit better.
In my defense, there seem to be "insiders" who also think strictness might be better. :) [Simon Peyton Jones](https://news.ycombinator.com/item?id=1924061) From what I gather he is saying that being not-strict by default is what forced Haskell to become great at pure functional programming. But now that the discoveries have been made, it might be better to be strict by default. What a very research language thing to do! (In a good sense) I've also read similar discussions elsewhere. 
I would no longer recommend wreq as it is not being updated, and thus it is not in stackage nightly. I've ported most of my stuff to http-conduit using the `Simple` API
&gt; but might not have the right semantics or performance for weird `Num` instances. I was hoping you were kidding, but indeed, there are basically [no laws listed for `Num`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#t:Num)! I would have hoped for at least those: * `(+)` is associative and commutative * `(*)` is associative (but not necessarily commutative) * `0 + x = x` * `1 * x = x` * `0 * x = 0 = x * 0` * `x - y = x + negate y` 
Personally, I've started creating Servant definitions for the APIs I consume, and use Servant.Client to get the request functions. Works quite well, because Servant takes take of parsing, and just leaves me with an IO action that takes zero or more arguments, with the response as a return type. Requires you to create data structures that accurately describes the endpoint, though. But my experience in Haskell says this is a good idea regardless. See: * https://github.com/runeksvendsen/bitcoin-clearing-server/blob/master/src/Types/Bitcoin/Fee.hs * https://github.com/runeksvendsen/bitcoin-clearing-server/blob/master/src/Util/Bitcoin/Fee.hs 
Those tests are not really well made IMO. For example, `y_number_huge_exp.json` is marked as OK as long as parsing succeeds. A json pretty-printer I have on my computer returns `[inf]`, which is clearly bad. With `aeson-0.11.2.1`, it seems to decode an incorrect value: &gt; BS.readFile "test_parsing/y_number_huge_exp.json" "[0.4e00669999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999969999999006]" &gt; decode &lt;$&gt; BSL.readFile "test_parsing/y_number_huge_exp.json" :: IO (Maybe [Scientific]) Just [4.0e-30000000995] 
Turns out the exponent is an `Int` in `scientific`, which would explain the problem.
I would recommend `http-client`. Also, `wreq` is built on top on `http-client` and `conduit` is not even an http library.
`http-client` is a very good low-level connection library for HTTP. You can use it directly. But a number of higher-level libraries are built on top of it that you might find more convenient. One of the best is [http-conduit](http://hackage.haskell.org/package/http-conduit). It supports fast, reliable, exception-safe streaming via the [conduit](http://hackage.haskell.org/package/conduit) library. It is only "old" in the sense of mature, stable, and widely used. It is very actively maintained and up to date. Yes there are some dependencies, but not a huge amount; I wouldn't call it "heavy".
Don't forget that `fix f` can be a non-function value. Like `fix (5:)` or `fix $ \fibs -&gt; 1 : 1 : zipWith (+) fibs (tail fibs)`
&gt; Haskell gives the vibe of an academic language with a focus on research with industry use being an afterthought? I've been using Haskell in industry for six and a half years and it's been great and getting better and better all the time. Standard Chartered has been using Haskell for a long time and has millions of lines of Haskell code. Don Stewart recently gave a presentation about how Haskell has been working [very well for them](https://skillsmatter.com/skillscasts/9098-haskell-in-the-large-the-day-to-day-practice-of-using-haskell-to-write-large-systems). &gt; How some would describe Haskell: Packed full of many high abstraction features that are diminishing returns for an ever increasing amount of effort. I would argue that the core set of abstractions used in lots of Haskell code are actually ubiquitous abstractions that are broadly applicable in all kinds of domains. It's a mark against other languages if they don't have them. &gt; Apparently there are compile time issues with GHC. Do GHC compiles take longer than most test suites at Ruby/Python shops? That's the real comparison that should be made here. I'll gladly trade longer compile times for the massive reduction in bugs and long-term maintenance costs. &gt; The library ecosystem might have some problems - What ecosystem doesn't? In my experience the average Haskell job applicant is an overwhelmingly stronger candidate than the equivalent non-Haskell applicant. I'd rather place my bets on an ecosystem created by the former than the latter. &gt; Hackage has quite alot of thesis abandonware This has never been a problem for me. &gt; Hard to tell what is good with little documentation In my experience it's always hard to tell what is good. This is a universal problem that gets worse the larger your community is. In that sense Haskell is better because the community is smaller. Also, Haskell packages without documentation tell you VASTLY more than packages in other languages without documentation because you have types and the types are visible in the haddocks. I'll take this any day of the week over the difficult-to-maintain and not compiler-checked tutorials you get in dynamic languages. &gt; A not insignificant part of the community library makers make use of un-haskell-like runtime exceptions Don't use those libraries. These are one-time costs, and once you figure out which packages you don't want it's easy to avoid them. &gt; Missing excellent libraries in some domains (ex: Databases) This is very short term thinking. The library situation is actually quite good now in a number of domains. But for me the value of Haskell's vastly better maintainability, refactorability, etc far outweighs a few libraries here and there. Note that of course there are situations where the existence of a particular library is the lynchpin to being able to do something. If you are in one of those particular situations then of course Haskell may not be directly usable. However, depending on how much you need to do with said lynchpin package, you may find value in doing your work in Haskell and building a layer that generates code that uses the non-haskell package. &gt; It is old and has quite a lot of baggage (features that are kept for backwards compatibility) But it's objectively way more advanced than any other mainstream viable language out there. &gt; Lazy evaluation can be troublesome more often that it is useful. I started with this view, but eventually came to appreciate the value of lazy evaluation a lot more. I'm not the only one that had this evolution either. Also, in day to day development lazy evaluation is rarely an issue. &gt; Lots of Template Haskell for some libraries, can make it hard to read others code Here the the same response as before. If you don't like it, don't use those libraries.
In the sense that the least-significant bits might be slightly different depending on the order of the operations? Bah, if I'm using floating point numbers, it must be because I don't care about small imprecision errors. So how about: * `(x + y) + z ≈ x + (y + z)` * `(x * y) * z ≈ x * (y * z)` I think the rest can still be equalities, since there's only one operation involved so the order doesn't have an impact on the precision.
A meta-comment: there are disencouraging thoughts about all languages. It is not clear how seriously you take these hearsay-objections you mention, but if you take the mere existence of disencouraging thoughts as intolerable, you are bound either to not pick any language at all or to switch languages every six months -- that is, once you find out that your perfect language isn't. (By the way, one of the great things associated with Haskell's academic background is that both language and community were never driven by hype cycles. Long may it continue.)
Also fun IMO: a `fix` that doesn't require explicit recursion at the term level: newtype Bad a = MkBad (Bad a -&gt; a) selfApp :: Bad a -&gt; a selfApp (MkBad x) = x (MkBad x) fix :: (a -&gt; a) -&gt; a fix f = selfApp $ MkBad $ f . selfApp 
I think the important bit here is *if you do the same kind of sharing in your recursion*. I would tend to write this: cycle xs = xs ++ cycle xs not this: cycle xs = go where go = xs ++ go But if I pretend that recursion does not exist and that I have to use fix, then I'll write this: cycle xs = fix $ \cycle_xs -&gt; xs ++ cycle_xs So using `fix` encourages the more efficient solution by hiding it under a more convenient API.
I have not contacted bos regarding this fork, or the maintainership of configurator. I haven't given that possibility much thought one way or the other. As for source file locations, yes I have been meaning to add filenames. I have not though about adding line and column information, but I'm not opposed. Right now the list of errors/warnings generated from the `MaybeParser`/`ValueParser`/`ListParser` infrastructure is wrapped up into a single error message and tagged with the (Filename, key name as it appears in the file) if a key was found, or a list of key names that were examined if the key was not found. (Though the filename isn't filled in at this time, which I should file an issue about.)
The Servant approach has worked for us fantastically. You have to model the response data, but if the response object is large you only need to model the slice of it that you care about. The bonus here is that now it's easy to work with the data in a type-safe manner Haskell.
Most APIs I've had to call just need a token sent in the request headers, so it's trivial to do this with Servant.Client without any special library. Amazon AWS has a really complicated auth scheme but there's the `amazonka` libraries for that.
It's hard to point to an instant in time when Haskell's wider commercial use "started". It's just been a continual growth. Galois, Inc. started in 1999, so there was *some* commercial activity going a long way back. But I'd say it's been a *lot* more obvious in the last 4 years or so.
I say this as someone who loves Haskell/GHC, has contributed somewhat to the ecosystem but also doesn't evangelize. It's not for everyone and I'd much rather see happy programmers first and Haskell programmers second. So given your concerns I'd recommend giving OCaml a shot.
https://github.com/tensorflow/haskell
The profiling tools will surely accomplish what you want, and will further do it in the context of your application and usage patterns. Why do you not want to use them?
The simplest example is all p = and . map p Without laziness, you have to fuse those operations manually to get short curcuiting. 
For Haskell, I would say this falls under the category of [syntactic heroin](https://wiki.haskell.org/Syntactic_sugar/Cons#Syntactic_heroin). If you look at an expression as simple as `f (-1)`, you wouldn't actually know what it means unless you know what extensions are enabled. This is not the case as it is now. That seems like a lot of mental overhead to impose (which would need to be imposed on *all* users of Haskell) just to avoid typing a few extra characters. If you really want to, you could define your own operator to do it, but I wouldn't really recommend it: (~~) :: Num a =&gt; a -&gt; a -&gt; a (~~) = (-)
But you can do it with iterators in any language.
Yes. For example F# has List module and Seq module https://msdn.microsoft.com/en-us/visualfsharpdocs/conceptual/collections.seq-module-%5Bfsharp%5D https://msdn.microsoft.com/en-us/visualfsharpdocs/conceptual/collections.list-module-%5Bfsharp%5D You can use only Seq and it would be similar to Haskell list. So it isn't a duplication. Haskell just doesn't have List module with strict versions of these functions. And for unification there could be a type class something like ListApi with all these functions in it. 
Yes, thanks! Updated
&gt; the entire point of Haskell existing. Quite an overstatement. What about purity, great abstraction capabilities, powerful type-system? I wouldn't call myself an outsider and I am certain in my stance that laziness was a major mistake.
Swift is the bug :) E.g., http://screencast.com/t/qJXR54Po
Interestingly, we can only define this function because the languages already supports general recursion. If it didn't, then with this single function as a primitive, we would *get* general recursion.
&gt; Because you want to return an iterator, even if you are given something like a list. No, I don't want to return iterator. If I work with lists I want return a list and if I work with interators I want to return an iterator.
https://en.wikipedia.org/wiki/Curry_(programming_language)
Haskell is non-strict because researchers were researching non-strict semantics and came together to define a standard language they could all use to share their work. In retrospect, (according to Simon Peyton Jones) purity is more important than laziness, and laziness has very real cost. https://www.youtube.com/watch?v=06x8Wf2r2Mc http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-retrospective/
Agda uses Normal Order evaluation (similar to lazy, but without sharing), according to [this answer](http://stackoverflow.com/questions/21210569/is-the-evaluation-strategy-of-agda-specified-anywhere) Also, you can simulate laziness in any functional language with thunks. So while Haskell is "lazy by default", it's certainly not the only language that can express laziness. Also, laziness doesn't play well with side-effects, since it's unpredictable what order things will evaluate. The languages you list (scala, F#, Clojure, ML) all have mutable variables, side-effects, etc. Is there really that much room in the design space? I guess there could be an untyped, pure, lazy language, but why?
&gt; And even languages implemented with Haskell (Elm, Oden, Idris, PureScript) are strict. Elm and Purescript both compile to Javascript. Matching the semantics of Javascript has some advantages - you don't need a runtime system, for example.
Just wanted to chime in on this bit: &gt; And even languages implemented with Haskell (Elm, Oden, Idris, PureScript) are strict. Implementation language has absolutely *no* bearing on what the implement*ed* language does wrt. lazy vs. strict (or, most anything, really). Once the compiler has done its job, it's of absolutely *no* consequence what language its implemented in. The rest of the question can probably be answered by the simple phrase: Fear, or lack of existing research proving either the viability/non-viability of combining the two models into one coherent statically-typed model. AFAICT essentially "lazy vs. strict" isn't a property that *itself* composes particularly well, unfortunately[1]. [1] Actually, you can sort of see this in O'Caml which has the `Lazy` module. When you start using it, you start discovering that almost nothing else works with it, so you'll have to reimplement all your own combinators, processing functions, etc. etc. Of course the dream would be if it were possible to **abstract over** strict/lazy, but as I mentioned, I don't think anyone's figured out quite how that would work.
If you don't have lazy evaluation, how do you construct thunks? Is it simply a way of using anonymous functions?
So code repetition...
Yet it does not stop people from calling that a Production language! At the same time a few long disproved myths about Haskell keep making people wary about it. Design, performance, quality, stability - those are all secondary. Marketing is what matters to the crowds.
A thunk can just be a function that takes unit as an argument (i.e., no parameters). You could use more sophisticated types like promises and futures, but a zero-arg function can get the job done. Edit: You might find this interesting: [SRFI-41](http://srfi.schemers.org/srfi-41/srfi-41.html) is a "request for implementation" of a lazy evaluation model called "streams" for the Scheme programming language. Particularly the "pitfalls" and "implementation" sections might help answer some of your questions. (Plus, you'll find a shout-out to Haskell in the references. :))
GHCJS absolutely has a runtime system. [Quite a large one, at that](https://github.com/ghcjs/shims). It emulates a GC (in order to support weak references) and threading, among other things.
I'll ignore fmap -- DeriveFunctor can give you this for free on strictly-positive types like yours. Let's look at apply `:: Parser (a -&gt; b) -&gt; Parser a -&gt; Parser b` First we just write down an implementaion, and fill in what we don't know with wildcards or holes. We know it's a function that takes two arguments: apply _idk_arg1 _idk_arg2 = _idk_result Now, I think I want to look into -- pattern match -- the arguments. Now, the `Parser` type only has one constructor, so we'll still only have one clause, but in general we'd also do case-splitting here. apply (Parser f) (Parser x) = _idk_result The result is also a `Parser`, but maybe we'd have some helper function generate it. I can't think of a good helper right now, so I'll just assume I'm going to use the (only) constructor directly. apply (Parser pf) (Parser px) = Parser _idk_runParser Continuing along this way, I know that `_idk_runParser :: String -&gt; Maybe (b, String)`. Again, if I knew a good helpful function to generate those, I'd use it. But, failing that, I'll need a lambda here. apply (Parser pf) (Parser px) = Parser $ \str -&gt; _idk_parseResult Hmm. Well, that's it for single-constructor expansions. Let's see what we have bound that we might substitute for `_idk_parserResult`. We have `pf :: String -&gt; Maybe (a -&gt; b, String)`, `px :: String -&gt; Maybe (a, String)`, and `str :: String`. Well, I could pass `str` into `pf` or `px`, but which one? Well, what's the semantics of `str`? It is the string I'm trying to parse, and because I'm an English-speaking prat, I think we should parse left-to-right. So, that means that the value from the left argument (`pf`) should be first (Huh... what's next then... ignoring that for now...) to get the input. apply (Parser pf) (Parser px) = Parser $ \str -&gt; _idk_parseResult where firstParse = pf str That doesn't get us very far. But, we do have this new `firstParse` value. It's not a single-constructor type, so maybe we need to decide what to return based on which constructor is used? Let's swizzle things aren't and case-split that value. apply (Parser pf) (Parser px) = Parser $ \str -&gt; case f str of Nothing -&gt; _idk_parseFail Just (f, rest) -&gt; _idk_parseSomething Well, in the `Nothing` case, our first parser failed... so lets just have our combined parser fail too. For the `Just` case, we now have `f :: a -&gt; b`, and `rest :: String`. Ah! I can take `rest` and use that with `px`, and case-split on that result! apply (Parser pf) (Parser px) = Parser $ \str -&gt; case f str of Nothing -&gt; Nothing Just (f, rest) -&gt; case px rest of Nothing -&gt; _idk_parseFail Just (x, final) -&gt; _idk_parseDone Again, if `px` fails, we can fail, too. But, what about `_idk_ParseDone :: Maybe (b, String)`? `pf` didn't fail and `px` didn't fail, so we probably shouldn't fail. Which means we definitely want a `Just` there... apply (Parser pf) (Parser px) = Parser $ \str -&gt; case f str of Nothing -&gt; Nothing Just (f, rest) -&gt; case px rest of Nothing -&gt; Nothing Just (x, final) -&gt; Just (_idk_result, _idk_string) Now, `_idk_result :: b`, but we have `x :: a` and `f :: a -&gt; b` so we can combine them to fill in that. `_idk_string :: String` and we have several of those around: `str`, `rest`, and `final`. We've already used `str` and `rest` so, I'll just put `final` there. That should give me something that type checks. Now, I can open a repl, and play with it, maybe come up with some good tests... If it doesn't do what I expect, then I'll review the choices I made and trying filling in the _idk_ stuff with different things that type check. Oh I'll also need `pure :: a -&gt; Parser a`, but that's even simpler: pure _ = _ pure x = Parser _px pure x = Parser $ \str -&gt; _px pure x = Parser $ \str -&gt; Just (_rx, _str) pure x = Parser $ \str -&gt; Just (x, str) HTH. Using development with typed holes is pretty cool.
/u/absence3 is just responding to the apparent surprise and dismissiveness about non-strictness in Haskell, not claiming it's the most important thing. You can't just use Haskell and pretend that feature doesn't exist. It's in the fabric of what Haskell is.
Servant was a pleasure to use (it's my first time with it). Code: https://github.com/alvare/latexpad
Pretty much agree. Defining a smallest gets you a wider definition of both Eq and zero, trapping many a divide by a small float bugs. 
Part about laziness starts here: [https://youtu.be/06x8Wf2r2Mc?t=13m29s](https://youtu.be/06x8Wf2r2Mc?t=13m29s).
I think it's because computer science education is usually done in strict, object oriented languages.
Doesn't seem to work in Safari but looks nice in Chrome.
I'm using haskakafka(github master), it's updated to support kafka 0.9 too, you may also want to ask the maintainer to upload a new version to hackage, or even better, join and help maintaining.
Personally, I use explicit type signatures always for top level definitions, and only when needed otherwise. Exceptions can be made when an explicit signature greatly increases readability, that is, when the inferred signature isn't intuitive or obvious. I don't speak for the whole community, but this style seems pretty consistent throughout code that I've read. It is also (more or less) the style to which [hlint](https://hackage.haskell.org/package/hlint) conforms.
I think of them as all being in the ML family, so "an ML". But yeah, haskell is the most successful ML
The convention is to always have explicit type signatures at the top level. This way it's perfectly clear what everything does just by reading the source. The question then becomes about what you explicitly type in local scopes. `let` and `where` bindings *tend* to have their types inferred, but there's not much concrete reason for this (there's a bit about scoped type variables that I won't get into). I usually find that if the mental overhead of type inference becomes too much, then the function is becoming too bloated, and those local bindings need to be factored out to the top level, where they'll be given explicit signatures anyway. Ultimately, I find myself more often asking when *not* to use explicit typing. And to me, the answer is: "Whenever the inferred type is unmistakable." A lot of proponents of dynamic typing will tell you that good code always has unmistakable types, which I think is obviously untrue. But there is truth in the idea that *when the type is unmistakable*, you can avoid static typing. And this is where inference really shines. It gives you that benefit of brevity and cleanliness from dynamic typing, while still giving you the safety and power of static typing. TL;DR: Invert the question. It's about when *not* to use explicit types. I think type inference should only be used wherever the code speaks for itself.
&gt; a zero-arg function can get the job done Little harder to do sharing this way, but it can work.
When I'm working on my own code, or before I publish code, I very often omit type signatures from the top level because it can make refactoring and changing values go much quicker. It also ensures necessarily that functions are as general as they can be until I'm absolutely sure that they should be specialized.
Ed's trifecta (and the parsers package) might be a useful place to look if you want good error reporting with source locations.
All top-level definitions for sure, but please, please, also give type signatures for anything but trivial where-bound definitions. As a Haskell consultant I work with other people's code a lot and not having type signatures on where bound variables makes reading other people's code so much harder. As other people have remarked, giving type signatures for where-bound variables isn't very common but I think it really ought to be. 
thanks for `for` and `for_`, didn't know about those.
Every **top-level** signature, few exceptions. I write pattern types for [**pattern synonyms**](https://downloads.haskell.org/%7Eghc/master/users-guide/glasgow_exts.html#ghc-flag--XPatternSynonyms), few exceptions. {-# Language PatternSynonyms #-} pattern Equal :: Ordering pattern Equal = EQ pattern None :: Maybe a pattern None = Nothing pattern Some :: a -&gt; Maybe a pattern Some a = Just a I use signatures for [**instance methods**](https://downloads.haskell.org/%7Eghc/master/users-guide/glasgow_exts.html#ghc-flag--XInstanceSigs) (some [examples](https://www.reddit.com/r/haskell/comments/59id6b/advice_for_a_newbie_on_learning_applicative/d99jlh3/)) except the most repetitive cases (I'm looking at you `Num`): {-# Language InstanceSigs #-} instance Functor (Either a) where fmap :: (b -&gt; b') -&gt; (Either a b -&gt; Either a b') fmap _ (Left a) = Left a fmap f (Right b) = Right (f b) I have no clear rules for **local** `let` and `where` **bindings** but I use them *more* than not (adding weird type signatures in weird places is good way to torment GHC), I am more likely to add a type signature: * if I don't want the most general type GHC could find, visible type application ([`TypeApplications`](https://downloads.haskell.org/%7Eghc/master/users-guide/glasgow_exts.html#ghc-flag--XTypeApplications)) helps here too * the type involves non-trivial polymorphism, constraints * if it hasn't appeared before, but if many bindings have the same type you can line them up: `str1, str2, str3 :: String` * if adding the signature required [`ScopedTypeVariables`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#ghc-flag--XScopedTypeVariables) * if any of the types are existential {-# Language GADTs, ScopedTypeVariables #-} data Ex where Ex :: (Int -&gt; a) -&gt; (a -&gt; String) -&gt; Ex ex :: Ex -&gt; String ex (Ex (fromInt :: Int -&gt; a) toString) = toString a_val where a_val :: a a_val = fromInt 42 * if it's a good day Rarely use signatures in **patterns** as in Ex (fromInt :: Int -&gt; a) toString unless I have to, ideally type application would be usable in patterns ([`#11350`](https://ghc.haskell.org/trac/ghc/ticket/11350)) and we would write something like this instead: Ex @a fromInt toString ---- **Edit**: /u/mmirman makes an excellent point: &gt; When I'm working on my own code, or before I publish code, I very often omit type signatures from the top level because it can make refactoring and changing values go much quicker. It also ensures necessarily that functions are as general as they can be until I'm absolutely sure that they should be specialized.
3 good things about explicit top-level signatures: - What you expect a function to be, and what it actually is are verified by the compiler - When you come back in 2 years, you can get more context quickly - You can make signatures less general, which is useful for using things like phantom types to disallow invalid program states.
Works for me in Safari (10.0.1, on Yosemite).
My take is more that the family tree splintered. Like Haskell used to be "an ML" but then its distinctive features spawned another generation. Languages that are much more "an ML" than "a Haskell" would be e.g. OCaml (of course) but also F#, Ur, and other more purely researchy projects.
Unfortunately though, Idris isn't a total language except in the theorem proving fragment. (http://docs.idris-lang.org/en/latest/tutorial/theorems.html#directives-and-compiler-flags-for-totality). So one has to be resigned to the different semantics of strictness in the programming section proper as far as I know. Not to mention which, even when the evaluation strategies _do_ coincide (over total functions) they can still have drastically different performance characteristics that can lead to inefficiencies if you write programs in the way most natural for performant lazy code. 
Agreed. Often I also don't even _know_ fully the type of my functions at first glance, just how they fit together sort of semi-instinctively -- especially when a variety of typeclass constraints are involved. Putting that into a proper type signature, and especially enumerating the correct minimal set of covering constraints, is something I prefer to ask the compiler once my function is written. But of course I also often develop the other way, "top down" and write down some core data types, then type signatures of my desired functions, and only later write code to flesh them out. And in that case I have the type signature from the get-go because its the first thing I wrote!
Like a grant for writing free software? I agree this is a good idea.
I think it's more about how to add it while keeping the language composable (ie, not having twice the code for everything).
Just noting that if you have a stack based project and intero on emacs, `C-c C-t` will show the signature for you.
Probably he's not talking about implementation in itself (Haskell's non-strict semantics are compiled to strict machine code after all), but how to integrate non-strict semantics in a strict language in a useful way. One example he mentions is if and how it integrates with the type system, and I'm sure there are plenty other subtle issues to consider. I'm not familiar with OCaml or how well the thunk datatypes you mention work in practice, but most Scala code I've seen is entirely strict and doesn't reap the compositional benefits of non-strictness (Scala has opt-in lazy values and call-by-name methods).
&gt; how to integrate non-strict semantics in a strict language in a useful way And that's exactly what I'm talking about too.
Classic material.
The other opinion is that if it isn't crystal clear without a type signature, it should be moved out of the where clause to the top level. I think I prefer that other opinion theoretically, and I strive for it. But in real life, /u/edsko is right.
The kind `*` is not an infix operator for multiplication. It is usually the kind of inhabited types. However, if you turn on the `TypeInType` extension, GHC might stop seeing it as a reserved keyword (I'm not sure about that though). If that doesn't work, try qualifying as well: type SizeOf (Vec n a) = (GHC.TypeLits.*) (SizeOf a) n 
https://en.wikipedia.org/wiki/Clean_(programming_language)
&gt;&gt; how to integrate non-strict semantics in a strict language in a useful way &gt; And that's exactly what I'm talking about too. While explicit thunks are a solution, do they typically lead to the right things being lazy in libraries? Or does code usually end up being overly strict when it doesn't need to be?
Even nowadays it's often relatively poor due to lazy evaluation. I've lost count of the 'why is this thing slower than python' posts I've seen.
How's it compare with kstreams and scala?
*Figure 5.2* should probably be ~$ (seven2Forty . forty2Seven) (28,-2)
swapping DataKinds for TypeInType won't change the error
(*) is a type family defined in GHC.TypeLits https://hackage.haskell.org/package/base-4.9.0.0/docs/GHC-TypeLits.html: -- | Multiplication of type-level naturals. type family (m :: Nat) * (n :: Nat) :: Nat and this even works instance Sized a =&gt; Sized (Vec n a) where type SizeOf (Vec n a) = SizeOf a GHC.TypeLits.* n and this type (:*) a b = (*) a b instance Sized a =&gt; Sized (Vec n a) where type SizeOf (Vec n a) = SizeOf a :* n I'm just curious that only * results in a parse error And switching on TypeInType won't change anything.
There are actually ways of defining `fix` without any recursion newtype Rec a = In { (·) :: Rec a -&gt; a } ($) :: (Rec a -&gt; a) -&gt; (Rec a -&gt; a) -&gt; a f $ g = f (In g) fix :: forall a. (a -&gt; a) -&gt; a fix f = w $ w where w :: Rec a -&gt; a w x = f (x · x) compare this to the [Y-combinator `λf. (λx. f (x x)) (λx. f (x x))`](https://en.wikipedia.org/wiki/Fixed-point_combinator#Fixed_point_combinators_in_lambda_calculus).
Then you throw out composability, because you can't just compose operations that transform lists. You can try to fake it with ad-hoc polymorphism, but then you end up with the types in scala's collections libraries. I'd much rather have laziness.
I smell homework. What are you trying to achieve here?
Do your own homework. This is a very common, very basic problem.
What if I use only iterators. How would it be different from Haskell?
What exactly are you asking? It's very hard to help with the information given. What is the context of your project? Is this a homework question? Is performance a concern? What exactly is allowed or forbidden in the solution?
List processing for integers using bottom up merge sort. Not homework as such, its a section of my FYP. 
OCaml can't abstract over type constructors, so no, we've never tried it.
About compile times: people use typescript and c++ daily, and their compile times are way, way worse. Just split your modules and youll be fine. About elixir: theres no reason why you shouldnt go for both. Haskell combinators are omnipresent in all these dynamic functional languages. And programming with combinators instead of control flow statements is 90% of the fun of Haskell.
You still lose certain things like where statements working cleanly. 
Finding a top-level definition that turns out to only be used in one place is somewhat confusing. Inside a where clause where it's used with a type signature is more clear, IMO.
I think you'll find that different paradigms are better under different circumstances. And there will be tons of uncontrolled variables. This exercise won't be very enlightening.
Might be fun for those playing though!
I would certainly enjoy it =)
&gt; You can find Kafka at practical any public library Good point! I actually need a translation in Haskell, which is pretty rare. 
&gt; `(x + y) + z ≈ x + (y + z)` This doesn't hold if you overflow when computing one side only (think large enough positive `x` and `y`, and `z == -y`). &gt; I think the rest can still be equalities, since there's only one operation involved so the order doesn't have an impact on the precision. `0 + NaN` gives `NaN`, and two `NaN`s aren't equal :) 
I think so. Thanks for the link!
&gt; Sure, or just make [] be a lazy list in the first place. But my point is that if you use it with a strict datatype ever using the strictness-by-default way of doing things you will get piss poor performance. So you would have to make to sure to always use a lazy data type. In Haskell because of pervasive laziness basically every `Foldable` `Functor` will handle that well, as `fmap` is basically never strict, and I don't think `foldr/foldl` is ever strict. &gt; There's no difference between let and where. If you really want to delay the evaluation of something then you can just wrap it in a thunk. I know that, my point is that with function definitions you can no longer really do it declarative style, as you would have to carefully order / thunkify things so that you don't waste computation calculating something you don't need, whereas with pervasive laziness you know that you will be fine.
How would: x = lazy something Make sense, would `lazy` be a keyword? Because if it isn't then isn't the whole point of strictness that `something` would be evaluated before it is even passed in to the `lazy` function. Also that seems annoying to have to put everywhere, as most of the time you don't want to evaluate a `where` clause until / if you hit the relevant guard. Also how would you do things like control flow as functions, it seems like strictness by default would ruin basically all of those, or at least it would require a bunch of extra annotations everywhere. Such as `if' True x _ = x` `if' False _ x = x`.
http://docs.idris-lang.org/en/latest/tutorial/typesfuns.html#laziness lazy : Lazy a -&gt; Lazy a lazy x = x
Ok so I guess: foo !x !y = ... Is the inverse of foo :: Lazy a -&gt; Lazy a -&gt; .. foo x y = ... I still personally think pervasive laziness makes more sense as it seems like any language with default strictness ends up with very few people putting in the effort to make things sufficiently lazy, making basically nothing composable. Because one strict function can just about screw up everything and be a huge pain to adjust. Whereas one lazy function that causes a space leak can usually just be augmented with a `seq` somewhere for a pretty trivial fix.
Oh thanks!
The way I see it, no, you didn't call `kill` in your example... &gt; main = let () = kill world in f world The subexpression `() = kill world` will never be evaluated because it's not only unneeded that the expression should be evaluated, there's literally no way to need to evaluate it. You've bound the result to a constant (frankly, I didn't even know you could do that, and I'll remain skeptical until I check - it seems more like Prolog than Haskell) which means the result isn't bound to any variable that you could reference from elsewhere. All that will be evaluated in that program is `f world`, the rest of the `let` is just redundant clutter that has no relevance to the semantics of that program. That's not "maybe", the semantics of Haskell require that that expression won't be evaluated. As `kill` is a pure function anyway, it probably doesn't matter that much - pure code can duplicate values, so once you have access to the world as a supposedly pure value, you can duplicate it as much as you want then discard all but one of the duplicates (or discard all of them, and use `undefined` as a replacement. It wouldn't work at run-time, but it should fool the compiler - assuming you can get a pure-valued world in the first place. The reason you can't normally duplicate the world in `IO` is because you can't directly get access to it - at least on first sight, the same should apply to discarding the world. Part of my confusion here is no doubt that I only know the bit of Wadlers claim that you've stated here. 
Me too .
What do you expect to learn from this?
Oops - yes, of course. Can you tell I've not used Haskell much recently? OK, so Haskell needs to check the pattern matches (bottom is always a possible alternative) so needs to evaluate the expression. You're right. But if the point of your example wasn't what I thought it was, I'm not sure what the point is. I'm sure you can get that to type-check, but what values do you define `f` and `world` to have? Where do you get a world from, and which `IO` action will take it as an argument? For `f`, the obvious solution would fool the typechecker but wouldn't run - define it as the `undefined` of type `World -&gt; IO ()`. But still, if you're really discarding the actual world, you can't just define that as `undefined` - where do you get it from? And how come there's an apparently separate world to thread through that black box `IO` action where you can't see it? [**EDIT** the above would be a tad clearer if `f world` was replace with just `f`. Having the `world` duplicated so you can reference it there is irrelevant to the discarding by `kill`, but talking about the world threading through an action is a tad more confusing for `World -&gt; IO ()` than for `IO ()`. It's even trivial to find an `f` of the right type that will run - but there's still a `world` appearing from nowhere just to be `kill`ed.] That's still the "I only know the bit of Wadlers claim that you've stated" confusion really, though - Wadlers example assumes you've got access to the world in a pure function somehow, so I can't really blame you for assuming the same. 
&gt; I honestly think it's better to ignore the monad-ness of thunks. Because you can't write the return you really want as a normal function; you'd want it to be a macro or syntax. Hmm, I disagree that that is the "return you want". `return exp` shouldn't have any side effects and running a thunk is a side effect so you genuinely do want `return exp` not to be delayed. Yes, there would need to be syntax for delaying computation. &gt; but that's a bit "unfrinedly", right? Well yeah, it's syntactically a bit awkward. As I say, we don't have good solutions to these things because we've never explored them! But I think they're worth exploring and I'm quite positive about the potential.
Apparently Haskell was suggested for this thesis because it has the potential to represent ambiguity better than other languages. I haven't found in the text how this helped; but I posted it anyway (from my phone, hence no accompanying text at the time) because I thought it a very interesting combination of two of my favourite things: Haskell and Bach :) I've seen couple of combinations of functional programming with music before (Chris Ford's "Functional Composition" talk is a classic) but this is the first time I've seen monads and music together. 
The site appears to be down right now and I only have hard copy up to p40. Can you post a fuller snippet?
Well every other language pretty much is strict. So why try and take away the one remaining language focusing on laziness?
`kill` is sort of like if you'd write main = do putStrLn "hello" kill putStrLn "world" and have `putStrLn "world"` fail because `kill` did not continue passing the world.
So first I'll tackle #3: there are [type defaulting rules](https://www.haskell.org/onlinereport/haskell2010/haskellch4.html#x10-790004.3.4) in certain cases such as let statements and the ghci interpreter that aren't there otherwise. If you type ":t inA" you will see that it shows as type `[Integer]` which is why you were getting the type mismatch errors. It's considered good practice to give all top level definitions an explicit type signature, and one of the benefits is that it stops the inferred type being defaulted incorrectly in the interpreter. For #1, you can bind the result of `process i` to another name using either let or where: **(edit: see end)** check i o = let result = process i in (i, result, i == result) check i o = (i, result, i == result) where result = process i For #2, yes you can of course, but it will require only one use of `&lt;-` as using multiple versions of them will give you all permutations of values taken using that. More clearly, `[(x,y) | x &lt;- [1..3], y &lt;- [1..3]]` is `[ (1,1), (1,2), (1,3), (2,1), ...]` instead of just `[ (1,1), (2,2), (3,3) ]`. You can apply a function in the first half of the comprehension though, so your solution will have the form `[f x | x &lt;- inA]`. I'll leave finding what that `f` should be to you :) Finally, my other thoughts: your code overall seems like it's doing a lot for a simple problem, but really your solution is just `process` on its own; the rest is your testing harness. And `process` is almost as simple as simple can be. Your definition of `checkAll` is not good conceptually though. You're constructing (`:`) a list, not appending two lists (`++`). It is also undefined for the case when the input lists are different lengths. That may seem inevitable with the way you have stored your data, but it may well signal that you need to consider storing your data in some other way where the inputs and outputs are always *paired* (hint, hint) **Edit:** actually, you can use `case` too. `let` and `where` are far more idiomatic though than a case statement with only one irrefutable pattern check i o = case process i of result -&gt; (i, result, i == result) **Edit 2:** removed incorrect thoughts about complexity of checkAll. It's not quadratic but it is still conceptually off.
Can't you encode that with (first-class) modules? I don't even think that's Oleg-only territory. Of course it's very possible this is too clumsy here...
Space leaks *can* be unacceptable correctness problems, just as infinite loops. I know the idea you have in mind, but I'm not sure it's useful. Haskell doesn't need to be oversold, laziness is a trade-off to consider and often an acceptable one.
&gt; I guess there could be an untyped, pure, lazy language, but why? Nix is an untyped, pure, lazy language. But it's very domain specific.
Thank you so much for such a quick and detailed answer! I managed a lot of the things you pointed out. I updated my source on Git. I also understand how to type my data with x = y::Type. I didn't find that yet, thank you for the heads up. Also, I like the `where` syntax, it's readable. I saw that in a couple of samples but didn't catch it until now. I found how to get a list of results with a list comprehension ;) [check i o | (i, o) &lt;- zip inA outB] seems to do the trick, was it what you had in mind? That might even be better than my `checkAll` function. Is there a way to compare performance? One part of your answer that I don't understand is when you say Your definition of checkAll is not good conceptually though. You're constructing (:) a list, not appending two lists (++). I replaced the `++` operator with `:`, it works but I doubt it is what you meant.
In fact, using let always produces a lazy pattern match, so `kill world` would actually not be evaluated. If it were discriminated with `case`, it would be. 
Thank you very much. I see that I can declare types for data like functions, it feels cleaner, I'll do that. Why do I keep my Input and Output data separated? Because in the further problems (of the game Tis-100) I'll have to handle multiples inputs and outputs and the process function will get more complicated.
Excellent. There's no end to how much FP tutorials I will at least read through once. I think my neurons are self-organizing into functors and natural transformations.
Forgive me for being off-topic, but should you be using `sum`, or at least `foldl'`, instead of `foldr`?
This isn't necessarily taking place in haskells type system or semantics
To cover the Idris question here (pure, doesn't target js) I also believe that Edwin Brady just decided to make it strict on a whim given that he'd written a bunch of similar lazy languages before that. I wouldn't quote this though since I think the only source is a tweet of his from a few years ago.
From my point off view, "A monad is just a monoid in the category of endofunctors" (did I even get it right???) could be a literal quote from Kafka's "Das Schloss". And I wouldn't be the least bit surprised if they would discover a lost work from him called "Der Kofunktor".
The Parser Type is along the lines of `a -&gt; [(a, Score)]` where the list can be multiple possible results. Forgot the chapter, but there's a default case where things that can't be labelled are labelled as interesting within a given potential result. Iirc, monadic parsers are more prevalent before the discovery of applicatives in ~2009, and parsers with list results being a common pattern for parser up around '05.
Yes, you probably can.
Got it, that makes sense. I did spot the "interesting" construction (Ch 7.1.1) - I seem to remember some connection with musical phrasing, but can't find the reference. It would be interesting (no pun etc) to see this idea developed on.
One of the dependencies says that it only expects to work with versions of the standard library ("base") up to but not including 4.9, but the version of base which ships with the latest version of ghc is 4.9, so cabal is telling you that it cannot install all of the required dependencies. Try adding the flag `--allow-newer` to tell cabal to try compiling the dependencies anyway. If you now get compilation errors instead of dependency errors, try downgrading to an earlier version of ghc.
For interest, why Haskore and not Euterpea?
For one thing, Euterpea's play didn't work on my system. Secondly, Haskore's docs writes that Euterpea is its successor. If you can solve the former problem, I'll definitely switch to Euterpea. I have it already installed.
Haskore isn't maintained anymore, AFAIK.
Why? :( Then how can I get the same functionality as Haskore with another library? I haven't yet learnt anything about Haskore, so it would be fine with me.
I think that's what Rust does. The compiler checks that each object has either one mutable reference or many immutable references to it at any given time.
Probably because the creator is dead. Although Hackage says that it is still being maintained, but it probably isn't getting major updates.
If you already have Euterpea then you don't need Haskore. The play not working is most likely something to do with your MIDI device. If you are on linux you need PortMidi installed. If you are on windows try VirtualMIDISynth with a SoundFont. I think you can also explicitly set which Midi Synth Euterpea should use. (If I remember correctly, it's been a while since I've used it) The nice thing about Euterpea is that there is a book that covers algorithmic composition with it - [Haskell School of Music](http://www.cs.yale.edu/homes/hudak/Papers/HSoM.pdf)
Rust has been recommended to me before, but I've not looked at that yet either, though I certainly believe you're right. Of course because Rust has language support designed to do the job, the safety is I assume guaranteed, rather than requiring the developer to reliably follow conventions such as respecting ownership. 
Any replacements you recommend?
&gt;If you are using a complicated Music value, first test something simple like play $ c 4 qn. Check that audio is working on your computer (that sound isn’t muted, etc.). Try using the devices function to see what MIDI output devices are listed. You may need to use playDev instead of play. From [Euterpea - Setting up MIDI](http://www.euterpea.com/euterpea/setting-up-midi/). Hope it helps
Thank you very much. I didn't know about `curry` and `uncurry`. It seems to rearrange function call and parameters, I'm still a bit confused on how it works. Regarding my specific exercise, it's specified that in and out will have the same length. Also, the up-coming problems will have different set of inputs and outputs combinations (2 in 1 out, or 2in 2 outs etc...). Can`uncurry` be useful in those cases as well?
As always, Simon is an amazing talker. I have seen him plenty of times, and I know he's great, but I still get surprised every time I see him. Yesterday, my colleagues and I were actually discussing what happened to Data-Parallel Haskell, as it seems to have died out these days, despite appearing quite good in the papers. Was it simply too ambitious? (My own, biased, suspicion is that the full flattening transform doesn't work too well in practice.)
linked for the lazy: http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/NdpSlides.pdf 
Regarding the `async` package, `mapConcurrently` fits the bill pretty well here: parGetSizes :: FilePath -&gt; [FilePath] -&gt; IO [Integer] parGetSizes prefixPath filesOrDirs = mapConcurrently (getSize prefixPath) filesOrDirs This is an example of how great the async package is-- concurrency with functions as easy to use as most of the Control.Monad module. edit: Also as someone mentioned elsewhere in the comments, using `foldl'` would definitely speed this up a bit because both `foldr` and `foldl` are pretty slow for reasons explained [here](https://wiki.haskell.org/Foldr_Foldl_Foldl').
While it's still relevant, DPH is kinda unmaintained, unless I'm mistaken, so a `(2008)` in the title would be good.
Speaking from an Haskell newbie to another Haskell newbie, it seems that what you've is a tree structure representing the heap. The root'll simply be the first element. Hence, &gt; getRoot :: Heap a -&gt; Maybe a &gt; getRoot Tip = Nothing &gt; getRoot (HeapNode a _ _) = Just a For the other functions I suppose, you'd apply a similar pattern in a recursive fashion to process the nodes. For instance, to convert it to a list you can use: &gt;getList :: Heap a -&gt; [a] &gt;getList Tip = [] &gt;getList (HeapNode a l r) = a : (getList l) ++ (getList r) 
Types defined with the ``data`` keyword are often called algebraic types. I'll show you an example with a different algebraic type. Here: data List a = Nil | Cons a (List a) This definition is more or less how regular Haskell lists are defined. You can use ``case`` to inspect the structure of an algebraic type: size :: List a -&gt; Int size xs = case xs of Nil -&gt; 0 Cons head tail -&gt; 1 + size tail Similarly, to inspect the structure of a ``Heap a``, we can write case heap of Tip -&gt; (do a thing) HeapNode root leftSubheap rightSubheap -&gt; (do another thing) In the second branch of this case, you have access to the values ``root``, ``leftSubheap`` and ``rightSubheap``.
It makes sense to me to enforce use at most once in order to get those nice properties (pure interface/imperative implementation/no GC) but do you really need to enforce use at *least* once to get these properties?
Yeah, it's quite sad that it didn't quite seem to make it to a state of production readiness. A bunch of hard work was put in. GHC's new system for generics, and I think even some of the type families stuff fell out of it as side-products, but the work on DPH itself seemingly ran out of steam somewhere. I don't think there was any sort of conclusion that it wasn't going to work, I believe it's just that there were a lot of details involved in making it interact well with all the other stuff in GHC. Hopefully someday someone will return to it.
I suggest you install `hlint` and use it regularly. It will help you write more idiomatic code. 
Páli Gábor János kindly provides GHC binaries distributions for FreeBSD (and has done so for many years; thank you Páli!) so at least this shouldn't be a problem. It's been many years since I've laid foot in BSD land but I don't see any reason why Haskell would be trickier on FreeBSD than any other language.
There's a function `zipWith` you might use here, which would make your example zipWith check inA outB
Thank you. I'm glad to see there is a single place to get an overview of what's being discussed.
Yea but that's not really the kind of solution I'm pining for. I wanna see an abstraction that's higher order than "strict vs. lazy" which dictates when things will behave strictly or lazily. I think codata/corecursion might be on the right path, though I don't know that they take it as far as I want
Haskell sorta conflates these things, too, though. We say this: mappend1, mappend2 :: Foo -&gt; Foo -&gt; Foo mempty1, mempty2 :: Foo -- "Foo is a monoid." instance Monoid Foo where mappend = mappend1 mempty = mempty1 -- "Foo, uh, wearing a fake mustache is a monoid." newtype Foo2 = Foo2 Foo instance Monoid Foo2 where mappend (Foo2 a) (Foo2 b) = Foo2 (mappend2 a b) mempty = Foo2 mempty2 Rather than this: -- "(Foo, mappend1) is a monoid with identity mempty1." instance Monoid Foo (mappend1) (mempty1) instance Monoid Foo (mappend2) (mempty2) (For various valid reasons.) Functors are also somewhat special here in that there is only one law-abiding instance. 
`logBase :: Floating a =&gt; a -&gt; a -&gt; a`, so both arguments have to be of the same type and of a type that has an instance for `Floating`, which `Integer` does not. `2 :: Num a =&gt; a` on the other hand can be typed as `2 :: Floating a =&gt; a`, since `Num` is a requirement for `Floating`. Furthermore you want to convert it back to an `Integer`, which will either involve rounding or truncating (in some way). Have a look at: http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#t:RealFrac
Have you had a look at [`criterion`](http://www.serpentine.com/criterion/) at all? 
Yes, sure. The standard counter (the cpu clock) is a coarse grain, so you can't get at the quicker loops. The same register I use is available somewhere in criterion I think, but I didn't need the rest of the library.
When it says first impressions it really means first impressions. Too shallow a dive to have an informed opinion. 
That is obviously not the point. Don't be pedantic.
&gt;That worked as expected, until I tried to call the previously defined function `square` on another line. Is this just an interpreter thing or is there more work involved in calling functions in the real Haskell world? There isn't more work. It is just that definitions in `let ... in` have local scope, and so the `square` in `let square x = x * x in square 3` doesn't exist anywhere outside the `square 3` expression. The tutorial kind of glosses over that. &gt;Another lesson bites the dust, and except for some slight, latent ‘suspicion’ about `(+1)` and `(&gt;5)` being functions (I’m always expecting labels/names like square out of habit) I feel good so far. Your suspicion is correct. What makes it nice is that it works uniformly for all operators (with the single exception of prefix minus, which has its usual meaning), and you can put the value in either side of them (so you can have `(1+)`, `(5&gt;)` or even `(5:)`). &gt;Right up until I tried `let (a,b) = (10,12) in map (*2)` I felt I really had it all figured out. I thought: let’s try mapping ‘multiplying by two’ over `(a,b)`. Two subtleties involved here. Firstly, `map` only works for lists. There is a generalised version of it called `fmap` which works with more structures, but it still can't modify both elements of a pair because they might have different types (to see the problem, consider what should happen if you could write `map (*2) (10, "orange")` with the meaning you wanted it to have). There is an easily available module from the core libraries which lets you write `let (a,b) = (10, 12) in bimap (*2) (*2) (a,b)`. `bimap` takes two functions, one for each component of the pair -- there are also `first` and `second`, which only modify one of the components. (If you are curious, the module has the fancy name of `Data.Bifunctor`)
Hum, Learning Haskell for Great Good is a pretty flimsy book that doesn't really have that many exercises. It didn't help me, so be wary. If you want an alternative, there's the information you can get from the freenode's channel [#haskell-beginners](http://webchat.freenode.net/) . Which lists the [haskell book](http://haskellbook.com/) [If not there are some problem sets in this small post in github.](https://github.com/bitemyapp/learnhaskell)
Looking at the linear regression and ignoring what it says about the standard deviation can itself be enlightening though. Yes, fast calculations can fluctuate due to system load or other factors but the actual point spread doesn't lie. Simply run it with output to a html file and look at the point graph yourself.
What? That is obviously the only point. First impressions are very valuable!
In contraposition, I started with LYAH and it was great for me. I had a project in hand to do, so I already had the practice material.
For me it was great when I already had some experience to get a bit deeper into it and understand things better.
They haven't updated the docs — you can just `pkg install stack` now :-)
Try this: let numbers = [1,1,9,9,2,3,8,7] in [(x,y) | x &lt;- numbers, y &lt;- numbers, x+y==10] This works like nested foreach loops, a bit like you might have done it in Python. There are a couple of different ways of doing the same thing without list comprehension syntax but I think it works well in this case. 
Tim, thanks so much for taking the trouble to share your experience with us in so much detail. It's helpful and highly appreciated.
At this stage don't worry about two passes vs. one pass. They're both `O(n)`. If you get down to the level of constant factors, the intuition needed for optimizing Haskell is quite different than for the imperative languages you are used to. You'll get to it, but for now I recommend focusing on the basics.
This is exactly the same logic I had. But yours is written much cleaner. I have done alot of projects in functional programming languages, but none of them ML/Haskell so syntax still gets to me in longer functions like this. Thank you very much!
well I would want two (1,9) and not one. And there is real no point why, its just an exercise problem that asked for that type of output, both outputs could make sense in certain contexts :D
What's the rationale for this reasoning?
&gt; But with your representation, we lose that completely. Not completely, only directly. You can still "interpret" this free monad into an initial encoding and recover all of that. But then you're back at the same problem pointed out at the start of this post.
&gt; I could see why you might want to return one (1,9), six makes logical sense too. But why two of them? I'm obviously missing the point. Because we want each item from the input list to appear at most once in the output list. There are two 1s and three 9s, so if we make pairs out of them we'll have 2 pairs.
This was just an example, I'm currently trying to generate low level code from a high level composable description and using SizeOf to know how much space i need for stuff, and because () contains no information besides its type i can safely remove it at runtime. To be a little more concrete I've written an Brainfuck Interpreter and tried to write code for it and it was quite inconvienient, so I wrote some code that generates Brainfuck code and because i wanted my code not to overwrite it's own stack I need to align the pieces, so that everything has enough space, that's what i need the Size information for. And a `Vec n a` is just `n` times an `a` on the stack ;) This is all for a programming contest from our University... and i had some free time, but i start seeing this exercise useful for other stuff now too, I might start to generate some other code then brainfuck using this, I currently thought of eBPF filters for the kernel.... or type safe glsl or something... 
Don't forget to "pkg install gmake" before doing "stack setup". Have fun!
actually on further inspection, can you explain how the M.insertWith works? if the key dosnt exist it just adds it. However if it does exist it adds one to the old value of the associated value, but in this case the old value should be Nothing right? so 1 + Nothing which is a type error, where am I wrong? EDIT: nevermind, I finally got it, the update function removes Nothing results from the map
"Simply pattern matching". This id the answer I was looking for. I was confused until I remeber about pattern matching in Haskell, which make it extremely easy to manipulate Heap in this case. I have another question about Functor. So I have to implement a function that changes the Bill of the Heap Node heapMap (Bill -&gt; Bill) -&gt; Heap Patient -&gt; Heap Patient I was able to create a helper function that change the Bill in tuple (Name, Priority, Bill) but Im not sure if it helps.
The tiny amount of extra implied documentation that you get from the function being syntactically located inside that where clause is not worth what you lose: the ability to run the function easily in ghci, refer to it in testing code, and avoid massive monolithic function bodies.
Wow, that went really well. It seems like by the end you had a handle on pretty much everything the tutorial threw at you (except maybe for how let...in construct works. the whole construct is an expression evaluating the part after the "in" with the variables before the "in" in scope) Good job :)
Here's a slightly altered version that looks up `x2` only once (my previous version looks it up twice, to see if it's there and then to decrement it): pairsOf :: (Num a, Ord a) =&gt; a -&gt; [a] -&gt; [(a, a)] pairsOf s = go M.empty where go m [] = [] go m (x1 : xs) = let x2 = s - x1 in case M.alterF (fmap decr) x2 m of Just m' -&gt; (x2, x1) : go m' xs Nothing -&gt; go (M.insertWith (+) x1 1 m) xs decr 1 = Nothing decr n = Just (n - 1) (`alterF` requires containers 0.5.8)
Me neither. But I can't take the results of the big (relatively speaking), “well controlled” academic studies with the vast statistical apparatus seriously *at all* because they don't seem to me to even be looking for effects relevant to industry on figures of merit of relevance to industry. But, y'know, feel free to secure a research grant and show me differently. I'm doing what I can with what I've got.
Thanks a lot for the step-by-step summary of your thoughts, it was fun to see your learning process, to see when my explanations worked and when they didn't. Your attitude is very generous to the author, I'm not so generous when I read. As the author, reading "`(_,(a:_))` doesn’t have any new, unseen tools — it’s just a construct based on the lessons so far. Fair enough: I failed at my first pattern matching test." is a bit tragic, haha! And also it's great feedback, it says that indeed all the tools were introduced that you needed (I hate it when a book assumes knowledge that it didn't introduce), on the other hand, probably I didn't explain best how to put them together. I think that's a great example of the conversation that has to take place when teaching/learning, and how imperfect it is. This kind of feedback is really quality, especially easier to get with an interactive tutorial. I wrote TryHaskell in 2010 and haven't really worked on its content since. Writing content is hard (particularly when you're trying to pack it into a small space), and the more I think about it the more I think about improving the medium. So I got into classic Second System syndrome and wanted to rewrite it in Haskell (this was pre-Fay, pre-GHCJS, etc.), support user-contributed interactive tutorials, let the user draw things on the screen and access web feeds, more interesting stuff like that, then perhaps the exercises could be randomly generated, etc. And of course nothing happened. I still think interactivity (with much more exercises and feedback) is the future of books, away from the paper/lecture metaphor.
&gt; running a thunk is a side effect `\() -&gt; expensive input :: Thunk Output` doesn't evaluate or execute ("run" is a bit ambiguous) a thunk; it creates one. It's like an introduction rule, not not an elimination rules. This is the return *I* want. :) But, maybe it isn't self-evident that `return $ expensive input` shouldn't evaluate `expensive input` when the goal of the monad is to model Haskell-style laziness, that's my default position. Unless someone shows that denotation is inconsistent with the monad laws (or something similar), I don't think I'd change that position.
This isn't the free monad (which is shorthand for the free monad generated by a functor). The `f` doesn't do anything in this implementation. The _only_ things you can do in a `Free` as you've defined it are monad things. You don't get any additional operations from your `f`, which you can witness by omitting it without consequence... This is instead the _initial_ monad, which is a different concept. You may want to take a look at the "free-er" monad for a different path... http://okmij.org/ftp/Haskell/extensible/more.pdf **Edit:** I stand corrected--there is an injection, which I had missed. Something still feels a bit weird in the explanation, but I agree its along the pattern of http://hackage.haskell.org/package/free-functors-0.6.5/docs/Data-Functor-HFree.html and gives something more than just an "initial" structure.
Lately there's been rush of these blog posts from mainstream programmers who spend 10-15 minutes on trendy topic in order to write a blog post and establish themselves as cutting edge explorers, without actually getting out of their comfort zone. Hopefully they'll get their fix and move on quickly enough to another trendy topic. 
&gt; This isn't the free monad (which is shorthand for the free monad generated by a functor). I thought the free monad was a monad with "least possible additional structure"? Wouldn't a nonfree functor generate a nonfree monad? Or are free structures different in Haskell?
It seems like you can define, prompt :: f a -&gt; Free f a prompt x = Free (\i -&gt; i x) and do the usual free monad stuff. The only real problem is that you eventually have to interpret it in a (non-free) monad. That being said, this looks more like [Prompt](http://hackage.haskell.org/package/MonadPrompt), which is the [free monad for a type constructor](https://www.eyrie.org/~zednenem/2013/06/prompt) and has some other nice properties as well.
Damn, Haskell is beautiful 
Methinks if you start by sorting the input your life becomes much easier, though this only works for finite lists. Otherwise an IntMap is probably your best bet.
A free structure can be defined as follows Now, given such a description, we can talk about the free structure over a particular set S (or, possibly over some other underlying structure; but we'll stick with sets now). What this means is that given S, we want to find some set M, together with appropriate operations to make M the structure in question, along with the following two criteria: * There is an embedding liftF : S -&gt; M * The structure generated is as 'simple' as possible. * M should contain only elements that are required to exist by i and the operations of the structure. * The only equational laws that should hold for the generated structure are those that are required to hold by the equational laws for structures of that type. [source](https://wiki.haskell.org/Free_structure) I however did not define the embbeding function, which was bad on me. It can be defined as liftF :: f a -&gt; Free f a liftF x = Free $ \interp -&gt; interp x As for it being as simple as possible, I have not sufficiently shown that this is true. I have only shown that it is closer to the definition then the usual implementation. I believe that it is as 'simple' as possible, however I could be wrong on that. 
I have never seen this library before, but this is the same concept. Prompt is indeed a free monad. I also believe it is more efficient than my implementation, but I'll have to look into that later.
While I also find that sort of post annoying, I wouldn't lump this specific one with them. For one, the author doesn't use his 10-15 minutes of experience as a springboard to pontificate or advance bold conclusions.
Well if your dealing with an infinitely associative monad (such as IO), then you can convert to the traditional free monad and use the extra power. To convert you can use the function `foldFree (Control.Monad.Free.liftF)` However if you have a monad that is not infinitely associative then, if you use the traditional free monad you will end up with some weird bugs. Using this new free definition just gives you more choice. 
No, converting your version back to original Free should result in the same non-termination
When talking about free structures one needs to specify the adjunction they're talking about. Haskell people are talking about the left adjoint to the inclusion of the category of Monads on Hask into the category of Endofunctors on Hask.
Doesn't look like O(n), since a map has logarithmic insertion/lookup.
There might be a better or newer approach, but this blog article from last year is the thing I recall... https://haskellembedded.github.io/posts/2015-12-15-arm.html And this ticket (https://github.com/commercialhaskell/stack/issues/2103) seems to point to this rather difficult discussion... https://github.com/blitzcode/hue-dashboard/#raspberry-pi
That is encouraging to hear. I don't mind the difficulty, as I'll learn something in the process. Glad to hear that it works on Raspberry Pi. [edit] So after looking at your instructions, I'm glad to see that I was on the right path. I've gotten ghc version 8.0.1 installed using the [precompiled binaries](https://www.haskell.org/ghc/download_ghc_8_0_1#linux_armv7) available on [haskell.org](http://www.haskell.org). I'm currently compiling cabal from source, as you indicate in the instructions. As you note, it's just really really slow.
Good clarification.. I've edited my post above. (and you may want to submit a PR to the free-functor package to add the Monad Fix instance...)
Thanks for the advice and links! Did you already have FP experience when you began reading it, or were you new to it as well?
if it helps at all, here is how I build cabal on alpine linux arm: https://github.com/mitchty/alpine-linux-ghc-bootstrap/blob/master/8.0/cabal/APKBUILD#L35 And stack built statically: https://github.com/mitchty/alpine-linux-ghc-bootstrap/blob/master/8.0/stack/APKBUILD#L22 As you note, its slow, welcome to building on arm boxes let things run while you're asleep and at work. :)
I really was impressed on how quickly I was exposed to the concepts, more of a function of a good tutorial than me, for sure.
The reason you got `* -&gt; *` instead of `*` is because `T` expects an argument. It would have been more comparable to your original attempt to try `:k T a`, but of course it's still not going to know what to make of `a`. The most faithful way to get what you're trying to do to work is `:k forall a. a -&gt; a` (which requires you to first run `:set -XRankNTypes`), which will indeed give you `*`. The `a` just has to be quantified somehow.
I agree that it was tragic, but first and foremost thanks again for building such a neat tool. Overall it works really well and lead me to natural conclusions the majority of the time. If there's a way to rewrite or hint around for that one specific structure I think it'd flow a bit better. Otherwise, thanks for giving me a taste of Haskell that reinforced my desire to learn and also built up some base confidence.
My pleasure, like I said, just peeking on the community showed me it looks very supportive of new learners - the reactions in this thread definitely show it!
Thanks /u/duplode, that helps a ton. I know it was just a tutorial but the mapping over tuple seemed at first like something reasonable, and now of course it makes sense that it wasn't (outside of the Bifunctor module you mentioned). Can you clarify about the prefix minus? Why is that one specifically special?
Indeed technically it's O(n*log(m)) where n is the length of the list and m is the range of the items.
What about `quot` and `rem`? Do these match your non-suprising expectations?
New to any programming, I don't know projects...
I'm interested in how you will receive some of the more advanced concepts once you get to them because in my opinion you cannot simplify such abstract concepts like you can with other things. Many tutorials will attempt such a thing (i.e. burrito monads) but fail miserably simply because it's harder to draw parallels and analogies to something that is already abstracted to the max. Anyhow, good luck and thanks for sharing.
The key part I suppose is "in", I've had a bunch of little nit picks when trying to learn Haskell over the last while but I've always found once you get over the hump (whatever it may be) it reads pretty nicely. 
Yeah, I love those problem sets. If you get interested in the whole Prolog link I sent you, here's the http://www.swi-prolog.org/Download.html
Yea I'd really like to hear their first impression of `Monad` when they get to it. I think a big part of why it's hard to teach someone monads is due to the fact that those of us who have learned them have forgotten what made them unintuitive. It'd be good to collaborate with someone throughout the learning process to see what the road blocks are.
It is building but like you said it is painfully slow: [current progress](http://i.imgur.com/vfC7K8l.png). Looks like it will go all night.
Cheers, I was showing divMod as these results may be surprising to those more used to the way mod is handled in say, Java. I have updated the blog to show that quot rem gives results more like some people may be used to from Java. Thanks for the feedback.
This start to look similar to the foldable-traversable mambo jambo or the Lens saga. It is ever a source of astonishment for me how the "professional" haskellers waste time in redefining everything again and again without adding anything essentially new or useful. Too much academic grants and too few ingenuity? The less useful something in Haskell is, the most it is prone have geniuses after it trying to redefine it again and again. This is not even bikesheedding since bikes are useful at last. Years ago, when I read "the evolution of an haskell programmer" I laughed loud about how the author exaggerated the haskellers mania to redefine useless things. Now I cry hopeless when I realize how clever and accurate it was. In the meantime there are lines of functional programming research that are completely abandoned. What a pity.
It isn't quite equivalent, for the reason D Doel pointed out in this discussion http://comonad.com/reader/2015/free-monoids-in-haskell/ ; fmlist is the 'true' free monoid, as lists aren't. The considerations above about the 'free monad' exactly parallel these and this solution was long ago present in the `free-functors` package with the `HFree Monad` type. The mistake in all this is that the reasons why we are interested in `Free` and co. have no more (or less) to do with their being the (almost) free monad than our interest in `[a]` has to do with its being the (almost) free monoid. The whole Haskell literature on free monads, translated into statements about the parallel list case, would basically be saying: "Of course, we only ever construct a list by explicit appends; and the only thing we ever do with it, once we construct it, is "interpret" it with `map` `foldMap` or `fold`". 
 $ hawk -d'\t' '[[show (divMod a b) | a &lt;- [-6..6]] | b &lt;- [3,-3]]' | column -t (-2,0) (-2,1) (-2,2) (-1,0) (-1,1) (-1,2) (0,0) (0,1) (0,2) (1,0) (1,1) (1,2) (2,0) (2,0) (1,-2) (1,-1) (1,0) (0,-2) (0,-1) (0,0) (-1,-2) (-1,-1) (-1,0) (-2,-2) (-2,-1) (-2,0) $ hawk -d'\t' '[[show (quotRem a b) | a &lt;- [-6..6]] | b &lt;- [3,-3]]' | column -t (-2,0) (-1,-2) (-1,-1) (-1,0) (0,-2) (0,-1) (0,0) (0,1) (0,2) (1,0) (1,1) (1,2) (2,0) (2,0) (1,-2) (1,-1) (1,0) (0,-2) (0,-1) (0,0) (0,1) (0,2) (-1,0) (-1,1) (-1,2) (-2,0) *edit*: or to make the patterns even more obvious: $ hawk -d'\t' '[[show (div a b) | a &lt;- [-6..6]] | b &lt;- [3,-3]]' | column -t -2 -2 -2 -1 -1 -1 0 0 0 1 1 1 2 2 1 1 1 0 0 0 -1 -1 -1 -2 -2 -2 $ hawk -d'\t' '[[show (quot a b) | a &lt;- [-6..6]] | b &lt;- [3,-3]]' | column -t -2 -1 -1 -1 0 0 0 0 0 1 1 1 2 2 1 1 1 0 0 0 0 0 -1 -1 -1 -2 $ hawk -d'\t' '[[show (rem a b) | a &lt;- [-6..6]] | b &lt;- [3,-3]]' | column -t 0 -2 -1 0 -2 -1 0 1 2 0 1 2 0 0 -2 -1 0 -2 -1 0 1 2 0 1 2 0 $ hawk -d'\t' '[[show (mod a b) | a &lt;- [-6..6]] | b &lt;- [3,-3]]' | column -t 0 1 2 0 1 2 0 1 2 0 1 2 0 0 -2 -1 0 -2 -1 0 -2 -1 0 -2 -1 0
lens is useful
1 - MusicXML has numerous professional backends, [musescore](https://musescore.org/) being a prominent open-source one, plus Finale and Sibelius. Lilypond is an excellent format for human-readable serialization, but for machine-generation I think MusicXML is better, as you don't worry about whitespace etc. Also, my non-algo composition uses WYSIWYG editors like Sibelius, which was important when I was touching up the individual parts. 2 - The recording will be coming out on Leo Records in a few months, and there's a [video](https://youtu.be/psvOOHTJ8fQ) of the live performance. I'm still working on how to release the score, and maybe even the source code (I've released the supporting libs but not the "artistic" code), which requires figuring out publishing etc. I'll update the site when these are available.
Sorry for the slow reply -- I got caught up with other stuff and had to put this on hold. So, first of all, you're completely right that the two workers weren't even functionally equivalent. That's an embarrassing oversight on my part, and I hoped that maybe fixing this would fix the performance discrepancy. At first, I tried switching from `Pipes.Text.readFile` and `Pipes.Text.writeFile` to `Pipes.Prelude.Text.readFileLn` and `Pipes.Prelude.Text.writeFileLn`. This achieved output parity as I hoped, but it didn't fix the performance issues. The pipes version was still drastically slower. Next I tried your second suggestion in this comment and it fixed it! I haven't tried out the patched version of pipes yet but just wanted to let you know that your suggestion did work.
And that's where keeping them connected will really pay off! Also I would probably use Map instead of lists of pairs, but just for clarity not because it's nessary, and that's arguably a bad idea. 
&gt; The purpose of this paper is to document these approaches in hopes that they will inspire new work by more composers in the realm of generative, algorithmic software. This is a very broad question but what materials would you suggest for a proficient programmer who wants to learn musical composition via software?
Thanks so much for this news! I wanted to have some independent verification of its utility before bugging Gabriel with it. 
If you mean software to help you study music composition itself, I'm unaware. If you mean materials in the realm of algorithmic software, I'm not much better :) but Xenakis' [Formalized Music](http://www.pendragonpress.com/book.php?id=558) is a classic. While it's not composition-oriented per se, [SuperCollider](http://supercollider.github.io/) is really an incredible programming language for synthesis that is quite mind-expanding in its concepts, and will get you issuing numbers that turn into sound, so is a good starting point too. Getting notation out of SuperCollider is more or less impossible though :) . Also Nick Didkovsky's site http://www.algomusic.com/ is a good resource. Lastly, for music theory itself, two great books: Schoenberg's [Theory of Harmony](http://www.ucpress.edu/book.php?isbn=9780520266087) and Persichetti's [Twentieth-century harmony] (https://www.amazon.com/Twentieth-Century-Harmony-Creative-Aspects-Practice/dp/0393095398). I'm also a huge fan of Messaien's [Techniques of My Musical Language](http://www.boosey.com/shop/prod/Messiaen-Olivier-Technique-of-my-Musical-Language-English/673934) but its expensive/hard to find.
Just to clarify, it's not "gcc-6.2.0" that breaks GHC here, but rather using a GHC bindist which wasn't properly `./configure`d for use with a PIE-hardened GCC. Unfortunately, GHC's current Autoconf scripts can't autodetect this (yet), so one has to tell `./configure` explicitly that the C compiler needs to be put back into pie-less mode; nor is GHC able to detect this property at runtime. For GHC bindists, the `./configure` script simply generates the `settings` file, so if you want to just fixup an already installed GHC bindist (which was optimised for a different Linux distribution release), you can simply patch the installed `settings` file as described in the email. However, there's also PPA packages of mine specifically configured and compiled for Ubuntu Yakkety over at https://launchpad.net/~hvr/+archive/ubuntu/ghc . Beyond being managed by the system packager, those packages have the benefit of being optimised for the specific environment they're destined for rather than being a "generic" binary distribution which uses older entry points into libc/gmp etc.
[removed]
I'll try with those as well. This will take a while :D
The `pointless-haskell` library allows you to write code like this: https://hackage.haskell.org/package/pointless-haskell-0.0.9/docs/src/Generics-Pointless-Examples-Examples.html#fib. I personally think Haskell isn't suited for point-free-all-the-way style of programming. If you are enticed by that style of programming, you might want to check out a concatenative language like Factor. You might not want to use it for any real software, but it is certainly one of those languages that changes the way you think! Check out this old blog post of mine: https://goo.gl/fl1FCv
I've watched all of them to date (there are more than a dozen so far), and I can't recommend these enough. Bartosz breaks things down perfectly and has allowed me to even dive in to the actual category math book (McLane's category book). 10/10, will be watching again to review.
"beautiful looking, elegant" "point free code" sounds like an oxymoron :P are you aware of the logical contradiction you are proposing? 
"Write everything with as few points as reasonable, but no fewer."
You might enjoy Pearls of Functional Algorithm Design, by Richard Bird.
Without going into much detail: Curry changes a function from (a, b) -&gt; c to a -&gt; b -&gt; c And uncurry changes a function from a -&gt; b -&gt; c to (a, b) -&gt; c Therefore, curry allows you to use a function that takes a pair as if it were a "curryable function" and thus pass each argument in individually. The opposite for uncurry.
Another random tidbit which you might find useful: There is a function called `id` which returns the value passed into it; it's essentially equivalent to the process function you have for the current problem: id :: a -&gt; a id x = x Therefore you could define process as: process :: Int -&gt; Int process = id and it'd still be a perfectly valid solution. I know that for other TIS100 problems this won't be so useful, but it's helpful to know it exists. EDIT: Also, anywhere that you see `do` syntax, know that the `do` is not required if you only have one statement in the do block. so main = do run could become main = run ALSO I just realized this post is 3 days old so for all I know this could be irrelevant information by now.
:) Beauty can be subjective. Since many people, like myself, start programming with years of OOP in C/C++ or Java or Python or what have you, wrapping your head around functional programming is already hard enough. Because recognizing patterns in a functional style is hard for an imperative programmer. Functional programming looks beautiful only *after* you have learned to recognize these patterns. Similarly, recognizing different ways of composing functions is hard when you see it for the first time. The first time I saw the 'blackbird' operator it seemed unnecessarily complicated to me `(.) . (.)`. But now that I have familiarized myself with it ... = (.) . (.) complexFunc = oneArgFuncA . oneArgFuncB ... twoArgFunc looks beautiful to me. That being said, I can definitely understand the *huge* downsides of introducing this style into a commercial development environment. But as I said, I'm just a hobbyist and I'll be writing code mostly for myself, either to better understand some concepts, or to create/analyze musical patterns.
So what is the correct `./configure` command line to fix this?
Depends on the GHC version (fwiw, PIE &amp; GHC have been known to run into issues already 7 years ago, c.f. [GHC #3668](https://ghc.haskell.org/trac/ghc/ticket/3668)), for recent GHCs something along the lines of ./configure \ CONF_CC_OPTS_STAGE2=-fno-PIE \ CONF_GCC_LINKER_OPTS_STAGE2=-no-pie \ CONF_LD_LINKER_OPTS_STAGE2=-no-pie should do the trick for the generic bindists. For building GHC from source, in addition to the `configure`-invocation above, you may also need to add `-no-pie`/`-fno-PIE` to `SRC_CC_OPTS`/`SRC_LD_OPTS` via e.g. your `build.mk` depending on the GHC version.
haskell development on OpenBSD is fine. I use it on daily basis with 6.0 current and GENERIC.MP. As a prerequisite just do: # pkg_add haskell-platform # cabal install stack important notes * it is ghc 7.10.3 currently, so you have to stick with lts-6.x * you need wxallowed in the /etc/fstab for the partition where you want to run things like 'stack exec -- yesod devel' All in all, haskell development on OpenBSD works like a charm.
Writing anything in haskell is already harder for me than writing in python. But it is a learning experience. I (tried to) read a book called "Topos of Music", that nudged me towards category theory and while looking for lighter and lighter resources I've found, and watched all of Bartosz Milewski's videos. They were a wonderful introduction. Then I randomly saw [this video](https://youtu.be/seVSlKazsNk). I liked the concept, I felt like being able to write point-free haskel without producing code that looks like arcane magic, or a random string of characters, would improve my understanding of category theory, combinatorial logic, and functional programming. I have started reading "To Mock a Mockingbird", looked around in Control.Arrow, but not Control.Category (I have to get on that). I guess I also have to look at Kmett's work. I'm bookmarking the nLab. And lastly, your example is really helpful. All in all, thank you very much for the wonderful response :) 
Definitely will check it out. Thanks!
Using de Bruijn shifts the pain from one place to another. If all you need is alpha-equivalence then de Bruijn is not worth the effort IMO. 
Traverse the two terms in parallel and keep two maps from bound names to something unique, like numbers. Extend the map at lambda, lookup at variables. 
I normally define alpha equivalence with swapping, or permutations, of names. The only interesting case is for binders: two lambda-abstractions are alpha equivalent if (1) they bind the same name and their bodies are alpha equivalent, or (2) they bind different names, the second of which is not in the free variables of the body of the first, and the body of the first with the two bound names swapped is alpha equivalent to the body of the second.
That's orthogonal. Shake only replaces the `make` part of the buildsystem, you still need to configure your build before you can invoke the shake phase. So we'll still need to write those Autoconf tests (or implement runtime detection inside GHC to detect the currently selected c-compiler's properties, depending on which path we want to go). (Fwiw, there's been the suggestion about reinventing Autoconf in Haskell, but I don't consider that a sensible thing to do as we'd have to invest time when we're already spread thin to make sure that our Autoconf re-implementation properly detects all the exotic configurations and corner cases, rather than relying on other people's work who already went through that ordeal for standard Autoconf sake, and not the least because distro packagers and users on exotic platforms already know how to tweak/fixup Autoconf projects; plus a few other reasons...)
It makes more sense if you focus on the inside of the `kill` function. Whatever it got as "a" is discarded without being stored or returned. That's the "discard". Perhaps "the world" isn't discarded if you think of the function getting a reference or copy of "the world", but whatever it is that `kill` gets, that thing is discarded. Linear types prevent implicitly copying or discarding values. For most state, just preventing copying (preserving uniqueness) is enough, and that's called "affine types". (Allowing duplication but not discarding doesn't seem useful for programming, but it's called "relevant logic" in math)
In case you may want something a bit more fundamental, the following is a great introduction to the topic: [Lambda-Calculus and Combinators, an Introduction](http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521898850) 
Thanks for this. I think starting this trend of github crowdsourcing Haskell conference preprints might be one of my greatest accomplishments on the internet so far.
Do not forget the `pointfree` [command line tool](https://hackage.haskell.org/package/pointfree).
 p target acc [] = acc -- find pairs that sum to target p target acc (h:t) = nub matches ++ acc ++ recurse where candidates = zip (repeat h) t matches = filter cond candidates cond = \(x,y)-&gt; target == x + y recurse = p target [] t main = p 10 [] [1,1,9,9,9,2,3,8,7]
The locally named method is how I've approached it recently.
Maybe annotate the fact that Double / Float violates absolutely all of those laws. And only optimize on lawful instances.
Ah! Now I get it. Thanks. All the replies to this post have been really helpful. So thanks to /r/haskell too :)
Hrm, I need to retry this with my alpine linux port as it has a similar issue I've hacked in another way. Also I think 9007 applies if hardening is enabled like this, this seems to becoming more common. https://ghc.haskell.org/trac/ghc/ticket/9007 I had started on a patch to try fixing this but ghc internals are a bit over my current pay grade. I'll keep poking though.
There are several ways to define Prompt. The definition you describe is lazy in the first argument of `&gt;&gt;=`, while the Church-encoded version is always strict. The Church encoding has a much simpler implementation of `&gt;&gt;=`, which doesn't rely on a Monad instance to be named later. Both implementations are more efficient than the initial implementation, data Prompt f a where Done :: a -&gt; Prompt f a Prompt :: f b -&gt; (b -&gt; Prompt f a) -&gt; Prompt f a 
I totally agree, I would be lost without seeing the whiteboard. But I think I would benefit from listening to the lectures a second time and forcing myself to picture the diagrams. I tend to do better when I encounter information twice and in different ways (I will often read a book and listen to the audiobook version at the same time).
Functions in Haskell are "curryable". For example, the type of (+) is Num a =&gt; a -&gt; a -&gt; a. Thus if we apply one argument to (+), we get a function of type Num a =&gt; a -&gt; a. This is what the curry function does, it allows one to"curry" functions that usually take pairs. This may not make much sense abstractly so let's have an example: (+) 3 2 = 5 (+) 3 = a function that, given an argument, adds 3 to it. AddPair (x, y) = x + y AddPair x is a type error curry AddPair x is a function that adds x to whatever the next argument is. Therefore, curry allows you to use functions that work on pairs as if they were "normal" functions taking two arguments, and uncurry allows you to use "normal" functions on pairs. In the end, they simply allow less code duplication by letting you write a function to take either a pair or two arguments and use them in both situations.
Thank you, glad you found it helpful! As an exercise, consider this similar code skeleton: pairstream z = catMaybes . snd . mapAccumL f M.empty where f m x = ??? maybe nopair pair ??? where y = z - x nopair = ??? pair m' = ??? decr 1 = Nothing decr n = Just (n-1) add1 x m = M.insertWith (+) x 1 m When you fill in the `???`s of this non-`State` monad example and compare, you may better see how the `State` monad is doing its thing. (This also shows how close this is to TarMil's code, just `maybe` instead of `case` and `map` instead of `fold`.)
Bartosz Milewski also has a book/blog series that is quite concise and closely follows along with the topics in his lectures. It's a good read if you want to review the topics. [Category Theory for Programmers: The Preface](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/) That being said, I'd highly recommend his lectures. They make learning category theory very approachable.
I teach an Intro to FP course and the first thing I learned was not to set up students to struggle with monads by being too technical or abstract about it, or even warning them that monads are hard to understand. And by avoiding analogies wherever possible -- the Typeclassopedia approach was a godsend. By the time they come to monads they are reasonably happy with typeclasses and they seem to be able to handle the idea of a typeclass that comes with a couple of laws, especially if you hammer home how they work with one example after another. Monad instances that take some thinking about can wait.
Well, if you believe the original abstract from the 1977 paper [M4 Macro Processor](http://wolfram.schneider.org/bsd/7thEdManVol2/m4/m4.pdf): &gt; M4 is a macro processor available on UNIX and GCOS. Its primary use has been as a front end for Ratfor for those cases where parameterless macros are not adequately powerful. It has also been used for languages as disparate as C and Cobol. **M4 is particularly suited for functional languages like Fortran, PL/I and C since macros are specified in a functional notation**. Then even C is a "functional" language ;-)
http://fpchat.com maybe? But there's probably more people on Irc.
You can take a look at Reflex-dom which may do what you want
Purescipt's Pux is a lot more like Elm than Thermite.
afaik, both ghcjs and reflex-* are actively developed. 
As correct as that is, it's not a helpful comment, and not a welcoming one to someone who (from their writing, I assume) is new to the community. Have you got any suggestions for HTML parsing libraries, or did you just want to leave a snarky comment? Edit: after a quick Google, [this](https://hackage.haskell.org/package/html-conduit) library seems to be most recently updated, but may be too heavyweight for some simple parsing. Does anyone have any experience reports of using it, or any other suggestions?
I built a little experiment on these ideas here https://github.com/boothead/oHm I certainly wouldn't claim this is maintained or production ready though ;-)
The example you give involving `unsafeCoerce` certainly would not work. There are two problems. The first is that you are trying to coerce from type `Command x` to `Aeson.Parser y`. That does not make sense. The second problem is that the instance itself probably does not do what you want it to. The way you've defined it, the callee of `parseJSON` can pick any type for the `rsp` that's in `Command rsp` to unify with and then it will try to parse the JSON into that. But what you probably want is for the JSON itself to dictate `rsp`. What does work is something like this: data Hide f = forall a. Hide (f a) instance FromJSON (Hide Command) where ... That requires `FlexibleInstances`, which I personally don't like, and to avoid using that extension, you can instead do what I suggested in the github issue you linked to. To answer your actual question about generating these instances, yes. For GADTs that have this particular shape, it should be possible to write some template haskell that derives the instances. With `GHC.Generics`, I do not think that it is possible.
Oh interesting. I would be interested in hearing. My patch was seeming a little too good to be true ...
yes, that might be it when I think about it...
We used react-flux and i can recommend it. Its strong points are: Its architecture is based completely on react + flux so very easy to understand for anyone who is familiar with react + flux. It has a top notch documentation. I personally had no problems picking it up and being productive in a very short amount of time. It allows you to reuse a huge number of ready react components, especially bootstrap-react. And it was very easy for me to do it. In the end we went with purescript, but not because of react-flux. Rather because ghcjs has a quite large footprint and you may have some problems with old (IE 8) browsers. 
Oh, might be.
Step by step, we start with splits -- list of ways of splitting a list into two parts splits :: [x] -&gt; [([x],[x])] splits [] = [([],[])] splits (a:as) = ([],a:as):[(a:bs,cs) |(bs,cs) &lt;- splits as] The type signature (first line) says it takes a list of `x` and turns it into a list of pairs of lists of `x`. The comment tells us this should give us all "splittings" of the first list. So we're first given a base case for an empty list, which is self explanatory. Then in the recursive case, we have the first item `a` and the following (possibly empty) list of items `as`. The first splitting is the one where the first list is empty. The list comprehension notation (brackets with the pipe) is natural like a set builder notation, but we can rewrite it as follows if it helps: [(a:bs,cs) |(bs,cs) &lt;- splits as] ---&gt; map (\(bs,cs) -&gt; (a:bs,cs)) (splits as) Either way, this calls `splits` recursively on the remainder of the list, and for each split, we prepend the _first_ of the two lists with the `a`. So we can think of this inductively. "All splittings of a list are generated by either having nothing in the first list, or taking all splittings of the tail of the list, and prepending the head of the list to the first portion of that splitting". Now we move on to the main function. -- perms of a list perms :: [x] -&gt; [[x]] perms (a:as) = [bs1 ++ a:bs2 | bs &lt;- bss, (bs1,bs2) &lt;- splits bs] where bss = perms as The type signature says it takes a list of `x` to a list of lists of `x`, and the comment tells us those lists should each be permutations of the original. Again we have a recursive function. It says "first take all the permutations of the tail of our list". Now, for each permutation consider all splittings of it, and in each case insert the head of the list at that cut point. Rewriting to move away from list comprehensions, which may make things more clear, we can look at it as follows.. [bs1 ++ a:bs2 | bs &lt;- bss, (bs1,bs2) &lt;- splits bs] where bss = perms as ---&gt; let splitToPerm (bs1,bs2) = bs1 ++ a:bs2 -- i.e. take a pair of lists, then put them together so you have the first, then the element `a`, then the second, all in one big list in concat (map (map splitToPerm . splits) (perms as)) So here we take the perms, then for each perm we take all its splits, and apply splitToPerm to them. So each perm gives us a whole list of new ones with `a` at each new possible splitting. Then we concat the whole thing so our list of lists of new perms (each of those generated by a single initial perm at all its splittings) becomes just a list of new perms. Hope that helps a bit?
Take also a look at transient. It is the first and only full stack reactive library. In transient everithing is reactive: the database, the servers in the cloud and the Web clients. All of them collaborate executing a single (reactive) computation or many of them combined using standard monadic applicative and alternative combinators since, in transient, everything is algebraically and monadically composable. No other is. It's hard to explain because it is simple. Look at the examples. https://github.com/transient-haskell/transient tutorial: https://github.com/transient-haskell/transient/wiki/Transient-tutorial ELM example rewritten in transient: https://www.reddit.com/r/elm/comments/4wq3ko/playing_with_websockets_in_haskell_and_elm/d69o11p/?context=3&amp;st=iv1f98bg&amp;sh=2a0c76a4
I came accross Factor but I have already sunk too much time into Haskell :) besides your example can be written beautifully in Haskell: import Data.Function (on) foo = (+) `on` square 
Thank you so much!
I concur. Reflex is fantastic.
Yes, this is the reason, some of the structures are not straightforward either (not even byte aligned) so you have to manually set the packing offsets, which would be really easily represented as a fold. Another reason the DSL would be handy is because it is possible to plug arbitrary numbers of sensor modules into a car, however each may be measuring a different type of signal (all are transmitted identical 0-5V, data is translated according to the database). It would be useful to express these as a function and pass an enumeration of different "sensor kinds" to automatically figure it out and spit out the .dbc with all the correct scaling factors and ID offsets. Cheers for the template too
Haskell comes up quite regularly at the London NetBSD meetups: https://mail-index.netbsd.org/regional-london/2016/10/26/msg000555.html They have a hack-day coming up soon - if you have any requests, now might be a good time to bring them up :)
Nix has GHCJS packaged. A binary cache is available so you won't need to build anything. See the [nixpkgs manual](http://nixos.org/nixpkgs/manual/#how-to-install-haskell-packages) on how to install it.
I think stackage is running yesod in docker on kubernetes.
What is wrong with `FlexibleInstances`? I thought that was a safe extension.
Note they're decoded differently between normal element content and attribute values, so given an arbitrary string you can't *just* decode entities [edit: in a generic way].
I'm currently forced to use it, because of the dyn lib problems on macOS Sierra :(... It's a godsend that stack makes using docker so easy!
I wasn't necessarily trying to advocate for the use of comonadic interpreters when I wrote those posts. Maybe some of the motivation for those posts would help explain where I was coming from. I'll try writing about that and we'll see - sorry for the wall of text :) **TLDR:** Go with the natural transformations unless you had identified a specific need for the comonadic version. There might be some wins that come from the comonadic version when it comes to working over a network or testing, but I haven't gotten around to prototyping those properly yet. When I was learning about / playing about with the free monad, a lot of various pieces clicked for me (including things that I thought had completely and finally clicked earlier). Monad transformers in particular - it was a big "aha" moment when I got a handle on the various ways of dealing with state in conjunction with free monads, etc... In parallel with that, I also finally digested the Monadic QuickCheck paper, and spotted the free monad hiding in there (along with Writer, I think) in the machinery for testing monadic code for observational equivalence. The generators in there are a bit clunky - you need to generate a list of actions that act on a queue that don't pop the value from an empty queue etc... This got me thinking if there was something similar to their neat free monad trick that could be used to help create those kind of generators. Then I came across Dan Piponi's [post on free and cofree](http://blog.sigfpe.com/2014/05/cofree-meets-free.html) and decided to dig in. Shortly after that I happened onto Runar Bjarnason's [talk on reasonably priced monads](http://functionaltalks.org/2014/11/23/runar-oli-bjarnason-free-monad/) which reminded me that I was probably ready to try to read "Data types a la carte" again, and also reminded me about the use of natural transformations as interpreters. I think I was also reading about HAXL at the time, and had some ideas about how these things might all come together. So I started playing around with the plan being something like: - interpret a free monad with a cofree comonad, to see if I can / what is required - bring comonad transformers into the mix, initially to try to learn about them and then to try to provide some materials about them if that ended up making sense in the free/cofree setting - do something similar with Data Types A La Carte, mostly to see if I can but also because composition is also handy There was a lot of bonus stuff building on that, that I may or not get back to one day (once I'm finished with several other rabbitholes that I'm burrowing into). The first thing I wanted to dive into was network transparency. I figured you could use the "classy MTL" techniques to have an open sum of errors that may or may not contain network errors, and then as long as all of the components of your Data Types A La Carte based free monad had Binary instances you could develop something a bit like `servant` + `servant-client` for your free monad. Then your free monad might be interpreted locally or on a server somewhere, and you wouldn't really need to change anything, since you'd just be relying on the API type the whole way through. I think I played with an actual `servant` based implementation as well at some point. There's probably some fun to be had in having a server that is comonadic interpreter which has a per-client comonadic interpreters in a client-id-to-interpreter map, which would mean that you could switch client interpreters based on configuration options / run-time conditions. I haven't really thought about what you'd need in order to be able to snapshot / restore those kind of servers, but having values to play with might help there as well. I was also keen to lean on the fact that we had data types for all of the domain actions to do something with event sourcing in this space. I'm a little less keen on that now that I've come up to speed on proper Event-and-Behavior based FRP, since it seems to feel like `FRP - theory = core idea = event sourcing - consultants`, although I need to do more in that area to be sure :) After that the goal was always doing interesting things with testing, and that is something that I will get back to one day. I think there are good things in there.
What are the alternatives?
I can't think of many things that can't be deallocated (maybe tracking hardware resources?), but there might be some things you don't want to be implicitly freed, like locks or files. Affine is probably good for a surface language, but even then, the compiler probably has to insert deallocation calls where values are dropped (see Drop in rust), and after that it might be helpful to have linear types in an intermediate language to prevent leaks. In the linear case, dropping values has to be explicitly allowed with "primitive" functions that take but don't return a value. That might be what you want for some things like lock handles or files, if they shouldn't be implicitly dropped. Affine is probably enough for the surface language, but for "no GC" you need to insert dealloction when 
Reflex is very active. So active that the version on Hackage is quite outdated =P Gotta use it from github
https://hackage.haskell.org/package/command That's shake's cmd extracted into IO (by me). But don't use that. Use shake instead! By now its cmd function can run in plain IO. 
It's definetly not React but it's similar in flavor. Pure FRP is closer to what Elm used to be like before Subscriptiogeddon
So that your code is less dependent on GHC, maybe?
X-Post referenced from [/r/linux](http://np.reddit.com/r/linux) by /u/rdnetto [Powerline-hs - a Powerline clone that is 15x faster](http://np.reddit.com/r/linux/comments/5at77y/powerlinehs_a_powerline_clone_that_is_15x_faster/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
[Shelly](http://hackage.haskell.org/package/shelly) has [`cmd`](http://hackage.haskell.org/package/shelly-1.6.8.1/docs/Shelly.html#v:cmd) which, at a glance, is quite similar to Shake's function of the same name.
I'm proud of the Haskell community that we haven't gotten sucked into that centralized corporate blackhole known as Slack. (I write this on a centralized corporate version of usenet...can't win 'm all)
How is this "the first and only?" Reflex can be used on the server too.
Is the list passed to `perms` required to be a finite list? I'm not seeing how this can produce before exhausting `as`, but since the behavior for the empty list isn't even defined I guess I must be wrong some how.
I think it is "the first and only" under the definition of reactive according to /u/agocorona, which I think I first came across [here](https://www.reddit.com/r/haskell/comments/4zugyb/my_thoughts_after_spending_5_month_writing_an_frp/d6z03cx/). Some of the other comments in that thread - [this one](https://www.reddit.com/r/haskell/comments/4zugyb/my_thoughts_after_spending_5_month_writing_an_frp/d6z2uzn/) in particular - caused me to consider that maybe they are conflating "event-and-behavior" FRP with "behavior instead of event" FRP in some discussions. It's also possible they have only come across behavior-only FRP and/or have only come across really poor examples of Event-and-Behavior FRP. Either way, I think that has lead to some misunderstandings in a few threads that I've come across. From my brief reading of it, `transient` looks like it's built on the actor model / has some similarities to Akka. I'm not a fan of that style myself, but it's great that we've got something like that in the ecosystem.
I think this is really helpful 
Note that I mean that trasient is functional and reactive and not FRP. I now tend to consider FRP as a copyrighted label in some way. Although transient is functional and reactive, it is not FRP in the Conal definition of it. But Who matters?. You miss the point. It is functional and reactive for the whole stack _at_the_same_time_ and for the whole application. No other can create a full stack reactive application, for any definition of reactive. That is the sense in which I use the expression "first and only". That is what matters for /u/EffBott the person who make the question of this thread. It is not like Akka since Akka has no browser side and Akka applications are not composable, so they are not functional in the strong sense. However Akka is reactive since it can stream events. Transient is functional AND reactive but it does not use the Conal FRP model. transient is push based. FRP is pull based. Akka is push-pull. but while push can simulate pull, pull can not simulate push. FRP lack many elements necessary to perform distributed streaming like transient or Akka. Streaming across nodes is necessary for a full stack reactive framework like transient. But unlike Akka, whose reactivity is added to Scala only for distributed stream processing, transient is natively reactive, since any transient computation can return zero, one or many results at different moments of time and maybe within different threads, so every element on the browser or the server can be reactive. a text box can be reactive. a console input too. Transient can implement the Akka model of distributed computing in the same way that It can implement other distributed computing models, like map-reduce. This is due to the composability and reactivity of the transient primitives. But this again it is not the question. The question is about a reactive Web application framework. And transient is definitively Web enabled, runs under GHCJS and it is reactive, for server and client, since both integrate a single seamless program.
I'm curious if you've done much with Conal's version of FRP in any of it's various incarnations? Some of your comments elsewhere about behaviors don't seem to apply to behaviors in the libraries based on Conal's work, which got me wondering. Also, while I'm asking questions and composability has come up: are there semantics for `transient`? I keep wondering about that and then forgetting to ask.
How are most people dealing with transporting these binaries to other systems? I'd love to use Haskell for scripting purposes but I have a hard time getting buy in from others when I have to install stack just to build something like Powerline. 
Just like this: $ stack install shake $ stack exec ghci &gt; import Development.Shake &gt; () &lt;- cmd "echo hi" hi 
I guess I should explain a bit how that works. (Based on https://hackage.haskell.org/package/shake-0.15.10/docs/Development-Shake-Command.html#v:cmd) Because `cmd :: CmdArguments args =&gt; args :-&gt; Action r` looks like it returns `Action r`, not `IO r`, right? So how can it be run as an IO action? Notice the `:-&gt;` ins there. It's a trick: type (:-&gt;) a t = a &gt; A type annotation, equivalent to the first argument, but in variable argument contexts, gives a clue as to what return type is expected (not actually enforced). So the `args :-&gt; Action r` is simply `args`, and thus the actual type of that function is `cmd :: CmdArguments args =&gt; args`. Now we just click on the Haddocks of `CmdArguments` and discover: instance CmdResult r =&gt; CmdArguments (IO r) So in our case, indeed `cmd "echo hi" :: IO ()`.
Have you had many issues when transporting it across different operating systems like OS X to Linux? Most people in my group are pushing python because it seems to have an easier transition history. 
That's in all likelihood going to take a re-compile, but otherwise unix-to-unix compatibility is rather painless in Haskell. Even less of a problem than in C as there's generally some kind of abstraction layer that blurs platform quirks.
'webdriver' works fine with browser automation. I used it with firefox, chrome and phantomjs too
Oh dear, now I understand why MitchellSalad is not a fan. But this is great! Thanks for the amazing explanation. 
This is like every day in Scala.
I posted a follow-up here: https://www.reddit.com/r/haskell/comments/5axh2j/two_methods_of_using_statet_with_pipes_one_of/ I believe the issue is easy to reproduce without reading/writing to files or anything else.
Use webdriver. Since it's just an RPC client you need to first setup selenium on the browser side manually. However if you really need scraping via a browser, simply writing a browser extension may be a better choice.
https://github.com/mitchty/alpine-ghc
Ah of course, that makes sense! Thanks.
Yeah, it's basically "react done right" now. Conceptually, I don't think it makes sense to filter subscriptions. you still need to read them somehow to know if you want to filter them or not - and inspecting the messages naturally takes place in update. To get what you want, you can make a proxy update function that just forwards the things you want to keep to the "real" update function. I don't know enough about elm internals to know if this is operationally equivalent to filtering the subscription - ie. if elm can optimize identity updates into noops or there's still some overhead. or were you thinking of something else and I missed the point?
Have you tried using a CPSed StateT: newtype StateT s m a = StateT { runStateT :: forall r. (a -&gt; s -&gt; m r) -&gt; s -&gt; m r } That may help out some. Pipes uses CPS, so having the CPSed monad closest to IO might be the issue. EDIT: Oh, instances for it, if you need help implementing it: instance Functor (StateT s m) where fmap f (StateT m) = StateT $ \c -&gt; m (c . f) instance Applicative (StateT s m) where pure a = StateT ($ a) StateT mf &lt;*&gt; StateT ma = StateT $ \c -&gt; mf $ \f -&gt; ma (c . f) (*&gt;) = (&gt;&gt;) instance Monad (StateT s m) where return = pure StateT m &gt;&gt;= f = StateT $ \c -&gt; m $ \a -&gt; runStateT (f a) c instance MonadTrans (StateT s) where lift m = StateT $ \c s -&gt; m &gt;&gt;= flip c s instance MonadIO m =&gt; MonadIO (StateT s m) where liftIO = lift . liftIO instance MonadState s (StateT s m) where get = StateT $ \c s -&gt; c s s put s = StateT $ \c _ -&gt; c () s I much prefer the CPSed `StateT` over the normal one because you don't have to mess with tuples at all. There's no "lazy" or "strict" `StateT`, just passing the state to the next continuation.
Depends on how hard the website is trying to prevent scraping. 
 let splits = go id where go fn [] = [(fn [], [])] go fn as@(a:as') = (fn [], as):go (fn . (a:)) as'
It's a tiny hammer relative to your computer's processing power. 
That definitely helps explain your thinking; thanks!
Because this is the Haskell sub and your best suggestion is to use Python? Whether you find your code more readable or not is subjective and irrelevant. Haskell has mutable data too but it isn't very idiomatic.
I'm actually wondering what powerline does that needs 1.25 seconds...
When Signals were still around, you could basically treat them like you would any other collection. There were your basic functions like `map`, `filter`, and `foldp`, but there were also some interesting ones like `filterMap`, `dropRepeats`, and `sampleOn`. http://package.elm-lang.org/packages/elm-lang/core/3.0.0/Signal Let's say you wanted to create a Signal that streams event numbers. You used to be able to do something like this: currentTime : Signal Time currentTime = Time.every Time.second |&gt; Signal.filter (\x -&gt; x % 2 == 0) setCurrentTime : Signal Action setCurrentTime = Signal.map SetCurrentTime currentTime Subscriptions can't be filtered. In fact, the only functions available are `map` and `batch`. Also, subscriptions are tied to Msgs. So to have the comparable behavior, you have to do something like this: type Msg = SetCurrentTime Time | NoOp setCurrentTime : Sub Msg setCurrentTime = Time.every Time.second |&gt; (\time -&gt; if time % 2 == 0 then SetCurrentTime time else NoOp ) So even though you don't care about odd numbers at all, you still have to generate a `Msg`, which in turn gets passed to your `update` function and forces a virtual DOM diff. In practice, the performance hit can be basically avoided by using `Html.Lazy.lazy` on your main `view` function, but it still feels wrong. Other than that, I have no gripes with the changes made in 0.17. The new subscription model is easier to understand and easier for beginners to pick up. Also, the replacement of port signals/mailboxes with subscriptions and commands makes way more sense.
I second that, my experiences with react-flux were wonderful.
Its rare that you find a clone of an existing package written in a different language, so I used David Wheeler's "sloccount" program to find out how many lines of code were in the Python and Haskell versions. If its true that SLOC/hour is roughly constant then this gives us a reasonable proxy for the relative developer productivity. I'm not sure I believe my results. To repeat the experiment: $ git clone https://github.com/rdnetto/powerline-hs $ git clone https://github.com/powerline/powerline $ sloccount powerline $ sloccount powerline-hs Note, you will have to install sloccount first. Fedora has it as a package, and I imagine most other distros will as well. Failing that, go to http://www.dwheeler.com/sloccount/ The results for code from the primary source directory, so excluding documentation build scripts and the like: * powerline: python=13869,sh=285,csh=51. Total = 14205. * powerline-hs: haskell = 1327. So it looks like Haskell is 10x more concise than Python. That is a stunning result. I would have expected it if I was comparing Haskell with C, or even C++, but Python is a modern language with garbage collection and higher order operators, plus it doesn't have type declarations adding to the SLOC count. My guess before I ran the test was that the Haskell would be around half the size of the Python. Of course this could be influenced by any of several special factors. Maybe the Haskell version has the benefit of a library that does the heavy lifting for something, while the Python version includes the relevant code in the project because there is no equivalent library. Or maybe the Haskell version isn't feature-complete yet. And of course there is the possibility that ANonGod is simply a much better developer. Does anyone know more?
[tagsoup-megaparsec](http://hackage.haskell.org/package/tagsoup-megaparsec) is also a good option. 
What are the drawbacks?
How is this any different from `hoist`?
The Readme does mention that powerline-hs still lacks some features. I'm not sure how much impact sloc-wise this would cause. 
You mean that yesod is so complex you can't expect it to run other than in a sandbox? I haven't seriously worked with Haskell since pre-stack so I didn't think docker is a common thing in haskell land nowadays. Is it?
You need different binaries for OSX and Linux, but that's about it. If your CI supports OSX build agents (e.g. Travis CI), then generating different binaries is pretty trivial. (i686 Linux distros are increasingly rare, so whether you need to generate binaries for it as well depends on your users.) Within x64 Linux, you can run into differences between distros if you're dynamically linking to a specific version of gmp or tinfo (ncurses). (Stack ran into this [recently](https://github.com/commercialhaskell/stack/issues/2302) with GHC.) Their solution seems to be to statically link the libraries in, so that's probably the best path forward. Compared to Python, I've actually had a much easier time moving between different systems. When running Python packages from other people, you either need to install the dependencies into your home directory, check if the packages are in your distro's package manager, or create a virtualenv for it. In comparison, with Haskell you just copy a single file and you're set - don't even need to worry about a directory of source files. 
Amazing, this fixed it! I also found this package on Hackage that provides a CPS version of `StateT` (minus the `MonadIO` instance): https://hackage.haskell.org/package/mtl-c
Benchmarking with criterion using [this code](http://lpaste.net/328034) and a list of ten thousand ints (instead of ten million) gave the following results: benchmarking main/StateT/outside time 2.591 ns (2.579 ns .. 2.603 ns) 1.000 R² (1.000 R² .. 1.000 R²) mean 2.584 ns (2.576 ns .. 2.593 ns) std dev 27.97 ps (23.86 ps .. 34.06 ps) variance introduced by outliers: 12% (moderately inflated) benchmarking main/StateT/inside time 9.535 ms (9.492 ms .. 9.606 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 9.533 ms (9.503 ms .. 9.577 ms) std dev 94.00 us (68.51 us .. 147.4 us) benchmarking main/CPSStateT/outside time 10.69 ns (10.65 ns .. 10.73 ns) 1.000 R² (1.000 R² .. 1.000 R²) mean 10.68 ns (10.64 ns .. 10.75 ns) std dev 157.2 ps (108.7 ps .. 246.5 ps) variance introduced by outliers: 19% (moderately inflated) benchmarking main/CPSStateT/inside time 2.577 ns (2.569 ns .. 2.587 ns) 1.000 R² (1.000 R² .. 1.000 R²) mean 2.575 ns (2.568 ns .. 2.583 ns) std dev 26.63 ps (21.63 ps .. 33.52 ps) variance introduced by outliers: 11% (moderately inflated) Conclusion: The version using `CPSStateT` on the "inside" of the pipe seems to perform comparably to `StateT` wrapped around the "outside" of the pipe. Disclaimer: I am by absolutely no means a benchmarking expert.
Won't work in this case as the term stuff in use is using some glibc-isms, yes I checked. :)
I've found that the best way to understand monads is to understand what problems they solve, dessugarize the do-notation and understand its implementation. Like: - List monad --&gt; List comprehension - Maybe -&gt; Avoid null checkings and partial functions exceptions. ... And also implement monads in other languages like OCaml or F#.
There will be activity if you ask questions. But it is a very bust channel - how long were you there? there's literally hundreds, probably thousands, of conversations (not messages) a day.
I'm friends with Gabe's sister! This was a great podcast, and it's great to see someone with a background somewhat tangential to FP be able to take up the concepts and bring it into their own domain of interest, only to then better express and work through problem material. It's super inspiring stuff.
It's the right prompt, so pygit is probably responsible for about 1 sec of that. Anecdotally, I've seen Powerline take multiple seconds to render the prompt in large repos on older hardware, which is consistent with that theory. Powerline-hs just calls the git executable directly, which appears to add only ~10 ms of overhead.
Almost. You can't define *arbitrary* Haskell types. You define record types whose field types are all instances of a persistence type class. You use a simple DSL, not Haskell syntax, to define those record types. Once you have done that, you get schemas and migrations for free. We're using this in production. It's powerful and easy to use.
Where functions have two differences: - They close over an environment - They are unavailable elsewhere These two properties seems to me to be fairly orthogonal to complexity. - Is it so that complex functions should not close over any environment? Why? - Should complex functions always be available to the whole module? Why?
Is this the problem? https://mail.haskell.org/pipermail/haskell-cafe/2016-October/125177.html
&gt; made me think that the phenomenon might be related just to `pipes` and `StateT` alone I'm pretty sure it's just a space leak in `StateT`. `forever` is broken in `StateT` (and `ReaderT`): https://mail.haskell.org/pipermail/haskell-cafe/2016-October/125177.html
[removed]
Maybe this wasn't such a great idea https://github.com/ghc/ghc/commit/6d69c3a264a1cfbbc7ecda0e704598afa45848c2 It is as if the old definition was benefiting from some hidden preference of the compiler for monadic operations. Nothing I can do with the `*&gt;` method can get the present `forever` not to be pathological. 
It works perfectly well in my experience. Do you have particular issues with it?
I just filed this: https://ghc.haskell.org/trac/ghc/ticket/12804#ticket
AFAIK, this slowness is mostly due to inefficient GHC's integration with LLVM (and LLVM backend being the only backend for ARM). On x86_64 Linux with `-fllvm` GHC is much slower than native gen too.
Regarding raspberry pi and similar embeddedish devices. I don't think it matters much whether building on the rpi is fast or not, but I do think cross compiling for it should be easier. It would be awesome with a open embedded meta layer for GHC with cabal integration, much like there is for rust and go. 
&gt; I don't think it matters much whether building on the rpi is fast or not, but I do think cross compiling for it should be easier. That I agree on. I guess I just meant that getting the Haskell ecosystem workable on raspberry pi is more than just a "bug fix" - it's a bunch of work on cabal, stack, *and* ghc that has to be done. And it wouldn't even work that well because the RPi has 1GB of RAM. 
8.0.1 working fine for me on Sierra also, installed via `nixpkgs`.
&gt; Switch to Linux That's what powers the Pi.
Haskell works fine on Mac...just don't upgrade to Sierra yet. The fact that the most recent version of GHC has some problem with the most recent version of macOS doesn't mean that Haskell isn't a robust practical tool. It is entirely a function of decisions that Apple makes, so it's completely out of our hands. They have a long history of major breakages with every new major OS release, so by now it should be pretty much expected. The ARM thing I can agree with you on, although that also doesn't preclude Haskell from being a robust practical tool either. But it would be so nice to be able to write iPhone apps in Haskell...
Its too bad because haskell executables actually run pretty well if you can get them to compile on ARM. Last time I checked it seemed like there were some significant bug fixes for arm, maybe things have backslid now? I was using haskell to build some minimal web servers for the PI and unfortunately that slowed my project down quite a bit. With haskell, 1.5G compile memory requirements (limiting compilation to a single cpu), 10-20 hour compile times for stack alone, 15-20 hours to build my project with all the dependencies and needing multiple tries to get everything to work. I made it work but I wish I'd given up sooner. Now I have a little rust webserver and it compiles all dependencies with a constant 120m of memory usage with all four cores maxed out in about 20 minutes. I haven't tried it but I understand that cross compiling is something they are serious about and is viable for arm. 
Honestly, it looks like a bug either in the type checker or how the `Data.Type.Bool.||` or `Data.Type.Equality.==` type families are put together. `("https://www.googleapis.com/auth/cloud-platform" Data.Type.Equality.== "https://www.googleapis.com/auth/cloud-platform")` should get turned into `True` when the `Data.Type.Equality.==` is applied. Then, `True Data.Type.Bool.|| _` should get turned into `True` when `Data.Type.Bool.||` is applied, twice. Instead, one of those type families is getting stuck. I *think* `Network.Google.Auth.Scope.HasScope'` is getting stuck on purpose, but I'm not sure -- it getting stuck could have caused a cascade failure in `Data.Type.Bool.||`, I guess.
&gt; Do you really want Haskell to be entirely absent from the world of ARM CPUs, No, of course not. But the raspberry pi is just one specific device. In fact, the problem arises because raspberry pi 3 uses a 64-bit architecture running on a 32-bit linux - it's not hard to see why there's a problem. ARM recently switched from 32-bit to 64-bit and Haskell is playing catch-up (basically). Armv7 *is* supported and I'm not sure how well it works. &gt;Also if a Quadcore 64bit ARM machine with 1GB of RAM is too low-end to be even considered for compiling, there's something wrong with our software... The problem isn't GHC alone. Cabal and stack will use all available cores, and often run out of RAM because of this. Unless you can figure out a better way to choose the number of jobs this will remain a problem. Basically, Quad-core+1GB RAM is going to be as efficient as single-core+1GB RAM for large packages. And I *have* gotten GHC and Cabal to work on RPi, just not stack. So I wouldn't say the fault is entirely theirs. Cross-compilation would be great though. &gt;Even if you had an ARM machine 10x faster than the RPi, same issue. Certainly, but ARM tends to be lower end. 1GB of RAM isn't that uncommon on ARM devices. &gt; And sure, I'd also very much expect older GHC releases to be patched. Keep existing projects compiling without changes! That seems unreasonable. Why not expect stack or cabal to change when this happens? Why should something released in 2015 support an operating system that didn't exist at the time? GHC has done everything they committed to and there's nothing unprofessional about that. &gt;macOS Sierra had a lengthy beta phase for exactly this reason. And also to catch bugs in its own software. I don't blame people for not wanting to write bug fixes for a beta version of an OS that will change.
Mid November is the plan.
Also, totally related: Fixes aren't backported to old GHC versions. This means that you can't use GHC 7.8.* (or was it Cabal?) with OS X anymore since at least El Capitano. The argument is always that a company which has the need for these backports could to that kind of work, which is valid, I guess. It's a shame that seemingly me nor anyone else wants to invest the time to backport fixes.
I don't think there's any issue specific to the RPi here, it's just a ARM problem in general. And you're right, it's mostly a Stack issue, GHC works OK, minus a few bugs which I think they fixed in 8.x. I have Stack working on the RPi, it was mostly just a pain because it took forever to build. If they had a bindist, they'd shorten the setup time on ARM from 1-day to 15min. But the bindist isn't there anymore because current Stack does not compile on ARM anymore (did not investigate myself why). I don't think it's a big deal to pass -j1 on ARM boxes to avoid running out of memory. I don't blame any build tool there, agreed. Maybe you're right that patching older GHC release isn't the anwer. But what I do know is that Haskell is particularly bad at breaking when the ecosystem changes and Haskell code rots way more than many other languages. I just copypaste what I said in the other reply: My basic issue is this: If I go out tomorrow and buy a new MacBook, I wouldn't worry much that my C++ code won't compile anymore. Generally does just fine, even my 5+ years old stuff. Probably the same for my Rust code, made surprisingly good experiences there. Can even keep compiling old libraries with new compiler versions. Haskell? Uh oh. GHC will be broken. And after it's fixed, I need to wait for a new Stack + snapshot, then I need to migrate all my code to a new compiler &amp; library versions, probably spend some time tracking down compiler errors, weird bugs, performance regressions. 99% sure it won't just work out of the box. My issue is simply that constantly last year's code doesn't work anymore on my current system. Other languages I've used are much, much better in regard to that. It can clearly be done.
/u/bgamari [sums it up](https://ghc.haskell.org/trac/ghc/ticket/12479#comment:33) rather well: &gt; Even better, it would be amazing if someone with an affected machine and knowledge of dynamic linking on OS X (a small set, I know!) could take ownership of this issue and carry it to solution. This may be darchon, but I'm a bit worried what might happen if he doesn't have time to finish, October 10th arrives, and we have no solution. 
With all those releases, most of that software had some breakages too. The difference is that with more people working on many of those systems, those fixes got made earlier (and backported as well). While there are plenty of Haskell users on the mac, for the most part these tend to _not_ be the same people as who would install a mac os beta, since they're "developers on a mac" and not "mac developers", often. I know that eventually bugs with new oses will get sorted out by GHCHQ. So I just hold off on upgrading my system until they do. As for backwards compatibility, I think it is more the case that GHC actually evolves more (especially due to the base/core lib coupling issue), so there's more incentive to actually care about older versions. Also, there are insufficient demands/resources to maintain older branches with patches, which is what's tended to happen with the big things on the toolchain. Also -- note that ensuring python and java and c++ work with new releases is done _by apple itself_ afaik. (my source for these clams? well, everytime people go run around trying to sort out what's breaking on the new mac os on the tickets I follow, it invariably leads to links to trackers from other open source projects encountering similar issues and evaluation of their solutions as well. so apple just breaks things more than other systems, and everyone is expected to play catch-up.)
Does anyone know roughly how many dependencies is "a large number?"
&gt; I don't think there's any issue specific to the RPi here, it's just a ARM problem in general. It most certainly was for me. Raspbian tells the compiler that it has a different architecture than the RPi 3, which means I have to use command-line flags for every build, passing the assembler the correct architecture. I considered making a pull request but I don't think it's worth it since apparently the RPi people might just break everything in weird ways with every release. Supposedly GHC 7.10.3 is included in Arch linux which might help. Or FreeBSD for older RPis. &gt;But what I do know is that Haskell is particularly bad at breaking when the ecosystem changes and Haskell code rots way more than many other languages. Yes, but I think that also has to do with faster change in the Haskell packaging ecosystem. Which is sort of a good thing. &gt;Uh oh. GHC will be broken. And after it's fixed, I need to wait for a new Stack + snapshot, then I need to migrate all my code to a new compiler &amp; library versions I think the bug is fixed if you grab the latest code that builds, actually. Less than ideal, but that's the nature of software - bugs are fixed only in releases. Personally I've found stack pretty good so switching snapshots never produced any problems for me. If it does, I'd recommend using cabal with `cabal freeze` instead, since it doesn't suffer from that problem. &gt;Other languages I've used are much, much better in regard to that. It can clearly be done. But part of that comes from the language being different - in the case of C++ just because it's been around so long. Rust working is great, but I'd have to look into that more - the bug you mention is not affecting everyone and I don't know the frequency of bugs in Rust. Also, Haskell has two build tools. If one of them isn't working, you should at least see if the other works.
HTML being defined by a DTD has never really been true, though. Sure, HTML 2 till HTML 4.01 were formally SGML applications, but aside from the HTML Validator AFAIK nobody actually used an SGML parser for HTML. Certainly no major browser ever has, from timbl's original WorldWideWeb (given, after all, it was only later that HTML was an SGML application!) to the major browsers today. From memory, the only difference is in cases with what the HTML spec calls parse errors (essentially, for each parse error you can implement it one of two ways: either you do what the spec says, or you stop parsing), which is how entities which don't end in a semi-colon are parsed (these are specially listed in the spec; it's not that you can omit the semi-colon off all): `&lt;div&gt;&amp;ampfoo` will result in a div element containing `&amp;foo` (i.e., having decoded `&amp;amp`), whereas `&lt;div class="&amp;ampfoo"&gt;` will result in a div element whose class attribute is `&amp;ampfoo` (i.e., having not decoded it).
&gt; What does the Haskell toolchain have, that these other language toolchains don't have, that makes it so brittle against OSX upgrades? To be clear, is this GHC or is this stack? Because there are two build tools and if only one is failing then "the ecosystem" isn't at fault. I suspect what makes it brittle is lack of developers using Mac OSX and lack of test hardware. 
Right I see it now.
&gt;I can't keep the entire chain stable because life. People buy new computers, want a new OS. That makes sense, but it can hardly be said that it's Haskell's fault OSX releases breaking updates and refuses to allow you to buy past versions. Besides, that example you cite is not something that would happen in production - that's more of a hobby use. &gt; And after it's fixed, I need to wait for a new Stack + snapshot, then I need to migrate all my code to a new compiler &amp; library versions, probably spend some time tracking down compiler errors, weird bugs, performance regressions. 99% sure it won't just work out of the box. But there are two build tools. I'm not sure you can say "the ecosystem" is at fault unless you've tried cabal under these circumstances. The fact that you won't get bug fixes for a year-old version of the software seems pretty reasonable. The GHC folks have supported everything that they said they would. 
&gt; It's not just recklessly upgrading that gets you in this situation. Right, but there's very little GHC devs can do if Apple's pattern is "break things every release" They just wait for bug reports and fix them (your example has been fixed). And reports won't come faster until more Haskell users are on Mac. &gt; Some people can deal with Cabal hell. Some people can stand super slow compiler times. Some people don't need an IDE. Some people are fine with code rot. Some people don't need ARM. Cabal hell is avoidable with sandboxes. It's incredibly poorly documented but it's not something anyone should have to deal with. I'm not sure where the "slow compile times" is coming from. Building yesod takes like 10 minutes on my 3 year-old computer. Code rot is a consequence of the pace at which Haskell moves. And "some people don't need ARM" is actually a pretty huge use case. Haskell is honest about its lack of ARM support (on stack) or second-tier support (for GHC) much like OpenBSD.
&gt; The ARM thing I can agree with you on, although that also doesn't preclude Haskell from being a robust practical tool either. I think ARM is a *weakness* of Haskell, I just don't think it's the worst thing ever. Especially when Haskell's #1 use is web services. Plus RPi 3 is a whole 'nother set of problems because the kernel reports the wrong architecture.
&gt; facts like "doesn't work on MacBooks and Raspberry Pis" fly straight in the face of that. QFT. This is why most of my day to day work is using python, even though I use it with a Haskell accent at times. 
there isn't any way for the type checker to infer what `s` is. you can either AllowAmbiguousTypes and use TypeApplications to specify, or give your function a proxy argument like `p s` to your function. unless MonadGoogle has a fundep. in that case the error might be something else, maybe give `comm` a type signature? it's hard to tell without line numbers and a more complete error message. 
With Cabal I'd have a much worse experience. Stack improved my code rot issues with stable snapshots and isolated compilers. I don't see how losing these features would somehow help me with this problem. The issue is simply that new compilers and libraries don't always maintain full compatibility while old ones stop working on new machines. Can't use the old, can't upgrade without breakage. The result is that perfectly fine code stops working, often quite quickly. Either I can't build with the old compiler, or the new compiler won't support the old libraries / my existing code. The end result is the same. Switching to Cabal would not help with any of this. I don't think constantly breaking existing code is acceptable in a production language. Either keep old compilers working or make sure I can upgrade to newer versions of the ecosystem without hassle.
Having thought about this a bit more, a similar (and close enough!) function can be defined which is more widely-applicable, using the help of `MonadTransControl`: class MonadTransControl t =&gt; MonadTransLift t where transform :: (Monad m, Monad n) =&gt; (m (StT t a) -&gt; n (StT t b)) -&gt; t m a -&gt; t n b instance MonadTransLift IdentityT where transform :: (m a -&gt; n b) -&gt; IdentityT m a -&gt; IdentityT n b transform f (IdentityT ima) = IdentityT (f ima) instance MonadTransLift (ReaderT r) where transform :: (m a -&gt; n b) -&gt; ReaderT r m a -&gt; ReaderT r n b transform f (ReaderT rma) = ReaderT (f . rma) instance MonadTransLift (StateT s) where transform :: (m (a, s) -&gt; n (b, s)) -&gt; StateT s m a -&gt; StateT s n b transform f (StateT sma) = StateT (f . sma) instance Monoid w =&gt; MonadTransLift (WriterT w) where transform :: (m (a, w) -&gt; n (b, w)) -&gt; WriterT w m a -&gt; WriterT w n b transform f (WriterT wma) = WriterT (f wma) instance Monoid w =&gt; MonadTransLift (RWST r w s) where transform :: (m (a, s, w) -&gt; n (b, s, w)) -&gt; RWST r w s m a -&gt; RWST r w s n b transform f (RWST rwsma) = RWST $ \r s -&gt; f (rwsma r s) instance MonadTransLift MaybeT where transform :: (m (Maybe a) -&gt; n (Maybe b)) -&gt; MaybeT m a -&gt; MaybeT n b transform f (MaybeT mma) = MaybeT (f mma) instance MonadTransLift (ExceptT e) where transform :: (m (Either e a) -&gt; n (Either e b)) -&gt; ExceptT e m a -&gt; ExceptT e n b transform f (ExceptT ema) = ExceptT (f ema) This looks like a clear generalisation of my first attempt (or specialisation, depending on your point of view). `MonadTransControl` is necessary (for the `StT` type family), but not sufficient, as neither `liftWith` nor `restoreT` let you change the underlying monad. The common-sense laws are a little trickier to state: 1. `transform id == id` 2. `Functor (StT t) =&gt; transform (fmap f) == fmap f` 3. `MFunctor t =&gt; transform nat == hoist nat`, where `nat` is a natural transformation. 4. `transform f . restoreT == restoreT . f` Some of those probably follow from parametricity, or are redundant. Also, the `(Monad m, Monad n)` constraint might be worth removing, as none of the instances use it and I'm not sure a law-abiding instance *would* use it.
Usually I solve "The type variable _ is ambiguous" errors by adding a type annotation to fix the ambiguous type to a specific type. Perhaps adding a type annotation (maybe just a partial one) to `comm` will make that error go away. If you want `s0` from the error message to be `s` from your constraint, you may need `ScopedTypeVariables` in order to provide the right type annotation. HTH.
&gt; Haskell works fine on Mac...just don't upgrade to Sierra yet. So it doesn't work on Mac, it works on *some* Macs. Please, let's not deny there is no problem when clearly there is. For me, GHC is the only thing ever that prevented me from upgrading quickly. And I'm still on El Capitan because of the GHC*. *ITT I've learned that brew includes patches already? Must verify. If that's true, I'll update today.
Fair enough, I have too. I obviously prefer stack but I've used cabal on RPi and it mostly worked (hackage has a version of optparse-applicative that doesn't actually work?) &gt;Stack improved my code rot issues with stable snapshots and isolated compilers. I don't see how losing these features would somehow help me with this problem. Cabal has freezes now which help a lot, though you'll still need to make sure it uses the right GHC. I guess my point was: if stack's approach is going to break things going forward, perhaps we should focus on improving cabal/hackage? &gt;I don't think constantly breaking existing code is acceptable in a production language. As you say, this is a problem with the package ecosystem as much as it is with the tools. Haskell moving quickly is also an advantage in many ways.
I think I've heard yesod has issues. I use snap and lens and it hasn't been a problem for me.
Pardon the pedantry, combinatory logic (combinators) was invented by Moses Schönfinkel and later rediscovered by Haskell Curry. 
What /u/bss03 and /u/mstksg mentioned is correct - the type checker is unable to infer what you mean by the `s` parameter in the `txCommit :: MonadGoogle s m =&gt; ...` constraint. It has a requirement that the `s` must contain `"https://www.googleapis.com/auth/cloud-platform"` due to the `projectsCommit` request you are `send`ing - but your type signature claims it holds 'for any set of authentication scopes in `s`'. 
How long were you on for, and how many times have you gone? While it's a pretty busy channel, it's not a *consistently* busy channel. It's not unusual, for example, to have a fairly quiet half hour followed by a dozen people having several concurrent conversations. You could have just gotten unlucky with when you were signed in.
Bartosz has a blog series which this video lecture series mirrors: https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/ The video series is in the process of catching up to the blog series; if you'd rather read than watch, this link will serve you well.
My problem has literally nothing to do with Stack. It has nothing to do with the build system. I don't understand at all why you bring Stack vs Cabal into this. If you neither support old compilers nor make sure new ones are transparently compatible, existing code will quickly rot and stop working. Keeping working code working is a pretty important aspect of a mature language!
It's not language-specific. Docker images are pretty versatile as a deployable format as you can deploy them in any cloud provider or your own iron.
&gt; despite the first beta of macOS Sierra being released like half a year ago, we still don't have a working GHC for it. Wat. Someone forgot to tell my GHC. I've been using Sierra since day 1 and I haven't had any issues.
Trying your suggestion like this: txRollback :: ( HasScope s '["https://www.googleapis.com/auth/cloud-platform"] , MonadGoogle s m ) =&gt; ProjectId -&gt; TxId -&gt; m CommitResponse txRollback projectId tx = Google.send (projectsRollback rollbackReq projectId) where rollbackReq = rollbackRequest &amp; rrTransaction ?~ tx I get this error: • Expected a type, but ‘'["https://www.googleapis.com/auth/cloud-platform"]’ has kind ‘[ghc-prim-0.5.0.0:GHC.Types.Symbol]’ • In the second argument of ‘HasScope’, namely ‘'["https://www.googleapis.com/auth/cloud-platform"]’ In the type signature: txRollback :: (HasScope s '["https://www.googleapis.com/auth/cloud-platform"], MonadGoogle s m) =&gt; ProjectId -&gt; TxId -&gt; m CommitResponse
wow, thanks for the offer! I'm exploring some other options right now, so won't waste your time, but I could see myself coming back around to opaleye! That lib looks awesome!
Stack associates a compiler with a snapshot specifically because we know that new GHC releases often break stuff. By not supporting old compilers I can basically chose between a compiler that doesn't work with my code (yet) or a compiler that doesn't work with my OS. The new compiler with the Sierra fix will likely not work with my old code / dependencies anymore. For instance, it might do changes like FTP, forcing me to add type annotations etc. to make everything compile again. If the old compiler would just keep working, I could build old code without changes. If the new compiler would be more comited to compatibility like Rust seems to be, that'd be fine as well. Unfortunately neither is the case, so Haskell code rot is a very real and annoying thing, no matter the build system. I don't care who's 'at fault'. Other languages seem to adopt to the OS changes in time, it's only Haskell that seems to be caught by surprise and then needs many months for a fix. You can argue as much as you want, but the fact is that other languages don't necessarily have this issue. There are many other languages that managed the transition between OS versions smoother by actually fixing the issues before they impact users. These languages also do a better job at keeping working code working, either by offering the old compiler / language standard or by making sure their new compiler has no breaking changes. Haskell is bad at that, I wish it was better.
Careful. Your error message mentions `HasScope'` but your new constraint mentions `HasScope` (no tick). The second argument to `HasScope` needs to be a type with a instance of `GoogleRequest` available. (Its definition uses the `Scope` type associated with `GoogleRequest` implementations.) The second argument to `HasScope'` (with tick) is a `[Symbol]`. I *think* the constraint you want is `(MonadGoogle s m, HasScope s CommitRequest)`, even though I can't find the `GoogleRequest` instance for that type.
FYI, I'm quite wrong. Search for `("Tesla Motors")[ON] and (4)[MD] and TESLA[WM]` in [TESS](http://tmsearch.uspto.gov/bin/gate.exe?f=search&amp;state=4803:rygv03.1.1) to find several results where they've trademarked the word "Tesla" with no special size, shape, or style.
What should we do? Work endlessly forever, never sleep, abandon a million other things to placate everyone, don't evolve the language or ever change it? Is there any limit to this kind of reasoning, even in the face of very low financial and human resources on our part? Or is it all just -- well, GHC team clearly needs to get their shit together, we need a Real Language today, and they don't "get it". Is that it? I don't mean to sound crass. I understand the complaint very well. But we are quite well aware of the issue. In fact, I was originally going to try and make a version of 7.8.5 for El Capitan when it came out. Guess what happened? I never had enough time to do it among 10,000,000 other obligations like the 7.10.x release and cat-herding 20 other developers, on a limited contract budget -- and nobody else ever did it, because everyone is mind-numbingly busy with everything *they* work on... I completely understand the complaint. And I understand: it's legitimate to have a problem, state it, and *not* have a solution or not be able to act directly on one. I understand. There is no conspiracy of people trying to agitate. But I'll be honest... Every time I hear this kind of stuff, and people talk about long-term support and this other crap, all I get -- an implicit reading of the message -- is, "Why didn't those people invest more of their life, their time, their energy *on my* behalf? Why aren't they helping me more?" Again, I understand. It's not really 100% entitlement or anything like that. But, if that's how people feel, I frankly don't know why I bother with the work or this community at all, if my only purpose in life is to endlessly be worked to death to satisfy everyone. I know that's not what you're saying. I know it's not what you wrote, or what you mean. But it's what I read, and frankly it's becoming unbelievably tiring.
yes, I've already figured out that might be it based on other people comments, but I generally find tools like slack better because they allow you to have different rooms within a "channel", like support, general discussion etc. which allows you to filter out the noise and focus on discussion better.
&gt; Does it take work, and time to invest in the skills necessary to be very productive? Yes, it does. But the idea you need to be some supergenius is just a totally false narrative, and plenty of examples exist to prove it This reminded me of [Richard Feynman's take on what it takes to become a scientist](https://www.youtube.com/watch?v=Cj4y0EUlU-Y)
There's a few different irc channels, which accomplishes essentially the same thing. There's #haskell, #haskell-beginners, #haskell-offtopic, #haskell-lens, etc.
Haskell is a community effort with no single head, though I dare say many of us often look to Simon Peyton Jones for moral leadership. Of the large list of organizations and people at www.commercialhaskell.org, many contribute work and/or funding; as do many others not listed there. On the financial side, historically some of the major financial contributors to Haskell have included Microsoft, Amgen, FP Complete (where I work, and where we strongly encourage work on open-source contributions such as Stack), and Facebook. Other companies with perhaps more skills than spare cash, such as Well-Typed, have contributed significant work; and of course absolutely essential work has come from people at a range of universities. You can find loads of resources and ways to get involved at sites including: www.haskell-lang.org www.haskell.org [The #haskell IRC channel](https://wiki.haskell.org/IRC_channel) www.schoolofhaskell.com [GHC Newcomers](https://ghc.haskell.org/trac/ghc/wiki/Newcomers) 
Combinators has more than one meaning. In this case it means "combinator library". This terminology was created by Hughes. &gt; "One of the distinguishing features of functional programming is the widespread use of combinators to construct programs. **A combinator is a function which builds program fragments from program fragments**; in a sense the programmer using combinators constructs much of the desired program automatically, rather that writing every detail by hand." See: http://www.cse.chalmers.se/~rjmh/Papers/arrows.pdf 
Hey Austin, I hope you know I truly did not write this post out of some sense of entitlement, but just genuine frustration with not being able to actually use Haskell. In the past few days I was considering picking up a new laptop and started to plan a new project that I wanted to deploy on ARM while also maintaining some code I already had working on an RPi. It became increasingly clear that not only would GHC not work on my new laptop, I wouldn't be able to maintain my old ARM code (tied to 7.x) without running an older GHC in a VM plus my new project would immediately hit a roadblock because current Stack doesn't build anymore on ARM and old Stack doesn't support GHC8. I was simply bummed that even though I'd actually like to use Haskell-the-language, everything about Haskell-the-ecosystem made me want to just use something else. I don't know what the solution here is. Clearly development tools have become something that people either expect to be provided by big companies for their platforms or made free of charge by the open source community. And the response to feature requests is frequently the old "why don't YOU pitch in and fix it" line. But being honest here, I don't want to work on GHC anymore than I want to repair the plumbing in my apartment or write my own WiFi driver. Most of us are very much OK with paying somebody else to do that. If there was a Haskell Ultra Premium Gold Edition for a couple of hundred bucks, fixing all these issues current GHCHQ doesn't have resources or motivation to address, I'd consider that a great deal. But I very much doubt that something like this would find much community acceptance. So it looks like unless Google or Amazon decides Haskell is the next great thing to base their tech stack on and throws a couple of billion at it, nothing much is going to change.
&gt; Haskell is a proper name of a deceased person; I doubt it can be trademarked (alone). Yes it can. People have trademarked colors, shapes, words, sentences, fictional characters; basically anything that is somewhat recognizable can be used as a trademark, and once you can show that you are indeed using it as a trademark, you can have it registered and protected. It's not so much about what the thing *is*, it's about whether you can plausibly argue that you're using it as a trademark. You may be confusing this with name requirements for registered companies; these do usually have stricter requirements, but that is mostly unrelated to trademark law.
I think the observation, which is important, is that frustration can go a few ways. One, less productive way, is a rant that effectively puts the burden on others. Another way is to at once seek reasons that are addressable and also potential solutions or initiatives, or at least point towards a recognition of the objective constraints on a genuinely open-source project that's as you note, not backstopped by a large company bankrolling significant development. Without the latter, it can come off towards others as though you're passing the buck to them, rather than just getting frustration off your chest.
Strange. That comment, though I can see it in your user page, does not appear anywhere in this thread.
Who owns C? For that matter, who owns English?
Looks like self-replies are shadowremoved. I've added it to the bulk of my main post.
It was just deleted by a mod. Probably automoderation gone wrong (like usual).
[removed]
It's not just that. As I read it, it's not about who owns the name, but rather, who "owns" the key contributions *to* Haskell. For example, suppose the Simons were reassigned by their respective employers. Neither works for a company whose critical success depends on Haskell. Sure, they could quit, but would they condition their next job on being able to contribute to Haskell? *Could* they? If not, could they simply chose to no longer work and spend their time on it? It's not just a simple question of legal ownership. Regrettably, there are market forces at work.
Please look at [Turtle](https://hackage.haskell.org/package/turtle-1.2.8/docs/Turtle.html). It seems to be what you want or close to it.
Ford, Dell, Boeing .. trademarking a surname isn't remotely unusual. 
I've always gotten a lot of information from your posts. If you need to leave out four-letter words to let me see them, I hope you will be willing to do so. Thanks!
&gt; How does one really influence the direction of Haskell without a PhD or decades of experience in compiler engineering? Participate in the community and get to know contributors to the infrastructure you're interested in. Then find out things on their roadmap that you especially want to see worked on and either work on them yourself or pay someone to work on them. Or if that amount of involvement seems too onerous for you, talk to the guys at [Well-Typed](http://www.well-typed.com/) and pay them to work on things that you want.
I try to keep this list up-to-date, feel free to add packages that I'm not aware of yet. :) https://wiki.haskell.org/Applications_and_libraries/Operating_system#Haskell_shell_examples
It is not possible for a single, small company to fund development of a complete language ecosystem. Only companies like Google or Microsoft can put their weight behind a tech like that. Hence the suggestion of Kickstarter campaigns and bug bounties to distribute the monetary cost, and allow multiple small companies with commercial interests to participate. 
This. It's not about who literally owns Haskell, but who is commercially interested and invested in its continued development and success. is this well documented somewhere?
The shell isn't strings, it's streams of bytes. When you say it that way it sounds much better and explains why it has held on so long.
Not even a little bit, so far as I know. Actually the hilarious part is that someone representing a corporation is asking this!!!! These sorts of questions are kinda why anti-corporate folk formed OSS in the first place.
I think "a little bit" is fair. For example the HaskCAR is sort of a limited who's-important-in-Haskell name list; it just lacks institutional affiliations. Maybe it would help if they were added to the next one? https://wiki.haskell.org/Haskell_Communities_and_Activities_Report
I think WarDaft's point is that "not even a little bit" is not necessarily criticism.
Dude I remember talking to you like ten years ago. You were aeons over everyone else in what you were doing. I remember looking up to you, and I still never reached your level of commitment to coding. Anyway, I would just like to point out that we have very capable dedicated people who are in charge of the language and there's no need to worry.
I'm curious how this compares performance-wise to airline, which is a lot lighterweight than powerline
Oh, hi! My sister mentioned you and I didn't realize you use reddit, too :)
I feel that commercial interests are very important for the health and longevity of a project, *generally.* I'm sure you can find examples of highly successful projects driven completely by unpaid volunteer effort, but my submission would be, that they're the exceptions to the general rule. So, if a highly visible single corporate sponsor is not how Haskell functions, what is the alternative funding model? Do you still feel the question is loaded, or based on false assumptions?
do you mean they are disparate in terms of being very different *types* of tech projects? if yes, then that was my intent. of putting forth examples where clear commercial interests have lead to successful projects across a spectrum of project-types. if you feel that the list has absolutely nothing in common, which means that these examples don't support my hypothesis, then please elaborate on why you think so. 
That's not really true. Even one or two developers being paid to spend a lot of their time on GHC / Haskell would be a big help, assuming they are good enough programmers to be able to contribute meaningfully.
Just google Austin Seipp and you'll find everything you're asking for. Though I encourage you to respond to his points directly. They stand on their own with or without knowledge of who Austin Seipp is. There is no reason to bring his personal journey into this.
I don't see how you got that. Free and open source projects are not removed from commercial interest by default. Facebook employees have spent considerable company time improving GHC, for instance. Their point was that Haskell is an open source committee language, and that this means the ownership is largely irrelevant (which, as OSS would say, is as it should be).
Even despite that, you can contribute by being an active *voice* in the community. There are definitely priorities and works in progress that can be influenced by community input.
There are definitely no orphans there.
In which case my conundrum is simplified. Is there a document which describes the stewardship and corporate backing (in any way, hard cash, paying a developer, etc) of important projects? To name a few: * GHC across various platforms and OS * cabal/stack * IDE plugins * Production monitoring tools * Batteries included standard library? 
Why should anyone care about your worldview? What is your contribution besides asking questions and then questioning answers?
Would you like to share possible contributing factors apart from corporate backing or commercial interests? Not to argue, but to complete the discourse. 
It looks like all of the scope related type classes could use some curation to evoke more friendly error messages, I apologise for the confusion and will look at improving it in the next release.
Oh, OK! I just vaguely remember that there were some GHC8 related bugs with Stack that got fixed around that time, so I thought I basically need the new Stack for GHC8.
It's probably just different priorities. For me personally, I'd value stability and compatibility much more than new features. I'd be perfectly happy with basically freezing the language and standard library for lengthy stretches of time in the name of keeping stuff working. Others probably disagree. Many people are very much interested in Haskell as a research language or as a playground for new techniques.
So, I could be completely wrong about this (and please assume I am as a default, until proven otherwise, and even then..), but my impression of the HCAR is that its a record of things that people use Haskell for and want to tell other people about. We're not exactly a community that can be picky about publicity... I'm pretty sure I could have possibly applied to be in it, if I wasn't horribly embarrassed by anyone seeing the quality of the Haskell code base that I manage. I definitely do not think that you could get any idea of where Haskell might be headed in the future by following HCAR. Not even remotely. And I think that's what the OP is kinda interested in. For example, to refer back to my earlier statement... Imagine if the Simons quit! That would be drawing close to anarchy in Haskellland. How would you know that though? It's not written down anywhere, it's just something you pick up.
&gt; Is there a document which describes the stewardship and corporate backing (in any way, hard cash, paying a developer, etc) of important projects? No, there is no such document. I say this confidently because the nature of the corporate backing changes all the time, and often in ways that is not visible. A company might want some feature contributed to GHC or Cabal, and they might start paying Well-Typed to do it. Or another company might decide to have one of their employees spend 20 hours / week working on reducing GHC compile times. None of these things will be recorded in a document anywhere. And in the latter case, no money will even change hands apart from the normal salary that company was paying to that developer. This document you are asking for is impossible to create because there is no registry of who's doing work, and if there was it would be immediately be out of date. Your above comments seem to suggest that you think there's a black and white distinction where something is either corporate sponsored and dependable or it is an undependable hobby project that you're not comfortable with. This is a false dichotomy and I think it is the root of why you're getting the response you're getting here. If you haven't read it, I would suggest you read [The Cathedral and the Bazaar](http://www.unterstein.net/su/docs/CathBaz.pdf) by Eric Raymond. I think it will give you some perspective that you might be missing.
I do believe the OP even went so far as to recently publicly post if Haskell was a good fit for their company, and even pay someone who was skilled here in this subreddit. And OP and does really seem to be giving it a go at trying to write Haskell. This subreddit isn't private, you don't need a toll to post here. I spend much less time on Reddit than I used to. Lots of people here still post much more frequently than I do these days, so many Redditors in these parts know me. And while I've seen OP before I don't really know too much about them beyond a few posts or so. But I don't think someone would go so far to troll in bad faith to the point they'd suggest job offerings and serious commercial consideration. That seems, uhhhh, weird. Knowing the difference between an obtuse troll, and a mere communication barrier -- "same page, different book" -- is a vital skill on its own. My spider sense indicates the OP may be eager, yet new. Deadlines linger in the horizon, the same for all of us. I think that's fine. Some people are just harder to jive with due to communicative norms/barriers (not pointing fingers). That's fine too. If you don't think that's fine then I guess we'll disagree or whatever. But, look: I do not agree with some of the OPs points. Hell, you know what, let's go further -- I just don't like some people at all in this world, and I dislike certain attitudes, and certain foods. That's legit. It's also not an unlimited blank cheque to act like a dick and accuse people of bad faith or whatever. It seems a bit unwarranted in this case, IMO.
What about a given name? Haskell isn't a surname.
Far fewer examples come to mind (a couple of fashion brands, ben&amp;jerry's at a push), but simply because given names aren't quite as strong (eg, think McAfee antivirus vs Norton antivirus - and how silly it'd have sounded if it was John's antivirus vs Peter's). The legality is no different though. You can claim a trademark pretty much anywhere where someone using a similar name could be seen to be causing confusion. The classic example is Apple Computer vs Apple Records (the Beatles' record company), which wasn't an issue (because their names were distinct within their fields) until Apple Computer started selling music (and related products). But neither prevent anyone selling fruit, just as Microsoft don't stop glaziers selling Windows. If "Haskell" could be seen to have a strong association within computing, it'd be no less enforceable than Apple or Windows - as long as someone could make a decent claim to 'own' it. Possibly most salient example, however, is the number of trademarks that already exist for "Haskell" in the US - in Clothing, Fishing equipment, Construction .. (you can search [here](http://tmsearch.uspto.gov/bin/gate.exe?f=searchss&amp;state=4809:ezvrmv.1.1), I can't seem to link directly to results).
Trademarks on generic words can be tougher to defend. Microsoft tried to sue an OS company named Lindows and came close enough to losing thier trademark on Windows that they ended up dropping the suit and paying Lindows a bunch of money to change their name.
My employer has funded a lot of development for packages on hackage as well as diagnosing and sometimes even patching bugs in GHC and cabal. We don't claim any ownership of Haskell though. We simply are the change we want to see. I think a lot of grunt work is quietly done by people with commercial interests who simply need something done. Obviously, we are not the ones pioneering things like new compiler extensions. But things like stack did start as an internal project created by some company trying to solve their own internal problems.
Why don't you try it? `chsh ghci`, and please report back after you used it for a while. Like 2 weeks or so. I'm saying this because I think that this post is somewhat dishonest and mostly lazy. You sound like you are already convinced that ghci as a main shell is a bad idea, yet you want others to tell you why without getting your hands dirty. So what you should do is actually go ahead and try it. There is nothing holding you back. And if you do it, you can then post actual valuable insight instead of self-important pseudoscientific paragraph headers.
Haha, I wrote an extremely hacky version of something like this with a bash script the other day! Nice to see a "real" approach - thanks for sharing!
I have a turtle shell that I use occasionally, but always having to quote everything gets tedious unsurprisingly quickly. In practice it would probably be better to use turtle or shelly as the basis of an actual freestanding shell.
For starters, here is a list of projects dritten in Haskell with [more than 1000 stars in github](https://github.com/search?q=language%3Ahaskell+stars%3A&gt;1000). People use haskell for all kind of things, and it's usually safer than using C and more efficient than weakly typed scripting languages. Is it worth learning? Can't say for sure -- it was for me.
My coworker, a member of the POSIX committee, likes to say “shared libraries are the UNIX concept taken into the 21st century.” And that seems to be exactly what you want to do. However, I'm not sure why you want to use Haskell as a foundation for that. 
This is awesome ! Thanks you a lot !
This one premise &gt; money (and therefore decision making) is a really good reason not to want corporate sponsorship. Right now the community is quite diverse and lots of people are taken seriously in it, and if corporate influence were to have priority then I bet this would go away to a large extent if your premise were true and we had corporate funding.
Each tool has its own list of contributors. There's no particular reason they'd be all pulled together, anymore than you'd find a single document for the java ecosystem that talked about all the various IDEs and editor modes and also apache commons libraries and e.g. maven and ivy plugins. That's just not how open source works.
OP is a troll. His entire history here has been 100% trolling, and /r/haskell is too nice to admit it and keep downvoting people who point it out. Stop feeding the troll, it is the reason he's spamming the sub.
I mean, yes, HCAR is just a record of things people are telling others about. But I think *that's what s/he wants to know*. I read /u/saurabhnanda as asking, "hey, I am on the fence about investing my workgroup in Haskell when I don't actually know how big and diverse it is." Consider some of the known topologies for different projects: * Solaris. Nominally open-source but the copyright is entirely owned by a hostile-to-open-source company which refuses to divulge the source code, making it de-facto closed source. * Go. This is a project which is undisputedly by Google, which hypothetically could close the doors on it if business interests aligned, but it would seem out-of-character for them and they're not really seeming to be heading that way. * PHP. The main developers are all part of one company, but at least that company's main purpose is to spread PHP over the globe. Also we're fortunate (maybe?) that Facebook was written entirely in PHP and open-sourced HHVM as a secondary implementation. It's unlikely that both of them would either drop or kill PHP. * Python. Development is a little slower because it's spread out over so many companies. There is a dictator who could take the language in disastrous directions. He works at Dropbox and worked before at Google so he's probably not hurting for cash or job security right now. So the threats are much more distributed. * Node.js. There is a not-for-profit company devoted to maintaining these libraries with strong support from a dozen or so companies. They survived an abdication of a benevolent dictator and both the cost of a major fork and its eventual merge back into the main branch. Might nevertheless die of attrition, but that's a slow death which will be visible well in advance. * Javascript itself. There are a bunch of major competing implementations sponsored by huge companies, which all have united together to try to form and fund committees to decide the future of a language which cannot die anytime within our lifetimes. * Linux. There is a 501(c)(6) not-for-profit which is supported by hundreds of companies, they've got a lot of money to give a small bureaucracy of core developers their own individual employment. What /u/saurabhnanda is looking for is the following information, "hey, Haskell is a bit decentralized. Most of the popular work is done on the Glasgow Haskell Compiler, in which the two biggest names are Simon Peyton-Jones (Microsoft Research) and Simon Marlow (Facebook), but then there's also an active but smaller project in the Utrecht Haskell Compiler which is explicitly being supported by the Computer Science department at Utrecht University, and there's another (academic?) project that has Haskell 2010 running on the JVM called Frege, as well as some smaller implementations. "There is a bunch of active academic support (at least I know of: Joachim Breitner at Karlsruhe Institute of Technology, Stephanie Weirich at UPenn, and the folks at Utrecht University) but also a lot of support from the finance industry (Dijkstra from Utrecht is now working for Standard Chartered Bank; Ed Kmett works at S&amp;P). The Haskell community is big enough now that a bunch of contributors are consultants (FP Complete, Well-Typed LLP)." I think what /u/saurabhnanda needs is the HaskCAR report[1] and the Haskell in Industry wiki page[2]. It's not about "where Haskell might be headed in the future" so much as "is Haskell nosediving into a dumpster in the near future?" And to that we can say "It's still a bit smallish but the risks are much more distributed; its chances of nosediving are probably a little less than, say, PHP nosediving because Zend gets bought out by another company who then decides that they don't want to keep maintaining PHP." Simon Peyton Jones has some talks where he starts off with a slide of "here's how most successful programming languages go" and it's got an exponential graph, then "here's how most programming languages go" and it's got a growth to like, 20 people, and then it goes back down to 0; finally "here's how most research languages go" and it's got a long flat usage of 1 person for apparently their entire life; and then "here's how Haskell has gone" where it's a bit of a creaky ascent followed by a steady upward line. We're supposed to be communicating that if both Simons quit then actually the interest from the financial services industry and the other researchers and consultants would probably keep the thing going for at least a decade or two, if not indefinitely. [[1] Again, HaskCAR linked here for convenience.](https://www.haskell.org/communities/05-2016/html/report.html) [[2] HaskellWiki: Haskell in Industry](https://wiki.haskell.org/Haskell_in_industry) 
Hasn't the Oracle v Google case (ruling in favor of Google) taught us that APIs/languages can't be owned? As far as I can see, Haskell is simply the API for GHC. At least practically speaking.
What do you use to write PDF documents? https://hackage.haskell.org/package/HPDF ?
&gt; My assumption is that there needs to be a single person, or a tightly knit committee to provide vision and ownership to a project. In my other comments in this thread (and everything I write really), I have carefully tried to use the principle of charity and make sure my words do not come across as dismissive/defensive/aggressive. I'm still trying to do that here, but let me be a little blunt for a moment. I think this assumption is just plain wrong. The very existence of Haskell as it is today is proof. There are plenty of other examples of successful open source projects, so you don't have to just place your faith in Haskell. Look at the wikipedia page on [Open-source software](https://en.wikipedia.org/wiki/Open-source_software). I think the sooner you move past this view the better off you'll be. At the very least, you'll find yourself better received in this subreddit because that is not a common view here. It's perfectly reasonable that you would want to assess Haskell's maturity to get an idea of whether switching to it would exceed your tolerance for risk. But I think you're asking the wrong questions. Rather than looking at the individual pieces, look at the overall trend. Haskell has been around for almost 20 years now, and it has established a very clear trajectory. Rather than asking who's in charge, ask who else has made significant bets on Haskell. I've been doing Haskell full-time professionally for almost 7 years now, and the last three companies I've worked at bet their existence or at least a very significant part of it on Haskell. At this point I think it's safe to say that I've pretty much bet my whole career on Haskell. If Haskell disappeared tomorrow, much of the experience I've gotten over the last 6-8 years would be useless. Fortune 500 companies have put and are putting tens of millions of dollars on the line dependent on Haskell's ability to execute. Haskell isn't a project. It's an ecosystem. There is no central management. There are only people working on little corners of it. To me this seems so obvious that I have a hard time understanding how you can possibly have been asking questions in this subreddit for 6 months and still not seem to appreciate this. But I'm trying my best to give you the benefit of the doubt. If the above lines of thinking don't give you enough confidence, then identify one or two areas that you are most concerned about and start talking to those people. Perhaps your biggest concern is GHC. Then join the #ghc IRC channel on Freenode. Follow it on Github. Read the issue tracker. Find a bug that is important to you and ask what you can do to fix it. Communities this big don't just go away. It's counter-intuitive, but I would say that this base of small organizations is actually MORE dependable than a big company. If Facebook decided to stop using Haskell tomorrow, it totally could, and it would be perfectly fine. But a 3-person company built almost entirely on Haskell code is going to have a LOT more incentive to invest in the future of Haskell. Also, even if all the GHC devs died today, we'd still be fine. The code would still be available on github and we'd still be able to use it to develop new software. In the long term we'd still need to do maintenance, but I can guarantee you that all the companies using Haskell would start allocating resources to fill the gap. If you look at all this evidence and decide that Haskell is still too risky for you, that's fine. But realize that most of the people here are probably going to disagree with you.
&gt;Is it worth learning? Can't say for sure -- it was for me. That's always such a weird question. It's hugely dependent on what you want to do with it I think. Although I can say Haskell has made me a much happier programmer.
I value honesty more than being nice.
IMHO - not a whole lot. C++, OCaml and Python all hit their respective sweet spots, while Haskell is powerful but super awkward. I think it's worth learning because it will make you better at other languages. But I don't think it's very much viable as a production language (hence the very high hobbyist:professional ratio in the Haskell community.) E: also, I think it speaks to the Haskell community's intellectual honesty that this post is upvoted in the positives. In most other programming language subreddits, a similarly tough assessment would have cratered.
This looks like a very useful document, and I hope you will keep it update as things change. Thanks for the work!
huh, I wonder why I'm not getting that. afaik I have a pretty normal (though minimalist) setup. Are you using a pre-built binary? I'm pretty sure I built from source (7.10.3) manually, but it was a while back.
Because complex functions need to be run separately during development, refactoring, unit tests, and regression testing.
I... don't really understand what you mean there.
That's fine, it's a free country, and we are all free to respond to trolls like OP however you wish.
You are quite close. It seems like you have trouble understanding pattern matching on lists. I would suggest beginnings :: [a] -&gt; [[a]] beginnings xs = begg 0 xs where begg n xs = ... -- your implementation here Your solution isn't far from that, you just need to figure out what `[x]` is (not what you want), and when to stop recursion of `begg`. Note that there is other solution using suggested `map`. You might observe that `map length ["", "a", "ab", "abc"] == [0,1,2,3]`. Can you transform `[0,1,2,3]` back to `["", "a", "ab", "abc"]`?
I switched from Ocaml to Haskell back in 2008 for two reasons; purity (strict distinction between pure and impure code enforced by the compiler) and larger more active community. I now work on a team of 10-15 haskell programmers working on data processing and web stuff. We have a coding standards, code reviews, a heavy reliance on QuickCheck and a bunch of custom tooling. Coding in Haskell for a living is nothing short of a joy! A combination of designing types to disallow invalid states and heavy use of QuickCheck to find corner cases means I spend far less time debugging than I do in any other language I have ever used. 
&gt;You have unequivocally used "adversarial language" and implied the OP is being "deliberately obtuse". I have not. I did not say he is obtuse, I said he is a troll. This is not a debate of an issue where judgements differ. This is me acknowledging that he is doing precisely what he brags about doing in IRC: trolling. So as I said, that email has absolutely nothing to do with this. That email is about people in the community with differences of opinions on technical matters remaining civil. And again, I have remained civil.
Yeah, quoting someone means you agree with them 100% and are on the same page. What a genius!
Hint: think about how to get `beginnings "abc" = [] : ["a","ab","abc"]` from `beginnings "bc" = ["","b","bc"]`.
Sorry, i dont get it.
dang you really are as smart as i thought. what about that autograph.
Thanks to Takenobu for taking the time to put this together! 
&gt; No they have not. Your link has nothing to do with this. While SPJ's mail may have been motivated mostly by comments by other people, his call for a more respectful discourse certainly applies to you too. By calling the OP a troll, you're suggesting that he's acting in bad faith, which sabotages our common goal of having respectful and constructive discussions. Actually, your recent comments in other subreddits (examples: [1](https://www.reddit.com/r/TumblrInAction/comments/5b2119/stop_the_straight_white_boys_from_majoring_in_film/d9lj15n/?context=3), [2](https://www.reddit.com/r/KotakuInAction/comments/5ajhm8/humor_brazilian_artist_ana_koehler_replies_to/d9jb28d/?context=3)), make me wonder if it is your racist and anti-Semitic views that keep you from treating some community members with respect.
Nobel prize winning Putnam-fellow on not needing to be a genius. I admit that rationality is worth a lot of IQ but it's hard to appreciate this kind of advice coming from a genius.
My understanding is that (at least) `reactive-banana` and `reflex` have both pull and push based parts, but since there are denotational semantics for both I don't think either of them end up with problems with composition. Have you written much code with either of them, or any other event-and-behavior FRP libraries? From some of the previous threads where you've discussed the troubles with Behaviors, it was hard to find a way to interpret what you were saying in a way that was compatible with a solid understanding of event-and-behavior FRP. Which is fine on it's own - not everyone cares about or has time to dive into everything. It just bothers me a little that `transient` is being brought up in a high proportion of /r/haskell threads that mention FRP, while some of the anti-events-and-behaviors FRP points being brought up alongside it seem to be factually shaky at best.
I'm somewhat puzzled by your use of the Mongo database at all. Why do you need it? Couldn't you just use a message queue as the intermediary between the scraping part and the backend Elasticsearch database?
Sorry, but I'm not playing your game, there are plenty of other people willing to here, ask them.
We use commercial pdflib C library. I wrote C bindings for it. 
&gt;saurabhnanda works in India and is presumably Indian How was I supposed to know that? And why do you think I hate Indians? &gt;Another target of your attacks was snoyberg who is Jewish. Huh? When have I attacked him exactly? &gt;try assuming that people on this subreddit act in good faith. That is rather hypocritical from someone blindly accusing me of attacking people because I am racist, despite no evidence of me attacking anyone for any reason, much less due to racism.
It all depends on what you intend to develop. If it is a GUI, the answer is no because you have to compile it for Linux, Windows and OSX and most of GUI individual and corporate users are on Windows, so you will have to compile and distribute for Windows. For GUIs you will be better off with Scala or F# ( similar to Ocaml). Libraries and compilation time and sometimes lack of documentation is another problem. A reson to learn Haskell is that the state of art of FP and most of FP research is in Haskell. You can apply what you learn in Haskell to other languages, I mean you can have algebraic data types, monads, functors in OCaml, F# and Scala as well. IMMO For getting stuff done quickly I would consider F#, that is similar to OCaml, runs on .NET platform and on Mono and has a huge number of libraries. I needed to create a script or app to extract images from clipboard and write it to a file that could work in Windows, Osx and Linux. I wrote in Scala because, if it was written in Haskell I would have to dive in 3 OS foreign function interfaces and compile and install all gui libraries to each of them.
&gt; "Adversarial language" is used by the OP in endless aggressive rhetorical questions. What does that have to do with whether someone other than the OP used adversarial language? Furthermore, you don't need a dictionary to see there is a highly relevant difference between asking "aggressive rhetorical questions" about the topic at hand and calling someone a troll.
Thank you. I'll keep it :) 
Thank you for your advice and great works for community =) 
&gt;there is a highly relevant difference between asking "aggressive rhetorical questions" about the topic at hand and calling someone a troll Asking aggressive rhetorical questions *is* being a troll.
I can write `f` for multiplying acceleration and mass, and then `g` for multiplying velocity by time. And about a zillion others. And then the same thing in reverse order. Oh if only we had some kind of language magic that would allow us to denote all of this with one symbol, say `*`... Edit: letters.
Here's another approach using phantom types: data Force data Mass data Acceleration data D a = D Double f :: D Mass -&gt; D Acceleration -&gt; D Force f (D m) (D a) = D (m * a) I'm not sure how this compares with the newtype approach.
In the above, I have claimed that some aggressive rhetorical questions aren't trolling. If you misread that as "aggressive rhetorical questions is never trolling", then it is not me who is making basic logic mistakes.
https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#data-science
Sure. And there are some Haskell libraries for that. There are issues though, like the fact that the ratio between a meter and a yard can't be exactly represented in the Double type or that the number of Plank lengths in a Megaparsec is out of range for many types. So, for now, I recommend not waiting on a "perfect" solution and getting something that has the right trade-offs between genericity safety and ease-of-use. Units are sort of an type-level abelian group with several generators and while the generators (length, time, mass/energy) are generally agreed upon in SI, what if I want to handle some fantasy (or weird, theoretical) physics?
This book looks good, though I haven’t read it: http://haskelldata.com/ And recently some folks have started working on improving the state of data science in Haskell: http://www.datahaskell.org/ 
Yes, in that logical case your posts are 100% irrelevant to determine whether someone is trolling or not, so you can choose that path too, if you like.
If you used newtype it's basically the same. The phantom type approach allows a bit more code to be reused without typeclasses.
Yes, exactly! You *have* to write down your own specialized function every time you need to open up newtyped values in order to combine their underlying representations, and that's a good thing. If you use the underlying representation directly, you can code faster but you are more likely to accidentally multiply a velocity by a number of potatoes when you meant to multiply it by a time delta. Newtypes intentionally slow you down every time you attempt a "dangerous" operation, that is, the first time you multiply values of different types. So you get a type error, you start to fix it by writing a specialized function for multiplying velocities by potato counts, and hopefully that slows you down enough that your brain has the opportunity to ask: do I really want to do this? Newtypes are a very basic form of type safety in which everything is disallowed by default except what you allow using GeneralizedNewtypeDeriving, but everything is also allowed if you're willing to jump through the hoops using the wrapping/unwrapping dance. So the safety is really based on the programmer deciding what is safe by deciding when to jump through those hoops. There are of course plenty of more advanced ways to express in the types which combinations are allowed or not. If you're so inclined, you can try to teach GHC how to do dimension analysis using something like this: type Position = Unit 0 1 0 type Velocity = Unit 0 1 (-1) -- m/s type Acceleration = Unit 0 1 (-2) -- m/(s^2) type Force = Unit 1 1 (-2) -- F=ma, so I guess kg*m/(s^2)? add :: Unit mass position time -&gt; Unit mass position time -&gt; Unit mass position time multiply :: Unit mass1 position1 time1 -&gt; Unit mass2 position2 time2 -&gt; Unit (mass1 + mass2) (position1 + position2) (time1 + time2) As always, it's a trade-off between the time it takes to construct or learn how to use such a library vs the amount of time it will save you compared to a simpler solution like newtypes vs the number of bugs each solution would catch.
There is already an advanced library that does that and more, called [units](https://hackage.haskell.org/package/units). I actually wrote something like it for C++ many years ago ;)
The problem with type aliases is that they require an indirection from the programmer reading the signature. Sure, I need a Mass, but is this a Double?
If I may suggest a less popular opinion, you probably won't be more productive than Python with Haskell if your metric of productivity is "getting stuff done". Python lets you takes shortcuts, Haskell doesn't. Haskell has an implicit philosophy that [*worst practices should be hard*](http://www.haskellforall.com/2016/04/worst-practices-should-be-hard.html). After four years of using Haskell on a daily basis, I have found I am equally or slightly less productive in Haskell than say C, Java, JavaScript, or Python if I just need to get something done now. However, my Haskell code is consistently a pinnacle of maintainability where the others aren't. This is because if I just want to "get something done" I can use the other languages which allow me to be sloppy and take shortcuts. If I want something to work reliably, efficiently, and be maintainable, I choose Haskell every time. For just getting something done, where these other factors are a minimal concern, I often choose another language. 
My preferred way would be: newtype Mass a = Mass a newtype Acceleration a = Acceleration a newtype Force a = Force a
I guess Mongo is pretty much used as a message queue in this relation, so I'm mostly just trying to understand how I can, in a functional way, consume items from a database/message queue and ensure that it has been properly added to Elasticsearch before moving on to the next item.
This is certainly helpful. However, how does it ensure that things are in sync in case of errors when updating the production database?
Sometimes newtypes are too much hassle, but the bigger issue is again that each newtype has it's own way of being worked with and constructed. You again need to use the documentation every time you stumble on a function that has an unfamiliar type as an argument.
Haskell Data is a really bad book
It's definitely getting better. A few data science libraries have come out in recent months (like tensorflow bindings and a dependently typed neural network whose name escapes me). Plus the Data Haskell group has formed, though I don't know how much progress they've made. I'd still say Haskell is very immature in this area, but I think it has great potential to have some of the nicest libraries in this area, if only people do the work.
&gt;I don't think I have written anything that suggests otherwise. Then it is not relevant, like I said. &gt;The subject of the e-mail is the "shared dialogue" of the Haskell community. Not the "Subject: " email header, the subject as in "a person or thing that is being discussed, described, or dealt with." &gt;For instance, one of the key moments in that flame war was a rather inflammatory pair of blog posts by Michael Snoyman about governance by the haskell.org committee Which was about "Some are technical judgements; others are to do with an assessment of what will be most helpful to new users". He was upset that stack is not being promoted by the core haskell group when he feels it is technically superior, and that this and the website in general are less helpful to new users. 
&gt; You have to write down your own specialized function every time you need to open up newtyped values in order to combine their underlying representations, and that's a good thing. Whether it's a good thing is at the *very* least somewhat context-specific. I am certainly not ready to adopt a Haskell programming convention wherein everything must be wrapped in newtypes. There's a much higher bar for wanting to change the types, versus just wanting to attach some documentation about what the parameter means.
Wow, who made such a sanctimonious rule? Like you say, it's a place for hackers to talk to each other, they should be able to swear if they want.
Almost, it's the speakers of English.
As always, it's a trade-off between the time using newtypes will save you compared to a simpler solution like type synonyms vs the number of bugs each solution would catch.
Newtypes are always constructed the same way. The only difference is the name of the constructor. There are also generic ways to work with newtypes, like the lens package's [`Control.Lens.Wrapped`](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Wrapped.html) or the newtype package's [`Control.Newtype`](https://hackage.haskell.org/package/newtype-0.2/docs/Control-Newtype.html). There's even [`Data.Coerce`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Coerce.html).
You can always make both the semantic type and the representation type visible in the type signature (if you want; there's a good arguments for making the representation of Mass opaque). You can do this both with aliases (`type Mass a = a`) or separate types (`newtype Length a = Length a`). In fact, my code examples in the thread do so.
The hint is asking you find the recursive case. I.e. the smaller / simpler case of `beginnings` inside the specific case of `beginnings` you were given. It's the same as the hint from /u/echatav, just less direct.
Or, point-free beginnings = uncurry map . (flip take &amp;&amp;&amp; enumFromTo 0 . length) Using `(&amp;&amp;&amp;)` from `Control.Arrow`.
&gt; It really makes me want to give up on Haskell for anything that isn't Linux x64 compiled on the beefiest CPU available. And that's a shame! Is it really fair to call non-RPi CPUs "the beefiest available"? My experience with the RPi is that it's excruciatingly slow, easily making an Intel Celeron look fast. I switched to the Intel NUC after playing with the RPi, simply because I couldn't get enough performance out of the Pi, unfortunately. I still think they're cool devices, but I would never have the patience to compile on the Pi itself. I can just imagine waiting 20 hours, only to come back to some typo error to fix, and then waiting 20 hours again. I've been building all my projects on my MacBook Pro, with its 2.2 GHz i5, and I've had no problems. That isn't exactly the top line of CPUs. RPi's are cool because they are low power, but that doesn't mix very well with actually compiling stuff on it.
The indirection can be made local. f :: (mass ~ Double, acceleration ~ Double, force ~ Double) =&gt; mass -&gt; acceleration -&gt; force Unlike your `Represents` (but like your `:::`), this is not restricted to types of kind `*`.
Another neat solution. I like it!
Yeah, but in this sense Mongo is already acting like a message queue, because it will also "deliver all messages" in the sense that they won't be removed until I make sure they are in Elasticsearch. But yeah, the part about confirming that they are in Elasticsearch is indeed what I'm trying to solve. Also, I might have phrased the problem badly, but my difficulty really doesn't lie in it being hard to compare the data in Mongo and Elasticsearch, and since the mongoDB package is pretty easy to work with, I have the data available from there just as if it was a message queue. My problem is that I have no idea how to structure the program after getting the data, i.e. the logic of ensuring that each item is in Elasticsearch before deleting it from Mongo (or releasing the message if you will). I've gotten a few useful pointers from other responses and on Stack Overflow though, so currently trying to work on it some more, but really not getting far at this point.
Um, no. I don't want every quantity to have the same type. Did someone imply otherwise? I'm saying that every quantity having the same type doesn't solve you the problem of the ratio between a yard and a meter.
&gt; The printDocs function is the same as in the example, and I don't really understand how it works; I don't know what liftIO or mapM_ do, other than liftIO somehow turning something IO into a monad (a different one than the IO monad?) and mapM_ being some kind of map operation acting on a monad (really have no idea what exactly it does), some explanation of these would be appreciated. Don't worry. I was in this state 6 months ago! Hang in there. It's a steep learning curve, but the payoff is worth the effort :) Do you understand how this alternative syntax works: printDocs :: String -&gt; [Document] -&gt; Action IO () printDocs title docs = liftIO $ do putStrLn title mapM_ (print) docs (I hope that compiles) Can you do the following basic tasks: * Print the length of an `IO (String)` (as opposed to a `String`) * Loop from 1..10, printing each number * Loop over `[String]` (list of `String`) printing each
I have the book, its garbage. You'd be better off saving your money.
I would argue, yes, it is a highly effective language that can be used to build complex and reliable applications in a fraction of the time compared to alternative languages. I gave a talk to Boston Haskell about this: https://www.youtube.com/watch?v=R4nLSxCKkNw
those aren't orthogonal lol
I do read the type signatures of all the functions I encounter, but at this point it's not usually that helpful. Take the signature of `mapM_`: `Monad m =&gt; (a -&gt; m b) -&gt; [a] -&gt; m ()` What I don't really get about this is the return value; how does this work for other monads than the IO monad? Also, I have some ideas about both `fmap` and `&gt;&gt;=` from my reading, but telling me to "really understand" them is not really going to help me much. However, no, I don't really need to understand `liftIO` yet, I was just wondering what it did since it was a part of the quick example program for the mongoDB package; what I really want is some progress on what my program is actually supposed to do, get items from Mongo, ensure that they are in Elasticsearch, then delete them from Mongo, rinsing and repeating ad infinitum. I understand that there are a lot of basic concepts I need to understand before it will be clear to me just how I can do this, so again, I appreciate all help.
&gt; Actually, that's a very interesting question. I think mapM_ will make sense for only those monads that have some side-effects. Else, it's just a no-op. That's not really correct though. `mapM_` with `Writer`/`State` can "submerge" a list of values into the state even if the return value of the action is `()`. `mapM_` with a `Producer` from pipes can yield a list of values downstream, and so on.
I didn't really understand that, is it possible to explain to a novice, or does it require more knowledge?
If you check my edit, you'll see that I found out I could use `fmap` to find the length og the `IO String` returned by `readFile`; is this how `fmap` is implemented for `IO` in general, unpacking whatever value wrapped by `IO` and passing it to the function? Alright, just to be honest, that didn't really enlighten me with regards to `liftIO`, but I'll just keep it in the back of my mind that it exists to combine `IO` with other monads in some way, feel free to correct me on that. `:i Action` yields: type Action = transformers-0.5.2.0:Control.Monad.Trans.Reader.ReaderT MongoContext :: (* -&gt; *) -&gt; * -&gt; * -- Defined in ‘Database.MongoDB.Query’ Looks like a monad transformer based on the line `transformers-0.5.2.0:Control.Monad.Trans.Reader.ReaderT`, not that that tells me very much yet.
&gt;which is a bogus measure anyway How so?
Alright, that actually makes sense, thanks.
In theory I think Haskell is suited to these tasks, its just that there is a _lot_ of room for the library ecosystem to grow.
What would you suggest as a better title?
I bought this book, and it is OK as a quick reference. It is a large collection of little recipes, and if just a few are useful to you, it might be worth getting if you can get it on sale (I got it cheaply on a daily sale).
That is clearly not what I am saying. Pretending that a single dispute cannot involve multiple disagreements in different levels simultaneously is silly. ---- Given that this is the twelfth message in this comment chain, and that we haven't moved an inch from where we started, I guess it would be wise to stop here.
I believe that's why they said "currently."
&gt;That is clearly not what I am saying. It is precisely what you said. You said it is not technical. &gt;Pretending that a single dispute cannot involve multiple disagreements in different levels simultaneously is silly And I did not do that. "Not technical" means it is not technical. "Yes it is technical" does not mean "it is 100% exclusively technical and can not involve anything else.
Yeah, you're going to run into some pretty nasty type inference issues with that -- `2 * quantity` is just going to be the first part of that.
I've been trying to use 'Present' like so: import qualified present as P ... print (P.toShow False targetStrings) ... However, I get the following error message: cabal/cabal-install/Distribution/Client/CmdBuild.hs:71:27: Couldn't match expected type ‘P.Value’ with actual type ‘[String]’ In the second argument of ‘P.toShow’, namely ‘targetStrings’ In the first argument of ‘print’, namely ‘(P.toShow False targetStrings)’ What am I missing ?
I'd debated about adding a `MealyT` to machines, but it really would want the ability to lift kleisli m arrows not just pure functions, and the classes started to get ugly. As a result I left it off for folks who were building on top of the package to supply.
Is there an advantage to lifting kleisli arrows as opposed to lifting simple monadic functions?
Yeah. If MealyT is even a monad transformer, which I don't recall if it worked out one way or the other, but I think it did, `lift :: m b -&gt; MealyT a m b` builds a constant Mealy machine that runs your effect once and gave back a constant answer ignoring your input completely, but you have to feed it a completely unneccessary and unused input to get at the result. Something that turned `a -&gt; m b` into `MealyT a m b` would run your action once per input `a` yielding each output `b` in turn with a side effect in `m`. This would be analogous to `auto` which lifts `(a -&gt; b)` into `Mealy a b`.
Good point!
One flaw is that it's been extremely slow. There are much better random number libraries available. Though, perhaps it's been fixed recently? I don't know if this is the flaw to which the video alludes.
Thanks for writing this down! P.S. There is a little typo: -------- \ v The dispalyException method in the Exception typeclass may allow for a better resolution to this point in the future.
Good catch, thank you! I've pushed an update.
There has been a GSoC project in 2015 on implementing splitmix in Haskell and I think it has been successful, but it looks like integration in the random package is still [pending](https://github.com/haskell/random/issues/31).
I think this throws the baby out with the bathwater on exceptions. Just because you have to be concerned about unchecked exceptions doesn't mean you can't mitigate them pessimistically, then use `ExceptT` to mitigate what you want to mitigate explicitly. I agree that all IO code should be treated like it throws. So at some point in the application, it needs to be wrapped in a pessimistic `bracket` or `finally` to handle *unexpected* unchecked exceptions. But once you have this sanity check in place, `ExceptT` does a wonderful job of avoiding the real problem with unchecked exceptions: Unchecked exceptions make it hard to reason about code. Using `MonadError` or `ExceptT` makes it much easier to work with exceptions. And again, I agree that you should expect IO to throw unexpectedly. I just think it's better if it *only* happens unexpectedly, because being forced to work around unchecked exceptions feels asinine for most work. For example, the `websockets` library uses an unchecked exception to signal that the websocket has closed. Cleaning up local resources after the websocket closes is not in any way enforced. Now, being a responsible developer, I will check the docs and handle things accordingly. But it's just the kind of thing that I truly expect to see in the type signature, and it's a pain that I had to go seek it out explicitly. As for composability and `MonadThrow`, I think `MonadThrow` is the same bad unchecked solution. For composability, just use `ether`, or one of the many other ways to compose exception monads. With `ether`, you can just have multiple `ExceptT` transformers seamlessly without them interfering. Though I will admit, I would generally favor returning `m (Either e a)`, since this is the simplest and most approachable representation. Now, this all comes down to discretion about which errors should be fatal and which should be recoverable. I think fatal errors should be unchecked (and secured with `bracket` calls), and recoverable errors should be in the type signature directly. I believe that discerning which category an exception belongs to is the hard part. I think there are some cases that quite obviously should be checked (such as websockets closing), and I think there are some cases that quite obviously should be unchecked (such as `blocked indefinitely on MVar`). But there are murky waters in between. I don't claim to have an answer for these (certainly, unchecked throws are acceptable here). But my point focuses on the fact obviously recoverable exceptions should be clearly indicated in the type signature.
dispaly == unfriendly :-)
From a pie-in-the-sky perspective, I don't think the current model is ideal. It's better to say exactly what you mean, everywhere. Let's say I want to write some app logic without worrying about errors (lookup user friend ids, then look up their profiles, w/e), but then after I'm done with that I want to worry about errors explicitly--none of this "SomeExcept from IO" nonsense. Well, then I should do exactly that: Fundamentally, I have some commands, and each command has some results and some potential errors. So I should say exactly that: Command = Lookup String | Store String String ... CommandResult : Command -&gt; Type CommandResult (Lookup _) = Maybe String CommandResult (Store _ _) = () ... CommandErrors : Command -&gt; Type CommandErrors (Lookup _) = NoDBConnection CommandErrors (Store _ _) = NoDBConnection U OutOfSpace ... Now, when I want to write logic without worrying about errors, I simply write in a free monad over Command/CommandResult, and later when I want to handle the errors, I can simply reify that logic into a free monad over Command/CommandResult U CommandErrors. You may of course fracture this to whatever granularity you like, but the principle is the same. In this way, you allow yourself to create whatever logic you need for the occasion, so you can create app logic without worrying about errors, while later cleanly handling exactly the potential errors that can occur for exactly the commands you're using. To me, this seems much closer to the ideal, but at present it has numerous major issues: namely, the overhead (both syntactic and runtime) of maintaining various free monads being interpreted into each other and an ecosystem which was definitely not written with this paradigm in mind.
For Javascript / Ajax / Single Page applications you need to render in a browser. If just parsing HTML i've used scalpel, tagsoup and conduit. Conduit made crawling really simple i found especially when dealing with pagination as a Source.
Lot's of large companies would disagree with you on that one...
Can you expand on that?
ExceptT is even more composable if you use `ExceptT String m`. When you're doing error handling, `String` (or `Text`) is the most general error type because it is the type that you ultimately end up logging. If you need to to change your program's behavior based on the error type, then having a more strongly typed error is useful (returning 4xx vs 5xx HTTP status codes for example). But most of the time I feel like if you're doing lots of this kind of error-oriented control flow, you're doing it wrong--kind of like how in OO languages the conventional wisdom is that it's bad to use exceptions for control flow. In Haskell, you most often make behavior / control flow changes based on whether it's Left vs Right, not E1 vs E2. So if you're not using the `e` for anything other than logging / displaying to the user, anything stronger than `String`/`Text` is unnecessary complexity.
&gt; most of the time I see people complaining about IO throwing exceptions, what they really mean is "this specific exception just bit me, why isn't this exception explicit in the type signature?" I don't think this is a bad thing to want. The problem with having it in the type signature is the same problem Java checked exceptions have, though: you have to deal with it all the time, whereas exceptions are usually not dealt with except in a) one specific spot or b) the very top level. What I would love if we had the exceptions a function can throw listed in the documentation at least. Ideally (and maybe crucially) this should be compiler checked and propagated. That would make it easy when you're writing code to decide if you're going to catch that one exception, or propagate it to the top catch-all handler.
D'oh! I misremembered my own PR! =P I remember coming to the conclusion that I liked that better. Being a more normal kleisli arrow, it fits the monadic style a little better. Anyway, I think the modified `withExceptT` would also make a good PR
&gt; We should strive to retain structure all the way to the boundaries of our program I don't think we should expend a lot of effort to retain structure if we're not likely to use it an and commonly held principles (not using it for control flow) reaffirm that. We can still log structured metadata even with `ExceptT String` because in my experience the typical metadata we want to log doesn't come in the `e`. It comes in the environment and the `e` typically just signals the location of the failure. Alternatively, with a structured logging library like [katip](http://hackage.haskell.org/package/katip) you're logging JSON structures, so if you really need structure inside the `e`, you can get that by using `Value`. Note that I'm not categorically saying people should use `String` (or `Value`) everywhere. I feel like there is likely to be a layer at which you want `String`--near the top level where there aren't many decisions to made and you're not going to do much other than log it. Then there may be a lower layer where you want something richer than `String`. But I would resist going crazy with structure until there's a clearly demonstrated need for it.
&gt; it pretty much forces you to do incredibly baroque things to work with it Got a snippet?
&gt; I don't think we should expend a lot of effort to retain structure if we're not likely to use it This can only be said if you're writing executables. The moment there is a potential for re-use, you can't say what is likely to be used or discarded.
I tend to prefer logging / data types for most of my error handling with conversions to strings around the outside. That is probably mostly related to my curiosity about how far I can push QuickCheck and other similar things :) 
I take issue with "lots". I've worked at large companies, every time Haskell was *verboten*.
&gt; The index can be used to check whether your programming skills are still up to date or to make a strategic decision about what programming language should be adopted when starting to build a new software system * 6 - Visual Basic .NET 3.167% * 7 - PHP 3.125% * 9 - Assembly 2.441% * 23 - Haskell 1.246% That index is irrelevant.
How can Scala be at #31? Seems like it should be way higher.
We usually go for the mathematically inspired interface so why not instead of `MonadError` just use monads but instead of being a monad in the type variable to the right, it's a functor/applicative/monad in the type variable to the left? Then we recognize `withExceptT` as `fmap`, `throw` as `pure` and `catch` as `(&gt;&gt;=)` analogues. class FunctorL t where fmapL :: Functor m =&gt; (l -&gt; l') -&gt; t l m r -&gt; t l' m r class FunctorL t =&gt; ApplicativeL t where throw :: Applicative m =&gt; l -&gt; t l m r lap :: Applicative m =&gt; t (l -&gt; l') m r -&gt; t l m r -&gt; t l' m r class ApplicativeL t =&gt; MonadL t where catch :: Monad m =&gt; t l m r -&gt; (l -&gt; t l' m r) -&gt; t l' m r instance MonadL ExceptT where .. (edit: since `type Except l r = ExceptT l Identity r`, it doesn't need a separate interface)
And how can ASM be #9 ? VB.NET #6 ? C #2 ? This index is clearly stupid. How can ladder logic be even here ?
i think it even fails at being a reflection on how much people talk about a particular language. I feel like Rust is the new hot thing everybody talks about, but it's on par with RPG and **ladder logic** in this index. No it's really bad, people should stop speaking about it.
Yep, unfortunately template haskell requires us to compile programs for arm on arm, since as I understand it TH compiles to arm and then tries to execute what it has compiled. Cross compiling support would be a game changer for PI-like environments. I've got a new arm board now that has arm64 and 2G of ram, that would be more tolerable I think, though still probably glacial by any sane standard. Simple haskell programs that have few dependencies I think are fine on the PI. 
&gt; Since there are many questions about the way the TIOBE index is assembled, a special page is devoted to its definition. Basically the calculation comes down to counting hits for the search query `+"&lt;language&gt; programming"`. [..] &gt; There are 25 search engines that are used to calculate the TIOBE index. [..] Based on these criteria the following search engines are qualified: * Google.com: 7.69% * Youtube.com: 7.38% * Baidu.com: 7.08% * Wikipedia.org: 6.77% * Yahoo.com: 6.46% * Google.co.in: 6.15% * Google.co.jp: 5.85% * Qq.com: 5.54% * Hao123.com: 5.23% * Google.de: 4.92% * Google.com.br: 4.62% * Google.co.uk: 4.31% * Google.fr: 4.00% * Ebay.com: 3.69% * Bing.com: 3.38% * Google.it: 3.08% * Google.es: 2.77% * Msn.com: 2.46% * Google.com.mx: 2.15% * Amazon.in: 1.85% * Google.ca: 1.54% * Amazon.co.jp: 1.23% * Google.co.id: 0.92% * Google.com.au: 0.62% * Google.pl: 0.31%
&gt; The problem with having it in the type signature is the same problem Java checked exceptions have Java has a LOT more problems than that. You can't abstract over exception types at all. In Haskell, I'm able to write a combinator that throws precisely the exceptions that are thrown by it's arguments. I can't do that in Java. &gt; exceptions are usually not dealt with except in a) one specific spot or b) the very top level. This is fine for first-pass or interactive programs. However, the longer I maintain a Java daemon the more local and specific my error handling gets. This is why I prefer ExceptT / MonadError with a [rich error type](https://hackage.haskell.org/package/cryptsy-api-0.2.1/docs/Cryptsy-API-Public-Types-Error.html).
&gt; For starters, here is a list of projects written in Haskell with [more than 1000 stars in github](https://github.com/search?q=language%3Ahaskell+stars%3A%3E1000). This is definitely the best answer I have seen to this question yet. Also, lots of interesting projects to check out!
Not that everyone would rely on it, but I've kind of wondered how far this kind idea could be pushed: class HasPrism r (l :: Symbol) a | r l -&gt; a where hasPrism :: Proxy l -&gt; Prism' r a liftEither :: forall l r e m a. (HasPrism r l e, MonadError r m) =&gt; Proxy l -&gt; Either e a -&gt; m a liftEither _ (Left e) = throwError . review (hasPrism (Proxy :: Proxy l)) $ e liftEither _ (Right a) = pure a Basically defer dealing with the error type just a little longer, so if you're dealing with a lot of different errors you don't need to keep type-wrangling them into a form to make them compose (at least for cases where you're going to deal with all the errors at the same level). 
It turns out that it doesn't really generate random numbers. Here we (Michal Palka and I) discuss what the problem is and an alternative solution: https://www.google.se/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://publications.lib.chalmers.se/records/fulltext/183348/local_183348.pdf&amp;ved=0ahUKEwjk7OP7j5fQAhXIWywKHUE4DLQQFggaMAA&amp;usg=AFQjCNG9Yc_8LekkXQgyY7PjXjiXFTiySA&amp;sig2=x7d01LnCCuVmTwVSh1XjXw EDIT: this is what the tf-random package is based on. 
cultural bias, outdated psychometric methods, test only certain types of intelligence. They exist mainly to make insecure people feel good, imo. 
I have to agree with the super-awkward thing. Doing something that would be super-simple in Python will most likely be super simple in Haskell as well, provided that you can use Lewis Padgett's "X factor" in your thought process ;-) . That however is seriously more fun than any computer game I have played recently. 
You don't really need a full `Prism` here, because you're not going to deconstruct things. I suggested a similar pattern for the `luminance` project, and you can see it in use here: https://hackage.haskell.org/package/luminance-0.11.0.4/docs/Graphics-Luminance-Framebuffer.html#v:createFramebuffer
Lol no I won't have to be more specific at all. If you have a particular fondness for IQ testing, be my guest. There's decades of peer reviewed research criticising the methods but you can find it for yourself. 
You made the claim that it is "bogus", if you want that claim to be taken seriously you need to be willing to support it. I can find no such peer reviewed research. In fact, all the peer reviewed research I can find involving IQ either supports it directly, or uses it to research something related. 
&gt; doesn't really generate random numbers This is referring to splitting, and it means splitting does not have some good properties you would want it to have. (Of course it doesn't really generate random numbers; this is a pseudorandom generator.) I really wish there were more research results for splitting. While I have no doubt that crypto techniques do solve the problem, that is obviously way overkill for many applications. We need a good set of measurable quantitative criteria to evaluate splittable PRNGs. And we need a choice of splittable PRNGs with different randomness and performance profiles for different applications, as we have for traditional PRNGs.
Anecdote: We have a Servant app at work that uses Persistent and MySQL as a data store, currently with one endpoint that does quite a lot with the database. Updating the bindings resulted in a positively absurd throughput -- we were capped at 40-50 requests per minute. Any more and the latency would shoot up and the AWS ELB would kill the connection. I updated the bindings and *immediately* the throughput went up to about 200 requests per minute. We've increased load on the service and it's now handling 4,000 requests per minute. Latency is under 100 ms. # update your MySQL bindings!
Sans the extra type classes, this is basically [`EitherR` from *errors*](https://hackage.haskell.org/package/errors-2.1.2/docs/Data-EitherR.html).
I second /u/ElvishJerricco's suggestion. Abstracting your effectful operations using a DSL like this gives you a lot of options, without necessarily impacting performance. It does come at the expense of a bit more type management, but I think it's often worth the effort.
I'll reply with my opinions (&lt;-- important, it's really just informed-by-practice opinions!) in order. There are some packages that translate repa to hmatrix (which can use openblas), e.g. [1]. A crossover point implies two lines in some 2d trade-off space. I don't think it's that easy. E.g. a single mistake like copying in a loop can cost 100x performance, even when using blas. Also, unless llvm got much better recently I think it's unlikely that repa will beat the hand-tuned SIMD openblas kernels. [2]. [1] https://github.com/marcinmrotek/repa-linear-algebra [2] https://github.com/xianyi/OpenBLAS/tree/develop/kernel
How can Free be used to generate another format if you can't inspect the AST?
The core is, yea - though written in a style that encourages the compiler to generate fast code so it doesn't always look like idiomatic Haskell.
I don't think it was an individual song but rather a dance and corresponding musical genre. The only song from the lambada genre I remember is "Llorando se fue", though, maybe that's the one you mean.
I tried that too, but the compiler didn't like it because of `Normalize` I think (it's an illegal type synonym or something). Perhaps if I rebuild type-level addition as a type class with fundeps...
What do you mean by "unchecked"? That the exception is not visible in the type signature? Also, about cleaning up local resources: I thought the way to handle that is to use "bracket" and the like to ensure cleanup. I do this whenever allocating any resource. If I'm in IO I have to assume that anything I am doing can be interrupted at any time, so I have to make sure resources are cleaned up. Rather like C++ RAII.
Because without interpretation you are stuck at bind ?
Oh god, not the libtool versioning scheme please. It requires at least level 3 posix compliant beard to understand. 
By "checked" and "unchecked", I just mean visible in the type signature, yes. Such that it's enforced that I deal with it somehow. As for local resources, most should definitely be done in `bracket`. But my point was mainly that a lot of exceptions should really be pure `Either` style errors.
As far as I know you can only interpret it by executing the actions. Bind only gives you the continuation after you provide a value.
I can't say I see the value in this approach. It just seems more convoluted and error prone.
afaik, you can rarely inspect binds. e.g. 1 data Controller a where TurnOn :: Controller () TurnOff :: Controller () IsOn :: Controller Bool ... -- monadic constructors. alternatively, write a "ControllerF" and use Free. instance Monad Controller where ... `Controller a` is only ever `Controller ()` and `Controller Bool`. Being finite, you can pass all values ((), True/False) to the continuation (i.e. the second argument to (&gt;&gt;=)). like "super compilation", the tree grows exponentially, but stays finite. e.g. 2 With Free (or a monad that's concrete enough), you can optimistically evaluate the computation through it's "non-monadic constructors" (e.g. TurnOn, TurnOff) till you reach the "monadic constructors" (e.g. IsOn). i've done both, and they're useful, but they're limited. Are there other ways to inspect bind?
I take issue with your very subjective experience of the few large companies you have worked at ;). Though you have a point, "lots" is a bit of an exaggeration...
Thanks @chrisdoner, I've tried that as well, but I get another error: ``` t: Not in scope```
The only reason against having both is it's more complexity/more choices; it's less obvious to users which one to pick. (Not saying this is an overriding reason, but when Auke and I discussed, this was the primary reason against having both.)
If you're referring to a (e)dsl, the following links may be of interest: * http://www.cse.chalmers.se/~emax/documents/svenningsson2013combining.pdf * http://www.cs.ox.ac.uk/jeremy.gibbons/publications/embedding.pdf
&gt; Basically, what I'm saying is, I just need to get a fucking blog to write all this down instead of using Reddit and IRC/Twitter as my blog. Yes! Any plans of actually getting this done? It's super-easy to setup a static blog these days, for example on github pages.
So the question is, what exactly is Mockito trying to accomplish? It's very easy to not answer this question, say something like "it's trying to use X.class reflection to let you write ..." yadda yadda yadda. Mockito wants to give you a way to design an X, based on some interface which Xes have, which is not really an X. Namely, you can ad-hoc decide what X does when various functions are applied to it. Well to do this in Haskell you *need* to have control of the living code that you want to be inserting the mocked object into, otherwise type safety is going to block you from inserting an object of the wrong type into those contexts! However the static type guarantees also *help* because the language itself will test you for completeness. You can do it with typeclasses. To get the typechecker on your side, take your IO-laden function which you want to make mockable, and change it around slightly {- starting from: -} myFun :: IO String myFun = do {- some stuff goes here -} {- then make -} myFun :: IO String myFun = mockable where mockable :: MyDB m =&gt; m String mockable = do {- move the block over here -} If I got that right, the type signature on the internal `where` clause will now get mad whenever you try to specialize to the `IO` type rather than leaving the expression general, so the compiler will just *tell you* what you need to start stubbing out in the `MyDB`typeclass and you can copy those over to the `instance` for `IO`. However if that's not your cup of tea you can *also* probably do it with sum-types *or* the module system. Sum types involve just typing `data MyDB x = Mocked x | Actual (IO x)` into the system, then writing new functions which explicitly handle the Mocked case. Modules require rewriting the `import` directives back and forth for testing vs. runtime, so one file says `newtype MyDB x = Actual (IO x)` and the other says `newtype MyDB x = Mocked x`. You tweak the import directive to match the tests you want to run.
Ok, I've realized that I needed to enable the `Template Haskell` extension.
I don't understand how one can do that. Even anything supporting functor (except const) turns opaque to me!
Not that this is a perfect solution, but a recommendation in the User's Guide to use libtool format "unless you need to specify a soname that isn't supported by libtool" could go a long way. I agree that using the libtool format is more portable, though I don't know if it's actually more standard -- unless LSB requires it. I'm also certain that neither SUS nor POSIX (the only other two standards I can think might apply to both Mac and Linux) require the use of libtool. I'd certainly be comfortable with such a recommendation at this time; it can always be changed later if needed.
&gt; Most of the time you really do just want to work with strings. Really? Most of the time I want to work with structured data, but Unix just gives me strings that I have to faff around with.
Assembly comes up often when talking about other languages in the context of Debugging/Optimization so that doesn't surprise me too much.
incorrect 
Author of that blog post here. From my perspective, using `test-fixture`, I don’t have to restructure the code to better fit the testing, which is one of the main points of that library! The examples given in the blog post demonstrate that all I changed were the type signatures to go from code written directly against `IO` to code written against abstract monadic typeclases. I suppose you may complain that you have to write those typeclasses, but I’ve found that those pay for themselves in readability many times over, even without the testing benefits. Having effects represented in your type system is *super* useful. Once you’ve set up those typeclasses (which usually takes a couple minutes) and your monad transformer stack of choice with the relevant instances (which takes under 30 minutes once you’ve done it a couple times), then you’re pretty much good to go, and you can mock things out as you please without needing to write any boilerplate at all. I don’t really view the typeclasses as boilerplate, since you get value out of them, but there’s probably a little bit of boilerplate in the instance writing. In a Lisp, it would be easy to macro that boilerplate away, but in Haskell, I think it’s pretty much as good as I’m going to get.
By generators, I mean things along the lines of `Foo -&gt; Gen Bar`. You'd typically also want shrinking functions, like `Bar -&gt; [Bar]`. You can use these with `forAll` and `forAllShrink` in the places where you'd usually use `Aribtrary` instances. I still like to have common `Arbitrary` instances around, but most of the time they're just using common usages of the generator functions.
Is there an easy way to know what dependencies have updated? I use stack is there a way to have it update everything as much as possible? ... guess I've got some googling to do lol
Mmm, I can see where you're coming from, but I also wanted to talk about how "Uniqhash" makes a good test-case for the expressiveness of various tools... Maybe I should have split it into two posts. Ah well too late now!
Those people get their PhDs primarily out of fear. Once they get to one of those Ivy League schools or whatnot, they assume now that they are in one of the inner temples, they will know. Some of the smartest people who supposedly know. But it stands to reason that if they all know they should really be grooving, but they aren't. Life just isn't beautiful enough. They're all talking about the rat race, looking drawn out in a highly competitive field, and what really bothers them is that they know they don't know. But if you look in other people's eyes to get the image of who they are, it's pretty good. Their parent all must be proud, and they have collected all of the symbols of success in society, and they sit on important committees and whatnot. But every now and then just before they be going to sleep or when they be in the bathtub or something, there is that moment, when there isn't somebody else's eyes to look into to tell them how wonderful they are, and they know that it isn't enough.
Yes, I completely agree.
This is good thinking! I wonder, though, if we shouldn't just let our tools pick up identifiers from function definitions. Writing, f :: ("mass" ::: Double) -&gt; ("acceleration" ::: Double) -&gt; ("force" ::: Double) f mass acceleration = mass * acceleration is a lot of redundancy. I get that in this case we'd eta reduce and not name the function arguments, but in more complicated uses we might, and then our tools really ought to be able to present, f :: Double -&gt; Double -&gt; Double f mass acceleration = mass * acceleration as taking arguments `mass :: Double`, etc. That is all to say, I'd rather we had proper units of measure, but if we're going to punt on that and just use unchecked annotations, we should get more ROI on parameter name choices. There absolutely are cases where we want some lightweight reflection on parameter names (e.g. `vinyl-gl`), and I do love an excessive eta reduction so some kind of fall back to annotated types like this makes sense, but it's always seemed a bit unfortunate to me that our documentation doesn't help disambiguate cases of multiple parameters of the same type, instead requiring either that the user view source to figure it out, or the author repeat their identifier name choices in their docs.
TIOBE index can sometimes be quite ridiculous, for example, what the hell is "Lisp" when there are AutoLISP, Emacs Lisp? I asked this question a few years ago and it turned out that: "Lisp" on TIOBE is a mixed bag, and isn't a very reliable statistic. (https://www.quora.com/What-does-Lisp-mean-in-TIOBE-index/answer/Robert-Smith-9?srid=3QAl)
I've considered writing unexceptional-prelude -- haven't got around to it yet
You can't really, because the function space in the second argument of `bind` is the Haskell function space; i.e. in (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b the `a -&gt; m b` function can be anything, regardless of you choice of `m`. If you want true static analysis of your effects, you'll need to restrict your interface to an applicative functor only. 
I think it applies for libraries to keep confusion down. But in one's own application, this is just too useful to avoid
Most of these are definitely doable with Repa AFAIR, it has specific support for ForeignPtr backed data, IIRC it is possible to fold over arbitrary dimensions, but might be a little annoying to figure out how to do that. Same manipulation is basically as simple as writing a function from one shape to the other. the SIMD thing is somewhat irrelevant - it should be possible to ensure the pointers are aligned properly when allocating them, which isn't necessarily done within Repa. Can you explain what you mean by no broadcasting? 
Stack uses resolvers by default, which ensure a consistent set of packages that all work together. You can add extra dependencies or newer dependencies in the `extra-deps` part of the `stack.yaml` file.
&gt; and you should find that the generalizations offered by the latter two generally make them a better first choice. Not trying to prematurely optimize or anything but just in the interest of complete understanding: aren't sum types and functional encoding more performant than mtl/free?
Simple example: consider the writer functor under Free data Writer w a = Writer w a deriving Functor data Free f a = Pure a | Free (f (Free a)) deriving Functor instance Functor f =&gt; Monad (Free f) where return = Pure Pure a &gt;&gt;= k = k a Free a &gt;&gt;= k = Free $ fmap (&gt;&gt;= k) a liftF :: Functor f =&gt; f a -&gt; Free f a liftF a = Free $ fmap Pure a tell :: w -&gt; Free (Writer w) () tell w = liftF $ Writer w () x :: Free (Writer String) () x = do tell "one" tell "two" tell "three" x = Free $ Writer "one" $ Free $ Writer "two" $ Free $ Writer "three" $ Pure () In this case, you're able to statically analyze the entire computation.
Thanks :) The Either wrangling is definitely something that I strived to avoid, but it certainly works!
I mean specifically [1]. let x = R.fromListUnboxed (ix1 3) [1..3] let y = R.fromListUnboxed (ix2 3 3) [1..9] x + y will work in numpy but not in repa because the shapes are not compatible. [1] https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html
I see you're point. You're correct that you cannot analyze all possible ASTs for a given `Free` program. The point is that you can analyze the AST for given path that your program has run. In this case, `Free $ Ping $ Free $ Ping $ Pure (4+1)`
I guess it probably also has a user base with electrical engineers writing software for extremely exotic microprocessors/controllers.
This might help https://vimeo.com/125038982 It'll teach you about free monads.
Do you know what the author is referring to when he says we recently learned how to inspect and bind free monads in constant time with high constant factors? Is that reflection w/o remorse?
In fact, it seems more or less the same
It may be because I primarily develop applications, rather than libraries, that I find this functionality more useful than the average Haskeller. I find myself wanting a way to easily have configuration options flow from the config file to the places where they are needed. For reference, here is [an example](https://github.com/runeksvendsen/restful-payment-channel-server/blob/master/src/PayChanServer/Config/Types.hs#L51) of how I like to construct the types for my applications: data ChanConf = ChanConf { btcMinConf :: BtcConf , openPrice :: OpenPrice , dustLimit :: DustLimit , settlePeriod' :: SettleHours , minDuration :: DurationHours } These config options are so important that I have chosen to create distinct types for each of them, and I would argue that referencing the record fields in this type is superfluous -- it's practically the same as the type, except with a lower case beginning letter. Here's another example: data Conf = Conf { _dbInterface :: Store.Interface , _callbackIface :: Callback.Interface , _listUnspent :: HC.Address -&gt; IO (Either String [BtcType.TxInfo]) , _settleChannel :: ReceiverPaymentChannel -&gt; IO HT.TxHash -- Dummy function if debug is enabled , _pubKey :: RecvPubKey , _chanConf :: ChanConf } again, when I try to fetch something from that config, there can be little doubt as to which value I want. I would prefer just having to focus on choosing the right *function*, eg. `doServerCallback :: Callback.Interface -&gt; IO ()`, and then having something else automatically fetch `Callback.Interface` when given a `Conf`. So the examples in the original post are poor, because the way I would use a feature like this, would be to always use descriptive types. For the the rest of the cases I would just use `RecordWildCard`. **EDIT:** Side note: Imagine I forgot the name of the record field for the `DurationHours` field in `ChanConf`. What would I do to find out? I would search for `DurationHours` (ie. by type), and it would tell me the name of the record field. So in other words: I'm already doing (lookup by type) what this proposed extension would be doing.
So essentially languages like Haskell with specialized search engines are penalized by this index definition.
That can already be implemented in current Haskell, optionally with existing GHC extensions. {-# LANGUAGE RankNTypes #-} {-# LANGUAGE ViewPatterns #-} module A where -- An example of record. data R = R { fieldA :: Int, fieldB :: Char } -- Record type R has a single field of type a. class FieldR a where unpackR :: R -&gt; a -- Boilerplate instances. instance FieldR Int where unpackR = fieldA instance FieldR Char where unpackR = fieldB -- We can merge fields like this. unpackR' :: R -&gt; (forall a. FieldR a =&gt; a) unpackR' r = unpackR r -- Action depending on one field. launchMissiles :: Int -&gt; IO () launchMissiles = undefined -- Action depending on the other field. shootLasers :: Char -&gt; IO () shootLasers = undefined -- Both fields are refered to as "a". f :: R -&gt; IO () f (unpackR' -&gt; a) = do launchMissiles a shootLasers a -- With zero extensions, we cannot define unpackR', but may use unpackR, though it is a bit more verbose: f' :: R -&gt; IO () f' r = do let a :: FieldR a =&gt; a a = unpackR r launchMissiles a shootLasers a 
[removed]
you could write that logic in Haskell, and then interpolating a sum type into different templates, with https://hackage.haskell.org/package/interpolatedstring-perl6-1.0.0/docs/Text-InterpolatedString-Perl6.html 
Isn't Heist limited to XML/HTML templating ? For all we know, that may not be what OP is looking for.
I want it to be independent of Haskell so that other people can modify it. And live reload is one of key factors.
Thanks, that makes total sense. So much advice is different for libraries and executables!
Well, there are [examples](https://github.com/snoyberg/yesod-alternative/blob/master/Yesod/Heist.hs) on using Heist with other frameworks such as Yesod, so I don't think that's a real concern.
the whole thing is a farce anyway. When the last COBOL server is finally shut off and there are the inevitable blog posts about it finally coming to an end, Tiobe would consider this as evidence for an increase in COBOL programming
Can't do what? I literally put your example into ghci and typed `program2` and pasted the result.