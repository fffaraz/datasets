It looks like GHCJS for 8.2 might be relatively [close](https://github.com/ghcjs/ghcjs/issues/602).
Sure, that comes under "Heterogeneous dictionaries".
&gt; I'm fixing bugs in Opaleye atm Uh oh. Could you give me a heads up on what bugs those are?
Bugs = issues/PRs. We have to solve the perf problem right?
We do this with purescript-bridge and servant-purescrcipt. You can reflect all of the types you want into purescript and you get generic json eoncoding and decoding (plus functions for each API call if you're using servant). It's worked really well for us! AMA :-)
The actual count is about 15x that, might get hairy 
`concatMap` is just a special case of `foldMap`. `t` is `[]` and assuming you're using `Int`, your `t a` is `[Int]`.You could have supplied `[1, 4, 6] :: Vector Int` and still gotten back the same result.
[removed]
Ah, so not anything new?
Heterogeneous dictionaries used as the default way to store almost any data, specifically. Typing them becomes ... interesting. Row polymorphism gets you part way there, certainly. However, many of the utility functions are still rather awkward. Think clojure's rename-keys. I guess dependent types might get you there? At this point, we are certainly talking about a type system that is significantly more complex than Haskell's. Could someone work using such a type system? Sure, and it makes some things much easier. Type systems definitely have value. However, there's a tradeoff there. You have to spend time and brainpower on the types. Some of that would be necessary in a dynamically typed language anyway, but a lot of the more complex stuff wouldn't -- you certainly wouldn't be keeping the [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia) in your head while writing clojure code. The mental ram that is being spent on the typeclassopedia and the difference between a lens and a prism isn't being spent on your codebase. For some people, that tradeoff is worth it. For others, it isn't.
[Pdf of the paper describing the system](http://www.cs.nott.ac.uk/~pszgmh/improving.pdf)
Purescript works on Windows at least.
Yes, definitely!
Competitive, depending on the role and experience
Sadly our work isn't really conducive to remote work :(
I find that very hard to believe. Care to give more explicit and precise examples?
&gt; I guess dependent types might be what you need? Everyone always guesses dependent types might be what they need. I fear they're going to be disappointed :)
The salary is competitive, depending on the role and experience.
&gt; Hmm, this seems like Linguistic relativity (Sapir-Whorf), which is very suspect. The statement that there is a bias in Western culture and language towards objects does not necessarily lead to the reverse conclusion that eastern societies have an easier time understanding category theory. The qualification with word "Perhaps" indicates that it is more a statement summarizing a general view than a specific claim about Western language. I try to find connections between category theory and (radical) constructivism. The cited paragraph refers to "Maturana and Varela, The tree of knowledge". That is the reason why I picked the quote. 
Yes! I am myself a fresh graduate.
Correct me if I'm wrong but dependent typing is inherently optional no? Like as long as you don't import a function that is dependently typed you can ignore them. I'm pretty sure you could even copy paste all of Haskell's base library into a dependently typed language with no problem. If I am wrong about this I would love to hear why, because I'm not all that convinced on dependent types and if their existence will get in the way of non-dependent coding I want to know so I can force myself to give them a full try or push back against them.
[Subhask](https://github.com/mikeizbicki/subhask) takes an approach like that, of having quickCheck verified classes.
&gt; Typing them becomes ... interesting. Which is really what I'm trying to get to the bottom of. I'm looking for examples of things that one wants to do in Clojure that are demonstrably hard or impossible to type. So far I've heard lots of claims but haven't seen any examples, proofs, or even intuitive explanations. So far it's all been rather vague. &gt; The mental cycles that are being spent on remembering the difference between monoids and functors or parsing a complex lens type signature aren't being spent on your codebase. Oh come on. *Of course* they're spent on your codebase. It's not only time spent typing at the keyboard that's spend "on your codebase". Those things may not be the most effective use of time (or maybe they're by far the most effective use of time) but they're certainly spent on your codebase.
Great! I still need a bit less then a year to finish my masters but maybe I’ll get lucky and you’ll be hiring then as well :)
One common misconception about CSV is that you can simply read it line by line and split fields by commas. Well, what if one of the fields contains a comma? Or (less likely, but still possible) a newline? According to RFC 4180, they should be quoted ("). And what if fields contain quotes? You need to quote the quotes. And when you read the file back, you need to unquote them. It's ok to use simple splits for the data you can control (e.g. all the fields are natural numbers without thousand separators), but if you want to build something real, it's better to use a proper CSV parser, like `cassava`.
Need to open a nice Australian office ;-) we have better weather than Cambridge.
More effective than Miso?
I'll allow myself for some hijacking of the WA thread :) I remember vaguely (it was quite long ago, now I can't find it), someone asked if, once WebAssembly lands in, it would mean GHCJS could be replaced. The answer was that WA standard was lacking certain things needed for that. Does it mean it has changed since then, and WA is feature-full enough to support everything GHC runtime needs without any compromises? BTW. I know you don't have much time for it, but extracting some useful info from here to a sort of FAQ could be fruitful, this thread contains some really interesting information.
Yea, thanks :)
In the real world, clients will complain if you give them a technically-correct CSV file that can't be parsed by splitting the row. And they'll give you unquoted CSV files where you'll have to guess which commas split the columns and which are part of the data.
Nearly all important use-cases of dynamic types revolve about unstructured data. But you can work with unstructured data just fine in good statically typed languages. The `metadata` field approach is just the simplest way to carry unstructured data. I wish there was a standard library on top of `aeson` that would reimplement parts of the prelude for `aeson`s `Value` type, with functions like `map :: (Value -&gt; Value) -&gt; Value -&gt; Value`. I presume that most people just use `lens-ason` instead, which allows you to modify unstructured data, while keeping at least some of the type-safety. 
it is a clue to what? what is your enlightenment in it? i do not see any.
Thanks for your insights! &gt; compositionality of `ExceptT` Well, Snoyman mostly argues about transformer stacks with `IO` at the bottom, where his arguments make sense. IIRC, `ExceptT` composes badly with `IO` because the `MonadBaseControl` instance is weird at best[^citation needed]. &gt; When does it make sense to introduce something like `MonadBalance`? I think it makes sense at the least when you can think of a pure instance, as in no `IO` needed, or when modeling a DSL that you can interpret in pure terms. `MonadBalance` is obviously perfectly modeled by `State Int`. But I think these patterns in general mostly apply to the top-level, impure plumbing code where `IO` sits at the bottom of the stack.
identifying a group of people as "privileged" contains an implicit suggestion to take away stuff from them; "privilege" = extra right; most people think extra right is injustice and hence should be compensated
&gt; but it isn't enforced by the type checker. And my point is, that it doesn't have to be enforced. Type checkers are meant to help your brain, not to replace it.
I would differ on Elvish on this. Miso is by far the easiest thing to get started with.
Do share please, WebAssembly seems like the way to go for sure.
But the insects in Cambridge can't eat your copy of TAPL whole.
What's in the files?
I thought that because PureScript does eager evaluation by default, it performs better in the JS runtime? And GHCJS has additional runtime (laziness, GC). Also, I am not sure how hard it is to integrate GHCJS Haskell with random JS library. I am under impression that PureScript handles it much better. If I am wrong, please correct me, because I am currently planning to start playing with PureScript in browser. Once GHC WebAssembly support kicks in, it could be really great...
&gt; Typing them becomes ... interesting. Haskell might be able to manage a partial solution, but you lose everything that makes haskell interesting if you use it everywhere. But you don't have to use it everywhere. Instead you can mix structured and unstructured data just fine and convert between those two whenever you want. Turning unstructured data into structured data is, essentially, just a parsing problem. But this only possible in a statically typed languages, since dynamically typed languages deprive you of this option and force you to use unstructured data everywhere.
Neither Conor's original paper or Bob's reformulation permit actual dependency on linear terms. What they do is let you *forget* the linearity in a term and then use it as an index. This apparent limitation arises from the fundamental nature of equality. Lawvere taught us that equality is left adjoint to contraction. In less jargon-laden terms, this means that showing that `x = y` entails `P` is equivalent to showing `P[z/y,z/x]`. If `x`, `y`, and `z` are linear variables of type `A`, then this equivalence is ill-formed, because we are replacing two occurrence `x` and `y` with the same variable `z`. This is only possible if there is a way to duplicate values of type `A` -- i.e., if there is a map `A ⊸ A ⊗ A`, so that we can get two copies of `z` to fill in for `x` and `y`.[1] However, the distinctive feature of linear type systems is that this map does not exist in general! So my paper[2] restricts equality to types for which contraction is valid (including the type of closed linear terms). Bob and Conor give a way of forgetting the linear content of a term, leaving behind an intuitionistic skeleton that you can be dependent on. Surprisingly, these two approaches are inequivalent! So there is probably a yet more general approach that encompasses both of these. I don't know what it is, but I do know it will require thinking very carefully about contraction. [1] Confusingly, this map is called "contraction", rather than "duplication". Sorry! [2] My work is a descendant of the work on linear LF. Mathias Vakar independently invented a denotational semantics for a calculus like this one, too.
BTW, do you need help with getting WA compilation via llvm working? You mention in the other thread you and Michael are committed but don't have much time on your hands to invest. 
Are there any pointers / documentation on Haskell backend / purescript frontend development in one project with type sharing &amp; seamless data marshaling? Thanks! 
Didn't even know it exists, at first glance looks very good and promising, thank you!
Please check out [Purescript vs GHCJS benchmarks](https://medium.com/@saurabhnanda/benchmarks-fp-languages-libraries-for-front-end-development-a11af0542f7e) Surprisingly, frameworks build on top of GHCJS are more performant that PS frameworks.
&gt; But this only possible in a statically typed languages, since dynamically typed languages deprive you of this option and force you to use unstructured data everywhere. Clojurists seems to be claiming that `core.typed` and `core.spec` are roughly equivalent to having an optional type system. If anyone knows anything about these and can compare them to Haskell's type system I'd be very interested.
Thank you for the below :) awesome. however, I would like to target some other point please. * do you create two project one for backend(using stack almost) and another for frontend (using what PS tool available). I think it would be better if just I can create a folder for the front in my existing backend project. * any suggestion regarding easy way to structure a PS app ( maybe Pux for exmaple?). 
I urge you to read [**[PDF]** Combining Deep and Shallow Embedding of Domain-Specific Languages](http://www.cse.chalmers.se/~josefs/publications/deepshallow.pdf). ---- You can retain deriving by factoring out the relevant piece data TwoTypeableExpr where TTE :: TypeRep ty -&gt; Expr ty -&gt; Expr ty -&gt; TwoTypeableExpr and declaring what it means for expressions of (potentially) different types to be equal, we write this code once: instance Eq TwoTypeableExpr where (==) :: TwoTypeableExpr -&gt; TwoTypeableExpr -&gt; Bool TTE ty a b == TTE ty' a' b' | Just HRefl &lt;- eqTypeRep ty ty' = and [ a==a', b==b' ] | otherwise = False And in defining `Expr` in terms of `TwoTypeableExpr` data Expr :: Type -&gt; Type where I :: Int -&gt; Expr Int B :: Bool -&gt; Expr Bool Eq :: TwoTypeableExpr -&gt; Expr Bool Lt :: TwoTypeableExpr -&gt; Expr Bool Gt :: TwoTypeableExpr -&gt; Expr Bool we can derive (with standalone deriving) `Eq (Expr a)` deriving instance Eq (Expr a) ---- You can make this more general and factoring out `Expr`, the constraint or ..how many times it appears: data TypeableExpr f where TE :: TypeRep ty -&gt; f (Expr ty) -&gt; TypeableExpr f -- https://hackage.haskell.org/package/linear/docs/Linear-V2.html data V2 a = V2 a a type TwoTypeableExpr = TypeableExpr V2 making the instances trickier.
Thanks!
I don't suppose there are implantations in the works? This would be a welcome addition to the tooling issue recently discussed in other threads. 
WebAssembly is interesting. The question is how much it has to do with DOM manipulation. For instance, what about FRP? Do you envision that Reflex will eventually be built on top of your Haskell/WebAssembly project, instead of on top of GHCJS?
You can use reflex-platform with miso. I didn’t mean to recommend Reflex (though I definitely do recommend Reflex). It’s just that reflex-platform works best for setting up a good GHCJS environment with jsaddle and whatnot
I would still recommend using reflex-platform to setup GHCJS and jsaddle for misl
Go check out the thread from yesterday :) Some of my comments answer that, particularly [this one](http://reddit.com/r/haskell/comments/7ax2ji/haskell_on_the_front_end/dpect66). Basically, a jsaddle backend using the WebAssembly &lt;-&gt; JS FFI is likely to be even faster than the jsaddle mobile backends, which are already much faster than GHCJS.
Could always use help :) I’ll try to put up some documentation on progress and roadmap, and then I’ll start using GitHub’s issue tracking to clarify what work needs doing. That should make it easier for people to help contribute.
I think most of the comments in that thread are wrong these days. I expect code size to be smaller, particularly if someone implements `--gc-sections` in the linker. The performance is definitely going to be *way* higher I believe. And interacting with the DOM isn’t going to be so bad.
We're able to adjust the salary to suit your experience. Also, you shouldn't be so hard on yourself! It's a bad habit to be in to ;)
Thankfully we can go from one to the other ### explicit → implicit data Expr :: Type -&gt; Type where EqRep :: TypeRep ty -&gt; Expr ty -&gt; Expr ty -&gt; Expr Bool ... pattern Eq :: () =&gt; (Typeable ty, Bool~resTy) =&gt; Expr ty -&gt; Expr ty -&gt; Expr resTy pattern Eq a b = EqRep TypeRep a b Using the pattern synonym `TypeRep` (defined elsewhere in this thread) which provides a `Typeable ty` constraint from matching on `TypeRep ty`. ### implicit → explicit data Expr :: Type -&gt; Type where Eq :: Typeable ty =&gt; Expr ty -&gt; Expr ty -&gt; Expr Bool ... we want to pattern match on `Eq` to get an explicit `EqRep` pattern EqRep ? a b = Eq a b but where does the `TypeRep ty` come from? One way is to smuggle it out alongside `a` smuggle :: Typeable a =&gt; b -&gt; (TypeRep a, b) smuggle = (typeRep, ) pattern EqRep :: () =&gt; Bool~resTy =&gt; TypeRep ty -&gt; Expr ty -&gt; Expr ty -&gt; Expr resTy pattern EqRep rep a b &lt;- Eq (smuggle -&gt; (rep, a)) b where EqRep TypeRep a b = Eq a b
[`filter`](https://www.stackage.org/haddock/lts-9.12/base-4.9.1.0/Prelude.html#v:filter) [`id`](https://www.stackage.org/haddock/lts-9.12/base-4.9.1.0/Prelude.html#v:id) should do what you want. &gt;&gt;&gt; let someBooleans = [False, True, True, False] &gt;&gt;&gt; filter id someBooleans [True,True]
I would like to help, but this is very hard to read as is. Can you fix the formatting? Code should be indented by 4 spaces. See https://www.reddit.com/wiki/commenting.
I looked at generic deriving for a subset of GADTs in the past. This paper might at least help explain the challenges: José Pedro Magalhães and Johan Jeuring. Generic Programming for Indexed Datatypes. In Proceedings of the 7th ACM SIGPLAN Workshop on Generic Programming (WGP'11), pp. 37–46, ACM, 2011. http://dreixel.net/research/pdf/gpid.pdf
Some of these, like "arity", aren't specific to functional programming per se. And others, like "value" versus "constant", are particular to JavaScript. Nevertheless, this looks like a good resource! I even learned a new term: [setoid](https://en.wikipedia.org/wiki/Setoid). 
If I might give you some advice: * Don't call lists arrays. Not only do you confuse others but also yourself. Both data structures have very different characteristics. * There are no loops in Haskell. What you can do is recursion. You can do recursion manually by pattern matching on the list (cons or empty) or with higher-order functions like `map` (*map* each element of the list with a function), `foldr` / `foldl` (fold the elements of a list) and `filter`. -- Manual recursion filterTrue1 :: [Bool] -&gt; [Bool] filterTrue1 [] = [] filterTrue1 (x : xs) = let r = filterTrue1 xs in if x then True : r else r -- Recursion with foldr filterTrue2 :: [Bool] -&gt; [Bool] filterTrue2 = foldr (\x r -&gt; if x then True : r else r) [] -- Use filter filterTrue3 :: [Bool] -&gt; [Bool] filterTrue3 = filter id
`any id` ?
Also, could you please comment on [the work I talked about here](https://www.reddit.com/r/haskell/comments/78zuul/documentation_for_simplified_development/)? It seems you're still not satisfied with the tooling for basic things with Reflex, and I'd like to try to keep fixing it.
Setoids are usually built as a 'set with an equivalence relation' rather than require it to be a member of the object in question, but the article isn't really big on rigor. Once you have setoids, you can talk about functions that preserve them as morphisms between them. e.g. given setoids `A` and `B`, you want functions `f : A -&gt; B`, such that if a,a' : A and a == a' under the equivalence relation, then `f a == f a'` under the target type's equivalence relation. If you push on this hard enough you can use it to constructively model a (dependent) type theory in which you can have functional extensionality baked in.
I was careful to elide culture from the quote as I only wanted to comment on the idea that language influences thought, I purposely mentioned languages which have complex verb morphology which might be verb/relationship oriented. Care to give a quick summary of what Radical Constructivism and those two books are about?
I worked with [Typed Clojure](http://typedclojure.org) (aka `core.typed`) and [Plumatic Schema](https://github.com/plumatic/schema) (similar to `core.spec`) a couple years ago at work on a small-ish project. Here's a very brief summary of my thoughts: - Typed Clojure was alright, but I ran into three problems that eventually made me give it up: 1. Libraries weren't typed. That meant I had to specify the types of the functions I used, or tell it to ignore them. Sometimes determining the type of a function was excessively difficult. 2. Running the type checker was terribly slow. Two of my coworkers were Clojure experts and I was running everything in [Cursive](https://cursive-ide.com), yet we couldn't figure out how to make it fast enough. 3. The error messages were terrible. Way worse than whatever GHC was spitting out then. - Plumatic (then Prismatic) Schema worked great as a contract system, but it's not really a type system. For example, at one point I changed the type of a field in a core "data structure" (really a type alias for a map with some key/value pairs). Schema could not tell me ahead of time everything that should change. I had to run the program (or test it) and use the runtime errors to guide refactoring. 
We have one stack project with a library and many executables. One of the exes is the servant web server which has a Raw endpoint and serves up the built purescript artefacts which live in &lt;purescript-directory&gt;/dist. The purescript project lives in a top level directory in the stack project and has a psc-package.json and a webpack config file. We then weave all the generated types and API call functions into a Halogen project I also use spacemacs, I think the purescript layer is awesome! This is a the best way I've yet found of doing front end development. It really extends the same experience of refactoring Haskell (i.e. make your change and fix all the type errors) all the way to the UI, which is like a super power :-)
Now that I think about it, the last sentence of my previous comment doesn't really make sense to me. Defining the stub implementation is, like you said, trivial. It was late yesterday... Good point about `MonadBaseControl`. Luckily, I was able to avoid `monad-control` by working with `ExceptT` instead of runtime exceptions and using simple `IO`-based `bracket`s in `main`.
Also known as `or` :-)
I use haskell-mode's ghci integration. I explicitly hit a key that saves the buffer and reloads it in ghci. I get all the warnings errors listed immediately and predictably. If a dependency has an error, I get that - and can jump directly to it. This method never misses warnings about unused variables/etc. Haskell tooling has a lot to improve - but it's a nicer experience than, say, Python tooling.
It'd be useful for the table of contents to be links instead of just text. :) Nice info though. I'm often at a loss with a lot of the terminology due to no higher education, so I have to have a few tabs to wikipedia pages open whenever I try to read up on haskell and other functional idea :P
What will implementing GHC on WebAssembly do for file sizes? GHCJS is already not so great, but I do worry that treating the browser as another c-like environment won't be good for code size.
&gt; The current design for DependentHaskell as described in the page you linked above claims that Type will always be "irrelevant". Could you point to the place where it's said so? I don't think that's true, and if it's written there, then the page must be updated. The latest design is in Richard's thesis. &gt; The only way "relevance" is denotationally meaningful is if you can case on Type That's exactly the proposed design. The idea is to decouple erasure from kinds. In Idris, for instance, everything but `Type` is denotationally relevant, then erasure kicks in. And `Type` is always irrelevant. But that's a not the best design possible, see this SO answer by Conor McBride: https://stackoverflow.com/questions/23220884/why-is-typecase-a-bad-thing/26012264#26012264 Having `pi` relevant and `forall` distinct quantifiers (one relevant, another irrelevant) makes perfect sense. I really recommend reading the thesis, it's brilliant!
&gt; one stack project wow this is amazing. so the prescript directory is built using stack? any GitHub repo or blog to setup such project ? thanks a lot
&gt; , I get that - and can jump directly to it. Haven't used Haskell-mode since 18 months, when I tried it for the first time and had a bad experience. Does this setup allow you to jump between each warning/error via keyboard shortcuts, or do you have to scroll manually? 
I'd expect it to be smaller than GHCJS, but it's hard to know for sure. WebAssembly kind of blows unminified, vanilla JS out of the water in file size. But once you minify it, vanilla JS can be pretty competitive. However, I'd expect the code generated by GHCJS to be a little unfriendly to minification, and GHC's WebAssembly output size ought to be competitive with GHC's native output size (at least if anyone implements `--gc-sections` in the linker)
Parametricity in types would allow the type-checker to use free theorems for type checking and/or type inference.
No, we have little shell script in the top level that we call from Jenkins to stick it all together and deploy it. Basically you need to: * stack build * stack exec &lt;your-pursript-bridge-project&gt; (this is what writes haskell types into a set of purescript modules) * psc-package &amp; webpack in the ui directory (We have a make file that does all this) to build the ui/dist app. This is the purescript code and a small html wrapper to be served by the server. * create an archive with the server executable and the ui/dist drectory * deploy archive I can't think of anywhere I've seen exactly this in the wild, /u/krisajenkins, do you know of one? Maybe we should write this up a bit better :-)
Amen. While dependent type might give us better syntax when talking about certain constraints, satisfying those constraints globally is not generally made any easier. I like have DT available, but it's certainly [no silver bullet](https://en.wikipedia.org/wiki/No_Silver_Bullet).
**No Silver Bullet** "No Silver Bullet – Essence and Accident in Software Engineering" is a widely discussed paper on software engineering written by Turing Award winner Fred Brooks in 1986. Brooks argues that "there is no single development, in either technology or management technique, which by itself promises even one order of magnitude [tenfold] improvement within a decade in productivity, in reliability, in simplicity." He also states that "we cannot expect ever to see two-fold gains every two years" in software development, as there is in hardware development (Moore's law). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Alternatively, I happily switched over to `envparse` from `optparse-applicative` some time ago. It appears to be a precursor for `optparse-generic`. 
Oh, interesting! Is there a particular reason not to do `let` as well?
[Key word is bridge](https://www.google.com/search?q=Haskell+Purescript+bridge&amp;oq=Haskell+Purescript+bridge&amp;aqs=chrome..69i57j69i64l3.3007j0j7&amp;sourceid=chrome&amp;ie=UTF-8) In general I'd sooner use a bridge/code-gen to share types between Haskell and PureScript than use GHCJS again. (I've used both PS and GHCJS in production)
It handles either `x &lt;- y` or `let x = y` in a do statement. Is that what you mean?
I used Filter, thanks all
No, I mean can you do things like $(trace [|let x = f g y y = n + m g = (* z) where z = 6|] 
I don't know if this exists, but if it did, it would definitely save some work for ad-hoc debugging. What would be interesting also is the capability to insert suitable invocations for `.eventlog` files, i.e. `labelThread` on forks, `traceEvent` on uses of `atomically` or similar. Anyway, I really like the idea!
The definition of applicative functor is incomplete in that it doesn't mention being a subclass of pointed functor (which arguably shouldn't exist on this list due to having no laws), and doesn't mention the applicative laws. I'd consider this nitpicking if functor and category didn't present laws.
&gt; There definitely is no consensus, because GHCJS was always sort of a 3rd party project that the committee never seemed to have an official opinion on. The committee sponsored WA as a Haskell SoC project though, so I can only assume they want that to succeed, but that doesn't say anything about what they want for GHCJS. &gt; FYI, SoC projects aren't picked by the haskell.org committee. They're picked by mentors and others. The committee just coordinates funding and organization. The choice of SoC projects only loosely reflects any sort of consensus or anything in the community -- they're just things that have good proposals, interested students, and look like they have a chance of success. GHCJS projects have also been conducted as SoC projects in the past.
Haskell has lenses and a bunch of other related typeclasses. Clojure has get-in, assoc-in, and update-in. Using lenses without a type system would get interesting very rapidly, but if you are using dynamic typing, you can use a vastly simpler system. Again, maybe dependent typing would let you use clojure's nested updaters in a type safe manner, but I'm sure that that brings its own complexity to the table.
Re-read my first post. There is value to using heterogenous maps as your core data structure, and haskell can't handle that well. Using structured types for your core data provides different benefits, and clojure can't handle that as well as haskell can. Unfortunately, you can't do both. If you are using structured data half of the time, then many of the benefits of using unstructured data go away. You may be stuck using unstructured data at the edges of your app, but you want to use structured data everywhere else. However, this doesn't mean that using unstructured data everywhere doesn't provide benefits of its own.
I know very little about dependent types, unfortunately. However, I can't imagine a system that could type check the issues I was talking about while still being simple enough to be practical.
Oh, you could include `let` expressions too, yep! Indeed; you could make it generate `x = (\x' -&gt; trace ("x: " ++ show x') x) (f g y)` I don't know why I didn't think of using TH for tracing before. It just works! 
Sure, if you restrict yourself to a small set of untyped collections then your indexing-related functionality is going to be simpler. But this seems to be letting the tail wag the dog. Do you *want* to be largely dealing with a small set of untyped collections or not? If so, great! You also get the benefit of a small set of combinators for accessing tem. If not, great! You also get a coherent set of typed combinators for accessing them.
Did you mean this? x = (\x' -&gt; trace ("x: " ++ show x') x') (f g y) &gt; I don't know why I didn't think of using TH for tracing before. It just works! Yeah, it's a very cool idea!
I feel like a lot of these wouldn't necessarily be called "jargon". Especially the ones that apply to basically all functional programming (currying, arity, partial application, composition, ...)
You asked what the cost of using a type system is. Here's part of the answer: in a statically typed world, you have to remember the "coherent set of typed combinators". In a dynamically typed world, you just have 3 functions to remember, so you can focus more of your brain on your problem domain.
I wish we could somehow avoid the need for `Show` in this case. I sucks when you want to debug a polymorphic function.
I jump with keyboard shortcuts, yeah. For me it works much better than intero - which just doesn't seem to work correctly on my setup for some reason.
Hmm ... maybe we're talking past each other. I didn't actually ask what the cost of a using a type system is. What I asked was &gt; Please tell me something else that would also make my life easier and then prove to me that it's incompatible with types. &gt; I'm looking for examples of things that one wants to do in Clojure that are demonstrably hard or impossible to type. I'm not looking for things that "Haskell can't handle well" or things that allow you to "focus more of your brain on your problem domain".
Wasn't there some magic typeclass that provided instances for everything or did runtime dispatch on whether a typeclass existed, or something?
I think this will end up being an "agree to disagree" issue. My argument is simply that typing involves a tradeoff -- some aspects of programming get easier if you add in a serious type system, and other aspects get harder. For you, the positives clearly outweigh the negatives. Great, use haskell. It's a fun language, and you'll be more productive in it. There certainly isn't a magical deficiency that will force you to use clojure. However, different people assign different weights to those issues. For a dynamic typing enthusiast, that tradeoff isn't worthwhile. The safety and structure that static typing provides simply isn't worth the increase in complexity and loss in expressiveness (define `rename-keys` in a way that works on the fields of arbitrary haskell records). For them, a dynamically typed language will be more productive.
I can't imagine how this would work but I'm very intrigued!
It was terrifying so I didn't look at it in detail but it was discussed here about a year ago.
Heterogenous maps that are usable as your primary record type and support `rename-keys`. How do you type that? However, I really think that you are missing the point. Static typing makes some things easier, but it does force you to keep more stuff in your head. You clearly believe that the benefits outweigh the costs. Great, use haskell (or idris, or whatever). Haskell is a very fun language, and it will make you more productive. However, for other people, that tradeoff isn't as clear cut. Different people think differently, and for some people, the benefits of static typing are outweighed by the costs. For them, clojure will be more productive.
&gt; Heterogenous maps that are usable as your primary record type and support rename-keys. How do you type that? You either accept type-unsafety and use `Dict String Dynamic` or you accept type safety and write adaptor functions from one type to another, perhaps implicitly using generics. &gt; I really think that you are missing the point [I started this thread](https://www.reddit.com/r/haskell/comments/7awn3h/on_types_and_intent_clojure_and_haskell/dpdvqde/) so I hardly think I can be missing the point! :) I have a quite specific question I'm trying to get to the bottom of. You seem to be answering something else. This is why I get the impression we're talking past each other.
Thanks for clarifying, I did not get that at first. :)
A setoid is a category with at most 1 morphism between any two objects and that morphism is an isomorphism!
&gt; You either accept type-unsafety and use `Dict String Dynamic` or you demand type safety and write adaptor functions from one type to another, perhaps implicitly using generics. For the first one, how do you define `Dynamic`? What if I need to stick a file handle or some other arbitrary type into it? For the second one, you simply can't write `rename-keys` as a generic function. You wanted a fundamental incompatibility?
&gt; how do you define Dynamic? https://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Dynamic.html#t:Dynamic &gt; you simply can't write rename-keys as a generic function. I think I'd have to know what rename-keys is so useful for first. There is another part to my request &gt; something else that would also make my life easier I don't particularly see why I'd want to rename-keys in a typesafe manner.
What about the term 'jargon' do you find objectionable?
It has a certain connotation of "overly technical" I guess. Many of the terms they list are necessary to talk about functional programming. It's kind of like saying "vector space" is linear algebra jargon. It just doens't quite make sense. 
&gt; and haskell can't handle that well And this is where I think you are wrong. Give me an example of something you think Haskell can't handle, some code that manipulates an unstructured JSON value, and I'll try my best to translate it. &gt; Unfortunately, you can't do both. Of course you can do both. I'd even say that you *should* do both. &gt; If you are using structured data half of the time, then many of the benefits of using unstructured data go away. Which benefits go away? &gt; However, this doesn't mean that using unstructured data everywhere doesn't provide benefits of its own. And which benefits does this provide? I don't see any. What I can see though is the enormous disadvantage that is the complete lack of types, which would make any kind of static analysis unfeasible.
It appears to be directly implementing this: https://en.wikipedia.org/wiki/Numerical_differentiation#Finite_difference_formulas
Regarding that, I find it helpful to write longish posts in a Markdown file end for instance use Pandoc to preview the rendered result in the browser, i.e. $ pandoc -t html -o reddit-question.html reddit-question.md $ firefox reddit-question.html I find this more convenient than using the test subreddit.
`core.spec` is a specification DSL and currently supports runtime verification and test generation. So, nothing you can't do in any other language. And since there are no types, you can't use techniques like [this](https://www.reddit.com/r/programming/comments/7a4l2r/dueling_rhetoric_of_clojure_and_haskell/dp94fq3/) to create types that witness your specification. Although there are some very early [experiments](https://github.com/arohner/spectrum) in static type checking. `core.typed` aims to be an actual static type system and looks decent, but it seems to get (very little love)[https://circleci.com/blog/why-were-no-longer-using-core-typed/) from the Clojure community, to no ones surprise. But I'm not a Clojure guy, so I might be wrong with everything. 
&gt; A monad is an object with of and chain functions. chain is like map except it un-nests the resulting nested object. Well, that makes it clear right away. Any questions?
For dynamic, that's pretty interesting, but it looks far too painful to use in your core logic. I can't imagine someone using `Dict String Dynamic` the way clojurists use maps. If you want something that makes some people's lives easier, the answer is "not having to remember 20 different sets of combinators" and "not having to remember the precise incantations necessary to work with a 5 deep stack of monad transformers". You aren't going to get a better answer than this. There is no big thing that is fundamentally impossible to do in a statically typed language. If you don't think that that is a sufficient answer, then keep using haskell. 
The relevant performance problem we had with GHCJS was the _PAINFUL_ initial load time due to the huge JS artifacts it generated, not churning shallow updates.
Have you tried to read about the Haskell type system? This might be a good place to start: http://learnyouahaskell.com/chapters The function takes an `a` and a function from `a` to `a`, where `a` is constrained to be a fractional.
Thanks for posting this! I'm the editor of Haskell Weekly and the author of the survey. In case it's not clear from the title, this survey doesn't actually have anything to do with Haskell Weekly; it's just hosted there. Next year I plan on calling it the "state of Haskell survey" to avoid any confusion. Regardless, please fill it out if you haven't yet! Over 1,200 people have already responded, and I'd love to hear from even more! 
When will the results be made public?
That benchmark compares Pux (not optimized), Thermite (not optimized, and also really just an experiment) and Halogen (optimized for a different use case, as noted) on the PureScript library side. It omits a detailed comparison of Spork which uses the optimized Halogen vdom with an Elm architecture on top (the article notes that it gets around 2x JS) and vanilla `purescript-react`, which should get much better performance than Thermite. There are definitely options to get good performance in PureScript, but I think it's just not really been the number one concern so far.
https://hackage.haskell.org/package/ifcxt perhaps? You still incur a constraint at the call site with that library though.
That's why I was thinking of! So maybe it won't work after all for this purpose. 
I was going to make a calculus joke, only to click on the topic and find it was a calculus topic. 
That's interesting, but it looks like something a correct debugger should do, instead of relying on a library for it.
Nice idea! There should probably be a separate version for `MonadIO` instances that uses `traceIO`. Also, this won't work at all when laziness is essential. Would it be possible to do something special with explicitly lazy binds? ~x &lt;- foo bar should ideally desugar to something like x &lt;- (\x' -&gt; trace ("x : " ++ show x') x') &lt;$&gt; foo bar That's still not entirely reliable, because `trace` is extremely strict in the trace message, but I don't know how to do better.
&gt; And this is where I think you are wrong. Give me an example of something you think Haskell can't handle, some code that manipulates an unstructured JSON value, and I'll try my best to translate it. If you are using haskell and want to include a function, a file handle, an `IO String`, or some other arbitrary data type in your dict, then you will be using a structured data type. You could use `Dict String Dynamic`, but I can't imagine someone using that the way that clojurists use maps. &gt; And which benefits does this provide? I don't see any. If everything is a map, then every function that works on maps works on every piece of data that you have. Similarly, if you choose the right field names, the same dict can count as a bunch of different types at once. Row polymorphism helps the second case, but it only goes so far for the first case.
Functors are also module-level functions in sml and ocaml. 
I don't know of anything that does this already. I for one would be happy to see this in a library :)
I'm a bit confused about what `jsaddle` is. On the one hand, it seems very related to GHCJS. (It's under the [ghcjs GitHub account](https://github.com/ghcjs/jsaddle).) It sounds like it was made specifically with GHCJS in mind, if GHCJS doesn't run on top of `jsaddle` altogether. On the other hand, your comment suggests that `jsaddle` is totally independent of GHCJS. After all, you're talking about building a JavaScript &lt;-&gt; wasm interface with `jsaddle`, having nothing to do with GHCJS. I could be wrong, but it seems to me like GHCJS does indeed make heavy use of `jsaddle`, but it thankfully turns out that the GHCJS team made `jsaddle` so modular and independent of the actual GHCJS that you can make use of it for your wasm endeavor. How would you describe `jsaddle`? This would be good to know, because it sounds like your Haskell -&gt; wasm compiler alongside `jsaddle` are destined to become the future of full-stack web development in Haskell! 
Good question =) TL;DR: `jsaddle` is just an interface for making JS calls from Haskell. It allows different implementations to define how those calls are actually made so that the same code can work on many platforms. `jsaddle` is a library that provides an interface for using `eval`'ing JavaScript code from Haskell. This interface is all that `jsaddle` provides. The different `jsaddle` backends are used to actually instantiate this interface with an implementation. On GHCJS, it just uses the JS FFI to call the browser's `eval` function. On GHC, `jsaddle-warp` provides an implementation that reaches out to a remote browser and asks it to eval the JS snippet. On mobile, a native web view is opened, and JS is issued to this view directly over the C FFI. `jsaddle-dom` is an implementation of WebIDL on top of `jsaddle`. So you can use `jsaddle-dom` to get a Haskell API of every browser API, and it'll run over the backend asking a browser to `eval` the JS calls you make. This turns out to be extremely fast when the Haskell implementation is GHC. But with GHCJS, I'm told `eval` can be kind of costly in terms of performance. Now, I've never benchmarked this, so I don't know if it's actually that slow or not. But `ghcjs-dom` provides a thin wrapper around `jsaddle-dom` that, when compiled with GHCJS, uses the actual JS FFI that GHCJS provides, which is supposedly much faster. And when compiled for GHC, `ghcjs-dom` will fallback to `jsaddle-dom`, so you can still run it on any `jsaddle` backend.
The other answers will help with learning more but I thought I'd try to add a quick and dirty summary. h - the dx value that we are approximating the derivative for f - the function that we are differentiating x - the value that we are differentiating f at So the function can take a dx (kind of like a resolution) and a function and then you have the derivative (or thanks to currying you can evaluate just at one point).
Hmm, couldn’t it be nice to not have beginner traps?
How should all of these things behave in the presence of null and undefined?
It'd be great to have some benchmarks for that too. One of the most important aspects in web development is first impressions. Nobody's gonna use a fancy and amazing ecosystem or library if it adds an entire 2-3 seconds to your load time (you lose an obscene amount of people if your page takes more than 3 seconds to load) but they'll likely put up with even 5-10x "slower" web pages if the initial load is pretty snappy.
I'll give this a shot sometime this week on my arch Linux setup. It's just screwy enough that quite a few "out of the box" things break on it, so if this works painlessly that's a good sign ;)
I mean I would prefer it removed, but it's hardly a "trap", seeing as it really isn't that big of a deal to use a slightly less general function then you could have.
I know that a debugger/stepper exists in ghci, but I've not yet taken the time to learn how it works. https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#the-ghci-debugger
Probably about a week or two. Watch this issue: https://github.com/haskellweekly/haskellweekly.github.io/issues/125
Our GHCJS app was ~3.5mb gzipped, fully compiled and defenestrated by the minifier. Uncompressed, it was like 10mb. This is after stripping some deps to save bytes. PureScript app was much smaller, &lt;500kb IIRC. [You can make small'ish demo examples w/ GHCJS](https://twitter.com/bitemyapp/status/762833585791389696) but it blows up quickly.
chain is also called flatMap or fmap. You can flatMap arrays for example. Imagine you have an array, and you map over it an such a way that your map function returns another array, resulting in a nested array. If you were to use flatMap instead, it would result in one single array, with each nested array flattened into the parent array. Promises in JavaScript are another example of this, since `then()` unwraps functions that return promises into the parent promise.
For all intents and purposes, I'd consider the Halogen example unoptimized as well. Halogen optimizes at component boundaries. It doesn't provide any means to optimize arbitrary virtual DOM (at least not yet). So in the case of these benchmarks, Halogen is diffing the entire virtual DOM tree on every render, since the example doesn't use Halogen components. _All_ virtual DOM based libs will likely be this slow if they have to diff the entire tree. The biggest performance wins are realized by pruning diffing branches, and none of the PureScript examples do account for this.
This is not what the author said at all. The author stated some theorems about the function, which were known just from the type, such as *"it cannot transform the elements of the list"*. There are several other useful theorems that are known from the type `[a] -&gt; [a]`. This result (the emergence of useful theorems from parametrically polymorphic types) is called *parametricity*, and it is an important tool. I recommend reading Wadler's famous paper *Theorems for free!* (1989): http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.9875
Those are pretty universal variable names. That function actually computes the slope of the secant line passing through `(x+h,f(x+h))` and `(x,f x)`. The limit of this function as h goes to zero is the derivative at `x`. When I implemented a derivative function, I made the following: data H a = H {real:: a,small:: a} instance Num a =&gt; Num (H a) where H x h + H x' h' = H (x + x') (h + h') H x h * H x' h' = H (x * x') (x * h' + x' h') (...) instance Fractional a =&gt; Fractional (H a) where -- pretty sure this right, but going off memory H 0 h / H 0 h' = H (h / h') 0 H x h / H x' _ = H (x / x') (h / x') (...) derivative f x = (f (H x 1) - f (H x 0)) / H 0 1 This computes the exact derivative
[removed]
Great answer! Things make much more sense now. Can't wait to do front-end web dev in Haskell with your brand new `jsaddle` backend. Maybe I should check back in 1 year or so? BTW, do you really believe you can create your `jsaddle` backend armed only with functions that can only have numbers as return values and parameters? It sounds like a bulk of the work will have to do with translating between numbers and other data types. (I'm not sure how many data types JS has.)
Well the numbers will basically just be pointers most of the time. JS and WebAssembly both have access to the same heap of “linear memory” (which is the contiguous chunk of arbitrary memory the browser gives your WebAssembly module). So if I want to tell JS about a string, I’ll just give it a pointer to that string in my memory.
If there is any way to make the PS benchmarks better without veering too far from idiomatic PS code, please contribute. 
What is halogen optimised for? Would you consider getting a few such use-cases included in the benchmark itself so that every framework can be compared on such use-cases? 
The JS benchmarks record startup time as well. All PS frameworks have a higher startup time than GHCJS/Miso. I'm not sure if network transfers are being included in startup time. But even if they aren't, I'm surprised yet again, why PS is slower that GHCJS (which has to start a complete RTS) after the bytes have been transferred over the network. 
I would like to hear the GHCJS adopters (who chewed my brains out the last time I put a "cautious adoption" label on GHCJS) to rebut this claim. My gut feel also says that GHCJS build artefacts are too heavy and I wouldnt use them for any serious end-consumer product (only use it for internal tools). 
I suppose you could do a bunch of overlapping instances: instance Show a =&gt; Debug a where debug = show instance Generic a =&gt; Debug a debug = genericShow instance Typeable a =&gt; Debug a debug = showTypeRep instance Debug a debug = const "&lt;unknown&gt;" I'm not sure if the correct one would be selected, but the `Debug a =&gt;` context would always be satisfied and discharged when you used `debug`.
How would you handle mutually recursive bindings?
I mean, I'd like to be able to say, "sure, the payload is heavy duty but Reflex is worth it!" but that's just not the case. I went to extreme lengths to try to get our GHCJS app down to a reasonable size and having seen the state of the compiler and how little progress it's made in the last 3 years compared to what PureScript's community has accomplish makes the choice pretty clear to me for now. In the past, I was strongly in favor of GHCJS because it was actually Haskell. Then I used it in production. I change my mind when I learn more facts or the facts change. I'm not a Haskell partisan, I'm a nice-things partisan.
&gt; tbqh GHCJS is a microcosm of the weird, Gollum-esque behavior some Haskellers in the open source part of the community engage in. This deserved a tweet-storm of its own :) 
GHCJS, like nix, gets a pass for cool points in the Haskell community. But it's important to remember that it's absolutely bleeding edge software, and has a ton of very sharp edges. PureScript's compiler is in Haskell and is quite a bit easier to work on than GHCJS, so if you need to extend/improve it, you have a clearer avenue to doing so.
I don't follow why the concern would be *untrusted* sources producing sensitive data. Without thinking too deeply, it seems like untrusted *consumers* accessing sensitive data produced by *trusted* sources would be the bigger threat.
Thanks :) one last question please. what do you use to check the code. I have tried intero with spacemacs and got error. any suggestion? 
This reminds me of a discussion about adding debug functions that do *not* interfere with laziness. We'd have to somehow "copy" the thunk and and its dependencies *as is* (no evaluation) and run the trace on the copy instead.
&gt; with of and ... un-nests O_o 
This is helpful in general. I think even *more* helpful would be a resource that presents the code examples in multiple languages. JavaScript is not an ideal lingua franca.
Good question
Maybe I'm misunderstanding, but that's sounds like nothing more than Haskell's `foldMap` function, which seems to be orthogonal to monads. I can't really seem to make a lot of sense of the article's definition of a monad, as it does not seem like an actual monad. The `of` definition seems reasonable enough, but it doesn't actually seem to say anything about the monad laws as far as I can tell. As I missing something?
I've worked on two fairly large projects in GHCJS by now. Both of them were roughly 6mb. In practice, this has not been a problem for either project. One of them has competitors written in vanilla JS that weigh more mbs. The other is basically a crud app that users usually sit in and use for extended periods without refreshing, so load time isn't super important. All in all, I'd say that unless you need to beat e.g. Slack in load times, GHCJS is fine.
That doesn't work unfortunately. The context isn't looked at when picking an instance so in this case all three of those are the same, not just overlapping.
Yeah, it's pretty much doing `f(x + h) - f(x) / h`, but I want to expand a bit because maybe the types are throwing off OP. The function `derive` you posted takes a fractional number `h`, and a function `f` on those same numbers, and returns a new function. Another way to write `derive` would be derive :: (Fractional a) =&gt; a -&gt; (a -&gt; a) -&gt; (a -&gt; a) derive h f = derivative where derivative x = (f (x + h) - f x) / h These two definitions are exactly the same. The way currying and functions work in Haskell means that the type a -&gt; (a -&gt; a) -&gt; (a -&gt; a) is exactly the same as the type a -&gt; (a -&gt; a) -&gt; a -&gt; a and so you can write `derive` as either a function taking three arguments and returning a number, or a function taking two arguments and returning another function. If you're coming from other languages, it can be hard to get used to this. It's really cool though!
So if you were to write an example expression with the function that I listed, how would that look? 
So just to be clear, if you were to use the derive function that you posted, how would that expression look like? Forexample, what would h and f look like?
The funny thing is how Ed hasn't showed up to this discussion at all. I know he's reading the arguments since he wrote this: https://www.reddit.com/r/haskell/comments/7a4l45/dueling_rhetoric_of_clojure_and_haskell/dp79btk/ but he hasn't actually said anything on the issue proper. My guess is that he's long given up on trying to argue with LISP people with their vague and hyperbolic claims all the time.
 λ&gt; derive 0.01 sin 0.0 0.9999833334166665 `h` should be something small, here `0.01`. How small this is (roughly) changes how accurate the derivative is. Then, `f` is whatever function you want to differentiate, either one you defined or one in the standard library (here, `sin`). Then you just provide the location where you want to evaluate the derivative (here `0.0`) This is a somewhat funny order, but it's done so that you can leave off the value where you want to evaluate the derivative until later: λ&gt; let sin' = derive 0.01 sin λ&gt; sin' 0.0 0.9999833334166665
Thanks a lot! I think its starting to make a bit of sense now! :)
Given the context that there are already so many things to learn to become a programmer, functions that you should probably not use accumulate, along with other things like String types that you should probably not use but most libraries require you to use etc..
Or reddit enhancement suite
And then you can simplify it a bit: foldr (\x acc -&gt; x - x * dis : acc) [] xs
I'm sorry, but this simply could not be further from the truth. Haskellers are almost obsessive about mathematical properties pertinent to functional programming. Haskell itself takes composition to the extreme, even making every possible side effect, mutation, state, or what have you into a value that can serve as input or output in composition. True mathematical composition is *always* well-defined in its inputs, otherwise it doesn't make any sense. In mathematics, a composition f ( g (x) ) *necessarily* requires that the codomain of g be equal to the domain of f (at least in category theoretic terms). And on the topic of category theory, if you think category theory is only about types, clearly you understand very little of it. Category theory is the entire theoretical framework for composition (and then some). If you took the sum of every use of composition you have ever read about or implemented, you would only have scratched the surface of category theory. It is the way at which we can provide solid foundations with which we can reason about *every* kind of composition. Haskellers as a whole are probably some of the most interested and informed people on the topics of composition, algebras, and laws. Indeed, the propensity for Haskellers to talk about category theory should drive home the point that we are extremely focused on composition, probably moreso than most programming language communities. Composition is quite literally at the heart of Haskell, and without it, the language simply could not exist. I (and most of the Haskell community) would never argue that dynamic typing does not have its uses, otherwise we would not have `Data.Dynamic`. To say that Haskellers only care about types and don't care about "real functional programming" is insulting and frankly, delusional.
fmap in haskell is *map* on the Functor typeclass *not* flatMap although I guess there maybe another language which has fmap = flatMap/bind/&gt;=
While I may not agree with some of the points in this article, I very much appreciate the far more nuanced position that the author takes compared to Eric Normand's thinly-veiled defense of Rich's talk that was posted here a little while ago.
[removed]
Note: many dependencies are updated too Recent additions: Wed Nov 8 10:46:10 UTC 2017 phadej servant-auth-swagger-0.2.8.0 Wed Nov 8 10:45:42 UTC 2017 phadej servant-auth-docs-0.2.8.0 Wed Nov 8 10:45:04 UTC 2017 phadej servant-auth-server-0.3.1.0 Wed Nov 8 10:44:41 UTC 2017 phadej servant-auth-client-0.3.1.0 Wed Nov 8 10:44:19 UTC 2017 phadej servant-auth-0.3.0.1 Wed Nov 8 09:11:06 UTC 2017 phadej servant-mock-0.8.3 Wed Nov 8 09:06:12 UTC 2017 phadej servant-multipart-0.11 Wed Nov 8 09:04:11 UTC 2017 phadej servant-js-0.9.3.1 Wed Nov 8 08:40:53 UTC 2017 phadej servant-swagger-1.1.4 Wed Nov 8 08:24:29 UTC 2017 phadej servant-docs-0.11.1 Wed Nov 8 08:24:05 UTC 2017 phadej servant-foreign-0.10.2 Wed Nov 8 08:23:39 UTC 2017 phadej servant-client-0.12 Wed Nov 8 08:23:15 UTC 2017 phadej servant-client-core-0.12 Wed Nov 8 08:22:56 UTC 2017 phadej servant-server-0.12 Wed Nov 8 08:22:36 UTC 2017 phadej servant-0.12 Recent revisions: Wed Nov 8 09:56:37 UTC 2017 #1 phadej servant-swagger-ui-0.2.4.3.4.0 Wed Nov 8 09:45:37 UTC 2017 #17 phadej servant-yaml-0.1.0.0 Wed Nov 8 08:54:10 UTC 2017 #2 phadej servant-cassava-0.9 Wed Nov 8 08:47:28 UTC 2017 #6 phadej servant-lucid-0.7.1 Wed Nov 8 08:42:50 UTC 2017 #6 phadej servant-blaze-0.7.1
derive 0.001 (\x -&gt; 3*x*x+2*x-1) 3
Could this be generalised to allow the user to pass in any `Show a =&gt; a -&gt; m ()` function? That way I could supply `logDebug` or something to use a logging framework.
emacs haskell-mode and ghcjsi
This is exactly the kind of thing the `Show` class is designed for. How can the debugger know what to print for something that doesn't have a `Show` instance? This is one of the reasons that it's a good idea to avoid polymorphism except where it's really needed.
It can be any `Name`, yeah. You would have to pass `'logDebug`, as this is generating code remember! So that's definitely possible!
Jargon is defined as &gt; the technical terminology or characteristic idiom of a special activity or group or &gt; special words or expressions used by a profession or group that are difficult for others to understand If 'vector space' isn't jargon then nothing is.
I've asked for something that fits in a JSON value. I doubt that it makes much sense to put file handles or impure functions into a JSON dictionary. Anyway, `Dynamic` is probably the wrong tools for this. What you need is an ADT like `data Value = Func (Vector Value -&gt; IO Value) | ...`, which allows you to encode impure functions as unstructured data, too. But such a value wouldn't fit into the JSON schema anymore and this shows that even unstructured data can be typed (e.g. serializable vs. non-serializable). That's why, in my opinion, statically typed languages are better at handling dynamic types than unityped languages. &gt; If everything is a map, then every function that works on maps works on every piece of data that you have. Well, it won't exactly work for *any* piece of data. If you naively map with a function that only works for integers, what happens if you map has strings in it? You also have to make sure to only map over the right kind of values. Here is an example in Haskell with `lens-aeson`: ghci&gt; json = "{\"n1\": 117, \"s\": \"string\", \"n2\": 1.5}" ghci&gt; print (over (members._Integer) (`mod` 100) json) "{\"n2\":1,\"n1\":17,\"s\":\"string\"}" Note that all this is as type safe. If you replace the modulo 100 operation by something that operates on text, it won't even compile. The error message will be something along the lines of *expected Integer but got Text*. As I see it, this is already decades ahead of anything you'll ever be able to do in Clojure.
This problem is practical in nature. Debugging polymorphic functions requires propagating the show context throughout your program and then removing it again when it's not needed. What you said is just an explanation of why this problem exists, not an explanation why it isn't a problem. Besides, "avoiding polymorphism" seems like really controversial advice, especially since in this case the argument for is it that it's a workaround for the lack of tooling .
If you have time please log some issues with specific things we could do to make things better. I'll get the ball rolling with [Replace www.ghcjs.org with something useful](https://github.com/ghcjs/ghcjs.github.com/issues/1). &gt; I went to extreme lengths to try to get our GHCJS app down to a reasonable size and having seen the state of the compiler and how little progress it's made in the last 2-3 years compared to what PureScript's community has accomplished makes the choice pretty clear to me for now. There was some deduplication code added that helped with code size a bit, was that working when you last tried GHCJS? A lot of work has gone into making GHCJS apps run super fast on mobile devices using GHC cross compiler (no JS load times at all). Unfortunately if your users need the web version to load fast it is of little value. I think you might be more interested in something like [this](https://github.com/ghcjs/ghcjs/issues/614). &gt; Unforced errors like rejecting contributions to GHCJS from people that just needed a little bit of fix-up. That sounds bad. What PR was that? Are you sure it was rejected or was it just left open until the fix-up was done? &gt; PRs staying unmerged for the better part of a year. Sorry about that. I recently went through a bunch of the PRs and merged the ones I understood and were clearly OK to merge. I'll go through the harder ones this weekend and try to work out the status or ask if they are still needed. I find one of the drawbacks of Nix is that I get lazy when it comes to upstreaming things. Typically I will add a patch to or point to a fork in my nix expression. I try to remember to also send a pull request, but if it's not merged right away I don't pursue it when I probably should. With so many people getting GHCJS from nix I think this happens quite a lot and I imagine it is annoying for anyone not using nix.
Whether it tells you much depends on your definition of *much*. I also think that the information from the type is valuable. But it appears that outside of /r/haskell, this isn't a view shared by many.
This is great, thanks! Yes, I will find it more easily later if it is a library. You anyway need to put it in a separate module and add it to the cabal file due to the TH stage restriction, so might as well add it to the cabal file as a library.
This is great! However nix-build takes &gt;20s after the tiniest change on my machine. Is there a way to get faster builds after the first one? Something like the "Building with cabal" section of [Tekmo's guide go haskell and nix](https://github.com/Gabriel439/haskell-nix/blob/master/project0/README.md).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/haskell-nix/.../**README.md** (master → 5b34448)](https://github.com/Gabriel439/haskell-nix/blob/5b3444871db03179752f6f6135bd2fbe085a79da/project0/README.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dpir2kn.)^.
This is still not a good definition of monads. An explanation should be short, intuitive, and precise, but in reality you have to pick two of these. Short and intuitive would be to say: * A monoid generalizes the idea of adding many values together, so that parentheses aren't needed. * A monad is essentially the same, but with a richer type structure. This is painfully imprecise, but a longer explanation will never be accepted by most people.
I've no idea why you got downvoted, because this is a legit question. Bind for lists is just `concatMap` with it's arguments flipped and `concatMap` is just `foldMap` for lists. Take a look at the type signatures: foldMap :: (Monoid m, Foldable t) =&gt; (a -&gt; m) -&gt; t a -&gt; m concatMap :: Foldable t =&gt; (a -&gt; [b]) -&gt; t a -&gt; [b] (=&lt;&lt;) :: Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b
The survey is now closed. Over 1,300 people responded. Thank you all for your submissions! Now we start on compiling the results. https://github.com/haskellweekly/haskellweekly.github.io/issues/125
IMHO Idris is consistent as Logic. So it may not contain Russel's paradox. Idris is more towards programming than Coq or Agda, but that is just a technical orientation, and it does not compromise on important theoretical principles. In Idris (Type = Type 0) and (Type n : Type (n+1)).
Check out transient. 
Yes and No. Mostly yes. Theoretically there is no reason why a language should not do both [see "proposition as type"]. Hence we can expect that in the future languages will help in both tasks. But now the tools are not mature enough for that. All have some practical shortcomings. The closest ones are probably Idris, F*.
Hey, I thought I'd tell you about two small additions I recently made to Concur which sort of do what you wanted. - 1. Now you can convert a widget to a [Remote Widget](https://github.com/ajnsit/concur/blob/master/concur-core/src/Concur/Core/Types.hs#L89), which allows you to send it data and it can change its UI in response. Sort of like Reflex's Events but much *much* easier to use because the data flows in the opposite direction (from the widget to the sender of the event). I used it to create a [simple postfix calculator](https://github.com/ajnsit/concur/commit/5e493db4e252e09ac3e703ed0fd8b6b7a411310d), though it's probably not the best example since the display and the buttonpad are right next to each other. 2. I added some experimental [Piping combinators](https://github.com/ajnsit/concur/blob/master/concur-core/src/Concur/Core/Pipe.hs) which allow you to treat widgets as pipes. This was mostly inspired from the [mighty Fudgets UI Lib](http://www.altocumulus.org/Fudgets/Intro/). I also wrote a [Running example for your first 8 Concur Pipes programs!](https://github.com/concurhaskell/concur-react-examples/blob/master/src/PipeWidgets.hs). There's an online [Demo](https://concurhaskell.github.io/concur-react-examples/pipes.jsexe/index.html). 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ajnsit/concur/.../**Types.hs#L89** (master → 5e493db)](https://github.com/ajnsit/concur/blob/5e493db4e252e09ac3e703ed0fd8b6b7a411310d/concur-core/src/Concur/Core/Types.hs#L89) * [ajnsit/concur/.../**Pipe.hs** (master → 5e493db)](https://github.com/ajnsit/concur/blob/5e493db4e252e09ac3e703ed0fd8b6b7a411310d/concur-core/src/Concur/Core/Pipe.hs) * [concurhaskell/concur-react-examples/.../**PipeWidgets.hs** (master → 007d47c)](https://github.com/concurhaskell/concur-react-examples/blob/007d47cedb70d38563ee90a6098b2d1c12b43308/src/PipeWidgets.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
It's not a problem because (free theorems and) parametricity is more important that "practical"ity. --- I'm not actually sure that's true, but parametricity is reason we don't have more functions that operate at literally any type. For some period of Haskell history even `seq` was isolated to a type class.
&gt;I'm sorry, but this simply could not be further from the truth. Haskellers are almost obsessive about mathematical properties pertinent to functional programming. Haskell itself takes composition to the extreme, even making every possible side effect, mutation, state, or what have you into a value that can serve as input or output in composition. True mathematical composition is always well-defined in its inputs, otherwise it doesn't make any sense. In mathematics, a composition f ( g (x) ) necessarily requires that the codomain of g be equal to the domain of f (at least in category theoretic terms). That is not my experience. Once impure effects are necessary any attempt of composability and lawfulness is abandoned. that's why haskellers expend a disproportionate amount of effort into prettyfying pure code. &gt; And on the topic of category theory, I think you are mistaken to say that it's only about types I was mentioning the perception of many haskellers, not my opinion about CT. Obviously it is not about types, but many people have this perception. &gt; Haskellers as a whole are probably some of the most interested and informed people on the topics of composition, algebras, and laws for pure code. Anythingh beyond that is considered as a necessary evil. I have called for the need for more composbility of effectful code that is 90% in real world programming without success
You should check out `transient` and its various components like `Axiom` or `transient/universe`. The `Unison` language that Paul Chiusano is working on is also relevant, but fails the "build completely in Haskell" requirement (the language itself is very resembling of Haskell though)
&gt; Auto scaling groups can help with this in theory, but I don't think they can achieve the kind of density I would like to see Why not? Do you have experience provisioning large-scale infrastructure using any of big cloud providers? If a VM is idle, you scale it down or scale the group in.
Please check out jsaddle-warp for serving your front end on GHC using auto-reloading. Also check out halive and rapid. They all rely on the great foreign-store by u/chrisdone, and everyone should know about all of them.
&gt; I thought that because PureScript does eager evaluation by default, it performs better in the JS runtime? And GHCJS has additional runtime (laziness, GC). The biggest difference I've found is the size of the generated JavaScript not the performance. 
My experience lies in AWS specifically. My understanding of ASGs is that each group represents a particular *kind* of microservice. The developer is responsible for isolating each service call (what I call "AST primitives/instructions") to a microservice, and that the choice is static. For example, an operation which analyzes a file attachment might be scoped to one microservice, which itself is scoped to one ASG. My proposal is to have a single *kind* of service (the orchestration platform itself) which is in a single ASG. The platform's scheduler will load balance AST primitives appropriately. This frees the developer from having to think about and separate service calls in to microservices. This is what I meant when I said "One kind of VM".
A book is peer reviewed. A blog/wiki is not -- apart from the comments, if everyone can be bothered to read all the comments.
Let's think about it a different way. If during a calculation you need to trace a polymorphic value, that doesn't even make sense unless the value is being used in a surrounding context where the value actually represents something concrete enough for it to be showable. So what we're really asking for here is for the runtime to preserve enough type information so that if we ask to trace a polymorphic value, the runtime can walk back through the call stack, find out the actual more concrete type of this value for this particular use, and use that to print its value. Any ideas about how something like that could be done?
Thanks! I read some of the documentation, and I noticed that the "wormhole" and "teleport" primitives require the developer to explicitly specify the node on which a computation is to take place. I want to abstract this away completely and let the platform decide the node on which an instruction is to be scheduled. This represents an architectural difference between transient and my vision. Do you agree?
Yes, `the` is part of the Idris Prelude. It's defined as such: the : (a : Type) -&gt; (value : a) -&gt; a the a = id Basically, it's used to specify the type you want some value at. It's often used to elide an explicit type signature for something. Type inference is a much more difficult problem when dependent types are around, so in Idris you usually have to provide more signatures/typings to the type checker. `the` is a lightweight way of doing that. Some examples: the Int 32 -- Interpret 32 specifically as in Int, not something polymorphic the (Int -&gt; Int) id -- Interpret id as the identity function of ints, not just any `a` In `recur`, it's used to say that `const True` should be considered `a -&gt; Bool`. I assume that without this annotation, Idris couldn't figure out unambiguous types for all the terms involved.
Thanks. My question was more specifically about this particular setup, being a Haskell+nix newbie. I feel like I would need to figure out too much to apply your advice to this nix configuration. But maybe I'm wrong. I'll have a look and report my findings here if I am successful.
What gives us anything math theory capability is type system expressiveness. Type classes do not increase the expressiveness of the type system. They are just a convenience feature, an implicit mechanism. What gives Haskell the category theory strength is abstraction over type functions [most people refer to it by "higher kinded type"]. I guess you got confused by these 2 independent features.
`the Type Type` is valid in Idris.
&gt; Pool coordination via DSL: The entire pool of VMs is orchestrated/coordinated by one ore more "scripts" written in a DSL, which is implemented as a Free Monad. Every single "operation" or "primitive" in your AST data type is Serializable, and when the framework interprets the DSL, it serializes the instruction and sends it over the network to a node for execution. The particular node on which the instruction gets executed is chosen by the platform, not the developer. See https://fugue.co/.
My point is that in haskell, unstructured data needs to stay at the edges of your app. It works great for json parsing and the like, sure, but you really don't want to use unstructured maps the way clojure does. Structured data in haskell vs unstructured data in clojure is a more interesting question. Haskell has a massive number of really cool accessors and combinators that each do a very specific thing on a specific type or typeclass. You use different sets of combinators if you are working with a monad vs an applicative vs a functor, and each piece of data has its own set of functions and lenses that work on it. On one hand, you can do lots of cool stuff and can rule out some common bugs. On the other hand, this forces you to keep a very complex system in your head while you work (or you have to constantly look up the function for X), and that is brainpower that isn't being spent on the problem domain. Also, if you want to use these complex systems with your own types and typeclasses, you have to do a fair amount of work yourself. The clojure method is far simpler. You lose all of those guarantees that haskell provided, but there are far fewer functions that you have to care about. The entire lens library is mostly replaced by `get-in`, `assoc-in`, and `update-in`, for example. Sure, lenses are more powerful, but those three functions handle the basic cases just fine, and you generally only have to keep those three functions in your head. Also, if your custom "type" is built on maps/vectors/etc, you get a bunch of utility functions that work on your data for free. Many of those functions don't really make sense, sure, but that means that even if you want to do something that looks nonsensical at first glance, there is probably a function that will help you do it. Which is better overall? Eh, it depends on the person and the project. What's the relative value of simplicity vs safety? There isn't a universal answer to that question.
I have not seen the phrase “tacit programming” before. Is it the same thing as point-free style, or is there more to it than that?
I use `stylish-haskell`. The rest seem to be a bit invasive for my tastes. 
Nice work, thank you!
exactly the same
Thanks. The page to which you linked describes "policy-as-code" and is similar to but not quite what I'm envisioning. This seems like a DevOps tool specific to cloud environments, however I'm describing a Haskell framework which runs business specific applications either in the cloud or on a set of physical machines. If you're in the business of scanning email attachments for viruses, you might have 5 to 10 basic operations ("primitives") which are CPU intensive (hash a file, etc), but that the scheduler can run on any node in the cluster.
I don't mean to simplify this, but take a look at both, one of the different styles will appeal to you, and you'll prefer that. Both are good. 
I have an app that is 11MB of raw GHCJS output but Google closure compiler and gzip bring that down to 600kb.
For cached JS is the load time still long?
what is iota? (haskell newb)
I'm using `stylish-haskell` on-save. I like that it removes trailing spaces, aligns `=` and formats import section. `hindent` formats code too ugly for me... I like `brittany` style of formatting and I would like to use it automatically! But currently I format most of the code manually.
Note that &gt; Factored out of servant-client all the functionality that was independent of the http-client backend. is mostly motivated by the fact that we're soon going to have [`servant-client-ghcjs`](https://github.com/haskell-servant/servant/pull/818).
&gt; Well it isn't. We don't get results for negative integers There's no negative integers in `Nat`. &gt; `fac = product . iota` For examples like factorial it looks nice. For examples lile `f x y = (x + y) / x^y` it looks less nice, and real life examples tend to turn into chapter soup. Especially in applicative languages without additional ways to combine functions. &gt; To answer that question we don't have to do anything but seeing if the functions we use are total. Except it's not that simple. Tacit code can be recursive too, SK calculus is a great example. 
I use `stylish-haskell` for my imports list. I'd like to use `hindent` or `brittany`, but `hindent` messes up comment placement, and `brittany` is pretty limited (last time I checked) in what it formats.
You are right about both points. Our fac however is not limited to Nats. And as for totality it depends on the chosen prelude what can be done with tacit code. 
Not a standard Haskell function, so no problem :) It is `enumFromTo 1` (so basically `[1..n]`)
Thanks, I wasn't sure if Happy learn was as good as LYAH (I mean in quality) because I've never heard of Happy learn until yesterday, gonna try to check both then.
Alright, that's suitably convincing for lists, thank you. I am certainly not used to the author's description of monads, but now that I think about it, the author is essentially describing the fact that we can write (flipped) monadic bind as `bind f = join . fmap f`, with the constraint that f is a function that lifts a value into a monad (I feel like there is a more precise term for this but I can't seem to find it). For my own understanding, is it accurate to say that a monadic bind is like creating another level nesting around the value in the monad, where such a level sort of "disappears" in the final monadic value due to the associativity of monads? 
What was the greatest motivation behind changing ‘enter’? Or, put another way, what does the update solve?
Look also at `callService` to call services without regard for the nodes where they are https://github.com/transient-haskell/transient/wiki/Transient-tutorial#services
Considering that Slack is almost unusable on my computer I sure hope we can beat Slack by a long margin.
"cautious adoption" does sound like an accurate categorization though. It's certainly not the "no-brainer" that we'd like it to be.
I would like some examples of how you think Haskellers have abandoned composability. Modern, production grade Haskell code is typically built using a monad transformer stack, which is precisely the composition of effectful code. Granted, monad transformers are not perfect, but the fact that we could further improve our compositional tools for effectful code does not mean that we simply don't care. We have dozens of libraries utilizing monads and monad transformers to create combinators for all kinds of tasks, making heavy usage of composition. The `pipes` library, for example, was built to facilitate the intersection of composition, effects, and streaming, and does a fantastic job at it. Or take the `optparse-applicative` library, which provides excellent, composable tools for easily building CLIs for applications. We have built composable tools for working with databases, webapps.. I could go on. There is always room to improve our tools and grow, which we recognize and is something that can be said for every single programming language. You are making rather sweeping generalizations about an entire community. There are plenty of us who are not CS researchers and who are interested in writing Haskell for production. Yes, we make a lot of use of types, *precisely because they give us a lot of tools with which we can reason about composition*. Types give us a framework with which we can describe how two things can compose with one another, which is something we can build on to great effect. They are an incredibly powerful compositional tool.
`enter` was/is an ambitions attempt to allow to work on whatever value of a type built from `:&lt;|&gt;` and `-&gt;`, and by being such general, it - was ambigious/overlapping (you couldn't have `(-&gt;) env` as your monad). That we tried to solve already in `0.11`. See [734](https://github.com/haskell-servant/servant/issues/734). Then breaking change was to require a type-annotation for natural transformation. - then, we got problems with type-family magic making either GHC think hard (simplifier ticks exhausted, [749](https://github.com/haskell-servant/servant/issues/749 )) or users (error messages [833](https://github.com/haskell-servant/servant/issues/833)), now (I guess), you don't need type-annotation on `nt`, because you annotate `API` type. So `hoistServer` is simpler, if you have `HasServer`, you can `hoistServer`. It's not as general as `enter` (you need `HasServer`), but works much better in practice. To rephrase: the original `enter` was "here's the `server` value and `nt`, natural transformation, do your magic, enter". It was a bit too much to ask from GHC. As a side note, I think one can write original `enter` with `reify` from `TemplateHaskell`, so the problems aren't there. I hope we had better meta-programming tools :)
The comments are a nice touch.
The problem has the same spirit as "could we just have zipListN`, well, we can, but it's not that simple. See "Double-generic zipWith, for any number of any collections" at http://okmij.org/ftp/Haskell/polyvariadic.html Note &gt; As the antithesis, we negate some design decisions and improve the type inference, avoiding the cumbersome type annotations. We make the combining function the last argument of `zipN`, and wrap it in the unique `newtype With`. 
 newtype Float = Foo -&gt; Int {- some interesting comments: Note [Monomorphism on an InitialKind] I lol'd
Yeah, those and the Simon-Marlow-style do blocks are my favorite parts.
Well slowness and initial load times are different =P
me too; it cased me to think that i am a weirdo; happy to see i am not alone
Can you make it generate code that compiles next? 
why restrict it to sub-export-lists? i want it for every sequence like language construct [term level list, sub-import-list, pattern matching on sum type cases [i mean : i want to put "| " before the first case]].
I think point-free vs. pointful is a red herring. They are just different ways to encode the exact same programs, with same semantics and problems. Just like your point-free library can exclude "fix", your point-ful language can exclude general recursion. Unless you mean in the context of an existing language like Haskell? Then it is still easy to be accidentally generally-recursive when creating a loop via a typo that uses the wrong combinator.
I recognized it from D's standard library. Where is your `iota` from?
I've never used servant. So I have a general question about it. What are the benefits of encoding the routes at the type-level over an ordinary value?
By having a flag that implicitly inserted a `Show` context on functions that require it?
There is a failure state for `fac` when the input is less than 0. A total implementation that's just wrong isn't really total, it's just silently wrong instead of noisily wrong, which can actually just be worse. So, you're still "stuck" thinking through the problem to figure out whether or not you're getting the behavior you want in non-total situations - Just because something cannot error does not mean you're going to get the right answer. I suppose this is more of a counter argument against premature totality than against tacit programming, but either way, it does not seem that tacit programming has successfully decreased the cognitive overhead necessary to produce / comprehend the desired solution.
This is very exciting!
What's the usecase for comment placement in export lists?
The very short version is: you have an arbitrarily extensible DSL (unlike with GADTs etc) for describing web apps where the descriptions themselves can affect the type of their interpretations. A server-side implementation for a JSON endpoint that returns an Int is not the same as one that returns a string, so their types should differ too. That's not the case in several libraries. Such a description also allows us to derive client functions for querying each individual endpoint (all strongly typed, with arguments in place of captures/query params/request body, etc), API docs and more. Another advantage is that we completely separate the content types / decoding and what's done with the decoded values, so server side handler don't need to deal with any of that, they just need to provide the suitable FromXXX/ToXXX instances. If it's still not clear, feel free to take a look at [https://haskell-servant.readthedocs.io/en/stable/](the tutorial). This is all a fancy way to solve the expression problem (we want full extensibility for the DSL vocabulary and the interpretations). I should one day really write down a post about this all.
Haddock uses comment placement in export lists for display purposes.
That would be a lot harder. :) The network output would need to be somehow constrained by things like name resolution, types, etc.
I can imagine a system where there are predefined edit operations defined and the NN selects which ones to apply. Defining non trivial "edit" operations on code so that it keeps compiling seems far from trivial though.
Thanks for the explanation. I think I still don't quite get it. I understand the advantages, but why aren't they attainable via regular polymorphic type params to a server without fancy type-level stuff and a fancy value-level DSL? A single value describing a service can be used to specify a client and separately the same value can be used to give an implementation of a server. (e.g: the value hides inside it the route path -- why should the route path live at the type-level?)
It's not just the routes, but a more complete description of the API: path, methods, request bodies, query parameters, etc. So you say something like: &gt; My api has two endpoints: &gt; - GET at "/people" with query params "start" (Int) and "end" (Int); returns a list of people, as JSON or plaintext. &gt; - POST ... "/people" with request body Person, returns no content. You say this in a (type-level) language. So now you have a formal description of your API, not unlike Swagger or API Blueprint or the like. Why is type-level? Well, one reason is that, given that you have a formal description of your API, you would like to make sure it's *correct*: that is, that matches what you actually implemented. `servant` does that by requiring that your handlers have the appropriate type. So e.g. the handler for the first endpoints above would be of type: Int -&gt; Int -&gt; Handler [Person] And moreover there should be a check to ensure `[Person]` has a `ToJSON` and `ToPlainText` instance. (Compare this with many other frameworks where all handlers have the same type - `Handler ByteString` or some such.) Your server can't drift from your specification, because that would be a type error. This also simplifies the writing of handlers since the encoding and decoding can be done by `servant` - it knows what to decode and encode as (from your API description), and can handle that and returning errors for you. Once you have your specification separated, you can do lots of useful things. Like generating clients to query your API automatically (servant-client, servant-python, lacky, etc.). Or generating mock servers automatically (servant-mock). Or quickchecking your entire API (servant-quickcheck). Or presenting that spec in friendlier formats (servant-docs and servant-swagger). But while this is true with e.g. `swagger` too, with `servant` you have a *proof* that all of these are actually correct. (Also, interestingly, with *Haskell* client generation in particular, we don't need to do *code-gen* and can instead just *return* client functions, which only would be possible if the description weren't type-level if all client functions had the same type, which would be significantly less nice.) There's more to be said about this: the type-level DSL approach in `servant` is also a solution to the expression problem, so it enables an *extensible* DSL for describing APIs. This turns out to be quite useful - we have tons of packages describing new combinators so that you can e.g. say that your API is auth-protected, and what that means. 
Looks like it's overfitted.
Hmm, perhaps it'd be possible to simultaneously train the network based on whether the output compiles? A bit tricky to do this, since it's just a 0 or 1 feedback. However, if you ask the same network to give many outputs, chances are some will compile, so that'd be one way to get a bit more of a continuous cost function. 
How would you handle QoS issues?
My job is secure.
I don't see an architectural difference. At the lower level you have a communications layer. At upper layers you have service discovery. Usually service discovery is what you want to use, as you say. That doesn't mean the lower levels should be opaque.
GHC source concated into a single file most likely does not yield a large enough training set for this network. I did plan to train using a larger corpus by f.ex. pulling Haskell code from Hackage. It'd probably work better if the source files in training set came from similarly formatted packages though.
Let's say my email virus scanning application must finish scanning within 2 minutes per some SLA. In addition to profiling each instruction, the script as a whole could be profiled and an appropriate action taken (spin up more VMs, send a PagerDuty alert, etc). Did you have a more specific problem in mind?
Problem 1. A decent RPC or queue system will give you traceability through multiple levels of the stack. This belongs to the RPC library. Problem 2: Apart from being able to trace RPCs, I don't see how the example where a 3rd party API's resource limits are used during debugging is helped by the architecture. Maybe you should add some more details in this section? Problem 3: For queuing it's maybe OK to not think about the distributed nature of your system, but for RPC you must differentiate between local and remove calls. Problem 4: As you say, resource consumption is hard to predict. This is the reason for having multiple ASGs or firewalls around resource consumption. You don't gain anything by packing everything into one server - your cloud provider does that for you anyways. What you gain is fault isolation, and your framework will have to replicate fault isolation domains, primarily out-of-memory and high CPU usage. Your framework also needs to figure out how you can handle multiple quality-of-service levels within one system. Basically it will have to re-implement some sort of container system. Property 1: Good idea, but you need to re-implement containers - i.e. resource isolation. Property 2: This is a RPC library. Something like Cloud Haskell, but with better traceability, plugins, scalable service discovery, group membership (non-consensus), service directory (consensus) etc. Property 3: I don't quite get this, but if the DSL is at the framework level, the only reason for it being there would be that the framework makes routing decisions based on the DSL? Not sure what this would look like. Property 4: This looks nice, and the framework could potentially use something like a neural network to figure out what to do. But I still think you need failure domains as this is only a prediction, and if will likely fail completely during the first DoS attack which the predictor hasn't seen before. Property 5: This is part of the RPC library. Property 6: I'd go a step further and define the logs in terms of software defined networking. I.e. the server executes a DSL over log entries as they are created with instructions on where to send them. On top of that DSL you can possibly implement RPC tracing, real-time log monitoring, and log shipping. Property 7: The example given seems like it belongs as a plugin in a http library. I've implemented this caching API call pattern a few times already, and like WAI has plugins, there should be a WAI-like standard on the client side as well with clearly defined plugins on the consumer side. Seems quite orthogonal to the other issues though. 
Maybe you can use a tool like `hindent` to get uniform formatting.
Username checks out somewhat. You do know we *can* sequence RNA right...?
The dynamic interface (https://hackage.haskell.org/package/sbv-7.4/docs/Data-SBV-Dynamic.html) of SBV has been designed precisely for this use case: It allows you to code and use SBV idioms over arbitrary but fixed-length bitvectors. The dynamic interface is generally used by tools that are built on top of SBV; i.e., when you want to "generate" the SBV code that further runs and generates the SMT-Lib query. Of course, you can directly use it as well. The down-side is that you lose the type-guarantees that come with the "regular" interface. You can write code that's no longer size-type correct, and SBV will happily chug along and at best give you an error, and at worst generate a query for the underlying solver that's just non-sensical. If the latter happens, if you're lucky you'll get an error from the solver, and if you are really unlucky you might get an incorrect answer without realizing. Really the same pros-cons of typed vs untyped API's/languages etc. Since you're not doing any arithmetic, I'd strongly recommend just using a list of `SBool` as you yourself suggested. The benefits of the typed-interface probably will outweigh the space efficiency you might get by packing the bits together. Feel free to ask further questions either here or on github site if you come across any issues: https://github.com/LeventErkok/sbv/issues
Hmm... I wonder if it's possible to provide the generator of information on layout. Perhaps let it know a few chars above the cursor?
https://stackoverflow.com/questions/9244879/what-does-iota-of-stdiota-stand-for
Pretty much my experience reading GHC code for the first time. Well done!
If it did compile, we wouldn't necessarily know what it's doing, but we would know it works. ;)
I'm happy to see that it even managed to write a Note.
I appreciate your detailed reply. &gt; Problem 2: Apart from being able to trace RPCs, I don't see how the example where a 3rd party API's resource limits are used during debugging is helped by the architecture. Maybe you should add some more details in this section? &gt; Ah, you're right, this is unclear. Being able to "replay" IO events (like what Haxl does) is what I was going for. That way I can re-run a message through the system without ever touching a remote API. This is excellent for unit testing as well. &gt; Problem 3: For queuing it's maybe OK to not think about the distributed nature of your system, but for RPC you must differentiate between local and remote calls. &gt; Something I don't think I made clear enough: The developer is responsible for creating the Free Monad/AST/DSL data structure, and the framework would automatically consider every instruction to be potentially remote (RPC style). Instructions without a return value could implicitly be considered queued. Here's an example: data MyAppInstruction next = AskForAThing (String -&gt; next) | SubmitAThing String next deriving (Functor) The framework would contain a generic interpreter, and when it encounters `AskForAThing`, an RPC style call would be dispatched to a remote peer. When it encounters `SubmitAThing`, the instruction would be queued. &gt; Property 2: This is a RPC library. Something like Cloud Haskell, but with better traceability, plugins, scalable service discovery, group membership (non-consensus), service directory (consensus) etc. &gt; I wish to use as many pre-existing tools as possible. For example, Consul is the king of service discovery, and it would make sense to use it. &gt; Property 3: I don't quite get this, but if the DSL is at the framework level, the only reason for it being there would be that the framework makes routing decisions based on the DSL? Not sure what this would look like. &gt; See explanation of problem 3 above. &gt; Property 4: This looks nice, and the framework could potentially use something like a neural network to figure out what to do. But I still think you need failure domains as this is only a prediction, and if will likely fail completely during the first DoS attack which the predictor hasn't seen before. &gt; By profiling each individual instruction, my hope is that the NN would be able to absorb the attack until the VM pool's resources are completely exhausted. At that time, the system will either auto-scale, refuse requests, and/or send a PagerDuty alert for human intervention. &gt; Property 7: The example given seems like it belongs as a plugin in a http library. I've implemented this caching API call pattern a few times already, and like WAI has plugins, there should be a WAI-like standard on the client side as well with clearly defined plugins on the consumer side. Seems quite orthogonal to the other issues though. You're right. I fully intend to make all of these integrations "pluginable". What I want is to keep all of these other issues (alerting, logging, monitoring, etc) completely separate from the business logic at hand.
I especially liked this line: module Bug where
Oh, derp. Thanks!
Thanks, this is exactly what I needed to know!
Interestingly dependent arrows with zero multiplicity are equivalent to parametric quantification.
Pass in the AST?
Hindent. I like the highly standardized format
That would work if we had dependent types. But we don't. If you want a _well typed_ endpoint you need to have something at the type level for the spec that tells you what type the endpoint should produce (resp consume)
We've only recently made Applicative a superclass of Monad. There are some technical wibbles and a large body of existing code that would have to change that get in the way of removing it terribly fast, but in the long run removing it, or at least upgrading it to the same constraint as traverse should happen.
&gt;messes up comment placement in export lists and in the start of `do` blocks and other weird places. I like `hindent` and usually use it, but man it annoys me that it fucks up stuff like that. There were some talk about needing to move it to use `ghc-exact-src` or something to avoid that issue, but I don't think any work is done on that at the moment?
All the modules where good.
You could try [GANs](https://en.wikipedia.org/wiki/Generative_adversarial_network).
**Generative adversarial network** Generative adversarial networks (GANs) are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework. They were introduced by Ian Goodfellow et al. in 2014. This technique can generate photographs that look authentic to human observers. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I don't really understand how copying the thunk would help. Maybe the trick is parallelism?
I think we can't do better than a GADT with the constructs of the DSL in it, if we want to go value-l pevel, but then we're not arbitrarily extensible anymore, your GADT will most likely prevent extensions that were previously possible. Without extensibility considerations, yes, we could have a GADT that accumulates what we now have in API types inside a type index or something. Also, you can "cheat" a little bit (see [this gist](https://gist.github.com/alpmestan/16b0473975b2e640364949b306cd23c3)) by accumulating the API type in the `Proxy` from smaller building blocks, which basically makes it look like you're declaring the API type """at the value level""".
I would strongly suspect one `putStrLn` + concatenation to be a lot faster than a bunch of `putStrLn`. Else the terminal would have to redraw a bunch of times.
My guess is APL, or at least that is where it probably originally came from. The equivalent code for this in apl is `f←×/⍳` which is apply iota then fold by multiplication.
Well, there seems to be a group dedicated to data analysis in Haskell: [DataHaskell][1]. But I am not sure whether they have already made an impact on the technical side of the ecosystem. In the end, I think it comes down to a matter of funding. As far as I am aware, the numerics packages in Haskell have been written entirely in the author's spare time, while the Python equivalent, numpy, has received actual funding — but [nowhere near enough][2]. [1]: http://www.datahaskell.org [2]: https://www.numfocus.org/blog/why-is-numpy-only-now-getting-funded/
vector, repa and accelerate all come from UNSW group and essentially received academic funding. What's missing is a batteries-included kind of package similar to sklearn and higher-level interface to Tensorflow (like Keras).
Was this compiled with all the optimizations enabled?
The first thing you should try is a (strict) `ByteString` version of `putStrLn`.
&gt; Haskell's numeric computing packages left a lot to be desired. In my ignorance, I would have thought that wrapping a standard numeric package would be a relatively small amount of work.
Tayacan mentioned that this could be simplified further. You might want to try out [hlint](https://github.com/ndmitchell/hlint/blob/master/README.md) out which gives tips to improve a ton of common patters like this. A lot of this becomes second nature after a while but I found it immensely helpful while starting with haskell. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ndmitchell/hlint/.../**README.md** (master → f4466ee)](https://github.com/ndmitchell/hlint/blob/f4466eed8a8bf6beccfd11052f2e3cfb074f2b44/README.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dpkfi67.)^.
Calling `write(2)` directly: $ cat yes.hs module Main where import Control.Monad import Data.Char import Foreign.Marshal.Array import System.Posix.IO import System.Posix.Types main :: IO () main = do let n = 32768 :: Int arr = concat $ replicate (n `div` 2) $ map (fromIntegral . ord) "y\n" withArray arr $ \p -&gt; forever $ fdWriteBuf (Fd 1) p (fromIntegral n) $ ghc -O2 yes.hs &amp;&amp; ./yes | pv -r &gt; /dev/null [10.9GiB.s] $ yes --version &amp;&amp; yes | pv -r &gt; /dev/null yes (GNU coreutils) 8.28 Copyright (C) 2017 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Written by David MacKenzie. [9.14GiB/s] 
No, no compilation optimisations at all, just the `stack ghc Main.hs`. If I use `stack ghc -- Main.hs -opts-O3`, I can get 164MiB/s, but my knowledge of compiler flags is lacking=)
Wow, great, thank you! Could you explain how (and why) you arrived to this implementation? Also, in [this reddit thread](https://www.reddit.com/r/unix/comments/6gxduc/how_is_gnu_yes_so_fast/diua761) people are recompiling `pv` and getting to 123GiB/s already=)
Previously concatMap had the type (a -&gt; [b]) -&gt; [a] -&gt; [b] So we start with a list of [a]'s and want a list of [b]'s. Either we return an empty list or we use the function `a -&gt; [b]` to turn the a's into b's. concatMap f ls = ... (map f ls :: [[b]]) ... Almost there but we have a nested list of b's. concatMap f la = concat (map f ls) :: [b] Which explains the name at least. It might be worth noting that this specializes concatMap is the same as the `&gt;&gt;=` implementation for lists. So [x * y | x &lt;- [1..2], y &lt;- [3..5]] is the same as concatMap (\x -&gt; concatMap (\y -&gt; [x * y]) [3..5]) [1..2] 
Servant is one of the most impressive and consistently nice-to-use packages I've ever had the pleasure of working with. Major props to everyone involved! Out of curiosity, is there any thinking about when, or under what circumstances, yall might cut a 1.0 release? I know some folks are trying to encourage more libraries to do this to avoid the perception that mature libraries are actually unstable. But I'm interested to know regardless.
The `Map` type from `Data.Map` almost has an `Applicative` instance. You can define `&lt;*&gt;` such that it preserves a key only when it exists in both `Map`s. The idea would be things like this: liftA2 (+) (fromList [('A',6),('B',9),('C',13)]) (fromList [('A',2),('B',43),('D',25)]) === fromList [('A'8),('B,52)] The problem is you cannot write `pure`. It would have to be an infinite `Map` mapping all possible keys to a single value. To my understanding, `reflex` has a similar problem with not being able to make an `Applicative` instance for `Event` (`pure` would be the `Event` that constantly fires).
&gt; For my own understanding, is it accurate to say that a monadic bind is like creating another level nesting around the value in the monad, where such a level sort of "disappears" in the final monadic value due to the associativity of monads? In a sense, yes. The function `join :: Monad m =&gt; m (m a) -&gt; m a` is joins two layers into one. If you had three layers `m (m (m a))`, you could join the inner two first, or the outer two first. The monad laws say, that the result must be the same. The same goes for more than three layers. Take the innermost parameter `a` away and you are left with only the functor `m`. Here your `join` could be interpreted as something with a signature `m * m -&gt; m`, where `m` is a monoid, in the category of (endo)functors. I prefer the view of categories and monads as monoids with a richer type structure. Monoids are for sequential composition of values. There are no inner type parameters and everything of type `m` fits together. (&lt;&gt;) :: Monoid m =&gt; m -&gt; m -&gt; m Categories are the same, but have a richer type structure. Only "matching" values can be "added". (&gt;&gt;&gt;) :: Category cat =&gt; cat a b -&gt; cat b c -&gt; cat a c Monads are closely related, but here you only have one type parameter, instead of two. (&gt;=&gt;) :: Monad m =&gt; (a -&gt; m b) -&gt; (a -&gt; m b) -&gt; (a -&gt; m b) In every case the important property is the associativity of the operator, together with a zero element. Or think of it like this: if there was no associativity, you'd have to use tons of parenthesis in do notation. Once you've understood that, the entire point of monads becomes clear.
&gt; working on getting GHC ported to WebAssembly How are you dealing with wasm's lack of garbage collection?
It would be easy to add such a constant map to the library, with essentially no cost. Do you think that is worth the trouble?
Let me clarify something: the plan is to reuse GHC’s existing RTS. So this question is similar to asking how we deal with the lack of garbage collection in x86_64 =P GHC is just going to do its own heap management with WebAssembly’s linear memory
Your naïve version in my environment still clocks at 158MiB/s, seems like either `pipe` and `pv` are the bottleneck, or you have a great machine=)
There's [this library] and [an accompanying talk](https://www.youtube.com/watch?v=sPjA6lS0GlQ) for modeling neural net construction in a type-conscious way.
Since Haskell's typeclasses were not made with constraints in mind, it happens very often, specially in numerical programming, due to `Storable` and `Unbox`. I have a library [constraint-classes](https://github.com/guaraqe/constraint-classes) that rebuilds a very large typeclass hierarchy by giving a domain (`Dom`) to type constructors via a type family `Dom`. In order to keep compatible with the prelude, every `$ClassName` is replaced by `C$ClassName` and every class method is preceded by `_`, that is `_fmap` instead of `fmap`. I use it a lot in my work, but I need to document and understand it better, it is somewhat clumsy to write constraints to polymorphic methods (due to the `Dom` constraints that we do not think about in general). Other than that, I have a partial collection of incidental counter-examples of typeclasses in [my website](http://guaraqe.gq/txt/en/2017-11-02-type-classes-incidental-non-examples.html)
`Closure` is almost-applicative for some definition of "almost"; I discuss this in [my talk about static pointers](https://skillsmatter.com/skillscasts/10632-static-pointers-closures-and-polymorphism).
Though it's not a small amount of work, it's been done already a [number](https://hackage.haskell.org/package/hmatrix) [of](http://hackage.haskell.org/package/eigen) [times](http://hackage.haskell.org/package/cusolver). One of the big outstanding questions is how to make a native numerical code that is transparent to the GHC optimizer.
Thanks! I was aware of Closure, but even with a constrained applicative you run into the problem that it does not form a proper Functor, because there is no general map function that works for any function.
Since the goal is to print an infinite number of `y`s, wouldn't a lazy bytestring make more sense?
I would have guessed a strict bytestring without any laziness tricks would be best but I think it's worth benchmarking them all.
Ah, of course. Makes sense :) However this will lead to at least 1 MB executable size. Do you think this is a cause for concern on the web? Personally, I don't think so because it may still be a massive improvement over GHCJS. (I'm not sure because I haven't checked.) 
If there were a pure operation these would still form a monad wouldn't they?
That would be nice, but how? Would `Map.toList (pure () :: Map Integer ())` return bottom?
When laziness is essential it would be better to observe the values by using the `Observable` typeclass, as is done in Haskell Debugger Hoed (http://hackage.haskell.org/package/Hoed). 
The [total-map](http://hackage.haskell.org/package/total-map-0.0.6/docs/Data-TotalMap.html) package does that. I don't think it's something people often need. Especially because for the most part, you don't really care about getting to reuse everything that works on `Applicative`s on a `Map`. You usually just want `liftA2`, which doesn't require an infinite-keys `Map and can be easily written from the things in [Data.Map.Merge.Strict](http://hackage.haskell.org/package/containers-0.5.10.2/docs/Data-Map-Merge-Strict.html).
 newtype Box n a = Box { forall m. m &lt;= n -&gt; a m } `Box n` has a `(&lt;*&gt;)` but it only has a `pure` for the `a`s that are downwards closed i.e. the `a`s such that `forall n m. m &lt;= n -&gt; A n -&gt; A m`.
They would. I misread the original question, so this isn't really an example of what you're looking for.
Are `n` and `m` type level natural numbers? Could you give the kind you expect `n` and `m` to have? Do you know of code that uses this construction?
Nontheless, worth being aware of! Thanks!
Yep natural number. But they could be anything really as long as `(&lt;)` is a well-founded order on these things. I don't know if it's used anywhere in Haskell but I do use this [in Agda to write total parser combinators](https://github.com/gallais/agdarsec/blob/master/src/Induction/Nat/Strong.agda#L9).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [gallais/agdarsec/.../**Strong.agda#L9** (master → 65e23d8)](https://github.com/gallais/agdarsec/blob/65e23d8f1bd3610f9fba409e27f7bfe44242ac77/src/Induction/Nat/Strong.agda#L9) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dpkiuzx.)^.
I was not aware of your library for constrained monads etc. Thanks for pointing me to it! Can you give me a concrete examples of a `CApplicative` (as in you library) that does not form a `CMonad`? As far as I can see the examples on your website mostly do not work because there is no obvious `pure` operation. Any examples where a suitable `pure` is available, but they still don't fit the `Applicative` class?
I see. I was suspecting that this was coming from Agda or Idris. I will look into it, but I would be more interested in Haskell examples.
Thanks for the kind words. &gt; Out of curiosity, is there any thinking about when, or under what circumstances, yall might cut a 1.0 release? As a matter of fact, /u/phadej and myself were thinking the other day that if we ever end up solving [this](https://github.com/haskell-servant/servant/issues/841) we might jump to 1.0, as it would clean up a lot of things and kill the last few "major" problems that I/we have with servant.
All of this could be done in Haskell AFAICT
IIRC putStr for string writes out each char individually. This way, if you do putStrLn $ "this bit always gets displayed in full, but not " ++ (let go = go in go) It'll print the prefix but block on the rest. Ot maybe &lt;&lt;loop&gt;&gt; but you get what i mean Changing the stdout handle to use block buffering might let you get better performance with the infinite string. Worth a shot!
total-map is about simplicity, not power. i.e: Not gaining all the power of Applicative on a Map, because the rich vocabulary in Data.Map obviously covers that. It was one of /u/conal 's running examples for his TCM (Type-class-morphisms) paper. Instead of learning 113 (!!) names in `Data.Map` you say: The denotation of `TMap k v` is `k -&gt; v` - and all the instances of `TMap` respect that denotation. Then you have 5 (!!) names in `Data.TotalMap` and the instances you already know from the `(-&gt;)` type. (To be fair, it probably should gain 5-10 more for usability). You don't only reuse `Applicative`, but also `Monoid`, `Monad`. I find it really nice that I don't have to learn `unionWith`, and can instead use a recursive `Monoid` instance. That I don't have to learn `intersect` but can reuse the `Applicative` instance.
When you see the horrible kinds of languages mathematicians invent when they need to tell a computer to calculate something(Magma, R, Matlab, Maple, etc.), I am just glad that we have something like python which is at least kind of sane for doing this type of work.
You can just take regular storable vectors from `vector` and they are unable to obey most class constraint of the form `(* -&gt; *) -&gt; Constraint`, even with the regular `(&lt;*&gt;)` that makes all the possible pairs, and when `pure = singleton`. That is, in this case the problem lies in the type system definition, not in the implementation.
Semantically, probably. Performance-wise, we don't want to follow or store the pointer that completes the cycle / ties the knot. We just want to start back at the beginning of the ByteString.
I'm assuming that `32768` was borrowed from a C implementation? If not, how did it come about?
I think this would work better as a GHC plugin. That way, you have access to the type checker, so you can see if `Show` is satisfiable or not.
Hey that's a great idea!
That reminds me of a nice almost-Applicative I came up with for combining `Map`s! In the case I was dealing with, I wanted the union, not the intersection, so in `op &lt;$&gt; mapX &lt;*&gt; mapY`, the `op` somehow has to know how to deal with missing `x`s or `y`s. My solution was for `op` to have type `Maybe a -&gt; Maybe b -&gt; c` instead of `a -&gt; b -&gt; c`: op :: Maybe a -&gt; Maybe b -&gt; Maybe c -&gt; d as :: Map k a bs :: Map k b cs :: Map k c ds :: Map k d ds = someJusts (op &lt;$?&gt; as &lt;*?&gt; bs &lt;*?&gt; cs) The `someJusts` thing is because of the problem you mention above, that `pure` needs to be an infinite `Map`. Conceptually, `op` has a value for every single possible key, so `op &lt;$?&gt; as` should not only store the value `op (Just a)` for every key which is present in `as`, it should also store the value `op Nothing` for every key which is not present. Similarly, `(op &lt;$?&gt; as) &lt;*?&gt; bs` should store: 1. `op (Just a) (Just b)` for keys which are present in both `as` and `bs` 2. `op (Just a) Nothing` for keys which are present in `as` but not `bs` 3. `op Nothing (Just b)` for keys which are present in `bs` but not `as` 4. `op Nothing Nothing` for keys which are present in neither Since cases 1, 2 and 3 cover a finite number of keys (the union of the keys in `as` and `bs`), I can store all that in a `Map k (Maybe c -&gt; d)`. Case 4 covers a potentially-infinite number of keys, but they all have the same value. So the result of `(&lt;$?&gt;)` and `(&lt;$*&gt;)` isn't a `Map`, it's a `Map` containing the results of the `op` applications in which at least one `Just` was seen, plus one extra value for the case in which `op` was applied to a bunch of `Nothing`s: data Aligning f a = Aligning { someJusts :: f a , allNothing :: a } The reason I'm calling this "Aligning" is that this doesn't just work for `Map`, it works for any instance of [`Align`](http://hackage.haskell.org/package/these-0.7.4/docs/Data-Align.html#t:Align): infixl 4 &lt;$?&gt; infixl 4 &lt;*?&gt; (&lt;$?&gt;) :: Functor f =&gt; (Maybe a -&gt; b) -&gt; f a -&gt; Aligning f b f &lt;$?&gt; fx = Aligning fy y0 where fy = f . Just &lt;$&gt; fx y0 = f Nothing (&lt;*?&gt;) :: Align f =&gt; Aligning f (Maybe a -&gt; b) -&gt; f a -&gt; Aligning f b Aligning ff f0 &lt;*?&gt; fx = Aligning (alignWith go ff fx) y0 where go (This f ) = f Nothing go (That x) = f0 (Just x) go (These f x) = f (Just x) y0 = f0 Nothing With a sum instead of a pair, a simpler version of that trick can be used to cover the case you are interested in, where you take the intersection of all the `Map`s: data Intersecting f a = Pure a | Intersected (f a) deriving (Show, Functor) instance Ord k =&gt; Applicative (Intersecting (Map k)) where pure = Pure Pure f &lt;*&gt; Pure x = Pure (f x) Pure f &lt;*&gt; Intersected xs = Intersected (f &lt;$&gt; xs) Intersected fs &lt;*&gt; Pure x = Intersected (($ x) &lt;$&gt; fs) Intersected fs &lt;*&gt; Intersected xs = Intersected (Map.intersectionWith ($) fs xs) The duality between `Aligning` and `Intersecting` makes me think that they probably have standard category-theory-inspired names somewhere, and this instance which barely uses any `Map`-specific functions makes me think that there must be a typeclass for Functors which support an `intersectionWith` method...
How much funding over what time period would be required to have a shot at getting Haskell libraries up to the standard of the python equivalents?
No, I just randomly chose a number that is power of 2 and greater than 4K (the page size). I just checked and it turns out that 32768 is about the optimal size on my laptop (16384 and 65536 are a bit slower at ~10 GiB/s), but of course it could differ on different machines.
I am not sure I understand. I can see that for storable vectors the applicative instance is not possible because neither (a -&gt; b) nor (a,b) is an instance of `Storable`. I can see how they would form a monad but I don't see how they would form an applicative. Your statement about the type system being the problem and not the implementation, also confuses me.
Thanks! I didn't know the history behind the library. Also, I feel that the `Monoid` instance for `Map` is really unfortunate. There have been various attempts to change it to the recursive `Monoid` one (actually the value only needs to be a `Semigroup` though), but there never seems agreement on doing this: - [Trac #1460](https://ghc.haskell.org/trac/ghc/ticket/1460) - [Mailing list, May 2014](https://mail.haskell.org/pipermail/libraries/2014-May/023045.html) - [Mailing list, Apr 2012](https://mail.haskell.org/pipermail/libraries/2012-April/017743.html) The biggest problem with changing the instance seems to be that it could induce silent breakage. However, avoiding this is trivial. The instance just needs to be removed entirely for 2 years and then replaced with the new instance.
Basically for a simple program like this, the fastest way would be doing the syscall directly, so it is basically a direct implementation of that in Haskell (`fdWriteBuf` is a thin wrapper around `write` I believe).
Just a wild guess: That's 2^15. The code creates a foreign array of half of that size, then writes it to a buffer. Thus, the number must have something to do with how buffering works on POSIX. The C code does the same thing. Whenever you see powers of two, and are working with C, then it probably is some low-level feature of the OS or CPU which has that number as some limit. These powers of two usually occur because of the bit width of some or other CPU bus (address bus, data bus, etc), and so the CPU/OS chooses as magic constants integers which can fit into these parallel buses. I wonder if this number is still optimal on 64 bit machines, seeing as how it seems to assume we're running on a 16 bit machine? 
And that module contains undefined f = pure () Oh yes indeed, a Bug.
There is also a `Bind` instance for `Map k`: instance Ord k =&gt; Bind (Map k) where m &gt;&gt;- f = Map.mapMaybeWithKey (\k -&gt; Map.lookup k . f) m which can be used to take maps of maps and collapse them down using the same key from the outer map to look up in the nested map. You can view the `Map k` as a memoized `ReaderT k Maybe` that way where k is `Ord`, but missing the `return`.
Limiting overhead on the syscall is important, but the most important part by far is limiting the *number* of syscalls (per output byte) by creating a large buffer and send that at once, if you check the /r/rust thread by doing that even Python gets very decent performances.
When you write a type declaration, you are making a type level implementation, not a value level implementation. The problem is not the type system, but the type declaration. In this context, writing `forall a =&gt; f a` is akin to writing `Int` instead of `forall a . Num a =&gt; a`, just at a different abstraction level. In the Haskell context, being a `Monad` implies being an `Applicative`, since `Hask` is cartesian closed. What you verified above (`Storable (a -&gt; b)` not being possible, while `Storable (a,b)` is possible and implemented in `storable-tuple`), is connected to the fact that `Storable` is cartesian, but not closed in `Hask`, which I deem to be a good example of what you asked in your original quetion: Haskell's implementation of typeclasses do not take into account subcategories of `Hask` (even if for good reasons).
$300K/sec
And removing the instance would not just be a step in the right direction, it would already be a big improvement because it would be less error-prone. I wonder if it would make sense to create a bunch of newtypes with different Monoid instances such as LeftBiased, RightBiased, etc., like we do for integers?
Wouldn't that risk overfitting even more? The adversary could memorize the entire corpus and thus would penalize any deviation from it.
The goal is to print some value without influencing its "evaluated-ness" in any way. So my idea was to "copy" (used very loosely here) the thunk and its dependency tree. For example, `let ls = [1..] in trace (show $ take 10 ls) (take 1 ls)` will evaluate the first 10 elements of `ls` even though the actual program (without tracing) only evaluates the first.
Okay, I figured it needed to be a multiple of page size. I just thought the best multiplier would be 1. :) Guess and test is a perfectly fine way to tune a constant, but I tend to leave a comment behind when I do it. ;)
&gt; since it's just a 0 or 1 feedback Maybe we can give it the number of errors and ask it to minimize that? Or, since early errors sometimes hide later errors, maybe give it the line number of the first error (infinity meaning no errors), and ask it to maximize that?
&gt; half of that size The `div 2` part is to account for the fact that `"y\n"` contains two characters. The allocated array is 2^15 bytes.
That's a tough question, it really depends on what you mean by "up to the standard of". Examples of more * Add support for sparse matrices to [`hmatrix`][1]. * Add support for 3D graphics to the [`Chart`][2] library. * Document and consolidate existing libraries (e.g. the above), making sure that they are easy to install, easy to learn, easy to interoperate, … i.e. not feature completeness, but the kind of polish you would expect for a 1.0 release. In each case, I would think that one full-time developer for at least one year is a minimum requirement. [1]: http://hackage.haskell.org/package/hmatrix [2]: http://hackage.haskell.org/package/Chart
Ah yes! I forgot that `yes` simulates **entering** 'y' on STDIN, not merely pressing the 'y' key. 
&gt; Would `Map.toList (pure () :: Map Integer ())` return bottom? Yes.... Well, probably. `Map.toAscList` certainly would, but if can ignore parametricity for a moment, any type with an which can be recursively enumerated _could_ be used in the keys without returning bottom in principle. &gt; how? One obvious (to me) way is to add a constructor to `Map` for "universal map" which is the unit of intersection.
If we had: instance Semigroup v =&gt; Map k v where ... We would actually already have all of the ones you suggested for free: import Data.Semigroup (First,Last) -- not the same as Data.Monoid.First, etc. m1 :: Map k (First v) -- left biased m2 :: Map k (Last v) -- right biased m3 :: Semigroup v =&gt; Map k v -- combines elements The main difference from what you suggested is that the newtype wrapper goes in a different place (but is still zero-cost if you use `Data.Coercible.coerce`). Anyway, I agree that even getting the instance removed would be an improvement, even if a new instance was never reintroduced. Of course, I'd love to see the one that uses `Semigroup` to combine values, but even if enough other people agree, we'd need to wait a long time before it would be safe to do that.
Man, I would love support for sparse matrices in hmatrix. I'm currently writing a piece of my program in C++ just for fast sparse matrices :(
Ok, makes sense, clever choice for the function name.
This is starting to sound just like how I fix type errors in Haskell. Change stuff at random and hope that the number of errors go down...
A possible explanation could be that the L1 data cache on my system (Intel i7-7500U) is 32K: $ lscpu | grep L1d L1d cache: 32K
If indexed applicatives qualify, [rank-2](http://hackage.haskell.org/package/rank2classes-0.2.1.1/docs/Rank2.html#t:Applicative) applicatives deserve a mention as well. The package README offers some practical examples of use. 
"Haskell Programming from First Principles" is an excellent book, and has an immense amount of depth. I've started the journey, will take a bit. &lt;grin&gt; http://haskellbook.com/
Numpy can do [sparse matrices][1], if that is any help. [1]: https://docs.scipy.org/doc/scipy/reference/sparse.html
https://github.com/Functional-AutoDiff has some pointers, and it looks like they'd welcome more.
Actually thinking about it, the error is still on the polymorphic function. Unless the plugin can say "let's just pretend that we had a Show here", it can't know if the constraint is satisfyable.
That's pretty cool. I'd never seen that package before, but I like the use-case on the readme. It blows my mind how many different libraries there are in haskell for extensible and/or functor-parametric records, but how they all end up just inconvenient enough that none of them end up being widely adopted. I say this as someone who has contributed to one such library and attempted to write another. One other slightly annoying thing about these is that you lose the ability to unpack fields into a constructor. If I write: data Person = Person { age :: {-# UNPACK #-} !Int , height :: {-# UNPACK #-} !Double , strength :: {-# UNPACK #-} !Int } I can save some indirections. However, we end up penalized with: data Person f = Person { age :: f Int , height :: f Double , strength :: f Int } Even when `f` is `Identity`, we end up with a pointer to each field instead of an unpacked value. Right now, I'm working on something that addresses this problem. 
There's essentially two ways to write a machine learning library in Haskell: (i) *Create bindings to an existing library.* There are many libraries on hackage that have done this. They are not popular with the ML community, however, because they offer no advantages over the python/C interfaces (and many disadvantages). All of these libraries were designed specifically for use with python/C, so they don't take advantage of any of Haskell's strengths. For example, it's not possible to pass a loss function written in Haskell code to any of these optimizers. If you can't pass functions to your functions, then why use functional programming? (ii) *Write a native library from scratch.* This is the approach that I've tried. I'm the author of [hlearn](http://github.com/mikeizbicki/hlearn) and [subhask](http://github.com/mikeizbicki/subhask), and my experience is that Haskell is not yet well enough developed to actually support a machine learning library *that machine learning people want to use*. Specifically: (a) The class hierarchy in the Prelude is not detailed enough for linear algebra. The `Num` class for example is the mathematical analogue of a ring, but there is no reasonable way to extend this to vector spaces and matrices. (b) The type system is not strong enough to encode even the most basic linear algebra operations. To illustrate these points, consider some of the existing packages for linear algebra: (a) The [linear](https://hackage.haskell.org/package/linear) package provides the nicest interface, but ML people would still laugh at the complexity. Why do we need 3 different operators for multiplication that all look so weird? No other language has this complexity, and if Idris-style operator overloading were allowed Haskell wouldn't need it either. Furthermore, linear is SLOW because everything is boxed. No serious numerical programming can be done with this library. (b) The other alternative is [hmatrix](https://hackage.haskell.org/package/hmatrix), which provides an interface into BLAS/LAPACK. This is closer to what ML people want because it is fast, but the interface is not as good. For example, these matrices cannot be `Functor`s because they have a constraint on what can be put inside of them. (c) I've tried rewriting an alternative Prelude with subhask, and you can see the [algebra hierarchy here](https://github.com/mikeizbicki/subhask/blob/master/src/SubHask/Algebra.hs). As you can see, it's quite complicated, and GHC just doesn't have good enough machinery to deal with complicated class hierarchies. There's a lot more to say about this topic (and a lot more examples I could give about specific improvements I want to GHC), but that's all I have time to write for now.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mikeizbicki/subhask/.../**Algebra.hs** (master → f53fd8f)](https://github.com/mikeizbicki/subhask/blob/f53fd8f465747681c88276c7dabe3646fbdf7d50/src/SubHask/Algebra.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I am hoping that this addition will make Haskell really good choice for game development. In theory, performance sensitive code would be written with linear types and ordinary Haskell would be used for "scripting". Sprinkle some multithreading and automatic computation dependency reordering in "coming soon Haxl" style over all the power Vulkan API provides. I am excited and really wonder how practical it will be :)
It's `ComonadApply` :D
Why -&gt;. and not -o? It seems to me that second one would be far more noticeable and readable in ordinary code.
True, but -o would set an unusual precedent in that it's a mix of a symbol character and and an alphanumeric character, when operators have all been symbol characters so far. But more importantly, an essential property of linear functions is that you don't need to know or care that they are linear when you're calling them from a regular (non-linear) context. So the syntax should make it easy to tell beginners: "-&gt; denotes a function type, if you see a dot after the arrow then just ignore it - it won't be relevant to any program you write until much later".
I think Haskell the language would likely be excellent. I think Haskell the community might have some serious problems. First off the Haskell community has a great desire for long lived backwards compatibility. If you try and tightly integrate into a fast moving stack you have to spend a fortune in QA and difficult management getting backwards compatibility to work well (Microsoft, DEC). So you end up having to either abstract the stack or pick a mature slowly moving stack. In theory of course a vendor could choose Haskell for their stack. However the Haskell community is rather unwelcoming to vendor driven design. Haskell started in academia, and mostly lives in academia. What's cool about Haskell comes from Haskell's uncompromising search for what's right not what's popular. Academia does not like tying itself to specific technology stacks that are commercial. I suspect a vendor trying to release a tightly coupled Haskell would hit mostly resistance. Consider how much suspicion there was around FPComplete's Stack which was open source and filling an area of deficit in a way mostly compatible with Cabal. Imagine if instead all the paranoid claims were true. It was tied to a specific commercial paid services of FPComplete and was obviously designed to advance their business. It was licensed in a non open source way. It wasn't compatible with Cabal. The Haskell community would just consider this a commercial application of Haskell and while they might be happy or unhappy it exists close collaboration would be off the table. Even an open source effort like something from the Apache foundation I don't think would get more than light support. I think the Haskell community likes to be a breeding ground for great ideas not an implementation language of great ideas. Haskell invents Darcs, C does Git. When Perl6 / Pugs was moving in the direction of Haskell (and Haskell had a lot of influence on how it turned out) the Haskell community didn't aggressively support it. So IMHO the best role for Haskell is likely in the area of Machine Learning simulation, teaching and theory. Let Haskell be a breeding ground for great ideas that move into mainstream implementations. Haskell is the language of the future not the present. 
[elm](http://elm-lang.org/) might be for you. It's a Haskell like language that directly compiles to html/js and is quite nice for simple graphical games.
Ok then, how about: `-&lt;&gt;` And when Unicode source is enable with 22B8 MULTIMAP work?
&gt; Even when f is Identity, we end up with a pointer to each field instead of an unpacked value. Right now, I'm working on something that addresses this problem. That sounds intriguing. What's your solution? 
I'm really lost by the paper. I've tried reading it several times, and I can't even get started. What does it mean for a pure value to be "consumed once", or for a primitive value to be "evaluated", when the [definition of the language](https://www.haskell.org/onlinereport/haskell2010/) explicitly denies any specific operational semantics for such constructs? Ultimately, all the pure functions of a program taken together describe mathematical relationships between values that might occur when the object code that the compiler writes is run. There is no guarantee of any direct correspondence between the values mentioned in a particular function and anything specific in the running object program. And in fact, it is this abstraction that gives Haskell so much of its power. So it seems that before we even get started with linear types, we must first formally specify *something* about an operational semantics, allowing us to define what it means for a pure value to be "consumed" or "evaluated". Only after that can we start talking about linearity. And then, the linearity guarantees will only be valid for a compiler that satisfies the operational constraints that we have specified.
What are your thoughts on writing such libraries in Idris, seeing as you mentioned it? It seems like its type system would facilitate optimizations necessary for ML folks to be satisfied with the speed while maintaining the flexibility for things like functor matrices.
The spec you linked does [talks about](https://www.haskell.org/onlinereport/haskell2010/haskellch4.html#x10-710004.2.1) "evaluation" at points: &gt; Whenever a data constructor is applied, each argument to the constructor is evaluated if and only if the corresponding type in the algebraic datatype declaration has a strictness flag, denoted by an exclamation point, “!”.
&gt;Is there a reason for the failure to be at the lexer level? The sequence `\ case` starts a layout context, which is something the lexer is concerned with, while other occurrences of `case` don't.
When I started this I didn't expect this to be such a deep topic. I simply wanted an easy fix. If I am going to make a bigger change I think I should also incorporate the next feature. I want the expressions to be extendable (with more functions from users) and also still comparable with ordering for maps. But I thought that would be too much for a haskell noob and maybe not even possible. Do you think the paper is still an applicable read?
The sequence `\` `case` starts a layout context—which is something the lexer is concerned with—while other occurrences of `case` don't.
Please, let's not try to reinforce this any more than we already are. I would actually like to get paid to be a 'real boy' Haskell developer someday, as would many people. If we continue to post that the Haskell community is 'experimental' and 'academic' first and foremost, that becomes a self-fulfilling prophecy. The more we continue to support this idea that we're all just building silly toys to prove out concepts and not real software that gets used by real people, the likelihood that anyone outside of academia is going to care about what we're building drops. It took the ideas presented by LISP like 30 years to resurface and start seriously affecting the way people write software after it was dismissed as an impractical academic curiosity. I would rather we see the ideas that Haskell introduce make their way into actual production software written in Haskell than get shelved for decades before becoming some badly thought out add-on to another, 'traditional' language.
I am a little swamped at the moment, but thank you for the reference.
A few things are unclear for me. When we read `write :: MArray a -&gt;. (Int, a) -&gt; MArray a`, it means that the input `MArray` must be consumed once, isn't it? Does it imply something on the output `MArray`? What is the point of the `Unrestricted` in the syntax? Is it to remove linearity of a subfield of a tuple which is linear ? For example `foo :: (Unrestricted a, b) -&gt;. c` means that the `b` type must be linear, but not `a`? Am I right when I understand that `a -&gt;. b -&gt;. Unrestricted c` is the same as `a -&gt;. b -&gt;. c` ? I don't understand why the high order function argument of `bindL :: IOL a -&gt;. (a -&gt;. IOL b) -&gt;. IOL b` must be linear. I understand that it must be `a -&gt;. IOL b`, but why the function by itself is linear (as denoted by the `-&gt;.` following the function, or I'm still missing something).
why not just -. which in some fonts with ligatures become similar to ⊸? 
Edit: ha, yes, and something else. Why do we need a `MArray` and a `freeze`? This API really looks like the one we have in `ST` or `IO`. Can we overload `write :: Array :p-&gt; Int :w-&gt; a :w-&gt; Array a` with a typeclass or similar to get a linear and a "normal" version?
There's an faster encoding on records I've seen several people invent independently that looks like this: import GHC.Prim (Any) import Data.Primitive (Array) newtype Rec (f :: k -&gt; Type) (xs :: [k]) = Rec (Array Any) Notice that both of the arguments to the `Rec` type are phantom. You don't export the data constructor, and you write a typeclass or a type familiy that decides elementhood and has a method which gives an index into the array. You index into the array and `unsafeCoerce`, and it all works out. The api exposed to the end user is totally safe. There's a library that already does this, but I cannot remember what it's called. It's not very difficult to write this. Here's an example of how indexing works under this approach: {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE RankNTypes #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE PolyKinds #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE TypeApplications #-} {-# LANGUAGE AllowAmbiguousTypes #-} {-# LANGUAGE TypeInType #-} {-# LANGUAGE UndecidableInstances #-} import Data.Proxy import GHC.TypeNats main :: IO () main = print (elemIndex @Color @Red @'[Green,Blue,Red,Orange]) data Color = Red | Green | Blue | Orange type family ElemIndex (x :: k) (xs :: [k]) :: Nat where ElemIndex a (a ': bs) = 0 ElemIndex a (b ': bs) = ElemIndex a bs + 1 elemIndex :: forall k (x :: k) (xs :: [k]). KnownNat (ElemIndex x xs) =&gt; Int elemIndex = fromIntegral (natVal (Proxy :: Proxy (ElemIndex x xs))) This disadvantage of the above approach is that everything in the record is always boxed. It has to be boxed because GHC's `Array#` can only hold boxed values. The approach I'm working on instead defines a record as: data Rec (f :: k -&gt; Type) (xs :: [k]) = Rec (Array# Any) ByteArray# There's a lot more computation that has to be done to figure out where things go. Two factors contribute to this: 1. All the elements don't go in the same array. 2. The unboxed elements vary in size and in alignment. So, the trick is to push this computation to runtime instead of compile time (since doing this as a GHC type level computation would be miserable or impossible) and rely on a combination of let-floating and memoization to avoid redoing the computation all the time. I've played around with this idea [here](https://gist.github.com/andrewthad/03037d8b8d0507917983e92c49762e67) only only supporting boxed values, and it actually can be made to reliably generate good GHC Core (unless a user disabled let-floating, then it just definitely cannot perform well). The drawbacks to this approach: 1. No duplicate fields are allowed (most people don't want this anyway) 2. Boilerplate. You have to write a little boilerplate for each field you want to store and for each type that can be represented by a field. 3. No wrapping in an arbitrary functor. As I reread my last comment, I realized that I misspoke about the exact problem I'm trying to solve. For functor-wrapped fields, I'll have to fall back to a different data type that just wraps `Array Any`. So, you'll lose out on the "one datatype to rule them all" benefit that the rank 2 applicatives package offers. But, I believe this can be made up for since extensible records avoid the need for data type declarations anyway.
Better yet, get it bootstrapping and have a truly mysterious executable
I've only used idris for toy problems, so I'm not sure I can fully answer that question. My sense is that if the idris compiler was as well developed as GHC, then it would fix all the problems I've run into. It's probably no harder though to improve the GHC/Haskell type system than it would be to improve the idris compiler. GHC has had TONS of engineering effort poured into it, and it'll take a lot of man hours before any compiler for new languages like idris to be anywhere near as good. I think the idris folks would agree that they can't keep up with GHC from an engineering perspective.
I'm usually a hiring manager when I have assignments work. I sometimes have choice of technologies and languages. I'm about as friendly an audience as the Haskell community can have. I would love to be able to push Haskell. But the problem with Haskell is not a public image problem. I'd argue almost the opposite. Among those who know Haskell it is regarded as extremely serious and credible. When I say an idea comes from Haskell that's an argument in favor of the idea. The problems with Haskell as the main development language for most projects are deep structural flaws in the community which make it hard to recommend Haskell as a primary language. And I'm saying this as someone who has wanted to recommend it for over 15 years and does occasionally for niches. For example in the early 2000s there was a lot of discussion about how to implement MVC application design. There was an approach to Haskell development using Visual Basic for front ends, Haskell for engines and Perl glue and text parsing. This triple had it worked out could with little modification been used on the web as well. A potential contender against J2EE and Ruby on Rails. It made a lot of sense using best of breed for MVC rather than one language. The community reaction "oh god Visual Basic...". I felt very much along the lines of, "*You are being offered a seat at the table. Stop complaining about who you are sitting next to!*". I agree with the poster about Machine Learning and Haskell. Given how easy parallelism is in Haskell and how natural map/reduce and deforestation is it would be a no brainer to win the big data war. If the community wanted to win. But it doesn't. And the fact that it doesn't is not something speaking positively is going to change. You want to work in Haskell write a binding from Haskell to any commercial or open source product out there that you like. Just pick one. And start creating a stack with end users that demonstrates the power of Haskell to them. 
That makes sense. Aren't dependent types supposed to be coming in a Haskell release soon anyway?
That sounds like a lot of work. I'm not sure I'll need that level of optimization but thank you regardless. Unpacked record fields are usually needed because people store lots of those records in an array, like a relational database table. For this use case, an alternative approach would be to store the table in column-major order. I believe the OP for the previous discussion I linked (/u/rampion) is working in that direction. This would seem safer and easier to accomplish, as each column would become either an `Array colType` or an `Array# colType`. 
I really don't understand. There are only so many combinations of ascii characters that can represent the type. Instead of coming up with more and more complex ascii art why can't we just embrace Unicode. It isn't hard to add typing shortcuts to ides for Unicode characters. 
Yes, but that is only in the informal description of strictness flags. In the formal definition, they are defined in terms of the `$!` operator, which in turn is defined in terms of `seq` in [section 6.2, "Strict Evaluation"](https://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1260006.2). There `seq` is defined by the equation: seq ⊥ b = ⊥ seq a b = b, if a ≠ ⊥ So the compiler actually isn't required to "evaluate" values in the presence of scrictness annotations. It only needs to ensure that bottoms are preserved. Of course, we know that in real life Haskell compilers some kind of evaluation is generally happening in those contexts. What I am saying is that for linear types, that concept of "evaluation" must be made more precise.
I'm not trying to say that we can solve this problem purely through words. But generally, affecting change in a community involves a lot of 'speaking positively.' The image that we present to the passerby is a part of our problem, and discouraging future efforts because they "aren't academically focused and may not mesh with the community" is not something that really lays good groundwork for bringing fresh developers with a commercial focus into the community. So, yes, we're not going to convince the die-hard academics that they need to change. But what we can do is, instead of blaming them for providing insufficient support for commercial efforts, just attempt to get more commercial users interested in Haskell. Speech alone isn't going to fix the issue, but it can at least be part of the solution, instead of part of the problem.
It would be really cool if we could get user defined mixfix operators since the language already has them for special-cased types (`[⎵]`, `(⎵,⎵,…)`) and is gaining another one (`⎵:⎵-&gt;⎵`). Also, will the `0` multiplicity be used for parametrically quantified functions?
Real talk: what are the expected performance gains from these features? Do you have plans to exploit the information offered to yield better programs? What do you need in order to achieve meaningful speedup? Does the current proposal introduce anything that can offer improved performance? Or is there further work needed to understand exactly *how* to improve the generated code (for GHC Haskell), with this as a starting point? How much work is that? A major motivation of the proposal is allegedly "performance of scientific applications". Yet there is not even an list of expected things that this change *enables* to achieve this, other than hand-wavy examples. I am not suggesting you show an example of code that *could* be improved -- I want to know exactly how you expect the compiler to go from "this is consumed once" to "this is not tracked by the GC anymore, and thus free from the working set". Perhaps this is not the right place to put such information, and I'm being too harsh. But I think it is extremely relevant to the proposal in question. Why? Because there are 1000 half-baked "ideas" and PhD papers with some SystemF extension, onpe we could add to GHC's type system that would rule out (some) incorrect programs, at the expense of excessive complexity and low usability. The only reason people actually entertain linear types, one of those thousands of ideas that add a lot of complexity, is because it promises improved performance. If it cannot deliver that, I would consider it to be in the "excessive complexity with very minimal gain" category. In short: do you actually expect *you can deliver* on what this change promises to offer? Or is that another research paper, yet to be written?
On what "evaluate" means: this is defined in the paper informally first, and then using a good old operational semantics. However if you prefer reasoning statically look at the type system first. Linear functions impose the contract of using the argument exactly once, which is useful to model all sorts of things, but mostly physical processes that cannot duplicate nor delete stuff without byproducts.
&gt; When we read write :: MArray a -&gt;. (Int, a) -&gt; MArray a, it means that the input MArray must be consumed once, isn't it? ... consumed once every time that the output is consumed once. &gt; Does it imply something on the output MArray? It links the amount of consumptions of the output to the amount of consumptions of the input. (Same thing I wrote in the 1st sentence). &gt; What is the point of the Unrestricted in the syntax? Is it to remove linearity of a subfield of a tuple which is linear ? It's more useful for output types. (see withArray in the paper) &gt; For example foo :: (Unrestricted a, b) -&gt;. c means that the b type must be linear, but not a? Correct. &gt; Am I right when I understand that a -&gt;. b -&gt;. Unrestricted c is the same as a -&gt;. b -&gt;. c ? Wrong. In the first case you consume an a, a b, and then get as many c's as you like. &gt; I don't understand why the high order function argument of bindL :: IOL a -&gt;. (a -&gt;. IOL b) -&gt;. IOL b must be linear. I understand that it must be a -&gt;. IOL b, but why the function by itself is linear (as denoted by the -&gt;. following the function, or I'm still missing something). You have a lot of options really here. There is a more generic type in the paper. 
The point is to get rid of unsafeFreeze in a low-level API. This is explained in the paper.
&gt; It isn't hard to add typing shortcuts to most IDEs for Unicode characters This seems like a really bad thing to require though. It becomes an extra hurdle for beginners, as well as a nuisance for anyone to start developing on a new computer. Right now we have unicode for people who want to spend that time, but it's not required, which seems like the best of both worlds to me.
&gt; Wrong. In the first case you consume an a, a b, and then get as many c's as you like. When you say "as many c's as you like", do you mean in this case: &gt; Consuming a function exactly once means applying it and consuming its result exactly once Because both ` a -&gt;. b -&gt;. Unrestricted c ` and `a -&gt;. b -&gt;. c` could be called from pure code and in that case they would behave indentically, wouldn't they?
Already with this proposal, you can implement new abstractions such as protocols, and safe resource management, enabling safe API to resources exposed as foreign resources allocated in a foreign heap. This is demonstrated at length in the paper https://arxiv.org/pdf/1710.09756.pdf There is a worked out operational semantics, we are sure that linear foreign objects can be outside of gc control. There is a partial prototype of a type-checker (modification of GHC) and we'd like some feedback on the design before investing more effort into polishing it. In the longer term, there can be more modifications deep into GHC. - use linearity to improve analyses and optimisations - to update the RTS. It is not known at this point how much performance gain to expect for these later stages. And of course as academics we are forced to drown you in an endless stream of papers.
Hard to say how things will evolve, but Julia could be a serious FP platform in the numerical analysis space. It interoperates well with Python.
You'll still do a lot of "how to" in Haskell source code. For example, you have to define functions as a mapping of their input, not though a general relation involving output and input values (generally). However, you won't have to allocate memory, and you'll only lightly control evaluation order, at least until GHC does one of these "wrong" and it affects your performance enough to be a bug. --- The "what is" are the types; the "how to" are the values (including function bodies). [Ref: Curry-Howard Isomorphism]
&gt; It becomes an extra hurdle for beginners Beginners probably won't be writing linear type signatures themselves. &gt; as well as a nuisance for anyone to start developing on a new computer. If they are on a new computer, they are going to be installing Haskell anyway and some sort of code editor anyway. I don't see how it is particularly burdensome. &gt; which seems like the best of both worlds to me. I don't think it is because other people also write code and a large part of writing code is reading code. Therefore, other people's preferences in this matter (albeit an insignificant matter) will affect you. In practice, supporting both really means only having ascii operators in libraries because of the convention it establishes. 
If you have infinitely many a's and b's at your disposal those types become equivalent.
I think you are missing the point. The problem isn't the language around Haskell the problem is Haskell. I think the Haskell community needs to have the conversation about whether they want commercial efforts and commercial developers of meaningful size. I'm not sure the answer really is "yes". I suspect the answer is no. Phrase the question this way.... *In exchange for 500k additional Haskell developers what things would you be willing to make standard in Haskell done in a way compatible with a vendor's interest and not compatible with what you consider best practice*? i think you'll find very little. And there is nothing wrong with that. What makes Haskell so amazing is that it is an uncompromising language. What makes Haskell difficult to deploy commercial is that is an uncompromising language. I almost never fix holes in my computer science as I try and understand a new library in another language. What is so bad with some language being a language of the future and not compromising? Scala exists to fill the popular niche. The reason you don't like it as much is because it does compromise. Heck I followed Postscript from an elegant extension and DSL based on Forth and RPL to becoming a mishmash to finally just devolving into .pdf. PDF is incredibly popular still and has been for 20 years, but no one would accuse it of being elegant or fun. The people who work in PDF do so strictly for professional reasons. 
"newtype must not accept non-linear arrow with -XLinearTypes." This seems very bad. We don't want working code that doesn't even think about or make reference to linear types to fail when we turn on this extension. But potentially any working code with a single arrow under a newtype seems like it will fail if I understand it accurately. Surely there must be a better way!
Something I ran into the other day is to consider places where we now need to use lots of freeze/unfreeze stuff for arrays and for efficient stuff we use the unsafe versions and check by hand. It seems that with linear types we may be able to have typesafe code inplace of the unsafe freeze/unfreeze ops on arrays.
Ah I see, you do that indeed. Introduced in 3.4, details in A.2. Thanks!
The jq JSON query language can be described as tacit.
&gt; Beginners probably won't be writing linear type signatures themselves. I didn't think you were suggesting that unicode operators be used solely for linear types. If we have ⊸ then surely we'll have → and ⇒ and ≤ and the rest. Making unicode required, but *only* for ⊸ seems like a dubious value proposition anyway. &gt; Other people also write code and a large part of writing code is reading code. It seems like a better solution is to have font ligatures or something similar that will render standard code using your preferred symbology. 
Real talk: can we discuss what's actually in the proposal? &gt; A major motivation of the proposal is allegedly "performance of scientific applications". I don't see the quoted phrase anywhere in the proposal... The proposal does *not* in fact aim to improve the performance of scientific applications because... *Anything you can do with linear types you can do without.* Hence... &gt; what are the expected performance gains from these features? None. Zero. That's why the proposal makes no performance claims. Here's what the proposal does claim: &gt; Linear types enable *safe* manual memory management for long-lived objects. Linear types won't magically make anything you do go faster. It's about enabling you to *statically check* invariants about how you use resources, or how you interact with other processes. It's about *providing extra safety to actions people already do today*. Now, if you know that the compiler has your back on more things, then it becomes *feasible* to do more things that would otherwise be very error prone to get right (even if it was previously *possible* all along). A concrete example of that is the zero-copy serialization/deserialization, which we dedicate a full 4 pages to in the paper. And yes, with performance numbers! In short - the linear types of this proposal (and of the paper) don't improve performance - they just make code safer (including performance sensitive code). This proposal has no ambition to change anything in the RTS or to generated code. Future proposals might. &gt; Why? Because there are 1000 half-baked "ideas" and PhD papers with some SystemF extension You seem to assume this is yet another PhD thesis. I can reassure you, it's not. Half of the authors of the paper are working on this *in an industrial setting* and the other half have a track record of turning innovative ideas from PL research into actual practical language extensions that exist in GHC today... I can certainly understand your skepticism about whether this is an extension worth having, but I believe the proposal is being pretty upfront about what it claims.
&gt; The only reason people actually entertain linear types, one of those thousands of ideas that add a lot of complexity, IMO, is because it promises improved performance in many ways Kind of an understatement I think. ATS uses them to great effect. ATS1 consistently beat C/C++ in terms of performance. 
To be clear, this is specifically about `newtype` declarations that use the GADT syntax. Regular `newtype` declarations are not affected. I don't remember seeing such a `newtype` declaration in the wild even once. But maybe this is something you write frequently?
The what is statement of course says nothing about how you might come about such a y. Which point SICP is making. Prolog and logic languages are closer to that: you define some constraints and the language engine searches the problem space, often in quite exponential ways, to produce results. But even then you have to come up with an algorithm pretty quickly. You just don't have to hold the computer's hand through every step as much. The way they use Scheme for that problem is the same as how you would Haskell, they write a pure function that implements Newton's method of successive averaging. Usually, unless you've come up some novel way of doing a process, there's a combinator like foldr, map, scanl, mapAccum, filter, dropWhile, etc. which are implemented as step-by-step functions but you compose in a rather declarative manner. This in in contrast with other languages where typically you might reach for a for/loop with mutable variables and implement a step-by-step algorithm every time. I encountered this when translating a small finance code base from C# to Haskell function by function, for a client of FP Complete and I had to take time to understand what these loops and ifs were doing until finally realizing, "oh, it's a scan!" and easily writing a Haskell version. 
Ah, I gotcha. This is where the "subtyping" perspective helps. What you're really saying is that newtypes should have a more general type for their constructor than the normal arrow, and so you want to automatically promote. So as such, this should _not_ cause compiling programs to fail? Nonetheless, let me play devil's advocate again -- what would happen if you didn't? Since linear arrows can always be promoted into unrestricted arrows, where do we lose any safety? Is it in the converse direction with pattern matching? Seeing where this actually leads to a failure would help me think understand the proposal as a whole better, I think.
Please ignore the odd timestamp on this message; it's a long story.
&gt; ATS1 consistently beat C/C++ in terms of performance Source? My understanding was that ATS was an elaborate types system that allowed you to reason about memory safety of what was essentially a C program. The ATS compiler emits unsurprising C code (unsurprising because it looks a _lot_ like the initial ATS code minus all of the types). _That_ is why it is fast and not because it uses linear types - those are just an artifact of the proofs that anyways get erased. That said, I'm no ATS expert, so I'm willing to be told I'm wrong.
This is code that would be affected. I didn't even know this was possible until I tried it now. Go on and tell me you have seen this in the wild. I dare you! ;) newtype Wrapped a where Wrapped :: a -&gt; Wrapped a
&gt;That is why it is fast and not because it uses linear types Part of how you reason about memory safety is linear types. Rust does something similar with references. You can *view* a value without consuming it but to modify it you have to have a proof that this is safe.
Typo: malloc :: Storable a =&gt; a -&gt;. (Ptr a -&gt;. Unrestricted b) -&gt;. Unrestricted b read :: Storable a =&gt; Ptr a -&gt;. (Ptr a, a) free :: a -&gt;. () I think that type of `free` should be `Ptr a -&gt;. ()`. 
... but linearity is unsafe in the presence of exceptions :( I’ve some more specific concerns about how this interacts with core, but that’s another topic entirely 
&gt; The only reason people actually entertain linear types, one of those thousands of ideas that add a lot of complexity, IMO, is because it promises improved performance in many ways. Rust uses a something like a linear type system to guarantee memory safety without a GC and be comparable to C/C++ in speed
Correct me if I'm wrong, but it seems to me that there are no direct performance implications. Seems like the proposal mostly just lets us provide the denotational semantics of immutable programming to the operational semantics of mutable programming. i.e. getting `Map.insert :: Ord k. k -&gt; v -&gt; Map k v -&gt;. Map k v` to operate with mutation, despite the API resembling `Data.Map`. It's definitely nice, but it seems a little disappointing compared to the deterministic memory management Rust uses linearity for. That seems much harder to accomplish in a lazy language, even given linearity.
&gt; we'll have → and ⇒ and ≤ and the rest The ascii approximations for those are much better. My main criticism was that we would run out of good ascii operator combinations and resort to ugly ascii art like `-.`. &gt; It seems like a better solution is to have font ligatures or something similar that will render standard code using your preferred symbology. No because then we waste two operators the prettified one and the ascii one. `-.` might not be a good operator for linear functions but it could work for subtracting a vector from a point. 
I always define newtypes as GADTs
&gt; Why do we need 3 different operators for multiplication that all look so weird? No other language has this complexity This is just an excuse for unprincipled design and overloading. If you want something to share an identifier, you must be able to fit it within the same abstraction or put it behind a qualified name. Just because math conflates notation doesn't mean we should. Scalar multiplication is conceptual different from matrix multiplication. Being principled about these distinctions rewards us with better type inference. &gt; I've tried rewriting an alternative Prelude with subhask This is pretty much necessary at this point as linear types have introduced the need for being able to abstract over multiple categories. Rather than going in that direction, people are already proposing ad-hoc hacks like they do in more mainstream languages to keep things moving (we need to remember avoid (success at all costs)). &gt; As you can see, it's quite complicated, and GHC just doesn't have good enough machinery to deal with complicated class hierarchies. This should solve some of the problems http://i.cs.hku.hk/~bruno/papers/hs2017.pdf
2019/2020
I was indeed missing your point. I appreciate you taking the time to elaborate, thank you. I disagree that compromise of architecture must go hand in hand with success. Arguably, the commercial success of Java was largely tied up in the idea that the compiler slapped your wrist if you tried to do the wrong thing. Generally, the features of Java that make it so profoundly popular in the enterprise are all built up around this idea of forward engineering and baking in safety. Java got so popular that it ate the entire dialogue about how to build good software and then plopped out the bible of design patterns in one spectacular movement. Now, I'm not saying design patterns are good. Far from it. But what I am saying is if you can convince hundreds of thousands of people to overengineer software in self-evidently terrible ways that waste millions of man hours... Why should it be so inconceivable that we could convince people to overengineer software in clean, elegant ways that save time?
&gt;That seems much harder to accomplish in a lazy language, even given linearity. I can imagine it helping the compiler.
&gt;Scalar multiplication is conceptual different from matrix multiplication. Being principled about these distinctions rewards us with better type inference. I used to feel this way, but I don't anymore. In practice, it is much easier to use `*` for all the notions of multiplication. An idris's type system is hardly "unprincipled".
&gt; it seems that before we even get started with linear types, we must first formally specify something about an operational semantics, allowing us to define what it means for a pure value to be "consumed" or "evaluated". Linear types as used in Rust or ATS do *not* in fact require you (the programmer) to define operational semantics.
&gt; This seems very bad. We don't want working code that doesn't even think about or make reference to linear types to fail when we turn on this extension. Plenty of extensions do exactly that. `OverloadedStrings` is one example.
&gt; Scalar multiplication is conceptual different from matrix multiplication. Being principled about these distinctions rewards us with better type inference. I used to think this way, but I no longer do. Idris's type system can hardly be called "unprincipled" and works well in practice. &gt; This should solve some of the problems http://i.cs.hku.hk/~bruno/papers/hs2017.pdf Quantified constraints would be awesome! I'd be super excited for this to land in GHC. But this solves about 2% of my difficulties with GHC :)
&gt;If Haskell is declarative language does this means that this thinking framework doesn't apply here ? You *can* write imperative Haskell, it's just not advisable. And once you're more familiar with Haskell you probably won't want to. Do be aware, however, that Haskell is *not* logic programming, it's functional programming. So you couldn't translate the example that you show above, unfortunately. 
oof. I guess good things take time haha
Couldn't have said it better myself. So many ideas where the "compiler can use X to generate better code", but after many years of X being available, it still doesn't. I hope you have copy-pasted this comment on official channels. 
Good point on exceptions. Can you spell put the issue in more detail? It may well be fixable with enough thought...
Underscore. Like “map-m-underscore”.
Yes, but reasoning about memory safety does not directly buy you speed. It buys you the ability to later build in primitives that are safe and fast. This proposal is only about this first step.
Like others have said, you'll still have to write some amount of "how to" code in Haskell. But "what is" &lt;-&gt; "how to" is a spectrum. The code you'd write in C to compute something is much closer to the "how to" end of that spectrum than the equivalent Haskell code. Take, for example, the problem of generating a list of the first 5 multiples of 3. In Haskell: take 5 . map (*3) $ [0..] Done. That line of code glosses over a whole lot of "how": the fact that Haskell will create a lazy infinite list; being able to operate on elements of a list "all at once" with `map`; memory management; etc. In contrast, the C: int threes[] = malloc(5 * sizeof(int)); if (threes == NULL) { // not able to malloc enough space for the array, abort exit(1); } for (int i = 0; i &lt; 5; ++i) { threes[i] = 3 * i; } // now threes is our result! This is probably not great C (I'm rusty), but you hopefully can see that we need to spell out a lot more of what the computer is supposed to do. We have to manually manage our memory; manually loop over the array; and operate on each array element individually. C makes us do all of this ourselves, whereas Haskell does it for us, because Haskell provides a much *higher level of abstraction* than C does. That basically means we get to specify less "how to" and more "what is". In some sense, mathematics is near the top of the tower of abstraction, which is why they say it's all "what is". (Disclaimer: some people, particularly in this sub, might argue that [category theory](https://en.wikipedia.org/wiki/Category_theory) is above math in the tower of abstraction. I'm neither a mathematician nor a category theorist, so I'll defer to them for that conversation :))
**Category theory** Category theory formalizes mathematical structure and its concepts in terms of a labeled directed graph called a category, whose nodes are called objects, and whose labelled directed edges are called arrows (or morphisms). A category has two basic properties: the ability to compose the arrows associatively and the existence of an identity arrow for each object. The language of category theory has been used to formalize concepts of other high-level abstractions such as sets, rings, and groups. Several terms used in category theory, including the term "morphism", are used differently from their uses in the rest of mathematics. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
May be you should understand that GHC started out as a research project. Lot of the great qualities of Haskell as a language and GHC as an implementation comes from its academic focus and from the "paper writing crowd". Do you want it to be pragmatic like Java, Javascript and the abomination called node.js
I call false dichotomy on this. Focus on research ideas AND pragmatic engineering, both, are possible. We need to start prioritising the latter as well. Isn't Raaz an example of both? Would you say it's not ready for real world usage? Or is it not performant enough? 
The fact that equations notation doesn't imply definitional equality has always seemed off to me. Here is a solution to that problem.
This is a fantastic post, thank you! I also wonder if this is a domain where the payoff is different, if not lower, than what you might expect from typical, low-brow Haskell. Namely, typed functional programming benefits quite a bit from the use of distinct types. This sounds vapid, but the ease with which we can create algebraic data types and start reaping benefits (e.g. incomplete pattern match warnings) is a pretty standard theme in static typing advocacy. The upshot is, we enforce that different things are distinct from each other. But in many machine learning contexts, we deliberately erase distinctions in order to let the tensors flow, as it were. If I want to cram disparate things into a `Vector Double` because I think there are interesting relationships between the components and, incidentally, I don't want any extra bits getting between my ALUs and all those `Double`s, I quite quickly leave the territory that Haskell has such command over.
&gt; "this is consumed once" to "this is not tracked by the GC anymore, and thus free from the working set" Surely there is a way that the GC gets object into the nurseries, no? Why wouldn't you be able to tell it "you don't need to keep track of this specific thing"?
This can be a moderately interesting discussion, but keep in mind this is really a discussion about system calls and the underlying OS more than a language implementation. Although I suppose it might provide a slight bit of insight into an implementation 's "naive" IO facilities. 
I actually feel the exact opposite about types in ML! As it is, essentially no programs incorporate machine learning directly into their code. That's because it's a huge pain in the butt to interface (for example) tensor flow into a regular python program. It's not meant for that. It's meant to be run in an offline manner. A strong type system would greatly facilitate the needed transformations to get the data into the machine learning pipeline.
Hah! I deleted the second half of my post because I felt it was rambling, but therein I had written (rambled) about how much types can help on both the front and back ends. I totally agree with that, but it's the reinventing the middle that is so painful because we're not adding as much there. Perhaps your point about interfacing the middle with the rest requires that it all be expressed in a lingua franca, but it's a big time commitment to see how it pans out. I'm still optimistic that wrapping existing libraries can pay off.
To be frank raaz is an example of neither: It has hardly produced anything path breaking in the academic circles nor is it having large scale adoption in the industry. May of the folks here do believe that pragmatism and academic elegance can go together. Depending on peoples taste the focus is often different. The main point is you have every right to do what you want with your time but you have no right to demand others to drop whatever their focus is right now and switch to what you think is most important.
&gt; scalar multiplication is conceptually different from matrix multiplication. Being principled about these distinctions rewards us with better type inference. there's a point where fighting with the type system invokes a certain kind of stockholm syndrome, and I think this falls into that range. When the type system is making actual work harder than it should be you're not benefiting from being principled any more. 
&gt; The main point is that you have every right to do what you want with your time but you have no right to demand others to drop whatever their focus is right now and switch to what you think is most important. To be pedantic, I have that right. It's called free speech :) But that's besides the point. Consider these outbursts as extremely aggressive calls to the community to prioritise certain kinds of activities over others.
It isn't fighting the type system. There is a difference between those two concepts and thus should be written differently.
Free speech gives you the right to your tongue (or vocal chords to be pedantic) but not to my ear. Extremely aggressive outbursts will not help your case.
Even if we had infinite as and bs, would the types really be equivalent if the function is passed as argument to another linear function? So far my idea of Undefined is: when a linear higer order function receives a function as argument, if the parameter function returns Undefined we can reuse its output many times without worries, even if we applied it only once. Is that correct?
I'll refrain from commenting on this thread any further. This is digressing into a completely different direction. Let's agree to disagree on this and move on.
Indeed. Thanks.
https://www.reddit.com/r/haskell/comments/6gpf24/driveby_haskell_contributions/
The problem doesn't arise when you are "constructing" a value at a newtype. But when you are eliminating it (with `case`). If the newtype has a non-linear, since case is implemented as a coercion the run-time behaviour is to silently coerce something linear into something that is not. And this breaks pretty much all the properties that you may want to enforce.
&gt; I really love Haskell, and I want to help. That's great to hear! I suspect the thing that will be most beneficial is to keep pursuing the things that you really like about Haskell. Sooner or later you'll find a place you can help that you're really excited about and because you're enjoying it so much you'll have a lot of energy to put into it.
&gt; Consider these outbursts as extremely aggressive calls to the community to prioritise certain kinds of activities over others. I think this may not be the most effective means of changing the community's priorities.
&gt; &gt; Linear types enable safe manual memory management for long-lived objects. &gt; &gt; Linear types won't magically make anything you do go faster. It's about enabling you to statically check invariants about how you use resources, or how you interact with other processes. It's about providing extra safety to code patterns people already write today. Memory management's already safe though so surely it's just other resources where you get additional *safety*.
Will there be a way to unsafely turn a nonlinear function into a linear one? I ask because there are functions which can't be done linearly in current Haskell syntax, but should be able to be. For instance, take this function: unsnocLastBit :: Natural -&gt; Maybe (Natural, Bool) unsnocLastBit n | n == 0 = Nothing unsnocLastBit n = Just (shiftR n 1, testBit n 0) This "feels" like it should be a linear function, and if `Natural` had a different, less efficient representation, it would be: data Natural = Zero | DoublePlus1 Natural | DoublePlus2 Natural add1 :: Natural -&gt;. Natural add1 n = case n of Zero -&gt; DoublePlus1 Zero DoublePlus1 n -&gt; DoublePlus2 n DoublePlus2 n -&gt; DoublePlus1 (add1 n) unsnocLastBit :: Natural -&gt;. Maybe (Natural, Bool) unsnocLastBit n = case n of Zero -&gt; Nothing DoublePlus1 n' -&gt; Just (n', True) DoublePlus2 n' -&gt; Just (add1 n', False) However, it would be an incredibly bad idea to implement natural numbers this way, just for the sake of being able to use linearity. So there needs to be some sort of `unsafe` mechanism or escape hatch to turn a nonlinear function into a linear one.
I believe you are wrong and it is the other way around. Linear types allow for deterministic memory managment. Unique types (in rust called borrowing, dual of linear types) allow for safe mutation (concurrency). I think that rust is able to achieve deterministic memory managment thanks to lifetimes. 
Cheers, hope that helps.
How can Haskell become better overall? As an easier language for people to pick up and get to building with? I'd say that we need user interaction the most. GUI stuff or non-GUI stuff would both work. It can be plain and stupid IO things, we don't need 12 levels of type magic abstractions for the core of it, in fact we actively shouldn't have that. We just need stuff that is easy to start using and keep using on windows, mac, and linux. In terms of something that just one beginner can do? Probably documentation. Which doesn't mean just haddock on individual functions libraries. It means that you need to provide high level explanations of how you actually put it together into something that works. A good example here is the main module of [Haskeline](https://hackage.haskell.org/package/haskeline-0.7.4.0/docs/System-Console-Haskeline.html), which gives a complete example program of how to use the library.
&gt; if the argument function returns Unrestricted we can reuse its output many times without worries, even as we apply the function only once. Yes. &gt; Also, what would break in the array example if we removed the Unrestricted from freeze? No unsafety, but it would be a useless function. Indeed, you'd have a single copy of the "frozen" array to work with in the rest of the program. So you might as well keep working with the mutable arrray. 
I get the feeling that a NN tuned to that would generate codes that's 99% comments and then something like main :: IO () main = return ()
All data types can be converted to their linear equivalent, so conceptually we can do it. However it would be a bad idea to offer such conversions as unsafe zero-cost operations before we have stabilized the representation of native linear values. (And no such representation is proposed at this time).
I was about to say, the gamma function (the generalization of the factorial) isn't total; it has poles on the negative integers. So the factorial function can't be total.
&lt;plug&gt; I know it's not exactly fast, but I also know of &gt;1 "real world" users out there: https://github.com/ocramz/sparse-linear-algebra 
One area that can always use helping hands is the IDE situation. For example, you could look at [Haskell IDE Engine](https://github.com/haskell/haskell-ide-engine) and the VS Code extension [vscode-hie-server](https://github.com/alanz/vscode-hie-server) :)
I see it more like the latter - an exploration of sorts of how IO works and what degree of hacking is required to tap directly into the system IO.
The proposal contains a bit more context to that quote: &gt; Systems programming with quasi real-time requirements can often benefit from easing pressure on the GC by taking long-lived objects out of the GC-managed heap entirely. Fewer long-lived objects in the heap means faster major collection times, hence shorter GC pauses. Linear types enable safe manual memory management for long-lived objects. So it depends on *how* you do memory management. It's true that if you leave it to be completely automatic (by the GC), then yes, it's safe so long as you trust the GC. If you want to do it manually, then there are a few ways of making it safe with various tradeoffs (e.g. most are limited to constrained patterns of allocation). Linear types is one of them. The paper goes into more detail comparing the alternatives.
I'll just leave this here: import Control.Monad import qualified Data.ByteString.Char8 as B main :: IO () main = forever (B.putStr buf) where {-# NOINLINE buf #-} buf = B.concat (replicate 16384 (B.pack "y\n")) 
Right, but the point of doing it manually is performance gains, which means your earlier comment is not quite right :) &gt; &gt; what are the expected performance gains from these features? &gt; None! Zero. :) That's why the proposal makes no performance claims
I completely agree. As a beginner, the documentation for Haskell made learning it that much more difficult. What would really be nice is a source like MDN where individual functions are described in excruciating detail. Basically what I'm saying is how do I get started contributing?
Please contribute to HIE.
I looked into it a few times on quiet evenings, it seemed quite hard to start contributing to for a beginner/intermediate.
To be clear, the point isn't *only* performance gains (the proposal and the paper talk about many others), and if we're talking about performance gains, the proposal alone offers none. What it does do is let those people who today have to live on the edge to get the performance gains they want, continue tomorrow to get the *same* gains but this time with the compiler having their back. Down the line, the compiler could well be a lot smarter about performance thanks to linear types. But this particular proposal doesn't include any of that, and we hope to convince you that the safety benefits alone are compelling enough for consideration.
Uniqueness types are actually fairly closely related to linear types: withArray :: Int -&gt; (Array a -&gt;. Unrestricted r) -&gt;. r This way you could do safe memory mutation without ST, ST can have some overhead because the CPR optimization doesn't kick in for nested constructors. Rust additionally handles automatic freeing which really helps when there are multiple exit paths like with exceptions. 
Pinging /u/alan_zimm (btw, is zubin duggal on reddit?)
I think fixing this would require something akin to rusts affine types. That probably needs significant support in the runtime system, though, especially the gc.
For the web side, I'd use PureScript or GHCJS.
&gt; there's a point where fighting with the type system invokes a certain kind of stockholm syndrome Well put. Would you know if anyone has more writing on this subject? I'd love to read more on it. I have a vague feeling sometimes that I've fallen into this ailment. I remember /u/deech (author of the fltkhs bindings library) expressing similar sentiments in one of his talks. I think [this one](https://www.youtube.com/watch?v=5hoQLovZBxQ). And I remember being surprised at the time: "Oh, so it is not just me who feels this way?" But then how come so few of us speak up about it publically? &lt;stream-of-consciousness&gt; Could it be that there is just very few of us that actually do experience this? And if so, where does the fault lie? With us, perceiving the type system as a 'captor' in some cases, which is looked upon as a 'savior' by many others? Or could it be that there is actually just very few of us who come from a background of dynamically typed languages, that persevere into learning a very strongly and statically typed language? Because if the syndrome is true in this context, then that could explain why many others turn back. &lt;/stream-of-consciousness&gt; And at this point my thoughts became even more incoherent than above (not that the above were sufficiently coherent to my liking), so I think I'll submit the comment as is, just so I get to have a chance to discuss the subject at hand. And maybe others chime in with "Me too!"s. Or suggest me articles to read on the subject. Or if all else fails, maybe I could try to wrangle those thoughts into coherence by enduring through writing them out loud on a keyboard.
I had a few problems with this until I realized that the linearity information is on arrows, not types/kinds. This means that a type like MArray can be used any number of times. But within the body of a function like f :: a -&gt;. b f a = e the free variable a has a linear context so it can be used only once.. Using once means pattern matching on it or passing it to a linear function. Alright, what happens if we pass a value with a linear context to a function? The result also has a linear context! So usually we will pass a value with linear context through a chain of linear functions until we get something that is easy to pattern match like `()`. So how do we make MArray memory safe? newMArray :: Int -&gt; MArray a is only safe if all consumers of the MArray promise to be linear. We have to force them to have a linear context to be safe, though. CPS to the rescue: newMArray :: Int -&gt; (MArray a -&gt;. Unrestricted r) -&gt;. Unrestricted r where Unrestricted only holds values with unrestricted contexts. This leaves two questions: - why has the callback a linear context? So that it can close over values with linear contexts. - Why is the result unrestricted as well? If the callback does have a linear context then the result would as well and we'd forget the unrestrictedness of the result. This way we can write withMArray 2 \arr1-&gt; withMArray 3 \arr2-&gt; let Unrestricted arr1' = freeze arr1 Unrestricted arr2' = freeze arr2 in Unrestricted (arr1', arr2')
&gt; What would really be nice is a source like MDN where individual functions are described in excruciating detail Ooh, that's a great idea! 
Surely any use of a polymorphic function saturates the inputs and chooses types? That is, when would this plugin ever see a polymorphic function?
&gt;EDIT: In a [blog post](https://byorgey.wordpress.com/2011/01/26/counting-linear-lambda-terms/) by Brent Yorgey, a "linear lambda" is defined as a lambda `λx.t` in which `x` occurs free in `t` exactly once. It's actually a bit more complicated. Like, `λx.(λf.g f f)(λy.t)` is not a linear lambda.
Yes but the error can exist without there being any calls to the polymorphic function.
I don't think we necessarily have to give up one to choose the other. I've definitely been attracted to Haskell because if its theoretical elegance many years ago. And now that I am here, I want to do more and more commercial things with it. To contribute to filling in that 500k quota by one. Would I want to do that if I knew that the only way to get there is to make another Java, C++, or JavaScript out of Haskell? I don't think so. I would still very much like if Haskell could be the breeding ground for groundbreaking research. And if nothing else, language extensions could (and already do) provide viability to this. At the moment, I think, the thing I am missing the most is some more "escape hatches" from 'pure elegance' to 'dirty real-world hacks', such as -- and kids, cover your ears, I am going to swear: `unsafePerformIO`, `unsafeCoerce`, `-fdefer-type-errors`, `-XPartialTypeSignatures`, `IORef`s, `Debug.Trace.trace`. That way, I can write software quick-and-dirty when I want to get something done fast. And at other times, I can take time to find more elegant and safe abstractions. And I am quite sure that if something I wrote quickly turns out to be useful, then I'll want to refactor it to be more correct, safe and elegant. Just like I do with other languages as well. And I think Haskell would prove wonderful at this latter step, much more so than other languages. And I'd love if it could help me more with the prior step as well.
I was hoping a `streaming` implementation would be competitive, but it isn't sadly: import Data.Function import Data.Int import qualified Data.ByteString as Strict import Streaming (Stream, Of(..), lift) import qualified Streaming.Prelude as S import qualified Data.ByteString.Streaming.Char8 as SBS main :: IO () main = SBS.repeat 'y' &amp; SBS.intersperse '\n' &amp; strictChunksOf 32768 &amp; S.mapM_ Strict.putStr strictChunksOf :: Monad m =&gt; Int64 -&gt; SBS.ByteString m r -&gt; Stream (Of Strict.ByteString) m r strictChunksOf n bytes = do (buffer :&gt; more) &lt;- lift (SBS.toStrict (SBS.splitAt n bytes)) S.yield buffer strictChunksOf n more 
You are welcome to join us on #haskell-ide-engine on IRC freenode. The project is quite daunting I agree, due to the many things it has to touch, but we are happy to guide you along if you choose to contribute. One of the valuable ways to contribute is to actually try it, and report bugs. Or update the documentation based on your experiences trying to get started.
Something else I would love to see get somewhere is https://mail.haskell.org/pipermail/haskell-cafe/2017-November/128130.html
Yeah, we definitely need better tooling, starting with the IDE situation, where HIE should (must?) become the de-facto standard. Also on my wish list is Debugging. Not just the kind of debugging in GHCi, but an actual debugger integrated into the IDE, possibly via LSP. 2. 
Does `===` differ between `ghc-proofs` and `inspection-testing`? More precisely, on the first example do these assert different properties: -- inspection-testing inspect $ 'lhs === 'rhs -- ghc-proofs proof = (\f x -&gt; lhs f x) === (\f x -&gt; rhs f x) 
Not quite, to safely allocate a resource haskell would have to emulate uniqueness types. Example: newMArray :: Int -&gt; (MArray a -&gt;. Unrestricted r) -&gt;. Unrestricted r That is because we don't have proof that the MArray is treated linearly otherwise. I find linearity on types much more intuitive than linearity on function arrows because of this. Linearity has to fit into the rest of haskell somehow, though, so this probably is the correct tradeoff.
There are a lot of libraries that need better documentation and examples of usage. Blogging about how to solve common problems could help.
Did you compare with my *numbers* or with with *my code compiled on your machine*? Because there is evidence that performance is *very* environment-depending. For instance, on my machine your streaming version clocks at ~990MiB/s and my naïve version is at ~160MiB/s.
To other newcomers thinking the same question as the OP, please join https://gitter.im/dataHaskell/Lobby We promise, there is good stuff on the way.
You could try reducing the number of parameters / storage capacity of the NN until what you see in the output looks like it isn't copied verbatim from the source code.
How did you install Haskell? Which version of Haskell are you using? How big are the Haskell files you're trying to compile? Does GHCi work? I've been using [Stack](https://docs.haskellstack.org/en/stable/README/) with GHC 8.0.2 and 8.2.1 successfully since High Sierra came out.
Now you have to prove that iota is total 🙂
Maybe you need to accept Xcode developer tools license to be able to invoke C compiler. Just run `cc` on Terminal, it'll let you know if you need to accept license. HTH
Eek I thought it was well known Haskell has issues with Sierra and I assume the same issues could be there for High Sierra. One major issue had something to do with a limit on the number of dynamic libraries iirc 
The fact that we'd be forced to use this CPS signature instead of the simpler one immediately makes me think of using something like ContT to provide a nicer API. Then I thought this wouldn't work because do-notation allows you to reuse bound variables. Then I realized this would work because we wouldn't be desugaring do-notation to nested `(&gt;&gt;=)` calls, but to nested `bindL` calls, of course! Hurray for rebindable-syntax :)
Very excited about client-ghcjs
&gt; GHC users will no doubt invent many more use cases over time. I'm looking forward to [making non-manifold 3D models unrepresentable](https://www.spiria.com/en/blog/desktop-software/making-non-manifold-models-unrepresentable) :)
This is a good conversation. &gt; Arguably, the commercial success of Java was largely tied up in the idea that the compiler slapped your wrist if you tried to do the wrong thing. True. But if you think back Java made major compromises. Originally it asserted a write once run anywhere philosophy. It was going to be a high level language where the JVM provided the performance tweaks. Performance would aim for good but not great. 1/5th the speed of C++ was essentially the target. That proved to be too much of a disadvantage. People were willing to tolerate about 2/3rds the performance of C++. So it had to compromise quickly. There also were platform specific tweaks to performance. Then of course once those existed almost immediately platform specific extension started being created. A full blown rejection of write once was rejected. So today Java is a large collection of things that sort of work cross platform. You end up with neither the advantages of blind cross platform ease of install and use nor the advantages of targeting specific platforms. It is intellectually incoherent but practically it turned out to be about the right level of two contradictory goals to win commercial success. And what's important here is that the Java community at the time they did this knew they were creating permanent flaws in Java in exchange for solving temporary needs. 1/5th the speed of C++ feature rich and fully cross platform would be a much better fit for 2017 than 2/3rds but iffy cross platform. They saw the problem and did it anyway. I don't think the Haskell community would have made the same choice. &gt; Generally, the features of Java that make it so profoundly popular in the enterprise are all built up around this idea of forward engineering and baking in safety. I don't think that's true at all. What made Java profoundly popular in the enterprise was * Strong vendor support from day 1 * Excellent tooling * Standardization * Modularization That is to say low and predictable staffing costs for program development and maintenance. &gt; Why should it be so inconceivable that we could convince people to overengineer software in clean, elegant ways that save time? I don't think it is inconceivable at all. I think it very likely that a FP language will become mainstream. FP concepts are clearly in fashion and bleeding into all sorts of languages: lambdas, map, folds... are becoming obligatory features. Java is slowly working towards implementing the Maybe Monad. What I do think is inconceivable is that business is going to standardize on a language whose design is driven by "what's right" not "what solves the problem reasonably well today". 
A language called "dirty Haskell" that tries to track with Haskell but turns all those things on could be successful. Haskell itself though is going to be incredibly fragile if you start using those things in combination. I suspect it isn't quick and dirty because you end up having really subtle bugs and even small changes in the program result in total collapse in terms of functionality. You would need something like Haskell but not Haskell. As an aside though, have you looked at Perl 6? It sort of aims to do what you are talking about, to be dirty Haskell. All the whipitupatude of Perl with the rather cool data structures of Haskell. 
I agree with your points mostly. But I feel it's extremely significant that Java ultimately fails to deliver as promised on most accounts. That's an extreme statement, so let's qualify 'failure' here to mean that ultimately it has not consistently out performed it's peers since, say, the early 2000s. Portability is achieved by many languages, whether via vm or interpreter. That's a wash. Modularity isn't really achievable in Java without an absurd degree of hoop jumping. Certainly I can achieve that sort of abstraction in many other languages with less pain. That's a wash. Maintainability is just a joke. Maintaining Java is like trying to sculpt a statue out of spaghetti. Tooling is definitely one are in which Java has NOT failed to deliver, it has exceptional tooling. Unfortunately the dominant Java paradigms make attempting to use Java in any significant context without that tooling essentially impossible. So I call this a victory, but I'm not sure the ultimate effect of that victory on the ecosystem has been a net gain. Ultimately the story of Java is the story of a decent idea that utterly ruined itself and continued to find widespread success because of vendorlock. I think that's a story that is starting to illustrate that maybe, sometimes it's better not to make these compromises, or at least, not to paint yourself into a corner with them. And I think the industry is starting to take that lesson, at least, in some subsections where project longevity is of strong concern. Given those two factors, I can definitely see Haskell gaining ground as 'the Java that doesn't compromise,' essentially. It hits or exceeds all of Java's main selling points save for portability, and tooling, which are both problems I'm fairly confident we will solve to an acceptable degree within the next few years. (Cross compilation being sufficiently not awful is, in my mind, sufficient for success here.) But the difference is that haskell gets there without cutting itself off at the knees, and I think that's going to be a message that sells well in certain circles. 
In spirit they are very similar, (and `inspection-testing` developed out of `ghc-proofs`) but they are implemented differently: * In `ghc-proofs`, we want as many semantically equivalent functions to be `===`. Therefore, `ghc-proofs` optimizes the expression differently than “normal” GHC, for example inlines very very aggressively. This makes it unfit for testing: https://github.com/nomeata/ghc-proofs/issues/2 * In `inspection-testing`, the code is optimized by GHC without any changes, and at the _end_ of the optimization stage, the terms are compared. So if you want to test optimizsations, use `inspection-testing`, if you want to prove program equalities, use `ghc-proofs`.
I compared against my own numbers. `yes` gives 6GiB/s, while my version gives 1GiB/s
Or like manpages for standard C functions on Unix systems. Those have helped me a lot. 
I think `persistent` handles `UUID` by default. For `JSONB` we tend to either use our own type with the `PersistFieldSql` instance handling serialization/deserialization with `JSONB`, or we use something like this: -- An EntityField type for postgres JSONB blobs newtype JSONB a = JSONB { unJSONB :: a } deriving (Generic, Show, Read, Eq, Ord, Functor, Foldable, Traversable) instance ToJSON a =&gt; ToJSON (JSONB a) where toJSON (JSONB a) = toJSON a toEncoding (JSONB a) = toEncoding a instance FromJSON a =&gt; FromJSON (JSONB a) where parseJSON = map JSONB . parseJSON instance (ToJSON a, FromJSON a) =&gt; PersistFieldSql (JSONB a) where sqlType _ = SqlOther "JSONB" instance (ToJSON a, FromJSON a) =&gt; PersistField (JSONB a) where toPersistValue = toPersistValueJSON fromPersistValue = fromPersistValueJSON
Opaleye supports UUID and JSONB, but if that's really all you want then maybe postgresql-simple is more appropriate for your needs.
ContT is a really nice approach and also handles some of the bracketing annoyance that happens because of exceptions. Alternatively you could even parametrize ContT on whether the current step can fail and give the continuation the correct multiplicity. Then you can handle linear resources however you want normally but have to add a failure branch that frees them in case of exceptions whenever you liftIO. At least with the current implementation that becomes stupendously verbose and impractical, though. You have to use the poor-mans-closure by splitting the data and function like `data FailureClosure r where Closure :: linearData -&gt;. (linearData -&gt;. IO ()) -&gt;. (linearData -&gt;. r)` or something since both branches need access to the linear data but you can only store it once.
 What about ` -&gt;|` ?
This was the issue! I hadn't dowloaded the Xcode 9. Since doing so i'm now able to install and run Haskell! Thank you so much for helping!
Guess I'm one of the idiots who didn't know then :\
:( any luck at fixing?
I believe [beam](https://github.com/tathougies/beam) supports both.
Why is `()` not `Unrestricted`? Or does multiplicity not matter for `()`? For which class of types does it not matter in general?
&gt; I have that right. It's called free speech If the best thing you can say about your position is that not illegal, you've already [lost](https://xkcd.com/1357/).
&gt; free speech I'm waiting for freer-speech. I hear it composes just as well and performs / optimizes better.
I'm not trying to win anything, because this line of conversation is not the argument I want to even be involved with. I stand by my point original point, and I'm sure Piyush stands by his. These are differing opinions, which may be hard to reconcile in the short term. I'd much rather focus on areas where we can agree and work together, rather than obsessing over where we disagree. 
&gt; That's an extreme statement, so let's qualify 'failure' here to mean that ultimately it has not consistently out performed it's peers since, say, the early 2000s. I'm not sure by the early 2000s Java had any peers. But if we are going to talk about the peers of Java in the early 2000s that is mainstream enterprise languages that companies could standardize on the closest competitors that come to mind are: C++, PHP, Visual Basic, COBOL * PHP never overcame the limitations on complexity and scalability. * Visual Basic did not transition successfully to .NET * COBOL has been continued to be unable to grow beyond its shrinking niche * enterprise C++depended on complex libraries. Thus in runtime and were long chains of wrapping and unwrapping of objects. Which made C++ slow and complex. There was certainly hope for the scripting languages but ultimately * Python and Ruby couldn't overcome their speed problems (seems to be changing for Python recently) * Perl lost ground due to several rounds of failure on Perl 6 and proved to have high maintenance costs. Are so sure Java didn't keep its position for good reason? Javascript has really been the only major exception because it crushed Java on ease of deployment. * Portability is achieved by many languages, whether via vm or interpreter. That's a wash. I'll note disagreement here. With the exception of ART/Davlik I'm hard pressed to think of any other major VM than the JVM. .NET is portable but locked into a specific vendor. Parrott failed. LLVM intermediate representation could get there with Apple and Sony leading the way. Who else is a player here? &gt; Ultimately the story of Java is the story of a decent idea that utterly ruined itself and continued to find widespread success because of vendorlock. I disagree here as well. I know lots of companies starting new projects in Java. &gt; Maintainability is just a joke. Maintaining Java is like trying to sculpt a statue out of spaghetti. It is not like software maintenance costs are unknown. The big factors are: * effective tools for software maintenance * modular design * cost of properly skilled maintenance staff * having considered the future when design the project in the first place Java is not weak in any of those areas. I'm not trying to be a jerk here. I love Haskell, I'm not fond of Java. But kidding yourself about where the bar is to beat the competition doesn't help. 
[3:33](https://youtu.be/BkTYHChkDoE?t=3m33s): &gt; Typically you can't specify which formats you're using between, but it does a pretty good job of figuring it out based on the file extension They're literally the first options listed, before the `-o` flag he already knows about: $ pandoc --help | head -n 4 pandoc [OPTIONS] [FILES] -f FORMAT, -r FORMAT --from=FORMAT, --read=FORMAT -t FORMAT, -w FORMAT --to=FORMAT, --write=FORMAT -o FILENAME --output=FILENAME 
How onerous is it to deal with Postgresql extensions in Opaleye? I just recently started using the `ltree` type in Postgres but I have only been writing raw sql for it. I think it's a cool extension, though. It also seems like an endless task trying to support every possible index or Postgresql type inside an ORM. I can't imagine how SQLAlchemy does it, honestly.
It's not very hard. You have to implement all the relevant functions and give them the correct type signature. I can help you if you're interested. [Just open an issue](https://github.com/tomjaguarpaw/haskell-opaleye/issues). https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/src/Opaleye/Operators.hs#L244
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tomjaguarpaw/haskell-opaleye/.../**Operators.hs#L244** (master → bdeb800)](https://github.com/tomjaguarpaw/haskell-opaleye/blob/bdeb800242738dcbd5f03de0307e94f89700f295/src/Opaleye/Operators.hs#L244) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Thanks for the offer of help. I was reading Opaleye the other day because I was trying to figure out if I really wanted to use profunctors for a problem and I couldn't think of any other codebases that used profunctors and where I felt like I understood the problem the project was trying to solve. I discovered that the Opaleye codebase (the few modules I looked at) was pretty readable (which surprised me, to be honest, because `product profunctors` and `arrows` sound intense). Good work on it.
Miso runs in a tight loop and supports 3D and 2D canvas. You can set the Eq instance for your model to always return False, prompting a request for a new animation frame. Example code runs in 60fps. https://github.com/dmjio/miso/blob/master/README.md#threejs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [dmjio/miso/.../**README.md#threejs** (master → 82ae55f)](https://github.com/dmjio/miso/blob/82ae55f2cd81c6120fdfbb0bbe801a44a37bd282/README.md#threejs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
&gt; You can set the Eq instance for your model to always return False That seems like a hack, is there another way to achieve similar results?
Embed the FPS onto the model with sufficient decimal places, or a frame counter. This would be equivalent to ensuring the model is always different (always passes the dirty check)
And it bears fruit already: https://github.com/kcsongor/generic-lens/commit/8f1c61e617a62173b99af532900b643289e08666
Thanks a lot!
Is there not a way to have things that intentionally vary over time instead of just as a side effect of the rerendering?
I still don't know exactly what "this" is -- like what is precisely the unsoundness with exceptions that needs to be fixed. What's a concrete example of it?
Ok, this leaves me confused. The proposal says that the issue is with e.g. newtype Unrestricted' a where Unrestricted' :: a -&gt; Unrestricted' a Ok, so say I have such a newtype (which the proposal would secretly promote to a linear one), and say it isn't promoted to a linear one. Now I pattern match on an `Unrestricted' Bool` and get back a `Bool`. That `Bool` could well be a thunk because we haven't been forced, just cast. But I still don't see the issue -- the linear thing as I understand it is the *arrow* `Bool -&gt; Unrestricted' Bool` right? The `Bool` itself isn't linear here anyway... Now, I guess, the issue is that we can get given an `Unrestricted' Bool` in a linear context, and consume its `Bool` more than once, since that itself isn't a linear field. But.. I still don't see where that breaks anything. Because I don't see how to pack a `Bool` into a nonlinear field to begin with in a way that would cause trouble. Basically I'd like to see a complete worked example that shows this violating something that's not supposed to be violated.
Reading more carefully I also saw something else weird -- most data types in `base` would now have linear fields. Which is fine, semantically. But practically does that mean that even if I don't have Linear Types turned on, `:t Just` would now show it as `a -&gt;. Maybe a`? So that beginners would then have to wonder what the heck that weird dot in the arrow was?
&gt; instead of just as a side effect of the rerendering? The model varies as new events enter the system. It is primarily an event-driven system. You can use subscriptions to enter new events into the system at user-specified time intervals.
Pretty much. The standard example seems to be a mutable array: newMArray :: Int -&gt; (MArray a -&gt;. Unrestricted r) -&gt;. Unrestricted r The linear arrow in the continuation guarantees that the MArray will be only touched by linear functions. The Unrestricted in the continuation result guarantees that all linear resources will be consumed before it returns, in the MArray case this means freezing it. Now lets look at the type of bind for IO: (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b Note that all these arrows are unrestricted and that we MAY NOT MAKE THEM LINEAR. Say we make the signature into `(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt;. IO b`which means that the continuation can close over values with linear context. If `IO a`throws an exception the continuation is never called and the linear values aren'T consumed. Best case this means resource leaks - unclosed sockets or handles, non-freed arrays and so on. But iirc its possible to make well typed programs segfault with this! So either we allow horrible bugs or linear resources can't be used with IO - both options suck. The alternative is to embed bracket into newMArray: newMArrayIO size cont = bracketOnError (unsafeNewMArray size) (\arr -&gt; when (marrayNotFreed arr) (freeMArray arr)) cont marrayNotFreed presumably would involve a separate`IORef Bool` for each MArray which sucks for the gc. So we could replace it by `IORef (Map MArrayID FreeCallback)` but then we are just reimplementing [ResourceT](https://hackage.haskell.org/package/resourcet-1.1.9/docs/Control-Monad-Trans-Resource.html#t:InternalState).
Would only allowing linearity in unexceptional IO work? https://hackage.haskell.org/package/unexceptionalio Although actually this raises something else I don't understand. What happens to type-classes? To even have a bind with a linear arrow, would it have to be an instance of a special `LinearMonad` class to begin with? If so, then IO is sort of a red herring here. Probably more to the point is the ubiquitous possibility of "error" not to mention async exceptions. 
Then why can't you have an event that fires 60 times a second that powers everything?
Oh, no, I don't think you're coming across as a jerk at all! I'm enjoying the discussion and I think it's valuable to have. RE : Java as an example - My undue focus on what I dislike about Java's marketing is getting us a bit off task. I was being a little hyperbolic to try to drive a point across, and got distracted by my frustrations with the industry. I am not trying to make the argument that Java is a bad choice for a project. What I am saying is that none of this is due to intrinsic value, AND, that the ecosystem, tooling, and paradigms that helped it achieve it's dominance are becoming less globally relevant. Note, less globally relevant - Not totally irrelevant. Java's typical API designs focus almost entirely on indirection instead of abstraction. In fact, it is frequently the case that a Java API will expose -more- surface area than it is attempting to wrap, as if adding complexity is somehow equivalent to adding features. This isn't unique to Java - But it is intrinsically tied to how Java and OO design patterns have butchered the discourse of software design for last 20 years. That's not necessarily their fault, it's that deliberately vaguely defined concepts became canonical references about how to do things instead of guidelines. This is the hole in the logic that Haskell can, and does fill. It can illustrate how to cover up the mess properly, safely, and without making 'gross' compromises - Or, at least, and in my mind more importantly, that the gross compromises you make today don't have to define your future. I think Haskell's killer application, and what can eventually drive it to a solid place at the table, is that it is fully capable of taking a horrifying mess and keeping it arms length while the rest of the project can continue to function, in a way that no other language can really match. It's because of this heavy and well executed focus on true abstraction (as opposed to indirection and isolation) that achieving compatibility does not have to be all about compromise, and ultimately, that's why I think that the academics and the commercial haskellers do not need to experience such friction - Because we don't -actually- need separate things out of the language, we just think we do because that's the way it's always gone before.
Regarding congestion control, when you are writing to the sink faster than you can redraw, miso will batch up all of the outstanding events and apply them in bulk onto the model (in the order they were written - left folded). A dirty check is then performed, a new frame requested and a diff initiated. The cycle cycles. This is just one technique for mitigating a high-load scenario, this occurs by default.
One thing that surprises me about this proposal is that it doesn't seem to have enough connectives. As we know from linear logic, there are two conjunction operations: * `A * B` the multiplicative conjunction it is introduced by pairing given by `Δ_1,Δ_2 ⊢ &lt;e_1, e_2&gt; : A_1 * A_2` whenever `Δ_i ⊢ e_i : A_i`. That is we split the available linear variables and pass them disjointly to the subexpressions. It is eliminated by a let-like construct: `Δ_1,Δ_2 ⊢ let (u1,u2) = e1 in e2 : C` whenever `Δ_1 ⊢ e1 : A1 * A2` and `Δ_2, u1: A1, u2 : A2 ⊢ e2 : C`. That is, we introduce two new linear variables that are bound to the components of the pair and you're obligated to use up both components. * The additive conjunction: `A1 &amp; A2`. It is used to lazily delay splitting resources. If `Δ ⊢ e_1 : A_1` and `Δ ⊢ e_2 : A_2` then `Δ ⊢ (e1, e2) : A1 &amp; A2`. It is eliminated by projections - once you decide whether you want the first component or the second you are obligated to use that one and discard the other: `Δ ⊢ fst e : A_1` when `Δ _ e : A_1 &amp; A_2` ; or `Δ ⊢ snd e : A_2` when `Δ ⊢ e : A_1 &amp; A_2`. Note that additive conjunction is necessarily lazy: the expressions `e_1` and `e_2` do not consume the resources in `Δ` until we've forced the decision of whether we want the first or the second component. It seems that the language in the proposal has problems with lazy pattern matches and mixed linearity. I would suggest instead of trying to hack out of the issue it is better to add the additive conjunction connective to the language (probably as a way of forming datatype constructors) and only allow lazy pattern matching to be applied to additive conjuncts.
Allocating resources usually happens in IO so async exceptions don't really make things worse. But yeah, most uses outside state macines I have seen involve some LinearMonad class + RebindableSyntax. 
I find it quite difficult in multiple ide's I use. 
 Yeah if you ignore the rest of that post. 
I did SICP in Haskell as well. It works. Here is what I think they mean. Imagine for a moment that you have no idea how to find a square root but do know how you would use it in the rest of the program. Just as they will do it in SICP you can create a dummy square root function that type checks (say square root of anything is 2.0) and then implement the rest of your program. You can use the above definition for quickcheck. Your test will of course fail, but you can everything else and just leave that one gaping hole. You still have the problem of how to fill the hole and that is imperative. And it is also epistemological. You can make a reasonable assertion you can in some sense assert you don't really know what a square root is until you know how to derive one. 
 What extra escape hatches would be useful?
This is terrific, thanks Joachim!
This sounds ok, I think it'd have to be concurrent with a change to cabal to generate a list of the extensions for the haddocks.
Please take time to read this some time: http://www.catb.org/esr/faqs/smart-questions.html Thanks
Is it possible (now or in future versions) to write a test to check for specialization?
I'm not sure I see the benefit of this over GHC plugins. Seems like anything that could be done with this could be done just as easily with plugins.
The distinction between "what is" and "how to" doesn't seem to me like a meaningful one in the context of a pure language (as opposed to when comparing languages vis a vis effects like mutation). Is SQL more declarative than haskell because we don't have a say in how `join` is implemented? What if I ported all of postgres to an in-memory haskell data structure library that exported a `join` function? Are my users now writing declarative code? It's libraries all the way down.
Oh, for a moment I thought this would enable libraries to act as plugins to GHC during compilation, similar to TH, but on the whole source file. Which could enable language extensions first developed and tested in user-land before being lifted into GHC. That does sound interesting to me. I guess to some extent that could already be possible with a custom pre-processor like `{-# OPTIONS_GHC -F -pgmF hspec-discover #-}`. Or do, as I suspect, custom preprocessors only have access to the source text, and GHC expects them to output valid hs source text too? Which could be quite limiting if, for example, we'd want to extend a GHC-internal ADT by an additional case. I've also heard about type-checker plugins. Maybe there are other plugin types too? For a simple example, would it be possible to implement the following language extension on a library level today, per chance? https://github.com/ghc-proposals/ghc-proposals/pull/90
I definitely would do HIE over the VS Code extension. HIE does look pretty cool, and it is certainly something that could get a lot more people into Haskell. Thanks!
I'm really having trouble understanding what concretely you think the community and/or the language needs to "compromise on" to achieve wider usage. Could you give an example? I write haskell full time professionally and have a good list of personal criticisms about the language and ecosystem, and applications for which I would not recommend haskell, but I can't figure out what you're getting at.
[Squeal](https://hackage.haskell.org/package/squeal-postgresql-0.1.1.4/docs/Squeal-PostgreSQL-Schema.html#t:PGType) supports both `UUID` and `JSONB`. I'd say it's relatively complete support for Postgres types, but not for array and composite types yet.
Couldn't this be solved via a change to cabal? In addition to listing the extensions required to -compile- a module, you could just have another section listing the extensions required to -make use of- the module. Then, you add a stage prior to executing the buildplan where cabal checks to see that the intersection of the 'required to make use of' matches up with the 'required to build'. It's a much simpler problem to solve than dependency management, since you don't really need to worry about versions or anything, it's just checking the intersection of two lists on a per cabal file basis. If the module you're depending on doesn't specify, it just doesn't bother checking and automatically passes, resulting in the current behavior. If you don't specify in your module's cabal file, it automatically fails with an error telling you to update the required extensions. 
LYAH is bad, maybe even harmful. happy learn looks better but I haven't gone through it myself.
The compromises that need to be made are known in advance. Its mainly a question of will. So my main answer is I don't know and couldn't know. The thing that has stopped me the most has been vertical integration. Since we are talking Machine Learning the sorts of things I'd want: * Cloud integrations which can run smoothly off standard cloud storage frameworks (or at least one fully vetted and tested) * Integration with at least one Hadoop distribution out of the box * A collection of industry specific data files available for the system (example synonym recognition and various word hierarchies) * A specific set of prebuilt statistics * At least one vendor willing to support all the above from a consulting standpoint * At least one vendor willing to support all the above from a managed service standpoint The core language community just has to be mildly supportive. I can't predict what the specific conflicts will be. I can give historical examples from other open source products that have had to get vertical integration right and what their conflicts were. 
Can you elaborate why do you think it about LYAH?
Probably ... what do you look for in `-ddump-simpl` to check for specialization?
No solution will work with only affecting Cabal, unless you give up the idea of being able to run `ghc` directly on files. (Maybe I am old-school and many people out there only ever work with `cabal` or `stack`…) Also there are language pragmas that affect how the `import` list is parsed (e.g. package-qualified imports, or the proposed `ExtraCommas` extension). This suggests a design where langauge extensions need to be resolved before the compiler looks at the import list. I am not sure what you mean by “update the required extension”.
Almost. You could define a TH function that you wrap around the code that uses this feature, similar to the recent post here on r/haskell about inserting tracing statements into monadic expressions.
GHC plugins cannot do any of the proposed features of library-defined language extensions (enabling other language extensions, enabling compiler flags or adding imports) – the features are complimentary! But GHC plugins becomes more easy to use with it, as the `inspection-testing` example in the proposal shows.
Yes, good point: `haddock` should be able to list the language extensions defined in a library and include the details of the definition there.
Wait, why is LYAH bad/harmful?
What does "vertical integration" mean to you? It sounds like you want a really good hadoop library. I don't see what that has to do with language compromises or community culture.
Wait they can’t? Those all sounds like things plugins ought to be able to do. Why not make the proposal about adding such functionality to plugins? Seems better to reuse an appropriate existing feature than to implement a whole new structure.
See http://bitemyapp.com/posts/2014-12-31-functional-education.html This mirrors what I've seen with many beginners, where they go through LYAH because it looks friendly and then hit a dead end getting stuck and frustrated when they sit down and try to write something new and realize they haven't actually learned anything.
See above reply https://www.reddit.com/r/haskell/comments/7bhhvx/question_learn_you_a_haskell_or_happy_learn/dpnimfx/
Two reasons: * adding that to plugins would be a “whole new structure”. Plugins currently are either Core-to-Core transformations or additional type inferences; this is far away from what we are trying to dhere. * plugins are code, while these are declarative specifications. An IDE can learn to understand the langauge extensions fields in the package data base and make sense of the code. Having an IDE require the execution of GHC plugins is, well, not going to happen,. So unless you need more power, declarative specifications should be preferred to executable implementations.
The IDE argument isn’t super convincing. It’d be nice if GHC plugins were fully featured enough to provide meaningful feedback to IDEs for all sorts of reasons. And I don’t think extending the plugin API is as big a change as adding a whole new file type for GHC to consider. It makes more sense to me to keep all the “GHC-modifying behavior” in one place if possible. It seems plugins would be a far more powerful way to accomplish this.
Vertical integration means considering yourself as part of a broader ecosystem not just a language. The compromises come when the interests of the ecosystem and the interests of the language in and of themselves conflict. This is more than a Library this is institutional support. Take for example C where I can talk about the compromises. Unix used C for the parts of the system that needed to run fast. C targetted running Unix code. As C started to replace Assembly as the systems programming language the definition of a good CPU was the ability to run compiled C code fast. CPUs were designed to run C compiled code. C compilers were written around CPUs. The C standards were designed to maximize the performance of the ecosystem. What really has to happen is the Haskell community would say we want to do this. The specifics emerge with time. 
“far more powerful” – you nail it! Far too powerful… There is a reason we have a `.cabal` file instead of a `Setup.hs` that just implements building and installing your libraries using code. Also, if you have not yet used or written GHC plugins, don’t be fooled by that name. GHC is by far not as extensible as the term “plugin” indicates, and as one might be used from plugins from, for example, Eclipse or emacs. I agree that GHC plugins could maybe be more powerful, but that is a completely different discussion.
And yet people do use `Setup.hs` for good things now and then (though I agree that `Setup.hs` is more problematic than it is useful). But the things outlined in the OP seem pretty tame for a plugin system. I wouldn't say extending plugins' capabilities is a different discussion when it seems like it could be a better solution to the same problems.
My choice of Haskell-to-JS languages is actually the inverse of what you suggested. Elm is for projects measured in weeks, PureScript months, and GHCjs, years. People seem to forget the GHC in GHCjs, and just how beneficial that is to the project. For all of the criticisms of it being immature, it seems to be the most forward-thinking in terms of expanding to current and future development platforms. E.g., reflex-dom has out-of-the-box support for Android and iOS, and can be run purely with GHC using jsaddle. And then there's the WebAssembly project, which I haven't seen in the roadmap of any other Haskell-to-JS languages. 
&gt; Reflex is supposed to allow you to precisely control changes you want to make to a page, (as opposed to a dom-diffing algorithm). This I believe is too much power, and too much bookkeeping. Bookkeeping is disempowering, so I don't see your point. Unless by power you mean to the power to optimize the DOM-updates? If so, that's one of the last things I'm concerned with when it comes to the power of a framework. &gt; So while Reflex is also more of a library than a framework, I found it less than satisfying (having built large apps with it). I wanted something midway in power between Elm and Reflex which led to Concur. Again, I'd like some clarification on what power you are taking away from Reflex.
Looks interesting, but I can’t quite figure out what this does, or where it fits in. Can anyone explain? What cool stuff can I build with this functionality?
You are absolutely right. Power in the sense of allowing to do more things, not in the sense of making it easy to do something. Reflex requires precise dom updates which is an overhead for the programmer (and actually disempowering in a sense). Also I am working on a backend for Concur which will allow precise dom updates (i.e. no virtual dom) but keeping more or less the same easy API. It works on diffing dom edits, which turns out to be much easier and more generic than diffing the entire dom. I released a part of that code as a separate library called diferencia (https://github.com/ajnsit/diferencia). 
I get the vague impression that Miso is Elm in Haskell but (sorry) with less documentation; what are the reasons someone might prefer Miso to Elm? I mean, the obvious one is "I'd really rather be writing in actual Haskell then a stripped-down web-based variant of it", but is there anything else?
What is the logic of this? I though that lineal types were an accepted proposal so it would be available in the second or third new version.... Are there more than one group working in lineal types for Haskell with competing proposals?
This could become really powerful with a richer notion of equality, e.g. Lean uses a fancy congruence closure algorithm to make more things "obviously" equal to the type checker. Given that this has to unify the overlapping branches of case analysis, having that congruence closure there to help that unification along would allow more interesting overlaps that are subtly different.
That's not what they meant. They were talking about operational semantics for Haskell.
Nope, this is the same people; but they wrote a paper about linear types first before tackling the hairy "OK, time to actually implement it in GHC." The same thing happened with Backpack.
Wow, I can't believe how rich in details is this post! Thanks for that, for real.
&gt; "Safe file handles" are an old-hat trick Oleg did years ago, and I'm not immediately convinced this is substantially better and worth the complexity (the lack of broad usage of Rank2 Handle tricks may say something about this.) This is funny, because way back when I interned at Galois I did all sorts of rank-2 tricks to ensure that I was using my C FII objects safely. I subsequently heard the first thing that happened to my code after I left, was they undid these type tricks for something simpler :&gt; I think linear types may be more an argument for usability, than for performance, in the sense that, yes, the encoding tricks are possible today, but they are too cumbersome for most people to consider using easily. And I think there truly is a phase change when you simplify things to the point where it's easy to play around with things; I often feel that this is what happened with type classes--they didn't add anything you couldn't do before (translation to dictionaries!) but they sure as hell feel really different. But this is hard to know before you actually have the implementation in your hands...
That's probably the most stupid question a beginner may ask about Haskell, and I did read some answers for this but didn't understand: Why IO doesn't affect function purity? I'd be really thankful if someone could ELI5.
You mean IO like f :: a -&gt; b -&gt; IO c?
This is a great, great question. There are a few ways that we can conceptualize this. The first comes from [Brent Yorgey](https://www.cis.upenn.edu/~cis194/spring13/lectures/08-IO.html). Essentially, his way of explaining IO is that a value of type **IO a** is a *description / recipe for* a side-effect. You can think of a *description* of plain data. Data is inert - pure, doesn't have side-effects, etc. It is only when the **Haskell runtime** interprets this data that side-effects happen. So your Haskell program remains pure, but when the Haskell run-time interprets a value of type **IO a**, it performs those side-effects. Another conceptual model comes from Simon Peyton Jones, and is used in Graham Hutton's book *Programming in Haskell*. The paper is called [Tackling the Awkward Squad](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/mark.pdf) and is fairly accessible to red-blooded programmers such as myself. Anyway, the way they explain **IO** and purity is that we can mentally replace **IO a** with **World -&gt; (a, World)**. What does that mean exactly? Well, it means that we can interpret values of type **IO** as functions that take in the entire world and spit out a tuple with some value (I think we use "a" because it represents some answer, but not 100% sure), and a modified value of type **World**. The function does not have any side-effects, because it doesn't change any global state. It merely returns an answer and a modified value. My favorite way to think about it is Brent Yorgey's explanation. There are many things that become clearer when we think of **IO** in this way, but that wasn't your question. ;-) Let me know if this helps.
It actually does. Everything seems really clearer now, thank you a lot. I honestly have been struggling with Haskell since I came from the usual imperative programming scenario, so functional programming has been hard. But I think I'm getting it better lately. Also, sorry if I made any mistakes, English is not my first language. 
I'm stuck on sequences of IO, I have two functions that are used to randomly generate an amount of enemies (based on game frames) and position the enemy randomly on the screen. It looks like this: generateEnemies :: Int -&gt; IO [Enemy] generateEnemies n | n == 20 = makeEnemies -- TODO based on levels | otherwise = return [] where makeEnemies = do amount &lt;- randomNumber 3 -- fixed val for now sequence $ replicate amount makeEnemy makeEnemy :: IO Enemy makeEnemy = do rn &lt;- randomNumber (length enemieTypes) x &lt;- fromIntegral &lt;$&gt; randomNumber (fst windowSize) y &lt;- fromIntegral &lt;$&gt; randomNumber (snd windowSize) let enemyType = enemieTypes !! (rn - 1) return $ Enemy enemyType (x, y) (makeEnemyHitBox ((0,0), (0,0))) The problem I am having is that it looks like that the enemies position keeps generating at random, I am expecting to have an IO [Enemy] with enemies, so far I have looked into using `seq` to get it to evaluate strict but no luck so far. What am I doing wrong? 
&gt; I can't imagine how SQLAlchemy does it, honestly. It doesn't.
Hasql is my personal favorite. It's very performant and supports both UUID and JSONB: https://hackage.haskell.org/package/hasql-1.1/docs/Hasql-Encoders.html
I don't entirely understand what the problem is. Should the enemy positions *not* be random? When I run the code (with dummy implementations for the functions and values you haven't provided), it generates 1-3 enemies at random positions. Is this not what you want? *Main&gt; generateEnemies 20 [Enemy "a" (29,593),Enemy "a" (450,310)] *Main&gt; generateEnemies 20 [Enemy "b" (417,483)] *Main&gt; generateEnemies 20 [Enemy "a" (482,103),Enemy "b" (638,184),Enemy "b" (791,327)] (I removed the hitbox part, because it didn't seem to matter for your question)
yes they are supposed to be at random positions, but what happens, and what is my problem is that the positions get randomised each state update 
Ah. Can we see your loop, then? 
of course, it looks like this: runner :: Time -&gt; GameState -&gt; IO GameState runner t gameState = return gameState { frames = frames' , enemies = enemies' } where frames' = (frames gameState) + 1 enemies' = do current &lt;- enemies gameState new &lt;- generateEnemies (frames gameState) return (current ++ new) I removed all the other stuff, 
I wish I could give you some examples but I don't have in that are open source, sorry 😔
Well, what you can do with `servant-client` today, which is the derivation of "client functions" for querying a webapp with a given API type, you'll be able to do in programs built by ghcjs. It's currently not possible, because servant-client uses http-client which transitively ends up using OS network libraries and what not, which are not available from the browser. But Julian abstracted out the http client specific bits so that you can derive your functions regardless of the "runner" you'll use. Once `servant-client-ghcjs` is released, you'll be able to just directly use those functions from ghcjs applications, by using the browser as your http client "backend" instead of the `http-client` library.
&gt; **The distinction between "what is" and "how to" doesn't seem to me like a meaningful one in the context of a pure language** (as opposed to when comparing languages vis a vis effects like mutation). Is SQL more declarative than haskell because we don't have a say in how join is implemented? What if I ported all of postgres to an in-memory haskell data structure library that exported a join function? Are my users now writing declarative code? It's libraries all the way down. But it is! Otherwise seq wouldn't exist. Strictness annotations are strictly about the "how to" and yet are a integral part of real haskell code.
Well, of course it generates new enemies every frame - you're calling `generateEnemies` every time! Instead, make an update function: updateEnemy :: Enemy -&gt; Enemy -- or IO Enemy if you wanted to, say, make it move in a random direction updateEnemy (Enemy enemyType (x, y)) = Enemy enemyType (x',y') where x' = ... y' = ... -- in your runner function: where ... enemies' = map updateEnemy enemies -- if updateEnemy returns IO Enemy, you'll have to use sequence here
Ohh, wait, I see it now! Ignore my other reply. Okay, so if I'm right, then your `GameState` type looks something like this, yeah? data GameState = GameState { frames :: Int, enemies :: IO [Enemy] } This means that every time you ask for the list of enemies, by doing: do current &lt;- enemies gameState ... You're asking it to run the IO action all over again, generating a new list of enemies. Instead, try to make your `GameState` type: data GameState = GameState { frames :: Int, enemies :: [Enemy] } And then adjust the rest of your code so the types fit.
makes total sense, but is it possible to have a pure type of enemies when I spawn them based on a random number? there is no way back from IO right?
That's right, *but* that's not as much of a problem as you think it is. Behold: initState :: IO GameState initState = do enemies &lt;- generateEnemies 20 return $ GameState 0 enemies runner :: GameState -&gt; IO GameState runner gameState = do print (enemies gameState) newEnemies &lt;- generateEnemies 20 return $ gameState { frames = frames gameState + 1 , enemies = enemies gameState ++ newEnemies } main :: IO () main = do gs &lt;- initState loop gs 10 where loop :: GameState -&gt; Int -&gt; IO () loop gs 0 = return () loop gs n = do gs' &lt;- runner gs loop gs' (n - 1) The `main` function just goes for 10 frames.
A few years ago, I wrote an [interface](https://github.com/tdox/hcholmod) to cholmod. I haven't touched it since so it may not even compile now.
I'm not convinced another layer of indirection here will help. Right now it's annoying to learn what ~100 GHC extensions do, but at least that's all there is. After this goes through there will be an infinite number of extensions. That doesn't seem like progress. What it gains: + Less keystrokes What it loses: + The ability to get an explicit list of what extensions are turned on by checking the top of the current file and the .cabal file alone.
Lunduke, Linux, and Haskell; three of my favorite things.
I understand now, thanks. Wouldn’t it also be useful to be able to derive generic JavaScript functions from a Servant API definition? I.e. functions that can be used by all JS code running in the browser, as opposed to just by GHCJS? Thus allowing Haskell backend developers to just hand over the JS code, needed to query the backend, to the frontend devs, and have them plug that into any framework they may be using.
+1 for how awesome Servant is! In fact, it was what finally helped me learn Haskell after attempting and failing twice prior to that. I ported my project ZoomHub from Node.js to Haskell using Servant and it was a very rewarding experience. Kudos to the team behind it!
&gt; I'd really rather be writing in actual Haskell then a stripped-down web-based variant of it This is a large part of it, yes. Type-sharing as well, which is more of a by-product of GHCJS. The universal part is something miso has that Elm does not (yet). 
It would be very useful, so much that it's something we implemented in the early days of servant =) See [`servant-js`](https://hackage.haskell.org/package/servant-js-0.9.3.1/docs/Servant-JS.html) on hackage and [the relevant section](https://haskell-servant.readthedocs.io/en/stable/tutorial/Javascript.html) from the servant tutorial. Note that packages exist for generating code in a few foreign languages, like JS, Ruby, Python, Elm, Purescript. But since quite a few people use ghcjs for writing frontend Haskell applications, it's also nice to be able to derive haskell functions that you can from a browser.
Vehemently disagree. It's only _within_ functional programming and type theory circles these phrases are used, which means those not involved and/or educated in those fields already, confused. That is _the_ characteristic definition of jargon. Things like "TCP/IP" or "LPT1" are "IT Jargon" for example. But to anyone _in_ IT it's not confusing at all.
&gt; type classes--they didn't add anything you couldn't do before (translation to dictionaries!) I've heard this often but I fail to see how you'd implement something like the `Eq` for polymorphic types. What is the equivalent of things like `instance (Eq a, Eq b) =&gt; Eq (a,b) where ...` without typeclasses which make things like `(True,("False",1)) == (True,("False",1))` work magically? 
https://github.com/cartazio/system-lf/blob/master/notes%2Bexpositions%2Bpresentations/2017_HIW_GHC_Core_Linear_Logic_Friends/latex/2017hiwslides.tex#L333-L346 as presented at HIW 2017 the code more formatted is https://gist.github.com/cartazio/a42054587f1f8bc0c5720d48ee166049 basically unless you cripple pure exceptions / break them, we can always drop stuff :) 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [cartazio/system-lf/.../**2017hiwslides.tex#L333-L346** (master → 93951fd)](https://github.com/cartazio/system-lf/blob/93951fd382a486b882e274a8ece3799754535c1b/notes%2Bexpositions%2Bpresentations/2017_HIW_GHC_Core_Linear_Logic_Friends/latex/2017hiwslides.tex#L333-L346) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dpoh2tp.)^.
This is is great! I’ve wanted something like this for a long time. 
Nobody would be 'giving up' on using GHC directly. You'd just be putting the required extensions at the level of the package, the primary unit of distribution in the ecosystem, instead of on the module. As this suggestion is largely about encouraging interoperability of code between projects, I thought this would make sense. I'm not saying you couldn't solve it with GHC, I'm saying it seems a more straightforward and appropriate match at the level of cabal, because your usecase sounded like a package interoperability concern. My apologies if I've got you all wrong here, but I just wasn't picking up on a need for this extension within a single project.
The important line there is the "Missing C libraries". You need to install packages like `libpng-dev`. 
yeah, you either need abort the entire program style exceptions (like rust) or some way to guarantee totality, or some sort of explicit type and effect system that tracks pure exceptions and friends 
Yeah, at one point LYAH starts dumping lists of library functions which is when it starts to go awry but this is before you managed to do anything useful with Haskell. Pity, a nice and colorful book you could give to kids (too) would be handy.
"Happy learn haskell" does look better than LYAH. Haskellbook is probably the best learning resource but it's not cute and welcoming in the same way.
What would need to happen in your opinion to remove this obstacles ? Do you think Haskell can become good choice for machine learning in foreseeable future ?
I'm reading about Parallel Haskell and I'm not able to understand what will happen when 2 cores try to evaluate the same thunk (computation) using the Eval monad in parallel. Now, I see that if the thunk has already been evaluated, the par will be ignored. But what if two cores try to evaluate it at the same time? Consider the following contrived example running on 2 cores: runEval $ do as' &lt;- rpar x as'' &lt;- rpar x Here x is some unevaluated thunk. When the two cores will try to evaluate the same data in memory, won't there be issues? Or does one of the cores *lock* the memory it's trying to update (i'm assuming the memory is getting updated as the evaluation goes on by expanding the thunk). Please correct me if there is something wrong with my understanding of evaluation in Haskell. Thank you.
Do you guys have, like, a manifesto for HIE somewhere?
&gt; I don't want Haskell to become this enormous blob of language features, where any time I look at the code of one of the libraries I'm using it enables 40 language extensions. I see your point, but just not wanting something does not mean it will not happen. Nobody wants half a dozen of streaming libraries, yet it happens. I think the same will happen here. &gt; Instead we should push for a new Haskell language release that enables more by default. Another wish that you share with most of the community (including me), but it is questionable if it will happen, and whether it includes the language extensions that you would want it to.
Two threads _can_ end up evaluating the same thunk, with regular threads at least, so with `rpar` too I assume. Since Haskell is pure, they will both compute the same value, so it doesn't matter. The only case in which it could make a difference is if `unsafePerformIO` is used, because the side-effects would be performed twice instead of once. (disclaimer: please do *not* use `unsafePerformIO`. If you think you need `unsafePerformIO`, you are probably mistaken. It is much easier to rearchitecture your program so that you don't need `unsafePerformIO` than to deal with the strange bugs which using `unsafePerformIO` is likely to incur. You can easily end up in a situation in which your side-effects occur twice even if no threads are involved.) Even then, the documentation for [`unsafeDupablePerformIO`](https://hackage.haskell.org/package/base-4.10.0.0/docs/System-IO-Unsafe.html#v:unsafeDupablePerformIO) explains that unlike pure thunks, `unsafePerformIO` _does_ lock some mutex in order to ensure that two threads don't end up evaluating the `unsafePerformIO` thunk twice. `unsafeDupablePerformIO` offers no such guarantee. So unless you use `unsafeDupablePerformIO` (which you really, really shouldn't), the fact that two threads can evaluate the same thunk doesn't have any noticeable consequence.
Universal?
It would if you had a "function" like `IO a -&gt; a`. That wouldn't be a function in the mathematical sense because it could associate one input with several different outputs (consider the input `readLine`). So that would violate the fundamental essence of what a function is! Haskell's basic model of computation is that functions are (partial) functions. Except... well, we do in fact have `unsafePerformIO` with exactly this type! So functions aren't functions and Haskell isn't pure unless you ignore `unsafePerformIO`. But this unsafe non-functional operator pretending to be a function doesn't change the fact that Haskell evaluation assumes that functions are functions. That's why you have to really "know what you're doing" to use `unsafePerformIO` because otherwise it will surprise you in ways that seem really strange. That's basically because of laziness. If Haskell were strictly evaluated then `unsafePerformIO` would be more tempting—you could make an alternative prelude that ignores purity and has `readLine :: String`. But laziness keeps us honest with purity even in the presence of `unsafePerformIO`. Actually you still need some operational understanding of laziness to use Haskell's regular lazy I/O. You can find many articles about this if you search for lazy I/O. The basic problem is that you can end up with a lazy value that depends on an I/O operation that isn't possible anymore at the time of actual evaluation. f &lt;- openFile "foo.txt" x &lt;- readFile f closeFile f return x The lazy value `x` will probably not work correctly if `readFile` is a lazy I/O operation rather than strict I/O, because it won't be actually read until after it is closed. That's a kind of bug in Haskell's old I/O stuff; it doesn't play as nicely with laziness as it should, and this is something most people figure out the hard way. So that's a rant about laziness, purity, and I/O that hopefully isn't too confusing...
IIRC https://github.com/gibiansky/IHaskell was bumped to 8.2.1 recently. Short of asking jupyter.org to update their image, I'd just `docker build` and `docker run` this on my local machine.
Not really. The intention is to provide a common context so that tool writers can focus on what their tool does, rather than worrying about all the issues around getting it to work with a given project being developed, or integrated into an IDE. And we have been working to make that happen, adapting as we go along.
In this thread I think you're describing a niche which is much more likely to be filled by F# than Haskell, and I don't think that's a bad thing! 
It's not linear according to Yorgey's definition either. The variable `x` does not occur at all, `f` occurs twice, and `t` is free.
https://medium.com/airbnb-engineering/isomorphic-javascript-the-future-of-web-apps-10882b7a2ebc, used to be called "isomorphic".
I help maintain IHaskell. If you run a Linux distro and have `nix` installed, you can clone the repo and do $ cd IHaskell $ nix-build release.nix $ result/bin/ihaskell-notebook to get a fully configured IHaskell notebook environment. You can extend it with additional packages as well, see [here](https://github.com/vaibhavsagar/notebooks/blob/1aa8fc3c0a8fe5c448d4218aa831dadcd92c8c3f/git-from-scratch/default.nix#L24-L46) for an example. 
The project was updated to support GHC 8.2.1, but the Dockerfile hasn't been updated in quite a while :(.
You can install the `libpng` from your system package manager. However I highly encourage you to have a look at [nix](https://nixos.org/nix/) integration with [cabal](https://github.com/haskell/cabal/blob/master/Cabal/doc/nix-integration.rst) / [stack](https://docs.haskellstack.org/en/stable/nix_integration/) / [whatever](https://nixos.org/nixpkgs/manual/#how-to-install-haskell-packages). Nix will handle for you the install of your dependencies, including external / system libraries, so you will never have to ask this question again.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell/cabal/.../**nix-integration.rst** (master → e3aa190)](https://github.com/haskell/cabal/blob/e3aa19026d847d6ed812e4c6b6bc7443c27b2665/Cabal/doc/nix-integration.rst) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
About the `metadata` coupling, I don't think the problem is bigger than the approach of always propagating all data. In your example, you coupled system D to A in a implicit way, which I think might be even more confusing. The rules of propagation for the `metadata` would be the same as used in Clojure maps (always propagate) but it is localized to a single field, not the whole input. Imagine que for whatever reason, you need to migrate service B from Clojure to language X. With the propagate all approach you are forced to propagate everything in the new system because you don't know what downstream system might be using. 
I would agree if Microsoft actually cared about F# I think it is a much more viable candidate to become a mainstream language. F# while it interacts with other OCaml and ML style languages is perfectly happy tying itself to .NET. And this was my point way back when I said that the Haskell community doesn't want to be a mainstream language. The F# community would be perfectly happy writing Azure right into the language. The F# community has written LINQ into the language. The Haskell community is much happier dialoguing with the people who write F# than the people who write in F#. 
OK, looks good, I'm writing a Dockerfile: ``` FROM nixos/nix RUN nix-env -i git RUN cd ~ &amp;&amp; git clone https://github.com/gibiansky/IHaskell.git RUN cd ~/IHaskell &amp;&amp; nix-build release.nix RUN adduser -D ihaskell RUN chmod ugo+x ~/ RUN chmod ugo+rx -R ~/IHaskell ``` running: ``` su ihaskell -c /root/IHaskell/result/bin/ihaskell-notebook ``` is promising, but I need to get a few things running yet...
While we're on the subject, I wonder if someone can help me out understanding the design of some OO machine learning APIs. For example, here is the API for the nearest neighbour classifier in scikit-learn &gt;&gt;&gt; X = [[0], [1], [2], [3]] &gt;&gt;&gt; y = [0, 0, 1, 1] &gt;&gt;&gt; from sklearn.neighbors import KNeighborsClassifier &gt;&gt;&gt; neigh = KNeighborsClassifier(n_neighbors=3) &gt;&gt;&gt; neigh.fit(X, y) KNeighborsClassifier(...) &gt;&gt;&gt; print(neigh.predict([[1.1]])) [0] &gt;&gt;&gt; print(neigh.predict_proba([[0.9]])) [[ 0.66666667 0.33333333]] http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html Does this strike anyone else as nuts? The "constructor" `KNeighborsClassifier` doesn't actually create a classifier. It creates a value holding the *hyperparameters* of a classifier. You then create a classifier by calling the method `fit`. But why is this a method? Why on earth would you want to *mutate* your classifier? Each call to `fit` should contain a new classifier trained on the input data. It seems to be this is a great example of mutable-design-gone-wrong. I would say it's also an example of OO-design-gone-wrong but there's not really too much OO about it. Does anyone else have any thoughts?
&gt; I think Haskell's killer application, and what can eventually drive it to a solid place at the table, is that it is fully capable of taking a horrifying mess and keeping it arms length while the rest of the project can continue to function, in a way that no other language can really match. I would agree that pure and lazy has the potential to be a killer feature for the language. Being pure and lazy has forced the Haskell community to work on solving how to make pure and lazy really viable. If we got to the point that pure and lazy was low cost or in some domain the advantages of pure and lazy were so large then it advances to at least being dominant in a domain and then from there mainstream language acceptance becomes more doable. I certainly think this thread is a perfect example. Data Science / Machine Learning is a good choice since the users are already mathematically inclined. The math is often simpler for lazy algorithms than eager algorithms. We agree this is a great domain for Haskell. The question then is why hasn't the Haskell community created a vertically integrated stack. Why isn't there a Haskell Anaconda? I think it is because the Haskell community doesn't really want one. Sure the door is still open. But if the Haskell community wanted to win the data science war they would be fixing these deficits urgently before that door closes for a generation. Big data was an example. What could be better for Haskell than an entire ecosystem where out of order execution was the norm. Big data parallelism is as easy a vertical to own as it gets. Pugs was a similar situation 10 years ago. Then (and now) there is a need for a really good compiler for scripting languages. Everything was in place for Haskell to own that domain. Python and Ruby were having terrible almost unsolvable speed problems. Handling the dependency rules in browsers for Javascript, CSS, HTML then and now is pure torture. This is the sort of thing that Haskell could have excelled at. The door was wide open. And the Haskell community was indifferent to this opportunity. Contrast this with say type theory where the Haskell community has has an attitude of advancing type theory in practice regardless of cost and regardless of complexity. This is true today, where we see extensions like Linear Types and true 25 years ago when things like the Num type were being invented. Type theory is where you see sustained focused effort. It is not that I disagree with you that in theory Haskell is better than the languages that won. My problem is I don't believe Haskell wants to win. Type theory is a computer science not a computer programming topic. ___ As for your criticisms of Java libraries doing indirection not abstraction, I think that's extremely well put! The Java community has managed to replicate the complexity of the Win32 C++ libraries that it was rebelling against. I'm going to say something unpopular here. But IMHO multiple inheritance is really where you get the advantages of inheritance. Haskell type classes manage to get you polymorphic methods that turn out to be much safer and more manageable. 
How well does Nix work on Windows, which OP appears to be using?
I've updated the [Dockerfile](https://github.com/gibiansky/IHaskell/blob/9fa62b04b1e4cf5d1c3e16c07b60494516fe80f9/Dockerfile). I'm not so familiar with Docker and I was unable to get container networking set up properly but please try this and let me know if it works.
Cool, I'm giving it a whirl.
Indirection doesn't always have to be a burden. Sometimes it can make things simpler to understand. With good documentation and proper design, giving a name to something with a clear meaning is helpful.
Actually no, `seq` and strictness annotations are not any more about "how to" than anything else in Haskell. In the [report](https://www.haskell.org/onlinereport/haskell2010/), the [definition](https://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1260006.2) of `seq` only mentions "bottoms" in the sense of denotational semantics, the "what is". There is no mention of operational semantics, the "how to". Of course, we are all aware of the actual operational implementation of those things in our favorite Haskell compiler, and we commonly exploit that when we need to optimize the performance of a program. But it's not part of the meaning of the program in Haskell. So I agree with /u/jberryman that this philosophical quote from SICP is decidedly skewed towards an imperative view of programming.
On windows 10 with the linux subsystem, most (if all) of the command line interface works. GUI packages needs an X server, but works, no OpenGL. I don't know about the support in any other "environment" (such as cygwin, or anything else).
Thanks
Why would async exceptions be a problem with `UIO`?
Because I think UIO just rethrows async exceptions in base like `1 / 0`. So those would still require bracket I think?
Does `singletons` count as "the wild"? :) https://github.com/goldfirere/singletons/blob/b7d8a221136b584e8d42879449a82d73822a5fb8/src/Data/Singletons/TypeRepStar.hs#L47-L48
Oh, so UIO without those rethrows. Or bracket. Am I misunderstanding something? It seems to me that for bottoms in pure code we only need to enforce linear lambdas at the type level; we don't need to enforce the operational guarantee. Whereas for IO, we can't expect to be able to just change types of existing functions to linear types. A linear IO operation must enforce linearity even in the face of exceptions, and it must be possible to verify that statically. So in general, IO code will need to be rewritten if we want to use linearity. Is that correct?
&gt; So I agree with /u/jberryman that this philosophical quote from SICP is decidedly skewed towards an imperative view of programming. So do I. I actually think the statement is neither useful nor all that true. (There have been plenty of algorithms in Math before computer since was a thing.) But neither is the claim that we don't need to care about the "how to" in Haskell. After all it's an essential tool for achieving good performance as you already alluded to.
GHC plugins are basically just core to core transformations. You can't really do much. 
I think this is probably doing unnecessary work and allocations. Certainly splitAt is probably allocating ByteStrings repeatedly, seeded by the continually used repeat and intersperse. Other implementations in this thread allocate a big buffer once and keep writing the same one to stdout. I suppose Strict.putStr also uses Handles to write which is slower than hitting the fd directly too.
Cool! I was kind of forced to write a similar tool a while ago because we have a system that depends on `jshon`, a stack-based JSON manipulation tool which sadly turns out to be an unmaintained C program with at least one buffer overflow (I submitted a pull request in March). So I spent an afternoon replicating `jshon` in Haskell—well, the subset of it that we actually use. It's around 200 lines of code, leveraging Aeson. https://github.com/mbrock/jays
Yea my comment was about making them able to do more. That seems like a really limited range of capabilities.
Very nice. And as advertised I learnt it within 10 mins. 
[Simula](https://github.com/SimulaVR/Simula) is a Haskell-based Linux 3D compositor that is actively looking for new contributors. The project helps the Haskell community by proving that it can be a capable graphics programming language. If you're looking to get your feet wet in VR + Haskell, we're willing to sponsor a free HTC Vive if it helps you get your feet wet. Look on our issues page for items tagged "new contributor" (right now there's only one such item, but we're planning on adding more). This is cutting edge stuff, so don't join unless you want to learn a lot :-)
jq
Sorry, typo. Meant `λx.(λf.g f f)(λy.x)`, which is linear (in `x`) by the brief definition you gave, but clearly not actually linear.
[jl](https://github.com/chrisdone/jl#readme)
It's awesome :) !
Certainly, I knew this wouldn't be as hot as the alternatives, but it is the closest to what I would call "actual" Haskell code, rather than writing C in Haskell. There are probably ways to stay declarative but have good performance.
You may want to look at [Charity](http://pll.cpsc.ucalgary.ca/charity1/www/home.html). It's a programming language base upon the idea of programming with generated folds and unfolds over data and codata respectively. As such, proving termination is generally trivial. All that said, _if Haskell has a proper `Nat` type_ then proving the recursive version terminations is not hard either: {-# LANGUAGE NPlusKPatterns #-} fac : Nat -&gt; Nat fac 0 = 1 fac (n + 1) = (n + 1) * fac n In the above definition, each recursive call is on a smaller argument and we match on all inputs. As such, this function is guaranteed to terminate. A basic termination checker (as present in something like Idris or Coq) can verify this automatically.
Dependently typed languages like Coq, Agda and Idris are already "statically propertied". There aren't really any conceptual levels beyond values and types. In a dependently typed language, properties (normally known as *propositions*) can be written using types, and values are proofs of those propositions. Consider the property `forall l. reverse (reverse l) = l`. In a non-dependently typed language, you have test this at runtime by generating many random lists and checking that the property holds. In a dependently typed language, this property is a *type*: `reverse_involutive : forall (a : Type) (l : List a), reverse (reverse l) = l`. Because it is a type, we can statically verify that the proposition holds. We do this by writing a program that has the type of the thing we are trying to prove. If the program typechecks, then the proposition has been proved.
While there are several other tools much like it, the extreme terseness and simplicity of jsn is attractive. And there's really no need to add features since the other projects can handle more complexity if needed.
I like hindent except for the fact that it doesn't line up equals signs and pattern matching arrows. Ideally, I hindent would format imports the way stylish-haskell does, or I could set stylish haskell to run after hindent in some way? If anyone has figured this out let me know, I'm using vscode.
We're certainly not in a fun position. I feel the draw of the quick fix when root problem solutions seem like they're years away, if ever. I just felt like someone should represent the "fix the root problem" side as well. (Especially since this is a change that we'll probably never be able to roll back for backwards-compatibility reasons.) Is the motivation of the proposal the tedium of writing the extensions? In that case I think better editor tooling would help a lot, and is almost always superior to complicating the language. If the problem is tedium of reading than I agree editor tooling isn't the right solution, but I question the premise since I find indirection usually hurts readability for me more than it helps. This is a very personal opinion though, and so might be a minority one.
&gt; Is the motivation of the proposal the tedium of writing the extensions? Yes, but not only. It is a matter of abstraction and packaging. To use `inspection-testing `properly, you have to * Enable a languate extension (`TemplateHaskell`) * Enable a plugin (pass `-fplugin=Test.Inspection.Plugin` to GHC) * Import a module to get `inspect` into scope. The effect is that you can write inspect $ 'foo === 'bar in the code. But I don’t want my users to think about `TemplateHaskell`, and about the plugin. I want to them to think about this as if it was, well, a language extensions. Packaging it up as such will make the user experience much smoother. Also, as any abstraction, it hides implementation details. Maybe the plugin name changes? Maybe the code spliced by `inspect` requires some additional language extension? These things can now change “under the hood”.
btw, here is one stack.yaml using lts-8.11 with ghcjs-0.2.1.9008011_ghc-8.0.2. Invaluable if you need ie aeson &gt;=1.0.0.0: https://github.com/reflex-frp/reflex-dom/issues/162
I would't bother with the Docker wrapper unless you are stuck on OS X, Nix already provides clean separation.
Not bad. I like it! 
He said *can*.
Docker is a great solution if you don't have a nice way to get Nix working on your distro (or windows) as well. Arch Linux still has a bunch of issues with Nix iirc
&gt;I'm impressed with coq and how it has "first-class" properties, is there something meaningful beyond them? Types already correspond to theorems, so I am not sure what the advantage of such an approach would be (aside ergonomics). &gt;This makes me think of "Statically Propertied" programming languages, and, whatever might lie beyond. Idris, Agda, ATS, and Sixten all have dependent types. They each use them for very different things however. You may be interested in some of them :) * Idris uses dependent types primarily for more safety, but it still tries to be as much like a normal programming language as possible. * Agda is a theorem prover. * ATS is basically an ML where you can prove things about memory safety, enabling code that's faster and safer than C/C++ * Sixten's types are unboxed by default, so dependent types exist to prove that constructors actually terminate.
This is phycs, a game engine I have been working on. It's not exactly production-ready as there are many missing features and loose ends. The reason I am posting it now is that together with apecs I think it's sufficiently advanced to be an interesting proof of concept of a performant game engine for games entirely written in Haskell: * Physics is C-speed, but the interface is all Haskell * Simple test rendering for prototyping in 1 line of code * apecs makes game logic easy to write * phycs is completely defined in terms of apecs Stores, which shows that apecs can adequately be extended with non-trivial features I fully admit that this might not be the most idiomatic solution. Things like FRP allow for elegant, functional games, while a monadic von Neumann style feels somewhat antithetical to Haskell. I wrote it in Haskell because I wanted a fast (mutable) ECS that I could trust to keep 60 FPS, and Rust's type system turned out to be too restrictive. apecs will happily handle thousands of objects, which more idiomatic solutions would probably have trouble with. That said, if anybody knows of a way to make this more Haskell-y, I'd be very interested. For example, the similarity between Stores and lenses has bugged me, but I never managed to work something out. There are many things missing (patterns/instances), as I'm still heavily experimenting, but that's precisely why I thought it might be a good idea to get some second opinions. You can definitely play around with the examples, let me know if you run into anything. ^(Also, if anyone knows `inline-c`, would you mind taking a look at the issue?) 
Have you commented on the proposal itself?
Haskell was a great choice. No one knows how to use it. Lowers the attack surface for hacks.
I think it's worth adding that of course you can also abstract out the property of, in your example, being involutive, to illustrate the "no need for a third level" statement: Involutive : forall (a : Type) (f : a -&gt; a) -&gt; Type Involutive a f = forall (x : a) -&gt; f (f x) = x allowing you to rewrite your type signature as reverse-involutive : forall (a : Type) -&gt; Involutive (List a) reverse
Can't comment on the code, but absolutely brilliant README. 
Sounds like German "fick's" which means "fuck it" 😂 
It would not; the "magic" of type classes is that it automatically assembles the dictionary for you, and the restrictions on type class instances ensures that this construction can be done unambiguously (at least until you turn on a bunch of language extensions). Roughly, it'd be something like: type Eq a = a -&gt; a -&gt; Bool tupleEq :: Eq a -&gt; Eq b -&gt; Eq (a,b) tupleEq f g (x1,x2) (y1,y2) = f x1 y1 &amp;&amp; g x2 y2 And then in your comparison you'd have to use `tupleEq` manually to construct a properly typed `Eq`.
I really think it boils down to a maniacal focus on user experience, even (and specifically) at the expense of language features. Elm has awesome guides and tutorials for developers with no experience with "functional programming". Also, it's decision to avoid anything like typclasses has a cost of duplicated code, but with the ability to now proved more helpful and clear compiler errors to new users. The same level of errors would be much harder, if not impossible, with Purescript give the higher level abstractions that are not only allowed, but commonplace amongst the standard libraries. I do also think Elm has put more time into better websites and marketing to some extend, though the Purescript community is doing great things there as well. Succinctly, I think Purescript has historically "been ok with" targeting folks who already understand Haskell's concepts and functional programming in general, with a philosophy towards philosophical purity/rigor, and maximal code re-use + abstraction possibilities, while Elm is always willing to trade those things for (in their opinion) a better user (read: developer) experience. Disclaimer: I'm much more a fan of Purescript personally, and have used Purescript much more than elm, so I'm biased towards Purescript.
Idea: Make a [moon-lander](http://moonlander.seb.ly/) game as a example.
How is it "entirely written in Haskell" if it's binding to a C physics library?
Isn't the illusion of writing programs entirely in Haskell unattainable when GHC's language runtime is written in C?
You can always promote purely first-order data types like this: ``` f :: () -&gt;. Unrestricted () f () = Unrestricted () g :: Bool -&gt;. Unrestricted Bool g True = Unrestricted True g False = Unrestricted False h :: (a -&gt;. Unrestricted a) -&gt; [a] -&gt;. Unrestricted [a] h f [] = Unrestricted [] h f (a:l) = case (f a, h f l) of { Unrestricted a', Unrestricted l') -&gt; Unrestricted (a:l') ``` But mostly, only purely enumerated types such as `()` and `Bool` don't care because this promotion involves deep copy. But even `Bool`, I believe that you have to make this choice carefully. For `()` there is really little point in returning an `Unrestricted ()` because you will not "reuse" the value.
I don't think that 0 will be used internally at first (or at all: there is a discussion whether there ought to be a 0 after all). But in a Dependent Haskell scenario: they might (I don't know enough about the plans in this direction to figure whether this can be useful).
You can take it to the proposal's comments. But in the meantime: first note that because of the way we're doing case, and because Haskell is a lazy language, `(a,b)` acts both as the tensor pair type in the linear case and as the lazy pair type in the unrestricted case. What if you need a lazy pair in the linear case? Well you can define it: type With k a b = Either (a -&gt; k) (b -&gt; k) -&gt;. k What is this `k`? It's the type of effects that you can perform while projecting. We believe it's not worth adding more complication for additive conjunction, since the encoding is actually more useful (we could have pure additive conjunction, but we don't expect it to be all that useful).
On Ubuntu, if I run ghc-pkg list, I see a list of packages and all ghc versions say 8.2.1. If I run ghci from the terminal, I get the Prelude prompt but the version says 7.10.3. How do I fix this? 
How is this relevant to which language a library is written in?
We can very easily infringe the constraint of linearity. Take any function `f :: A -&gt;. Unrestricted' B`. Define `g` as follows: g :: A -&gt;. () g x = case f x of { Unrestricted' _ -&gt; () } Under the newtype interpretation of `Unrestricted'`, `f x` would never be forced. Therefore consuming the result of `g x` exactly once would not consume `x` exactly once. So this shouldn't be a linear function.
I'm happy to give that up, and having to write `stack ghc ...` instead. If you're using `stack` now, `ghc` isn't even in your path. `stack` could add the required language extensions to the file on-the-fly, or add them as parameters to ghc.
The games are, not the engine
Oh, apparently I've misread that sentence. Sorry!
&gt; all the fancy type-level stuff But no type families.
I'm curious what UI library are you using in PureScript. Is it some Elm-like thing (Pux) or something entirely different?
With respect to b, what limitations remain after accounting for fake-dependent types a la singletons? I'm aware of the problems involving the prelude hierarchy and the compatatbility required to implement constrained or subcategories as in subhask, but it seems from my experience that dependent types (fake or otherwise) offer considerably more in the case of bindings over their python et al linear algebra counterparts as well as going quite a long way when combined with more Haskelly backends like accelerate or hmatrix. Given the number of man-hours that have already gone into optimization in accelerate and tensorflow, it seems like dependently typed apis over those backends are a reasonable short term goal for ml / numerical libraries (until someone plops down a big chunk of change to pursue a comprehensive GHC work, anyway). 
I'm not sure why this keeps popping up. I'm happily running nix on arch without issues. It's probably from people who install nix with pacman? If you're going to use nix, you're going to want to have nix manage nix. Just [install](https://nixos.org/nix/manual/#ch-installing-binary) it as described in the manual.
which ghci tells you the binary that is called. Maybe a wrong symlink?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/haskell] [A great thread, in case you missed it. The future of front-end web development in Haskell may well be a Haskell -&gt; WebAssembly compiler that \/u\/ElvishJerricco is working on. GHCJS is great and still serves us well, but it may well be displaced over the coming years in the WebAssembly revolution.](https://www.reddit.com/r/haskell/comments/7cmppn/a_great_thread_in_case_you_missed_it_the_future/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I think I like yours better actually! It seems to compose better. `jsn` only performs a single operation, so we're clearly meant to be composing it with other `jsn` and non-`jsn` transformations via `|`. Any such sequence can be given a name and reused in bigger compound transformations. It's a combinator library! `jq` and `jl`, on the contrary, provide a rich language of operations with an *internal* notion of composition. As a result, while it is certainly possible to compose `jl` transformations via `|` and to give names to those compositions, those new transformations cannot be used in the internal language, so they feel second class. Now, that internal language is of course much more powerful than what can be accomplished strictly by composing `jsn` commands. `jl` has `map`, and a `filter` which supports more complex predicates than just a set of acceptable keys. But now that your tool has shown that the shell seems like a better place for composing json transformations, I think a better way to get those more powerful commands would be to write a higher-order shell command like `xargs`. `jmap`, for example, would take a shell command as an argument and apply it to every member of its input json array.
Planning on putting most of that thread in something like an FAQ when I have time. Or if anyone wants to open a PR to [wasm-cross](https://github.com/WebGHC/wasm-cross) ;) Worth noting a lot of things have to go as planned for this to work out. I see no reason right now that they wouldn’t (hence the plan), but there’s just a lot of potential points of failure.
Why is there no or little effort into creating higher level laws for these VMs? The client will never be able to verify these things given the ever increasing compute and storage requirements.
Oh. Stupid easily-to-misunderstand English language! "can't" should be much longer than "can" and should sound as different from "can" as possible. Maybe "unsafePerformCannot"? For what it's worth, youtube's auto-generated captions also thinks it's "can't".
If the Haskell runtime has its own GC (running in WA, independent of the JS GC), doesn't this mean that there's going to be a lot of unnecessary copying and data duplication? And it would mean double the GC pauses, too? I want to be optimistic about WA, but to me it seems like the justifiable use case is where you have your own renderer (i.e., running a 3D game in WebGL). The whole paradigm just seems like a poor fit for DOM webapps.
What data would be duplicated? As for pause times, the JS heap ought to be very small, so JS pause times ought to be negligible.
Well any time you need to display text, that text would need to exist in the logic of your application (the Haskell heap) and the DOM (the JS heap). These two can't share, can they? It seems like the GCs would be fighting each other ("well *I* don't need that data! *dealloc* "), hence, you'd need to copy, right?
By higher level laws you mean something like dependent types?
Ah. Yes, JS will frequently have to copy strings out of linear memory. However, this shouldn’t represent a very large percentage of the heap for most apps. But more importantly, if WebAssembly’s raw runtime speed is anywhere near that of native code on mobile, then jsaddle has already proven that performance benefit drastically outweighs the cost of marshaling. In my experience, native mobile jsaddle apps can be at least 10x faster than the GHCJS counterpart running on a desktop even; and that’s with the JS VM in the mobile web view running its own GC independently of Haskell’s.
Are there any libraries that work by reading the schema from the database (like [postgrest](https://postgrest.com) does) and using that to provide a (typesafe) Haskell interface? If not, why not :) ?
Well that's good to hear! I'm surprised marshalling is so cheap. Is it cheap enough to hook something like a scroll event without hurting framerate? For example, one trendy thing in UI is to have a big header initially, then collapse it to a mini version when the user scrolls down far enough (e.g., Twitter). Is marshalling fast enough that even this is no issue?
opaleye-gen does that for Opaleye https://github.com/folsen/opaleye-gen
Should be. I’ve seen fairly high frame rate drawing to a canvas before. But worst case, you can always use [`eval :: String -&gt; JSM JSVal`](https://hackage.haskell.org/package/jsaddle-0.9.4.0/docs/Language-Javascript-JSaddle-Evaluate.html#v:eval) to write JS directly.
They were exploring switching to react when I left.
That would be why it keeps popping up. Call me weird, but if the default solution (is package manager) doesn't work for a distro but it works for most others, then I basically consider it to be a pain to set-up, regardless of how nice it actually is to set up.
I made a [moon lander game prototype](https://youtu.be/aGLE1WQ2npY) using Chipmunk, and occasionally try to do things with Haskell, so this sounds like a fun project. I’ll have to check out this library tonight after work.
Looks neat. SelfAndDescendents is really just SelectMany if your Expr implements IEnumerable&lt;Expr&gt;. It might look more .NET-idiomatic that way, and I think implement IEnumerable means you wouldn't need Sawmill for some of the queries you describe. I'm a little confused by your switch with a 'case .. when'. I've never seen that before, but I take it that's supposed to be pseudo-C#?
It doesn't. Nix on Win 10 Ubuntu is a non solution, unless you want to build Ubuntu binaries, which is probably a bad fit here. 
Nope, it's a new-ish C# feature. You can `switch` on an object's type and use `when` for guard clauses. They call it "pattern matching", though it's rather under-powered for my tastes because I've been spoiled by Haskell. On the point of `SelectMany` - yes, it's a recursive application of `SelectMany`. `IEnumerable&lt;T&gt; SelfAndDescendants(this T t) =&gt; t.GetChildren().SelectMany(SelfAndDescendants)`. I decided against having `IRewritable` be a subtype of `IEnumerable` because it breaks symmetry with [`IRewriter`](https://github.com/benjamin-hodgson/Sawmill/blob/master/Sawmill/IRewriter.cs) (a version of `IRewritable` which allows you to use Sawmill's stuff with pre-existing objects).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [benjamin-hodgson/Sawmill/.../**IRewriter.cs** (master → 0349742)](https://github.com/benjamin-hodgson/Sawmill/blob/0349742ff598e4e63cf9c9b9ac49aeb8cd36bcd8/Sawmill/IRewriter.cs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
If you reuse GHC's existing RTS, wouldn't that make the WebAssembly code platform specific (as in x86 vs ARM, not as in Windows vs. Linux)?
WebAssembly is a host agnostic architecture. Anything that is host specific in GHC will need to be specialized for WebAssembly as opposed to arm / x86 / etc.. The same binary produced by GHC will work the same on all WebAssembly implementations, regardless of host.
I'd be cautious using anything from Blockstream. Their pockets are very deep.
Yeah. It definitely does. :/ I hadn't thought about GADT `newtype` data family instances. I may even have written one of those myself at some point. :(
What have you done to debug? More information is definately needed: ghci-8.2.1 # Does this work ls -l $(which ghci) # What does this say? ls -l $(readlink $(which ghci)) # If the above is a symlink ls -l $(which ghc) # and this?
&gt; Anyway, the point is, learning Haskell and its ecosystem has made me a more effective OO programmer. And mainstream OO languages often actually turn out to be pretty good at functional programming. Everyone asks me why I'm learning Haskell even though no (or incredibly few) companies in the area use it, but this is exactly why. It gives me a better perspective on my work in other languages. (Also, upvotes for working on a job search page I'm currently using :P )
Upvotes for using our software! 😉
I'm working on a platformer example that I hope to publish soon, but I need to wrap up callbacks and forces before I can complete it
It isn't. Some folks interpret "you're priviledged" to your "success isn't yours". Which leads to some bitterness. 
Yes actually! It turns out the issue was that I had forgotten to install xCode 9. I didn't think to check this as I hadn't needed it in the past to run Haskell, guess I do now though.
It might be best to wait for collision callbacks and forces, I hope to wrap those up ASAP
&gt; I decided against having IRewritable be a subtype of IEnumerable because it breaks symmetry with IRewriter (a version of IRewritable which allows you to use Sawmill's stuff with pre-existing objects). I'm not sure I follow. Don't pre-existing objects simply need to be lifted which you can do by passing in a delegate or defining an extension method?
Ok, I see. Very clarifying. I still find it sort of grotty, but this motivates the proposed solution well. 
https://functional.works-hub.com/job-board/Senior-Haskell-Engineer-Contract-London-Nov-2017-ca115?utm_source=Reddit&amp;utm_medium=post&amp;utm_campaign=peter&amp;utm_content=HaskellContract
&gt;Scala enjoyed what I think was the peak of its success during that time period mostly because of spark. I thought it was partly due to akka? I may be mistaken; I don't follow Scala as closely. &gt;I'd still need a plotting package to see experiment results as with matplotlib. Python/R are still unequivocally better for data exploration for the time being. I'm not sure how to change that but perhaps others would have more insight. &gt;Why isn't there more discussion around building the ecosystem in this direction or putting similar efforts into ETA/Frege!? No idea. You'd have to ask them. I think denizens of /r/haskell mostly use GHC. 
Oh well that's decided then.
&gt; When the type system is making actual work harder than it should be you're not benefiting from being principled any more. Huh? I realize mathematicians use the same notation for multiplying scalars by matrices vs. matrices by matrices, but that doesn't mean *we* should. It's a different use case.
&gt; Idris's type system can hardly be called "unprincipled" and works well in practice. Idris' type system overloads operators, but you can't have the same identifier twice in the same module. But this is probably a case where they should be separate anyhow. 
&gt; there's a point where fighting with the type system invokes a certain kind of stockholm syndrome I think it's recognized that type systems suffer from ergonomics problems in many ways. But there really isn't any other better way. &gt;Or could it be that there is actually just very few of us who come from a background of dynamically typed languages, that persevere into learning a very strongly and statically typed language? I predominantly did Python (and VHDL...) before Haskell. I can unequivocally say Haskell's type system is better than Python. Doesn't mean it's perfect of course :)
&gt; What are your thoughts on writing such libraries in Idris, seeing as you mentioned it? It seems like its type system would facilitate optimizations necessary for ML folks to be satisfied Idris is 100% a research language at this point. There's no package management tool so you have to download + install dependencies manually. 
&gt; I think the idris folks would agree that they can't keep up with GHC from an engineering perspective. As I understand it the speed isn't really there. Not to mention profiling and all those flags :)
&gt; I think Haskell the community might have some serious problems. First off the Haskell community has a great desire for long lived backwards compatibility. Huh?? &gt; However the Haskell community is rather unwelcoming to vendor driven design. If you want to Haskell be "enterprise-ready", pay developers. GHC has two full-time developers. &gt; I suspect a vendor trying to release a tightly coupled Haskell would hit mostly resistance. I think you're underestimating how hard it is to beat GHC :) &gt; I think the Haskell community likes to be a breeding ground for great ideas not an implementation language of great ideas. I'm not sure this is true. Try writing a recursion schemes library in C or C++.
Really? How does it deal with dynamic types and FP?
&gt; And this was my point way back when I said that the Haskell community doesn't want to be a mainstream language. Haskell is relatively mainstream already. "Avoid success at all costs" means avoid "success at all costs", not "avoid success" at all costs. Haskell already has a good amount of cruft, and I'd say continuing on the present path is the best hope for a language people *like* to program in. 
 interface IRewritable&lt;T&gt; { IEnumerable&lt;T&gt; GetChildren(); T SetChildren(IEnumerable&lt;T&gt; newChildren); } I can't add an `IRewritable&lt;T&gt;` implementation to a type I don't own. So if you want to use Sawmill's tools with an existing type (like Roslyn's syntax trees) you have to use the following interface, which allows you to implement the `IRewritable` methods on a separate object: interface IRewriter&lt;T&gt; { IEnumerable&lt;T&gt; GetChildren(T value); T SetChildren(T value, IEnumerable&lt;T&gt; newChildren); } I could have written `IRewritable&lt;T&gt; : IEnumerable&lt;T&gt;` and dumped the `GetChildren` method, but there's no equivalent of that design for `IRewriter`. I think having symmetry between the two interfaces is valuable. Does that answer your question?
interesting, thanks :)
&gt; I think you're underestimating how hard it is to beat GHC :) Most likely I don't think they be doing much with GHC. Possibly nothing and attacking the integration further up the stack. If they did they likely would be forking and tweaking performance around certain extensions. 
Agree completely Like (or maybe love) to program in and want to force lots of other people to program in and not the same things. 
What is the difference between biplate and uniplate?
For example if there is some contract that stores data in a sorted array and encodes that as a binary tree, then a client can verify properties in this array by downloading and storing just a subset of the data. These sort of laws will be required in order to build a fully distributed system - otherwise the storage and cpu requirements scales linearly with the transaction volume.
Sure it'd be nicer if there were no issues getting nix installed with pacman. You said there was no nice solution to get nix installed on arch and that's simply a falsehood. Docker's often used too much like a hammer and it's not even a great [hammer](https://blog.codinghorror.com/content/images/uploads/2012/06/6a0120a85dcdae970b017742d249d5970d-800wi.jpg) ; )
IMO exceptions and bottom should be deprecated from the language. 
Traversals where the type you are manipulating and starting from are the same (uniplate) or different (biplate). 
I am big fan of type system capabilities, and making Haskell good for developing efficient software. But i am afraid this will make the type system more complex, hence the implementation of dependent types more difficult. Which already feels like a huge task, especially if only one person [Richard Eisenberg] will do most of it. Dependent types are more important. 
&gt; I could have written IRewritable&lt;T&gt; : IEnumerable&lt;T&gt; and dumped the GetChildren method, but there's no equivalent of that design for IRewriter Right, which is why I said that you'd need to lift pre-existing types by providing delegates for get/set. Something like: public static IRewritable&lt;T&gt; AsRewritable&lt;T&gt;(this T obj, Func&lt;T, IEnumerable&lt;T&gt;&gt; getChildren, Action&lt;T, IEnumerable&lt;T&gt;&gt; newChildren) Then you'd just have a single general wrapper returned by this extension: class WrapperRewritable&lt;T&gt; { Func&lt;T, IEnumerable&lt;T&gt;&gt; getChildren; Action&lt;T, IEnumerable&lt;T&gt;&gt; getChildren; } I'm not sure it suffices, but just a thought. Makes a different tradeoff between defining rewritables at the type level with new class definitions vs. at the value level with lambdas.
Out of curiosity, is there a reason you chose to implement this functionality in C# rather than F#? I assume it's because you want the resulting API to be more idiomatic with your existing C# code, but I'd love to know if there were other considerations as that would've been my first impulse for a Haskell port.
I'm using Bootstrap and PS (Halogen), and it's very tedious! It's not at all obvious which aspects of the UI belong in separate Components, and I found it quite difficult to manage complexity in a SPA.
I think writing an internal set of linting rules would probably make more sense for this particular problem than modifying the compiler
Yay, if I do: ``` $ docker run -it --rm -p 8888:8888 -v $(pwd):/notebooks 8fd6e40a06d0 ``` it works perfectly.
As the Docker image was updated, that was much easier than figuring out nix- I want to play with nix for other purposes, but for this Docker was really much more convenient.
Or using HLint, which has exactly this rule
That's very similar to my design, except I put those two functions in an interface because it's more C#y that way :) This might interest you, hopefully it's not too tangential: it actually turns out to be a lot simpler to make everything go through `IRewriter` (and for `IRewritable` I have [an `IRewriter` implementation which just forwards the calls](https://github.com/benjamin-hodgson/Sawmill/blob/c697c5138a67b56d08851ffd44f2dbb10eff1518/Sawmill/RewritableRewriter.cs)). The problem is this: if you have class WrapperRewritable&lt;T&gt; : IRewritable&lt;T&gt; { T _t; Func&lt;T, IEnumerable&lt;T&gt;&gt; _getChildren; Func&lt;T, IEnumerable&lt;T&gt;, T&gt; _setChildren; public IEnumerable&lt;T&gt; GetChildren() =&gt; _getChildren(_t); public T SetChildren(IEnumerable&lt;T&gt; newChildren) =&gt; _setChildren(_t, newChildren); } you can't write recursive functions like `SelfAndDescendants`, because the recursive calls have nowhere to go. IEnumerable&lt;T&gt; SelfAndDescendants&lt;T&gt;(WrapperRewritable&lt;T&gt; wrapper) { foreach (T child in wrapper.GetChildren()) // how to get child's children? T is not rewritable, remember. } So you end up making `WrapperRewritable` return `WrapperRewritable`s, and plumbing in the delegates... class WrapperRewritable&lt;T&gt; : IRewritable&lt;WrapperRewritable&lt;T&gt;&gt; { T _t; Func&lt;T, IEnumerable&lt;T&gt;&gt; _getChildren; Func&lt;T, IEnumerable&lt;T&gt;, T&gt; _setChildren; public IEnumerable&lt;WrapperRewritable&lt;T&gt;&gt; GetChildren() =&gt; _getChildren(_t).Select(x =&gt; new WrapperRewritable(x, _getChildren, _setChildren); public T SetChildren(IEnumerable&lt;WrapperRewritable&lt;T&gt;&gt; newChildren) =&gt; new WrapperRewritable(_setChildren(_t, newChildren.Select(x =&gt; x._t)), _getChildren, _setChildren); } ... but now you're doing a ton of extra work, wrapping and unwrapping every node in the tree. It's a lot more efficient to treat `IRewriter` as the base abstraction and layer `IRewritable` on top. I totally glossed over all these decisions in the post: `IRewriter` barely even gets a mention, let alone the underlying design tradeoffs. I'd intended the post to be a tutorial, not a comprehensive design doc, and I think `IRewritable` is a bit easier to grasp than `IRewriter`. So, thanks for asking about it!
It seems like the hlint rule in question only warns on: module Foo (module Foo) where instead of module Foo where
Good question! Thanks for asking. You've basically hit the nail on the head - it's a library for C# consumers, so it makes sense to write it in C#; plus, it was extracted from some existing C# code. But hopefully I can add a bit of colour to that which might be interesting: 1. At Stack Overflow I'm lucky to work with a team of talented - legitimately famous, in some cases - C# developers, and of course we have a large C# codebase. Even though F#'s barriers to entry are relatively low for a .NET system, it just doesn't make sense to adopt F# at work. Not everyone on the team is interested in learning F#, let alone on work hours. Most of us would prefer to continue being productive in C# than become beginners in F#. In other words, it'd be a waste of the team's brainpower to start writing F#. 2. More on C# at SO: We have nine machines serving tens of thousands of requests a minute. While it's certainly possible to write fast F# - and for certain types of code it can beat C# - _in general_ it's a bit more difficult to tune than C# is. In particular, idiomatic F# tends to generate a lot of garbage. For 95% of applications (and indeed for 95% of our codebase) it doesn't matter, but when it does matter it's important to be able to drop down to fast imperative code. 3. As you say, I envision my lib's consumers will generally be written in C#. Again, while it's possible to write a F# library which is idiomatic for consumers, you end up with not-particularly-F#ish F# (eg interfaces rather than records-of-functions). Additionally, library consumers like to know that they could crack open your source code and make sense of it if they needed to. Occasionally - not usually, of course - you need to read the code to make sense of a function, or to fix an upstream bug, or just to learn something. 4. F# is a great language, with lots of innovative and useful features for which C# has no equivalent (type providers, units of measure, etc). But _for this library_, everything that I would find useful in F# is present and correct in C#: lambdas, generics, local functions, etc. (That said, I wonder whether type providers might help with some of [the stuff for which I'm currently using reflection &amp; codegen](https://github.com/benjamin-hodgson/Sawmill/blob/master/Sawmill/AutoRewriter.cs)?) C# is actually a pretty good functional language! 5. I'm not actually very good at F#. I'm still at the stage of making frequent syntactic errors. So, although writing a library might be a good way to learn F#, I'd definitely pay a productivity tax for a little while. I'm not ragging on F#, but _for me_, as C# grows more functional language features, it makes sense for C# to be my default choice for functional code.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [benjamin-hodgson/Sawmill/.../**AutoRewriter.cs** (master → c697c51)](https://github.com/benjamin-hodgson/Sawmill/blob/c697c5138a67b56d08851ffd44f2dbb10eff1518/Sawmill/AutoRewriter.cs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dprv77w.)^.
HLint catches both, but the one this post refers to is off by default. Pass `--show` to see all hints, or enable the hint `Use module export list` in a `.hlint.yaml`.
&gt; This might interest you, hopefully it's not too tangential: it actually turns out to be a lot simpler to make everything go through IRewriter Right, this is kind of where we're headed in this thread: your interfaces wouldn't hold any state, they should be type-class like, in providing a dictionary of methods that operate on a T without holding a reference to T, like IEqualityComparer&lt;T&gt;. Then the delegate approach I described works in recursive descent as well. In fact, you can be clever and eliminate all interfaces and just use a struct: public struct Rewriter&lt;T&gt; { public Func&lt;T, IEnumerable&lt;T&gt;&gt; GetChildren { get; private set; } public Func&lt;T, IEnumerable&lt;T&gt;, T&gt; SetChildren { get; private set; } } Where you would implement IRewritable&lt;T&gt;, you instead provide an implicit conversion to Rewriter&lt;T&gt;. It's a little dirty and non-idiomatic, but it's also a little more flexible, and sometimes needed because C# doesn't allow implicit conversions to interface types.
Ah, I was using - warn: {name: Use explicit module export list} not - warn: {name: Use module export list} 
[Cloud Haskell](http://haskell-distributed.github.io/) sounds relevant.
Surely it should be the other way round. If I explicitly say module Foo (module Foo) where then I *really did* mean to export everything.
Thanks, this seems useful.
&gt; ghci-8.2.1 # Does this work Yes &gt; ls -l $(which ghci) # What does this say? lrwxrwxrwx 1 root root 44 Nov 13 04:00 /usr/local/bin/ghci -&gt; /usr/local/haskell/ghc-8.2.1-x86_64/bin/ghci &gt; ls -l $(readlink $(which ghci)) # If the above is a symlink lrwxrwxrwx 1 laptop laptop 10 Aug 22 00:02 /usr/local/haskell/ghc-8.2.1-x86_64/bin/ghci -&gt; ghci-8.2.1 &gt; ls -l $(which ghc) # and this? lrwxrwxrwx 1 root root 43 Nov 13 04:00 /usr/local/bin/ghc -&gt; /usr/local/haskell/ghc-8.2.1-x86_64/bin/ghc &gt; ghc --interactive Now. Let's see if it stays that way.
But it is not linear in `f`. Yorgey didn't give a formal definition in the blog post for me to copy, but he meant that the condition must be true recursively for all nested lambdas as well. I'll try to clarify that in my post above. Thanks.
Thanks! Those are all really good reasons. C# really is becoming quite good for FP, despite the sheer amount boilerplate involved. Type providers are really nice from the perspective of a developer using them but implementation is extremely unsavory. [It results in much denser, less coherent files](https://github.com/Mouaijin/DynamicsCRMProvider/blob/master/src/DynamicsCRMProvider.Runtime/XrmRuntim.QueryExpressions.fs) than your example and the documentation doesn't really exist; I've found that the number of types created seems to have an exponential impact on design-time performance and there's no good memoization story either. If you ever decide to look further into the language, that would be about the most masochistic way to start!
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Mouaijin/DynamicsCRMProvider/.../**XrmRuntim.QueryExpressions.fs** (master → 6fc0143)](https://github.com/Mouaijin/DynamicsCRMProvider/blob/6fc0143f7d804a98037ae5e177ddb837abdb6709/src/DynamicsCRMProvider.Runtime/XrmRuntim.QueryExpressions.fs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dps1axf.)^.
I think *diplate* would have been better! *Runs*
Why is that a bad thing?
There's nothing special about working on caching/locking/etc in Haskell. Learn about them from a theoretical standpoint, then implement them in Haskell.
&gt; Since Haskell is pure, they will both compute the same value, so it doesn't matter. This is the part I need to understand. The boxed values are stored as heap closures and the evaluating the value has the side effect of overwriting the heap value with the result of the evaluation. So, does Haskell ensure that the process of overwriting the heap memory is sufficiently atomic so that even if multiple threads are evaluating the same thunk, it'll still end up being the correct result on the heap? If that's the case, how is the atomicity ensured?
good termination checking / totality checking is still an active area of research. :) 
There are morally correct incoherent instances, yes. For instance, `Coercible` can take different paths to arrive at the conclusion that two types have the same runtime representation (see the paper). The trouble is that even in those cases, the code is fragile and it's hard to guarantee that the instance you want will be selected.
A law-abiding version of Functor (where = is strict equality, not just what you can observe) is "property-like structure". That is to say that no matter where you get the definition from, if it passes the laws, it is uniquely determined. (Similarly, if you forget a monoid was really a group, and someone comes along and offers you a mechanism for finding its inverses, if it works, its uniquely determined.) In these cases the provenance of how you get from Monoid -&gt; Group or from (* -&gt; *) to Functor doesn't matter, as the proof of uniqueness comes from somewhere else. In those cases I'd say incoherence was morally correct. This would require a language in which you could actually express those laws, though. So, Haskell is not that language. ;)
This looks pretty sweet. May I ask what editor/OS you use for C# development? I figure it's probably Visual Studio and Windows, but since Haskellers are often more Linux/emacs/vim oriented (and I guess atom?), I'm curious if you used something different. Personally, I found trying to work in C# on Linux to be pretty difficult, as I never did get mono to work quite right. But that was a long time ago, so I may have done it wrong or times may have changed.
I don't I understand, what would be an example interleaving which would give the wrong result?
You might also want to look at some work on zippers and file systems http://okmij.org/ftp/continuations/zipper.html#zipper-fs
May be /u/BayesMind is refering to Prop which although is a Type has some restrictions on how you match on it. For example, if I remember correctly you cannot `match` against a prop to yield a value of type A when A is not a prop. I think of it as an interesting property of Coq's type system and is more like a "performance" enhancement in the sense that Prop is definitely a Type.
I have a recursive formula type, that can hold for example `((5 = 2) and ((3 &lt; 5) or (6 &gt; 0))`. I want to turn it into an (Automata state glyph) type recursively. For any "primitive" (a=b, a&lt;b, a&gt;b), I can construct a corresponding (Automata Int Bitvec). Then if I need to `and` `(Automata state0 glyph)` and `(Automata state1 glyph)`, I can construct `(Automata (state0, state1) glyph)` that does what I need it to do. For, `or`, it's `(Automata (Either state0 state1) glyph)`. The problem is, I don't know what type this recursive transformation should return. I can't do something like `recurse :: Expr -&gt; Automata state glyph` because if we say `state` corresponds to `Int` in the example above, the final state should actually be `(Int, Either Int Int)`, which doesn't match `Int`. I don't really know how to express this sort of thing with the type system, any pointers? Thanks.
&gt; This would require a language in which you could actually express those laws, though. So, Haskell is not that language. ;) We already don't verify that `Applicative` is compatible with `Monad`. Is the requirement that users write compatible instances of Functor more onerous than that? If we stuck to simple ones like those in my example, and didn't do satanic things `instance Monad hom m ⇒ Functor hom hom m`, I don't see a problem but maybe I'm missing something. The crazier instances that require care like the ones in my example would be confined to the library that defined those type classes mostly anyway. Most uses by end users would `instance Functor (→) (→) MyType`. The reason we need a `Functor` type like this is because of linear types. The old type-family-based definition won't do if we want our functors to be multiplicity polymorphic, so I was hoping this approach would pan out. &gt; Consider deriving Functor from Monad. Sure fmap = liftM defines fmap, but if you defined Monad using return, and join, expecting to get (&gt;&gt;=) from join and fmap, then getting fmap from (&gt;&gt;=) and return leads to a cycle. Once you start being able to dictionaries from multiple sources you find all sorts of new Heisenbugs to squash! In such a languages, it should be probably a requirement that subclass dictionaries carry superclasses and rely on a separate defaulting system rather than the incoherency allowed by the property-like structure. 
I got ghc and gloss to work in linux. Now I'd like to do the same in Windows (powershell and/or MSYS2). I tried setting it up in both. I have ghc 8.2.1 working and compiling in both Powershell and MSYS2, but I can't get gloss, or glut, or gtk, to work in either. I've spent enough time on this that I'm ready to just pay someone to help me getting it working. So, who wants to make $10 bucks? \**desperate*\*
&gt; Instance search for Coercible is special-cased in the compiler, instance search for your custom class is not. I don't see where a problem would arise. Regardless of which instance is selected from those above, the result will be correct. 
Maybe you can find some info in [CS240h's final projects (section: Distributed storage and computation)](http://www.scs.stanford.edu/16wi-cs240h/projects/projects.html)
1. The result might be the same denotationally, but not the same operationally. You potentially lose performance. 2. Depending on what instance is selected, the compiler might succeed or fail to satisfy instance context constraints. Since there's no backtracking, you might face an issue when the code *would* compile if only a different instance was selected.
WoW addons are implemented in Lua. One route you could take is generate Lua code from Haskell. https://hackage.haskell.org/package/language-lua
1. I only intended on use the above strategy on operationally similar instances like the ones I used. The goal is to make a few fundamental instances automatically available at the appropriate types 2. This is a good point and definitely not something I had considered. I wonder if there is a way to hack in the category of constraints to get around this problem. 
How can I test the truth value of doesFileExist? Everytime I pass arguments to a function and use an if then statement, I get "Can't match expected type 'Bool' with actual type 'IO Bool'". 
It was originally called Play and PlayEx so anything is better than that!
And no explicit imports either 
For on-disk storage, the b-tree package looks relevant. https://hackage.haskell.org/package/b-tree For consensus (i.e. locking, consistent writes etc), a raft implementation like https://github.com/NicolasT/kontiki https://github.com/kfish/raft I'm not sure if they are production-ready, but if there's something to fix maybe that's what you can do. For communications, cloud haskell, or maybe not since you want to use cloud haskell. You can use the raft database for group membership as well and store the endpoints to the other servers and just bootstrap using a single server. That's what docker swarm does. 
I get the same issues for quite some time - there where quite a few tickets with GHC(i) for similar issues (for example https://ghc.haskell.org/trac/ghc/ticket/11317) all thought to be solved in GHC-8.2.1 which is wrong in my case. I tried to recompile all of stack etc. (if I try long enough I can get ghc, stack etc. working - it's like 20 segfaults on 1 successful run of the command) Overall for me Haskell development on Windows is *dead* - my solution is to either use a native Linux system or a VM inside Windows. Thankfully I don't have to run Haskell code on Windows machines or it would be game over.
I have some data which is partially updated in chunks, and I would like to expose the fields as they are processed, but continue to hide the ones that aren't. To do this I was thinking of something like `data T f = T { x :: f Int, y :: f String }` from which I could then map over the f to make it work, e.g. data Partial a = Partial a | Complete a updateData :: sumTypeWithUpdateOps -&gt; T Partial -&gt; T Partial transformData :: T Partial -&gt; T Maybe consumeData :: T Maybe -&gt; a pipeline = consumeData . transformData . updateData op -- You could have some functor mapping function like so weirdMap :: (forall a . f a -&gt; g a) -&gt; T f -&gt; T g weirdMap f (T a b) = T (f a) (f b) or something like that. Has anyone had some experience with anything similar, or am I just being dumb and should pursue some other avenue?
Great article. Thanks for sharing!
For work, I run Visual Studio on a Windows VM, since running the website requires a whole Windows environment (SQL Server, IIS, etc.) For spare-time stuff like this library I use VS Code, with .NET Core, on OSX. It works remarkably well, I’d do all my work like that if I could.
Ah ok cool, thanks for the reply. I have heard a lot of good things about VS Code. One of these days I will have to sit down and figure out how to make C# development workable for me using Linux+vim. Java too while I'm at it.
[removed]
Is github the best place to monitor progress?
React-in-plain-JavaScript? Did they encounter some serious issues with PureScript?
Some non-examples: We like lazy lists, but they're recursive. Interpreting free monads one operation at a time is a great application of laziness, but they're recursive. One good answer to this question would be to tell me what would go wrong if we had data (,) a b = (,) !a !b data Maybe a = Nothing | Just !a data Either a b = Left !a | Right !b 
Thank you, I appreciate it :)
PureScript has those types and I haven't really noticed the difference. You can avoid doing work, I guess. 
These things are only head-strict I believe ?
`fst (1, undefined)` works with non-strict pair, breaks with strict pair. You can pattern match against a non-strict `Maybe` without it breaking on `Just undefined`. Same with `Either`. Marginal cases perhaps, but there's a difference nonetheless. Some libraries that don't need that end up defining `Pair a b = Pair !a !b` because they don't need the laziness.
This is a really bad example, but I've sometimes used `error "this code path should not use this value"` as an internal assertion instead of providing a dummy value like 0. Yes, yes, I should refactor! But I'm... lazy.
As other comment said, it's not supported at all by Blizzard, although you can try to work around it. That being said, to add this to a game would either require writing a custom Haskell compiler (probably a bit too much work) or using libghc, which shouldn't be too much of an issue because it's BSD licensed... but then if that hasn't changed since I last checked, GHC uses GMP which is LGPL, making it a bit tricky to distribute, because you have to allow end users to link a different version of GMP to comply with the license (iow you have to distribute at least the object code of the rest of your library).
One interesting consequence of strict tuples but lazy functions would be more significant difference between (a, b) -&gt; c and a -&gt; b -&gt; c
You mean you sometimes fill fields of records with `undefined`? This seems to be a Haskell trick with some history and is or was done inside GHC, as I recall.
How about `isJust` It returns true if the value is present and false if the value is not. In a traditional language it would be ridiculous to do an entire, potentially expensive, calculation to get the answer and then just throw it out because you only wanted to know if there _would_ be an answer. So you'd want to write two methods. One to check if there might be an answer, and one to actually compute it. With a lazy language it only has to do enough of the computation where it gets to the point where it constructs a Just, and then it can stop doing the work of of figuring out the real answer. 
&gt; Marginal cases perhaps, but there's a difference nonetheless. Indeed, what I'm really after is good examples (preferably or real-world code) of why these differences should exist.
FWIW: I'm reworking Kontiki quite a bit, and the goal is for it to be 'production grade' for sure.
IIRC in TH the structure for QuasiQuotes has some fields that aren't relevant depending on where you allow the quasi quoter to be used, so you just fill them with undefined or error usually (unless the QuasiQuote somehow is valid in every context), so that's sort of a real world thing?
Similarly with tuple I think it's even more plain because you have two values. You can write a function like `(user.name, expensiveOperationToProveSomething)` Then do ``` hasAccess ("Jimmy", _) = True hasAccess ("Alice", _) = True hasAccess (_, value) = computation involving value ```
Exactly. Glad to hear I'm not the only sinner. :) (Though I know GHC's codebase is probably not to be viewed as a collection of best practices!)
Not sure it's good example (because I don't really understand what's written there) but seems like /u/edwardkmett exploits lazy behavior of `Writer` type. https://www.reddit.com/r/haskell/comments/3faa02/what_are_some_real_world_uses_of_writer/ctn104u/
Packrat algorithm is a pretty good example. Yes there is a list involved, but it doesn't need to be lazy: the algorithm would work just as well with an array. What makes the complexity linear is the (true) laziness of record fields that store the intermediate results. 
Tying the knot results in a recursive data structure by definition, does it not?
Laziness allows us to define our own control structures in Haskell using ordinary functions. For example, `maybe` and `either` can be used as code branches, since the branch not selected is never evaluated. You can even use these homegrown control structures to control a top-level IO loop.
Another example is phantom parameters. These allow us to fix the type of a polymorphic function by supplying an expression with the chosen type as an extra parameter. This works even the calculation of the expression would crash, due to laziness. This technique, though still used, is less common than it used to be, now that we have more advanced features in the type system.
This is true, but in a language with only strict data you'd just (no pun intended) wrap the computation in a lambda.
I often end up using a lazy `Map` of results to avoid infinite recursion. The map itself is not recursive, even though the thunks stored inside it refer to the map. One example is [this simple pull request](https://github.com/UweSchmidt/hxt/pull/20/commits/333a3dbe596240225c9326637a0bc15359dc9250) that got merged into [hxt-relaxng](http://hackage.haskell.org/package/hxt-relaxng) a while ago. It reduced the code size, sped it up by a factor of 3 or so, and fixed some bugs, simply by replacing the map of inputs by a lazy map of outputs. 
Here is code that I wrote for GHC recently (although ditched it later): 227 -- Runs the ExitifyM monad, and feeds in the final es_in_scope_acc as the 228 -- es_in_scope to use 229 runExitifyState :: InScopeSet -&gt; ExitifyM a -&gt; (a, [(JoinId, CoreExpr)]) 230 runExitifyState in_scope_init f = (res, es_joins state) 231 where 232 (res, state) = runState f (ExitifyState in_scope_init in_scope [] emptyTM) 233 in_scope = es_in_scope_acc state Here I am running a state monad with some portion of the state (`in_scope`) fed back into the state. This way, I can do a single pass over the code while “knowing” a summary of the whole pass.
The problem is that you get no sharing with just a lambda in a strict and pure language.
It's a fine pattern. [Here is another discussion](https://www.reddit.com/r/haskell/comments/72sct8/posting_an_incomplete_record_to_an_api_handler_in/) where it was brought up (first comment), I'm sure there are others. Abstractions like Functor, Foldable, etc. can be defined too, see the [rank2classes library](https://hackage.haskell.org/package/rank2classes-0.2.1.1/docs/Rank2.html) for instance.
I'm not sure what you mean. Can you give an example?
Maybe, I'm not sure.
Yes, but they require lazy functions, not lazy data.
GADTs can reflect information about values in types. This works by giving more specific types to constructors than what regular ADTs would define. data Expr state where Primitive :: Prim -&gt; Expr Int And :: Expr state0 -&gt; Expr state1 -&gt; Expr (state0, state1) Or :: Expr state0 -&gt; Expr state1 -&gt; Expr (Either state0 state1) Then you could write `recurse :: Expr state -&gt; Automata state glyph`.
This is true, so I guess my question is "When is the sharing in lazy data structures really critical?"
Sure, by making a lambda you've just implemented laziness in a strict language. That's how it's done. But you had to decide that this is a time when you wanted it and added it in explicitly, and then the caller has to handle the fact that it's no longer a value, but rather a function that returns a value when they use it. And what if the value in there was itself a lazy value. You call your lambda and get a tuple, and then you have to make the second item in the tuple lazy too because somewhere else way down in your algorithm you don't always use that either. Now you have to change everything to support that value threading through your system. In the end laziness doesn't break Turing Completeness, every language can run a computation roughly the same way Haskell can. It's just that Haskell makes it easier and native feeling, and takes the burden of deciding when something is ignored on itself. The tradeoff is obviously when you know you want something more strictly you have to tell it that, but it still doesn't necessarily change the way you use the value. It looks the same to the rest of the code, it just has already been computed this time. 
Could you explain a bit more? I don't see a `Map` in there.
A mere `Proxy` seems to work around that!
This seems to be a good reference for the packrat algorithm: http://bford.info/packrat/ There are implementations in many languages, most of which are not lazy. How would you characterise the benefit of lazy data with respect to implementing this algorithm? Clearly it can't be *necessary* but is it "extremely useful", or just "useful"?
No, I haven't seen this setup written up anywhere. You're right - we should! :-D
You could write your addon in PureScript and take a look at its rotting [`psc-lua`](https://github.com/osa1/psc-lua) backend. Or go straight to [Idris](https://github.com/melted/idris-lua), which is probably the route I'd go.
That's quite unpractical TBH. Also, that's not really programming WoW AddOns in Haskell either.
`maybe` requires lazy data for its first argument.
It turned out it was my antivirus. But you're right. It is a smoother experience on nix
What about lazy arrays? They're useful for e.g. dynamic programming or memoization. Everyone has had to manually implement a lazily initialized array at some point in a strict language. 
I don't think it is. For that it would still be required to form a functor, which it does not.
A [QuasiQuoter](https://hackage.haskell.org/package/template-haskell-2.12.0.0/docs/Language-Haskell-TH-Quote.html#t:QuasiQuoter) basically consists of 4 parsers for quasiquoted strings, one for each context a quasiquote can appear in (as part of a type, as an expression, as a pattern or as a declaration). Normally a quasi quoter will only make sense in some contexts, for example an identity quasi quoter (something like `[qId|Some Text|] == "Some Text"`) only really makes sense as an expression.
I think they're referring to [this](http://hackage.haskell.org/package/iso8583-bitmaps-0.1.0.0/docs/src/Data-Binary-ISO8583-TH.html#binary) common pattern when defining quasiquoters which only make sense in expression context: binary = QuasiQuoter { quoteDec = binaryQ , quoteExp = undefined , quotePat = undefined , quoteType = undefined } It's not a very good example though, because those fields are functions, so there is no need for laziness, the quasiquoter can simply throw the exception after receiving its input, like [`aesonQQ`](https://hackage.haskell.org/package/aeson-qq-0.8.2/docs/src/Data-Aeson-QQ.html#aesonQQ) does: aesonQQ :: QuasiQuoter aesonQQ = QuasiQuoter { quoteExp = jsonExp, quotePat = const $ error "No quotePat defined for jsonQQ", quoteType = const $ error "No quoteType defined for jsonQQ", quoteDec = const $ error "No quoteDec defined for jsonQQ" }
True, it's not strictly necessary at all, I didn't mean to imply that it was.
what antivirus is that? ... we switched to sophos endpoint security and that might be around the time the trouble started (not sure - at first I only had the problem with intero but now it's with everything compiled with Haskell: ghc(i), stack, the purescript ide-server, ...)
I would take the approach I always take to undefined behavior: assume the compiler is evil and out to get you. Incoherent instances allows you to define instances that the compiler is free to choose from at a whim, so as long as you're ok with the compiler actively trying to break your code by picking the worst possible instance, you might be fine.
I wouldn't call it a property of the function. It's a property of the expression and its value. Haskell-the-language intentionally has no operational semantics that rigidly fix when to evaluate expressions. And GHC-the-Haskell-compiler represents expressions at run time as either thunks or values. The run time turns the thunks into values at whatever moment it decides is advantageous, usually as late as possible. Given church encoding on the one hand and first-class functions on the other, it's hard to draw a sharp line between "lazy values", "lazy functions", and "lazy data". Are you looking for examples where the laziness happens to occur in an expression passed to a constructor defined in a `data` declaration? Is that what you mean by "lazy data"? If so - why is that specific case of laziness special for you?
The code is not very readable unless you're intimate with arrows, but I'll try. Take one of the several type signature changes: -createPatternFromXml :: Env -&gt; LA XmlTree Pattern +createPatternFromXml :: PatternEnv -&gt; LA XmlTree Pattern The `LA` above stands for `ListArrow`, but we can pretend it's a function instead. The `Env` and `PatternEnv` are type synonyms, and we can expand them to get -createPatternFromXml :: [(String, XmlTree)] -&gt; XmlTree -&gt; Pattern +createPatternFromXml :: [(String, Pattern)] -&gt; XmlTree -&gt; Pattern The first argument is not a capital-case `Map` but it is a map represented as a list of key-value-pairs. My pull request didn't go as far as replacing the data structure, so there's further easy pickings here. The next diff line to note is - . lookup n $ transformEnv e + $ lookup n e Here you can see how a reference to an environment used to recursively process the value looked up from the input environment, and now it's a simple lookup in the output environment with no recursion. Finally, there's this change at the top: -createPatternFromXmlTree = createPatternFromXml $&lt; createEnv +patternFromXmlTree t = patternFromXml env t + where env = map (fmap $ patternFromXml env) definitions +patternFromXml env = head . runLA (createPatternFromXml env) that creates the lazy output environment and passes it in instead of the input environment. 
The non-lazy languages have to implement the memoization logic themselves, in Haskell the laziness means it's free and easy to reason about. I guess that could be a decent summary for any other use of laziness. A language with futures and promises might be on equal footing here, but I'm not sure. 
Actually, Haskell is great at code gen via eDSLs. It's would be pretty easy (relatively) to write a monad that could produce working Lua code. This is a technique done a lot in Haskell.
Strict functors break the laws. Strict products aren't really products (Haskell doesn't have products because products are too strict and the products you suggest are even more strict). For the latter two, sometimes one knows with branch a result will fall in before having fully computed the result. In those circumstances, the laziness is useful.
It absolutely possible to draw a sharp line. I'm exactly looking for cases where the constructor functions are strict in their arguments.
In fact we can do even better. We have `runIO :: IO a -&gt; Q a` so we can raise the exception inside `IO` itself!
I think I have an example: ```haskell data Bin a = Leaf a | Bin (Bin a) (Bin a) deriving Show bfl :: forall void b. [b] -&gt; Bin void -&gt; Bin b bfl l t = result where bfl' :: Bin void -&gt; [[b]] -&gt; ([[b]],Bin b) bfl' (Bin l r) (xs:xss) = let (xss' ,ll) = bfl' l xss (xss'',rr) = bfl' r xss' in (xs:xss'',Bin ll rr) bfl' (Leaf _) ((x:xs):xss) = (xs:xss, Leaf x) (xss,result) = bfl' t (l:xss) ``` the `bfl` function relabels a `Bin` in a breadth first manner by using lazy lists to pass remaining items of every row to a deeper level. 
Ah, no, it doesn't. For `maybe` to work as we expect we only need function application to be lazy, we don't need data constructors to be lazy. Perhaps I have been unclear about how I explained this.
That's a very nice use of laziness but it's about as recursive as you can get :)
Yes, that's a nice example, which is in the same class as the Packrat algorithm answer.
&gt; One good answer to this question would be to tell me what would go wrong if we had Isn't pattern matching on tuples strict by default though? Not that you can't work around it of course. 
Ah, so something like [`ivory`](https://hackage.haskell.org/package/ivory) does? Might work, but if you can write idiomatic Haskell that way.
[Matrices](https://hub.darcs.net/vmchale/lazy_matrices)! Makes fusion trivial.
To be sufficiently lazy, you would have to put laziness annotations everywhere. To be sufficiently strict, you just need one strictness annotation in the right place.
Ah, I didn't read the question well enough.
I see. So this is basically creating a memotable?
That's a very interesting claim! What exactly do you mean?
I'm not sure what you mean. I don't consider this strict. Do you? Prelude&gt; case (undefined, undefined) of (_, _) -&gt; () ()
I do not think this falls into that range at all. Scalar multiplication and ring-like multiplication are very different things and I really really do not want them to use the same operator. They model entirely different mathematical concepts and obey very different laws. In an ideal world I would like multiplication of numbers, matrices and vectors, combining regexes and so on (ring-like) to use one operator. Then I would like multiplication by a scalar, such as replicating a list or scaling a vector or repeating a regex, to use another operator. There is a lot of room for silent mistakes with one shared operator (accidentally multiply a number by a matrix but no type error for example), and type inference is down the drain, and I do not see any significant benefit.
All the data types that carry the computation that one wants to be lazy in would have to have laziness annotations, but if you want to be strict, you just have to force the value at the right place and you are done. 
That's a bad thing. All of a sudden, the following doesn't mean what it should uncurry (||)
There's a page on the Haskell wiki: https://wiki.haskell.org/Lazy_pattern_match A bit old but it does explain.
Well, call it what you like, but those data definitions I gave above have different semantics from the usual Haskell ones.
That's not an incorrect summary, but I find it a bit misleading because it implies creating a whole new data structure. In this case, as I find in too many others, the data structure already exists. It's just not used to its full lazy potential. 
Or maybe it's a good thing, and you just have to read the `uncurry` operation as being useful for un-lazifying short circuiting operators like `||` and `&amp;&amp;`. :)
Java was the first language to make me give up and use an IDE. I ended up compromising with intellij and ideaVIM. It's not perfect but it gives me enough vim to be able to work with it. You might want to look into something like that?
We use a variant of https://www.youtube.com/watch?v=rtfbQJGQj0Q for most of our reporting needs. I talk about it a little here https://www.youtube.com/watch?v=o3m2NkusI9k This is used from the Haskell side and reflected in the purescript side. But Haskell wasn't the sum total of the web applications that we built. Most of it was content being embedded in other applications. e.g. Charts and widgets in an otherwise straightforward C# based web app. Nothing we were building was greenfield. There our reporting stuff had to interact with the surround page, so we spoke from purescript to whatever is around us in the javascript ecosystem.
I think it's more on account of, I didn't quite realize that properties live at the same level of abstraction as types, ie properties are types. 
That actually works better with a very strict writer. =)
"Using laziness as a convenient way of implementing a memotable"?
Ah, now I see what you mean! The `~` annotation in that page explains how to make pattern-matching on tuples more lazy than the default, by not forcing the spine of the tuple. The `data (,) a b = (,) !a !b` definition is a way to make pattern-matching (among other things) on tuples more strict than the default, by also forcing the elements of the tuples in addition to the spine.
&gt; Is github the best place to monitor progress? Yea basically. When I have time, I’m hoping to put up a number of issues on GitHub to give people an idea of where we are, what’s being done, and how people can help.
Yeah I've tried vim plugins for IDEs but they always felt pretty lackluster. I honestly think Java is doable provided one is familiar with Java build tools in a terminal. Vim can generate all the boilerplate, and I'm pretty accustomed to a non-IDE build process. The only real thing you miss is semantic refactoring (which doesn't bother me tbh, particularly since I can search and replace across a project, which covers 99% of the refactoring I would do) and automatic imports (which isn't too much of an issue to me either, but should be scriptable nonetheless) C# is a different beast, because at least when I tried last, I had a lot of difficulty even getting it to compile on Linux. I'm sure part of this is that the team I was working on was using (closed-source) testing libraries with dependencies on Visual Studio itself (which is so fucking insane by the way, I could not believe my eyes when I saw that travesty of design).
How long ago did you last try to run C# on Linux? .NET's cross-platform situation has improved a _lot_ in the last couple of years.
About a year? I couldn't spend a ton of time wrangling with it as I had to get up and running, so there might have been a solution. Also now that I think about it, I think I was running a VM which may not have been configured correctly as well.
No that isn't a good thing at all. `uncurry` isn't just something Haskell made up. It is a mathematical concept that should follow certain laws from category theory. It is supposed to be an isomorphism. Making it arbitrarily add strictness makes it not so. 
Any time you want to decompose functions using data as a boundary, then you need a lazy data structure to preserve laziness in the functions. Otherwise, the pre-composed version will have preferable laziness/performance to a decomposed version. As a contrived example, consider taking the average of a list: preAverage :: (Num a, Num b) =&gt; [a] -&gt; (a, b) preAverage = foldl' (\(s, c) n -&gt; (s + n, c + 1)) (0, 0) average = uncurry (/) . preAverage sum = fst . preAverage length = snd . preAverage We've "suspended" the average computation, and instead have a single-pass `sum` and `length`. If we want to only know the `length` of the list, then we can call `snd . preAverage`, and if we want to only want to know the sum, then we can call `fst . preAverage`. This avoids having to do the work of calculating the sum *and* length if we only require one.
Isn't memoization essentially the difference between call-by-need and call-by-name? You can simulate a call-by-name value with a function `() -&gt; a`, but that will duplicate work if you force the value more than once. To simulate call-by-need, you need a way to memoize the function.
What do you mean by recursive and what do you mean by lazy? A function from natural numbers to another type is an infinite sequence but so is a lazy list. So a lazy list is recursive and lazy but a function from the naturals to the given type is non-lazy and non-recursive but these two types behave *almost* the same way semantically.
Great. Hopefully you can collaborate with /u/ElvishJerricco on [this](https://www.reddit.com/r/haskell/comments/7cmppn/a_great_thread_in_case_you_missed_it_the_future/). The future is in my making as we speak.
It was in fact Sophos. Specifically Hitman.Pro, the real time monitor. It was injecting into the ghc process and causing confusion. 
Interesting! FYI, it doesn't seem to like this: id x = x; id_id = id id; run = 42 It gives infinite: _0 = TV "_0" :-&gt; TV "_0" I think it might be better to not call this a Haskell to X compiler though if it doesn't support laziness. Do you have any plans to extend this to something like Haskell 98? 
Another idea I've explored is getting a Haskell wrapper around the SCII client protobuf API, to do reinforcement learning and AI dev. https://github.com/Blizzard/s2client-proto. I tried using `proto-lens` to generate the initial bindings and it almost worked, but proto-lens made some mistakes I need to go and fixup by hand.
Seems to be a compiler for a small, small, small subset of Haskell. It's a compiler entirely separate from GHC. Cool project (I know how fun it's to write a compiler), but not practical.
Why the downvotes, y'all? This is a job opportunity to convert Ruby code to Haskell, and likely create more Haskell jobs in the process.
hm, this hides the same problem as using ghc-mod related plug-ins that we have to match versions for different projects. Maybe different ghc-mod versions could be packed inside dynamic libraries and selected at run-time? or selecting whole HIE executables from "selector" executable.
Here's one in Haskell :-P https://bitbucket.org/varosi/cgraytrace/overview
This was fun! Good job.
The basic structure here is called the "banker's method": http://www.cs.cornell.edu/courses/cs3110/2013sp/lectures/lec21-amortized/lec21.html https://www.westpoint.edu/eecs/SiteAssets/SitePages/Faculty%20Publication%20Documents/Okasaki/icfp96amort.pdf 
I think it's a cute demo for learning how to compile core to STG to assembly, and to see webassembly in use. Definitely more educational than useful.
Does Okasaki's work address knot-tying definitions or infinite structures?
For correctness, equational reasoning with recursion works the same: l (x : xs) = result where result = zipWith (+) (l xs) (replicate x 0 ++ result) -- By definition = zipWith (+) (l xs) (replicate x 0 ++ result) -- By the above auxiliary equation = zipWith (+) (l xs) (replicate x 0 ++ l (x : xs)) -- By the first equation in this sequence. So we get the same equation that *defines* the first naive version. When we write l (x : xs) = zipWith (+) (l xs) (replicate x 0 ++ l (x : xs)) *as a Haskell program*, its meaning is actually that `l` is the *least defined* value that satisfies the equation. Consequently the knot-tying version is at least as defined as the naive one. People may be more familiar with recursive functions, where one expects to see some decreasing argument, whereas for things that are not functions there are no arguments to see. But from a denotational point of view it's all the same: recursive things, functions or not, are formalized as fixed points of functions that behave nicely enough to guarantee their existence. Operationally, variables are just pointers, so they are cheap to pass around (even in the definition of the value they are going to point to), as opposed to creating a whole new computation by calling `l (x : xs)` recursively. Of course, since you have an infinite structure, you can't fully evaluate it. Instead, you can focus on a finite part of it to reason about performance. To estimate the complexity of evaluating a finite fragment of a recursively defined structure (e.g., the first `n` elements of `result`), you estimate the complexity of evaluating the body of its definition by assuming that the very same fragment (but no more) is already evaluated to its value when you need to access it. If you find during that process that you need a part of the result that is outside the fragment you initially chose, then you should have picked a larger fragment (or you may have an infinite loop). For example, the cost of evaluating the first `n` elements of `l (x : xs)` is to evaluate the first `n` elements of `l xs`, `zipWith` on this prefix we just evaluated, and the concatenation `replicate x 0 ++ result`, but we assumed (a prefix of) `result` to already be evaluated. `Cost(x:xs) = Cost(xs) + O(n)`, by induction we can thus get `Cost(xs) = O(n * length xs)`. To justify formally why this method accurately models the gains of sharing in a recursive structure, you can go through a more precise operational semantics (see Launchbury's Natural semantics for lazy evaluation) but that sounds like a hassle in the same way that we usually don't make explicit all the details of an abstract machine to reason about imperative programs.
Hi, excellent, I want exactly that (a production quality distributed POSIX file system in Haskell, out of disappointment with current alternatives). I recommend you to study GlusterFS's design and behaviour in detail (also using `strace`). It has all the features you're talking about. Also look at CephFS a bit. The concepts in Gluster are really good but in my opinion it suffers tremendously from being written in C. There are lots of issues with Gluster, most importantly by-design performance problems related to not bundling enough data in a single request to reduce round trips, such as: * [https://bugzilla.redhat.com/show_bug.cgi?id=1478411]( Directory listings on fuse mount are very slow due to small number of getdents() entries) The way I'd go about it is to use a publicly available consensus implementation (e.g. consul or etcd, but perhaps also the https://github.com/kfish/raft mentioned below) as a substrate for master election, implement all file system operations via FUSE (as gluster does), make a sum type containing all FUSE ops, make an ADT for batching those ops, use that everywhere. If you care about performance, use high performance system calls. `writev`, `sendfile`, Linux 4.14's just released [new zero-copy sendmsg() functionality](https://lwn.net/Articles/726917/). I'd first use the FUSE thing, then later (probably post-masters) write a kernel module to get past the limitations of FUSE. You can ping me if you want to talk more about this btw. The only thing that confuses me in your post is: &gt; Each component has to be developed using the servant REST library Why would you use REST, or HTTP at all, for making a file system? Filesystem ops are typically not hyper-text, I suspect you'd be better off with a proper long-living TCP connection with TCP hearbeating enabled (of course you could use HTTP keepalive and all that stuff to avoid extra connection-reinitiation, but HTTP is just extra stuff doesn't really give you much for this use case).
CloudHaskell will not help you with any of those. These are features of your application. CloudHaskell _may_ help you to send messages from one machine to another. If I were to use it for such application, I'd mainly use `call` though and none of the underlying Erlang-style message-passing functionality, as for a distributed FS you typically want to do synchronous RPCs. Also take into account that CloudHaskell's serialisation uses `binary` which is quite slow. Distributed FS are often used with 10-Gbps-Ethernet, `binary` would probably bottleneck you immediately on that.
Does this use of `ViewPatterns` and `PatternSynonyms`achieve a good enough result ? newtype Flags2 = Flags2Packed Word -- Use bitwise operations here mkFlags2 :: Bool -&gt; Bool -&gt; Flags2 unFlags2 :: Flags2 -&gt; (Bool, Bool) pattern Flags2 :: Bool -&gt; Bool -&gt; Flags2 pattern Flags2 a b = (unFlags2 -&gt; (a, b)) where Flags2 a b = mkFlags2 a b -- There are now also pragmas to satisfy the complete-pattern checker... There may be some room for a generic library here: Template Haskell can surely do all of this; generics can't make the pattern synonym, but there may still be enough material to make good use of view patterns. 
https://hackage.haskell.org/package/MemoTrie iiuc
Is this a reimplementation or a new backend for GHC? Because I want a backend for GHC not a new compiler.
I'm my opinion, I think this is probably one of the most important projects in making Haskell a more accessible and successful language. Thank you for your efforts. If you would like the additional help of a beginner writing docs, writing boilerplate or in any other capacity, please let me know!
Did you mean implicit? And I would agree that implicit imports are bad except for Prelude and internal modules.
Unfortunately I am in Canada and not the US, but this sounds quite interesting.
Agreed, especially since the title is "Ruby and Haskell" not "Haskell and Ruby". Honest in tiny things goes a long way.
As far as I know there's no obstacle to applying it in that setting. It really shouldn't matter.
Sorry I missed this one in the survey :) 
This is awesome.
I'd love to see some serious work thrown at this type of thing. It's definitely something, I think the community could really benefit from. It'd be nice to get a list somewhere of what every page should be so that people could start filling in pages in their spare time. Perhaps even a semi sort of template could be proposed? This might also be a good idea for next year's summer of Haskell...
Perhaps a question about what prelude people would like to use the most would also be useful. I know I would like to have a differently designed prelude but I tend to stick with the normal one despite that.
This is basically the best possible thing. I am not sure how I feel about the wiki as a host, but the writing style and content are spot on.
Note: it says it doesn't support laziness presently, so technically it's not really Haskell in that it will behave differently for some programs.
Yes, that is what we are considering doing.
Any prelude recommendations? I've been using classy-prelude since it comes standard with Yesod, but I'm going to be introducing people new to Haskell to this project, and I think something simpler might be preferable (classy-prelude has pretty complicated concepts and type errors).
This is nice! Also, this remind me of [stackoverflow documentation](https://stackoverflow.com/documentation)
Nice ;) Nice idea to present the result using a webpage. PS: I had a quick look, and I think your sphere intersection routine is bugged in case of a ray starting from inside the sphere, because you take the `min` of both roots of the second degrees equation, but one can be negative. I have not tested that however.
Not helpful, but Abbotsford must be a pretty great place for an office!
Definitely! I followed along converting from Gloss to CodeWorld. Not too much different, and easy to play around with on the web. https://code.world/haskell#PNR9yg-2S7JlSMyp3xRYFMA
agreed. http://dev.stephendiehl.com/hask/ , https://github.com/Gabriel439/post-rfc/blob/master/sotu.md , and https://wiki.haskell.org/Typeclassopediaare currently the most comprehensive references imo, but definitely not an MSDN. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/post-rfc/.../**sotu.md** (master → fe87d1d)](https://github.com/Gabriel439/post-rfc/blob/fe87d1dc3785f92a61b9f2c4d208ba1f6f79a862/sotu.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I’m quite fond of `protolude`. It’s basically just reexports, a few name changes and a class to convert between strict and lazy string types and bytestring (I’m split on whether including bytestring is a good idea but it hasn’t been a problem for me so far).
&gt; Isn't memoization essentially the difference between call-by-need and call-by-name? Yes, I think you are correct.
What do you think you would prefer as a host?
Its probably not, but it could still be non strict. Haskell's does not *technically require* laziness. Anyway, I think this page is interesting, well worth a read: https://wiki.haskell.org/Lazy_vs._non-strict 
This example has a space leak so it's not especially compelling.
Thumbs up from me. Very nice.
Wow! Never heard of that CodeWorld thingy! Looks really cool. Thank you!
&gt; The Functor typeclass represents the mathematical functor: a mapping between categories in the context of category theory. Perhaps it's just an unfortunate choice of words, but I wound't say that the `Functor` typeclass represents a functor from CT — there are many functors it couldn't possibly represent. Instead, I would say that it represents covariant endofunctors on `Hask`.
And this is exactly why I think this should be a wiki. I definitely can't get everything correct and it shows, but this way, documentation can be readily corrected.
I was making a bad joke by swapping the first two letters of each word. 
According to SPJ, it could be the *next* Haskell!
For obvious reasons, this poll will over-represent the people that use non default preludes.
Let's say you have a function f :: A -&gt; (B, C) f a = let i = intermediate a b = computeB a i c = computeC a i in (b, c) You want to support the following three use cases: a user needs just the `B`, a user needs just the `C`, a user needs both the `B` and the `C`. If we had only strict tuples we'd have to write three different functions, one for each use-case. The third one is necessary because we want the intermediate result `i` to be shared between the computations of `B` and `C`. Another option would be to expose `intermediate`, `computeB` and `computeC` and have the user assemble them herself.
Let's say you have a function f :: A -&gt; (B, C) f a = let i = intermediate a b = computeB a i c = computeC a i in (b, c) You want to support the following three use cases: a user needs just the `B`, a user needs just the `C`, a user needs both the `B` and the `C`. If we had only strict tuples we'd have to write three different functions, one for each use-case. The third one is necessary because we want the intermediate result `i` to be shared between the computations of `B` and `C`. Another option would be to expose `intermediate`, `computeB` and `computeC` and have the user assemble them herself.
I never found a prelude worth changing to, even if there are many good ideas there. When I need something more comprehensive, I drop into `base-prelude` or `rerebase`.
I like to use something simple. So basically re-exports only. Something like: module MyPrelude (module E) where import Control.Applicative as E import Control.Concurrent.Async as E import Control.Concurrent.STM as E import Control.DeepSeq as E import Control.Error as E import Control.Lens.Combinators as E import Control.Monad as E import Control.Monad.Except as E import Control.Monad.Reader as E import Control.Monad.State as E import Data.Aeson as E import Data.Bifoldable as E import Data.Bifunctor as E import Data.Bitraversable as E import Data.Char as E import Data.Default as E import Data.Foldable as E import Data.List as E hiding (uncons) import Data.Monoid as E import Data.Proxy as E import Data.String as E import Data.Traversable as E The sibling post mentioned a class to convert between lazy and strict string types. I'll say this is unnecessary in this Prelude because it re-exports lens which contains a class with a Iso between strict and lazy types: https://hackage.haskell.org/package/lens-4.15.4/docs/Control-Lens-Iso.html#t:Strict So conversion is as simple as `view strict` or `review strict`.
Btw, you may be interested in https://hackage.haskell.org/package/base-noprelude
Bitfields aren't frequently used, even in C, because of issues like you can't take the address of them. This means when you for example extract one of them, it's not just taking a pointer, but GHC has to recompute with bit operations (shift and mask). I would say it is preferable to make a separate type if you want bitsets. C++ has `std::bitset&lt;N&gt;` for example. 
Thank you for that hint! It is exactly that - there is a .dll injected that is called just before the crash happened - we could not find the exact settings or how to exclude paths/files so I had to deinstall the "exploit" part of the sophos endpoint security and this seem to fix the issue. You restored my faith in Haskell and my sanity ;)
Duet's evaluation stepper is non-strict and not lazy: http://chrisdone.com/toys/duet-delta/ You can see this in e.g. data Tuple a b = Tuple a b main = (\x -&gt; Tuple x x) (2 + 7) Is expanded like this (\x -&gt; Tuple x x) (2 + 7) Tuple (2 + 7) (2 + 7) Tuple 9 (2 + 7) Tuple 9 9 In a lazy evaluator, the `x` would be shared and only evaluated once. --- You can write a `seq` on integers like this, though: seq = \x f -&gt; case x of 0 -&gt; f 0 n -&gt; f n main = seq (2 + 7) (\n -&gt; Tuple n n) That produces: seq (2 + 7) (\n -&gt; Tuple n n) Tuple 9 9 
The examples are great, I appreciate the style. The examples are so much better than tutorials.
If you wanted to implement `f` in a strict language what would you do?
It manages the universe levels behind the scenes for you.
&gt; Coq isn't Turing-complete, so there are some programs it can't express. This statement is very similar to the idea that pure languages can't perform side effects. A proposition that we already know to be false. Non-termination can be expressed as a monad in a total language.
Why should `(,)` be interpreted as the categorical product? Even if the base libraries' implementation of `(,)` was a strict pair there's nothing the base libraries (or any user) implementing `data LazyPair a b = LazyPair a b`.
There is no reason it has to. Both types are useful actually. I would say make `_×_` be categorical product and make `(_,_)` strict product. The only concern I have is if people get too obsessed with strict data we lose out on the whole point of having a non-strict language. It is easy to add strictness because the user of an API can do that; however, a user of an API cannot add laziness to a strict API. Using strict data where a need hasn't demonstrated itself is premature optimization.
&gt; It is easy to add strictness because the user of an API can do that; however, a user of an API cannot add laziness to a strict API I don't think this is quite correct. A user cannot do anything to `foldl` to make it more strict, for example.
Can you demonstrate some MDN documentation that already exists that perhaps motivated this? I do like what's going on in the wiki page, too!
Fair. I'll weaken to my previous claim that you just need one strictness annotation in the right place while laziness annotations must be chained up the entire call chain.
Some of the types are wrong like the two related functions of Functor ($&gt;) and (&lt;$&gt;), the Functor constraint is missing from most type signatures and the explanation of (&lt;$) has a's and b's swapped. Otherwise I think it is very nice to have this kind of documentation.
Fair point. I'll refine my point a little. When we are speaking about data, adding strictness to lazy data API is trivial because the user can always `deepseq`, but a user cannot `deeplazy` with strict data. More broadly, in code one controls, only limited amount of strictness annotations are needed in the right places to be sufficiently lazy, but laziness annotations must be place throughout call chain, so that it doesn't get forced too early.
I think using HTTP2 isn't too bad these days, and I think you can get mostly the same performance out of it as a long-lived TCP connection. 
Either break it up into 3 functions or use a lazy product type (which almost no one else will be using)
It is. :) Our lunch room/balcony overlooks Dight's falls on the river. Shortish walk/ride from Victoria St. Close to the train/bus. We used to be closer to Victoria St, but moved slightly further away this year as we grew too big for our old office. 
&gt;Most of us don't write programs for NASA everyday and Haskell Type System is already powerful enough even without dependent types. I agree that Haskell's facilities for dependent types are not the easiest, but I think that understates the case for dependent types! One thing I *really* like about Idris is that you can get type-safe matrix multiplication. I imagine more examples of how to use dependent types productively will be discovered as well, much like applicative functors in Haskell.
[ReadTheDocs](https://readthedocs.org/) seems to be a good hosting solution for documentation.
Why not the wiki? This is an informational question, not a disagreement. The wiki is very old. I have no doubt there are many ways to improve on it. Besides that, some of the information there is old and out of date, but other information is important to preserve. What do you suggest?
I hope we can improve haddock to write "tutorial style documentation" more esaily.
Most of this information is in the Haddocks. Almost all of it *should* be in the Haddocks if it isn't. If the Haddocks are reasonably complete but you had trouble finding that, we need to find a way to make sure that beginners will immediately find out how to access Haddocks. Longer description and background might not be appropriate for Haddock. If so, we need to find a good place to put them to make them accessible without effort.
let me requalify that. MarkDown is the most popular marking language, [according to Google Trends](https://trends.google.com/trends/explore?date=all&amp;q=reStructuredText,asciidoc,markdown), so you may want to stick to it, despite [what Eric Holscher says](http://ericholscher.com/blog/2016/mar/15/dont-use-markdown-for-technical-docs/). GitHub has a nice hosting solution, called [GitHub Pages](https://github.com/blog/2233-publish-your-project-documentation-with-github-pages), based on MarkDown and Jekyll.
Disclaimer: still pretty new to Yesod, so there may be a better way to do things. I have been writing separate executables but with in the same cabal/stack package. They import what they need of my app (Import, Application, and whatever if my helper code they need) and thus get all the types and database access, but I can run them separately. (In my case they are for manual injection of data into my database, but I think the same idea should work for shoving them in a cron job). There were a couple of speedbumps getting things structured, if you want to follow my route and run into trouble, let me know, I can clean up an example to show you (and probably have more experienced people here tell me how I should be doing it better).
The functor constraint is missing in the source code as well, since they are defined in the typeclass If you find any wrong information, feel free to edit! It is a wiki after all!
Sorry to be obtuse, but this ad doesn't mention Haskell at all. What exactly is the link between the advertised jobs and Haskell?
Admittedly the page on Functors is very similar to that of the haddocks, but the page on foldr is far more detailed. A quick google search inevitably leads there. Documentation in the haddocks of individual functions is often cryptic (assumes a fair amount of background knowledge), short, and usually have no examples of usage. I feel that long descriptions would pollute the source code, and cannot be readily modified. My motivation for this project is to have exactly what you describe: a place which is readily available, readily editable, and contains long descriptions of types, classes, and functions.