This was what I was hoping it would be useful for when I wrote it.
The state of Haskell documentation is poor because a beginner has not one iota of a clue as to what a type-signature is or what the types mean, not because a sufficiently advanced Haskell developed can use familiar type signatures to put together a program. For example: Today a friend attempted to use `head` by reading the [documentation](https://hackage.haskell.org/package/base-4.6.0.1/docs/Data-List.html). The code they came up with was ``` test = head :: [myvar] -&gt; a ``` In the real world, people coming from languages without type signatures rely on example code. It is familiar and useful to them. Example code helps them understand what the type signatures represent. If the Haskell documentation makes it harder for them to learn Haskell the documentation has failed them. As a Haskell developer, What happens when you reach a type signature you don't understand? What happens when you first encounter `forall`? My friend had trouble with `[a] -&gt; a` using a function that is defined in other languages! and they aren't the only person having trouble. Your contention that &gt; They already have better documentation than most tutorials could ever hope to be. is fundamentally flawed towards your current understanding of type systems. Tutorials help people get to the level you are on and arguably the lack of them is preventing people from having an easier time learning Haskell. As a side-note, being dismissive: &gt; But a complaint that you can't use most libraries because they involve math you don't understand and don't have any tutorials? That's one of the silliest things I've ever heard. of people that tutorials help is not going to help convince people to learn Haskell.
Someone who has that much trouble with `head` doesn't need a tutorial for `Prelude` or `Data.List` or all of `base`. They need a tutorial on what Haskell's type system means. I was very precise. I strongly believe the complaint that *libraries* are too cluttered with math and need tutorials shows a fundamental lack of awareness of the source of the complainant's confusion. This piece is intended to illustrate for beginners, people who don't understand most of what's going on in it, that it's possible to do a lot with types - and hence inspire them to learn about types. Maybe we need better resources for that last bit. Maybe I should add a line to the end suggesting resources for improving understanding of Haskell's type system. It's quite likely that the fact that I can't think of any resources for that job is a hint that they're a weak point in Haskell's documentation. But none of that contradicts my point - most *libraries* don't need tutorials, and demands for such are usually misguided.
Even if that font is (maybe) ok for “about” etc, it is not for the actual content. Also, as others said, small WTF example would be appropriate.
This is like saying that you have a wonderful documentation, the APL program itself: `X[⍋X+.≠' ';]`. It's so obvious, clear, concise about what it does. Just like haskell types. Read the damn source. You don't need documentation. If you don't understand the source, read awesome `[MARCUS 90]` and `[JOE 11]`. Also look at their bibliography. Spend the next two years or so. Good luck. 
&gt; Hah. And understand their implications! That is the hard part. Precisely, but since you also say "maybe the hard part isn’t strictly necessary" I think I have a slightly different point. My view is that knowing the names and types of all the functions can be all you need to infer an understanding of the abstractions you're dealing with, but often it isn't. What's missing from a list of names and type signatures... 1. An understanding of the kind of problem the library is intended to help solve. 2. An understanding of the abstractions the library uses, and how they help solve that kind of problem. 3. Typical usage patterns. "An understanding" doesn't just mean a name. It's like getting a kit without instructions - just a packing list. OK, so the kit includes a three inch floblicoddle with a triply bliblactive tropzlicator - but how much does that description help you if you never heard of a "floblicoddle", and don't know what a "tropzlicator" is or how it helps for the "tropzlicator" to be "triply bliblactive". If the kit is simple and obvious enough, you'll probably assemble it correctly. Or maybe your imagination will allow you to use the pieces to solve the problem (or some other problem) your own unique way. But if the kit is large, complex and unfamiliar, figuring out how the pieces fit together may be an intractable problem. At all levels, the lack of instructions have turned a ready-baked solution into a new kind of problem - sometimes trivial, but often not. If you like puzzles that's fine, but if you just want to get a job done, that's not so good (though I want to know how you found a kit providing a three inch floblicoddle with a triply bliblactive tropzlicator and why you think it's the right tool for your job). What I don't really understand is the claim that there's a lack of tutorials. Plenty of the papers out there are written to an academic rulebook, but some of those are still perfectly readable and there's still plenty of tutorials for most things anyway. Some is incomplete, confusing or far too vague (like the many metaphors for monads) but there's quite a lot out there, especially given that Haskell doesn't have the same scale following as languages like C, Java or C#. In fact the number and quality of the videos of Haskell tutorials and presentations is extremely impressive. And if you're really confused, there's always StackOverflow (and of course /r/haskell). 
&gt; you end up with type-correct programs that may or may not terminate, and that may provide a perfect solution if only you can figure out to which problem FTF~~W~~Y [I can't believe I got that wrong] 
That sounds to me like &gt; Warning: the code is broken, so use this to work around it. which seems much more PHP style than Haskell style. I'm also reminded of [this](http://thecodelesscode.com/case/104). Documentation is better than no documentation, but it's no substitute for a correct (or at least improved) api.
This is a really interesting read. I tend to approach (haskell) program architecture in pretty much the same way, but when I see things like `Coyoneda` in my program, I get giddy and try to understand them just because. :) Excellent article though, you did a great job conveying the way you think and definitely shed light on some of the "black magic" that newbies inevitably see with more complex haskell programs.
:) What’s your pouët account ?
Here is an example: https://www.fpcomplete.com/school/to-infinity-and-beyond/competition-winners/how-haskell-can-solve-the-integration-problem 
Asked for some more time and wasted it on hacking around the problem.
The etymology list has "unfacilitated" but not "facilitate". Weird.
MFlow is the best Web framework ever made by Mankind. Period. http://mflowdemo.herokuapp.com/ Disclosure: I´m the developper of MFlow, but that does not influence the above said ;D MFlow is the most haskellish Haskell web framework. and therefore, if rails is pure Ruby, MFlow should be the next "Haskell on flows" ;D ;D - It uses STM for data caching - You can create a REST web service APIs using parsec-like parser combinators - You can create an entire page with applicative combinators, not only forms. - You can express the routing and the navigation within a "do" block . That is a navigation monad unique in MFlow. for the imperative programmer it is the natural way to express a navigation. - You can create widgets with dynamic behaviours using monadic and applicative combinators. And yet, it is friendly for mewbie imperative programmers. But if you are an expert web programmer you have to unlearn a lot. 
* http://www.pouet.net/groups.php?which=9910 * http://www.pouet.net/groups.php?which=7355 * http://www.pouet.net/groups.php?which=8081 and there are some more from the last century, but those did not contain any haskell :)
Essentially the thesis is that it's possible to work out the right solution without having to understand how it works, therefore resources to help understand it are unnecessary. While this is really cool, it doesn't quite do it for me. What if I *want* to understand it? I *like* understanding things. It's useful sometimes! It's also somewhat like the difference between being to able to use a computer and program a computer. Just because you can use Minesweeper doesn't mean you could write Minesweeper. Ditto powerful abstractions. What I would rather say is that, due to its powerful type system, Haskell is more robust to a lack of explanatory documentation than most other languages, but this doesn't negate the value of such documentation.
&gt; There is also the observation that any good-quality splittable RNG gives rise to a good-quality hash function It depends what you mean by "good-quality". We don't need cryptographic quality here. We need the quality - and speed - that one would generally expect from the default random library of a general purpose programming language. &gt; Mersenne twister has a huge state, so I don't see how anything like it can be splittable. First, I wasn't trying to suggest that we use MT. By "like MT" I mean, with some comparable level of randomness and speed. Second, I don't see why big state is a disadvantage for implementing split. Indeed, greater complexity should be an advantage. &gt; there are 2n possible paths. If any pair of these paths is correlated over the set of all starting seeds then you have lost And I could say just the opposite. Even without actual correlation, it will *look* correlated for some paths if you have enough paths. We need the probability of apparent correlation to be about the same as what you would expect if they are not actually correlated. With `2^n` paths, that becomes easier. But we are talking in the air. Someone must have worked this out. &gt; we can't really hope to turn a bog-standard sequential RNG into a splittable RNG any more than we can turn it into a hash function. My intuition is the exact opposite. And even if you are right - there must be a much faster hash function that would be suitable for us. But I really think we need the input of an expert here.
That's almost exactly the opposite of what the article is saying, not that I expect you actually read the article before deciding how to complain about it.
I didn't mind (I upvoted this). `pipes-process` still needs some tweaking anyway 
see also: [Lockhart's Lament](http://www.maa.org/external_archive/devlin/LockhartsLament.pdf). If memory serves me, the comparison he makes is to teaching an art class by memorizing names of colors and timed drills on doing paint-by-number pages.
I just want to say that this was a great article.
Except when you need to go through the process yourself later on for a new problem, and a tutorial hasn't yet been written. If you are always content to follow and never lead off and do something new, then this approach works. There is a reason why students solve problems in math class, it gives them faculties for using the tools. I tend to view whatever I'm writing as only being about 25% about accomplishing the task, and being 75% about improving my ability to solve tasks I don't know that I don't yet know about in the future.
A lot of the intuition I have for these abstractions I earned by doing us what i advocated for in this post. I took a bunch of category theory, transliterated it into types, and started plugging in what fit and checking laws. In many ways what he is advocating does offer a road to understanding, it is just one where you have to spend a while feeling around in the dark first.
Of course I am not discounting the fact that this approach is very useful. I just wanted to say that I do think that tutorials and good documentation are very useful, and I disagree with the statement that it is "silly" that people are in need of them when there are the type to follow. It does really take a lot of time to learn a new library solely based on the types and the code. Of course I can do it, but having just one or two code pieces demonstrating the use goes a really long way.
You can do this with `pipes`. This is basically the same as asking how to implement `ArrowChoice` for `Pipe`s. The type signature of the function you want is: (+++) :: Monad m =&gt; Pipe a b m r -&gt; Pipe c d m r -&gt; Pipe (Either a c) (Either b d) m r In other words given a `Pipe` that translates `a`s to `b`s and a separate `Pipe` that translates `c`s to `d`s, we build a new `Pipe` that translates a stream of either `a`s or `c`s to either `b`s or `d`s. The reason this addresses your problem is that your upstream network `Producer` can have a type like: producer :: Producer (Either a c) m r ... and any time it wants to yield a value to go down the first branch (the one that transforms `a`s to `b`s) you just wrap that value in a `Left`. Dually, any time you want to yield a value to go down the right branch (the one that transforms `c`s to `d`s) you just wrap that value in a `Right`. On the downstream side you would have a consumer of type: consumer :: Consumer (Either b d) m r Any time it awaits a value you know which branch the value came from. Left values came from the first branch and Right values came from the second branch. So your pipeline would look like this when composed: producer &gt;-&gt; (firstBranch +++ secondBranch) &gt;-&gt; consumer I will do my best to sketch out how to implement this operator, but I am on my phone so there may be type errors or syntax errors. However, I know this works in principle because I have implemented this before in a more general form. import Pipes (+++) :: Monad m =&gt; Pipe a b m r -&gt; Pipe c d m r -&gt; Pipe (Either a c) (Either b d) m r pL +++ pR = left pL &gt;-&gt; right pR left :: Monad m =&gt; Pipe a b m r -&gt; Pipe (Either a x) (Either b x) m r left p = await' &gt;~ for p yield' where yield' b = yield (Left b) await' = do e &lt;- await case e of Left a -&gt; return a Right x -&gt; do yield (Right x) await' right :: Monad m =&gt; Pipe c d m r -&gt; Pipe (Either x c) (Either x d) m r right p = await' &gt;~ for p yield' where yield' d = yield (Right d) await' = do e &lt;- await case e of Left x -&gt; do yield (Left x) await' Right c -&gt; return c
Thanks very much for your reply. Looks like the gauntlet has been thrown down! So reading your code here (I've worked with conduit some but I haven't worked with Pipes yet), I assume I will need an "input" pipeline and an "output" pipeline? I can't see having it all in one pipeline because e.g. the output stream might appear blocked waiting for user input but it may actually be processing telnet options during that time. The client may also asynchronously initiate telnet protocol negotiations at any time.
I'm not sure if a simple pipeline will make sense here. You're going to want to respond to two kinds of events: network events, and user events. In other words, you'll probably want to have two different threads of execution, not one. This is similar to what we have in network-conduit, where an application is given a Source and Sink separately so that it can fork separate worker threads, instead of assuming an application will just be a Conduit. If you need that kind of setup, stm-conduit may be a good approach. But assuming for the moment that you don't need anything so fancy, you can accomplish this with the newly added [ZipConduit](http://hackage.haskell.org/package/conduit-extra-0.1.5/docs/Data-Conduit-Extra-ZipConduit.html). The basic approach would be: * Write a `Conduit` that converts a stream of `ByteString`s into a stream of `Either ByteString ByteString`, where `Left` values would be sent to (A) and `Right` values would be sent to (B). Let's call this `conduitS`. * Write `Conduit`s for A and B, e.g. `conduitA` and `conduitB`. Each will have a type `Conduit ByteString m ByteString`. * Now we need to adjust `conduitA` to only accept `Left` values. This is pretty easy: `CL.mapMaybe (either Just (const Nothing)) =$= conduitA`. Do the same thing for `conduitB`, and call the new `Conduit`s `conduitA2` and `conduitB2`. Both of these will now have type `Conduit (Either ByteString ByteString) m ByteString`. * Now you want to combine these two conduits together in such a way that they consume all the same input and merge their outputs. That's where `ZipConduit` comes into play. It would look like `getZipConduit $ ZipConduit conduitA2 *&gt; ZipConduit conduitB2`. Call this `conduitAB`, and it has type `Conduit (Either ByteString ByteString) m ByteString`. * Now combine it all together: `conduitS =$= conduitAB =$= conduitJ`. I've put together [a simplified example](https://gist.github.com/snoyberg/9212694) involving splitting up a stream of `Int`s into evens and odds, I hope it clarifies things a bit. All of that said: I think using STM for this will probably be the far simpler route to go. Have `conduit` put the incoming data into the appropriate channel (either A or B), have worker threads processing each of those channels, write output to some shared output channel, and have `conduit` read from that shared output channel and write to the output socket. 
You wrote all that on your phone?
Then in that case you can use `stm-conduit` (the `pipes` analog is `pipes-concurrency `). See Michael's answer for more details.
&gt; most libraries don't need tutorials, and demands for such are usually misguided I think the reason this is called for is because both parties are asking for different things. Imperative programmers are used to library meaning a large chunk of flexible functionality. After all dependencies are expensive, so you need bang for your buck. Something like lens is what they are picturing (a dynamic package that does something for everyone). In contrast most stuff on hackage is lightweight and single purposed, because cabal makes dependencies cheap. Something like esqueleto is probably what you are picturing (a small package that serves a single purpose). Sorry if this is a poor example, just the first one that popped into mind.l However your points on following the types are valuable, I just think that might not be the core problem that people are thinking about.
Of course, with an insufficiently strong type system, blindly using proof techniques will get you a well-typed, but likely wrong, program. In the presence of richer types, it becomes harder to get the program wrong, but also harder to get any answer at all. Plus there is the task of coming up with the right types to begin with, which is itself a programming task that you lack guidance on.
&gt; I was able to use the absolutely fantastic documentation we call type signatures to quickly figure out how to assemble pieces I don't fully understand into an effective final result. The whole process of doing this from scratch the first time only took about an hour, and that was including the decision of which implementation strategy I was going to use. This is why most Haskell libraries don't have tutorials. They already have better documentation than most tutorials could ever hope to be. This attitude is not only wrong, it's incredibly harmful and is a very risky way to use Haskell. The opportunity for error is very high.
This is why I loved math and physics in high school. It's just a puzzle, where the pieces give sufficient hints about how they fit together.
Oooh, the article is getting some notice over at HN. https://news.ycombinator.com/item?id=7298646
Just imagine that this project is something you came up yourself while walking home (ok, it's probably less fun than the kind of things we think about hacking on our free time). You don't have specifications, you just have an idea of something to accomplish -- you figure the details by yourself then, there is no spec. That's quite often like this in companies too -- you're asked to do something by non-IT people, and you figure out the storage, architecture, libraries, tools, etc and come up with the code that does what they want. Knowing the haskell ecosystem obviously helps here. EDIT: A few years back, I did an internship [for this](http://en.wikipedia.org/wiki/Tore_Supra) where the physicists told the 2 other C++ devs and me that they wanted to be able to spot anomalies, dangerous stuffs etc on IR camera footages (and various sensor signals), and make an AI learn to do it automagically. That's literally all we had, with a C library letting us fetch the signals/footages from the servers. I could wrap up my part in 3 months thanks to some great libraries and by writing *really* generic stuffs. I just see the coding task as a similar situation.
In general the article is not so much about CQRS, but about a way to do validation transactionally over the effects of any monad. Turns out this can be done by only restricting the API to the applicative interface as control flow is then predictable ahead of running effects. As to what CQRS is, is in a way what I'm trying to figure out. It's usually described in OO style terminology, which makes it hard to get a clear understanding for me. My goal is to put together some kind of abstraction, e.g. a transformer stack that allow you to solve this type of problem. Haskell does not solve this by deafult, as the queries are over a read model that is usually a database, likely separate from the database you write to. You'd probably have some kind of eventual consistency between the two. Haskell by default does both effectful reads and writes in the IO monad. It's not the same as separating pure and impure code, although it makes sense to give a read model as a pure value, and a write model as a monad, at least in some part of the code.
Your example is actually reliant on the documentation you claim is unnecessary, else how would you have known you could use `F` in the first place? People don't want documentation for code to tell them how to put functions together, they want the documentation to tell them what the function with the cryptic German name is *actually supposed to do*. Furthermore the purpose of a tutorial is to: * give a person a basic understanding of how to use a library without having to first read the documentation for the whole thing. * help the new user avoid pitfalls that aren't obvious, such as the need to call `withSocketsDo` in network code, which you wouldn't be able to infer from the types. * demonstrate the capabilities of the library so that the new user doesn't end up reimplementing half of it because they don't realize it already supports what they want to do via some poorly named function in an opaque namespace. Anyway, the complaint isn't that Haskell lacks documentation, it's that the pages of functions with cryptic one line descriptions aren't really useful unless you already know how to use the library.
Apparently it's been flagged - it's now dead.
It's very hard to understand why that was flagged.
This kind approach works well with libraries that expose functions that are pure, total, and strongly coupled with their type representation and not so well for libraries that are impure, contain partial functions and are loosely typed. It's a testament to Haskell's design that this kind of approach toward program construction can work in some cases, but I don't think it comes close to replacing the need for documentation in most cases. 
Of course, there are at least two ways of interpreting that paragraph. One is correct, and one is, indeed as you say, wrong. It would probably be more helpful to elucidate your interpretation before saying it's wrong, otherwise people will disagree with you based on their own, equally unmentioned, interpretation, under which the blog post is correct.
My favorite way of thinking about CQRS is that it is a commandment to not seek isomorphism between your domain and your canonical data store (i.e. the way that something like persistent works) but instead to think of your domain as on one side being a pipeline for taking inputs and serializing them to your canonical store (including validation and the like) and on the other side converting your canonical to views which are presented (and possibly cached). So once you've separated those two sides you end up with a few nice things: (1) no desire to OO your database, (2) no need to conflate presentation and domain operation, (3) a nice upgrade pathway to SOA/event-sourcing/message passing (something that's extremely nice in Haskell since you can start to think of everything as being a big fold or pipe), and (4) a nice justification for Mongo-alikes (caching your view data).
Agreed--though I find that for me the gotcha is that I think of the specific case that I want to deal with when the spiffy library routine is far more general. Case in point: doing an exercise in an online Haskell course about insisting that a password meet some constraints (having upper and lower case characters, digits, and not being too short), I thought I'd try to write something more general that I could feed a [String -&gt; Bool] and ask whether they all return True on the proposed password. I wrote a sort of dual of map, foo :: [a -&gt; b] -&gt; a -&gt; [b] foo fs x = [f x | f &lt;- fs] after looking around on Hoogle and not finding what I wanted. The problem was that I wasn't general enough--I wrote a very special case of sequence. I'd never have known had I not noticed a blog post that applied sequence in the very way I needed.
Type signatures help a lot once you get fully submerged coding against a library you don't fully grok yet, but in my experience the barrier to entry for a Haskell library is figuring out the entry points. I contend that for quite a few Haskell libraries the activation energy required is much too high and is going to turn off a lot of people, but is a very remediable problem. For instance kmett's [reducers](https://github.com/ekmett/reducers) library is a great example of a very useful library that is not at all intuitive where to actually start with. After spending a two hours reverse engineering it, I can come up with 10 line example of usage that could have convinced me a) that the library was suited for my task b) saved me a few hours of reading through code that wasn't relevant to my task. If I wasn't as motivated as I was to learn the library I would have given up immediately and probably rolled my own solution that would have been much buggier than the ``reducers`` solution. 
Is this http://www.mega-nerd.com/erikd/Blog/CodeHacking/Haskell/telnet-conduit.html the one you were looking at? 
I'm happy to take patches that improve the state of documentation for pretty much any of my packages. `reducers` was written before I went on my document-all-the-things spree, and hasn't seen much love since.
&gt; I wrote a sort of dual of map, `foo` is expressible with `map`: foo fs x = map ($ x) fs EDIT: If you dislike understanding code, you could write foo :: [a -&gt; b] -&gt; a -&gt; [b] foo = flip $ (&lt;$&gt;) . flip ($) -- or with (&amp;) = flip ($), (&lt;&amp;&gt;) . (&amp;)
so much true here. Hindley-Milner is in prenex form and is therefore just intuitionistic propositional logic--which is *decidable*. The consequence of that is if you are going to "program" in HM and all you are doing is this gamified solving types...then you are doing it wrong--you could instead just use an automated tool like Djinn and safe yourself the effort. Okay, Haskell 98 isn't HM since it also has recursive types. If you limit yourselfs to only regular strictly positive data types though you still end up with something very ameniable to automation.
I call bullshit on this. There a ton of libraries on Hackage that only have functions documented, if you're lucky. Writing code in Haskell doesn't exclude writing helpful geting started docs with examples on how to use your library. Just look at the docs for reactive-banana vs netwire for example, and tell me that both are of equal ease to use. 
How come you guys aren't banning this troll? I don't think I ever remember him making a single constructive comment.
I'm perfectly happy to add documentation that reflects common usage scenarios. My comment was mostly alluding to the idea that a by-example culture, like you often find in, say, the PHP or javasacript world (though, the latter has started to mature out of that phase) tends to lead to a room full of followers and cargo cultists. I do agree though that not every library has a "deep meaning" and needs to be explored by being up to the elbows in its guts.
Yeah, I was calling you in particular or anything, and I do think that documentation has gotten way better even over the last few years. I just hate the attitude that types are enough documentation for library consumers.
I don't know which "interpretations" you're imagining. The paragraph is pretty straight forward to me.
I was trying a different bus route today and it was a bit long...
Well, not just. IPL lacks quantifiers. But the real point is that automation/unthinking proofs get you very little here. Consider, for instance, what you get by unthinkingly trying to give a function of type `(a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b`. Should be a fold, right? except `\f z xs -&gt; z` is valid. As is an infinite number of other solutions. That's is why this kind of naïve "let the types do the programming" kind of approach is so fundamentally misguided -- you only get a very rough shape for the goal, you don't get much else.
Nice tool! You might be interested in something similar that I wrote https://github.com/djv/VisualProf. It colors each expression in your code according to the amount of time the runtime spends in it. For your example it generates the following html: http://imgur.com/7hYgERI. It would be cool to hook your UI to the html that I generate.
Cool. I have a version of hpc-markup that is combined with hscolor to have both the syntax highlighting and the coverage information. It feels like it would be possible to generate HTML that combines all those rendering together. [https://github.com/noteed/covered](https://github.com/noteed/covered)
Neither of those seem to be even remotely what the quoted bit of text is saying.
Have you ever done CQRS in Haskell? Would love to hear some insights.
Perhaps your English comprehension is better than mine. Could you give me a clarification on what the quoted bit of text is saying, and what its implications are with respect to risks when using Haskell?
If anyone seriously disputes what you wrote here, then they are delusional. However, I think it's reasonable to take what people write about this topic with a big grain of salt and accept that it is possible to let "the types do the programming" in Haskell in a relative sense, but not in an absolute one.
Oh, I think you're really just being too suspicious :) They don't care about the precise libraries used, instead of forcing some arbitrary libraries/frameworks (that some applicants may not know). This just gives more freedom, it's not a bad thing. It's not a huge task, but that doesn't mean there's no room for showing how you think about programs. The only "clue" I can give here is, do what's better according to _you_, your thinking, your vision of good programming. That's what you want to show, and that's what they want to see.
reminds me of this blog article trying to type derivation http://winterkoninkje.dreamwidth.org/91282.html
The gist of it that I get is that we don't need good docs or explanations because the types are good enough. This is just completely and utterly wrong and backwards.
&gt; IPL lacks quantifiers So does HM. Prenex form *is* the lack of quantifiers. You have propositional variables in propositional logic. Being HM inhabited is the *exact* same thing as IPL validity. 
If that is the what the author meant then I agree with you. However, that is not what I understood the article to communicate.
I'd be interested to know what the version using Proxy would look like. I read before Proxy should be used instead of 'undefined', but I never saw how this would work 'for real'.
You said: &gt; we transform the type PgInt to the type String.
I've written a couple prototype applications using the style. I don't know that I've got particular tips; I'd be more likely to say that CQRS is a good philosophy for structuring an application when want to think of it as a pipeline instead of an object graph and that's likely to be a favorable perspective for anyone interested in functional programming. Most likely I'll have more to say here if I get a chance to continue growing some of these prototypes.
we are talking cross purposes. IPL validity is HM type inhabitability. There is an (implicit) forall in the notion of validity "forall truth assignments phi, [[v]]_phi is true" (okay, that is the classical notion of validity, but we also have the forall in Kripke semantics or the "uniform validity" of Constable et al). The STLC is HM for these purposes. Since in "STLC" you don't assume any particular structure for the base types. Base types=constants=prenex type variables. The only technical difference is let generalization, but that is standard practice in logic--see for example any traditional logic textbook such as Lemmon or Smullyan where let generalization is implicit in the notion of "theorem." Now haskell isn't HM. I mentioned recursive types, but it also has real quantifiers in the form of type classes having polymorphic components.
I'm talking nonsense then! I'll get that fixed, thanks :)
I have (at this point still unpublished) work showing that HM style quantification *does not* correspond to universal quantification in the setting of constructive classical logic. That is a proof of $\vdash T$ with free variable $t$ *does not* correspond to a proof of $\vdash \forall t.T$. The reasons for this are related to the value restriction. The point is that we can get HM style prenex quantification over "theorems" (which is to say top level definitions) yet this can't just be syntactic sugar for universals.
Well, if we have: data Proxy s = Proxy Then whenever we create a `Proxy` value, `s` can be inferred to be whatever makes sense. Thus if we only need a type somewhere, we can pass it a `Proxy` value, and the type is now useful but we didn't have to create questionable values. In `hint`, this is a trivial change, we'd just need: interpret :: String -&gt; Proxy a -&gt; m a Now even if `interpret` wants to look at that `Proxy` value, it's free to do so. However, at the moment it could potential try and evaluate `undefined`, which will explode all the place.
I'm not sure I buy this. I'd need to see the paper. But maybe. Classical logic is whacky anyway. :)
I tried Yesod and decided it was too much to learn for my simple needs and the time I had. But I wanted to use persistent, and had a lot of pain with it. The errors I had were difficult to understand. I finally understood that I had to use a few monad transformers like resourceT for example, but It was not clear to me what was going on under the hood. I did not take the time to make a proper answer to Zalora, but tried it as a excercise and I used simpler tools. I always prefer something I understand, even if it is a few more lines of code, or less elegant. Then I can learn new things peacefully, instead of being in a hurry to try to debug things that should work.
I really don't think we disagree. The point is that "IPL" as shows up in logic doesn't have things like `Nat`. It has "zero parameter conectives/type constructors" like "true" and "false", but any correspondence to PLs will be to HM. `P -&gt; P` is an IPL formula and has only one proof (up to normalization). We all the time claim that STLC=IPL but this is the lie. Or well, it is only true if you don't have structure for your base types. "IPL without propositional variables" is a hopeless system. It has only one model (ish). It is barely "intuitionistic": for any formula `P` the statement `P \/ ~P` is provable (since one of `P` or `~P` is provable). That isn't the system you want, and it isn't the system of "intuitionistic propositional logic" that shows up in logic books.
&gt; Then you should install template-haskell with Cabal: cabal install template-haskell Nonononono. You should use the version of `template-haskell` that comes with your GHC. Don't install template-haskell by hand!
Well, I followed a similar approach when I wrote `free-operational` (which, sadly, I just don't have time to maintain lately, hint hint), but the process of writing it helped me actually understand `Free` and `Coyoneda` at least at a very elementary level. `Free f a` is the free monad for the functor `f`. What makes it the "free monad" is that it obeys the monad laws and nothing but the monad laws. `Coyoneda f a`, as I understand it, is just the free `Functor` over `f :: * -&gt; *`. It obeys the functor laws, and nothing but the functor laws. I think that insight is enough to understand the implementation: data Coyoneda f a where Coyoneda :: (b -&gt; a) -&gt; f b -&gt; Coyoneda f a instance Functor (Coyoneda f) where fmap f (Coyoneda g v) = Coyoneda (f . g) v The `Coyoneda` constructor records some opaque `f b` thing that`fmap` never touches, and a function over that `b` type. If we think of this function as a "list" of functions assembled by composition, we can think of the `fmap` operation as prepending a function to the list. So basically, what `Coyoneda` does is "delay" the `fmap` operation by recording the function and an opaque `f b`, so that something can come later and put them together. In the case of `free-operational`, that "something" later is the instruction set interpreter functions, whose types generally look like `for all x. instr x -&gt; x`. You use types like this one as the `f` for `Coyoneda`: data TerminalI a where PutStrLn :: String -&gt; TerminalI () GetLine :: TerminalI String And one of the strategies for interpreting such programs is to supply a function that replaces `Terminal a` with some `Monad m =&gt; m a`. So you get things like: GetLine :: TerminalI String Coyoneda id GetLine :: Coyoneda TerminalI String fmap length (Coyoneda id GetLine) :: Coyoneda TerminalI Int == Coyoneda (length . id) GetLine == Coyoneda length GetLine Now, this `Coyoneda Terminal` functor can be treated in two ways. First, you can pattern match on `Coyoneda f instruction` to recover a concrete `TerminalI` instruction and a function. In the case of free monads, this function can be seen as the continuation of the program—the function that you invoke with the result of the instruction. This is how the `ProgramView` approach from `operational` is reimplemented in `free-operational`—by handing these two things to the operational-style interpreter. Or, second, you can supply a natural transformation from `Coyoneda TerminalI` into some `Monad m`: toIO :: forall x. TerminalI x -&gt; IO x toIO (PutStrLn str) = putStrLn str toIO GetLine = getLine Given such a function, the `interpret` from `free-operational` can turn a `Free (Coyoneda Terminal) a` into an `IO a`. Now, this relies on two things (and I hope I'm not wrong here): 1. Given `Functor f`, `Coyoneda f a` is isomorphic to just `f a`. `liftCoyoneda :: f a -&gt; Coyoneda f a` is the "upwards" direction, and `lowerCoyoneda :: Functor f =&gt; Coyoneda f a -&gt; f a` is the inverse—which works by `fmap`ping the `Coyoneda`'s function over the embedded "base" value. 2. Likewise, for any `Monad m`, `Free m a` is isomorphic to just `m a` (`liftF :: m a -&gt; Free m a`; `retract :: Monad m =&gt; Free m a -&gt; m a`). So what `interpret` does is: 1. Use an "instruction interpreter" like `toIO` (a natural transformation) to change `Free (Coyoneda TerminalI) a` into `Free (Coyoneda IO) a`. 2. Use `lowerYoneda` (an isomorphism) to change `Free (Coyoneda IO) a` into `Free IO a`. 3. Use `retract` (also an isomorphism) to change `Free IO a` into `IO a`. This sounds a bit like "type tetris," but I feel there is an important distinction, which is that the reasoning here appeals not just to types, but to certain functions being **natural transformations** or **isomorphisms** as well. Even if we don't understand the math that says that `Free IO a` and `IO a` are isomorphic, we can still exploit the knowledge that they are—and this gives us more than just the types do. Basically, the programs that we build out of our `TerminalI` instructions will combine in ways that respect the `Monad` laws, and the `interpret` function, being based on a natural transformations and an isomorphism, will tend to preserve the structure of these `TerminalI` programs (the `TerminalI x -&gt; IO x` function may collapse some distinctions.)
IPL can have `Nat`. The particular base types you have are really not the essential character of IPL. What makes it IPL is the nature of the connectives: no quantifiers, no values. And the intuitionistic part, but. Tho I suppose restricting to the 5 typical connectives is a common thing to do when talking about IPL. It doesn't really have any essential qualitative content tho, in my opinion. Maybe there is something to it tho. But there's no such thing as IPL-with-propositional variables, that's just not IPL. That's HM. &gt; It has only one model (ish). It's got at plenty of models, in a variety of different ways, from topological semantics to categorical semantics to kripke semantics. &gt; It is barely "intuitionistic": for any formula P the statement P \\/ ~P is provable (since one of P or ~P is provable). The same is essentially true of any logic. When you go meta, you can prove things that weren't provable before. Every intuitionistic logic exhibits this property, not just IPL, afaik. &gt; That isn't the system you want, and it isn't the system of "intuitionistic propositional logic" that shows up in logic books. We must be reading different books. I've never seen anything else. Type variables, in my experience, are always discussed as moving us into a new type theory that is no longer IPL.
What I really want is qualified exports: module Data.Map ( Map , qualified fromList , … ) where … module FooBar where import Data.Map as M fooBar :: Map fooBar = M.fromList [("foo", 1::Int), ("bar", 2)] If you want `fromList` unqualified, you would just import it explicitly. It could also give you the ability to re-export with qualification: module Blob ( qualified module Bow as B , qualified module Cow ) where import Bow import Cow module Spoon where import Blob f = B.borp Cow.empty { Cow.length = 42 } 
What is involved with audio stream processing?
Maybe only slightly related, but Monads are very similar structurally to the continuation passing style in how they model computation using one will typically make the other easier to use. A fully monadic factorial bears great semblance to one written in CPS: data Box a = Box a; instance Monad Box where { return = Box; (Box m) &gt;&gt;= f = f m; }; (&lt;=!) :: Int -&gt; Int -&gt; Box Bool; n &lt;=! m = return (n &lt;= m); (*!) :: Int -&gt; Int -&gt; Box Int; n *! m = return (n * m); (-!) :: Int -&gt; Int -&gt; Box Int; n -! m = return (n - m); factorial :: Int -&gt; Box Int; factorial n = (n &lt;=! 0) &gt;&gt;= (\isSmallerThanZero -&gt; if isSmallerThanZero then return 1 else (n -! 1)) &gt;&gt;= (\predOfN -&gt; (factorial predOfN) &gt;&gt;= (\factorialOfPredOfN -&gt; (n *! factorialOfPredOfN) &gt;&gt;= return))); Bears a pretty strong resemblance to: [define (&lt;=-cps x y cont) (cont (&lt;= x y))] [define (+-cps x y cont) (cont (+ x y))] [define (--cps x y cont) (cont (- x y))] [define (factorial n cont) (&lt;=-cps n 0 [lambda (smaller-than-zero?) [if smaller-than-zero? (cont 1) (--cps n 1 [lambda (pred-of-n) (factorial pred-of-n [lambda (factorial-of-pred-of-n) (*-cps n factorial-of-pred-of-n cont)])])]])] Both monads and CPS are a very similar way of turning "normally" written expressions inside out. The whole idea of normal return values to a surrounding context is something that was imported into computer programs to make them look like normal mathematical expressions but it actually doesn't serve to build a good understanding about a sequenced process like computation. 
This is pretty interesting. I found this translator from ACE to various languages: https://github.com/Attempto/ACE-in-GF It even has a smidge of Haskell code in there! Also this online parser (not Haskell as far as I know) where you can submit ACE sentence(s). It also spits out various representations of the input which is pretty interesting.
anyway. I'm out. I have work to do. 
&gt; ... Here the notion of "logical constant" corresponds perfectly to HM variables. Er.. no, it definitely doesn't. It corresponds perfectly to HM base types. You seem to have confused base types with variables, which are completely different things. Base types are fixed and well-defined. They are distinct, but typically aren't treated as having known or established truth values. HM variables are completely different. They're not strictly distinguished (can be unified), there are infinitely many of them in the type theory, etc. As a good example of why they are completely different, consider: you can have a base type `A`, and this is a wholly distinct thing from some other type, say, `B -&gt; C`. The two are just not equal. On the other hand, in HM you can have a variable `a`, which can perfectly well be unified with `B -&gt; C` providing that `a` hasn't already been unified with something else that conflicts. Base types *can* be seen as variables at a certain level, but it's at the level of the whole logic: the entire logic is parameterized by the base propositions. On the other hand, HM variables are not parameters, they are a part of the proof system from the ground up and serve a very distinct purpose. You're confusing two very different concepts, and it's important to distinguish them in your mind. HM is strictly more powerful than IPL. For one thing, HM with only `-&gt;` has all of the standard IPL connectives (true, false, conjunction, disjunction), plus all sorts of of fancier ones (nats, lists, trees, ...), by way of Church encodings, right out of the box with no additions. IPL has no church codings, and only has these fancier types by explicit modification of the system to include new propositions. &gt; The reason for the "ish". The point is that you don't have models which validate different sentences. Well, this isn't exactly true. You actually have two types of models: those that validate exactly the provable sentences and those that validate all sentences (including |) I don't know what this means. &gt; In IPL or IFOL it is not the case that P \/ ~P is provable for all formula P. Consider the case where P is a variable: p \/ ~p is not provable. This is an important distinction, and gets to the core of being "constructive". I'm not sure what the provability of an open proposition is supposed to tell us about constructivity or the intuitionist program. &gt; In addition to ... Again, you're conflating parameters of the logic with variable propositions within the logic. HM wouldn't've been as big a deal as it was if it were just IPL -- people know how handle IPL for decades. HM was an important *novel* type theory that *did new things*.
It's not about "wasting time" on people with the wrong style. We review all applications ourselves. We figured out that CV had almost no correlation with expertise, for example there's several college dropouts in the team. With the test, we check that people are able to write production Haskell at a sufficient level of experience. That's it. The guys reviewing are jekor and one of the creators of Nikki and the Robots, both of which will review everybody. They are also working full time and we got hammered by over 80 applications (currently at a rate of 10 a week). Reviewing code takes time, you have to load it in your head and try and understand the way people think. Perhaps an hour per candidate at least, if you want to be fair. 10x1h = 10h/week, almost a quarter of your week spend reviewing code instead of working. This is a high cost to the business but hiring is probably the most important predictor of the success of your team. Since we're hiring people to write code, we're testing their ability to write code. Sometimes, someone has written tons of great open source code and we don't need the task. These cases are rare - perhaps 30% of the people we made offers to. It's not a perfect system - in part for reasons you hint at - but it's as good and fair as we could come up with. Happy to hear any other suggestions.
No, I said that type signatures *are* really good docs. About 80% of the time, they're enough for me to figure out how to use a library, even if I don't understand all the underlying details. And isn't the point of a library to insulate you from those underlying details anyway?
Yeah, I noticed you didn't have time to maintain it when I sent you a pull request fixing the infinite loop in interpret and it didn't get any response. :) As far as the details of your post - I've actually figured out most of them by now. I didn't stop thinking about it when I got it working.
I said I was going to stop, but... &gt;Er.. no, it definitely doesn't. It corresponds perfectly to HM base types. You seem to have confused base types with variables, which are completely different things. Base types are fixed and well-defined. They are distinct, but typically aren't treated as having known or established truth values. HM variables are completely different. They're not strictly distinguished (can be unified), there are infinitely many of them in the type theory, etc. As a good example of why they are completely different, consider: you can have a base type A, and this is a wholly distinct thing from some other type, say, B -&gt; C. The two are just not equal. On the other hand, in HM you can have a variable a, which can perfectly well be unified with B -&gt; C providing that a hasn't already been unified with something else that conflicts. &gt;Base types can be seen as variables at a certain level, but it's at the level of the whole logic: the entire logic is parameterized by the base propositions. On the other hand, HM variables are not parameters, they are a part of the proof system from the ground up and serve a very distinct purpose. Right no that is exactly what I'm saying. HM (modulo let which I will get to) is exactly the system variables in this sense. STLC only is if you have base types that YOU DON'T KNOW ANYTHING ABOUT. The **entire** notion of "validity" is about variables. It says that given any interrpretation of the symbols the statement holds. Provablity coresponds to validity. The point of let generalization is that it is just standard practice to be able to reuse theorems with different valuations of the variables. This has been standard practice in propositional logic since long before HM was invented. The point is that the distinction doesn't hold water...well it might. But my original claim is that given an HM type, if I interpret the type variables as "constants" in this sense then I get out a formula, the provability of which coresponds to the inhabitation of the HM type. &gt; You're confusing two very different concepts, and it's important to distinguish them in your mind. HM is strictly more powerful than IPL. For one thing, HM with only -&gt; has all of the standard IPL connectives (true, false, conjunction, disjunction), plus all sorts of of fancier ones (nats, lists, trees, ...), by way of Church encodings, right out of the box with no additions. IPL has no church codings, and only has these fancier types by explicit modification of the system to include new propositions. &gt; Wait, what are you talking about? System F does all that, but HM? HM does not have Church encodings! &gt;I don't know what this means. Sorry. Anything in particular? &gt; I'm not sure what the provability of an open proposition is supposed to tell us about constructivity or the intuitionist program. It tells us that the proof of a statement does not depend on the specific instantiation. And so `P \/ ~P` not being provable in general is a fact about intuitionistic logics not true about classical ones. I think this is clearest in the "uniform validity" interpretation of [Constable and Bickford ](http://arxiv.org/pdf/1110.1614v3.pdf). Here is their explanation &gt; Using evidence semantics, we then introduce the idea of uniform validity, a concept central to our results and one that is also classically meaningful. This concept provides an eﬀective tool for semantics because we can establish uniform validity by exhibiting a single polymorphic object. For example, the propositional formula A ⇒ A is uniformly valid exactly when there is an object in the intersection of all evidence types for this formula for each possible choice of A among the type of propositions, P. We write this intersection as ∀[A : P].A ⇒ A or as T A : P.A ⇒ A. In this case, given the extensional equality of functions, the polymorphic identity function λ(x.x) is the one and only object in the intersection. So the witness for uniform validity like the witness for provability, can be provided by a single object.7 Truth tables do this for classical propositional logic. The idea is that a formula is *uniformly valid* iff it has a realizer that works in every interpretation. This is the only complete meaning of validity with respect to the computational semantics. &gt;Again, you're conflating parameters of the logic with variable propositions within the logic. HM wouldn't've been as big a deal as it was if it were just IPL -- people know how handle IPL for decades. HM was an important novel type theory that did new things. Several of the examples I cite use the phrase "propositional variable." Even the ones that call them "Constants" have an infinite number of them...and discuss the notion of validity! HM is a major innovation. But not in the way you claim. "Propositional Logic" was studied extensively long before the "HM type system" had been invented. The biggest innovation of HM is algorithm W. I'm really going to stop commenting. You are free to not consider HM "propositional." The point is that the classic results about decidability apply since they were actually in a setting identical to HM.
&gt; Wait, what are you talking about? System F does all that, but HM? HM does not have Church encodings! HM most certainly does. http://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system The Church nat type is a perfectly valid HM polytype: `forall a. a -&gt; (a -&gt; a) -&gt; a` I think you're somehow very confused about what the Hindley-Milner type system is. I don't think we're going to benefit by going round and round in circles again.
you can't write a function that takes a Nat and returns a Nat. Or a function that takes a function from nats to nats and produces a nat. This is not "having the nat type" &gt;I think you're somehow very confused about what the Hindley-Milner type system is. I don't think we're going to benefit by going round and round in circles again. Honestly I would say the same.
And you learn the vocabulary in the process, which makes further explanations meaningful
&gt; you can't write a function that takes a Nat and returns a Nat. Or a function that takes a function from nats to nats and produces a nat. This is not "having the nat type" That much is certainly true. &gt; Honestly I would say the same. It's probably true, too! But no one ever learned anything by keeping their mouth shut. :)
I once tried to make an audio program, but I never finished. But my plan was to create an unboxed IOArray of integers (IOUArray Int Int) as the buffer containing the waveform. This was stored in an MVar so it could be read by one thread and written by another. One thread would basically read the MVar, convert the array to a bytestream suitable for the audio device, and dump the bytestream to the file descriptor of the audio device using synchronous file IO as often as it could. It relied on the synchronous IO to block this thread until the waveform had been output and needed more input. After every write, the elements of the waveform array were all set to zero. That way, if no audio events occurred by the time the next write cycle began, it would simply dump a bunch of zeros to the audio device. To convert the array to a byte stream, I used the "Data.Binary.Put" which can easily create a stream of 32-bit integer big-endian encoded bytes which was written to the file descriptor for the audio device I was using. I then had one thread for handling audio events. Whenever an audio event occurred, it would immediately try to write the waveform to the array using 'Control.Concurrent.MVar.modifyMVar'. This basically involved taking all the audio events that had been queued and summing their waveforms together into the array. I noted the peak amplitude of the sum of the waveforms and then divided each array element by that peak amplitude to do a normalizing pass before returning from modifyMVar.
I have to say, there is a **lot** of truth to this article. When I was first learning Haskell, I was able to intuit that the type system was key to understanding how to use a library. Now as a more experienced programmer I always look at the Haddock-generated documentation first to get a sense of how to use the library, and the types alone are so useful in understanding that. But I did have to write a lot of programs before it finally occurred to me how to make deductions about how to use APIs just based on their types. Without experience I could see a type and think, "OK, so this function is used with that function." But I would get lost seeing code that made heavy use of Monoid, Monad, MonadPlus, Applicative, Functor, or any functions that let you compose other functions together. Like now, I can see "OK, that's both a Monad and a MonadPlus, so I can use mplus and mzero," and I have a strong intuition about what mzero is for (like doing backtracking), so I already have a general idea of how that Monad is intended to be used. A beginner doesn't know this. I can see, "Oh, that's a Monoid," so I know it is intended to be used with folds with mappend and mconcat, and I will also start scanning the documentation for functions that of the form: forall a . Monoid a =&gt; a -&gt; a -&gt; a because I know functions like this can be used in place of mappend in fold operations, which lets you easily build very complex data structures. A beginner doesn't know how to think like that. Beginners don't know that in Haskell, one of the most common ways to solve problems is to model it as a domain specific language, and define maps and folds over the DSL. Without knowing this, you can look at the types of all of the functions but you will still try to think of the solution in a procedural way, where you get data, analyze it modify it, and then put the data back. You just don't know how to really compose functions, even if you know about functions like (.) and ($) and (&gt;&gt;=) and (&gt;=&gt;), so you do everything in the IO monad with too many "let" statements. So for getting beginners used to thinking like this, I think tutorials are important. But experienced programmers don't need nearly as much documentation, especially for smaller libraries. 
Yes.
&gt; PostgreSQL has an open universe of types, as types can be defined at runtime. However, we’ll keep things a little simpler by considering a closed universe of the following types how can we extend this to an open universe? i guess, it has something to do with the expression problem.
It's been pointed out to me in times past from ekmett and cmcann (iirc) that `Proxy a` should generalized to `proxy a`: &gt; interpret :: String -&gt; proxy a -&gt; m a I've also found it useful and do this now as well. Also helps in a pinch if you're on an old GHC and don't have tagged installed... `[] :: [a]` and `Nothing :: Maybe a` work just as well though it's not a clear style.
Multiplexing a stream, running is through DSPs (filters, effects, etc.), combining streams, and sinking them to ALSA or Jack[1]. Puredata[2] is a graphical programming language that is specifically tailored for stream processing. 1: http://hackage.haskell.org/package/jack 2: http://www.youtube.com/watch?v=-y2SP2B6NxY
Thanks!
It seems that `lens` the most popular lens library is flexible enough to enable this. Let's first examine a simple case like the one you described above: {-# LANGUAGE RankNTypes #-} import Control.Lens data Id = Id { _id :: forall a. a -&gt; a } -- Lens for Id idL f (Id poly) = fmap Id (f poly) willNotCompile = view idL You will immediately see even a simple combinator such as `view` will fail to type check when applied to the lens we just defined. If you run the above code through GHC the very last definition will be rejected as there is no way to unify the type (a -&gt; a) with the type (forall a. a -&gt; a). Obviously this is because the former is bound by the outermost implicit universal quantifier, while (forall a. a -&gt; a) has its own binder forcing the argument to be a polymorphic function. If we inspect the type of idL we will see that GHC infers the type: idL :: Functor f =&gt; ((a -&gt; a) -&gt; f (forall a1. a1 -&gt; a1)) -&gt; Id -&gt; f Id The real reason unification is failing here is that we haven't properly annotated the type and GHC is inferring a quantifier in the wrong place. What we really want is for the first argument have to be universally quantified as well. In order to do that we are going to need ImpredicativeTypes, as RankNTypes is not powerful enough to allow such a signature. If we follow this to the logical conclusion we will turn on ImpredicativeTypes and see what happens. {-# LANGUAGE ImpredicativeTypes, RankNTypes #-} import Control.Lens data Id = Id { _id :: forall a. a -&gt; a } idL :: Lens' Id (forall a. a -&gt; a) idL f (Id poly) = fmap Id (f poly) -- willNowCompile :: a -&gt; a signature doesn't work here even though GHC -- infers `willNowCompile :: a -&gt; a` willNowCompile = (view idL) (Id $ \x -&gt; x) That seems to do the trick. There are a few caveats though ImpredicativeTypes wreaks havoc with inference possibly reducing the usefulness of this approach. It is also important to note that in this case we just produce a function (forall a. a -&gt; a) which is isomorphic to (a -&gt; a) (there is some issue with annotating the type here, but I did not figure it was worth investigating). Overall I'm not sure how useful this would be in practice but I've demonstrated you can at least type check such a definition. 
What do **you** do (not interested in the job)? The biggest reason for me not using Haskell for anything more than toys is Cabal and breaking dependency problems like that? You can use containers or whatever to help I guess, but that's so much more painful than say maven.
Well we don't have to construct the values - we can hand that off to PostgreSQL. Every type has a constructor from a String representation, so as long as we can construct that String, then we can make the type itself. I'm thinking that maybe I can make a module that is used inside the interpreter session that defines a type class to ease the pain here.
Currently, it's done every time the function is called - but it won't be like that in the future. PostgreSQL has support for storing arbitrary data in some state that persists over a query, so I imagine I will interpret the function once and then keep hold of a function pointer.
I think this would be a very interesting field to explore. I need to convince my sound guy to learn haskell so we can start exploring this stuff. Is it worth to take a Look at http://haskell.cs.yale.edu/euterpea/haskell-school-of-music/ or is that more aimed at music generation instead of music processing?
Oh, I was making more of a philosophical point: those who make it tend not to give up that easily (here, or anywhere, I think). This may have been a bit unfair to the OP as he might just have been asking the community for help, but I thought since this is a thread on a job posting... Our first developer had Amazon Redshift in production in 2 hours, and he had never seen it before. Part of the job is reading documentation, inferring from your existing knowledge, seeking new knowledge and finding ways round problems until you can deliver the product. Unlike, say, data science in R, few people will have come across your specific problem before and googling a solution might not work out. I do think this makes for a more exciting team to work in.
Do you use GPipe? I’m writing my own engine and writing some articles for my blog about my engine and 3D applications in Haskell.
You work for the hiring company though, right? I'm curious what budding Haskell based companies are doing to solve this. 
&gt; Looking at your git code, it appears that the strategy in both conduit and pipes is to duplicate the stream and send it both directions (a unix "tee"), is that correct? If so, are there any efficiency concerns here? Yes, that's correct, and yes, there could be. As usual, it's difficult to give a performance answer without actually profiling code, but my gut reaction is that the I/O itself will far eclipse any overhead from the streaming layer. An alternative would be to drop down a layer in the abstraction and essentially reimplement `ZipConduit` for your use case using the raw constructors. It could work, but given that STM is probably the right answer here anyway, I don't think it's worth exploring much further. I'm not sure I entirely understand your new model, but let me try explaining one other thing. Both libraries (pipes and conduit) work on the concept of having a single incoming stream and a single outgoing stream. Things like `ZipConduit` are fancy ways of composing simpler components into more complex ones, but you're still stuck with a single stream at the end of the day. You want to deal with two different input sources. This *requires* using two threads (or some other concurrency abstraction, like direct polling). By using STM, you can take those two input sources and put them into a single channel, so that conduit (or pipes) can read from just a single source containing both streams of data interleaved. Whether this is the best approach in this case is not something I feel like I can answer, since I don't know the details of telnet.
it might be nice to hook into gstreamer, as that's a nicely done multimedia streaming solution.
&gt;Yes, that's correct, and yes, there could be. As usual, it's difficult to give a performance answer without actually profiling code, but my gut reaction is that the I/O itself will far eclipse any overhead from the streaming layer. Ok, fair enough. I just wanted to clarify. &gt;since I don't know the details of telnet. Ok, I'll just give a quick explanation that might make some of this more clear: telnet is a clear text TCP protocol with the exception of telnet options. Negotiation of various properties happen right in the stream. The way this is done is that when a telnet server or client wish to send an option they will send 255 followed by one of: DO, DONT, WILL, WONT, followed by the option number (there are also sub-options that are more complex and variable size). This is mixed right in with the text of the communication. Either side of the communication can negotiate on a variable. Negotiation starts with either a DO or a DONT and is answered by a WILL or a WONT and this is acknowledge by a WILL or a WONT (e.g. server: DO:line-level-processing, client: WONT:l-l-p, server: WONT:l-l-p == line-level-processing offered by server, rejected by client, rejection acknowledged by server). And that's pretty much all there is to it. But what this means is, when handling a telnet connection some of the data must be displayed to the user and some must be responded to by the system. The important thing to note here is we can't cheat and e.g. defer option processing until the user has output because the server will often stop sending data until certain options are negotiated (meaning the user would see nothing). This is why a telnet client needs a way to shoot off an answer in the middle of processing a stream. &gt;I'm not sure I entirely understand your new model incoming data | | | (telnet options manager) / \ / \ / \ (User) (Unprocessed output) \ / \ / \ / (Stream joiner) | | | network output stream This may be what you had already envisioned but for me the logic was moved around substantially from this more simplified approach. &gt;but you're still stuck with a single stream at the end of the day. Ok, so it sounds like my plan of having the "user" conduit block on user input is not a realistic approach. So if I've understood your suggestion, then model becomes: thread 1: incoming data | | | (telnet options manager) / \ / \ / \ (User) (*PO*) (User) | | (*PO*) (thread 2: PO reader) | | | network output stream Yes, this model probably works much better with having an elaborate graphical client in any case. The only potential issue I see here is that thread 1 above is pointing to the same socket that thread 2 below has. It is just that thread 1 should only read and thread 2 should only write. Is all this thread safe? I would also be a bit concerned about ordering issues. Data coming from User can freely be split up, but data coming from the telnet options manager cannot (e.g. if the user sends "hello world!" right as the telnet manager sends [255:WONT:terminal-colors], the former can be split at will, the latter must not be split). I suppose this issue could be addressed by having the telnet options manager write any option output to the socket directly but then I'd be even more concerned about thread safety since two threads could be simultaneously writing to the same socket.
I write in "point free" style a lot, not as a matter of principle but as a matter of practicality when I think it makes the code clearer. The second example is unnecessarily unclear in my opinion. I would write it as follows mf :: (b -&gt; Bool) -&gt; (a -&gt; b) -&gt; [a] -&gt; [b] mf p f = filter p . map f Firstly note that *as always* type signatures make everything much, much clearer. Secondly I have no hesitation in shortening the predicate to `p` and the map function to `f`. This is very standard usage. I think the `foldr` example is much better, though I would still probably write concat :: [[a]] -&gt; [a] concat = foldr (++) [] concatMap :: (a -&gt; [b]) -&gt; [a] -&gt; [b] concatMap f l = concat (map f l) -- Under some circumstances I might use .: from Data.Composition -- concatMap = concat .: map (&gt;&gt;=) = flip concatMap Again, type signatures help hugely. Naming the functions `concat` and `concatMap` makes it clearer what they do. I wouldn't write `foldr ((++) . k) [] m` unless there was a compelling performance reason. 
Here's a README: https://github.com/silkapp/bumper 
Your `mf` example looks like a parody on point-"less" programming. I don't think anyone would seriously recommend that style in Haskell. The `&gt;&gt;=` example is more interesting. When I was new to Haskell (coming from an ML background), I would have absolutely preferred the explicit recursion. However, I now prefer the fold version and believe that it reads extremely well. The `foldr` makes the nature of the recursion immediately obvious, and understanding it all becomes as simple (or hard) as understanding `(++) . k`. I feel that the composition of a binary operator with a unary function is so common as a folding function that it becomes natural quickly.
Expressing functions as compositions of other functions is a wonderful part of functional programming. But that `mf` example is obscurantist, abusing the `(.)` operator in a way that contributes nothing to clarity. There are tools that automatically generate such nonsense given the more sensible pointful code as input. Some Haskell programmers are excited by this kind of occult [Voynich-style](http://en.wikipedia.org/wiki/Voynich_manuscript) line noise; it can certainly be fun. But it's an amusement, in the vein of [Perl golf](http://c2.com/cgi/wiki?PerlGolf), not a serious way to write programs.
I agree with you that you version is more easily read in isolation. However, lots and lots of functions use a similar pattern: the empty list goes to the empty list, and otherwise you do something with the first element of the list, recurse on the rest of the list, and then join the two pieces together. This idiom is so common that reading (or writing!) it over and over turns it into boilerplate code that just gets in the way. That's why higher order functions are so useful: they can abstract these idioms away. Getting rid of boiler plate makes code easier to read, faster to write, and easier to maintain. I personally have trouble when `(.)` is applied to non-unary functions. I read it as "of", as in "length of filter (greater than 5) of map countFeatures" or whatever. As the others have said, I think better ways of phrasing these are m &gt;&gt;= k = concat . map k $ m -- Recall that concat is simply (foldr (++) []) mf criteria operator = filter criteria . map operator Note that `mf` is a _terrible_ name for a function. A name like `mapFilter` explains better what it's actually doing. Naming things properly can really help with readability! I admit I'm having a lot of trouble understanding that last line with all the dots, and this goes back to `(.)` being applied to non-unary functions. In particular, I've never grokked `(. (.) .)` despite trying pretty hard. Point-free programming has the potential to make code easier to read by abstracting away the common idioms, but it can also obfuscate things if they become too abstract.
Great article, thanks! I almost skipped it, because it looked like it's about PostgreSQL, which I am not particularly interested it. But, it turned out to be very interesting and educational! 
What is `ts` here? &gt; SPgArrow :: SPgType t -&gt; SPgFunction ts -&gt; SPgFunction ('PgArrow t ts) 
a type variable that is the tail of the list.
Your mf isn't really fully point-free, right? Since you're referring explicitly to arguments p and f? Which isn't to say that's wrong - I think it's quite readable and I think it benefits from dropping the one point it drops.
Point-free for it's own sake isn't the issue. No one advocates doing that for real programming; it's only a game. Combinator-style programming means expressing functions as a series of steps using function composition. That is a very worthwhile technique in my opinion. It often makes code much more clear and readable. However, combinator-style code does often end up being point-free, so it does take Haskell beginners coming from traditional programming languages a little while to get used to it. Neither of your examples are really combinator style. In your first example, I like the first way you wrote it better than the second, but the main difference between them isn't really point-free or combinator style. It's the use of `foldr` instead of explicit recursion, which also takes a little getting used to. Here's a better example: -- | Express an integer in radix b. toRadix :: Integer -&gt; Integer -&gt; [Integer] toRadix b = map (`mod` b) . takeWhile (&gt; 0) . iterate (`div` b) It reads as: Divide repeatedly by `b`, take only the initial results that are positive, then take each of those results modulo `b`. I think that makes the algorithm quite clear. It does take a little thinking to see why that works if you haven't seen the trick before, but using an extra variable would just add extra noise and wouldn't help make the code more understandable.
Like OP did. Which is very hard to read. I think /u/tomejaguar strikes a nice balance.
&gt; Your mf isn't really fully point-free, right? `concatMap` isn't either (in fact, it has more points than the Prelude's own implementation of `foldr ((++) . f) []`)
Look at your right hand site (within this webpage) and you'll see quite a lot of useful links.
Seems fine. Only leaf modules should be defining orphans anyway, if at all. 
&gt; I think it benefits from dropping the one point it drops. Well, that's the gateway in. First you get comfortable just with making Haskell go zoom. Then you start running HLint over your code, and you let it tell you where you can "eta reduce" your code, so you start removing the variables that appear on the right end of both sides of the equation. Very simple, no problem. After a few rounds of that, you feel comfortable with it, and end up going a bit more and more pointfree over time naturally. Hopefully you end up more on tomejaguar's `mf` implementation and less on the dogmatic side, as it certainly can be taken too far. I don't really think "point-free" is something you sit down to "learn" deliberately. It's something you just pick up over time. HLint can help you along quite nicely.
&gt; mf = (. map) . (.) . filter This looks to me like the result of asking lambdabot to convert something into point-free style, [e.g.](http://www.haskell.org/haskellwiki/Pointfree#Tool_support)
Another way to make it point free which I think is nicer than OP's (but still not as readable as tomejaguar's less point free version): curry $ uncurry (.) &lt;&lt;&lt; filter *** map
Marginally better, but I'd still not touch such code with a ten foot pole.
Ah, this topic again. I've have someon previously interested in adding a historgram function to jp-repa. If you'd like to see the task through and send in a patch then I'd likely accept it. See his attempts here: https://gist.github.com/hirschenberger/4079202 EDIT: and the related github ticket is here: https://github.com/TomMD/JuicyPixels-repa/issues/2#issuecomment-10357726
Seems reasonable. It can't cause much breakage.
 The point(heh) isn't necessarily to make it completely 'point-free', but perhaps rather to express it clearly that a function is the composition of two(or more) of some other functions. Looking at it that way, and having it in mind when writing code can make some of your functions much clearer. doThingM x y list = do -- .. lots of code here using both x and y .. -- on the list argument You then figure out that what `doThingM` is doing can be refactored out into a `doThing` and then you have something like doThingM x y = mapM (doThing x . doOtherThing y) where doThing x' = -- uses x -- ... doThing y' = -- uses y -- ... In this latter example, we've eta-reduced the list, and we've made it much clearer what the function is actually doing. We don't really need to pass x and y to `doThing` or `doOtherThing`, because they are still in scope in the where clause, but doing so makes it much clearer what is happening. Someone glancing over your code immediately see that it's a monadic function(thus the M) that does doThing and doOtherThing on each element in a list using the x and y parameters respectively. The first example would've required reading the definition of the function to understand it properly(this is all assuming we are using sensible function names). 
The burden is actually not nearly as onerous as it used to be with the many great new Cabal features that allow manually tweaking cabal's build plan in various ways. But yes, I agree, this is a good change to PVP.
If you deal with ByteStrings or Builders as a chunking mechanism (which I strongly suggest you do), the chunks containing the commands won't be split up. I think you have the right idea on how to implement this.
Thank you. ImpredicativeTypes is probably too heavy handed a tool though, but I see why it's required in this case.
It'd be great to see a video of this in action!
nah, i'm oldschool, just write ugly imperative code mostly, plus some simple shaders manually. But you know, haskell is the best imperative language out there :) (Also, most of the time i targeted the infamous intel gma950, not exactly the high-end of gpus - i guess gpipe would fail spectacularly on such hardware. Now i updated to intel hd4000, feels much better but still low-end. Haven't had time or energy to finish anything in the last few years, unfortunately) 
Start with a book on learning haskell, e.g. http://learnyouahaskell.com/, and start reading and coding. 
Right. *Don't* blindly apply every eta-reduction just so HLint doesn't complain. *Do* look at them and think about whether that can make things clearer and/or suggest places to abstract. I think a general rule is that when you're conceptually building something by combining a few building blocks in simple ways, just mention the building blocks. If you're not - can you be?
I am pretty sure the `Monad []` instance is nicer for optimization than the pattern-matching style, since it allows rules on `foldr` to fire. Although I'd probably use `concatMap` instead, which should reduce to something at least as optimized.
I always recommend Hutton's [book] (http://www.cs.nott.ac.uk/~gmh/cover-med.jpg). Great style and good for getting used to the language without pain but without cutting corners. You probably also want to check out [Real World Haskell](http://book.realworldhaskell.org/). I have been using Haskell daily for at least couple of years and I still refer to it, just today in fact. The other thing I try to do is read code on [hackage](http://hackage.haskell.org). I learned a ton reading the code for parser libraries. There are a number of other good resources, so if you find one you like then go for it. Most importantly, type ghci at the prompt and start entering code. 
[This](http://www.seas.upenn.edu/~cis194/lectures.html) is my favourite link for getting people started. It links to Learn You A Haskell and Real World Haskell were appropriate, it has awesome homework exercises and I think it's pitched at a pretty nice level.
Oops! Yeah, I meant `(+++)`. I'll fix it.
I just learned a lot of phrases. : )
&gt; Note that mf is a terrible name for a function. It's quoted from the Wikipedia. And it's weird that argument names are meaningful, but function name so terrible.
I do use point-free style a lot. The main pro of doing so is that it helps simplify and elucidate code. The key trick here is that point-free style helps you think in terms of manipulating entire structures in a holistic way, rather than getting bogged down in the components of that structure. You see this same key trick used in other places too. For example, in category theory we focus on whole "objects" (e.g., sets) rather than the parts that make them up (e.g., elements); this is a big part of why I prefer category theory over set theory. For another example, when we use folds and unfolds this forces us to follow the inductive structure of our data; this is nice because it naturally eliminates a lot of corner cases, and therefore tends to simplify and streamline algorithms. As everyone else has said, simplifying code is the "right" reason to go point-free. However, this goal is generally at odds with the full-blown combinator style of programming where you remove absolutely all lambda abstractions. Thus, `mf p f = filter p . map f` is good point-free style whereas `mf = (. map) . (.) . filter` is combinator madness.
Is that better/different than the PVP?
I think asking about resources for learning Haskell is completely appropriate for this reddit. I wouldn't expect this post to get a lot of upvotes. But downvotes? WhY?
I view folds as still somewhat low level, intermediate between raw recursion and higher-level combinators. For example, as /u/tomejaguar points out, the `foldr` version of bind for lists is indeed nicer that the explicit recursion version, but really the best way to write it is: (&gt;&gt;=) = flip concatMap
@jaspervdj, How portable is this to output from other tools?
The tough aspect in real-time signal processing will be predicting the GC behaviour. I'd say the current state of the art in Haskell clearly suggests the use of something like conduits/pipes. Anyways the only alternative is to use no stream-processing abstraction at all and work on a lower level (explicit strict I/O).
If you don't have a hard real time component it should be fine. If you have a hard real time component you'll probably want to design a DSL that let's you compile a description into some faster rep. Eg generating straightline code via llvm 
ImpredicativeTypes is the only tool that can be used. Sadly, they don't work most of the time and support left in for them is sketchy at best. You'll have problems generalizing this to more complex examples. In general higher rank or existential members aren't supportable directly by lens. Consider the simpler existential case: data Dynamic where Dynamic :: Typeable a =&gt; a -&gt; Dynamic As you don't know 'a' you can't have a lens, `view` doesn't make sense. We can offer you a `Prism` though, which is what we do in `Data.Dynamic.Lens`, though the way we do so relies on the `Typeable` instance being available and isn't a general approach to existentials.
That's an awful lot of boilerplate code though, isn't it? It seems backwards to me to have to manually unroll things at the top level of my program, as opposed to hand-optimizing critical inner loops. Maybe it's still the best option right now, but is it at least reasonable for me to find it gross?
I feel it's definitely pretty gross, but usually only something to be done when your monad stack has reached a certain level of fixedness. If you've always been using the stack abstractly then you can make flattening and CPS transformation happen without changing the API or semantics at all. If you must make a major change later then you can do so again without losing backwards compatibility. I would suggest keeping the equivalent mtl stack around somewhere, even if just in comments, because CPS code can be quite tough to read and it's fairly easy to use that as a starting point for later extensions of the base monad. You can also get some fluidity by keeping the monad transformer interface on your primary stack and continuing to add new layers as you extend functionality. These extra layers can be absorbed into your primary, abstract "stack" later if needed.
I'm new to Haskell so I'm sure I'm being naive here, but it seems like a code smell to have that many monad layers in use all at once. One of the biggest advantages of Haskell seems to be the ease of writing and composing small modular pieces. Is it not possible to break this code up in such a way that the monads aren't all needed at once?
These monad transformers are the small modular pieces. It's how you model the state and effects of your program without turning it into a monolith. The more layers you have the cleaner your abstraction is, in some sense. Subsystems can have their own limited transformer stacks that only reuse the parts they need. But if you switch contexts a lot this may hurt performance as well since you need to wrap and unwrap the stacks. Another approach is [extensible effects](http://hackage.haskell.org/package/extensible-effects) which I don't really know much about :-) 
I would recommend putting the Xen hypervisor inside of something easier to provision like a vagrant box. Debian has a good guide to creating a Xen host: https://wiki.debian.org/Xen.
If you're looking for graduate programs, your best bet is to look through the last few years of published papers in the three big programming languages conferences: ICFP, POPL, and PLDI. Look at the universities that the authors are affiliated with. These are going to be good ones to look at applying to.
It would help if you shared a bit more details about your use case, depending on the case maybe monad transformers may not be the best solution. If you end up needing a lot of mutable variables or are working with impure data structures then there are other cases where using a single level of ST, IO, or even explicit state threading can be much more efficient. There's also quite a few cases where it's easy to introduce unnecessarily laziness ( especially with WriterT ) that we may be able to catch before you start profiling.
Looks a lot like Free [] String, where the normal rose tree equivalent would be Cofree [] String.
Galois uses it, from my understanding, to prototype operating system designs and many other things. But it could be used for a lot of things. I've long had an interest in "Xen applications", similarly to what Mirage does for OCaml - you have a minimal substrate to talk to the hypervisor and set up your domain, and it kicks off your code which does all the "important stuff" - your own filesystem, memory manager, networking stack. It is reminiscient of the Exokernel school of thought: build a set of libraries for *creating* an OS, and let developers piece them together to make their own platforms. That's how the Mirage project works. There are other aspects to it. One is that by utilizing a hypervisor with paravirtualization like Xen, you can do passthru to efficiently talk to the host kernel in certain ways, as opposed to full system emulation of every device. So you can leverage the performance and design improvements of talking to a dedicated OS stack - without the overhead of the stack *itself* on your application (no kernel context switches, no system call overhead, etc). Talk over that fast block device interface offered to you, that is backed by a disk - but just write your own filesystem layer for it. No need to manually talk to the disk controller. Some of the benchmarks by the Mirage developers seem to indicate these performance wins are entirely possible and within reach. You could also do more exotic things. Technologies like the "Non-pause GC", the C4 collector, must do things like rapidly manipulate TLB mappings in the processor using hardware virtualization tech, in order to achieve its high throughput and collection rate. Such techniques are almost always out of reach for users in, say, Linux - but you could achieve this talking directly to the hardware. Something like the C4 would be a big thing - it's extremely impressive stuff, and pairing it with a custom built "Xen app" could be a huge win, for example, in terms of throughput, latency and responsiveness.* But the idea is about what you'd expect: let's write Haskell programs without that OS getting in the way. In the views of some, it's a step back maybe - for others, it's an interesting opportunity to build new stuff. These days however, I feel that instead of Xen, the [Jailhouse hypervisor](https://lwn.net/Articles/574273/) from Siemens actually fits these credentials better: Jailhouse is ridiculously small (and thus simple and auditable, and hopefully secure) gives exclusive access to certain devices, works with KVM, and is expressly designed to run 'bare metal' applications with Linux as a cohort to manage thing and keep control. I hope that sometime, we can see HalVM actually supported in GHC itself, not as a fork - or something like it, at least. ------------- * This nuttery isn't always necessary, though - for example, you *can* talk to certain kinds of PCI devices directly through `sysfs` in Linux, which is a less well-known trick. You can even use this to write userspace device drivers - it should be possible, I think, to design a high performance software switch or packet router in Haskell, with a Haskell driver, that runs on Linux in userspace. But you can't talk to the virtual memory subsystem to leverage hardware or manipulate page mappings in any meaningful way, from what I understand.
I saw stuff like http://c10m.robertgraham.com/p/manifesto.html pushing for moving your app outside of the kernel, taking charge of your own resources, etc. This sounds kinda like what HaLVM is doing.
But using monad transformers is precisely that: composing modular pieces. Take a simple webserver for example: it needs to read in a configuration file once and pass that data around, write to a common log file wherever an event occurs, and maintain state about open connections. You can't really get around having to pass all that around, so it's if it's not via monad transformers, it's via something else.
I'm writing an entity-component system. I'm not concerned about the library's internal data management, which I'd imagine would just use ST, I'm thinking of the benchmark program which makes use of such a library. A practical simulation/game would certainly need configuration and log files. I'm looking for practical ways of passing that data around. In an OO language I would probably use singletons for the logger/config, and I'd pass state objects around as parameters for the rest.
Have you seen [Data.Timeout](http://hackage.haskell.org/package/data-timeout-0.3/docs/Data-Timeout.html)?
In all honesty I didn't, I was looking for libraries on the Time category, I really like the way it sums up different time units, something that is lacking in tiempo. I think something that would be a merge of the two would be pretty great
To use your example, m &gt;&gt;= k = foldr ((++) . k) [] yes this is pehaps too tacit, and not easily understood at a glance. [] &gt;&gt;= _ = [] (x:xs) &gt;&gt;= k = k x ++ (xs &gt;&gt;= k) This however will negatively affect the performance of your program by disabling foldr/build fusion through this function. (&gt;&gt;=) = concat .: map Is this not an acceptable middleground? Yes, it is fully tacit, but it is also not excessively low level like the first definition. it is explicitly concat composed with map. How is it composed? By passing both input values to map. The (.:) operator captures this common requirement, and provides a simple visual analogue to it's operation. Sometimes the pointfree version is just coco-bananas. mf = (. filter) . (.) . map And the fully expanded version just looks far too complicated mf criteria operator list = filter criteria (map operator list) The key to a tacit approach is that there is a middleground. mf criteria operator = filter criteria . map operator
Right, that makes sense. So this a more an optimization problem: to make it fast, you have to make it less clean :-)
Ok I see better now, thanks. So the monad layers are more like the cross-cutting concerns in aspect oriented programming. The only downside is that there is a performance cost associated with them :-)
I'm pretty sure that's what [RWST](http://hackage.haskell.org/package/mtl-2.0.1.0/docs/Control-Monad-RWS-Lazy.html#t:RWST) in mtl does. I wonder if the unrolling could be done with rewrite rules instead of by-hand
That's pretty nice. I've implemented parts of this tens of times.
There are two simple rules to get fast monad transformer code: **Rule #1:** Never use `WriterT` (or any derived transformer, like `RWST`). Instead simulate it using `StateT` and a strict `put`. I discussed this in length here: http://comments.gmane.org/gmane.comp.lang.haskell.libraries/18980 The equivalent `StateT` approach generates really good core and the above link contains the details. **Rule #2:** Use the monad morphism laws for `lift` to use the simpler bind whenever possible. The monad morphism laws for `lift` dictate that: do x &lt;- lift m == lift $ do x &lt;- m lift (f x) f x This means that if you have multiple consecutive lifts in a row you can group them into a single lift. This has the effect of using the bind in the base monad, which will typically be more efficient. You can also use this rule to optimize things like `forever` or `forM_`: -- The right-hand side is more efficient in each of these equations forever (lift m) == lift (forever m) forM_ xs (lift . f) == lift (forM_ xs f) If you follow those two rules you will get excellent performance with your monad transformer stacks.
No problem! And yeah, I have no idea if this is an inherent penalty of monad transformers (I'm guessing not) or just a lack of optimization in mtl or the compiler. I'm no expert myself, evidently.
now if I only understood monads.
You and me both.
Nice and simple. :) Thoughts: * The SI prefixes are prefixes, not separate words; `milliSeconds` should be named `milliseconds` etc. * The old-fashioned English word for a period of time is `Tide`, which is shorter and fancier than `TimeInterval`. 
But, [you could have invented them](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html‎), of course you already understand them.
[And the same to you.](http://www.reddit.com/r/haskell/comments/1z4ifw/c17_i_see_a_monad_in_your_future_xpost_rcpp/cfqnzs0)
Or, use an existing DSL like [Atom](http://hackage.haskell.org/package/atom).
I always get this wrong too, it's () then []. might want to fix the comment.
I've always thought that was a bad article for learning to understand monads. The best thing I think to is open ghci and ask yourself what each of the following will output, and why. If you get them all right, you probably have a decent understanding: Just 3 &gt;&gt;= \_ -&gt; Nothing Nothing &gt;&gt;= \_ -&gt;Just 3 Nothing &gt;&gt;= \x -&gt; return (x + 1) Just 3 &gt;&gt;= \_ -&gt; Just 4 Just 3 &gt;&gt;= \x -&gt; return (x + 1) Just 3 &gt;&gt;= \x -&gt; return (x + 1) &gt;&gt;= \x -&gt; return (x * x) Just 3 &gt;&gt;= \x -&gt; return (x + 1) &gt;&gt;= \y -&gt; return (x * y) Just 3 &gt;&gt;= \x -&gt; Nothing &gt;&gt;= \y -&gt; return (x * y) where (Just x) &gt;&gt;= f = f x Nothing &gt;&gt;= f = Nothing
try taking a look at [this](http://www.reddit.com/r/haskell/comments/1z4ifw/c17_i_see_a_monad_in_your_future_xpost_rcpp/cfqplq3)
I understand monads better than I do C++...
If only it was so simple. I guess what they are trying to do is get as much benefit from their existing investment in C++ as possible. I don't follow C++ evolution at all, but I've heard from people who do that C++ is gaining more and more features from FPLs.
What about [dimensional](http://hackage.haskell.org/package/dimensional-0.13) and the *-tf* variant thereof? Served my propose well when constructing a DSL for machine control. There is a new feature in one of the latest ghc builds that will allow you to write `timeStuff (6 milliseconds)`. But I just can't find it on my mobile. 
To a Haskell programmer, it is monstrosity. To a C++ programmer, the ability to list which variables a given lambda closes over is immensely useful, I think, as is the ability to specify if variable capture is by reference or by value. I think a more serious issue with C++ is that everything is unsafe by default. This is, however, an opionion of a person who isn't well-versed with C++, therefore it ought to be taken with a grain of salt.
I see! Yes, all of those different ways of capturing bound names would be necessary. That's what happens when you try to force functional programming into a procedural language: you need a way to create an expression that is evaluated lazily (the lambda function) and you also need a way to specify how the evaluation of the lazy expression is supposed to behave in the strictly evaluated imperative world in which it lives.
OK, I still have one more question along the original lines though, hope that's OK :-) One of the plus-points of Haskell is its purity, which means you don't have to reason about side effects when looking at a function. Surely that gets compromised if you add monads like IO and State together into a stack and then work in that stack? Haven't you just reinstated the ability for any code to have side effects? So, is the idea that you should use that stack only for a thin layer of 'glue' code, and then the 'business logic' sits outside of that in pure code?
Is this powerful enough to be used as a diagrams backend?
Thanks! Some remarks: - why not locating this in the Data.Time hierarchy? - I would try to use dimensional as suggested, that's an underestimated package. It brings a lot of safety and flexibility to physical values. - currently I cannot add two timeInterval: (microSeconds 10) + (microSeconds 10). Should this be an instance of Num?
it's nice to see a tutorial using `operational` instead of free monads. not that it theoretically matters much :).
You seriously think this is the best way to learn Monads? Honestly, I agree with kqr: just *use* them and stop trying to *understand them* first. Eventually you'll start picking up the paterns and making new ones on your own. But "you could have invented" actually takes this exact approach: start doing stuff and then showing how a pattern could be recognized and applied.
&gt;with a single line of ***elegant*** Haskell FTFY. If someone can condense 50 lines of C++ to a single line of Perl I'm not impressed because that single line will be completely unmaintainable. But if they reduce 20 lines of C++ to a single haskell line *the haskell will be vastly more clear*. *That* is what makes it powerful. Having said that, though, this realization won't make C++ go extinct because it's always been this way. I used to be a C++ programmer until I realized that you could have a language *more* powerful without all that complexity. Virtual base classes? FFS.
&gt;That's what happens when you try to force functional programming into a procedural language: No, it's not that. C++ has *always* had a history of not using one single byte more unless you *must* have it. So they want to have their cake and eat it too, so to speak. They want the functional goodness but they're not comfortable saying "references must be heap values" or any other such restriction. They want to mix and match to squeeze out every last drop of performance, even if doing so renders the whole thing an unreadable mess (something they've proven conclusively that they're not worried about).
&gt;This post provides a rough outline on how I moved from an IO based monad stack to an isomorphic pure representation of the computation. Isn't this what the IO monad is doing? It's just a monad that is building up the pure computation that will be ran by the "interpreter phase" or whatever?
Two things: 1. Although it's nice pedagogically to explain IO in Haskell by saying it's like 'building up the pure computation that will be ran by the "interpreter phase" or whatever', it's quite a stretch to say that's actually what's going on under the hood. 2. If you use an IO based monad stack your monad stack can do *any* IO operation. If you use a pure representation that is later interpreted in IO then you know that the IO effects that this monad creates are limited to those that are specified in the interpreter. 
Wait, so you agree with us or no? Because that's exactly what we're saying, don't try to read explanations, just play around with code. That article is bad because it's obscuring code with imprecise explanations. Also, the motivating examples are really poor, especially if you're coming from another language. Why do I need to learn this weird concept just to add printfs and use random numbers?! That's ridiculous! Maybe is simple, and more importantly, can easily be seen why it would be useful in not just Haskell, but any language. If you work through the (only) 10 lines of code I listed, you understand what's going on. But you're not going to develop intuition reading, you need to experiment. 
&gt;it's quite a stretch to say that's actually what's going on under the hood. Is what goes on under the hood important though? If I define a program and then hop in ghci and then assign the results of "main" to a variable, every time I print that variable all those actual IO actions happen again. It really *feels* like what I've done is describe a pure function on what to do with "outside world" data and that printing the variable is "interpreting" it again. &gt;2.If you use an IO based monad stack your monad stack can do any IO operation. This is the complaint I usually see and seemed to me that that was what this article was *actually* doing as well: IO tends to be extremely generic and it might be nice to have something more granular with the types. But in the case described the article it looked like the Program monad was about as general as IO and the author just had to hand code the interpreter. I'm guessing I missed something pretty substantial?
I agree with kqr as I said. What you said is run these statements and predict the output. But even in that case, what you would be predicting is how bind works for this particular monad. I don't think it gives me much insight in how StateT works, for example. I just felt that the article had reasonable examples that basically *did* go do some implementations. Yes, the article seems to be reliant on the person actually coding along with the article and not just reading it (so they see right away why just adding a printf is a problem). I agree with you about Maybe being simple and applying to any language.
&gt; Is what goes on under the hood important though? Not for a discussion of this article, I was just throwing in some background so we know what we're talking about. &gt; is the complaint I usually see and seemed to me that that was what this article was actually doing as well. How is the derived monad equally as powerful as IO? If you had `launchTheMissiles :: IO ()` how could you make the new monad run that, if it is not part of the interpreter? 
&gt;I agree with kqr as I said. What you said is run these statements and predict the output. But even in that case, what you would be predicting is how bind works for this particular monad. I don't think it gives me much insight in how StateT works, for example. I guess it differs for everyone, but for me, once I understood Maybe, I pretty much instantly understood all the other of the common ones (except ST with the rank 2 type, but that's more exotic and not really central to the idea of monads). What I had problems understanding when learning was how the nesting was working. Once I understood that, the implementation of &gt;&gt;= for a given class was just a detail. And really, it should be, we're not trying to learn how StateT works, we're trying to learn about what the interface of &gt;&gt;= wants and gives.
One exception to the rule about Writer is when you want to share the result. Not that I disagree whatsoever about its performance costs. 
C++ for hard real time systems? More likely it would be C or ADA (SPARK) for the really paranoid people. BTW, people do use Haskell for real time too in the form of DSLs that generate C code; take a look at Copilot: http://leepike.github.io/Copilot/
The [funarg problem](http://en.wikipedia.org/wiki/Funarg_problem) (mostly of the upwards flavour here) is a problem for many languages and/or their implementations, of many kinds. That some languages throw a GC at it (which in turn might net you non-strictness cheaply) doesn’t mean it would be appropriate for C++. (I am not aware of how to implement non-strictness without a GC.) D and Rust are two examples of kitchen sink languages (my own words) that target a lot of the same problem domains as C++. They use GC and affine types / regions respectively. If capture-lists are a show stopper for you, you can consider the lambda functions of C++ as relatively lighweight object literals if you feel so inclined.
&gt; The old-fashioned English word I'm not sure that's something to use. Simple International English is probably the safest&amp;mdash;I can see that TimeInterval is something similar to TidsIntervall, but Tide is a lot more nebulous.
Why is a "Drawing" monad exposed, if all API's all use it with () values (i.e: purely for its effect)? Why not expose a simple pure Image type that you compose monoidically, as in graphics-drawingcombinators, etc?
I don't understand. What's the difference to diagrams?
Conceptually that is the right way to think about how `IO` works. In practice, the way it works under the hood is a bit different. It's a mix of `StateT`, strictness, and "magic", although done in such a way that none of those concepts leak.
It's exposed because giving a do notation can be less scary than a pure monoidical interface. It's clearly a design choice, and as Diagrams already provide this kind of library interface, users wanting it should use Diagrams instead.
Maybe they want to make it a transformer? 
Diagrams is a more high-level API, using different drawing backends to render images. It uses SVG for a vectorial output or Cairo for bitmap output. Rasterific could be used by Diagrams instead of Cairo, avoiding a dependency on an external C-based library (which are hard to build on Windows).
Sounds great! I just had a little shock moment when I thought "Damn, another syntax to learn beneath TikZ and Diagramms."
You're conflating impurity with complexity or a lack of encapsulation. That monad stack is going to produce the exact same value every time; it's still pure. The State, Reader and Writer monads are all equivalent to just passing values around (i.e. passing a state value, a reader value, or a list of written values). Not impure in any way :). The issue of how much to pass as parameters is no different than in imperative languages: typically if a function is designed to act on a particular object, then it should take the object in as a parameter, even if it only uses a few bits of that object. If the function could reasonably be used more generally, then it makes sense to just pass in the bits of the object that it needs. Similarly, your high-level code can often be working on big collections of data quite generically; the main loop of a physics simulation for example. It doesn't mean that function isn't calling monadless functions on all its data though. So in a sense I think your last statement is backwards.
Oh I see. But in that case the trade off is using an interpreter that already exists vs building one by hand. The article mentions how ugly the interpreter code is so personally I would want to avoid it if I could.
But this isn't true. First of all, when you see the monad then you know instantly that you're dealing with some kind of effect. And from the signature you can tell the *type* of effect as well. Are they using State? I know what that means. Are they using Maybe? Writer? Reader? I know what all those mean. Are they doing a transformer with ListT, StateT and List? Again, I know what the behavioral characteristics are going to be in that case. And what's more, I know *nothing outside of those behaviors can happen*. That's a huge deal. In an impure language I literally don't have any clue without looking at the code.
Yes, we have every intention of making and supporting a diagrams backend that uses this! Edit: I forgot to mention that we hope to be able to use this in the future to support new interesting features in diagrams too such as better integration of raster data in the whole scheme of things and support for fancy bitmap effects.
Here is a rough outline of a map based approach that is straight forward and fairly efficient: module DoHist where import Data.Array.Repa as R import qualified Data.Map as M import qualified Data.Vector.Unboxed as V -- define rows, columns, and color planes for test image imr::Int imr=5 imc::Int imc=5 imp::Int imp=3 -- define constants for the color planes type Color = Int red::Color red = 0 green::Color green = 0 blue::Color blue = 2 -- define the color value type type ColorVal = Int maxColorVal::ColorVal maxColorVal = 255 -- unboxed vector to generate our test image v::V.Vector ColorVal v = V.fromList [1..imr*imc*imp] -- define a histogram for a single color using a map type ColorMap = M.Map ColorVal Int -- define a histogram that covers all 3 colors type MHist = M.Map Color ColorMap -- define and create a test image type ImageR = Array R.U DIM3 ColorVal testImage:: ImageR testImage = fromUnboxed (Z :. imr :. imc :. imp) v -- define an initialized single color histogram initMap::ColorMap initMap = M.fromList [(x,0) | x&lt;-[0..255]] -- create an initialized 3 color histogram initHist::MHist initHist = M.fromList [(x, initMap) | x&lt;-[0..2]] -- update the histogram using the color values for a pixel -- we do this all at once so a new histogram is only generated one time -- which minimizes copies updateHist::(ColorVal, ColorVal, ColorVal)-&gt;MHist-&gt;MHist updateHist (r, g, b) thist = nHist where rmap = thist M.! red nrmap = M.adjust (+1) r rmap gmap = thist M.! green ngmap = M.adjust (+1) g gmap bmap = thist M.! blue nbmap = M.adjust (+1) b bmap nHist = M.fromList [(red,nrmap), (green, ngmap), (blue, nbmap)] -- extract the (r,g,b) values from a pixel location getColorVals::Int-&gt;Int-&gt;ImageR-&gt;(ColorVal, ColorVal, ColorVal) getColorVals x y tarr = (tval0, tval1, tval2 ) where tval0 = tarr R.! (Z :. x :. y :.0) tval1 = tarr R.! (Z :. x :. y :.1) tval2 = tarr R.! (Z :. x :. y :.2) -- generate a histogram for a 3 color image with 8 bit pixels doHist::ImageR-&gt;MHist doHist tarr = foldl (\hist (r,g,b) -&gt; updateHist (r,g,b) hist) initHist [getColorVals x y tarr | x&lt;-[0..imr-1], y&lt;-[0..imc-1]] 
Looks great. Nice docs. Is there an obvious package to use this with if you want to render to the screen or maybe even animate?
Happy to see a low level rasterizer being revisited in terms of purely functional composition! It's a good reminder that there are weaknesses thinking in terms of popular 3D APIs. Nice work! 
Yes, 1) I replicated the names from the original library in distributed-process-platform, it looked funny to me, but not too weird. I was considering even just doing millis and micros, instead of milliseconds or microseconds. 2) I think I prefer to stick with TimeInterval, although is a bit longer than Tide, it's a more known word. 
I still prefer that libraries stick to the simpler abstractions. Wrapping a monoid in `Writer` is pretty easy, but getting the monoid back out of a monad is more verbose and annoying.
Ha cool. I implemented a lot of functional programming functionality in Python before realising that if I really wanted to program in Haskell I should just program in Haskell.
Good catch, My first idea was to do a runtime check, but I find that less than ideal. Ideally it would be nice if one could do static validations on numbers that are way beyond their bounds, like you can transform "(days 365) into micros dummy", but statically :-). Thanks for your feedback!
No, it's `[]` for the text and then `()` for the URL. He just forgot the `http://`
Not that widely known. Here is tweet example of what Peaker is referring to for the uninformed. https://twitter.com/HaskellTips/status/435530857286074368 
Drawing could just have a monoid instance, if it doesn't already. Update: I looked at the doc. Drawing could be a newtype, and then one could make a Monoid instance. I agree with you in theory, but my actions prove otherwise. Shake Rules (I think it is Rules) has both a monoid and monad instance, but I end up using the monad instance plus do notation.
I'm pretty new to cabal but the `.cabal` file that `cabal init` generated doesn't look really useful to me. Anyway this is pretty much all it takes: Graphics.Gloss Graphics.Gloss.Raster.Field System.Random Data.Colour.SRGB Data.Colour.RGBSpace.HSL Debug.Trace Since I didn't really want to bother with file IO, config is done by editing the source files and recompiling. Because of the nature of the project I decided not to care much about this. EDIT: ... and thanks for the feedback!
Or just straight up: mempty == execWriter $ return () m &lt;&gt; n == execWriter $ tell m &gt;&gt; tell n
+1! Much better pedagogical example.
For fun there's also always mempty == getConst $ pure () m &lt;&gt; n == getConst $ Const m &lt;*&gt; Const n
neat
The only real downside to unbounded-delays's approach of calling threadDelay repeatedly with chunks of the delay is it can end up delaying for slightly longer than asked for. Probably only a few ms. Worth documenting, but much better than a partial function IMHO.
I strongly agree!
It's also very poorly maintained right now…
I tried the example, it needs to import Graphics.Rasterific.Texture for function "uniformTexture"
No screen rendering yet, but you can render GIF antimations through Juicy.Pixels.
Does dimensional have a milliseconds instance? Using (milli second), yields a Quantity DTime Double, which is problematic if wanting good accuracy. *Common Numeric.Units.Dimensional.Prelude Numeric.Units.Dimensional.SIUnits&gt; 100000001 *~ (milli second) 100000.00100000002 s
Nope, look at the type of (+) (+) :: Num a =&gt; a -&gt; a -&gt; a All the a's have to be the same. Try in ghci: (i :: Int) + (2 :: Double)
Cool! I didn't know about this workflow. Thanks for the heads up (and the pull request)
But there's also fromInteger :: (Num a) =&gt; Integer -&gt; a So `(seconds 3) + 5` would compile.
http://www.stroustrup.com/JSF-AV-rules.pdf , C++ is commonly used as well. Even if that copilot had proven itself enough to be used in an industry settings (which it obviously hasn't), I'd still be suspicious about how it does memory management, it doesn't look like the programmer has control of it. I'd much rather use ADA or even C++ instead.
Too many complications from Num, but it should be an instance of Monoid.
And arguments in the reverse order of what you'd expect. mf - map filter, yet we pass in filter predicate, map function.
Author of Sodium talks sparsely about it [here](http://youtu.be/gaG3tIb3Lbk?t=13m38s).
`second :: Num a =&gt; Unit DTime a` ... GHC just chooses `Double` as an default fallback. `time = 10 (milli seconds) :: Unit DTime Int` should do the trick. [*edit*] oh wait ... `milli :: Fractional a =&gt; Unit d a -&gt; Unit d a` ... There is the problem ... You could of course use a `Rational` but that probably won't help you.
Neither Reactive-banana nor Sodium (as far as I understand) offer Events or Behaviors that work across a network connection -- but this is clear from the types. You certainly can connect different machines, but it's always clear that events will not necessarily be simultaneous, because you have to manually import and export them into the `Reactive/Moment` monad. The problem mentioned in the paper is simply that the systems failed to take the cross-machine dependencies into account -- you may not update `hInFirstPage_c` until you have made sure that `filteredEmails_s` is up to date. Implementing proper dependency tracking across different machines doesn't seem to be any more difficult than implementing on a single machine to me. However, I'm not entirely sure how useful it is, because these dependencies obviously require a round-trip communication, which may be very expensive. In this case, I would lean towards the idea that the programmer should think about these things explicit -- and account for the case that the list of filtered emails from the server may lag behind the user input.
Something that worked with GLUT would be cool. Or maybe WebGL would be more in style these days.
The interpreter is [here](https://github.com/bartavelle/language-puppet/blob/bbe2f64879d502f77146dc9216e29b6925da9718/Puppet/Interpreter/IO.hs). It's 106 lines long, including all the boilerplate. The important function (evalInstr) is 50 lines long. This is not comparable at all to a general purpose interpreter. It is just providing an implementation of the few functions that are needed by the compiler. While I feel this is ugly, it is a far cry from what it was. Before the refactoring, all these lines were all over the place. Now it's ugly, because there is so much repetition, but it's much shorter, and in a single place. The important part of the program, where the logic is, is cleaner.
Really? I find it incredibly frustrating because a lot of the paradigms from FP are so hard to "do right" in Python. Even simple recursion is somewhere between painful and just wrong. Am I missing some tricks?
I don't understand why do you consider writing "9 seconds" weird, while "seconds 9" is considered *standard*. The only weird thing here is to write "1 seconds".
In terms of distributed STM, I believe that is one of the problems [lvish](http://hackage.haskell.org/package/lvish) sets out to solve.
Haskell's like that sometimes: if caring about the performance of a particular routine becomes important (and it doesn't necessarily have to be), you have to put in some elbow grease. Unfortunately if you've architected your program around some central monad or group of monads (a good and standard pattern), there comes a point where you care about how much `&gt;&gt;=` costs. Time spent to optimize bind is usually well invested though, because a few hundred lines of code here (incl. all the boilerplate instances you'll have to write) makes the whole program magically faster. The #1 guideline I follow, even when I'm just using stock transformers, is to provide an opaque newtype to customers of the module. Otherwise people downstream will start to rely on being able to pattern match or run "execStateT" on your monadic values. As far as making monad stacks faster goes, here are some things I've learned: * use monad-control if you can for exception handling, it's faster and more sound than the alternatives, and works for some kinds of continuation monads (where the typeclass approach often doesn't) * the codensity transform for StateT (Google it) is often faster, see e.g. [the new Snap monad coming up for snap-core 1.0](https://github.com/snapframework/snap-core/blob/5adedd7fe7f48f352ef3041e7c804ec55e244bf7/src/Snap/Internal/Types.hs#L253). * like Gabriel said, use the fact that running bind is cheaper in the lifted monad. Even better: if you're in a MonadIO, spend as much time as you possibly can inside a liftIO block. * if your monad has Alternative or MonadPlus semantics, order the constructors for your value datatype in descending order of likelihood, and pattern match on the common case first. 
Good piece of evangelism.
By "weird shit", he's probably referring not to the syntax, but to writing a `Num` instance for a function type, meaning that the literal `60` could now be interpreted as a function. Since `TimeUnit` itself also has a `Num` instance, one obvious strange effect I can think of is that if you miss a comma in a literal list of numbers, ~~e.g. `[60 50, 40, 30, 20, 10]`~~, you no longer get a compile-time error, but a `[TimeUnit]`. **edit:** yep my first example was wrong, that's what I get for firing off my mouth instead of firing up GHCI, but here is an example that actually "works": eggBoilingTime :: TimeUnit eggBoilingTime = 3 minutes timesTypo, timesRight :: [TimeUnit] timesTypo = [60 eggBoilingTime, 50, 40, 30] timesRight = [60, eggBoilingTime, 50, 40, 30] If you are really heart-set on using OP's code but you want to avoid this specific problem, I guess you could rewrite OP's example so that `seconds`, `minutes`, etc. are of type `Unit` which is not a `Num`, and then write an `instance Num (Unit -&gt; TimeUnit)` (and the `TimeUnit` should really be renamed `Microseconds`). Though I'm sure that way has other problems I have not foreseen. Which is why I stand by my original gut reaction: `instance Num (a -&gt; b)` is an extremely weird thing to write, especially for such a small gain.
Pretty sure that'd still give you a type error on the second element -- it has type `(TimeUnit -&gt; TimeUnit)` instead of `TimeUnit`.
Weird in that usually a number is not a function -- so unless you know what the trick is (and to expect it), this is really confusing for a reader to parse.
(seconds 3) + fromInteger 5 would compile. 
&gt; Imagine for a moment that programmers were constantly baited with snake oil: [...] But now imagine that, among all the noise, all the tools and frameworks created for specialized use cases but recommended for all, lies something useful I think it was Bruce Schneier that noted: snake oil was a real thing that had limited but true beneficial properties. The problem was that the customer didn't know enough to determine if they were being sold real snake oil or fake, nor could they sort the true claims from the false. In other words, the OP chose a great analogy. /tangent.
So David Turner was exaggerating when he said Type Classes were a 'mistake'?
In this series, I will describe the how to write the practical dependently-typed program and prove its correctness in Haskell. For the Part I, I described the singletons technique to simulate dependent types in Haskell by vector examples. In upcoming articles, I will describe how to write the Agda-style equational / relational proof and how to reduce runtime overhead introduced by proof. All the contents are implemented in packages [equational-reasoning](http://hackage.haskell.org/package/equational-reasoning), [type-natural](http://hackage.haskell.org/package/type-natural), and [sized-vector](http://hackage.haskell.org/package/sized-vector).
I don't really mind so long as *somebody* does a MOOC for Haskell. My ex-coworkers were raving about Scala after they took the Coursera class. I know it would be huge plublicity for the language, and I think my current coworkers would appreciate a more consumable resource for learning Haskell. However, if I could pick anyone to teach the class, it would be [Philip Wadler](https://en.wikipedia.org/wiki/Philip_Wadler). His course on [youtube](https://www.youtube.com/watch?v=I1zhPHBiVBk&amp;list=PL4C1D41CB47EED318) was how I learned Haskell. It's awesome; I recommend it to all beginners.
It is unfair to judge a person by what he said in history only. See what he will do in the future.
It's looking type safe to me. Both: print ([60 50, 40, 30, 20, 10] :: [Integer]) and: print ([60 50, 40, 30, 20, 10] :: [TimeUnit]) throw a type error for me. Although it is a confusing error: main.hs:37:9: No instance for (Num (a0 -&gt; Integer)) arising from the literal `60' Possible fix: add an instance declaration for (Num (a0 -&gt; Integer)) In the expression: 60 In the expression: 60 50 In the first argument of `print', namely `([60 50, 40, 30, 20, 10] :: [Integer])' 
I like this a lot. You should put it on Hackage.
It's a decent heuristic and the comments I am referring to are *recent* and *unrecanted* and came from a position of relative ignorance. This makes the overall relevance and timeliness of my objections more, rather than less, significant. I am legitimately worried that he will end up strawmanning Haskell.
Thanks for the info, I'll fix the doc for the next version
Can the 'read' function be implemented for the dependently typed Vector? It would have to take a string and return a different length vector depending on the contents of the string. Is this possible? If the string comes from user input, how can this work?
why the first item is 2 space separated integers?
One way to do this is by returning an existential data type. Basically, you express that you'll return "exist n. Vector n". When you pattern match on the vector value, you can get back this number and work with it. In full dependently typed languages, you can do this via a dependent pair, where the type of the second element in the pair depends on the first element. For example, the type you're looking for would be written: parseVector : String -&gt; (n : Nat) * Vec n Int
The one with `:: [TimeUnit]` only gives an error because of type ambiguity, though; It would still work with the right type annotations. Fortunately that's still enough to save you from doing this accidentally.
Because quant18 was suggesting the compiler wouldn't detect a missing space with the weird Num instances defined. Fortunately, because type classes are open, it still complains, since the types are ambiguous.
`5` means `fromInteger (5 :: Integer)`. Try asking ghci what it thinks the type of `5` is.
According my [exprience](https://gist.github.com/anonymous/9288025) of the channel9 course by Erik Meijer, I think this course no so many about Haskell language, that is about some basic idea of functional programming. And the textbook Graham Hutton's Programming in Haskell, it introduces more FP thing than the Haskell , such as the classic example : mondic parser and the classic problem countdown problem. 
Have you looked at [timber](http://www.timber-lang.org/)? Disclaimer: I don't undersand much of your post. So timber might be totally irrelevant.
To be honest 60 egg boiling times makes sense to me. It's 180 minutes. You can say things like "3 timeoutLimits" then.
IMHO, It's just a matter of practice. An unexperienced programmer would also complain about using laziness as confusing and unpredictable. It doesn't make it necessarily wrong to use.
Making Haskell look like English via type class hackery is cute, but please don't do that in production code.
As it seemed interesting to several redditors, I published an update to this post, as I had a great suggestion from operational's author. The interpreter is now a lot nicer.
I would not worry. Erik will do a great job. 
You mean [this](http://www.reddit.com/r/haskell/comments/1rx7mk/whats_the_deal_with_erik_meijer/) post?
u liek? http://www.yellosoft.us/cspace
That sounds great! When I once tried the typenats branch its solver is not so clever, so I haven't chased the changes in recent version. If the typenats solver is strong enough, it saves much effort to write proofs although still there would be the case programmers have to provide the proof (by the incompleteness theorem). I can't wait GHC 7.8 and HP release!
I'm interested as to what you have for "write such proofs comfortablly": the best I can do is write out enough different ways to calculate the result. For the append, this means defining something like `type AddEq a b c = ( (a :+ b) ~ c, (b :+ c) ~ c )`, so that I can have: appendImproved :: AddEq a b c =&gt; Vector x a -&gt; Vector x b -&gt; Vector x c appendImproved = append That doesn't seem to be much of an improvement over the older style using -XFunctionalDependencies (see the Add clss in http://okmij.org/ftp/Haskell/PeanoArithm.lhs for example). Changes in ghc-7.8 related to `-XAllowAmbiguousTypes`, (https://ghc.haskell.org/trac/ghc/ticket/8390 / https://ghc.haskell.org/trac/ghc/ticket/8392) make it harder to compose these "proofs", at least for me, since I have to write more type signatures. Previously I could write something like: hReplicate' e = let es = hReplicate (hLength es) e in es But now I have to write a type signature that pretty much repeats what is happening at the terms-level in order to please ghc-7.8.
http://www.haskell.org/haskellwiki/Num_instance_for_functions
append is the same as appendImproved except type inference is better in this one case: let v2 = 1 :- 2 :- Nil :t \x -&gt; append x v2 `asTypeOf` v2 :: (n :+ 'S ('S 'Z)) ~ 'S ('S 'Z) =&gt; Vector Integer n -&gt; Vector Integer ('S ('S 'Z)) :t \x -&gt; appendImproved x v2 `asTypeOf` v2 \x -&gt; appendImproved x v2 `asTypeOf` v2 :: Vector Integer 'Z -&gt; Vector Integer ('S ('S 'Z))
&gt; such as the undecidability of type-checking for dependent types, and so on. A little nitpick, but type-checking for the common dependent type systems is quite decidable, even very mechanical in a lot of cases. 
Oops, it's a typo of "type inference". I'll fix it.
That's a rather bold statement. Haskell is still relatively new, and it has proved itself in many programming domains, I think your claim that Haskell sucks at making games or real-time systems is premature. Haskell for gaming is on the way, but the gaming industry is huge and will take a long time to figure out how to make it work for them over C++. Haskell's strength is how easy it is to create Domain Specific Languages. Haskell can compile to the DSL which can compile to the real-time system. Several projects are experimenting with this idea right now: http://archhaskell.wordpress.com/2009/08/01/atom-a-domain-specific-language-for-hard-realtime-applications/ 
Any reason why you wrote the `ifs` function in [Types.hs](https://github.com/evacchi/chaosgame/blob/master/src/Vacchi/Fractals/Types.hs) this way? let result = ifs_def point param1 param2 in (fst result, snd result) This is the same as let result = ifs_def point param1 param2 in result which is the same as ifs_def point param1 param2 but slower as it deconstructs and then reconstructs a tuple.
Type classes are not a mistake. Not even close. There are a couple things about them that are problematic, but the good outweighs the bad by a huge margin.
It's not a bold statement. If you want to use to make games one generation behind like XNA/C# did, I can see that. But it will be never used for game that are on the cutting edge. And this, &gt;Oh man, C++ is going to go extinct when everyone realizes that you can do all the same stuff with a single line of Haskell that you can do with like 20 lines of C++. is a bold statement, not what I said.
If j is a "function" and j(y) = j(4), then it doesn't necessarily follow that y = 4. If you want to conclude that y = 4, j needs to be injective. For instance, if j(x) = 0 for all real x, then j(4) = j(5) but 4 /= 5.
It can only understand things like basic reductions and identities, like if you have a constraint like `(x + 2) ~ 5` then it can infer `x = 3` in later constraints you might have. It can also understand things like `x + 1 &gt; x` or `x + 0 = x`. It *cannot* yet do things like associativity or other properties, so you will still have to write proof terms there. Later on, a new solver will come along which will hopefully be much better and robust (Iavor is working on it).
I think the author is way too silly.
This is a surprising amount of work to produce something hopelessly unreadable
Maybe it's common knowledge, but I didn't know: Erik Meijer has done a series of [lectures](http://www.cs.nott.ac.uk/~gmh/book.html#videos). I find the introduction to Haskell enjoyable and appreciate that you can download the videos.
Well, this is how this man usually writes and speaks. So I assume for him it was no effort.
No, this is not the problem. The second element can have type TimeUnit. The problem is that the "50" in the first element is ambiguous: it can be a TimeUnit, a (TimeUnit -&gt; TimeUnit), an Integer ...
(off topic) as I read through the sample chapter, I got preoccupied by the idea that the user would be entering the time of the tweet, rather than the server figuring that out for him/her. I wonder if it might help the reader keep focus, to either change the handler to do that, or to mention that issue and defer the solution to a later chapter?
Very cool. I have a doubt though. Would it be correct to say that any attempt to take the first element from the parsed monomorphic vector wouldn't compile, because **n** could potentially be **Z**?
I really suggest you change that heading text you're using, it looks awful. Also, you should make the hero text smaller in general and put a button like you have at the bottom underneath it. If I want to buy you book, it's bad that I have to search how to do so.
Number of downloads in the last 30 days.
Awesome! I started my first Snap + Persistent + Esqueleto project a while ago, and this looks like it might be just the sort of thing to help put Snap and Haskell on the map. Any word on when Snap 1.0 will arrive, approximately? Update: there's this: http://www.reddit.com/r/haskell/comments/1zendj/snap_100_merged_in_and_is_iostreams_based/
Interesting. Thanks for the feedback. I'll fix it in a future revision, most likely by doing a forward-mention in that spot and writing some content on dealing with time.
It all makes sense
Suggestion: `Given x = j(4)` is confusing between all that haskell. Add a colon. Or even better, set it in latex! 
where's websockets, redis, fanout proxy, clusters... etc?
"seconds 9" would be weird English. "9 seconds" is weird Haskell. Haskell is not English. Trying to force the structure and idioms of one language into another just makes a mess.
What a lame post.
they're waiting for you to write a book on them
If you provide me with a list of links to libraries you'd like to see written about I would be happy to consider them for the next version of the book.
Yes. `readVector` can return the empty vector (e.g. `readVector "[]" == Monomorphic Nil`) so you can't take `head` of it. One way to ensure the non-empty of the parsed vector is to quantify over `n` of `Vector (S n)` instead of `Vector n`: newtype (:.:) f g a = Compose { getComposed :: f (g a) } fromNonEmpty :: [a] -&gt; Monomorphic (Vector a :.: S) fromNonEmpty (a : as) = case fromList as of Monomorphic vs -&gt; Monomorphic $ Compose $ a :- vs readNonEmptyVector :: Read a =&gt; String -&gt; Monomorphic (Vector a :.: S) readNonEmptyVector = fromNonEmpty . read main = case readNonEmptyVector "[12]" of Monomorphic (Compose vs) -&gt; print (head vs :: Int) One disadvantage of this style is too much `Compose` things. Also, you can just define the new existential type for non-empty vectors like `data NonEmpty a = forall n. NonEmpty (Vector a (S n))` and use it. 
Just match on a gadt equality proof, don't change the type sig.
Interesting. Does anyone know if there is a way to get the all-time total?
I got it. In such case, once you know that `n` should be `Z`, you can just use `ScopedTypeVariables` and write `\(x :: Vector Integer Z) -&gt; ...`. Anyway you have to prepare specialized `appendImproved`-like function, and I think the cost is smaller. Also, you can write the "proof" that `n + m = m` iff `n = 0` using GADTs although GHC can't infer that `n = 0` from that proof.
By the way, with so many great parser combinator libraries, why do we still need Happy?
How does it handle alternative mutually exclusive flags (i.e. either `--foo X` or `--bar Y`)?
When you use parser combinators, you code Haskell. When you use Happy, you describe an LR grammar. You can mechanically verify things about LR grammars (the parser generator does this for you), for instance if all productions are used, and you just can't do the same with arbitrary code.
First step is making a list of them, I suppose. Here's what I found on Hackage; anyone know of others? * [base:System.Console.GetOpt](http://hackage.haskell.org/package/base-4.6.0.1/docs/System-Console-GetOpt.html) * http://hackage.haskell.org/package/argparser * http://hackage.haskell.org/package/cmdargs * http://hackage.haskell.org/package/cmdlib * http://hackage.haskell.org/package/cmdtheline * http://hackage.haskell.org/package/console-program * http://hackage.haskell.org/package/getflag * http://hackage.haskell.org/package/getopt-simple * http://hackage.haskell.org/package/hflags * http://hackage.haskell.org/package/hskeleton * http://hackage.haskell.org/package/hsshellscript * http://hackage.haskell.org/package/multiarg * http://hackage.haskell.org/package/options * http://hackage.haskell.org/package/optparse-applicative * http://hackage.haskell.org/package/parseargs * http://hackage.haskell.org/package/parse-help * http://hackage.haskell.org/package/ReadArgs * http://hackage.haskell.org/package/uu-options * http://hackage.haskell.org/package/yaop
But constructors are injective, that's the beauty of them. 
Thanks. The reason I was asking is that I was wondering if there might be a neat way to do this using an `Alternative` interface, i.e. something like: alternative1 :: DefineOptions A alternative2 :: DefineOptions B both :: DefineOptions (Either A B) both = fmap Left alternative1 &lt;|&gt; fmap Right alternative2 However, I don't know enough about how `options` works alternatively to know if such an interface is feasible.
Could you explain what the gadt equality proof should be for defining `appendImproved` without that `AddEq`? Do you still get the type inferred as I showed in [above](http://www.reddit.com/r/haskell/comments/1z8wsv/prove_your_haskell_for_great_safety_part_i/cfrs9ld)?
cool - big results from little code!
Importantly, a parser generator can tell you whether your grammar is ambiguous. It also deals with left-recursion in a more natural way.
Also, grammars for programming languages are often in LR form in the language's specification. Putting them into an LL form requires some work even for Parsec which has arbitrary lookahead. 
It both weakens the type assurances you get from code and breaks the expectations of a reasonable Haskeller: Other comment threads showed how it can cause missing comma to type check as wrong code. Additionally, it means that if I accidentally swap two arguments, both literal integers, and one is a Time value, error will not be detected. Code using this would break my expectation about applying numbers as functions. It would also require FlexibleInstances. It's type class hackery because the instances aren't sensible on their own but about achieving some syntactic effect.
Frankly, I'm disappointed with the level of explanation. (I've read the first 3 chapters so far.) The structure of the book seems to be pieces of code followed by description of what the code is doing. Those descriptions are what anyone with decent Haskell knowledge would mostly infer anyway. [Example](http://i.imgur.com/EmiqOBK.png) It's not even explained what snaplets are and how they work, apart from them being "modular units of stateful code that are usable between applications". What I hoped to see is more of a whole picture, a high-level description of what's going on and how things are glued together. Also I'd like to gain intuition about the semantics of snap's API, so that ultimately I'd be able to write code myself instead of copy-pasting code samples from tutorials. IOW, I hoped that the book will turn me from a snap beginner to something closer to a snap expert. In reality, the book's goal seems to be to *produce* snap beginners who have written their first web app using a step-by-step instruction. 
What are your plans about fonts?
Can you abstract out your PNG serialization code? Right now, that isn't even in a separate module. In particular, what is the relationship between this and your great [JuicyPixels](http://hackage.haskell.org/package/JuicyPixels) library? Couldn't this just be a client of JuicyPixels?
First off, I would be happy to offer you a refund for your purchase. Please contact me via PM if this is the case and we can get that process started (I will need the name and email you used to order). Now that that's out of the way, I do have content in mind that is aimed at a higher level but it is not this book. This book was intended to enable beginners to get started with Snap. It is my assumption that Haskell experts are capable of looking at the documentation, website and current open source code and understanding what is going on. It sounds like what you are looking for is a book on the internals of Snap, complete with design decisions. This would be possible post-1.0 since the internal API is changing from using Enumerators to [using io-streams](https://github.com/snapframework/new-snap-server). I would love to hear what, specifically, you would want from a higher level Snap book (I am going to send you a PM with my email after I post this comment so we can have such a discussion). I have a couple of ideas for higher level resources and it would be great to get feedback from someone who is invested in that goal.
Um. Is that forcing the rest of iterator? If so, the `next()` method still works better as a poor man's `uncons` in Python.
As I learned from the author's response to my own question - that wouldn't be a feature of Rasterific. It would just be a matter of adding an appropriate backend to the [JuicyPixels](http://hackage.haskell.org/package/JuicyPixels) library.
You can be almost as cute without any loss of type safety. newtype TimeUnit = TimeUnit Integer -- how many microseconds deriving (Eq, Show, Num) exactly :: TimeUnit -&gt; TimeUnit -&gt; TimeUnit exactly = (*) seconds, minutes, hours, days :: TimeUnit seconds = TimeUnit 1000000 minutes = exactly 60 seconds hours = exactly 60 minutes days = exactly 24 hours although having a `Num` instance for time is a bit sketchy (and not necessary for this to work).
And that's exactly the point. In many cases - more cases than most Haskell beginners realize - the type signature already tells you everything you need to know about what the function does and how to use it. And it's not cryptic at all, once you've learned how to read a type signature. That's actually pretty amazing. &gt; give a person a basic understanding of how to use a library without having to first read the documentation for the whole thing It's the opposite - I don't want to have to read a whole long wordy tutorial when I can get the same information in an instant just by reading the type signature. Of course, even though the type signature is sometimes everything you need to know, sometimes it's not. Perhaps the real problem is that relying on just the type signature so often gets us in the bad habit of being skimpy on human-readable comments, and then we do that even in cases where you really can't.
I agree that the type signatures aren't always all the documentation you need. It's certainly possible for Haskell code to be under-documented, and that's just as annoying in Haskell as it is in any other language. But the type signatures add a huge amount. And in some cases, yes, they really are all you need.
I know this was just an example. However, I'd argue it's better for the latter option out of --foo and --bar to win, instead of throwing an error. For example, the user might have aliased "ls=ls -h". A shell script might call "ls -l --si", which gets expanded to "ls -h -l --si". --si contradicts -h, and --si wins without error. This means the shell script works and works identically, with and without the alias.
Yes, tragedy! &gt;&gt;&gt; x, *y = fib() Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 7, in fib MemoryError
I'd like to see an interpreter that is pure and that is mockable, and tests written on top.
Thank you all for those great answers!
I like the idea of docopt which creates a parser from the usage text: https://github.com/docopt/docopt.hs . Unfortunately, its not on hackage and hasn't been actively developed for a while.
Any nice example of this? Except the classical "generate and filter" one from whyfp.
That is a distant goal. It is an awful lot of work though.
It'd probably help if you tried to reduce the problem first, i.e., cut parts out of your project until it behaved as expected again.
found it! http://stackoverflow.com/questions/6280585/need-to-know-what-and-do-in-haskell had to search on symbolhound.com
I don't think you've earned $35 yet but I'm excited about what you've got planned. The application pattern you've documented with heist templates and form processing is a pattern I've used when I'm building a CRUD app where HTML fallback is important. I've also built a number of apps where Snap provides a bunch of JSON endpoints and essentially acts as RPC layer for a Javacscript front end. This code is all proprietary but I'm happy to let you take a look at it. I'm Bay Area based. I'd love to learn more about web sockets, streaming and caching. These are all things I have yet to figure out. 
They are related to the Functor and Applicative type classes, respectively, and available by importing `Control.Applicative`. I think [this link](http://www.haskell.org/haskellwiki/Typeclassopedia) does a pretty good job of explaining the main category theory-based type classes. Also, when searching for Haskell functions and operators, rather than Google, use [Hoogle](http://www.haskell.org/hoogle/?hoogle=%3C%24%3E) :)
multiarg author here. I wrote multiarg because, to my knowledge, there wasn't a library out there that easily allowed for options that take more than one argument, such as --optFoo bar baz There are moderately complex command-line interface programs that can benefit from this. See for instance mplayer, which has several options that effectively take more than one argument. When the option parser library doesn't handle this, higher-level code has to do this itself. multiarg is based around a Parsec-like parser combinator, though I now have a higher-level module which uses the combinators so ordinarily I just use the higher-level, declarative interface. multiarg gives you the options and option arguments in the order in which they appeared on the command line; some libraries I have seen will put the options in a custom data type for you. Those other libraries can be easier to use, but from what I have seen it can be impossible for them to tell you the order of the options. Think of find(1), for example, where the order of the command line is important. I don't use anything like Template Haskell so multiarg might be more verbose, but to me it is simpler than one of the libraries I saw that made extensive use of TH. I also did not make an automatic help generator because in my experience these don't get things right in complicated scenarios and they don't save much work over just writing the help myself in a text editor. As the author of multiarg I hesitate to compare it to other libraries too much--I'm not terribly familiar with them because now that I wrote multiarg, I just use that. I can say, though, that multiarg is quite powerful and flexible but you pay the price through increased verbosity. Even I will use optparse-applicative if my needs are simple and I want something terse. But some of my programs have more powerful interfaces and for those, from what I can tell, I simply can't use something like optparse-applicative.
Try compiling with -O0 or -O1, you might have written code that tricks the optimizer into looping by being too clever 
I can make it compile if I change OptimalBlocks.hs line 40 from in Blocks (reverse rlist) end to in Blocks [] bs That basically eliminates the entire program though (all the actual calls to things are probably being elided by dead code removal). I have tried a few other permutations: hashed = hashes winSz bs becoming hashed = V.empty or locs = V.map ... becoming locs = V.map fromEnum hashed or lens = V.filter ... becoming lens = locs or even making almost the entire thing be dead: lens = V.filter ... becoming lens = V.empty All of these make ghc hang. I guess I should have asked "how do you debug ghc". If somebody has some sort of nifty techniques for figuring out exactly where it's getting stuck, that would probably be ideal. EDIT: One thing I tried that does have an effect is changing the "-O2" in optimal-blocks.cabal to "-O1". That lets it compile. I'll run my benchmarks to see what impact is has on performance, I guess. I'd still love to see if anybody can figure out why -O2 is hanging.
Looks like you beat me by two minutes; it does compile with -O1. I'm checking what performance impact that has now. Maybe I can just given -O2 flags to the files where that matters...
(For the purposes of this example let's pretend all numbers are Int). Functors provide a definition of fmap/&lt;$&gt; :: (a -&gt; b) -&gt; f a -&gt; f b, which allows you to take a Functor, and apply a function to the value inside the Functor. (For example, (+1) &lt;$&gt; (Just 5) returns (Just 6)). However, with just fmap alone, there is no way to do multiple argument functions on a functor. Let's say you want to add (Just 5) and (Just 6) to eachother, to get (Just 11). If you do (+) &lt;$&gt; (Just 5), you get Just (5+), which is an expression of type Maybe (Int -&gt; Int). Using only fmap and &lt;$&gt;, there's no way to take a function inside a Functor and apply it to something. We want a function of type :: f (a -&gt; b) -&gt; f a -&gt; f b. This is exactly what Applicative functors give us, with the &lt;\*&gt; function. &lt;*&gt; Takes an f (a-&gt;b), and an f a, and applies the function inside the first argument to the function inside the second argument, resulting in an (f b). (+) &lt;$&gt; Just 5 :: Maybe (Int -&gt; Int) Just 6 :: Maybe Int (+) &lt;$&gt; Just 5 &lt;*&gt; Just 6 :: Maybe Int Maybe's instance of Applicative will cause the entire computation to return Nothing if any of the arguments are Nothing. Meaning that the expression (+) &lt;$&gt; Just 5 &lt;*&gt; Nothing will return Nothing, etc. Functors only allow you to apply a unary function to the value inside a functor, whereas Applicative functors allow you to perform any arity of function (in our above example, a binary function) between functors. Without Applicative, you'd have to write the above addition as: a + b = case a of Nothing -&gt; Nothing Just x -&gt; case b of Nothing -&gt; Nothing Just y -&gt; Just (x + y) or using Functor, as: a + b = case (+) &lt;$&gt; a of Nothing -&gt; Nothing Just f -&gt; f &lt;$&gt; b instead of simply just (+) &lt;$&gt; a &lt;*&gt; b, or even more shortly, liftA2 (+) a b
Here's a motivating example for how you might use them: import Control.Applicative main = do c &lt;- (+) &lt;$&gt; readLn &lt;*&gt; readLn print c It's an easy way to lift an operation over a functor that implements `Applicative` (like `IO`). It's equivalent to: main = do a &lt;- readLn b &lt;- readLn let c = a + b print c
Ohh, it is like the |@| in Scalaz, except the function is at the beginning instead of the end. That's what was confusing me, I didn't understand the function definition. Thanks dude!
Thank you! Your explanation of `&lt;*&gt;` has clicked for me in ways that several previous explanations have not.
Would you be at all interested in an intern?
#**In simple words:** `&lt;$&gt;` (`fmap`) is a universal `map`, that you can use to apply a function to ANY structure whose elements are of the same type. As long as that structure implements the class. `&lt;*&gt;` is for if the function you’re applying has multiple parameters. Because then `&lt;$&gt;` returns a structure with *partially applied* functions as its elements. And to apply *another* such structure of data to that structure of functions, you need this. A typical use case is let xs = [0,1,2,3,4,5,6,7,8,9] let ys = xs (+) &lt;$&gt; ys &lt;*&gt; xs which results in [0,1,2,3,4,5,6,7,8,9, 1,2,3,4,5,6,7,8,9,10, 2,3,4,5,6,7,8,9,10,11, 3,4,5,6,7,8,9,10,11,12, 4,5,6,7,8,9,10,11,12,13, 5,6,7,8,9,10,11,12,13,14, 6,7,8,9,10,11,12,13,14,15, 7,8,9,10,11,12,13,14,15,16, 8,9,10,11,12,13,14,15,16,17, 9,10,11,12,13,14,15,16,17,18] It’s also useful if you *start out* with a structure of functions and you want to apply data to that. Things like that. The reason they are named like that, is because `&lt;$&gt;` is the Functor version of `$`, and `&lt;*&gt;` is the functor version of function application. See also: `ap` – the monadic version of `&lt;*&gt;`. `liftM` – the monadic version of `&lt;$&gt;`.
It's worth separating the answer into two parts: PART I: Common Usage Suppose you have some "context" for a value. It could be a monad, like `IO`, `[]`, etc. Or it could be something called an "applicative functor" that's not quite a monad. But 95% of the time, it's probably a monad just because there aren't a lot of examples of things that are applicative functors without also being monads. In your case, we're probably talking Aeson's `Parser`, which is a monad. Now suppose you've got some function, with an arbitrary number of parameters: say, `makePerson :: Name -&gt; Age -&gt; Gender -&gt; Person` with three parameters. But you're trying to build up a parser, not work with values. Using monads, you could write this: parsePerson nameParser ageParser genderParser = do name &lt;- nameParser age &lt;- ageParser gender &lt;- genderParser return (makePerson name age gender) But that's really verbose. You'd really like to just take this function, and it's parameters, and move them all into this Parser monad. That's what `&lt;$&gt;` and `&lt;*&gt;` are there to help you do. You can write: personParser = makePerson &lt;$&gt; nameParser &lt;*&gt; ageParser &lt;*&gt; genderParser And you're done. `&lt;$&gt;` goes between the PURE function and the arguments, and `&lt;*&gt;` acts as an argument separator. It's a good idea to become familiar with that pattern. PART II: What's Really Going On Okay, so now what's going on behind the scenes? Here are the types for those operators: &lt;$&gt; :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b &lt;*&gt; :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b Here, `f` is probably `Parser`. So `&lt;$&gt;` is about taking a pure function (of only ONE parameter) and changing it to act on values in the context `f`. You can actually do that for any arbitrary `Functor`; it doesn't need to be an applicative functor. (That said, most common examples of functors are also applicative, and are even monads, too.) The `&lt;*&gt;` comes in when you need multiple parameters, which of course are commonly represented in Haskell with partial application. So if you take your `makePerson` function, and use `&lt;$&gt;` on it, you can pass in the first parameter... and get back something ugly: makePerson &lt;$&gt; nameParser :: Parser (Age -&gt; Gender -&gt; Person) Oops! Now the rest of the function, including those extra parameters, is wrapped in the `Parser` context. This means exactly what it says. You've described a parser which looks at some text, and gives you back functions. (Specifically, it expects the text to look like a name, and the function it gives you back is makePerson partially applied, with that name as its first parameter.) This is where `&lt;*&gt;` comes in, because it can break apart the `Parser` context on functions, as you see in its type. So we can start applying `&lt;*&gt;`, as such: makePerson &lt;$&gt; nameParser &lt;*&gt; ageParser :: Parser (Gender -&gt; Person) And one more `&lt;*&gt;` does the trick: makePerson &lt;$&gt; nameParser &lt;*&gt; ageParser &lt;*&gt; genderParser :: Parser Person And you win! Note that the last steps need something more than a regular functor. There's a lot more to applicative functors and the ideas they represent. You should be aware that there are functors that are NOT applicative, so it's impossible to play this trick with them. But in practice, this kind of thing is what `&lt;$&gt;` and `&lt;*&gt;` are there for.
I prefer the `PostfixOperators` solution of writing ``(60 `seconds`)`` which is a nice compromise between Haskell and English.
Leon thanks for the suggestion! I've written both a systemd unit and an upstart job that handle process monitoring without much effort at all and was able to remove the dependency on Angel! Great suggestion!
I very much disagree, we shouldn't become accustomed to using language hacks to attempt to make the source code read more like English sentences. There's a well-defined structure to the language and abusing it just leads to programmer confusion and/or quite likely confusing type errors compared to the standard idiomatic way. 
This is slightly tangential, but I found working through Learn you a Haskell perfect for explaining functors, applicatives and monads. I was also fairly lost with all the symbolry, but reading that and everything that seemed overly abstract prior became clear :)
optparse-applicative has an `Alternative` implementation..
For the curious: [it was a GHC bug which is fixed in 7.8](https://ghc.haskell.org/trac/ghc/ticket/8836).
This is probably an opaque answer and you've already got a lot of answers, but it really is worth "reading the manual" first with Haskell - as slow as it is. [1] Not because people here aren't friendly and don't want to help but because your understanding will evolve better, I think, and you'll be reasoning about your programs or seeking answers to questions using first principles that you won't get from not doing so. Understanding the type system is also critical because otherwise you'll feel like GHC is a cold lover - knowing how to line up your types will go a long way. You'll eventually notice too that you can search for functions that may have already been implemented by someone else just by searching for the type signature! [1][EDIT] I should probably list resources if I'm going to say that, you may already know them though: 1. Learn You A Haskell 2. Real World Haskell 3. [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia) 4. [Hitchhikers Guide](http://www.haskell.org/haskellwiki/Hitchhikers_guide_to_Haskell) -- NOTE: this is a bit more like being thrown in the deep end but it's an excellent resource from top to bottom. Hope this helps.
You could also rewrite that example as: import Control.Monad import Control.Applicative main = print =&lt;&lt; (+) &lt;$&gt; readLn &lt;*&gt; readLn 
Very nice, I was recently thinking about something like this myself. The downside of this method is that `tred` is too rough — if `a` depends on both `b` and `c` and `b` depends on `c`, the dependency from `a` to `c` won't be shown.
Fantastic! Does it output anothing other than dot files?
In case the existing posts here aren't enough, I recently wrote a little introduction to some different coding styles in Haskell - it includes monads and applicatives (which are related to the `&lt;$&gt;` and `&lt;*&gt;` operators you're seeing). It's [here](http://crabmusket.nfshost.com/2014/01/21/you-may-say-im-a-monad.html).
"options" handles multi-parameter types by letting the user specify the separator, for example `(optionType_list ',' optionType_string)` parses to a comma-separated list of strings. On the commandline, this would look like `--optFoo=bar,baz` or `--optFoo bar,baz`. I think it's not a good idea to have programs which have options like `--optFoo bar baz`, because that format requires readers to have a deep understanding of the options if they're even to understand which tokens are part of an option and which are non-option argv. `find(1)` is, to me, a perfect example of a failed argument parser. Every time I try to use it I have to examine the manpage, because I can't remember which parameters need to be written in which order. An automatic `--help` generator isn't strictly needed for very small programs, but it's useful when the number of options grows large and some of the options are being pulled in from libraries.
This isn't tred's fault; it's a limitation of the dependency discovery mechanism. If you run [cabal install --dry-run a] and [cabal install --dry-run b], there's no good way to figure out that A has a direct dependency on C. The ideal solution would be to extract cabal-install's dependency resolver to a library so it could be queried with a fake package database, instead of having to round-trip through the command-line UI.
Don't go out of your way! I'm working on web-based graph rendering so my ideal format is JSON - lists of nodes and edges. But that should be fairly easy to extract from dot files on my end.
Right — that's what I meant when I said it's the downside of the method (not `tred`).
First I read it and I was like _yay_ ... then I read that it's going to be _this fall_ ... :sadface:
I'm finding this tutorial quite helpful for getting started with an existing back end. Some additional examples of exotic db schemas translated into quasi-quoted entity definitions might add to it... For example, I'm currently trying to define some entities against an existing mysql database that has id fields that look like this: id int(11) unsigned auto_increment with auto_increment=1000 Is it possible to change the sqltype of the id field? Is auto_increment=x on a table supported? In general, better cheat sheets for persistent (and other yesod DSL's) would be super helpful... Perhaps some generous souls could add some examples to https://github.com/yesodweb/yesod/wiki/Persistent-entity-syntax if that makes sense to the yesod team. PS Thank you FPComplete for helping to make Haskell more accessible to industry.
&gt; I think it's not a good idea to have programs which have options like --optFoo bar baz You just have to tell the parser how many arguments an option takes. It can even read a variadic number of arguments by stopping whenever it hits a word that starts with a dash. &gt; --optFoo=bar,baz Works, true, but I figure the shell already splits the command line into argv. Why put another layer of argument splitting on top of that? &gt; find(1) is, to me, a perfect example of a failed argument parser. I think the problem with find is not so much the argument parser but its semantics. I think find would work a lot better if it were RPN; then there's no need to remember precedence. I might write an RPN find to test this out.
The last few slides show an interesting email from the author of ATS to the OP talking about how ajhc is not really ideal for these projects because it has a GC (amongst other things) and recommending ATS (of course) instead. What about a language like rust? Is it suitable for these projects? 
the associated bug should be gone in ghc 7.8. Try building with the 7.8RC ant O2 and you should be fine
On someone who is ignorant on snap, what does this mean to me?
what is the relationship between io-streams and conduit, if any?
The sqltype is used for migration, to create or alter the table so the type used on the Haskell side and the type in the database are the same. If you're working with an existing database, you presumably can't change them, so the only reason to set the sqltype is to get the migration code to quit complaining about columns that don't have the type it thinks they should. Basically, you shouldn't need to do set the autoincrement size unless you're creating the table. The id column should never be written via persistent. I'm not sure there's a way to write it should you want to, since it's not part of the Entity created for the table. So the only thing you need to do is make sure the type on the Haskell side can hold all the values the id column can take on. It may be worth investigating making Integer an instance of PersistFieldSql.
they are completely seperate, but of course both are streaming libraries (as is `pipes`). `io-streams` is (arguably) much simpler, but only works within `IO`. the other two work for any base monad.
see https://github.com/gregwebs/ParseHelp.hs for a similar approach for `cmdargs`.
I believe Rust is also good for system programming. The reason choosing ATS is that it has proof feature. http://www.ats-lang.org/DOCUMENT/INT2PROGINATS/HTML/x3628.html Perhaps, we will choose Rust on 3rd iteration. 
indeed. i guess in the next months we will find some benchmarks showing which performs best in which scenarios, and on ghc-7.8 (because that contains large improvements to the IO subsystem). i'm looking fwd to this a lot :) ultimately this will push Haskell's performance for web-related applications.. it would be nice if it matches/exceeds Java's (apart from memory consumption) performance in this area. 
For the uninitiated, there is a lot missing from the documentation. I imagine that you need to read some Xen documentation before using this tool. But can someone just give a very basic top level view of what the steps are to actually run a HaLVM program, and where to find the information you need?
This is a very nice tutorial. I'm not surprised that some people have already found it helpful in practice. Did I just miss it, or is this tutorial anonymous? Does anyone want to take credit?
The Snap team spent a lot of time writing the Streaming IO library for Haskell to provide better performance (beacause...streams) for the Snap Web Server. They finally finished it and released it about 8 months ago, I think. Snap 1.x means the Snap Web Server is using IO Streams and the framework has all of those changes merged in now. In addition to whatever other framework fixes, heist, etc... In short: it's a big deals, particularly with GHC 7.8 on the horizon. You'll see highly performant web applications coming out of Snap, which is exciting.
I would start [here](http://www.haskell.org/haskellwiki/Heterogenous_collections) and also look into [type witnesses](http://www.haskell.org/haskellwiki/Type_witness). Alternatively, you can just cheat with [dynamic](http://hackage.haskell.org/package/base-4.2.0.1/docs/Data-Dynamic.html).
what's the benefit over simply running `cabal install` into a newly created sandbox, and running `cabal sandbox hc-pkg dot` afterwards? moreover, you might want to mark packages part of the GHC distribution, as those are "for free".
Also, would it help if cabal let you specify dependencies whose types are not exported? Then it would be easier to lock a dependency to a particular version without the concern that others who depend on your package will have spurious diamond dependency problems. 
Utterly and completely importable. It's only a 1000-ish lines of code though, and you can steal what you need.
You can certainly, but it obfuscate the main point of the example, which is 'c &lt;- (+) &lt;$&gt; readLn &lt;*&gt; readLn'.
Can you tell us what you don't like about the answers you have already received in that thread?
I'm on it! :)
Why not: data Container = Container [forall s . (Widget s, s)] 
Congratulations to Doug, Gregory and all the other amazing engineers to have spent months crafting the 1.0 release. Proud to have been a very small contributor (not on this 1.x branch though) to this fantastic project.
"cabal-db graph" already does exactly the same things and some other extra things [cabal-db-announcement](http://tab.snarc.org/posts/haskell/2013-03-13-cabal-db.html)
Why make a smartass mess ~~when you…~~?
Because it gains you nothing.
Wow. Very interesting lineup! i am certainly looking forward to listening to it.
So it's `pandoc` &amp;rarr; `http-conduit` &amp;rarr; `http-client-tls` &amp;rarr; `connection` &amp;rarr; `tls` &amp;rarr; `cipher-rc4`.
Did you or a student of yours complete a PhD in functional programming in the last three years? If so, please submit the abstract for publication in JFP! No refereeing, simple submission, deadline 30th April. Best wishes, Graham.
*rewinds 4 years and chooses computer science as a major instead of communication* Well, now that I have relevant experience, I'll just go ahead and submit my resume...
I find this behaviour suprising (not exactly the example from your post, but related) Prelude&gt; let list1 = [20..70] Prelude&gt; :sprint list1 list1 = _ Prelude&gt; null list1 False Prelude&gt; :sprint list1 list1 = 20 : _ Why does it force the `20`?
Mine is even better (GHCi, version 7.8.20140130) Prelude&gt; let list1 = [20..70] Prelude&gt; :sprint list1 list1 = _ Prelude&gt; null list1 False Prelude&gt; :sprint list1 list1 = _ 
It needs to check that it's less than 70. After all `null [71..70]` is true.
If I understand correctly, a lot of the performance overhead and complexity in Pipes and Conduits comes from error handling etc. If you make the simplification that you're always in the IO monad then performance improves and the interfaces simplify. Greg or Gabriel would be the best to answer this though.
This is one thing I think Java gets right and Haskell gets wrong. It should be idiomatic to name a package "MyUniqueName.Data.HashSet", not "Data.HashSet", which is just asking for trouble. Though in practice, I guess there hasn't been many conflicts from looking at the list.
Congratulations! Switch to monad-control is a big win. I wish there were a changelog file.
It becomes more clear when one desugars list range notation into Enum function applications.
This one is interesting. I'll have to install GHC 7.8 to test that. PS: I'm not the author.
&gt; Laziness is the separation of equation from execution. This is vague and at best misleading. I can't even begin to figure out what this should mean.
This is because the monomorphism restriction is off by default in ghci in 7.8. (Be careful what you wish for!)
I think it's quite good and concise. I mean when you say something like `list = [1 .. 10]` it only equates `list` with the expression `[1 .. 10]`. They're equal and exchangeable. When coming from imperative languages the naive impression might be that it is an instruction rather than an equation. Execution on the other hand is postponed. Or am I missing something here?
That's true about call-by-value evaluation too. In fact, it's just true about functional programming.
In strict languages, functional or not, saying `list = [1..10]` means instructing the computer to allocate a list of 10 elements and initialise them to the first 10 natural numbers. In other words, saying `list = [1..10]` is an order to execute code to perform an assignment. In non-strict languages, saying `list = [1..10]` simply reserves the name `list` for a computation that might or might not be performed later. There's no execution in it.
&gt; io-streams is (arguably) much simpler This is arguable? The io-streams library sacrifices several degrees of freedom vs. pipes or conduit in order to achieve this simplification -- we don't do bidirectional streams and we're not parametric over the base monad, to name two. One of the reasons I wrote this library in the first place is that I thought the other approaches were too complicated and that we needed a solid interface to streaming that anyone could understand. In practice as a user of io-streams I don't find that I miss the features that I cut out when it was designed --- the combinators are plenty expressive enough and lots of transformations can be written as pipelines as you would hope. This falls out of the fact that transformations of type "InputStream a -&gt; IO (InputStream b)" form a Kleisli category (well, a restriction on the Kleisli category for IO, anyways) and can be combined with `Control.Monad.(&gt;=&gt;)`.
Here's the article that Gabriel mentions towards the beginning about Coroutine Pipelines, if anyone cares: http://themonadreader.files.wordpress.com/2011/10/issue19.pdf
Oh yeah, of course. Nice one!
Iiiiin the left cornerrrrrrr.... :)
I agree with you, although I think some people don't like Java-style qualified names because they can become unwieldy. If and when Haskell becomes as popular as Java, though, there will be a lot of name conflicts!
Not quite. As far my understanding goes call-by-value only works in strict languages. Hence the expression `list = [1 .. 10]` would be evaluated to `list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]` and then given to a function. But laziness or call-by-need means that expressions are only evaluated when absolute necessary. E.g., for the statement `print $ head [1 .. 10]` only the first element of the list needs to be evaluated.
I am not convinced. list = [1..10] is a declaration. The rhs is an expression. That is always true for lambda calculus regardless of the evaluation strategy. I think it's weird to say that it "means" instructing the computer to do something in one case and not the other. The semantics don't reference a computer at all, so this seems just as vague and imprecise as the statement I took issue with. Both call-by-name and call-by-value define how to reduce an expression. Regardless of the semantics you choose the mechanism of computation is always the same (beta reduction), the only difference being which and when beta redexes are to be reduced and whether you're doing reduction over a tree or over a more general graph. Which brings me to another objection: How does that original statement distinguish call-by-name and call-by-need? Lazy evaluation refers always to the latter, never to the former.
&gt; call-by-value only works in strict languages That is a weird thing to say because call-by-value is definitionally a strict semantics. &gt; Hence the expression list = [1 .. 10] would be evaluated to list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] and then given to a function. But laziness or call-by-need means that expressions are only evaluated when absolute necessary. E.g., for the statement print $ head [1 .. 10] only the first element of the list needs to be evaluated. That is not the definition of call-by-need.
I remember reading stuff by Kazu (regarding Mighty and Warp) about the dramatic increases between the GHC versions thanks to IO manager improvements, and other tricks, like yielding before requesting more data and whatnot. The comparisons seem too dramatic between snap and wai-based services. I think it was part of a snap for beginners post that had the PDF for this. 
There's no need to go as far as Java's "we'll just assume you own a web domain" naming scheme, just `LibraryName.&lt;Module Hierarchy&gt;` would be fine (You're going to end up with silly names no matter what scheme you choose, and `Lens.Control.Lens` isn't very bad comparatively). Name conflicts aren't a huge problem *yet*, but the current system makes it very likely that more will happen, because people aren't likely to go search Hackage every time they add a new module to their projects. It's much easier to just check that no one has a library with the name you're using. The article also touches on a side benefit of a naming scheme like this, which is that you can tell immediately where a module is from when you're reading the source.
I have two web-applications that use Snap, both ~7000LOC. How much work will it be moving them to Snap 1.0.0? It's funny, I converted one application from Happstack to Snap somewhere in 2011/12 because I wanted the sexy enumerators, now I can't wait for IO-streams!
http://www-03.ibm.com/systems/power/hardware/ Remember Watson, the *Jeopardy!* destroyer? That is a power7 based system.
The makers of the podcast should have posted a warning this was going to be recorded early so we could place bets!
Thanks! We'll add that to the links!
I thought pipes performed just as well as io-streams? Or am I misremembering things?
I don't have an OS X 10.6 build machine available to me, so I can't verify anything. Several users have reported that the Lion builds work fine on 10.6 however, so [please do give it a try](https://www.haskell.org/ghc/dist/7.8.1-rc2/ghc-7.8.0.20140228-x86_64-apple-darwin-lion.tar.xz).
Thank you! I'll try it.
&gt;should we keep supporting .tar.bzip2 distributions? Considering how trivial it is to support, I don't see any reason to take it away. In fact, it would be nice to bring back gzip support. There's a fairly sizable group of people who prefer having as little misc. stuff installed on their systems as possible, so don't have bzip2 installed.
&gt; what's the benefit over simply running cabal install into a newly created &gt; sandbox, and running cabal sandbox hc-pkg dot afterwards? Running [cabal install] takes a very long time. &gt; moreover, you might want to mark packages part of the GHC distribution, &gt; as those are "for free". I considered this, but figured it was better to be explicit about what's being depended on. The --exclude flag can be used to ignore packages that the developer assumes will already be available.
This is the most recent version of my lens tutorial. I've decided to finally put it on School of Haskell, with all the markup and junk. Hopefully the explanation of how to derive the Lens type makes sense. Last time I posted a draft of this, one comment was that getting Lens was a big, mysterious thing that a lot of people didn't understand, so I've tried to do a rational reconstruction of the discovery of such a type.
Right, the com.* style of java would be way overkill. As far as I know, there's no one selling commercial libraries in Haskell, so they're pretty much all on hackage. In that case, just using the package name as the the unique name would be fine.
Tried going with RC1 for a while, too painful due to too many package constraints not being updated for new versions of packages.
How hard was it to implement the improved error reporting? I tend to think that bidirectional checking makes this a lot easier, but I'd certainly appreciate an independent report by someone whose didn't already have a research interest in bidirectional methods.
Excited to have the iOS binaries in — please do give them a try and let us know how you get along! (we're in #haskell-mobile and the mailing lists, and you can report bugs at https://github.com/ghc-ios/ghc-ios-scripts)
&gt; I have two web-applications that use Snap, both ~7000LOC. How much work will it be moving them to Snap 1.0.0? We've tried very very hard not to break too much user code in Snap 1.0, but of course anything dealing with enumerators would have to be rewritten.
I agree. Also if I recall correctly, xz can sometimes take up a lot of CPU time to decompress. Not always, but there can be a performance hit.
About your type error, did you make sense of it yet? Check the type of the (:) constructor (which determines the type of d in your pattern) and check the type of (++). Are you sure the types line up? :)
&gt; without _looking_ our sense of where it is in the structure? Bit of a glaring typo in the first sentence.
&gt;(p1 ++ d) should return a list of cards, Here's what hoogle has to say about [the type of ++](http://www.haskell.org/hoogle/?hoogle=%28%2B%2B%29). Reread the type definition, then think about how that applies to p ++ d.
Thanks for giving my brain a 15th explanation of lenses. In all seriousness, I need all the lens tutorials I can get. Eager to read this when I have a minute. How would you feel about putting the markdown for this on github, and I could fix typos for you and send pull requests? ... Come to think of it, it would be neat if school of haskell could fetch files directly from github - keep everything in sync.
If only we had usable grammar checkers. :\
Assuming you have enough RAM to avoid swapping, XZ is consistently fast to decompress, and consistently much faster than bzip2. Decompression speed is approximately a constant number of compressed bits per second, so better compression -&gt; faster decompression, up to the limit of your output bandwidth. The caveat is that depending on how it is compressed, XZ can require as much ram to decompress as you effectively want. This is detemined by the dictionary size compression parameter. These were compressed `-9e`, so decompression requires a touch over 64 MB of ram. This is the largest preset dictionary size, though you can specify larger dictionary sizes. XZ compression speed is largely determined by the CPU effort parameter, although the dictionary size does have a small (but not consistent) effect on compression speed in my experience. The CPU effort only directly effects compression, so more effort spent on compression actually *speeds up* decompression if it results in a better compression ratio. Using the defaults, XZ usually takes more time to compress than bzip2, however you also tend to get better compression ratios. If you scale back the CPU effort parameter (thus always speeding up compression and usually slowing down decompression) so that XZ takes a comparable amount of time as bzip2, you still tend to come out significantly ahead. XZ is a bigger improvement on BZ2 than BZ2 was on GZ. Software that handles xz compression is easy to come by, and most linux distros now use it to distribute packages. And we may be able improve on this result by tweaking the compression parameters further, such as experimenting with BCJ filters. I know it'll be the only format I download going forward. (And consider this; xz is small, even if you download xz for one decompression of GHC and then delete it, you are still coming out *way ahead* in terms of network bandwidth.) In one case where I'm using xz, I'm getting a 230x compression ratio with `-2e`. This is 10.6 times better than `bzip2 -9`, and 36.9 times better than `gzip -9`. XZ takes about 3x longer to compress than `bzip2 -9`, and 14x longer to compress than `gzip -9`, with decompression speeds 18x faster than bzip2 and 2.5x faster than gzip and plenty fast enough to saturate a slow output channel (such as piping to `wc`). Before, when I was using the default `-6` option, compression was roughly as fast as `bzip2 -9` and still 7 times smaller. Of course, this is a rather extreme case, most of the time the differences won't be nearly as dramatic, but this is still a *real* use case.
I believe the author is mike_meyer who replied to me above.
Hmm. W: Failed to fetch http://ppa.launchpad.net/hvr/ghc/ubuntu/dists/saucy/main/binary-amd64/Packages 404 Not Found W: Failed to fetch http://ppa.launchpad.net/hvr/ghc/ubuntu/dists/saucy/main/binary-i386/Packages 404 Not Found edit: I guess these are *Only* for 12.04 edit2: The child comment workaround is good
downloaded and built source just for kicks. Did sudo make install and now ghc has version 7.8. Problem is, right now cabal-install tells me that its version is 1.16; I want to try compiling someting that needs 1.18. If I do cabal install cabal-install, then I get a long list of libs that will break. Going ahead with --force-reinstall doesn't work. What now? 
Type witnesses don't seem very open to extension. I can see you can combine them, but it seems you can only combine other types which have witnesses already. How would you go about making something similar but more open for extension?
What I don't like about that kind of naming scheme in other package systems I've used is how it elevates the importance of one author. It seems to discourage collaboration, and make transfer of ownership slightly awkward. Things like github groups can help, but I still prefer to think of projects as having their own identities beyond the one person who wrote version 0.1.
We actually [moved in the other direction](https://github.com/feuerbach/tasty/issues/37) recently, exactly to avoid an external dependency. It's true, however, that regex-tdfa takes its time to build. I would easily trade it for a simpler (but well-tested) implementation.
Keeping track of this is a great contribution to our ecosystem. Thanks for doing it!
&gt; xz is small, even if you download xz for one decompression of GHC and then delete it, you are still coming out way ahead in terms of network bandwidth A really cool point, but on the systems where xz isn't already installed, network bandwidth may not be the metric of interest. e.g. it may be a locked-down corporate system where getting xz up and running involves user effort. A configure/make/make install cycle + one error of any of the kinds familiar to people who have been through this rigamarole before will shoot your bandwidth savings to hell when you switch to measuring wall-clock time spent. For that kind of scenario, knowing there's a widely-supported format to fall back on is kind of nice.
At the end Gabriel predicts that lens will have as significant an effect on Haskell programming as monads. Does lens have type classes that fit in the Typeclassopedia interrelationships diagram ?
Lens is a type, not a typeclass. type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; (s -&gt; f t)
Ah ... finally someone who is better at remembering extensions than me :)
...wouldn't that rather be a reason to keep `.gz` as an alternative around (which is even lighter in terms of decompression resources) instead `.bz2`? Otoh, if you don't have enough CPU power to decompress the GHC tarballs, I wonder if using GHC itself will provide a good experience at all...
His math logging example is problematic, because you'd use writer or either, not state. It would still be cumbersome to use though. Of course, monadic combinators for each type would solve that problem. Then the generalization step to a type class solves the next problem which is hard to discuss before understanding the first problem. Which goes to explain why monads are hard to teach.
Output is Prelude&gt; :sprint l2 l2 = _ : _ Prelude&gt; :sprint l1 l1 = 1 : _ under 7.6.3, so that seems unusual unless I'm missing something.
Warning: There are probably thousands of people in this forum alone that are vastly more qualified to talk about this than me. In case none of them post in here, I'll give it a try: Type witnesses are the visitor pattern of strict static typed programming and have all the same draw backs. If you find yourself needing witnesses this is probably already a smell and you should step back and see if there might be a more type friendly alternative approach. If there really isn't and you don't want to have to make all these updates any time a new type is introduced then I think the only alternative remaining to you is go all-in and just use Data.Dynamic.
OK, I found my answer [here](http://www.reddit.com/r/haskell/comments/1zfz5m/a_superficial_exploration_of_haskell_part_2_lazy/cftcg7b). I am, admittedly, a bit confused in the first place. ;)
There are a few different answers to that. On the ground, the new DDC has better error reporting because I have a much cleaner understanding of programming language semantics now than when I was doing my PhD. I spent a year of evenings learning Coq and formalising functional core languages, and it really helped. I also have more software engineering experience, and less pressure to get something working in a short amount of time. I think in many cases compilers have bad error messages because the developers haven't put the time into producing good ones, rather than there being a fundamental algorithmic problem. The type inferencer in the original DDC was constraint based, inspired by Bastiaan Heeren's PhD work. I added justifications to the extracted constraints saying where they were from, but all of that is defeated if you don't put the time into generating good justifications, or you don't really understand the language you're working with. I've recently started using Scala professionally, and the error messages produced by the compiler are also pretty bad. However, much of the immediate pain is because the compiler doesn't format the messages in a nice way, and doesn't elide module prefixes from identifiers. You really have to cover the basics before starting to worry about deep algorithmic concerns like sub typing, or what to do about EDSLs. 
&gt; The major difference between a thing that might go wrong and a thing that cannot possibly go wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out to be impossible to get at and repair. Douglas Adams 
Don't worry, lens (the library) has plenty of scary classes.
Java's packages are designed for computers, not humans to use. I'm a human.
As a follow-up to this, I do think that doing unification / solving constraints while walking over the source AST makes it easier to generate error messages. Bidirectional checkers naturally work in this way. The alternate approach of first extracting a set of constraints and solving them HM(X) style is nice conceptually, but tracking the provenance of each individual constraint requires significant engineering work. I haven't yet implemented a type checker like GHC's, which has some bi-directionality but also defers solving some constraints (as I understand it). 
I have run a few HalVM programs recently. Though I have not used Xen enough to install it fully and correctly, in particular the xenstore is having some issues. Since the xenstore is used for some of the cross domain communication in the examples they are not all working. The others in example/Core seem to work though. I installed xen 4.4 from source, I was able to install this version of Xen compared to the last time I tired to install halvm with a different Xen the last time it was on reddit. I forget where I set the option for limiting dom0's initial memory, this is necessary to free up memory for the other domains where the halvm programs will run. The option to automatically steal memory from dom0, which is on by default, was not working for me. Compiled HalVM from source, following the instructions on the github readme. As with compiling most cutting edge software I let `./configure` complaints and `make` complaints be my guide. That is run configure see what is says I am missing install the missing component and run configure again. I think there was a library or two that I need to add when make failed as well. If you are not familiar with the above process stick as much as you can to using the recommended version of linux and what ever standard package management system it comes with. Install from source only after that does not work. It was easier to install then the last time halvm was on reddit. I was able to figure out all roadblocks in my first three attempts. There was some conflict with libraries that I had being installed counted twice(the normal version of GHC that is used to compile halvm not halvm's GHC) which was throwing off the compile so I stripped it down to a bare GHC installation which seemed to fix the problem. To start Xen: /etc/init.d/xencommons start -- start xen in ./examples/Core/Hello /usr/local/bin/halvm-ghc Hello.hs -- I think the make file actually takes care of this make This will run the halvm Hello executable(is that what you call it? I do not know the right Xen terminology) which prints a message to the debug console and prints it out. The contents of the make file are something like: /usr/local/bin/halvm-ghc Hello.hs xl create Hello.config -- Create and run sleep 1 -- wait xl dmesg -c --- print out debug console I also had to delete a setting in Hello.config that my broken install of Xen did not support to get the examples to run. This was my most time consuming project last weekend. Communication between domains is key to having fun though and the examples I looked at use xenstore to do this so they were not working for me. I will have to fix my installation of Xen to start having some real fun. Edit: Thanks to the developers who have made it much easier to install over time.
There's definitely a case you haven't handled in your pattern matching. See if you can spot the unhandled case. distribute [] (y:ys) = ... distribute (x:xs) [] = ... distribute (x:xs) (y:ys) = ... If you can't spot it, try listing out the possible cases for 2 lists.
A few notes on the code you have here (and in other comments). You do not need to constantly equate with Bools. turn n (p1, p2, deck) | haveAny n p1 == False = ... | haveAny n p2 == True = ... -- becomes --------------------------- turn n (p1, p2, deck) | not (haveAny n p1) = ... | haveAny n p2 = ... Similarly, checking against empty list is best done with `null :: [a] -&gt; Bool`. | haveAny n p2 == False &amp;&amp; deck == [] = ... -- becomes --------------------------------- | not (haveAny n p2) &amp;&amp; null deck = ... Try to extract common subexpressions into a name. remember you can use pattern matching within let and where bindings. ... = ((p1++(fst(pickOut n p2))),(snd(pickOut n p2)),deck) -- becomes ----------------------------------------------------- ... = (p1 ++ fst cards, snd cards, deck) where cards = pickOut n p2 -- or ------------------------------------------------------------- ... = (p1 ++ ns, rest, deck) where (ns, rest) = pickOut n p2 In other comments, you are pattern matching lists into pieces and then immediately recombining them (and visa versa). It is possible to pattern match and also bind the original list to a name, and it is incredibly useful. turn n (p1, p2, deck) | ... = (p1, p2, deck) | ... = (((head deck):p1),p2,(tail deck)) -- becomes ----------------------------------------------------- turn n (p1, p2, deck@(d:ds)) | ... = (p1, p2, deck) | ... = (d:p1, p2, ds) Finally, guards are checked in-order, just like pattern matching. For you third guard, you specify `haveAny n p2 == False` but this must be true if `haveAny n p2 == True` has not matched. Similarly, the final guard will always be true if no other guard has matched, and can be replaced with `otherwise`. turn n (p1, p2, deck) | haveAny n p1 == False = ... | haveAny n p2 == True = ... | haveAny n p2 == False &amp;&amp; deck == [] = ... | haveAny n p2 == False = ... -- becomes --------------------------- turn n (p1, p2, deck) | not (haveAny n p1) = ... | haveAny n p2 = ... | null deck = ... | otherwise = ... I hope these tips help you write clearer, cleaner haskell code. Remember, the easier it is for you to read, the easier it is to revise come exam time! 
I think you would be very interested in these two functions from `Data.List` any :: (a -&gt; Bool) -&gt; [a] -&gt; Bool partition :: (a -&gt; Bool) -&gt; [a] -&gt; ([a], [a]) Using these two functions and a new helper function, cardHasValue :: Int -&gt; Card -&gt; Bool cardHasValue n (Cd _ m) = m == n we can rewrite `haveAny` and `pickOut` in a far more natural way haveAny :: Int -&gt; Pack -&gt; Bool haveAny n pack = any (cardHasValue n) pack pickOut :: Int -&gt; Pack -&gt; (Pack,Pack) pickOut n pack = partition (cardHasValue n) pack
Why does that affect the result in this way? I would expect an "undecidable" error here instead.
Very good and helpfull read! Finally I start to understand the magic of this lens construction...
My Raspi likes pre-built binaries a lot. (Probably because it doesn't have the RAM to compile it in the first place.)
Sorry, too much spam for me. If it would *only* post the challenges and not random conversation, I might subscribe.
I don't have a twitter so here is [my solution](http://codepad.org/YCiatGxS) to [today's problem](http://codepad.org/TMdPTm7C).
Aside the Q&amp;A, most tweets are replies and should be filtered by your twitter client. I will try to be less verbose anyway, thanks for pointing this out.
Sorry, I'm using the twitter website which does not filter at all / displays one thread for every conversation.
Yes, I can understand that it may be painful if you don't have a way to filter replies. In this case, you may want to follow the github: https://github.com/1HaskellADay/1HAD which provides the same Q&amp;A, without the twitter noise.
You keep misspelling exercise...
Sorry, I don't want to filter out things. I subscribe twitter accounts where most of what they post is "ham". If I used a filter, however good, I might miss some of the ham. It's up to the twitter channel what it posts and I don't want to impose any censorship. If the ham-to-spam-ration is too low, I just don't subscribe.
beginner here, but really liking it. opposed to some other comments here I actually enjoy the Q&amp;A on twitter as well :) keep up the good work!
I don't think the beginning of the post really does a fair comparison of io-streams and pipes APIs. By emulating the highly imperative io-streams approach directly, the pipes-based solution ends up looking less elegant. Instead of looking at the bare-bones APIs for comparison, I'd rather see a comparison of a slightly more involved problem using idiomatic implementations for both libraries. At least for me, that would be a more informative explanation. (Not ripping on the blog post, it was answering a question that users were asking. I just think a different question would have been more helpful.)
`checkSort = and . (zipWith (&lt;=) &lt;*&gt; drop 1)`
This is the right way to give a monad tutorial -- i.e, link to good monad tutorials that already exist.
See my [reply to Michael](http://www.reddit.com/r/haskell/comments/1ziqge/haskell_for_all_how_to_model_handles_with_pipes/cfu47pe.compact), which contains such an example. Another example is RPC-like interfaces where you want to parameterize each request for input with an argument. Edit: Danny Navarro release a library today that provides an example of this. The following code includes a request for a bytestring chunk that is parametrized by the size of the chunk to return: https://github.com/jdnavarro/pipes-p2p/blob/master/src/Pipes/Network/P2P.hs#L298 Another example is prompt finalization. Michael once privately circulated a draft of conduits where `yield` would return a value to signal whether downstream is dead or not. This is exactly what `respond` does, the bidirectional generalization of `yield`. Another example is one somebody showed me on IRC where somebody used an abstraction isomorphic to a bidirectional client to build up a printable decision tree.
`tail` works too as `zipWith` doesn't evaluate it if the first argument is `[]`.
Monads are hard because the meaning of the word "monad", as used by people in the community, is often vague. In Haskell, users are not really exposed to *monads*. They are exposed to *types* that *offer* a monadic interface, among other convenient interfaces, for cases where it can be useful. So instead of speaking of "monads", I think we should speak of types with the monadic interface and what this interface does. Most of time, the monadic interface is not mandatory at all, and we only use it because it is *convenient* for a particular usage. 
Lenses form a hierarchy of abstractions because they reuse Haskell's existing hierarchy of types and type classes. For example, `Lens`es type-check as `Traversal`s because `Functor` is a superclass of `Applicative`. Both `Traversals`s and `Lens`es type check as `Setter`s because `Identity` implements both `Functor` and `Applicative`. `Lens`es type check as `Getter`s because the `Constant` functor implements `Functor`.
Mine was similar: `(and . uncurry (zipWith (&lt;=)) . (id &amp;&amp;&amp; tail)) `
Nice! But I'm in the habit of just not using `tail`, ever.
"It’s all so simple now. The key to understanding monads is that they are Like Burritos. If only I had thought of this before!" http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/
Maybe I'm insufficiently familiar with `io-streams` and too familiar with pipes/conduit style streaming, but to me the post felt a bit confusing: let's build up an intuition in the direction of io-streams, but we actually don't want to use it because io-streams has all of these limitations. (Though I think your blog post *does* do a very good job of demonstrating some of the limitations of io-streams-style composition.) Maybe I just made a bad assumption about the goal of this post based on the introduction. Maybe a few lines in there about the setup of pipes-streams and its purpose would have helped me out a bit.
Hm. I wish `dup` were part of `Prelude`. checkSort = uncurry (==) . first sort . dup where dup s = (s, s) (obviously not the coolest way to do this)
This one was too tricky for me. I had to work through the types to figure out what it was doing. Here's the work for anyone else that was confused: zipWith (&lt;=) :: Ord a =&gt; [a] -&gt; [a] -&gt; [Bool] drop 1 :: [a] -&gt; [a] (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b The `Applicative` instance in this case is `((-&gt;) a)`, or `((-&gt;) [a])` specifically. That turns `&lt;*&gt;` into: (&lt;*&gt;) :: ([a] -&gt; [a] -&gt; b) -&gt; ([a] -&gt; [a]) -&gt; ([a] -&gt; b) `b` then is `[Bool]`, which gives us our final type for `(&lt;*&gt;)`: (&lt;*&gt;) :: ([a] -&gt; [a] -&gt; [Bool]) -&gt; ([a] -&gt; [a]) -&gt; ([a] -&gt; [Bool]) At which point everything fits into place simply enough.
Unfortunately, after that they also become hard to motivate (for users of other languages). People often grasp the motivation that "Monads are required because without them IO is impossible in these weird freaky pure languages" and then run to understand IO. They later learn that Maybe and Writer are also Monads and wonder why the hell anyone would bother giving a name to that stuff. I think the real key is to rush past Maybe and Writer and get to something like Parser or some transformer stack with LogicT in it. These might not be as easily understood from ground zero, but they provide such a completely different and interesting `do` semantics that they motivate people to figure out what else you can do with a clever transformer stack.
"The files are IN the computer... It's so simple!" --zoolander quote
FWIW, for @HaskellTips I have a separate browser signed in to my real account that I use to respond to people.
My version `checkSort = sort &gt;&gt;= (==)`
&gt; implying trying to write code in a sound way makes you paranoid.
the `zipwith (&lt;=)` variant can work nicely with `dup` as well (but then: arrows are cleaner i would say)
Thanks. I don't know how I will deal with it yet but it may end this way.
arguably because while in iostreams you have more types, but which are themself much simpler, there is only one (core) type in pipes, which certainly is more complex. i am speaking without diving deeply inside iostreams though.
join (,)
I think you need to call unloadAll (or similar) before you load the symbol again.
There is a 'View Markdown Source' link in the upper right
Ha, clever
Not as such. You might write one -- | (pretty insufficient, but the general idea is intact) class Lensy s t a b l | l -&gt; s, l -&gt; t, l -&gt; a, l -&gt; b where get :: l -&gt; s -&gt; a modify :: l -&gt; (a -&gt; b) -&gt; (s -&gt; t) But the cleverness of the `lens` encoding is that it sneakily uses Haskell's subtyping facility to express how lenses relate to a bunch of other "focusers" like Prisms and Traversals. The whole thing is *almost* "just a design pattern", except that as you use that pattern more and more and abuse that clever encoding more and more you end up in a totally different world which completely changes your notion of how to "look inside of" structures and do stuff.
Great suggestion. I will update it tonight. Edit: Updated to clarify that the goal is not to reimplement `io-streams`, but rather that `io-streams` is just a useful metaphor on the way to understanding `pipes` handles.
&gt; Is there some reference that documents what you need to do to switch compiler versions All you need to do is put the appropriate `ghc` executable in your `$PATH`. For example, if you install the binary distribution: $ tar xf ghc-... $ cd ghc-... $ ./configure $ make install Then it will by default install into `/usr/local/`. Your system compiler (possibly installed by your package manager) probably lives in `/usr/bin/`. You can just delete the `ghc*` binaries in `/usr/local/bin`. You can also prefix the installation when running `./configure` $ ./configure --prefix=$HOME/ghc-7.8.1-rc2 $ make install (This is what I do). Now, the binaries will all install into `/home/a/ghc-7.8.1-rc2/`, and you can just delete that whole directory to get rid of it. You can also make it the 'default' compiler for your shell: $ ghc --version The Glorious Glasgow Haskell Compilation System, version 7.6.3 $ export PATH=$HOME/ghc-7.8.1-rc2/bin:$PATH $ ghc --version The Glorious Glasgow Haskell Compilation System, version 7.8.0.20140228 Now, when you use `cabal`, it will use the new version of GHC. Just close your shell or open a new one to use the old GHC. You can install multiple copies of GHC binaries this way. ----- Now, to answer your initial question: You need `cabal-install-1.18` at least. You can upgrade to that with your existing `cabal-install`, after removing the old copy of GHC you installed in `/usr/local`. ---- &gt; Does that install haskell libraries that are incompatible with 7.8? They are built with your package manager, and are not compatible. You can install new copies once you install a new GHC like above, though. &gt; Do I need to remove haskell-platform if I'm going to be switching between ghc versions? It kind of seems that way. No. They won't conflict as they won't overwrite each other - you just need to set your `$PATH` to switch.
Actually that's what I came up with too. For me it's much easier to understand what &amp;&amp;&amp; does (apply 2 functions on the same input, collect the results in a pair). Is there a rule of thumb to go from &amp;&amp;&amp; to &lt;*&gt; ?
Yep but that's a one-way street. I'm thinking more about easy ways for keeping the tutorials updated, without troubling the authors beyond them having to click 'merge'.
I've now released Cabal-1.18.1.3 and cabal-install-1.18.0.3 so it's possible to upgrade if you're testing 7.8.
This is great, thanks for setting this up. I wasn't aware you could donate (and now I will!). I think the last point deserves some attention. Quoted for those that haven't taken the survey: &gt; Would you be more likely to donate in order to fund a project of interest to you, rather than to support haskell.org in general? I understand the desire to put money towards specific projects, people like to contribute to things they use. My worry is that this would cause huge differences in the quality of the Haskell ecosystem. The fact of the matter is that the Haskell community is still relatively small, and that pooling together our resources is likely to be more effective at improving the state of the Haskell ecosystem (specifically tools!). I think it's more likely that the members of the haskell.org committee will see which areas need the most attention and allocate accordingly. As long as the discussion is transparent of course. Maybe I'm just being a worrywart :) EDIT: In case you didn't take the survey and you didn't know that you could donate, [this is the link.](http://www.haskell.org/haskellwiki/Donate_to_Haskell.org)
When you write (re: InputStream transformations): &gt; The pipes version does not need to explicitly thread the input stream We don't either -- usually you use the Kleisli category and connect with `(&gt;=&gt;)`.
Spoiler: I could have guessed that Cabal and Hackage would be the top priorities, and it's interesting to GHC matching them. From my perspective it seems like Cabal and Hackage don't get the attention they deserve, whereas GHC seems to get a lot of attention. I might be wrong about the latter though, and in any case would certainly be happy to see even more work go into GHC!
Just donated $100. Here is the link to the donate page. https://co.clickandpledge.com/advanced/default.aspx?wid=69561
Ah, interesting. In a sense it'd be a compromise between the two approaches. I could definitely see that working. Thanks for the clarification. :)
To me a lot would depend on how we ensure the success of those funded projects (and how we define success). I assume you'd need to get some experienced developer. I'm not sure how much of the issue is money rather than time. I guess that leaves mainly consultants (e.g., Well-Typed).
I think you'll get more money if you display the Donate button prominently 'above the fold'. It is really hard to find on Haskell.org.
What I meant was that you thread the input stream in the definition of the transformation. I wasn't referring to the way you connect it. However, now that you bring this up, even connecting `io-streams` transformations is slightly trickier if you want to feed one input stream to multiple transformations. This requires making the input stream explicit or layering a `ReaderT (InputStream a)` monad transformer on top of `IO`.
 https://docs.google.com/forms/d/1rEobhHwFpjzPnra9L1TmrozWNFFyAVNPmdUMCcT--3Q/viewanalytics
I answered "yes" without really thinking, but agree with you. I think my thought process was "I (embarassingly) don't really know what the haskell.org commitee is and does (although I'm sure they're awesome)". Off to donate.
There's a common abuse of notation in mathematics which on the n-lab wiki they refer to as "equipment". The definition of equipment is: when we say "`X` is a `Y` equipped with `Z`" we mean that technically `X = (Y,Z)`, however in practice we often leave the `Z` implicit and consequently say "`Y` is an `X`" instead of the more technically correct "`(Y,Z)` is an `X`". For example, in category theory, we say "`A×B` is a product" when in fact the product is the triple `(A×B, fst, snd)`. However, the `fst` and `snd` projections are typically considered to be equipment, and therefore left implicit. So, in terms of capturing the typical **use** of the word "monad" in Haskell, it would perhaps be best to say that "a monad" is a type constructor, `M`, *equipped with* a `Monad` instance. This is why we say that "`M` is a monad" when in fact the monad is `(M,i)` where `i` is some representation of the `Monad M` instance. I'm not saying this is the best use of the term "a monad"; just that it captures the actual use of the term in practice.
http://www.reddit.com/r/haskell/comments/1y9e2x/thank_you_for_the_donations_to_haskellorg/cfjg5pm
I think the issue is that a Monad as a concept is such a general idea that it just takes time to sink in. You can't read a single tutorial and expect it to click immediately.
That's a beautiful use of the Reader Monad. I came up with `all (null . tail) . groupBy (&gt;)`.
donated! thanks for the direct link
Nice line up. Although I definitely do enjoy having these well known names on the Cast I would also be interested in hearing from some unknown names that still use Haskell. Maybe developers using Haskell in their day job or some of the less known library authors. This might help to widen the community and make it less enclosed. Of course I could understand if people would prefer that The Haskell Cast stuck with big names, that's a valid aim as well.
Cabal has had good success with GSoC (cabal test, cabal sandbox, parallel builds.) Perhaps we could run a small scale program like that for 1-2 promising students.
That looks like a definite improvement to me, thanks!
Ah, that's nicer than my `join ((==) . sort)` solution. Unfortunately, the `sort` approach doesn't give a result for input like `[0,-1..]`. :(
One that's on my list (Hey I'm a student, might do it): Add authentication schemes to the Remote modules in acid-state.
The post did mention it
Excellent news. In my opinion, diversity of its members is one of the challenges the haskell community is currently facing. In that light, having some of the funding be project-agnostic but explicitly targeted towards that goal could be neat.
Actually I would like more uniform *package naming*. And, as a side effect, this would solve the issue (well, mostly).
I have an idea for hackage. Maybe include [discourse.org](http://www.discourse.org/) on package sites. It is a blog commenting/forum software, but mainly it sets up a local forum. Opensource/free etc. 
I think is a great research level Haskell paper. - Well written. - Content motivated by multiple applications (that are clearly relevant and not merely constructions to illustrate a point). - Apart from very sparse mentions of natural transformations and forgetful functors - which are not *required* to understand the content - light on category theory (sections 1-6). The proofs in section 7 mention some more category theory, but definitions are introduced/defined with respect to the current topic, and not in all its categorical g(l)ory, making it reasonably understandable without deeper knowledge. Only section 8 dives a bit deeper into the mathematical section, but this is merely a nice thing to have (for some value of nice) and not required to benefit from knowing about FreeA. - Detailed equational reasoning to prove definitions correct.
That paper was rejected from ICFP and Haskell Symposium 2013. This the version accepted to MSFP 2014. It's basically the same paper, with some minor changes, reworked examples, and a new section about interpreting the construction in locally presentable categories.
Thanks! We strived to make the paper readable and enjoyable by people who don't necessarily know much category theory, but are familiar with Haskell and equational reasoning.
The diagrams [here](http://ro-che.info/articles/2013-03-31-flavours-of-free-applicative-functors.html) may be handy in understanding how various free applicatives work.
Wow! I had never thought of using a construction like `liftA2M`to embed a free Applicative into a free Monad. I feel that's exciting because it would let you seamlessly use a restricted, statically analyzable language within a more expressive monadic language. Has anyone experimented with a technique like that (which might just be spoilers, I'm still reading the paper). Offhand it seems there remains a challenge since once you embed an Applicative fragment inside of your Monad you'll lose access to the available analysis up until the point at runtime you evaluate your monad sufficiently to reach that Applicative.
Typo: in section 1.4 (Example: applicative parsers), 4th paragraph, second line "is necessarily \emph{just} an**d** applicative".
Another reason to wrap `ghci` rather than using `cabal repl`: You can only use `cabal repl` if there is a `cabal` file in the sandbox directory. You can't install packages into a sandbox and then use them with `cabal repl`. And if the `cabal` file is present, `cabal repl` will install its build dependencies; I think that is the "side-effect" that Roman is referring to. 
&gt; Another reason to wrap ghci rather than using cabal repl: You can only use cabal repl if there is a cabal file in the sandbox directory. In HEAD `cabal repl` works anywhere. &gt; And if the cabal file is present, cabal repl will install its build dependencies; I think that is the "side-effect" that Roman is referring to. Precisely.
Oh certainly for general loop breaking you can get away with just a failure-alike monad, but I don't know how to do that nicely within the pipes universe and the particular Producer-for therein. And I suppose I was arguing that yeah, it is worth it. The complexity is nicely hidden unless you import Pipes.Core, though it could be more hidden with a newtype wrapper.
Well, you could decorate your monad with the analyses perhaps—this is similar to what was suggested in the paper. liftA2M' :: (Embed f, Analyze f) =&gt; FreeA f a -&gt; Free f a liftA2M' fa = embed (analyze fa) (liftA2M fa) class Analyze f where type Analysis f analyze :: f a -&gt; Analysis f class Analyze f =&gt; Embed f where embed :: Analysis f -&gt; Free f a -&gt; Free f a data EmbedF f x = = Embed (Analysis f) x | Other (f x) Or something
How about auto finding the closest ancestor that contains a `cabal.sandbox.config`: ascend(){ file=$1; if [ -n "$2" ]; then dir=$2 else dir=`pwd` fi if [ -e $dir/$file ]; then echo $dir/$file elif [ "/" = $dir ]; then false else ascend $file `dirname $dir` fi } alias cabal='CABAL_SANDBOX_CONFIG=`ascend cabal.sandbox.config` cabal' 
Thank you!
In this snippet: data FreeAL f a = PureL a | ∀b.FreeAL f (b → a) :*: f b infixl 4 :*: I don't really understand why `f b` isn't instead `FreeAL f b`. In my mind I expect `:*:` to be applied repeatedly until a `PureL` value is encountered, but it seems that if you don't have the `FreeAL` constructor in there, it wouldn't chain (This is all horribly informal language, I realize, but hopefully you get the idea). Is this a mistake or am I missing something? Edit: Does it have something to do with `:*:` being left-associative?
I'm not really sure what you're asking, but with `f b` you essentially get a "list" of applicative applications, whereas if you had a second `FreeAL f b` you would get a tree. Does that help?
Fuck, I thought this Haskeleton package was going to set up all these glorious build/testing tools for me. So disappointed when I got to the end and finally realized. Thanks for introducing me to a bunch of new tools though! (HSpec in particular) *Edit:* Idiot me didn't realize the point (so far) is to clone this repo to get a starting point. So effectively I can get what I was disappointed about not having. My bad!
So, I think I understand the right-associative definition more (maybe since it's more similar to a list?), since I can see that I can unwrap the definition of `:*:` to get a chain of applicatives, for instance, I could do this: f (b → a) :$: f (c → b) :$: Pure b ...and so on. I'm having difficulty seeing how to chain these things together in the left-associative definition. Actually while writing this, I think I am starting to see it, but I'm not sure if this is really correct. Would this be a valid type for a `FreeAL f a`? f (c -&gt; b) (PureL (b -&gt; a) :*: f b) :*: f c Or am I still off?
I really want it to do that! For the time being, you can clone [the repository](https://github.com/tfausak/haskeleton) and search/replace "haskeleton". 
Nice paper. I hadn't realized the role that accessibility plays in constructing initial algebras, though in retrospect it's not surprising given that there is no fixed point for the covariant power set functor on Set (a standard example of a non-accessible functor). In section 9 you write that when C is locally presentable, then C* (the free monoidal category on C) is too. But C* is the disjoint union of powers of C, so it doesn't have an initial object, for instance. C* is still accessible though (it has connected colimits) and I think, though I didn't check carefully, that everywhere you make an assumption that "B" is a locally presentable category, you only really need it to be accessible.
Ah, thank you. That actually makes much more sense.
I've been a bit afraid to do that so far. I'm probably going to try in a presentation very soon (calling it the Command pattern "on steroids"). Really, though, if you want to get a great picture into monads then Free is a *fantastic* choice.
&gt; With that purpose in mind, I plan to launch some Haskell cheat sheets, with different levels of complexity and applicability, but with some clear goals in mind: This is a wonderful goal! I look forward to the results. Do you plan to maintain these out in the open (Github + issues)? I imagine many would love to contribute to this effort. The beginner -&gt; intermediate space in Haskell needs a bit more love, in my opinion. :)
Oh, you're right, thanks for catching that. I think it's true that accessibility of B is enough, but I'll have to check. I just assumed that everything was locally presentable to keep things simple, but somehow forgot to notice that C* doesn't have coproducts. :)
Hi, is there any update as to the status of the effort to open source this? I'm interested in seeing the code.
I was going to, but fujimura beat me to it with [hi](https://github.com/fujimura/hi)! I'll probably implement Haskeleton as a template for that. 
I know that feel bro.
The `sandbox` function looks perfect for putting in a prompt.
Do you mean the dependency on `hscolour`? I wasn't sure if it was required or not. I kept it in the post because I wanted to explain how to use flags in Cabal.
Creating this sort of project skeleton repository is a good way to manage the boilerplate associated with projects. Unfortunately (for this instance), git (and github) are bad tools for executing this strategy.
I think that makes sense, particularly after taking a look at the diagrams in the document listed earlier in the thread. It sounds like the free applicative you wrote corresponds more closely to the other definition given in the paper, namely: data FreeA f a = Pure a | ∀b.f (b → a) :$: FreeA f b infixr 4 :$: Which makes more sense to me. 
The problem with git is that its data model does not include information about file renames. It just tracks adds and deletes, and tries to use a heuristic to determine when something was moved. And I've seen it do some things that make me not very confident in its heuristic, even. Now, for something like this, one thing you may eventually want to do is tweak the skeleton and merge the changes forward to all your projects uniformly. However, by that point, you'll probably have made significant changes to your renamed .cabal file, and it's possible or even likely that the heuristic git uses will not figure out that a rename happened. So things will mess up. Many other version control tools---mercurial, bazaar, darcs, even subversion---properly track file moves (mercurial can even know about copies), so they don't have this problem. I don't know about stuff like yesod init; I assume it doesn't support this use case at all. The problem with github is that it only allows you to have one clone of a repository per account. So for instance, if someone forks your skeleton on github to start a project, that's the only one they can do that way. Others would have to be cloned outside of github and then imported, so that github doesn't know about the relationship. And similarly, I don't think you can fork your own repositories. It's probably not a huge deal, but it's silly. I'm not sure how other code hosting sites compare to github on this.
Oops. I missed that on my first reading.
might be worth thinking about a file-watching plugin for tests too - something like guard for ruby. I tend to hand-script mine with inotifywait, but it's a pain to do each time.
The aptly named [guard-haskell](https://github.com/supki/guard-haskell) handles that well. I didn't include it in Haskeleton because it seemed silly to do everything in Haskell then require Ruby for Guard. 
This is what it looks like. http://eviltrout.com/2014/01/22/embedding-discourse.html It seemed like a good idea at the time. I was just hoping, for ocharles style collection of user tutorials, and faq's to take care of themselves. 
When I read "Server 1: ALL CAPS" I could hear* the melody from the track linked below: Madvillain's All Caps. http://www.youtube.com/watch?v=ewc1hixzYPY *) I did not actually hear it.
Have you thought about using branches for the use case we're talking about? The point is to create a repository with the boilerplate for a Haskell project, clone it as a base for every other project, and forward merge all common changes to the boilerplate. To suggest git branches for this is to suggest storing every Haskell project you ever create in a single git repository, with branches for each project. I think git branches are not very good in general, but for this case it seems obvious that they are not the right solution.
We have a similar script at Silk that wraps cabal-dev and picks the sandbox by finding the closes .git directory and using or initializing eg ~/.cabal-dev/Users-adam-silk-repo/branchname/. That way you don't have to worry about switching sandboxes, and the branchname is really nice for big projects. We also use the normal ghci (but wrapped through that same script).
The `sandbox` function output is too verbose for a prompt (it's something like `/home/feuerbach/prog/tasty/sandbox/cabal.sandbox.config`). That's why I didn't do this as a prompt originally. But your comment made me think more about the issue, and I realized that I can define a name for a sandbox in its convenience function. See the updated post :)
Forward merging changes to the boilerplate isn't a goal of mine. I don't even think it would be a good idea. Consider the following: A new version of HSpec is released. It has some great new features that aren't backwards compatible. I update the skeleton to use it and make the necessary changes to my specs. What should projects that were created using my skeleton do? Automatically updating them is bad; it will break things. If they want the latest and greatest, they're going to have to read the diffs to see what changed and apply those changes to their project.
I think it would be easier how to understand how it works if the examples were a little more practical. Realistically, you're pretty much never going to want to transform the data per-character. It's almost always going to be line based, delimited (ie, by some token like CR LF CR LF as in HTTP) or binary chunks. One also will almost always have handlers in separate functions. Putting everything in `main` only works for toy examples. Having separate functions in the example *with* the types defined helps a lot in learning the interface. When you have an example that does one single thing very simple with no types visible it doesn't really help (me at least) to understand the interface. I've noticed that it's really common for people to make examples in this format, and unfortunately they don't really help me. I end up having to go into the documentation and figure out the types to get stuff to work and the functions that will actually do the sort of things that are typical use cases for the package. I hope this doesn't just come off as griping and is at least a bit constructive. Certainly I appreciate that people put the work into creating these sorts of useful packages! 
Sometimes I want to use different sandboxes for the same source tree. (For example, I have separate sandboxes for ghc 7.4, 7.6, and 7.8.) There are some other cases where relying on the current directory doesn't work out. But if it works for you, why not :)
I really like and appreciate these intuitive-discovery articles, it really helps me get the concepts better when I know the start and I can follow through to the end.
works, thanks
I don't know what to tell you. Forward merging is useful. I've used this kind of skeleton setup just for making little distro packages of stuff to install on my own system, and it was already useful there. If you're not using this capability, then you're basically using git as a boilerplate generator, not using any capabilities of it as a version control system (although git doesn't have capabilities that work here, I guess). In that case it's probably not much better at boilerplate generation than several other tools, unless everyone decides to delegate all their decisions to you, or some other pre-canned git repo. And if _other_ people are basing their projects on your skeleton, and just unconditionally merging in changes _you_ made to the skeleton without looking at what you changed, then their development practices are awful. You yourself would know beforehand if you didn't want to forward merge part of your changes into one of your repositories. That's not that hard to do in any system (darcs is easiest, of course), although if you accidentally pull instead of fetch, git might screw you in this respect.
This is an issue wheee there's no way to make everyone happy with a single tutorial. When I've written longer, more realistic examples, it necessarily involves multiple concepts, and people have complained that it's too difficult to follow. The purpose of the post was to show the basics of using network-conduit to deal with multiple connections in an asynchronous manner. It's not about proper chunked data processing or program structure, which is why the examples don't touch on it at all. I think both forms of examples have their place, and I'd be happy to write up a more realistic example as well. Did you have any requests?
The hlint check would be nice as a git hook or equivalent. "No committing unless you follow my custom hlint rules . Ha ha ha!" Thanks for the article I find it useful. 
There is http://hackage.haskell.org/package/steeloverseer
Purely functional data structures: `vector`, `containers`, `unordered-containers` Everything else (parsing, numerical algorithms, library bindings) are more or less doable in a wide variety of languages, but purely functional data structures make it really easy to write sophisticated algorithms and they cannot be easily replicated in other languages.
I'd love to use this and help if I can (I may be too juniour in my haskell abilities). I've been processing some log files with haskell for the past two days and was thinking about this, too. My only contribution right now is that `pipes` is great for doing ETL in constant memory. Once you get a grip on things it has a nice, clean API and is simple to use if you've ever piped text around using bash.
For "simple" statistics, there's the 'statistics' package. There is a nice probability monad in 'probability'. There's hmatrix for linear algebra, but GPL. repa provides parallel arrays and accelerate GPU array operations. There's hlearn for machine learning. Now, there isn't much of a "go-to", standard, efficient and powerful linear algebra library, so that kind of makes the efforts a bit disparate. Carter Schonwald is working in that direction and will probably comment later on, on this thread. In the past, I called for a numerical/scientific computing task force but its success was very limited. I definitely want this to happen and Carter too. I have a few sketches here and there of tentative implementations, I have released a few related libraries, and have unreleased code for some other things (quite heavily math/AI oriented, as well as experiments with linear algebra / numerical stuffs APIs, some in Haskell98, others using many recent language extensions). So yeah, I'm interested, because I'm not happy with the current ecosystem, and we could build some really *awesome* things, leveraging automatically the power of GPUs or multicore processors, but exposed under a more or less common API, SIMD-enabled when run on the CPU. With a carefully thought API, this would make for a great experience and would help much more than get in your way for writing any scientific code without caring about things you shouldn't be caring about. We could also plug [ad](http://hackage.haskell.org/package/ad) and other cool packages like that almost for free.
Thanks!
&gt; I actually spent a bit of time the past week helping some numerical computing folks port their models from Matlab to sanish Haskell, and they are over the moon with how the new codes support them working in their problem domains. I'd really love to hear more about this. I'm about to embark on a thesis that will probably involve a lot of Matlab, and I've been trying to find ways to use Haskell instead.
I'd be really excited about something like [bioconductor](http://www.bioconductor.org/). Of course you couldn't duplicate it all, there's a lot. All I would want is types that efficiently represent biological sequences: * DNA, RNA, protein * next-gen sequencing reads * genomes, annotations on those genomes * etc Then some intuitive ways to fold or map custom functions over them: * monomer-by-monomer * in sliding windows * apply to exons vs introns * etc And finally some simple demos that tie everything together: * finding G+C content of a DNA sequence * translating DNA --&gt; RNA --&gt; protein * finding transmembrane helices in a protein sequence * etc Most biologists now use slower languages like Python (although fast with SciPi) or Perl just because they're easy to learn and start out with. But the low level of abstraction really gets in the way in bigger projects. You worry about line breaks, escape sequences, etc. and the biology is lost. Haskell is MUCH closer to how we think of biology normally--lots of things are a fold over a big list and/or described in differential equations. It would be great if all this came in parallel/concurrent versions. It would be great if you could prove statistical properties about your code. Being able to distribute a single binary of your code would also be great, because: * non informatically-inclined biologists (colleages, end users, peer reviewers) have trouble with CLI operations like package managerment or running scripts * most projects are stable/stagnant between publications, so wouldn't need recompiling often And of course graphics like in R would be great too! EDIT: Looks like a lot of this exists already! Now my biggest complaint is interactivity. It would be great to optionally install packages on import, and have Cabal hell hidden/done with, but that's not data-science-specific. It would also be great to have interactive plotting like in R.
Interactivity is far from overrated. At the end of the day your product might not need much interactivity, but for me much of statistics is about throwing data at interesting models, visualizing the results in a creative variety of ways, and determining the next iteration of modeling. I *live* in R doing this process because I can ping-pong between data frame manipulation, wide scale statistics and model computation, visualization, and hooks into samplers for large scale models. It saddens me to say in this forum that the entire process outlined there relies heavily on sloppy ambient state. Further, ghci is a terror for doing modeling since it throws away all bound variables (all computation) every refresh. Above this, though it's been less common in my life, you also want to have the ability to rapidly plot and dive into a high-dimensional data set. In my (again limited) experience, this is really interactive as well. Being able to speedily hunt down a hunch is really vital.
When you follow an account on Twitter, it doesn't show you when it posts replies (unless you're also following the people they're replying to, which is unlikely in this case). You can see the all the reply tweets by going directly to their profile page, but they won't be in your timeline at all.
I've been working off and on with a linear algebra library; the goal of which is to be type-correct (unlike Matlab, NumPy, etc). I don't mean *safe* (though that's also nice, and Haskell gives that), I mean *correct*. For example, keeping arrays, vectors, and covectors distinct. If I'm multiplying a bunch of vectors and covectors in NumPy, I have to manually keep track of things and decide whether to call `np.dot` or `np.outer`. That's ridiculous. Or, the fact that most optimization libraries require you to exhibit the proof that your parameters are stored in a representable functor; and all that packing and unpacking is a performance sink as well as a hotbed for bugs. Or all the libraries out there which believe that it's okay to zero-pad things in order to get the dimensions to work out. Or... One thing that puts this project at odds with most of the linear algebra projects out there is the fact that I'm focusing on the types. Everyone else seems to only focus on the implementations and trying to make things fast. Of course, I'd like my library to be fast, but that's not the immediate goal for me. As far as why various other projects haven't taken off, IMO, it's the types. There are plenty of cruddy systems out there already. We could reimplement them in Haskell, but what's the point? They'll still be just as cruddy in Haskell as they were before, if we don't work on fixing all the type errors inherent in the received wisdom on how to make these libraries. Unfortunately, once you start to focus on the types, that leads quickly into the mire of needing to overhaul the numeric hierarchy. And it's very easy to never make it out of that tarpit. ((*Edit:* This linear algebra stuff is a side-project for me, though it seems to keep coming up more and more often. Mainly the scientific computing stuff I work on is statistical modeling for natural language processing.))
which sorts of interactivity are we talking about? I agree that many styles are valuable, such as those you lay out, BUT those are not necessarily what other folks mean when they say it. The popular interpretation of interactivity more often veers into tools that resemble D3 than the exploratory tooling you alude to. I agree that the latter is valuable. I just quibble about what we mean when with interativity. you're talking about interactivity as "low friction iterative exploratory work". Most people think about it as "the gui data viz pretty thing". Also, try iHaskell, its maturing quite nicely for that exploratory stuff. (didn't you see my shout out thereof?)
well, first write down a list of what you need to do your thesis effectively in whatever tools you choose. Then pick the one the helps you focus on the science most. Anything else is a bad idea. If the relevant haskell tooling is mature enough, great! if not, focus on your research for now :) 
That's true. I really understand monads when I was doing a type system for a language and I "made" a monad for that (actually wrapping Either).
I personally like Mathematica. I think the main thing that sets it apart from the things mentioned here is its computer algebra features. The ability to solve indefinite integrals and symbolically solve differential and sophisticated algebraic equations just by issuing a command has proved valuable in the past. SciPy has SymPy, but it feels incomplete and a bit convoluted. On that note, you may want to look at DoCon. It's a sort-of computer algebra library for haskell.
&gt; Ambient state (I'd only have to bring this one up here, which makes me smile, but I toss data all over the place and clean up later—forget elegance, I want a bunch of global variables) This is exactly how I use `ghci`...
I love this paper, but a related topic that must be considered alongside is that since [applicatives can be composed in so many ways](http://comonad.com/reader/2012/abstracting-with-applicatives/), there are many alternative solutions for many of the things where you'd think of using free applicatives, and I feel it's an open question about what technique to use when. For example, the paper's `count` function can be tacked on top of an existing applicative by using the [`Product`](http://hackage.haskell.org/package/transformers-0.3.0.0/docs/Data-Functor-Product.html) and [`Constant`](http://hackage.haskell.org/package/transformers-0.3.0.0/docs/Data-Functor-Constant.html) functors: {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Control.Applicative import Data.Monoid (Sum(..)) -- instance (Applicative f, Applicative g) =&gt; Applicative (Product f g) import Data.Functor.Product -- instance Monoid m =&gt; Applicative (Constant m) import Data.Functor.Constant newtype Counted f a = Counted (Product (Constant (Sum Int)) f a) deriving (Functor, Applicative) runCounted :: Counted f a -&gt; f a runCounted (Counted (Pair _ fa)) = fa counted :: f a -&gt; Counted f a counted fa = Counted (Pair (Constant (Sum 1)) fa) count :: Counted f a -&gt; Int count (Counted (Pair (Constant (Sum n)) _)) = n In particular, I wonder if `instance (Alternative f, Applicative g) =&gt; Alternative (Compose f g)` might be relevant to the questions about free `Alternative`. It's very much a dodge of the question, but maybe instead of asking what laws `Alternative` ought to follow, just let people pick between `Compose []`, `Compose Maybe` and so on depending on which equations they'd like to be true of their functor...
Ah it's not exactly research - just honours at the end of undergraduate. Thanks for the sound advice! Wanting to use Haskell is, for me, partially about learning to use Haskell better, but also discovering whether it is suited for the domain, whether there is a deficiency in tooling/libraries in the area, etc.
Wow, that's coming along very nicely already! I should have searched before posting. Guess the next few things on my wish list would be R-like plotting, interactive installation of packages like R/bioconductor that hide the cabal-install messiness, and just general integration/documentation.
For plots and charts, nothing beats Gnuplot or R IMO.
Has anybody gotten an error based on unqualified imports?
&gt; The beginner -&gt; intermediate space in Haskell needs a bit more love I second that!!!
HTML is pretty verbose, so it would still not be so easy to get to other formats. Better than ODT though, for sure. How about just CSV? The actual formatting is minimal, so it would then be easy to get to any other format. Or maybe markdown - but there are variations on how giant tables work among the various markdown dialects.
It's very convenient - but only after doing the infrastructure work that you did. Thanks for sharing!
Hey Michael. Thanks for this post, I really enjoyed going through it. I actually liked the fact that the examples were simple. It shows that simple things are simple with conduit (and the authenticated proxy showed that more complex things aren't that much harder).
...and now let's make a few for the [`lens`](http://hackage.haskell.org/package/lens) package :-)
&gt; HTML is pretty verbose, so it would still not be so easy to get to other formats That's my concern also. &gt; Or maybe markdown Another possibility... I have to think about it a bit more
Have you tried python recently? I'm just asking because pandas/statsmodels/ipython seems to fill in a lot of the things on your list.
&gt; Once you read that and understand it, you will ought never to want to touch R again. If the authors are right (and I see no reason to doubt them), programming in R should be considered positively hazardous. And we probably ought to re-evaluate the level of trust we put into any data produced by R. Why? There's nothing unexpected in that paper. 
&gt; PS: Anyone who is … aware enough of PLT to be reading /r/haskell and yet who still uses R should read the following paper: &gt; http://r.cs.purdue.edu/pub/ecoop12.pdf You don't happen to know where to find a similar analysis of the Matlab language and its usage? It would be very interesting to read.
only allow pointfree functions... hahaha.
Nice examples. This provides an additional answer for [this](https://www.reddit.com/r/haskell/comments/1yw72y/telnet_client_using_conduit/).
You are doing great! Keep it up, please.
I really like hmatrix, it makes matrix and vector operations simple but uses gsl/lapack/blas in the background so it is fast. I don't quite see the appeal of data frames in R, the main feature I feel is missing in Haskell libraries is to operate on subsets of rows/columns of arrays using a simple syntax. In R being able to do things like a[which(rowMeans(a)&gt;0),] to select subsets of an array both as a destination to write to and to read from can make certain things much quicker to write. Random-fu is incredibly useful for sampling from random distributions and nicely encapsulates the results as RVar's so that it is clear whether something is pure or a random variable. An interesting alternative to R is Julia - http://julialang.org. It seems to fix some of the things that can be a problem in R. I'm not quite sure how stable it is yet though.
Yeah, its probably two to three weeks away. 
The official solution to the first puzzle, for 2014-02-24, is slightly incorrect. The problem is stated as: &gt; Filter a list, keeping an element only if it is equal to the next one. The official solution is: filterByPair = (tail =&lt;&lt;) . group But that keeps elements that are equal to the *previous* one. Here is a counterexample: data OnlyFirst a = F a a deriving Show instance Eq a =&gt; Eq (OnlyFirst a) where F x _ == F y _ = x == y *OnlyFirst&gt; filterByPair [F 10 1, F 10 2, F 10 3] [F 10 2,F 10 3] 
There is no need to sort the list; an element-wise comparison is enough.
Short answer: No, I don't. And yes, I agree it would be interesting. Longer answer: The paper I quoted above was (also) presented at a 2012 seminar series held at Schloß Dagstuhl in Germany, entitled “Foundations for Scripting Languages”. The (very interesting) theme for this seminar series was the rigorous investigation of the theoretical underpinnings of a number of popular "scripting" languages — languages which, by and large, were designed (or simply evolved) without any such theoretical basis. Unfortunately, Matlab was not among the languages covered. The web page for the seminar series is here: https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=12011 And here's a published note that provides an overview: https://www.cs.purdue.edu/homes/jv/pubs/dagstuhl12.pdf Wouldn't it be great if this could become an annual conference? There are so many other common languages I'd love to see theoretical analyses of.
Fortunately, the authors use metaphors to keep things understandable for non-PL geeks: &gt; As a language, R is like French; it has an elegant core, but every rule comes with a set of ad-hoc exceptions that directly contradict it.
Maybe not if you know R a fair bit better than I do! I, for one, was quite surprised by a language that can use normal order evaluation… except when it doesn't. (This was in fact what put me onto this tack in the first place: A friend send me two R snippets, one of which seemed to be lazy, the other strict. I told him he must be mistaken. I then did the research and found he wasn't.) I was also quite surprised by a language that is *so* implementation-dependent that the addition of a syntactically valueless `return` or `()` could meaningfully impact performance. And that's just two of the many bizarre discrepancies uncovered in that paper. I must admit that the authors remain more polite throughout than I would have, though at times their bemusement shows.
&gt; I tried to love NumPy for a long time. It has a lot going for it. &gt; But R. Oh R. Well, numpy is not really the equivalent of R. If you want to single out *just one* Python package to put up against R, it would probably be pandas. But if you really want to build an R-equivalent system in Python, you're probably looking at SciPy+numpy+pandas+matplotlib+IPython notebooks *at least*; arguably you'll need to look at the full spectrum of [PyData tools](http://pydata.org/downloads/). Staying in the Python milieu, matplotlib has come a long way. And if ggplot is your thing, then you should look at [Bokeh](http://bokeh.pydata.org). Granted, and as I said in another comment here, it's only as of *very* recently that I'd regard the Python ecosystem as a viable full-spectrum replacement for R, even though the scientific Python community have been working towards that goal for a long time.
This is great. I have always wanted a cheat sheet with precedence of common operators (Prelude, Applicative, Monoid, etc.) Never got around to doing it myself--am too lazy to figure out the LaTeX, and I puked at the thought of LibreOffice :) It always annoys me that Haddock does not show this critical information and I have to look at the source or use ghci.
I find it really challenging to. My primary concern is :r wiping the environment, but but there are other troubles as well. It just isn't as fully-feature a REPL as others: both R and IPython (not the notebook, which I have little experience with) come to mind.
Python has many of these things yeah, but unless I'm programming something it's always a quality gap issue. For one: Python does not have a charting library I know of that even approaches lattice or ggplot.
Sorry, you're correct. I've used the majority of the tools you mention and nearly call it "NumPy" since that forms the foundation. All together the ecosystem there is very nice, but still immature in my opinion compared with R. You can get away with using Python now, in my mind, and this is a feat unimaginable 5 years ago. But I never want to. (As a side note: Bokeh looks nicer than raw matplotlib, but I'm not sure why it reminds you of ggplot—it has very few similarities in my mind. Copying Matlab style plotting has always been a mistake in my mind. It'd very imperative, not declarative)
For me the killer feature would be the ability to reload a source file without losing locally defined values. In the octave/MATLAB interactive environment I can do octave&gt; A = expensive_computation; octave&gt; % edit my source file octave&gt; run_my_thing(A) Without worrying about A. This is is impossible in Ghci, ghci&gt; a &lt;- evaluate expensiveComputation ghci&gt; -- now edit source file ghci&gt; :r ghci&gt; -- where did `a` go?
I used haskell for plotting some pretty things for my undergrad thesis (in math), and even attached the code at the end of my report, in the hope of making them discover some elegant programming language.
Is managing some state (say, the last message received) within a `Conduit` just a matter of using a `StateT` as the inner monad, or is it more involved than that? This tutorial actually came at a nice time, I've been thinking about writing a simple networked game and realized I had no idea how to implement it. Conduit seems like it could be a nice, lightweight fit. Thanks for the tutorial!
&gt; You can get away with using Python now, in my mind, and this is a feat unimaginable 5 years ago. But I never want to. Not even with the interactive beauty and wonderfulness of IPython Notebooks? :) &gt; Bokeh looks nicer than raw matplotlib, but I'm not sure why it reminds you of ggplot Because both are explicitly based on [The Grammar of Graphics](http://amzn.com/0387245448) (the "gg" in "ggplot"). &gt; Copying Matlab style plotting has always been a mistake in my mind. Again, it's explicitly a goal of Bokeh to leverage the experience of existing R/ggplot users in much the same way that matplotlib tried to appeal to Matlab users. Agreed that I don't like matplotlib's imperative style, but much of its functionality is now exposed via multiple APIs — it's now possible to use it much "less imperatively".
Most of the things you mention are really specific to bioinformatics, and not data science in general. BioHaskell is a good idea, but still tiny compared to the rather mature Bio{Perl,Python,Ruby} projects. &gt; * next-gen sequencing reads Have you looked at [biohazard](https://github.com/udo-stenzel/biohazard) by my colleague in the office next door?
A lot of your concerns are addressed with current options, but I hope to have some nice coherent tooling that covers that sort of case soon Also look at what I said to tel to clarify what I mean by interactivity 
As I've stated a few times here elsewhere, I don't much like IPython notebooks. I sort of like Mathematica ones, but they're still not my favorite. While Bokeh might be based on GoG I didn't see the tutorial taking too much advantage of that. I'll give it a more in-depth look at another time.
There are some deficiencies, but the basic tooling is ok. I hope to get some tools out soon that I think will address the basic stuff 
Yeah, the interact tools applied to functions are kinda amazing
Yes, you can definitely take that approach. If you want to share that state among the entire pipeline, then just use `runStateT` like you normally would after constructing your pipeline. However, if you want to use that mutable state in just one component, have a look at the [Data.Conduit.Lift module](http://hackage.haskell.org/package/conduit-1.0.15.1/docs/Data-Conduit-Lift.html). For more information, see [the announcement blog post](http://www.yesodweb.com/blog/2014/01/conduit-transformer-exception) for that module.
I want to choose which projects to give to, and I want to do it though a Kickstarter clone (a free one). Kickstarter-style campaigns provide more transparency because they have very clear lists of goals and tell you who's involved. They provide a sense of connection to the authors through introduction videos and links to social media. They increase donor confidence by using social proof (who else has donated). They reduce donor risk because they don't take your money unless a project is funded. You can post them to social media to help them go viral.
Agreed. The language itself is already light-years ahead of others; there's no pressing need for innovation on that front. If the goal is to drive adoption, we need better tools, we need to rationalize some core libraries (get common typeclasses for Data.List, Data.Map, and a vector library), and WE NEED A POPULAR MOOC. We should also fix all the known warts in the language; I couldn't care less about backwards compatibility.
*edit*: Ooops! Did you edit the post to add the "proxy with authentication" example? If not, then I am a gigantic moron and I somehow missed it. That is pretty much the type of example I was asking for. Although I would prefer the handler in a second top level function with the type illustrated like the auth function. &gt; When I've written longer, more realistic examples, it necessarily involves multiple concepts, and people have complained that it's too difficult to follow. I can appreciate that. I guess all I can really say is what makes examples/blog type documentation (as opposed to stuff like Haddock docs with the lists of functions/types/etc) is when it actually has enough information to do something real with the package *without* having to go into the real docs and hunt things down. At least as important to me as a practical example (possibly even more so) is to see the types. It makes the example longer and probably more complex looking, but it's information that anyone who actually uses the package will almost certainly need to know. &gt; The purpose of the post was to show the basics of using network-conduit to deal with multiple connections in an asynchronous manner. It's not about proper chunked data processing or program structure, which is why the examples don't touch on it at all. I'm not completely sure what you mean by chunked data processing. Does that mean anything more complex that mapping single characters? It seems to me there are several possibilities for potential users: 1. Beginner Haskellers that would need such information to do *anything* practical and would benefit considerably from an example of reasonable program structure. 2. Intermediate/lazy (I definitely fit into the lazy category) Haskellers that would benefit at least somewhat for an example of doing something practical in an idiomatic way, even if they probably could figure it out by digging through the documentation. 3. Advanced Haskellers that can just skim the documentation and start using it. These may still benefit from a practical example because it will allow them to see how many hoop/boilerplate is required to put it to their use case. I'd say that personally I'm between #2 and #3. Again, I want to stress that I don't want this to take the form of a gripe — especially one directed at you personally. For examples to be useful for me I need to see the types, a relatively idiomatic program structure and a use that is at least somewhat practical. I don't only need that information to use the package, I need it to determine if I *want* to use the package. 
Did you have a list of warts, or know of one? It would be nice to collect them somewhere and think about how to address them.
This might some day fill the gap: https://github.com/yhat/ggplot It's been actively developed, many of the more simple things already work.
Ooh. That seems like a direct port.
I used to use Matlab in my day job and agree with all points, except perhaps regarding the plotting. I mean, it is easy to plot and there are lots of plotting functions and options, but when you try to make it look good it can be a pain. The lack of modules and usable name spacing is really annoying, btw. I would like to add lack of concurrency and parallelism to the negative parts of Matlab. This is where a Haskell-based solution would shine. (Well, some MathWorks-supplied functions run in parallel, but you can't create your own without the Parallel Computing toolbox (afaik) and that is often neither elegant nor flexible.)
No, you're not a gigantic moron. I *did* add that example based on a comment in the blog. Normally I'd add an "Updated" message to the blog post at that point, but my plan is to just keep expanding this post with more examples over time. I understand where you're coming from. I think the missing idea here is that I don't intend this documentation to be the only thing you need to read in order to write something meaningful with network-conduit. Understanding conduit itself (to one degree or another) is essential to doing something more advanced. I can't make the documentation for each individual conduit add-on a full-blown conduit tutorial; that's what [the conduit overview](https://www.fpcomplete.com/user/snoyberg/library-documentation/conduit-overview) is intended to cover. What we're *really* talking about here is the fact that there are many different ways to write documentation: * API docs (Haddock) * Cookbooks * Tutorials with short examples * ... and many others I think you're looking for cookbook-style docs. I think they're great, and serve a wonderful purpose. But this specific blog post isn't fitting that niche. (Though if I continue to add more complex tutorials, that line is going to blur even more.) tl;dr: You can never have too much documentation :)
&gt; This means that if you clone your skeleton, rename your cabal file (to the one appropriate to the project), then modify your cabal file sufficiently, you will never be able to make common changes to the boilerplate repository and merge them into your other projects. It doesn't work at all. Changing the rename threshold made this work exactly as expected. Git works fine.
Is there anyone with ghci core experience who knows how tough this would be to patch? To work around this these days I tend to abuse cmdargs incredibly to build a series of largeish Haskell processing chunks that all are CSV -&gt; IO CSV then use R to massage and visualize the data between steps. Honestly this is a major reason why I don't play with diagrams as much as I had thought I would.
Forward merging changes from a skeleton feels weird to me. I have never encountered something that did that. I see code scaffolding as necessarily snapshot style behavior. You start a new project with the scaffold and work from there. If the scaffold updates, you'll have to figure out on your own if and how those changes can be incorporated into your project. That being said, I definitely recognize the utility. 
In response to the excellent feedback from y'all, I have started a [`hi` branch for Haskeleton](https://github.com/tfausak/haskeleton/tree/hi). Once I finish it, I'll merge it into master and update the post. Thanks! Edit: Post updated! Haskeleton is now a template for hi. hi --module-name An.Example --repository git@github.com:tfausak/haskeleton.git
I went to town the day I discovered Matlab had lambdas and built a few HOF-based stats programs. They are still running... ... not in production, just still haven't gotten a response.
My major gripe: one exported function per file. Makes sense due to the Matlab global namespace. But doesn't make any sense at all.
I'll add one more point to the two from /u/cartazio and /u/tailbalance I did 5 years of solid matlab for my research before picking up Haskell as a side interest. Matlab was the 'best tool for the job' by most measures... except for the fact that Haskell is a much more productive language in general - wrt the whole 'if it compiles it works' thing (which you can say without qualification if matlab is the thing you're comparing against). So matlab has the domain-specific advantages... but since being spoiled by Haskell, those advantages are outweighed many times over by the fact that debugging matlab is so soul-crushing. Every time that license expires I feel a glimmer of hope: maybe this time, I will let my 5-year-old matlab codebase die and port over to Haskell.. but the sense that graduation is just around the corner makes me keep going w/ matlab. ... That's just my experience. There's definitely something to be said for going-with-what-works (maybe R instead of matlab?). But do give very high weight to the qualities that draw you to Haskell when you make your decision.
This is not git being "fine." This is git sucking and you posting a work around for my _one_ example. The very first example I came up with, as I mentioned. I actually expected git to not fuck that one up, and that I would have to try harder to confuse it, but I didn't. For one, you had to presumably know what the problem in the data model was, and look for an option to 'fix' it. But we don't really know if this will fix all test cases I can come up with. We also don't know what else changing the rename threshold will screw up. I've seen git infer renames that didn't happen (and didn't even make sense) with the default settings. I can only assume that boosting it will lead to more of that. This is for something that pretty much every other VCS in wide (and non-wide in some cases) use just does correctly without the user having to do anything. Anyhow, I don't really care if people use git (except that many are constantly evangelizing it as if it were actually good). I'm going to go use tools that don't suck instead of apologizing for tools that do. Kind of like how I use Haskell instead of any number of more popular languages.
Same experience here when I tried to give myself things like listMap, cellMap, cellFold. It's a shame I can only use them in the outermost loops, or the thing never finishes running.
As there is no need for `zip &lt;*&gt; tail`. Don't spoil our party!
Yeah, I think a partial solution is the only thing possible. I think that's fine for a lot of my use cases, or at least an improvement. I'm pretty sure augustuss has mentioned that he "knows of at least one implementation" which solves the problem.
&gt; No, you're not a gigantic moron. I did add that example based on a comment in the blog. Whew. I might still be a gigantic moron, but at least it's not for that particular reason! &gt; I think the missing idea here is that I don't intend this documentation to be the only thing you need to read in order to write something meaningful with network-conduit. That's okay and I doubt examples can ever entirely obviate the use of the actual documentation. But I would like it to have enough information for me to determine if I even want to use the package. I don't use conduit for anything, although I've seen stuff about it from time to time and it looks interesting. I haven't seen a way that it would actually make my life easier — and it may well be the case that if I dug through the documentation and tried to write some programs with it I would find that this is definitely the case. Unfortunately there are so many packages that doing this for *potentially* useful packages is impractical. You know what I mean? And it goes without saying that I don't feel like I'm entitled to any of this. I'm just talking about my preferences. I think something generic like conduit and its subpackages requires a bit more selling to get people (me at least) to use it. If I want to do something specific like AES encrypt stuff I go to hackage and search and there are probably one or two packages for that and it's relatively clear that I'm going to use one of them. One can live without abstractions like conduits, pipes, lenses, etc and how they are applicable is less clear. I hope that makes some sense. &gt; I think you're looking for cookbook-style docs. I think they're great, and serve a wonderful purpose. Well, cookbook-style docs would be great but that requires writing a great many examples. What I'm talking about is an example that is at least somewhat practical, has functions split out and type annotated and shows a relatively idiomatic way of using the package. Just one example like that is fine. In this particular case, I'd say the number of people that would use network-conduit to do character by character transformations is extremely tiny, while probably 90% or more would use it for line-based protocols. So while it might be nice to have an example for each sort of use, a line-based example would help (in my opinion) help the most people.
R indeed. My current workflow is something like * Get some data, and do the initial cleanup / reshape with R, dump to CSV files * Import CSV into Haskell. Do numerical awesomeness. * Haskell dumps results out to CSV files * Import CSV into R and use ggplot2 to plot If this loop was tighter, that would be extremely useful. For example, there is a a natural mapping between R's dataframe and a list of tuples and a corresponding list of column names. I think a very good fist step in this direction is not to try to re-invent something, but try to utilize the best of both worlds. One of R's best strengths is plotting, particularly through ggplot2. If it were possible to generate R plots from Haskell programs or ghci, that would be great.
I'm not totally following the discussion, but are you aware that there's now support for Haskell in IPython? http://gibiansky.github.io/IHaskell/
I'm not taking your comments *at all* as griping or feeling entitled to something. You're providing very useful feedback. It's very difficult as a library author to know what documentation users will find useful. As I often say, I'm the worst person in the world to know what's difficult to understand about my libraries, since they already conform to all of my own personal idiosyncrasies. I'm definitely trying to increase the documentation/example coverage for conduit right now. If you have some concrete examples that would help make the library clearer, please let me know. The examples you saw in this blog post were completely user driven as well.
It just means that the type on the left has an instance of that particular typeclass. So, for example, Text is a Monoid (without any other restrictions), and thus it has an 'x'. But for example 'Product a' is only a Monoid if 'a' also has an instance of the 'Num' type class. Which makes sense, since you cannot have a "Product of strings".
Just to be clear, Ihaskell is not a interface to ghci! Ghci is rather limited, so we reimplement a lot of it and use ghc api directly.
Please file any issues you have or features you want as issues on github, too :)
I feel like the hard part of describing Free is that you have to explain what the hell you're doing with Functors like data ExpF x = Get String (String -&gt; x) and how they translate to get :: String -&gt; Exp String get nm = liftF (Get nm id) but at least that's all fairly mechanical and seems to require only that you get the "adversarial" POV in mind between the `Exp` definer and the `Exp` interpreter. But I'm hopelessly sunk on how to describe Operational without handwaving, even if the Instruction interface is easier to read.
why is "`seq`" a functor, but "`seq a`" not a functor?
The same way [] is a functor, but [a] is not a functor. See http://learnyouahaskell.com/functors-applicative-functors-and-monoids
You might want to throw a GHC-Options in there, e.g. I tend to use something like &gt; -Wall -O2 -threaded -fno-warn-unused-do-bind -fno-warn-name-shadowing -fno-warn-orphans -rtsopts "-with-rtsopts=-N" For application repositories I also like to use some scripts to setup the sandbox and add any packages I might have to modify for it (often just bounds or a single, simple fix that is not fixed upstream yet, I keep them in a subdirectory modified_packages). init_build.sh: #!/bin/bash set -e if ! hash alex; then echo "Please install the alex program globally" exit 1 fi if ! hash happy; then echo "Please install the happy program globally" exit 1 fi cabal sandbox init find modified_packages -mindepth 1 -maxdepth 1 -type d | while read modified_package do cabal sandbox add-source "${modified_package}" done cabal install --only-dependencies --enable-tests --enable-benchmarks cabal configure --enable-tests --enable-benchmarks cabal build clean_build.sh #!/bin/bash set -e cabal clean cabal sandbox delete That way I can just use init_build.sh once when I check out the repository and get compiled code (and can use cabal configure, cabal build,... as appropriate afterwards) and if I want to get e.g. updated dependencies after a week or two or switch branches to something very different I can use clean_build.sh to clean up before doing another init_build.sh. There is probably a better way to do it but it works well for me so far (on Linux).
This is because some type classes (the original ones like Num, Eq, Show, etc.) are for types of kind *, while newer type classes like Functor, Monad, Applicative, etc. are for types of kind * -&gt; *. Before they became part of the Haskell language, these higher-kinded type classes were known as "constructor classes" because they allowed you to abstract over type constructors, not just basic types. So, if you ask ghci the kind of a type, e.g. `:k Seq`, you'll get * -&gt; * as the response. That's because Seq is a type constructor; it takes a type-level parameter of kind * and constructs from it a new type of kind *. That's why, if you ask ghci the kind of a type constructor applied to another type, e.g. `:k Seq Int`, you'll get * as the response. 
I haven't read that paper but the abstract: &gt; R is a dynamic language for statistical computation that combines lazy functional features and object oriented programming. This rather unlikely linguistic cocktail would probably never have been prepared by computer scientists... R is an odd language. But, combining lazy and oo isn't weird at all. As Noam Zeilberger (among others) established it is the natural thing: objects (like functions) are co-data and co-data wants to be lazy. Data (think ADTs) wants to be strict. Imperative and ML style/data centric functional languages seem to be more naturally strict, while languages that emphasizes functions or objects above all else want to be lazy. You can have strict languages (like ML or Java) that use co-data extensively, and non-strict languages (like Haskell) that make good use of data. It is just that you give up some equational properties.
I discovered recently that not only do you have to have a cabal file, but that cabal file actually has to be configured to build some source file. :) My attempt to use it to just specify the libraries I wanted in my sandbox fell flat without a dummy source file to build.
I discovered recently that not only do you have to have a cabal file, but that cabal file actually has to be configured to build some source file. :) My attempt to use it to just specify the libraries I wanted in my sandbox fell flat without a dummy source file to build.
Nice! I should do this too, then I'll have: current dir, git branch, ghc version and cabal sandbox :)
I think that's a really good insight and I agree about 80%. Atop that, however, I tend to find that there's a middle layer of "pipe building" which is also important and responds both to interactivity and reloading some significant amount of user code. In my experience this tends to be possible due to fixity of a set of standard types (Python ndarrays, for instance) which are manually passed through stages of transformation and retained even while the stages update.
If you want to convert to other formats, perhaps [pandoc](http://hackage.haskell.org/package/pandoc) can help?
Agree that making publication-quality plots is hard in Matlab, but I don't publish my work - the plots just have to be good enough for internal consumption.
have you tried my http://hackage.haskell.org/package/Rlang-QQ? It'll do the "dump to file and read in the other language". Conversions for data.frame can be a bit tricky to get right, since you have to go through list: http://code.haskell.org/~aavogt/lmqq/ex1.hs
I know, I'm just mentionning it as one of the major AI efforts in our ecosystem. It's already definitely worth a look, not only because of its instructive use of ConstrantKinds and friends, but also because we can already have some fun by using its classifiers and distributions code ;)
ok! thanks for the instructions. I did the ./configure and etc as above, and now I see 7.8.0.20140228 when I do ghc --version. I updated cabal-install as well, so now I get this: ~$ cabal --version cabal-install version 1.18.0.3 using version 1.18.1.3 of the Cabal library However, it seems like cabal is unable to find installed libraries. well specifically I'm having a problem with GLFW. I get this when I try 'cabal install glfw': /usr/bin/ld: cannot find -lHSOpenGL-2.9.1.0-ghc7.8.0.20140228 /usr/bin/ld: cannot find -lHStext-1.1.0.0-ghc7.8.0.20140228 /usr/bin/ld: cannot find -lHSGLURaw-1.4.0.0-ghc7.8.0.20140228 /usr/bin/ld: cannot find -lHSOpenGLRaw-1.4.0.0-ghc7.8.0.20140228 collect2: error: ld returned 1 exit status But it looks like all those libraries are installed in cabal. I can install each one individually and I don't get error messages. It looks like some object code is in the cabal lib directory in the right place, as below. Is there something else I need to do in order to get libraries working? bzzt@bzzt:~/.cabal/lib/text-1.1.0.0/ghc-7.8.0.20140228$ ls Data HStext-1.1.0.0.o libHStext-1.1.0.0.a 
what interesting is that I can get glfw to install fine into a 1.18 sandbox. not sure what I'm doing wrong for the regular cabal. 
We haven't revisited that yet, but we will at some point. It is certainly an area we want to have better support in.
* DEFINITELY a Data Frame library like Pandas (python) or Dplyr (R). This is a must have. It makes life as a statistician/data scientist MUCH easier. I can't live without this. It makes some data cleaning, preprocessing and exploratory analysis that would take hours and hundreds of hacky ad hoc lines of code into a matter of a few minutes and few lines of beautiful functional style code. Things like reading and joining data from multiple sources in a columnar format, applying transformations to different columns, grouping, counting, aggregating, pivoting, indexing, making a hierarchical analysis... It's a pain to do without Pandas or similar tools. * Plotting library using the Grammar of Graphs like ggplot2. This would be really interesting to see done in Haskell. It has nice compositional/algebraic interface that would probably feel very natural in Haskell. * If you're feeling ambitious, a MCMC framework for Bayesian inference or some other approach to probabilistic programming. Take a look at Pymc, Stan and Church. If I was more adept at type theory I would surely attack purely functional probabilistic programming, but I am not so good at it. There are quite a few fascinating blog posts about it based on a few articles (which I am unable to follow) on how this problem is naturally monadic. I'll try to remember to post some links later, but googling about probability monad should bring then up.
I've used Mathematica extensively over the years, and have actually constructed some fairly large software systems in it, and I would have to say that its ability to easily do pattern-matching and rule-rewriting on symbolic expressions makes it exceptionally useful. There are a lot of things wrong with the package in terms of its implementation and the semantics of the underlying language, but the ease and power with which you can manipulate complicated symbolic expressions more than makes up for it.
I think a lot of the time writing bindings is exactly the way to go. At the absolute least, there's no reason to ever rewrite BLAS/LAPACK. But with that said, bindings will often feel artificial compared to a native library.
That did the trick, thanks! If you have a stackOverflow account, you can write that up as an answer and I'll give you the reputation for accepting it, if not I'll just write it up myself. Thanks so much!
In controls engineering (not exactly what you asked about but related) I regularly need Bode plotting, Root Locus plotting, Nyquist plotting, transfer functions like MATLAB (not sure if you would call them symbolic or not but its kinda close), Gain/Phase margin calculation, and some other simulation functions. Maybe when I have some time I'll see how far I can get implementing these myself, that would stretch my skill level quite a bit and be pretty interesting.
I felt the point of the article was to examine applying monads without regard for what a monad is. "Who cares what it is, here's what you do with it." In light of that, it worked for me. It succeeds as a tour of a variety of monads and how you can combine them to accomplish concrete tasks. What isn't clear to me is how the stacking works without monadic code having to worry about explicitly pushing functions into one monad or another remains obscure, and why it is we suddenly have to sprinkle in a `liftIO`. But the article makes clear that it's intentionally hand-waving there, and the point of the article is "pay no mind to what's behind the curtain, but behold! Haskell's got some marvellous curtains from this side."
I agree with both points. But bindings don't have to be poorly designed. They can feel idiomatic, if enough polishing goes into them. [conjecture] I think that "pure" language implementations of an idea feel more idiomatic because the creator is architecting in ideas native to the language. Bindings are unnatural, because the creator thinks of them as a bridge, borrowing concepts from both languages. Realistically, it would require a heroic amount of effort to reimplement the existing numerical libraries of Python. R's libraries are at least two orders of magnitude bigger. If you reduce this to popular libraries, it's still one order of magnitude bigger. The dream of course, is you offer a readable codegolf terse way to implement numerical libraries. The operations I use in Python, Matlab and R are actually relatively few. By and large it's all fancy indexing, linear algebra and shape manipulation. If it's possible to tersify this even more at the same level of low-level control of memory and algorithmic reasoning, then it's possible to tersify. [/conjecture] You can achieve greater reward to effort by writing idiomatic Haskell (not stringly typed) bindings to R/Python, but if you have a superdoctorate in linear algebra then I'll be a permanent subscriber to whatever you publish.
[Sage](http://www.sagemath.org/) is what I use for getting data to and from specialized tools. It's pretty nice to have data shuttled around in python objects instead of data files. I'm not really sure how it rates, because my needs aren't very sophisticated. I'm curious if anyone else has thoughts on it.
Please keep me posted about this, I'm very curious to see the kind of tricks you came up with and how you're exposing the well-thought machinery behind a simple API.
What I said was precisely because that was the point of the article.
Yes please!
Haskell programmers at are quite familiar with the notion that trivial structural changes can have massive performance impact.
This is just a slightly less flexible version of lens' `Each` class. The major feature of this is that the package has fewer dependencies than lens. Any place you are using `otraverse` you should be able to use `each` instead. http://hackage.haskell.org/package/lens-4.0.5/docs/Control-Lens-Each.html
I've done something similar for my larger projects. I have a base sandbox at `~/src/sandbox` and use `cabal sandbox add-source` to pull in the deps I need in `~/src`. Inside the project folder I run `cabal sandbox init --sandbox ~/src/sandbox/.cabal-sandbox`. Works wonderfully. It stops you from having to compile hundreds of files every time a new sandbox is initialized, and using `add-source` will automatically track changes and recompile which is very useful if you're modifying several different packages at once. Launching ghci in a sandbox can be accomplished already via `cabal repl`. Anyone looking for some further reading I can't help but recommend this: http://coldwa.st/e/blog/2013-08-20-Cabal-sandbox.html . This is my go-to reference when I forget a cabal sandbox command/option.
&gt; Starting from GHC 7.8 and later, the {-# MINIMAL #-} Pragma will be supported that so the compiler can err or warn you if you have not met the minimal complete definition. Is this an experimental feature or why isn't it activated by default? 
&gt; But, combining lazy and oo isn't weird at all. That is, of course, true. What's odd about R is that it does so seemingly at random, with some constructs being lazy and others strict for no other reason than that's how they were implemented. Worse, the language is insufficiently specified, and the evaluation strategy of various constructs are not explicitly or implicitly implied by the specifications (such as they are) in many cases. Which raises the horrific spectre that they may be implementation-specific. In which case it's rather fortunate that R really has only the reference implementation! (All this is in the paper, which is truly worth a read if one is even remotely a PLT geek!)
No, it's pretty good. There's a lovely graphical typeclass relationship diagram in *Algorithms — A Functional Programming Approach* by Rabhi and Lapalme (a great book that's unfortunately somewhat obscure even in the Haskell community) — fig. 2.4, on p. 46 of my edition.
This one always gets me: data T = T Int instance Eq T where (T a) == (T b) = a == b instance Ord T where (T a) &gt; (T b) = a &gt; b main = putStrLn $ show $ T 1 &lt;= T 2 This program generates a stack overflow with no other message. The bug is that the minimal definition of Ord should be either 'compare' or '&lt;='. I got it quite early in my "Haskell life" and it took me a long time to figure out :)
I was bit by the same bug. A newbie friend of mine used: instance Eq Foo -- empty definition and also got infinite loops. The lack of enforcement of minimal definition is annoying. The lack of debug information upon runtime crashes (at least by default) is one of Haskell's glaring holes at the moment.
The issue is that when default method implementations are provided in the class, and some of them must be overridden by instances, the compiler can't work out for itself which methods must be overridden. e.g. /u/kaukau's [example](http://www.reddit.com/r/haskell/comments/1zshcu/til_the_hard_way_that_ghc_does_not_enforce_the/cfwlt0a) of `Ord`, where you must at least override `&lt;=` *or* `compare`, because the default implementations are defined in terms of each other (and the other methods are defined in terms of those two).
Ah I see, thank you.
Because it's equivalent to solving the halting problem.
I implemented some very basic state-space control helper functions a while ago for uni using hmatrix. I thought it turned out [quite nicely](https://gist.github.com/eightyeight/5657087), especially the `controllability` function itself which pretty much follows the mathematical definition exactly. I'll be doing a thesis in control which I really hope to use some Haskell for! We'll see...
Thanks, fixed it!
What's with the massive use of fully qualified names? Makes the code hard to read.
Because MINIMAL is not a flag which turns on warnings! It is a way to specify *which* methods are required for a minimal complete definition. If you don't specify them, the default is that the minimal complete definition consists of all the methods which don't have a default implementation, which is quite reasonable IMO.
The MINIMAL pragma is [how you tell ghc](http://www.haskell.org/ghc/docs/7.8.1-rc2/html/users_guide/pragmas.html#minimal-pragma) what the minimal set of methods that must be implemented are.
I tried this, actually, but it did not tell me which instance was undefined! Either the profiling scheme had the same problem as GHCi and was not able to step into the instance, or it did see the instance was undefined but it could not specify which instance for which data type was causing the crash.
That was a good question. I thought as you did that it should have worked more like Java. I didn't know the answer to that myself.
Ok, thanks for pointing this out.
Tip: use map toComponent [ x, y z ] instead of [ toComponent $ x, toComponent $ y, toComponent $ z ] 
C++ style?
From my pleasant experience with clojurescript, if I had "atom"s, go style CSP channels, an embedded DSL that can generates Facebook react code, and utilities like: Throttling, Debouncing; I'd be quite well off,
You might be interested in my pet project Textlunky: https://github.com/5outh/textlunky It's still pretty early on in development, but the game loop (print command /process command / step game one tick) basically looks like this: P.recursively runCommand (P.showP &lt;.&gt; P.updateP &lt;.&gt; P.stepGame) cmd (in `Game.hs`) I'm sort of learning a lot on the fly, but still thought you might want to check it out. :)
There are many cases of git, or the culture around it, being flawed (if you ask me). For one, the interface lets you make mistakes that other systems will rule out. I think the first git commit I ever made had no name and e-mail, because I'd forgotten to put it in the config. Darcs will just ask you for that information. Darcs' record interface is obviously a good idea, too, I think. You review all your hunks interactively and select the right ones before committing. People will probably claim that the staging area encourages the same review, but it's so much clunkier that I don't believe them. Git commit/add does have -p for a darcs-like interface, but you have to know about and ask for it, and usually when I mention it to people, they've never heard of it. Defaults matter. I've come to the conclusion that git (and mercurial) branches are often harmful. For instance, if you switch branches in a repository, you likely to clober all your build state, and have to clean and build fresh. So at work (where we use mercurial) I have one directory per branch. I've seen cases where people have this problem, someone suggests using multiple directories to solve it, and they actually can't comprehend the solution, because they're only thinking in terms of git branches; so apparently the very possibility is a handicap to some people. It is convenient to have all branches available after a single checkout (usually), but I think that can be accomplished by bazaar, which uses a directory-per-branch model by default (darcs is behind in that it has no way to group branches like this that I know of, without setting it up manually). Git usage and culture encourages lots of rebasing and squashing. This is a bad idea. I don't know if this is primarily memes, or whether the tools are at fault. For instance, I've heard bisect can't handle branching history well, even though it should have _more_ information in that case; and people like to go on about 'clean' history, but branching history can be presented in a clean way with the right tool, rather than everyone manually following a tool usage discipline that throws away information. I hear bazaar is good at this. Git pull is also a problem, since its auto merge creates an uninformative merge message that will look 'messy'. Mercurial's policy here is better; it's pull is equivalent to git fetch, I belive, and then you manually merge and informative merge messages can be encouraged, at least. But, the rebasing/squashing culture is bad because it destroys (easy) access to information about how the code was written, which can aid in finding bugs later. The easiest example is at merge points. The act of merging can introduce bugs, either through incompatible semantics of the two branches, or due to mistakes reconciling the two branches during the merge. It is, thus, useful information to see that two branches were both working before a merge, but the combination does not work. Rebasing rewrites things to make it look like one of the branches was incorrectly written code from the start, which makes it harder to understand where things went wrong. Or, I mentioned bisect: if both branches work, but the merge fails, you've identified the merge as the problem. But bisecting on a rebased history may well take you to some other intermediate commit from the branch instead of identifying the real problem. I'm certain there's other stuff I've forgotten, or that I haven't used git extensively enough to be bothered by. For instance [this guy](http://jordi.inversethought.com/blog/enough-git/) has some more technical stuff that I've fortunately never had to deal with. I use darcs for my own stuff because I like it, but it's probably not the best. However, I think git is obviously inferior in almost every way to at least mercurial, and is probably inferior in every way that matters to bazaar (which actually has some really well designed things in it that aren't replicated in either mercuial or git). I guess this is going to be a huge wall of text. But maybe that should tell you something. :)
That's exactly what the new pragma is doing, if I understand correctly?
http://www.purescript.org/ ?
So the function name is presented as (&lt;), regardless of which instance it's coming from?
"Om" makes me thing "Object-Model", and my experience has been with cloact / reagent. Hmm.. what about "ReHask"?
Well, the output should include which data type the function method is defined for, so it would look something like: (&lt;)(MyType) But since the problem was that the function was **not** defined, the stack trace had nothing to report. I had very little information to help me find which methods were missing. Probably if they were showing up in the stack trace, the instances *were* defined, so they were not the source of the freeze. If there were a way to to make the RTS output source file names and line numbers of the functions that were being called in the stack trace output, it may have helped. But I don't know if there is an option for that. So I used GHCi, but GHCi didn't help either because it too would freeze before telling me where it was frozen.
Is that C++ style? Eugh
Oh! I forgot one other random small stupidity (which bit me last night). Git's data model knows nothing about directories. If you remove all files in a directory, the directory disappears. So, for instance, in a Haskell project skeleton, you might want to auto-create a `src/` directory with no files, because your skeleton `.cabal` file specifies `hs-source-dirs: src`. But, this is impossible in git, so what people do is create something like `src/.empty` to work around it. Then you either have to remember to delete it after you have real files in there, or have the hidden files laying around. Or, in the case of haskeleton here, there are dummy `Foo.hs.template` files in some directories. This seems like a bad practice to me, as either I want to delete it (so it's no better than `.empty`), or I want to rename it to my actual first source file (I probably wouldn't do that, but....). But this is definitely something you don't want to forward merge, then. So it's either blocking you from taking advantage of some advantages of using a VCS as your skeleton manager, or you have to really hope that your settings to make git's renaming actually work for the things you want don't make it work for the things you don't.
&gt; I was planning on it being almost purely functional (with intmaps), but hashtables are too nice and updating a 1024x1024 grid demands mutable arrays. Or Repa :D
I'm actually thinking very seriously that the clojurescript / react eco system is a pretty good place to go looking for people at the moment. If my experience hiring Haksellers has taught me anything it's that targetting a niche where very talented and passionate people end up but not very many people are hiring from is an extremely good thing to do. What's you experience of clojurescript and react been so far? Any lessons for people building Haskell front end tools?
Then use a quad-tree?
`MINIMAL` addresses the second condition, but I'm basically suggesting that it should be mandatory or, if omitted, requiring every method to be implemented.
It's not a bad idea for presenting it in a blog. At least you can read it and understand what's going on, even if it's a bit lengthy. 
Well, here's what I've done to make a really refactorable app while I am simultaneously prototyping the HTML. 1. Never ever have state in react components. 2. Have all logic in a completely different `.cljs` (A "core reactor") that reacts to events over a channel and modifies the model. 3. Have a `.cljs` sending events on different intervals to the "core reactor" while prototyping, acting as if receiving events over a web socket. 4. Have a view-model, which represents the model. clojurescript + react seem to have difficulty maintaining DOM existence when you filter in the for loops. (You'll get jumpy elements if you have any transitions going) 5. Always dereference your filter condition values before you do the loop 6. If the loop contents are not trivial like an option-value list, separate the functions that do the looping and the individual part. 7. Delegate hooking the model to the view elsewhere, so you can have a different view for desktop vs mobile, rather than putting the logic for such detection in the view. (Think of the facebook chat bar on the bottom, doesn't translate well to mobile without a whole view change) ---- I use map destructuring **all the time**, so I expect that RecordWildCards would be prevalent ;) ---- Working with clojurescript has been frustrating as heck because the compiler just tests to see if it is a valid AST or not, and yet it can be translated to JS. Compiler errors are very unhelpful, with a spew of multi-colored stack traces. It makes me fearful of trying anything server side as well. I miss my types, dearly. (I have more to write, but I have to tend to other duties)
Very interesting! Thanks for sharing your experience :-)
Here are a couple of libraries that are works in progress for front end development in PureScript: * [jQuery Bindings](https://github.com/purescript/purescript-jquery) * [Reactive-jQuery](https://github.com/purescript/purescript-reactive-jquery)
One thing that I don't love about MINIMAL is that there's nothing relating the MINIMAL statement to a new method I've added, to say "I've considered this since touching that." I'm not sure how to solve that, without also being cumbersome. I should note that I mention this here because because your proposal inspired the line of thought, not because it makes it worse. Your proposal should probably become a warning and later an error, once MINIMAL has reached fixity.
I can't help but think that a stuff-&gt;JS compiler would introduce bugs to the resulting JS, as well as making debugging hard as hell. Why not simply learn JS and write it as well as you can?
Runtime debugging is definitely one of the lesser polished things in Haskell.
You make some good points and you've definitely convinced me to give Darcs a try. About those template files, though. Take [`ModuleName.hs.template`](https://github.com/tfausak/haskeleton/blob/master/package-name/library/ModuleName.hs.template) for example. `hi` will replace `ModuleName` with your module name, so you won't have to rename or delete it.
The top two contenders in my mind are Haste and PureScript. We're just finishing up a complete rewrite of a nontrivial Javascript GUI in Haskell with Haste. So far it's looking like a huge improvement. It would be nice if there were some higher level GUI libraries analogous to Angular &amp; co in Haskell, but we'll get there eventually.
http://elm-lang.org/
You realize that hashtables and quadtrees are both logarithmic in the maximum resolution of your spatial coordinates, right? The worst-case time complexity is the same, ignoring constant factors, and quadtrees have the benefit of far more predictable performance in actual use. Quadtrees additionally allow efficient lookups by region for things like nearest-neighbor search that are more broadly useful than just looking up single coordinates. Geospatial databases and 3D games and anything else where spatial search performance is critical almost always use quadtrees/octrees or related tree-like data structures. Updating a few things at a time in a small discrete grid is probably not going to be your performance bottleneck in anything resembling a real game anyway. Also, most of those comments are horrible and completely useless for understanding the code.
* Ghcjs — generates immense amount of js * Fay — limited haskell. I believe fpcomplete (and silk?) use it in production * Haste — full Haskell. Tiny JS output (comparable to fay). But getting much less love than Fay. I will show you all some jumping svg circles generated by haste later. PS: about non haskell: TypeScript — static typing, backward compatible to js (!), production-ready (iirc it's officially in VS), generates pretty much the same amount as the input.
`toComponent` is part of a type class, isn't it? That's like suggesting `map show [5, True, "foobar"]` instead of applying `show` three times. It would be nice if that did work, but them's the breaks.
Nope, this is definitely your imagination at work.
Well it's definitely not as smart as GHC, but it does have an expressive type system which allows you to say quite a lot of things.
MINIMAL lets you say what methods need to be defined by default. In the case of `Eq` `(==)` is defined in terms of `(/=)` and `(/=)` is defined in terms of `(==)`. So both are 'defined'. The compiler is perfectly happy with that definition. In the new version of `Monad`, you can define a monad with `fmap` (in `Functor`), `join` and `return` or you can use `(&gt;&gt;=)` and `return` and get a 'free' definition of `fmap = liftM`. There is no general purpose way given the power for arbitrary recursion that Haskell gives you to check these definitions would loop _and_ that looping isn't what you wanted.
I have been experimenting myself with game dev in Haskell, mostly using SFML: https://vimeo.com/85088966 Doing component-based programming in Haskell (with a bit of Netwire twist) in a functional way has been both tricky and rewarding. I might even get to write a blog post on it eventually :) 
&gt; {-# MINIMAL #-} Why in the world would they *not* make that the default? Sometimes GHC has a rather weird choice of priorities. (See also: Amazing high level optimization, but no low-level optimization until LLVM was integrated.)
So why is that a pragma, instead of in the language? I mean infix declarations are in the language… It could be like this: class C x (a, Either b c) where a :: x b :: x b = c c :: x c = b Just like module exports and imports. (They’re all interfaces, after all.) **Edit:** Yes, it conflicts with existing syntax. It was just a quick nudge in a better direction. I didn’t intend to create well-designed new syntax in 2 minutes in a Reddit comment. :) All that mattered is that you knew what I meant.
I know that Fay has had some work put into it to optimize performance a bit. I've heard next to nothing about Haste. Are there any benchmarks that have made comparisons of the two? Haste being "full Haskell" is quite an impressive feat. How has it not completely eclipsed Fay? Is there any good reason that it has gotten "much less love"?
Here's one possible reason, as explained by the Haste readme: https://github.com/valderman/haste-compiler#effortless-type-safe-client-server-communication
Well, you can always debug GHC itself. Let it run until you think it hung, fire up the debugger, and look where it’s stuck. At least I hope it’s possible to get meaningful information that way… (?) At the very least, the line number or something.
Perhaps you could try asking the folks at FPComplete? I hear their frontend is written entirely in Haskell.
I haven't done any benchmarks, but it is quite impressive how much of Hackage Haste is able to compile. For example, with a little bit of massaging here and there I was able to get lens 4.0 to compile in Haste.
I was always uncomfortable with the Game Loop over GameState state monad approach for many reasons, and I actually feel that it is an attempt to shoe-horn imperative ideas into Haskell. In fact having a state loop makes you "think imperatively". In Haskell, you have a language that is best when you "think declaratively" --- show how things "are", not how things happen. Ideally you would never even have to consider that there is a discrete loop in your entire game logic. One big problem also, I think, is global state. Unless you play fancy and non-composable games with hoisting and morphing your monads, or running a whole bunch of mini-states everywhere, every single action and every single character/monster/whatever that affects game state has the ability to affect *the global game state*. There are no guarantees with the naïve approach. Aren't we moving *backwards* from Object Oriented programming? If we move into state loop, we are basically imperatively programming in FORTRAN. And even *imperative* programming has seen much development since those days. Nowadays all trendy Object-Oriented languages have encapsulation, data hiding, and a proper hierarchy of what can affect one part of the state. Having a state loop, I feel, gives up much of the benefits of functional programming and much of the benefits of modern imperative programming. (edit: this problem is addressed in a pretty nice way with the Lens library, particularly Control.Lens.Zoom, which provide ways to "run" certain state monads with only access to a small part of the global state. this mitigates some of the classic problems with global state.) The ideal to me would be a framework where 1. Everything is declarative. You just say how things "are", and never describe things at the imperative, this-then-this level. 2. As a consequence of the above, you should never have to consider discrete time. Time should be a continuous abstraction, or some meaningful topology separate from its implementation. 3. No concept of global state. Yuck! Didn't we run away from imperative languages to escape this? All state, if any, should be highly localized. 4. *Composable* semantics. This is the whole point of Haskell. Every component should be composable, and everything 'complex' you do should be seamlessly assembled from smaller, simpler components. I am a little biased :) But this is what I've found over my time experimenting with Haskell game development. There are people who will argue that these aren't really the best values to look for...but to me, they are the reason why I'm in Haskell in the first place. Anyways, you might want to consider looking at FRP and AFRP, which I feel is a contender to answering all of these problems and more :) See ocharles's great series if you have not already :) http://ocharles.org.uk/blog/posts/2013-08-01-getting-started-with-netwire-and-sdl.html http://ocharles.org.uk/blog/posts/2013-08-18-asteroids-in-netwire.html
I agree. Sometimes it's a pain in the ass to try to dig around docs to find that one function especially since everything in Haskell is so compartmentalized. The index page in haddock docs is nice, but it still doesn't help that you have to dig regardless.
We are using Haste at Soostone for a fairly complex application's UI and the experience has been good so far. It produces relatively small amounts of JS and we have not run into any implementation bugs yet. The resulting code is typically quite fast - it's certainly been enough for an application, though I don't know if it would be adequate for, say, a game. The project started off using Ember, but we quickly reached a point where we could not refactor, enhance and solidify our design as rapidly as we typically do on the Haskell backend. With increasing complexity, the lack of static typing and all the wonders of Haskell (purity, etc.) took a large toll on our Ember codebase and our ability to evolve (let alone understand) it. Perhaps some of this was due to us not being JS experts, but I am inclined to think that was not the full story. It was also a very fascinating realization that you can actually cross-compile many of the packages already on Hackage. Others, though, will give you headaches (see below). There are a few gotchas, however: - It is easy to blow stack - beware of any operation over a long list. You may have to occasionally mess with your code a bit to make it constant-space. - The standard library is not comprehensive; we augment it with new JS FFI calls all the time. - The standard library is not always fool-proof either. It was, for example, not properly escaping AJAX query URLs. Some of these issues we have fixed on our haste-compiler fork and other we fix in our own library code. - Anything with text, time, bytestring and some other common dependencies on Hackage with C-calls will NOT compile on Haste. This is very annoying, and we have had to fork a number of packages to extricate all such dependencies. A principled solution to making existing libraries cross-compilable to JS would be very nice. - No template haskell - have fun defining lenses by hand! Hopefully, some of these things we discover/develop will be absorbed back into haste at some point, but we have not initiated any conversations around that yet. 
I believe GHCJS is better at handling many of those gotchas and generally seems to provide the smoothest experience in terms of writing the same Haskell code you always would. I'm under the impression that the JS code Haste generates is significantly better, though.
Yeah, I believe so as well. We picked Haste for now as it was readily working and the small output it generates makes it usable today. Porting would likely not be that hard, though, once GHCJS matures - only the DOM and JS FFI touching parts would need to be adjusted.
FRP looks interesting, but an advantage of having a global state is that it can be serialized and inspected, which is especially good for hot swapping out code. I haven't dug too much in to FRP, but my initial impressions is that this is impossible. This is the main reason I'm not going to use FRP and am instead looking in to different ways to write a bit of a 'functional' engine. Another problem is communication between objects. In the OP, you can see it'd be easy to implement using the system of IDs, but I've thought that a better way would be to group objects and have object communication be scoped. Any thoughts?
Hmm, thanks for the suggestion... I think I've been defaulting to the fully qualified names because I haven't gotten used to making a local qualified name and generally run into errors with unqualified imports of standard libs. Maybe if I make the last component of the module name unique within the project then that will be a good default that I can remember across files. Will try it out.
Interesting, I will try that out!
Since this feature doesn't change the actual behavior of a program, just warnings given, then by making it a pragma, it can be automatically ignored by Haskell compilers that don't understand it.
FRP is really neat and definitely has the advantages you describe, netwire seems to have a lot of good ideas. I think I mainly went with the game loop as it allows me to follow the book better and there are some interesting things that can be done with global state. I'm also trying to see how much can be done with basic Haskell before trying out more powerful libraries.
Looks good! Would be neat to see components and Netwire used together.
Perhaps someone else manually uploaded the package.
&gt; Why not simply learn JS and write it as well as you can? Or just distribute a `.exe` by `ftp`, or better by BBS. Jokes aside; I don't believe "just" JS is possible for anything more then the smallest 'apps'. Quickly jQuery (or similar) is added, and then Angular/Ember/Backbone/Sencha/etc... These very much push your JS into a particular style: imperative and with lots of implicit state. Not so strange that the Haskell community that knows the pain of developing/maintaining JS apps is search for alternatives. http://www.haskell.org/haskellwiki/The_JavaScript_Problem
Angular is not a gui lib. But there's angular-ui that builds on it. Perhaps you meant reactive framework akin to angular. 
I wrote this program: https://github.com/emmanueltouzery/cigale-timesheet using Fay. It is a localhost web app though: server and client on the same machine. I am happy with the result compared to what i would achieve using a "real" GUI library (Qt or Gtk), especially given the state of Qt bindings from haskell. I use the knockout JS library to separate the code and markup. I ran into a few Fay bugs when writing this, but now I'm happy. I do miss typeclasses and you cannot write Fay haskell and not be deeply aware that it's running over JS. Still you get the nice syntax and type-safety until the point when you call JS. I am not convinced however at looking at the JS code generated by Fay, I never quite managed to do that. But the Fay syntax for JS FFI is intuitive enough that it never blocked me too long. I wish I had the time to research Haste or maybe GHCJS more but Fay is workable. NOTE: the Fay part is there: https://github.com/emmanueltouzery/cigale-timesheet/tree/master/src/WebClient
What can you say about [PureScript](http://www.purescript.org/)? As far as i see it is similar to fay/haste. Written in haskell and has a haskell syntax.
Hmm.. perhaps some useful gdb scripts could help here, but it's going to take a lot of GHC internals' knowledge.
hi, I have trouble on Windows 8x64bit. I have the latest platform and a binary GHC version 7.8.0.20140228 on my PATH and cabal-install version 1.18.0.3 using version 1.18.1.3 of the Cabal library. However, I cannot install many packages. For example, when installing Text, I get this: $ cabal install text Resolving dependencies... Configuring text-1.1.0.1... Failed to install text-1.1.0.1 Last 10 lines of the build log ( C:\Users\mantkiew\AppData\Roaming\cabal\logs\text-1.1.0.1.log ): Configuring text-1.1.0.1... setup-Cabal-1.18.1.3-x86_64-windows-ghc-7.8.0.20140228.exe: does not exist cabal.exe: Error: some packages failed to install: text-1.1.0.1 failed during the configure step. The exception was: ExitFailure 1 Am I doing anything wrong? I'd like to test the new GHC and Cabal with my projects on Windows.
Most FRP implementations allow you access to IO as an underlying monad, so really I don't think it's inherently less capable at that than anything else in Haskell. I'm assuming here you are talking about computing the state of the next time step and not the actual rendering --- the rendering is actually an implementation detail that is unrelated to FRP as an abstraction for the most part.
Right, I'm not sure if adding this warning is worth it, since these cases are most prevalent in base, and the pragmas have been added there. However, if it's easy to add, it could be worthwhile! Might spare someone some head scratching.
&gt; There are people who will argue that these aren't really the best values to look for...but to me, they are the reason why I'm in Haskell in the first place. Let me do a little bit of that :). I agree that these are great values, and that having a beautiful declarative framework for doing this sort of thing would be wonderful. At once, I disagree with the notion that this and things like it "attempt to shoe-horn imperative ideas into Haskell". Haskell is great for imperative programming, and there's nothing wrong with writing imperative code. It may well be the best way to express your solution for domains like game development, where it's important to be able to understand exactly what is happening per iteration of your game loop. Abstractions like FRP, though concise and powerful, can make it unclear when things happen at runtime (like when memory is freed ;) ). The classic solution here is having a "thin layer of IO around a pure core". Ideally, that'd be the case. However, even if all of your functions are in IO, the benefits of functional programming / Haskell's value extends far past the separation of pure code from side effecting code. So, if your goal is to spend a bunch of time exploring the design space, have at it! Haskell is a great place to explore the frontiers of designing abstractions that lead to fast, correct code. However, I don't think someone is "doing it wrong" by ignoring FRP. It's certainly important to emphasize avoiding side effects and mutation when people are starting with Haskell, coming from other languages, but I don't think it should be upheld as a rule that overrides reason.
Perhaps, but at least one reader stopped reading after looking at a few identifiers
Will do
Let me see if I understand your question: You have something like: data Foo {- ... -} deriving (Eq, Ord) deduplicate :: Vector Foo -&gt; Set (Vector Foo) -&gt; Vector Foo deduplicate xs set = if Set.member xs set then maybe xs id (Set.lookupLE xs set) else xs and you want to test that `deduplicate` behaves properly: if `a == b` and `Set.member a set` then `deduplicate a set` is the same object in memory as `deduplicate b set`. From looking at the docs, I think [StableName](http://www.haskell.org/ghc/docs/7.0.2/html/libraries/base-4.3.1.0/System-Mem-StableName.html) can help you do that. You can write a test function: -- if a == b and a/b is a member of set, then this will probably return True. -- If it does, then deduplicate is definitely correct. If not, then deduplicate might -- or might not be correct (because StableName doesn't make that guarantee). test :: Vector Foo -&gt; Vector Foo -&gt; Set (Vector Foo) -&gt; IO Bool test a b set = do let !a' = deduplicate a set !b' = deduplicate b set (==) &lt;$&gt; makeStablename a' &lt;*&gt; makeStableName b' I *think* this will work but I haven't tried it out. (So I'm sorry if this is misinformation!) Also note that `StableName` might return `False` when using `(==)` on two `StableName`s of the same object, but the documentation isn't specific about the circumstances. (If it does return `True`, however, then you're OK.) The strictness annotations (the `!`s) are needed here because otherwise you end up comparing the unevaluated `deduplicate a set` against `deduplicate b set`, and those will always be different (and will have references to the original `a` and `b` in memory). This is important for your usage of `deduplicate` too: if you don't force its value, you might be carrying around a thunk with a reference to the duplicate vector, rather than the `deduplicate`d version.
how is that cleaner? toComponent is pretty self-describing. "c" doesn't mean anything and means the reader needs to look elsewhere to know what is going on. (Fewer characters /= cleaner)
A few comments from someone who has used Git but not other DVCSes: &gt; Git commit/add does have -p for a darcs-like interface, but you have to know about and ask for it, and usually when I mention it to people, they've never heard of it. Defaults matter. I consider git commit -p (and achieving similar results using the staging area) harmful, since it creates versions that never existed on the FS and thus were not tested. &gt; Git usage and culture encourages lots of rebasing and squashing. I don't know about large-scale rebasing, but I like using rebase to maintain a set of commits as a "patch set" organized by functionality or slowly building up code: each patch can be reviewed separately, but my ability to look into the future and see how the code should be organized is limited, and there will inevitably be bugs, so I want to be able to go back and edit things. &gt; However, I think git is obviously inferior in almost every way to at least mercurial, and is probably inferior in every way that matters to bazaar ( I know a lot of people don't care as much about this, but since you said "every way that matters": the speed of git, especially its low latency for simple operations, fundamentally improves the experience for me, so I'm wary of other VCSes which don't put as much emphasis on speed. In particular, both hg and bzr are written in Python, and git log or commit in a small repository can complete in less time than Python takes just to start up. I haven't tried Darcs, so I don't know whether it has a similar feeling.
Having explicit global state that can be trivially inspected and swapped out (provided your state doesn't involve storing callbacks) is pretty awesome and it's something that people from imperative languages envy, so score for Haskell for that one :) (and for this global state monad method) Hot swapping in FRP and serializing global state is not impossible, in fact, Elm (the FRP language that targets JavaScript) proudly boasts it as a major selling point, and it's pretty impressive --- see http://elm-lang.org/blog/Interactive-Programming.elm and http://www.youtube.com/watch?v=FSdXiBLpErU . It's actually pretty "easy" in Elm because the signal graph is static. As for serialization, the author of netwire, Ertugrul Söylemez, is working on a new FRP abstraction that gives you serialization "for free" --- that all wires/behaviors can be easily (from both an implementation and an abstraction standpoint) frozen in time, saved to disk, and reloaded later with all connections intact. Even without this, with netwire now, remember that all Haskell functions are still 'pure', and all state is immutable and pure, so it would be fairly straightforward to create new wires/behaviors and load them with the state of previous ones. Communication between objects is actually a problem I have been working/researching on myself. I have been working on a Haskell Real-Time Strategy game/Battlefield clone (the engine should be good enough for both but I haven't decided which yet), where you could imagine that effective and efficient object-to-object communication with scoping is very important. From my results so far, the solutions is definitely expressible within the framework of FRP. But, like all FRP solutions, it requires a fair bit of vertigo in abandoning your ideas of a game loop and thinking about things declaratively. One thing exciting about FRP is that a lot of problems like this are still open and being solved very day :) You have the potential to change the world :)
Elm is implemented language level which kind of bypasses the issue. That's news to me about the new abstraction. Do you have any links to that (emails, etc?) I understand right now the functions are pure, but I'm mainly worried about wires that create new wires containing 'hidden' state, and losing that. Will you be publishing your research anywhere?
Pragmas are not language features that you turn on or off, you just *use* them where it is necessary. So asking to turn them on or off by default is nonsense. If you meant to ask why isn't it a normal language feature, I don't know. It is probably because the problem was unforseen in the early days of Haskell and they cannot agree on a way to modify the semantics of the Haskell language to make it a regular feature.
Beats me. I tried out Fay a couple of days ago and pretty much instantly switched to Haste. It seems to do the same thing, just better.
They use Fay, as far as I understand.
Sorry for the double post, but thinking more about it, but my main problem is this: https://github.com/mstksg/inCode/blob/master/code-samples/machines/Auto.hs#L49-55 There's no way to serialize /internal/ state which is used when swapping.
I use Ubuntu 12.04. Recently I deleted all packages with apt-get (ghc*, haskell-platform, etc). Now I want a new install. So I download [this](https://www.haskell.org/ghc/dist/7.8.1-rc2/ghc-7.8.0.20140228-i386-unknown-linux-deb7.tar.bz2) tar.bz2 file and `./configure` and then `sudo make install`. Everything goes well. I need to reinstall `libgmp` to start `ghci`. But where is `cabal`?
Ah. Well in this specific example, the point to take from it is that you are only given access to explicitly offered state. Perhaps in a real world FRP example, you would maybe wrap everything serializable in a data SerializeWith a s = SW !a s and attach a lazy data structure carrying internal state enough to reconstruct. Then when you start a new, say, Soldier wire: soldierWire :: SoldierState -&gt; Wire Input (SerializeWith Soldier SoldierState) cleanSoldierWire :: Wire Input (SerializeWith Soldier SoldierState) cleanSoldierWire = soldierWire defaultSoldierState so that only what is strictly required will be offered, and you also can't modify it, only view it. If you wanted to hide it further you may do something like data SerializeWith a = SW !a (FilePath -&gt; IO ()) where it comes with an IO action to write the serialized data to disk. The whole point is that you have to explicitly specify the state you want shared. But....hey, that's basically the same thing with the giant state monad too --- you have to explicitly specify the structure of your global state. But you only have to do it once there, but once for every serializable structure here. :/ It's not quite "for free"...but it's possible. There actually might be better ways; I haven't quite put too much thought into this. It's mostly because of the nature of the Automation Arrow abstraction. But remember that Automation Arrows are only *one* implementation of FRP/AFRP, and many implementations might not use it. What is important isn't the *implementation*, but the *semantics*. Perhaps there is just an expressive/powerful FRP implementation that does not use Automation Arrows with inaccessible state that makes serialization trivial? I am awaiting Soylemez's new serialization techniques myself though and I assume they are much more elegant than mine, and do not require any of this explicit stuff. You can keep track of some of his progress at http://hub.darcs.net/ertes/wires as he pushes new commits, and you can catch him on freenode's #haskell by the name mm_freak ... although recently he has stopped idling on #haskell to devote more time to his new FRP abstraction, but is available to query. conal might also know more about this than I. As for publishing my research ... it's not really that formal, I am planning on putting up a write-up on my blog :) And i'm planning on open-sourcing my entire project. you can sort of catch my ongoing process at the github repo https://github.com/mstksg/netwire-experiments and when I manage to write something up i'll post it on http://jle.im . Also, about Elm being language-level --- yeah, it is, but its principles --- a static signal graph --- can be adapted to Haskell with, presumably, its hotswapping abilities. Well...aside from the hot swapping problems that every other Haskell program faces.
I have made a first pass docker image for this, you can play with it by doing docker pull alanz/ihaskell docker run -d -p 2222:22 -p 8778:8778 alanz/ihaskell and then point your browser at http://localhost:8778 
GHCJS generates a *lot* less js than it used too, iirc.
&gt; Usually when you use an undefined method, the GHC runtime will report "&lt;&lt;loop&gt;&gt;" and terminate, but this is not guaranteed. In a large program like mine, it simply freezes the whole program. You seem to have gotten stuck on this wrong explanation of what happened with your program, and I think it's contributing to the confusion in some posts in this thread. Your program didn't "freeze because it used an undefined class method". That isn't a possible cause-and-effect. If you fail to define a particular method in an instance declaration, the compiler will supply one, in one of two ways. If there is a default definition for the method, the compiler will use that. Otherwise, the compiler will use a definition like `error "unimplemented method Foo for type Bar"`. You did supply default definitions, so you are in the first case. The problem is that the default definitions that you wrote were an infinite loop: something like `b = c; c = b`. So, when you failed to provide a non-default definition for either method in your instance, the compiler generated code to enter an infinite loop when you called either method for that instance, exactly like you asked it to. Now under some circumstances GHC wlil print "&lt;&lt;loop&gt;&gt;" when it can detect that your program has entered an infinite loop: specifically when it is evaluating some heap object and finds that it needs the value of that same heap object to do so. Many infinite loops are not of this form, and anyways it can be difficult to see exactly how your program will be translated into heap objects, so you can't rely on this happening.
Hello, I'm a programming newbie I ran into a few problems Installing libzmq git clone git@github.com:zeromq/zeromq4-x.git libzmq with arch error: cannot run ssh: No such file or directory fatal: unable to fork with ubuntu Permission denied (publickey) fatal: The remote end hung up unexpectedly I then just clone https://github.com/zeromq/zeromq4-x.git and then install Trying to install ihaskell cabal install ihaskell Configuring zeromq4-haskell-0.3... cabal: the pkg-config package libzmq version &gt;=4.0 &amp;&amp; &lt;5.0 is required but it could not be found
I think this solution focuses a bit too much on the "interactive" bit. Not all of us are just going to load files and display some relevant plots/analysis. As a matter of fact, I'm interested in all these stuffs mostly for implementing AI and math algorithms whose performance should be production-grade. That's one of the reasons I started working on [accelerate-blas](http://github.com/alpmestan/accelerate-blas), I think it can play a nice role here, although I haven't worked on it since I started my job. So to sum up, I'm pretty sure we can get great results if we actually pay attention and time to doing it a bit more "our" way, using types, rearranging the AST of expressions maybe, etc. The key thing being to make sure that we use the great raw routines from optimized libraries when actually performing the operations, or to implement such routines ourselves. We have a few tools already, but there's quite some work :)
I can't imagine what lens looks like in JS.
Ok. I was initially focusing more on the interactive bit for a few reasons. - This seems to be the main use case of statistics environments for most scientists. It's clear now that there's another use case, for developing robust, efficient analysis algorithms in a painless way. (Did I get that right?). I may have severely underestimated the proportion of users for which this is critical. To get a better idea, If you work in AI/math algorithms, what do you currently use for development? What bugs you the most about your current approach? These two goals are certainly related, but at this point in time, for the purpose of building a cohesive environment I think it may be more useful to "band together" on the first one because: 1. as /u/cartazio mentioned, the second goal is a research grade problem, where as I think most of the pieces for the first goal is already there, but just needs to be brought together. Making haskell accessible for data munging will increase the number of people interested in the second goal, and so accelerate the process. 2. There already seems to be a reasonable amount of work on the algorithmic front, with repa, accelerate, hmatrix etc. This side is certainly important, and will be the bit that blows every other package out of the water in the end, but we haven't seen that many people use them in actual science. As far as I know, Haskell is _already_ more performant than (for example) R on these things, and it could bind into existing solutions wherever it is not; so this doesn't seem to be the barrier to entry. I think part of the problem is that in order to get good performance in haskell, you need to make your types really clear so the compiler can do its magic. This is great, this is what we want, this is what makes haskell so scalable (rather than having to choose between good design/documentation, you NEED it to get good performance). But in practice, most people's approach to data analysis is to mess with the data until they get it right, and then build up from there. For this approach, I have found the type noise in tools such as repa to be counterproductive. I think the incremental typing that seems to be in the works for GHC will go a long way to unify these two use cases, but it will be easier to build the interface first and then add fancy typing and a performant backend (assuming we are careful about the initial design) 
Have a look at https://github.com/alanz/IHaskell/blob/master/Dockerfile#L32 for one way of installing zmq4, and the from line 72 on for the IHaskell install. I had to sepcifically install classy-prelude-0.7.0, the one on hackage gives a compile error in IHaskell This is on Debian testing
it still depends on cereal-0.3.5.2 which breaks with ghc-7.6.3 (because of a newline right before a `#-}`), but once I patched that manually and installed it (with profiling libs), it seems to be working! Good work!
Hey there! Author here, let me know if you have any questions. Development has been a bit stalled due to lack of free time, but there's an open PR with a working docker file. Should be merged this weekend.
I believe [LYAH](http://learnyouahaskell.com/) should have answers to all the questions you have. It appears to me from your code that you don't get how Haskell works yet, so I would suggest you to start from the beginning of the book.
Could you come up with a more generic title? I can still barely make out it might be something about Haskell … in /r/haskell!
Start with the basics. It looks like you're trying to run before you can walk. When learning Haskell, start by forgetting everything you thought you knew about programming.
- Do: this is just syntactic sugar to make it easier to work with monads. Personally, I'd suggest you avoid `do` altogether until you have the hang of `&gt;&gt;=` and other monad stuff. After all, [do notation considered harmful](http://www.haskell.org/haskellwiki/Do_notation_considered_harmful). - Let: In Haskell, a function can have only one line in it: it takes some arguments and turns them into something. However, sometimes, it's useful to define intermediate steps along the way, and that's where `let` notation comes in: it allows you to define a bunch of helper functions and variables before you get to the thing the function gives back after it's called. - Where: `where` is exactly the same as `let` except that it lets you define your helper functions/variables after the function's implementation instead of before it. Stylistically, use `let` when the meat of the function is in these helpers (and the actual function statement just ties it all together), and use `where` when the part to focus on is the actual thing the function produces (and the purpose of the helpers is obvious from their names). As for your code, get rid of the `do` (because you can do this without monads), and personally I'd use a `let` instead of a `where` (though either is okay). Also, remember that all variables in Haskell are immutable (read: const), so you shouldn't have a line like `count = count + 1`. 
Very cool resource, thank you! I almost missed the part where it said I could read online for free!
Excellent response, thank you very much! &gt; Also, remember that all variables in Haskell are immutable (read: const), so you shouldn't have a line like count = count + 1. Okay, that's a tricky shift in thinking. How does one go about constructing a counter in Haskell? Could I append items to a list, and then "return" the length of the list?
You can start with a list of all the values, filter out the ones that are too small, and take the length of the resulting list.
As /u/kqr said, it sounds like you're trying to run before you can walk. As /u/pkmxtw suggested, go read LYAH for a while. Lists, like all variables, are immutable. You're still thinking about programming in a very imperative style (first do this, then do that, then do this other thing), whereas functional programming is much more about the concepts behind what you want to accomplish and less about the nuts and bolts of exactly how to go about doing it. Let the compiler figure out how to accomplish the things you tell it to do.
So you have a problem with writing multiline functions? Put everything on a single line :) countAboveAverage :: [Int] -&gt; Int countAboveAverage xs = length $ filter (&gt; avg) xs where avg = (sum xs) / (fromIntegral (length xs)) Which reads almost like a sentence: how many items (`length`) are above average (`filter (&gt; avg)`) where avg is (=) `sum` of a list divided by its `length`. Then your howManyAboveAverage would be `howManyAboveAverage x y z = countAboveAverage [x,y,z]`
Maybe you want to make a counter that triggers every 0.5 seconds, and make from it another event that triggers only on even ticks of it? That would be possible without complicating the design, so unless you want to make it *really* dynamic(move gradually from 0.5 to 1) or something it would be a good solution. But my experience with FRP is only from Elm, whose networks are always static atm, so don't quote me on that.
Two `StableName`s of the same thing in memory always compare `True`. If you make two stable names to two different objects, if the optimizer kicked in and spotted could be made the same thing, then it'll return `True` some times when you expect `False`. The only time you'd get a `False` where you expect `True` by control flow is when you let some non 'work-safe' optimization slip in and recompute the thing twice.
Presumably if you need an intern table like this, then you should be returning the unique ID associated with each vect. By just returning the unique copy of the vect, you lose its identity information— which you'd have to recover by looking it up in the set again, which is slow. This is why the standard way of interning things is with something like: newtype ID = ID Int -- Do not export this data constructor! data Intern a = Intern {-# UNPACK #-} !ID !(Map a ID) !(IntMap a) -- Make sure to wrap with ID ...with the appropriate functions. Of course, in practice we'll usually want to monomorphize `a` so that we can specialize `Map a`. The reason to use some `ID` newtype in lieu of `Int` is to prohibit users from manufacturing invalid IDs, or adding IDs together, or other nonsense. Though I'm not sure I understand your problem domain. Is interning vectors really what you want? (E.g., if they're vectors of `Double` or `Float` then that's not going to work very well because of the ieee-754 fuzz...)
It'd you want to know if it works in general, define a new pair type where the Ord and Eq instances only consider the first element of the pair. Then test with elements where the first elements match but the second elements don't. You'll be able to see which version you're getting back. If it works with that type, it will work with your vector type.
Looks like you're getting the hang of it! I just wanted to point out that this: (fromIntegral x + fromIntegral y + fromIntegral z) / 3.0 Could be: fromIntegral (x + y + z) / 3.0
This is the simplest way to ensure that, but it adds another level of indirection. They are vectors of (Int,Int)
I did exactly the same, I already have main tick event for framerate(in my case it's 50fps = 20ms), and I'm filtering this event in my period function. But problem is - be able to create new event from "period N"(where N is value from behavior) when points changed to 2000. Hope you can understand what I mean.
[EclipseFP](http://eclipsefp.github.io/) was not exactly easy to set up last time I tried it, but once that is done it supports nice things like source look-up and cabal configuration management.
[This](http://tim.dysinger.net/posts/2014-02-18-haskell-with-emacs.html) blog post might be useful to you. I use most of the tools mentioned including stylish-haskell, ghc-mod, structured-haskell-mode, etc. Jekor has some decent instructions for Hoogle search within ghci [here](http://www.youtube.com/watch?v=QpDQhGYPqkU&amp;list=PLxj9UAX4Em-Ij4TKwKvo-SLp-Zbv-hB4B). As far as I know, emacs with the above modules has the "best" support for Haskell.
Before I come off like a dickhead, let me say thanks for making this :) I am always happy to find examples of people demonstrating things that are interesting for people outside of type-hackery circle jerks. I can appreciate how nice it is to have such a modular implementation like this, but damn. In C these things are clear as day. You can literally figure out how Jacobi iteration works by just looking at an implementation. By contrast, this implementation is so terse I barely followed it, despite having implemented this in several other languages and having written some simpler things using Repa in the past. I don't know why Haskell programmers pretend like they don't need to comment their code. It would help a lot!
Why does it matter?
You forgot Standard Chartered Bank among the Haskell users. We probably have more Haskell code than anyone else. 
Alternatively ,does any editor support Haskell as well as Emacs supports Agda?
In all seriousness, it depends on the problem I'm solving. I like to use the appropriate tool for the job. Just last week I wrote utilities in Haskell to scrub through subversion logs and they're all way less than a screenful of code, because "interact" is so powerful for stdin/stdout streaming. But, for some tasks I'll write the front end in Haskell, a control interface, and using a unix pipe pair, talk to a C program to do something low-level (like talk to Controller Area Networks CANBus)
Can you explain panel 3? Is this true? I thought golang was built for high concurrency stuff. How can Haskell be more efficient at it?
&gt; but I do not have to worry that much about the type system, nor dribbling the lack of IO with monads. First off, [monads don't "dribble around the lack of IO"](https://dl.dropboxusercontent.com/u/7810909/docs/what-does-monad-mean/what-does-monad-mean/html/index.html). With that out of the way, I would argue that these (type safety, controlled effects, etc.) are exactly what make me more productive in Haskell. I don't have to fuss around with things that "seem like they should work" because if they don't work, the compiler will outright tell me so, in most cases. In other languages (CoffeeScript in your example) your code will "compile" (i.e., be syntactically valid) but you might not know that your code is doing something wacky until way later, possibly when it's already in production. The type system of languages like Haskell, when used properly, give you a much better idea of when something is not right. This means that I am more productive because I don't have to spend a bunch of time fixing bugs later. I can usually code something, make it typecheck, and move on with my day. Things like refactoring later on become much easier because of how programs are laid out. Instead of having to change the entirety of my program, I can change small pieces of it. And when I do have to change something that affects a large portion of my code, the type system can once again help out by pointing out where I need to update my code. So yes, Haskell makes me more productive because it forces me to get things right the first time and not have to dilly-dally around code that I think is supposed to work but actually ends up biting me in the ass when I least expect it/when it matters.
It depends on the problem domain. Any anecdote I give would only apply to myself, my problems, and only after a serious time investment spent learning the language. So you probably shouldn't listen to me. That said, yes I definitely feel more productive, but it's a different kind of productivity. In other languages I feel like I arrive at a "solution" faster but spend the majority of my time debugging and rewriting it as problems occur. In Haskell the language really guides my development of the solution to the point where once I get the OK from the GHC, I rarely if ever need to debug much at all. I feel like the net time ( development + debugging ) comes out significantly less with Haskell. That point aside, I think there's a big psychological component to which style of development you prefer. I've worked with a lot of programmers that genuinely prefer having a quick incomplete solution, watching it fail at runtime, and debugging it into some semblance of stability over time. I really despise debugging code ( or worse other people's code ) and would much rather just nail the problem up front with some type error and be done with it. Haskell accommodates this much more than any other language I've worked in.
A technical nip: a topology must be closed under arbitrary union of open sets as well as binary unions. Of course, it sounds like you might be working in a finite topology, where binary unions would be sufficient. But if it is, call it as such. Also, if you're going to define a presheaf typeclass, you should keep it closer to the original definition. Don't use `Functor` if what you have is actually a `Data.Functor.Contravariant`. And don't defined `Presheaf` as a typeclass on `f u v`: being a presheaf is a property of `f` all by itself, and is not related to `u` or `v`. I won't comment on the sheaf definition, since it hinges on presheaf. It's an interesting perspective. From a Google search, it looks like there's already some research on sheaves and concurrency. 
Open sets in a topology must be closed under arbitrary unions, not just finite unions.
&gt;With it I can do most of what Haskell can do, semantically - but I do not have to *worry that much about the type system*, nor dribbling the lack of IO with monads. The more you use Haskell, the more confident you'll become and the less you'll worry about the type system. Eventually you'll look to cause type errors *on purpose* in order to help you better understand your system. With a little help from tools and new features such as [TypeHoles](http://www.haskell.org/haskellwiki/GHC/TypeHoles), you'll start to see the type system for the powerful tool that it is.
Thanks for your addition! I guess my answer to freyrs3's is valid for you too. I know you guys are so passionate about the type system and how it actually boosts your productivity in a lot of ways - I hope a day I will able to have a taste of this effect myself!
Interesting. How hard is C interop, may I ask?
I may be harmfully wrong, but I see the type system as a sort of secondary language that lies on top of the main one, that kind of permeates it by enforcing certain structures defined by the programmer. So, in a way, programming in Haskell is like programming in two languages simultaneously. Is this too far off?
TypeHoles look awesome. Lean on the compiler in a major way.
&gt; Maybe I live a strange phenomena but I spend almost to no time with bugs, at all. Most of my scripty code come clean from the first time I write it and when I do have some bug I manage to track it really fast because 99% of the times Chrome just points the line where something went wrong. This is like saying that "driving is easy if there is nobody else on the road". Types will help you the most when you need to read or maintain code written by other people (or written by yourself a long time ago so that it's no longer fresh on your mind). At some point you are going to exhaust the limits of what you can remember or fit in your head and that's when you need types the most.
It's a useful property of a language to be able to trace bugs quickly, but I think it's even more useful to be able to claim that no bugs ( at least of a certain class of errors ) can exist *at all* in a certain region of code and to be able to say that with absolute certainty. There's a lucid [article written](http://blog.ezyang.com/2011/12/bugs-and-battleships/) by another Haskeller about debugging as related to the game battleship, worth a read to better understand this philosophy. 
When beginning, sometimes yes, sometimes no. There is more impedance (today) to writing, say, a web GUI in Haskell than in JS. The first moment the project hits a major curve and I need to refactor: yes, absolutely yes. I feel like a, say, coffeescript project evolves smoothly so long as I keep it in my head and grow it like one perfect crystal. If it ever shatters, however, it's very hard to get the pieces back together. You end up using a lot of glue and memorizing pitfalls and hacks. It's especially bad bringing a new person on to a project who never knew the original shape and can't predict the sharp edges. They just have to cut themselves on each and every one of them individually over time until the shape and frailty becomes clear. Type systems prevent that. It hurts the first time because glue and hacks get things to 10% again quickly, but the remaining 90% needs a lot of time and patience. Type systems point the way and don't give up until it's back together.
This is interesting! i've been starting to think about how to make writing distributed computations sane/accessible myself. But this does seem to capture a really really nice notion of locality! (no surprise, given the origin in geometry)
I'll try it out soon!
Well I have never used CoffeeScript so I don't know if Haskell would be a better productivity tool than that for web development, but I am tempted to say it would be, if it could more reliably compile to "asm.js" What makes Haskell the best choice for me is that the static typing catches so many potential errors at compile time and I waste comparatively little time debugging my code at runtime. Also, tools like QuickCheck make debugging easier. This makes me very productive.
To be fair, this is relevant when you do more advanced stuff, and as the implementer. As a user (of things that might sometimes make use of those selfsame advanced features), I don’t do logic programming. The compiler does.
Oh wow, that's going to be a roller coaster all right. I've worked at least a little bit in each of them, and they all have to be approached differently. Coq is a really crazy thing... you are sort of programming in a *really* pure and minimal functional language, except you're often doing it interactively via a command-based "tactic" language that helps you build the program based on the type you gave it; sometimes you don't look at the resulting program (or run it) at all! And it doesn't feel a lot like programming in the sense you're used to (even if you're used to Haskell and Prolog). Anyway, hang in there, and try to embrace the madness. Those are three very interesting languages and you can get a lot out of learning them as long as you don't let yourself get too frustrated. For haskell questions, there are usually people on #haskell (freenode IRC) who can give advice or a quick code review.
trivial. Like, linking into fortran/c code that uses the standard types is really really really really really trivial. easy. 5 seconds, happy ending. done. foreign import ccall "functionSYmbol" functionNameHS :: CInt -&gt; IO CDouble done
This is absolutely the case—except I'm not sure which language is secondary. The funny part is that since the two languages are in such concert you don't fight them, you just pick whichever one does the job best and then patch up the second. Sometimes, I just write the type I want and then fiddle with type holes until the implementation appears. It's completely mechanical (and even automated in some systems, i.e. Agda). Others, I write the implementation I want and then ask GHC to just infer a general type for me so I can understand how my new fragment fits into the larger type program I'm writing. Finally, most of the time, I do both of those together. It's like pair programming with yourself. All of the good parts anyway.
Just merged that PR. Seems to be running for me with no issues. Let me know if you have any (via Github issues).
Thank you very much for these exercises. Please keep them coming!
In the last three months I have programmed in C++, Javascript, PHP, Objective C and Haskell. Hands down Haskell. It wasn't always that way, but it definitely is now. Of course that begs the question, am I productive programmer ;) 
There are some great answers here that have pretty much said what I would have said, but I'll add my own summary: For any project that is going to exist longer than throwaway I feel more productive in Haskell. This is exactly because of the type system and purity. I never feel I'm fighting the type system. On the contrary, I feel that the type system is guiding me to correct code.
Big smile made.
I'm actually significantly faster in F# for most problems, though for certain exploratory tasks Haskell can be more straightforward. The biggest issue I have is with reading/writing point-free form.
SHM is totally amazing.
Sorry, I tried to make my description understandable to a variety of people here, but it was getting a bit long. A sheaf is basically a very, very abstract jigsaw puzzle, in that two subpuzzles are equal if their pieces have the same shape and region of the picture, and you can glue smaller subpuzzles together to get larger ones that smoothly connect the picture on the surface. Except with sheaves, the pieces can overlap, so the picture has to agree on the overlap if you want a clear picture. Don't worry about finding this stuff dense, abstract, and hard to understand; I do too. One of the classic texts in category theory ends with a section titled "All Concepts are Kan Extensions". The author isn't joking either: almost every concept the book covers besides the ones required to define Kan extensions is (mostly implicitly) defined in terms of Kan extensions in about 3 or 4 pages.
Personally I don't see the type system as a secondary language -- probably because I don't do fancy stuff with it. I see it more as a 2 levels language. I first think about data and how it flows. Data goes in, (read on a socket, user input, file read) Data goes out (displayed, printed, written to a file or socket) and that gives me input and ouput types. Then most of what a program does is figure out how to compose new or existing functions that takes you from the input type to the output type, while trying to leave IO for the outer layers as much as possible. Once everything is set up in terms of what function I will need with what types, then it's time to actually code the functions that doesn't already exist. The type is a powerful guideline for that too. Well when I code in haskell it's a bit more interleaved, I don't really figure out every types of every function I will need at once before I start coding the actual functions. There's a lot of back and forth, GHCI helps a lot there. So, I've been using haskell for a few programs at my job, and the type system has been very useful in preventing all sorts of mistakes. For example, I newtype'd almost every int data so that I wouldn't be able to mix order number with packing machine ids, etc. That helped catch bugs before I even tested the program. It made me much much confident when it was time to put the code in production. I've make a couple refactorings on the code base that all went without trouble, with the compiler reporting to me what was impacted by my changes, until everything compiled again. 
That explanation was illuminating, thank you so much! &gt;Don't worry about finding this stuff dense, abstract, and hard to understand; I do too. The difference is that I don't have a formal education in maths, meaning that learning about the properties of abstract structures gets me nowhere; I need to grasp how they physically manifest before I can actually understand what's going on.
I'm going to try this, I've never actually used a rest api in Haskell before. 
The biggest advantage would be the availability of the .NET ecosystem; Visual Studio is an absolute joy to work with, with integrated build systems, unit test tooling, a profiler, a debugger, and various third-party memory inspectors. F# just hits the right spots in a lot of ways. Its signature features are heavily centered around data access, and it has great special syntax for monads, monoids and embedded DSLs. One language extension (joinads) has a special syntax for non-deterministic concurrency which I LOVE. In addition to that, it's cross-platform, open source, and it compiles to a variety of back-ends (CIL, CUDA, JavaScript). Haskell definitely has its cool stuff, but the lack of tooling is a huge downside. I also don't trust lazy-by-default because it's prone to memory overuse/leaks, and memory stuff is a bitch to debug. I first used F# in a corporate project of around 250kLOC, and it was still very readable and approachable, in large part because of the tooling and conciseness. I haven't felt that ease of use in any other language.
True, Haskell doesn't execute IO actions that are not bound to main. (Except ... there are unsafe functions that allows you to.) But that doesn't mean that they are interpreted ! The haskell runtime doesn't *interpret* IO actions. There isn't one haskell runtime. You have GHC's, jhc's, etc. Also, if you look at the assembly code produced, you see that the IO actions are compiled. 
http://benchmarksgame.alioth.debian.org/u64q/performance.php?test=threadring
Is there any "Intro to Erlang for haskellers"?
I'd think disallowing function types and enforcing deepseq would go a long way. Maybe even require "show", or something analogous for this particular purpose, "storeable" perhaps, would simplify things and expand the range of reasonable implementation strategies.
Sure. I view the type system not necessarily as a secondary language, but as a specification language that is interconnected with your code. So when you write code, you naturally get a specification for your program that evolves with your code. The best part is that when you wonder what the code you wrote does, you can poke at this generated specification and see what happens. If you don't know how to code something, you can write up the specification and then ask your tools what code is missing. It's really helpful for quick prototyping *and* making it easy to maintain stuff.
Yeah, the standard way people go about defining sheaves is pretty ugly.
How does F# handle impurity? The impression I've got is that it doesn't, which would be one major difference to Haskell.
&gt; Can any editor support Haskell as nice as Emacs supports elisp? I assume an editor needs to be (partly) written in X to best support language X. Last time I checked the most advanced editor written in Haskell is Yi, and I had a look at the repo just now to find that it's actively developed. http://www.haskell.org/haskellwiki/Yi https://github.com/yi-editor/yi
&gt; Maybe I live a strange phenomena but I spend almost to no time with bugs, at all. Most of my scripty code come clean from the first time I write it and when I do have some bug I manage to track it really fast because 99% of the times Chrome just points the line where something went wrong. There are few possibilities: - You only worked (so far) on relatively simple scripts. It's different if your project has many kLOC and/or many people involved and/or deal with 'complicated things'. - You are exceptionally skilled (a second Knuth) in such case your experience might be very different then the ours. - You don't know about bugs you missed. As an anecdote I'd add that I saw priority queue implementation which had 100 % C0 coverage (i.e. every branch was executed) and in a add method a 100% C2 (i.e. every path was executed) yet it contained a bug. If a given sequence of inputs was added the invariants were violated and produced an incorrect results (the code was in library installed on most desktop Linux systems). Recent GnuTLS/Apple bugs show that bugs can live for years even in libraries a) under scrutiny and b) written by smart people - static types gives and additional edge to encode some invariants and enforce it on all imaginable inputs. &gt; Now, I do have a very defensive way of writing code which evolved naturally. Think about static typing as combination of semi-automated written preconditions and postconditions combined with symbolic execution of program
That argument makes no sense, since you could use the same argument for every change from Haskell 98 to Haskell 2010. And for every other change of every language before that. It’s not like the Haskell community can’t stomach progress… :))
Thanks, that was helpful, particularly the tooling bit. Scala's IDE story is nowhere near as Rosy as that of M$'s VS, but it's pretty good these days for vanilla Scala (for exotic syntax like Scalaz and Play's @ templates, however, fairly horrible in comparison). I really like Type Providers, impressive language feature that Scala won't be seeing anytime soon. I'm sitting on the fence with Haskell, but likely not for long, Scala's a bit of a mess after Paul Phillip's departure, and the community here is clearly vibrant -- Haskell seems to be on the rise, not so sure the same can be said of Scala. 
You didn't think that all this FP superiority was all talk and no action, did you? Eh?
Well, how about just disallowing default implementation loops (= the actual problem)? I think that would automate the whole thing, wouldn’t it? Are there cases where complex recursion like that would make sense in default implementations?
I find your `res f u ui` notation really difficult to follow— even knowing the ideas you're trying to express! If we have that `res :: (u :&lt;: v) -&gt; F v -&gt; F u` for suitable `F`, then why are you passing three arguments when describing the locality and gluing laws? There should only be two... For example, the locality condition says that given an open set `U`, and a family of open sets `{U_i}` whose union is `U`. Then `s == t :: F U` whenever for all `i` we have that `res p_i s == res p_i t :: F U_i` (where `p_i` is the unique value of type `U_i :&lt;: U`).
I'm usually much faster at coding something up working in dynamic languages such as Python or Ruby (which have have nice "batteries included" optimized libraries for common tasks). But at a certain program size, or when I start hardening the error-handling, or even just hunting bugs, I start paying the technical debt of not using a strongly typed language right from the start :-/ I think Haskell's only problem right now is its base/core library, which makes common tasks more cumbersome than they need to be (compared to Python/Ruby), even more so, if you want to do the "right thing" and use mostly `Text`/`ByteString` instead of the inefficient (and sometimes semantically wrong) `[Char]` abomination that is used all over the place in the standard libraries :-(
Was the post automatically generated by IPython? It looks nice; care to explain how?
In hindsight, it probably wasn't the greatest notation, but I wanted something I could explain the real definition with on Reddit without just linking the massive wikipedia page. (I noted the arity discrepancy because I'm not sure about what should be a value and what should be a type, because a description of the cluster's topology would be a useful value to have, but it is also a useful type. Would singleton types be useful here?)
It makes sense because the feature *does not actually change the semantics of a running program*, which is not the case with most changes, although it *is* the case with INLINE and UNPACK which was mentioned in another message in this thread, and which you will note are done as pragmas in the same way.
Yes, there are cases where mutual un-overridden defaults make sense. I was earlier going to point at the `some` and `many` methods of `Alternative`, although I looked at the code first and it turns out their defaults are not *actually* defined in terms of each other, presumably for better sharing. But they could have been, by simply using their laws as definition: some v = (:) &lt;$&gt; v &lt;*&gt; many v many v = some v &lt;|&gt; pure [] 
Very cool. I would be interested in more. I have been recently working on a pipes-distributed library which works on top of Cloud Haskell. I just posted to the haskell-pipes google group a little while ago: https://groups.google.com/d/msg/haskell-pipes/_1lX6zxsY2M/u1EhAV612wsJ &gt; In any case, this means that if you can distribute a computation over a workgroup, you can run it on a smaller workgroup, and that subtasking composes in a sensible manner. This seem like what I was trying to describe in my response to Gabriel: https://groups.google.com/d/msg/haskell-pipes/_1lX6zxsY2M/6gCoqCNzx8IJ You can have a following which represents a computation over a cluster: reach = (mconat $ replicate 3 getTweeters) &gt;+&gt; (shuffleGrouping . mconat $ replicate 12 getFollowers) \ &gt;+&gt; (feildGrouping followerLens . mconcat $ replicate 3 partialUnique) &gt;+&gt; (shuffleGrouping $ countAggregator &lt;&gt; countAggregator) Which you can visualize as: http://i.imgur.com/SVqNGKR.png can also be expressed on a single core as: getTweeters &gt;-&gt; getFollowers &gt;-&gt; partialUnique &gt;-&gt; countAggregator It would just take longer because nothing is happen in parallel. &gt; Sheaves seem like a solid conceptual basis for distributed concurrency, and might be a good choice for a future, distributed version of pipes/conduit/machines. I have not absorbed the implications of the other laws yet. I would definitely be interested in knowing more seeing more work formalizing this area. I find out where you want to go with this so if you have the time please contact me at: Patrick&lt;&gt;John&lt;&gt;Wheeler@gmail&lt;&gt;com with `&lt;&gt;` -&gt; ``.`
As a side note, you might want to consider applying the advice given here in the imperative languages you use. For instance, the if ... then count = count + 1 approach is not ideal, regardless of whether your language supports it.
At this rate, I predict it's only a matter of time before someone manages to find a use for topoi in Haskell. Unless they already did.
&gt; Wild Tangela appeared! &gt; Tangela attacked you with Rest dealing 0 damage! &gt; You have 200 hp! &gt; Tangela attacked you with Rest dealing 0 damage! &gt; You have 200 hp! &gt; Tangela attacked you with Rest dealing 0 damage! &gt; You have 200 hp! Ahh, just the relaxing Sunday afternoon brawl I'd hoped for. Anyway, here are some links that might help: [http-client](http://hackage.haskell.org/package/http-streams-0.3.1.0/docs/Network-Http-Client.html), [MonadRandom answer on StackOverflow](http://stackoverflow.com/questions/9266765/how-to-use-monadrandom).
The FFI is really easy overall. I started working on a (still unfinished) blog post about it, and have a presentation that covers the basics as well as some of the really really hairy details if you're interested. http://www.rebeccaskinner.net/blog/2013/09/07/understanding-the-haskell-ffi/
If there's any type system around in the web world that you'd do well to worry about, it's Javascript's (or Coffeescript's for that matter). Here's a fun experiment: take a large production Javascript codebase and update some dependencies, or move a function elsewhere, or shuffle its parameters around. You get no errors. Now put it back in production. Try not to 'worry'.
This is actually a pretty neat problem and I'm sad to see you downvoted. I suspect you get downvoted because it's a "write this program for me" kind of question, but I think it's a valid question despite that. An implementation of this program in Haskell would be very ~~teaching?... teachful?... taughtful?...~~ useful for learning idioms in the language. I'd like to give it a shot if I get time. I fiddled around a little and arrived at pretty much the same point that /u/Tyr42 is at now, but I have lots of schoolwork to do.
Well color me surprised. Can someone explain, in not too technical terms, why is this the case. If I remember, Haskell was not built with high speed as a goal, but go was (and Erlang). Am wrong, and speed was actually a goal of Haskell or is this high speed just a side effect of some other aspect of Haskell?
I have a related notion of locality (albeit a bit more kludgy) in my pending numerical array Apis. One thing I don't know if classical geometry addresses is what happens when your geometric structure is discrete. Doesn't the turn into convex geometry? I'm actually spending a bit of time teaching myself convex geometry, and it's very very nce language for a lot of discrete arrangement stuff. I'm actually start to explore how to mod "cloud Haskell" to admit better reasonig about locality and paths. Hope to have something to share after a few more weekends of hacking. Edit: I'm told that the works of Maurice herlihy such as http://www.amazon.com/gp/aw/d/0124045782/ref=redir_mdp_mobile?adid=11Q692XVB3PRZGKNS6PE&amp;camp=14573&amp;creative=327641&amp;creativeASIN=0124045782&amp;linkCode=as1&amp;ref-refURL=http%3A%2F%2Fcs.brown.edu%2F~mph%2F&amp;tag=mauriherli-20 cover this whole perspective in some depth! His academic page is http://cs.brown.edu/~mph/
erlang was designed with reliability rather than high speed as main objective.
There is Learn You Some Erlang for Great Good! which is a great introduction following the same style, but it doesn't compare the languages.
Yeah, I didn't mean it as a condescending remark. I was genuinely curious, because if Haskell and F# differ in that area, it's a pretty big difference that probably shouldn't go unmentioned.
Yes, we use a bit of Fay in production now at Silk. Small blogpost about it coming soon.
I think the mistake you made is that composition of subset relationships is not a property of `res`. It follows solely from the fact that `(:&lt;)` represents a poset, and that subset relationships compose: r1 :: u :&lt; v r2 :: v :&lt; w r3 :: w :&lt; x id :: a :&lt; a (.) :: (b :&lt; c) -&gt; (a :&lt; b) -&gt; (a :&lt; c) (r3 . r2) . r1 = r3 . (r2 . r1) id . r = r r . id = r Saying that you can compose the functor `res` with itself is not well typed, since its codomain does not necessarily match its domain (i.e. the codomain is not necessarily a poset). Also, note that even if `res` is a functor from `(:&lt;)` to an arbitrary category, you get the following functor laws: res (r1 . r2) = res r1 . res r2 res id = id
I'd say forget about “for haskellers” part. As a language Erlang is exceptionally simple (esp after haskell). You will fibonacci like a pro rabbit after 1 hour introduction. The only really new and important stuff is OTP and all their “let it fail” philosophies. It's mostly what distributed-haskell does in its own way. 
Yes that's why I just used a random one. Great work, thanks! Can't wait to see it finished. I'm glad to see some things are much easier than I thought (random numbers, http request) while others are really tricky (You **have** to formularize the JSON structures you are going to access? What if they don't have a fixed form?)
I wouldn't be able to explain why, other than telling you GHC has a very efficient lightweight thread implementation. You can read more here http://stackoverflow.com/questions/5847642/haskell-lightweight-threads-overhead-and-use-on-multicores edit: and there's a small section about lightweight threads here http://www.aosabook.org/en/ghc.html 
Well, in fairness, it's a legitimate use case. Some jobs *can* be accurately described as "driving when nobody else is on the road," their past selves included. And so quick 'n dirty scripts can be appropriate for accomplishing those kinds of tasks. Perl is notoriously for being a "write once, read never" language, but that's actually fine for certain tasks. However, most software development jobs involve code that is read much more than it is written. Automatically enforced contracts (such as types, regression tests, etc) make such code bases easier to work with. The unfortunate reality, however, is that startup-y projects often treat the latter scenario as though it were the former. They just want to crank out something that works, and they accrue huge amounts of technical debt along the way.
You don't have to make custom types for the JSON structures you fiddle with. There are at least two kinds of "dynamically typed" JSON structures available in the same library – one of which is pretty similar to the one you have in JS. It's just that when people do Haskell for a while, they start to become more and more dependant on describing their program with the types they use.
Haskell's language model just turned out to be perfect for a lot of optimizations and powerful memory models. If the programmer needs to make pure code, the compiler can mess with the memory as much as it wants. The encapsulation of IO also makes thread management super easy. That reminds me of the story of STM. I think it was SPJ who got the idea to put it in GHC. They learned about attempts to implement it for C which were ongoing for over two years (and later abandoned), but in Haskell it took a grad student only a weekend and a bit to get it working.
FYI this is an intro to programming course. My general thoughts on it when I did it in 2010 as part of my degree was that it felt very very slow.
Definitely! And while it gives you more leeway to shoot yourself in the foot, free impurity is quite useful when doing prototyping, unit testing, and implementation of certain things like hash functions. The idiom for handling state in F# is the actor model, in which local mutable state can be useful depending on your coding style. F#'s lack of typeclasses is also a big difference from Haskell. F# is less oriented towards beautiful mathematical solutions than Haskell, though still more than most languages. It's a good language for when your primary concern is shipping a product.
For a "Let me just do something quickly…" problem I would recommend Python or whatever language you feel familiar with. It probably will be faster. For a "this is a bit more tricky, but it should be done right" Haskell is a very good choice - and probably faster created then a good solution in an other language. 
Beware headphone users, Phil has a habit of starting to shout at random moments.
Types really start to pay off when you want to refactor a function or change the fields in a data structure because the compiler points out all the places that you will need to update. In Javascript this is a bit tricky because you need to make sure that your runtime tests have 100% coverage (something that is non trivial, specially if you consider that coverage depends on user input and UI-stuff is harder to automatically test). Additionally, not all bugs will reflect as an exception being thrown. For example if you get rid of or rename a boolean field any code you forget to refactor will silently treat the field as false instead of noticing that its undefined.
&gt; They just want to crank out something that works, and they accrue huge amounts of technical debt along the way. And then they get massive funding on the the promise of scaling their awesome product, and use some of it to dash hundreds to thousands of programmers against the brick wall of debugging their quick hack...
FYI, it looks like (as of recently), nbviewer supports non-Python languages! So you can also post your notebooks to nbviewer if you don't want to bother with conversions: [NBViewer notebook](http://nbviewer.ipython.org/github/rampion/CopyPastePuzzle/blob/master/CopyPastePuzzle.ipynb) Not everything is exported perfectly, though (errors, etc, things requiring custom IHaskell CSS)
also sometimes break out as [Lambda Man](http://bodil.org/more-than-functions/m/lambda-man.jpg)
&gt; "write once, read never" language, but that's actually fine for certain tasks. You mean, like, a script for [fetching git submodules](http://git.haskell.org/ghc.git/blob/HEAD:/sync-all)? I think the problem here is deciding when you really never want to modify this again upfront. 
That clearly calls for downloading the videos and using VLC to watch at double speed!
Pretty easy. There's a good FFI or you can do what I did and use pipes to other processes with light IPC.
.NET and C#/F# are actually really impressive and I say this as someone who despises Windows 
From a different perspective than the other responders, I have used Haskell for one-off scripts. The most recent example is when I wrote a simplistic monte-carlo simulator for figuring out how often one would be mana-screwed with a 20-40 land/non-land split in MtG. (that is, if you're not familiar with the terms, if you have a deck of 60 cards, simulate the chances of failing to draw some number of a distinguished 20 within your first several draws; in my case, it was 2 within your first 10). The types helped me more as documentation and they may have ruled out some of the trickier cases to debug (where I get something that looks like it's of the right form but is the wrong answer), but mostly, I was just more comfortable with the language and its libraries than with anything else.
Note that [that implementation](http://benchmarksgame.alioth.debian.org/u64q/program.php?test=threadring&amp;lang=ghc&amp;id=1) is doing some interesting things. In particular, `forkOnIO` binds all the Haskell threads to a specific OS thread, so there is no inter-CPU thread communication going on. Changing that slows it down considerably -- from about 4 seconds to about 90 on my computer. Of course, a real multithreaded haskell program wouldn't consist entirely of MVar operations either. But it is worth noting both that if you have threads that spend a lot of time communicating with each other, you might want to ensure that they don't end up on separate OS threads, and that GHC (uniquely in my experience of languages with green-thread based runtimes) gives you the tools to do that.
I am not positive that it was only structured haskell modes fault, but I think it has locked up my emacs for a few times. But overall I really like it alot, together with flymake mode and an interactive session you never need to leave emacs.
Could you please elaborate on those "dynamically typed" JSON structures? I have also used Data.Aeson, but I have implemented by own "dynamic" version of `.:` because I didn't know that there were already two. [Here](https://gist.github.com/gelisam/9451664) is my implementation of the entire program, and [here](https://gist.github.com/gelisam/9451664#file-main-hs-L47) is my "dynamic" version of `.:`.
Nitpick: this isn't *quite* the whole story since changing a type annotation can change the meaning of the underlying program, so you can't just view the type system as sitting on top. (This is like Church-style versus Curry-style semantics.) But "programming in two languages simultaneously" isn't such a bad intuition.
I finally managed to get something done! [Here's a gist with the program.](https://gist.github.com/kqr/9452455) Noteworthy observations: * **The import list is huge** compared to your little snippet. This is probably due to the fact that large portions of "Haskell" is actually implemented in the standard library. Where other languages create new language features, Haskell developers create new libraries. * I have chosen to create **custom data types** and rules for converting JSON data to those objects. I could have gone a softer, more dynamic route like you have in your JavaScript, but this data-focused approach is better in the long run because it is much easier to ensure compatibility when you are passing around custom types. * **Hitting the API** and getting an object out was in the end much easier than I thought it would be – just three lines! * This particular code has a couple of places where the **error checking** is... lacking, to say the least. (Lines 86 and 118 are particularly terrible.) Don't get me wrong; your code barely has any error checking either, which is why I decided to keep it that way. In a real application, I would be much more careful. * One of the things that I expected to be really troublesome was the bit where you have to first get a pokemon but you can't give it any moves yet, since you have to **hit the API a second time** to know enough about the moves. It turns out it was pretty elegant to store download functions instead of the moves, and then run the pokemon through a downloader that actually downloads the moves when necessary. I have a feeling there's some even neater way of doing it, but I don't know quite what it would be. 
The preferred way is to decode the JSON into a `Value` and then traverse that using normal Haskell libraries. `Value`s are pure "dynamically typed" JSON structures but using Haskell data types. It looks like you're pretty much already doing that, though, so I'm not sure I can help you further.
That is awesome. 1. That sounds fair and healthy to me. 2. I feel that understandable and perfectly OK as you only had to declare data for the parts of the JSON you actually needed. 3. Nice! 4. I don't mind that at all, as you can see from my code. 5. Uh-huh, I've noticed that in you MoveList type. That's interesting, but I too have that feeling. I'm really content with what I saw. I'm mostly happy because, excluding some operators `&lt;$&gt; &gt;&gt;= &lt;*&gt; etc`, I can understand most of it and what each thing does, which doesn't happen so often when reading arbitrary Haskell code. Being able to map those Haskell concepts to my program was as enlightening as I thought it would be. I wish I could give you more than just a thank, but I'm just a student so a bitcoin tip from me would probably sound rude, I guess?
Yes.
I use eclipsefp on Linux. It's by far the best Haskell IDE I've encountered. I found it easy to setup. It integrates cabal, hoogle, autocompilation, syntax highlighting etc. I find it sad that more people don't use it. Emacs is great if you've already spent many years mastering it. 
I am significantly more productive in Haskell in just about every stage/style of development, including rapid prototyping for the same reasons others have already described here. Hands down. The only exception *may* be when the problem domain benefits greatly from a large ecosystem of libraries and Haskell's does not yet have all that is needed. An example is general statistical analysis and machine learning: If all you're looking to do is try 10 different models on for size on a dataset inside of a regular morning, you may not be able to get there as "productively" in Haskell as in R or Python.
This comment begs a question. If most of the machine learning state of art is on Python/R, can I safely assume that their solutions today are far from optimal, in a performance sense? After all, other than being compatible with Pypy, I guess, most of Python's programs run orders of magnitude slower than most static language's equivalent.
Think about it this way: if you have ever written a library you'll know that the smaller and more opaque your API the more freedom you have to internally rearrange things without breaking downstream code. Haskell is like a library that has a very limited and carefully restricted API that gives it a LOT of freedom to optimize things internally.
To be honest you have a huge problem there, then. I don't know what I would do in your case. The way FP came to me was naturally: it evolved over 10 years of programming in other languages, selecting the things that made sense and slowly noticing that the concepts on FP were the right way to do most of the things. I couldn't imagine myself making the migration in that amount of time. But to try answering one of your questions. Before anything, mark this in your head: *there is nothing in Haskell other than creating functions and calling them*. This is like so for reasons that you shouldn't have to care about now, but trust me for now that this is a very good thing. The problem is that if programmers are limited to that, there is not much they can do, right? Wrong, they can still do everything you can do in, say, C. The only problem is that it would be unreadable. For example, try understanding that program: ((((λx -&gt; (λy -&gt; x*y)) 7) 5) This is a perfectly valid program in the "natural computation" form: it only has one-argument functions and function calls. But is also an ugly, unreadable mess. What does it even do? Here comes Haskell: it is pretty much a language with lots of syntax sugars that allow humans to program in a way they understand, while at the same time enabling the computer to easily convert the code into that function-only form! So, for example: z = let x = 5, y = 7 in x * y and z = x * y where x = 5 y = 7 Both are equivalent to the ugly example I gave you, and are converted by the compiler to it! But now you can easily understand what they do. If not, it is the same as the following python program: x = 5 y = 7 z = x*y Oh and the only difference between **where** and **let**, as you see, is the order. Just use what makes your code look better. They're the same.
C# does have static analysis stuff for nulls. It's obviously not better than having option types, but it's better than nothing.
Thanks, I must have something set up wrong. Are you using cabal sandboxes?
Sweet. This is my first use of IPython/IHaskell so I wasn't sure how to share stuff.
Go was designed for high *compilation* performance, see [the first point](http://golang.org/doc/faq#What_is_the_purpose_of_the_project) : "It is possible to compile a large Go program in a few seconds on a single computer."
I'm thinking of trying to play with Data.Record.TH to see if I can make a generic magical json that's pretty.
It is possible to accomplish so much in libraries, as opposed to in the language specification. Making a language that allows for that, yet isn't a dynamic mess like Lisp, kind of automatically makes it very well designed.
Something interesting I realised with this exercise is that, as pointed here, I did have a bug that passed unnoticed on my JS-solution: the script could chose a Pokémon with number 0, which does not exist. But in a second thought, I'm not sure Haskell typesystem's would help much in that case, would it? 
Hmmm, what you've got there looks pretty interesting. I've thought about it a bit more, and had an idea for a very simple core API for distributed pipes. 95% of `pipes` is built around composition laws and two simple operations: `await` and `yield`. In a similar way, you might be able to build distributed pipes on top of different laws and two operations with the approximate type signatures -- the node doing these operations must be a part of the open set -- it should be possible to enforce this invariant with types -- recall open sets are groups of similar processes push :: Serializable a =&gt; a -&gt; OpenSet -&gt; ChannelID -&gt; Producer m a () pull :: Serializable a =&gt; OpenSet -&gt; ChannelID -&gt; Consumer (NodeID, a) m a `push` would send a value to all the other nodes in an open set, and `pull` would receive a value from every other node in an open set. In your example, the open sets would look like {GetTweeters1, GetFollowers1, GetFollowers2, ..., GetFollowersN} {GetTweeters2, GetFollowers1, GetFollowers2, ..., GetFollowersN} {GetTweeters3, GetFollowers1, GetFollowers2, ..., GetFollowersN} ... {PartialUniquer1, CountAggregator1, CountAggregator2} {PartialUniquer2, CountAggregator1, CountAggregator2} You can visualize some of these sets like this: http://i.imgur.com/bY5NuBk.png What's really nice about this is that if you intersect the sets containing the GetFollowers nodes, you have a new open set, whose nodes can communicate with every other member -- so the GetFollowers are their own workgroup. But you can easily make finer-grained communication possible as well. For example, if each GetFollower node only needs to talk to its neighbor, you can add some more sets to the cluster's description that communicates that fact. Failure works very nicely too. If a node fails, then we can `push` a notification to the smallest "special"[1] open set containing the failing node that can sensibly deal with the error. These nodes can then take the appropriate action, a la Erlang. [1] The user will specify certain certain open sets on their own, and the others can be generated from these via repeated union and intersection. Generally, these open sets should contain spatially close nodes. Such user-specified elements are elements of what is called a *subbasis* in topology, and they make up the fundamental components of the program's logic, so they should have special error handling privileges.
It seems like a very strange practice to have your webapp framework do websockets stuff, instead of just splitting that out, but I guess it's easier for some people to think about that way or something?
W.r.t. the resume builder, take note that Pandoc is written in Haskell and is definitely a great example of a PDF generation system, though it relies on LaTeX. A similar system exists in the HaTeX package.
As an intrepid Haskeller, I really enjoyed a number of the examples where he rewrote simple programs a number of times to make them simpler/more concise and illustrate various styles/features. If you've never seen Haskell before, that might have been hard to follow (although I only saw the slides, not the actual talk), but for someone with some background already, it was very enlightening. Would love to see more of that.
It can help, if you ask it to. First you create a new data type that wraps a normal `Int`. newtype PokemonNumber = PokemonNumber Int Then you create a function to construct such a number. mkPokemonNumber :: Int -&gt; PokemonNumber mkPokemonNumber i = if i &gt; 0 &amp;&amp; i &lt; 152 then PokemonNumber i else error "Pokemon Number out of range!" then you make sure that you export this constructor function instead of the automatically generated constructor. This will make it impossible to create `PokemonNumber`s that are out of range. Securing your types this way is common practise in Ada, so much so that it got it's own syntax. In Ada, it's extremely common (you basically don't do it any other way) to come across code like type Pokemon_Number is Integer range 1..151; but it's not as common to see that kind of thing in Haskell. Maybe because it isn't built into the language and nobody has created a widely used library for it, so it's a little cumbersome to do. In Ada, you get arithmetic and built-in runtime checks for every operation when doing that.
I've been programming for 5 years and found the course lectures and associated labs a wonderful resource for learning pure functional programming.
On the word "variable", I recommend the 3rd and 4th paragraphs of [a post by Robert Harper](http://existentialtype.wordpress.com/2013/07/22/there-is-such-a-thing-as-a-declarative-language/) that happens to address the meaning of variable you've given. Main point of note: &gt; The declarative concept of a variable is the *mathematical* concept of an *unknown* that is given meaning by substitution. The imperative concept of a variable, arising from low-level machine models, is instead given meaning by assignment (mutation), and, by a kind of a notational pun, allowed to appear in expressions in a way that resembles that of a proper variable. But the concepts are so fundamentally different, that I argue in [\[*Practical Foundations for Programming Languages*\]](http://www.cambridge.org/us/academic/subjects/computer-science/programming-languages-and-applied-logic/practical-foundations-programming-languages) that the imperative concept be called an “assignable”, which is more descriptive, rather than “variable”, whose privileged status should be emphasized, not obscured. 
Presumably your web app framework includes a HTTP server. And Websockets are upgraded HTTP connections. Why does including them in a webapp framework seem like a very strange practice?
These look really good. Perhaps a bit ambitious near the end, but with the right words to accompany them, I think these slides would work well. :) Also, any Haskellers interested in server type applications should really look into Erlang/OTP for some inspiration (and then go build something with distributed-process).
&gt; `type Pokemon_Number is Integer range 1..151;` it's `is Integer range 1..719;` nowadays 
At first I thought it was a remark on the Ada syntax which I'm totally rusty on, and I couldn't for the life of me figure out the difference. And man, last I played they were 250, and I had some trouble remembering them all!
&gt; Oh and the only difference between where and let, as you see, is the order. Just use what makes your code look better. They're the same. Scope can be different, especially when using `let` in a `do` block.
the 2009 version of this course [Informatics 1: Functional Programming](http://wadler.blogspot.com/2013/03/informatics-1-functional-programming.html) &gt;Videos of my 2009 lectures on functional programming in Haskell are available online from the University of Edinburgh. There seems to be a cottage industry in videos explaining monads, see Lectures 18 and 19 (and 16 and 17) for my version, or see How to Declare an Imperative and Monads for Functional Programming if you prefer text to video.
Pointed is actually not needed if I am thinking correctly. That for a presheaf F, every open set U gives rise to a set F U is part of the definition of a functor! And a presheaf is nothing less than nor more than a set-valued contravariant functor. A confusion of levels—adding pointed would mean that every point in U may be lifted to a point in F U.
In contrast I found the course to be at just the right speed, though I did take it a year before yacoby. As a tutor for the course now I also find that the majority of my students find it just about right. Yacoby is just a bit of a smart bugger ;)
I don't think that's a safe assumption at all. For many languages, either Emacs or some Java-based IDE has the best set of IDE-like functionality. Although Yi is written in Haskell, I'm not sure it's any better in its current state for supporting Haskell development than Emacs or vim with associated Haskell support packages.
I think you might be looking for [fpnla](http://hackage.haskell.org/package/fpnla) (with [examples](http://hackage.haskell.org/package/fpnla-examples)). Never used it myself so can't vouch for quality, but it seems to be solving your problem; was posted to the mailing lists a few weeks ago.
Sometimes. I think the biggest problem is the slow compilation on my large, template-haskell-heavy project. Failing to grok a type error can stall me occasionally, but having the type system available speeds me up in a few ways as well - I'm not sure what the net is there before we consider correctness. Monads in particular are almost entirely a win, as I'm able to talk *about* actions I wish taken (be they IO actions or otherwise).
I bloody love Haskell, but I would honestly say that it depends on the task. There are fantastic libraries in many programming languages for most anything I'd ever want to do, so whichever language has the appropriate library would be the fastest solution. Most work should be glue code. For example, I prefer Haskell for writing libraries, Ruby for writing shell scripts, Node.js for single service sites.
That seems to be exactly what I was looking for, thank you! It looks like most of the Accelerate functions aren't implemented yet but at least hmatrix and repa are. Thanks again!
I use cabal-dev because current version of eclipsefp doesn't support cabal sandboxes afaik.
This is very much the case for me also - i need some specific examples of what is being abstracted *from*, i.e. what is being generalised, and what restrictions are being relaxed in order to make such generalisations. i'm also a big fan of counterexamples, which help me find the boundaries of the abstractions in question. 
I'll try that, thanks.
I'm releasing something thats a step towards an (opinionated) vision of this later this month. Note that you actually don't want quite what you're descrbiing, because theres some deep implications for supporting both deep and shallow embeddings nicely. all will be made clear in a few days 
You actually get that by keeping some pet examples of common structures (sets of numbers with the usual operations are usually a great place to start) and trying to come up with other examples that satisfy a given definition. A good textbook will lead you to doing this and give you further examples to consider. Algebraic (abstract) structures are ways of looking at the various examples such that they all look the same, and the definitions should capture that same-ness.
Performance is still an issue. For example, a recursive function that isn't optimized for tail calls is not only slower, it can cause a stack overflow. I was listening to the haskellcast with Brian O'Sullivan, and he was discussing how the use of strictness in certain places was necessary to get good performance out of his code. Getting good performance out of Haskell can be tricky, just in different ways than C.
Didn't you guys write a blog post a while back about how ember was doing some truely nasty things behind what looked like a pure function? That's really great to hear that you're getting some mileage out of an end to end Haskell solution! How do find sharing types between Haste and the server side (you guys are using snap right?). We're using Text quite heavily in our data types and the the database layer.. &gt; No template haskell - have fun defining lenses by hand! We have some pretty Chuck Norris esqu Haskellers: They sit down at their text editors and the lenses just define themselves out of fear! 