I won't say that I was completely turned away from the community but I completely agree with your that the thread was shockingly toxic, and it is amazing me that this thread is so different.
a lot of thinks going to break
&gt;With respect to allegations of absurd logic, sarcasm, and trolling They are not allegations. You said you could not believe I can disagree with your logic. I am simply telling you, I do in fact disagree with your logic. &gt;Your posting history on the other hand is full of slurs like "clamhurt legbeards" Go read SRS and get some context. It is a very deliberately chosen "slur" intended as a sarcastic counter to SRS's favourite "slur". &gt;such gems as calling drinks spiked with date-rape drugs a "myth" So, I'm a troll because you don't like reality? Date rape is a huge problem, and alcohol is the only date rape drug in use. Convincing women that they are safe as long as they only consume alcohol is very much the opposite of helpful.
still thinking about it, you might be right about just consuming 7gb. but i'm not yet ready to take that pill. there might be a few of these files. i'm pretty sure to only need part of the data for one request (request as in http request, i.e. many requests to the mapped file). and i'd rather like to express the algorithm as if it were all in memory, which i can in c (but i usually don't want to express anything complicated in c).
I suppose it's what you asked about. It wasn't clear to me exactly what portion of the design space you'd mapped out. Maybe some hybrid solution is OK. Only build the "directory" lazily (paying the cost of all evaluated bits sticking around), and use ByteStrings/ForeignPtrs to point to the payload. Alternately, it may turn out (I have no idea) that your directory can be represented highly efficiently in some compressed format. Trie-like structures are often very amenable to run-length encoding and other lightweight schemes. Hmm... here's another thing you can do with the directory. Have a call in IO every time you want to use it, which yields a "new" lazy-IO backed structure. That's a pretty safe/blunt way to prevent sharing/force throwing away evaluated thunks, etc.
&gt; Yes they are. If he were simply discussing the comment, he would have. But he repeatedly refers to the person who made it. He is deliberately associating that person with the imaginary bad things he is pretending his comment creates. Not only that, but he makes accusations about the character of attendees even: "I expected more from the people who attend the Haskell Symposium" So it is not okay to castigate the people who go to a symposium when they participate in sexist behavior? Or is laughing at a joke because it is sexist (as say, Simon Marlow *says he did*) definitely not sexist? I don't think anyone who laughed intended to cause harm. I don't think Swierstra even meant to make a joke! That is not what this discusion is about. Tim is *not* engaging in character assassination. He is not attacking Swierstra. Several paragraphs in the original article make this clear, and he has since attempted to clarify that point repedetly. Your argument boils down to "its not that you intend to attack someone, it is what the effect of your actions are that counts." But, elsewhere you advance the position that we should evaluate the original comments based on there intent. Regardless, Swierstra is a great academic who was talking about his desire to improve participation. He is not a sexist. Everyone reading these discussions should take that away. You are not the effect of your words. Saying something that people interpret wrong does not make you a bad person. &gt;His gender identity is male is it not? What is complicated about that? Are you suggesting that he is not a "real" man so his opinion doesn't count as a male opinion? And then by extension the other men making patronizing and dismissive comments to defend the non-existent offended women are also not hypocrites because they share the same opinion as someone who is magically immune from hypocrisy? I was in artfull. Describing his gender identity as "complicated" was problematic, and I should not have done it. Tim has spent much of his life with other people perceiving him as a women. He does have experiences that most men don't. Without doubt some of the other commenters said some inappropriate things and should not be immune from criticism (not all of them attacking Swierstra, like "its funny because it is true"). &gt; Yes, that is totally inappropriate if what was actually said was "lets get more women here so the conferences are more attractive to everyone". You can feel however you like, but if you are choosing to take offence to an innocent statement, and attack someone who is trying to do the very thing you want, you can't seriously expect nobody to criticize you for it. Multiple people were hurt by the stament/audience response/other statements at the event. Read the first reddit discussion. We have a responsibility not to exclude people or produce a hostile environment--full stop. I doubt Tim expected "nobody to criticize him for it" but the more I think about it the more convinced I am those criticisms are *wrong*. &gt;Unfortunately, that doesn't seem possible. Tim will be offended no matter what anyone says, and having influential haskell community members act as they did in response sure as hell scared my wife away. I don't think "Tim will be offended no matter what anyone says" but he was not the only person to react negatively. I am sorry that your wife was scared away. In the same way people laughing at what they interpreted to be a sexist joke should not define the Haskell community, neither should that reddit thread.
The person was trying to do good, but in the process did something which may have had the opposite effect. Trying to help people trying to do good to not make the same mistake in the future is not denying that everyone had good intent. People need to know that talking about gender disparity in technical fields, even when your intentions are good, must be done very carefully. see: http://en.wikipedia.org/wiki/Stereotype_threat for a particularly well documented (via controlled psychological experiments) for a phenomenon that makes doing this right hard. 
Okay, let's say for concreteness' sake that you're doing an in-order traversal of a large balanced binary tree, to sum some function of the leaves or whatever. Ideally you want to do this using an ordinary strict fold that doesn't know about your funny lazily-constructed tree. So you're going to pass the result of the lazy parser to this fold. Let's imagine what will happen when the fold has just finished consuming the entire left subtree of the root. The fold needs to still be retaining the root node of the tree, since it still has to consume the nodes in the right subtree. But at this point the entire left subtree has been expanded into a tree of computed values, and all of these values are still reachable from the root. So the GC cannot collect anything (it has no way to know you will actually never access that left subtree again) and your maximum heap usage will be half of the tree stored as Haskell values simultaneously, which is very bad. But imagine instead if when the fold enters a subtree, it duplicates the thunk representing the subtree, leaves the parent node pointing at the old thunk but evaluates the new thunk. (Equivalently, imagine that the thunks comprising the tree are not replaced by their values after being evaluated.) Then the Haskell values produced while expanding the tree will *not be reachable* from their ancestors as seen by the GC, so at any given time only the nodes which are "on the stack" will need to be retained. Now the Haskell heap usage is logarithmic as it should be. The real cost of duplicating the thunk before computing it is that if you ever want to get the value of the thunk again, you will have to recompute it. That never happens in the in-order traversal case. For other access patterns, it might happen. But still, in this general situation you would presumably be willing to give up a constant factor in runtime to achieve logarithmic space usage.
This was never about defending or supporting Doaitse Swierstra or not. Tim didn't "attack" him, for one, but wrote about an unfortunate turn of events. Lots of highly voted comments on that thread said that what happened could have had to do with language issues, misinterpretation, etc. And Doaitse has shown here that he didn't need people jumping up to "save" him. Instead, he graciously acknowledged that his remarks "gave ample opportunity for misinterpretation", clarified where he _does_ stand, and what he _does_ believe, and did so in a respectful way. People on the other thread didn't want to tar and feather him. At most, in fact, they expressed hope for a response _exactly_ along the lines here today. I was very happy to read Doaitse's response, and I'm now just disappointed that this thread itself is going the way of the last one, which isn't at all helpful.
Pinker's grand theory is probably inconsistent with information theory (only a few megabytes of genetic material can't explain everything). Regardless, enormous evidence suggests that gender performance has more to do with society: female math performance strongly correlates with other measures of gender equity across societies. 
Hopefully not since it's a bug-fix release!
I suppose you could just run `cabal sdist` and then `pgp --detach-sign dist/*.tar.gz` and pop the resulting .sig file in the .tar.gz. I think it should be kept by Hackage, then to verify it as a user just run `cabal unpack`, `tar zcf package-unsigned.tar.gz package-123` and then `pgp --verify package-123.tar.gz.sig package-unsigned.tar.gz`. Someone could make a util for it, like `cabal-sign` and `cabal-sign verify`. Or you could wait a few years to get it in Hackage.
&gt;I find the assertion that alcohol is the only date rape drug laughable, give that any central nervous system depressant will have a similar effect as consuming alcohol Nobody suggested that no other drug is capable of short term memory loss. I suggested alcohol is the one used for date rape, contrary to the common myth that people are putting something else into alcohol to make it have the effect that it already has. &gt;No one is saying this Yes, the people I responded to were saying it. If you are going to try to argue with old posts, you can't try to insert yourself into the conversation and pretend I was replying to you.
exactly. I find this thread disappointing in comparison to the first one, to quote FluffySauce on that thread &gt;Yes, same here. As a woman in IT, I hate the fact that I can hardly even read these kinds of articles or comment threads because the vitriol I tend to see in response is so discouraging. It seems that much like it is in many other ways, Haskell and its community are setting themselves apart here. And that makes me happy :) if only this discussion were as non vitriolic
&gt;The person was trying to do good, but in the process did something which may have had the opposite effect. Except that it didn't have the opposite effect. The people being offended on behalf of poor defenceless women had that effect. &gt;Trying to help people trying to do good to not make the same mistake in the future I do not believe he was trying to help at all. His tone was not helpful, his accusations were not helpful, and his responses to the very reasonable correction of his assumptions were not helpful.
thank you. reading the paper i came to the same but still a bit dusty understanding. your comment made it crystal-clear.
Yeah, unless it is used in a way that does not grow the free space bookkeeping structures. If the data does not have sharing and is a simple tree, it could be built in one step and the free space structure would remain tiny. I hope I find the time to replace it with something better...
I put my mouth up to a type signature, but all I got was a smudged screen. I don't feel any more secure (actually I feel a little violated).
&gt;So it is not okay to castigate the people who go to a symposium when they participate in sexist behavior It is not ok to make up absurd "offences" that don't exist, and then pretend you are defending women, when women didn't take offence to begin with. &gt;Tim is not engaging in character assassination. He is not attacking Swierstra. Yes, he did. &gt;Several paragraphs in the original article make this clear and he has since attempted to clarify that point repedetly No, one afterthought attempted to make that clear. But once again, saying "this person is evil" 5 times and then going "but I am not saying this person is evil" doesn't actually negate what was said. &gt;You are not the effect of your words The effect of your words reflects on you. I do not know what alternate reality you inhabit, but in this one people are judged by what they say, and by what people falsely accuse them of saying and doing. &gt;Multiple people were hurt by the stament/audience response/other statements at the event. Read the first reddit discussion. I did read it. I didn't see multiple hurt people, I saw multiple people defending theoretical hurt people who didn't actually exist. &gt;I don't think "Tim will be offended no matter what anyone says" He has made it very clear that he will take any statement, no matter how innocent, and deliberately distort it so he can use it as part of his gender warfare agenda. There is literally no way to have a conference without upsetting someone like that. They will always make up a reason to be offended. &gt;In the same way people laughing at what they interpreted to be a sexist joke should not define the Haskell community, neither should that reddit thread. Unfortunately, it is hard to make that case when you have high profile members of the community literally saying "your experience means nothing because you don't live under my made up horrible version of women's lives". It is much harder to disregard an outright attack than it is to disregard people laughing.
&gt;we attract few women because we attract few women We attract few women because jobs in CS, mathematics and the physical sciences *suck*. This is a very unpopular opinion to have in these communities because everyone wants to think they've made the right choices in life, and they are so very smart they could have never made the wrong ones? The reason why they suck are different: * Science is underpaid with no job security. * Maths needs to be done in complete social insulation for at least half the time you spend working on it, possibly more. * CS involves very anti-social working hours with few prospects for the average programmer once they turn 40. To paraphrase [this](http://philip.greenspun.com/careers/women-in-science), the only reason why we see it as a problem that are more men in science/maths/computers and not janitoring is that for whatever reason a career in the former is more prestigious than one in the latter. I was told this point blank in grad school as to why the theoretical particle physics group paid 25% less in stipend than the solid state physics group, mind you both were laughable. Unfortunately you can't eat prestige, you can't raise a child with it and you can't turn it back to the 100+ hours a week you would be working to get it. Edit: can't say I really expected anything but mute downvotes I'm getting. 
&gt; This is an excellent example of why I don't like political correctness: person A says X, person B hears Y, and instead of B asking A if A really meant Y, or explaining to A why what they said is wrong, B attacks A in a public forum (usually in the most visible way possible), hurting their reputation. Inasmuch as this happens, this isn't a problem exclusive to 'political correctness' (ugh, that term. The only people that ever use it any more ar the people attacking what it represents.). This happens *all the time*, especially in geek communities. 
&gt; If reloading some modules fails, GHCi will now not unload modules that are unaffected by the failure. \o/
now that you say that, it looks more interesting :). just a short question: if i store a tree, will that tree be loaded lazily? if so, i'll play around. mind if i ask you some questions about it by mail?
Did not even see that. That is huge news. :D
&gt; The original thread was disturbing, and the toxic and childish nonsense spewed by some of the well known members of the haskell community completely changed my mind about wanting anything to do with the haskell community. Man, for someone who wants nothing to do with the Haskell community, you sure put a lot of time and effort into posting these antagonistic flamebait comments! Somehow I *seriously* doubt you're participating in good faith here. Make up your mind. Either stop manufacturing drama by being a jackass, or stop commenting.
If Hackage was compromised the signatures couldn't be trusted either.
This thread of discussion is neither interesting nor constructive. Drop it. Any further remarks attacking specific individuals will be deleted.
Honestly, I've never been a massive fan of literate programming, until I saw this. I can absolutely see myself using this in the future. Nice work!
Actually I prefer Hayoo over Hoogle. I often find better results there. Sometimes Hoogle doesn't have any results at all. Is there a reason for this?
&gt; Is Hackage safe? Not at all. As everyone can upload a new version of any package, it should take about 5 minutes to break Hackage.
It amazes me that this hasn't happened yet. I took over the HMM package from someone else a while ago, and I was shocked that there was no sort of authentication or anything.
Hackage2 will fix this, don't worry.
&gt; And if I tell the world that you do those things? I would characterize it as telling the world that I limit women, alienate homosexuals, and make spaces unsafe for women. I would not characterize it as character assassination.
...perhaps a Valentines gift to the community. How nice.
&gt; As a native English ...which OP is not, and even if it was a mistake, he still doesn't need to apologise because he wasn't sexist and one usually doesn't accept blame and offer apologies for not speaking a foreign language perfectly. In any case, re-read the comment you replied to and notice that "for both".
It doesn't interact with those at all. It only reports on identifiers that are both unbound and start with `_`, so no currently valid Haskell code is changed by this.
&gt; if only this discussion were as non vitriolic It is often easy to miss the vitriol in a discussion when you agree with the people being vitriolic. In that thread I was criticized very heavily and often personally for things that I was not even saying (and took considerably effort to make it clear that I was not saying) because I disagreed with the majority and therefore must be sexist. This was incredibly disturbing to me because I would have characterized /r/haskell as being the last place to unleash vitiolic straw man attacks at someone merely for disagreeing.
You were criticized in that discussion unfairly. Point well taken about positions people agree with. I still think the original discussion was amazingly un-vitriolic. 
&gt; Tim didn't "attack" him, for one, but wrote about an unfortunate turn of events. Yes, but in my and others' opinions he wrote about them with an uncalled for degree of harshness towards Doaitse and what happened, and there were a lot of people in the ensuing discussion that were likewise much more harsh towards Doaitse and what happend then I believe was appropriate. This was what caused me to write some defensive posts, because I hate seeing people get beaten on for things that they didn't do; I suppose that you apparently consider this kind of attitude to be a bad thing given that you say that "Doaitse has shown here that he didn't need people jumping up to "save" him," but having been beaten on myself for things that I didn't do I can say from experience that it can make people being unfairly attacked feel better to see someone speak up and say, "Hey, he wasn't actually doing that." Furthermore, if people had replied along the lines of, "You are reading too much into what Tim was saying," then I wouldn't have minded. Instead, they tended to criticize me for things like being unwilling to acknowledge that sexist remarks exist, even when I made it clear that I did not believe it at all. I am 100% fine with being criticized but it drives me absolutely crazy when people criticize me harshly for something that I not only did not say, but made clear repeatedly that I was not saying it. So in short, if you think that this thread is *more* vitriolic than the other thread then it might be because you agreed with most of what people were saying and so didn't missed that it was often very vitriolic. &gt; Lots of highly voted comments on that thread said that what happened could have had to do with language issues, misinterpretation, etc. I agree, but lots didn't.
A monad is *like* a burrito. They aren't the same thing!
&gt; You were criticized in that discussion unfairly. Point well taken about positions people agree with. Thank you. :-) &gt; I still think the original discussion was amazingly un-vitriolic. We can agree to disagree on this, as I might be biased about the original discussion myself. I will say one things, though: I suspect that the people being vitriolic in this thread are most likely those who (like me) were shouted down unfairly in the original thread, so one reaps what one sows.
And you uploaded it first with the wrong capitalization, hehe. Seriously though, respect for keeping it alive.
It seems like you could have this ambiguity both with and without nested comments. For example, it's equally not obvious what: /* printf("*/"); */ should do.
I find it very American to be offended by the word attractive. (And I hope this statement offends Americans.)
I can only find one contact address, on the "user accounts" page. You could ask him? But honestly, who names their package "hmm"? 
No, the person creating the most noise in this thread was not involved in the other one. He just has a political ax to grind and wants attention. And frankly I don't think anyone other than you was treated "unfairly" in the previous thread.
To be fair, the proposed solution had no method to verify the authenticity of the public key, so it would still be possible to spoof the signature.
Arbitrary users get to upload a detached signature, and then what? Uploading the code and signature together doesn't necessarily make it secure, particularly so if Hackage were compromised.
Geeze, just apologize for saying something stupid. That's what you have to do, and that's all you have to do. I wish I had someone like Tim to intervene in the periods of my life when I was being an asshole. He's trying to do you a favor.
Well, presumably, if signatures were used, there would of course be a means to validate the signature against the authors (legitimate) public signing key, which it is presumed they created and gave someone when they got authorization to upload packages. That is, there's likely that whole 'I' part to 'PKI' somewhere in there. I imagine something like this was implicit in the overall plan of signing packages, while reading Chris' post (hand-waviness isn't ever really good in the crypto world, though. :) All that said, it's pretty much the most difficult, time consuming, easy-to-mess-up part - that whole 'I' thing. On the other hand, detached signature uploads in tarballs and a validation of the signature against a key in `cabal` is a 20 minute patch at best.
&gt; can't say I really expected anything but mute downvotes I'm getting. You write that down-votes met your expectations at most three hours after posting — and currently you’re at neutral score. Mm. I’ve just eaten, and the bitter taste of preemptive down-vote whining pairs well with my dessert course. Anyway, to address your claims a bit — well, what exactly is your thesis? Pare away the discussion about how STEM careers suck lemons, and I read an argument that women’s career choices reflect a differentially greater desire for social interaction and job security. Do you have any proof of that? For what it’s worth, I find your three statements rather dubious: &gt; Science is underpaid with no job security. &gt; Maths needs to be done in complete social insulation for at least half the time you spend working on it, possibly more. These follow for the social sciences just as well. Here’s a joke for you — what do you call a baccalaureate-level psychology grad? An orderly in a mental hospital. Where are you going to find a graduate student in sociology or economics at 3:00 in the morning? Running statistical models on a cluster somewhere, totally alone, and sweating bullets about an advisor meeting the following day. Yet the proportion of women in such fields is much greater than in CS or mathematics. &gt; CS involves very anti-social working hours with few prospects for the average programmer once they turn 40. What do you call a programmer after the age of 40? A manager, if they want to be — same as in many other fields. And in my experience the working hours aren’t that bad at all, outside of a few special cases (e.g. finance or video games). Edit: Long story short, I suppose, your argument feels like a retread of that classic "but men do all the hard, unpleasant work, so women shouldn't complain!". 
Yes, the 'I' portion is the tricky part. If the list is stored in Hackage it can't be trusted if Hackage were compromised. Detecting an evil Hackage2 mirror that sends out a backdoored package (signed with a 'trusted' key) to 0.1% of the downloaders would be very difficult. Having a small set of trusted keys (a second signature, perhaps) makes it easier to detect tampering, but that requires even more infrastructure (trusted hosts, etc)...
Yes loading occurs (only) when you dereference a Ref that happens to refer to a distinct disk allocation. This is presented as a pure operation, like in lazy loading. Also above I should not have said "build in one step" since that would be inconvenient: 1) you don't want to create the structure in ram first, and 2) lazy creation would probably require unsafePerformIO on your end. But you could probably populate the structure in a few hundred steps without generating to much garbage. Unfortunately I've never pushed it to that scale.
Hmm.. it will create an oddity though, although not necessarily a terrible one. If I try to to write `_foo` but accidentally spell it `_floo`, I'll get a type hole message instead of an unbound identifier message. Nonetheless, it should be sufficient to point me in the right direction, I suppose... (The one drawback being that ghc now tries to suggest identifiers spelled similarly to unbound ones -- so while I'll know that `_floo` doesn't exist, I won't get the handy spellcheck that automatically suggests that I maybe meant `_foo`)
Yep, stuff like that is a good reason to be very careful about magical behavior of names. So now it's basically impossible to know which of two vastly different things you meant until you get to name resolution. It would be very nice if there were syntax available for this feature instead of a magic name policy.
Yep, stuff like that is a good reason to be very careful about magical behavior of names. So now it's basically impossible to know which of two vastly different things you meant until you get to name resolution. It would be very nice if there were syntax available for this feature instead of a magic name policy.
I happen to be the author of this package. Would you actually want an explicit signature for such code? With the hackage auto-generated the signature otherwise, I guess the answer would be yes, as the explicit signature could be more succinct? Does the signature make you ignore the package? Assuming you wanted to write a dynamic-programming algorithm, not necessarily one for bioinformatics. I might add that the whole point of the underlying library is to provide close-to-C performance. 
Why? I think there are some implicit assumptions in your comment. The whole point of this approach is to assume that Hackage will be or has been compromised. That's the point of a signature.
[Let's just wait.](http://media.giantbomb.com/uploads/6/63795/2278965-op-will-surely-deliver-lets-just-wait_super.jpg)
I believe he was commenting under the assumption that in your plan, there wasn't a way to securely validate the package signature against the authors signing key to verify integrity. PKI is a huge thing, so leaving it underspecified leaves a lot of 'implicit assumptions' in all honesty. There needs to be some actual PKI scheme with a root authority. Or something like that.[1] How in the hell this should *actually work* I do not know. There are multiple implementations to possibly use and/or support that are popular for keys/signatures, and we have a lot of users. It'll need Haskell.org infrastructure time, money and patience. It will also need time and patience *from every existing developer* to implement and roll out. I am not a cryptographer. But someone should submit a proposal or something, really. [1] Strangely RubyGems actually [provides a feature similar to this](http://docs.rubygems.org/read/chapter/21), but it is more of a Web of Trust model than classical CA style PKI. Whether or not that's good or bad is up to you, but I'm not a fan personally (not necessarily based on anything rational, mind you.)
I was just hand-waving public key authentication, yeah, and assuming a web of trust would be sufficient. I doubt anyone will pay for a trusted CA, and trying to be a CA would be silly.
I think Dan Burton was treated quite unfairly. He made some remarks he probably shouldn't have, but the sheer volume of anger directed at him was very unsettling. Some people had more constructive discussions with him, but there was a lot of vitriol too. When someone says condescending or inappropriate remarks, it's appropriate to call them out on it, but it's not appropriate to engage in the sorts of childish stuff that I saw in that thread. Others, like tailcalled, where trying in good faith to engage in discussion and were being shouted down for it.
I must admit that I'm struggling to see how claiming that someone does these things is not the malicious and unjustified harming of a person's good reputation (the merriam-webster definition of character assassination). 
Hey, can we please not continue the irrelevant and non-constructive comment threads? The signal/noise ratio here is bad enough already. I don't care if ccshan is feeding the troll, if this continues I'm going to delete the entire comment thread.
I'm sorry you're irritated; I edited it down to the minimum point: that underspecification here is problematic. People are mostly sensitive to it on a topic like this (and it's my day job.) Like I said, RubyGems already had a 'solution' to this problem and it failed apparently - so when you just throw out signatures on packages, you really don't leave much else beyond a lot of assumptions to be had. And I don't think assuming anything beyond what's specified by someone is necessarily wrong, here. It's just a matter of clarity on a sensitive topic. (I think the intention of a CA scheme backing it was sort of implicit from the get go, but really you didn't give much; sorry for the misinterpretation.)
**we attract few women because we attract few women**: exactly! The opposite thing is happening in nursing, to the point that it has a major impact and helping to create a nurse shortfall so bad in the US many states are creating programs JUST for getting more nurses. Less than 6% of nurses are men, despite good pay, growth opportunity and future prospects. There isn't any inherent reason men aren't nurses, except history and role models. I don't know the right answer, but lack of role models seems to be a very good way to discourage future involvement in a field. 
I think deleting the thread would be an overreaction. The discussion hasn't descended into nearly the levels of vitriol and vague emotional arguments as the previous thread, and no moderation action was taken there. 
This is really cool! Would it be possible to have an option to treat all unbound identifiers as named holes? Then we could avoid users to having to type _. I don't think there is a reliable way to add the _ automatically in the editor (doing so at the cursor location alone will break valid code and will not help if you have used the identifier twice in the current scope). Also what if you want an infix hole? Do you currently have to put `_Op`? Would removing the _ requirement fix that too?
I use implicits for figuring out what type I need somewhere all the time. It is a good trick. 
But you won't have to do that anymore if you have named holes
Who was being offended "on behalf of poor defenceless women"? Was it perhaps the women themselves? I know most of the commentary *I* read was written by women talking about their own responses. The main exceptions being Tim ---who certainly knows a thing or two about being marginalized and excluded in CS due to gender issues--- and some reddit comments by bos and pigworker. Now, if you have issues with one of those three, I suggest you take it up with them personally. Otherwise, I suggest you get of your high horse and learn a thing or two about why you're being so offensive right now.
&gt; You don't seem to understand signatures. r/haskell is an interesting place thanks to the politeness and respect coming from all side, and this comment does not fit.
No, I'm on Linux.
Tim is the author of the main blog posts that caused the furor, and Tim has [ample experience](http://tim.dreamwidth.org/1780115.html) being perceived as female and consequently excluded from CS because of that fact. Describing Tim's posts as "hypocritically being tossed out by men" is disingenuous at best.
Tim can be a jerk, but such is life. What reason has he for retracting the post? Intent isn't magic. What Doaitse meant by his comment has no bearing on how that comment is received. Ideally the listener will try to factor such things in, but life ain't always ideal. Sometimes communication fails even if both sides act with the best intentions. If someone had as heated a response as Tim did then I don't see how Doaitse's response is really going to change that. Tim is totally entitled to be upset, just as he's totally entitled to be wrong, and just as we're totally entitled to pay attention or ignore him as is our wont. Fostering social justice isn't about getting people to admit they were wrong ---indeed, making people to apologize is an act of social dominance used to reinforce existing hierarchies of power---, fostering social justice is about getting people to behave better next time. That includes Tim, and that includes all of us.
Yep, that's what I mean by trusted host. Once an automated signing process is in place it becomes less trustworthy though, a real pain to deal with.
http://hackage2.willfixeverything.com/
As a non-heterosexual man, I was not in the least bit alienated by Doaitse's remarks.
Perhaps I need to remind you that the entire misunderstanding is because Tim misheard it?
But doing just otherwise = False should be able to cause plenty of joy
In my experience, it happens significantly more often because of political correctness. After all, there is an entire subreddit dedicated shaming people for politically incorrect comments, /r/ShitRedditSays . Sure, the negative effects are lessened because of the amount of posts, but I don't see a similar subreddit for geek communities.
A signature by itself doesn't provide any trust. Building the infrastructure to provide trust is unfortunately rather complicated, but this is what CAs help to provide. One issue with self-signed packages is just that, they're self signed with an arbitrary keypair you generated. How do I know to trust your key? If the list of trusted keys comes from Hackage, a compromised Hackage can serve up a list including an attacker's key rather easily. To help go unnoticed, attackers may choose to serve tainted packages randomly (at a relatively low rate) or biased towards more profitable targets (large companies, financial institutions, etc). Judging by how often cabal-install downloads a corrupt files today, I bet this would go unnoticed most of the time even if one is looking for it (delete the file, try again... now it works!). It's also a very common approach for phishing attacks, it's been going on for about as long as I can remember. Even without compromising Hackage, it should be fairly easy to brute force your way in to an account since all the usernames are public and the passwords are short (6 characters?) and mostly lowercase characters. So now that we have access to some account, adding a new self-signed key to your account and uploading new packages should be easy. Targeting the best accounts to attack is also easy, just email the maintainers to see who's around. Pick a relatively unknown maintainer of a package with reasonable number of indirect revdeps and you're good to go. This is more of a problem for Haskell than Ruby due to the large number of small packages, so you can probably get something through unnoticed for a while. Of course there are lots more attack vectors and lots of mitigation techniques but the fact is: security is hard. I certainly don't know what the right balance is for Hackage.
While that's true, I think it's worse for nested comments. The rationale for having nested comments in the first place was the ability to comment-and-forget blocks of code, no matter the contents of the block. I actually think strings should also be parsed too inside nested comments. Not being able to use unmatched `"` in comments (and proper escape sequences inside strings) seems only a small price to pay, considering you can't have bare `{-` and `-}` in block comments either.
I had no side effects and still feel pure.
&gt; shouldn't you call it hocco ? Please don't. ProseDoc is much more human-friendly than some arbitrary collection of letters.
Looks like a lot of people like this format but I can't get used to it. To me, it's much harder to read than literate programming where text and code are interleaved, or simply than code with comments. The only way I can imagine using this is by looking at the code only and sometimes at the left hand-side column if I need some additional information, but definitely not as a story illustrated by code (like the textual description of an algorithm with the corresponding code). Each time my eyes jump from the left to the right column, it takes me 1-2s to make sure I'm still on the right line, so I can't easily jump back and forth between the text and the code. 
AFAICT from 'darcs log', this may actually be already implemented, every package has a 'maintainer group'.
Regardless of the content of the discussion, I'm sorry about responding with all that negativity when you were spending your time participating in good faith. 
That may be what you want it to be, but (in my experience) in practice it ends up being more like SRS. I'm fine with the "kind of political correctness" you are talking about (and sometimes "enforce" (I need a better word for this) it myself), but those who actually are called politically correct tend to use it more like SRS.
Added as #19. This also made me think of #20 - have fun finding the space leak!
Here's one of the examples from the userguide: sum :: [Int] -&gt; Int sum xs = foldr _f _s xs Which results in: [1 of 1] Compiling Main ( example.hs, interpreted ) example.hs:2:16: Found hole `_f' with type: Int -&gt; Int -&gt; Int In the first argument of `foldr', namely `_f' In the expression: foldr _f _s xs In an equation for `sum': sum xs = foldr _f _s xs example.hs:2:19: Found hole `_s' with type: Int In the second argument of `foldr', namely `_s' In the expression: foldr _f _s xs In an equation for `sum': sum xs = foldr _f _s xs Failed, modules loaded: none.
Bias against trolling? Sure! We've already had this discussion once, and I've learned my lesson about letting political discussions continue endlessly. If you or anyone else want to argue further, do it somewhere else. It's not constructive or on-topic for /r/haskell.
Oh, I see. So if you name a hole as I did in my previous post and you accidentially use it in your function, you get helpful error messages. Right?
In what way is ohgodmypreciouskarma's comment an "antagonistic flamebait"?
Not quite. In your previous post, there were no holes, just a variable whose name starts with an underscore. Holes are _unbound_ variables whose names start with underscores (or at least, so I've gathered from the rest of the comments).
It's sad to see that every major linux distribution signs their packages (using GPG/Web of Trust, not PKI), but of all the external package repositories that I know (rubygems, hackage, npm), nobody has implemented signing in a usable way. And I don't think most people are aware of the implications of downloading code from third-party servers and run it on their production machines without even verifying the authenticity. It's a nightmare for any person who is even mildly concerned with security.
&gt; trusted CA Funny joke :D
+1 WoT
I wouldn't feel safe deploying production code that wasn't from my distro in general anyway. Both for security and stability reasons.
So a hole is like an "undefined", only that it has a name and doesn't compile? Should I see it as useful placeholder notation then?
Yes, it is already implemented.
Yes, that's all it is. You could even say that `undefined` is just the crappiest, most unhelpful hole there is. :) [I wrote a decently sized tutorial](http://www.haskell.org/haskellwiki/GHC/TypeHoles) on how to use holes when writing some code. It came up when the feature was implemented. It should give you a good idea about how to approach it (a lot of people liked it, anyway.)
There are backups.
&gt;Who was being offended "on behalf of poor defenceless women"? You answered your own question: Tim, and the majority of people posting in the original thread, some quite condescendingly pretending they know what it is like to be a woman (bos). &gt;Otherwise, I suggest you get of your high horse and learn a thing or two about why you're being so offensive right now. I am not being offensive. Offense is taken, not given. Anyone can take offense to anything anyone says. If you find my opinion offensive, then I am very sorry about that. But suggesting that my comment is in some way malicious or intended to cause offense is not reasonable.
Your comments in this thread are terrible and need to stop.
Maybe her comments are almost as undiplomatic as your comments, but they have actual arguments instead of asking the other person to show why ohgodmypreciouskarma's opinions are wrong. Sure, as a mod, it might feel like you can just press a button and make people fear and respect you, and ignore your lack of arguments, but that is not the case.
&gt;Intent isn't magic. That is ignoring the point of my statement, to address a piece of additional information. The point was that first part, that "the something in question isn't contributing to the climate". &gt;and I will defend to the death his right to do so. I never suggested that Tim was not allowed to voice his opinion. He certainly has every right to say whatever he wants. I merely shared my opinion that attacking an ally over an imagined offense is not in fact heroic. &gt;This is simply a fact, and there is ample evidence for it online The only evidence of that I've seen has been social justice warriors though. Is taking a tiny vocal minority who is offended by everything as representative of all women really a useful thing to do? Have you spoken to any women in IT about it? Or women who use haskell? I have, and nobody was offended by the video. &gt;because it silences and drives away the very people we could be drawing into our community So does treating women as though they were incapable of speaking for themselves, or having their own opinions, which is what the first thread on reddit boiled down to. &gt;there were other women who were heartened by the fact that we Haskellers take this issue seriously. And still others who were driven away by the very reaction you believe is entirely beneficial. To quote my wife: &gt;I am not a trophy to be won in a contest of "who is more enlightened and tolerant". I am not a prize handed out to the man who pretends most convincingly to understand how I feel better than I do. She was very hesitant to be part of the haskell community to begin with, not because of her gender, but because of the perception of haskell people as being all PhDs in math or computer science. The only time a gender issue came up is when Tim created one out of nothing, and the haskell community seemed to overwhelmingly feel that making a huge issue out of gender would be the best way to make women feel comfortable. Some women would prefer if gender were not an issue. Can you imagine if you were the only women at a haskell talk after that last thread? Everyone afraid of causing another huge uproar by accident, and feeling as though everyone thinks it is your fault because you are a woman? Even though you had nothing to do with the uproar to begin with? At least some women feel that way, and the constant assumption that people who are arguing against Tim's position are by extension arguing for making women feel excluded is fallacious and harmful. Both sides are making some subset of women feel excluded, there is no "right" side here.
&gt; Maybe her comments are almost as undiplomatic as your comments, but they have actual arguments instead of asking the other person to show why ohgodmypreciouskarma's opinions are wrong. This is so completely and utterly missing the point I can only assume it's intentional. Stop trying to drag politics into absolutely everything. It doesn't belong here.
&gt;given that you don't seem to contribute anything useful compared to the trouble you cause I'm not sure why you're even here Because, contrary to your insistence that I am just flaming, I am carrying on conversations with others, as reddit was designed for. I do not have a desire to silence people who disagree with me, and quite the contrary I feel that I gain from discussion with people like winterkoninkje. &gt;The fact that your terrible comments are actually getting upvoted only further convinces me that the entire comment thread needs to go. The fact that you can't step back, take a deep breath, and assess things objectively is unfortunate. Clearly many people feel the issue is important to discuss. Why do you feel that my contribution is objectively garbage?
no it is not. Its because lots of people interpret it the same way (many laughed accordingly) others were upset. I wasn't there, but the video made me uncomfortable. Tim is not unique. Acting like he is and is the cause of the issue is incredibly inappropriate and silencing. Stop.
Very thorough paper. The examples really make the paper, giving new users the opportunity to critically think about the concepts covered ingrain them. Rather read for comprehension, they read and think for understanding.
&gt; Because, contrary to your insistence that I am just flaming, I am carrying on conversations with others, as reddit was designed for. I do not have a desire to silence people who disagree with me, and quite the contrary I feel that I gain from discussion with people like winterkoninkje. Yes, the natural state of reddit is conversations where the signal/noise ratio drops to zero. Have you noticed how most large reddits, especially ones prone to political arguments, are uniformly terrible? I'm not sure why you think this is going to convince me. Please feel free to carry on your conversations somewhere else. I'd rather that /r/haskell contain worthwhile content. &gt; Clearly many people feel the issue is important to discuss. It was already discussed, in the previous thread. The amount of useless arguments that created is a large part of why I have no patience for this thread or your efforts to start pointless drama and political arguments. &gt; Why do you feel that my contribution is objectively garbage? Because you're good at sounding just reasonable enough that people take you seriously, you consistently make remarks that are divisive and guaranteed to provoke argument, your comments have little substance but lots of superficial arguments that make responding to you tedious and time-consuming, and you hide behind superficial politeness when called out on the fact that you're stirring up trouble in hopes of motivating naive bystanders into taking your side. Your net contribution to this community is deep in the negative and you show no sign of improving.
&gt;Yes, the natural state of reddit is conversations where the signal/noise ratio drops to zero And yet, the conversations you are participating in are the only ones here with no signal. &gt;I'd rather that /r/haskell contain worthwhile content. Well clearly some people would like it if /r/haskell wasn't alienating women. But of course, rather than something productive like discussing how we can avoid alienating people, lets just make entirely useless posts calling people trolls and their opinions worthless garbage. That certainly sounds like worthwhile content. &gt;Because you're good at sounding just reasonable enough that people take you seriously My posts are garbage because they are reasonable, but you don't like my opinion, and thus people might think I actually hold that opinion? Do you really, honestly not see the absurdity in your attacks on me? &gt;your comments have little substance Really? How is "the statement offended me" substance and "the reaction to it offended me" not substance? The only posts in this whole thread that lack substance are these ones discussing your desire to silence me, and my naively responding as though you actually care about making /r/haskell a good community. &gt;you hide behind superficial politeness when called out on the fact that you're stirring up trouble in hopes of motivating naive bystanders into taking your side. No, I am the same as I always am. I am am not trying to convince anyone to "take my side". I am trying to convince people that alienating women is not only done in the way Tim suggests, but also in the response to it. And that dismissively pretending anyone who is against engaging in attacks on innocent statements must be in favour of alienating women is detrimental to the very goal they seek: not alienating women.
I can tell you that it's not intentional. Now, if you would actually tell me what your point is, this would be a lot easier. Also, why are you accusing me of dragging politics into absolutely everything?
&gt; No, I am the same as I always am. Utter nonsense. Browsing your comment history says otherwise. You're as much of a jerk as you think you can get away with, in every subreddit you post in. And don't try to drag other arguments into this, it's irrelevant. Frankly, I was being charitable by assuming that you're doing this intentionally, rather than sincerely being as unpleasant as you seem. Is it really so much to ask that you drop the martyr act and take responsibility for your behavior?
&gt;Is it really so much to ask that you drop the martyr act and take responsibility for your behavior? At what point have I ever done anything to suggest I am not responsible for my behaviour, demeanour, tone, posts, phrasing, or anything else? Get off your high horse and stop pretending I am destroying this subreddit by having an opinion you don't like while you sit there shitposting "I am right because I said so, shut up because I don't like you" over and over.
Perhaps you mean an edge pointing to its binder? In which case program graphs are just ASTs anyway cause not all variables are bound.
Have you tried piggybacking [blank-canvas](http://hackage.haskell.org/package/blank-canvas) or [ji](https://github.com/chrisdone/ji) or a similar package that controls a web browser from a Haskell server? 
No but yours basically are!
I didn't intend to come off mean about it, but yes, I should have elaborated on my comment more.
Chris did specify PGP, and I understood this to implicitly refer to the Web of Trust. And I think there is a lot of merit to both Web of Trust and heirarchical PKI, and that they are complimentary not competing models. I agree that we do need to discuss exactly how to deploy PGP in this scenario, but it could be as simple as setting up a hackage PGP signature to sign all the public keys used to upload to hackage, that people could delegate trust to in their own PGP keyring, creating a very simple hierarchical PKI scheme of sorts. And then, preferably, a hackage administrator would be responsible for manually reviewing and signing those PGP keys, with a private key that does not exist on a public server on the internet.
PGP has the web of trust built in. Personally I think hierarchical PKI has merit too, but these considerations were already implicitly included in Chris Done's comment. (And Chris Done knows about the issues of authenticating signatures.) Of course, we would need a more complete discussion of how would use the web of trust if we were to use PGP on hackage.
While I see where you are going it probably wouldn't be a significant difference in effort to do it for all packages vs some packages.
A step in the right direction would only require the root key to be used to sign some developer keys who can then sign other developer keys and packages. Of course that leaves the whole certificate and key revocation issue which might need a centralized server of some kind.
Well, it did came out mean, but you'll do better next time ;) Apologies for being harsh myself.
Well, to be honest that comes off as a little condescending but I doubt you intended it that way. =) This entire comment section has been pretty prickly, I think we all need to realize we are all reasonably competent and chill out a bit. 
Your "program graph" sounds more like the union of a syntax tree and a UD-chain.
Yes, that is something along the lines of what I imagined. But there are other problems too we might need to think of. OK, so I think that at this point with all the technical discussion, my actual main worry now is getting people to do it, and applying it to existing infrastructure. With Gems as an example, they have a WoT model for signing packages. That's great - but nobody used it. I don't think I've ever seen a signed Gem. Personally after reading a bit about it, I can probably see why: it's tiresome for developers. And gems also don't enforce signing, so if people don't have to do it, they won't. Just the way it works. So if we're to learn something here, it's that signing should be mandatory. That means we're also going to need a plan to accommodate all existing packages, and their authors'. And that brings in the question of what happens to maintainers or people who are unresponsive (or heaven forbid, worse off than that.) And a lot of other stuff. And needing to deal with a lot of people. I will bet this accommodation will end up surprisingly tricky (tools, presentation, etc etc.) So, now, there will need to be large-scale organization to enforce it. That's going to just be a pain in the ass no matter how you slice it, Hackage is not small by any means anymore. We're lucky we don't have to do manual audits like Gems/Heroku folks are right now, because frankly I don't think we have the manpower. People have gotten very up in arms about security discussions before, especially with respect to Hackage. This is a *much* larger scale change than just any old security audit or fixing upload permissions.
Your effort in doing this would be much appreciated by me at least.
I'm not an expert, but I have a little experience with such things. Compiling has 5 phases: 1. Lexing 1. Parsing 1. Semantic analysis 1. Optimisation 1. Code generation For simultaneous lexing and parsing you can use Parsec. Play with it until you get it to generate correct Abstract Syntax Trees (AST). It's quite easy to convert a formal grammar into working code, just take care about whitespace and backtracking. Semantic analysis is a harder one. You need to add data (a symbol table) to your AST about what each identifier means: whether it's a local variable, a parameter, a global variable, etc. Since I don't know details about how these will work in your language, I can't give you any pointers. You can skip optimisation step, it probably won't be important. As for code generation, for starters you can just generate straight C/Java/C#/JavaScript/whatever. If you have your AST with type info, it should be straightforward. If you are more ambitious, you can target LLVM or generate JVM/CLR bytecode directly. But first, you must think how features of your language will translate into the features of chosen runtime. For example, if your target is JVM: what will be a class, what will be a field of that class, what will be a method, should that method be static or not, what should be the runtime representation of my datatypes, etc.?
Sure, but the loops in this case are essentially irrelevant to the structure. A pointer here is indistinguishable from a non-looping leaf node that counts back up to some higher node. I.e. a pointer is a de Bruijn index. De Bruijn trees are "graphs" but in a way that doesn't matter if you also have free variables. It doesn't interact with the monad structure at all.
[This might help, or not.](http://stackoverflow.com/questions/1669/learning-to-write-a-compiler) edit: I wrote a compiler for a subset of Java in a school course (The course page is [here](http://www.cse.chalmers.se/edu/year/2012/course/DAT150/)). It compiled to jasmin (textual rep of jvm bytecode) and llvm. I don't seem to have it online at the moment though. :/ Tell me if you are interested. We used the [bnfc converter](http://bnfc.digitalgrammars.com/) to generate a parser from a "labeled" bnf grammar. BNFC will also generate some boilerplate code for creating traversals of your abstract syntax tree - a visitor pattern and pretty printing facilities if you work in Java. (I didn't know Haskell back then. I wouldn't recommend writing it in Java (=Lot's of code) :) ) edit2: [Here](https://github.com/Tarrasch/Functional-language-interpreter/tree/with_ignored_parameter) is a public repo with a small example by another student using bnfc (for a small interpreter for lambda calculus).
The purpose of holes is really to use them in conjunction with an interactive editor which lets you inspect holes and refine them piece by piece. A prime example would be, say, defining `map` via `foldr`: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f xs = _f Inspecting `_f` says `_f :: [b]` so you say, "hmm, what can I do here? I know, I'll use `foldr`! map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f xs = foldr _step _base xs Now you inspect and `_base :: [b]` and `_step :: a -&gt; [b] -&gt; [b]`. Ok, I can give you a `[b]` thats easy map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f xs = foldr _step [] xs But what about this `_step` thing? Hmm. Well, if it's `a -&gt; [b] -&gt; [b]` it has to be a function of two arguments. So: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f xs = foldr (\x ys -&gt; _body) [] xs hmm so now `_body :: [b]`. How can I do that? all I have is `f :: a -&gt; b` and some other `ys :: [b]`. Well ofcourse I could just give back `ys` but that wouldnt do what I want. Maybe I should cons something onto it. map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f xs = foldr (\x ys -&gt; _head : ys) [] xs hmm ok we're almost there, just have to handle `_head :: b` now. Hmm. I have `f :: a -&gt; b` and `x :: a` ... Well now it's obvious! map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f xs = foldr (\x ys -&gt; f x : ys) [] xs Obviously a lot of this would just be done in one move as a programmer, instead of these small increments, but you get the point. In Agda mode in emacs, we use this kind of interaction a lot. Because we have holes, we can put in candidate code, and try to use it. The type check checks it and immediately tells us yes or no. If yes, it accepts it and refines the program. Some of it can even be automated. The step where I introduced `\x ys -&gt; _body`, for instance, would be done just by hitting control-c control-r in Agda mode. The type gives you, and Agda mode, all you need to know. This becomes *incredibly* powerful if you have dependent types. Sometimes to the point where the program almost writes itself because the type tells you so much.
Thought I'd add some meat to [our previous discussions](http://www.reddit.com/r/haskell/comments/17kynn/rubygemsorg_got_hacked_today_is_hackage_safe/c86gium) as a base for ideas. Whipped this up after I got home, it's not a big deal to say you think it's a waste of time, I had fun messing with some pretty nice Haskell packages to make this (the zlib, tar and MD5 libraries, really nice work!). I'm not convinced it's a good idea, or a bad idea, it's *an* idea. I've tested uploading such a package to Hackage to see if the .sig is preserved. It is: $ cabal install cabal-sign-0.4.0.0 $ wget -q http://hackage.haskell.org/packages/archive/cabal-sign/0.4.0.0/cabal-sign-0.4.0.0.tar.gz $ cabal-sign verify cabal-sign-0.4.0.0.tar.gz gpg: Signature made Sat 02 Feb 2013 07:56:54 PM CET using RSA key ID A2C5C589 gpg: Good signature from "Chris Done &lt;chrisdone@gmail.com&gt;" $ If you want to test it, you can try adding my test key: $ wget -q http://chrisdone.com/test.key $ gpg --import test.key And then run the above stuff. **EDIT**: Updated the above example to use cabal-sign-0.2.0.0 which uses SHA2 256-bit sums instead of MD5. **EDIT 2**: Updated to work with 0.3.2.0 which moves the .sig inside the project directory to allow `cabal install` to work. **EDIT 3**: Updated to work with 0.4.0.0 which signs the whole .tar.gz rather than the innards.
The video (https://www.youtube.com/watch?v=sW327nUDf5g) is very professional! Really nice job with the presentation -- very engaging.
My main advice is to start by writing the simplest possible compiler that can work, and incrementally build out your language. Arguably, start with an interpreter first, then start to think about code generation. Don't use strings directly for code generation. Find a nice library that represents the AST or bytecode of your target platform and go through that. If you don't have a library available, roll a simple set of combinators up. (Take a look, maybe, at how GHC does LLVM, which is waaay simpler than the llvm package on hackage, for example). Worry about scope, representing scope, managing scope, etc. a great deal -- people get that wrong all the time. Spend some time learning about debruijn representations, vs locally nameless, vs etc. Simple HOAS is quite nice. Since you're starting with a dynamic language, you don't have to worry about type-checking or inference, which is quite nice. Even if you were building a typed language, I'd start with explicit types all around and build up inference once the core was down.
Oh, I guess you just wanted Debug.Trace
To be clear about your meaning, you mean that Hackage is a key authority? So, e.g. 1. Authors all have keys and they are stored on Hackage. 2. Hackage also has a key. 3. A user downloads Hackage's key first. 4. If a user wants to download a package, Cabal downloads the key for the corresponding author. The author's key is signed by Hackage's key, so the user knows that this key is legit in the eyes of Hackage. Is that an accurate summary of what you're thinking?
Basically, yes.
Hm. If Hackage is compromised, won't that allow it to give a user a made up key and say "this is totally legit, I signed it and everything" and then give them a malicious package signed by that made up key? I think I'm missing a piece of the puzzle.
Oh marvelous! You've created [jsfiddle](http://jsfiddle.net/) for Haskell with a working server component. Ever since I was first introduced to jsfiddle I was struck by how beneficial it was to the js community, so I expect great things of this. Being able to link to a jsfiddle in a StackOverflow question changes things completely. I've only just started looking around but is it at all possible to use the hsfiddle features outside of the tutorials (i.e. could I write my problematic haskell code here then link it on StackOverflow)?
Yeah, definitely. It'd need to be a key signed from some higher-up authority to protect from that.
That is helpful, thanks!
Thanks for the overview, I didn't expect "named compiler-crashing undefineds" to be *that* useful. Sounds like a whole new style of coding: growing code by type intuition.
My assumption is that Hackage's private key would not be kept on the public server. It would be up to the hackage admins to come up with a workable solution, and I am on the list to help admin Hackage 2. Right now we have to flip a bit to allow a new account to upload, and I hope to move to a system were you have to submit an account request and having that approved before having an account at all. So we certainly can add signature signing as part of this process. Also, I'm not familiar with the finer details of the WoT, but my understanding is that we wouldn't necessarily have to deal with this logic too much ourselves, that people might be able to simply delegate some level of trust to Hackage's private key. Ideally, the private key would be on a seperate, non-public server, perhaps even kept inside a TPM chip, smart card, Iron Key, or something similar. I suppose a simpler, but much much riskier solution would be to give a copy of the private key to each admin. Another possibility would be splitting the key up among hackage admins so that it could be reconstructed from say, any two of our parts. I know these types of cryptographic primitives exist, I don't know how well they are implemented in open-source software. Or, perhaps each admin would have their own key signed by a private hackage key, that they use to attest to a new account's public key. That way hackage's private key wouldn't need to be connected to the internet in any way except when authorizing new admins, who would be responsible for their own private keys. I'm sure WoT can handle these kinds of use cases reasonably well, but again the finer details I'm unclear on. 
Even without all this stuff, its quite useful when you're writing extremely polymorphic higher-order code, and particularly for generic-ish things like definitions of typeclasses for compositions of continuations and functors and etc. At points like that, you often end up with one function you need to write, but only half a clue as to what its signature should be.
For the record, I disagree that ohgodmypreciouskarma is a troll, and I don't think his/her posts should be deleted. Furthermore, I don't think you are using your moderator powers objectively in this instance (usually the quality of your moderation is very high).
&gt; I had no idea you could sign tar files like that Me too. Could someone explain how that works?
Let me tell you my concerns: I don't think we can require mandatory author signing along with full client checking. There are two problems: * Windows users without gpg cannot check signatures (that's half our userbase), * making all authors set up gpg keys raises the bar too high for authors. Here's my suggestions: * We're already going to be getting per-package maintainer groups with the new hackage server. * The next thing we want is the server signing the package index (which should be extended with crypto checksums of package tarballs). That doesn't require anything of package authors, and at least tells you that packages have not been modified since being uploaded to hackage. This will also let us have mirrors without having to fully trust them (at worst mirrors can serve stale sets of packages). * Additionally, we can have optional author signing using gpg or whatever, and users could opt-in to checking signatures for packages that have them (which itself needs to be part of the signed index so we know for sure which packages ought to have signatures). This means we don't raise the bar for authors of new packages. We can encourage authors of popular packages to sign them, and for things like the Haskell platform we can make it a requirement. Also, having some crypto libs in the Haskell platform would enable cabal-install to depend on them (which would also solve the Windows issue, just like cabal-install has native support for .tar.gz without needing a tar.exe on windows).
Ok, that's great. For the record, your persistent compulsion to feed the trolls is one of the reasons I'd rather delete the junk instead of ignore it.
I think the main thing to note here is that Tim's reaction was directed at Doaitse when it perhaps should have been directed at those who laughed.
Yes, we only use .tar.gz. Compatibility and well-established formats are a great thing. The size of our source packages isn't really a problem. The bigger issue is the size of the hackage index file, but we can solve that with incremental updates.
Hey now, I'm not trying to start an argument here. I'm trying to get you to relax, and step back for a moment. I really can't see how you think that I'm feeding trolls. I've made a dozen or so comments in this thread and I don't see how any of them are troll-feeding.
Right. Pull the other one, it's got bells on. I already did relax and step back. That's why this comment thread still exists. If you're trying to convince me it was a mistake to not delete all this garbage last night instead you're doing an *excellent* job so far.
I'm sorry, I didn't realise you had made a change of heart since the post I responded to, and assumed you had already deleted comments. I'm also sorry that I appear to have angered you, and that you think I'm dishonest. I assure you neither are the result of intentional action, on my part.
The sign-up is more a matter of having some control over the numbers early on. I bet if you sign up now, you'll get an account before you know it :-)
See also the [roshask project at github](https://github.com/acowley/roshask). It's just a little bitrotted (it took me a few minutes of manipulation to get it compiling in Ubuntu). I'm planning to make use of it in my current dayjob. WillowGarage's ROS software platform is designed to support multi-process component architectures on-board a robot, based on a publish-subscribe model. Today, it is one of the more popular architectures in the industry. ROS has its faults. ROS isn't especially scalable or bandwidth efficient. There is no synchrony hypothesis or deterministic FRP semantics. It is easy for ROS systems to have race conditions, glitches, and other oddities. But it is widely used. (Yay, industry.) roshask provides effective integration of ROS with the haskell process, and also some nice properties when multiple ROS nodes are defined within one haskell process. I'm enjoying that roshask is enabling me to use Haskell in my current dayjob tasks. 
Some of these statements on reddit, "mansplaining" and "condescend[ing] in such a treacly way" were aimed at me. Though my main point was that I felt Tim was approaching the topic in a counterproductive way, I also made certain tangential and foolish remarks, which was a grave mistake on my part due to my own naivety. It was these tangential remarks to which my critics were often responding so harshly, and the criticism was well deserved. I deleted most of the tangential material, so this may not be apparent in retrospect.
Right. It totally makes sense to take into account package signing when it comes to defining subsets of hackage based on quality and consistency measures.
I recommend this video http://skillsmatter.com/podcast/scala/making-edsls-fly It starts of with an eDSL, but later on it goes to LLVM code generation.
These are reasonable suggestions, but I do have one quibble I don't understand, and that's our nearly total aversion to external dependencies. Sure, I do understand that external dependencies are not something we should just causally add. But it hardly seems as though gpg is an insurmountable problem, given how popular it is, and that it is available for free in command-line form for Windows, and really isn't *that difficult* to generate a private key. All I'm saying is that these discussions seem to look solely at the cost of an external dependency, and never at the benefit, even if the external dependency is free of charge, free and open source software, commonly available, and widely used. As for signing the package index, how would we do that in a way that leads to improved security for the scenario that Hackage is compromised? I mean, as a practical matter the private key would have to be available to the public server in a programmatic way. So even if we put the key in a TPM chip, compromising the server could still let an intruder sign a fraudulent index. It seems to me that signing the index would indeed improve the security of mirrors, but I don't see how we could improve the security of the main repo without the widespread use of signatures on the package themselves. (Oh, and as an aside, please don't hash .tar.gz file, and please don't hash signatures included with the tar file; just hash the portion of the uncompressed .tar file that contains the source code proper. Otherwise that commits people to a particular run of a particular compressor, and makes it difficult to add signatures after the fact.) 
There's a lot to be learnt from real-time systems here. If locking primitives have a a ceiling priority inheritance protocol (if a lower priority thread is locking something which is stopping the higher priority thread needs, its priority is bumped up to the priority of higher priority thread). This can completely avoid deadlocks in the system (though I believe this may only be true if each priority is unique). See [Wikipedia](http://en.wikipedia.org/wiki/Priority_ceiling_protocol) for more details. Also see these slides for some proof of these claims: [ANU's COMP4330: Real-time and embedded systems course](http://cs.anu.edu.au/student/comp4330/Lectures/RTES-07%20Scheduling.01.pdf).
The greatest learning tool lets the learner teach themselves and this has that in spades. I got to use this during Alpha and it works really wonderfully. I plan to use it a lot because asking readers to download the Haskell platform is too much.
so we essentially moved on from comparing penis sizes to comparing posting histories. progress!
The problem with this approach is that it doesn't scale; as soon as you need more than one polymorphic parameter, everything goes bonkers. Inference doesn't work, writing out types suddenly becomes a huge pain, etc. etc. Look at the Value datatype in my fork of Atomo, for example: https://github.com/Mathnerd314/atomo/blob/master/src/Atomo/Types.hs. Continuations are IORef's, Objects are IORef's; replacing them with something else is basically impossible. There are 3 mutually-recursive datatypes of Expr, Value, and Pattern, so each one would have to get 2 or 3 extra parameters to be IO-free; but you can't use type synonyms since they're mutually recursive and the occurs check prevents recursion in synonyms. I tried, and gave up, so I might be biased; but the [original author](https://github.com/vito/atomo) also gave up and is using [Ruby](https://github.com/vito/atomy) now, about as far away as you can get from static typing.
&gt; These are reasonable suggestions, but I do have one quibble I don't understand, and that's our nearly total aversion to external dependencies. The dependencies is not simply a question for cabal, it creates an issue for the Haskell platform and all the systems we and other people prepare that for. There's also the issue of licenses of the things we distribut. It's not an absolute but it's a lot of effort. I'd rather start at the other end, with things that are not so hard to do, and give us significant improvements without imposing costs or inconvenience on everyone. Note that for optional signing and verification, the caculation is quite different since it's then not a strict dependency. &gt; As for signing the package index, how would we do that in a way that leads to improved security for the scenario that Hackage is compromised? Right, it's not intended for that threat. Signing the package index is to address the issue of tampering of packages between hackage and the end user's machine, like your example of mirrors. BTW, I'm not sure hashing inside the .gz wrapper buys us a lot. We already have to commit to having stable checksums for the .tar.gz files, so we can't modify the contents and we cannot recompress anyway (Linux distros for example require stable md5sums or it creates headaches for them). Is there a problem with us just having blah-1.0.tar.gz and including into the index the blah-1.0.tar.gz.sha1 and blah-1.0.tar.gz.sig ?
Looks great! Will there be a REST api or something we can use to get the content of the tutorials people create (preferably the raw markdown)? I would quite like to make them available and runnable it Leksah with WebKitGTK+ to display the HTML and using ghci to run the code.
Have you tried using newtypes instead of type synonyms?
was it? I didn't get that impression, but I respect that you interpreted it differently. Perhaps if you had quotations from that post that you thought were attacking Doaitse? I don't know...I saw a lot about how the comments/the reaction could make people feel uncomfortable (which it did) and an essay on why comments like it are problematic, but no attack on Doaitse. I might be missing something. People respond to words differently. I respect Doaitse as an academic immensely, and always thought he meant well. I found his comment here very respectful. I also think Tim was doing good by standing up and saying "Hey, that kind of joke isn't okay, and it doesn't help you achieve your goals." Should I interpret it differently? I don't get on reddit to argue about stuff like this normally. When I started posting in this thread it was because I thought the characterization of Tim and the other people who expressed similar sentiments as "character assassins" and engaging in a "witch hunt" was problematic. That it was silencing voices who were pointing out at least perceived injustices, and that it communicated that it wasn't safe to disagree. Many of the people who expressed those views were well respected people in the Haskell community, others were young researchers vulnerable to retribution. My point is I understand the desire to defend someone you think is being treated unfairly.
&gt; It's not an absolute but it's a lot of effort. I'd rather start at the other end, [...] Fair enough, but I don't think we really have many options here, and it's probably well worth the effort. I don't think using gpg is a very high bar to require if everything has been integrated into cabal-install; the hardest thing the end user should have to worry about is generating their public key, which is easy. I've helped several very non-technical customers set up PGP using Cryptophane on Windows over the telephone without a common view of a desktop, and we haven't had too much trouble with that. &gt; BTW, I'm not sure hashing inside the .gz wrapper buys us a lot. We already have to commit to having stable checksums for the .tar.gz files, so we can't modify the contents and we cannot recompress anyway (Linux distros for example require stable md5sums or it creates headaches for them). Sure, the main site might need to commit to stable md5sums, but the difference is that we could easily end up committing all of Hackage's downstream consumers to stable md5sums as well, which would eliminate some potentially interesting possibilities. &gt; Is there a problem with us just having blah-1.0.tar.gz and including into the index the blah-1.0.tar.gz.sha1 and blah-1.0.tar.gz.sig ? You are splitting one conceptual object into two. (I assume the .sha1 is there just for the purposes of a cache and/or so you can download the sha1sum without downloading the file itself, so I'll call it two objects) It's a flagrant violation of the spirit of chapter 2 of SICP, and a personal pet peeve of mine, but very common in industry. Personally I like what Chris has done here, quite a bit, and it appears he's computing the hash more or less as I described.
While I agree that the criticism was well deserved, I think you were treated with an unfair level of hostility in that thread. Everyone is entitled to civility in discussions, even when they're under criticism.
I am in a unique position that hopefully will allow me to build a few bridges here. The original discussion occurred around mid-September. I read the article and voiced the opinion that Tim had taken a counterproductive approach to addressing the issue. I did my best to be sensitive about the issue, but in attempting to do so, I made naive and foolish tangential comments which were quite disastrous. bos criticized me (and received an astounding number of upvotes for doing so) &gt; No, you don't sound rude, but a different species of unfortunate: that famous mansplaining mix of well intentioned, tremendously condescending, clueless, and tone deaf. I ended up mentioning that I was a Mormon attending Brigham Young University at the time. In another comment bos (and I don't mean to pick on him, but his are the memorable comments) proffered: &gt; If you're interested, it's really easy to learn by reading about what it means to not be a well-off white male in a place run by, and for the benefit of, well-off white males. I think there are few better things you could do with your time. It'll really make you uncomfortable, be warned. Now, speaking of minorities, it seemed to me that bos was perhaps himself a bit biased against Mormons, but that's besides the point. I learned a great deal from the criticism that was dealt to me, although it was perhaps overly harsh on occasion. I am very glad that I read Tim's article, spoke my mind, and participated in discussion about the issue, even though it wasn't all rainbows and roses and everyone holding hands and being nice. ---- Now, little did anyone know, but at this time in my life, I was gradually planning to leave the church, and I had long since admitted to myself that I was gay. I discontinued schooling at BYU in late October, and then I spent the month of November getting to know the Salt Lake City gay community. I started reading books about being gay, and the struggles of gay people. (I highly recommend Carol Lynn Pearson's "No More Goodbyes.") As I listened to and read about gay people's life stories, I started understanding even more of what Tim was trying to get at. I started to understand, from an insider's view, what it was like to be a minority, to be discriminated against, to be silently oppressed. For quite some time I had maintained the facade of a straight Mormon boy, and for much longer before I had lived like that believing that's what I indeed was. In late November and early December, I finally came out of the closet. (What a relief!) ---- Now, I can't pretend to understand what it's like to be a woman in Computer Science, but I can say that in light of my new experiences in recent months, I have a much better *general* idea of why Tim reacted the way he did, why some women were in fact offended at the laughter that followed Doaitse's comment. So having seen the issue from "both sides of the fence," let me try to draw a better picture. When someone gets angry about minority issues, you shouldn't take it personally. Let me describe it from a gay perspective: throughout my life, I have silently been "battling" with same-gender attraction. I have lived through a life where even when people are politically correct, they somehow manage to belittle or insult anyone who chooses to pursue gay relationships. Little comments, little chips, or sometimes big stabs; for some gay individuals it becomes death by a thousand cuts. We are all trying to put on a happy face and make it look like we are alright when we really aren't, because that's the world we live in, a competitive world. I have recently been through job interviews, and there were several times that I just wanted to cry, but somehow I managed to put on the happy face again and pull through each interview. I know I am worth it, that I can do the job spectacularly. But what if I slip up and let them see the hell that I've been going through? What if they see how mentally fragile I am right now, having just come out of the closet? It's stressful. It's hard. And at times I'm very emotional, though I try very hard not to let it show. So take someone in such a position, and then jab them one more time. What will happen? Even if you just accidentally poked them in the rib with your elbow, it might be the case that the rib you poked is already badly bruised. Or perhaps you were reaching out to lend them a hand and didn't realize that their wrist was broken. So here's a person, who is trying to shoulder their own burdens, trying to appear as though everything's OK, trying not to trouble you with their problems. And then here comes you: the final straw on the camel's back, intentional or not, and the emotional facade collapses. The person you poked lashes out against you in what seems to be irrational and unfounded anger or even hatred. To you, this appears unjustified, unfair, just plain odd. That is possibly because you haven't seen past the facade, possibly because you've never had a bruised rib or a broken wrist. ---- It's not your fault, and it's not their fault. It's not Doaitse's fault and it's not Tim's fault. It's just the way things are. Though Tim is not a woman, he is transgender, and therefore unsurprisingly sensitive to gender issues. (If you think women or gays are treated poorly... I can only imagine the emotional difficulty of being lesbian or a trans man, the complicated intersections between the two oppressed groups, plus some.) I used to think "So [minority X] wants equality? Why then do they seem to demand special treatment? Wouldn't it make more sense to *not* be so sensitive about issues like this?" How naive I was. Two things I now know: 1) it will require *additional* effort and sensitivity (not just "equal" effort) to reverse trends of discrimination and imbalance. 2) Perhaps one day we can all treat each other equally, but for now, "the oppressed" have very real "bruises," so we need to treat them *unequally*: with *additional* sensitivity. ---- I stand by my original criticism: I feel that Tim's methods were excessive and possibly counterproductive. However, I much better understand *why* Tim wrote what he did. Some accuse him of "character assassination," I think this is inaccurate, but even if he were "guilty" of this, I still wouldn't fault him for what he said. Tim's words are the manifestation of someone who is justifiably very sensitive to gender issues. I was originally told that the intent of the delivered words didn't matter. It didn't matter that the people in the room might have been laughing because of self-deprecating humor. I did not understand this at all. How on earth could you get angry without taking intent into account? Now I understand. It's not about rationality. It's not about carefully analyzing the statement and *deciding* to get angry. Rather, it's about emotion. It's about being poked in a bruised rib: can you blame me for shouting in the moments right after you just poked me in a sensitive spot? I was skeptical of those that sided strongly with Tim. "We shouldn't encourage hypersensitivity," I said, as if hypersensitivity were a learned or controlled state of mind. Perhaps to some degree it can be controlled, but now I know better. Now I know that sometimes people are sensitive because of the hell they've been through, and so they have a right to be sensitive, moreso than the average person. ---- So the moral of the story is this: be mindful of your actions around sensitive people, and don't take it personally when they let the facade fall and cry out in pain. Make yourself aware of in what areas a person might be hurting, and be extra careful not to accidentally poke those areas. But sometimes mistakes happen, and it's nobody's fault. The other moral of the story is this: just because a dialogue doesn't go "oh I 100% agree, you are so right" doesn't mean that it was a bad dialogue. In fact it is from those dialogues where we disagree that we can learn the most. Caution should not stop us from "taking chances, making mistakes, getting messy!" (~ Ms. Frizzle)
No, but I'd have to stick in wrap/unwraps everywhere so I'm guessing it would still be a pain.
You are probably right about that, some people might still be put off, but it wouldn't have been nearly as bad. Tim should probably have focused on the audience response more...but at the same time, people do make jokes like that in technical contexts a lot and that is a problem (read Adam Foltzer comments from the first thread) and I think it is good that it is pointed out that it is a problem. 
Yours is undoubtedly the best post on this thread. Congratulations on coming out. I hope things are going well for you now. If you are still interested in schooling, and willing to leave utah, there are many great graduate and undergraduate programs I am sure would be interested in you given your track record as a member of the Haskell community. You know, bos didn't attack you. He didn't foam at the mouth and call you a terrible person. He made fun of you. He teased. It might have crossed over into bullying at times, but mostly the rhetorical strategy was one of mockery. Doesn't make it fair. Bos is older and more famous and more successful in this community than almost anyone. That is, there is an inherent power asymmetry that meant he could respond to you in the way he did. Still, I think we could all learn something from the fact that he was in a certain sense brutal, but did it without any of the wild accusations you see in this thread. The case of anti-Mormon bigotry (and similar bigotry directed towards Catholics, Muslims, and Evangelicals) is a particularly hard one for liberals like me to confront. Its real. I've seen it. At times I have probably participated in it. The problem is that when you have part of "religious practice" (in the form of quotations from holy books, actions by members, and statements by leaders) that you perceive as attacking you or people you care about it is easy to form negative opinions. Making jokes about magic underwear and polygamy makes sense emotionally after something like the role of the LDS in the fight over prop 8 (I have a gay brother who lives in California and is now married...so that one is kinda personal). That doesn't excuse bigotry though. I'm suspect though that you already knew all that.
Wow, thanks for the overwhelming interest, folks -- it may take us a few days to accept all the beta signups that have come in today. In the first 5 hours we got more signups than we hoped for in the first week! Appreciate your patience as we want to ensure everyone has a good quality experience. 
Thanks for sharing that, Dan, even though I doubt many people are still reading the comments here at this point I'm pretty sure I said before that I respected the way you handled the (rather strongly worded) criticism in the other thread; apparently even so I didn't give you enough credit. Congratulations on coming out, and on all the other courageous steps you've taken. If there's anything I can do to support you, feel free to ask.
If I may, you can implement your `Topic` type as a `Proxy`: type T p m a = forall r . Producer p a m r Now why would you want to do that? Well, you'll notice that in the paper that it mentions that this type `T` is a `Functor`, but if you implement it as a `Proxy`, then it's actually a `Monad`, too, where: return = respond (&gt;=&gt;) = (/&gt;/) -- The "pointful" version of (/&gt;/), which will be in pipes-3.2 -- See the "operators" branch on github (&gt;&gt;=) = (//&gt;) fmap f p = p //&gt; (\a -&gt; respond (f a)) join p = p //&gt; id In other words the "respond" category for proxies is a Kleisli category. Plus, by implementing it as `Proxy`, you get a bunch of other machinery for free.
A compiler is just a "function" (probably in some monad) from one representation to another. If you can write an interpreter for you language in Haskell than you can get from textual representation to something useful. That is, interpreters and compilers do essentially the same thing initially. After that I would say the main thing is thinking about how you will represent your core datatypes/language components in the target language (JVM byte code). A good way of doing this would be to think about "how would I translate various pieces of code into Java" and "what are the core types I am working with, how would I implement them." For example, if you want to implement the untyped lambda calculus (with tuples, numbers, and strings thrown in for good measure...okay, this is just scheme), you can think "what is the kind of thing that I am working with" and you come to the realization that it is lambda terms. What is a lambda term, abstractly? well, it is this Haskell type data ULang = Func (ULang -&gt; SomeMonad ULang) | Number Integer | String String | Cons ULang ULang | Bool Bool | Nil You can write a Haskell interpretor for this no problem, but that is not the point. You will probably want to use a representation other than the HOAS one here in your compiler for various reasons. Rather, we use it because it as a sort of denotational model of what we want to work with. how would you implement this in Java? Well you might want to begin by defining a base interface...one that perhaps encodes what we can do with values of this type...psuedo java public enum Tag { FUNC, NUMBER, STRING, CONS, NIL, BOOL } abstract class ULangObject { public Tag tag; } public class ULangCons extends ULangObject { public Tag tag = Tag.CONS public ULangObject car; public ULangObject cdr; public ULangCons(ULangObject car, ULangObject cdr){ this.car = car; this.cdr = cdr; } } the other types would be similar and boring except public abstract class ULangFunc extends ULangObject { public Tag tag = Tag.FUNC abstract public ULangObject apply(ULangObject argument); } then what does compiling a definition in this fictional scheme like language entail? It means producing a piece of byte code corresponding to a value of one of these types. Usually you are defining functions, so it means compiling a definition into a subclass of `ULangFunc`. How might you go about doing that? Well you have an abstract syntax represented using primal operations like this data ULangExpr = Lambda String ULangExpr | Var String | App ULangExpr ULangExpr | MkC ULangExpr ULangExpr | Car ULangExpr | Cdr ULangExpr | IsCons ULangExpr | ... that is, in terms of abstract operations. You can then imagine how you would represent many of these as Java functions. For example `compile (App a b)` might become `app(&lt;compile a&gt;,&lt;compile b&gt;)` where `app` is defined public static ULangExpr app(ULangExpr f, ULangExpr x){ if (ULangExpr.type == Type.FUNC){ return ((ULangFunc) f).apply(x); }else{ throw new ULangException(); } } or something like that. Most of them will compile into simple non recursive Java functions (which means that you can instead compile them by "macro expanding" them, or more likely expanding them into suitable definitions in a different intermediary language). The hard part comes with names. Okay, not that hard. Store in the compiler a table that turns names in one language into names in to what is needed to produce code in a new language. Often times those names can be the same. My understanding is that you can quickly learn how to translate java into javabyte code mechanically just be reading the output of a java compiler (I don't know JVM bytecode myself though). This is probably a good step. Think about what the appropriate java types correspond to values in your language. Come up with suitable intermediary representations. I would imagine you would want at least * An abstract syntax tree for your language * An AST for a simpler "core" language * An abstract representation of bytecode but more representations are always helpful. 
Author here. I'll spend some time working the bitrot off next week once I'm past a deadline tomorrow night. I'll investigate tying into pipes, but key points with these Topics are that they are infinite, and that we're mostly concerned with combining/intertwining/splitting them. The combinators to help with these specific use cases are the core value of that part of the project. I'd love to get more feedback and suggestions from Haskellers, though!
I think it would help if you gave a couple of examples of problems that you would solve using metaprogramming and we can see if there are idiomatic Haskell equivalents.
I'm a pretty big fan of macros and the bottom up philosophy of lisp...extending the language to fit the problem at hand. I use macros quite a bit, and shamefully, I occasionally use nested macros (the most common use case for me is writing Clojure pipelines of arbitrary, runtime defineable composition). But even things as simple as closures can be mind numbingly difficult in some languages. Many languages throw in some hastily contrived closure construct which fail on you when you try to anything slightly complex, like generating recursive functions at runtime. I'm interested in everything, from the basic to the complex. The only aspect of Haskell metaprogramming that I have seen explained in newb language was currying. It was what initially made me consider learning it. I'm hoping to get just a little more.
Thanks, i'll take a look.
Thank you for sharing that with us. As a fellow GLBT Haskeller, I have some understanding of what you're going through (although being a gay man at BYU who was raised a mormon sounds a great deal more difficult than any situation I've been in). You have all our support :) 
It's a very difficult problem when any automated signing is involved.
&gt; just take care about whitespace So goddamn hard until you've got the knack, I think. 
I see comments with '@' characters in them. Since the first post is not an introduction, can I find one somewhere else?
I think the reason we'd really like concrete examples, is that an awful lot of what people use macros for in lisp can be achieved using other features in Haskell. Template Haskell exists, but it's mostly used for generating boilerplate definitions.
Eh, lexing and parsing are the same thing, really, only that lexing parses a regular syntax. It's an optimization that is both unnecessary with newer approaches to compiling as well as problematic, especially when the tokenization rules are contextual (consider, for example, JavaScript embedded in an HTML document, or a PHP scripts that alternates between PHP syntax, heredoc strings, and direct HTML output). If you want to use a dual lexer/parser approach for these, you need two passes: one to split the input by sub-syntaxes, and another one to process the resulting chunks.
Yep, I looked through the API before settling on calling out to GPG. &gt; but no high level key management interface has been implemented yet. That's why I settled on calling out to GPG, expedience. Also the newness thing, I don't know at one point you're supposed to start trusting new security implementations?
Moving off-topic, but… If you matter-of-factly describe R as a functional language, you need to read the following paper really, *really* carefully: http://www.cs.purdue.edu/homes/jv/pubs/ecoop12.pdf Edit: I came across this paper after an R-using friend of my highlighted a problem to me that seemed to indicate that R sometimes used natural order and sometimes predicate order evaluation. Trash, I told him, evaluation order is so basic to the underlying theory of any programming language, there's no way you can play loose and fast with it. But this paper — which is the first thorough attempt to specify R thoroughly — does indeed show that, crazy as it sounds, R is lazy… except when it isn't. And there's more in there that's equally head-asploding. But I'll leave it to you to discover.
The about link seems to help http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/about/
"It's a verifier for Haskell code that is an active research subject." - not very informative. I don't see the point in page-long blog posts if no effort is made on getting an audience interested.
Yep, [I have piggybacked it on ji](http://apfelmus.nfshost.com/blog/2012/10/31-frp-ji.html) and it works nicely. Unfortunately, ji has never been released to hackage. I'd happily contribute if someone picked it up and brought it into shape.
Perhaps you should try clicking the first link in the OP.
Maybe we could have holes suggest spelling corrections as well if something close in spelling is found, but still behave like a hole?
For GHCi and `:t` you can use `ImplicitParams` with older GHC's, BTW: Prelude&gt; :set -XImplicitParams Prelude&gt; :t map ?f [1,2,3] map ?f [1,2,3] :: (?f::a -&gt; b, Num a) =&gt; [b] Tells you that `?f` is a `Num a =&gt; a -&gt; b`.
I didn't want to impede playing by adding the friction of setting up the libcrypto stuff for non-Linux users. (GPG is comparatively easy to setup.) Having another look, I see there's actually a pure (not pure Haskell, but at least self-contained) SHA2 implementation which is cool. I replaced the implementation with that. The new cabal-sign (0.2) now uses SHA2 256-bit hashes (and is now signed with a SHA2 hash).
Looks Vincent Hanquez also made something very similar yesterday too: http://article.gmane.org/gmane.comp.lang.haskell.cafe/103157
&gt; You are splitting one conceptual object into two. ...but in a way that is pretty much standard for signing zips and tarballs, so even if it isn't the nicest solution from a theoretical standpoint it is certainly good enough, especially given that people are already familiar with it and the gains for having there only be a single file are marginal.
Cool stuff! 1. Will it be open source? I expect maybe not, given FPComplete is commercial, and I can live with that. 2. Will it be and remain a gratis service? 3. I know FPComplete is focused on Yesod, but is there any hope of getting Happstack and Snap etc integrated the same way as Yesod? Assuming people step up to maintain the relevant packages in Stackage, at least.
Did that before. No explanations of why and how and anything. "We use {-@ to ..." - great, where does that syntax come from? When I use it nothing happens. It looks like a tutorial, but it's more like looking over someone's shoulder. Edit: Oh, you have to read the readme on GitHub! That makes so much more sense! Spoiler: it's an external program and not just a Haskell module, it parses the special comments and checks the code based on that; HLint looks for style errors, Liquid Haskell checks assertions before compilation. &lt;- For future reference, that's what the "about" page and the first blog entry should say.
In all honesty, most of it is pretty familiar if you have even a modicum of experience in the theorem prover/verification world. That said it could probably use a better intro. Basically, the `{-@ ... -}` syntax is used to define an annotation for some function, that the LiquidHaskell tool will read. That annotation essentially specifies a predicate that holds, who's proof is discharged by an external theorem prover/solver. Normally, when you write things in a theorem prover, you have to actually not only state your propositions, you have to write the proofs. That's great but this can get tedious quickly. But for certain classes of problems, it's pretty easy to automatically construct a proof by just looking through the search space (roughly speaking.) SAT solvers are a pretty classic example of this. A more modern example is SMT solving, which is a lot like SAT solving but turned up to 11, and applied to theories of first order logic with equality. So rather than just dealing with boolean satisfiability, you can state things like linear inequalities over real numbers or whatever. Or more abstractly, you could say things about lists and arrays. You pretty much do this by actually stating your problem, and asking whether that proposition is or is not satisfiable. Or if the predicate is satisfied for every input. That's something roughly like what Liquid types are buying us here, if you squint. It's a very lightweight, sorta 'fire fast' approach to verification. The tools are great too because if a proposition isn't satisfiable in terms of some variable, or your proposition doesn't hold for every input, you get counterexamples. So, what this thing *is* is basically a program that reads Haskell source, reads the annotations, and then constructs a model representing your Haskell program, and then tries to discharge the specified predicates against the model. This is essentially outsourcing correctness verification to an external program. It adds a nice extra layer of validation to our program (and unlike tests, these properties are *guaranteed* to always hold. But maybe that doesn't tell the *whole* story, either! I also have a rather [humorous](http://neocontra.blogspot.com/2011/04/rc4-in-haskell.html) tale about using SMT verification in Haskell. But I'm also a noob, so there.) Automated reasoning theories like this are actually tremendously useful for certain classes of problems, because computers pretty much handle the hard part. But you can't just ask anything, and like all theories, it's very possible to state something not very useful (like in my post. And stating good properties can be hard! Just go look at the fibonacci demo in the OP.)
&gt; Right, it's not intended for that threat. Signing the package index is to address the issue of tampering of packages between hackage and the end user's machine, like your example of mirrors. You can not protect against tampering without keeping the secret key secret. And putting the secret key on a public server is not really secure.
Thanks for the explanation, that helped a lot.
If you want pipelines and closures, you really should check out my `pipes` library, which implements exactly that. `Pipe`s form pipelines and `Server`s are closures (in the imperative sense). More generally, the key words you are looking for are [free monad](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) and [free monad transformer](http://www.haskellforall.com/2012/07/free-monad-transformers.html). Those are the idiomatic way to implement rewritable abstract syntax trees in Haskell and they have a very elegant basis in category theory. I can briefly summarize them like this: * Free monad = abstract syntax tree * Free monad transformer = dynamically generated abstract syntax tree If you give me an even more specific example, I can show you how you would implement it using free monads and it would be just a couple lines of Haskell code.
The problem with giving concrete examples is that they are infinite, and the examples I give you today might not ever present themselves in the future, and problems that I have never had before may present themselves tomorrow. I could literally be here all day, and only touch 1% of the use cases that I have used. You could be right and Haskell could further simplify 99% of what I have done...but going through every use case is an awfully imperative way of finding that out. I don't think a required use case is necessary for an explanation of a tool. If you explain what a hammer is and does, you no longer need to explain the millions of possible situations in which you could use it. EDIT: I'm not trying to make things hard or be demanding with you as a community. I'm not expecting someone to tutor me. I was just hoping to find some existing resources (blogs, books, whatever) that take an approach to explaining this kind of stuff without assuming that I have already learned a great deal of the language. 
My usual workflow for` ImplicitParams` is to turn it on in a file when I need deugging, then just drop in `?foo` placeholders as a poor man's holes and compile, inspecting the now ensured type errors. I don't expect that `TypeHoles` will appreciably change this workflow. I also use it while exploring in `ghci`.
You can program imperatively in Lisp, and you can program functionally in C. R, in its idiomatic form is a very functional language. It is not a purely functional language...and I've heard Haskell proponents are pretty dogmatic about that issue...but I'm okay with that. 
You don't have to present an infinite number of examples. We just need 1 or 2. The beauty of Haskell idioms is that they are very general, so once you solve a couple of problems you end up accidentally solving all of them.
Thanks, I'll take a look. Still trying to wrap my head around monads. For a simple example, how could you accomplish this transformation: (func1 (func2 (func3 data))) -----------------&gt; (-&gt; func1 func2 (func3 data)) And then for a more complex example, how would you do the same if you had to, at runtime, choose between func2 and func4, as well as swap the position of func3 and func1? 
Thanks for the explanation aseipp! Also, from the first entry: "Note: We will use @-marked comments to write refinement type annotations the Haskell source file, making these types, quite literally, machine-checked comments!" :) 
just to not duplicate the comment, here are two examples as a response to someone else: http://www.reddit.com/r/haskell/comments/17oad6/metaprogramming_in_haskell/c87ji3m
I'm going to just make an unrelated point here :-P I notice that out of the unanswered questions in the Haskell tag on StackOverflow, there are a _lot_ of Yesod questions. Especially since Yesod is part of the stack that FP Complete has settled on, I think it would be good to encourage a culture in the Yesod community of paying attention to supporting new users who will often post questions on Stack Overflow: http://stackoverflow.com/questions/tagged/yesod?sort=unanswered&amp;pagesize=50 I'm not saying fp complete employees should do this per se, but I think y'all can play a role in trying to encourage it, at least :-)
While I can read and understand your example, it's very difficult to say how you'd do this in Haskell, because I have no idea what you are trying to accomplish. It is often the case that the Haskell way of doing something is different from most other languages.
This is awesome, thank you. If you don't mind, I'd like to use your code for reference in my project.
&gt; Also the newness thing, I don't know at one point you're supposed to start trusting new security implementations? For cryptography, never trust new implementations. Experts [routinely](http://rdist.root.org/2009/05/28/timing-attack-in-google-keyczar-library/) [make](http://www.daemonology.net/blog/2011-01-18-tarsnap-critical-security-bug.html) [implementation](http://svn.ruby-lang.org/cgi-bin/viewvc.cgi?view=revision&amp;revision=33633) [errors](http://threatpost.com/en_us/blogs/new-crypto-attack-affects-millions-aspnet-apps-091310) that weaken or negate the security guarantees of the cryptographic functions being used. The people making these mistakes often have PhDs and have published fundamental results in the field. [And never mind the average open source developer](https://github.com/dlitz/pycrypto/pull/14). Call out to well tested, widely used crypto libraries only. If you're interested in how one might start to learn the skills required to discover the aforementioned implementation errors, the standard text is [Cryptography Engineering](http://www.amazon.com/Cryptography-Engineering-Principles-Practical-Applications/dp/0470474246).
‎(let (code f b m) (setq f 'format) (setq b t) (setq m "This is runtime metaprogramming in Common Lisp!~%") (setq code `(,f ,b ,m)) (eval code)) (def command 'print-str) (def string "This is runtime metaprogramming in Clojure.") (def code `(~command ~string)) (eval code) My question is: Is it possible to achieve runtime metaprogramming in Haskell through Template Haskell or through whatever method? http://www.haskell.org/haskellwiki/Template_Haskell
Given the direction you are going, you may want to consider using [this](http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours) as your tutorial. Compiling new Haskell expressions into a running program is doable, but not easy, comes with lots of caveats, etc. Creating a true interpreter is Haskell is extraordinarily easy, truly one of the strengths of the language, and worth knowing how to do anyhow.
Willem Obbens generously wrote this BrainFuck interpreter in Haskell and published it in our "Programming Haskell" facebook group: https://github.com/willem-o/brainfuck/blob/master/brainfuck.hs https://www.facebook.com/groups/programming.haskell/
It is one thing to take a config file and return a function from a pre-determined set of possibilities. It is something else entirely to take a config file and write a completely new function. The config file could be as simple as a csv file, with a sub-function name in the first column and parameters in the subsequent columns: *,10000 stochastic-rounding, concat, " bps"
This is interesting. What is the difference between the . operator and the $ operator? I have never thought about the whole type issue with regard to metaprogramming. I'm not anti-static-typing by any means...in fact, I have long wanted to try a statically typed functional language, and have considered OCaml, Scala, and Haskell. But now that you have me thinking about it, I can see that it might throw some kinks into metaprogramming, because if you can't fully define the program at compile time, then you can't define types at compile time either. 
This looks really interesting, thanks for the link. 
The type is defaulting to `Double` (which uses floating point). Consider using `Rational` (which doesn't) instead. Prelude&gt; let x = 0.1 :: Rational Prelude&gt; x * 2 == 0.2 True Prelude&gt; x * 3 == 0.3 True 
Generally speaking, I'd say turning arbitrary strings into code is a pretty hacky way to implement a config file, as you're doing no validation of the input. I suspect the only reason you'd think to implement something like this, is because it's the simplest possible solution in Lisp, and you need it quickly. Haskell's purity means these kind of hacks aren't really possible.
Thanks, good suggestion, we are looking into it now. In fact we are going to go farther, and not just for Yesod. We expect to provide permanent, every-day participation in selected forums by someone from our staff. We also expect to help connect active online super-contributors from the community with identified needs, just as you have done in your comment here. And we'll see if we can do something nice for people who post the most helpful tutorials, at least put a big FP Heart by their name or something. Also, the School of Haskell fully supports incoming links from forum posts (or from anywhere) to user-written tutorials with real running Haskell code embedded. This should enable people to post more powerful answers since any user can write a tutorial.
Or `CReal`, for `Floating` operations ([`cabal install numbers`](http://hackage.haskell.org/packages/archive/numbers/3000.1.0.1/doc/html/Data-Number-CReal.html)): &gt;&gt;&gt; import Data.Number.CReal &gt;&gt;&gt; let x = 0.1 :: CReal &gt;&gt;&gt; (x * 2 == 0.2 ,x * 3 == 0.3 ,sqrt (x * 3) * sqrt (x * 3) == x* 3) (True,True,True) &gt;&gt;&gt; (sin y ^ 2) + (cos y ^ 2) == 1 True
On the last point, I don't think we can/should expect unification among things like web frameworks. Libraries yes, but there's too much taste involved in frameworks. No other language has settled on a "one true framework" and in cases that have seemed the closest, like Rails, there have been waves of diaspora and reconsolidation. Similarly with python and django, etc. But also, for this reason, I don't think that Haskell newcomers (assuming they aren't programming newcomers in general) would be surprised or disappointed by multiple approaches (many, myself included, take it as a sign of health!) and if anything they might be taken aback by how _compatible_ almost everything is in Haskell (web-stack related and otherwise).
I do not think most people here would consider currying metaprogramming in Haskell. Hopefully this helps you understand why everyone is asking for examples, because the majority of what you want very well may not be what is considered metaprogramming in Haskell.
&gt; dogmatic I do not know about dogmatic, but the definition that Haskellers use for functional derives from mathematics and how the term is used in many other languages or other areas of programing it mostly means a style of programming. There is some correlation and causation between the two and it is the lack of a huge divide between the two that causes misunderstandings most of the time.
That makes me less likely to take time to learn R. The fewer edge cases I have to learn the better and I can only learn so many so I better choose wisely.
I just did this last week. It's surprisingly easy in Haskell - with the right abstractions! https://github.com/chris-taylor/Life/blob/master/Life.hs
[Here's mine](https://gist.github.com/aa25f5f34c0decc18f28) that can output the animation in text and with OpenGL. I've probably done something poorly on the graphics side of things, this was my first attempt at it in any language. It works (on my windows 7 computer), but it isn't perfect by any means.
I think you might have a rather unconventional idea of what “metaprogramming” means...
&gt; I notice that out of the unanswered questions in the Haskell tag on StackOverflow, there are a lot of Yesod questions. This seems to happen for any non-trivial library or framework that's accessible enough to newcomers for them to try and get stuck, unless someone with deep knowledge of the specific package is active enough on SO to field the questions as they arise. Maintaining that kind of presence on SO is probably a good idea for anyone closely involved with such a package. Example: See if you can find even a single question on SO about `pipes` or `netwire` that wasn't answered by the author of the package, then try to estimate how many of those questions would have gotten answered as well (or at all!) were the respective authors not active on SO.
This is no good and unnecessary. Rather sign devel keys with n of m well known community keys. Each done personally wot style. This avoids crazy and unattainable single secret security.
Exactly. Also create a security policy that covers what is expected out of a few well known gatekeepers' keys in the community.
Interesting. So you convert everything to ints with fixed precision?
Oh, I absolutely agree. Yesod just stands out at the moment as the non-trivial library/framework that has the least activity/presence from SO heavy users. Every now and then I get on a tear to try to knock down our unanswered questions in the Haskell tag as close to zero as possible, and Yesod happens to be one of the places that needs some attention :-)
The lower down on the power scale of languages you go, the more features that are considered "metaprogramming" :-) On the other hand, the more techniques you get under your belt, the more you tend to think that genuine metaprogramming is something like Adam Megacz's work with [theorem-prover verified compiler passes](http://www.cs.berkeley.edu/~megacz/garrows/) and _nothing else_.
The advantage of CReal over Rational is that it is an instance of Floating, correct? How much performance penalty does that come with?
The first few times I looked at Megacz's work I got completely lost. Eventually I realized that he was just *computing with monoidal categories* and it made perfect sense. The categorical combinator model for functional languages is ancient: CAML stands for the "categorical abstract machine language"! Not to say that this stuff isn't cool, just that it is not as hard to understand as you might think.
Not necessarily fixed, it could be a bignum with arbitrary precision.
The basic idiom to follow with Parsec is to always skip whitespace as soon as possible after parsing a lexical token that can be followed by it. I.e. what the lexeme parser in [Text.Parsec.Token](http://hackage.haskell.org/packages/archive/parsec/3.1.3/doc/html/Text-Parsec-Token.html) does. If you try to skip whitespace *before* tokens instead, you end up with far too many try combinators and too much backtracking. This lets you parse free format languages easily. For indentation sensitive languages like Haskell itself, you're on your own. :P (I vaguely recall there's a package somewhere to help with that?)
I don't know about performance penalty...but `==` may diverge with any correct implementation of Real numbers. It is impossible to tell if .99999... == 1.0000... in general (okay, decimal expansion is only one implementation, but all have some case like this due to the Halting problem). Reals are a pain. Floating point approximations of reals are also a pain though, and you can't represent lots of numbers (like $\pi$) using Rationals. 
Thanks for the analogy. For me, the big appeal of Haskell is its type system, inherent concurrency, and static compilation performance. The big appeal of lisp is the power and expressiveness (I basically agree with Paul Graham verbatim here). I would love to invest what seems to be a substantial amount of time into learning Haskell if I knew I wouldn't have to give up that power and expressiveness. Unfortunately, I don't know if I would or not, and it has been difficult trying to find out (which is why I asked). 
`Bound` is just about how to deal with names in your syntax tree. The jargon arises because this can be a deceptively complicated problem and it has many potential solutions. I'll try to steer clear of jargon: When you write an interpreter, and put together a syntax tree you 'bind' or abstract names for function arguments, and instantiate them when you call the function. Monads are all about substitution, so in `bound` I literally use the `Monad` on your syntax tree for just that. Using `(&gt;&gt;=)` substitutes in terms for free variables in the tree. A free variable is just that. The expression `a + b` has both `a` and `b` as free variables in it. You could replace `a` with `(6 * 8)` and still have a reasonable expression. So with `bound`, you give me an expression type like data Exp a = Var a | App (Exp a) (Exp a) | Lam (Scope () Exp a) and I give you a monad transformer `Scope b` that you can use to capture some variables so that they won't be replaced by `(&gt;&gt;=)`. You need this because when you have something like `(\f x -&gt; f x)` those names are captured by the lambdas in question. In `(\f y -&gt; f x y)`, both `f` and `y` are bound, and the lambda acts as a 'binder'. The `x` remains a free variable and could be captured by a surrounding lambda, or supplied by an environment of some sort. If you were to take the program (\x -&gt; (\x -&gt; x) x) and change the name of the outer x it'd become `(\y -&gt; (\x -&gt; x) y)` The inner 'x' is bound by something closer to it. But what matters here is that bound variables are captured by lambdas, let expressions, foralls, or whatever other kind of binder you are using in your code. They arise in all sorts of syntactic constructs. Getting back to `bound` and `Scope b`: One way to think about `Scope b f a` is as if it was `EitherT b f a` or `f (Either b a)` such that the monad only does the substitution on the 'a' variables, leaving the b variables bound by a given scope alone. newtype Scope b f a = Scope { runScope :: f (Either b a) } -- simplified You can build a scope by abstracting: abstract :: Monad m =&gt; (a -&gt; Maybe b) -&gt; f a -&gt; Scope b f a and you can instantiate them to tear down a scope, e.g. for applying a lambda to a value: instantiate :: Monad m =&gt; (b -&gt; f a) -&gt; Scope b f a -&gt; f a Note: these two operations work on _any_ monad you give me, and factor out name capture from what you need to worry about. There is one more big part to bound, a combinator `(&gt;&gt;&gt;=)` for binding into scopes, patterns, case alternatives, etc. instance Monad Exp where return = Var Var a &gt;&gt;= f = f a App l r &gt;&gt;= f = App (l &gt;&gt;= f) (r &gt;&gt;= f) Lam b &gt;&gt;= f = Lam (b &gt;&gt;&gt;= f) Note the `(&gt;&gt;&gt;=)` in the `Lam` case! thats the only thing you need to change in an otherwise completely obvious monad to support name capture. You have to write a few boilerplate instances on top of it, but thats pretty much it. The API for `bound` is around 6 functions and it can be hand implemented at the top of a source file if you don't want to incur an extra entirely haskell 98 dependency. One of my coworkers is fond of doing so when he works in Agda. That said, the library does take care of some extra tricky details. It deals with the 'equality of names' for you, makes that `(&gt;&gt;=)` _much_ faster than the naive `EitherT` version I described here, etc. Putting the jargon back in, the `EitherT` straw-man version would give you a "locally nameless de Bruijn indexed" syntax tree. `Scope` is more efficient than that, because it can figure out that it doesn't need to walk whole branches of the tree to slide a 'Right' constructor for the Either into it, by basically letting it wrap the branch itself. This lets it also know that it can't find any 'Left's inside the branch, so it can avoid skimming it later when it comes time for instantiation. It does this by replacing newtype Scope b f a = Scope { runScope :: f (Either b a) } -- simplified from above with newtype Scope b f a = Scope { runScope :: f (Either b (f a)) } This actually speeds up de Bruijn indexing an alarming amount. This was first discovered by [Bird and Paterson](http://www.cs.uwyo.edu/~jlc/courses/5000_fall_08/debruijn_as_nested_datatype.pdf) 13 years ago, but the pattern wasn't abstracted out into something reusable until `bound`, which took it from needing higher rank types and a fairly complicated one-off encoding for each would-be monad to a simple monad transformer you can reuse.
Are you referring to something similar to Clojure's -&gt; macro? Although the syntax for that is different from what you describe. But Haskell has an easy to use composition operator . that operates like Clojure's comp and CL Alexandria's compose. If inc is a function that takes a number and results in 1 greater than the number, and double is a function that takes a number and gives twice the number, (double . inc) is a function that takes its argument and returns twice the incremented argument, so that (double . inc) 5 is the same as double (inc 5) is 12. In Clojure, this would be analogous to using ((comp double inc) 5) instead of (-&gt; 5 inc double). In the notation which you provided, which does seem to be different from Clojure's -&gt;, it would be (-&gt; double (inc 5)). I am aware that this is only an example of functions subsuming what is often made convenient by macros in Lisp, but there are more examples of this sort of thing. Admittedly, there's no guarantee that every convenience that a macro can provide can be translated into non-macro Haskell code. EDIT: It should be noted that (.) is just a function like any other, named in such a way that it's easy to use as an operator, and not particularly special metaprogramming, the way -&gt; above is.
Data.Fixed.Fixed Data.Fixed.E2 is nice, and comes with ghc.
Very neat. How is LiquidHaskell dealing with the fact that Haskell is lazy, though? Is it just assuming that it isn't? When you write your measure on lists, it's effectively treating lists as an inductive type, which could mean that in some cases the verification might not be correct in the presence of infinite values. If it isn't handled right now, do you have plans for dealing with it somehow? I'm fine with it not being 100% accurate, but am curious to see if there are clever ways it can be handled in a principled manner.
The question here seems to be how can Haskell be more like Lisp. The answer is it can't. You come to the land of Haskell to things to Haskell way.
Do you mean "weird" or "wired" (or "iwerd" or "wiedr")? 
The counter part of that is that everyone using the same "vetted" implementation are all at the risks to burn and fail all at the same time at the next asn1 buffer overflow (openssl, i'm looking at you). And also you'ld have to consider the bindings for haskell, which is all new. Truth is all the crypto available in haskell are fairly new in a sense, and those scaremongering just doesn't help anyone. Of course shit happens, which is why you should consider the risk of what we're doing here: we're not sending state secrets or shifting millions across banks account (yet), we're "just" in the non-repudiation/integrity field. The good thing about this, is integrity isn't necessarily a crypto only field, there's lots of other thing that can be done on many layers to assure it. So overall i think this risk is relatively low, and while i'm biased, haskell implementations are just a fine way to go.
A bit of blue-sky-thinking-out-loud. Imagine if you collected the "wrong" answers to the interactive parts, removed differences due to whitespace (and more if possible), and then threw the most common errors over to a community site - something along the lines of stackoverflow - with the goal of providing either hints or explanations to the common wrong answers. I have no idea if the typical errors would be tightly grouped into categories that would make that easy - which might offers some insights into haskell training either way - or if the community would get behind it.
I doubt he meant that it's entirely impossible to represent "0.3" accurately, after all, I just did. I think he meant that it's not possible in *binary floating point*, which is what the entire comment was about.
They are used for financial calculations all the time; Excel uses 64bit FP. 
Yes, I have only recently worked out that greedily consuming space after a token is the way to do it. Using applicative style really helped too. 
I learned very early in my programming career to *never ever* compare floating point values with “==”. They aren’t exact. You’ll end up in big chaos when you do. Instead use things like “x &gt; 2.9 &amp;&amp; x &lt; 3.1”. Or round it to an integer, before doing the “==”.
it's nice but also slow and you can't use it in unboxed arrays/vectors (as it uses `Integer` internally)
No, he meant “i drew”.
As I said, it's not intended for the threat of a break-in on the server. The threat it is addressing is to detect changes that happen between the data leaving the hackage server and being used by the end user.
The reason is that, in Haskell, literals such as `0.2` and `0.3` are polymorphic. This means that their type depends on the surrounding context and is not always Double (as is common in other language) type is 
Noone said yet but the basic rule of thumbs for floating point arythmetic: 1. Don't compare for equality. Only powers of two and integers (up to 2^53) have exact representations so if you use anything else you might introduce errors 2. Avoid having operations between two numbers with very different magnitudes (a big one and a small one), since this can lead to rounding errors. (try adding one to a number above 2^53 and nothing will happen) 3. Don't use floating point for money!
No, they don't. Unless you turn on this feature, but that's not the norm. They do have a twisted version of addition which rounds small values to 0, which adds to the bizarre of Excel. 
It was just an example. I don't actually do this...at least very often. I have done it once in an AI robotics project where we abstracted out a few functions so that they were built from interpreting a PMML file that was sent to it from a server that was performing the model training. In scheme, hotloading a new file during runtime was trivial. In every other language we tried, we had to restart the runtime. I'm sure there were ways to do it, but we gave up after realizing how difficult and verbose it would become. 
&gt; can user defined types be inferred Yep! The most basic type-system that Haskell's type system is built on is called [Hindley Milner](http://en.wikipedia.org/wiki/Hindley–Milner). In HM, every expression can be given a type even in the absence of explicit type signatures. Now, Haskell's type system goes beyond vanilla HM, so there are some corner cases where you have to explicitely tell the compiler the type of a sub-expression, but this is very rare. 
I don't mean accidental corruption.
Sticking with the internal-approach for a sec, how about something simpler: checksum the entire .tar file and append the signed checksum in a new tar entry at the end. As far as I can see, there's no need for the checksumming to take into account the structure of the tar file at all.
Oh, surely. But I'm thinking of ease of use, not ease of development. There's usually a trade-off between security and convenience, and it's better to secure a small domain, strongly, than a large one, weakly. If we limit the packages for which we require signing, community trust, and so on, we can afford to be more draconian about what we do require in those cases. [This seems relevant](http://cristianobetta.com/blog/2013/02/02/ruby-gems-are-not-safe-to-use/): &gt; Yes, you can sign your gems but almost nobody does (and neither do I). 
Clojure is a Lisp dialect, so its grammar should be pretty simple. But in a more complicated grammar like Haskell's, I can imagine that something like `#_` would be much harder to formalize.
Do you still want to add the signature to the package tarfile? If so, you'll have to jump through some hoops to deal with the two-zero-records trailer.
Sure. That's the point of this package.
So what's the signing strategy going to be? 1. Gunzip the package to get the tarball. 2. Produce a detached signature for the tarball. 3. Append the signature to the tarball. 4. Gzip the new enriched tarball. 5. The gzipped enriched tarball is the signed package. If it's something like that, you'll have to be careful when verifying the signature that skipping the signature file itself within the enriched tarball doesn't open the door for attacks.
Parsec isn't really what I am describing, I don't think. Basically, I am talking about an algorithm that implements KMP, BoyerMoore, TurboBoyerMoore, Hash based searches, etc. I.e., a library of specific algorithms implemented in Haskell.
The strategy is: **Signing** 1. Just call `pgp --sign` on the .tar.gz file, then add the resulting .sig to the archive. **Verifying** 1. Open the .tar.gz, take the .sig and strip it from the tar archive. 2. Write out a .tar.gz.stripped file, which represents the original .tar.gz before signing. 3. Run `pgp --verify the.sig the.tar.gz.stripped`. Implemented in cabal-sign-0.4.0.0. I don't think skipping the signature file will be a security problem. If there's a projectname.sig in the archive in the first place, then verification will simply fail. I could write a filename like $md5_of_the_tar_gz.sig but it's probably overkill.
In step 2 of the verification are you recompressing? I don't think we can guarantee the compression is the same everywhere (so in this case between the machine where we signed and the machine where we verify). The C lib can change, we could decide to change the compression level in a different cabal-install release. Can we use the checksum of the original .tar (rather than .tar.gz) instead?
I just noticed your new commit on GitHub. It looks pretty reasonable. I'd still recommend verifying that there's only one signature, though. Instead of (find isSig) maybe something like: case partition isSig entries of ([], _ ) -&gt; error "unable to find signature" (_:_:_, _ ) -&gt; error "found multiple signatures (attack!?)" ([sig], entries') -&gt; {- ... as before ... -} Also, you're signing the original tarball but verifying a presumed identical post-stripped tarball written by Codec.Archive.Tar. Since (Tar.write . Tar.read) won't necessarily produce byte-identical output for all original tarballs, however, you may get false verification failures. P.S. Would you rather that I throw these comments up on GitHub or continue posting them here?
As the code is written now, if there are multiple signatures, only the *first* will be used for verification, but *all* will be removed before generating the stripped tarball to verify, and yet only the *last* will be left on the filesystem after the package is expanded post-verification. So an attacker could take your original signed package, modify its code to insert an attack, sign the corrupted package with a stolen key, and then append your original signature onto the new signed package. The corrupted package would then verify as valid (against *his* signature) but have a post-expansion signature on disk that would match your originally published signature byte-for-byte. Unless end users (or automated-install scripts) paid close attention, they wouldn't notice that the package had been verified against some other signature, now hidden from view.
Hi. This may be a bit less ambitious, but one thing that would be nice to have is a unix diff implementation in Haskell (as a library). I think I saw an almost ready-to-use implementation in darcs, but it has to be split off, cleaned up, tested, made configurable and released as a separate package. I've been meaning to do it myself, but haven't got around to it yet. If you think this is something you'd enjoy spending some time on, drop me an email at roma@ro-che.info and I'll be happy to help/mentor.
I'm having a hard time seeing where the actual security risk is. But I'm happy to just add the check rather than have you produce a test-case to demonstrate it. We can take the rest to Github.
 I was shocked to find that hackage packages can be changed by anyone. It had never even occurred to me that this was the case. What sort of timeline are we talking about for Hackage 2? (roughly: 1 mth, 6 mths, 1 year?) It's just that package signing is great but at this point hacking in a quick-n-dirty basic username/password setup to Hackage 1 would be a significant improvement unless Hackage 2 is out RSN Package signing will only help if it's mandatory (I like the idea of it being mandatory for the Haskell Platform, with some reduced requirements for other libs) but surely it comes after implementing some very basic auth? 
Yes. That's why I wrote a few comments up that an attacker could "sign the package with a stolen key." Since there are a lot of package authors, we can expect there to be a lot of trusted keys. So we don't want an attacker who has stolen *one* of them to be able to also compromise packages normally signed with *other* keys. If an attacker tries, we want to force him to change the on-disk signature file that's left after a package is expanded. That will make it more likely that someone will notice that the signature file has been changed w.r.t. the one published upstream. 
In both cases it would say: gpg: Good signature from "Mr Other Person &lt;other-guy@not-the-right-one.com&gt;" The `cabal-sign verify` command doesn't remove signatures from the given .tar.gz, it just reads from it. * In the two-signature case there'd be _two keys_ inside the archive, not one. * In the single signature case it'd just be a single signature and wouldn't look odd. So an attacker would actually have a better chance to just replace the signature rather than add another signature, which would raise suspicions (if the user didn't look at gpg's output).
Is this the stuff tied to htf? Or a separate project?
I'd like to see this as a standalone library, but it'd definitely be useful in many testing frameworks and libraries. I would use it in test-framework-golden, in particular.
&gt; The counter part of that is that everyone using the same "vetted" implementation are all at the risks to burn and fail all at the same time at the next asn1 buffer overflow (openssl, i'm looking at you). The counter point to this is that a vulnerability in OpenSSL is so valuable that no one is going to burn it on compromising Hackage. A vulnerability in a small Haskell library on the other hand, not so much. Value also drives the likelihood of finding a vulnerability due to the number of people that have searched for it, i.e. why it harder to find gold than copper. &gt; So overall i think this risk is relatively low A PhD who *invented* a [cryptographic scheme that is state-of-the-art](http://en.wikipedia.org/wiki/Scrypt) made [a simple implementation error in his crypto system](http://www.daemonology.net/blog/2011-01-18-tarsnap-critical-security-bug.html) that resulted in a critical security vulnerably. Writing cryptography is hard and experts routinely make errors. I linked to four such instances in my original post. Asserting the risk of an implementation error in a cryptographic library is relatively low is not only wrong, but irresponsible. I'm a huge Haskell fan too, but cryptographic libraries are an area in which you do not want to mess around.
In the *verify* function, take a look at [line 92](https://github.com/chrisdone/cabal-sign/blob/master/src/Main.hs#L92). It looks like you're extracting just the *first* signature from the signature-enriched tarball to use in verification. And yet, in [line 95](https://github.com/chrisdone/cabal-sign/blob/master/src/Main.hs#L95), it looks like you're stripping *all* of the signature files from the signature-enhanced tarball when you generate the stripped tarball that's verified against the extracted signature. So if there are two signatures, the code will extract the first and use it to verify the tarball made by stripping both. Have I got that right? If so, here's the attack. Let's say there are 25 widely trusted keys, yours among them. And lets say that an attacker has stolen one of them but not yours. Now you release your package PerfectAttackVector that the attacker would just love to corrupt. He downloads your package and changes some code. Of course, this breaks your signature. So he signs the corrupted package with the stolen key. And now it's got a trusted signature, so it will pass automated verification checks. That is, when *cabal-sign verify* outputs gpg: Good signature from "Mr Other Person &lt;otherguy@nottherightone.com&gt; many people aren't going to see it (think automated installs) or know enough about package authorship to know that Mr Other Person isn't the the right person. But, fortunately, when the package tarball is expanded during installation, the signature file will end up on disk. Where someone might see it and notice that its sha1sum doesn't match the one published upstream. And raise the red flag. The attacker, naturally, doesn't want this to occur. He would very much like to hide any on-disk evidence of his trickery. So he doesn't just sign the corrupted package with the stolen key. After signing it, he modifies the tarball to append *your original signature*. As we already discussed, this signature will be ignored during verification. But it will not be ignored by tar during installation. It will end up overwriting the attacker's signature. So he will have effectively covered his tracks on disk. The on-disk signature will match the one published upstream and look 100% legit. Make sense?
&gt; I have been lecturing for many years in Bolivia, where over 50% of the audience was female, and they liked what they learned, they were good at it, and I see no way why this should not be the case in at least the place where I am coming from. In poorer countries with less gender equality, more women tend to study STEM-fields. The explanation that has been given for that is that STEM-jobs pay more, and getting a job that has the allure of more money is more important in poorer countries, since more money means more freedom. And more freedom means that you are less dependent on a man, assuming that the society isn't very gender-equal. In the developed world, however, there are more jobs that pay a decent middle-class salary, plus there is often more welfare programs and less social class differences (and more gender equality). The other side of the coin are countries where there is a high degree of gender equality. There the professions become more sex-segregated, presumably because people are more free to choose the work that they want (or so said the explanation given). 
Ah, the bit I was missing that the extracted tar will write foo.sig to file and then foo.sig again overwriting it because there are two. So I see that the attack hides on-file evidence of the supplanting. The supplant of signatures is the actual attack, and the covering of tracks is the advantage gained by dual signatures. So yeah, I don't see harm in throwing an error on duplicate signatures if it helps a bit in the case of a stolen private key.
Optimized edit distances for fuzzy matching, including Levenshtein and restricted Damerau-Levenshtein algorithms. http://hackage.haskell.org/package/edit-distance
&gt; I was shocked to find that hackage packages can be changed by anyone. It had never even occurred to me that this was the case. It's bad but not quite as bad as what you're thinking. Hackage currently does have very basic auth for people uploading packages. The issue is that any of the 1000 or so registered users can upload any package (this is why the signup procedure is currently manual). The new server adds a per-package maintainer group, and you have to be in that group to upload new versions of the package. In addition it uses a somewhat more secure authentication mechanism (http digest auth rather than http basic auth).
Does leksah work with cabal-dev or hsenv ? 
Feel free to ask as many questions as you like! For example, let's say that you define: data MyType = T Int String Then if you write: if foo (T 1 "Test") then ... ... the compiler infers that foo has type: foo :: MyType -&gt; Bool The compiler deduces the type of every single value in the program as part of the type-checking process.
I cannot claim to be any sort of guru in lisp, but I've found Haskell to be far more powerful and expressive. Contrary to lisp ideology, a large part of that power comes from having types. In general, expressivity is a matter of ensuring what cannot be said (just consider how context-free languages are more powerful than regular languages--- because they allow more restrictions). If what you *really* want is metaprogramming (i.e., staged computation), Haskell isn't especially remarkable. However, I'm guessing that like most lispers, when you ask for metaprogramming what you really want are other things which Haskell does quite well.
There's no such thing as a "completely new function". In every language, including lisp, everything has to eventually come down to some finite set of primitives. The only question is what those primitives are and how you can combine them. This isn't metaprogramming; it's just programming.
User-defined types are the same thing as built-in types. Implementation details may differ, and some types may not be user-definable (e.g., unboxed primitives), but there's nothing special about their status as types. All types are inferable until you get into advanced territory like rank-N types, GADTs, or multi-parameter type classes without fundeps. That last one just introduces ambiguity you may have to resolve; the first two start hitting fundamental undecidability issues of logic. That said, in practice you generally do not want the most general type anyways; usually it's some more specific type you had in mind, and that's why it's good to give type signatures even when they could have been inferred.
It's in the readme &gt; Q: Can I use it together with tools like cabal-dev or capri? &gt; A: No. All these tools work more or less the same (wrapping cabal command, setting GHC_PACKAGE_PATH env variable), so something will probably break.
&gt; I don't think a required use case is necessary for an explanation of a tool. Bollocks. A computer is a tool. I have some problems, please explain to me whether a computer is suitable for my solving my problems. Oh, don't worry about my exact use cases, I have a bunch of problems, I just want to know if a computer can help. The problem with lisp is that lisp itself is functional assembly code. Nobody wants to write assembly code, so all manner of macro systems have cropped up so that people can write code at a reasonable level of abstraction. In lisp, you don't write lisp, you write macros. Because lispers conceive of the macro systems as generating raw lisp code, that gets called "metaprogramming". In Haskell, we don't use macros because, in Haskell, we write Haskell. Sure, there's an underlying functional assembly language, but generating that is what the compiler is for. When we write Haskell code to tell the compiler what to do, we just call it "programming". Thus, whatever you have in mind is almost certainly something you can do in Haskell. But without examples to ground the discussion, there's no way we can say "oh, you do that with X", or "yes that's easy", or "no that's hard", etc. The short answer to your question is: learn Haskell. If you want a longer answer then you're going to have to come up with some example of what it is that other languages have failed to offer.
so, the editor is unusable (given you need to wipe your ~/.cabal and ~/.ghc when switching to another project due to dependency issues)? ):
&gt; Have you spoken to any women in IT about it? Or women who use haskell? Of course I have. They are the folks I alluded to, both those enraged and those heartened. You seem convinced that this is the only time gender issues have come up in our community. This is demonstrably false, however, as you will find if you search the archives of the Cafe. And previous disputes have driven participating women out of the community due to reactions like yours. Those of us who refuse to accept misogyny in our presence do not do so in order to "make women feel comfortable". I, for one, do it because I refuse to accept misogyny. It is not a matter of making friends or earning cookies or any such thing. Misogyny is intolerable, and as such it shall not be tolerated.
That is from the hsenv readme saying you can't use hsenv and cabal-dev. I think vagif wanted to know if you could use (leksah and cabal-dev) or (leksah and hsenv)
oops sorry that was a stupid mistake
What a pathetically childish response. First, your analogy is incorrect. A reasonably intelligent person can infer that a hammer can break glass without being told that a hammer can break glass. A computer is a device that performs multiple forms of mathematical computation. Using that definition, you should be able to name at least 5 use cases that I haven't explicitly mentioned. In fact, others in this thread have given me exactly what I was looking for, with explanations of Template Haskell and Free Monads, not to mention several links that I have already found useful. Your snark suggests that you actually think that lisp is a low level language, and that Haskell is obviously a much higher level. I'd be interested in your opinion as to how hundreds of thousands of first year undergrad compsci students have managed to learn "assembly" so well as to be able to implement a compiler in their first semester. How many have done so with Haskell? I wouldn't be here if I didn't think the language was worth learning, but let's not kid ourselves here...its not an easy language. I came here to learn about Haskell. Thankfully, some people here have been incredibly patient and have tried to explain this difficult language. I can imagine it is difficult for them to do so too, given that almost every concept explained requires new vocabulary that doesn't exist in 99% of the languages out there. Understanding how regular Haskell can accomplish tasks that would be called metaprogramming elsewhere has been difficult, but several people have tried to help and I'm grateful. However, the difficulty of the language is starting to seem like one of the smallest barriers to becoming a haskeller. Dealing with the snark like yours was completely unexpected. I have no formal computer science training and everything I have learned has come from books, blogs, stack overflow, and reddit. This is the first time I have felt openly mocked for not understanding something immediately. I'm sure you don't care about what it feels like to be in my position, nor do I expect you to given the fact that you were writing cuneiform monads on the walls of your womb. All I can say is that maybe it isn't worth the humiliation to try to be a part of your exclusive club. 
I use it on a Mac some times, though I do prefer using it on linux. Have you tried the Gtk 3 version? It has a dark mode (View -&gt; Dark) and can go full screen. Use Ctrl+T to hide the tool bar and it looks quite nice... http://leksah.org/images/LeksahOSX.png Latest dev builds are these... http://leksah.org/packages/leksah-0.13.1.2-ghc-7.0.3.dmg http://leksah.org/packages/leksah-0.13.1.2-ghc-7.0.4.dmg http://leksah.org/packages/leksah-0.13.1.2-ghc-7.4.1.dmg http://leksah.org/packages/leksah-0.13.1.2-ghc-7.4.2.dmg http://leksah.org/packages/leksah-0.13.1.2-ghc-7.6.1.dmg 
**hsenv + leksah** I have not tried this but you could try running leksah in your hsenv. You will need to exit leksah and rerun it to switch hsenvs. **cabal-dev + leksah** I have just pushed this... https://github.com/leksah/leksah/commit/f56bda87f015a5fc40c4475875dc29399e51f97c To enabled it go to the Build section of the preferences. It replaces "cabal" with "cabal-dev" in all the build commands. Please let us know how you get on.
&gt; I'd be interested in your opinion as to how hundreds of thousands of first year undergrad compsci students have managed to learn "assembly" so well as to be able to implement a compiler in their first semester. I've taught hundreds of undergrad compsci students Haskell so well they've been able to implement a compiler in their first semester. I don't see how this substantiates your argument. The elegance and annoyance of lisp (well, scheme) is the only real abstraction you have is the lambda. Everything else is implemented based on macros. In this sense, it is very "low-level" in that the language does not have the expressive power built-in, but instead is built-up by the programmer on an as-needed basis. This makes a highly flexible, but also perhaps _too_ flexible language. Haskell is based on the notion that the language exists as a development aide just like everything else. You don't need to come up with solid abstractions from the nuts and bolts (lambdas and syntactic macros), the language provides abstraction capabilities built-in which are remarkably powerful and able to cover most use cases. This enables the language to provide general _guarantees_ about correctness and behaviour in a way that Lisp cannot. For example, Haskell uses its type system to provide a variety of guarantees about run-time behaviour. Some of these guarantees are very strong and powerful aides to reasoning. In Lisp, if I wanted a static typing system, I'd have to implement one, and there's no way the compiler can force me to use it, meaning that a programmer that wished to violate those guarantees would have no trouble doing so.
Cool, thx for the quick response. It looks from the patch that it only fixes the compilation. What about library discovery? You know how you tell leksah where the lib sources are and then it shows you cool tree of available llibs/functions etc? 
IIRC Leksah calls "ghc-pkg list" to get a list of packages for the "System" metadata. With hsenv it may work as it is (because I imagine "ghc-pkg list" will still return a list of packages). However Leksah only keeps track of one source location for a given package name and version. So you will may need to "Update system data" or possibly "Rebuild system data" when switching environments. Unfortunately for cabal-dev I don't think the current metadata will work very well. The libraries in "ghc-pkg list" should show up in the metadata. But I suspect, as it stands, the rest may be broken. I am not sure if it will pick up the workspace metadata.
 Ah! That's not so bad. Not ideal, but with manual signup, it's ok. Thanks for the clarification
It would be more correct to say every monad gives rise to an arrow. They aren't literally the same type constructor in the sense that, say, all monads are also functors.
Which algorithm is in darcs? Patience diff?
No, I think it's a simple LCS diff. [Here's the code.](http://hackage.haskell.org/packages/archive/hashed-storage/0.3/doc/html/src/Storage-Hashed-Diff.html) It would be great to have patience diff as well!
I came here pre-convinced of the benefits of static typing. But I also came here because Haskell's learning curve is more like a learning cliff-hang. Learning scheme, I had memorized the syntax in about half an hour, and in two weeks had implemented more than a couple optimization algorithms that I use at work. You may be right that lisp only has one abstraction, but if I can accomplish that with one abstraction, then it is a pretty damn powerful abstraction. In my two weeks learning Haskell, I havent been able to accomplish much more than an extension of a Hello World. And that is coming from someone that didn't need the imperative or object oriented mind shift to be able to understand things like pure functions, higher order functions, recursion, or immutability. Like someone else said here, understanding functional programming is like starting on second base instead of first. My argument is not that Haskell is bad, low level, or inferior to lisp. It is that Haskell is complex and difficult. I came here looking for an improvement to lisp in the same way that I found lisp to be an improvement over VBA, Python, and R. I still think it is possible that Haskell is an improvement, but I also think that in order to get there (if it exists), I'm not only going to have to overcome the difficulty of the language, but also the condescension of the community every time I ask a question or dare to bring a concept from another language with me. 
Indeed, many of the problems with priorities have been solved, it just seems like a hack in a principled language like Haskell. A numeric priority is an abstraction that is at least one or two levels of indirection from the global property you *want* to specify, like a latency constraint, or a throughput constraint. It just seems like a cheat, where Haskell has rarely taken the easy way out, eg. monads.
I notice you're using the lcs package? Have you looked into Diff instead? http://hackage.haskell.org/package/Diff
`(==)` *is* exact. It's the other operations that aren't. Also, most operations are specified to be exact under controlled circumstances. You just have to understand the spec.
Lots of downvotes for a simple misunderstanding. It's not as if you were even snarky about it. I wish haskit would be less downvote-happy.
I contributed to StringSearch. But it is now long in the tooth. I expect current GHC's will benchmark differently than when the library was written. Thus someone could do a new benchmark &amp; profiling pass.
A pretty big penalty.
Be my guest
(Continuing on the blue-sky-theme...) Furthermore, you might use this to create a database of "potential fixes" that could be used to improve GHC error messages. That is, one might record both the "wrong" answers and the eventual "right" answers, throw the thing at some sort of machine learning black box and try to infer rewrites. Then, rather than the baked in error messages, you could /also/ report a suggested "fix" based on the inferred rewrite rules. A kind of "crowd sourced" error messages. 
Many of today's prominent writers (who write great blogs, books, etc.) do so either to help the community, or to build their professional reputation, or because they love teaching &amp; writing, or because they are already educators by profession, rather than hoping to be paid by their readers. But we'll consider adding the feature if a lot of people want it. Certainly the prospect of payment has led a lot of people to write apps for smartphone app stores.
It's more downvoted to avoid misinforming the person asking the question I think. If I see an answer to my question with a lot of downvotes, I know right away to avoid it.
Thanks for the links. They look interesting. 
I agree.
Now that you've shown yours, I'll show you mine: comes with LLVM and direct IA-32 compiler: https://github.com/gergoerdi/brainfuck/tree/master/language-brainfuck
I do indeed think lisp is a low-level language. The language is, afterall, the untyped lambda calculus plus a few primitive types (and CLOS if you're in CL). There's nothing wrong with being low-level. Indeed, the quip about lisp being a low-level language is one that I've picked up from lispers themselves! The lispers I know revel in that fact, it's part of what they love about the language. I never claimed that "Haskell is obviously a much higher level". What I did claim is that you should rethink your framing of things. Haskell is not meaningfully comparable to lisp (i.e., lisp sans macros). The lisp-sans-macros language behind Haskell is either Core or STM, depending on how exactly you want to make the comparison. Haskell itself is comparable to the macro system. This is part of the reason why people are having such difficulty answering your questions. What you call "metaprogramming" you call metaprogramming because you're working in the macro language to manipulate the core lisp language. That's not how we view things in Haskell. Because we view Haskell as the language itself, not as a mechanism for manipulating Core or STM, when we write Haskell we just call it "programming". Given this, yes, I suppose Haskell is higher level than lisp sans macros--- but that's not the point, that's really not the point. The point is, there's a terminological clash because we look at the world differently. And this terminological clash is why noone can figure out what you're asking for. Numerous people have asked for clarification and, with the one exception of the `(-&gt; a b (f c))` example, you have not been forthcoming. You then accuse us of not answering your questions adequately, which indeed we haven't, because we don't know what those questions are. A programming language, just like a computer, is a bit more complicated than a hammer. We could describe it's uses but we'd be here all day. I do apologize for being rude. But that rudeness stems from the arrogance and superiority lispers show whenever entering other language communities. If you're interested in learning Haskell, if you really are interested, then you must leave behind the preconceptions you've learned from lisp. This is not a claim that lisp is wrong or inferior or any such thing; it is merely a claim that Haskell is different, and that you must be willing to confront things in their natural habitat. Whenever interested people come from languages like C or Java, we give the same advice: leave your preconceptions behind. It is only after learning Haskell that you can go back and make the comparison with your language of choice. I believe this is true of any language worth learning, but it's absolutely true of Haskell. Haskell has (a) a type system worth discussing, (b) actual type safety, which forces coercions and side-effects to be explicit, (c) ubiquitous implicit laziness, (d) currying by default, including native support for partial application as well as over-application, (e) algebraic data types, and (f) type classes. While none of those may be truly unique, all of them are quite rare, and the combination is unheard of outside of Haskell and languages which are very similar to Haskell. Ubiquitous laziness is especially alien to anyone who doesn't already know a Haskell-like language. This laziness means that we can implement our own structures for control flow. In lisp you'd do that with macros, so you say "who cares?" But this isn't metaprogramming. In Haskell you just do it. Those control structures are functions, just like any other function. Nothing special to it; it's just programming. Similarly, if we want to compose functions, then we write a function like `(.)` do do it. Yes, this operator is "building new functions from old ones"; no, it isn't metaprogramming. The fact that we can define `(.)` so easily stems from the native support for partial- and over-application. If you came from Java, I could explain how type classes give you dependency injection while still being considered a banal day-to-day thing in the language. Or I could explain about how polymorphism solves all sorts of problems you needed to work around manually. There's nothing special about defining control-flow functions, about defining function composition, about adding an `Ord` constraint, or about defining polymorphic functions; it's all just run of the mill Haskell code. That people ascribe any special significance to these things is because they're coming from a language that makes them hard to define. I could enumerate other examples, but without some guidance about the examples you'd be interested to hear, I'd only be boring you silly.
You're welcome!
I thought this was an Onion headline when I saw it in my feed.
You can [write Scheme in Haskell](http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours) so it shouldn't be too much trouble ...
Oh, certainly. I'm just saying that writing a good tutorial is a whole lot of work and few people will tackle that if it goes unsupported. I've asked Phil Wadler once whether he'd write something for the Haskell Wikibook, but he declined, even though he has written many tutorial papers. Makes sense, though: researchers earn "academic street cred" by writing widely cited papers, and while his tutorials would be a great read, I think he'd rather spend his time on writing equally interesting articles. SPJ called it "academic brownie points". (An app-store like paywall is probably not a good idea, hence my suggestion for [Flattr](http://flattr.com).)
&gt; Leksah is horrible-looking and unreliable on any mac, because of its use of GTK. Agreed. But I'd go further than that: any project that acts primarily as a text editor, however attractive or usable it might be generally, should not bother with OS X unless it employs native text entry. Emacs emulation is usually a pile of utter sadness — Eclipse, Intellij, Komodo, all of them fail to match the Emacs-ish bindings offered in text widgets across the OS. And it confuses my poor fingers mightily. (Vi users presumably suffer in comparable ways, although since such bindings are not ubiquitous across the OS I suspect there's less opportunity for confusion.)
Any plans to integrate hlint with Leksah?
Pane -&gt; HLint shows the output of HLint for all your source folders and will jump to the location of each hint when you select it. No automatic fix-up yet, but it would be a fun project to add. EclipseFP has an implementation in Java that might be a good guide for how to do it (haskell mode probably has a LISP version).
The versions with and without an explicit lambda perform the same. But you're right, that got lost in a refactoring and more explicit is better.
I can use IntelliJ at least without difficulty on OS X.
I 100% agree that you don't know lisp
I'm the author of vty-ui; please let me know how it goes! I'm happy to discuss pull requests and library changes on the Github page at https://github.com/jtdaugherty/vty-ui. Also, be sure to build the demos as they should be a good starting point for showing what the library can do.
Your site did in few hours what I couldn't do for past 1 year. Awaiting for more chapters/lessons. Thanks again.
I only understand ros modestly and from afar, but as Saturday9 said, it seems like it has weird stateful quirks and quirky impure semantics. So my sense is one can only paper over that so much while writing code that's idiomatic to it.
http://shuklan.com/haskell/lec04.html#/0/4 from http://en.wikipedia.org/wiki/Caesar_cipher Plaintext: the quick brown fox jumps over the lazy dog Ciphertext: wkh txlfn eurzq ira mxpsv ryhu wkh odcb grj solution A: λ cipher "the quick brown fox jumps over the lazy dog" 3 "wkh#txlfn#eurzq#ira#mxpsv#ryhu#wkh#odcb#grj" solution B: λ cipher "the quick brown fox jumps over the lazy dog" 3 "wkhqtxlfnqeurzqqiraqmxpsvqryhuqwkhqodcbqgrj" solution C: λ cipher "the quick brown fox jumps over the lazy dog" 3 "wkh#txlfn#eurzq#ira#mxpsv#ryhu#wkh#odcb#grj solution D: λ cipher "the quick brown fox jumps over the lazy dog" 3 "wkh^C^C^C hmm.... WTF???
Nope, it's only slides.
One of the very interesting things about Elm, and some other implementations of CFRP, is the fact the Signal type is *not* a monad, but **only** an applicative functor. It's a useful limitation, both for simplicity of language implementation (as expected with a static signal graph, and event loops are a proven pattern) as well as as proving properties about the programs (as expected, when you consider the benefits of proofs in more limited logics).
non-ascii will get very messy (or unreadable). you are right, that it should handle them though.
Note that this is a 'Java JIT', not a 'JIT for Haskell'. For that, you should check out Thomas Schilling's work.
Great work! We need more VM examples implemented in safe languages. Already have reading material while commuting. :)
I would guess that once you are emitting x86 instructions and jumping into them some of the safety goes out of the window
You'd guess wrong, especially if you can build the proper abstractions. See also http://edwinb.wordpress.com/2012/11/25/whitespace-in-idris/
Among the three I mentioned IntelliJ does best, Komodo worst. The IntelliJ editor annoys me primarily in its failure to recognize the remapping of caps lock to option, a problem that may be idiosyncratic to me.
With a lot of work you can make it very safe: ["Verified Just-In-Time Compiler on x86"](http://lambda-the-ultimate.org/node/3768). At some point in the future I might consider trying to do something like this myself. However, invariants for a real JIT compiler are rather complicated (especially if it includes the GC), so I wouldn't say it's practical to encode them in Haskell's type systems.
Looks like fun project, nice work! Was there a particular reason to use Hoopl instead of LLVM?
Just curious about how promising is using haskell in parellella. This looks like an interesting testing platform of various concurrent/parallel programming concepts haskell has explored recently. 
According to that forum the most promising direction is GHC -&gt; LLVM assuming someone will build "epiphany" support for LLVM. So I guess there's two projects to contribute to. I'm assuming that your code won't need to know anything about the epiphany architecture. I'm imagining that you would code using repa and par monad and friends and the backend works out the rest. 
Now the question is when we can get a usable Haskell for android...
We get custom literals of a sort with algebraic datatype. Pair that with arbitrary custom operators and you get pretty close I think. If you want to go further, there's the unicode language extension.
You mean Harpy, right?
The existing answers are about overloading existing literal syntax for e.g. numbers and strings. You can also add arbitrary syntax within quasiquotation blocs. This is used in, e.g., pads-haskell: https://github.com/GaloisInc/pads-haskell
Also, note that function application syntax is pretty lightweight in haskell. So arguably `cm 10` is not terribly noisier and more intrusive than `10cm`.
No, I mean (Hoopl + Harpy) vs LLVM. You're right, though. By using Hoopl you can keep a little bit more high-level information which may come in handy for some optimisations, but translating directly from Java bytecode to LLVM IR would be much simpler. I personally found Hoopl very annoying to use, especially since you have to move outside of it for doing register allocation.
I would need to learn more words in order to understand this post.
Really? I often get lost in posts that go heavy on type theory (especially on Greek characters), but had no particular trouble following this one.
&gt; I personally found Hoopl very annoying to use Relative to what? Just relying on LLVM?
Can you link to a specific example of that arbitrary syntax being used inside normal Haskell?
You could use -XPostfixOperators and then it's literally just: (10 `cm`)
I'm the tintin guy, I've only looked at it superficially, and right now I've been busy trying to get some BB10 apps out the door. I was thinking jhc was nice, cause it matches to pthreads, but there is no OS on the epiphany, so what good is that. I do not know enough about the respective compilers and rts. When I get the parallella, I will put some more effort into it, or try to program in ATS. 
I've considered it and will eventually have some hardware. Some day I'll have time to play around. The first priority at the moment is getting GHC reliably running on ARM. At very least, I can see an approach like accelerate's approach to GPGPU working well here.
There is no need for RebindableSyntax; overloading fromInteger and fromRational are done like that already. Using RebindableSyntax just lets you redefine fromInteger or whatever to be /anything/ - rather than just providing instances.
Please do! I tried to install GHCi 1-2 months ago, but failed. Maybe the time is good to try again. edit: I have resumed my attempts to install GHC 7.6 on my Raspberry Pi, but I still have no luck.
We do have a working [cross-compiler](https://github.com/neurocyte/ghc-android)
Not really. I'll allegedly have two boards here in a few months time. Their SDK is out but with no emulators or other toolchains available for public use (and this greatly confuses me,) it's really not worth getting hyped up about right now, IMO.
It can have a very intimidating API, the only real reference for use is A) the paper, B) a sample in the repo and C) GHC itself. The API tries very hard to be type safe and it mostly accomplishes that. But the combo of RankN (for N=2) typing and GADTs means you lose some inference properties. The type signatures can get a bit long, so I end up `type` aliasing things a lot for specific monads or graphs or whatever. But it also means the fully polymorphic type signatures look like hell sometimes. Type-safety at that level is great, but it's another thing to become keen to. It doesn't really have a story for performing advanced things like interprocedural optimizations from what I can tell. This could probably be done, but the library works pretty heavily on the assumption you are dealing with a singular (closed) graph of basic blocks at a time. Essentially, at function-scope. I guess if your program is one graph, that's OK. GHC doesn't really suffer from this because a lot of 'whole program-y-ness' can 'basically' be achieved in its current scheme of stuffing unfoldings in interface files. And this is done at the core level, so the simplifier/inliner is really where all of the 'interprocedural optimization' magic happens, you could say. Hoopl only touches very low level imperative code that comes later. But compilers generally need a very different combination of optimizations, at different scopes. This approach works for GHC, but for example, the GRIN IR is specifically tuned for whole program analysis and compilation, using a points-to analysis (go read Urban Boquist's original PhD thesis.) GRIN is certainly amenable to optimizations before and (especially) after this analysis, and Hoopl *would* be great for this since GRIN is very imperative. But Hoopl would provide no way to do points-to, from what I can see. This is annoying, because it otherwise tries to capture the idea of dataflow influencing rewrites, incrementally. It's just not the right scope. Finally, the same kind of idea applies to just something like register allocation as `nominolo` said. I actually just ran into this because I wanted to use Hoopl. For example, I can certainly figure out all the live intervals of a function's body for doing some basic optimizations. But I also need live interval analysis if I want to do something like, say, linear scan register allocation. But while I can use it to *do* a liveness analysis with no rewrites, I couldn't entirely figure out a way to structure a pass to perform allocation over a set of basic blocks, too. This isn't very hard to write otherwise, though, as the above paper shows. But you can potentially do more optimizing after that, even then! (like sinking stores further, if possible.) And so on.
Heh, that's a pretty clever name! Best of luck to the attendees and organizers.
some reasons: * JVM -&gt; LLVM already [exists](http://vmkit.llvm.org/publications/ladyvm.html) (aka. LadyVM aka. VMKit) (not Haskell though) * It's more fun of course, for example implementing register allocation * [Harpy is much faster than LLVM](http://www.reddit.com/r/haskell/comments/1014ct/runtime_codegeneration_in_haskell_harpy_vs_llvm/) regarding compile-time * Some implementation details. For example, I don't know how to force hardware exceptions in the generated code (e.g. segmentation faults). Of course, other techniques could be applied. * I like to fiddle with low-level stuff :) But yeah, I also had hard times with hoopl. But I can't say if it would be easier to write passes in LLVM---the advantage of LLVM is, there are already a bunch of passes out there. btw, your code was very inspiring for me, especially the register allocation part :) In fact, lambdachine is the only hoopl client I found in the wild (except GHC).
Ah, sure, with callcc I meant Felleisen's C, which, confusingly, I've also heard it called callcc.
I think the [haskellwiki/Quasiquotation](http://www.haskell.org/haskellwiki/Quasiquotation) might be a better reference... Example from the [regex quasiquoter](http://hackage.haskell.org/packages/archive/regexqq/0.6/doc/html/Text-Regex-PCRE-QQ.html) linked there: ghci&gt; maybe [] tail $ [$rx|^([+-])?([0-9]+)\.([0-9]+)|] (show $ negate pi) ["-","3","141592653589793"] But judging from the note on the wiki page, I'm guessing that with ghc 7+, the `$` in `[$rx|` would now be gone.
I wanna try it on my raspberry pi.
Oh, yeah, I hated writing that register allocation code. It felt like fighting windmills. I guess the main reason was that you had to do it outside of Hoopl, but Hoopl only gives you the analysis result on the labels. For the actual allocation part, I tried to re-use GHC's graph colouring code, but that turned out to not handle pre-coloured nodes, so I had to write my own linear scan register allocation code. 
The [Yesod web framework](http://www.yesodweb.com/) allows you to include HTML, CSS, and Javascript within Haskell code. This is described in the chapter about "[Shakespearean templates](http://www.yesodweb.com/book/shakespearean-templates)" in the [Yesod book](http://www.yesodweb.com/book).
Now, where the heck is your blog post about that? You're making ton of good comments here on reddit that would be fine (not *perfect*, but fine) if posted on a blog unprocessed. And you could certainly spend more time reshaping them to a post format, if that does not hurt your productivity too much. We can absorb more "advanced introductory blogs" on type theory. Get a real space for your content! It would be getting more visibility and more viability, for greater good. I'm tempted to threaten you of writing self.{haskell,types...} posts with just questions. "Philip (or someone else), tell us what you think of Filinski's work". "Philip (o.s.e.), what's the relation between Zeilberger's and Munch-Maccagnoni's approaches to control and focusing?". That could be fun. Maybe I should just send that to the Theoretical Computer Science Stackexchange.
Seconded, would third if I could. The world needs more good type theory blogs. 
A related reference: http://en.wikipedia.org/wiki/Double-negation_translation And a great discussion of negative embedding from the cstheory stackexchange: http://cstheory.stackexchange.com/a/5263
Hmm, you may be right. I was thinking of http://www.haskell.org/ghc/docs/7.2.1/html/users_guide/syntax-extns.html#unicode-syntax
Ugh you are actually correct. I had already tried using the --disable-library-profiling option and I ran into errors when trying to view certain data types: "Warning: Maximum data structure depth reached, output is truncated Failure, trying again" I assumed this was because the libraries who's data types were giving this error were compiled originally with profiling. I just tested this, and ghc-vis behaves equally badly without anything being compiled with profiling. However, the point of config-select remains. The only thing faulty is my example usage case.
I'd suppose another syntax for named holes. What about `_{foo}`, for example? or `{foo}_`? Current syntax seems too confusing, with usual variable names.
That's awesome, thanks.
&gt; I always keep: &gt; &gt;&gt; executable-profiling: True &gt;&gt; &gt;&gt; library-profiling: True &gt; &gt; in my ~/.cabal/config IIRC, `executable-profiling: True` will considerably slow down your executables even if you don't run with `+RTS -p` (`library-profiling: True`, on the other hand, is fine because separate profiling versions of the libraries will be built and will only be used for executables with profiling).
I like it. Regular expressions was what I first tried to craft in C++ UDL, but ran into the backslash-escape gunk, which doesn't really make it worth it.
Thanks for pandoc, I use it all the time and love it.
[What is Classical Reasoning Good For?](http://queuea9.wordpress.com/2012/11/21/what-is-classical-reasoning-good-for/) is the follow-up
Cant input using any of the android browsers, guess I'll have to try on desktop later.
I have been able to get the base package cross-compiling. However, I have had issues with cabal. How exactly are you getting cabal to work?
Never [tried this](http://hackage.haskell.org/trac/ghc/wiki/Building/CrossCompiling#Usingcabal), but it's straight from the horse's mouth.
You need a full `Num` instance if you don't use Rebindable syntax. Then you'd need to provide various weird methods implementations for Bool besides `fromInteger`.
This was a great session. I'd urge anyone in London who's interested in Haskell to come along to hoodlums, you learn tons! http://www.meetup.com/hoodlums/
Odessa sounds like an awesome location. Looking forward to it!
I thought they were pretty nice. Haskell is on my "list to learn".
Yes I'd like to hear an answer to this too. The big insight to me was that using a comonad is like an fmap that also has access to the rest of the structure. Now you mention it though, I am struggling to see the wider utility. Anything the uses the rest of the structure must be fairly specific...
Well, the access to the rest of the structure would be similar to the `MonadFoo` typeclasses, but I don't know of any non-`Reader`-equivalent case where you would add more `ComonadFoo` constraints later.
awesome!
I genuinely do not know what you're asking, and your example makes no sense to me.
I think the question is basically: are there compelling use cases for abstracting over comonads? i.e., when would you want to write your own non-trivial code with a type something like.. foo :: Comonad m =&gt; ...
I'm asking why we have a comonad typeclass, other than for overloading reasons. A more slightly real version of my example would be: stuff :: (MonadState m Int, MonadReader m Int) =&gt; a -&gt; m b Basically, without fiddling with free monads or monad transformers everywhere, I can still get exactly the effects I want in a way that is easy to maintain.
Isn't that just an example of a specific comonad being interesting?
No less so than a Reader monad is interesting, or a List monad for another application, these were just specific examples. The broad point of the comonad structure being interesting is that it naturally describes history retention (or conversely, the ability to "look" into the future). If you're looking for some other kind of answer, you may want to rephrase your question a bit. It seems like others are interpreting your question the same as I am.
Next meetup will be announced officially soon, but will be on 7pm on Wed, Feb 27, and feature Gregory Wright discussing symbolic algebra in Haskell. I'm really excited about this one. 
Next meetup will be announced officially soon, but will be on 7pm on Wed, Feb 27, and feature Gregory Wright discussing symbolic algebra in Haskell. I'm really excited about this one. 
I think the property you're using—that lots of useful monads exist, can be layered, and have specific definable properties—*emerges* from there being lots of useful monads discovered. Stream transformers and generic programming are two major instances of comonads, but ~~perhaps we just don't yet have a well-stratified library of comonads to pick and choose and combine from yet~~.
This is a good point. Half a dozen years ago, we knew the math behind them, but no one really knew what it meant in the context of applying them to programming. Monads went through a similar transition, but a decade or two earlier.
It's an example which characterises a wide variety of comonads, with a little imagination (thinking about various sorts of cellular-automaton-like structures).
This is one of those talks where having links to the slides themselves is essential to 'getting' the material
Monads are an elegant way to solve to solve function a challenge with function composition. Haskell has not yet offered a widely accepted way for composing data structures. Current methods are very heavy on boiler plate. Haskellers have largely designed around this problem by embracing flatter data structures which can be easily decomposed. Comonads and Lens seem to be leading us towards a future of boiler plate free data structure manipulation and composition. ie. A future where it becomes bearable to manipulate a zipper of lists of tuples. This should lead to better abstractions and more complex programs.
So when is this going to Hackage?
&gt; Only powers of two and integers (up to 2^53‌‌) have exact representations so if you use anything else you might introduce errors What about 0.75? Prelude&gt; 0.75 == 3/4 True 
Multiples of powers of two are also ok (as long as the multiplier isn't too big). In this case 3/4 is a multiple of 1/4
They're important iff monads are coimportant.
http://hackage.haskell.org/package/comonad http://hackage.haskell.org/package/comonad-transformers http://hackage.haskell.org/package/comonads-fd http://hackage.haskell.org/package/comonad-extras http://hackage.haskell.org/package/streams http://hackage.haskell.org/package/free http://hackage.haskell.org/package/adjunctions http://hackage.haskell.org/package/kan-extensions These all provide comonads and comonad-transformers that stack just like the ones from the monad transformer side of the world. The trick is you use comonads very differently than you use monads. You tend to structure our program around one monad and many little comonads.
`wfix` is definable once and for all for all comonads. The thing is to 'chain' comonad operations in any way that is like working with a monad you need some way to stick the answers together, this leads you to to the notion of `ComonadApply` from the `comonad` package of the `ComonadZip` abstraction from Uustalu and Vene. That leads to a number of symmetric context merges that are quite useful for dataflow programming.
It works out that the dual of `Traversable` (which gives `mapM`), `Distributive` can get away with only requiring a `Functor` rather than a full `Comonad` due to a quirk of Haskell's strength.
Is Lance teaching that? I didnt see it on the course list.
I didn't realize there were such developed comonad transformer libraries— I feel like there's still less a common appreciation for how/why to stack them leading to the belief that the pattern itself isn't as useful. Your point about them tending to be myriad and small instead of monolithic is curious though. It feels like it respects the dual.
Thanks very much for posting videos. Wish more haskell groups were able to do so.
There are also ~8 more comonads floating around inside of the `lens` internals, which get used quite heavily in ~200 type signatures, if you want examples of comonads "used in anger." ;) In particular a number of powerful variants on the `Bazaar` comonad are used to power lots of combinators that would otherwise be impossible to write.
I use lots of little comonads for things like pushing a typing environment into a syntax tree rather than carrying independently, tracking the name of things, separating structure from data. Now, that said. There are provably fewer interesting comonads in haskell than monads. This also contributes in some way to their general "boringness" at first glance. Notably every comonad gives rise to a monad transformer, so you won't find analogues to IO, STM, ST s, etc.
I've been exploring the `lens` internals which prompted the "generic programming" instance of comonads I mentioend. Things derived from "Functor is to Lens as Applicative is to Biplate" in particular. Used in anger indeed.
In terms of "practical comonad" usage, do you have any practical examples of "kissing" comonads and monads via a distributive law? I always thought that was an interesting effect, but have never thought to use it.
In my opinion the notion of using a distributive law to mix a monad and a comonad is basically universally a bad idea. You wind up with monadic-effects being played over from scratch in all sorts of contexts, and general terrible performance. A much more interesting concept is the idea of a comonad that happens to live in a Kleisli category. This gives you the same signature, but _very_ different semantics. You can then have things like comonads that tabulate and cache their results using effects, etc.
Would you like to talk about your relation to Haskell? Is there anything you could see yourself coding in it, whether play or professional?
Typeclassifying monads is useful to me, because I can worry about the monad stack at one place only. That technique requires monads. I do not know of any patterns that require a comonad typeclass.
I often encounter problems where I think "*a* monad (with structure XYZ) would be good here". For comonads, the equivalent is "*this* comonad would be good here".
I had to leave right before the ending of the video, so I apologise if this question is covered there but I've a question: What is, if any exist, the point of using the fix combinator in my code? If in writing `factorial_rhs` I must explicitly call a recursive parameter x then isn't that basically the same as doing explicit recursion? It doesn't seem to add benefit to recursive definitions.
Great video, but I feel it would have been really beneficial to look inside `fix` and really see what's going on. For me, it felt really weird that `fix :: (a -&gt; a) -&gt; a`, but we could write `fix rhs_factorial`, because `rhs_factorial` is `(a -&gt; a) -&gt; a -&gt; a`! First I took a step back, and tried taking the fix point of something simpler, `id`. So I tried `:t fix id`, and was told that it has type `a`. Huh, that's weird! Well... what's `fix id :: Int` then? It doesn't terminate! We seem to have defined `undefined` (how meta). But this makes sense, if we expand `fix id`: fix id = id (fix id) = id (id (fix id)) = id (id (id (fix id))) = id (...) Or fix id = id $ id $ id $ id $ id $ id .... Ok, so that's clearly not going to terminate. Now, let's look at `fix rhs_factorial` under the same light: fix rhs_factorial = rhs_factorial $ fix rhs_factorial = rhs_factorial $ rhs_factorial $ fix rhs_factorial It's clearly tail recursive again, but I still struggle to see how Haskell know's to stop constantly building up this `rhs_factorial $ rhs_factorial` pattern, and actually start evaluating when I put some input in. Where's the gap in my knowledge?
The gap is laziness. (**EDIT:** Huh, I just reread this and had to make it clear that I mean that the gap is *Haskell's* laziness, not yours :-P) You've basically hit on the way I like to think of `fix` in Haskell. We use this definition (which is not the best one for performance AFAIK, but the best for didactics): fix :: (a -&gt; a) -&gt; a fix f = f (fix f) As you notice, informally speaking we have this equation: fix f == f (f (f (f ...))) Now, why is this useful? Because we have also these two things: 1. Partial application—`f` can be a two or more argument function. So with a two-place `f` we have `fix f x == f (f (f ...)) x`. 2. Lazy evaluation—calculating the result of the `f (f (f ...)) x` forces the first argument (the "infinite stack") only as deep as it is needed. So consider this function: fact' _ 0 = 1 fact' f n = n * f (n - 1) fact = fix fact' The trick here is that when `x == 0`, `fact'` ignores its first argument. **EDIT:** Another way to understand it is just to use hand evaluation: fact 2 == fix fact' 2 == fact' (fix fact') 2 == 2 * fix fact' (2 - 1) == 2 * fact' (fix fact') (2 - 1) == 2 * fact' (fix fact') 1 == 2 * (1 * fix fact' (1 - 1)) == 2 * (1 * fact' (fix fact') (1 - 1)) == 2 * (1 * fact' (fix fact') 0) == 2 * (1 * 1) == 2 * 1 == 2 Also note that while I stressed the role of partial application, `fix` is also often useful with one-argument functions: fix ([1..3]++) == [1..3] ++ ([1..3] ++ ([1..3] ++ ...)) And just to throw it in, the "real" definition of `fix` is this: fix f = let r = fix f in f r The motivation is how the compiler compiles one vs. the other; as I understand it, it's like the difference between an infinite list that keeps allocating new cons cells as you consume it, vs. a circular one where the "last" cons cell points back at the first one.
Since nobody mentioned about this paper yet, I link this ;-) http://www.cl.cam.ac.uk/~dao29/drafts/codo-notation-orchard12.pdf 
I think this paper captures some essential idea in comonad in terms of context-dependent operation. BTW, will 'codo' notation be experimented in a foreseeable version of ghc soon? I would love that!
Modularity? fib' _ 0 = 0 fib' _ 1 = 1 fib' f n = f (n-1) + f (n-2) fib = fix fib' badlyMemoisedFib = memoise fib wellMemoisedFib = fix (memoise . fib') 
I think Dominic got it wrong. I'm going to propose an alternative syntactic sugar very soon.
That seems a sound point. Why is the first memoised `fib` 'badly' memoized? Is it a matter of code readability or maybe of efficiency? And followup question: besides memoising functions, what other 'modules' could be composed with `fib`, `fix` and `memoise`? Are there that many or is memoisation kind of out there on it's own?
You can't write a fixpoint combinator in Haskell without recursion? Really? Are you sure? In the video it says it "can be proven". Which is suprising because it's wrong. newtype Rec a = In { out :: Rec a -&gt; a } fix :: (a -&gt; a) -&gt; a fix = \f -&gt; (\x -&gt; f (out x x)) (In (\x -&gt; f (out x x))) Poof.
Isn't there a recursion in your newType definition?
I suppose it depends what you mean by recursion. The video is about recursion is functions. I suppose I have swapped such recursion for recursion on types, but this is fairly different (IMHO).
Lovely! How does one create a video like this? (i.e. do you have a scrolling editor with tablet/handwriting support? Something else?) 
Slides aren't available yet, but code is online: https://github.com/ryantrinkle/memoise and the site is up at: http://memoi.se
Should really link to the permalink: http://chris-taylor.github.com/blog/2013/02/09/io-is-not-a-side-effect/ . This way, if people find this thread in the future, they will be able to see this post, and not just the front page of the blog.
Yeah, I should have done. Don't think I can edit now. Thanks for pointing it out!
Nice! It's how I show people how you can do loops in Haskell, by using "lambas that call themselves": sum list = fix (\loop l acc -&gt; if null l then acc else loop (tail xs) (acc + head xs)) list 0 It clearly shows them how you can simply replace the variables that the loop modifies in imperative code by parameters of the lambda. I even use sometimes this pattern in my code. Of course I don't use it for examples as trivial as ```sum``` but you see my point.
See also: * [Purify code using free monads](http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html) * The [IOSpec](http://hackage.haskell.org/package/IOSpec) package. I think that IO should be implemented as a free monad where the base functor is extensible. It's much easier to reason about how IO works that way than as `State Realworld` plus strictness.
see also: [The C language is purely functional](http://conal.net/blog/posts/the-c-language-is-purely-functional)
It's worth pointing out, I think, that recursion is more or less a given as soon as you have an environment model of evaluation. No combinators needed. AFAIK no ones ever studied the relationship between this and combinator recursion.
The side-effects are in the interaction with the outside world. Sure, the program is composed in a pure way - that just means it's not until you run that fully composed program that the side-effects take effect. The impurity is in the run-time behaviour of the primitive IO actions. Even in C, Pascal or Basic, the program is compiled in a pure way from the sources. The result doesn't depend on the state of the universe, only on the compiler and the source files. Admittedly file reads and writes are normally considered impure I/O - to work around this, imagine the sources as parameters and the target code as a returned value. It's not until the program runs that the side-effects of the program occur. The purpose of a program is *not* simply to be translated. The purpose of a program is to be run. If you don't understand the run-time behaviour of the program, side-effects included, you don't understand the program. Understanding how the building blocks are combined isn't the same thing as understanding the run-time behaviour of the program. The whole point of the primitive IO actions in the IO monad in Haskell is to support run-time behaviour that includes side-effects. Any input action refers *implicitly* to the "world state" that it extracts that input from. There's a common mental model where the IO monad translates to pure functional code that threads the world state through, but that's just for intuition. In reality, Haskell explicitly defines the primitive IO actions to be "black boxes" that cannot be translated this way. The reason is simple - the outside world is *outside* the program by definition. The world state is continuously changing irrespective of your programs IO monad, and not just when I/O actions cause a change. The IO monad is a nice trick that makes the need for impurity explicit in the type system, and allows impure actions to be composed using the same tools that are used for pure code, but the impurity is still there. If it wasn't, it would be possible to write the same functionality *without* using the IO Monad. Whether you're understanding the Haskell source or that string, you have the same problem understanding it's behaviour that you would in any imperative language. The behaviour of a particular chunk affects and is affected by the context in which it is used, particularly the state of the outside world. "Your name" and "Your age" are determined by the state of the outside world at the time the code is executed. Run the same code with the same parameters at a different time, or in a different place, and you'll likely get a different answer. Referential transparency is also broken because e.g. if you embed that code in a larger program that starts with a `put "switch seats with your neighbour"`, you will probably get different results from the same code with the same parameters. That's more practically important if you're thinking about interactions with files or databases rather than users, but the principle is the same each way. When using the IO monad, the result of running your code may depend on things that happened in other parts of the program or in the outside world, not just on the piece of code you're looking at and it's parameters. The whole point of purity and referential transparency is that that should never happen. Therefore IO is impure. 
Yes. That's a fine testament to how utterly ridiculous the OP argument is. "Oh! This behaviour specification (code) is pure because it's represented as a value/string/whatever!" Bah! A better way to distinguish features of code is by how subprograms can be modified without altering meaning of surrounding program (commutative? idempotent? independent of computation time or context? can be eliminated if return value isn't used? Pure expressions are all of the above.)
&gt; Pure Haskell takes it a step further and separates evaluation from execution. Either evaluation and execution are the same thing, or else this is just code for saying that Haskell uses the type system to keep impure code separate. &gt; A description is declarative. The same is true in C, Pascal or Basic. The pure functions are slightly better hidden, but if you're going to have a mental model where the mutable variables and outside world are threaded through, and even in Haskell there's a black box involved, well - why not make this implicit. Every imperative procedure and statement has an implicit parameter passing in the state of the aggregate of mutables and the world, and implicitly returns a tuple of its explicit result and the resulting mutables-and-world state, trivially converting those procedures and statements into pure functions. After all, in Haskell, the bind operator abstracts away the world-threading (if any exists - with IO actions it explicitly doesn't), and the `do` notation abstracts away the `bind` operators. The only difference in Pascal is that you get the equivalent of the `do` notation whether you like it or not - you can't see the bind operators any more than you can see a world-threading implementation of bind for IO actions in Haskell. Haskell is better, but I don't see any substantial theoretical difference. If nothing else, you can always translate Pascal to Haskell, showing a mathematical equivalence between the two. A C, Pascal or Basic program is a declarative specification of imperative functionality. Just like Haskell code that uses the IO monad. 
I would go for a simpler representation of the graph. I think you'll save a lot of headaches that way: data Vertex a = Vertex a data Edge v w = Edge { to :: Vertex v, from :: Vertex v, weight :: w, directed :: Bool } data Graph v w = Graph { edges :: [Edge v w] } Adjacency lists are one of the simplest and easiest to understand representations of the graph. No need to maintain a separate list of vertices.
&gt; Either evaluation and execution are the same thing, or else this is just code for saying that Haskell uses the type system to keep impure code separate. What justification do you have for this claim? It sounds completely off base, to me. It has nothing to do with the type system. What do you think happens when I run this program? import Control.Exception (evaluate) main = evaluate $ putStrLn "foo" Despite the fact that I evaluate `putStrLn "foo"`, it is not executed. This is what I mean when I say that evaluation and execution are separated. It's kind of a lie to say that this is a language-level feature, though. It's more of a feature of the standard library. &gt; The same is true in C, Pascal or Basic. The pure functions are slightly better hidden, but if you're going to have a mental model where they mutable variables and outside world are threaded through, and even in Haskell there's a black box involved, well - why not make this implicit. Every imperative procedure and statement has an implicit parameter passing in the state of the aggregate of mutables and the world, and implicitly returns a tuple of its explicit result and the resulting mutables-and-world state. In Haskell there is *not* such a thing. That's the difference. In Haskell you *explicitly* control this flow of information. This distinguishes a side effect, as in C, Pascal, or Basic, from an effect, as in Haskell. A side effect happens implicitly when an expression is evaluated, but an effect does not. As an aside, I don't like this analogy that the IO monad "threads the world" through your program for the same reasons you stated before. It's not accurate. It's more accurate to say that the IO monad allows you to dynamically create descriptions of the effects you intend to happen. &gt; the do notation abstracts away the bind operators It does no such thing. It only sugars them. &gt; The only difference in Pascal is that you get the equivalent of the do notation whether you like it or not - you can't see the bind operators any more than you can see the implementation of the bind for IO actions in Haskell. The difference is bigger than that. In Pascal, you can't express a computation that doesn't have them at all. You can't evaluate an expression without triggering any side effects associated with it. &gt; A C, Pascal or Basic program is a declarative specification of imperative functionality. This is true. However, that specification is evaluated in its entirety at compile time. Haskell code is not. In Haskell, every time you use `(&gt;&gt;=)`, you dynamically *create* the next effect to run. This power is not available to you in C, Pascal, or Basic.
Consider reading Wouter Swierstra's "Data Types a la Carte". `IOSpec` is a free monad over an algebra of functors which together implement much of `IO`'s behavior.
Edward, do you know of any tutorials/have you thought about writing a tutorial on using `TracedT`, `EnvT`, and `StoreT`? I know of materials on what these are categorically, in terms of duals, and some of the constructions (like `lens`) that come from them. It would I think be nice to have something that explains how to use these as code structuring tools (think the monad chapter of "Learn You a Haskell" or "Real World Haskell"). I would write something like this, but have never used comonad transformers in my code (I've occasionally rolled my own comonad, but never used the transformers).
Well, the second implies the first.
&gt; In Haskell, every time you use (&gt;&gt;=), you dynamically create the next effect to run. Only in a mental-model intuition sense. In reality, and in the general case, the information isn't available to evaluate the right-hand argument of the bind operator. Without that information, even the non-IO operators in the right hand expression cannot be fully evaluated - there are quite literally references that cannot be evaluated because their values come from the outside world. You can only do *partial* evaluation. Partial evaluation and other symbolically-evaluated transformations of code are, of course, very common optimizations in compilers for imperative programming languages. Even in Haskell, the difference between a `+` operator that can be evaluated at compile-time and one that must be deferred until run-time is implicit - that is determined by information flow, and that depends on the context in which the `+` operator occurs just as much as it does on the arguments of that operator. Exactly as happens in imperative languages. &gt; However, that specification is evaluated in its entirety at compile time. Haskell code is not. In Haskell, every time you use (&gt;&gt;=), you dynamically create the next effect to run. Again, in a mental model sense, that's true. But for a compiler writer, all this means is that the Haskell program is only partially evaluated at compile-time as an optimization - otherwise, it is compiled by translating the remaining unevaluated expressions to another form. This is really no different to any imperative language. And by that trivial translation from Pascal to Haskell, the same logic applies to Pascal. Those dynamically evaluated functions are still there, just better hidden. Each new dynamically evaluated function (based on a different mutables+world state at the point it's evaluated) maps directly to a dynamically evaluated behaviour of a chunk of imperative code. &gt;&gt; the do notation abstracts away the bind operators &gt; It does no such thing. It only sugars them. In one case, the abstraction is an expression that manipulates actions. In the other, the abstraction is a sequence of actions. The expression is hidden behind the abstraction layer and you only see the sequence of actions. Sugar, sure, but syntactic sugar can provide an abstraction too. 
You might find [Gremlin](https://github.com/tinkerpop/gremlin) helpful. It's a graph traversal library for the JVM; I've used it briefly while evaluating neo4j.
&gt; What's so different about it? It's not a function. If I had the Haskell type system, but no named functions, I could still define fully recursive functions using this `fix`.
&gt; I think that IO should be implemented as a free monad where the base functor is extensible. I've actually got some local code that implements `IO` as a free monad over this functor: data FFI x = FFI FunName [FFIArg] (FFIR -&gt; x) Of course, you'd want to wrap most operations up a bit to get good types: hPutStrLn h s = FFI "puts" [encodeArg h, encodeArg s] (const $ return ())
actors in erlang are great because you have the great let-it-crash philosophy, which does not really work in haskell. (it works in distributed process, but i doubt that's a good library for single machine workloads.)
It's not only in a "mental-model intuition sense," and I don't see what partial evaluation has to do with it. Partial evaluation of imperative languages is just detecting pure subexpressions and evaluating them in advance. The IO monad *literally* separates evaluation and execution, and it *literally* allows you to create new actions dynamically. You conveniently ignored my code sample from earlier. Here's another, to demonstrate actually building an IO action dynamically: buildAction action = do str &lt;- getLine if str == "done" then return action else buildAction $! (action &gt;&gt; putStrLn str) main = do action &lt;- buildAction $ putStrLn "begin" putStrLn "built the action" action You can't really do this in Pascal. You would have to explicitly create some data representation for the action you're building up and then interpret it when you want to execute it. Would you argue that whatever data representation you choose is for some reason impure? Also note that partial evaluation really doesn't have anything to do with this ability or the lack thereof in Pascal.
What about [Data.Graph](http://hackage.haskell.org/packages/archive/containers/0.5.2.1/doc/html/Data-Graph.html)?
This looks very useful!
Part of the exercise is to implement different graph representations myself to appreciate the trade offs between performance and ease of implementation.
Oh ok. :P
Haha, sorry. Free monads are built using a method of transforming functors into monads by layering and the resulting monads take their properties from interpretation of the functor at each layer. Data Types a la Carte takes a variety of simple functors with orthogonal interpretations and sets up ways to combine them together. This creates a combinatorially rich set of base functors which, when mapped to their free monads, can be interpreted as various components of `IO`.
No, no, don't worry about it. I come to the haskell subreddit because I'll encounter new things. That was a very understandable high level summary, thank you.
I wrote a post that explains how you use free monads to implement [abstract syntax trees](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html). You can imagine an alternate reality where IO commands just add steps to a syntax tree, and then the compiler translates that syntax tree to an executable.
Every time you run pure code, some memory is used up in your computer as a side effect, therefore every language ever written is impure unless it perfectly models memory usage at compilation time. EDIT: Apparently people don't get sarcasm...
Wouldn't that force me to have vertices that are always connected to at least one other vertex?
I don't have much IO in my types precisely because I believe IO is impure. As others said, you could add auto lifting to IO and remove it from type signatures, and then you've got a language like ML. The difference between an ML program and a Haskell program with IO is superficial/syntactic and nobody claims ML is pure. If purity amounts to such a superficial syntactic difference (a few extra bind/return calls and IO wrappers around some types) how do you believe it amounts to much? 
If you have unconnected nodes, you could maintain a list of those separately, or redefine an edge as: data Edge v w = Edge { to :: Vertex v, from :: Vertex v, weight :: w, directed :: Bool } | Unconnected { vertex :: Vertex v } I think the main thing is not to maintain a list of all the vertices *on top* of the adjacency list. That's just a headache.
&gt; The difference between an ML program and a Haskell program with IO is superficial/syntactic I use OCaml almost exclusively for my day job and Haskell almost exclusively for my hobby projects. For me, the difference is very significant. &gt; If purity amounts to such a superficial syntactic difference (a few extra bind/return calls and IO wrappers around some types) how do you believe it amounts to much? It's much more than a few combinators. It's peace of mind that no expression in my entire program, whether its type has IO in it or not, is going to perform *any* side effects when I evaluate it; it has to be an IO action *and* I have to bind it in `main`. At the point where I bind it, I care about its effects, but only there and nowhere else. I do not have this luxury to any extent in an impure language. I can't count on all my fingers the number of times I've wanted to create some sort of an interface in OCaml but couldn't think of a safe way to do because it included higher order functions which would have to provide some sort of guarantees about when it applies their arguments due to side effects or because I would have had to force users of the interface to awkwardly thunk all their inputs to avoid causing the side effects too early. There are also plenty of OCaml libraries in existence already that look nice on the surface but are far too easy to screw up with because you have to be careful with side effects. For example, take the following monadic (doesn't matter which monad) OCaml code: let foo = printf "blah blah blah\n"; bar &gt;&gt;= fun x -&gt; baz x It's probably obvious what may be wrong in this case, but in practice it's usually much harder to notice.
Not a bad idea. But you know what? I'll go ahead with your first definition, and I can explore unconnected nodes later.
I think the issue is not that abstracting over monads is nice, but rather, abstracting over monads is necessary. Because you cannot freely get values out of a monad, it's tricky to combine multiple monadic effects; and therefore we use transformers and overloading in order to paper over the boilerplate. With comonads, however, you can always freely pull values out, so that isn't a compelling reason to thread one comonad through another. Monads, being primal/initial, are naturally all about *structure*. But comonads, being dual/final, are not about structure--- they're about *costructure*, or context. Since the problem with monads leading to their type classes has to do with the fact that you can't freely leave them, the place to look for comonad type classes will (almost surely) have to do with the fact that you can't freely enter them. 
But it is actually true. The entire genius of Haskell is that it reifies something's behavior as data so that you can still equationally reason about your code, unlike every other language where equational reasoning is nonexistent because they don't treat behavior as data.
`badlyMemoisedFib` will only memoize the initial input; e.g., if we call `badlyMemoisedFib n` it will only memoize `fib n`. Thus, unless you happen to call `badlyMemoisedFib` on the natural numbers in sequential order, you're going to be repeating work. Conversely, since `wellMemoisedFib` shoves the memoization into the fixed point, this ensures that every recursive call that `wellMemoisedFib` makes ---i.e., that `(memoise . fib')` makes--- will be memoized. Because of the structure of Fibonacci numbers, this means calling `wellMemoisedFib n` will memoize each of `[fib i | i &lt;- [0..n]]`.
And when you read a math paper, the neurons in your brain use glucose, making you hungry. Therefore math is impure.
I certainly don't think you should raise an error when inserting an existing vertex or edge---that would be an easy source of unexpected errors. And `Maybe` in that case doesn't seem right, either (ahem). `Maybe` is great for looking things up in a structure; but it's not as intuitive for adding the wrong node to a graph to "destroy" the whole graph. For your other options, analogy with `Data.Map` and friends suggests that replacing is the way to go. But the really right thing to do might be to write functions with each behavior, and see what works out best.
People understand your post as sarcasm but not mine. I need to learn to be more absurd I guess.
Good idea on having both functions; I'll have `insertEdge` and `replaceEdge`. And you're right about errors/Maybe on insertion, it doesn't feel intuitive at all (I gave it a try, and ran the other way). Really appreciate the input!
Erh, appears I accidentally the title. Sorry.
There is a logical difference between: "IO is pure" (meaning: IO is a type of specifications of pure behavior) and "this program that constructs an IO is pure" (meaning: I have specified a pure behavior that constructs a value of type IO). In the latter case, I've said nothing about the purity of IO. The only reason this topic seems interesting is because there are vociferous people who fail to distinguish the two concepts, or who idiotically believe that Haskell platform and Haskell developers are concerned about only one of the two possibilities. Ability to reify and compose behavior specifications is useful. Typeful control of effects is useful. But we don't need pure IO to explain or understand why they are useful. (Besides, reification doesn't even imply pure construction. I wield types of the form `IO (Foo -&gt; IO ())`.) Your argument about "the entire genius of Haskell" is, simply put, a non-sequitur. 
Another way to put it is that the order of side effects in Haskell is independent of evaluation order. In other languages, even functional ones like OCaml, evaluation order is deeply intertwined with side effect order, which is why it is more difficult to reason about side effects in those languages.
&gt; I have a hard time believing that the really experienced Haskell programmers aren't frustrated by all this. I'm sure they are, but they have learned how to deal with the issues and work around them. With that knowledge and those workarounds they are still happier with Haskell than with other languages they have at their disposal. I blogged my first real experience of space leak that should not have happened. However, I found a solution to the problem pretty quickly and felt really proud myself. For that project I simply could not see myself using C or C++. I couldn't even see myself using Ocaml because I would have had to write a huge pile of code for which there were already libraries in the Haskell ecosystem. As I see it, yes, this is a pain in the neck, but I'm willing to put up with it because of all the positives for Haskell and the Haskell ecosystem. I'm also convinced that all the smart people using Haskell who care about performance will end up improving GHC and providing better tools to find and fix these problems quickly. 
Where is the docuemntation? What does hello World look like? How similar is it to Haskell? Would a typical Haskell program run in Frege? And why is Frege supposed to be so hard to pronounce? Most English dialects don't have exactly the 1st vowel, but /'fri g@/ or /'frei g@/ are pretty close.
Sure, I understand fixed points mathematically - in fact, I've just been studying them in my degree. The bit I can't quite wrap my head around is how `fix rhs_factorial x` can start evaluating before "unwrapping" all of the calls to `fix rhs_factorial`, which would be infinite. So presumably, something causes it to evaluate with `x` before constantly appyling `rhs_factorial` to itself, and I'm not quite able to mentally parse how that is.
I often use a representation like this, which should also have decent performance: data Graph v e = Map v (Map v e) Then you can essentially directly use the Data.Map api for manipulating the set of nodes. For edges there is `Map.insertWith` and `Map.unionWith`.
Evaluation order is specified in these languages as a function of their syntax. Just like execution order in Haskell is specified as a function of the operators used (which is also syntax!). If you use f(g(), h()) in an impure language, you need to know whether the language evaluates args left-to-right, right-to-left, or in an undefined order. If you use: `f &lt;$&gt; g &lt;*&gt; h` you need to know whether `&lt;*&gt;` executes effects left-to-right or right-to-left. It's not really that much of a difference.
Sarcasm does not mean auto-immunity from criticism. I personally haven't downvoted, but I don't really understand how your point add to the topic.
Because when evaluating `f (fix x)` you start by evaluating the body of `f`, not by evaluating the argument. "Outside-in" evaluation or "lazy evaluation". For example, a trivial example is: fix (\x -&gt; 1) = (\x -&gt; 1) (fix (\x -&gt; 1)) = 1 We don't evaluate unused arguments. Another example: fix (1:) = 1 : fix (1:) = 1 : 1 : fix (1:) Once we have `1 : ...` we already have a useful result, and if we don't "dig deeper", lazy evaluation will never evaluate the "..." part. 
This is the wrong forum for such a proposal.
In addition to modularity, it can be syntactically convenient. Instead of: myFunction x y z = go where go x = ... go (f x) .... I can use: myFunction x y z = fix $ \go x -&gt; ... go (f x) ...
Hopefully you don't use *this* pattern, because branching on `null` and then using `head` and `tail` is a pretty terrible pattern :) Pattern matching in lambdas is annoying, which is the main reason I rarely use fix like that.
I think you need to elaborate...
But the difference is that in Haskell evaluation order is completely orthogonal to execution order. Yes, you do have to have to learn the syntax of execution order to learn Haskell, but Haskell completely isolates that from its evaluation model so that you can reason about evaluation independently of execution. This is why refactorings in Haskell are always completely safe because they only affect evaluation and not execution, unlike OCaml where refactoring are not necessarily safe because OCaml conflates evaluation with execution.
An "unsafe refactoring" isn't a refactoring at all, as refactorings don't change the meaning of the program. Haskell makes it clearer what actual refactorings and what aren't, by separating the equals sign from binds (or the `&lt;-` sign). This syntactic markings of where effects may happen are useful, but only if you actually work to extract things from IO into pure code. After all, you could make a trivial ML-to-Haskell translator (wary of calling that a compiler, since it is such a direct mapping) that builds code in IO. Surely you will not claim the result Haskell code in IO is easier to reason about than the original ML code? Pure Haskell code has huge benefits, though, of course. The disagreement is only about whether IO code is really better in Haskell than impure code in other languages.
By unsafe refactoring I mean this: let x = effect in f x x ... versus: f effect effect Last time I checked, OCaml would give different behavior between the two versions. Or to give a more concrete example using Applicatives, I can take the following Haskell code: f &lt;$&gt; getLine &lt;*&gt; get line ... and safely refactor it to: let x = get line in f &lt;$&gt; x &lt;*&gt; x ... and trust that both versions execute the value twice. In OCaml, you would have to play tricks like hide the effect behind an unapplied function in order to have get line execute twice in the latter version because OCaml needlessly mixes its evaluation model with its execution model.
hoodle is good, but still not entirely stable. I've done some playing around with it and it does work, but sadly the rendering is not as good, and it's very easy to get it into weird inconsistent states. When I get a bit more time, I can hopefully file some bug reports.
Hey, thanks. It's cool that their "IO teletype" data structure is nearly identical to my "IOAction" type. Obviously I knew I wasn't doing anything new here, but I still didn't expect to see something so close to what I was writing about in a published paper!
&gt; branching on null and then using head and tail is a pretty terrible pattern :) Be reassured: I find it awful too (note that it's not equivalent to regular pattern matching though: you have to use _irrefutable_ patterns to have this equivalence). It just easier to show neophytes when you don't throw in pattern matching as an extra complexity. (and using pattern matching here would make it more verbose than the regular explicit-recursive-function version) &gt; Pattern matching in lambdas is annoying Yep, but it'll be better with the lambda-case extension.
I agree. Refactoring is slightly easier because it's easier to figure out if what you're doing is a refactoring. In OCaml you could use: let x = \() -&gt; getLine () in ... It's not as nice, and slightly more error-prone. But this difference is not what I believe is taunted as the benefit of purity. Purity's benefits are those of denotational programming. You don't get denotational programming when you do IO.
Well, denotational semantics is not my strong point, but my understanding is that Haskell offers an improvement in this regard by making the distinction between execution and result.
Almost no language with recursion uses fixed point combinators. They just use the fact that self referential definitions are inherently possible thanks to the existence of variable environments. For example, consider `fac`. The reason we need a fixed point combinator in, say, the substitution model LC is because we can't just factor out the value like so: 1: let fac = \n -&gt; if n == 0 then 1 else n * fac (n - 1) in M 2: (\fac -&gt; M) (\n -&gt; if n == 0 then 1 else n * fac (n - 1)) The definition in (1) works fine, but in (2) it fails because `fac` is now free on the right. We need to avoid this, which is what fixed point combinators do, by going through hoops like turning (1) into something like 3: fix (\facRec -&gt; \n -&gt; if n == 0 then 1 else n * facRec (n - 1)) But why does (1) work at all in Haskell? It's not because Haskell turns (1) into (3), no. It's because Haskell has an environment model of evaluation. the variable `fac` is bound to the expression as in (1), and evaluation of lambda expressions produces a closure that captures that environment with `fac`. No combinator required, just an environment. Same thing happens in Lisp/Scheme.
&gt; How many values of type Add Bool () are there? We can list them out: &gt; addValues = [AddL False, AddL True, AddR ()] &gt; There are three values, and 3 = 2 + 1. This is often called a sum type. Sudden epiphany! So this is why Either is said be a _sum_ of types while tuples are _products_.
This is the feeling I aim to inspire whenever I write a blog post. If I cause just one person to have a "sudden realization" then I'm happy. So, thanks for making me happy!
Well done, then, can't wait for the next post in the series!
There is a very long manual in https://github.com/Frege/frege/tree/master/doc A version is linked here https://code.google.com/p/frege/downloads/list
So, I suppose I should be more specific. If the compiler doesn't support named functions, but is restricted by a type system equivalent to Haskell's, then this can be written. If it additionally did not support named types, obviously this would not work.
Some more function laws are: `Add a b -&gt; c === (a -&gt; c, b -&gt; c)`, `a -&gt; Void === Void` and `Void -&gt; a === ()`. That last one is a bit funny, it is the function sometimes called `absurd` or `magic`. (Try implementing `(a -&gt; Void) -&gt; Void`, it is a nice puzzle.)
Thanks for this. I was sad not to be able to make it. Hopefully next time!
ghc-android has a cabal wrapper now, that enables as close to cross-compiling as cabal seems to support. See https://github.com/neurocyte/ghc-android/issues/4 I have run haskell "hello world" on Android now, successfully. Also have a pile of libraries cabal installed for Android. Sadly, ghc-android does not yet include ghci, so no template haskell yet.
That's weird, it's always correctly identified Haskell for me.
I've implemented a pretty simple directed graph library that works by using Ints internally to represent "vertex keys" (but the Key type is kept abstract) and represents a graph using a map from Keys to tuples of the value stored at that vertex and the set of Keys of successor vertices. data Digraph v e = Digraph { adjacencies :: M.Map Int (v, M.Map Int e), count :: Int } deriving (Show) data Key = Key {unKey :: Int} deriving (Eq) https://github.com/imeckler/haskellgraph/blob/master/Digraph.hs
Have you contacted github support?
That was my first guess too.
A compiler may support named types and named functions without supporting recursion. For instance, if you replace the `newtype` keyword with the `type` keyword, then GHC will complain about the recursive use. Similarly, ML distinguished between `let` and `letrec` on the value level.
Nice explanation! In fact, the justification for the mapping between types and algebra goes much deeper than just counting -- though I am not yet sure of a good way to explain it that does not require any CT.
I just did.
If you look at what the api says: https://api.github.com/repos/timthelion/config-select/languages You get: { "Perl": 35147, "Haskell": 9905 } The Perl bytes matches the size of `COPYING` so it appears the GPL is Perl ;).
Ah, I didn't know that was called an "environment model".
Pretty sure it is.
Absolutely. I've been meaning to write these posts ever since I asked that question, but am only just getting around to it. I actually gave a talk on this stuff in November (http://www.youtube.com/watch?v=YScIPA8RbVE) and meant to write the posts then, too. Can't thank you enough for your answer on that question - it set me off on a path that I'm still thinking about and exploring!
I've been following your series on combinatorial species for a while, and I hope to write a beginners level explanation of that at some point too - although I'm skeptical that I could do a better job of it than you!
You're absolutely right - e.g. see this post by dons at Stack Overflow: http://stackoverflow.com/a/5917133/546084 However, I wanted to write an explanation that doesn't involve the phrases "initial algebra" or "final algebra" or "category theory" - mostly because I think this stuff is cool, and that you don't need a graduate level education in math or theoretical computer science to understand it!
The other answers there mention a lot of interesting material, as well. I'm hoping you can work in a reference to the 7-1 tree isomorphism; I always found that a particularly striking example.
How does x0 = 1?
Sure they do. Small boy's hat (small boy's) hat | small (boy's hat) ?
&gt; A compiler may support named types and named functions without supporting recursion Sure, but you're just dodging the point, which is that supporting recursion on functions is a completely seperate from supporting recursion on types. For ones things, functions still exist in the generated code. `newtype`s do not.
Erm, that's supposed to be `x^0`, that is, `x` to the zeroth power. Maybe the formatting didn't render properly for you? If so, sorry about that--I'm going to blame Reddit for it :).
Well, the whole idea of monad transformers. We have transformers because given effects A and effects B we want to write a program that uses both of them. Transformers, in turn, are the big motivation for type classes like `MonadState`, since those classes allow abstracting over the concrete monad stack and therefore generalize the associated monad to all (unambiguous) transformer versions of it.
I thought most of the ambiguity is seen in colloquial use not the formal definition.
I think you're jumping the gun a bit. It isn't recognizing Haskell as Perl. It's recognizing the GNU GPL as Perl (presumably because it includes bare words). Look at how it's syntax highlighted: https://github.com/timthelion/config-select/blob/master/COPYING
Actually, many non ambiguous precedence rules do exist. For example, the word 'of' and other such prepositions have strong right fixity(just like $). Though this precedence changes based on where the preposition is used. "He looked AROUND (the corner)." "He looked AROUND (the corner (of the building))." You see that /around/ and /of/ both have strong right fixity. But there is more to it. Verbs have stronger fixity than prepositions. "(The Duke of Nottingham) asked the queens permission to speak." You see that of no longer has the strongest fixity in that sentence. 
Got an answer already. That's 3 hour response time and I don't pay them a dime for the service(nor see any ads)! Edit: though they haven't fixed it yet. They told me to report the issue to https://github.com/github/linguist/issues A search for "Perl" in the issues there is quite amusing. Apparently GPL isn't the only misidentified text... https://github.com/github/linguist/issues/search?q=Perl
Those aren't precedence rules either. The fact that prepositions combine with NPs is a fact about the prepositions syntactic contexts/syntactic types, not about precedence. For example, "of" in your look sentence has to show up after a noun and before an NP. The alternative parse that has "(around the corner)" as a unit won't admit of "of", not because of precedence rules, but simply because "around the corner" is not a noun, so "of" can't come after it. You don't need to specify that "of" binds tighter than "about", or tighter than verbs, because the *type* of "of" prevents the other parses. You're confusing precedence with syntactic type. Both of them force certain structures, but precedence is a property of lexical items which forces structure by specifying which of two possible parses counts as the only valid one.
Well there can be ambiguity as defined by formal grammar, what I ment by the later, and then there is how people actually use the language, the former. I guess there have been some efforts to invent languages to at least remove ambiguity from the grammar and pronunciation like [Lojban](http://en.wikipedia.org/wiki/Lojban) though I do not know enough of the language to know how successful it is in practice.
I just sent them here: https://github.com/github/linguist/issues/378 
Human languages are not formally unambiguous. That is to say, there's no reason to think that the class of grammars humans use are necessarily unambiguous, and plenty of evidence that they're *rampantly* ambiguous. Anyone who's done some real-world parsing can tell you this. They're more ambiguous than most people realize, with the average sentence having not one or two parses but sometimes dozens, hundreds, or thousands.
**Note** reply to the other comment, this is just to flag your attention. Just to extend me previous comment with an example, verbs *don't* bind tighter than prepositions, as we can see with prepositions that are usable with both verbs and nouns: 1) I saw the man with the telescope. This could mean either 2a) I saw (the man with the telescope) where it's a guy holding a telescope, and I see him maybe using binoculars, who knows, and 2b) I (saw the man) with the telescope where I used a telescope to see the guy. Examples like this abound. You can make it even more complicated 3) I saw the man from the ship with the telescope this can mean lots of things: 4a) I saw (the man from (the ship with the telescope)) so the man maybe was on-shore, but I know he got off that cruise ship that has a telescope 4b) I saw ((the man from the ship) with a telescope) so the man maybe was on-short, but he himself has a telescope with him 4c) I ((saw the man) (from the ship with a telescope)) So my vantage point was on the ship that has a telescope, not the ship without a telescope, but I could've used my own binoculars to look at him 4d) I ((saw (the man from the ship)) with a telescope) So I saw this guy, who was on-shore at the time, but I know he came from the cruise ship, and I used a telescope to do it 4e) I (((saw the man) from the ship) with a telescope) So I saw this guy, and I was standing on a ship, and I used my telescope (the ship doesn't have to have a telescope) This sentence is multiply ambiguous precisely because of this fact that "from" and "with" can come after both nouns and verbs, and there's no precedence rules at play. *All* of these parser are valid.
I'm not quite sure if I understand your complaint, but I presume your issue is with the lack of apparent ambiguity in my examples. Take a more clearly ambiguous case. The case of the left fixity of the preposition at the end of a two clause sentence: "Take out the trash and hang the laundry before leaving for work." The preposition /before/ has a left fixity stronger than the conjunction /and/. This is not due to any deterministic rule of parsing, but due to an arbitrary rule applied afterwards, a precedence in much the same way as * has precedence over + /and/ has precedence over /before/. Of course one exception immediately jumps up with the example of the anaphoric it: "Take out the trash and hang the laundry before it gets creased." The anaphoric it changes the precedence rules and gives the preposition stronger precedence. 
Ok, how about "chilled ice cream". aren't (chilled ice) cream and chilled (ice cream) equally grammatical? but is there a real ambiguity there for us?
My point is that your sentence is indeed unambiguous, but not due to precedence issues. It's due entirely to type issues. Your example here doesn't make any sense to me. I don't understand why you've provided it. It's an ambiguous sentence, which demonstrates that there is no precedence relationship between "and" and "before". Both parsers are possible in principle, neither is ruled out for type reasons, and both are valid readings of the string. Therefore there is no precedence relation here. Your second example demonstrates that anaphoric reference can act as a disambiguating mechanism: for "it" to get reference (barring the presence of some other contextually salient thing which could get creased, like paper), it must be in in an environment with an unambiguous most-salient object. Thus, if you tried to parse this sentence as "(... and ...) before ...", there would be no unambiguous most-salient object, so "it" couldn't get reference, so that parse is impossible. But so what? That just means that some parses are impossible. That's *not* the same as there being precedence in a grammar. Precedence is a very specific thing.
Parse these: I ran up a big hill. I ran up a big bill. In Transformational Grammar, we detect such ambiguities by observing the set of transformations that can legally be applied to each utterance: Up a big hill I ran. *Up a big bill I ran.
They're both equally grammatical, and both have a meaning. The problem with this example is two-fold tho. First is that "ice cream" is a single word (even tho it has a space in it). Hence, it's not technically ambiguous as a parse, except at the level of orthography. Secondly, "ice cream" isn't cream made from ice or whatever, it's some specific kind of frozen cream and sugar and whatnot. If we wanted to interpret "(chilled ice) cream", we'd have to figure out what it means for cream to be of the "chilled ice" variety, and I just don't know what would mean. Furthermore, ambiguity can be eliminated in all sorts of ways. Lots of sentences are unambiguous not because there are precedence rules but because of semantic factors analogous to type rules. For instance, if you have a pronoun that must get a referent from inside a sentence, and there's only one parse that makes that possible, then there's only one parse that will be semantically valid. This isn't due to a precedence rule like `*` has higher precedence than `+`, but rather just a semantic well-formedness requirement on parses. As a simpler example, imagine a language much like Haskell, except where an operator's Haskell type is also relevant for parsing, along side the operator's syntactic type. The expression `x + y * z` in SuperHaskell would be ambiguous without precedence rules, because both `(+)` and `(*)` are infix operators of type, say, `Int -&gt; Int -&gt; Int`. On the other hand, `x ! y * z` would not be ambiguous because `(!)` has the type `[a] -&gt; Int -&gt; a`, and there's no way to parse this as `(x ! y) * z` without getting a type error. `(x ! y)` has the type `a` (for whatever `a` we're using it on), and that is not, in general, going to be `Int`, so it can't be an argument to `(*)`. Thus, the only parse this allows is `x ! (y * z)`, and that's because the *semantic* type prevents the other parse. There is no precedence statement at all, nothing that says `(*)` binds tighter than `(!)`, just some other semantic requirement. Of course real Haskell isn't like this, but you could easily design such a language, and it would lack precedence rules. One such language is the kind we're speaking right now.
The README page on github has all the pointers. There is a document in the Wiki that details the differences to Haskell. There are example programs to look at and the compiler and library is all written in Frege (though github ignorantly claims it was mostly java). A typical Haskell program nowadays uses half a dozen extensions, so, unfortunately, no. A standard Haskell 2010 will most likely get going with minor adaptions, but note that many libraries are not yet existing (and, for the I/O stuff with C heritage most likely will never exist). .
Yes, we can use transformations like WH movement and clefting to disambiguate between possible parses. This is somewhat irrelevant to the point, however. Furthermore, the second sentence is in fact ambiguous, but not on the reading you intend. Obviously if there's a giant duck, with a giant bill, I can run up that bill! Also, if there's a giant envelope, with a giant electricity bill sticking out of it, I can run up that too! But your intended reading is a different one: the one where "run up" is a verb-particle construction that means "spend lots of money" or something to that effect. So the sentence is ambiguous in multiple ways, one of which was most salient to you when you wrote those. :)
Why would you include all of GPL anyway? [This](https://github.com/timthelion/config-select/blob/master/COPYING#L634...L648) is enough.
Well, the source files themselves only contain a brief notice.
ah, yes: mobile browser. Looks much nicer on the desktop.
[It apparently identifies random data as Perl too!](https://github.com/search?l=perl&amp;q=path%3A.ssh%2Fid_rsa&amp;ref=searchresults&amp;type=Code)
Obviously `Rec` is recursive. But, you can write the fixpoint combinator in Haskell (with extensions) using recursion at neither the type nor value level: see http://okmij.org/ftp/Haskell/impredicativity-bites.html 
It would appear from this thread that Github considers anything it can't parse/understand to be Perl.
To be fair, that is a pretty good assumption.
Right, I don't object to teaching this stuff, but I think calling it the "Algebra of Algebraic Data Types" is a bit misleading.
Style guidelines are irrelevant for discussion of what characterizes human languages, just like they're irrelevant for what characterizes Haskell code. There may be best practices but that's not part of Haskell, it's part of how we've decided to shape our code to best suit our purposes. Furthermore, formal definitions of human grammars *can* have precedence rules. But there's no evidence that the actual human grammars that people know are employing such rules.
Don't ruin our fun! :) Besides, I'm all for retconning the term's origin.
Just try! The worst that happens is that you get more practice explaining it.
This is a straightforward version conflict problem. It has nothing to do with cabal or cabal-dev at all.
It's certainly a side-effect of the way Cabal and Hackage handle versions and constraints. It turns out that indexed-extras-1.0.2 builds (and probably works) with pointed-3.0.2. Since Cabal is likely working to spec, perhaps it's time the spec is updated.
There seems to be a few haskell related videos and channels on youtube too, it would be nice to have a centrally curated repository somewhere. **edit** Here's the haskell wiki video page: http://www.haskell.org/haskellwiki/Video_presentations
Read the section you just quoted ;) " You should have received a copy of the GNU General Public License along with this program." The cabal file also has a license: field....
In what way is this related to not being able to "leave" a monad?
It seems that Perl does too: http://stackoverflow.com/questions/11695110/why-is-this-program-valid-i-was-trying-to-create-a-syntax-error
Nice post! I missed the associative laws (but those are rather boring) and `Either a b -&gt; c === a -&gt; c, b -&gt; c` as the equivalent of c^(a+b) = c^a * c^b. That one has `uncurry either` as part of it's implementation :-) If you introduce `Maybe` (or `Either ()`; that might be clearer, perhaps) as the successor function, you can also easily write the following equivalences between types: Either a (Maybe b) === Maybe (Either a b) (a, Maybe b) === Either (a,b) a Maybe a -&gt; b === (b, a -&gt; b) which together with the equivalences for `Void` can serve as the definition of addition, multiplication and exponentiation.
Yes, and Android runs on ARM and I'm not sure you can run a GHCi on it (I believe it's due to glibc being missing from Android SDK). I could really use it to show some Haskell samples during Haskell meetings.
&gt; Sadly, ghc-android does not yet include ghci That's due to glibc being missing on Android SDK, right?
Don't you need an explicit `absurd :: Void -&gt; a` to express that `Void` has indeed no inhabitants (eg to prove that there is a function of type `Void -&gt; (Void, a)`) ? With this presentation it looks like a mere existential (abstract type).
This is like the following in regular "high school algebra" (when counting number of values)? a + (b + 1) == (a + b) + 1 a * (b + 1) == (a * b) + a b^(a+1) == b * a^b Makes sense, thanks!
Category extras isn't really maintained -- look at the haddocks! Typically you're encouraged to pull in what you need, not rely on this huge meta-package that just documents where the fragments of the obsolete category-extras ended up. It looks like indexed-extras just isn't as actively maintained as the rest of that ecosystem, and probably has too-restrictive bounds. But more to the point, you really shouldn't necessarily be trying to install that package.
Not sure I'd call "Frege" difficult to pronounce for native English speakers, full-stop, but it's not immediately obvious *just how* to pronounce it. Until fairly recently I was saying "Freh-geh", and I have a math degree (with minors in philosophy and CS)!
&gt; it's not immediately obvious just how to pronounce it That's true, unless you know a little German.
The link to the [corresponding paper](http://www.macs.hw.ac.uk/~trinder/papers/HdpHIFLDraft.pdf) with some examples and benchmarks.
Great overview! (Btw, you can use `newtype` for data families as well, useful with `NaturalMap` and `PairMap`.)
I've used Haskell seriously for years, both for work and hobby projects, and at times doing serious crunching with it. I've never used INLINE directly in my code, never used unboxings directly except for messing around with shootout entries, and actually avoid bang patterns in favor of explicit `seq` or strictness annotations on data structures. I view the first two as tools used to produce efficient libraries, and since I haven't written those precise libraries, I'm just a satisfied customer :-) That said, I've used profiling to fix plenty of performance issues. Typically it has pointed me to places where my algorithms have been stupid. Only very rarely has it pointed me to something resembling a space leak.
“If not, see” ...
It's a PVP spec problem, not a cabal problem. I've already weighed in with my opinion that the PVP policy of suggesting that people always use tight upper bounds has worked out badly in practice.
Is there an abstract for this talk? Sounds interesting.
Yes, that's the specification I was referring to. The issue is compounded by Hackage/Cabal treating package descriptions as immutable and authoritative. If someone (like the package maintainer, or the community, or a buildbot...) figures out that the version bounds should be modified in some way they can only do so by uploading a new version. Depending on the PVP approach the package maintain chose, this treats users to cryptic error messages and build failures. Consider: * Tight version bounds: A package and all of it's dependencies have to be upgraded just to get a new constraint set * Loose version bounds: An older package that is known to not build is selected for installation because it's constraints match their installed packages Both of these were particularly unpleasant as a new user. The only reason I find it marginally palatable today is that I know what packages I'm going to use (about 200 of them) and can install them all at once. It's not exactly bombproof either, my first stab at doing a package build for GHC 7.6.2 about a week ago (we're upgrading from 7.4.2) *totally annihilated* Cabal's modular constraint solver. It was using up 20GB of RAM and not obviously making progress in coming up with an install plan. I had to iteratively add packages to the set with --dry-run in order to find the offending package and locally modify the package description. Hackage courtesy states that I shouldn't upload a new package version of someone else's package (sent an email instead and crossed my fingers), anyone else attempting a similar build will suffer the same fate... for a while, at least. 
I recall an intriguing result about the type of trees `T`. It is something like `T = T^4`, i.e. the type of trees bijects in some natural way with the type of 4-tuples of trees. I can't remember the exact result and it's hard to search for but hopefully someone here will know! 
I'll add you in the next few days :)
Sweet, ty!
It's T = T^7 - there's an exact bijection between trees and 7-tuples of trees. You can read about it at http://blog.sigfpe.com/2007/09/arboreal-isomorphisms-from-nuclear.html
Why wouldn't a functional language be designed for text adventures? In fact, it would teach you a lot about the IO monad, parsing and displaying text, and having to create your own data structures. Functional programming can do anything and everything procedural can do. You seem to be interested in making games. I would suggest that you write Conway's Game of Life in Haskell, it's a fun little side project, and if you really want you could write a simple OpenGL front-end for it. What else are you interested in? Look at your hobbies. What about something to help you with your homework? The sky's the limit really.
Andreas Blass, ['Seven trees in one'](http://arxiv.org/abs/math/9405205)
This was a good excuse to learn more Haskell, and I wanted to create something simpler than Nettle. I am a Haskell toddler, so criticism / feedback greatly appreciated!
Right. Some people have problems sometimes if bounds are too lax, and more people have more problems more often if they aren't. By being less restrictive with upper bounds, we minimize the surface area of people affected by this sort of stuff. I note that `cabal init` now seems to auto-add deps, and it does so with exceedingly restrictive bounds. This probably encourages this sort of behavior as well.
GHC has lots of tests: - https://github.com/ghc/testsuite/tree/master/tests I know of two works that try to generate random test programs: - [Testing an Optimising Compiler by Generating Random Lambda Terms](http://publications.lib.chalmers.se/records/fulltext/157525.pdf) - [Lazy generation of canonical test programs (PDF)](http://www.cs.york.ac.uk/fp/jason-docs/ReichNaylorRunciman2012.pdf) I don't know if either works are available on Hackage. You could try emailing the respective authors if they can send you some code.
If I could freely exit a monad, then there's no problem about mixing effects. Given some `M a`, I could just extract the `a` and feed it into some function to generate `N b`. Then, if I like, I could extract the `b` and return it back in `M`. The problem, of course, is that I can't just extract a pure value in order to use it as I please. If I have that `a -&gt; N b` function then it's easy to convert `M a` into `M (N b)`. But the tricky bit is converting `M (N b)` into `M b` (or `N (M b)` or `N' (M b)` etc). It's that menagerie of conversions which gives rise to the need for transformer stacks and type classes to abstract over them. Whereas with comonads, since I *can* freely leave them, I don't care how the values will be used in the future; so this whole problem is moot. However, the dual problem (entering the comonad) is where we'd expect to run into similar complications.
Since your the designer, can you tell me how its different? I'm actually curious.
You could just suck down the current package contents of http://hackage.haskell.org. That should be pretty close to what you are looking for in terms of volume.
If you're into text adventures, why not write an adventure or even a roguelike in haskell? Even better would be if you can write it to communicate over telnet. Have your friends login and play the game that you wrote. Just remember that haskell encourages you to focus on the data (the gamestate) first, and design your algorithms around that data. Try to keep the IO surface of your program as small as possible, and you'll have something up and running in no time! And keep us updated on your journey. We'd love to see what you can do.
Elm is primarily focused on [Functional Reactive Programming](http://elm-lang.org/learn/What-is-FRP.elm), aiming to make highly interactive GUIs, so the design goals are different: * Elm has a more flexible record system: [intro](http://elm-lang.org/blog/announce/version-0.7.elm#records) and [overview](http://elm-lang.org/learn/Records.elm). If you have used records in Haskell, you will appreciate this :P It is also really nice for games, as seen in some of the examples above. It also makes it fairly easy to have optional arguments for a function (strategy used in [Preselm](https://github.com/grzegorzbalcerek/Preselm)). * Graphical elements are first class citizens (see the [display examples](http://elm-lang.org/examples/Basic.elm)) * Elm compiles to "the web platform", so it works on any modern browser out of the box. * Evaluation strategy is strict, not lazy, to ensure predictable memory performance and to play nice with FRP and concurrency * [the syntax](http://elm-lang.org/learn/Syntax.elm) of Elm [does not overlap exactly](http://elm-lang.org/learn/Syntax.elm#things-not-in-elm) (note: type annotations and type aliases are in the dev branch and will be in the next release) * Currently no typeclasses or monads or any of that. We are currently discussing more flexible ways of doing these things, so they will come in time and they will come with more accessible names (my reasoning here is "teach addition before group theory"). Also, no IO monad, everything happens with signals. * Elm is ~1.5 yrs old and Haskell is 20+ yrs old, so Elm is less mature. I use both Elm and Haskell depending on what I want to do: Haskell for compilers and servers (such as the Elm compiler and server!) and Elm for graphics, web-pages, animations, games, and anything more human-facing.
Hi, for me it was almost the other way around - I really enjoyed Euler but after all getting stuff done *is* mostly (and sadly) UI and stuff like graphics, database, web and so on - so I am looking for some non-ebony-tower stuff, and games are fun so ... After watching a very great video on Sokoban (http://www.youtube.com/watch?v=mtvoOIsN-GU) I had my own go and really enjoyed it (even if the game is not really finishe - guess I got bored out by the highscore stuff but the basiscs are working ok): https://github.com/CarstenKoenig/SokoHaskell maybe you get some inspiration from there. Aside from this the web-stuff is always worth looking into but it's kindof the point where Haskell can get very nasty with advanced stuff you just don't find in your friendly learning resources like LYAH (YESOD comes to mind ...)
Still haven't looked at the code, but if you write a correct cabal manifest you won't have to worry about external dependencies, cabal will fetch them for you! So no need for "cabal install network-info cereal" :)
Of course, there's a boring bijection between T and T^n for n &gt; 0 because T is countably infinite. So Blass's paper is more interesting in that it tells us about an *interesting* bijection.
So, here's an idle question: what's the Taylor series for sqrt x? (Nevermind, just looked it up. Damn...) Also, I have the vague feeling that the topic of continued fractions is going to come up in the near future.
Interestingly, there isn't a Taylor expansion for Sqrt[x] around the point x=0. The statement of Taylor's theorem starts "Let k ≥ 1 be an integer and let the function f : R → R be k times differentiable at the point x ∈ R." But Sqrt[x] isn't even once differentiable at x = 0, so the condition k ≥ 1 isn't satisfied and there isn't a Taylor series there.
&gt; The new parallel I/O manager scales much better than the current one*: the number of requests per second scales almost linearly up to 32 cores I believe. Perhaps Andreas could post the numbers. I'm sold.
I'm sure this is awesome but I'm not sure why. Can someone explain?
The I/O manager handles quite important things at the runtime. You can find more about this here: http://stackoverflow.com/questions/4446700/what-io-activity-does-the-ghc-io-manager-support These patches will make the I/O manager scale much better as the number of cores it runs on grows. That was an issue with the previous one.
Elm looks very interesting, ill check it out later!
Thanks, that link was helpful. I need to learn more about concurrency. 
The source for "Lazy Generation of Canonical Test Programs" is on [GitHub](https://github.com/jasonreich/ProgGen). However, it currently only generates a first order subset of Haskell although the paper includes suggestions for improvements. You can see me using it to naively test a [supercompiler implementation](https://github.com/jasonreich/FliterSC).
There would be the problem with mixing effects that you wouldn't be mixing them. You would be changing between different effects, but their only connection would be the "returned" value. It wouldn't be very interesting to leave monads. In a sense you're right about it being costructure, since it's cokleisli arrows instead of kleisli arrows, but: 1. It's still called structure. 2. It's about exiting the comonad, not entering it. For example, comonad typeclasses could look like this: class ComonadReader w env | w -&gt; env where conf :: w a -&gt; env class ComonadStore w ix | w -&gt; ix where pos :: w a -&gt; ix peek :: ix -&gt; w a -&gt; a
Does this I/O manager work for the majority of the ghc users? I.e., does it work on Windows?
Project Euler is mostly about finding a clever brute force algorithm. It won't teach you Haskell at all in my experience. Even now when I'm looking at my PE code files, I'm happy when I discover something that's especially Haskelly. In normal Haskell, that should happen on a per-line basis.
Not being able to write stuff immediately was one of the major pain in Learning Haskell at first. Fortunately things are moving this way. Recently fpcomplete opened a beta for its School of Haskell. Ask for an invite. It actually contains few but very good interactive tutorials. https://haskell.fpcomplete.com/beta Once you'll have an account you'll can check mines at: https://haskell.fpcomplete.com/user/yogsototh Also, it took me some time before making some first projects. Here are the list with links to help you: level 1: [ASCII Art Mandelbrot set generator Bonus animated](https://plus.google.com/117858550730178181663/posts/GoycejFfmq3) level 2: [A mandelbrot set generator taking advantage of purity to generate it in parallel](https://github.com/yogsototh/mandelbrot) level 3: [A progressive example, OpenGL Mandelbrot set, from 2D and imperative paradigm to 3D and functional paradigm](http://yannesposito.com/Scratch/en/blog/Haskell-OpenGL-Mandelbrot/) level 4: [A reddit like website](http://inmanis.herokuapp.com/) I didn't released the code yet. Also, there is a short and neat [tutorial](http://jamiltron.com/2012/07/Code_Us_Some_Roguelike_in_Haskell.html) explaining how to make a rogue-like game in Haskell (the imperative way thought).
Hmm, does the Haskell Platform usually skip over a version of GHC?
When was the last time that GHC has not fully supported Windows?
Are you missing name, start and projectBuilder arguments to the project function? project :: String -&gt; Day -&gt; Writer (Endo Project) a -&gt; Project project = appEndo (execWriter projectBuilder) Project { projectName = name , projectStartDate = start , projectEndDate = Nothing , projectTasks = mempty } Or am I reading it wrong?
I believe I suggested this to you, if I remember correctly. Nice post!
This looks like a right fold. Does this imply that you couldn't use this technique on an infinite stream? I've become interested in Event Sourcing [1], [2] of late and I wondered how you would best model a left fold of many different types of events onto an aggregate value. This looks like that idea in reverse, where have the stream of events collapsed into the empty object at the end. [1] http://cqrsguide.com/doc:event-sourcing [2] Haskell implementation: https://github.com/tcrayford/data-ringbuffer
There have been quite a lot of discussions about this. GHC 7.8.1 most likely won't make the next HP. It contains many new things, including ones that need to be tested and worked with for a while before clamining it's stable and reliable for production. You can read about about this on this thread: http://www.haskell.org/pipermail/ghc-devs/2013-February/000309.html
I think you meant WriterT instead of Writer, when you talk about involving IO actions to get data from bugtrackers and whatnot. Aside from that, nice post.
Even if it doesn't work on Windows, it still works for the majority of GHC users... :)
Where would you recommend to start? The RWH book is good but it is sort of dry. I am trying to find some project that will keep me motivated. Haskell is pretty sweet, and I can see where it would be useful, but it seems to have a learning curve that is larger than other languages. It's probably because I am not used to thinking this way. I know several languages and I use some of them professionally and still, I spent a month on learning Haskell (maybe 10 hours a week) and still can't do much useful.
They pay for him to do research, and some of that research manifests as code in GHC.
Windows is absolutely a second class platform for GHC (and MacOS is more like third class...). But, &gt; But, also, I don't see Microsoft promoting Haskell very much. Instead, they let SPJ and the functional friends think of and field-test powerful concepts that they then adapt in their "real" languages, e.g. LINQ, F#... Right. Microsoft Research is actually a pretty awesomely forward-thinking organization. They hire researchers and give them a lot of flexibility to work in the ways they think are best, and do not tie them tightly to Microsoft's product offerings, marketing focus, etc. Kudos to them for it.
My point is that there isn't a language/runtime/compiler that isn't specifically made by Microsoft that treats Windows as a first-class platform, i.e. equally as important as UNIX/Linux. GHC comes fairly close to having the same performance and features for Windows as it does there.
When was the last time it did? 
Fixed, thanks for noticing!
The timing is great. I wrote this a couple of days ago: https://gist.github.com/MasseR/4749508
Perhaps some company that depends on GHC on Windows to work well could sponsor some Windows development? ;)
Haha. Yes, that is the problem. I am spoiled by being able to learn new procedural or OOP languages faster because I already know the concepts. Functional programming on a meta-level makes sense to me, i.e. currying, no side effects, type safe, function composition, etc. but holy shit Haskell has much more to it than that. Thats part of the reason learning it is #1 on my to-do list. Thanks for the advice.
I think it would take perhaps a month or two of work to get the current generation of the I/O manager working well on Windows. The platform APIs are quite different than on Unix, but not in a way that is fundamentally incompatible. What we lack is Windows developers. Everyone who develops system libraries primarily uses Unix, and we do our best to support Windows as we go. If you want to actually see Windows move forwards as a supported platform, you are welcome to either submit patches or engage consultants who can do the work. I'm otherwise very, very tired of hearing complaints from Windows users that they're not being well served. If they are in a huge numeric majority and yet do almost nothing to help in a community that is mostly powered by volunteers, I find my stock of sympathy severely depleted by each fresh complaint.
I wanted to get this out of my blog queue so it's rather light on details. Hope you find it interesting.
I have under my hat a plan to add a `--loose-upper-bounds` option to `cabal configure` and `cabal install`, but that may take a while to come to fruition.
[Cabal is a system for building and packaging Haskell libraries and programs. It defines a common interface for package authors and distributors to easily build their applications in a portable way. Cabal is part of a larger infrastructure for distributing, organizing, and cataloging Haskell libraries and programs.](http://www.haskell.org/cabal/) A package is essentially synonymous with a library. If I say "cabal install mersenne-random", cabal will download the [mersenne-random](http://hackage.haskell.org/package/mersenne-random) package from [hackage](http://hackage.haskell.org/packages/hackage.html), compile it, and then install it on my local system. I'll then be able to say "import System.Random.Mersenne" in a haskell source file, and use the mersenne-random library in my code.
We skipped over 7.2.x completely, partly for timing reasons and partly because the ghc devs told us it was a somewhat experimental release to get new features out and tested by users.
OSX ships with telnet
I was actually reading that exact paste in the supermarket while doing food shopping yesterday, which motivated this very post! Fancy that :) (I googled 'writer endo monad' and it's on the first page at the moment).
Yes, I was just pointing out that the T ~ T^n iso is expected, and that people should read the paper because it's actually saying something more interesting than just "there's an iso between T and T^7".
http://hackage.haskell.org/trac/ghc/ticket/7353 (ignore the title of the ticket, it's about the IO manager on windows)
Is the Endo the "trick" also Sum and Product use? I vaguely remember something like that :) Nice post though!
It will make Haskell servers be able to handle many more requests per second. It mostly makes sense if you write servers (or do something else that involves lots of concurrent I/O).
I would like to add that the *implementation* is async, the *programming model* is still synchronous. This a huge boon compared to the callback spaghetti and explicit state machine programming that you see talked a lot about today on e.g. Reddit or Hacker News.
I don't know, I thought that it always has. Can you give me an example of where GHC itself didn't support a feature in Windows?
In the convert example, you use guards to regress to Bool checking rather than pattern matching on the MetricUnit constructors. Why? Why oh why?
We have in the past had some very talented Windows hackers (e.g. Sigbjorn Finne), but they've all moved on. Position open! 
I think it's there as a convenient lead-in to deriving Eq.
And now, thanks to some great patches from people working on HTF, the `Diff` library has an interface that produces files that match those generated by the Unix diff utility (modulo some efficiency hacks the unix utility uses as input sizes grow large).
We have sponsored IHG to do various Windows related work, like the Windows shared runtime and the Win64 port. The I/O manager is not something we rely on, but if we need it we will sponsor work on that too. 
For general purpose (possibly infinite) merging we use conduits with some combination of these: -- do a merge sort synchronousMerge :: (Monad m, Ord) a =&gt; [Source m a] -&gt; Source m a -- or a priority merge, favoring sources towards the head of the list asynchronousMerge :: MonadResource m =&gt; [Source (ResourceT IO) a] -&gt; Source m a There may be faster or better ways of doing this though, depends on the application.
Haskell is also *more functional* because laziness creates semantic composability. So, you can use higher order functions more. 
&gt; (and MacOS is more like third class...) Really? The only issue I've ever run into on OSX was when the precompiled binaries decided to stop supporting older versions of XCode (it still compiles from source just fine). That's nothing compared to the stories I hear from Windows folks. Even if it doesn't get a lot of personal affection, the fact that OSX is a BSD and that Apple only offers Intel chips anymore both mean that OSX isn't that far off from Linux, Solaris, and any other *nix they support.
Most of http://hackage.haskell.org/packages/archive/pkg-list.html#cat:game seems to be bitrotten. Two games I wanted to try out: * http://hackage.haskell.org/package/armada * http://hackage.haskell.org/package/frag I wonder if we should start a haskell-games project, to maintain some of these gems so they build with the latest Haskell Platform.
GuiTV
projectBuilder is still missing.
Of course English has precedence rules, e.g. "one plus five times two equals eleven". ;)
My personal experience with this was very, very bad; but also limited, so I'm mostly saying this from what I hear secondhand. That said... the experience I had was really awful. I taught middle school students last year (age 12-13), in a "bring your own laptop" kind of setting, and half of them had MacBooks. Several times, we tried to install GHC and the Haskell platform so they could build local copies of their games. It went so badly that for several months, those students borrowed laptops from their classmates with Windows so they could see their programs running locally. Luckily, I taught most of the class using a web-based system running the programs sandboxed on the server -- so this didn't block them from working. It just meant they didn't get as much chance to have their programs on their own computers. But a good part of why I spent last school year fiddling with SafeHaskell and sandboxing was to work around students with MacBooks. Granted I only got to see those students a few hours a week, and when I did have teaching time, the last thing I wanted to do was spend it figuring out how to install whatever XCode component was missing or troubleshooting their OpenGL libraries. So I didn't spend a lot of time with it; this was one of those situations where 40 hours of my own time building a sandboxed Haskell server was less valuable than losing a few hours of teaching time with students' devices. Haskell on Windows may not perform great, or scale well; but at least you can type "haskell platform" in Google and have a working Haskell installation in 5 minutes.
Logged in just to upvote this- it's my favourite Haskell tutorial by far, and it teaches "by doing". This definitely helped me get into building bigger Haskell projects, and tackles a lot of real-world problems. You should definitely try it out.
Oh look! My chart! For reference, the more recent version (and web site) is here: http://www.ozonehouse.com/mark/periodic/
HaRe.
Oh, that's nice. Judging by glancing over the TODO, it seems to be a complete rewrite. There shouldn't be much left if you change parser/pretty printer and kick out everything that uses strafunski.
Thomas Woolford is my name
&gt; They most of the times lacks a sophisticated type system to help us catch bugs. Granted, they do have sophisticated mechanism under the hood to do type inference What? No they don't
The ideas are left!
Great, kudos to you and HTF guys! Would it be possible to support the unified diff format as well?
Doesn't this imply that all the events and the aggregate have to be the same type?
You need to unify the types first, e.g: asynchronousMerge [src1 &gt;+&gt; C.map Left, src2 &gt;+&gt; C.map Right]
I think it overreached in scope with its "emacs/vi/..." and "console/gtk/..." approach. This was visible even when it still built by e.g. the fact that apparently nobody ever started it with the default colors on a white on black console window because some of the default colors were black on black that way.
The "thing" under the square root is 1 when a=0, so the "good" function to turn into a Taylor series at 0 is λx.√(1+x) (or equivalently, λx.√x at 1).
Oh I didn't know this one, it looks seriously useful. Too bad it is not maintained.
Yi is more active than ever but not as active as it should be. It needs users and contributors and has a lot of potential as the Xmonad of programmable editors.
I suspect that you misremember. The fold is just implemented using the Monoid type class. Endo is also an instance of this class, which may be the source of your confusion.
This is incredibly cool; Does someone know something about the stability (e.g. experimental and it will remain so or there are plans for a production-quality package)
&gt; Sometimes we forget that functional programming focuses on two different aspects: &gt; Purity &gt; Type safety We certainly forget it because it's completely wrong. Haskell does, FP in general does not. 
From what I know, this only applies to Clojure and (*EDIT*) some Common Lisp implementations, and those are not even mentionned.
Going a bit far to say it's completely wrong. Purity and type systems are major components of functional programming. Haskell takes them to extremes.
CMUCL and SBCL both do type-inference without declarations, but ja, I don't think he meant those.
Why would light weight matter in this day and age? Gtk works fine from GHCi. Did it not use to?
Well, one thing against Gtk is that I gave up trying to install it on MacOS X.
Indeed, if you take "functional" to mean just having features that make it easy to operate on functions: first-class functions, static nested scope, lambdas, sometimes partial application, and you take typed to mean just statically typed (hopefully that won't start a language war here, at least)... then there are 8 possible combinations: NOT functional, NOT pure, NOT typed: Original BASIC, PHP NOT functional, NOT pure, YES typed: C, Pascal, Java NOT functional, YES pure, NOT typed: SQL, Spreadsheet expressions NOT functional, YES pure, YES types: ??? YES functional, NOT pure, NOT typed: Lisp, Ruby, JavaScript, Python YES functional, NOT pure, YES typed: ML, sort of C# YES functional, YES pure, NOT typed: Erlang (mostly) YES functional, YES pure, YES typed: Haskell There is only one that I don't know how to fill in, and that's due more to the paucity of popular pure languages than any correlation.
http://community.haskell.org/~ndm/catch/ That was part of the set of tools xmonad used to ensure the level of reliability they achieved. It would be great to get it working again.
I would say tailcall elimination or some means of unbounded recursion is necessary for functional programming, so that rules out Ruby, most implementations of JavaScript and Python, which sounds right, as I wouldn't call them functional languages.
I was reading the scala style guide recently. They have style suggestions for indicating that code is impure vs pure, but I feel like it would be fairly easy and greatly beneficial to start enforcing purity somehow beyond just val versus var.
I'm not sure I want to classify languages based on the optimization choices of their compilers. Would Python then become a functional language if more of its users moved to a different implementation? I'd say the reason that JavaScript, Ruby, and Python don't seem like functional languages isn't tail call elimination, but the fact that they aren't widely used that way today. But that's equally problematic. By that standard, I don't know that I could call Common Lisp a functional language either.
sql is typed. or do i misunderstand something?
or all languages on the jvm. including the usually called functional ones.
&gt; Next up is showing people how types really help when writing sync-looking-async-code (e.g. using greenlets in Python or fibers in Ruby) to 'track' async'ness (and potential yield points, assuming non-preemptive scheduling). Do you know of any good articles on this?
Yes, tail call elimination is not a necessity. See Clojure loop/recur way to handle recursive calls while JVM cannot provide TCO. Just like "object-oriented" is applied to a very wide range of very different languages, "functional programming" is also a term that encompasses a lot of notions, the most important being that FP _is about transforming and composing data_. Like cdsmith says, a language will seem functional if people use it as such (i.e. make a heavy use of HOFs, data transformation and maybe recursion and a low use of references and mutability). Then you separate "functional languages" from languages "that _provide some support_ for FP".
I think Common Lisp is refered as a "destructive functional language": your code looks functional, but internally functions like mapcan will mutate your data to save memory, so you better not use the old references. But in the end it really is a multi-paradigm language (maybe the most multi-paradigm of all, as it provides equitable support for each paradigm, where Python's support for functional is intentionally lousy).
Any GUI toolkit. ;D (Not that I *want* to use them; it seems to be a law of software development that GUI toolkits be satanic.)
Hmm... maybe. I guess you could take the view that the current set of table definitions counts as part of the program text rather than state, and that the so-called "DML" subset of SQL is statically typed with an extremely large set of implicit type coercions. But if you expand SQL to include the "DDL" subset -- things like CREATE TABLE and DROP TABLE -- then you're forced to admit that column types are runtime state, and that SQL's type enforcement is an example of dynamic types, and not really a static type system.
Clojure at least has an explicit "recur" construct that allows unbounded recursion. "Strongly typed" is not a term that has any accepted definition. Certainly it does not mean "statically typed". It depends on the Lisp as to the style of code. Clojure or Scheme code looks quite functional. 
Clojure and Scala both can handle local tail recursion, Clojure explicitly with recur, and Scala does it with the compiler. Tail calls generally are not eliminated, but they do have mechanisms for unbounded recursion.
So we agree that Lisps are diverse and not clear cut. What about the actual question I asked?
Tail call elimination is not an optimisation, it's a fundamental difference in language costs semantics, as it changes space complexity characteristics. It's required to write code in a functional style, because without it the functional style imposes a huge space cost and can't efficiently iterate over data structures. The reason JS, Ruby and Python are not written that way is because you _cannot_ write code that way with those sorts of space characteristics.
&gt; Do you conclude that purity and type safety aren't major components of functional programming as a field? They're major components, mostly because Haskell and ML are major components of functional programming, but that's not the original statement being made. The original statement said that FP focuses on purity and type safety. Normally, we assume least fixed-point when people enumerate things this way, so it's natural to assume the statement means "FP focuses on purity and type safety _alone_", which is not true, when there is a wealth of functional languages that focus on neither. In other words, "purity and type safety" does not define FP, and FP does not indicate "purity and type safety".
Odersky was thinking about effect types, but he wasn't ambitious enough. Without regions effect types are pretty coarse.
I think it's based on the material from: http://www.spaconference.org/spa2010/sessions/session317.html
My favorite.
I wrote this myself (initially for a couple of Python/gevent-using colleagues): http://blog.incubaid.com/2012/04/02/tracking-asynchronous-io-using-type-systems/
Hi. HdpH should be regarded as experimental software. We've ran it on a few hundred nodes (a few thousand cores). This release marks the use of the network transport API ( http://hackage.haskell.org/package/network-transport-0.3.0.1 ), and so shares the same transport layer as Cloud Haskell - which was one of the main goals of splitting CloudHaskell into separate layers, having multiple Haskell middlewares. What we'd love to see is for someone to produce an MPI implementation of this transport API ;-)
Mmm, seems a bit reading into a statement too much. Afterwards in the post he brings up Scala, and names it as another functional language that doesn't focus on purity, in contrast to Haskell. I don't think the author was saying FP focuses on *only* these things, otherwise the rest of it wouldn't make sense. But w/e, this discussion is splitting hairs about splitting hairs.
I installed it via nixos a while back so you can still install it and try it out if nothing else. It is pretty easy to keep it running in parallel with a modern ghc.
It was a little bit of pay be a few months back I got it working via the nixpkgs. It took a little fiddling with the dependancies in the nix expressions but it worked well after that.
I'm sure its possible. Patches welcome :-P
I stopped working on this for now, since I'm well overdue on a contract, and can work around it easily enough using [socket timeouts][1]. My goal was mainly to fix a semantic issue: network I/O can't be interrupted on Windows, as it uses blocking FFI calls. The program I'm writing doesn't need to do a lot of I/O, it just needs to run unattended for long periods of time without hanging for strange reasons. I wrote an [I/O manager using IOCP][4], but it involves sending all the overlapped I/O to one or two dedicated threads, meaning one or two context switches per operation. This is because the Windows overlapped I/O functions are sensitive to the calling thread. For example, when a thread exits, its pending I/O is canceled. This may help application writers, but is problematic for us, since the GHC RTS moves lightweight threads across OS threads. Thus, implementing an *efficient* IO manager for Windows will involve some integration with the scheduler. Fixing the I/O situation on Windows is certainly possible, but takes more work than I have time to do right now, and will likely involve some API changes, such as: * Deprecate [hWaitForInput], [hGetBufNonBlocking], and [hPutBufNonBlocking]. Nonblocking I/O isn't available for most device types (and not just on Windows; consider seekable devices), and is a pain to compensate for on Windows. * Rethink the [BufferedIO] interface (see [#4144][2]). Currently, [hGetBuf] and [hPutBuf] sidestep it using casts to operate directly on the device, meaning custom Handles [still aren't possible][3]. * Treat [threadWaitRead] and [threadWaitWrite] as Unix-specific. On Windows, only sockets support these operations, through [select]&amp;nbsp;(which doesn't scale without tricky thread pooling) or [WSAEventSelect]&amp;nbsp;(which scales so-so (up to about 25000 before running out of address space) and cancels other [WSAEventSelect] calls). [1]: http://hackage.haskell.org/packages/archive/network-socket-options/0.2.0.1/doc/html/Network-Socket-Options.html#g:6 [2]: http://hackage.haskell.org/trac/ghc/ticket/4144 [3]: https://github.com/haskell/network/pull/79#issuecomment-11637676 [4]: https://github.com/joeyadams/packages-base/blob/windows-iocp/GHC/Event/Windows.hs [hWaitForInput]: http://www.haskell.org/ghc/docs/latest/html/libraries/base/System-IO.html#v:hWaitForInput [hGetBuf]: http://www.haskell.org/ghc/docs/latest/html/libraries/base/System-IO.html#v:hGetBuf [hGetBufNonBlocking]: http://www.haskell.org/ghc/docs/latest/html/libraries/base/System-IO.html#v:hGetBufNonBlocking [hPutBuf]: http://www.haskell.org/ghc/docs/latest/html/libraries/base/System-IO.html#v:hPutBuf [hPutBufNonBlocking]: http://www.haskell.org/ghc/docs/latest/html/libraries/base/System-IO.html#v:hPutBufNonBlocking [threadWaitRead]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Concurrent.html#v:threadWaitRead [threadWaitWrite]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Concurrent.html#v:threadWaitWrite [select]: http://msdn.microsoft.com/en-us/library/windows/desktop/ms740141%28v=vs.85%29.aspx [WSAEventSelect]: http://msdn.microsoft.com/en-us/library/windows/desktop/ms741576%28v=vs.85%29.aspx [BufferedIO]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/GHC-IO-BufferedIO.html
And here's the announcement for the next meetup: http://www.meetup.com/NY-Haskell/events/104481892/
And here's the announcement for the next meetup: http://www.meetup.com/NY-Haskell/events/104481892/
I was trying to brush up the tardis package, when ghc 7.6.2 suggested that I should use `RecursiveDo` instead of `DoRec`. Of course, ghc 7.4.2 suggests just the opposite, asserting that `RecursiveDo` is the deprecated one, and `DoRec` the recommended one. &amp;#3232;\_&amp;#3232; In fact the two are not interchangeable. If I replace `DoRec` with `RecursiveDo` in the code base, then the code fails to compile in ghc 7.4.2. &amp;#3232;\_&amp;#3232; So then I tried and failed to use the C preprocessor. I learned that the way to ascertain what version of ghc you are dealing with is via `__GLASGOW_HASKELL__`, which is just a single integer. &amp;#3232;\_&amp;#3232; I was ramming my head against the wall, not figuring out what was wrong with the CPP, so eventually I ended up raging and just trying to turn off deprecation warnings (and sticking with `DoRec` for now). The flag for this is the very longwinded `-fno-warn-warnings-deprecations`, and it turns out that this flag doesn't even work. &amp;#3232;\_&amp;#3232; Not my best experience with GHC...
All functional languages are "destructive" in that sense. The functional paradigm governs how you logically structure your program, *not* what a compiler can do to take advantage of the FP-style structure you impose on the program. 
Instead of wanting to see a package resurrected here on reddit, just resurrect it yourself for dead projects (fork it, patch it, maintain it) or for slow moving project just get involved ! It goes a long way to have more people actively contributing to packages.
Thanks :)
The last two lines srsly made me chuckle. Only smile to be seen on this train today I'd bet :-) **edit** damnyouautocorrect!
 -XRecursiveDeprecation
&gt; The flag for this is the very longwinded -fno-warn-warnings-deprecations, and it turns out that this flag doesn't even work It works, but it's for functions and types (which are annotated with the `DEPRECATED` pragma), not "internal" deprecation warnings like those for flags or pragmas.
The content in english is [here](https://github.com/kazu-yamamoto/unit-test-example/blob/master/markdown/en/tutorial.md)
https://github.com/timthelion/fenfire/blob/master/RESURECTION-NOTES.md It took me about a day to do that one... And it was 6 years since it was last build-able! I think you're a bit quick to judge me ;)
OMG! The black monolith is behind him! Cool video. I'm glad to see more Haskell activities going on.
 Warning: -XRecursiveDeprecation is deprecated: use -XDeprecationRec or pragma {-# LANGUAGE DeprecationRec #-} instead
I don't know if Michael is checking the reddit comments, but I'll repost it there if he doesn't see it here. My main observation is that RankNTypes causes problems when users try to define functions of pipes. Now, I actually consider it bad style to do so, especially in library code, but there are some cases where it's warranted and users will do it anyway. In those cases, the user will get a rank warning from the type-checker and they have to enable the extension to define the function. However, it's still very useful. If you go with the RankNTypes approach then you can get rid of the extra type variable and the user never needs to convert between types manually, as you have to do with `pipes`. So there's no obvious solution to this tradeoff as there are advantages to both approaches. `pipes` gets away with not having the leftovers in the type by default by defining them as a proxy transformer. However, I know that Michael would prefer a single type and a monomorphic composition operator, so I think his choice is probably the next best alternative. I also agree that Michael should definitely not switch to `pipes` yet. `pipes` still lacks many basic utility libraries that he needs for `yesod`. I'm working on these, but I'm currently on vacation and they take time.
I am really excited about this. Kinda freaking out a little.
Love the reference to [this](http://www.gnu.org/fun/jokes/ed-msg.html).
I don't see objects here, it's only post-fix function application and transformation of data. Most of object oriented features are mentioned in these slides: http://lucacardelli.name/Talks/1996-10%20A%20Theory%20of%20Objects%20(OOPSLA%20Tutorial).pdf
pg(12) Slide 45. Untyped Object Calculus, "An object is a collection of methods. Each method has a bound variable for self. and a body that produces a result. The only operations on objects are method invocation and method update." I think tekmo covered those bases. edited in the following claim. "The combination of w[type constructor], extract, and extend is a comonad"
I feel the urge to bring up my inner Racketeer's criticism of Haskell: that `do` notation is Haskell's only macro. What we are trying to do with `pipes`, `conduit`, and friends, is to essentially create a DSL for stream programming. Haskellisms, however, restrict us to two forms of DSLs, those that fit into `do` notation, and those for which we can use visually suggestive infix operators. If we had a better macro system, then it would be easier to create the ideal stream programming DSL, without having to shoehorn it into the traditional Haskell style.
oh! this is great! I experienced this when I develop hoodle and got rid of all IORef by using FreeT monad transformer for purification. I made an OOP system in terms of coroutine and I realized that this was isomorphic to Store comonad ( Thermostat in OP's example). I was looking for a nice written-up for comonad-to-object correspondence. This is the most illuminating article that I have read these days!
In the iterator example, extend recurses over the stream and calls the function for the whole stream starting at the current point. So next3 for instance would choose the third element from each position and so the whole stream is shifted by three positions. But is this not rather inefficient? Would it not better in terms of performance just to do the shift, as this needs to be done only once and not for every element? Is it possible to write a more optimized version of extend, which takes this into consideration?
The ability to exist in a context isn't really the same as a self pointer / recursive type. It's true that comonads are about data more than control flow, but I certainly don't think the object connection rings very true. In my mind the first and third examples aren't very compelling, and calling a stream and iterator doesn't actually help clarify what's going on. Comonads are useful, and in a sense they relate to the "encapsulation" quality of objects (or at least the costate comonad does), but most useful comonadic patterns I know of don't really map well to how i think of objects.
Like the 'pure' keyword in Rust?
I showed this to my Java dev coworker, and he grinned maniacally... If I find the code-base has been suddenly converted to OO style out from under me, I'm blaming you, Tekmo.
You know, I actually started to! And it was working great until I started parsing complex expressions. I looked around but sadly I didnt find alot on the topic.
&gt;What we are trying to do with pipes, conduit, and friends, is to essentially create a DSL for stream programming. Is that a good thing though? I don't want to learn dozens of specific languages to be able to read and write haskell code. I already know haskell, so when I use haskell libs to write haskell code, I'd really like to be able to do it in haskell. There needs to be really huge gains of some sort to justify making everyone learn a specialized language.
This is excellent, you really have a knack for relating Haskell and Categorical concepts back to "real world programming." I feel that you missed an opportunity to talk about `lens`, though. Lots of the example comonadic code seemed to take the form: thing # method this # a this # b this # c With lenses we would just be doing thing^.a.b.c A little more elaboration on how lenses compare, and what additional power you get from the "prompt" notation, would be much appreciated.
Well in the end it's a classic tradeoff between flexibility and convenience. I just wish that Haskell at least *provided* the possibility of more syntactic flexibility, rather than forcing me to choose the convenience that comes with current Haskellisms. Convenience should come by convention, not by the lack of flexibility.
Isn't this solved by quasiquotation?
&gt; How about comonads that live in a Kleisli category - won't they need some kind of domethod notation? We need a categorical language. The funny thing about comonads that live in a Kleisli category is that they end up being exactly equivalent to the pure versions so you gain no extra power. For example, if you take that idea to its logical conclusion, you get the following generalized version of cokleisli composition: (f =&gt;= g) w = g =&lt;&lt; (f &lt;&lt;= w) ... and then if you derive the category laws for that and require that the the generalized store observes those laws, you derive the following definition for `extend`: extend k (Store s f) = return (Store s (\s' -&gt; k (Store s' f))) In other words, it punts and uses a trivial embedding of the comonad in the Kleisli category using `return`. If you then were to generalize `method` notation to desugar to a monad, you get: extend :: (w a -&gt; m b) -&gt; w a -&gt; m (w b) method this # f this # g = \w0 -&gt; do w1 &lt;- extend f w0 extract (extend g w1) ... where `extract` has type: extract :: w a -&gt; m a Note that if we insert our generalized `Store` into that, the `return` cancels out with the binds to recapitulate the original `let` notation, and our solution turns out to be isomorphic to just using `Store s (IO a)` using ordinary pure comonads. You get the exact same behavior if you try generalizing other comonads to the Kleisli category. All that is a fancy way of saying that you gain nothing by generalizing comonads or `method` notation to a kleisli category. Comonads have this weird property where they always "resist" interleaving monadic effects in between comonadic steps. Perhaps the easiest way I can articulate why is that comonads always allow you to backtrack and not perform any computation at all, but you can't do that if you interleave side effects in between steps. The best you can do is just return the target monad as the final result and then run that within a `do` block. &gt; It seems to me that part of what makes this so nice is that method notation simulates the state monad. Could that mean something? It simulates state in the same sense that the Kleisli category or the ordinary category of functions simulate state by having intermediate steps. &gt; I have trouble seeing what the parameter to the comonads means in general. For builders/iterators, it's what they produce. For thermometers, it's... what to display? My intuition for this is that the parameter is the comonad's "representation" or "summary". For example, if we took the suggestive prompt notation of `method` syntax to its logical extreme, that parameter is the type of what the prompt would display at that point in the computation (assuming it were showable). However, I want to again caution that any attempt to embed effects to actually implement such a prompt will fail because comonads like to backtrack and you can't backtrack from side effects. It still makes a good mental model, though. &gt; Cofree f - the simplest class with methods listed by f? I believe so. I want to think about it some more first, though, because I believe that in the same sense that `Free` is the best approach to understanding `IO`, `CoFree` should be the best approach to understanding `CoIO`. I predict that in the same way that `IO` is the dominant approach to structuring run-time computations, `CoIO` will be the dual dominant approach to structuring pure compile-time computations. To make a play on a joke made recently here, if `IO` is important for side effects, then `CoIO` should be "coimportant" (i.e. for pure computations).
If the stream type is an infinite linked list then I don't believe you can generate more efficient code. In order to reach the `nth` element you must traverse `n` steps of the linked list, so I don't believe there exists a more efficient way unless you use an entirely different data structure, such as a stream of arrays.
You're welcome! :)
You are totally right. I was meaning to add a note at the end saying something like "oh, and this plays REALLY well with lens post-fix notation", but I forgot to do so. I will do that a little bit later.
Builder is a `Traced` comonad. Thermostat is a `Store` comonad. Interator is a `Stream` comonad, which a memoized `Traced Nat` comonad.
found a relevant article along this line of discussion. http://yi-editor.blogspot.com/2008/12/prototypes-encoding-oo-style.html p.s. from your id, are you the author of the article?
I must be misunderstanding you because every Monad is a Comonad over its Kleisli category, and there definitely are other impure comonads over Kleisli categories.
I'm talking specifically about arrows of the following form: (Comonad w, Monad m) =&gt; w a -&gt; m b You can view such an arrow either as a comonad in a Kleisli category or a monad in a CoKleisli category. I was discussing the concept of comonads in Kleisli categories, which is what I presume tailcalled was asking about.
... these sound like song lyrics to me. o.0 
[This help?](https://github.com/m4dc4p/haskelldb/wiki)
But that's not quite the same thing; that's what *ekmett* called `Dikleisli`. A comonad in a kliesli category would have the operations: kextend :: (KComonad w) =&gt; Kliesli (ComonadBaseMonad w) (w a) b -&gt; Kleisli (ComonadBaseMonad w) (w a) (w b) kextract :: (KComonad w) =&gt; Kleisli (ComonadBaseMonad w) (w a) a With the laws: let f0 =&gt;!= g0 = Kliesli (\wa -&gt; kextend g0 $ runKleisli f0 wa) in f =&gt;!= kextract == f kextract =&gt;!= f == f (f =&gt;!= g) =&gt;!= h == f =&gt;!= (g =&gt;!= h) In other words, *the comonad is dependent upon the monad it's built on top of* in its fundamental operations! `class KComonad` has an associated type family that states what monad the comonad is built on. More generally: class (Category (ComonadBaseCategory w)) =&gt; GComonad (w :: * -&gt; *) where type ComonadBaseCategory w :: * -&gt; * -&gt; * gextend :: ComonadBaseCategory w (w a) b -&gt; ComonadBaseCategory w (w a) (w b) gextract :: ComonadBaseCategory w (w a) a gduplicate :: ComonadBaseCategory w (w a) (w (w a)) gduplicate = gextend id class (Category (MonadBaseCategory m)) =&gt; GMonad (m :: * -&gt; *) where type MonadBaseCategory m :: * -&gt; * -&gt; * gbind :: MonadBaseCategory m a (m b) -&gt; MonadBaseCategory m (m a) (m b) greturn :: MonadBaseCategory m a (m a) gjoin :: MonadBaseCategory m (m (m a)) (m a) gduplicate = gbind id
Sure, you could, but those aren't all of the comonads over `Kleisli m'. For example: {-# LANGUAGE FlexibleInstances, MultiParamTypeClasses #-} -- I use these Kleisli variations on standard classes for the following. class Monad m =&gt; KFunctor m w where kmap :: (a -&gt; m b) -&gt; w a -&gt; m (w b) class KFunctor m w =&gt; KComonad m w where kextract :: w a -&gt; m a kextend :: (w a -&gt; m b) -&gt; w a -&gt; m (w b) data KStore m s a = KStore (s -&gt; m a) s instance Monad m =&gt; KFunctor m (KStore m s) where kmap f (KStore g s) = return (KStore (f &lt;=&lt; g) s) instance Monad m =&gt; KComonad m (KStore m s) where kextract (KStore f s) = f s kextend f (KStore g s) = return (KStore (\s -&gt; f (KStore g s)) s) I verified one of the easier comonad laws but haven't done all of them. I'm very certain they hold, and this thing might actually be interesting.
I felt rushed to put something out there. What I mean by fungible objects. Objects whose differences don't matter to the caller. As in David Deutches description of the Uncertainty Principle. [http://www.themoneyillusion.com/?p=19290!](http://www.themoneyillusion.com/?p=19290) . That's not a very good description, but it's where I got it. 
Exactly.
If you unwrap the Kliesli newtypes, your `kextend` and `kextract` functions have the exact same types as the ones I proposed, other than the use of type families to identify the dependent base monad: kextend :: (w a -&gt; CBM w b) -&gt; w a -&gt; CBM w (w b) kextract :: w a -&gt; CBM w b Moreover, if you define any KComonads, you will discover that they all observe the following isomorphism: (KComonad w) =&gt; Kleisli (CBM w) (w a) b ~ (Comonad w) =&gt; w (CBM w a) -&gt; CMB w b Further, you will discover that your `KComonad`'s `kextend` and `kextract` definitions relate to the equivalent pure `Comonad` by the following equations: kextend f w = return (extend f w) kextract w = return (extract w) ... and all your kcomonadic computations will be equivalent to the equivalent pure embedding. In other words, you can always embed a `KComonad` within a `Comonad`. Also, your composition operator does not type-check, and I'm reasonably sure when you fix it you will derive the same generalized composition operator I mentioned before: (f =&gt;!= g) w = g =&lt;&lt; (f &lt;&lt;= w) -- add newtypes as necessary I strongly suggest you define an instance of `KComonad` so you can see first-hand what I'm describing. I've repeated your exact same line of thinking and multiple variations on it for all kinds of comonads and I always end back at the trivial embedding of the equivalent pure comonad. You can certainly define comonadic operations that are dependent on a specific monad's operations, but those monadic operations will never actually get interleaved with the comonadic operations. The comonadic operations will keep guarding the monadic operations behind `return`, preventing them from being run.
Yeah. I have gone through so many variations on this process, and the only way to satisfy the KComonad laws ends up just embedding the pure Comonad according to the isomorphism I described. You can use comonads to assemble monads, but the comonad composition operator always refuses to execute any effects before the final result is assembled.
Ok, I think I'm beginning to understand your line of reasoning. You're saying that the reason that all the KComonad instance I've tried have failed is because I required them to be polymorphic over the base monad, which means that they must use the trivial embedding. However, that raises an interesting question. Let's assume for a moment that you were to find a type that took advantage of specific details of the base monad to define a novel instance for KComonad. Wouldn't you find that a bit ... odd, for the the type to have multiple instances for KComonad, one that is polymorphic over the base monad and one that is not? Anyway, I asked Edward the same question and maybe he can demonstrate a useful instance.
You definitely need the comonad to be a comonad over a particular Kleisli category. For instance on top of kleisli IO, you can make a memoizing version of Store that uses the IO monad to store a memo-table of the values it was tried at. I've done this to good effect before when working with a version of the .NET reactive API ported to haskell. I can't make observables form a `Comonad`, because i can't safely force them but they can form a `Comonad` over kleisli `Task`. The problem with building one of those silly Dikleisli constructions is that because of the distributive law you wind up running the same effects over and over. This is the problem with the original Uustalu and Vene paper on comonads for the essence of dataflow programming. You get a comonadic semantics but when you go to mix it with the monadic effects you wind up with this huge explosion because each distribution is building up entirely new 'monadic stuff'.
Given a coworker orks cows, would that mean a worker is someone orked by w's?
Ok, so I found one such that I couldn't figure out how it reduces to a pure comonad. I got it by applying the StoreT transformer to the base monad thought of as a comonad. My first guess is that it should look like a restricted form of StoreT Identity, but it's rather late and I haven't managed it yet. I've proved for sanity's sake that it satisfies the laws but perhaps my calculations are in error. data KStore m s a = KStore (m (s -&gt; m a)) s instance Monad m =&gt; KFunctor m (KStore m s) where kmap f (KStore mf s) = return (KStore (fmap (f&lt;=&lt;) mf) s) instance Monad m =&gt; KComonad m (KStore m s) where kextract (KStore mf s) = mf &gt;&gt;= ($ s) kextend f (KStore mf s) = return (KStore (return (\s' -&gt; f (KStore mf s'))) s) I'd appreciate it if you could demonstrate how it reduces since I didn't manage it myself.
How abou these: [My blog post on haskelldb](http://users.utu.fi/machra/posts/2011-07-15-haskelldb.html) [HaskellDB examples](http://users.utu.fi/machra/posts/2012-08-23-relalgebra.html). They are for an older version, but last I looked there weren't any too big changes with 2.2.1 and 2.2.2
Little typo at the end of the "comonad laws" section: The right hand side in the side-by-side comparison should be [...] this # g this # h
`KStore Identity` is trivially isomorphic to `Store`.
Your ed example was hilarious and all too realistic: ed help ? quit exit bye hello?
Sure, but what about KStore IO?
Looks interesting, and no I'm not. Just someone with a semi-similar name. 
It should really be: data KStoreT m s w a = KStoreT (w (Kleisli m s a)) s type KStore m s = KStoreT m s (KIdentity m) Which is dual to: newtype CStateT w s m a = CStateT (Cokleisli w s (m (a, s))) type CState w s = CStateT w s (CIdentity w) Thus, `KStore IO` is just like `Store`, except that `peek` has access to `IO`.
What I meant was: you can implement `next :: Int -&gt; Iterator a -&gt; Iterator a` as follows: next 0 (a :&lt; _) = a next n (_ :&lt; b) = next (n-1) b And you see, that this travels n elements only once. `next' :: Int -&gt; Iterator a -&gt; a` is implemented similary, but `extend (next' n)` (which does the same as `next n`) effectively gives something like that: extend (next' n) (a1 :&lt; a2 :&lt; a3 :&lt; …) = next' n a1 :&lt; next' n a2 :&lt; next' n a3 :&lt; … **Edit** This is obviously wrong. Instead of a1, a2 and so forth, it takes the streams started by a1, a2, …. But the conclusion is the same. So the cost of traversing by n elements is now payed for every element of the new stream, instead of only once for the whole stream. Or do I miss something?
You're right. The definition of extend does duplicate the cost for each element. I'll have to think if there is a way to reformulate the definition of the stream and extend so that the compiler can share computation between elements.
Thanks for the example. I was particularly interested in using effectful comonads for, say, browsing a file system or VCS history, so I was wondering if you've ever explored that tack.
And in the "Syntactic sugar for comonads" section, the second and third code blocks both end with `wb` but should both end with `wc` (I think).
Yes, I agree that your method notation is a step forward, but your talk of it being related to OOP confused me: your objects are not OOP-objects as I understand them.
It is simple. Just as a worker will nvert the de-base, a coworker will convert the code-base.
The proposed `method` syntax seems fundamentally sound, so I am going to constructively criticise it. * "prompt" syntax `w &gt; expr` * how do we distinguish between `&gt;` delimiting a prompt -vs- `&gt;` being a comparison operator? * my Haskell eyes want that to be `w -&gt; expr`... is there some deeper meaning behind the choice of `&gt;` that I'm missing? * `this` referring to the "current" object This looks potentially confusing in the same way that `return` is confusing: beginners assume it has the same meaning it does in other languages; then they wonder why Haskell "ignores" their `return` "statement" and continues with the rest of their function. Similarly, I suspect beginners would assume `this` would refer to the same object throughout the method, whereas not only does it refer to a different object on each line, those objects don't even have to have the same type. I'll suggest that `it` would be a better choice. But the important thing is that we pick a word that doesn't bring any baggage with it.
The [visual-prof](http://stackoverflow.com/questions/3276240/tools-for-analyzing-performance-of-a-haskell-program/3276557#3277111) answer below that one is pretty amazing as well.
Here's how it would desugar if you generalized `method` syntax to `KComonad`s \wa -&gt; do wb &lt;- kextend f wa wc &lt;- kextend g wb wfinal &lt;- kextend h wc kextract wfinal All the `kextend`s return a value wrapped in `return`, so you don't execute any monadic actions in between each step: \(KStore mfa sa) -&gt; do KStore mfb sb &lt;- return $ KStore (return (\sa' -&gt; f (KStore mfa sa'))) sa KStore mfc sc &lt;- return $ KStore (return (\sb' -&gt; g (KStore mfb sb'))) sb KStore mffinal scfinal &lt;- return $ KStore (return (\sc' -&gt; h (KStore mfc sc'))) sc mffinal &gt;&gt;= ($ scfinal) = \(KStore mfa sa) -&gt; do let KStore mfb sb = KStore (return (\sa' -&gt; f (KStore mfa sa'))) sa KStore mfc sc = KStore (return (\sb' -&gt; g (KStore mfb sb'))) sb KStore mffinal scfinal = KStore (return (\sc' -&gt; h (KStore mfc sc'))) sc ffinal &lt;- mffinal ffinal scfinal = \(KStore mfa sa) -&gt; do let mfb = return (\sa' -&gt; f (KStore mfa sa')) sb = sa mfc = return (\sb' -&gt; g (KStore mfb sb')) sc = sb mffinal = return (\sc' -&gt; h (KStore mfc sc')) sfinal = sc ffinal &lt;- mffinal ffinal scfinal = \(KStore mfa sa) -&gt; do let mfb = return (\sa' -&gt; f (KStore mfa sa')) mfc = return (\sb' -&gt; g (KStore mfb sb')) ffinal &lt;- return (\sc' -&gt; h (KStore mfc sc')) ffinal sa = \(KStore mfa sa) -&gt; do let mfb = return (\sa' -&gt; f (KStore mfa sa')) mfc = return (\sb' -&gt; g (KStore mfb sb')) ffinal = (\sc' -&gt; h (KStore mfc sc')) ffinal sa = \(KStore mfa sa) -&gt; do let mfb = return (\sa' -&gt; f (KStore mfa sa')) mfc = return (\sb' -&gt; g (KStore mfb sb')) h (Kstore mfc sa) Note that this reduces to only executing one action at the end just like the original KStore, although I hesitate to say it's isomorphic to the original store because I'm not sure that it is. However, the key point is that it does not interleaving actions between the comonadic steps. Instead, it just collects everything into one action you run at the end, which you could do simply by using an ordinary comonad that returns a monadic action and immediately running the final action: w a -&gt; m b
Yes, the `&gt;` is ambiguous because you can't distinguish it from a comparison expression. It's supposed to denote a `ghci`-like prompt, which is why I chose it. Perhaps I could just use `-&gt;` (i.e. flip the monad bind arrow), which I believe would not conflict with existing Haskell operators. I think I could find some semantic justification for `-&gt;` such as saying that "wa -&gt; expr" binds `wa` to `this` (or `it`) within that expression. I also like `it`, too, since the notation is also supposed to suggest some sort of `ghci`-like prompt, where `it` is the result of the last computation. I'd be happy with that, too.
So, my language of choice before Haskell was C, so I view OOP through the C-like lens of "structs + methods on them" and that's how my post interprets objects: intermediate structs with more information than the final result needs, and methods on those structs. There's a subset of object oriented programming, though, that's uncannily similar to the brand of OOP that I'm proposing, which is the JQuery-like fluent programming style, where every method returns a new object. You can think of `method` syntax as a slightly more intelligent fluent programming where it automatically knows when to return an object and when to return the naked value.
I stole it completely from this [brilliant satire](http://www.gnu.org/fun/jokes/ed-msg.html).
It's not like the decision to mutate-or-recreate can be entirely up to the implementation: if you reuse old references, you program could work on some implementations but fail with others. Common Lisp HyperSpec, for instance, explicitely provides non-destructive/destructive alternatives (mapcar/mapcan for instance) in the standard: mapcar *cannot* alter the list you give it. (See the "Side effects" section [for the nconc function](http://www.lispworks.com/documentation/HyperSpec/Body/f_nconc.htm#nconc) for instance) . I don't have a copy of CLtL within reach to check if this element is also specified in it.
Suggestion: make the binding explicit: method this -&gt; this # a this # method self -&gt; self # b self # c
You're welcome!
That's actually a really good idea. Do you think there should be an implicit default if the user doesn't specify the binding?
I think it (and all binders, really) should be consistent with lambda, i.e. no default unless it changes in the future. Also, coming from Scheme, I think all implicit bindings are inherently evil.
The lambda analogy makes the most sense. Explicit definitely sounds better.
Meh, Scheme is the easiest to write. Some even write Scheme in awk, does awk suitable for metaprogramming? i think not. Beside, the problem is with OP not clarifying what metaprogramming is or we simply don't know enough yet. Frankly though, the best existing metaprogramming language is probably **TeX**, notice that meta has semantics of "beyond", not just "for all" or something like that. If you buy the idea that programming or computing is to make life efficient, TeX definitely saves us manpower on publishing, without TeX one cant write (reasonably!) readable mathematics for everyone efficiently. Without fancy mathematics on the internet you probably aren't programming in Haskell or doing math today!
SO activity means that people are using Haskell, and asking questions. It also reflects on the community in two ones: one that the community actually exists, and also that the community is helpful. We wouldn't have reached 10k questions if there were never satisfactory replies.
I [wrote a long tutorial](http://chrisdone.com/posts/haskelldb-tutorial) about how I use it but I think this kind of stuff deserves to be on the wiki or in the Haddock itself.
It does provide it, what are you not able to do with th and qq? I still don't understand what this tradeoff is. It seems like it is just making things worse and gaining nothing. Can you give an example of what this DSL might look like that would be so compelling that it warrants having different syntax? We're just talking about reading data from src, doing something to it, and writing it to dst aren't we? I don't see how a DSL is even a consideration, much less a huge important thing.
It's a shame the 10,000th Haskell question was (to paraphrase) [“How do I translate this Haskell code into F#?”](http://stackoverflow.com/questions/14895412/remove-elements-during-infinite-sequence-generation)
In my course on functional programming I intriduce the fix combinator as part of the lecture on lazy evaluation, with an application to memoisation. There is really a neeed for the fix combinator here. See: http://www.cs.uu.nl/wiki/pub/FP/LectureSlides/9-LazyEvaluation.pdf from slide 10 on, Doaitse 
Also note that we only have roughly 1.5% unanswered (i.e. completely not answered, not just with unaccepted answers). That's an awesome ratio! That said, I'd like to see if we could knock it below 1%. :-)
That's only a humorous remark, but indeed It's striking that the best languages in your list are definitely in the second half, that is those with the least question asked... I suspect that comes more from selection bias (you listing languages you care about, plus languages with a lot of questions) than anything, but still...
A pain point for higher rank types is that let statements do not generalize in GHC by default anymore (breaking compatibility with Haskell98) which means local definitions require explicit type signatures to be as polymorphic as they need to be, which can be cumbersome to write.
I go though periodically and urge people who answered in comments to answer properly. I figure in a few more months, if they haven't done so yet, I'll just go in and post their comments as answers myself :-)
[There's always an opportunity to be a little bit friendlier.](http://stackoverflow.com/questions/14893001/authorized-connection-to-mongodb-in-haskell/14893748)
Hmm... Do you think the answer is not friendly enough? Why?
The answer is friendly, but downvoting the question and voting to close it before even asking the questioner to clarify his question... that was not friendly.
The hallmark of a well-designed language is that people *don't* have to ask questions about it. At the very least, this should be divided by the number of users get a measure of how often people feel they have to ask about it. On the other hand, to the extent that Haskell is a genuinely different language that comes introduces a new perspective (relative to the typical programmer), there *should* be a lot of questions about it.
Sorry, it's accidental. I edited the link now. It's really great that you've answered that question. Thank you!
Well, it's primarily a popularity metric, with a handful of predictable biases. Languages more people learn than actually use are likely over-represented (Haskell, among others), and StackOverflow has a slight bias toward Microsoft technologies because of its origins, so F# is probably over-represented (C# certainly is). Incidentally, if you plot number of new questions over time, Scala and Haskell seem to have a more clearly upward trajectory compared to other functional languages.
Stack Overflow is, in general, not friendly to poorly-asked questions, and the new-ish "review" system makes it far more likely that questions in niche-tags like ours will come to the attention of people with close votes waiting to be cast. If anyone thinks a question was closed unfairly (such as this one, which may have been very poorly written pre-edit but is a legitimate question) leave a comment to that effect, and either vote to reopen (if able) or catch someone on IRC who can.
Honestly, I suspect that the number of questions on SO *is* one of the better metrics for how many people are using Haskell--or at least, how many people are learning Haskell.
For a narrower perspective, [here's a graph of questions per month for various functional programming tags](http://data.stackexchange.com/stackoverflow/query/98002/questions-per-month-in-functional-programming-tags#graph). The fluctuations in Haskell questions seems to follow the academic year pretty closely. The same might be true of some of the others, but it's hard to tell with the ones that are all bunched-up together. However, Scala clearly doesn't. **Edit**: Here's another fun one: The Haskell tag has a pretty high [average answer score](http://data.stackexchange.com/stackoverflow/query/98010/tags-with-highest-average-answer-scores-among-tags-with-at-least-1000-answers#resultSets) compared to other tags.
I'm consistently amazed at how friendly the haskellers at SO are, and how much they teach me when I ask a question. Also, it's really cool getting both practical answers and in depth researchy answers to my questions. Thanks everyone for helping me out.
This is probably as terrible a metric as the Tiobe index, but it is interesting nonetheless. Here are a few others added in. I followed the same pattern of truncation for rounding: * *C#: 417k* * Java: 367k * *PHP: 344k* * *Javascript: 328k* * C++: 178k * Python: 164k * Objective C: 121k * C: 85k * Ruby: 65k * *Perl: 24k* * *R: 24k* * *BASH: 18k* * *Matlab: 15k* * Scala: 13k * Haskell: 10k &lt;-- yay! That's us! * Clojure: 4.5k * F#: 4.3k * *Fortan: 1.7k* * *Go: 1.7k* * OCaml: 1.3k * Racket: 0.8k * *COBOL: 0.38k*
- erlang 3.1k - coq 0.13k - agda 0.04 k - isabelle 0.004k - idris 0.003k
Have you seen [this](http://hackage.haskell.org/package/codo-notation)?
Nah -- we have over 2% by that metric. I went and counted the actually unanswered ones specifically. But I didn't know upvotes knocked questions off the list. Off to upvote some answers now :-P (only deserving ones of course).
And TemplateHaskell? (they're close to eachother, but not identical)
That there are more java devs?
Not the package, but I did read the corresponding paper. Half the reason I wrote the post was because I was unhappy with Dominic's desugaring. There were several things that rubbed me the wrong way about his desugaring: 1. The desugaring is complicated with all sorts of corner cases. I expected comonadic syntactic sugar to be as simple as `do` notation and to be easy to intuit and desugar. 2. It assumes that each intermediate stage needs to remain "live" within a certain scope, where the scope of liveness is defined by the block grouping structure. This breaks the associativity of the notation, in contrast with `do` notation where grouping and blocks are arbitrary and have no impact on the interpretation. It also seemed like a very narrowly useful feature. 3. It doesn't "match" the laws. What I mean by that is that the notation does not illuminate the laws, nor do the laws illuminate the notation. With `method` notation, the laws go hand-in-hand with the notation very well. The `codo` notation version of the comonad laws are not insightful and if anything the `codo` version of the laws expose the very corner cases in the notation, which he has to work around by rebinding certain values using `current`. 4. The notation doesn't build any sort of intuition for what is going on. Only the people who already get comonads understand the notation. However, there were several ideas I did borrow from his notation, most notably the idea that each line has an associated context and you must introduce an initial context to kick off the entire process. The only way I differed from him was to shift all the contexts variables down by one line to simplify the notation because I found that doing so introduces all sorts of nice syntactic properties and makes the notation more suggestive. To use Dominic's own terms, my notation is "context-oblivious" by default, whereas his is "context-aware" by default, and I found that the "context-oblivious" approach leads to much more elegant properties and that it's better to selectively opt in to context awareness (which you can easily do with `method` notation, but I didn't really discuss it in the post).
Thank you. I fixed it.
Wow, weird to see a blog post of mine from 2008 suddenly show up on here. That post was one of a series. Here are the other two, also from 2008: http://evan-tech.livejournal.com/246296.html http://evan-tech.livejournal.com/246692.html
Thanks. That might actually be kind of fun.
Oh Reddit, why won't you let me put a bulleted list inside a numbered list?
1. Are * you 1. sure?
Hint: add 4 spaces for each nested level
Don't forget THE comonad: the cofree comonad (and the equivalent transformer), which reside in the `free` package. I vaguely remember there used to be a Density comonad somewhere in one of Edward's packages. You can also add non-empty trees, zippers, and other non-empty data types. However, I find that for most of the non-empty data types that are comonads, the simplest solution is to formulate them in terms of `Store`/`StoreT`, where the state is the data type and the function is the view on that data type. Edward's packages are the canonical implementations on Hackage, namely `comonads` and `comonad-transformers`. Also, use the the comonad transformers whenever possible, with `Identity` as the base comonad. I will soon write an upcoming post explaining why you want to do that.
Yeah, I tried that. Looked fine through Pandoc on my laptop, suddenly appeared as preformatted text when I posted it. And yet you have it working fine in your comment. This is why I hate software.
Is there anyone that can explain what the: * **store** * **environment** * **traced** comonads *do*? As far as I could glean from the linked hackage pages the environment comonad "adds an extra value" that can be read. That seems a little arbitrary.
That's right. I fixed it.
Actually enum types are sum types. Union types (in the C family) are (kind-of) sum types too, but make it easy to confuse the summands.
There's Ed Kmett's streams package, which includes things that are like what you are calling streams and non-empty lists, as well as a bunch of slightly different things. And also, Ed Kmett's comonad package is DEFINITELY the comonad typeclass you want.
Environment is really simple but really useful. This post of mine has an example of its use: http://productivedetour.blogspot.com.es/2012/12/evaluating-probabilistic-cellular.html Also, I recently used Environment in combination with Data.Tree (which is a Comonad) to easily annotate each node of a tree-like type with the same context (my purpose was to associate each node of a Swing component hierarchy with the containing window, but that's another story). Here's the gist: https://gist.github.com/danidiaz/4967054 As for Traced, I had no idea of any possible use for it until Tekmo's recent comonad post. Traced is the Builder comonad mentioned in the post.
Would it be possible to also post a "dumb text" version of the tutorial somewhere as well?
Why don't you use a single atomically to read and write the new value? If you do it in two separate atomically, it means you can overwrite the results of a middle concurrent write: P1 -&gt; reads m P2 -&gt; reads m P1 -&gt; writes x on m becoming m' P2 -&gt; writes y on m becoming m'' Therefore the write of P1 has been deleted. Instead you should both read and write in a single atomically, which ensures serialization of P1 and P2. Here is a nice helper function appV: http://www.haskell.org/haskellwiki/Simple_STM_example 
When I think sum types I think of Either, like a tagged union in C.
Just like Python having closures doesn't give it real support for the functional paradigm, enum and unions doesn't give a language support for sum types. And enums are certainly not sum types in the algebraic sense, they're just flags, they can't combine types like Either does.
Nice! Why the monad, though? I would have expected that parsing into a pure data structure is the way to go.
PDF file is imperative data structure. It is designed for random access. E.g. PDF viewer will load only data necessary to render current page, but it is hard to predict what it will actually need. Some higher level layer can handle it better.
BTW, there is already haskell library with the approach you want: https://github.com/dylanmc/Haskell-PDF-Parsing-Library
Last modified 2 years ago and never released on hackage...
&gt; It is impossible (?) if you parse file into pure data structure. Thanks to lazy evaluation, this is actually possible.
Yes, but load data to where and random access on what? Do you load the PDF file into RAM first? Then you can parse on demand with lazy evaluation. Or do you keep the PDF file on disk? Then random access involves a lot of disk seeking.
I use random access input streams (based on `io-streams` library). So it is up to library user to decide. You can work with in-memory buffer, mmaped file, regular file, http stream (if http server supports seeking), http stream + memory buffer (if server doesn't support seeking), etc. Yes, random access == a lot of seeking. But not necessary _disk_ seeking. You can rely on disk caches or provide input stream with custom cache. It depends on memory/performance trade of, selected by library user, not by library author.
Well, I don't see how to do it. You can't just `unsafeInterleaveIO`, because you need to serialize all seek and read calls. Note: you can't parse PDF file sequentially. Conforming reader _should_ perform seeking and reading. And all the existent PDF readers do that.
It is better to keep most of your functions in `STM` monad and then run them when you actually need to use them inside the `IO` monad.
I definitely need to think about it more... It maybe be possible to implement having self-constraint primitives (seek-then-read). Anyway, if it is possible, then it can be implemented on top of the library. Thank you for the idea!
I did wonder about how to do that! Looks like the last line in the example does it. It's pretty elegant. appV fn x = atomically $ readTVar x &gt;&gt;= writeTVar x . fn Done here. https://github.com/honza/redish/commit/319e66c61565b1520e0f4af84108114bcb419bf0 Thanks!
This example from the above page probably gives the best example of what I mean: ['0' .. '9'] :: Set Char [1 .. 10] :: Vector Int [("default",0), (k1,v1)] :: Map String Int ['a' .. 'z'] :: Text
Don't Builder and Command examples boil down to fmap? Also, I think you're confused about what OOP essentially is. I highly recommend the presentation in Benjamin Pierce's TAPL. Also, be sure to read about the coalgebraic semantics of OOP, e.g. "Objects and Classes, Coalgebraically" by Bart Jacobs.
That is very ... Overloaded, although usually modules have only 2 or 3 overloadable types.
Stream comes in several variants in the [streams](http://hackage.haskell.org/package/streams) package. NonEmpty is in [semigroups](http://hackage.haskell.org/package/semigroups).
Would it be possible to also overload the cons (:) operator for non-list containers?
Fair enough. Of course, the question is whether this use case merits the additional API complexity (monad instead of pure data structure). I occasionally open a 20MB PDF file in Preview and the machine seems to cope, but I have never read a PDF file from the web that I didn't download first. (Also because web browsers and the Acrobat plugin do a bad job at streaming PDFs.) Personally, I would always opt in favor of a simpler API, but that is just me.
Initially I used `iteratee`. AFAIK it is the only IO library (except pure `Handle`s) that supports random access out of the box. But 1) it uses exceptions to implement seeks 2) iterators with and without random access support has the same type (I need them both, so it is important for me) 3) it is hard (but possible) to count number of consumed bytes. `io-streams` doesn't support random access out of the box, but it was easy to add. Plus easy API and good performance. Not sure about other applications, but for my case it seems to be the best alternative. Nice library, looking forward to see it on hackage. Thank you!
Yeah, even without this extension, it seems like `-XTypeOperators` should make `(:)` a regular data construction operator (and thus rebindable).
The problem is that : isn't really an operator, it's actually a type constructor for lists (the other one being called []). If : was overloadable then the type constructor would have to be renamed Cons or something.
Seems like it would be simpler to just make the `Enum` functions subject to `RebindableSyntax`. I'm not sure why they aren't already.
What is the use case for this library? Is this for viewing pdfs only? Can it create pdfs? add images to it? Right now i'm using commercial pdflib via C FFI. Works quite well. 
Well, it is just a collection of tools :) Right now it contains nothing for PDF generating, and I don't have immediate plans to work on it. Right now I'm focused on content stream parsing (use case: extract text or individual glyphs, render page content, etc). Then I want to start working on incremental updates. When done, it'll allow to edit PDF files: add/delete/edit annotations, add/remove images, etc.
-XMonadComprehensions
&gt; you want to just get the numeric range into memory directly rather than creating a gigantic list and copying into the vector. Exactly. Stream fusion (vector) and foldr/build fusion (Data.List) ain't compatible. &gt; Maybe it's an issue of needing extensions the MPTC extension? Well, that wasn't proposal, just illustration. Where is more sensible alternative: class Range c where rangeFromTo :: Enum a =&gt; a -&gt; a -&gt; c a -- etc. instance Range [] where rangeFromTo = Prelude.enumFromTo
That's what dynamic typing is. Every value in the language is of the same type, and that type is a sum type that encompasses all the data representable in the language.
Don't forget `-XOverloadedCase` and `-XOverloadedWhitespace`!
Oh hey, good point--didn't think of that. Perhaps that could even be part of the `IsList` class?
I don't quite get the advantage of the overloaded notation vs slapping a `pack` or `fromList ` or something in front of it.
I don't see why. If you have imported the Prelude unqualified then `:` is a constructor, but you could easily not import the entire Prelude unqualified! BTW, `:` and `[]` are data constructors not type constructors. 
I was recently given data for a project in the form of a 32000 page PDF. It appears the only feasible way for our supplier to export data to us was to run a report which generates a PD; we had to extract raw data by PDF parsing. I, at least, appreciate an API which can parse PDFs without reading the whole thing into memory.
I still love the idea so much :) Perhaps an extension of Z would be interesting, where any `(` would trigger an S-expression parser, (and reverting upon the matching `)`), and a `$` (S-stroke, not-S, Z) would trigger a Z-expression parser (and reverting whenever you fall below the indentation threshold, or an enclosing `)` is found). foo bar mu zot bob foo bar (mu zot) bob foo (bar $ mu zot) bob I'm not sure where `$` will be of use, but at some places, parentheses definitely help.
You don't get it or don't think it is worth the trade offs in terms of safety and type inference?
Wow, 32000 pages, that is impressive! How did you handle the next situation: you are parsing content stream and find that you need some other resource, e.g. font? It seems almost impossible to resume parsing from the same point after seeking to different location to load the resource (content stream can be encoded by a number of filters with own states). Preload resources? Load the entire stream into RAM before processing? Thanks.
`OverloadedRecords`?
I don't think this is what you want. If `:` is a regular consym, as a data constructor you can only redefine it for one type. I think what you really want is additional desugaring to `cons :: IsList l =&gt; Item l -&gt; l -&gt; l` or `(uncons -&gt; (x,xs))`.
Yeah, overloading some other constructors/functions might make more sense for rebinding the list syntax. I just meant being able to overload `(:)` would be a logically consistent property of the existing extension.
wow. excellent. I am so lucky to have this because I implemented embedding pdf file in hoodle data format when annotating pdf just a few days ago. Great coincidence! Right now, I have used pdftk as an external tool for making a new embedded pdf file (converted to base64 after collected) for doing this, but now I can use native haskell library for this. I will try to incorporate this into hoodle and tell how the result will be. 
Stop right there! Use a proper streaming library! `pipes` can do this, but even if you don't like `pipes` I would rather you use `conduit` than lazy `IO`. This is 2013! We don't need lazy `IO` any longer.
you can always abuse OverloadedStrings
Does this mean it might now be possible to define a non-empty list with standard list notation? For instance, if I do: data NonEmptyList a = a :| NonEmptyList a | NELast a deriving (Eq, Show) infixr 5 :| can I now define: list :: NonEmptyList Int list = [ 1, 2, 3 ] and have badlist :: NonEmptyList Int badlist = [ ] be a compile time error? 
The same could be said of numeric literals. *shrug*
It should be an error because there is no way to convert [] to a NonEmptyList. 
But how could the compiler *know* that? Keep in mind that you also have to define the `fromList` function for it, according to the proposal. Its type is predetermined for you. You can't encode this invariant.
Yeah, I understand the difference now. If you assume the base monad must be polymorphic then you can only do a trivial embedding, but if you allow a specific base monad then there might be a novel embedding.
Nope. (As far as I understand) according to the specs, `[]` gets turned into `fromListN 0 []`, which would become a runtime error, not a compile time error.
Enums are absolutely sum types in the algebraic sense. -- 2 = 1 + 1 data Bool = True | False There are plenty of sums which are not enums, but every enum is a sum. Even enums in C are sum types.
Pity!
Unions in C are more like, er, union types. That is, sum types are disjoint unions whereas union types are (non-disjoint) unions. Of course, this is problematic in general, not just in C. We can make things a bit better (in non-C languages) by being able to recognize when a given value of a union can only belong to one of the base types (i.e., does not lie in the intersection where things are necessarily confused). But that's impossible in C since everything is just bits, so there's no non-overlapping part of the union.
&gt; If I was I would have titled the post "Objects are comonads". I noticed that. But I also noticed that you don't stick to this direction in your post: &gt; Object-oriented programming is the long-lost comonadic programming that Haskell programmers have been searching for. Comonads are "object-oriented programming done right". In any case, even if you want to show that comonads are objects, you should show how each comonad gives rise to something like `exists u . (u, u -&gt; f u)`, because that's what objects are.
I wonder if I could use it to create HPDF documents. HPDF goal is not modifying already existing document but perhaps some edit may be possible (like adding pages, adding annotations ...)
Ah, but lazy IO and lazy evaluation have features that streaming combinators cannot offer. Even in 2014. Here a [small demonstration](https://gist.github.com/HeinrichApfelmus/4968229) of parsing a binary format (from RAM) into a pure lazy data structure. Only the parts of the data structure that are demanded later on will actually be parsed.
Woah. How big was the PDF? 1GB? Could it fit into RAM?
So, in a nutshell, 1. you can't embed every object into your comonadic framework 2. you can't represent every comonad as an object (as is generally defined by OOP theorists) Instead you show *some* comonads and call them objects *by definition* (since you don't provide any other definition of an object). Is this correct?
Not quite. The need for numeric literals (and type classes) arises quite naturally once you try to write `5 * 2` and `5 * 0.2` and mean integers in the first and floating point values in the second case. ML has different operators for both and it sucks big time.
Can't see a problem with it. As long as length invariants are enforced by the compiler it should be OK.
You need incremental updates, right? It is not implemented yet (but can be done based on the existent tools). Please email me if you need help (see email in github profile).
Not quite, because (:) allows you to *deconstruct* lists as well as constructing them. So if Set overloaded (:) to mean "add an element to the Map", then to be consistent it would somehow have to offer away for someone to pattern match on `x : xs` to extract an element `x` from the set.
Loading a PDF file with your tools and convert it into the data structures used by HPDF. But not sure it is possible ... 
if HPDF will expose more internals (like PDF monad), then I think it would be possible. Nice idea btw.
\#1 is correct, but \#2 is not. I believe every comonad has a meaningful object oriented interpretation, specifically in the context of fluent programming. If you want me to define what I consider an object, it is an intermediate that may have more structure than the desired final result. This is the same use as the fluent programming approach to objects. I think the comparison to objects was apt and did more to build an intuition for comonads than any other treatment on comonads that I have ever read, so I don't feel that I overstepped in that regard.
I think it would take another syntax for lists, not overloading the existing list syntax.
Yeah, I gotta admit I was one of the early downvoters, but in hindsight, I gotta agree that a little edit job to actually fill in the (suspected) question would have been a lot more constructive of me.
You're right that stream processors won't solve irregular types, but I am not convinced that lazy IO is the answer. For example, even today you could probably use lenses to elegantly handle lazy loading from complex structures, although that is just a vague intuition of mine. Anyway, the comment I made was specifically in the context of loading a web-optimized (I.e. linear) PDF document, in which case a stream processor works just fine
If you use `-XTypeOperators`, operators beginning with `:` are infix constructors, and can be pattern matched on normally, so that isn't a problem. (EDIT: I just checked, and apparently you don't even need the extension...) Although Set is a bad example, since there's no sensible way to deconstruct it, so the internal structure wouldn't normally be exposed anyway.
It's correct if you interpret it as "there exists a system that is ARM on which ghci is working" rather than "for all systems which are ARM, ghci is working". The latter sounds improbable anyway.
why should list syntax imply list laws? Integer syntax does not imply integer laws. 
As an experienced Haskell programmer, I've almost never done GUI programming in Haskell because I haven't yet found a reasonable, Haskelly, GUI library to develop with. At one time I used GuiTV which was great for the simple GUIs that I wanted to make. But GuiTV cannot build anymore and my attempts to resurrect it have failed.
What I don't understand is why Cabal hasn't solved this. Okay, I know you can't have two libraries with the same name in a filepath, but you can encode the list of dependencies in a hash, name the path to the package version plus that hash, and then maintain a database of what the library was built with for future dependency resolution. This is a tractable problem, the solution isn't the prettiest thing, but that's not an excuse for not solving it.
&gt; why other package systems don't appear to have as serious a problem as Cabal They do have the problem, but most other large ecosystems don't do compile-time type checking (e.g. Python, Ruby, Perl...) so you can get further before problems arise. The package version system is also a poor proxy for type checking, which doesn't help. See also * Go [package dependency hell](http://areyoufuckingcoding.me/2012/04/16/dpendency-hell/) * Scala [dependency hell](https://groups.google.com/forum/?fromgroups=#!topic/akka-user/f22UKZUaZ3g) * Clojure [dependency hell](https://twitter.com/justinabrahms/status/208020941937184768) etc etc. --- Back in 2008 when I was first experimenting with the Haskell Platform, we used to use cabal --dry-run to construct maximally consistent install sets, as a way to verify the solver. Typically we could get around 85% of Hackage to install together, but I used to have to patch .cabal files for other people to do this. (The ability for Hackage "curators" to do this would be good...) New solutions to improve the dep resolution and installation process should be easy to judge: what % of Hackage packages can you now install on a system, all at the same time? Continuous integration would also help -- warn (or block) package uploads that break things. Again , this would require some root-level curator team, just like we do in the Linux distros. Tools that took version specification out of the developers hands -- by automating the versioning -- would help as well, and could use types to get things right.
Oh, that work has been underway but I'm not keen on it. I've gotten yesod, snap, and happstack all building on windows at various points in the past. The main thing is just not to trust what they say about their deps, and be ready to roll up your sleeves and experiment with them a bit yourself. I know this is never a newb-friendly approach, unfortunately. This article may be helpful: http://www.vex.net/~trebla/haskell/sicp.xhtml 
Last time this came up the answer was something along the that it required some tweaking of ghc to work not just tweaking cabal. The answer was not perfectly clear to me but if I remember correctly dealt with how cabal interfaces with how ghc handles packages with ghc-pkg. Please correct me if I am wrong or fill in the missing details. Making sandboxing a native feature of cabal is the solution I have been reading about recently. It seems to be the solution that is being tried first. It is currently in the head of cabal-install repo last I heard. I do not know what the eta is on getting it into the stable version however. I know of few packaging systems that have solved this in whole. Nixpkgs and at least one experimental system based on git comes to mind do you know of any others? I try to maintain a list of systems that are comparable to nixpkgs. Read more about the solution being working on at the following ticket: https://github.com/haskell/cabal/issues/1101. A nixpkgs like solution requires changes to ghc, ghc-pkg, ghci, and cabal.
Purely functional package management (encoding dependencies in a hash) doesn't solve the problem either. Consider this scenario: you're working on package foo which depends on package bar. You originally installed bar from hackage, then you change and need to install your own local modifications to bar. The reverse case is also valid. You were using your own local modifications of bar, then the package maintainer merges your changes and releases the new version to hackage. But in his release he also includes some other features that you want to use. So now you want to switch from your local copy to the one from hackage. This is not a contrived situation...it has actually happened to me. And since I'm working closely with other developers on large systems with several projects being modified in concert it's not exactly a rare occurrence. The question here is not whether we can disambiguate different package versions fully based on all their inputs. The question is whether we can disambiguate with the information we use to specify dependencies. I'm not going to put bar-3148b443ea1c2257925adaee2ae7dfcc as my dependency in foo.cabal. I'm going to use version bounds on bar. Encoding dependencies in the hash makes it very easy for there to be several different versions of bar in the repository that all satisfy foo's version bounds, and now cabal won't know which one to choose. Some of them may not even build successfully because they were intermediate versions that no longer have the appropriate API. But you won't know that ahead of time, so you will still have to expend some effort to disambiguate.
That explains something else I've never really understood, which is why you can use ":" and "[]" in a pattern match, but not regular functions.
&gt; Purely functional package management (encoding dependencies in a hash) doesn't solve the problem either. Consider this scenario: Can you go in to some detail of the problem with your scenario? I regularly encounter this situation in Nix (not with Haskell packages though) and I have no serious problem. I just clone nixpkgs, make my modification, and build.
GUI + Haskell + Windows = forget it.
I'm posting separate links to each of these 3 case studies in case they lead to quite different discussion threads.
Question caught me by surprise. Is this because of any problems reading the live format, or just to avoid having to log in, or ...?
In a nutshell, Silk emphasized the benefits they got from Haskell: time savings, reduced errors, and scalability.
&gt; Far too frequently I encountered packages that, when trying to install, would say installing this package will break a dozen others. When cabal gives you this message, the solution is to mention all the names it says will "break" on the command line along with the new one you want to install. Cabal will then figure out what to rebuild. I'm not sure why there isn't a switch to do this automatically.
Interestingly in this case, Haskell was used as a replacement for Ruby.
This was a case of switching from Python to Haskell. My favorite quote: "The Haskell compiler is very reliable and refactoring in the language is, well, glorious."
One of the problems is that most GUI toolkits are massive frameworks of doom. Every now and then I think about writing one, but I'm afraid it would end up being just as bad and we'd just have Yet Another Doom. This is why I sort of skirted the issue for haskad.es and said "use QML!"
We posted the first three case studies on the same day, so it is interesting to see the common themes as well as the differences. One big surprise: Haskell being used as an alternative to Ruby and to Python. One big non-surprise: the companies pretty consistently rave about the productivity, quality, and scalability.
Yes, its s that I don't have an account to log in. Not that I want one right now, I just think it would be nice if people could read at least a preview of things without an account if you go though the trouble of posting the guide on a link aggregation website.
It appears that module is part of the [unordered-containers](http://hackage.haskell.org/package/unordered-containers-0.1.3.0) library on hackage. Type in `cabal install unordered-containers` and if you made sure to sacrifice a goat first it'll install it for you. (I say that about cabal because most everyone hates it as a package management system.)
In type Sum a = Add (DF a) (DG a) do you mean type Sum a = Add (F a) (G a) ? 
The instructions are rather involved, but the Nikki and the Robots guys seem to have had success w/ Qt bindings: http://www.patch-tag.com/r/shahn/nikki-wiki/wiki/Compiling%20on%20Windows
Great! Yes, ok it did take a goat or two, but that did the trick. Thanks!
Hint: If your "case study" says absurd things like this: &gt; Unlike other languages Haskell ensures that companies can enable their development and production teams to maximize their productivity, performance, quality, scalability and agility in ways that no other programming language can. then you immediately lose all credibility. This isn't a case study, it's just marketing, and the piece as a whole has successfully turned my assumptions about FP Complete from "interesting to see and potentially useful for the programming community" to "PR mouthpiece spammers". Please send this junk to somewhere other than useful subreddits.
&gt; One big surprise: Haskell being used as an alternative to Ruby and to Python. Btw, why exactly should this come as a *big* surprise to us? 
It looks like that would be tricky, since the class also includes `toList`. So you would have: instance IsList Html where type Item Html = Html fromList = mconcat toList = error "Impossible to implement?" Looking at the source, it might be possible to implement by deconstructing the `Append` constructor, although the existential in there makes me worry. It probably would have been better to split the `IsList` class into two classes (`FromList` and `ToList`), so that one gives you literals, and the other pattern matching. Then blaze-html could implement only the former.
From a technical standpoint, these feel very surfacey. But if I was in a position to be pitching Haskell to management, case studies like these feel tremendously useful. It would be good to collect them along with the various experience reports that have been published in journals or conference proceedings to make a more comprehensive "selling Haskell" resource.
we only hate cabal because we have absurdly high standards. Goat sacrifice is entirely unnecessary in cases like this (if you installed the platform). 
Thanks for the suggestions. We'll work on it. You are right, these are especially meant to help people who need to explain to management "why Haskell." That's why we made PDFs! 
I guess the main point is that in the course of active development, you'll end up having multiple instances of the same package version installed. You don't want to have to rebuild things every time, because that slows down development. So you end up having to disambiguate yourself by uninstalling things. That isn't actually a problem, except that our current tools don't support it well. 
I don't know if this series has any use whatsoever, but please keep it coming -- it's fascinating stuff!
We don't really need a separate post for each bit of collateral FPComplete releases. As much as I support this kind of propaganda, keep it to one post.
Note that other pairs of conversion functions *are* in separate classes: in particular, the conversions to/from `Integer` are in `Integral` and `Num` respectively, and conversions to/from `Rational` are `Real` and `Fractional`. (Have I remarked recently on how silly it sounds to have `toRational` provided by a type class called `Real`?) Anyway, we already *have* a `ToList` class, except that it's called `Foldable` instead.
I disagree. I think we have quite reasonably high standards. Even if every other language does something wrong, and how to do that thing right is an open research problem, I feel it's entirely reasonable to *expect* things to be done right. What's not reasonable is expecting everything to meet those high standards immediately. :]
Yup, "extraordinary claims require extraordinary evidence". And this claim has no evidence. 
Ninja has a similar feature: http://martine.github.com/ninja/manual.html#_pools (Not saying it's a bad idea, or that Ninja does all the things Shake does, but just responding to the "I know of two approaches used by other systems"... bit.) Chrome's build used file-based locks to prevent concurrent links. Another reason it's nice to have the build system help with the problem instead is that it's hard to write flock-based systems that allow n&gt;=2 jobs at a time.
I certainly don't interpret too many questions as bad, I just think that the number of answers will be a function of many things such as: the quality of the other instructional material a language has; other language specific forums available for a language; the distribution of skill levels within the languages user base; the length of time the language has existed/been popular; plus the the number of current users of the language. One can clearly derive some information about the relative popularity of Java vs Haskell by comparing those numbers, but be careful of using it as too strong a proxy.
I would be interested in a minimal browser-based GUI framework. Not really a large framework, but just something were you can get a couple of buttons for cheap. Something along the lines of * https://github.com/chrisdone/ji * https://github.com/bertm/Modular-Haskell-GUI I've used `ji`, it works nicely. If someone were to take on a little maintainership, I would hop the boat instantly.
I can hardly imagine how to write rendering engine atop Repa or Yarr. Maybe you used [Gloss](http://hackage.haskell.org/package/gloss)? Yes. Unfortunately GHC isn't currently allowing to be 100% confident about proper optimization. In the library I have invented [something](http://hackage.haskell.org/packages/archive/yarr/0.9.1/doc/html/Data-Yarr-Base.html#v:force) in this direction. As you can see in version number, the library is designed in the 9th majorly different approach. First 3-4 of attempts had Repa in dependencies...
I had a talk with Ben Lippmeier, probably he will incorporate parameterized loop unrolling, but not the functionality which i call "fixed vector flow".
I can be wrong about bang patterns in the docs.
It is not worth it. In terms of safety, in terms of type inference, and most importantly, in terms of clarity of code. To me, the biggest cost of polymorphism is when readers of the code must apply Hindley-Milner or worse in their heads across multiple modules in order to understand the type of each line of code. And I have seen this happen on a large scale. To me, gratuitous polymorphism is one of the worst kinds of premature optimization, especially when it involves a gratuitous type class.
Yeah, those are popular languages. Haskell is more likely to be used as a replacement for a popular language than a less-popular one, simply because there are more people that might switch. Simple statistics.
No, I used Repa. Don't misunderstand, this wasn't anything too fancy. Mostly blitting (I had to get pretty low level to do this with Repa) and functionality equivalent to fragment shaders.
&gt; For example, even today you could probably use lenses to elegantly handle lazy loading from complex structures, although that is just a vague intuition of mine. That sounds interesting, but I'm not entirely sure. The thing is that lazy evaluation gives you a side effect in pure code, namely a write-once that replaces an unevaluated expression by its evaluated version. No matter how you use lenses, if you don't want to use lazy evaluation, then you have to make this side effect explicitly and suffer the monadic API. &gt; Anyway, the comment I made was specifically in the context of loading a web-optimized (I.e. linear) PDF document, in which case a stream processor works just fine So does lazy IO. :-) If you don't care about making the write-once side effect explicit in the API, that is.
Well, here's the thing, I'm not thinking of just taking the version numbers blindly and putting them in a hash. What I'm thinking is that there should be a database file that lets you look up what bar-3148b443... had as constraints when it was built. And it'll have the full list of constraints and the actual version used. So it'll say: bar-1.0.0.0.1-3148b443... foo in:[1.2.1.*-1.*,1.1.8] out:[1.2.3.*] with:1.2.4.3.8172 baz exact:2.8.7.3.5-mightybyte with:2.8.7.3.5-mightybyte That way if you want to build quux with a constraint on baz-2.8.7.3.5-mightybyte it knows that bar-1.0.0.0.1-3148b443 is fine to use, but maybe another bar-1.0.0.0.1 that didn't build against your version of baz isn't fine. The point isn't to use the hash as the total sum of how the dependency is resolved, it's to use it as a lookup to determine if it's an acceptable version to work with. I dunno, maybe what I'm describing is completely crazy, but I'm pretty sure that this would make it easy to see what versions you have and how uninstalling something would affect other libraries or programs you're building, for example.
Hi Don. I think that automated versioning would be a really powerful approach. I've done a lot of work on this in the Java world with OSGi; in Bndtools (an IDE that I work on) we can both automatically generate the import range for the consumer of an API, and also tell the developer what kind of version bump they need to make when releasing a package, by comparing it against the previous release. Of course this is limited to changes made in pure interfaces, we don't try to statically analyse behavioural changes in imperative code! If this kind of thing can be done for Java interfaces, it doesn't seem too much of a stretch to do it for Haskell.
While they're very surfacey, I don't think that's a bad thing. A lot of the experience reports out there are more in-depth, but that means you have to dedicate some time to reading them; whereas these are short marketing blurbs to pique interest. Both have their place. Keep up the good work
Thanks, I didn't know of that, but it does seem like Ninja gets that right - I'll update to include it. I am consistently impressed with Ninja, it seems to have many desirable build system features.
Thanks! My first experiments that eventually led to Ninja were written in Haskell, so it's cool to see how Shake shapes up. (Shake is much more clever than what I had envisioned.)
&gt; One big surprise: Haskell being used as an alternative to Ruby and to Python. To me, this is not a big surprise. Haskell is a very high-level, productive language so it really fills the same role as Python or Ruby. It just also happens to have relatively good performance and produce very maintainable code. To me, at least, Python et al have always been the main competition to Haskell. If you're using C or C++, either you're a crazy masochist (I kid, I kid, you're probably not *crazy*) or you have some very specific needs in terms of low-level system access. If you're using Java, you're either tied the JVM or you have a gigantic pool of cheap but plentiful enterprise Java developers to use. If you're using Python or Ruby, chances are you really care about productivity and Haskell is a very good substitute with additional benefits. Of course, maybe my perspective is skewed because I tend to hang around startups which disproportionately use Python/Ruby and are also less afraid of change and trying new things than enterprise software companies. 
Wouldn't each of these additional installs carry the possibility of breaking other things? Seems like we would be trading 1 install that would break stuff for many that might. Any issue there?
I am a complete novice when it comes to haskell. Nonetheless, I find this sort of reasoning nice. Can category theory be unfolded in a similar manner into an algebra for algorithms? Another point I don't quite grasp is what is the final goal/achievement once there is a complete algebra of algebraic data types. What does this new tool allow? Thanks for the articles I like reading them. 
In practice it can often find a solution for all of the desired packages, but not always.
Not the list laws, but the monoid laws. I treat overloaded syntax the same way I treat type classes: it is better if it has laws, otherwise you can't reason about it.
Alright. I will accept that lazy IO is okay if you are fully aware of all the risks of doing IO in pure code.
What would it take to make Haskell easier to install on Windows. I don't understand why the Haskell Windows install process is more difficult than other languages. Is it by necessity or just a lack of resources dedicated to improving it?
From memory slightly under a GB. Certainly on a good machine it would fit into RAM, but I'd still prefer to be able to process large files more parsimoniously. With other formats such as CSV that would be streaming the data line by line, with PDF it means random access.
We were doing very simple processing. The format was very regular and all we needed was to extract text to get the fields we needed. Fortunately, the kind of issue you raise didn't arise. Really, the fact we were getting large datasets delivered as PDF rather than CSV, or perhaps XML, is a testament to the results of IT outsourcing. Our contact wasn't IT literate enough to export the data in another format themselves and either didn't have access to IT support who could help with a non-routine task, or didn't think they could afford what it would cost. In the end the path of least resistance was to just do text matching on lines in the PDF and recreate a table from the data ourselves.
Will it end up supporting the string syntax for [Char]? In other words: &gt;&gt;&gt; "abca" :: Set Char fromList "abc" Which does not seem to work in todays build 7.7.20130217.
Never mind you jus need the right instance of IsString.
I've found Haskell itself to be fairly easy to work with on Windows so far (granted, I am still working on programming anything substantial). However, installing some (very useful) packages has me running into a wall. I'm not sure why the state of things is this way. There are a lot of professional development platforms on Windows, and maybe it just costs too much to compete with those. A lot of developers are more comfortable on Nixes, so it makes sense that's where the focus is. Then you have some that think Windows is a toy, and treat it as such. One of the comments on my blog post makes that opinion quite clear.
I think that comment is rather extreme for the Haskell community as a whole and most people would not describe windows with the language found in the comment. That said most do use *nix and find it easier to work on then windows and that is reflected in where developers apply their attention and perhaps more importantly where most of the bug reports come from.
&lt;3.
&gt; Although Set is a bad example, since there's no sensible way to deconstruct it, so the internal structure wouldn't normally be exposed anyway. Exactly. My point is that if we are going to allow overloading of [] and of (:), then it would probably be a bad idea for us to not insist that they be used consistently so that any non-empty value constructed using [] can be deconstructed using (:). We could do this by, for example, having there be a typeclass one has to use to overload [] that forces coming up with a sensible definition for deconstruction using (:), such as minView in the case of Set.
Sure. Worst case scenario is that you reinstall everything. That has never happened to me in practice. Though apparently I'm odd in that I only install from cabal if something is not available from my distribution, and I am willing to use older versions if they are more compatible with whatever else I have installed and also have the features that I need.
&gt; minimal browser-based GUI framework Nothing browser-based should really be able to classify as "minimal" ;)
she's a [keeper!](http://graphics8.nytimes.com/images/2008/05/25/sports/25soccer.1.600.jpg)
Hey look, everybody! This guy has a girlfriend who seems pretty awesome!
That's so awesome!
It compiles for me as long as I put a "module Foo where" at the top (otherwise the compiler assumes it's the Main module but cannot find the *main* function). A better way of asking questions like this is 1) posting the whole code, 2) reporting your compiler version, 3) pasting the exact error string you got.
The code you have posted does appear to compile, maybe the problem you're having isn't what you think it is?
really awesome that she took the time to figure out how to write that! :) 
She asked us about it too :) Good thing you don't sub /r/learnprogramming or it would've ruined the surprise. [link1](http://www.reddit.com/r/learnprogramming/comments/181w5r/girlfriend_of_programmer_seeking_valentines_day/) [link2](http://www.reddit.com/r/learnprogramming/comments/186950/girlfriend_of_programmer_seeking_valentines_day/)
You're working with TemplateHaskell here, which causes the problem, because it is not your standard Haskell. Try moving $(makeLenses ''GameObject) to the bottom of your code. The problem is, that GameWorld is declared after makeLenses and thus not known to the compiler. The order of declarations does matter in the presence of TemplateHaskell unfortunately.
According to Ahman, Chapman, Uustalu. Comonads are Directed Containers. [a different subreddit](http://www.reddit.com/r/dependent_types/comments/xc6l5/when_is_a_container_a_comonad_pdf_slides_and_code/) 
I'm going to make `fix ("a Lisp written in a Haskell written in "++)`. Or maybe `(fix (lambda (x) (append "a Haskell written in a Lisp written in " x)))`. Or something.
The difficulty in making guis is something I keep hearing about with Haskell. Is there some fundamental piece of the language that makes this difficult, or is there just a lack of a decent gui library? If it's the latter case, why isn't there a good library?
Dude ?! You must be a blast at the parties....
Well, in this post you see one use: defining zippers.