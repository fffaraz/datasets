Should we have received a confirmation email? I believe I signed up over a month ago but never received anything.
After following the types through the composition of `boss.health`, I was expecting to see a follow-through with `-=` to reach the final state type. It's obvious what it does, but I was curious about how it does it.
I prefer the Haskell approach to scala's, but this has to do with the Scala's handling of implicits (and types for that matter) as arguments rather than through a subsumption relation. Both Coq and Idris have the same problem to varying degrees. But, this is fundemental to the language, and shows up places other than typeclasses. I know of no reason why proper scoping is inconistent with typeclasses. In fact, the implicit parameter implementation in Haskell is a pretty good example of how it could be done right--except for the limitation that you only have one implicit parameter of each type. My observation is that in those languages you don't get the `Set` map operation the way you do in Haskell which is essentially the only argument against having scoped (and with them multiple) instances I have heard in Haskell. I proposed a trivial solution that resolves that problem by giving each instance a "name" that is reflected at the type level (it could its own kind) and thus makes it easy to ensure safety of those data structures that depend on getting the same dictionary always. Perhaps not all instances could get "names"--ones constructed at runtime would presumably have existentially scoped names not available because of the lack of true dependent types. Regardless, such a system could be identical to haskell except for those few cases of data structures like `Set` and `Map`. I am convinced this is a place where you can have your cake and eat it too. Unfortunately, I don't know how to add scoped instances to Haskell in a way that does not break existing code since you need slightly more meaningfull types for a few data structures.
Good suggestion. I will add that tomorrow morning. Edit: Actually, I can't figure out how to work it in and preserve the flow. I hope you don't mind if I save this topic for my next lens post, where I was planning on showing how getters and setters work.
As for sharing keys, you can use alternative record systems like Vinyl to get around this issue. Vinyl also supports lens operators.
No no, we're (well, me really) hardly that formal! I'll send out email about a week out with logistics.
&gt; Can lenses deal with records that share key names or do they suffer the same limitations as regular Haskell? I believe you can use `makeClassy` when data types share the same field names. It type-classes the lenses so they work on multiple data types. However, I haven't tested what it does if the fields have different types. &gt; Does the state object get updated in place or is it a regular immutable value that needs to get partially copied during updates? If you pay careful attention to core you can get the state modifications to compile to the optimal primop loop. I did some [cursory performance studies](http://www.reddit.com/r/programming/comments/1880d1/introduction_to_haskell_io/c8ehz3u) some time ago in a discussion thread that showed a toy example of this. \[skipping 3 because I don't know Javascript that well\] &gt; What are the options if you want to keep track of more than a single kind of state? (Or is bundling all state in a master state like your Game example allways the "right way to do it"?) Well, the purpose behind `zoom` is that you can limit sub-computations to only the state they actually need, and then you have a top-level context that zooms to sub-states as necessary to execute these sandboxed computations. However, you still do need that top-level global context if you want to link those diverse sub-computations together.
OK, makes sense.
"Haskell gets a lot of flack because it has no built-in support for state and mutation." -- er, come again? Last time I checked, making no concessions about purity was maybe *the* defining feature that made Haskell worth trying. Excellent reading though.
Care to explain? I don't see what's wrong with it.
&gt; "One consequence of this requirement is that a Traversal needs to leave the same number of elements as a candidate for subsequent Traversal that it started with." from the documentation for Control.Lens.Traversal &gt; [True, False] ^.. traverse.filtered id [True] &gt; [True, False] &amp; traverse.filtered id %~ not [False, False] &gt; [False, False] ^.. traverse.filtered id [] It only obeys this law when you do not modify whether the traversed elements succeed the predicate.
Soo... is it a legal something-else?
i might be wrong (not unlikely), but it is a `Fold` (Control.Lens.Fold) which (if i understand the lens doc correctly), should only allow getters, not setters. i don't know how one could implement something like what tekmo is doing with filtered without looping oneself. see http://hackage.haskell.org/packages/archive/lens/3.9.0.2/doc/html/Control-Lens-Fold.html#v:filtered 
isn't that a little early?
I'm sorry, I don't see which part of this doesn't make sense.
Wouldn't it make sense to add a dynamic check (a contract, morally, for a property we cannot check statically), then?
I can't see how this can be built into the code efficiently. The obvious way to do it would be to count the number of elements in a traversal before and after traversing through it, and that would be O(n) in the best case, and I do not want to be the one adding that to something which is perfectly valid when used correctly (and it is very easy to use it correctly, just don't modify what you check), and is a perfectly valid Fold no matter what!
Compare: [True, False] &amp; traverse.filtered id %~ not . not [True, False] &amp; traverse.filtered id %~ not &amp; traverse.filtered id %~ not These are completely different when you'd expect them to be the same. On the other hand, filtered is VERY useful a lot of the time. For a start, you can't make invalid folds with it. Second, if you know that you aren't affecting whether the predicate will succeed when you traverse over it, as is the case in the tutorial, filtered is absolutely fine.
Hear, hear!
I'm not familiar with these issues (or the advanced uses of Lenses in any general case), so pardon me if this is dumb, but: - you could maybe have a check only on `filtered` rather than all basic combinators (which makes it less costly) - you could provide a `filteredUnsafe` method for people that think they know what they're doing; but my very own guess would be that the performance difference wouldn't be that big in the first place - of course you could expose different functions to return either a Fold or a Traversal, and have the dynamic check on only the Traversal one
Aside: you could either count the number of elements, or remember the selection function that was used, and add a hook on modification to check that this selection function still holds. That may be slower for complex selection function, but potentially better behaved with respect to laziness, etc.
I am having difficulty seeing that `(Key f,)` is correct. I think it might have extra terms? Let's say I have the type data RFoo a = R a a a and I implement its left adjoint as data LFoo a = LOne a | LTwo a | LThree a Now, RFoo is representable by `LFoo Void`, so `Key RFoo` is isomorphic to `LFoo Void`. On the other hand, take a term like `(undefined, bar) :: (LFoo Void, Bar)`. This term doesn't correspond to an inhabitant of `LFoo Bar`. 
Yeah, a strict language would never need or use the [`Maybe`](http://www.scala-lang.org/api/current/index.html#scala.Option), [`List`](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.List), [`Cont`](http://www.scala-lang.org/node/2096), [`Parser`](https://github.com/djspiewak/gll-combinators/blob/master/src/main/scala/com/codecommit/gll/Parsers.scala#L225), or even [`IO`](https://github.com/scalaz/scalaz/blob/scalaz-seven/effect/src/main/scala/scalaz/effect/IO.scala) monads, because Monads are just a big hack to get IO working in Haskell, right? :)
I know they seem really useful, if we stop pretending there're lens can we give them another home, maybe with some associated laws, so we can continue using them?
The thing is that since strict languages are kind of "monadic by default", you don't necessarily need to create separate monad interfaces for those things you mentioned. For example, in strict languages you usually use continuations or generators instead of Cont. But yes, the Maybe monad is something that I tend to miss a lot :)
If you export a traversal that uses "filtered" without warning people, it could very, very easily blow up in your library's user's faces. If you're just using it yourself, and you know what you're doing, everything will be perfectly fine.
No, strict languages tend to be (but aren't necessarily, if you look at Idris, for example) IO-like by default. I dare you to get the List-like nondeterministic behavior "natively" in a strict language any more easily than you would in a lazy one. Or parsers, or some of the weirder ones like the K search monad. Looking back, my earlier response is a bit snarkier than I intended it to be, but I see way too many posts that either implicitly or explicitly suggest that Monads are "about" IO, and you seemed to be doing that too. I use monads all the time that have nothing to do with IO, and most experienced Haskellers do the same. This talk about about effect typing/tracking is actually not as satisfying as Monads, although I do hope for more research into it. First of all, very few people actually go into what they mean by "effects" when talking about tracking them (is nondeterminism in a List an effect?). Second, do we have any evidence of arbitrary effects being possible to infer? Type systems are notoriously hard. What if I took some Scala code with a hypothetical effect tracker and threw in some nondeterministic List-like behavior, or some Parser-like behavior, or even some weird non-local behavior like Cont? How would the effects get interleaved? And although linear/substructural types are cool, they pretty much only "solve" the IO (or IO-like) portion of Monads, which is exactly what I'm saying is not what Monads are about. Finally, how would you embed something like the awesome STM monad "natively" even in a strict language that has built-in IO? The ability to actively restrict what is possible is specifically what makes STM feasible in Haskell where the giants like C# have concluded that it's not a good technique, because they need to expend way more effort book-keeping around arbitrarily complex mutable variables.
I'll add a note explaining that! Thanks for point that out.
Furthermore, Dan Doel also often brings up a counterpoint to the point you're making about reusing pure and monadic functions. Take `map` vs. `mapM`. The former is just the latter instantiated to the identity monad, right? Yes, functionally, perhaps. But by being more generic, `mapM` can make fewer assumptions about its inputs, too. `mapM` must process its arguments in order, lest the effects happen out of order. `map` can easily operate in parallel (hence the map half of map-reduce). We don't currently have a good principled way to talk about "if the type parameter to this function satisfies these extra requirements [commutativity here], we can use a more efficient implementation" so a `map`-as-`mapM` implementation would be a lot harder to optimize.
Oh, I completely agree that Haskell's unapologetic purity is its best feature. I was just building an imperative straw man to motivate the post. :)
That's right. In reality Haskell interleaves both phases of computation. I only use it as a mental model for reasoning about their interplay.
&gt; That zoom feature makes me think of Javascript's much maligned with statement. Am I correct in saying that the only reason this is not completely evil is because key names are global functions and not things that are dynamically scoped depending on your local state object? Pretty much. `zoom` doesn't bring any new names into the local scope, it only changes which state monad they access, so there isn't the same kind of confusion as with `with`. A given `do` block always lenses into the same object, so even if you `zoom` into an object of the same type as the parent (say you're lensing into a tree), there is little risk of confusion.
Yeah, `ghci` behaves like it operates within an `IO` monad. That's why it requires `let` when you define pure computations.
It also supports let binding for bringing pure variables into scope too.
I remember it took me amazingly long to realize this, even though it followed from what I knew. It should probably be included in standard tutorials as it makes a sort of shell-like use of ghci much simpler.
In the documentation, `filtered` does not claim to be a `Traversal`. It merely claims to be a `Fold`. =) I merely loaded the gun and pointed at his foot. He chose to pull the trigger. It works perfectly fine as an improper `Traversal` or even, *gasp*, an improper `Prism`, if you know where it is appropriate. ;)
I'm perfectly happy to continue housing it in `lens`. It doesn't claim to be a valid `Traversal`, the types merely work out to let it compose with them and "do what you mean".
You don't get enough control with the types in `lens` to add such a check.
It is a legal `Fold`, and what I call an "improper" `Traversal` (of the first kind, IIRC). We can formalize what those improper traversals mean. They compose with each other and preserve the form of improperness that they satisfy. The fusion laws become unidirectional, and as long as you don't mix improper things of the first and second kind everything can still be reasoned about, just to a lesser extent.
Thanks, very nice! More motivation for me to go on with lgtk! :) There will be a new release soon. I would like to make a gallery of lgtk applications (it does not matter whether these are outdated or not). May I include your png (just the picture, not the code)? If you give a caption or link, I will attach it as pointer.
So would the correct type be `Fold [Unit] [Unit]`? I'm still a little bit unclear to how `Fold`s work.
&gt; In practice no lens police will come after you for breaking them and its occasionally quite useful to be able to do so, though. I will come after you.
Yeah, but you'd have to fly in from Canada. Thats plenty of time to set up traps.
In lens we actually do avoid exporting them using the `Traversal` type synonym and document in the signature what they mean. When you see something merely claiming to be `LensLike` with lots of caveats and weaselwords in the documentation, it is usually for this sort of reason. One main reason these improper types haven't found their way into the library is that in the end there are something like 60 types needed. =/
i see. thanks for clarifying.
 safeFiltered :: (i -&gt; Bool) -&gt; Traversal' a (i, b) -&gt; Traversal' a b safeFiltered p f r a = f (\(i,x) -&gt; (\x0 -&gt; (i,x0)) &lt;$&gt; (if p i then r else pure) x) a `safeFiltered` should be safe to use. Unfortunately, it is also quite a bit more akward to use. I don't know if edwardk provides a function like this. Edit: Sorry, the above function is insufficiently general. secondIf :: (a -&gt; Bool) -&gt; Traversal' (a,b) b secondIf p f (x,y) = (\y0 -&gt; (x,y0)) &lt;$&gt; (if p x then f else pure) y is better. Then you could define `safeFilter p t = t.(secondIf p)`, but you'd probably just use `secondIf` directly. ... Also, you'd come up with a better name than `secondIf`. I'm terrible with names.
Considering that lens has the `(&lt;&lt;%@=)` operator, I don't think it would hurt to have `safeFiltered`.
I will note that, although `around target 1.0` is not a valid traversal, `(around target 1.0).health` is a valid traversal. If I were a compromising man, which I am not, I would suggest that you add a parameter to around: around :: Point -&gt; Double -&gt; Traversal' Unit a -&gt; Traversal' Unit a around center radius field = filtered (\unit -&gt; (unit^.position.x - center^.x)^2 + (unit^.position.y - center^.y)^2 &lt; radius^2 ).field Allowing the `units.traversed.(around target 1.0 health) -= 3`. Although this doesn't prevent the users from writing `(around target 1.0 id)` to make invalid traversals, it at least will encourage users to pass a field that excludes `position` to the `around` function; especially if you include suitable documentation. Of course, if I were writing it, I'd use `safeFiltered` and all the awkwardness that it entails, leading to a messy tutorial. 
I'd prefer the `safeFiltered` solution myself. If you're going to enforce safety then you might as well go all the way.
We have the safe one too, its called `indices` and it works on the index of an indexed traversal. I advocated it as a the principled version of this solution to Tekmo when he asked on IRC.
"funfunc is of type Int to ... Int to Int" in order to emphasize binding direction. Can be arbitrarily extended with "a to ...".
I knew let binding, but have spent years using `let upio = unsafePerformIO` in order to do the `(&lt;-)` binding more conveniently...
That... makes perfect sense. I've never thought that metaphor would continue through.
the `FoldableWithIndex` instance for pairs includes the first half of the pair as the index, so we can use `indices` on the result. &gt;&gt;&gt; (4,2)^@..ifolded.indices (&gt;= 2) [(4,2)] &gt;&gt;&gt; (1,2)^@..ifolded.indices (&gt;= 2) [] You should then be able to recover something like safeFiltered p l = l.ifolded.indices p
hmm are there six independent properties? I'm curious where 60 came from.
I clarify by saying it's a function in case I could be talking about a map or some other lookup data structure.
Haskell doesn't use a state monad under the hood for IO. Ghc does, but Haskell leaves IO abstract. 
And you can't really separate them since what phase one builds depends on values from phase two. Still, I think it's a useful mental model. 
improper _ of the first and second kind, depending on which way the fusion law gets invalidated. There is a third kind where neither version is safe, but we can have indexed and non-indexed versions of most things, plus some offer index-preserving variants. so 3 * (2 or 3 depending) * several definitions ~ 60.
It seems to resemble the cofree comonad. data Cofree f a = a :&lt; f (Cofree f a) But this doesn't allow `Many` to be used by itself (unless you wrote something like `newtype Many f a = Many (f (Cofree f a))`, I suppose.
Having forgotton the original name and had to make up my own, I wouldn't be at all surprised if I'd also given a name to something that didn't have a name to start with :-)
Apparantly also called the [“f-branching stream comonad”](http://hackage.haskell.org/packages/archive/comonad-transformers/1.6.3/doc/html/Control-Comonad-Trans-Stream.html).
It doesn't continue forever, since you can run import statements and (nowadays) data declarations. But everything you can `do` you can `do` in ghci.
Thanks!
I know these as Generalized Rose Trees.
Very interresting. I played a bit with it, here are some value examples: l1 :: NonEmptyList Int l1 = Some 1 (Many (Compose Nothing)) l2 :: NonEmptyList Int l2 = Some 1 (Many (Compose (Just (Some 2 (Many (Compose Nothing)))))) l3 :: List Int l3 = Many (Compose (Just (Some 1 (Many (Compose Nothing))))) t1 :: Tree Int t1 = Some 1 (Many (Compose ([ (Some 2 (Many (Compose []))), (Some 3 (Many (Compose []))) ]))) I also tried to make a binary tree, the best I can come up with is: data Bin a = Bin (Maybe (a,a)) type BinTree a = Some Bin a tb1 :: BinTree Int tb1 = Some 1 (Many (Compose (Bin ( Just ( (Some 2 (Many (Compose (Bin Nothing)))), (Some 3 (Many (Compose (Bin Nothing)))) ))))) 
A cons cell is a list monad which can be transformed by higher order functions such as map and remove-if. It uses NIL as it's zero. Seeing how you are not "wrapping" your monad with contextual data, they seem equivalent to me, but I don't understand Haskell very well. EDIT: Also, a cons cell is just a data structure which contains two pointers.
`Some` is the `Cofree` comonad, aka the f-branching stream comonad. I have it in the `free` package as well as `comonad-transformers` and there are algorithms for using it in my `recursion-schemes` package. There are a number of related structures in my `streams` package as well. `Many` is an 'off-by-one' version of the same. My major use case for this is in modeling (in)finite state automata. When you instantiate `f = (-&gt;)` you get the relationship between (infinite) Mealy machines and (infinite) Moore machines. You can find these in my `machines` package. newtype Mealy a b = Mealy (a -&gt; (b, Mealy a b)) data Moore a b = Moore b (a -&gt; Moore a b) which can be encoded newtype Mealy a b = Mealy (a -&gt; Moore a b) data Moore a b = Moore b (a -&gt; Mealy a b) which gives the same recursion pattern as `Some` and `Many`. Lots of structures look like cofree comonads. * `Cofree Maybe` gives you a nonempty list. * `Cofree Identity` gives you an infinite non-terminating list. * `Cofree Bin` for data Bin a = Bin a a gives you an infinite binary tree * `Cofree Node` for data Node a = Bin a a | Leaf gives you non-empty binary tree * `Cofree (Const b)` is isomorphic to a pair `(a,b)` * `Cofree Proxy` is isomorphic to a `data`-based version of `Identity` ... They are also particularly useful for [annotating syntax trees](http://comonad.com/reader/2009/incremental-folds/).
I see, but why do you need a function here? It's confusing to me. I tried with Either: data Bin a = Bin (a -&gt; Either a a) type BinTree a = Some (Compose Maybe Bin) a Could we get rid of the Bin data definition (make an anonymous Type declaration right in BinTree)?
I don't think I'll ever understand Haskell. Maybe when I can dedicate time to *just* learning it, but I don't yet.
&gt; I see, but why do you need a function here? It's confusing to me. The type Pair a = Pair a a is a representable functor, which is a functor that is isomorphic to the reader monad ((-&gt;) e). In this case, it is isomorphic to ((-&gt;) Bool). Intuitively, it is a structure that contains two values that are indexed by True and False. You can imagine it as a fixed-size Data.Map.Map Bool a, if that helps. I chose to represent it this way to avoid defining a separate Bin type.
Expand new type in many to get newtype Many f a = Many (f (Some f a)) "uncurry" Some to get data Some' f a = Some' (a,(Many f a)) Subsitutute Some' into many to get newtype Many f a = Many (f (a,(Many f a))) This is similar to newtype FixF f = FixF (f (FixF f)) In fact type Many f a = FixF ((Compose f (,)) a) Now the fixed point of a functor is the essence of all recursive types. e.g FixF Maybe represents the natural numbers (ignoring bottom) As the pair is a common functor it comes up often. Similarly I think Some is type Some f a = FixF ((Compose (,) f ) a) 
nicely written.
Clever!
Do you mean when you need to differentiate between: Int -&gt; Int -&gt; Int and (Int -&gt; Int) -&gt; Int ?
No, rather when I need to differentiate between String -&gt; Int and Map String Int .
Awesome. Great news about trimming the fat too. Sounds like pipes-4.0.0 is going to be a pretty nice release. It also sounds like the highly volatile stages might be nearing and end? Does that mean it might be time to start putting together a comprehensive test suite? You mentioned in this blog post the existence of several bugs -- would be nice to flush any remaining ones out. 
Yeah, I need a test suite. A good place to start is testing as many laws as possible, since those laws define the library's contract. This would mean testing the proxy laws for all instances of Proxy and also testing the functor laws for the prelude utilities. If people want to contribute, this is an excellent way to begin, otherwise I can do it myself.
See the comments for where he explains that it is in Haskell (source sadly not yet released).
At first thought it seems like you wouldn't want the non-local state. It's interesting that that provides an invalid implementation.
[`Control.Lens.Indexed.indices`](http://hackage.haskell.org/packages/archive/lens/latest/doc/html/Control-Lens-Indexed.html#v:indices), I assume?
Yep.
The definition of a Cofree Comonad is data Cofree f a = a :&lt; (f (Cofree f a)) Where f is some Functor (i.e. a type that has a map function defined). If we expand this out for the Id Functor, we get: a :&lt; Id (a &lt;: Id (a &lt;: Id (a &lt;: ... ))) Which is equivalent to a :&lt; (a :&lt; (a &lt;: ... )) i.e., infinite streams For Maybe, we get a &lt;: Maybe (a &lt;: Maybe (a &lt;: Maybe ... )) i.e. regular lists For [], we get a :&lt; [a &lt;: [a &lt;: [a &lt;: ... ]]] i.e. multiway trees. So far, modulo laziness, these are both similar to simple cons cells. However, -&gt; also forms a Functor: instance Functor (-&gt;) r where fmap :: (a -&gt; b) -&gt; (r -&gt; a) -&gt; (r -&gt; b) fmap = (.) Now, expanding out the type of Cofree, we get a :&lt; (r -&gt; a &lt;: (r -&gt; a &lt;: ...)) which is a kind of state machine: at each level, a is your current state, and you have a function that returns the next state machine. It's also valid to have a &lt;: IO (a &lt;: IO (a &lt;: IO (a &lt;: IO ... ))) Given that you can generate cofree comonads using the function unfold :: Functor f =&gt; (b -&gt; (a, f b)) -&gt; b -&gt; Cofree f a which specializes to unfold :: (b -&gt; (a, IO b)) -&gt; b -&gt; Cofree IO a Cofree IO a could be the contents of a file, or a linked-list represented in the file system, or something. I suppose that you could represent all of these (modulo laziness) using cons cells. However, AFAIK the last two are more exotic than people generally use cons cells for.
Not using it myself but interested in it, my main editor is Sublime at the moment. I'm waiting for Yi to implement a proper native OS X interface (I can't get myself to using GTK apps on OS X, it's horrendous) though not a lot of open-source projects that create Cocoa interfaces as a side-step actually end up with a good interface ... (I'm not a UI designer, otherwise I'd contribute to it myself).
Still using Vim. Played around with Yi a while back, but moved back to Vim, maybe too fast.
The thing that irritated me about Yi is that it's design is very complex and poorly documented, plus it uses its own prelude to define a bunch of stuff and a lot of the code feels very crufty. 
I mostly use visual studio to edit Haskell. When on unixy systems or macs, whatever vi-clone is available.
You know you're onto something good when you get to start *removing* code. :) What's the rationale behind a `T` suffix on some things (e.g. `RequestT`, `ProduceT`) and `P` on others (e.g. `EitherP`)?
`RequestT` and `RespondT` are purely monad transformers and not pipes (i.e. you can't compose them until you `runRespondT`/`runRequestT` them).
Not a teacher, but I really liked that book when I was learning. I emailed my professor about this, who didn't rely much on it but suggested it
That `respond` doesn't do what you think it does. `toListD` never terminates, so the only way you can successfully bind a result from it is to compose it with a pipe that does terminate. It will then fold the pipe you composed with it: foldMeValues &lt;- execWriterK (foldMe &gt;-&gt; toListD) ()
Ah, I see. Thanks for the clarification.
You're welcome!
 [**@ID_AA_Carmack**](http://twitter.com/ID_AA_Carmack): &gt;[2013-05-06 23:52](https://twitter.com/ID_AA_Carmack/status/331557216488390656) &gt;I want to do a moderate sized Haskell project, and not fall to bad habits. Is there a good forum for getting code review/criticism/coaching? ---- [[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/1dv4ij%0A%0APlease leave above link unaltered.) [[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [[Translate]](http://translate.google.com/#auto/en/I%20want%20to%20do%20a%20moderate%20sized%20Haskell%20project%2C%20and%20not%20fall%20to%20bad%20habits.%20Is%20there%20a%20good%20forum%20for%20getting%20code%20review/criticism/coaching%3F) [[FAQ]](http://www.reddit.com/r/TweetPoster/comments/13relk/) [[Statistics]](https://www.stathat.com/decks/PJSe8OF5J44Y) 
As I tweeted, #haskell on irc freenode is the premier place to get expert professional help for Haskell. hpaste and lambdabot are there for code review and interaction. I have personally given a code review to someone of their github project through irc. I believe I responded within 5 seconds to the request. I'm sure most of the channel would trip over each other trying to help John Carmack :P The other major communication channel in my opinion is stack overflow. Use stack overflow if you want dons to answer your questions about Haskell infrastructure ;) Of course many other Haskell experts answer stack overflow questions regularly. Edit: Of course I should not discount FPComplete! How could I forget about them!?
It's listed as recommended course material in the [advanced functional programming course](http://www.cse.chalmers.se/edu/course/afp/) at Chalmers, but there are not any mandatory parts of it in the course.
I tried it, but its VIM mode is missing some basic functionality that got in my way, so I switched back. One thing that Yi really needs is some sort of architectural guide to get contributors up and running. Yi is a huge project and contributing seems a little overwhelming at the moment.
Sounds like something FP Complete ought to jump on. Good press for Haskell. 
What vim functionality do you miss most? Feel free to contribute vim emulation tests, existing ones are here: https://github.com/yi-editor/yi/tree/vim2/yi/src/tests/vimtests
There is hardly such notion as "configuring yi properly", since you can write any code you want in config. Sadly, reading source seems the only reasonable way to understand what's available.
Nothing new. He already expressed his interest in the ML family of languages two Quakecons ago. It is quite nice the high profile developers give such statements as an incentive for younger developers to expand their horizons.
For folks particularly interested in the “write-up” bits of this, starting a thread here for papers related to Natural Language Generation in formal proofs. So far * http://www.aaai.org/Papers/AAAI/1999/AAAI99-041.pdf * http://www.cs.rutgers.edu/~mdstone/inlg02/145.pdf Add more! :-) It's great to see some real world NLG out there. Also a good chance for me to sniff around and see what bits of literature I should be reading, and also what we can learn about this system in particular. (sorry, not-Haskell, although I do my NLG work in Haskell if that counts…)
I would also add that most of the people who are really worried about the ghc version, would roll their own "platform" with exactly the packages they need. I am not yet one of those people.
One sort of problem that Yi might have an interesting time contending with non-expert users with long history of text editor usage, ie that .emacs or .vimrc from over 10 years ago and that was sort of cobbled together and you are now a bit reluctant to touch. I guess there it may be a matter of convincing us to just make a clean break of it. That or maybe this demographic is very much a minority 
(note that they spent years on the proof part of this and ~ 1 mo on the 'write-up' bit -- its important but evidently not the central goal of the project).
Mmm yes, I had that impression. Apologies for my clumsiness in not conveying this point. I'm just particularly interested in the NLG bits :-). What might be interesting is also the places where the two have to cooperate (not a clean post processing step)
Yes, like that or you could create a ticket and attach some tests to it. vim2 branch has better visual block support than master and current hackage version, by the way.
This is going to be interesting. It could really help the language if John gets excited about it. Or hurt it if he comes to the conclusion it isn't practical. Maybe that's why Erik Meijer seems to be trying to dissuade him.
Well, the purpose of isolating effects in the first place is so that we can retain equational reasoning, but `MonadDebug` doesn't give us that since it has no contract. When use use proxies, the proxy laws are the contract and they allow you to prove what your code does without knowing what underlying `Proxy` instance is selected. With `MonadDebug` you don't have any contract. Type classes without laws ruin equational reasoning just as much as doing everything in `IO`.
Oooh. Makes me wish I still owned a personal laptop (I'd feel odd bringing my work laptop to hack on non-work projects).
I'm surprised nobody's suggesting he posts here. People have posted code for review here before, and I think they're generally well received. Plus he already has a Reddit account.
- Surround. - Various text objects (some of which I have from plugins that won't exist for yi). - gj gk (I remap j and k to those). - diw deletes the word under the cursor, but viw visually selects from the cursor to the next word (etc). - Undo is character-wise. - Line numbers. Relative line numbers. - Vertical splits. - Macros. - 256 color terminal support. Terminal mouse support. - gq - Ctrl-Z to pop out to the shell. - ... There's a bit of work to be done. Oh, also: 1000OHello, there&lt;Esc&gt; error: Stack overflow. :)
I thought it was death
&gt; variance introduced by outliers: 84.152% &gt; variance is severely inflated by outliers I'm still not sure what this means but I think it hints that the numbers might be way off, ie. that criterion wasn't able to make reliable measurements.
you mean "haskellishly"
Your algorithm for determining if something is prime is slow in either case. In haskell things get really slow when you try to use regular lists for these kinds of problems. The problem here is lazy evaluation and the fact that : builds up a huge stack of uncomputed operations (thunks). Try copying this classic determining algorithm and see how far you get. import math def xth_prime(n): primes = [] c = 1 while len(primes) &lt; n: c += 1 if try_prime(c): primes.append(c) return primes[-1] def try_prime(n): limit = math.sqrt(n) + 1 for i in xrange(2, int(limit)): if n % i == 0: return False return True if __name__ == '__main__': print(xth_prime(10001)) 
These aren't directly related to the performance question you're asking, but I think it's useful to state them anyway: - Never return `-1` as "failure" value in Haskell. That's a hack other languages use because they don't have a `Maybe` type. - `rem` is faster than `mod`. `rem` calls the ordinary computer modulo operation, while `mod` refers to the mathematical version of it; the two behave differently for negative numbers. [See also this SO post.](http://stackoverflow.com/questions/8111120/integral-operators-quot-vs-div) - Use `null` to check whether a list is empty. Using `== []` requires an unnecessary `Eq` constraint. - Instead of `x == True`, you can simply write `x`. - Instead of `(x:[])` you can write `[x]`, which I think is more readable. - For `n /= 0` and a non-empty `primes` list, your function `xth_prime` calls `head` on a non-empty list, which causes a runtime error. You should use `head` only when you're absolutely sure it will never be used on an empty list. Even if in your small program `nth_prime` is only called with arguments that don't make it crash, it's very useful to be aware of the issue. Also never assume you won't reuse the code at some later point when you've forgotten about what the "right" parameters are. In short: put your best effort into making all functions *total*, i.e. that it maps any valid (in the sense of well-typed) input to a valid output. - Primes are a prime example (AWESOME PUN WASN'T IT) of how to use infinite lists. You can easily generate a list of all primes in Haskell, it's almost possible to translate "the set of all numbers that aren't divisible by a number smaller than it" directly to code. This is still not a very efficient implementation, but it'll teach you something cool about Haskell, namely list comprehensions. I don't want to spoil the solution (unless requested), but if I check for divisibility by "2 or an odd number smaller than the square root of the candidate", I get the solution to the PE problem in a second using unoptimized code in GHCi - and that's still using a *very* basic algorithm. - The `Integral` type class contains `Num` implicitly, thus you don't have to name `Num` in the type signature when there's already an `Integral` constraint. (`Integral` is a `Real` is a `Num`.) - Your `counter` variable is a memory leak. Due to Haskell's laziness, it'll be stored as a long chain of `1+1+1+...` in memory until its value is needed. This is probably a little advanced for a beginner though, but at least keep it in the back of your head that stuff like this can happen. There's a chapter in RWH that discusses the issue ([#25, Profiling and optimization](http://book.realworldhaskell.org/read/profiling-and-optimization.html)).
The big algorithmic difference is whether you first test the largest or smallest prime you have found. Changing primes.append(c) to primes.insert(0,c) loses about an order of magnitude in the Python as well. Conversely, even with the inefficiency of appending to a list, at this problem size changing (counter:primes) to (primes++[counter]) - and returning (last primes) rather than (head primes) is enough that it runs (compiled -O2) a tiny bit faster than the Python. Starting with (2::Int) rather than the implicit (2::Integer) is another significant improvement. Deleting the second clause of try_prime and switching mod to rem are minor improvements Not quite as fast, but much more haskellish: primes (p:ps) = p:primes (filter (\n -&gt; n `rem` p /= 0) ps) main = print $ primes [(2::Int)..] !! 10000 
I strongly suspect that garbage collection is the explanation for the huge variance and Greg thinks the same thing, too.
Project Euler won't teach you what you need to know. If you really want to learn Haskell idioms, then build a project using Haskell and solve real problems with it.
The replacement of `mod` with `rem` along with a few of the other things brought it down to 12 seconds (it's a very unscientific process but it's helping me learn what I'm doing wrong). I've used list comprehensions in Python, I was delighted to find they'd been copied from Haskell because you're right, they're very good. I am however unsure how I'd use a list comprehension to generate a list of primes. The counter is something that's been really bugging me but I couldn't figure out how to write the function without it. As you can see from the Python I'd normally use a loop which is where the counter comes from. What's sort of patterns should I become familiar with for things like this?
Don't let that stop you. Come by for the talks and conversation regardless! (And honestly, improving your programming skills _is_ work related!)
I take my hat off to you sir (or lady), there's no way I could write anything nearly that complicated first try, without backspace and have it even parse in any language! This actually helps me understand what you mean regarding List Comprehensions, thank you. Oh and that was an excellent pun in the first post.
Here are 3 suggestions for speeding up your code without changing the algorithm. I was able to increase the speed several times by making these changes: * Compile with -O2 * Make sure you are using Int instead of Integer when you can be sure that nothing over `maxBound :: Int` will be stored. In this case specify `(2 :: Int)` when you call xth_prime. Integer will be chosen by default unless you are explicit. * Prefer folds and maps over explicit recursion. I recommend changing try_prime as follows: try_prime n xs = and . map (\x -&gt; n \`mod\` x /= 0) $ xs 
Suppose you replaced `fireBurst` with `shockWave`, which pushed everyone within a certain radius of a point out from that point. This kind of effect, by the definition given earlier in the thread, can't be a valid traversal (even if it could be implemented as a `Traversal`), because it changes the criteria used to select the points. But if not a `Traversal`, what *would* it be?
https://twitter.com/ID_AA_Carmack/status/331918309916295168 It's getting more interesting! He's porting Wolf 3D in Haskell!
Carmack has seemed sold on functional languages for a while now in terms of structuring a program. If he were to ultimately disapprove of Haskell, I suppose it most likely would be for performance difficulties.
This is not going to end well...
Also, once you've solved it yourself, I highly recommend reading [Melissa O'Neill's "The Genuine **[spoiler removed]**"](http://www.cs.tufts.edu/~nr/cs257/archive/melissa-oneill/Sieve-JFP.pdf), which does a great job of discussing both the proper algorithm for this and the many ways one might fail to implement it in haskell.
What's the worst that could happen? Carmack declaring Haskell's bad for making games? Something that basically no one's doing anyway? The current OpenGL bindings work, the project's a PC port of a game written over 20 years ago so performance shouldn't be an issue, and he's already shown he's not the type to be dismissive of technologies he's not very familiar with. I think realistically he's going to find some things that he likes, some things he doesn't like, and hopefully point out some low-hanging fruit that can improve the Haskell ecosystem. All while keeping Haskell in the minds of a group of programmers that wouldn't otherwise think about Haskell. Seems like a good thing to me.
That's how I got initially acquainted with it as well -- do whatever you need to learn. :) Larger projects help later on, but Project Euler is great for diving into new languages.
I'm really interested in hearing his thoughts on what the experience is like. I've always felt that programming in Haskell feels so much more "reliable" (for lack of a better word) than programming in other languages, but I'm in no way highly experienced in a variety of languages. I hope that he'll keep the community updated in some way, because I'm quite interested in what he'll have to say.
The tricky part is that it is often very hard to just port existing code to Haskell, as it largely robs you of the flexibility to rethink how to design it to use the new tools at your disposal.
I think the fact that he wants to do this is awesome! What I meant was that I don't think he is going to succeed. I played with the OpenGL bindings recently, and discovered that they were quite low-level, imperative, and not Haskelish at all. I have been trying to implement [pure layers](https://github.com/gelisam/glut-events) on top of it, but so far the rabbit hole is only getting deeper and deeper. Maybe I'm just heading in the wrong direction. The point is that if he likes Haskell because it's purely functional, then creating a game based on the OpenGL bindings is not going to shine a light on how good Haskell is at it.
wow, very interesting read. Thanks for linking it!
Have you tried GLPipe? It's a purely functional layer over OpenGL. I've tried it. Here is the result: https://github.com/food2games/sandbox/tree/master/sandbox01 (There is a little hackery with GLUT and the reinversion of control, which has been taken from here: http://blog.sigfpe.com/2011/10/quick-and-dirty-reinversion-of-control.html )
"Avoid success at all costs" 2008 - 2013 R.I.P.
It's "avoid (success at all costs)," not "(avoid success) at all costs."
I ported some C code for Othello/Reversi AI to Haskell. https://github.com/geon/Othello-in-Haskell It was small, just 2 pages of C, but enough to ge a feeling for the language. My first issue was the lack of mutable state for the board, since the C version depends heavily on that. I saw another implementation in Haskell that used lists of lists like they were arrays and concatenated the old board around the replaced piece. It seemed really stupid and inefficient. In the end, I used a map from (Int,Int) to Piece, and it seems to work fine. 
Haskellesquedly.
The difference between Int and Integer is that Integer is an unbounded mathematical Integer, while Int is the ordinary computer int, which is limited in size. The latter is faster. By explicitly writing, say, "`(42::Int)`" you get an Int instead of an Integer. (...but I thought you would get an Int by default. I could be wrong.) But I, too, think the example a little bit confusing compared to the list comprehensions. I would stick with the list comprehensions.
How much time will it take for Carmack to eventually get to FRP, acknowledge that it needs boost, come up with the best FRP lib ever, and get it to become mainstream? He standardized the FPS genre, after all... :D
Makes sense.
Ahhh, now I understand. Thank you.
Useful, thanks.
Request for help is at the bottom of the page. Source is on [Gitorious](https://gitorious.org/snowdrift/snowdrift). I'm not involved in any way, just saw the link in a crowdfunding discussion.
For an added bonus, use `all p = and . map p` for: try_primes n xs = all (\x -&gt; n `mod` x /= 0) $ xs (for a smaller added bonus, eta-reduce by removing the `xs` from both sides) EDIT: Not a performance bonus, mind you. But a "Haskellishesqueness"-bonus. Or whatever word.
A little late here, but as an Erlang dev I'm interested.
I have used Yi sometimes and watched it improving over the years. As a long time Emacs user I miss interactive help and many of the M-x commands don't seem to operate generally. But I am keen to see the day it can be my main editor, including a compile-mode, etc.
Maybe he'll just improve whatever libraries we currently have to make the usable. That's what OSS is all about, no? somebody starts and somebody else improves.
Haskelloquently
Great name! Great idea. 
&gt; I saw another implementation in Haskell that used lists of lists like they were arrays and concatenated the old board around the replaced piece. It seemed really stupid and inefficient. This is the thing, you really have to think about the structure differently, rather than just trying to shoe-horn it in to Haskell. Which on a big project, will take time.
Removing the `xs` increases its haskellosity by 15%.
&gt;Maybe that's why Erik Meijer seems to be trying to dissuade him Link?
The SNR here isn't great. Code reviews here tend to get some good advice, but also get dozens of useless or even outright bad replies.
Help is another issue, yes. Vim also has tons of documentation of every function, indexed for jumping around interactively inside vim. That takes a lot of work to replicate, and especially so if you want to do it for several different emulation mode as Yi will have to.
Glad to hear pagination is coming. Although at the rate I post it would be years before I really need it anyway :P
I did look there, and only saw a single tweet from Erik, which does not appear to be attempting to dissuade him. That's why I asked for a link. Is this what you call "trying to dissuade him"? &gt;You mean Monads and FP? My mantra is think in Haskell, hack in objects. The essence of FP is to juggle with code.
&gt;idiomatic game/UI programming in Haskell is an ongoing research topic. Nobody knows how to do it right Nonsense. You can do it just like you would in any other language. If nobody knows how to do it "right" (a completely absurd concept) in haskell, then nobody knows how to do it "right" in any other language either. &gt;writing a fairly complex video game as babby's first Haskell project is not a particularly good way to learn or get a impression of the language. It isn't a complex game, it is quite small and simple. And he has already written it, more than once, in more than one language. And, it is not "babby's first haskell project". I think John is more qualified than you are to decide what is a good project for him, and the more you say, the less qualified you appear to be.
not many objets in haskell
I don't think it will happen. FRP is still pretty firmly academic from his point of view, and I don't think he's especially interested in it. He has even said things on Twitter in the past that I interpreted to be fairly critical of Conal's work. I, too, don't think FRP is ready for anything realistic yet; there is some breakthrough still necessary, in my opinion. However, I'm a big fan of the idea and really want it to succeed.
Perhaps you should take some time and learn haskell. Claiming there is no benefit to haskell if 5% of your code is in an imperative style is ridiculous. And imperative haskell is much nicer than imperative C++. Idiomatic code has nothing to do with using the languages features to the fullest extent possible, at all.
And? You actually think a single tweet saying "this is what I do" is him trying to dissuade Carmack from using haskell? Seriously?
Honestly, I don't think Carmack would be able to write a good FRP library. His brilliance is primarily in performance and technical areas, not in inventing novel abstractions or theory.
If you are worried that John will be disappointed with the language, then now is the time to start fixing what you perceive to be the flaws.
&gt; Writing it down in a readable way may be a good start Well, when list comprehensions get hard to read, you can always start by writing them in monad syntax: primes = do n &lt;- 2:[3,5..] let divisors = 2:[3,5..(floor.sqrt.fromIntegral) n] guard $ not . any (\x -&gt; n `rem` x == 0) $ divisors return n prime n = primes !! n
Is there any intuition behind the name of that operator? I saw this the other day and thought "wow, that's a ridiculous operator," but I've seen plenty of weird operators in Haskell to date and they all end up making some sort of sense in context after I've used them for a while. I know you're not the author of `Lens`, but I'm curious about the naming scheme of this particular operator. Any thoughts?
Very good question! Let's consider the this from a purely type-oriented perspective. First let's just consider the structure of lists. A list is formed by two constructors: [] :: [a] (:) :: a -&gt; [a] -&gt; [a] Notice of course that these three types are the same: [] :: [a] &lt;--- these / \ / \ v v (:) :: a -&gt; [a] -&gt; [a] That makes sense -- we want to be able to use the output of any constructor as the input to any constructor. `[]` is a list, so we want to be able to give it to `(:)` which should also produce a list, which we can give to `(:)` which ... So really, as long as we have values with these types, we can swap out `[]` and `(:)` like so: z :: [a] f :: a -&gt; [a] -&gt; [a] -- swap [] for z, (:) for f: [0,1,2] = 0:1:2:[] = 0 `f` 1 `f` 2 `f` z -- or if you prefer [0,1,2] = (:) 0 ((:) 1 ((:) 2 [])) = f 0 (f 1 (f 2 z) So for instance if we have z :: [Int] z = [-1,-1,-1] f :: Int -&gt; [Int] -&gt; [Int] f n ns = n:n:n:ns -- i.e. [n,n,n] ++ ns We can do some swaps. Applying these swaps to `[0,1,2]` as before: 0:1:2:[] =&gt; 0 `f` 1 `f` 2 `f` z = [0,0,0] ++ [1,1,1] ++ [2,2,2] ++ [-1,-1,-1] = [0,0,0,1,1,1,2,2,2,-1,-1,-1] But wait, as long as these types are the same (repeating the picture from before): [] :: [a] &lt;--- these / \ / \ v v (:) :: a -&gt; [a] -&gt; [a] we can still do this! It doesn't matter that they're all `[a]`, they just have to be *the same*, so why not `b`! z :: b f :: a -&gt; b -&gt; b So how about this replacement: z :: Int z = 0 f :: Int -&gt; Int -&gt; Int f m n = 1 + n Now we replace again on `[0,1,2]`: 0:1:2:[] =&gt; 0 `f` 1 `f` 2 `f` z = 1 + (1 + (1 + 0)) = 3 Of course, we want to do this by recursing on the list but that's obvious. Given `z` and `f` we know that swap f z [] = z just by the definition that we're swapping `z` in for `[]` and `f` in for `(:)`. Similarly, we know that swap f z (x:xs) = f x ... But what to put for `...`? Well, of course, it has to be another swap, because we have to swap *everywhere* in the list, not just the beginning swap f z (x:xs) = f x (swap f z xs) But wait, isn't this just `foldr` with a different name? It is! So can we do this same sort of thing for other types, you ask. Well, we need to swap constructors everywhere. So how about for trees? data Tree a = Leaf a | Branch (Tree a) (Tree a) We have two constructors Leaf :: a -&gt; Tree a Branch :: Tree a -&gt; Tree a -&gt; Tree a But all we need, to make the swaps work, is that in the replacements for `Leaf` and `Branch`, these types are the same: Leaf :: a -&gt; Tree a ^ \ these these these these / \ \ v v v Branch :: Tree a -&gt; Tree a -&gt; Tree a So: z :: a -&gt; b f :: b -&gt; b -&gt; b And swaps should be obvious: swap f z (Leaf x) = z x swap f z (Branch x y) = f (swap f z x) (swap f z y) It should be clear now how to do this for any type defined with the `data` keyword, more or less. But there's a hidden unity here, that I can't really explain without going into some more general stuff. I'll add more later. Watch this space!
Well it wasn't very encouranging. He did not say great idea, good luck. He's basically saying that even he does not write haskell anymore. This is consitent with what he has said in some recent talks. &gt;My mantra is think in Haskell, hack in objects. I don't think that's going to disuade John. But I interpreted that as "why bother". 
This program is a copy of the above and it is also wrong :)
Performance .. I hope he learns all the tricks
Upon reflection, it seems I was misleading when I concluded that the different implementations allowed different interpretations. You can implement the "bookends" interpretation of `runSeerTWith` for RWSeerT just as easily: run past future (RWSeerT rwma) = liftM fst $ mfix (\ ~(_, w) -&gt; runWriterT (runReaderT rwma (past &lt;&gt; w &lt;&gt; future))) The same is not true of the "false reality" interpretation, which I do not believe can be implemented by a Tardis. Let's be fancy and say that this is because of Tardis Laws or something.
Correcting it is left as an exercise to the reader. (Also it says the first prime is 3, not 5 as claimed.)
&gt; Stop checking at sqrt n. Stop checking when your divisor candidate i, squared is greater than n. (i.e. check only if i * i &lt;= n)
Indeed. I am often disappointed by the aversion to C. We have a large haskell codebase and use C when necessary in certain cases. Then both languages get used for what they are good at.
my point was that haskell list comprehensions can be converted almost verbatim into do-notation for style… :)
Ordinary records don't solve the porblem of multiple records with the same key name.
The `@` signifies that it includes index information. The ` =` signifies that you are assigning something in the State monad. `&lt;` signifies that it also returns the assigned value (i.e. "passthrough") and if there are two possible values to pass through (as there are in this case, because the setting function has different input and output types) then the `&lt;&lt;` signifies returning the second possible value. I couldn't figure out what the `%` signified.
I think this may be a TL;DR of psygnisfive's comment. My co-student Andy (tho he got his degree; I flew across an ocean to try again) pointed out that folds basically just replace the constructors of a list with functions with the same type. so a Foo list has a 2-nary CONS constructor that takes arguments of type Foo and list-of-Foo. OR the list is a nullary constructor that is just NIL. all fold does is to replace every CONS with f and every NIL with z. You can then generalize this to any type: each of its constructors is replaced by a function of the same type as the constructor. Andy then went on to show how this replacement could be used to remove intermediate results from chained computations (deforestation).
We were discussing this back in August or September (my file has a timestamp of Sep. 25) and I wrote my own version of the Reader/Writer implementation as an exercise. I'll just go ahead and [put it in a gist as](https://gist.github.com/sacundim/5544704), but it's basically the same as yours. However, my toy example #2 in that gist is worth quoting here: -- A Program is a sequence of Statements. A Statement is -- either a Definition of an Identifier (a String), or an Use of an -- identifier. type Program a = [Statement a] data Statement a = Definition Identifier a | Use Identifier type Identifier = String -- An example Program: program1 = [ Use "y" , Use "x" , Definition "x" 5 , Use "x" , Definition "y" 7 ] elimDefinition :: Program a -&gt; [a] elimDefinition = catMaybes . evalSeer . mapM elimStep elimStep :: Statement a -&gt; Seer [(Identifier,a)] (Maybe a) -- Each time we see a Definition, we send it to the Seer. elimStep (Definition v x) = send [(v,x)] &gt;&gt; return Nothing -- Each time we see a Use, we consult the Seer. elimStep (Use v) = liftM (lookup v) see example2 :: [Integer] example2 = elimDefinition program1 I see that you've come up with some rather interesting new ideas since, though. I'm going to have to chew a bit on this `contact` operation, and the concept of initial "bookends" vs. "false reality." In particular I want to see if I can find a third implementation without using either Tardis or Reader/Writer... **EDIT:** [Applicative Seer](https://gist.github.com/sacundim/5546074)
For those who don't know, Timothy Gowers is one of the most renowned mathematicians of the modern era and a recipient of the Fields Medal, the highest honor in the field.
If I had 6 years of experience (just coming out of my undergrad this month), didn't already have a job lined up, and had the ability to move to Oregon I'd be all over this job. Double major systems engineering (EE focus) and math with a CS minor, and I love functional programming.
Honestly I *much* prefer the `OpenGLRaw` package to `OpenGL`. `OpenGL` is needless abstraction and overhead for very little gain, and makes it difficult to search e.g. the OpenGL wiki to learn about OpenGL functionality.
I perceive the biggest flaw will be.. stop-the-world garbage collection. On the upside .. libraries may not be a problem since it should not really require many. Though, getting over the initial hump of getting OpenGL+keyboard/mouse input can be a bit of a pain last time I checked. [aside: It sounds a bit like I am bashing Haskell library support -- but I am actually quite happy in that regard. I've just heard others grumble about their particular needs.]
Well, I spent tonight playing around with an applicative version of seers, because I just saw that the examples I have in mind don't need the power of `Monad`—`Applicative` and `Traversable` will do. For example: -- -- Example: divide each element of the traversable by their sum. -- normalize :: (Traversable t, Fractional a) =&gt; t a -&gt; t a normalize = runSeerA . traverse step where step x = send (Sum x) *&gt; ((x/) &lt;$&gt; fmap getSum see) [See my Gist for the full code.](https://gist.github.com/sacundim/5546074) I don't know what you mean by "multiple levels" of a traversal, but it seems that if all you need is for each step of the traversal to see data sent from all other steps, `Tardis` is more power than you need.
I have been working on a vision library for Haskell myself built on Repa so a lot of this is familiar. But I have to say the code looks great and I am blown away by all of the examples and documentation. Great work!
The documentation is simply fantastic. Please keep it up! Any plans to expand into computer vision (ml) functionality?
Another idea I just had. What if we changed the class signatures to this? class (Monad m, Monoid w) =&gt; MonadSeer w m | m -&gt; w where see :: m (w, w) send :: w -&gt; m () contact :: w -&gt; m (w, w) The idea is that now `see` and `contact` return a `(past, future)` pair, so that at each step that the computation consults the seer, it can distinguish visions of the past from those of the future. The Tardis implementation can be easily changed to do this, but I'd be surprised if the Reader/Writer implementation can do this. Example usage: foo :: (Traversable t, Num a) =&gt; t a -&gt; t a foo = runSeer . traverse step where step x = do (p, f) &lt;- contact (Sum x) return $ (x + getSum p) * getSum f example = foo [1..10] -- [(1 + sum []) * sum [2..10], (2 + sum [1]) * sum [3..10], -- (3 + sum [1, 2]) * sum [4..10], ..., (10 + sum [1..10]) * sum []] The obvious question, however, is why would one ever want to have this "in between" interface instead of just Tardis. But more generally I think we can ask that question of this whole possible hierarchy: 1. Original Seer: you can `send` messages that can be `see`n at any time in the computation. You're guaranteed that other `send`ers in the past or future cannot discard your message—they can only `mappend` to it. However, `see` cannot reliably distinguish between past and future data. Known to have a statically analyzable `Applicative` version that exploits this inability. 2. Modified Seer: Like original Seer, but `see` can distinguish between past and future data. (Is there a statically analyzable Applicative?) 3. Monoidal Tardis: you can separately send messages of different types to the past or to the future at your discretion. Other senders can only `mappend` to your message, not discard it. Recipients can distinguish between future and past data. 4. Full Tardis: you can send messages to the past or to the future, but other senders can discard your message and replace it with one of their own. Superset of the State monad, so definitely no applicative-only version.
If you want to play with purely functional GPU programming, you might want to look into [LambdaCube](http://lambdacube3d.wordpress.com/), which is a much more recent project. Both its performance and expressive power is significantly better than GPipe’s at the moment, and there’s still a lot of improvements coming.
Is this a paid position?
I tried Yi a number of times, but it looks completely broken for me. At least vty frontend (never tried other frontends). It doesn't use advanced terminal capabilities (e.g. scroll regions), and that's why it is extremely slow and not responsive. Complete redraw at every keystroke! And I don't see how it can be fixed: 1) vty doesn't provide the api, 2) yi is not designed for that. (I'm using vim)
They say it will be a non-profit system, so I guess not.
Awesome! I will be using this in a project im working on right now. Thanks a ton!
The lens module does solve that problem, while being built on top of ordinary records. The post you replied to makes this very clear.
Great stuff Jasper!
Great to hear! Please email me if it is lacking features.
I would love to take *full* credit for the doc but I mostly followed the format of unmscheme: http://www.cs.unm.edu/~williams/image-ref.html The professor is a big computer vision guy. I want to implement Boldt Line Finding over the summer to match his scheme library: http://www.cs.unm.edu/~williams/boldt-ref.html Unfortunately, I have not been able to find the paper for free. However, I do have access to the source code for the above API.
That is awesome! I would love to see what you've been doing.
Thanks! I hope we have enough features for you to do what you need. We definitely don't match that python library. If you find we're lacking something, let us know!
Well, at least in just-released version 0.6.7 we ask vty to redraw the screen asynchronously, so that if you manage to input N keystrokes in one frame this will result in only one or two redraws, not N. Also, there was a memory leak in vty frontend, that caused performance to rapidly drop over time, it's also fixed in this release.
Hi everyone, I'm cofounder of Snowdrift.coop. I'm happy to see the interest. I don't have time to get into details here; I'm working mostly on clarifying things on the site. The fact is, we don't have much budget, it's a couple of us just dedicating our lives to seeing this vision through to making a difference in the world. So we're hoping most to inspire others and have them join us and volunteer their time. Besides the broader purpose, it will be a chance for a great resume item, great experience, etc. Also, as soon as we are functional, we hope it will be a way to fund your other projects and so get a better income doing work you love in general in the future. All that said, we might be able to put some funding in to pay for some development now, depending how budgets work out. Also, as soon as we start functioning at all, even just beta level, we will start having an income ourselves, and that will be dedicated directly to pay developers for ongoing work. So getting in on the team now means the prospect of it turning into a paid gig soon. As ocharles says, non-profits do pay salaries to employees! Lastly, we have more content than is currently published. We're in the process of transition to more public state and finalizing many drafts and details. Aside from older tickets on GitHub, we are building an integrating ticketing system to go with the service and moving things there. Also, we have a general next-steps page which is currently not public, but we can send interested people an access code to see that and other drafts. Please feel free to send us any questions. Also we have a freenode IRC channel #snowdrift Cheers, Aaron Snowdrift.coop
Question ends with this remark: &gt; Basically whenever anybody talks about folding, they always talk about folding *lists,* which is inherently sequential. Although it's true that lists are typically implemented sequentially, their *denotation* doesn't preclude something more sophisticated. The very type signature of `foldr` itself points toward opportunities for parallelization: rearranging the arguments a bit and adding parens for emphasis, we have the equivalent `[a] -&gt; (a -&gt; (b -&gt; b)) -&gt; (b -&gt; b)`. So we see that a `foldr` can be interpreted as an evaluation of a monoidal expression with an endofunction on `b` replacing every `a` element and with function composition as the associative operation. This monoidal reduction can be efficiently performed as a so-called *parallel prefix sum* albeit requiring evaluation under lambdas. E.g. Consider `sum = foldr (+) 0`. Then `sum [1,2,3,4]` can be thought of as `(1+).(2+).(3+).(4+)` applied to `0`. Notice that we can evaluate `(1+).(2+)` in parallel with the other pair to obtain `(3+)` and `(7+)`, respectively. (I mentioned this to Guy Steele after his "Folds considered harmful" talk at ICFP Edinburgh. Iirc, a good chunk of his talk was based on the assumption that folds are sequential, and therefore their use is a stumbling block in an increasingly parallel world. Nonsense, think denotationally.)
Seems interesting and I would be interested in trying it out, but it seems to be missing documentation on [hackage](http://hackage.haskell.org/package/HLearn-distributions-1.0.0.1) for any version after 0.2.1 and the module structure seems to be drastically different now.
append is faster than Python - it's a mutable list kept as an array with extra space at the end, so you just use up an extra slot and bump a limit (and rarely reallocate if you overflow). The speed difference is backwards in Haskell because [] is an immutable linked list, so let l2 = x:l1 allocates one new cons cell and shares all of l1, while let l2 = l1++[x] has to also copy everything in l1 because l1 and l2 don't share any suffixes. In general, constructing a list by appending single items one at a time takes O(n^2) time, while prepending items takes O(n) time. However, for your algorithm and for the input sizes, those speed difference either way are totally swamped by the algorithmic benefits of first checking whether candidate numbers are divisible by small primes like 2,3,5... rather than first checking the largest primes. (and going large to small you have to waste tons of time on every number checking or skipping primes larger than sqrt n before you even have a chance to find a divisor, while going from small to large you only ever visit those numbers for candidates that have already passed the smaller tests and are actually prime). Incidentally, Data.Sequence is a list-like type that gives amortized O(1) access to either end and nice bounds for other operations too (though worse constant factors), but it doesn't really change the runtime for your algorithm.
Hackage just hasn't gotten around to building the documentation yet. It'll be there soon.
Performance on the current version is just okay. It is good enough for the class that it will be used in but the Boxed images would not be good enough for anything that requires high performance. We plan to implement Unboxed image types for better performance and we are looking at repa (http://hackage.haskell.org/package/repa) to add parallelism on most of the operations (convolution, fft, etc.). Another option we are looking into is a way to use the GPU to increase performance. We know of accelerate (http://hackage.haskell.org/package/accelerate) but we have not investigated it yet.
I finally got the chance to work through this (after some dependency issues with `Lens` on Windows) and I'm incredibly impressed. Thanks for putting together an easy-to-follow `Lens` tutorial -- I've been meaning to play around with it for a while and this was clear as day!
&gt; Unfortunately, I have not been able to find the paper for free. /r/scholar
You're welcome! I've only implemented a small fraction of what a more established Markov Model library can do, but I think this interface is much more expressive, flexible, and easier to incorporate into an average program.
http://www.reddit.com/r/haskell/comments/1929xn/are_lispstyle_macros_a_code_smell/ If that's not it, use the [same search I did](https://www.google.com/search?q=site%3Ahttp%3A%2F%2Fwww.reddit.com%2Fr%2Fhaskell+lisp&amp;aq=f&amp;oq=site%3Ahttp%3A%2F%2Fwww.reddit.com%2Fr%2Fhaskell+lisp&amp;aqs=chrome.0.57j58.2771j0&amp;sourceid=chrome&amp;ie=UTF-8#sclient=psy-ab&amp;q=site:http%3A%2F%2Fwww.reddit.com%2Fr%2Fhaskell+macros&amp;oq=site:http%3A%2F%2Fwww.reddit.com%2Fr%2Fhaskell+macros&amp;gs_l=serp.3...2995.4126.0.4419.6.6.0.0.0.0.93.499.6.6.0...0.0...1c.1.12.psy-ab.PtMtNwrBd0M&amp;pbx=1&amp;bav=on.2,or.r_qf.&amp;bvm=bv.46340616,d.dmQ&amp;fp=55c01cc7178c2856&amp;biw=1252&amp;bih=967) and look through the results.
Yeah, I definitely need to get on that. They're not implemented yet, mostly because I haven't decided on what the "right" interface for it should be.
&gt; So we see that a foldr can be interpreted as an evaluation of a monoidal expression with an endofunction on b replacing every a element and with function composition as the associative operation. This monoidal reduction can be efficiently performed as a so-called parallel prefix sum albeit requiring evaluation under lambdas. God, I'm having such an "aha!" moment right now. I'd references to parallel prefix sums before, and even stared uncomprehendingly at papers, but only now I feel like I start to get it. Think denotationally indeed!
Looks interesting. I'll bookmark this for later. The examples look great but support for .png (and jpg + bmp if possible) would make people much more likely to try it out in my opinion. I only bring it up because it seems a shame to hold back a promising bit of work with something relatively easy/standard like that.
And the technical term for the collection of "functions of the right type" is: "`F`-algebra" ...where `F` is the functor such that `fix F` is the recursive data type you're interested in; e.g., `fix (ListF a) = [a]` so the pair of arguments to the fold function `(a, a -&gt; b -&gt; b)` is an `(ListF a)`-algebra. Well, technically, the `F`-algebra is the pair of the concrete type `b` along with however many "functions of the right type" you need. But we often elide over this detail and just assume the concrete type is there when we need it.
You're absolutely correct. Support for these is on the top of the list of features to add.
That's really helpful and informative, thank you :)
It should only be necessary to use on data structures whose invariants depend on which instances of a type class are used. I think philipjf's idea goes something like this: module A has instance OrdInt : Ord Int where ... module B has instance OrdInt : Ord Int where ... Map gets an implicit extra parameter like data Map key {ordKey :: Ord key} val = ... then functions like "union" are elaborated to require the same instance, so a full signature might be like union :: (ordKey :: Ord key) =&gt; Map key {ordKey} a -&gt; Map key {ordKey} a -&gt; Map key {ordKey} a and most of the time the instance arguments can be inferred and you don't have to worry about them, but they make sure you get an error if you mix a Map Int {A.OrdInt} a with a Map Int {B.OrdInt} b.
Hackage builds docs every ~12 hours or so.
&gt; Gitorious instead of github &gt; Largely experimental and immature Haskell/Yesod ecosystem instead of a popular web language Self-marginalization doesn't seem like a good start to me.
Is there anything in your lens library that would let me determine at compile time which record a lens accesses in the same way that what I called a "type lens" in the blog post does? It would be nice if I could rely on existing code if it does exist.
Cool, I've had a few things in mind where crit-bit trees really would be better than Data.Map, and implementing them myself has been a low-priority potential project in the back of my mind for a while now. I'll take a look!
&gt; How do you usually decide what to use? Understand the pro and cons and pick the best approach for what you need. It all depends on what you are trying to do and what you actually need. 
My bad, I totally forgot about this one. ^(On the other side, you have mentioned in the blog that eventually you will switch to a DSL approach rather than an EDSL one. I thought I'll wait until that.) Amúgy sok sikert :D
Also try [LambdaCube](http://lambdacube3d.wordpress.com/). It is more recent, and (according to its creators) more efficient. I should've mentioned that earlier.
Yes, anytime I make a choice, I usually weigh the pros and cons and go with the best approach instead of the second- or third best :) Let me rephrase that: what are each of these most suited for?
I love the idea behind this library (encouraging more people to contribute) however one thing seems to be lacking from the readme - what a crit-bit tree *is*. I guess explaining that might be a good first pull request!
threads + STM FTW
Well, if someone is actually wondering: http://cr.yp.to/critbit.html
&gt; How do you usually decide what to use? Usually? I did it once. Use STM because it's composable, then design on top of that. The nice thing is that it's almost impossible to engineer deadlocks, though starvation can happen if your transactions are too big. MVars etc. are more traditional locks, which *may* make sense to use in simple situations *after* you benchmarked. Because doing anything remotely non-trivial with it comes with the aspirin needs of lock-based systems. There's one other case: If you actually want to reason formally about your concurrency, use [CHP](http://www.cs.kent.ac.uk/projects/ofa/chp/), which is an implementation of CSP. For straight-forward hacking, though, I'd recommend building everything directly on top of STM, home-brew, because then you can tailor your abstractions to your needs.
Given a lens that accesses one of two fields of the same type in a structure, there is no way to distinguish between them via types alone. The field path leaves no 'residue' on the type. It isn't possible to do this and support the full gamut of lens types that the lens library supplies, because a lens isn't necessarily a field, it could be an isomorphism on the whole structure to pretty much anything, etc.
I wonder how fast it really is.
I'm confused - I though that `Data.IntMap` was implemented this way, with crit-bit/PATRICIA trees. Am I wrong, or is the only difference that you allow arbitrary keys, as long as they can be serialized?
`IntMap` is indeed a crit-bit tree.
I guess task parallelism must be relatively simple, but I'm interested too!
SPJ himself gives good intro to most of he topics you mention here: https://www.fpcomplete.com/school/beautiful-concurrency 
I use the lightest abstraction (in terms of interface first and implementation second) I can get away with. That means I'm happiest if I can get away with just using IORef, although that can't always work out. Since TVars are roughly as easy to use, they typically are the first thing I reach for when I need more than one shared reference used together. Message passing is not common for me because I guess I just don't tend to work on the most appropriate projects for that. 
Source is on both gitorious *and* github, and we keep both in sync.
I use (T)Chan and message passing for everything.
I started with CHP, but (T)Chan solves the 80% case
I see a few people have already signed on to the experiment. Might I suggest a list of bounties? You'll catch more fish with bait :) Edit: The export list of Tree.hs already covers that! I am sorry I missed it.
&gt;Lens does have makeClassy (which does solve this problem) So, you know it solves the problem, but are arguing that it doesn't solve the problem? &gt;but Vinyl offers instead another approach Yes, and the question was how much are you giving up to get that compared to the lens solution.
According to Bernstein (dbaupp's link) crit-bit is a simpler variant of PATRICIA. It's Bernstein, so absolutely minimalism is definitely considered a design virtue.
Haskell/Yesod is nowhere near experimental. It is a rock solid foundation for building "type-safe everywhere" web applications, and a very good choice for Snowdrift. "Popularity" does not mean efficiency nor robustness.
I found this more helpful: http://en.wikipedia.org/wiki/Radix_tree
does *not* answer your Q but: this PhD dissertation starts w/comprehenensive survey of concurrency mechanisms http://www.ccs.neu.edu/home/turon/thesis.pdf
&gt; So, you know it solves the problem, but are arguing that it doesn't solve the problem? No, I'm not saying that lenses can't deal with it, I said that ordinary records can't deal with it. Specifically, you can't define multiple record types with a shared key name in the same module.
Maybe you need to re-read the thread.
Thanks, that's what I thought.
That seems to be mostly about STM.
Hello bos, thanks for another fine library! If I can suggest something it would be to create a bunch of issues in the form of "enhancements", breaking them in small "pills". This way people willing to contribute could just pick one task from the list and hack around :)
The answer is "it depends". I've written a crit-bit tree map in C with Python bindings. With a region allocator and a couple of other tricks, it performs extremely well *if* you build it up in fully or mostly sorted order, and use sorted lookups. It's also faster at sorted traversals than Python's hash table implementation is at random traversals. However, it is significantly slower than hash tables at random lookups or if you insert values in random order. Ultimately, we found that for our use cases crit-bit trees were faster than hash tables, but not faster enough to justify supporting ~1500 extra lines of C code.
I use MVars, typically. But I use MVars only with with_ constructs, and I use Chans more than MVars. Honestly I should move to doing things with STM, but I'm comfortable enough with lower level tools that I seldom care to. And in fact, when possible, and when I'm doing performance-sensitive stuff, I try to use IORefs with atomicModifyIORef or as guarded by a single MVar. Cloud Haskell is great, but I don't think it beats chans + co until you need distribution.
In what sense is it more compact? A PATRICIA tree (at least the one we use in `IntMap`) does collapse extraneous branches so you only have interior nodes for the bits that actually differ. Edit: Scanned the page dbaupp linked to. That page suggests that more modern crit-bit trees use fewer words per node. However, both `IntMap` and Bryan's crit-bit tree use 4 words per internal node. Edit 2: However, if crit-bit trees are simpler to implement, perhaps we can replace the current `IntMap` code with them.
Couldn't you create a Sum instance for HP since it expects a Monoid?
Unless you want to compose more StateT on top, but that way lies madness. Madness, I tell you. (Actually, it's quite common to compose a ReaderT for, e.g., config options.)
I believe there is a language extension to set which type you get by default too so I wouldn't rely on the default too much anyway.
If it helps [here's](http://nothings.org/stb_image.c) a nice stand alone (easy to integrate and build) image reader in C. Supports png and jpeg at least (although I've only tested with png.) I've used it in my own work (C++ projects, not Haskell) and it's been great! If you end up wrapping C code to do the job of reading images I'd recommend using this instead of total overkill/tircky to build/overfeatured libraries like DevIL.
I'm using my work laptop!
That's a good idea. I mostly haven't done it because I needed to go to bed last night :-)
You can build a blind trie, but then it has to scan upward from the leaves once it fails to match. This actually has non-trivial impact on the memory access pattern. [TANSTAAFL](http://en.wikipedia.org/wiki/There_ain't_no_such_thing_as_a_free_lunch). There is no real benefit compared to the PATRICIA approach for the `IntMap` case, where there is no need for the reverse pass and the fact that given that the keys are fixed size, the branches are already fixed size. For a larger key, its a trade-off. However, it really only performs well in the RAM model. In the I/O or cache oblivious models which are "closer to true", it does a lot of very scattered reads. There are more efficient points in the design space than the strictly blind trie if you are searching for efficiency in access after having frozen the structure. The (Succinct) Cache-Oblivious String B-Tree comes to mind. The amortized version of it can even be implemented functionally, though the amortization accounting necessarily assumes ephemeral access as do all cache-oblivious data structures of which I know.
Yes.
Open long enough file (~1000 lines), hold DOWN arrow for 10 sec and then release it. yi-0.6.7 will continue scrolling ~3 sec (most likely it is busy redrawing screen for every keystroke, but I may be wrong here). One-line-at-a-time scrolling is generally ~2x slower then in vim, it is not smooth, sometimes yi stops responding for ~0.5 sec when scrolling.
I guess you are more familiar with this than most, Simon :)
Just from this article, I don't think you really understand. A "monad" is not some particular value in your code, but the type constructor along with those two functions in the type class.
Kudah, first of all, we welcome constructive feedback and will encourage the same attitude in all projects. Please feel free to continue expressing your concerns. There are various reasons for decisions we've made so far. **Our top priority is being absolutely 100% Free/Libre/Open**, even if that compromises other things. I am not dogmatic about this personally nor do I expect most users to be hard-liners, but there needs to be no doubts about the site. Snowdrift.coop will set the standard for these things as a model for projects. And **GitHub is proprietary**, whereas Gitorious is Free/Libre Open Source (we are on BOTH however, we just point to Gitorious primarily). Besides this, our reasons for different decisions range from believing in the effectiveness of something to simply not knowing better. Anyone with constructive input can help us continue to make the very best decisions from here. Cheers!
Libnitz's [monads](http://en.wikipedia.org/wiki/Monadology) have nothing to do with Haskell monads other than the name. Similarly, studying the [Ant and the Grasshopper](http://en.wikipedia.org/wiki/The_Ant_and_the_Grasshopper) won't teach you anything about CPU cache.
Not using language right, and not understanding, are not the same thing. The author says "a monad" instead of a "value in the monad" or something of that sort, but this occurs right after the "a monad is anything that supports these two operations" which is quantified correctly.
&gt; Similarly, studying the Ant and the Grasshopper won't teach you anything about CPU cache. Pf. Clearly, it is a potent allegory about the perils of having too many cache misses at the wrong time.
I don't know much about oauth2, so this might be a silly question, but why would you need to keep updating `~/.hfreeagent`? I usually use `unsafePerformIO` to read config files into a variable once...it's easier than dealing with a `ReaderT` for such a small reason. Also, afaik there is no library to derive your types for you from JSON...if you were to use such a library, it would have to derive your types during compile time. So you would need to give it the JSON at compile time. If you want to parse arbitrary JSON you can always use aeson to decode it as a `Value`: result = decode json :: Maybe Value But that can be a bit of a pain to use. I would suggest writing the types yourself. You *can* derive `ToJSON` and `FromJSON` instances using [Data.Aeson.TH](http://hackage.haskell.org/packages/archive/aeson/0.3.2.12/doc/html/Data-Aeson-TH.html).
&gt; What should my monad stack look like? You don't need to pick one monad stack and stick with it. You can just implement each function in the subset of monads in needs, then use [hoist and lift](http://www.haskellforall.com/2013/03/mmorph-100-monad-morphisms.html) to plumb those together once you discover which stack you need.
&gt; What should my monad stack look like? My first impulse is to say that this question is itself a problem. It seems to skip right over, "Do I need a monad stack?". And, even that is not really a good question. I usually write code as pure functions. If I need some IO then I do things in the IO monad. I only start adding other monads and monad transformers when something starts to annoy me enough. So, to give a somewhat concrete example, there is a stripe library on hackage which uses a 'StripeT' monad transfomer (which is basically ReaderT) to hold a bunch of account authentication stuff. So functions like like this: getCustomer :: MonadIO m =&gt; CustomerId -&gt; StripeT m Customer I have my own (soon to be released) alternative to this library for which the same function has the type: getCustomer :: CustomerId -&gt; StripeReq Customer `StripeReq` is not a monad, it just a simple data type with a phantom type parameter: data StripeReq ret = StripeReq { srUrl :: String -- ^ request URI , srQueryString :: [(ByteString, ByteString)] -- ^ query string parameters , srMethod :: SMethod -- ^ request method (includes form-data for 'SPost' method) } deriving (Eq, Ord, Read, Show, Data, Typeable) In the original library, the `StripeT` forces your calls to be in a specific monad for some reason and ties you to a specific HTTP client library. And you still need to call some other function to actually perform the remote API call. In my library creating the request is a pure operation. To actually make the request you just call a function like: stripe :: ( MonadResource m , MonadBaseControl IO m , FromJSON a ) =&gt; ApiKey -- ^ Stripe 'ApiKey' -&gt; StripeReq a -- ^ request -&gt; Manager -- ^ conduit 'Manager' -&gt; m (Either StripeError a) But, that functionality is actually provided by a separate library. You install stripe-core to get the core functionality, and then a library like stripe-http-conduit to get the `stripe` function. Don't like conduit? It's easy to add support for a different http backend. Anyway, the point is -- the original library, IMO, reached for the monad transformer hammer way too early. It doesn't seem to really add anything aside from complexity. Are you really going to write your whole app in the `StripeT` monad.. and if so, why? Keeping the library 99% pure, and have just one function that involves IO and monads at all seems like a lot cleaner design that is easier to integrate into a wide variety of different applications. tl;dr -- avoid monad transformers until it is clear that you should obviously be using one. For example -- if you tried to write a parser, you would very quickly realize that you need something like a Monad or Applicative abstraction.
&gt; So, my advice there is usually just to write plain code using whatever libraries you need, and if it looks like you're repeating some pattern over and over then consider ways to abstract that out. I've been pondering this myself the last few days, and I'd like to go ahead and amplify this. Don't worry about leaping to the "Haskelly" solution at first. Start by just bashing something out that essentially works. Go nuts. Do it all in IO. Write partial functions. Copy and paste without fear. Use undefined to fill in gaps in the functions. Realize halfway through you're sort of doing it wrong, but _don't_ stop to correct it, just keep going straight for something that works for some task. Not even the whole task, just something. (Do avoid mutation, though, that will wreck up the rest of this process. Stay out of IORef, and for the moment I'd even suggest avoiding Control.Monad.State, although that's much less bad.) You will quite likely look at the resulting code, and wonder why you bothered doing it in Haskell. It looks like crap, was harder to write than whatever language you are comfortable in (for the compiler was probably still pretty cranky at you even so), and looks nothing like the beautiful code you see in blog posts. Don't sweat it right now. Make it compile and make it pass some basic test case that you can easily replicate, ideally in GHCi, as some sort of unit test if not. The key is to not stop here. First, even if this is a tiny little project of your own doing that will go nowhere, commit it to some source control. You need to be able roll back for what I'm about to say, and you're going to need frequent checkpoints. If nothing else, you can use `git init`. `git add $FILE`, `git commit`, and `git checkout $FILE` to roll back will be enough to get going, and for all I care call every commit "Checkpoint.", since you'll only ever need to roll back to the previous commit, most likely. Next step: Run hlint over your code. Do almost everything or everything it says. It's simple, but it's a great step, and already you'll start learning some things. Next, iterate roughly through the following things: * Micro-scale refactorings. Find anything you can that is the least bit repeated, even just twice, and figure out how to factor it out. Haskell's just about the best language there is at this sort of thing. If you've got a five parameter function that you often call with three of the same parameters, factor the similarities out. If you've got a function that you're continuously calling "flip" on, reverse the parameter order and remove the flip. Save after every thing you do, and see if it still compiles and passes the test. Commit frequently. Look at all your lambda functions; is there a way to write them without the lambda just using currying? (Sometimes they really are the best solution, but often they can be turned into usage of curried functions.) Did you write an explicit recursion? Can it be turned into a fold or something? You'll find these also feed on each other, which is why I suggest breaking the "Rule of Three" and refactoring even the second time you see some similarity; you'll often discover that what you thought you did only twice you actually did even more often, it's just that there were parameters in the way that prevented it from being obvious. * Pull out pure functionality: Anywhere you do IO, then do "something" to the result, pull the something into its own function. If you get a value from IO, then do something, then get another value from IO, then do something with that, then get another value from IO, and produce a final result, reorder it so it does all three IO operations in a row and passes it to a pure function. * Look carefully at your data types. Do you have some data type that contains a value that could easily be computed from the other values in the data type? Eliminate that field, replace it with a function. Do you have three data types with the same set of three fields in them? Pull that out into a new data declaration. * Now the mystical bit: Listen to your code. It's trying to tell you things. Do you have partial functions? Figure out how to unpartial them. Does it require you to rewrite half your code? As a learning exercise, go ahead and do it. It sucks at first, but it's the best way to learn how to do it right next time. You'll almost always find the result is surprisingly better. Use more Maybe. Use Data.Maybe's shortcuts and the monad/applicative interface. Do you have incomplete case statements? Look carefully at the missing clauses, and think about what it _means_ if you hit them. Perhaps the thing you are switching on is actually two bits of data you've jammed together accidentally. Look for bits of code where you have to usually call `f x y`, but sometimes have to call `f' x y z` in one case clause; is there a way you can unify those functions into one case overall? * Also, learn from your errors. Did you cross two Int types accidentally? Newtype yourself up some new types so you can't. Make your errors impossible, if you can. (This one does take some time to learn, I think.) * Run hlint again. It's easy. By the time you've passed through all of the above you've probably got some things you can write in point-free style now, and when you do that yourself it makes a _lot_ more sense than when you just read about it in a blog post. * Look at some of the functions you've factored out, and look at your most generic ones (type signatures with just `a`s and `b`s). Odds are good you've reimplemented something that already exists. If you've got some function with really generic signatures, pop them into Hoogle and see what pops out. Data.Maybe, Control.Monad, and Prelude are full of things you probably don't get until you need them. I stared at `Control.Monad.&gt;=&gt;` for a while without getting it until I finally ended up reimplementing it myself one day. Now I get it. Once that stabilizes, you can move into macro-scale refactorings. Why? Because already you've probably watched your code collapse quite significantly. Now you're trying to macro-scale refactor something that's already simpler than it was before. You've brushed away enough of the accident that the essence is that much clearer. Do a huge number of your functions take the same set of parameters? First, pull that out into a new data type, and then consider a reader monad. Do a huge number of your functions take an X and return a modified X? Consider state. For now, try to avoid "stacks" of monads. They're not bad, but you should use them because you find that it makes your code cleaner at the end of this process, not because you start with one. Having done that, take a pass back through the bulleted list. Finally, depending on your task, consider the use of some of the libraries you've read about on /r/haskell. Are continuously recomputing some values from old values? Consider lens. (And don't forget lenses are themselves first class values; a function can easily take "a thing to do" and "a lens to do it on". This is where it shines and this can once again collapse code in surprising ways!) Do you have a pipeline? Consider pipes. Etc. (This depends too much on exactly what you are doing to have general advice.) At the end of this process, step back and compare your final code to your first working model. Odds are your final code is really starting to resemble the Haskell code you've seen online! Odds are you've got something where you can clearly say "Here is the clear specification of the problem that can almost be read out loud as the solution, and here are the functions supporting the specification, and there's no confusion which is which." Odds are you've got something you can post to a blog yourself, and drive the next newbie batty because they don't know how to write code like that. It's just too much pressure to think you need to leap to pristine code without any intermediate steps. Don't be paralyzed by that. Maybe the experts can leap straight to the pristine answer, but then, they had to hone their skills somehow too. It would be interesting to hear if some of the experts still essentially work this way, or what kind of intermediate steps they pass through themselves. Haskell is the easiest language I know to do safe microrefactorings by hand in. Take full advantage of that. It gets easier as you go along. (That got a bit larger than I expected, but it seems like it covers some useful middle ground that is rarely talked about.)
No, both monoids and monads are generalized monoids, but in different (monoidal) categories. Monoids have kind * and monads have kind * -&gt; *, i.e. monoids are types and monads are type constructors, so both are at the type-level. Arrows, which have kind * -&gt; * -&gt; *, are also generalized monoids!
Wow, this and the previous answer are absolute gold - thank you!
&gt;"Popularity" does not mean efficiency nor robustness. Sure, Yesod is a good choice if you're confident in your abilities. But you'll have lots less contributors than if you used something like Rails, and judging by their site, they already seem to be looking for more contributors.
This is phenomenal. My first Haskell project has made some good strides lately, and I've been making similar "just get it working" design decisions. I sometimes wonder if cleaning it up later can erase any bad habits I may have fallen into. And thanks for the note on ReaderT. I tried to use it but found it so much easier to just stick all my stuff into an Environment record and pass that around where it needs to go explicitly.
Well, I expect to burn through a few more versions before we arrive at something we’re happy with. At least we reached the point of being usable, even if we’re not terribly user-friendly yet. És köszi, még él a remény. ;)
One of the arguments used is haskell's laziness. Though clojure is lazily evaluated and still makes heavy use of macroes. could anyone clarify? 
Came here to see this, was not disappointed.
Why do you need `unsafePerfromIO` to read a config file? I just read files in IO, usually main itself, and pass it out to whatever it needs it. Is that not suitable for what you usually do?
That you have negative karma from this point just goes to show the sad state of the world where everybody is looking for a golden hammer.
We're confident we can get it done; we'd just rather get it done faster, which we can do with a couple more people. Choice of framework had much to do with my own preferences, in terms of what I knew best and wanted to work in.
I agree with this. You'll never find the elegant way until you've tried all the ugly ways first. Edit: One of the biggest advantages of Haskell is that it is safe to make mistakes because the compiler is your safety net. Take advantage of this and make lots of mistakes!
Slightly off-topic but one of the things to keep in mind is that certain foreign calls (unsafe ones) can block the whole runtime system so you occasionally might run into trouble with something like e.g. using different MySQL transactions in different threads if the transactions block each other.
I'm asking what the strengths and weaknesses of each approach are. [This is the exact opposite of a golden hammer](http://24.media.tumblr.com/29be324f2c38a2a8ff3c4919414f4ddf/tumblr_miybgfwBzB1s6ezlmo1_400.gif).
You asked "How do you usually decide what to use?" He answered that question.
This is a great post. I'd just like to add that I like to use `pointfree` on any chunk of code I write using lambdas to see if it can make the code clearer. I don't have a very good intuition for when I can do those kinds of rewrites, but `pointfree` has had a good track record for me. Plus, it lets you hammer out crappy code without fear of having to do the whole refactoring yourself.
Indeed. And in fact, in my actual use of the library I created a stripe' wrapper function like this: stripe' :: (FromJSON a) =&gt; StripeReq a -- ^ request -&gt; StripeM (Either StripeError a) stripe' stripeReq = do mKey &lt;- query GetStripeSecretKey case mKey of Nothing -&gt; escape $ do internalServerError () template "Stripe Error" () &lt;p&gt;This operation can not be completed because the stripe key has not been configured yet.&lt;/p&gt; (Just key) -&gt; liftIO $ withManager $ \manager -&gt; stripe key stripeReq manager And then I can simply do: ePlans &lt;- stripe' $ getPlans Nothing Nothing The nice thing about `stripe'` is that I wrote it as a user and was able to integrate it into the existing monad I was already using, tie it into the error reporting system for my application, and pick the HTTP client library I am already using in my application. If I am extra paranoid I could log each request since I have a nice pure value representation of the request parameters. I could also add a similar wrapper around the `StripeT` variant.. and that's when I realize that the `StripeT` wrapper isn't really buying me anything. If I am feeling generous, I can also offer a `StripeT` monad transformer in my library -- but one people are not obliged to use. They would still need to prefix with requests with a function like: ePlans &lt;- stripeT $ getPlans Nothing Nothing But, I kind of like that, because it makes it easier to find where I am actually making stripe calls. That could be avoided by creating a `MonadStripe`class and use `Identity` for for the pure case. But I'm not really clear that is buying anything. Assembling a request is a pure operation -- the only IO part is the one function that actually makes the call. Disguising that fact behind a bunch of classes and monads and monad transformers does not seem like an obviously improvement to me. I used to love building monad transformer towers.. but not so much anymore. 
Thanks for pointing me to the library, even though is in its infancy I saw it's written by Ed, which is definitely a good sign :)
At KU, we use html-kure to generate our website. It might be useful to you. As the names implies, html-kure use kure for the traversal.
Just a couple quick comments. 1. You should create a .cabal file so that it can be built with cabal. (Additionally, this lets you specify what additional packages it depends on.) Just running "cabal init" (make sure you have cabal-install installed) is sufficient to do most of the work for you. 2. Your file names should match the names of the included modules exactly, including case.
Nested if-else clauses are generally not used much in Haskell. Guards and pattern matching are generally better practice and produce cleaner code. You also have a lot of unnecessary actions wrapped in IO. It's good practice to separate the action from the IO. The gameAction function is a good example. Given the same game state an action will always have the same result. When writing something like this that has a changing state I personally like the ST monad but it's also a slightly advanced subject. It's also probably slightly over kill for something this small. Algebraic data types would be good for passing information from IO functions to internal functions in a type safe manner. EDIT: You should probably also include a LICENSE file
I think coroutine is better than ST monad for this kind of program (and in general). Separation of pure operation from IO monad often results in an inversion of control and so to recover original line of business logic, one often needs reinversion of control using coroutine monad (or free monad as you like). There are lots of explanation about this in the literature and blogsphere, but I found that the following simplest self-contained article: http://random.axman6.com/blog/?p=231 anyway, this is rather advanced topic for the haskell beginners like OP. I personally think discouraging to use IO monad too much is not very helpful for beginners who do not understand the alternative at that moment of implementation. Starting with IO monad is just fine. At least, to have IO monad tag is still better than without any sign of impure IO operation in other language. And once done as a prototype implementation, start refactoring them into smaller codes. It is very easy to identify who is pure and who is impure in terms of type signature. Keep reusing your code, and then you will find the best of your library API as a result of evolutionary procedure. 
I made some quick changes to `Map.hs` and `Player.hs` without changing the overall structure too drastically and [saved it as a gist here](https://gist.github.com/isomorphism/5561968). More detailed remarks are scattered throughout in comments, preceded by a tilde. The biggest overall issues with the code are: - Using magic numbers to represent results or game state rather than a meaningful data type. This is a very, very bad practice in any language. - Game logic is scattered across many functions and mixed together with I/O. Stick to pure functions that either 1) Report the status of something in the game 2) Update the game state in a single, specific way. The checks for the wumpus's location and the arrow shooting are the biggest offenders here. - Not using higher-order constructs, like passing functions or `IO` actions as arguments, where it would simplify things. This results in a lot of code duplication and makes the structural issues already mentioned even worse. I didn't try to correct this in the gist because it would have been so drastic as to make commenting on the other issues impossible.
Your title is incredibly irritating, not the least because it doesn't make complete sense.
There's a long list of problems with your code, so I rewrote a lot of it as an example. I'm just going to use this comment as a little stream of consciousness scratch pad while I rework your code. You can find the final result here: http://hpaste.org/87785 Ok, so I'm changing how you represent the map. Rather than store it as a graph, I'm just using a lazy and cyclic data structure. It's much more elegant and efficient to traverse. This simplifies the `map.hs` code a lot. There's another advantage of doing it this way, which is that you don't need to pass around a `Map` variable any more. You can just store the map directly within the player and the wumpus location fields and it will already intrinsically have all the information you need to traverse it. Because of laziness, there won't be any wasteful computation or excessive storage. Your `getDirection` function makes a lot of mistakes. For example, rather than input the direction as a `String`, you should be using an algebraic data type like: data Direction = Back | ToLeft | ToRight This is not only safer (since you don't need to have a fallback case like you do with strings), but it's also more efficient to compare. Another problem is the fact that you call the same `getAdjRooms` function multiple times. Just use a let binding to store its result once: let [room1, room2, room3] = getAdjRooms p m That leads to another problem: You're storing the results as a list. Better to return a tuple so you can statically guarantee the number of elements returned and not have to use partial functions like `head` or `(!!)`: let (room1, room2, room3) = getAdjRooms p m However, because of the nice cyclic data structure you don't even really need this helper `getAdjRooms` function since the cyclic data structure just directly stores the adjacent rooms as fields: let here = location player room1 = clockwise here room2 = counterclockwise here room3 = across here Also, your `getDirection` function uses `IO` for no reason. It's pure: just get rid of all the `return`s. Also, you should really avoid using integers as enums. Haskell has algebraic data types, which you can use to restrict the range of possible values and give them more descriptive names. For example, in `checkForWumpus`, this would make it much easier to understand what each return value means. In fact, a good rule of thumb is to use `String`s and `Int`s as little as possible. `Int`s are still useful for counting and `String`s are useful for communicating with the user, but try not to use them as representations for what could be more strongly typed values. Also, whenever you modify just one field of a data type, you can use record syntax. For example, if you want to change just the `location` field of a `Player`, you can write: p { location = newLoc } Another bad habit you have is mixing the logic with the user input, like in the `shootArrow` and `movePlayer` functions. You should be requesting and verifying the input in a single self-contained `IO` function, and then embed the game logic in a pure function, otherwise you unnecessarily contaminate your game logic with `IO`. Mixing `IO` with game logic makes it much more difficult to reason about what your game actually does and it also makes it harder to change the source of input later on (such as if you want to make this a GUI game or even a completely pure game with a fixed input). Also, you should use variable names that have more than one letter for readability. You should name module files the same as their module names so that `ghci` can load them correctly for interactive testing. Also, Haskell lets you use string continuations so you can split long strings onto multiple lines. Just use `\` to end the first line and `\` to begin the next line. Also, even when testing strings for equality (which you really shouldn't do and you should instead parse into an Enum), it's much easier to write: case str of "move" -&gt; ... "shoot" -&gt; ... ... than to write multiply nested `if` statements. Haskell lets you pattern match on `String`s as a nice convenience. Also, even for multiply nested ifs, you can use use this trick to emulate guard syntax in the middle of a function: case () of _ | cond1 = ... | cond2 = ... ... and newer versions of `ghc` have multiway `if` which means you don't even need this hack any more. Also, you check for the wumpus encountered the player twice per turn. You can simplify your code a lot just by checking for proximity at the beginning of the turn and collision at the end of the turn. That's all I did for now. A more full rewrite into something a bit more professional would include the following extra features: * `StateT` * Lenses * More vigorous separation of `IO` and game logic
The `on` function makes comparisons a little cleaner. instance Ord Room where compare = compare `on` roomID or instance Ord Room where compare = comparing roomID
Because nothing ~~solves~~causes Real Problems like accidentally implementing a scheduler
So this is basically ordered container for `ByteString`, but for other types (that can be serialized to `ByteString` and I guess even for `Text`) it would most likely be used only as an unordered container (it is unlikely that the ordering given by the serialization makes much sense for the user).
Well, it said "We plan to put out a new major release 7.8.1 soon after ICFP 2013" which would put it at end of september or later.
Just a side note: &gt; There's a long list of problems with your code, If I read that after working on something for a long time I'd probably feel _slightly_ crushed. :-P
I actually think Project Euler is a great way to learn some Haskell, if you enjoy that kind of problem. Keep in mind, though, that these are primarily *math* problems. Generally, if your code is taking much too long to run, you probably should work harder on the math, not the programming. That attitude will also protect you from falling into the bad habit of premature optimization. Learn how to write nice code first. If you really suspect that your attempted solution is failing because of bad Haskell and not incomplete math, then yes, do try to solve it using the same algorithm in a different language. But at this point, don't worry about an order of magnitude difference in performance. Just see whether the algorithm solves the problem at all, within some reasonable amount of waiting time.
Ok, that does sound tempting. I'm curious about trying it out with my own library that uses `ReaderT` heavily. I use `ReaderT` to thread a PostgreSQL connection handle around 50 (or so) custom types of query, so I'm wondering if it might end being more burden. Only way is to try it and see though, I guess. Can we see the code for your Stripe library anyway? I'm curious to see it all in the context of the rest of the library.
&gt;Also, Haskell lets you use string continuations so you can split long strings onto multiple lines. Just use \ to end the first line and \ to begin the next line. String continuations will break if you use CPP, rather stick to ++.
Yeah, this is exactly equivalent. In fact, you can basically show how the starting room + pure function that generates new rooms on the fly is the `codata` encoding of the `Room` type.
Similarly, instance Eq Room where (==) = (==) `on` roomID
&gt; Also, even for multiply nested ifs, you can use use this trick to emulate guard syntax in the middle of a function: It is also worth mentioning that GHC supports ifs with multiple branches. (The `MultiWayIf` extension which basically replaces your `case v of _` with just `if`.)
It's a shame that nobody gave you feedback! I don't know Template Haskell that well, but your coding style is definitely clean and idiomatic.
Personally, I prefer this style: longString = concat [ "Lorem ipsum dolor sit amet, consectetur " , "adipisicing elit, sed do eiusmod tempor incididunt " , "ut labore et dolore magna aliqua." ] Or the same using `unwords`/`unlines` when appropriate.
Really, the only things I would go so far as to say is *wrong* with his code is the use of `Int` magic numbers and tuples of low semantic content types, instead of enum types and more structured game logic updates. That has the potential to cause serious issues immediately even in simple programs. The rest ranges from "would make things harder in the long term in real code" to just "not idiomatic in Haskell".
We want to encourage people to prefer FLOSS, so pointing to gitorious is part of that. Re: Tickets, we struggled to find a good option, so we originally went with GitHub (Gitorious has no ticket system, and we weren't happy with other options). Our plan is to transfer to stictly using an in-house (thus FLOSS of course) ticketing system once we have it working well enough. Our own in-house ticketing will fit nicely with the rest of the system because it will be part of community engagement and the connection of tickets to patrons. For better or worse, we are wanting to be welcoming first to those who are most dedicated to FLOSS and then build up from there rather than aim for the general public right away. If you aren't a hard-liner and don't care that GitHub is proprietary, you are still welcome, but know that you are joining a community that is rooted in caring about these things.
Yeah, but I still think I could have phrased that more nicely. I was just careless with my words.
Did you fix the black on black text (in a regular old terminal window with a black background) in the default theme too? That was always one of the things about it that gave me an instant bad impression from the start when I tried it in the past.
A good paper, but why post it here 13 years after it was written?
&gt;For better or worse, we are wanting to be welcoming first to those who are most dedicated to FLOSS and then build up from there rather than aim for the general public right away. If you aren't a hard-liner and don't care that GitHub is proprietary, you are still welcome, but know that you are joining a community that is rooted in caring about these things. That's bad, actually. The problem you're trying to solve, or the solution to it, all have no intimate connection to FOSS. By rigidly pushing your view where it doesn't matter, you as a project, are being rude. There is no reason to outlaw proprietary projects — the financing model doesn't match anyway, so they won't be very popular. And, if they will, it won't hurt, only lead to more adoption by FOSS projects, which is good.
Because not everyone here was reading Haskell papers 13 years ago?
I see your point, but I'm happy that OP posted this link, because I hadn't seen it before. I'm new to the field of automated reasoning and even more recently discovered Haskell. This paper is a useful insight into how Haskell works, and more importantly why it was designed the way it was. In other words, I think 13 years is long enough for a repost. ;)
I'll try. I don't have an account, so hopefully the account creation information is still correct. I'll see about cleaning it up, too, since this was just a bashed out comment on reddit.
I'm excited that you're looking at integration with hledger, and to learn about Freeagent (coincidentally I just started evaluating Xero, must check out Freeagent too). Drop in on #ledger if I can help at all. After you get it working it might be fun to experiment with integrating your library with hledger, under Hledger.Read or as an add-on executable..
Yeah, and why even talk about things like [SICP](http://mitpress.mit.edu/sicp/full-text/book/book.html), [Theorems for Free](http://ttic.uchicago.edu/~dreyer/course/papers/wadler.pdf), [Why Functional Programming Matters](http://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf), ...? All written more than 20 years ago!
Me too, but the long, detailed list of constructive advice certainly makes up for that. 
(augustss was typing things more complicated than Haskell in Haskell long before this paper was written, so he gets to remind us about it every now and then :-P)
I don't care if he's Alonzo Church resurrected. Comments like that are useless and annoying, and actually incredibly counterproductive, since they encourage young programmers to read "new" things like "node.js package management explained" instead of classic, and obviously much more profound literature.
'why' is always a good question though :) I am also interested why it was posted right now, even if the answer is 'just'.
Do you have a more recent paper that discusses the same topics? Why not post a link!
And it contained useful information too. I didn't have to click on the link to recognize that it doesn't have anything new.
I'm willing to bet you wouldn't be making these comments if the commentor wasn't augustss. I'm sorry, but somebody screaming "OLD!!!" does not qualify as useful.
To clarify my json -&gt; haskell types comment: I meant that I don't really want to type out the data type and all the fields mechanically I can generate it from the json snippets that exist in the documentation. I intend to use the Data.Typeable method for getting the encode/decode functionality anyway and I don't need the types to have different field names. Does that sound a bit more reasonable?
I recently read several papers about typing/type checking/... (the one this thread is about was one of them). I also liked http://research.microsoft.com/en-us/um/people/simonpj/papers/constraints/jfp-outsidein.pdf
I sympathize with your point (though it is not at all obvious to me that augustss's question was intended to be rhetorical), but on the other hand hyperbolizing what augustss wrote as screaming "OLD!!!" isn't exactly going to help you gain the moral high ground. :-)
Fanboy levels: Amazing.
Fanboy? That's hilarious; I don't even know who augustss is. How could I be a fanboy? :-) Perhaps you would be kind enough to explain to me why he is apparently so famous around here?
Two of the three reasons are not actually reasons. 1. Doesn't matter much where the exception is raised. 2. This is a general phenomenon with sharing and doesn't have anything to do with laziness or IO, except that people who are familiar with lazy evaluation might expect this piece of code to run in constant space. For everyone programming in a strict language, this is clearly nonsense. Also note that using a streaming library does not automatically avoid 2. It's perfectly possible to accidentally keep around the whole file contents.
Giving "seemingly random output" doesn't sound like a very good test. Could you give instructions how to determine which entropy source is being used?
As a lightweight with Haskell, providing examples as well as explanations as to why using the suggested libraries would be better would be more beneficial to us rather than just saying "use them". 
On OS X it is clear that it is not building in such a way as to give `CryptHandle` the `UseRdRand` constructor. Would it work anyway? In newtype CryptHandle = CH Word32 #ifdef arch_x86_64 | UseRdRand #endif it won't be a `newtype` if it gets `UseRdRand` as a second constructor.
I vote for PNG, BMP, JPG, and GIF support as the most important missing feature. That said, most image file formats are actually just containers which can used for a variety of image encodings. Support for the basic containers plus a few of the simplest and most common encodings would be a great start.
You're just jealous because you weren't born yet when the paper was first published.
He is a top computer scientist who was on the committee that originally designed the Haskell language.
That doesn't make any sense.
Never say never unless you test it. What you found was under the windows x86_64 section - so you wouldn't compile that code anyway. Thanks though and fixed!
Thanks! In light of codensity's bug report I'm confused how this worked. Are you running 32 bit GHC?
It does matter where the exception is raised. You can't reasonably `catch` exceptions when using lazy `IO` because they can be thrown in the middle of pure code. I agree with point 2, though. The streaming libraries only protect you against this solely by virtue of making it awkward to traverse the stream two separate times.
(and wrote the first Haskell compiler!)
I'm pretty sure 3. actually also only opens one file handle at a time! In other words, the only _actual_ problem evident in this post is a lack of ability to reason about lazy IO, as witnessed by it being wrong in all three examples.
The simplest explanation is that lazy `IO` makes it very difficult to reason about when `IO` actions occur. Lazy `IO` does not even necessarily preserve their order. Normally, when you use ordinary non-lazy `IO`, you have a nice and simple guarantee: If you sequence two `IO` actions, the effects of the first action occur before the second action. Lazy `IO` eliminates that simple guarantee. The effects could occur in the middle of pure code, occur completely out of order, or not occur at all. Using a streaming library solves this problem because you can reason about when effects occur and you prevent effects from occuring in pure code segments.
Very cool!
Is it unreasonable to suggest some kind of uniqueness type system for a future iteration of the language? Haskell seems to have a labyrinthine RTS but in principle it could be done. 
He didn't say "hard", he said "reasonable" ;). Now your exception handling code has to follow the data instead of the operation that throws the exception, that doesn't sound very reasonable.
But the operation that throws the exception is the compound operation of reading the file, calculating the length, then printing the length! That's because `readFile` just opens the file for reading, and conceptually we're consuming it incrementally as we're calculating `length`. So if we wrote the longhand strict way to get the same performance, we'd do the same thing and wrap the exception handling code around the whole sucker _anyway_. The confusion is people think of `readFile` as "gimme the whole file" not "make this file available for reading from". If you're used to thinking lazily, the introduction of IO effects (unless you have overlapping reads and writes) is really no weirder than working with any other lazy object.
This is a bug in the binding. If you use `-threaded`, FFI calls should not block the whole RTS. If they do, then an author incorrectly marked a blocking FFI call `unsafe`. This [appears to be the case](https://github.com/bos/mysql/issues/2#issuecomment-17825437) with the `mysql` package.
I highly recommend reading these slides by Oleg: http://okmij.org/ftp/Haskell/Iteratee/IterateeIO-talk-notes.pdf They are his old annotated talk notes and they give a really thorough description of real problems that lazy `IO` causes with lots of examples. Edit: Here's a select quote from the talk: &gt; I can talk a lot how disturbingly, distressingly wrong lazy IO is theoretically, how it breaks all equational reasoning. Lazy IO entails either incorrect results or poor optimizations. But I won’t talk about theory. I stay on practical issues like resource management. We don’t know when a handle will be closed and the corresponding ﬁle descriptor, locks and other resources are disposed. We don’t know exactly when and in which part of the code the lazy stream is fully read: one can’t easily predict the evaluation order in a non-strict language. If the stream is not fully read, we have to rely on unreliable ﬁnalizers to close the handle. Running out of ﬁle handles or database connections is the routine problem with Lazy IO. Lazy IO makes error reporting impossible: any IO error counts as mere EOF. It becomes worse when we read from sockets or pipes. We have to be careful orchestrating reading and writing blocks to maintain handshaking and avoid deadlocks. We have to be careful to drain the pipe even if the processing ﬁnished before all input is consumed. Such precision of IO actions is impossible with lazy IO. It is not possible to mix Lazy IO with IO control, necessary in processing several HTTP requests on the same incoming connection, with select in-between. I have personally encountered all these problems. Leaking resources is an especially egregious and persistent problem. All the above problems frequently come up on Haskell mailing lists.
I believe the correct response to such a request would be, "patches welcome," which is the nice way of saying, "sounds like a lot of work; I hope someone else does it."
It's also wrong about where the file not found exception will be raised in #1 (readFile fails immediately if the file does not exist).
Don't post it to me, post it to the subreddit!
Don't post it to me, post it to the subreddit!
I agree with everything jerf said, and I'd like to add one more thing. When I was a newbie, I would try to write things in point-free style WAY more than I should have. I wasted untold hours because of that. Sometimes point-free is really hard to write--hard enough that it's effectively impossible for a human. My problem when I was starting out was that I couldn't reliably see when a nice concise point-free formulation would exist and when it wouldn't. In fact, in a lot of cases I still can't. So don't try. Just get it working in pointful style (with lambdas and explicit parameters) first, and then it will be easier to see where some things can be done point-free. This advice is actually a special case of jerf's initial advice to just get it working. But it seems like the lure of point-free in Haskell is strong enough to warrant it being mentioned specifically.
I do think using memoized streams as the data type for streaming I/O is rather error prone. You get zero help from the type system if you happen to accidentally leave a reference to the head of the stream around, turning what should be a constant space operation into one that leaks memory. And memoization and lazy I/O go hand in hand - a lazy I/O function cannot return an unmemoized stream, something like: data Stream a = forall s . Stream s (s -&gt; Maybe (a, s)) since if it did, the side effects really do become observable, even to pure code.
This is an absolutely horrible way to try to understand continuations. :(
&gt; I agree with point 2, though. The streaming libraries only protect you against this solely by virtue of making it awkward to traverse the stream two separate times. No, I do see a major difference - a list is memoized, which makes it prone to memory leaks if sharing is not controlled (which the type system provides no help for!). A stream in conduit/pipes/io-stream is not memoized.
Did anybody ever librify this into a proper package?
Care to elaborate?
Whoops! Missed that. I just "read in" a more logical error that might occur in the midst of the read of a file.
but its easy to accidentally rememo a stream, or effectively do so with a lazy fold on it or the like.
This is the only sentence I've found in the article that deals with the general topic of continuations: &gt; In essence, a continuation is a function which represents the next block of code to be executed. Everything else is about using a continuations package in haskell. Perhaps you could expand by on the general topic of continuations before leaping into haskell code. Pointing out that even the lowly subroutine call is using continuations (I.e. the return address on the stack is the "function" the subroutine "calls" to execute the rest of the program) would be a good example. That callwithCC is, in effect, just a way to give a name to (or create a closure for) the return address.
It's just a complicated way of grasping what continuations are doing. It's too Haskell-code-oriented. It's good to have Haskell code, but teaching through it builds no intuitions. Good tutorials build intuitions via good conceptual tools.
The part of concepts which is identical to type classes is that in both cases, you define a bunch of functions whose signatures include one or more abstract types (typename T, etc): concept Eq&lt;typename T&gt; { bool operator==(T, T); } class Eq a where (==) :: a -&gt; a -&gt; Bool All the examples I see on the wikipedia page use &lt;typename T&gt; template parameters. I know that C++ templates also allows non-type template parameters, such as &lt;int n&gt;; those are not supported in Haskell (but they are supported in Agda [1]). Haskell does, however, support the equivalent of template template parameters, and in fact they are quite common: class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b concept Functor&lt;template&lt;typename&gt; F&gt; { template &lt;typename A, typename B&gt; F&lt;B&gt; fmap(std::function&lt;B(A)&gt;, F&lt;A&gt;); } I also notice that in addition to functions, concepts allow you to define typenames. Those are not typical in Haskell, but are possible using ghc's TypeFamilies extension. concept Iterator&lt;typename I&gt; { typename value_type; value_type operator*(I); I operator++(I); } {-# OPTIONS -XTypeFamilies #-} class Iterator i where type ValueType :: * dereference :: i -&gt; ValueType next :: i -&gt; i C++ Axioms are not supported by Haskell (but they are supported in Agda [1]). Type classes define laws which implementations should satisfy, but the language doesn't enforce them. [1] Agda is a Haskell-like language which does allow you to specify properties like associativity (and much more), requiring implementers to prove that their implementations satisfy those properties. It does not support type classes per-se, but its type system is expressive enough that you can easily specify equivalent type restrictions. axiom Associativity(Op op, T x, T y, T z) { op(x, op(y, z)) == op(op(x, y), z); } Associative : (T : Set) → (_op_ : T → T → T) → Set₁ Associative T _op_ = ∀ x y z → (x op y) op z ≡ x op (y op z) 
Of course, you'd need some exception handling after taking the file with the STM action and before being able to startDownload it. Nice post!
This was my first talk at the Brisbane Functional Programming Group. I'm very new to Coq, so the Coq code itself might be pretty terrible. The talk was less about Coq and more about the fact that most of the tools that let you reason about correctness aren't scary. This follows on from Nick's Introduction to Laws talk, which is [here](http://vimeo.com/58236838). I figured I'd post this here a) in case anyone found it interesting and b) so that people could blast me with masses of feedback from the vast anonymous reaches of the internet. I'm very keen to skill up on presenting stuff like this, and am thinking about doing a follow up on laziness, strictness and evaluation, so more feedback will *probably* be good for me...
you can only follow bottoms of types where you have access to the representation. Given abstract types this is not possible (you can only follow one bottom).
&gt; In order to see them, let's desugar that do-notation. You can click on the Run button to convince yourself that this code has the same behavior: This part suggests that there's a meaning of `do`-notation independent of its desugaring. There isn't! What's verified by the Run button are (1) the manual desugaring is correct, and (2) the webserver-based compiler is correct for both snippets. The crux is, what exactly is this click supposed to convince us of? Surely not the trivial equivalence between something in `do`-notation and its desugared form?
Yeah I've looked at Clean. Haskell has many things I love, I don't want to get rid of them just for one thing. Hrm. 
That's true to a degree. A well designed abstract type has a semantics that is exposed to the reader through documentation. For example, `Map` from `containers` is abstract, by grasping the API it is possible to do the relevant strictness analysis: You mostly care about partially defined `Key`s and `Value`s. There are still `Map` values that are partially defined which you can construct (think of `union`ing partially defined Maps) but cannot reason about, but these probably don't matter for analyzing lazy IO. The degree to which you can reason about partially defined values of given abstract types is one measure of the quality of an API.
For educational purposes I think it's helpful to show both forms since the de-sugared form clearly shows how the "rest of the program" can be a function in haskell. 
&gt; Surely not the trivial equivalence between something in do-notation and its desugared form? What is trivial for you may not be trivial for others.
you missed that the third example isn't a problem at all. you can get a problem if you are perverse and mapM readfile over the file names _then_ mapM print . length over the resultant strings, but to me the problem then seems obvious. a slightly trickier version is also given by philipjf above.
On OS X 10.8.3 it fails with Resolving dependencies... Configuring entropy-0.2.2... Building entropy-0.2.2... Preprocessing library entropy-0.2.2... /var/folders/_7/0pvhd37d2fj9xc48zt494_xm0000gn/T/ghc7102_0/ghc7102_0.s:32:0: no such instruction: `rdrand %rax' /var/folders/_7/0pvhd37d2fj9xc48zt494_xm0000gn/T/ghc7102_0/ghc7102_0.s:57:0: no such instruction: `rdrand %r8' /var/folders/_7/0pvhd37d2fj9xc48zt494_xm0000gn/T/ghc7102_0/ghc7102_0.s:93:0: no such instruction: `rdrand %r9' /var/folders/_7/0pvhd37d2fj9xc48zt494_xm0000gn/T/ghc7102_0/ghc7102_0.s:109:0: no such instruction: `rdrand %rcx'
Both this and the last post on continuations made a sudden leap that was poorly explained and as a result it has been difficult for me to form intuitions around these things. In this case it's the block immediately following "We can get even trickier when there is logic within the callCC block:" import Control.Monad.Trans.Class import Control.Monad.Trans.Cont main = flip runContT return $ do lift $ putStrLn "alpha" num &lt;- callCC $ \k -&gt; do if 42 == 7 * 6 then k 42 else lift $ putStrLn "uh oh..." return 43 lift $ putStrLn "beta" -- k lift $ putStrLn "gamma" -- j lift $ print num -- l Initially I was thinking that k should represent the computation of the last three lines, but now I realize the k actually includes both the binding to num AND the last three lines. So I guess when you call k with 42, what that's really saying is, "Make num 42 then run the rest of this computation."
But this is a case where we should use `withFile` composed with `hGetContents` instead of `readFile`. I'll grant that even with that we need to take care to ensure we don't close the file before evaluating the length. I agree there are 'gotchas', but they're not hard, and you just have to learn them once. I've run into actual use cases where iteratees are the absolutely most natural thing. But they're rare, and any old hand-rolled formulation will do. On the other hand, for day to day stuff, lazy IO is fine, and the biggest confusion seems to come from people explaining, poorly, what others have _told_ them the problems are.
The original concepts proposal was scrapped for being too complicated. Bjarne really wants concepts, but desided his original design was not workable. Concepts are an effort to bring strong types to the relm of meta programming in C++. Typeclasses were one of the main motivations. Some form of concepts will probably make it into C++, but they are likely to differ considerably from Typeclasses: * Not for overloading. Concepts are unlikely to provide "less ad hoc" ad hoc overloading that typeclasses were defined for. C++ will most likely preserve the existing name resolution mechanisms. Concepts are just a type system on top of an existing untyped template language. * Non parametric. C++ templates will continue to support non parametric specialization. * 100% compile time resolution. Unlike Haskell, Concepts will not be resolvable at runtime--so, no local evidence! And, no dictionary passing interpretation or implementation. * Concepts may end up being implicit. They may end up having no "instance" declarations. Axioms in the C++ proposal are not checked, and as such are much like the "laws" associated with typeclasses plus the possibility of some associated rewrite rules. 
&gt; people who are familiar with lazy evaluation might expect this piece of code to run in constant time. I don't get it. Perhaps you mean constant space instead?
C++ doesn't require you to prove axioms, so they're more like rewrite rules, which can be given to GHC, at least.
Ah. Well, instead, I just thought I was stupid :) Thank you for trying to explain continuations. I feel like continuations are probably the most important thing in computer languages but also one of the hardest things to understand.
Last time I checked, cabal had no method of automated package removal. I don't know if that's reason enough, but it's something.
It is a pain but I don't think it's a compelling reason to use apt over cabal.
Advantages of using apt/yum/etc: * non-Haskell dependencies will be automatically installed as required * people new to Haskell already know how to use it * installing a package never breaks a previously-installed package * packages can be uninstalled Advantages of using cabal: * access to all packages on Hackage * access to more recent versions of packages Speaking for myself: when I started using Haskell I installed Haskell packages using aptitude, and only switched to using cabal when I really wanted to use something I couldn't get via aptitude.
Well, it has already been posted: http://www.reddit.com/r/haskell/comments/7r4hx/typing_haskell_in_haskell_we_present_a_haskell/ 
About the libraries ecosystems: **conduit** has currently the biggest ecosystem, with many HTTP related libraries available; **io-streams** is quite recent so its ecosystem is just growing, **pipes** has been moving quite fast lately and its ecosystem just growing, too. **enumerator** has seen a decrease in usage since the other libraries have been gaining adoption. I can tell a bit more about **pipes** since I'm involved in its development. There's a handy “[Pipes homepage](http://www.haskell.org/haskellwiki/Pipes)” at the Haskell wiki which can point you to some pipes related resources and a general overview of what you can expect from **pipes**, and also there is Tekmo's blog [Haskell for All](http://www.haskellforall.com), which is full of pipes (and non pipes!) related wisdom and examples. If you want to write an HTTP server comfortably you'll need, at least, TCP networking support and HTTP parsing support. **pipes-network** and **pipes-attoparsec** can help you there, though be aware that **pipes-attoparsec** is currently undergoing a big API change so that interleaved parsing, delimited parsing, and leftover management can be supported, by relying on the upcoming **pipes-parse** library. You will certainly want the interleaved parsing support, since it enables, for example, parsing only parts of the stream and doing something else with the parts you don't want to parse. There's also **pipes-zlib** available, which you'll need sometime, and I expect to release **pipes-network-tls** this week, in case you need TLS support in your TCP connections. Also, Tekmo is currently working on **pipes-safe**, simplifying its API a bit, and upgrading it so that both safe and prompt finalization can be happily supported. I know Jeremy Shaw started working in a pipes based HTTP server for Happstack, I guess is [this one](http://hub.darcs.net/stepcut/hyperdrive). I know [I started working on one](https://github.com/k0001/seldom) too, but currently it's almost non-existent and in stand by, until **pipes-parse** and the upgraded **pipes-attoparsec** are published. I plan to continue contributing to developing a friendlier pipes ecosystem for client side and server side HTTP, so no worries there :)
Four years ago. Are you kidding me? This supports augustss' statement that "this is 13 years old. I've already seen it"? It's so ridiculously obvious that you and the other commentors in this thread are molding every little thing into something in favor of augustss' statement, because of his status in the community. This despite the fact that he was just being an asshole grumbling "OLD!"
Haha! I accidentally hid a mistake in plain view. I mean space, indeed. Fixed, thanks!
I had the same problem. I suppose in the end I still don't understand what the dependency graph between all of these functions looks like. The `k` in `callCC $ \k -&gt; ... ` gets bound to the function tagged with `-- k`, but *how* does that happen? Under any normal circumstances that I can think of, you couldn't bind a name like that. Also what eventually happens with `return 43`? It appears to be called after `k`, but does that mean `main :: IO Int` here?
I see your point.
Are there slides available somewhere? I generally don't watch videos.
And here http://www.reddit.com/r/programming/comments/6eiox/typing_haskell_in_haskell/ But seriously, I'm not molding anything into anything. Read the top lines of http://web.cecs.pdx.edu/~mpj/thih/ It says: "Last updated: Thanksgiving day, November 23, 2000". My point was that according to http://www.reddit.com/wiki/reddiquette you should avoid posting duplicates. Yes, I agree that just because something is "old" it doesn't become redundant, but we should perhaps not keep posting it again and again to reddit?
Sure, I agree with that. But that's not what has happened, IMO. It is fine to post something years after the fact, or in different subreddits. Lots of people who haven't seen it before will see it. You can keep going down this road until everything that is posted to reddit is "OLD" in some way, but the take-away should be that if it is genuinely interesting to a large group of people, it is worth posting. Typing Haskell in Haskell is, regardless of when it was written. It doesn't *have* to be news. (Even Hacker News has this philosophy, despite having "News" in the title, and you must agree it makes sense to post something that few people have seen, which is old, but also very good, compared to posting something lackluster or mediocre that fails to explain something similar, just because it was written last week.)
Enumerator is long dead. Conduit is the most popular, and already has a bunch of fast http-servers written with it — warp and mighttpd2. Pipes documentation is excellent, and the library itself is simpler. If you're new to iteratees, I'd suggest to learn with pipes then switch to conduit.
Is there something wrong with Markdown? With a few minor extensions, Markdown is easily superior to troff (the stuff man pages are made of), and vastly superior to Boy. Take vanilla Markdown, make the following changes, build a reader, build a troff-to-extended-Markdown converter, and you're done: * Header blocks (a bit like Pandoc's title blocks... but more Markdown-ish) could carry some useful metadata. * Definition lists are pretty much mandatory in manual pages, when explaining what command line flags do. * Footnotes may be used to improve the flow of a manual page. * Pipe tables. Could be pretty useful, but not necessary. * Drop support for manual line breaks. In Markdown, they're made by appending a few spaces to the end of the line. Invisible markup is bad markup. I've seen better alternatives as well (pipe quotes, for instance). * Drop inline HTML support while you're at it.
Except it doesn't use them for rewriting either. AFAIK it doesn't use them for *anything* by default, they're for documentation and the benefit of external tools, e.g. static analyzers. 
There are slides [here](https://github.com/dalaing/bfpg-2013-03/blob/master/latex/slides.pdf), although I switched to CoqIDE for the Coq part of the talk.
Why is it that `enumerator` died? Was it due to API complexity? As a second unrelated question, why do you suggestion people later progress to `conduit`?
More fun facts: HBC (the first Haskell compiler) had a REPL long before GHC, and was written in ML. 
Thanks! I had no idea this existed. 
I think Alonzo Church ressurected would have a lot of catching up to do.
Touché.
Thanks! A few minor remarks skimming through the PDF, in case you want to improve/reuse the presentation. I think your animation for the sum does not work well -- it is certainly better with the audio on, with you explaining the coloration, but I think making animations easier to read offline is a beneficial constraint. My main problem with it is that it is rewriting in place, erasing the initial operands as you go, which mean that people have to use their memory to see the whole picture. It would be better, I think, to add equations line after line (you have space left anyway), something like this, progressively appearing: S(S(O)) + S(S(S(O))) 2+3 -- color to highlight the outer left S before the transition, with (2) temporarily turned into (1+1) = S( S(O) + S(S(S(O))) ) = 1+(1+3) -- color again = S(S( O + S(S(S(O))) )) = 1+1+(0+3) -- color the "0 + " part = S(S( S(S(S(O))) )) = 1+1+3 = S(S(S(S(S(O))))) = 5 (The next animation for the "proof on the right" already works well.) You should have a page count; people may have a question/remark on a specific slide and what an adress to refer to it in later discussion. Plus it's less frightening than the "210 slides" the PDF reader shows. In the part of list as a monoid, I found it strange that the local Goal was at the top, with the definition of concatenation under it, while the concatenation was the same code for all different goals you want to prove. I usually prefer to have the upper part of the slide be the most slide (so that people know where to look for changes). I have seen a presentation about [a similar work](http://www21.in.tum.de/~huffman/icfp2012.html) by Brian Huffman, checking typeclass laws in a theorem prover, done with Isabelle rather than Coq (jumping through hoops to support higher-order types because Isabelle isn't based on dependent type theory); it was relatively advanced (and had interesting remarks about which laws are broken by lazyness); maybe you could mention it. 
Yes, both `enumerator` and `iteratee` died mainly for two reasons: * Only sinks are monadic (making sources and transformations difficult to write) * Their behavior is difficult to reason about Generally `pipes` is the most elegant library with the best documentation and is a super-set of all other streaming libraries, but `conduit` has a **MUCH** better ecosystem (although I'm hard at work on the `pipes` ecosystem). Since the two libraries have a reasonably similar API people train on `pipes` and then get stuff done with `conduit` and I fully endorse that until the `pipes` ecosystem matures.
Don't forget the [`pipes` tutorial](http://hackage.haskell.org/packages/archive/pipes/3.3.0/doc/html/Control-Proxy-Tutorial.html)! Also, don't forget about `pipes-concurrency`, which I believe is the best streaming concurrency library. I can sum up `pipes` development pretty simply: everybody is waiting on me to complete `pipes-parse`, which adds leftovers and end-of-input support to `pipes`. Fortunately, it's nearly complete.
Iteratee was the first library but it was incomprehensible to me and many others. Enumerator was the first library I was capable of figuring out and it came quickly to prominence. Then conduit came out and proved that it could be even better. It quickly gained ground on enumerator. Everyone acknowledges that it does everything worthwhile that enumerator did, but better and more easily understandable. Then tekmo wrote pipes and both pipes and conduit are sort of duking it out. I prefer pipes, but both are very good libraries. Pipes have the ability to send chunks in both directions up and down the pipe and so the types system around that is a little more difficult to grasp at first.
As I'm out of the loop concerning these things and shelved my prototype iteratee implementation that could do it long ago, one question: Can any of those deal with [splice()](http://linux.die.net/man/2/splice) transparently? That is, inject direct fd-&gt;fd zero-copy transfers managed by the kernel into whatever else you're sending from userspace?
I see that you depend on the tls package for pipes-network-tls. I really wanted to use this stuff a little while back, but it's incredibly hard to believe in a TLS package until it's been through some incredible battle testing from a skilled attacker. Being able to either trust that tls package or exchange in HsOpenSSL is pretty important for any security conscious development.
We switched from `enumerator` to `conduit` due to a better API... and this was in the `conduit-0.2` timeframe, before the days of `Pipe` and `ConduitM`. Another bonus: the conduit port was faster than enumerator on day 1.
Both `iteratee` and `enumerator` offer `mapM` style monadic transforms... unless you're referring to something else?
AH! That is what I was afraid of, thanks. This means I'll have to add a more complex build system to check that the tool chain supports rdrand.
Yes! **pipes-concurrency**! How could I forget that? And in general, keep in mind that the documentation throughout the various pipes libraries is quite extensive, trying to be self contained and introductory, so you won't lack resources for learning. One could say that “giving documentation the same respect you give to code” is one of the design guidelines for the pipes ecosystem.
I take the sentence as written to obviously mean (1). But there is a meaning of do-notation independent of its desugaring! This meaning simply coincides with that given by first desugaring it. There's no reason we can only assign meaning directly to one specific set of syntax :-P
Yes, I agree with your concerns, and an `HsOpenSSL` based library should be available too, but there are a couple of reasons why I decided to build my work on top of `tls` first. One important reason is that I'd like `tls` to gain popularity so that skilled attackers start seeing it as an interesting target, and both [network-simple-tls](https://github.com/k0001/network-simple-tls) and [pipes-network-tls](https://github.com/k0001/pipes-network-tls) should help reaching that goal. These two libraries follow the interface laid out by [network-simple](http://hackage.haskell.org/package/network-simple) and [pipes-network](http://hackage.haskell.org/package/pipes-network), which are all about simplifying the usage of network connections. I shared my work on `network-simple-tls` with Vincent Hanquez, the author of the `tls` library, and he agreed that it was a step forward and said that he would like to see the adoption of `network-simple-tls` in the future. Also, before starting my work in these libraries I didn't know much about TLS and had never used, directly, any of the TLS libraries available. I picked the one that seemed to be the friendliest one, so that I could concentrate my efforts in just understanding how TLS connections are dealt with and coming up with an API that abstracted the common use cases. After some weeks of work I think it was the right choice to begin with `tls`, since I could successfully understand what a simple TLS API should be concerned about, and in the future it will be easier for me (or anyone else) to implement similar abstractions for `HsOpenSSL`. I'd like to take this opportunity to request feedback on [the current API](https://github.com/k0001/network-simple-tls/blob/master/src/Network/Simple/TCP/TLS.hs). It would be nice if someone tells me if I'm doing something funny with TLS. For what is worth, I'm quite happy with how the code looks today, and I'll probably be releasing it after adding more documentation and performing some tests. There are some example programs in the repository.
I would just go with something simple, that has just enough syntax to be identified as Haskell. fillMeUp :: Nationality -&gt; Beverage fillMeUp Italian = Coffee Add whitespace alignment to taste.
You could poke fun at some other Nationality's around you too: data Nationality = Italian | Swiss | French ...
Not bad, but I feel smug and superior about Haskell, so I would prefer something that just the intermediate/seasoned Haskell could fully grok :P :P
I think this won't make sense to me until *after* I understand `callCC`. What exactly are `k_out` and `k_in` here? They're "continuations" I guess right? Where do we get them? What are these lambdas applied to?
What I mean is the ability to build sources and transformations using a monadic DSL like `pipes` and `conduit`. For example, if you want to yield a list using `iteratee`, you write (I'm taking this from the source code): enumList :: (Monad m) =&gt; [s] -&gt; Enumerator s m a enumList chunks = go chunks where go [] i = return i go xs' i = runIter i idoneM (onCont xs') where onCont (x:xs) k Nothing = go xs . k $ Chunk x onCont _ _ (Just e) = return $ throwErr e onCont _ k Nothing = return $ icont k Nothing To do the same with `conduit`, you would just write: mapM_ yield chunks Similarly, compare their `take`: take n' iter | n' &lt;= 0 = return iter | otherwise = Iteratee $ \od oc -&gt; runIter iter (on_done od oc) (on_cont od oc) where on_done od oc x _ = runIter (drop n' &gt;&gt; return (return x)) od oc on_cont od oc k Nothing = if n' == 0 then od (liftI k) (Chunk mempty) else runIter (liftI (step n' k)) od oc on_cont od oc _ (Just e) = runIter (drop n' &gt;&gt; throwErr e) od oc step n k (Chunk str) | LL.null str = liftI (step n k) | LL.length str &lt;= n = take (n - LL.length str) $ k (Chunk str) | otherwise = idone (k (Chunk s1)) (Chunk s2) where (s1, s2) = LL.splitAt n str step _n k stream = idone (liftI k) stream ... with `pipes` (the one in the standard library is slightly more complex because it forwards values both ways): replicateM_ n $ do a &lt;- request () respond a
`pipes-parse` can. I'm going to discuss this in much greater detail when I release it, but you can set it up so that instead of actually transferring information you can directly inject another `pipe` to handle that subset of the data without any data passing. This involves two separate tricks: * Using the "request" and "respond" categories to inject pipes into certain segments * Sharing leftover buffers with the injected pipes using the newly fixed `StateP` proxy transformer
I thought about that (for example my British colleagues drink only tea) but I'm afraid I will run out of space :)
I have a mug with this on it: http://www.haskell.org/haskellwiki/Image:NarleyYeeaaahh.jpg and one with this: http://www.haskell.org/haskellwiki/Image:Haskell-logo-revolution.png As for code -- maybe a continuation reference of some kind that referes to caffeine would be funny.
 fact = fix $ \r n -&gt; if n == 0 then 1 else r (n - 1)
I'd proudly drink tea in a mug with the `fix` combinator written on it. fix :: (a -&gt; a) -&gt; a fix f = f (fix f) I'd drink the tea until I reach the fix-point of the mug being empty.
I advise you [this](http://www.ozonehouse.com/mark/enemy/).
Haskell is only verbose if you use IORefs, but I never use them.
Yes! I was thinking exactly at a pun of that kind, and Cofree was an option I considered! Thanks!
Not bad! I had an idea though: paraphrasing the Beatles, and with reference to [this](https://www.fpcomplete.com/user/dpiponi/the-mother-of-all-monads) article, what about "All you need is Cont"? :D
Yea, I know that logo and I like it, my only concern is that might be a bit too "bulky" on a mug :)
Well, [here's without IORefs](http://www.reddit.com/r/CMVProgramming/comments/1dxon9/i_believe_haskell_is_the_worlds_finest_imperative/c9uv09c). Can you do it better? Now, obviously there are better solutions, but are there better imperative solutions?
How about: coffee :: Maybe (drink -&gt; Either sleep drink) coffee = Just Right
That's fantastic. I am not sure I have the time to examine your code at this moment—I'm much more in a manager's mode than a builder's mode—but I'd love to take a close look in the future. I think airtight security is a powerful keystone for the Haskell platform (lowercase) to go alongside the safety bought by the HM types. I really want these projects to succeed generally, even if I can't use them for my purposes today.
It doesn't have to be verbose with IORefs if you define some utility functions. You can even get variables to be both l-values and r-values.
If you're willing to wait a couple of months, Edsko de Vries will give a talk on this topic [1]. Disclaimer: I'm not affiliated with any of the people/organizations, I just think they do decent work and came across the announcement. [1] http://skillsmatter.com/podcast/home/lazy-io-and-alternatives-in-haskell/ 
Not bad at all!
That's hilarious!
* a mathematician is a machine for turning coffee into theorems * a comathematician is a machine for turning ffee into cotheoerms 
&gt; Sharing leftover buffers with the injected pipes using the newly fixed StateP proxy transformer By this you mean how it shares state now correct? You glossed over that in one of your posts but it seemed like a very big deal in what it allows.
Yes, it does share state and it is a big deal. It's a feature I've wanted for a long time to correctly implement zero-copy streaming, but I wanted to wait until I had a working demonstration up before advertising this. `pipes-parse` actually does even more than that. It makes it very easy to compose pipes that have different states by being a lax monoidal functor over the state type parameter.
Your solution is nice, but what are you referring to when you say the previous solution conflates imperative and C-like?
The imperative solution (both Haskell and Python) is too verbose, here's a simpler one in C: int lastElem( struct list *l ) { assert(l); // Pretend we're handling this better while( l-&gt;next ) { l = l-&gt;next; } return l-&gt;value; } Haskell version: lastElem :: [a] -&gt; Maybe a lastElem [] = Nothing lastElem xs = flip evalState xs $ do while (not . null . tail &lt;$&gt; get) $ modify tail Just . head &lt;$&gt; get I don't know where you draw the line between imperative and not-imperative, but the two programs do the same thing in the same (roughly) way.
I think that comes out of the fact that while Haskell is a pure language -- many large scale systems, like GHC, spend a lot of time in the IO monad doing a lot of boring IO operations. And so it might seems like the promised benefits of Haskell are not really appearing because you appear to be doing pretty standard imperative programming in the IO monad, not beautiful, abstract, pure code. But, in truth, even in code that spends most of its time in the IO monad, not being pure, and not using cool abstractions, it turns out Haskell is still really nice. You still get all the power of type-checking, you never have uninitialized variables, you never forget to malloc or free your data-structures, etc. So, if you are trying to look at specific algorithms and compare imperative Haskell against something like C/C++/Java -- you aren't going to be to impressed. But, most of the code in the world is not nice algorithms -- most code in the world is a big messy pile of sequencing IO operations and pushing data around. Code that has been around long enough to grow from a small mess into a big mess. Even when you do a really poor job managing and structuring your application, Haskell seems to hold it together. If you want to refactor stuff, you can attack it pretty willy-nilly and the type-checker will do a pretty good job about telling you what you still need to update. And, even in the IO monad, there is still generally not a lot of hidden state to screw you up. While functions can perform IO -- they still largely rely on the values pass in as arguments. If you do have some sort of mutable variable, it is typically in an IORef or something else explicitly shown in the type signature. So, while it is not true referential transparency -- you still reap many of the benefits. And the concurrency mechanisms are very robust, even in the face of sweeping code changes. And, even if you do no planning when coding -- it hard to go too far wrong. You don't need to think really hard about objects and subclasses and what might happen in the future. If you are just hack something up.. it's still probably going to be quite usable. Bad Haskell code tends to be more verbose than required because it does not use abstractions it could have. But, as far as sins go, that is mostly a mild annoyance. So, if you compare best-case imperative Haskell against best case imperative C -- C will be the winner. But if you compare real world code -- you'll likely find the Haskell code far easier to work with.
Fix us a cuppa, Y not?
Write a lot of C code and you get way too familiar with the various ways to traverse a list...
It uses `C` idioms like partial functions instead of pattern matching and while loops instead of iterators. I think the only thing that is properly "functional" is using recursion.
Here's a Haskell solution that mimics the C version: lastElem pl = do l &lt;- initialize pl assert l while (l.next) $ do l .= (l.next) l.value And here's the supporting code that makes it work: import Prelude hiding ((.)) import Data.IORef class Truthy a where isTruthy :: a -&gt; IO Bool assert :: Truthy a =&gt; IO (IORef a) -&gt; IO () assert mrx = mrx &gt;&gt;= readIORef &gt;&gt;= isTruthy &gt;&gt;= \x -&gt; case x of True -&gt; return () False -&gt; error "assertion failed" while :: Truthy a =&gt; IO a -&gt; IO () -&gt; IO () while mcond mbody = isTruthy mcond &gt;&gt;= \x -&gt; case x of True -&gt; mbody &gt;&gt; while mcond mbody False -&gt; return () (.) :: IO (IORef a) -&gt; (a -&gt; IO b) -&gt; IO b mref.attr = mref &gt;&gt;= readIORef &gt;&gt;= attr (.=) :: IO (IORef a) -&gt; IO (IORef a) -&gt; IO () mref .= mrexpr = do ref &lt;- mref rexpr &lt;- mrexpr expr &lt;- readIORef rexpr writeIORef ref expr instance Truthy (ListOf a) where isTruthy NullPointer = return False isTruthy _ = return True instance Truthy a =&gt; Truthy (IORef a) where isTruthy ra = readIORef ra &gt;&gt;= isTruthy instance Truthy a =&gt; Truthy (IO a) where isTruthy ma = ma &gt;&gt;= isTruthy data ListOf a = NullPointer | Cons { _value :: IORef a, _next :: IORef (ListOf a) } value :: ListOf a -&gt; IO (IORef a) value xs = return (_value xs) next :: ListOf a -&gt; IO (IORef (ListOf a)) next xs = return (_next xs) fromList :: [a] -&gt; IO (IORef (ListOf a)) fromList [] = newIORef NullPointer fromList (x:xs) = do rxs &lt;- fromList xs rx &lt;- newIORef x newIORef (Cons rx rxs) initialize :: a -&gt; IO (IO (IORef a)) initialize a = do refA &lt;- newIORef a return (return refA) Oh and here's the fun part. Serendipitously, this allows us to call any properly set-up function with suffix-invocation. Invoked as a suffix, `return` can be used to perform a sort of dereference. &gt;&gt;&gt; :t (.return) (.return) :: IO (IORef b) -&gt; IO b &gt;&gt;&gt; (fromList [1,2,3]).lastElem.return 3 &gt;&gt;&gt; x1 &lt;- fromList [1,2,3] &gt;&gt;&gt; x2 &lt;- fromList [4,5,6] &gt;&gt;&gt; x3 &lt;- fromList [7,8,9] &gt;&gt;&gt; (fromList [x1, x2, x3]).lastElem.return.next.value.return 8 The reason this takes so much supporting code is because it represents all of the assumptions you make in C about pieces of code and where mutation can happen. This obviously won't be as fast as the C code (though optimizations *should* be able to...in theory...), but it is easier to reason about, since all of the assumptions are written explicitly.
If you wanted to make a bullet-proof version of this, your use of `.=` in analogy to lens makes me wonder if you couldn't write a small number of lenses into IORefs and recover a relatively robust imperative programming with less special-case work there.
That may be the world's best imperative code but it also produces the world's most difficult to understand compilation messages. Any idea what's wrong ? $ ghc loop.hs [1 of 1] Compiling Main ( loop.hs, loop.o ) loop.hs:11:1: Couldn't match type `m1' with `StateT [a] Data.Functor.Identity.Identity' because type variable `a' would escape its scope This (rigid, skolem) type variable is bound by the type signature for lastElt :: [a] -&gt; Maybe a The following variables have types that mention m1 loop :: forall r a. EitherT r m1 a -&gt; m1 r (bound at loop.hs:6:1) loop.hs:11:1: Couldn't match type `m0' with `StateT [a] Data.Functor.Identity.Identity' because type variable `a' would escape its scope This (rigid, skolem) type variable is bound by the type signature for lastElt :: [a] -&gt; Maybe a The following variables have types that mention m0 return' :: forall e a. e -&gt; EitherT e m0 a (bound at loop.hs:8:1) ghc loop.hs: /usr/hs/ghc/7.4.1/bin/ghc failure (return code=1) 
The iteratee version allows optimized implementations for different chunk types -- it's basically a two-layer API, with some convenience functions that allow you to just think in terms of the higher-level stream API for simple tasks. It seems pipes only allows the higher-level, inefficient API with no possibility of chunk-level optimizations for different stream types. Of course this means it has a smaller programming interface but it is strictly less powerful.
I can confirm that I am working hyperdrive which is a modern HTTP server based on pipes. It is currently awaiting pipes-parse. There is code there now, but it is total proof-of-concept at the moment. Totally not useful for anything. I started it back when the first BSD3 release of pipes was made and have been using it as a way to follow the development of pipes. So far, every pipes release has resulted in hyperdrive becoming even more readable and sensible. Expect to see some actual interesting development when pipes-parse is released. 
The key thing to realize is that an iteratee is equivalent to the following pipe type: Iteratee s m a ~ forall p . (Proxy p) =&gt; Consumer (StateP leftovers (EitherP SomeException p)) (Stream s) m a ... and iteratee composition corresponds to "request' composition (i.e. `(\&gt;\)`). So these same chunking optimizations are implementable in `pipes`, and `pipes-parse` is mainly about setting a standard chunking API for the whole ecosystem (among other things).
Brilliant, thank you!
I'm surprised noone has mentioned continuations as an upside. 
It needs type signatures (or `NoMonomorphismRestriction`). My mistake was that I was testing it with `NoMonomorphismRestriction` on and then forgot to check with it off: import Control.Monad import Control.Monad.Trans.Class import Control.Monad.Trans.Either import Control.Monad.Trans.State loop :: (Monad m) =&gt; EitherT e m r -&gt; m e loop = liftM (either id id) . runEitherT . forever return' :: (Monad m) =&gt; e -&gt; EitherT e m r return' = left lastElt :: [a] -&gt; Maybe a lastElt xs = (`evalState` xs) $ loop $ do xs &lt;- lift get case xs of [] -&gt; return' Nothing [x] -&gt; return' (Just x) _:ys -&gt; lift $ put ys 
The reason Edward hasn't done this is that monadic lenses into `IORef`s would have to both read and write every time, even if all you wanted to do was only read or write. He's holding out for a better solution.
You're welcome!
It now builds. However, on one of the machines where it didn't build I was able to build and use RDRAND with Intel's [library](http://software.intel.com/en-us/articles/user-manual-for-the-rdrand-library-linux-version). Can you use the same technique it uses to support RDRAND on OS X? 
The links to your graphs are broken, unfortunately.
I don't understand why anybody would revert back upon C++ from Haskell. Of the many main benefits of Haskell is that it has a sound type checker; C++ is a weakly typed language. No matter what you do with the template library, casting a pointer to any of your classes to a void * will always be possible and produce dangerous results-and this is just one example of a class of problems with C++. That's why C++ produces so many vulnerabilities-you literally just cannot avoid it. What you may be doing may be strictly for interest, but I think what I mean to say is that each tool has its specific purpose-C/C++ is born for drivers and kernels. I don't shun your effort, but I lament the lack of a system level language that has some of the properties from the functional world, (*cough* type safety). As an alternative, I would say that, were there ever a language that could replace C++ fully as a systems language, this is where those features would go.
I didn't deny the usefulness of Haskell's concepts to other applications. *well written. See, this right here is the problem-perhaps you equate the quality of authorship with the typed-ness of the language. In reality, it is the design of the language that is flawed. Furthermore, there is no guarantee that good authorship is a strong component of every piece of software, so the premise that, if a piece of C++ software is well written, then it is strongly typed is surely sunken. Also, I don't understand where you are on the meaning of strong and weakly typed languages. Strong typing in a language-to me-guarantees the soundness of execution. It says that there will be NO segfaults. Yet, the same loopholes that exist in C, also exist in C++. Take the very loophole that I mentioned in the previous post-you can still cast to void * in C++, and it happens ALL the time. Furthermore, (on the topic of well written software) I promise you that if you just but barely explore just about any variety of software libraries out there, even just restraining yourself to C++, that you will see all manner of disgusting code use out there. Everything that you may have (well, hopefully) ever said you yourself on the verge of "this will never happen, its just too silly/bad/stupid" that can happen, *will happen. In my outlook, I cannot possibly be harsh enough on C/C++. I only but listed the very first outset of my reaction to this post. But don't mistake me for being harsh on you. I hope that you stay and learn from this. 
Haskell is my favorite language. I use Haskell for almost everything. Yet, I am actually a fan of C++. Yes, there is a lot about C++ I hate, but it also brings some very cool things to the table. C++ is *not* C. I like C also. But they are radically different languages. C++ is not even "backwards compatible" with C. For example int * foo = malloc(SOMELENGTH); is not valid C++! This is like the most basic C code ever. The reason is that C++ has a much stricter type system. There is no implicit cast from `void *` in C++ and that is huge. The problem was never casting *to* `void *`, it was comming back. And C++ fixed that a long time ago. C is a much *cleaner* language than C++, but it is also a much more limited language. Note: the malloc example is particularly interesting, because an explicit cast is bad style in C since it can lead to the possibility of a double cast if the proper header is not included which can cause code to work on some platforms (32 bit x86 say) and not others (64 bit x86). What is more, modern C++ does not look much like C. My most recent C++ project does not use a "raw pointer" anywhere. Thats right--no pointers. Modern C++ encourages immutability but not everywhere (const correctness). Modern C++ has lambdas and higher order functions all over the place. Modern C++ avoids control structures keeping branching complexity down or hidden in well behaved "algorithms". C++ is not type-safe, in that there are probably some dark corners of the language that can "go wrong" and that you have the ability to cast anything to anything. Okay. The same is true in GHC. The dark corners are called defining your own Typeable instances and GeneralizedNewtypeDeriving. The cast is called unsafeCoerce. Despite not having the full "does not go wrong" property, like Haskell, C++ has a type system that allows programmers to enforce many invariants in their code. A type system that is there to help. I think [this paper](http://www.stroustrup.com/Software-for-infrastructure.pdf) is pretty compelling on this among other issues. Compared to Haskell, I see several advantages to C++ (I still prefer Haskell for most things) * Raw speed. Unlike Haskell, C++ has very low overhead for most things. Haskell regains speed through clever optimizations. In C++ it is much easier to write the fast code to begin with. Also, expression templates are like rewrite rules on steroids. * Control of memory layout. This is a must for some applications. C++ does this "right" in the sense that you pick the bit layout you want. * Control of argument passing convention. In C++ you get to decide when an argument is CBV or CBR. That is really nice * RAII--Deterministic control of resources is C++'s killer feature. It alone makes C++ among the best imperative languages. * Templates--templates are a different approach to genericity(sp) than parametric polymorphism or template systems, sitting somewhere between these. Although not quite able to do what either of those can do, they also can do things neither of those are well suited for. Templates let you write very high level code with the performance of low level C. C++ has too much complexity. Too many features interact in complicated ways. It is really not a programming language for the masses the way Java is and a Haskell like language with a less research-y bent probably could be. The complexity is all there for a reason. Nothing exists to replace C++ right now. Rust, Go, and D all make valiant efforts and are in different ways improvements over C++. PL theory based systems languages like Cyclone and ATS have much to contribute (also Deca). Still, C++ is *the* systems language, and given how big an improvement C++11 was, I don't see that ending anytime soon.
I agree with you almost-completely. My two preferred languages for a long time were Haskell and C++ (causing some people to look at me strangely), for the same reasons you write. I only disagree with how strongly you downplay its unsafety. C++ does not have "some dark corners". C++ is a vast cavern of impenetrable darkness, where in a few small nooks and crannies along the sides, advanced practitioners have gathered with their glo-orbs and hand-constructed lanterns, so that they might see what they are doing and not hit themselves and each other in the face with their nunchucks so frequently. For many years since I found Haskell I've been dreaming about a language that would be like a modernized C++, stripping out the object-oriented and C-inherited gunk, and adding in nice Haskell things like algebraic datatypes, type classes, and closures. Now as far as I can tell, Rust *is* that language, except it's cooler than anything I could have come up with (affine types are the shiznit). Where do you see it falling short? Its generics are not as advanced as either Haskell's or C++'s (no higher kinds / template template parameters, no non-type template parameters, no associated types / template specialization, no variadic templates), but it's still young, and could potentially be extended with (some of) those along the way.
&gt;The link above are some reasons it's not Where? All I see are a few random non-sequitur responses. The link to SO that is linked to from there has some reasonable discussion, but that reddit thread is terrible.
Agreed. I think the least OP can do is summarize, citing where appropriate. As it is, dumping a big ball of opinions (with bits of evidence) here is just rude.
I also love both languages C++ &amp; Haskell. I even have a belief that C++ developers who likes to play with templates will also like Haskell. This is somewhat confirmed by the fact that one of the http://www.boostpro.com/ guys is now in FPComplete. However I do not completely agree with "expression templates are like rewrite rules on steroids." (For those who don't know C++ expression templates - look here: http://cpp-next.com/archive/2011/01/expressive-c-expression-optimization/). Expression templates do not allow you to rewrite any regular C++ code (i.e. not related to type-level DSL created by expression templates), whereas Haskell rewrite rules do. I think things that can be created with Template Haskell are "expression templates on steroids".
`[IO a] -&gt; IO [a]` is `sequence`, not `mapM`. 
Woops, thanks. FIXED.
Fair enough :)
&gt; Strong typing in a language-to me-guarantees the soundness of execution. It says that there will be NO segfaults. The loopholes also exist in haskell, in the form of `unsafeCoerce`, `unsafePerformIO`, `castPtr`, etc. If you want your Haskell to be safe, you have to use SafeHaskell and/or carefully avoid these functions. Similarly, you can (try to) write safe C++ by avoiding the dangerous parts. I.e. by restricting yourself to a safe subset of the language. In modern C++ you can get such a subset by using smart pointers and containers instead of raw memory.
I thought this would be the case when switching from `enumerator` to `conduit` but found the opposite to be true... the switch improved performance by a fair margin (double digits %) and it simplified our codebase. My investigation at the time showed that our `conduit` port did fewer allocations and had better GC behavior (more reliable gen0 collections)... which accounted for a decent chunk of the gains. Most of the expensive stream processing we do is in a compiled eDSL/DSL and it's less likely we were seeing any tangible benefit from chunking in the first place.
Oh, I thought that Haskell was like OCaml in this regard. Anyhow, I'd like to remark that, again, you've hit that limit-you can only try with a language like C++. There's no way-that I know of-to unequivocally prove properties about a program or show that it is typed soundly, or else the masses would already be doing it.
https://github.com/DeTeam/haskell-broadcaster here's mine :D takes messages via TCP from jenkins and broadcast to websocket connections.
I don't know if anyone's really done that as yet. It might be easier to do with the hackage2 code base too. I do think that making it easy to host an internal hackage would probably be really awesome for seeing some more interesting business adoption of Haskell, but I don't think there's been much work yet on making it easy to host your own hackage. Though I could be wrong on both counts
You should use withSocketsDo in main to make your code work in Windows as well. 
*Insert tekmo-signal here* Ok, here's a super simple example and it is using lazy IO. So what problems does this code have because of lazy IO, and what does the pipes version that solves those problems look like?
This is something I'd also be interested in hearing. A friend and I are attempting to write an ssh client in Haskell, and we chose to use pipes simply because we didn't know if lazy IO would be a problem. I don't really think it would be, but I thought it best to be safe. 
It's because in the example, `sos -c "cabal install" -c "git status" -p "md|cabal|hs"` I give it two commands to run whenever an event occurs that matches the expression md|cabal|hs. Essentially whenever .md, .cabal or .hs files change sos will attempt to run the commands given in serial until they all succeed or one fails or hangs. I should probably clarify that in the readme + usage. There's a subtle relationship between clarity and succinctness that makes writing docs tricky for me. 
That code is likely fine, though also buggy :) For example, if you hit the ulimit for open file handles, `accept` will throw `Resource Exhausted` / `isFullError` and your server will die. Probably not what you wanted. So, you might want to catch that exception and ignore it or something. Another thing to consider is space usage. In this case, things are probably fine. It depends if hGetLine is lazy about returning the line or not. I believe it is, so even if the user sends 1GB of data before the '\n', it is being printed by `hPutStrLn` as it comes in, and is being garbage collected as it goes. This is, of course, the essential of the issue with lazy IO. While this code is probably fine -- it can be hard to reason about it. You would want to actually run it and check that sending really long lines works correctly. Also, it might work fine now, but it is also easy to screw up. What if you decide to print the length of each line? echoImpl :: Handle -&gt; IO () echoImpl client = do line &lt;- hGetLine client hPutStrLn client (show $ length line) hPutStrLn client line echoImpl client Now it will force the whole line into RAM. Obviously, if you calculate the length before you print it, then you have no choice. If you want to be 'streaming' you need to calculate the length as you print it and print the length at the end. In pipes, it is harder to accidentally force everything into RAM. You will realize that you can't calculate the length of the string before you have received it. In order to accumulate the whole string in RAM you would have to actually do it explicitly. Also, what happens if you want to use IO to lazily generate the response instead of just echoing the response? You may need to rely on things like `unsafeInterleaveIO`. In my experience, lazy IO is less bad that it is made out to be. The problem mostly comes down to uncertainty about: 1. space leaks It is easy to accidentally modify streaming lazy IO code so that it is now reading everything into RAM 2. freeing resources When resources get freed can be a bit vague -- especially if exceptions get raised. As a result, you have to be more alert, and you have to perform more tests to ensure you have not broken things. Happstack is still based on lazy IO (waiting for pipes-parse to be ready), and actually works really well in practice. Almost all the tricky lazy IO related stuff is in the internals and not exposed to the user. So, users of Happstack do not generally see these issues. But, when working on the internals, the developers still have to be careful. One way to avoid those pitfalls is to use plain old strict IO instead and explicitly make an IO call when we need more data from the stream. That is how `acme-http` works. But, then you basically end up rolling your own mechanisms to handle freeing resources on exceptions, left-overs, stream transformation, etc. pipes and the like offer a structured, formalized method of doing that rather than ad-hoc. As Haskell programmers, we dislike having to be careful -- we like the compiler to tell us when we have been an idiot. The fundamental flaw in lazy IO is that it assumes you have unlimited resources. If that were true, then reading a 1GB stream into RAM would be fine. But, when reality comes into play, that assumption causes big trouble. 
&gt; But essentially, it is able to use type checking to automatically determine that startDownloadThread is not safe to put in the middle of a STM transaction. Ironically, annotating his top-level functions with types would have revealed an identical bug for `getQueuedFile`.
README updated! Thanks.
Just because some code uses lazy IO doesn't mean it necessarily has problems. This code looks fine to me. The only danger is that it uses `hGetLine`, which only stops when it reaches a newline character, which might not be for a long time, or ever. However, lazy IO actually saves us here by allowing the read and the write to interleave, so as soon as at least *some* of the input is read, it can immediately begin to be written, and then it can be garbage collected incrementally as it is written. You don't even have to input and output it by lines. You can replace `echoImpl` with this implementation and it works just the same: echoImpl :: Handle -&gt; IO () echoImpl client = hGetContents client &gt;&gt;= hPutStr client Here's an echo server written using the pipes ecosystem. Note that `serve` implements the loop/forkIO portion of the code. import Control.Proxy.TCP import Control.Proxy magicNumber = 1024 main = serve (Host "127.0.0.1") "8000" $ \(sock, remoteAddr) -&gt; do putStrLn $ "TCP connection established from " ++ show remoteAddr runProxy $ socketReadS magicNumber sock &gt;-&gt; socketWriteD sock Not knowing exactly how this library is implemented, I'm not really sure what `magicNumber` should be here, or what its effect is on the code, but it has something to do with controlling how much is read at a time. tl;dr Lazy IO isn't *that* bad, sometimes. It can sometimes cause trouble when you try to reason about interactions between pure code and IO code, but there is no pure code in this example so it's not a concern.
Thanks for the response. I'm still a bit confused on a couple things though. &gt;Also, what happens if you want to use IO to lazily generate the response instead of just echoing the response? You may need to rely on things like unsafeInterleaveIO. What exactly do you mean here, like reading some file and sending that as the response? I don't understand what situation would lead to needing to use unsafeInterleaveIO. &gt;When resources get freed can be a bit vague How vague? Like, when I close a socket, the file descriptor doesn't get freed up until a gc happens? Or something longer than that? &gt;The fundamental flaw in lazy IO is that it assumes you have unlimited resources I guess it just doesn't seem any different to me from normal IO in every other language. I know I can't read 1GB of data from a socket and then call length on it, regardless of whether I am writing my code in C or python or haskell. I'm still unclear how haskell's ordinary old IO functions pose a problem that isn't present in C and pythons ordinary old IO functions. Am I just interpreting "lazy IO is bad" the wrong way? As in, people are actually saying "haskell is better than other languages, except its IO support is equal to other languages" and I am misinterpreting it as "haskell's IO support is worse than other languages"?
I don't see any lazy `IO`. He's reading and writing single lines (or is `hGetLine` lazy? I thought it wasn't). There are still problems with the code, though, mainly that it will overflow memory if the client transmits a really long line. The solution to that is to request fixed length chunk sizes and then reassemble them as necessary. The best place to begin is to use the `hGetSome` and `hPut` functions from `Data.ByteString`. If you want to do anything fancier (like parsing an input spread across multiple chunks) then you need the features that streaming libraries provide.
this is true. 
Yes, I saw that in one of the code's comments. I also noticed the problem of closely occurring events when I was working on my own file watcher. Another problem I noticed at the time was that it wasn't obvious how to handle the case where files changes while the triggered command is still running. How did you decide to handle this case?
At [Silk](http://silkapp.com) we run a local Hackage2 instance, and we deploy our internal libraries and executables to it. We don't use the builder (yet), though. Setting up Hackage2 is pretty easy, it just cabal installs. We have it behind an Apache proxy to handle security and such. Do take note that the (Happstack) state is not compatible if you upgrade a running instance to a new version of the code, so you should do an export beforehand. For building we use Jenkins, by the way. Much harder to get right than Hackage :)
Is this blog post relevant, http://www.yesodweb.com/blog/2010/12/announcing-yackage? Edit: I should have clicked the link before posting; it doesn't respond. Try this instead http://hackage.haskell.org/package/yackage
What your argument boils down to is that you're more familiar with C and Scala over Haskell.
I've written something similar but without parsing command-line arguments. Maybe you'll find something helpful there: https://github.com/CGenie/haskell-dirWatcher
lift $ pure coffee &lt;*&gt; Just milk
&gt; What exactly do you mean here, like reading some file and sending that as the response? Yes. Keep in mind that `readFile` does call `unsafeInterleaveIO` internally. Honestly, there are not many cases where you would need to explicitly call `unsafeInterleaveIO`. Imagine, though, that you wanted to make a TCP clock server that spits out the time every second. You could create a `getTime` function like: getTime :: IO String getTime = do now &lt;- getCurrentTime later &lt;- unsafeInterleaveIO $ do threadDelay 1000000 getTime return $ (show now) ++ "\n" ++ later timeImpl :: Handle -&gt; IO () timeImpl client = do now &lt;- getTime hPutStrLn client now [full code here](http://hpaste.org/88028). So, `getTime` lazily returns the current time. In this case, that is a bit contrived. It would be more sensible to just have timeImpl be a normal loop. But, if you had a web application, for example, that expect the response body to just be a plain old 'String' then you would have to do something like that. &gt; How vague? Like, when I close a socket, the file descriptor doesn't get freed up until a gc happens? The issue is more about when you read things using `readFile`, when does that file descriptor actually get closed? &gt; I know I can't read 1GB of data from a socket and then call length on it, regardless of whether I am writing my code in C or python or haskell. Right. But, in C, it would be awkward to do 'by accident'. You are not accidentally going to write a for loop that reads a bunch of chunks and appends them all to a buffer so that you can calculate the length of the string. You have to actually work to do that. With lazy IO, you can have a String that is being processed incrementally and then very easily be reading it all at once if you are not paying attention. Pretty much all type errors are the resulting you accidentally doing something you know you can't do. Yet, we still have type-checkers, because sometimes we screw up. With lazy IO, it can be very easy to screw up and accidentally read a bunch of stuff into RAM, or accidentally hold too many file descriptors open at once. With pipes, you only get one chunk of data at a time -- so you can't accidentally read the whole stream into RAM -- you have to explicitly take that chunk and append it to some list. Now, the fact that you are telling the computer the particular sequencing of IO operations and managing that stuff at a pretty low-level is the opposite of what we want. The model presented by lazy IO is the type of high-level abstraction we strive for. But, since we need our code to run on real computers, we have to deal with real resource constraints. Basically, lazy IO is bad because it is hard to reason about *when you have to deal with the reality of resource constraints*. Pipes and friends solve that by going back to a lower-level chunk at a time model -- which is similar to how most imperative languages do things. In languages, like C, when you call `read()` you have to tell it how many bytes you expect. Now, we can do that in Haskell already by just using plain-old strict IO functions that have been around for ages. Things like pipes improve on that by providing higher-level abstractions while still retaining the predictability of dealing with things a chunk at a time. The important thing to realize is that lazy IO can get the job done and can perform reliably. It's just that you have to be extra careful about certain aspects. So, abstractions like pipes that allow us to think more about the interesting stuff and less about 'Am I screwing things up' is good. Things like pipes can also give us better control over when resources like file handles are freed -- which is also good. But, the pipes mental model is also more complex than *lazy IO on a system with unlimited resources*. So, we don't get anything for free :) 
When a file changes an MVar is set with a list of file events and a process handle. If the list is empty the MVar is set with the event (in a list) and Nothing for the process handle. A forked thread then delays ~1sec, empties the event list and forks a process that runs the commands and sets the MVar with a Just ProcessHandle. If the list is not empty when a file changes then a forked thread has already been started so the event is simply cons'd onto the others. After the process finishes it empties the MVar. If a process hangs the handle will be stored in the MVar so it can be terminated if another event is fired after ~1sec has passed. Here's the related code: https://github.com/schell/steeloverseer/blob/master/src/SOS.hs#L28
So - long story short, the running process is terminated and probably not in a safe way. https://github.com/schell/steeloverseer/blob/master/src/SOS.hs#L108
Thank you!
I didn't read the article, but &gt; You don't need to know anything about category theory to use Haskell as a programming language. But if you want to understand the theory behind Haskell or contribute to its development, some familiarity with category theory is a prerequisite. is not true, and it's just wrong to perpetuate this myth. You don't need any category theory to understand every part of Haskell, the language, even from a theoretical standpoint. There are so many things that are more important... type theory and denotational semantics come to mind, for example. I would be curious to know if high profile Haskell programmers (e.g. dons, bos, tibbe) are familiar with CT, my guess is that maybe they've taken a look but nothing more (obviously I could be very wrong, given that I don't know them---but their work doesn't seem very category theoretic :). That should make a good argument to convince people that category theory is not a requirement...
My point is simple: for a practitioner of Haskell, category theory is not a requirement. But if you are curious about ideas behind Haskell, you might have a look at category theory. For instance, if you're curious where names like functors and monads come from. For some people it's a distraction, for others it's fun and inspiration. 
I don't think this code is using lazy IO anywhere - A quick look at the source for hGetLine doesn't show any obvious unsafeness.
Denotational semantics without category theory isn't a thing. While category theory may or may not be the best way of understanding many things in PL theory, lets not be silly here. Like it or not a basic knowledge of CT is needed to read many of the papers in FP/PLT/Haskell. You need to also know other things, but you need CT.
This answers my question perfectly, thanks. I think using type SOSState = Maybe ([Event], [String], Maybe ProcessHandle) makes the purpose of [this](https://github.com/schell/steeloverseer/blob/master/src/SOS.hs#L63) part of the program quite obscure. In fact, I still have a hard time following it even now that you have explained its purpose in details! Instead of a type synonym, I would have defined an algebraic datatype listing the program's possible states as precisely as possible: data SOSState = Idle | Pending { accumulatedEvents :: [Event] , triggeredCommands :: [String] } | Running { runningProgram :: Handle , pendingCommands :: [String] } 
This whole discussion is arse over tit. Haskell provides many practical examples which give good concrete intuition to categorical concepts that might otherwise seem far too abstract. Free monads, for example: they're trees with values at the leaves. GOOD NEWS! Category theory is a good way to recognize familiar structure and transfer understanding. BAD NEWS! The ground's shifting under your feet. (I say "your" because I'm one of the people doing the shifting.) This change largely amounts to recognizing that the structure you're used to is still present, even though the underlying category is not what it was. DIGRESSION/RANT! Yous *extreme expletive deleted* who use "Hask" to name the category of Haskell sets and functions have done a great deal of damage by obscuring the vast diversity of categorical structure in Haskell. Unwittingly, you've sent the message that there's only one category for understanding Haskell. Oops. POSITIVE OUTCOME SMILEY FACE! Awareness of categorical structure is undeniably helpful for Haskell programmers. Fortunately, Haskell programming is a really good starting point for becoming aware of categorical structure --- for the most part, it's learning to give names to things you already understand.
Well, more accurately, any object can be represented up to isomorphism(ish) by the set of arrows into it (or the set of arrows out of it). the `m :-&gt; (n :-&gt; m ∘ n)` thing is merely part of how to achieve this.
For a practitioner of Haskell, sigfpe's tutorial is probably more insightful and better at building intuitions.
Of course it's a thing. Denotational semantics has existed since before category theory when it comes to mathematics, logic, and natural language, and lots of program language semantics is done without category theory. CT just lets you be more general than set theory allows.
You're both wrong! Chronologically denotational semantics (in the scott-strachey sense) comes after category theory. However, denotational semantics (in the scott-strachey sense) was initially conducted purely as topology. So you do have den sem without cat theory. (what you don't have, obviously, is categorical semantics). On the other hand, denotational semantics is an actual and specific thing which was developed well after category theory had gained some traction. And finally, a categorical account of the semantics of mathematical languages was begun by lawvere roughly contemporaneously with the development of programming languages, but independent of them. (and a non-categorical account of computability as a semantics was begun by kleene among others slightly earlier. it took many years for these things to begin to be unified).
relying on Juicy.Pixels to just focus on the actual image processing algos could be cool! you'd get a few image formats for free.
Model theory predates CT but thats not the same thing as denotational semantics as used in PL. Set theory has no non trivial solutions to t = t^t and thus can not serve as a model for the untyped lambda calculus or any system equivalent in power. This has beeen known since the 1930s. Dana Scott solved that in the 60s via category theory and it got called Denotational Semanatics. Although System-F may not have been categorical originally, it also turns out to lack set theoretic models and both Reynolds and Girard ended up doing very "categorical" work, not to mention things like parametricity and monads which were directly inspired by CT. I am not objecting to the claim that there are noncategorical denotational semantics for some things, but the mainstream Scott/Strachey domain theoretic approach is both historically and in practice highly categorical. The *field* of denotational semantics is categorical.
What you describe as purely topological I would have described as categorical, but I suppose I can accept your characterization.
Agreed! Haskell has inspired many people to study category theory and abstract algebra, because it tipped them off that there is actually some cool stuff there, if you are into that type of thing. Personally, I think it is pretty neat stuff. But, there are very few things in Haskell land that would actually require you to understand anything about category theory, beyond what you implicitly learn by using monads, etc, in ordinary day-to-day programming. That is the meta message of [You Could Have Invented Monads! (And Maybe You Already Have.)](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html). While category theory formalizes certain things we do in Haskell, there is no real magic there. If category theorists had not gotten around to bringing monads to Haskell, we might have just as well ignorantly reinvented them ourselves. Except we would have called them "warm, fuzzy things" and not stressed out about category theory. tl;dr: Learn category theory and abstract algebra if you think it sounds like fun. If not, then don't waste your time -- you can still be a top-notch Haskell developer. Remember that Haskell existed *before* they introduced Functors and Monads. Adding those changed the IO model and adde the mtl library -- but a lot of what makes Haskell awesome existed before it was infected with category theory. 
The *unsafe* aspect of `hGetLine` is that if it is strict, then someone could send an very long line with no carriage return and force tons of data into RAM on the server until it crashes. The fact that many people in these comments (including me) are uncertain if `hGetLine` is lazy or strict is part of the problem. If `hGetLine` is lazy, then the code should actually be fine. But if it is strict, then it contains that potential DoS. The fact that we have to think about whether `hGetLine` is strict or lazy and what affects that will have is the core of the problem I think. 
But to manipulate `unit -&gt; a` you need first-class functions, at which point you have a functional language with side effects.
This should be at the top! Network-simple is awesome. Here's the same echo server using pipes and pipes-network: import Control.Proxy import Control.Proxy.TCP main :: IO () main = serve (Host "0.0.0.0") "8000" $ \(socket,_) -&gt; runProxy $ socketReadS 4096 socket &gt;-&gt; socketWriteD socket 
Er.. denotational semantics of computer programs was. In terms of other things, it was not, as I said. Model theory has been a thing for a while now.
&gt; monadic syntax in a lazy-by-default language sucks compared to just writing some low level C code that does exactly what you want Expressiveness is not entirely free, there is a trade-off involved. But I would trade expressiveness for C every day. When seeing bugs [like the recent post to /r/programming][2], I think "Dude, in my language, it is not even possible to write this kind of bug." &gt; Yeah, you can do it, but honestly, if there were a zero friction way to just write some C code (or something similarly low level, with better typing) embedded in Haskell, you'd just go ahead and do that. There is a package that embeds LLVM as a run-time compiler. Henning Thielemann uses that for his [synthesizer stuff][1]. [1]: http://hackage.haskell.org/package/synthesizer-llvm [2]: http://www.reddit.com/r/programming/comments/1ed2pv/so_piranaha_games_finally_found_the_cause_of/
&gt; ciaranm: a category is just a category in the category of categories Not quite true. A category is an *object* in the category of categories (of course), or a category in the category of *sets*. A category in the category of categories has a lot more structure, and is called a [double category](http://ncatlab.org/nlab/show/double+category).
I don't think the discussion is 'arse over tit'. Let me say again that my intention wasn't to discredit CT usefulness as a reasoning tool or as a starting point for research/inspiration: I'm not qualified to do that. BUT, there are many good programmers out there that have been convinced by looking at the community from the outside that to do great work in Haskell you *need* category theory: this post explicitly states that. This scares them off, and it's silly, since when I think of the people who made the Haskell community better (both with research and with software), many of them have nothing to do with CT and still contributed substantially to the language and to the community. Of course there's a part of the community that's advancing the language and what we do with it through CT: that's great and the more people do that the happier we'll be; but I'm just tired of telling people 'look, you are "smart enough" to do Haskell! Everybody can do Haskell!' when they think that it's some weird languages for high level mathematicians.
I'm not an expert in denotational semantics, but the few I've been thought or read never mentioned category theory (it simply used sets). I'm surprised when you say that to read 'many' of the FP/Haskell papers you need CT. I've been reading more or less everything put out by the GHC team in the past few years and I've never enocuntered CT (or if I did, it wasn't that relevant). Same for most functional pearls, for example. In what I'm interested now (intuitionistic type theory) category theory features much more prominently but still I wouldn't say that you *need* it to understand it (for example 'Type Theory and Functional Programming' barely mentions it). I attended POPL a few months ago and again I can't remember one presentation that required CT. etc. Obviously there are areas of PLT that require category theory. But so what? Isn't that true for a lot of of maths too? I'm not a mathematicians but as far as I can see CT is hardly considered 'required' to do maths... 
This is a great video and a great demonstration, but it really drives home how little I know of both data structures and Haskell.
This is more a library than a language issue. I think the fight over that is a bit different from the Haskell' process.
PackageImports is not a pretty extension. I think if you find yourself needing to use it, someone ought fix their module names.
There are no set theoretic models of interesting languages (theorem). sclv objected to my comment because many denotational models are/were framed in topological language, which is a reasonable point. My impression is that [many](http://lac-repo-live7.is.ed.ac.uk/bitstream/1842/377/1/ECS-LFCS-00-422.pdf) [papers](http://lac-repo-live7.is.ed.ac.uk/bitstream/1842/377/1/ECS-LFCS-00-422.pdf) some [classical](http://www.disi.unige.it/person/MoggiE/ftp/lics89.pdf) and [others less so](http://research.microsoft.com/en-us/um/people/akenn/coq/Domains.pdf) in denotational semantics have been pretty darn categorical. Also, I tend not to impose much of a wall in my mind between Algebraic topology and category theory as anyone working in the former will use techniques that in the rest of math are thought of as belonging in the later. Take from that what you will. I don't think the GHC team is representative. And, while it is certainly the case that not everything (or even most things) are particularly categorical, many papers are. As to how "required" CT is for math, I am also not a mathematician. Several of my family members are though, and lets just say I don't share your impression. CT provides a language that essentially all professional mathematicians speak "functor" "f is natural in x" even if they are not "doing category theory" and diagram chasing is widely used across subfields. CT is a top ten tag on mathoverflow, which is pretty amazing given that it is not a major traditional area of math. Few people are category theorists, but many use category theory. It really isn't clear to me where "algebra" or "topology" stops and "category theory" begins. For example, is a "Sheaf" a topological notion, or a categorical one?
You will definitely get fired for that one.
In fairness, *most* languages are incapable of having that particular bug, as it was an example of one of the many, many weird behaviors that can result from C++ template instantiations. :-)
I take your point on denotational semantics. &gt; I don't think the GHC team is representative. How is the GHC team not representative of Haskell research? In my mind they are the epitome of Haskell research... Also, the GHC team is not the only example I brought. &gt; And, while it is certainly the case that not everything (or even most things) are particularly categorical, many papers are. But this is a vague statements: CT is required in *some* PL studies, but not required in most. The author of the article makes a *much* stronger point. &gt; As to how "required" CT is for math, I am also not a mathematician. Several of my family members are though, and lets just say I don't share your impression. [...] Here I'm talking about my impressions, so I might be very off. But as far as I can see no maths undergraduate is presented with the notion that CT is required to do good work in maths, and I think that's a good thing: I think we should do the same here...
i'm just going through the process at the moment and i'm at the step 6, so you're mostly right i think, except for 5. You're not suppose to issue pull requests. The right way is to attach feature request/bug with patches in trac http://hackage.haskell.org/trac/ghc/. If your feature/bug cross multiples repositories, you would have to attach multiple patches for each one them. step 6: wait for someone to merge it or discuss it.
I don't think the GHC team is representative because what I find interesting in Haskell and in greater PL almost always has a categorical aspect to it. [This functional pearl](http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf) avoids explicit CT when that is clearly what is going on, while [this](http://isi.uni-bremen.de/~cxl/habil/papers/icfp02.pdf) closely related paper goes fully categorical. Simillarly, neither Curien and Herbelin's "Duality of Computation", nor the more recent "Duality of Computation Under Focus", are particularly categorical, and the same could be said about Wadler's "CBV is dual of CBN" stuff, but if you go back to Selinger and Felinski it gets *really* categorical and I know that some of the people who's papers might be highly syntactic or type theoertic have thought a lot about CT in relation to denotational/realizability/monadic theories to think about things like normalization. This shows up again and again. You will have some less CT heavy stuff, and then some more CT heavy stuff. We are not talking a lot of knowledge here, but I really think basic CT is necessary. This level of CT is easy to pick up for Haskellers because Haskell provides so many practical examples. I don't think the OPs claim is either wrong or objectionable. 
No, IMO. When there's a conflict, the transaction computation that "loses" will be retried automatically. If there is another conflict, it will be retried again, and so on. (`retry` lets you retry manually, but it still happens if a TVar changed during a transaction.) In most cases you'll have 0 or 1 retry. This saves you from having to worry about this kind of conflict resolution and lock order.
Oh, so the GHC developers are the only ones actually editing the code? That certainly makes sense. Basically, using Git is just a convenient way of obtaining the sources and generating patches then?
Your example with matrices is actually a special case of the Yoneda lemma.
Ah, so there's never a "list" of computations that conflict. Either we retry at the first sign of conflict, or we finish without conflict?
a, c, t are just type variables, they are strictly equivalent and can freely be renamed. Different names appear as ghci generates fresh type variables, before unifying them.
very very well made video
I see what you mean. If the transaction log shows several TVars have changed, you want a list of the values? I think this is cumbersome for four reasons: 1. Whether the transaction is retried immediately or at the end of a transaction is an implementation detail 2. You need to distinguish between the values in the `[a]` somehow 3. Your "resolution action" would still have to execute inside the `STM` monad/in a new transaction (unless you want the old values?), and 4. STM transactions themselves are lightweight: because Haskell is lazy, most of the heavy lifting is deferred until the point at which the values of the TVars are actually needed--most often outside a transaction--meaning that you often don't perform the expensive computations repeatedly inside the transaction. So the cost of a retry is negligible in most cases. Perhaps you could describe in a little more detail what the motivation behind this, as well as what `a` actually is, and I might be able to give a better answer.
Oops, thanks! Fixed.
something that starts with a lowercase letter is a variable in type expression. For example, a b c z hola wAT are all type variables that you can replace with anything that meets constraints (context). For example, f :: Num aNumberPlease' =&gt; aNumberPlease' -&gt; aNumberPlease' `f` takes value of type that's instance of Num typeclass and returns a value of the same type (that is, **the same** instance of Num typeclass) Since `aNumberPlease'` is verbose, you can use short names like `a`, `b`, ... `etc`. 
The core group of committers (of which I'm one) are the only people who commit code directly to the primary repository, of course. You put the patch on the issue tracker and put it in the appropriate state (i.e. you own the ticket, and the state of the ticket is set to *patch*.) We will take your patch - perhaps unadulterated, perhaps with some of our own modifications/light tweaks - and commit it to the tree and give you credit for your work. If you need to modify libraries, attach ALL the relevant patches. Please try to keep things down to 'one thing at a time.' You can have a series of patches for different libraries, but ideally you're working on something component-ized enough it can fit all into one commit-per-repository-involved, without being considered gargantuan or crazy. This will make everyones life a lot easier. Git is the same thing it always was: version control. Patches over the wire are just one way to share completed work, besides a pull request. 'Pull requests' are a pretty NEW invention in this regard, really. :) GHC's horrendous combination of both submodules and free floating git modules tracked by an external script doesn't really help creating patches and stuff. Sorry. :( Again, please set the status of the ticket to *patch*, because it makes it easier see what work is outstanding and what bugs need attention. Please be patient if it takes time, it's not saying anything about your patches if it takes a few days to reply (I'm not paid to work on GHC, for example, so all the time I give to it is voluntary.) You are also encouraged to come by #ghc on freenode and ask for help or if you want to expedite your review process and get people looking at the code actively or whatever. I find this helps when I want people to look at stuff. You're free to yell at me (thoughtpolice) if you need any help with something in particular. I'm in a traditional US timezone (CDT) so communication is easy in the afternoonish times and evenings. The varying time zones amongst hackers around the world means sometimes you'll be talking to an empty audience.
I'll also go ahead and roll out the basic commands for exporting patches, for people unfamiliar with this way of doing it. Basically, if you're on a branch in a repository with your patches, and you want to export all those commits which are *not* on the master branch, so you can attach them to a ticket, you can do: $ cd ~/ghc/ghc-work # my working-tree GHC $ git checkout bugfix-t1234 # fix for ticket #1234 $ git format-patch master -o patches This will output a series of `.patch` files in my `~/ghc/ghc-work/patches` directory, one for each commit. Do this for all the repositories you've modified. Then attach them all to the ticket on Trac.
Like the others said, using other type variable names make no difference. But where are the names coming from? Prelude&gt; :t head head :: [a] -&gt; a Prelude&gt; :t tail tail :: [a] -&gt; [a] so it makes sense that head (tail xs) would reuse the name. head . tail also uses (.) (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c so there you have the c. swap does not use any predefined functions so ghc defaults to using t-prefixed names. If you change it to use (,) instead: (\(x,y) -&gt; (,) y x) :: (b, a) -&gt; (a, b) (,) :: a -&gt; b -&gt; (a, b) So if a type variable is already present it's reused, if not some arbitrary name is picked, and if there is unification one of the variables are picked but I don't know why one would be picked over another, but it's not important. 
What are some categories (other than Hask) in Haskell? The only ones I can think of are subcategories of Hask.
Here's a non-plt context -- Urs Schreiber's "The geometry of physics", which I claim to have occasionally tried to tackle, but not nearly to have understood (or come close to completing), establishes a great deal of its groundwork via the yoneda lemma: http://ncatlab.org/nlab/show/geometry%20of%20physics In a plt context category theory probably plays one important role via the notion of _adjunctions_ between domains of meaning. Abstract interpretation was developed by Cousot using Galois connections, which are a particular sort of adjunction. Arguably we could just use Galois connections and forget about the CT component entirely. But then when we want to do things like give a denotational (rather than set-theoretic) account of abstraction interpretation, things get a bit icky. Stepping back up to a categorical approach smooths things out nicely. Another example would be Barwise &amp; Moss' book _Vicious Circles_ which is a great introduction to how we can deal with circular/non-wellfounded definitions, with many applications to CS. They in fact _don't_ use category theory, but instead use set theory with a bunch of ur-element nonsense thrown in and the usual set-theoretic definitions of functions, etc., and this makes things quite messy. Barwise in a later paper on the topic moved to a category-theoretic presentation, yielding an approach that is at least in my view, much cleaner and more straightforward. Another example is the very neat stuff going on with _combinatorial species_ (that byorgey among others has been working on folding into CS). Among other applications of species, we can get a better way to enumerate random elements for quickcheck-like libraries. Species were defined categorically from the get-go. These are some very modest examples, not intended to capture the scope of the utility of CT at all. The reason cayley's theorem is slightly more interesting than what you wrote, btw, is that at least it describes the relationship between different sorts of things, such as groups and symmetric groups, and subgroups. With matrices, on the other hand, everything lives in the same "world" entirely, and therefore the result feels more immediately intuitive. (Though can you imagine living in a world where we _didn't_ know it?)
I'm glad mathematicians have finally developed a formal notion of which languages are interesting. Can you link to a reference?
Reloading of snaplets individually is...*awesome *.
This is one of the reasons I haven't given much thought to pipes. I already spent my time fixing http-conduit and don't want to have to do it again with another stack that doesn't seem to have more compelling features.
It's not true that actors are deadlock-less, it just manifests differently. If you have an actor A that is synchronously waiting for B to do something and B won't do it, you're effectively deadlocked.
is it a run-time or compile-time behavior?
that was supposed to be a little bit funny, sorry
Higher kinds give rise to handy categorical structure, e.g., type s :-&gt; t = forall i. s i -&gt; t i where `i` can have any kind `k` you like, so objects are `s, t :: k -&gt; *` and the morphisms are *polymorphic* functions. The standard categorical notion of monad applied to these categories rather than "\* with -&gt;" gives a nice way to index effects with respect to a `k` which represents the state of the system with which the effects interact. Not all such monads concern "effects": spot the monad in [this file](https://github.com/slindley/dependent-haskell/blob/master/Box/Box.hs). Another source of categorical structure are the "type class morphisms" (see much work by Conal). For classes at kind \*, these give you subcategories of "\* with -&gt;" for functions which suitably respect the operations (e.g., Ord with monotonic functions). For constructor classes, e.g., Functor, Applicative, Monad, ... you get categories where the morphisms are polymorphic functions (aka natural transformations) which respect fmap (which they can't help but do anyway), pure and &lt;*&gt;, or return and &gt;&gt;=. A Monad `m` has a Kleisli category, with objects from `*` but a morphism from`s` to `t` is a function in `s -&gt; m t`. Comonads come with the dual notion. GADTs let you define lots of categories. You just declare some data MyMorphs :: K -&gt; K -&gt; * where ... and show how to implement myId :: forall (i :: K). MyMorphs i i myComp :: forall (i j k :: K) -&gt; MyMorphs j k -&gt; MyMorphs i j -&gt; MyMorphs i k For example, define data Nat = Z | S Nat data Ge :: Nat -&gt; Nat -&gt; * where GQ :: Ge n n GS :: Ge m n -&gt; Ge (S m) n so the objects are `Nat`s, `GQ` is identity and composition (aka transitivity) is an exercise. Now define a functor `suffix` from `Ge` to `Vop x` where newtype Vop x m n = MkVop (Vector m x -&gt; Vector n x) suffix :: Ge m n -&gt; Vop x m n What else? Every endofunctor `f` has a category of algebras, where the objects are functions in `f t -&gt; t` for some `t` and the morphisms from `foo :: f s -&gt; s' to `goo :: f t -&gt; t' are those functions `h :: s -&gt; t`such that `h . foo = goo . fmap h`. I could go on, but I won't. Haskell's hip-deep in categories. I'd be happier if the category often called Hask were called Hask-\* instead. That would start to make room in the nomenclature for Hask-others, and with that, perhaps, room in people's heads.
Ah - yeah - I totally should do that. Thanks!
 second :: [a] -&gt; a second :: [c] -&gt; c Suppose I define `f(a)=a`. Someone comes along and defines `f(c)=c`. Is my `f` the same as theirs? Why or why not? 
There is the twilight-stm package available on hackage: http://hackage.haskell.org/packages/archive/twilight-stm/1.2/doc/html/Control-Concurrent-STM-Twilight.html ; papers at http://proglang.informatik.uni-freiburg.de/projects/twilight/
The point is that "Haskell is the finest imperative language because you can define these control operators" is not a good argument since you can do so in pretty much any language with first class functions (which nowadays is basically any language that isn't assembly or C). In fact in most languages it works much better than in Haskell because in Haskell you'll quickly end up with space leaks, loads of garbage being generated by loops that shouldn't allocate anything in the first place, and with a verbose syntax.
Correct. That's why I said "*If actors never block*, you will never have deadlock"
Note that `retry` is a primitive operation that is much different from what happens when a transaction discovers that it has an inconsistent view of memory. In fact a transaction cannot `retry` (put the thread to sleep and wait for a change to some `TVar` in its access set to wake it up) unless it can prove it has a consistent view of memory. There is existing work in other STM systems for supporting partial rollback of transactions where the state is rolled back to a point in the computation where things are again consistent and continuing from there. But none of this is in GHC's STM.
Oof! That came out poorly. &gt; Wait: so you're saying that the GHC team is not representative of Haskell research because you are not interested in what they do? Very poor wording on my part. My claim was supposed to be a epistemologically humble one--although I see why it didn't read that way. I don't know every subfield of PLT or Haskell. The ones I do know because they are my interests are highly categorical. I don't have any empirical data, so my impressions are the best I can go with. I did not want to suggest that what GHC team does isn't awesome. It is awesome. Just that Haskell research is a lot bigger than a couple of guys at Cambridge. &gt; Yes, there are some functional pearls that build on categorical concepts: a small minority. Again, what have you proved here? That sometimes some CT is useful. Yes. Sometimes CT is useful. I started in PL and Haskell not knowing any CT. Then there was stuff I wanted to understand. So I learned CT. Why would claiming that there are important things (like say Moggi!) that you need CT for at all controversal? A lot of people thing that stream fusion is pretty cool. data Stream s = forall r. Stream (r -&gt; Step r s) r where does that come from? Well, it may have been invented independently, but Walder described essentially that type in [1990](http://homepages.inf.ed.ac.uk/wadler/papers/free-rectypes/free-rectypes.txt) based on a categorical investigation (the ideas are older still), an understand of which made stream fusion make more sense to me. Regardless, I'm done. This discussion has already taken more time that it would to learn all the CT concepts you need to be able to read PL reaserch.
Let me preface this post by saying that my training is in mathematics; in particular, a particularly 'categorical' branch which is concerned primarily with homological algebra. I am quite interested in Haskell for several reasons. A nice side-benefit is its type system which allows one to use category theory in programming, if you are so inclined. This post inspired me to finally understand the (covariant) Yoneda lemma, and what it means in Haskell. Great job :) This is my interpretation of what is happening, if for nothing else but for me to look back on later. I am going to try to avoid conflating category theory with Haskell, but I feel that for me at this point it is still unavoidable as I am a very novice Haskell user/programmer. This is also my first post of this type, so if I am wrong or missing something please correct me! For brevity, let X be the category of all Haskell types (Hask in the post). The objects of X are types, with morphisms usual Haskell functions. One has another category Fun(X,X), of functors from X to X. From these, we can form the cartesian product category X x Fun(X,X). To my understanding, the Yoneda lemma is concerned with two functors from the cartesian product category X x Fun(X,X) to X. One is quite easily defined, so let's discuss this one first. I'll call it eval: eval : X x Fun(X,X) --&gt; X is a functor that takes an object (x,F) of X x Fun(X,X) (which is a haskell type, and a functor) to the Haskell type F x. I call it the eval functor, since it is just evaluation of the functor on the type, giving another type. One can also easily define what eval does on a morphism (x,F) -&gt; (y,G) using the fact that F and G are functors and x and y are types. The other functor is more complicated, and has to be defined in pieces. First, let x be a fixed Haskell type. We can get a functor from X --&gt; X which we call h_x in the following way: Given a Haskell type a, h_x(a) is the set of functions from x to a, i.e. elements of h_x(a) are of type (x -&gt; a). Note that these are themselves types, i.e. elements of X (a category whose morphisms are themselves elements of the category are called(at least by me) auto-enriched). When defining a functor, we must also specify what h_x does to morphisms, i.e. functions. So, given a function f of type (a -&gt; b) and an element g of h_x(a) (i.e. a function of type (x -&gt; a), we do the obvious thing, which is compose the functions to get h_x(f)(g) = f . g. Now, given any Haskell type x, we have a functor h_x. Finally, given any functor F from X to X, we get a new type, namely the natural transformations from h_x to F. This is the other functor from X x Fun(X,X) to X in the Yoneda lemma. For lack of a better word, we'll call it Phi. Spelling this out a bit more, we have a functor Phi : X x Fun(X,X) to X defined by Phi(x,F) = Nat(h_x,F). Let's make sure this definition 'type-checks'. That is, is the set of all natural transformations from h_x to F a Haskell type? I claim that given a particular type x and a particular functor F, the type is forall b. ((x -&gt; b) -&gt; F b), which is exactly found in the post, and is a valid (polymorphic) Haskell type. We can now finally state the Yoneda lemma. The Yoneda lemma states that there is a quite easily defined natural isomorphism (i.e. a natural transformation in which all the maps are isomorphisms) Yon from the functor Phi to the functor eval. What does this mean? It means that for every object (x,F) in X x Fun(X,X), there is an arrow in X (i.e. a function) Yon_(x,F) : Phi(x,F) --&gt; eval(x,F). To give the definition, let Psi_x be a natural transformation from h_x to F, i.e. an object of type forall b. ((x -&gt; b) -&gt; F b). Then Yon_(x,F)(Psi) = Psi(identity function on type x). So, the line in the post (with types changed to match my notation): forall b . ((x -&gt; b) -&gt; F b) ~ F x is exactly the statement that Yon is a natural isomorphism. Phew! That was longer than expected.
BTW, you can do `let swap :: (a, b) -&gt; (b, a); swap (x, y) = (y, x)`. Then `:t swap` will give you `(a, b) -&gt; (b, a)`. If you're getting a letter other than `t`, it's coming from an explicit type signature somewhere.
Thanks. What should I read to learn how exactly category theory helps in one of these cases (or another case)? To be honest on the face of it these don't sound like category theory was such great help let alone essential, except perhaps in the Vicious Circles example, which I know nothing about. For example for Galois connections I don't really see what insight category theory would bring us that isn't already obvious. (In fact I'd say that the whole Galois connections topic is already in a sense obvious in itself once you get to a concrete situation, and the theory can pretty much be ignored if your goal is to develop an actual abstract interpreter. The properties that an abstract domain has to satisfy are easier to derive on the spot.) Or maybe I'm just expecting too much? I'm not sure how to view category theory. On the one hand from what I've seen it's basically a bunch of definitions that further generalize sequences like reals, field, ring, group. E.g. after that sequence one would have monoid, category, semicategory, which are respectively a group without inverses, a monoid without closure, and a category without an identity. So while one will often encounters such things simply because they are so general, knowing that the thing we have is one of those (e.g a category) is not very useful for exactly the same reason. A category is so general, that there are only trivial things we can say about them. Does this mean that category theory has applications in the same sense that set theory has applications? i.e. technically you use it everywhere but in reality unless you're a set theorist nothing interesting happens on the set theory level, and professional mathematicians regard work by set theorists as largely irrelevant to what they do. On the other hand, you often see people claiming that category theory is so great, or even indications that category theory provides a whole new foundation for everything with great advantages. This leads me to think that I've completely misunderstood what category theory is. &gt; Though can you imagine living in a world where we didn't know it? Well, I can't either imagine living in a world where we didn't know Cayley's theorem or the Yoneda lemma (if I have correctly understood it). For example it puzzles me why Cayley's theorem is called a theorem, and not "Cayley's lemma" or no name at all. Then we could use the name Cayley's theorem for what's currently called [Cayley's formula](http://en.wikipedia.org/wiki/Cayley's_formula), which is far less trivial. BTW Cayley's theorem is just saying that we can represent a group G with operation `*` by sending each group element g to the function `h -&gt; g * h`. The notions of "symmetric group" and "subgroup" are just different names for this, they don't bring in any new insights. "subgroup of the symmetric group acting on G" is just different language for "some group of functions `G -&gt; G` where the group operation is function composition".
Yes. You can prove that for every input both f's will produce the same value, namely the input value.
My question was a bit ill posed. What I was thinking of was an extention to STM that would let you do conflict resolving. It looks like twillight-stm that nwf posted provides this.
K - done and updated in github. The code definitely reads better, thanks.
TH is 100% compile-time
sifgpe's (Dan Piponi's) tutorial was an inspiration for my post. However, Dan would not write his tutorial if he weren't inspired by CT since Yoneda is formulated in CT. So I went back to basics to get better intuition about it. In fact it's a back and forth process: you get some intuition by looking at Haskell examples; they help you understand the CT formulation which, in turn, gives you more insight into Haskell, and so on. Physicists work this way. Dirac came up with this crazy idea of a delta function that's zero everywhere and infinity at the origin but whose integral is one. There is no such function! Well, there isn't a *function* but there is a *distribution* with these properties. Can you do quantum field theory without knowing distribution theory? Yes you can. 
Yes, you got it, although I hope my explanation is a little easier to follow ;-)
IMHO, type-safe is important for run-time metaprogramming like meta-ocaml. It's not worth for compile-time
Not a full answer here -- too much to chew on and I should do some real work today, but a few small points. A) galois connections become particularly handy when you not only have a single AI, but a tower of them, and you want to compose them. the adjunction approach becomes more powerful when you want to talk about not only AI, but the relationship of AI to other ways of interpreting some language. See this paper, for example: http://www.di.ens.fr/~cousot/COUSOTpapers/TCS02-1.shtml B) Furthermore, in order to say "an actual abstract interpreter" one must have a definition of what an "abstract interpreter" _is_. galois connections are how we provide that definition. C) I happen to be interested in foundations and semantics, so categorical approaches are naturally interesting to me. If you are not interested in these questions, then they are probably less interesting to you. In pure foundations, an extremely neat application of category theory is that we can take one of the deepest results of 20th century set theory -- cohen's forcing -- and reformulate it categorically with sheaves. Doing so yields something very dense (I don't claim to fully understand any formulation of forcing), but much tighter and more general than the original approach, and that flows very naturally (I am assured by those who understand it) from standard constructions on sheaves. So if you want to understand Cohen's forcing, one approach would be to understand sheaves first. D) You're talking about the simplest things in category theory and pointing out how simple they are. What is interesting is not what is internal to them, but the constructions on them -- we move quickly to adjunctions, 2-cells, kan extensions, bicategories, etc. Much of this becomes fairly esoteric with regards to what us more PL types can construct and work with in our systems of choice. So there are many nontrivial constructions in category theory -- enormous, complicated, beautiful diagrams and diagrams with strings and diagrams in three dimensions which compose in peculiar ways. Now which of _these_ will eventually cast a shadow back into PL-land is a much more open question :-) However, it should indicate that what category-theorists find interesting are not categories themselves, but the structures built out of relations between them, and the structures built in turn on those...
Sadly my response got a bit longwinded there at the end. I think the key for me was realizing that the object X and the functor F can vary and so really we are comparing functors from X x Fun(X,X) to X. Thanks for your post.
When is the haskell port of Fan coming ;D? 
&gt;IMHO, type-safe is important for run-time metaprogramming like meta-ocaml. It's not worth for compile-time Not so sure about that. It's true that if the code compiles, this is not much help, but having written enough TH code, this would come very handy _during development_ to ease debugging and provide better documentation for future changes.
I am planning to port it to javascript, bootstrapping the ocaml compiler on node.js XD
I don't disagree, but I like looking toward the future as well. I'm really excited to see pipes come to the forefront because I think Haskell libraries that are law abiding and mathematically based are valuable contributions and representers of the language.
&gt; pretty much any language with first class functions (which nowadays is basically any language that isn't assembly or C) Nowadays meaning a couple of years. You still end up with all that imperative cruft, though, like the question whether closures capture variables by reference or by value (JavaScript got it terribly wrong). It doesn't come natural. Space leaks can be avoided by familiarity with lazy evaluation. Loops aren't that bad, and you can always use an embedded DSL for LLVM if you need to control the assembly. The claim that Haskell's syntax for closures is verbose strikes me as odd. 
Ooh that's great, the non-sharing of StateP was the one thing that made me think "this is ugly" the last time I looked at pipes.
Oh, yes it is definitely unsafe in that regard. I guess that is the other side of the coin in terms of not being lazy.
&gt; -- for the most part, it's learning to give names to things you already understand. Resonated with me big time. Reading category theory texts is like "oh, so _that's_ what you call that!" Mostly thanks to Haskell and its community. (Though I suppose the type system of any ML might give me similar intuitions, but Haskellers do like a bit of CT.)
For me it was the non-sharing `WriterP` that was ugly. I was like "Oh god, this is useless."
You already had to pay that dependency cost* with the old scheme however, so this really doesn't change anything in that regard if you're using TH. Lots of people use it. If anything I think this removes some of the wartyness of Template Haskell and makes it more attractive - as far as I can tell this is strictly an improvement over the existing infrastructure IMO, modulo no backwards compatibility (which hasn't stopped us in the past, and should not stop us here.) I hope this is a first step towards a true MetaHaskell. Or something like it. EDIT: removed irrelevant fluff.
I'm thinking of this post: http://blog.sigfpe.com/2006/11/yoneda-lemma.html .
Here is the [original version](http://www.vex.net/~trebla/haskell/sicp.xhtml).
I use yackage all the time; in fact, several yackages to keep various level of unstable-ness organized. It's *much* better than trying to keep things straight with, say, `cabal-dev add-source`. You just don't need all the hackage machinery for that. Yackage is lightweight and trivial to use. The only thing is, it has a ton of yesod-related indirect dependencies. I don't want all those cluttering up my ghc package database. So I built yackage once using cabal-dev, and I've been using that executable.
You missed my point. The dependency caused by TH is *extremely huge*. Think about if you write a DSL compiler which happens to use a third party library in the quoted code, for example, wxHaskell, which means all the libraries wxHaskell depends on will be loaded at compile time. Even a hello world example in Template Haskell would cause the executable 20M bytes. Type safety is important, but here the price is so high that it does not work well in practice. Also, the compilation time downgrades significantly
Those are all fair points. I think there was a communication error somewhere, so I want to backtrack. I went ahead and removed the fluff from my old post. The original discussion/thread was spawned by the idea this will significantly help developers who *do* use TH, despite its downsides. That was `iusty`'s point as to why this would help people who use TH. Your point seemed to be TH as it stands at all isn't worth it. There are a number of people who use it, despite the downsides. The problems you mention *already existed* with the old scheme, and continue to exist with the new scheme, when you brought this up against him. So it's not really a slight against `iusty`'s point that it *will* help developers who use it. I was slightly confused by this, hence my reply. Again I think there was a communication error somewhere, I just magnified it with the other words. You are right, the dependency is large in all the respects you mention - the path to this argument was not so clear.
Assuming that you've played around with Java: a, b, c, t and t1 are essentially generics. In Java, A second&lt;A&gt;(List&lt;A&gt; list){ ... } and C second&lt;C&gt;(List&lt;C&gt; list){ ... } are exactly the same. You can rename a generic all you want, but the only properties you care about are structural - namely, where in the type the name appears. So the following two types are exactly the same: [a] -&gt; a [c] -&gt; c also, the next two are the same (a,b) -&gt; (b, a) (t, t1) -&gt; (t1, t) but the following two are not the same, of course: (a,b) -&gt; (b, a) ([a], d) -&gt; [d] &gt; Does the c indicate that it is a curried function? Currying is something different. Normally, in C/Java/most math, you define n-nary functions: int foo(int i, int j, int k) { ... However, Haskell is based off of lambda calculus, which has only 1-ary functions. However, there's an easy technique to transform a n-ary function to a 1-ary function: Func&lt;int, Func&lt;int, int&gt;&gt; foo(int i) { ... We can then invoke it by saying foo(1)(2)(3) Haskell has some nice syntax for this: The haskell equivalent of that mess is foo :: Int -&gt; Int -&gt; Int -&gt; Int which can also be written as foo :: Int -&gt; (Int -&gt; (Int -&gt; Int)) All this indicates that second isn't curried, it's just a 1-ary function that returns a value. Currying really only applies when you have a 2-ary or higher function.
I think that link overemphasizes the use of monads for managing side effects. There's much more to monads than `IO`.
Read the `pipes` tutorial if you want lots of diverse examples of alternate categories: * The four pipes categories * Monad morphisms (i.e. the objects are monads) * Proxy morphisms (i.e. the objects are proxies) * `MFunctor`s and `PFunctor`s (the objects are monad morphism categories and proxy categories, respectively) I've even encountered weirder ones in my `pipes` research, but those are so esoteric that they aren't worth including in the libraries.
You forget that scala has crummy performance even when you write imperatively. In my opinion, getting Haskell to do 'exactly what i want' is more tractable in that I _can_ inspect core/asm while in scala i'm left inspecting bytecode, which is a few more levels of abstraction away from the runtime. Plus our profiling tools are way better (than scala, that is, not Java). I agree on the other point though -- I'd love to write zero friction low level systems code, or better yet, generate it. I've played with the llvm stuff apfelmus describes, and its pretty awesome, but there are some rough edges (interactions between bindings to stdlib functions plus optimizing plus compiling plus interpreting are weird + i can't get it to play well with ghci). And beyond those rough edges we need a much bigger library of structures and algos, etc. to work with. Otherwise when we want to do it our own way we have to do _everything_ our own way and instead of writing what we thought we wanted, we're off implementing linked lists in assembly. What I think we _really_ need is a high-level more c-like dsl over llvm that maintains all the nice features of the binding while making everything feel less yicky. A rewrite pass that took jump-where-you-feel-like code and annotated it back up into llvm's SSA would also be very handy, since getting used to writing in that style is somewhat painful.
What's the point/difference?
IMHO ghci should normalize to using just a, b, c, ..., always starting with a.
Thanks for the feedback. I did start from the position that I'd rather make you pause and rewind a bit (or even a lot) than risk putting you to sleep. But maybe I went a bit too far. :-) And thanks for reminding me to explain (even briefly) syntax that might be unfamiliar. Sometimes I forget that these things aren't in the standard. In case you haven't already figured it out, "S p ~ n" is a type equality constraint, described in 7.10 of the GHC 7.6.3 user's guide. Equality constrains are enabled with either the GADTs or TypeFamilies extensions.
The key thing missing is that `[a] -&gt; a` is short for `forall a. [a] -&gt; a`. Just as you can alpha substitute a variable in a function, you can likewise substitute in a forall. The confusing thing for me first coming to Haskell was no one explained that a and c were variables. it gets easier when you can read the elementary notation. 
The only difference I see is that Cabal has been replaced with COBOL. And the point? None. At least I don't see any. It's not terribly funny. It does show that Reddit's code for detecting "other discussions" is case insensitive though. So I *did* learn something from this.
And then the next version can use [||| ... |||] and $$$( ... ), and so on and so forth
Thanks, that explains it pretty well.
Template Haskell brackets can be used to build Haskell terms at run time. Now that there are typed brackets, TH can be used to build type correct Haskell expressions at run time. One could use the GHC API to run the generated expressions, and it would all be type safe...assuming the implementation of TH is correct. So typed TH + some "unsafe" library code gives us "type safe" runtime code generation in GHC. There are other uses for typed quotations. F# programmers use them extensively; the paper [The essence of language-integrated query](http://homepages.inf.ed.ac.uk/wadler/topics/links.html#essence-of-linq) by James Cheney, Sam Lindley, and Philip Wadler describes one such compelling use.
Certainly types are less useful for compile-time metaprogramming because the compiler will type check the generated anyway. However, they're certainly useful when you are the consumer of a library meant for compile-time code generation. Without types, your code generator may generate type-correct code on Monday, and then on Tuesday, when you change the way you use the library, the generated code is no longer type correct. Typed quotations would have caught that. Instead, you now have to dig through the library code to figure out why the untyped quotations it generated are not type correct.
&gt; a misleading idea to what it is like to learn Haskell. It explicitly does **not** refer to learning Haskell. Stop saying this.
I said, it *gives the wrong idea* about learning Haskell, by saying that CT it is a requirement for Haskell studies.
Section 4.3 is about how FRP in Elm fits with Arrowized FRP. I think I finally figured out how to describe the connection in a somewhat reasonable way, and hopefully it answers some questions about expressiveness that people have :) The short answer is that AFRP fits within Elm's system, and you can use it as much or as little as you want.
Please don't post useless content.
This was an excellent video! I wouldn't change a thing about your presentation style. It is fast enough to keep the audience from falling asleep, and you throw in enough pertinent details that they can get the gist of it on a first run through and go back and find out the details they missed if they have any questions.
Could someone explain to me why there never needs to be a lock on the read values? To clarify where I am coming from with this question: I understand that the point of STM is to be optimistic and *assume* that read values will not change until the end, and that this assumption is theoretically just fine because you can always check the read values again at the end of the transaction to verify that they indeed have not changed. However, after you have verified the read values for the last time, they still might be changed while you are in the process of writing out the values to be written, thus causing the resulting memory to be inconsistent. It would seem naively to me that when doing the final validation you would need to lock both the write values *and* the read values so that the latter are prevented from changing until all of the writing is finished, thereby preserving consistency. If someone here could explain what it is that I am missing then I would be very grateful. :-)
you could(and should) use locks to read the value at the end.
That's what I thought, but then it means that the article is significantly mistaken because it claims that locks are only taken for the write values.
You are right, I should add some discussion on this topic. The reads might change, but the committing transaction holds locks on all the writes. So the transaction that changes the reads must have not included any of the writes in its access set. You can think of locks on the writes as a promise of writing consistent values. These promises are as good as having written the values themselves, but doesn't commit the transaction to being unable to rollback if a read was found to be inconsistent while the locks were acquired. I'll write this up more formally and include it in the commentary when I get a chance. Note that this is discussed a little in the section about tokens.
No. The write set is all that is locked when invariants are not involved.
So in other words the lock on the writes guarantees consistency because nobody can read from them until the write is released, so although the writes might not be consistent with the current values of the reads at a given time they will be consistent with each other. I am extrapolating that the reason why this works is because the only way that an invariant over any arbitrary subset of variables could potentially be broken by the transaction is if it the subset has overlap with the write values as they are the only values being changed, so guaranteeing consistency of the writes is good enough because either an invariant depends on them and and will be blocked from being observed until the locks are released and everything is consistent or it does not and so it doesn't matter what is happening to the write values.
Right. There are formal ways to state this and I will track down the best one for GHC STM in particular. It is important to note that not only the writes are consistent, but a time period was observed with the read_only_check where the reads were also consistent. That that time is in the past when the actual writes of values happens does not matter.
`Maybe` is not a type class. It's a type constructor. It modifies other types. For example, `Maybe Int` has values like `Just 5`, `Just 6`, or `Nothing`. `Maybe String` has values like `Just "foo"`, `Just "bar"`, or `Nothing`.
thanks for the explanation but it doesn't really help me. Why do we need to use maybe?
How much of this can be/is already implemented in Haskell with the various reactive frameworks? They mention a Haskell implementation of some of it in the paper, but not where to find it. Has anyone seen it? There's an elm package on hackage, but it's the compiler for the JavaScript version I think.
`Maybe` is a data type. Technically it's a "strictly positive functor" type, but please ignore that (for now!) because the more common name for such a thing is "typed container" which is pretty much the right intuition. Typed containers tend to let you put things into them and get them out—in some fashion—later. Let's look at a slightly simpler "typed container" first. This is a less popular datatype called `Identity` we define it like so `newtype Identity a = Identity { runIdentity :: a }` and we can put things into the container with `Identity :: a -&gt; Identity a` and take them out with `runIdentity :: Identity a -&gt; a`. There's not a whole lot more to this guy. But compare `Maybe` to `Identity`. We can put things into `Maybe` with `Just :: a -&gt; Maybe a`, but we don't have such a simple way to take them out. This is because `Maybe a` is a box that either contains a single `a` or... `Nothing`. This is nice because `Nothing :: Maybe a` for all types `a`---it lets us say "we don't actually have a type here" wherever we use `Maybe`. Thus, we "maybe" have a type `a` when you see `Maybe a`. This is reflected in how we get things out of the box. That function is called `maybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b` and lets us provide a default value for when `Maybe a` is actually `Nothing`. A simpler version is this maybeDef :: a -&gt; Maybe a -&gt; a maybeDef a maya = maybe a id maya which gets to the idea of having a "default" value more directly. `Maybe a` shows up all over the place. A great example is `lookup :: k -&gt; [(k, v)] -&gt; Maybe v` which "maybe" returns the value corresponding to a key `k` in a dictionary list `[(k, v)]`... but returns `Nothing` if no such key exists.
thank you :)
Your "maybeDef" is more commonly called "fromMaybe", which is defined exactly the same way. Still a great post. A slightly more advanced point: these two functions are rather interconvertible, when combined with fmap. You showed how to define fromMaybe with maybe, the others work too: maybe def f may = fromMaybe def (fmap f may) fmap f may = maybe Nothing (Just . f) may The latter shows also that (=&lt;&lt;) = maybe Nothing.
As an example where we'd use maybe, consider this function: tail :: [a] -&gt; [a] tail (a:as) = as tail [] = error "empty list" The `error` clause is not good, it kills your program. Of course in this case the error is easy to avoid by checking before calling `tail`, but it isn't always so easy to tell if a function is going to fail, so instead we could define: tail' :: [a] -&gt; Maybe [a] tail' (a:as) = Just as tail' [] = Nothing A more interesting example `Data.Map.lookup` lookup :: Ord k =&gt; k -&gt; Map k a -&gt; Maybe a Now if `k` is in map `m` then `lookup k m` will return `Just a` where `a` is the result of the successful lookup, but if it is not in the map then we get `Nothing` instead.
To add on to what /u/Araneidae has said about it indicating an error but without failure, there are some really cool things that come out of this, and it has to do with Monads. You're probably already familiar with the syntax for doing IO, something like this: main :: IO () main = do putStrLn "What is your name?" name &lt;- getLine putStrLn $ "Hello, " ++ name ++ "!" Well, this syntax isn't restricted to just doing IO. IO is an instance of the Monad typeclass, which has the `&gt;&gt;=` and `return` functions (just as the Num typeclass has `+`, `-`, `*` and `negate`). The `&gt;&gt;=` (and a similar function `&gt;&gt;`) are what you're actually using with the `do` syntax, the compiler glues those three statements together with it. But what are `return`, `&gt;&gt;=`, and `&gt;&gt;`? `return` is the easiest one. It takes a normal value and "boxes" it up in whatever monad you're in. If it's IO, `return` has type `a -&gt; IO a`, so it simply takes a value and wraps it in an IO. If it's a different monad, such as Maybe, the obvious implementation is `return a = Just a`, since we're just "boxing" up the value. For `&gt;&gt;=`, or bind, it has the type signature `Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b`. This tells us that we take a boxed up value, then a function that returns a boxed up value, and applies that function to the value inside the first box. A simple example would be onlyEvens :: Int -&gt; Maybe Int onlyEvens x = if even x then Just x else Nothing anotherFunc :: Int -&gt; Maybe Int anotherFunc x = Just (2 * x) example :: Int -&gt; Maybe Int example x = onlyEvens x &gt;&gt;= anotherFunc The first function replaces evens with `Just x` and replaces odds with `Nothing`. The next just takes a value, doubles it, and "boxes" it in a `Just`. The third checks if x is even, then shoves that value into `anotherFunc`. But what happens if `onlyEvens` spits out `Nothing`? The implementation of `&gt;&gt;=` for Maybe says that if you provide `Nothing`, then it just returns `Nothing`, without applying anything to the function. This allows a value of `Nothing` to pass through remaining functions without having to actually do anything. The last function is a lot like `&gt;&gt;=`, except it doesn't pass a value on. It's type signature is `Monad m =&gt; m a -&gt; m b -&gt; m b`. This just takes two boxed up values and returns the second one. Why is this useful? Well, `putStrLn "A Message"` has type `IO ()`, so if you want to print multiple lines, you could do `putStrLn "A Message" &gt;&gt; putStrLn "Another message"` What does this have to do with the do notation? Well, instead of writing `main = putStrLn "A Message" &gt;&gt; putStrLn "Another message"`, you could write it as main = do putStrLn "A Message" -- Used to have a &gt;&gt; here putStrLn "Another message" Why do we care when talking about Maybe? We can use this same do notation for it, and it makes a lot of things easy example x = do anEven &lt;- onlyEvens x -- Unbox the value returned, anEven has type Int nextValue &lt;- anotherFunc anEven return nextValue If `onlyEvens` returns `Nothing`, the entire function will return `Nothing`. Otherwise, it will return a value wrapped in a Just. This becomes very useful later for other monads, and for more complicated expressions using Maybe values.
[Learn You a Haskell](http://learnyouahaskell.com/a-fistful-of-monads#getting-our-feet-wet-with-maybe) has an explanation of Maybe that I found useful.
Awesome! Really appreciate all the work going into the Snap framework and other snaplets! I'm planning on using this snaplet in a project I'm currently working on. thanks.
Yeah we needed it to be able to reload heist templates fast upon change. Will write a blog post demonstrating the setup soon. 
You ever use a sentinel value in another language where you want to indicate that the value is a non-legal value? -1 is commonly used in C for positive ints, for example. "Null" is often used in OO languages to indicate that an object hasn't been created; C# even slightly formalizes these things with the nullable types construct. But regardless of what you use, you have to have something that checks for that sentinel value and breaks out appropriately ("if(someVal == -1) {...}"). Maybe is basically the same concept but on speed. If you have something that returns a Maybe, then that function is basically saying that it'll return a legal value (Just a) or it will return a sentinel value indicating an illegal value (Nothing). However, as I said before, they are sentinel values on speed. Rather than having to check if the return value for you function is Nothing or Just, you can just use fmap. main = do let a = Just 1 let b = Nothing putStrLn (show ((*3) `fmap` a)) putStrLn (show ((*3) `fmap` b)) This little dumb chunk of code will output the following: Just 3 Nothing As you can see, we didn't check that b was Nothing, we just used fmap and it did it for us. Now, this is just the tip of the iceberg. There is a lot of stuff you can do with Maybe that goes beyond that that a lot of other redditors talk about.
So... what is it you don't like about currying, again?
Why be so concerned with what the arity of a function is (under the interpretation that Haskell functions have arity at all)? 
There is something very wrong about the premises this article is assuming without mention, and I can't even get my head around what the author is trying to say in order to formulate a response.
The "alpha takes less than two arguments" case doesn't make sense. With currying, a function that takes an argument and returns an anonymous function is not a new case: it's the same thing as taking two arguments. It doesn't seem like the author understands partial application a lot. This is seen too when he talks about `flip`: the "syntactic ambiguity" he talks about doesn't exist, because `(a-&gt;b-&gt;c) -&gt; (b -&gt; a -&gt; c)` and `(a-&gt;b-&gt;c) -&gt; b -&gt; (a-&gt;c)` are two ways to say the same thing. 
There is no ambiguity here. `alpha`, like all haskell functions, is a function that takes a single argument. Author seems confused about the distinction between currying and partial application.
`tail` is a terrible example for the need for `Maybe`, because it could be implemented as `drop 1` so makes a less compelling case for the need for Maybe. Sure, it'd be ambiguous and have drawbacks, but it's much less compelling than the use of `head`.
&gt; Make partial application *explicit* That's called typing. &gt; Use a grouping operator for function application Everything is already function application.
How would you write, for example, a function to lookup a key in a dictionary and return the associated value? What type would it have? How would you represent the result when a key is not present in the dictionary? Maybe is just the simplest way to represent something with possible failure, some languages use the NULL pointer to represent a similiar idea to Maybe's Nothing, with a non-NULL value representing Maybe's Just.
Great point! I couldn't remember `fromMaybe` offhand, but wanted to talk about `maybe` since it's has the most obvious name.
&gt; This more C-like (or ALGOL-like) notation is a lot more readable to mainstream programmers than the original Haskell syntax. I give thanks every day that Haskell is not a C-like language. More curry on my functions, please.
That's like saying: “Doing sums in your head is difficult indeed. That's why we need better computers that help with adding up.” Learning and practising type inference in your head is the difference between a constant battle with the type-checker and (maybe) actual understanding of the code you're attempting to write. Don't knock it. Also, show your working out (type signatures): you get much better feedback on where you went wrong. 
The key thing the author misses is that the following types are the same type: flip :: (b -&gt; a -&gt; c) -&gt; a -&gt; b -&gt; c flip :: (b -&gt; a -&gt; c) -&gt; (a -&gt; b -&gt; c) flip :: (b -&gt; a -&gt; c) -&gt; (a -&gt; (b -&gt; c)) This is what makes currying so elegant: we can think in terms of all three versions and they are all correct interpretations.
You might want to look into concurrent revisions which is a deterministic concurrency model which is build around such a thing. For haskell, there's a paper http://research.microsoft.com/en-us/projects/revisions/ and implementations are here https://github.com/ekmett/revisions and here www.research.microsoft.com/~daan/hsrevisions.
I want the whole thing to be powerful, but yet of course sub-turing, though I'd already be content with LL(k) or something. Just as a random example, XML DTD doesn't let you say things like "in a `&lt;foo&gt;` element there must be a `&lt;bar&gt;` element xor a `&lt;baz&gt;` element". IIRC the most powerful schema language for XML that I ever came across is [DSD](http://www.brics.dk/DSD/dsd2.html), which lets you write regular expressions over element sequences.
Yes. Inconsistent state can be seen inside a transaction. In the case of the inconsistency leading to an exception, the exception handling code validates before raising the exception. For non-termination, when a thread yields and it is in STM it is validated and aborted if inconsistent. This is covered in the original Composable Memory Transactions paper and in my commentary. Is there a way I could make these points clearer?
I agree. I watched it letting things wash over me. I'll be watching it again a second time, which I imagine will go much more easily now that I know what to expect, minute-by-minute.
Yes there is. Markdown is a markup language, but no one would say it's a data language. Data and markup overlap a lot, but they are not the same thing.
The distinguishing part is actually "data" and "document", not "markup". EBML is a markup language, it says so in its name, but aside from subtitles you won't find much text in matroska containers.
&gt; That's like saying: “Doing sums in your head is difficult indeed. That's why we need better computers that help with adding up.” And? Weren't computers invented for exactly that? But yes, if you are going to write code, you should actually understand it.
I think you've just said what I intentionally left unsaid. My point being that just because we have computers, doesn't mean we don't need to teach kids how to do sums manually. As for writing type signatures: sure, let the compiler infer types for the trivial parts, but otherwise treat it as a type *checker* only. The working out (e.g. signatures for where/let clauses) not only help you distil and make concrete your intentions, it also helps the compiler to give you more informative errors when your intentions are nonsense. The ability to infer types (or run code) in your head really does not hurt here.
Let's make the best of the situation and ask: Is there ever a situation indicated by title? Can we imagine a situation where currying is NOT one-half of an isomorphism?
&gt; `(/ 20)` &gt; This last form uses left-currying, which is obvious since / is a well-known built-in infix operator that requires an unspecified left argument, which would be filled in by currying. * Currying needs to be distinguished from partial application, as others have pointed out. * Writing `(/ 20)` is properly called *taking the right section of the (/) operator* [1]. It can be confusing that `(/ 20)` and `((/) 20)` are inequivalent expressions. * The (/) operator is *built-in* only insofar as it is predefined in the so-called *standard prelude* of the Haskell spec. Moreover, unlike other operators that are built-in at a *deeper* level in other languages, the prelude is suppressible and (/) can be redefined to mean something else. This flexibility is considered the bees' knees in DSL projects. [1] http://www.haskell.org/haskellwiki/Section_of_an_infix_operator 
&gt; The confusing thing for me first coming to Haskell was no one explained that a and c were variables. Yes, they are *type* variables, as opposed to the *term* variable `x` in `\x -&gt; x`. Alpha equivalence takes place at both the term and type levels. System F makes variables at both levels explicit, so a `forall` reveals itself to be a type lambda, indicated by the capitalized greek letter in some contexts. 
I did not release the Haskell version of this FRP system. I may take another pass at it at some point, but it needs a bit more work to be ready for public use. I don't know of any libraries that do concurrent FRP, but it can be implemented in Haskell. I have found that laziness + concurrency is not a great match, but it can be worked around. You also would lose a lot of the portability and simplicity of the graphics part of Elm. I think that is a pretty big deal, and it is a big reason why I did not go with Haskell from the start.
Yeah the laziness thing was mentioned in your paper, and it's obviously an issue to be careful of in Haskell for parallelism/concurrency (hence all the parallel strategies stuff). I'm sure it might miss out on the graphics part, but it seems to me that it would be useful anyway, and it could probably be adapted for other UI stuff. Plus, it would be interesting to see it simply for the sake of curiosity.
I don't agree that imperative Scala has crummy performance. It's the same as Java, roughly par with native code. I also don't see much difference between reading bytecode and reading Core. It's what you're used to (although I consider Core to have some problems even as a low-level representation). Anyway, for the sake of argument let's just say that Core and bytecode are both low level representations that programmers shouldn't have to look at or think about 99.99% of the time. And the point is that I never need to read the bytecode when I write imperative Scala. The compilation model is totally predictable, just like Java and C! I know exactly what will result from the code I write. Haskell's compiler is simply doing way too much, lacks a clear programming model, so even Haskell experts are forced to write code, look at Core, tweak code, repeat, to get high performance code. IMO this is a failure, or how about we just say it's a situation that could be improved. Would you agree with that? In my opinion, Haskell has gone too far down the path of putting intelligence in a _static_ optimizer. As a result, there is a penalty to abstraction in the language (runtime structures are not optimized the same way as static structures) and the way to get good performance in Haskell is to control what gets inlined, which is _just wrong_. The intelligence belongs in the runtime, where it can be consistently applied and form part of the reasoning model. I'll draw an analogy with tail call optimization - in Scala, this optimization is applied, but only in _static_ situations, when a function is calling itself. As soon as you move to higher order code, where functions are being passed around, it's no longer applied. This means I have to reason about the performance of higher order code in a completely different way than first order code. It's the same in Haskell - the optimizer does lots of things, but they are applied statically. If the same structures arise at runtime in higher order code which isn't inlined away, these nontrivial optimizations are not applied. All that said, I still think Haskell is a nicer language overall than Scala. :)
Doing type inference in your head is not a prerequisite to understanding code. I am working on a ~10KLOC Haskell project called [lamdu](https://github.com/Peaker/lamdu), and when I need to change some existing code, I have to "swap in" (to my brain) all the intermediate subexpression types by recomputing the type inference in my head, after I had already done so when originally writing the expression! This is extremely annoying and time consuming. I want my IDE to show me the types of the subexpressions so I don't have to re-do the inference each time I edit the code (and indeed, lamdu is an attempt at an IDE which will do that and much more). Also, when the types get interesting, the amount of working memory I need to have in my head as I do the type inference is pretty large. I *understand* my types very well, but writing a large expression is a tedious experience because I have to keep so many types in my head. When it becomes too much, I often have to temporarily enable ScopedTypeVariables, add explicit foralls, and start annotating tons of sub-expressions manually because I can't keep track of it all purely in my head.
Sure we teach them how to do sums -- but we don't deprive them of calculators because it's important that they understand their arithmetic. They already understand their arithmetic, and now they want to do it efficiently. I understand type inference. I've *implemented type inference*. I don't want to do type inference in my head *every single time*.
[Evaluating cellular automata is comonadic](http://blog.sigfpe.com/2006/12/evaluating-cellular-automata-is.html) may be of interest too.
Type constructor, while accurate terminology, probably isn't useful. Here's the same thing, said in Java terms: Maybe is a generic type. Maybe is almost equivalent to the following Java code: public interface Maybe&lt;A&gt; { // assorted Maybe functions } pubic class Just&lt;A&gt; extends Maybe&lt;A&gt; { A item; ... } public class Nothing&lt;A&gt; extends Maybe&lt;A&gt; { ... } Values in Haskell are non-nullable, which is why you use Maybe - Nothing is isomorphic to null. The reason this is good is because null is only rarely required, and most languages don't give you non-nullable references. Also, null doesn't have a very useful interface in most languages, but we have lots of useful functions for wrangling Maybe values.
Yes I'm using the latest version of the Haskell Platform which includes 32bit GHC. Sorry for the long reply, I just figured out how to get back here (new mail). I have the dumb.
Just for the record: Dan Piponi is a mathematician and he wrote many posts about category theory: http://blog.sigfpe.com/2010/03/partial-ordering-of-some-category.html .
Nice Eli.
Wow. I can understand how the author confused currying and partial application. For instance, in the javascript world those terms are confused. That is forgivable. Also I can understand why he can be confused about partial application. Or the right associativity of the arrow type and the left associativity of function application. These are all errors that people make when learning the language. Fine. But I don't understand why you would lecture a community from point of such ignorance on how they should do things. 
I think we're in a very subjective place here. Like not only am I satisfied with the status quo, but I'm pleasantly surprised by it! I find strictness, unboxing and specialization easy to specify (with an occasional problem), and typically inferred by the compiler better than I expect. And I don't think we have to profile any more than in less lazy languages. This is, I think, because I have a good cost model of non-strictness (or at least think I do). And I don't understand what optimizations we could "always" move to the runtime. I think the analogy with tail calls is very incomplete here. Tco specifies a semantically observable effect. Meanwhile the entire trickiness w/r/t genuine optimizations such as specialization is that they should not ever have a semantically observable effect. If we're willing to specify strictness directly (and this is easy!), this problem goes away. Anyway, we don't need profiling due to our lack of a mental "cost model". We need it because at a certain scale, our locally good cost model becomes bad at predicting which costs will dominate. Nobody writes highly-tuned code everywhere -- determining where to expend that effort is the heart of profiling.
To see another example (with more emphasis on visualization), see gloss-conway from the Gloss examples: http://hackage.haskell.org/package/gloss-examples
 [Position xx yy | xx &lt;- [x-1..x+1], yy &lt;- [y-1..y+1], xx /= x || yy /= y] is even correct. (This is one reason why listing each individual position is bad: it makes it easy to miss that the centre position is absent.)
So much for working once it passes the type checker!
you probably want &amp;&amp; intead of ||
That was my initial reaction too, but think twice and you'll see he's right. Hint: de Morgan's law.
[I also did one about a week ago.](https://gist.github.com/AndrasKovacs/5591393)
Take a minute to try it in ghci.
Ah, now I get it. I think it's clear enough in the paper, I must just have glossed over that part. That's a very clever solution. 
This is of course an implementation close to the definition, thus not necessarily very efficient. What would an implementation of Hashlife look like? Or an implementation that threw array pages into the GPU for calculation? Would those two share a common type class?
 bar (Position 1 2) = Alive bar (Position 2 2) = Alive bar (Position 3 2) = Alive bar (Position x y) = Dead This is kind of like using functions as data structures, right? Is this good practice? How is the performance?
Applicative functors mofos. neighbours pos@(Position x y) = filter (/=pos) $ Position &lt;$&gt; [x-1..x+1] &lt;*&gt; [y-1..y+1]
This implementation uses time exponential in the number of generations: To evaluate `Generation` n at one position, generation n-1 is evaluated at 8 positions. For each of the evaluations, generation n-2 is evaluated at 8 positions, and so on. Some of the 64 positions at the second level are the same, but the results are not shared. You could fix this by memoizing the `Generation` functions. Edit: If we really want to keep the convenience of not having to compute the used size of the board at each generation, we can memoize using a lazy integer trie as provided by [MemoTrie](http://hackage.haskell.org/package/MemoTrie): import Data.MemoTrie memoEvolution :: Generation -&gt; Generation memoEvolution g = let g' = memo (\(x, y) -&gt; evolution g (Position x y)) in \(Position x y) -&gt; g' (x, y) Otherwise, I think an (unboxed) array would be a lot more efficient (I guess one could also combine the two and use a lazy trie of array chunks :)). 
I'm not sure what you mean when you say that you are 'pleasantly surprised' or whether you feel you have a 'good mental cost model'. :) That is too subjective - how about we talk about whether it is in principle possible to do better. And the answer is clearly (to me, anyway) yes. Unboxing and strictness analysis are both controlled by what is inlined, which at least in principle feels like a completely unrelated axis of optimization (and tracing JITs do the right thing here IMO by not optimizing by function), don't you think? What I have in mind is something like changing what it means to call a function, so that, for instance, before passing each argument, which can be passed 'one at a time' in the slow case, we ask the function if it would like the argument strict (and if so, unboxed), or lazily, and this information is propagated in a totally predictable way at runtime. Something like this effectively continues the same static analysis, but at runtime, so that it can be applied consistently even to higher order code that isn't inlined away. Anyway, that is just an idea, but hopefully you can see the *kind* of solution I'm going for. &gt; Tco specifies a semantically observable effect. Meanwhile the entire trickiness w/r/t genuine optimizations such as specialization is that they should not ever have a semantically observable effect. 'Semantically observable'? With respect to what semantics? Let's be more precise - as programmers, we care about more than just what our programs compute - we care about their time and space complexity, and we write programs with the intent of achieving some particular time and space complexity, in addition to considering what we are trying to compute. 'Optimizations' like strictness analysis can affect the asymptotic memory usage of my program, which means I am leaving a critical aspect of my program to be _implicitly_ determined by the optimizer, whose output I now must inspect to ensure it did what I actually intend (perhaps via profiling, perhaps via inspecting Core, whatever). I consider it a problem that this 'optimization' of SA is only applied to first order code and higher order code that is inlined away. To me, this breaks the 'black box' abstraction of functions. What I want is something totally predictable. The 'optimizer' should do silly things like convert `x * 2` to bit shifting and other things that give me a few percentage points here and there. I should be getting some small constant factors out of optimizations, not completely different time or space complexity. Unboxing and specialization is arguable, too: even though it is just a constant factor, in some cases it is such a big constant factor (the difference between a loop requiring zero allocation and GC and one that does not) that it would be nice if there were a more consistent model for when it is applied to runtime structures. 
Learn Haskell Fast and Hard http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/
One of the biggest differences between ML and Haskell is type classes. I recommend Wadler and Blott, the canonical reference.
Ok so the argument is the mvar operation itself is fine -- the issue is that scheduling jumps between lots of threads costs time regardless?
That was *really* neat. And I sort of wish I understood it.
Scala syntax, but the concepts transfer to Haskell, where they originate anyway.
I think `flip` is often a code smell, so what originally triggered the author wasn't necessarily beautiful code.
What about http://hackage.haskell.org/package/set-monad?
Just so I'm sure. It took so long to figure out that you were doing round trips because the abstraction of MVars hides what happens at tread memory level, right? And focussing on details like allocation made you overlook that?
Huet wrote "The Zipper" in ML. I wrote my original type derivative code using LEGO.
Well, I'm a little embarrassed to admit that by the time I had heard about the figure, we had in fact lost the original benchmark code, and, not convinced that I could reproduce it in a correct way, I decided to read the code being instrumented instead.
I don't believe what I'm reading.
I can barely comprehend Hinze's paper, but Huet's is simple enough. McBride's (= pigworker's) zippers are also very nice, and as usual, his papers are infinitely more enjoyable to read than anything else on the subject.
I think the easist way is to use the "mother of all monads" construction newtype SetM a = SetM (forall r. Ord r =&gt; (a -&gt; Set r) -&gt; Set r) which is trivially a monad (its isomorphic to `forall r. Ord r =&gt; Cont (Set r) a`) and allows an injection function inject :: Ord a =&gt; Set a -&gt; SetM a inject s = SetM $ setBind s where setBind :: (Ord a, Ord b) =&gt; Set a -&gt; (a -&gt; Set b) -&gt; Set b setBind x f = unions . map f . toList $ x you also have runSetM :: Ord a =&gt; SetM a -&gt; Set a runSetM (SetM f) = f singleton the end.
Please feel free to correct me! I definitely have seen "weaving a web" cited as a generalization of huet in plenty of literature.
JMacro-RPC (which I've failed to really promote) also provides a server and uses aeson to translate between Haskell and JavaScript types :-)
Constraint kinds? {-# LANGUAGE ConstraintKinds, TypeFamilies #-} import Prelude hiding (Monad (..)) import qualified Data.Set as S import GHC.Exts (Constraint) class Monad m where type MConstraint m :: * -&gt; Constraint (&gt;&gt;=) :: (MConstraint m a, MConstraint m b) =&gt; m a -&gt; (a -&gt; m b) -&gt; m b return :: (MConstraint m a) =&gt; a -&gt; m a instance Monad S.Set where type MConstraint S.Set = Ord m &gt;&gt;= k = S.unions $ map k (S.toList m) return x = S.fromList [x]
fixed
Oh dear. Apologies for coming off of as a 'not-invented-here' type! I also acknowledged the inspiration, noting that hinze built on huet. I recognize the slides/initial post didn't, and I endorse your correction as a friendly amendment. I just wanted to make a friendly amendment to your friendly amendment, noting that a key piece of the zipper story _was_ conducted in Haskell -- i.e. zippers weren't purely external to Haskell in their development. If my post came off as otherwise, I'm quite sorry.
&gt;When it becomes too much, I often have to temporarily enable ScopedTypeVariables, add explicit foralls, and start annotating tons of sub-expressions manually because I can't keep track of it all purely in my head. At this point I usually take the hint and make a separate function, or at least a type-annotated `where` clause. But I agree that subexpression typing would be a great IDE feature.
Anyway, if you want more people to feel left in the dust as dependently typed languages stretch away, I'm anxiously awaiting epigram :-P
This provides a broad overview of the current approaches, as well as a "universal" approach that solves the `Set` problem and a variety of related ones: http://www.ittc.ku.edu/csdl/fpg/files/Sculthorpe-13-ConstrainedMonad.pdf
It's always nice to see a descriptive commit message.
Epigram? Good news, Epigram 1 has been out for years! Oh you mean Epigram 2? :(
It looked like you were compounding the false claim that these concepts originate in Haskell by the suggestion to give greater weight to the more local, later account. I'm willing to believe that's not what you intended.
I can only advise you not to and apologise for your anxiety.
It would be really nice if these web slideshows would warn you in advance that you're about to add fifty-odd items to your browser history and the "back" button will be largely useless to get back to where you started.
Is there a way to allow n, rather than 1, constraints?
Perhaps it's simple if you already know Ruby and Sinatra. For example, I didn't find any documentation for the syntax of routes, other than a link to the Sinatra site. If I have to learn a whole new programming language, this is already starting to sound not so simple to me. Also - what is the type of `get`? The README.md on github points me to the file `examples/basic.hs`. There, most of the examples start with something like get "/some/route" $ do ... But then the last example says: get "/lambda/:foo/:bar/:baz" $ \ foo bar baz -&gt; do ... Oh noes! Is this one of those libraries that tries to imitate dynamically typed languages using all kinds of horrible type system hacks? If so, then this isn't simple at all, in my opinion.
Straightforwardly? Unsure; these areas of Haskell are still a bit of a black box to me. I thought a simple constraint tuple would do the trick, but I can’t finagle it to work: type MConstraint S.Set = forall a. (Eq a, Ord a) So I hope a better mind than I steps in here.
[It's just syntactic sugar.](http://hackage.haskell.org/packages/archive/scotty/0.4.6/doc/html/Web-Scotty.html#g:4)
I'd say instead that there is no difference between the cases - alpha beta gamma can only possibly be well typed if alpha has a type of the form (b -&gt; c -&gt; d), where b and c are the types assigned to beta and gamma. Then the whole expression has type d, and returns whatever you get by applying alpha twice. This is true if d isn't a function type (case 1), if d is a function type (case 2), or if you decide to parenthesise the type as (b -&gt; (c -&gt;d)) (case 3).
Only necessary for programming if you play with the likes of unsafeCoerce, but there is some interesting stuff about arity in GHC: http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution/FunctionCalls I think the arity of a function might also be greater than you would guess from the type, if f :: A -&gt; B -&gt; C, and C is a newtype.
&gt; I'm willing to believe that's not what you intended. Then you might consider offering corrections more gently? Thereby avoiding abrasion, fractious discussion, comebacks (say, about the progress of a participant’s pet project). I don’t really know sciv from Adam, but not everyone here is an academic and some confusion as to proper citation is inevitable.
This operational notion of arity also plays a role in whether GHC will inline the function (it has to be saturated), etc. C could even be polymorphic, in which case the arity (from the point of view that functions in haskell have arity at all) varies with the call site. 
You are right about the syntax of routes, it drew my attention from the first moment using an string with an special syntax to capture variables. Not what I am used to in Haskell, but I see it just as a small quirk, the rest of Scotty seems fine. About the get, well, the doc explains everything.
Just focus on this part: Position &lt;$&gt; [x-1..x+1] &lt;*&gt; [y-1..y+1] You can think of it as saying "Apply the Position constructor to one of these x values and one of these y values". It then returns a list of Positions using all possible permutations of x and y. The above code uses the Applicative instance for lists. For things that are both Applicatives and Monads (like lists) you can translate the Applicative style to a monadic style (but not necessarily the other way around): do currX &lt;- [x-1..x+1] curry &lt;- [y-1..y+1] return (Position currX curry) In fact, this is equivalent to what you were doing. List comprehension are syntactic sugar for the list monad.
&gt; horrible type system hacks It's just a type class.
that sounds great! a a sidenote: i'm secretly thinking about developing a php-bridge, because i am kind of stuck in php codebases.
Seen hubris?
`&lt;!ELEMENT foo (bar | baz)&gt;` DTD content specifications are regular expressions. In fact, much of the early theory of regular expressions was developed in the context of SGML. The syntax of attribute values that can be specified in a DTD is very limited, but that is extensible using NOTATION. What DTD does not let you do though is specify the syntax of PCDATA content inside an element. That is what languages like DSD and XMLSchema add.
Hard to do, since Scotty is older than that standards document. :-) I'm actually not a huge fan of the string routes, and would like to move to either a combinator language or a quasiquoter, but I haven't been using Scotty much lately so haven't gotten around to it. Well-reasoned patches always welcome!
Here's a highly parallelised version of Game of Life using REPA 3 and gloss-raster than I prototyped with the primary author of the falling-turnip project. https://github.com/tranma/falling-turnip/blob/f00ebc4da8461295f30c56321bb16aac079a1696/Main.hs We later managed to completely specify a falling-sand style physics simulation as a (rather intricate) cellular automata, which means the exact same parallelism is possible. This leads to the falling-turnip game we developed: https://github.com/tranma/falling-turnip/ Playing with it is pretty fun! Not bad for a weeks work. 
Yeah, I guess it's mostly the specific interaction between `&lt;$&gt;` and `&lt;*&gt;` that allows them to apply multiple arguments that's a mystery to me. Only `fmap` I do understand.
I'd say if you've been taught ML reasonably well, you'll find the basics of Haskell relatively easy to pick up. However, there are quite a few more intermediate concepts that get used in practical code. Here's a quick list of new features you'll have to learn: * Lazy evaluation. While it takes a little getting used to the possibilities this allows, I find that in practice I only ever use lazy behaviour in very limited circumstances. However, you might see slightly strange performance characteristics if you assume eager evaluation, so it's important to keep this in mind. * Type classes. I believe ML has a concept of "equality types", which are a vastly more limited special case, but might give a good intuition for the more general technique. * Higher kinded data types. ML has algebraic data types which should provide a reasonable basis to introduce this concept. In short, type constructors can use the same curried pattern as functions, and they can also be passed as arguments to other constructors. * Monads. A pretty pervasive part of idiomatic Haskell, and of course necessary for IO (although you can do basic things without a fundamental understanding). I think a lot of the difficulties people have with the concept is that you really need to understand type classes, and higher kinded data types in order to understand monads. (EDIT: I should probably expand this to include the various other higher kinded type classes, like `Functor` and `Applicative`, not least because understanding them first is one of the better ways to learn about `Monad`. On this topic, I'd check out the relevant sections from [Learn You A Haskell](http://learnyouahaskell.com/functors-applicative-functors-and-monoids).)
&gt;Isn't it painfully obvious that any framework won't be simple unless you know the language to write code against said framework? He does. The issue is that he needs to learn other stuff, which is not documented. This was quite clear from his post, you should try reading it.
Simple implementation with sets and SDL: http://normalorder.blogspot.com/2011/05/life-reincarnated.html
Are you more curious about how they work for lists specifically or how they work in general?
&gt; Hard to do, since Scotty is older than that standards document. :-) I hadn't seen it was so recent, oops :D &gt; I'm actually not a huge fan of the string routes, and would like to move to either a combinator language or a quasiquoter, but I haven't been using Scotty much lately so haven't gotten around to it. Yeah, a combinator language is good too. In fact I'm myself working on a [web framework in F#](http://websharper.com/docs/working-with-sitelets) which uses combinators for routes. Although you actually rarely use them directly, since we can generate the routes from an ADT using reflection.
HS Colour? The [hscolour](http://hackage.haskell.org/package/hscolour) package is for colorizing Haskell source code, I don't think you mean that. Perhaps Russell O'Connor's [colour](http://hackage.haskell.org/package/colour) package? Or Andrew Coppin's [AC-Colour](http://hackage.haskell.org/package/AC-Colour) package?
Thanks for the clarify! I learned the concepts in haskell land from haskell folks, which is why I wrote the above.
[Learn You a Haskell](http://learnyouahaskell.com) is a good choice, even for someone with your background. You'll just be able to skim through it faster than normal.
Is this one of those instances when "both" isn't a valid answer?
Not sure if this is a good idea, but one could do: {-# LANGUAGE ConstraintKinds, TypeFamilies, MultiParamTypeClasses, UndecidableInstances, FlexibleInstances #-} ... class (c1 a, c2 a) =&gt; And c1 c2 a instance (c1 a, c2 a) =&gt; And c1 c2 a instance Monad S.Set where type MConstraint S.Set = And Num Ord ... 
Feel free to skip around tutorial sections. I'd recommend [Real World Haskell](http://book.realworldhaskell.org/) by O'Reilly. The online version is free.
You are joking right? His complaint is that he needs to learn a totally different language, explicitly NOT "the language to write code against said framework". He isn't complaining that he needs to learn haskell. Your response makes absolutely no sense.
Sorry if this appears to be drive-by criticism, but the type synonyms in https://github.com/ixmatus/prizm/blob/master/src/Data/Prizm/Types.hs are a code smell to me. If you replace them with newtypes, then you'd be able to check, in compile-time, that you're not using a transformation from RGB to XYZ instead of a XYZRGB by accident. For example, in https://github.com/ixmatus/prizm/blob/master/src/Data/Prizm/Color/SRGB.hs toXYZMatrix :: RGBtoXYZ -&gt; RGB Integer -&gt; CIEXYZ Double In your version of the code, you can actually pass a XYZtoRGB value, and everything will typecheck. The type system is your friend!
I'm pretty sympathetic to this idea... but in this case, I find that a very difficult argument to make. The difference in utility between finding errors at compile time versus at application startup - since we're talking about string literals in the routing specification - isn't really particularly important. I can't think of a strong reason to prefer one over the other; and indeed, as a weak reason, I think I prefer the application being able to provide intelligent error messages over what you'd get by trying to encode this into the type system. As far as type safety issues go, I'm much more concerned with the `Action` type class that lets you mix arbitrary route strings with functions that may take the wrong number of parameters, and that really won't fail until that specific route is actually used.
I believe you can just use tuples of constraints.
Do you understand how the list `Monad` instance works? The reason I ask is that it's very easy to explain the `Applicative` instance for lists in terms of the `Monad` instance.
gloss: http://gloss.ouroborus.net 
Took me a second - I like it. As a bonus, it's symmetrical!
I only need bind fmap sequence compose?
 Is there a video or transcript of this talk?
Agree this would be a good thing to do.
Given that the type of the function must match the value of the string on the left side, this type-class is a horrible type system hack...
&gt; Can we get TypeSafe, FPComplete, &amp; friends to put some muscle behind this? Sure, who's paying?
oh there we go. haskell died right here
I thought the Haskell Platform was a sneaky way to do an end run around Haskell Prime?
I don't think typesafe will jump on bandwagon http://corp.galois.com/programming-languages-research http://www.well-typed.com/
Posting just so I can downvote it.
I think any company that's not putting useful for the entire community patches into ghc and/or the core ecosystem probably shouldn't be involved in a standards process. Type safe is the scala company so they definitely shouldn't. To make things happen, all it really takes Is making the time to help out. That's what I'm doing and so can you! (I'm even building a Haskell backed tech biz. Remains to be seen how well that bit works out though. )
Cross-commenting from G+: Haskell has some shortcomings in terms of third-party libraries, but that can be fixed. IMO even having to fix it (i.e. write your own libraries / bindings) is less hassle than be forced to write in Python where you can't even express half the ideas cleanly. 
It doesn't get much more basic than [SDL](http://www.haskell.org/haskellwiki/SDL).
So yeah, arity is not enforced. That's ugly.
How much money would this take? Don't get hopeful - I'm just a programmer, and long-term unemployed due to disability, definitely not a generous millionaire. I'm just curious. Are we talking a few peoples time for a few weeks collaborating online, or are we talking months of debating, organizing huge meetings with international travel etc or what? 
They do different things. The platform is about bundling a stable version of GHC with a selection of stable, widely used libraries. Haskell' is about changing Haskell the language itself and writing a new language standard.
Yes, I do.
I would conjecture that part of the problem is that writing up specification documents requires that one have a solid background in theoretical computer science in order to be able to correctly and precisely specify the semantics, though I would be welcomed to be proven wrong here. :-)
Are there efficiency implications? I remember that LANGUAGE MonadComprehensions has a side effect of making ordinary list comprehensions slower. Would moving Foldable and Traversable into Prelude similarly make ordinary list use of mapM etc slower?
Same here.. I was willing to fill it out, until I found out the authors is using "fuck" like he's Quintin Tarantino.
Yes, it would. For example, `Data.Foldable.for_` slower than `Control.Monad.forM_`. But this is mostly fixable. You just need to add `{-# INLINE traverse_ #-}` pragma to Foldable.hs and so on.
Hi there. I am T. Morris. There is a talk and workshop associated with these slides for the LambdaJam 2013 conference held in Brisbane last week. There was a video camera in both the talk and workshop, so I would expect it to be published some time soon. Keep an eye out.
When you want to return a list that has a maximum length of 1. 
Yeah, no, I'll wait until that is rephrased too.
I guess not. That's a good example. I was mostly referring to ghci's use of t, t1, t2, etc. As far as I can tell ghci outputs the original type signatures from library sources, which is probably fine, but seems to use t, t1, etc for user functions defined inside it. I am not sure what it does for locally defined typeclasses (if that is possible). The local ghci case is probably less important anyway.
This list is a function of n, it's a pure function that takes an integer n and returns a list of integers from 1 to n.
It's kinda like a function where x is the parameter. X is not be modified while others have references but just being called with a different value. It was probably inspired by [Set builder notation](http://en.wikipedia.org/wiki/Set-builder_notation)
&gt; where x is the parameter Is this used in places other than list comprehensions? I would love to have some examples, because without them I still can't understand why list comprehensions are considered functional.
Concerning SDL, you can check out the snake implementation: https://github.com/CGenie/haskell-snake
[Read what the Haskell 2010 Report has to say about list comprehensions.](http://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-420003.11) Then try to translate your list comprehension according to the translation rules.
As I've said elsewhere I think the "problem" here is on the demand side. Updating the standard is hard and boring work, and no one has enough of a material interest in it (as opposed to wanting to have a standard for sentimental reasons) to feel like doing the work. All existing programs work well enough with GHC as the de-facto standard, and there's no pressing issues of vendor incompatibilities that would need a standard to resolve them. The flipside is that time and money aren't free, and it's entirely possible that whatever the uninterested parties are choosing to do with their time and money *is more beneficial to the Haskell community* than it would be if those resources were allocated toward working on the standard instead. After all, presumably they are making their choices for reasons. It would be nice to have FPComplete's perspective on this for example, given that they explicitly have growth of the Haskell ecosystem as one of their goals.
Just a daily reminder that while Scotty is really nice it's not the most "minimal" framework, you can still write minimalistic code in Snap or Happstack
I really like [JuicyPixels](http://hackage.haskell.org/package/JuicyPixels). It has a concrete domain and clean API, very much fun to play with!
Would be nice to see more than a black screen in Firefox.
(And now for some comments since discovering it works on Chromium) &gt; Leading causes of dependency rage: ... Dependencies that throw exceptions OMG yes library writers, please do not throw exceptions. 
Which version of firefox are you using? It shows up fine for me on firefox 21. Or do you have noscript on? The presentation is reveal.js, so it does kind of need javascript. 
It sounds like you're in the old "functional languages have immutable variables" mindset, which is kind of true but kind of not. Remember, variables in purely functional languages are just names used to tag things for the duration of some substitution process. They're not places to store values, they're just tags so you know what to do with which values.
...any chance we might also get the audio that goes along with the slides? :-)
None. You explicitly import the functions you want (or the whole prelude, if you wish).
Both of those! Nothing hurts more than finding a library only exposes an IO interface. Suddenly your entire codebase is unpredictable.
Is `+RTS -xc` insufficient for stack dumps on crash? I've never used it in anger....
Also, the whole `parseJSON (Object o) = ...` pattern has been abstracted in the library using `withObject`.
It's supposed to be a joke. You will know this if you complete it.
Coming from ML, you should be able to get a quick start on Haskell 98 with the "Gentle Introduction" http://www.haskell.org/tutorial/
I understand that, but I don't think that analogy will answer my question about why 'x' takes on the different values. (Now I know that list comprehensions are syntactic sugar.)
Doesn't work for me using Mozilla Iceweasel 10.0.12 (the version in Debian Wheezy). I can see a snazzy yellow line at the bottom though, it moves when I press ← and →. I don't use NoScript anymore but do have Ghostery.
The main problm with using a CPS'd set is that it does an explosive amount of work when you use the regular (&gt;&gt;=).
The issue with using a free monad or a codensity-based approach is that you get no deduplication until the end of the calculation. You get a full Monad in the interim, but in exchange you basically are working in the list monad until the end.
[http://en.wikipedia.org/wiki/Firefox_release_history](http://en.wikipedia.org/wiki/Firefox_release_history) 10.x is an "Extended Support Release" that was kept stable and secure from January 2011 to January 2013, which must correspond to the Wheezy freeze period. (The next ESR release, 17.x, started in November 2012 and will be maintained upto December 2013). This means that you got no major new feature since beginning of 2011. That is quite a long time in this world of fast-moving browser featureset, and it is no surprise that some fancy stuff doesn't work. Debian testing doesn't quite keep up with the fast release rate of Firefox/Iceweasel releases, so it makes sense to use the sid/unstable mirror for this package (17.x, with 21.x in experimental) -- or bare Firefox from Mozilla.
Consider a 'Set Bool'. That has at most 2 values in it, right? Because you aren't collapsing possibilities, you wind up repeating work for the same step on the same value. This leads to up to 2^n work instead 2*n over n steps. In particular using the codensity variant non-determinism monad for more than a few steps tends to just explode. It is an interesting theoretical construction. I even use a variant of it in my algebra package to make a monad out of covectors. It just doesn't scale to any sort of computation of even medium length.
unsafeCoerce, really?
&gt; I'd be really interested to know more about Haskell theory, and why things were made the way they are. This is the best reference I know of for such purposes: http://research.microsoft.com/en-us/um/people/simonpj/Papers/history-of-haskell/history.pdf
Thanks!
You're welcome! Enjoy learning about Haskell. It's fun :)
Yeah, I'm aware that I'm using a *rather old* version and cannot complain too much about the site not working ideally just for me. On the other hand I'm a bit surprised that it does not degrade in a way that at least makes the content marginally readable or indicating the cause of the problem. FWIW, the [reveal-js](http://lab.hakim.se/reveal-js/) demo works fine in my ancient Iceweasel, indeed it looks quite decent in Lynx (although some slides appear to be missing). Anyway, I could see the Haskell in Production slide show in Midori so I'm OK.
Gloss really is the right answer here.
There's things like [basic-prelude](http://hackage.haskell.org/package/basic-prelude) and [classy-prelude](http://hackage.haskell.org/package/classy-prelude) which you might want to use instead of the standard Prelude. basic-prelude for example, defines map = fmap: map :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
You have the type signature: numbefier :: Int -&gt; [Int] So the compiler expects the return value to be a list of `Int`s. However it looks like it is returning a `string`, same as `[char]`, That is what the error message is trying to tell you: Couldn't match type `Int' with `[Char]' Expected type: [String] Actual type: [Int] In the return type of a call of `numbefier' In your numbefier function you are wrapping everything in the `show` function which will return a string: numbefier number base= show( (h ... I hope that helps you over this bit of frustration. The GHC compiler should definitely be taught to communicate better, at some point, until then it you might want to try Helium: http://www.cs.uu.nl/wiki/bin/view/Helium/WebHome I have heard it has beter error messages and is at least targeted at those learning Haskell.
As a starting point: 1) `numbefier` has a type signature which indicates that it takes a single `Int` and returns an `[Int]`. But then you are pattern matching on two arguments. 2) in the second case you call `show` which is going to return a `String`, aka `[Char]`. But you are supposed to be returning an `[Int]`.
I think last time I used it, it segfaulted my program (and not in a good way). Don't know if it has any performance penalty, when it does work.
There are a one or two other problems that I would be happy help with, but usually people learn best by applying effort and I do not want to take this chance to do so way from you with giving you the opportunity to do so.
[It started 21 days ago.](http://www.reddit.com/r/haskell/comments/1dhbjr/haskell_2014_committee_has_now_been_formed/)
Note also that GHC optimizes list comprehensions more than the list monad.
In `[x|x&lt;-[1..]]` and `map (\x-&gt;x) [1..]`, `x` is a [variable](http://en.wikipedia.org/wiki/Variable_%28mathematics%29), because its value _varies_ with each instantation of the scope. That's all.
Think of it like this. If you called - map f [1..n] it is the same as [ f x | x &lt;- [1..n] ] in either one x eventually represents each item in the list. But on the left hand of the pipe symbol is a function that gets invoked for each element in the list to generate the new item in the corresponding list. This is just one example but hopefully it helps.
Then let's start with the case of an `Applicative` that has a monad instance. If you convert the definitions of `(&lt;$&gt;)` and `(&lt;*&gt;)` to monadic style, you get: f &lt;$&gt; mx = do x &lt;- mx return (f x) mf &lt;*&gt; mx = do f &lt;- mf x &lt;- mx return (f x) So if you do something like: f0 &lt;$&gt; mx &lt;*&gt; my &lt;*&gt; mz ... it groups like this: ((f0 &lt;$&gt; mx) &lt;*&gt; my) &lt;*&gt; mz ... so if you implement that monadically you get: do f2 &lt;- do f1 &lt;- do x &lt;- mx return (f0 x) y &lt;- my return (f1 y) z &lt;- mz return (f2 z) Because of the associativity law for monads, this is equivalent to: do x &lt;- mx f1 &lt;- return (f0 x) y &lt;- my f2 &lt;- return (f1 y) z &lt;- mz return (f2 z) ... and because of the monad left identity law, this further reduces to: do x &lt;- mx let f1 = f0 x y &lt;- my let f2 = f1 y z &lt;- mz return (f3 z) ... and we can further simplify that to: do x &lt;- mx y &lt;- my z &lt;- mz return (f0 x y z) So in other words, whenever you see: f0 &lt;$&gt; mx &lt;*&gt; my &lt;*&gt; mz ... you can think of that as running `mx`, `my`, and `mz` and then applying `f` to all of their results. Of course, that reasoning only works if you are using something that is also a `Monad`, but you can still reason about things that are only `Applicative`s in a similar way. Let's just look at the types: f0 :: A -&gt; B -&gt; C -&gt; D mx :: (Applicative f) =&gt; f A my :: (Applicative f) =&gt; f B mz :: (Applicative f) =&gt; f C f0 &lt;$&gt; mx :: (Applicative f) =&gt; f (B -&gt; C -&gt; D) (f0 &lt;$&gt; mx) &lt;*&gt; my :: (Applicative f) =&gt; f (C -&gt; D) ((f0 &lt;$&gt; mx) &lt;*&gt; my) &lt;*&gt; mz :: (Applicative f) =&gt; f D In other words, we just keep successively applying the wrapped function using `(&lt;*&gt;)` until it's no longer a function.
As far as kool-aid goes, "solid standard" is much kooler.
Example, please?
Wow, that makes sense! Thanks!
Or you can do it with a single pointfree function: import Control.Arrow numberfier = reverse . map (read . uncurry (flip (:)) . (flip replicate '0' . (subtract 1) *** id)) . zip [1..] . reverse . show :: Int -&gt; [Int] 
"functional programming" is an extremely vague term that has a lot of meanings. &gt; isn't x taking different values from 1 to n? Isn't that exactly what functional programming stays away from? No, you are thinking of *pointfree* programming, where you avoid referring to *points*; in other words, you avoid giving names to values. List comprehensions are *not* good pointfree programming. (But who says that pointfree programming was ideal in the first place?) List comprehensions are *declarative*, *non-side-effecting expressions*, another concept that is connected to "functional programming." You'll hear people use the term *purely* functional programming, which generally refers to no untracked side effects and immutable values. tl;dr avoid the term "functional programming," it has become so vague that it is rather meaningless.
true. But if all you want is some do notation I don't think this is much of a problem.
A terse, but inefficient solution: numberfier :: Int -&gt; [Int] numberfier 0 = [] numberfier x = map (*10) (numberfier (x `div` 10)) ++ [x `mod` 10] I haven't checked its handling of edge cases against the original though. 
While I trust it probably works, to someone who has never seen Haskell before, including myself, it looks like gibberish lol
Lol, when I read in the runtime stack "Z" for a moment I thought about Chris' [Z](https://github.com/chrisdone/z) experiment, and I said "are they crazy"? :D Then I realised was something different :P
I was very turned off by classy-prelude, but basic-prelude is basically my import list on 90% of modules!
Do I understand correctly that you use Yesod with Postgress and Riak? Nice stack! 
Respect for their open source attitude: http://apiengine.github.io/
While we're all showing answers: numberfier :: Int -&gt; [Int] numberfier = foldr (\c xs -&gt; read [c] * 10 ^ length xs : xs) [] . show Fails horribly on negative numbers, of course.
Just to reply a bit shorter than cdsmith, if it's a tag, then who cares if I "reuse" it? Indeed, who says I'm really "reusing" anything at all? I can use a completely different tag, and as long as it ends up doing the same thing -- ensuring that some value get's substituted into the right place -- then the tag itself doesn't matter. So the answer is: because 'x' isn't taking on *any values at all*.
It only matters if you want to use said do notation for more than 3-4 lines.
Most Haskellers will yell at anyone who put that kind of code in :) I tend to abuse point-free myself, but probably not as bad as that..
Here's a tail recursive, numerical solution: f x = reverse $ let l = (length $ show x) in f' x l l where f' x 0 o = [] f' x b o = (x `mod` 10) * 10^(o-b) : f' (x `div` 10) (b-1) o
The number of digits in a number is floor ((log10 n) + 1). Constant time numDig instead of log time numDig
I went through [these notes] (http://www.seas.upenn.edu/~cis194/lectures.html) plus all the homework, from a course by Brent Yorgey. It's at around LYAH level but with exercises. I was doing little bits after work and it took less than a week to get through (I'm already familiar with Haskell, I was just seeing what it was like before I started recommending it), so it might be good for a fast dash at Haskell if you already know ML. I'd already been through pretty much all of the suggested reading previously, so that might have sped things - I'm recommending the reading to FP newcomers, you might be able to get away with trying the notes + homework first and returning to the suggested reading later or if you find that you're not grokking some concept. They also cover the features that Aninhumer listed, which I heartily second as important.
Umm, is there a good way to segfault your program?
Well the intent wasn't to lecture. I was expressing my confusion on how to read this syntax in a reliable fashion. It looks like I at least succeeded in that. Tone is remarkably hard to convey over the internet. I imagine the part that came across as lecturing was the "Reducing the Ambiguity" section. I was grabbing at potential ways to make the syntax more explicit in my mind, but you might have read it is as "I think this is the way that things should be done instead (and I know better)!". If I didn't have that section and just stopped at the Summary section, do you think your impression of the article would be different? **Edit:** I've added a "Purpose" box to the top of the article, which hopefully should make my intent more clear.
#2 is more correct. See the Common Lisp standardization process for one path that a standardization can take.
&gt; he tries to do the type inference in his head and comes to a faulty conclusion since the actual behavior doesn't match his mental model. Correct. Although I am aware of using :t in ghci to inspect types, I am not accustomed to using tooling to examine simple subexpressions when reading through code in most languages. Is this something you do often when reading Haskell?
&gt; When I've read other explanations, they speak of using applicatives to model some kind of "actions" but where you don't need to branch based on previous values (because then you have a monad.) The idea is that a monad lets you do something like this: do b &lt;- someAction0 if b then someAction1 else someAction2 With an Applicative-only interface, you can't do that. However, in many cases this is actually a feature: by exposing a weaker and less flexible API to the end user the author of the Applicative type can often do more work under the hood. I highly recommend you read the [original paper on Applicatives](http://www.soi.city.ac.uk/~ross/papers/Applicative.html), which gives a very clear and accessible presentation of the theory behind them. The basic idea is that the Applicative operators are essentially equivalent to the following alternative class: class (Functor f) =&gt; Monoidal f where unit :: () -&gt; f () mult :: (f a, f b) -&gt; f (a, b) So applicatives are just generalized monoids, where `unit` generates a functor-wrapped value out of thin air and `mult` squishes two functor-wrapped values into one.
&gt; I am not accustomed to using tooling to examine simple subexpressions when reading through code in most languages. Is this something you do often when reading Haskell? If the simple subexpressions involve functions I'm not previously familiar with then I often hoogle them or using something like [ghc-mod](http://www.mew.org/~kazu/proj/ghc-mod/en/) in my editor. When I do make a mistake the type checker usually gives me a pretty good idea what's wrong and how to fix it. I think the "syntactic ambiguity" you speak of is pretty natural coming from C like languages, it took me a while to figure the precedence/associativity rules especially when reading Haskell that is written in the pointfree style.
With the digits package: import Data.Digits solution = reverse . zipWith (*) (iterate (10*) 1) . reverse . digits 10
It makes it easy to create expressions that are difficult to typecheck visually.
I mostly just objected to calling labeled values as variables, since they don't vary at all. They only go out of scope.
Well it's a terminology that predates imperative programming entirely, so I don't know what you want. :P
Yeah, thanks, I guess I couldn't get this right immediately, so chose the simplest path :)
At least it is not like with C where it would have mutable global memory and you have to read all the source to still not know how it is supposed to work in selected cases.
&gt; OMG yes library writers, please do not throw exceptions. Since we aren't programming in Agda, many of our functions cannot avoid being partial in the sense of having preconditions on the input that cannot be enforced at compile time. Given this, when the preconditions of a function are not met (assuming that they are being checked), how would you recommend the library writer handle the situation *without* using something like exceptions?
Slides? What slides? In latest stable Chrome on Linux, this is a static page.
I will take some timn for the paper. Thanks a bunch for your time.
I think you need to reverse again at the end. I always viewed going via `Show` to be somewhat of a kludge. Mine even wins at golf if you count the fact that yours requires a type signature: numbifier = reverse . zipWith (*) (map (10 ^) [0..]) . map (`mod` 10) . takeWhile (&gt; 0) . iterate (`div` 10)
Much better than pure code throwing exceptions.
No. Unlike I/O which you can't (safely) escape, you can just pattern match or Either and Maybe (or use the various utility functions that safely extract a result) at any point in the middle of a pure calculation. In contrast, exception can only be handled in I/O, so using exceptions forces all client code that wants to react to errors to live in the I/O monad. Another point is that Maybe and Either are self-documenting, whereas exceptions can be hidden in the implementation and you need to either read the whole source code or rely on the documentation to cover every possible exception. 
The code that links the functions together has to live in those monads. The *implementations* of the other functions that are linked together don't have to. combined = pureFunction' &lt;=&lt; possiblyFails where pureFunction' = return . pureFunction 
It's not clear to me whether a strict Haskell would be more or less pleasant. I think it would be worthwhile producing a simple strict Haskell to see if such a think would be nice to use. (Disciple doesn't count. It's too different.) I guess this could be done by automatically adding strictness everywhere but I haven't though about it hard enough to get a good sense of how much work this would take.
First trivial example I can think of: sum $ [ x | x &lt;- [1..1000000::Int] , mod x 2 == 0] and sum $ do x &lt;- [1..1000000::Int] guard $ mod x 2 == 0 return x List comprehensions are subject to fusion, list monads… I haven't seen documented anywhere.
I'll give you that any function that doesn't rely on that function need not love in the monad, but any function that does rely on it does need to live in the monad, and if a significant fraction of your program relies on that function then a significant fraction of your program will need to live in that monad; it contaminates everything that it touches.
So giving up the major advantage of Haskell, which is that we can write pure functions rather than having side-effects taint everything, by putting *everything* into a monad is *really* better than just throwing an exception?
Err yes, because if it throws exceptions it isn't pure. 
Monads are pure, you don't give up any advantage. If you have such a partial functions which you cant deal with locally, maybe you have the data structures wrong? Since exceptions can only be caught in IO, I would prefer to have a Maybe or Either monad instead of IO. 
Sure, but once you pattern match and discover that you made a mistake somewhere which you have no idea how to correct (as if you did then you would have prevented yourself from being in this situation in the first place), what exactly are you going to do then? Most likely you are going to terminate the program with an error message because, again, you have no idea how to recover from the situation. The other major alternative is that you reboot the thread with the computation and hope that the problem does not come up again ala Erlang style, but this can only make a difference in the I/O monad as if you are in pure code then running it a second time will return the same result. Thus, nearly always you will need to be in the I/O monad to handle the error properly anyway, and so the fact that you are forced to be in this monad isn't a big deal.
Is a function which runs forever pure? Because if so, then you agree that a pure function can have \_|\_ for its return value, so given that exceptions act like just another form of \_|\_, why does their use suddenly make the code be "impure"? What you are arguing in favor of here is akin to saying that all code should live in the `Maybe` monad where the `Nothing` means "infinite loop"; if a given function terminates it returns `Just` and if it does not --- as the runtime can exactly detect some kinds of infinite loops and approximately infer others (such as when the recursion depth gets too high) --- then it should return a `Nothing`. There, now all functions are truly "pure" because the side-effect of non-termination has been moved into the pure value `Maybe`.
I agree that for some situations and for some classes of errors you can't really do anything more sensible than abort or retry an I/O operation, but in my experience these are much rarer. And even with I/O operations, I nowadays prefer using monad stacks like `EitherT MySpecificError IO` because that way the types themselves function as documentation and the error boundaries are made explicit.
&gt; Monads are pure, you don't give up any advantage. It depends on what you mean by pure; I consider a calculation to be impure if its result carries with it some side-effect, which is the case in the Maybe and Either monads. By forcing everything to live in one of these monads, we force everything to carry around a side-effect with it. This makes things more awkward and provides no advantage. &gt; If you have such a partial functions which you cant deal with locally, maybe you have the data structures wrong? Suppose we have a data structure with the invariant that if the first part is true then the other is an empty list, and if the first part is false than the other part is a non-empty list. Once you have looked at the first part you know that you can safely call `head` on the second part; pattern matching would be pointless because it would require you to handle the empty list case, but if an empty list turns up then you don't know what to do next because it means that your data structure is corrupt so you'd probably just signal an error anyway. This, of course, is a silly example, but it makes the point that it is possible to have invariants in a data structure that can be broken without it being the fault of th data structure. In my own code I have two trees which I know much match in a particular way because of the way that they are generated; if, for some reason, they do not match, then there is a bug in my code and so the best thing to do is to fail noisily as the program state is now corrupt. &gt; Since exceptions can only be caught in IO, I would prefer to have a Maybe or Either monad instead of IO. But what exactly are you going to do with a value that tells you that your assumptions about the program state are wrong and thus you have no idea how to recover? The main options I can think of include terminating the program and rebooting the thread (assuming in the latter case that it won't just do the exactly same thing a second time), both of which need to be done in the I/O monad anyway.
&gt; Firstly, I don't think this contamination is a bad thing. So you really don't find having a large chunk of your code in a Maybe to be significantly more cumbersome than not? &gt; Secondly, others in the thread have explained that you can escape Maybe and Either. But not satisfactorily, since the Nothing values gives you no information other than that your program is now seriously broken in some unknown way, which means that it probably just needs to be terminated immediately, which is exactly what would happen if an exception was thrown.
&gt; I agree that for some situations and for some classes of errors you can't really do anything more sensible than abort or retry an I/O operation, but in my experience these are much rarer. I will happily agree that when a failure can be expected to happen at run-time then you shouldn't use an exception; does this mean that you agree with mean that there are, however, cases where throwing an exception is the best possible option?
Or the third choice, error types.
&gt; It's kinda like a function where x is the parameter. More than that - list comprehensions are syntactic sugar for constructs built up from functions (I'll return with a link for details once I find a good one). Once you desugar, `x` literally becomes a parameter name in a function. **EDIT** - OK, I found [a link on Stack Overflow](http://stackoverflow.com/a/8029698/180247). This example translates from this list comprehension... [(i,j) | i &lt;- [1..4], j &lt;- [i+1..4]] first to this do-notation... do i &lt;- [1..4] j &lt;- [i+1..4] return (i,j) then to this expression using bind operators... [1..4] &gt;&gt;= \i -&gt; [i+1..4] &gt;&gt;= \j -&gt; return (i,j) At this point, `i` and `j` are both parameters to lambda functions - the lambda syntax being `\args -&gt; expr`. This formatting is idiomatic for expressions using bind, but maybe slightly confusing to newbies because of the way the lambda is split over lines. 
I agree that partiality is no the same as impurity. However, given the choice, would you rather have a library function return `Nothing` when you provide invalid input or go into an infinite loop (since, as you as said, that's just as valid for `_|_`)? Given the nature of the real world and the halting problem, sometimes `_|_` is the best you can do. The point is that especially library authors should take extra care not to introduce unnecessary bottom values in cases that can be expressed with proper values (for example Either or Maybe).
Perhaps there's some misunderstanding here. When I say "OMG yes library writers, please do not throw exceptions" I'm talking about functions like [Network.HTTP.Conduit.simpleHttp](http://hackage.haskell.org/packages/archive/http-conduit/latest/doc/html/Network-HTTP-Conduit.html#g:1) which throw exceptions when an HTTP connection 404s. There's no need for that. If some invariant internal to your API has failed, then yes, throwing an exception is one sensible option. 
I think we're talking at cross purposes. See http://www.reddit.com/r/haskell/comments/1etssw/haskell_in_production_slides/ca4diy6
The Monad Reader [issue 12](http://www.haskell.org/wikiupload/f/f0/TMR-Issue12.pdf) contains a description by Max Bolingbroke of a GHC plugin that probably does some of what you want. If nothing else it might be interesting and give you some ideas.
In the absence of javascript it's probably better to fall back on a linear document rather than failing silently.
Yeah, I started learning image processing algorithms with JuicyPixels. That calls for an image processing library, but I'm not knowledgeable enough yet. JP is a great "backend" for such a library though!
&gt; However, given the choice, would you rather have a library function return Nothing when you provide invalid input or go into an infinite loop (since, as you as said, that's just as valid for _|_)? If I broke a precondition in my inputs to the function then that means that my program has been shown to be in an invalid and irrecoverable state. Since, in this case, there is no way to recover, I am going to have to do something like terminating it with an error message anyway. Thus, having it throw an exception is far more convenient because it saves me from having to do the same thing myself --- or worse, if I made a pact never to use exceptions like some here want, I would have to *manually* propagate this error back down to the I/O monad rather than letting the runtime do it for me automatically. &gt; The point is that especially library authors should take extra care not to introduce unnecessary bottom values in cases that can be expressed with proper values (for example Either or Maybe). Sure, but I was never arguing against that point. I suppose that another way of stating my point is that a Nothing value that effectively means "there is a bug in your code" is not any more helpful then throwing an exception as in such a case the caller has no option but to do something like terminating the program anyway. Here, let me give you a simplified example from my own direct experience. At some point in my program I have two trees which were generated from the same code but were explored in different ways, and I needed to merge them. The merging code needs to make the assumption that the two trees are compatible in the sense that where they overlap they agree on the branching structure. Because the two trees come from the same generating code, this is an invariant that will *always* hold unless there is a bug in my program or memory corruption. Now, I could make the function return a Maybe value, but it should never, ever, ever return Nothing *unless* there is either a bug or something else that has caused the state of the program to become corrupt in a way that I don't know how to recover from. Thus, the only thing for me to do when I see Nothing is to terminate the program with an error message. Given that, it makes far more sense for the function to just throw an exception itself which will usually result in program termination with a message, saving me the trouble.
It's not pure! The question which is a huge unknown, in my opinion, is how would a strict Haskell handle monads, and what's probably more difficult, applicatives.
Error types?
&gt; If I broke a precondition in my inputs to the function then that means that my program has been shown to be in an invalid and irrecoverable state. No, not at all. So what if you broke a precondition? The function should report that to you somehow and the rest of your program and valuable data can continue undisturbed.
I suppose another option for strict Haskell would be to write a transformer that converts normal Haskell code to strict Haskell code. Just sprinkle bindings and type decls with bangs and import [strict-base](http://hackage.haskell.org/package/strict-base-types) rather than normal base. Wouldn't change the way GHC's runtime works, but would at least give GHC chance to take advantage of the strictness anotations and runtime behaviour might be more predictable.
Just because the possibility of bottom is necessary in a Turing complete language does not mean it is a sensible failure case which should be used instead of perfectly good total alternatives like `Maybe`. If you expect the user to catch your exception, you should almost certainly replace it with real data. I only ever use exceptions in Haskell for code paths which I don't think should ever be taken.
Why is handling monads strictly difficult?
Why is that even a question? `Monad` and `Applicative` are just typeclasses, `&lt;*&gt;` and `=&lt;&lt;` are just functions. What am I missing? And why do you say `Applicative` is probably more difficult than `Monad`? Every monad is also an applicative, so if there is a problem here I would expect the reverse of your statement to be true.
&gt;So you really don't find having a large chunk of your code in a Maybe to be significantly more cumbersome than not? You only need code which can produce a new failure case to be in a `Maybe`, and this is just the inevitable complexity of proper error handling. And with `do`-notation it's usually pretty much the same overhead as using an exception anyway. &gt;But not satisfactorily, since the Nothing values gives you no information It feels almost like you're being deliberately obtuse here. "If you use a type that provides no information, you get no information!" Well of course, if you *need* that information you use `Either` instead.
Yes, but the point is that in some cases there is no way to meaningfully continue because if you got the precondition wrong, then there is *necessarily* a bug in your code (or memory corruption), which means that something is going wrong that you do not understand and hence cannot recover from.
 x :: Maybe a x = f &lt;$&gt; c &lt;*&gt; anExpensiveComputation In a strict language `anExpensiveComputation` is evaluated even though it may not be needed. Denotationally everything is the same (if we ignore bottom) but from an operational point of view applicative style has obstacles to overcome. Monads won't suffer quite the same issue because the second argument to `(&gt;&gt;=)` is explicitly a computation anyway, though `(&gt;&gt;)` may have problems.
&gt; I only ever use exceptions in Haskell for code paths which I don't think should ever be taken. Which is a nicely succinct way of expressing my position. :-) The parent to which I was responding was claiming that if something throws exceptions then it is not pure, and I was just arguing that if you accept that \_|\_ can be returned by pure functions then you already agree that exceptions can be thrown in them as well.
Do you know about [Disciple](http://disciple.ouroborus.net/)?
Not necessarily. Maybe a programmer error led to the wrong argument being passed to the "Show the About box" function. That doesn't mean I want to lose my entire day's work.
But I don't want to do it in IO when I can do it purely.
I would find it equally objectionable if an HTTP client library went into an infinite loop to signal 404.
No, the point is that are cases where if a precondition has been violated then something can only have gone horribly wrong in the code itself or a memory corruption occurred. In such cases, a Nothing value gives you absolutely no information about how to proceed since you have no idea where things are broken and hence recovery is impossible. Given that recovery is impossible, there is nothing to do except to either terminate the program with an error message or to restart the thread ala Erlang style. Of course, *if* recovery is possible --- i.e., if there is a subset of inputs which cause a failure but which are expected to possibly occur during run-time conditions, such as when parsing an input file --- then I agree that something like Either is better than using an exception. But I have been trying very hard to make it clear that I am *not* talking about those cases.
I like this one, but it's pretty opaque. This looks like some variation on Horner's representation of polynomials, yes?
Well that's reasonable enough, but the start of the discussion was talking about libraries. I think libraries throwing exceptions for invalid inputs is bad API design. Ideally you limit the inputs to valid ones in the type system, using a `newtype` for example. Or if this isn't really practical, just return a `Maybe` and let the *user* decide how to handle it. They can wrap it in `fromJust` or `fromMaybe (error ...)` if they really don't want to handle the error.
Indeed, thank you.
Is this not just confusing laziness with applicatives? `head (1 : anExpensiveComputation)` will also evaluate `anExpensiveComputation` in a strict language, even though it may not be needed. In your example, in Haskell `c` and `anExpensiveComputation` are pure (possibly non-normal form) values that evaluate to a `Maybe a`, and `&lt;*&gt;` and `&lt;$&gt;` and `&gt;&gt;=` combine them to make another `Maybe a` value. In a strict language the `c` and `anExpensiveComputation` are normal form, and their composition is the same. Both `Monad` and `Applicative` seem unscathed so far. Consider: main = if True then x else y where x = const &lt;$&gt; getLine &lt;*&gt; print "Hey!" y = const &lt;$&gt; getLine &lt;*&gt; print "Yow!" How is this different in a strict or lazy language? In both cases, `getLine` and `print "Hey!"` are `IO` values that will get evaluated when `main` is ran. In the strict version, `x` and `y` are forced in the where, but the `IO` isn't _ran_ until the `if` calls it. What am I missing?
I think it would be a worth research project to develop an ecosystem around such a thing. I for one would be very interested in how such pure, strict programming turns out.
Hmm, I just felt like it was an obvious recursive solution? Essentially I'm treating the digits of the number like a list, where `head` and `tail` are `mod10` and `div10` respectively. As I said, it's rather inefficient, since without optimisation it's quadratic in the number of digits.
How is this any different to complaining that `&amp;&amp;` also won't work? Can you give a complete code example? Consider: &gt; (+) &lt;$&gt; throwError "Oh noes" &lt;*&gt; throwError "Yay!":: Either String Int Left "Oh noes" &gt; parse (string "hey" &lt;|&gt; many digit) "" "hey" :: Either ParseError String Right "hey" &gt; runErrorT $ (+) &lt;$&gt; throwError "Oh noes" &lt;*&gt; do lift (print "Ye gods!"); throwError "Yay!" :: IO (Either String Int) Left "Oh noes" How would these behave differently in a strict language? 
https://github.com/thoughtpolice/strict-ghc-plugin
This is kind of a basic question, but I really don't know the answer: what are the advantages of strict evaluation that are so important that lazy evaluation isn't good enough? This is one request I hear often which I honestly don't understand -- I don't get why something so low-level such as evaluation strategies are so important, it should be more important to just write elegant, correct code. Whether or not it is lazy is not going to change semantics of your program in any way. Furthermore, strict evaluation is not necessarily more efficient, as we know from other functional languages. Strict evaluation requires an *entirely* different optimization strategy, and special syntax is usually required to indicate which functions are recursive so the compiler can evaluate (or translate and optimize) those functions differently in order to prevent those functions from blowing the lid off the stack. With lazy evaluation, you let the compiler decide when to be strict and when not to be, no extra syntax necessary, and you can use strictness annotations to help the compiler decide, which you do when it comes time to optimize your code. I understand that lazy evaluation is possible in Lisp/Scheme with additional layers of parentheses, but they are strict by default. I am not a Lisp or Scheme guy, but as I understand it, lazy evaluation was useful more often than strict evaluation, and it was nice to have a language that would take care of that for you instead of writing all those extra layers of parentheses.
Yes sure, but the way you're building up the 10^x coefficient by repeatedly mapping (10*) over the sublist is reminiscent of it. Also you should look at `divMod`.
Yeah I considered using `divMod`, but given that it's already inefficient, I felt this form was slightly cleaner.
idris, with the termination checker shut off, and a restriction of attention to Haskellish types, and explicit signatures of course, is a sort of strict Haskell. It's a lazy Haskell too, again sort of, since potentially-infinite values come under types declared as codata. It is wonderful, but my limited experience suggests that mind-numbing explicit recursion, limited polymorphism in practice, and thus increasing stupidity, are the order of the day programming in it, as they certainly are in Agda, but this may be due to the inevitable tendency to try fancier types or inadequate facility.
In your code you can do anything, we were talking about libraries. Now imagine what your library function which can fail returns Either instead of throwing, and let me decide in my code how I want to deal with it. Wherever to throw it or supply another input or use some 'default'. 
See, for example: http://web.jaguarpaw.co.uk/~tom/blog/posts/2013-03-29-avoiding-space-leaks-when-deleting-data.html
&gt; how would a strict Haskell handle monads Lazily, of course, or you'd get completely different semantics. I think the main issue people have is laziness *by default*, not *explicitly, where it makes sense*.
I think some strict languages give special semantics to `&amp;&amp;` and `||` (or `and` and `or`) to make then non-strict. Not totally relevant to the current conversation, but an interesting note.
What would be the definition of the `Either` datatype, for example?
With a strict, unboxed Either you have the same problem [I described to chrisdoner above](http://www.reddit.com/r/haskell/comments/1ew6js/ghc_modding_qs/#ca4dutb) x :: Maybe a x = f &lt;$&gt; c &lt;*&gt; anExpensiveComputation Do you really want `anExpensiveComputation` to run if `c` trivially evaluates to `Nothing`?
It's not a huge unknown at all, I just can't tell you the answer. ;) Seriously, strict Haskell works quite well and doesn't feel that different from regular Haskell. But to make strict Haskell usable it's essential to be able to declare some things to be lazy. E.g., you want (&amp;&amp;) and (||) to be lazy, you also want lazy bindings sometimes, as well as lazy constructors. Monads et al work quite well in a strict language. You do want a few functions, e.g., 'when', to be lazy.
Exactly the same.
You're welcome!
That's a matter of how you define `fmap` and `ap`. Hence why I mentioned `bind` having to be lazy in its second argument. Of course, a strict `Either` *is* going to change the semantics: Its contents will be always evaluated. But it *is* possible to write a proper monad instance for such a thing.
So how would you define `ap`?
But has nothing to do with Applicative, it's just the way things work with strict evaluation. There's lots of examples where strict evaluation will do more than lazy evaluation. But most of the time it doesn't matter. When it does matter you have to make things lazy yourself.
I'm wondering why heap allocation using register cannot be as fast as stack. In stack allocation, when function starts it decreases stack pointer for an amount of memory needed for local variables. Could GHC figure out (in some cases) how much memory local variables would take and bump the pointer accordingly (compiling code to use register relative offsets)?
I'm not sure strict Haskell breaks the monad laws any more than regular Haskell already does. :)
As a user of strict Haskell, I've never noticed it to be a problem.
Johan Tibell is working on a lot of these features you are requesting. He's already added support for your second feature request with a compiler flag that automatically unboxes all strict fields and he's working on a module pragma that makes the code in that module strict by default. You should talk to him if you are interested about these things. He goess by `tibbe` here on reddit. The heap allocation thing is much more difficult. That's a very intrusive change.
&gt; what are the advantages of strict evaluation that are so important that lazy evaluation isn't good enough? No space leaks. Space leaks are far and away the biggest technical issue for large Haskell projects. They are comparable to segmentation faults in terms of difficulty to resolve.
It's not the same thing. It would make sense for Monoid(specially if you are going to put Traversable and Foldable), but I don't see why you would use Applicative as well. Is there any function already in the Prelude that is a monomorphic version of the applicative ones?
Half the Traversable members use Applicative, therefore if Traversable is in the Prelude so should Applicative. Imo.
&gt; Should undefined x be legal when x is defined? I wouldn't expect it to be (although it apparently is). The definition appears to be creating a polymorphic constant (which makes little sense) or perhaps a function that takes no arguments (which does not make sense in a pure functional context). Yet the type of (undefined 1) is 't' (according to GHCI). I must confess I do not understand how (undefined 1) could be a legal expression if undefined has type 'a'.
They are called variables in mathematics too, since they may have different values, although the value doesn't change. The "opposite" concept are constants, which can be substituted statically.
Yeah, that's pretty much what I had in mind.
One with a stack trace :)
&gt; This means that we can easily unify it with function types. [...] I'm starting to lose you here. I suppose the crux of my confusion is that (undefined :: a) is not a function type. Therefore how could you use it in an invocation expression such as (undefined 1)? You can't call something that isn't a function. &gt; Anyway, it seems to me that your difficulty to typecheck stuff visually comes from complete ignorance of Haskell's type system. I would prefer to phrase it as "an incomplete understanding". :-)
Essentially, think of it this way: when you mention a lowercase type in a type signature, you're not actually mentioning a type. Instead, you are making the value generic. In normal Haskell code, a generic value is actually just a sufficiently natural family of values, one for each type. In this case, it means that given some type, we can produce `undefined` for said type, because undefined does not add any other requirements. Haskell is smart, so it can automatically find out which type you are referring to when you use such a family of types. In this case, it can see that it is a function type. However, your revised type system is not really detailed enough that I know which function types exist.
That is really exaggerating the difficulty of resolving space leaks. In my experiences, they are extremely easy to resolve. 1. Skim recently changed code for anything that seems like it should be strict but would rely on the strictness analyzer. Add bang patterns and such to eliminate any ambiguities. 2. If that fails, heap profile and find where the allocations are happening. Once you're used to thinking about space leaks (took only 4-5 tries before I got the knack), the whole process typically takes less than five minutes.
Actually, having said that there's another issue. let ap' = ap in ap' x y will not evaluate in the same way as ap x y This has implications with regards to allowing users to define their own combinators.
&gt; So you really don't find having a large chunk of your code in a Maybe to be significantly more cumbersome than not? Well.. to be honest... no, it isn't really that difficult to deal with. The Applicative interface shines here, and Data.Maybe has a lot of useful stuff. When all I really understand was case matching on deconstructed Maybe values it was a right royal pain, sure, but as you get better at idiomatic Haskell usage it becomes easier than dealing with imperative errors. Compare with Go's error handling in particular. (That language badly needs the Either monad grafted on to it. Then again, that language badly needs several things...)
Yeah, it's not quite as bad as I made it sound. The only thing I don't like is that you have to exercise your whole code base to detect them ahead of time, otherwise if you discover them later then you don't know where you introduced them.
to have stack allocation you first need a stack, right? so i suppose this presupposes some serious strictness.
The quickcheck properties given there are not that thorough. Why not just flatten the tree and show refinement to data.list functions?
Yes. Using the Either and EitherT monads makes it easy to ignore failures while still documenting the possibility of failure in the e type.
I think what made you come across as lecturing was the title, "Currying considered harmful". In my experience, such titles usually mean a lecturing tone.
I really admire this guy. He's very intelligent and it's obvious he loves what he's doing.
Not quite. Currying is *not* an implementation detail. In fact, GHC tries to undo the currying of functions in order to implement efficient code, as functions returning functions leads to more indirections. Currying is rather a really useful conceptual way of thinking about multi-parameter functions, which encourages you to think about them in the most general way. Take your example (alpha beta gamma). The most general type of alpha is a -&gt; b -&gt; c Unlike what you seem to think, however, all of the interpretations in your original post are compatible with this type. I'll go through them point by point: * **alpha takes exactly two arguments:** In curried form, this means that alpha has type as above: alpha :: a -&gt; b -&gt; c * **alpha takes 2+k arguments:** No problem! We just instantiate the c variable to a function type, say d -&gt; e. This gives alpha :: a -&gt; b -&gt; (d -&gt; e) Since the arrow type is right-associative, this is exactly the same as saying alpha is a three-argument function. * **alpha takes 1 argument:** In this case, as you correctly point out, alpha has to return a function. Thus, the type will have to be something like alpha :: a -&gt; (b -&gt; c) Again, since the arrow is right-associative, this is equivalent to the above type for alpha. Thus, there is no possibility to be confusing three concepts, since they are actually *one* concept!
Wait, what? The fact that Maybe and Either e are monads doesn't mean much here. You can completely ignore the fact that they're monads if you're not concerned with the operations provided by Control.Monad. Also, you typically deconstruct the result of one of these things immediately with a case expression or something, so it's not like Maybe is going to spread globally through everything -- you just use it where you need to represent that possible absence of a value.
&gt; It depends on what you mean by pure; I consider a calculation to be impure if its result carries with it some side-effect, which is the case in the Maybe and Either monads. By forcing everything to live in one of these monads, we force everything to carry around a side-effect with it. This makes things more awkward and provides no advantage. ... do you actually know how Maybe and Either are defined? They're just ordinary algebraic data types. Maybe carries with it about as many "effects" as natural numbers do. Compare: data Maybe a = Nothing | Just a and: data Nat = Zero | Succ Nat Would you call the potential for a natural number to be Zero an "effect"? You can and should use them like any other data structure. The fact that there's an instance of Monad changes nothing about this (but does give some convenient operations you can choose to use or not). If you get some x :: Maybe a from somewhere, the first thing you can do with it is case x of Nothing -&gt; ... Just v -&gt; ... and now the Maybe types don't spread throughout the rest of your code. I am being a bit glib here. It *is* possible to think about Maybe as providing some kind of failure effects, if you want to think about it that way. But this is by no means required in order to make good use of the data structure. About the larger picture. I totally agree with you that sometimes there are cases where it's acceptable for pure code to just give up and die. However, as a library author, you'll want to expose as few of these as possible to your clients. Ideally, you will only need them in cases where you suspect you could prove that the case can't occur. If you allow your clients to give you bad input by having imprecise types, you'll ideally be able to provide them with some kind of feedback in your output which is better than terminating the entire program (and you should always assume that throwing an exception from pure code does this, even though it is technically possible to catch them from IO).
There are still space leaks, but they are different in strict code. More intuitive, according to some. 
I just implemented half of this writing a scraper earlier today. Does it exist on Hackage anywhere?
A strict language is certainly going to behave differently than a lazy language in many situations. It just doesn't matter most of the time.
Of course you bump (and check if it's time for GC) less frequently than per allocation. The start of every basic block is a good place. There will be some extra cost for the GC, though. Also, a stack can often have somewhat better locality.
Yeah, I came here to remark on the height of this building. 
Thanks for the explanation. So (a -&gt; b -&gt; c) can be transformed to (a -&gt; b -&gt; (d -&gt; e)) which is equivalent to (a -&gt; b -&gt; d -&gt; e), a "3-argument function", in the composed expression. *brain explodes* &gt; Thus, there is no possibility to be confusing three concepts, since they are actually *one* concept! I can see how this works if I do the type transformations mechanically, as you have done here. However I can't do this type of transformation intuitively. Can you? That is, should I expect to gain such an intuition as I read lots of Haskell code?
&gt; ... do you actually know how Maybe and Either are defined? They're just ordinary algebraic data types. Maybe carries with it about as many "effects" as natural numbers do. &gt; [...] &gt; I am being a bit glib here. It is possible to think about Maybe as providing some kind of failure effects, if you want to think about it that way. Okay... so then why did you waste all that time arguing that I was thinking about things the wrong way when you later *admit* that my perspective on calling Maybe a kind of failure effect is a valid one? &gt; If you allow your clients to give you bad input by having imprecise types, you'll ideally be able to provide them with some kind of feedback in your output which is better than terminating the entire program [...] Again, though, when bad input only happens because of a bug in the client code, using a Maybe or an Either or whatever doesn't actually improve the situation because the client's code is broken and memory is in some unknown state that cannot be recovered from. Ergo, these values provide no benefit over throwing an exception with an error message. &gt; [...] (and you should always assume that throwing an exception from pure code does this, even though it is technically possible to catch them from IO) Another alternative is that one might get a different result of the computation is re-run (due to some impurity in it) so one could simply restart it ala Erlang style.
How do you plan on recovering from such a failure purely? Again, given that your whole computation was relying on the output of this function and it failed in such a way that gives you absolutely no information about how to fix the problem and complete the computation.
If the function fails then there is no alternative input you can try or sensible default; there is nothing you can do to proceed because your program state is invalid and irrecoverable. Thus, of your three options, *only* throwing an exception is a valid solution.
Bug mutjida -- I'm sure he'll be happy to encourage you to put it up, if nothing else :-P
&gt; Okay... so then why did you waste all that time arguing that I was thinking about things the wrong way when you later admit that my perspective on calling Maybe a kind of failure effect is a valid one? Because it's not the *only* valid one, and the conclusions you get about how its use affects your program are only true if you keep thinking about the type in that way, i.e. you write everything in the Maybe monad, which is not something that you have to do. &gt; Again, though, when bad input only happens because of a bug in the client code, using a Maybe or an Either or whatever doesn't actually improve the situation because the client's code is broken and memory is in some unknown state that cannot be recovered from. Ergo, these values provide no benefit over throwing an exception with an error message. The client program might want to present some more meaningful kind of "I failed" message to the user, rather than simply vanishing with a message on the terminal (which may or may not even be visible) -- it might want to log the failure somewhere before shutting some things down cleanly. You shouldn't make this harder to accomplish than necessary. &gt; Another alternative is that one might get a different result of the computation is re-run (due to some impurity in it) so one could simply restart it ala Erlang style. Except that if evaluating a pure expression results in an exception, it will do so again if reattempted. For IO actions, the situation is different. It might be a very reasonable thing to reattempt them (though it will cause effects to occur for a second time, so doing it automatically is dangerous), and I don't think people have problems with exceptions being thrown by the execution of IO actions. It's just the unnecessary cases of asynchronous exceptions thrown from evaluation of pure functions (i.e. applications of error :: String -&gt; a) that's an issue here.
400 storey elevator trip ahead? Time to teach these mofos some func-pro!
Awesome job! I might be wrong, but if I were to implement merge sort in Agda, I'd probably approach it the way the lazy "pairwise" one works in Haskell's Data.List rather than the more traditional "split it in half" approach. Not sure it'd be more pleasant, but it feels less scary. 
&gt; Because it's not the only valid one, and the conclusions you get about how its use affects your program are only true if you keep thinking about the type in that way, i.e. you write everything in the Maybe monad, which is not something that you have to do. But you do have to write your code in that way because there is nothing useful that you can do with the Nothing value so every function that is dependent on the original function must also return a Maybe. This is true independent of whether or not one prefers to use the term "impure" to describe a Maybe --- the fact remains that you can't escape from the Maybe monad in this case because there is nothing meaningful you can do with the Nothing value so you have to keep propagating it. &gt; The client program might want to present some more meaningful kind of "I failed" message to the user, rather than simply vanishing with a message on the terminal (which may or may not even be visible) -- it might want to log the failure somewhere before shutting some things down cleanly. You shouldn't make this harder to accomplish than necessary. And *all* of these things can be accomplished by catching exceptions! That is party of the beauty of exceptions: they let you essentially jump directly down to the I/O monad, which is the only place where the actions you listed can be taken, rather than having to manually propagate the error. &gt;Except that if evaluating a pure expression results in an exception, it will do so again if reattempted Yes, of course; I got tired after writing that possibility out so many times that I neglected to include the caveat that it only works if something different is expected to happen this time. &gt; For IO actions, the situation is different. It might be a very reasonable thing to reattempt them (though it will cause effects to occur for a second time, so doing it automatically is dangerous), and I don't think people have problems with exceptions being thrown by the execution of IO actions. It's just the unnecessary cases of asynchronous exceptions thrown from evaluation of pure functions (i.e. applications of error :: String -&gt; a) that's an issue here. I guess that my point just remains that there are classes of errors for which no recovery can happen within pure code and in such cases the more convenient and sensible action is to throw an exception to jump down to I/O because the alternative is to build a lot of infrastructure to propagate the exception down to I/O manually and that strikes me as making your code messy for no good reason. That is, exceptions aren't always the best idea, but can be less ugly than the alternative of complicating all of your code by putting it in the Either/Monad monad to propagate the error down to the I/O monad when exceptions can leverage the run-time system to do all this work for you.
Very cool. I hadn't seen your blog before, but I'll certainly be subscribing to your feed. Seeing good examples in Agda is definitely pushing me to use it more.
Some people in IRC ##traders and some people on Nuclear Phynance use Haskell.
Yes, people are using Haskell to trade stocks, options, futures and derivates. Last I checked there is were not any open source libraries equivalent to MT5. I semi-half-assed-released our old [QuickFIX bindings](https://github.com/alphaHeavy/quickfix-hs) but it's not a trading platform by any stretch of the imagination. I think we'll eventually release some market data feed libraries but probably not much in the way of strategy modeling and execution.
&gt; if (undefined :: a) then the expression (undefined 1) could actually fail if 'a' turned out to be Int, String, or some other non-function type No, because `undefined` can only have any type because it doesn't return. So, in a sense, any value it returns has every type, because it doesn't return anything. The Scala equivalents would be these: def undefined[A] = undefined[A] def undefined:Nothing = undefined
Why there's so few?
If that exception means "bug in library, please report" then it's OK. If this exception throwing in pure function is a part of interface - then no, you are doing it wrong.
&gt; When all I really understand was case matching on deconstructed Maybe values it was a right royal pain, sure, but as you get better at idiomatic Haskell usage it becomes easier than dealing with imperative errors. I am well aware of all the mechanisms that you described, but they still require extra work and clutter the code, and the fact remains that even easier is not having to use them at all. :-) Again, I am referring to errors that are only caused by a bug or by something catastrophic like memory corruption, not errors to which anyone could respond meaningfully to in pure code, so all that you would be doing is adding a duplicate infrastructure that propagates errors down to I/O rather than using the one that already exists. As an aside, although I am aware of the existence of the plumbing to propagate Maybe values and have occasionally used it myself, I honestly have rarely found it to be that useful in practice because I have almost never been in a position where I received a Maybe value that I wanted to propagate along rather than turning it into a more useful value in a different type. Also, I agree with you about Go 100%, but then I think that this is true of nearly anyone who posts in /r/haskell.
Yes, see for example http://www.tsurucapital.com/en/ (jaspervdj did an internship there)
Have you seen [this paper](http://www.iis.sinica.edu.tw/~scm/pub/aopa.pdf) where they derive a sorting function from the specification (in Agda)?
It depends what you think a problem is. Yes, it can certainly happen that you have to rewrite some lazy code when you switch to strict evaluation. But it's not very common in practice.
As I am sure we've discussed elsewhere, that situation is handled easily with exceptions: catch the exception at the I/O level and abort drawing of the Abort box. Since the drawing of the abort box is something that fundamentally lives in the I/O monad anyway, it wasn't even something you were doing with 100% pure code anyway. As I have stated before, there are multiple ways to handle an irrecoverable error that occurred in pure code at the I/O level by using exceptions: You can terminate the program. You can restart the thread in question if you have reason to believe it was a fluke. You can kill and clean up the bad thread and allow the rest to proceed drawing whatever UI elements they are working on. Etc. The only reason that I don't write out the entire list every time is because it grows tiresome. :-) Furthermore, the problem with me enumerating a list like this is that it people here sometimes seem to act as if all they have to do is find something that doesn't quite fit on the list and then call me out on it, rather than considering if it could be easily added to the list as another example. :-) Edit: Fixed a typo.
&gt; I am well aware of all the mechanisms that you described, but they still require extra work and clutter the code Either you are not as aware as you say you are, or you are aware that the subsequent remark is bullshit. Pick one, but not both. Only one. Not two. Not zero. 
OK, let's take a specific example. Suppose I want to parse a URI. Should the function have signature parseURI :: String -&gt; URI and throw an exception when the input is malformed? Or should it have signature parseURI :: String -&gt; Either ParseFailure URI
If space leaks are a persistent issue, I would suggest making your non-control-related data structures all strict with bang patterns for each field. Also, if your data structures have types like "Maybe" or "Either" or lists, use strict versions of those. For everything else (flow control related things), lazy is good enough, at least in my limited experience. Lazy State monad transformers with lazy stateful data structures, lazy list monads, lazy "Maybe" monads, things like that. If anything, I would think this calls for a modification to the Haskell language to provide strict data structures with a new keyword. We have "newtype", "data", so the new keyword could be like "strict" or something which would have exactly the same syntax and semantics as "data" except all fields are strict by default without using bang patterns. Doing this instead of modifying the evaluation algorithms of the compiler and interpreter seems to make more sense to me.
In an appropriate use of Maybe, the value Nothing will signal something specific about the failure in question. If you expect the user will need to know more about what went wrong in the pure code, and multiple things can happen, you should represent them and reflect those conditions as values of some type (and perhaps use Either, or some other type to represent all the possible cases.) The reason not to use exceptions for this is that catching exceptions thrown by pure code is extremely tricky, and relies on the operational semantics of evaluation. You have to ensure that the *evaluation* of the precise subexpression which throws the exception happens inside of the *execution* of the catch. Since evaluation of expressions and execution of IO actions are somewhat separate, this is fiddly at best. While there is evaluate :: a -&gt; IO () this only evaluates to weak head normal form, and so won't on its own work to catch exceptions hidden somewhere inside a larger structure.
Good question. Technically my answer would depend on whether it would be considered a bug to give parseURI an invalid URI string, but almost certainly in this case (and in most such cases) we would want the second one because the input is coming from the outside world which is a messy place where inputs tend not to be valid all the time. I would only choose the first option in the exceedingly unlikely event that I was only planning on ever parsing URIs that I generated in another part of my program, and hence the generating strings were guaranteed as an invariant of this program to always be valid. Since this will almost never be the case, the first option will almost never be the sensible one.
So your point is that it takes **exactly zero** extra effort to deal with a bunch of functions that return monads over a bunch of functions that are all pure? That it requires **not a single extra bit** of typing at all? And that **absolutely nothing** is added to the types of the functions when you have their return values be wrapped in monads? You accuse me of being ignorant or disingenuous, but *you* are the one here who is asserting that a positive number is actually equal to zero and then has the gall to say that it is *my* fault for not recognizing this obvious fact. :-)
Functions don't return monads. That is a ridiculous non-concept and only indicates lack of understanding. Following that you refer to pure functions as if your hazy notion of monads somehow affects purity. I didn't read the rest because you have no idea what you are talking about. The point is you can change the fact that you have no clue. Plenty of people will help, including me. However, we need to take the first step, which is to accept that you are not well aware of much at all in this context, then start all over again and gain understanding. Like I said, pick one. Only one. It's fun, try it.
Definitely. He transcends haskell and software development. His work with computing at schools is super impressive. 
&gt; In an appropriate use of Maybe, the value Nothing will signal something specific about the failure in question. Yes, but my point is that in some cases Nothing means: "your program has a bug is in an unknown state", and that is the best that you can do. &gt; If you expect the user will need to know more about what went wrong in the pure code, and multiple things can happen, you should represent them and reflect those conditions as values of some type (and perhaps use Either, or some other type to represent all the possible cases.) Yes, but you can do the same thing with exceptions by creating a special exception type that contains all this information; realistically, though, all you need to know if an invariant fails in your code is what error message to display to the user so that he or she can contact the programmer and let them know what went wrong. &gt; The reason not to use exceptions for this is that catching exceptions thrown by pure code is extremely tricky, and relies on the operational semantics of evaluation. [...] I understand how lazy evaluation works, though people here seem to think that I don't understand how Haskell operates merely because I disagree with them on this issue. :-) You make an interesting point that the exception might never be thrown, but in a way it is not a big deal. If the function's value is needed then it will eventually be forced and the exception thrown. If it is not needed then arguably the fact that it was given bad inputs was not a problem. That's one of the cool things about laziness: you can have \_|\_s floating around in your code and they won't hurt anything unless you force them. Your point seems to be (if I understand correctly) that it would be better for us to use the Maybe monad to force every function involved in the computation to report whether it has an error or not rather than letting some fail without affecting anything. I don't consider this to be an obviously superior alternative, though, since if a value is never needed then the fact that it is equal to \_|\_ shouldn't matter since, again, that's part of the whole value of non-strict semantics. If you really wanted to make sure that values which you are assuming meet some invariant actually meet that invariant, then it might be better to just make that check explicit rather doing it indirectly by (roughly speaking) forcing everything in your calculation to see if any function failed. And, to say this one more time, we are talking about a situation where a function will succeed 100% of the time if there is not a bug in your program. So given this, adding extra code to deal with the case where the function fails doesn't really gain you anything except to add the extra complexity of plumbing your code through a monad --- which, while hardly a Herculean task, is still effort and type noise that is not doing anything useful.
I don't think that's true outside of the C family. Dynamic languages like Python and Lua have objects, and namespaces where you can put "references" = variables that point to those objects.
The fact that I occasionally use sloppy terminology ("monads" when I obviously meant "monadic values" or "values contained within a monad", and you *know* that this is what I meant), because I write my replies quickly as Reddit is on the bottom of my list of things to care about doesn't mean that I don't know what I am talking about. You acting like it does, however, and employing it to give you the opportunity to lecture me on my ignorance and how only people like you can save me from it makes you a condescending jerk that I am uninteresting in continuing to converse with as there are plenty people here who, unlike you, are not jerks and are actually worth my trouble. :-) Feel free to fill the last word with whatever further belittling nonsense makes you happy. :-)
Me too! And I highly recommend applying to anyone who wants to work on Haskell to do "real world" things. It's an awesome experience, and the people you work with are fantastic.
I've eliminated space leaks in my projects already, but it is not trivial. &gt; If space leaks are a persistent issue, I would suggest making your non-control-related data structures all strict with bang patterns for each field. I already do this. However, the biggest sources of space leaks are boxed vectors and I have no control over these, nor can I always used unboxed vectors.
You seem to be flipping back and forth between all sorts of standards as you post along; one moment standing firm on the standard you advocate here that it should somehow require zero effort (which makes no sense to me; error handling in any language requires non-zero effort, there is irreducible complexity involved, alas), but when someone defends the amount of effort involved you flop back to complaining about IO exceptions. I've been reading all your posts here but I can't pin you down to a particular _point_ you're trying to make (and note that's not because you don't have one, but because you have _too many_), because as soon as someone says something about A you immediately start talking about B again... you seem to just be arguing, losing sight of any overarching goal you may have while in the heat of moment contradicting someone. If you feel like you're not getting good results here, this is why.
From memory, people without much experience are preferred. Also it seems you have more work to show than I did.
I don't understand the problem you are talking about. If a function may fail I have it return an `Either`. If it doesn't then I don't. The only time that these functions need to sync on the same monad transformer stack is at the last moment when I combine them all into my application's top-level logic, which is pretty easy. I don't encounter this "mass infrastructure" problem you are describing.
Oh you work there ? (: Cool.
I did work there, on a three month internship.
Well-Typed are doing something. You may have noticed the [Haskell committee for 2014](http://thread.gmane.org/gmane.comp.lang.haskell.general/19796) has now been formed, and Well-Typed's Ian Lynagh is the committee chair. Further, Well-Typed's Andres Löh is planning to put in a proposal for a careful selection of existing extensions that are ready for inclusion in the standard. 
Are you using fixhs for anything? The complicated bits of a FIX engine aren't implemented (connection logic, logging, persisting sequence numbers, message replays, customization, etc). The choice of license also impacts who will contribute patches.
You're better off taking a look at [rust](http://www.rust-lang.org/) instead, it sounds like it's what you're looking for.
In addition to Tsuru, and JSC (awesome company by the way in spite of using the lesser OCaml), AlphaHeavy in SF is doing a ton with haskell based trading platforms.
I explicitly stated that I think it's okay to use error in a case where you know that the value will never be evaluated (because in a more ideal setting, you'd have some proof of that which would remove the obligation to provide a value there). The case of "if this value is required then the algorithm that this library is based on is actually incorrect", is not what anyone is complaining about.
What is the better choice of license? I don't have the overview.
That is a long elevator ride. =)
[Haskell in Industry](http://www.haskell.org/haskellwiki/Haskell_in_industry). There was a humorous Haskell analysis performed on the 2008 financial crisis, but I can't find the link. :( Anyone remember this one?