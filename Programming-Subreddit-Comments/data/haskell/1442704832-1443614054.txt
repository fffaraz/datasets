A little while ago I designed a different kind of Neural Network algorithm that would allow a [neural network to emulate a 1-bit register](http://rickdzekman.com/thoughts/emulating-1-bit-register-neural-network-short-term-memory/). I implemented it with unboxed mutable arrays using [`STUArray`](http://hackage.haskell.org/package/array-0.5.1.0/docs/Data-Array-ST.html#g:2). If I was to do it again I would probably look at unboxed mutable vectors with [`MVector`](http://hackage.haskell.org/package/vector-0.11.0.0/docs/Data-Vector-Unboxed-Mutable.html). If your neural net is [recurrent](https://en.wikipedia.org/wiki/Recurrent_neural_network) then your best bet is to use indirection. Consider a tree (inherently acyclic), it is pretty easy for nodes to reference each other, e.g. `Tree (Tree a) (Tree a) | Leaf a`. But with a cyclic graph you have to start doing things like "Tying the Knot" which IMHO gets pretty out of hand if your graph is complex enough. With indirection nodes don't reference each other but instead reference the "id" of target nodes - then when modifying the target node you look it up in some sort of data structure and modify it there. Indirection can lead to its own problems. When you create a system of indirection you've basically rolled out your own pointer system. This means you can get null pointer errors and this starts to defeat the point of using Haskell. So if you do go with indirection you need to be careful with your design. On another note, there is a really interesting paper called [Functional Programming with Bananas, Lenses, Envelopes and Barbed Wire](http://eprints.eemcs.utwente.nl/7281/01/db-utwente-40501F46.pdf). It describes the concept of a "Hylomorphism" - this is where you both construct (unfold) and fold your structure in a single pass. I would be really interested in knowing if there was a good way to create a hylomorphism for a recurrent neural network - though at this stage I haven't been able to think of one.
In addition to the other answers, a concrete law that you get is if you apply a monad transformer to a monad then the result is another monad. This is also useful for refactoring - for example, if your monad `m` subsumes the reader monad (if it has the ability to read a value `a` from the environment, but not to modify it) then you could factor out that reader functionality and use a `ReaderT a m` instead. That way you can really get to the essence of your data type. 
works, thanks! did not know about the bounded class
Also `do` notation will work on any type that implements `Monad`
There is a nice introductory article in the [Monad Reader 21](https://themonadreader.files.wordpress.com/2013/03/issue214.pdf), which might help you out. Disclaimer: I have no experience with artificial neural networks
I've wanted the same thing. My usual solution to cabal's usual error "can't upgrade foo because it would break bar and baz" is to add bar and baz to the same cabal install command. It feels really dumb to have to repeat the same list of packages back to the tool that reported it, especially when the list may contain &gt;20 packages. I think I understand why Cabal can't perform these upgrades automatically: there may be a downstream package it doesn't know about because it's not registered in the same database, that depends on the existing registered packages. If I had this flag to tell Cabal that the set of packages is closed and it's safe to rebuild them all, it would be perfect for my purposes. cabal install --only-dependencies --closed-world package-foo package-bar ./my/project-1 ./my/project-2 
I'm not sure if it will be of any interest, but recently I've been working on an open union type that is "flat" (i.e. it uses a simple integer tag internally, like regular ADTs) as opposed to the nested Either-analogues that are usually used for this sort of thing. It's somewhat inelegant, as it requires `unsafeCoerce` in its internals. It's a WIP but I can post the GitHub link if anyone's interested.
Regarding your first question, there exists a functional graph library `fgl`, which should fit the bill. Regarding State; let's step back for a second: your NN needs to have a variable topology, so program state in this case would be the connectivity table. Am I understanding what you mean?
I have found CompData useful for letting me skip reimplementing some fundamental data types and type classes, but I think I agree with you: there is a documentation problem. If I didn't already know some of the names/problems solved by the CompData package, I would have no idea where to start (even now, there are pieces which I have no idea how to use).
+1. As-is the current state of the branch would be a regression for me because of [this issue](https://github.com/haskell/cabal/pull/2752#issuecomment-139727197). My specific use case is that a develop a little cluster of packages in tandem and use "cabal install ./foo1 ./foo2 ./foo3 ..." routinely (in a sandbox) to make sure everything resolves consistently, builds, etc. If this new functionality were gated behind a flag then there'd be no regression from my perspective. (I can just ignore the new flag.) (There's a danger here of incurring even more option-itis, but I suppose it would be easy enough to just remove the flag entirely once dcoutts' patches make it in.) 
You just said the same type of thing he's heard before without providing any new information.
Rule #5 seems like good style advice for any programming language - don't avoid naming things if the names would make the code more readable!
I think Haskell totally shows this relation! thing = if condition then x else y
I'm not sure what you're asking. ApplicativeDo won't help when you want to pass an argument into a query arrow, for example.
I had a golden opportunity because my friend had written a toy program in C that did something to sort out his bibtex entries. (We were both writing papers with LaTeX and I understood the string/text file processing he had automated.) I wrote a haskell program that did the same thing, literally on the back of an envelope, and the conversation went "Is that it?" "Yes. Why, was yours about 10 times as long? " (pause for thought) "... Yes." "That's what I tend to find with Haskell. More thinking, less typing, which makes it a lot more fun. Here's how it works...." 
The point is, people usually just bump B when they are adding new features, so its more than likely that 1.2 didn't introduce incompatibilities from 1.1. However, 1.3 changes the kind of the main type `Diagram` which you have to use the help the compiler instanciate the correct backend class. This mean, this version will break 100% of existing code. With semantic versionning, I would have had 1.1, 1.2 and 2.0 (instead of 1.3) and I could have put confidently a `1.*` bound. Let's look at GHC and `base`. Every time a new version of GHC come out, it bumps base.B . From what I understand, appart from version 7.10, GHC is pretty conservative and absolutely every new features is isolated in it's own extension. Moreover, I also have the Haskell standard in my cabal file, so in theory, code which I'm writting now, should work with pretty much every single version of GHC coming in the next 20 years ... Except as I said with GHC 7.10 which I think correspond to base-4.10. Had we called it base-5.0 I could have put `base &gt;=4.0 &lt;5` change it to `base &gt;= 5.0 &lt;6` and wait happily 10 years until base-6 come out. But because of `4.10` I have to write `base &gt;=4.10 &lt; 4.12` and and bump it every 6 months for 10 years until a real incompatibility arise. I assume, if GHC 7.12 is called 8.0 that base-4.12 will be called base-5.0. It's bringin lots of nice stuff, but as far as I understand, they are all isolated behind extensions, so everything workign with 4.10 should work with 5.0.
No worries. I guess I can come off a little more combative than I really am. :)
&gt; The point is, people usually just bump B when they are adding new features If they do that then they're doing it wrong.
&gt; This is nice because it leaves A free for the developers' discretion ...Maybe 0.x means "too early to expect long-term support" and 1.x means "the API has stabilized" That's the point of semantic versionning, to remove those maybes** &gt; Upper bounds with the PVP are not at all close to freezing because the bound you should choose is something like this: `diagrams &gt;= 1.2.1 &amp;&amp; &lt; 1.3` Yes but in practice, they only release 1.1, 1.2, 1.3 (and few 1.2.0.x to fix bugs, but all new features bumps B) as you can see the list of diagram releases 0.1 , 0.2, 0.2.1, 0.2.1.1, 0.2.1.2, 0.2.1.3, 0.2.2, 0.2.2.1, 0.2.2.2, 0.2.2.3 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.7.1, 0.7.1.1 , 1.0, 1.0.0.1 , 1.1,1.1.0.1, , 1.2 , 1.3 However, I realized that SemVer A.B.C.D is equivalent to PVP 1.A.B.C ;-) 
I can vouch for this video being the one that got me to look at haskell.
Monads are for packing, Comonads for unpacking. It pretty much says so in the class definition: return :: Monad m =&gt; a -&gt; m a extract :: Comonad m =&gt; m a -&gt; a `bind` and `extend` then allow you to replace the contents of the m-package, but in opposite ways. `bind` lets you take a peek at the *value inside*, so you can deliver a new package: bind :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b -- we take a peek inside the m a to deliver the m b `extend` allow the opposite: you can take a look at the *package* to deliver a new naked value: extend :: Comonad m =&gt; m a -&gt; (m a -&gt; b) -&gt; m b -- take a look at the packed m a to deliver a naked b. **Monoidal structure** The associativity and the neutral element probably make more sense when you consider (&lt;=&lt;) :: Monad m =&gt; ( b -&gt; m c) -&gt; ( a -&gt; m b) -&gt; ( a -&gt; m c) (=&lt;=) :: Comonad m =&gt; (m b -&gt; c) -&gt; (m a -&gt; b) -&gt; (m a -&gt; c) (.) :: ( b -&gt; c) -&gt; ( a -&gt; b) -&gt; ( a -&gt; c) One can intuitively see why (.), which chains functions, should be associative. For the same reason, (&lt;=&lt;) and (=&lt;=), which chain (co)monadic functions, should be associative as well. What you said pertains to monoids in general, not to (co)monads specifically: &gt; Monads allow you to modify the structure of something but without any context of where you are in the structure If you see a series of functions chained via `&gt;=&gt;`/`.`/`=&lt;=` as a list, this amounts to "an element in a list shouldn't know where it is".
I think it's better to simply think of the Monad and Comonad and Applicative and Functor classes as only a set of methods with types and laws that your implementation of the class should conform to, and if it does, then you have access to a small library for your data type. Any kind of analogies or attempts to characterize what an instance of those things "is" is inherently more informal and lossy and limiting than just the few lines of Haskell code that describes the class and the laws. 
I support enabling no-reinstall for GHC 7.10 and later. (I've probably missed a lot of the earlier discussion, was it on the cabal-devel mailing list?) Re Problem 1: let's not worry about old versions, support the new functionality from 7.10 onwards Re Problem 2: this only affects GHCi and standalone GHC, and there are workarounds: either use cabal repl to get a consistent view, or use -hide-package/-package arguments to GHC to manually edit your view. It's already possible to get into funny situations with GHCi even without no-reinstall. Re Problem 3: I don't have a good sense for how much of a problem this is in practice. 
Yes, probably having a good example is the best choice. Maybe just taking examples from "Why FP Matters" is what I'll stick to next time.
Hey, the author went to Harvey Mudd College. I'll bet he met Melissa O'Neill, the author of the famous [sieve of Eratosthenes paper](https://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf), in person.
Can you explain what you mean by "showing tuples"? Like just the fact that you can put few things into a tuple or something? Good type inference example would be also great, if you have any on your mind (if not – that's ok, I think I can work finding that by myself). Hello world and showing "Maybe vs null" are few things I believe I succeeded at :)
Exactly. And the thing is, you can't just silently start walking away when you get these questions asked, you'd better be prepared.
I am not sure how much this is going to help. But [this](https://bitbucket.org/sras/haskell-stuff/src/b58f3fc017ce303fd9733184f599e916739f6ed2/snn.hs?at=default&amp;fileviewer=file-view-default) is a simple neural network with back propagation I wrote when I was learning Haskell, that can recognize alphabets 'A', 'B', 'C', 'D' written in a 8 x 8 grid pixels. The input of the network is the flattened list. The output is a list with four elements. If the input was recognized as 'A' then the first element will be nearer to 1, if it was B, then the second element of output will be nearer to 1 and so on.
The wiki has two explanations. Which one is more accurate?
It's something that has been bothering me for a while, so let me mention it here. Almost everyone uses the terminology "type X is monadic" or "type X is a monad". This terminology is inaccurate, and for me it was by far the biggest obstacle to understanding monads. The right terminology is "type X *has* a monad". The triple `(List, flatmap, \x -&gt; [x])` *is* a monad. Or is the monad instance of a type necessarily unique? Edit: after some thought, the answer is no, there are types with multiple different monad instances that nonetheless satisfy all the laws.
Mainly because the `f x y` syntax is usually just awful compared to `f(x,y)` and this is one way to eliminate the distinction between curried and uncurried functions. Church booleans and church naturals don't quite have this same boon. Also, if you treat it as the church encoded analog, you can do Nermele-style holes in the tuple while still passing it around like a first-class thing. `(3,_,5)` becomes a value without needing to be applied.
I don't remember the exact details, buy I changed the version of a package in my cabal file and stack just kept telling me I had the wrong version. I dug around in stack --help to no avail, so rm -rf got me there in like 30 seconds. Next time I'll submit a report with details.
Yes: Prof. O'Neill was the instructor when I took CS 70 (Data Structures and Program Development), which ended up being one of my favorite classes!
This is a good point. We're not very good at making these subtle differences in our terminology.
So basically, you're complaining because people don't break *enough* things on a major version bump? The point is, if semantic versioning was used, it wouldn't be `diagrams` 1.1, 1.2, and 2.0. It'd be versions 9.0, 10.0, and 11.0.
Also, in your IO, since a (f b) is always equal to `a . f $ b`, even with IO, your IO is equivalent to: exec = do input &lt;- readFile "input.txt" putStr . createIndex $ input ... and this do notation is actually syntax sugar for the following: exec = readFile "input.txt" &gt;&gt;= \input -&gt; -- a lambda of your variable name putStr . createIndex $ input ... which is just another kind of chaining. This time, you can use &gt;&gt;= or =&lt;&lt; (which is just the same but with arguments flipped) to chain them instead of (.), producing: `exec = putStr . createIndex =&lt;&lt; readFile "input.txt"` The trick is to see that here (=&lt;&lt;) is specialized to the type `(b -&gt; IO c) -&gt; (a -&gt; IO b) -&gt; (a -&gt; IO c) where (.) has the type `(b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)` which is the same modulo the IO. Also of note, the ($) has disappeared. This is because function application binds more tightly than operators, so `a . f b` means apply f to a, and then compose that function with b, while `a . f $ b` means compose f with a, and then apply that to b, so ($) was needed. Since (=&lt;&lt;) has a lower precedence than (.), it can serve the place of ($) here, and the resulting statement putStr . createIndex =&lt;&lt; readFile "input.txt"` says 'compose createIndex with putStr, then read the contents of "input.txt" and pass that into the created function'.
would love to see workflow on how it can/could integrate with stack
This is where we start to see the inherent limitations of semantic versioning. Since one version number is being applied to a whole API there's no way to know how much of the API changed in each major release. The reality is that the majority of he API has stayed the same the whole time. It would be nice to have more powerful tools that would check to see whether any of the API functions that you are actually using changed. The problem with that is it can only be applied to pairs of packages. We still need a way to succinctly characterize the API of a single package. Another alternative would be to make packages smaller (fewer API functions being described with a single version number), but that comes with its own set of difficulties. So far the idea of semantic versioning (which includes the PVP) is the best thing we've come up with and actually works quite well in practice.
It shoudn't be too hard to make a such tool : extract all the public symbols and the signatures from a given package. Then it's just doing a diff with the previous version to see if things have just been added or some have been modified. 
Original dicussion here: https://www.reddit.com/r/rust/comments/3li3by/rust_and_the_monad_trait_not_just_higher_kinded/
The most interesting one, to me, is the alternate monad over [] that chooses one element from every list. I can't remember what it's called.
Adding new symbols only requires only a minor version bump. All those `diagrams` versions undoubtedly break *something* in the API, even if it's something only a few people used. To be fair, I'm not really sure *why* we do the two-major-version-component thing. AFAIK there's no consensus on what A and B mean independently and it seems unnecessarily confusing since many people will, like you, assume it works like standard semantic versioning. I also suspect that Haskell libraries are more likely to make a series of small breaking changes as releases rather than bundle them up into infrequent major breaking changes.
Thanks, that is very interesting. A lot of the details are still unclear though. For example - cabal has always had a single-version-only constraint, and that is exactly what is now being relaxed for better flexibility and more power in finding build plans. Rather than going too far off topic here, is there a link to where the conceptual approaches of cabal vs. stack as build tools (as opposed to surface UI differences) are explained in more depth?
&gt; (=&lt;=) :: Comonad m =&gt; (m b -&gt; c) -&gt; (m a -&gt; b) -&gt; (m a -&gt; c) How does this work? You plug in an `m a` and get a `b` out, but then you need an `m b` to get to the final `c`, and you don't have `return`? What am I missing?
Comonads allow you to "extend" the context so that you can feed it into another context consuming comonadic operation. Concrete example: I have a "blur" operation that takes any zipper over a 2Darray (position + data structure + way to move around, remember?) and blurs that point by mixing in the RGB values of its immediate 8 neighbors. `blur :: 2DArrayZ RGB -&gt; RGB` So blur is a function that takes a grid of RGBs and gives you back the new RGB at the focus. The comonadic extension of the grid context would run this blur function over every point in the grid and give you back a new grid where every point has been blurred. `newArray = oldArray =&gt;&gt; blur` But maybe I want to run a lot of operations like that. Like a sequencing of comonadic computations... extend is here to help. `filter array = array =&gt;&gt; blur =&gt;&gt; b&amp;w =&gt;&gt; smudge =&gt;&gt; twist =&gt;&gt; instagramify` We can write this in a pointfree style using the comonadic compose. `filter = instagramify =&lt;= twist =&lt;= smudge =&lt;= b&amp;w =&lt;= blur`
 instance Monad [] where return a = [a] as &gt;&gt;= f = f (head as) If you newtyped Oracle a to be some list of as paired with a random number generator... you would have an even more interesting monad. :o
Hunh. Does that really obey the monad laws? Even with empty lists? EDIT: No, it obviously doesn't, as duplode points out. Oh well.
It uses `duplicate :: m a -&gt; m (m a)` and `fmap`.
I strongly advice against trying to do any multiple things at the same time. The argument is very pragmatic in nature. Consider how efficient you'd be in brushing your teeth, while trying to put your pants on. Now take a notice that studying a language is quite more demanding compared to either of the mentioned tasks. You see, people can only think of one thing at a time. Even those who seem efficient at doing multiple things simultaneously actually are just good at switching their focus fast. Using the computer-world analogy, we're single-core. Doing multiple things simultaneously is like hyperthreading. The efficiency of such a model depends on your personal qualities, but it inevitably comes with some overhead caused by you having the problem of management of switches between the problems on top of the actual problems. Concerning the choice of a language to learn first, I strongly advice Haskell, but not because it is a way better language. The main reason is that unlike in case of most of other languages, including Ruby, learning Haskell comes hand in hand with evolving your skills in problem decomposition and abstraction. From all my experience I have concluded that those two skills are the key qualities of a good programmer. Mastering them is what will make you good at any language afterwards. Some languages (Haskell) can be way more expressive than others (Ruby) and have other benefits, but essentially they are just a tool for expressing your thoughts, and it's your thoughts what matters essentially. Haskell makes you learn to think properly.
Another scientific programmer here. On the one side, I can tell you that most of my production code is still written in Python, so I'm not going to say that Haskell is ready today, despite having drunk the kool-aid. On the other hand, I do see a lot of progress with it that makes me hopeful for things to come. Here's a bit of a scatter shot. * Trivially parallelizeable problems (which I encounter constantly) are now trivial to parallelize. I usually don't need to change more than a single line to move my code to multiple cores. Similar changes to Python code takes more work. I can't comment on systems which are tricky to run in parallel, but at least the simple cases are dead easy. * Handling huge, streaming data sets tends to be easier. In python, I can do everything through generators, but it always feels like swimming upstream. Haskell makes these kinds of problems almost fun. * The accelerate library has the potential to be huge game changer. Prototype your code on your processor in Haskell, then run that same code on your GPU. Being able to write GPU code in a higher level language is glorious. * I don't know what kind of data science you work on, but, if you're doing physical sciences and controlling lab equipment, concurrency in Haskell just makes sense. I'm still pretty much a schlub at Haskell, but getting control of a motor system over a serial port was far easier and friendlier than it had been in Python. I now have a private little library that will provide either terminal or gui control for a variety of motor controllers, both providing live feedback. Adding a new motor controller usually only takes two trivial lines of code. * Running Monte-Carlo calculations under Haskell feels much prettier with the Random package. It's not that there's anything that you couldn't to in another language, but it encourages you to think about the problem in a different way. With Python, I always thought about generating random numbers, then combining them to get the distribution I want. With the way that random numbers work in Haskell, I find it more natural to think of it as defining the distribution I want, then generating numbers from that distribution. It's mostly a philosophical difference, but I find it maps more naturally to my problems.
I'm by no means an expert but I'm pretty sure not all Traversable types are Applicative (and hence Monadic). Mapping over all the elements of a container doesn't change its shape. But if you use bind you concatenation the results of crating sub containers. This is not unlike flatMap in other languages. The problem is with something like a Map or Set. In Set if you map everything to a Bool value the set will have at most two elements changing the shape. Map has the problem of not having pure/return. So unless I'm confused about what your asking, you can't get a Monad instance for something just because it's an iterator. Put another way, you don't get Monad for free just because you have Traversable. There aren't many (none that I can think of) situations where one type class inherently gives rise to another. Perhaps someone with better type-foo can give a more accurate response. 
What does that mean? Other than "Nix-style package management is awesome and can do anything!". (I'm not being sarcastic, btw.)
In this case, it's essentially synonymous with join.
&gt; What does that mean? It means that Nixpkgs (the community-driven default package repository of the Nix package manager) now contains the Haskell packages from various Stackage releases. This enables people, who are using Nix for Haskell development, to easily build their projects against packages from a specific Stackage version. Before, the only option was to build against a package set comprising all Hackage packages, almost exclusively in the most recent version they could be found on Hackage.
My first impression: yikes. This looks harder than Scala levels of complexity for dealing with generics, something I did not think possible.
And, it also means they don't have to build these packages locally, just get the binaries from Nixpkgs. Is that true?
This is incredibly cool. Nix is what got me into Haskell after many terrible cabal experiences and Peti is totally rocking it for Haskell/Nix at the moment.
What a beautiful paper!
I don't know if the following matches your experience, but what you describe looks a lot like collateral effects of analogy-driven monad tutorials ("a monad is a container", "a monad is a factory", etc.). As the author talks about things that have no immediate connection with the code, the reader is led to look for more, for a mythical quality which makes something a monad... and which of course does not exist. That is why, however one chooses to introduce monads, some variation of "`Monad` is an interface" must be one of the very first things to be said. As for the specific turn of phrase "`[]` is a monad", I fear it would be very difficult to police its usage (when compared to, for instance, that inexcusable gaffe, "this function returns a monad". If you hear anyone saying that, please point it out). I gather that the two main complications are: * [As ninereeds314 points out](https://www.reddit.com/r/haskell/comments/3llt0g/so_ive_deduced_my_datatype_is_monadic_what/cv8bgpc), saying that "`[]` is a monad" when talking about Haskell (as opposed to category theory) is not ambiguous, as *de facto* there is just one instance. * There is no convenient way of talking about, for instance, `(IO :: * -&gt; *, return :: a -&gt; IO a, (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b)` other than "the `IO` monad", that is, "the monad defined for the `IO` type constructor". From there, it only takes a tiny step to read "the `IO` monad" with "`IO`" being the main noun rather than the qualifier (i.e. "the `IO` type constructor, which is a monad"). That said, calls for using expressions such as "the `IO` monad" less often, specially when teaching beginners, have been increasingly common. I agree with that stance, and the issue you raise is (another) good reason in favour of it.
This is the first serious side project I've done in Haskell and would love some feedback. My background is advanced functional Scala (eg. typeclasses, HKTs, etc) so no need to pull punches. Thanks in advance!
It's possibly possible to do a "Finally, Tagless" method. class CoreTT m where app :: a -&gt; a -&gt; m a universe :: Natural -&gt; m a class DevTT m where hole :: Variable a -&gt; m a class OpTT m where sseVect :: String -&gt; m a type Foo m = (DevTT m, CoreTT m, OptTT m) universe1 :: Foo m =&gt; m () universe1 = universe 1 This feels nice for this particular example but note that you'll eventually have to realize the `Foo` stack, but when you do the structure you choose can be defined all at once. Before you do that you can pass in and out of "classy" representation and also write functions like `Foo a -&gt; X a` in your notation as instances of `Foo` in my representation for a type `newtype CompX a = CompX (X a)`. As long as you can formulate your algorithm as a simple catamorphism of `Foo` then you won't need to go through an intermediate concrete tree.
Hell yes! This was one of the main reasons that I stopped using nix; time for a reevaluation?
In large part because typeclasses, as a design compromise, aren't good at *expressing* these things. Typeclasses force you to choose a single canonical instance for any given type which leads to lots of useful properties but can also muddle thinking if the instance is *not* unique.
Neat. Examples in the documentation (perhaps at the top of the `Text.Mustache` module?) would be great, especially for people who aren't immediately familiar with mustache templates. Perhaps you could use [doctest](https://hackage.haskell.org/package/doctest) to kill two birds with one stone? Somebody contributed doctests to my [modular arithmetic](https://hackage.haskell.org/package/modular-arithmetic) library and they were pretty awesome.
&gt; That would be the opposite extreme of the situation in the cabal hell days. ...the opposite would be cabal paradise then? ;-) Anyway, I was referring to the paragraph quoted below from the blogpost. This is basically the behaviour you'd get today if you reset your sandbox everytime you `cabal install --dep &amp;&amp; cabal configure` (unless I misunderstand what /u/dcoutts is working towards). &gt; He also wants to make it so that `cabal-install`'s install plan doesn't depend on the local state of the Nix database: it should give the same plan no matter what you have installed previously. This is done by dependency resolving without any reference to the Nix database, and then once IPIDs are calculated for each package, checking to see if they are already built. This plan would also make it possible to support cabal install `--enable-profiling` without having to blow away and rebuild your entire package database. 
(=&lt;&lt;) is actually the monadic version of ($), not (.): (=&lt;&lt;) :: (a -&gt; IO b) -&gt; IO a -&gt; IO b ($) :: (a -&gt; b) -&gt; a -&gt; b For monadic function composition, there is [Kleisli composition operator](http://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Monad.html#v:-60--61--60-): (&lt;=&lt;) :: (b -&gt; IO c) -&gt; (a -&gt; IO b) -&gt; (a -&gt; IO c) (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) (signatures specialized to IO for clarity)
Definitely. Racket has a superb ecosystem for learning materials that are (in my opinion, as this was the case for me) far more appropriate to what a newbie would actually benefit from. The Haskell type system is a big pitcher to swallow; perhaps it's better to start with arbitrary datatype lists and recursion.
Oh awesome, I had no idea this existed. I was looking for ways of finding the exact specs, this makes my life a ton easier. I'll tell you once I've tested it.
Knowing NixOs hydra will get those binaries, *in time*. HEAD is notoriously and understandably binary-free, unstable... well, on the well-treaded paths you generally get them, only stable has 100% coverage. They're going to, as usual, prioritise things and packages are going to roll in as they're built. I'm not sure whether I'm going to stop using specific nixpkgs snapshots for development, though: { nixpkgs ? import &lt;nixpkgs&gt; {} }: let mynixpkgs = import (nixpkgs.fetchzip { url = "https://nixos.org/releases/nixpkgs/nixpkgs-15.08pre67216.20d6f87/nixexprs.tar.xz"; sha256="0shz2m15ajpabln9qrbii7ds8z5nwi7ryap6g83lks6m8sriqip9"; }); in with mynixpkgs {}; let drv = pkgs.haskell.packages.ghc7102.callPackage (import ./.) {}; in if pkgs.lib.inNixShell then drv.env else drv ...even if I'm going to use a stackage release. It's not a good idea to tie your dev environments to your OS-level nixpkgs, I *loathe* random bitrot.
Hi.. I am Vishal, the author author of original code which has been part of GSoC project. I know its kinda late to comment here, but I was completely unaware of this and the previous discussions and the blog posts till now. It is great to see discussions and posts regarding this. Working with haskell this GSoC had been a excellent experience :). It is the best community that I have seen. Thank you all, specially my mentors, Duncan Coutts and Edward Yang for that. If you have any questions regarding no-reinstall or anything other regarding this project, I will be more than happy to answer.
This is really nice. My first interesting project in any language was writing a desktop client application for working with this api. That was years ago. I'm glad there's a cleaner way to do it these days. I recently tried [porting it to Haskell](http://github.com/johntyree/traduisons-hs) and ended up with an unprincipled mess. I might write one for Bing and try this approach as well.
The Unicode lookup seems very similar to detexify, which is also in Haskell.
I am in love with Servant. And I feel validated that this talk goes over many of the same points I covered [in my talk](https://youtu.be/Bnf7AmoQ6lc) (and [associated blog post](http://taylor.fausak.me/2015/08/23/type-safe-web-services-in-haskell-with-servant/)). If you're curious about Servant, [the tutorial](http://haskell-servant.github.io/tutorial/api-type.html) is excellent and a great place to start.
I agree – Servant is fantastic. My only minor complaint is that you *do* have to think about HTTP. You write: &gt; Notice that the handlers don’t really have to think about HTTP … yet in the line before you explicitly return an HTTP error code (404). :) Having been spoilt by Webmachine-like libraries it feels weird having to explicitly remember and return HTTP codes and remember semantics about when to return a 201 vs a 203 and so on.
We struggled with these issues of excessively cautious bounds. When using cautious bounds, you can't tell whether something is just caution or whether we actually *know* that this is a breaking point. To me, that is the *primary* issue: the lack of distinction between marking known breakage versus just conservative caution. We ended up deciding to remove all caution-based bounds entirely (now that there's cabal freeze and Stackage and Stack…). For all *known* breakage points, we not only mark the bounds, we add *comments* in the .cabal to explain why this is a breakage so that if something changes (on either side), we can understand when it's safe to drop that bound. If nothing else, I *urge* everyone else to consider making comments in .cabal to identify the source and type of a bound (cautious vs known breakage, why)
no risk, no fun!
I think that's the wrong interpretation of bounds. Version bounds are a statement to the world that you have tested your project with the specified versions and expect it to work. With this interpretation the idea that the bounds are "pre-emptive" or "excessively cautious" doesn't even make sense. Some may argue that this is not how cabal behaves, but with the `--allow-newer` flag users can easily ignore certain bounds, making this a perfectly reasonable interpretation. But if you insist on not using my interpretation, the problem with your request is that it depends on information not yet known. If I release version 1.3.0.0 of a library today and it has `text &lt;? 1.4` (a notional syntax for a cautious bound), and then text 1.4 comes along and it turns out that it made a breaking change to something I use, then the `&lt;?` bound is now wrong. On the other hand, I can't even use the bound `text &lt;! 1.4` yet because I don't know that 1.4 is going to break me. So in practice you should just assume that bounds are always cautious. Therefore `&lt;` is effectively the same as `&lt;?`. If we had a `&lt;!` notation for a known breaking bound I suspect that operator would not be used much because maintainers would have to go back and retroactively add it to old versions of their package. I'm doubt many people will take the time to do this. It's also not obvious that it's even possible for the solver to make effective use of this information. That being said, I have argued for the addition of `&lt;!` before and I don't think it would be bad to have it available.
Thanks for highlighting sized generators. That's one of the areas I've been unable to understand from reading the library documentation. How does the `sized` function obtain a value for the size parameter?
&gt; To me, that is the primary issue: the lack of distinction between marking known breakage versus just conservative caution. I was thinking the same. There is indeed no way to tell if a bound is a guess or if it's been tested and we are sure it doesn't work with it. Maybe a new syntax like `package &lt;? 1.3` or something would help. That will allow either solver to remove guessed bound if needed (with a special flag) or users to push them manually if needed.
One feature: the web api you create is type checked. If the server can handle a GET request at `/hello` with parameters `yarr` and `snarp`, then that information is reflected *in the type* and you can have compile time guarantees about routing correctness.
The type of an API can also be used to derive a client, [both in Haskell](https://haskell-servant.github.io/tutorial/client.html) and [in JavaScript](https://haskell-servant.github.io/tutorial/javascript.html) (though the JS generation isn't quite perfect). The API type serves as a single point of truth about nearly every facet of the API: what it responds to, what it returns, what headers are necessary, etc. The content-type information is powerful enough that you can [run an image conversion service just about entirely through type inference](https://haskell-servant.github.io/posts/2015-08-05-content-types.html). All of your route handlers must properly handle the parameters given to them, and this is enforced by the compiler. Since the API definition is just a type, you can do anything as you'd normally be able to do with a type. Defining a sub-API is super easy: type UserAPI = "users" :&gt; Get '[JSON] [User] :&lt;|&gt; "users" :&gt; ReqBody '[JSON] User :&gt; Post '[] () defines the User API, and if you want to plug that into your application, you just use the `:&lt;|&gt;` combinator: type WebAPI = UsersAPI :&lt;|&gt; OtherAPI You can [define CRUD as a type](https://hbtvl.wordpress.com/2015/06/28/servant-persistent-and-dsls/) and do `type MyAPI = CRUD User :&lt;|&gt; CRUD Post` and have all of that boiler plate taken care of. 
I might be biased, but: The servant type-level DSL is all about describing your API. Servant can then verify (via type-checking) that your implementation matches the specification you've given. This happens because your handlers etc. are more strongly typed than in most web frameworks; their types actually state the deserialized (or not-yet-serialized) types they expect (via bodies, path captures, headers, etc.) and return. But having an API description is not just a way of ensuring you don't go wrong. It also simplifies a lot of things. For example, you can generate documentation or client libraries (or indeed, an entire Arbitrary-backed mock server) with little extra work. And the routing to your handler is already figured out as well, as is how to do serialization and deserialization, picking stuff out from the headers, etc. The end result is that you get to write handlers that (as taylorfausak mentioned) feel like Haskell should feel - in the land of tight types. It's not all `SomeMonad ()`. And your handlers are eminently composable: type API = "users" :&gt; Get '[JSON, PlainText] [Users] :&lt;|&gt; "user" :&gt; Capture "age" Age :&gt; Get '[JSON, PlainText] User server :: Server API server = getUsers :&lt;|&gt; getUserByAge data User = User { ageOf :: Age, ...} getUsers :: EitherT ServantErr IO [Users] getUsers = ... getUserByAge :: Int -&gt; EitherT ServantErr IO (Maybe User) getUserByAge age = getUsers &gt;&gt;= return . (find ((age ==) ageOf)) Conal Elliott talked about how unfortunate it is that UI is where compositionality goes to die. So was request-handling. But not with servant. Not only do they compose with one another, but they don't even have anything HTTP-specific in their signature! (And the IO isn't an escape hatch so you can pass info to the framework via an IORef - it really just is there in case you want to, you know, access a database.) Notice also how the API description above says we can return JSON. Servant checks that in fact there's a ToJSON instance for `User`. So you have safety. Given that we can then assume this, servant just uses Aeson's `encode` to return the body (if that's the content-type requested - otherwise perhaps text/plain). So you have convenience - you as an end user don't have to check the `Accept` header and program accordingly. The insight, I'd say, from a specifically web perspective, is that an API is both the right thing to use for types (it's a higher-level notion of correctness), and the right thing to reify (it's what a lot of different tooling would like to know about). Conveniently, the technical solution servant uses - an interpreter for a type-level DSL - also gives us an *extensible* DSL (in both directions of the expression problem). This has been an absolute wonder for Servant development, since it makes things quite modular. And it also gives users a lot of freedom - there's very little you can't do because you'd need to modify the library itself. This extensiblity also compares very favourably with API description languages such as API Blueprint, Swagger, RAML, etc. (As does call-site type-checking.) *EDIT* : formatting. *EDIT* : As for an example, http://haskell-servant.github.io/posts/2015-08-05-content-types.html shows specifically how nice it can be to have the application handle things like content-types for you. I should also say that we've had a very large number of contributors, and I think that in part we have the extensibility aspect of the DSL to thank for that - you can extend servant within your application, and if you think your extension is useful, you can just copy-paste the code into a PR. 
&gt; Version bounds are a statement to the world that you have tested your project with the specified versions and expect it to work. But if the latest is 1.3.5, and you write &lt;1.4, you haven't *actually* tested 1.3.6. Sure, it *shouldn't* be breaking, but you didn't test it. All bounds are either tested or not. But there's currently no distinction, so it's appropriate to call them "speculative" or "cautious" when they aren't strictly the actual tested range. &gt; then text 1.4 comes along and it turns out that it made a breaking change to something I use, then the &lt;? bound is now wrong What if it comes out and the 1.4 is fine and *doesn't* break? Now your `&lt;1.4` is wrong. We already have the case where you need to manually update the bounds to actually reference what has been tested. Things still get out of date without manual updating. &gt; That being said, I have argued for the addition of `&lt;!` before and I don't think it would be bad to have it available. Okay, agreed! I wasn't proposing what the solution would be. My actual preference would be use bounds *strictly* to be "this is the range *actually tested and known to work* without any speculation about points in the future. The information could simply cover the precise versions, not just the major versions. In that sense, if I tested 1.3.5 but not 1.3.6, it should indicate such, and just knowing this is much more information and also enough to know that probably 1.3.6 will be fine and we're less confident about 1.4. It's much more clear information to say "this was tested through v. 1.3.5" than to just have a `&lt;1.4` in the code which has less information. In summary, info we want (for both upper and lower): * known tested breaking points * known tested success range I don't see the value in .cabal having fuzzier information than that. The way the dependency finding is done can be fuzzier but needs no more input than those two points.
I'm not sure I think "automatically instantiating Monad instances for all list like things" makes sense, at least if i'm understanding the question. I'm sure we can all agree that lists are list like. We should also consider that, although there may be a great many functions whose type is `(a -&gt; b) -&gt; [a] -&gt; [b]`, only `map` satisfies the functor identity law, `map . id == id . map`. On the other hand, if i have an operation, `&lt;*&gt; :: [a -&gt; b] -&gt; [a] -&gt; [b]`, there are two (non-trivial) ways to get a `[b]` out of these ingredients. On the one hand, I can take a cross-product, and on the other, i can work pairwise. One obvious difference is that these operations have quite different *units*. For the cross product, we only need a singleton list (1 * n == n), but for the pairwise operation, we need an infinite stream of the same value (max (n, inf) == n). together, each of these apply and pure operations form an applicative. If we consider these operations, we'll note that a monad is automatically an applicative, since we can give use `return` for `pure` and `\m1 m2 -&gt; m1 &gt;&gt;= \f -&gt; m2 &gt;&gt;= \x -&gt; return (f x)` for `apply`. We would like, if some type is a monad, to also be an applicative in the same way. For the case of cross-product apply, this is no problem, `(&gt;&gt;=) = flip concatMap` for lists also works as a cross product, and the above definition for apply gives us the desired behavior. However, there's no definition we could give for `&gt;&gt;=` that will give us a pair-wise `apply`, even though there's a perfectly good implementation for apply itself! `ZipList` *fails* to be a monad, even though it's as list-like as can be!
`instance TCA t =&gt; TCB t where ...` is a bit troublesome, which is why it requires extensions to work at all. Normally, this either means that the type class inheritance hierarchy needs to change (which requires a bit of coordination in the best-case scenario) or that your instance isn't really unique; type class instances are supposed to unique -- this coherence condition is necessary to prevent some very surprising behaviors (at one point including enabling an unsafeCoerce). In the later case, a wrapper of some type is generally recommended. Something like the `Sum` or `Product` newtype wrappers in `Data.Monoid` could be done in many cases. Alternatively, you could have a data type that exactly captures the monad operations (think MonadOps from Scala) and a way to generate an value of this type from and `Traversable` value, e.g. As another alternative, you could so with existentials via GADT, similar to `CoYoneda`.
NICE! That was a personal project of mine, too. Glad to see it was handled better than I would have :)
/u/kosmikus was a little humble, but he contributed hugely in the past few months. Most of the major issues we've had and didn't quite know how to solve, he did (and usually very quickly). My development pattern has in essence become: try doing it and if it's difficult, complain on the servant freenode channel until kosmikus takes a look, then type up his suggestions. Indeed, there are a backlog of good ideas he's had that I've been desperately trying to find the time to implement.
You have to be very careful to get a law-abiding Monad out of `[]` that isn't the normal one. I *think* the one where `join` is diagonalization works. I'm pretty sure the one where `join` is `concat . transpose` also works. But, one or both might violate associativity of `(&gt;=&gt;)`.
Use `(&amp;)`? Or does it have the wrong precedence?
I'd imagine that any reactive/dynamic implementation you're thinking of is possible with Haskell/Servant. What do you have in mind?
You're right. I misinterpreted what you said about "breakage points" - that would be combinations of versions of a *set* of packages which result in build errors. Obviously, you cannot test all combinations... Tough...
The type level is compile time data, It can not access to runtime data so it can not express some behaviours. 
`(&amp;)` has the right precedence. But I think `"bananas" &amp; reverse` is not as easy to understand as `"bananas".reverse`. 
Well it is composability in the way of a sum or product types. It is data or data description composability, not the composability of monadic/applicative or algebraic operations that return values. Is a static description of an aggregation of functionalities, not the building of more complex functionalities from simpler ones. Or at least it seems so. Sorry If I´m wrong
this just looks like a good way to rewrite something perfectly readable into a slightly less-so one liner :) 
Yeah, it isn't just signalling a type of failure, `Maybe` signals that type of failure. It is encoding HTTP specifics into our apps. I don't want to return a 404 error in my command line utility that is used to run cron jobs. But right now servant still pushes HTTP details like this into application logic code.
I disagree slightly. I don't think `Maybe` can capture all of the various failure modes that HTTP requires. Sure, if the resource wasn't there, return `Nothing` and turn that into a 404 somewhere down the line. But what if the resource was deleted and you want to return a 410 instead of a 404? You can't express that with a `Maybe`. 
I guess it depends how long you spend doing what style of coding. To me, the original is actually (marginally, it's still trivial) harder to read as it requires that you mentally allocate meaning for this particular piece of code, whereas my version says "do x, then y, then z" without any unnecessary information.
Er, right. I'm going to blame that one on a lack of sleep.
"the first projection of a pair."
Stack chose to solve different problems than cabal, in a different way. As such it is optimized for certain workflows, and _nails_ them. Lots of this is tied to stack _not_ trying to solve the difficult (but I think still solvable) problem of solving dependencies generally, but instead relying on a curation model. Duncan's post from last year (prior to stack, but covering the "stackage model" addresses some of this: http://www.well-typed.com/blog/2014/09/how-we-might-abolish-cabal-hell-part-1/)
See this issue: https://github.com/haskell/haskell-platform/issues/205 Long story short, the new OS X "rootless security" means that we can't write to /usr/bin but only /usr/local/bin. This is a fix that will have to be rolled into the next platform release.
I think that my example below is clear enough
You can use `servant` to write both clients and servers. Here is a pretty clean example of using `servant` to write Haskell bindings to the Google translate API: https://github.com/dmjio/google-translate/blob/master/src/Web/Google/Translate.hs
&gt; My minor complaint is that the marketing could use some more focus on normal web development. Due to the initial focus on JSON api apps, everyone I mention servant to thinks that is all it is for and has no idea it is a general purpose framework. Point well taken. On the other hand, JSON APIs are normal web development nowadays.
What if you tested only 1.2.4, 1.3.2 and 1.3.5? How do you annotate that e.g. 1.2.7, 1.3.3 and 1.3.4 weren't tested? But let's assume we had that additional hints you suggest for upper/lower bounds. How would the solver take those into account? What would be different from our current `&gt;=`/`&lt;` bounds?
That is the kind of composability that I need for calling it composable. I mean to compose handlers, so that I can create bigger units of web functionality out of simpler ones. There are some web frameworks that do it. Well I know just one. And there were more a few years ago. What Is happening with functional programming. Is it going backwards?
Haven't seen one. If you create one and append it to that ticket for reference until the new platform release, it would probably be useful to lots of people :-)
Just go to IRC and ppl will help you with all your struggles
Putting stack size constraints in the test suite is a great idea, assuming they're valid cross-platform and so on! Testing with `-O0` sounds counter to what I've heard done by others; is it generally a bad idea to _rely_ on `-O2` for optimisations?
The various articles/docs could probably benefit from such updates indeed! If you have any of those in mind with your comment, please send me/us the links so that we can update them soon.
https://www.codementor.io/haskell-experts
Do you know of any other framework that accomplishes what you want from Servant?
[removed]
\#haskell (freenode) is a bit high traffic. \#haskell-beginners is a great place to hang out and get answers too. I agree with you this is the best community that I have been with, but that may not be saying much since I used to hang out in \#\#c most of the time.
https://github.com/helium/airship I haven't tried it but it is inspired by webmachine and claims to handle the details of HTTP for you.
What about `x y f`? Imagine if Haskell had've used reverse polish notation by default, Joy-style.
Thanks for the correction.
[removed]
I'll see if I can't bang this out tonight/tomorrow. Thanks!
I think it makes sense to still use bounds that signify, "I tested versions as old as 1.2.4 and as new as 1.3.5". That's useful enough and specific enough. If it was *known* that 1.3.3 was broken, there should still be a way to indicate that. The difference is more informational for developers than for the solver tool. The solver, as I said, wouldn't need to be that different. It could easily see that 1.3.5 was tested as working and *treat* that as `&lt;1.4` effective *depending on how cautious the settings for the solver are* — people could set extra cautious "only if tested" or exta loose like the ignore upper-bounds argument thing. Overall, I do think the ability mark points of known breakage is the more valuable thing vs the (also valuable) ability to distinguish between actually tested vs speculative.
My expectation would be, "test with `-O2` for accurate benchmarks; test with `-O0` for precise semantics."
Agreed, but if you benchmark at O2 and share those results you should add it to your ghc-options so library users see the same performance. 
Thanks Neil! Your "space leaks tales" have been one of the pillar of my Haskell education ;)
Significant reworking of the symbolic floating-point (SFloat/SDouble) types and a few new examples. Change log: http://hackage.haskell.org/package/sbv-5.0/changelog
It's exciting, but still in alpha. 
That is good, but the type level description does not determines the code below so it can not be generated by it, since there is an infinite quantity of runtime programs that match the type description. So the claim of "we can generate the program from the types" is not entirely backed. unless you adhere to some rigid interpretation: if John appears int he signature it means that the generator should check for the /= condition (which is just one of many possibilities) . That's is the inherent rigidity that I mention. Of course more complex type signatures can be done to encode this, but that also would force the generator to be updated for new type constructions until a turing complete language running at compilation time is made. If that can be ever possible. I wonder how monadic binding or state can be coded that way. And I wonder the advantage of this over doing it at runtime with haskell.
I'm still not following. What transformation turns `go (x:xs) = f x (go xs)` into a constant stack operation? I see that the strictness analyzer knows not to make a thunk for either argument, but shouldn't that just mean it evaluates `x` and `go xs` beforehand, with `go xs` requiring a stack frame?
With people not familiar with functional programming in general, normally just showing them map, filter and folds will do the trick. And you'll have time to explain why purity is good too :-)
Are they using elm yet? I would like to hear "haskell failure cases" too. "we tried haskell and it didn't work because X"
Is there a java client? Sounds like a fun thing for me to build if it's not been done yet. 
OK, agreed, my comment explains a little bit of the story, but it still looks O(n) stack to me. I've commented on the ticket https://ghc.haskell.org/trac/ghc/ticket/10830#comment:12
&gt; Of course more complex type signatures can be done to encode this You might be interested in dependently typed programming languages. See [Agda](https://en.wikipedia.org/wiki/Agda_(programming_language)) and [Idris](https://en.wikipedia.org/wiki/Idris_(programming_language)). The short of it: &gt; With dependent types, it is possible for values to appear in the types; in effect, any value-level computation can be performed during typechecking.
I haven't looked at Airship before. It looks awesome, but [the resources](https://github.com/helium/airship/blob/667f768e7de244a27a23bde316991def54c9d952/example/Main.hs#L61) have to know about HTTP methods and content types. 
Just goes to show that fantastic technology can be used to underpin a massively overrated piece of fluff that students hate. All that zooming in makes you feel nauseous especially with a stella and kebab hangover. 
I see there a single level routing composition. i see alternative routes that do not interact among them except sharing mutable state. I don´t see compositions with &lt;*&gt; or &gt;&gt;= 
&gt; Otherwise you run the risk of having to query the same information from external sources (like a database) multiple times, once in each of those separate functions. I've mostly used [cowboy](http://ninenines.eu/docs/en/cowboy/HEAD/guide/rest_flowcharts/) (Erlang) and there you simply thread a state variable through the calls, so each function returns their value together with a (possibly updated) state. So you could for example have state `Maybe DbConnection` and in `service_available` you would attempt to open a database connection and if successful return `(True,Just myConnection)` – or use a state monad to avoid manually passing state. Basically, anything you might need later is passed as explicit, updateable state through the chain of function calls for a given request.
Yes, but with the arguments swapped.
At Well-Typed we offer [Haskell training](http://www.well-typed.com/services_training/), usually via in-person courses, but we'd happily provide remote tutoring as well.
+1 for the work, and another +1 for the transversal analysis of Stackage
Writing a GHC plugin was much harder than I was expecting. I was expecting manipulating core to be roughly as easy as using template haskell, but it was significantly more challenging for me. The GHC-devs mailing list (and in particular Edward Z. Yang, Andrew Farmer, and Ömer Sinan Ağacan) were very helpful in answering my many questions.
Interesting, are there any good write-ups about numerical instabilities in general to help avoid or at least recognize them when programming?
I think Haskell library quality (include those pre 1.0 versions you mention) is generally much higher than JVM libraries, particularly outside the Scala ecosystem. Haskell libraries also tend to mostly avoid the over-engineering Java authors seem to love. I also think Scala has had to make too many compromises to accommodate the JVM low level Bytecode structure. I would use Haskell for both and wouldn't even consider Scala unless you are fighting an uphill battle against a team which only knows Java so far.
&gt;others that have not made it to version 1 yet. Don't think that means the functionality is underdeveloped :) Haskell is actually *very* nice for web applications: * Servant is an outstanding library for defining RESTful APIs. * Hakyll is a very, very nice static site generator. A little underdocumented, but that is improving. * Yesod is a bit of a "kitchen sink included" web framework that has a good community around it. * Scotty is a great microframework for clean-slate development. * Clay is a powerful CSS preprocessor like SASS. * There are many templating libraries available, such as Blaze. * Reflex is a FRP-focused library to generate React code. * Elm is a separate language that also focuses on FRP-based web development. The Mario demo is pretty amazing. * GHCJS allows you to compile right about any Haskell code that does not depend on C code down to Javascript. Example: http://markup.rocks . * and lots more
I agree that Scala did compromise to maintain JVM compatibility but the Libraries written afterwards purely for Scala I don't think you can find equivalent in Haskell but maybe I'm wrong. Slick, Spray, Json4s they all allow you to describe your models in a compile time checking fashion. Kafka, Spark, Cassandra all provide scala front ends. I think it's good to have tools like that at your disposal especially for scale. 
&gt; others that have not made it to version 1 yet. That's very good to hear, because it doesn't apply the same in Javascript where everything is 0.* and that means functionality as well a lot of the times
I second this recommendation, but to be honest, recognizing numerical problems in code is extremely difficult. I cannot do it easily, despite writing Herbie and seeing it run on many examples. There are classic problems, like subtracting similar large numbers of the same sign. But then there are the surprising ones: overflow, correlation, and so on. One of the reasons my colleagues and I wrote Herbie was because manually finding and fixing numerical problems is beyond the capabilities of most programmers—us included.
It looks like trolling, but if it isn't, you should take a look at [this document](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md). I think that database part of the document is unjustly undersold. It's rated as "immature" because of the lack of "good" ORM solution. I am not sure such a thing exists anyway. I found using `esqueleto` much better than Slick. That wasn't hard, because I rely on type inference a lot when writing code, and it never worked with Scala when I needed it.
&gt; having been burned with spray / akka. Could you tell us how you got burned?
Herbie does discover log-space transformations, though if you were computing factorials, I'm guessing you were using recursive calls, which are still imposingly difficult to handle.
Yes, agreed. But there are other higher priority issues that we need to address first.
many haskellers are used to versioning schemes that converge asymptotically to 1.0 which are very popular in the Linux community. I could just as well use 1.0.0 instead of 0.1.0 for initial releases or some other arbitrary number. But I'm certainly not going to artificially bump to 1.0 just to attract a few more users based on some arbitrary convention in another ecosystem I disagree with. 
Is this the stuff from UW? I remember something similar from a presentation to Microsoft interns this summer. I'd like to do a Master's degree there, but the professional experience requirement is turning me off. I'm pretty sure I want to do a PhD, in which case spending two years in industry is a waste of time; but I don't feel comfortable starting my PhD with only an undergraduate degree, so I'd like to do a Master's first. 
By the way, note that the package versioning scheme expected of Cabal packages on Hackage treats the first two components as the major version, not just the first component, so in theory when you see something like version 0.1.2.0 on Hackage that's equivalent to seeing version 1.2.0 in most other contexts. In practice the "minor major" version numbers on Haskell packages tend to represent more incremental changes than major versions elsewhere, and 0.x major versions sometimes indicate that the author anticipates significant changes in later versions. But for a package like `containers` which has a stable API and a huge number of reverse dependencies, its current version of 0.5.6.3 is absolutely what would be just version 5.6.3 elsewhere.
Nick Higham's Accuracy and Stability of Numerical Algorithms is a standard modern reference. Some of the material might be a bit tough for non-specialists. But it is exceptionally well written and the first few chapters provide a very good introduction to round-off error analysis on relatively simple problems. (e.g. summation, inner products, Horner's method). After that, it covers more involved problems in linear algebra. Great book, but probably only worth looking at if you are REALLY interested in this sort of thing. One of the things that is difficult about providing concise introductory material in this area is that while the rules are the same everywhere for IEEE arithmetic, the application of those rules is different for every problem. It's difficult to avoid the need to sit down and start trying to prove inequalities for the round-off errors in the specific problem you are considering. 
I usually call this `(=&lt;*) :: (Monad m) =&gt; m (a -&gt; m b) -&gt; m a -&gt; m b`, as a visual mnemonic of smushing `(=&lt;&lt;)` and `(&lt;*&gt;)` together. Conceptually it's uninteresting and the implementation is trivial (`ap` followed by `join`, basically), the only purpose it serves is to write expressions like `doMonadStuff &lt;$&gt; foo &lt;*&gt; bar =&lt;* baz` where `doMonadStuff` has a type like `a -&gt; b -&gt; c -&gt; m d`.
I've thought about it, but I'm not sure how helpful I could be. Every GHC plugin is going to use different parts of the GHC API, and GHC moves so fast that the API changes with every release. Even to support both GHC 7.10.1 and 7.10.2 requires using CPP pragmas. The file [Herbie/CoreManip.hs](https://github.com/mikeizbicki/HerbiePlugin/blob/master/src/Herbie/CoreManip.hs) is reasonably well documented though. Seeing something like that would have certainly helped me get going faster.
Feature creep and bad requirements analysis is a common problem for all languages. I think Java with its popularity just attracts more developers, and thus more novice developers, and the cost of entry to that language is relatively low. So you end up with a lot of bad design. People who are clever enough to understand the how and why of Haskell are likely to have a bit more comp sci expertise out of the gate, i reckon.
Probably it is a selection bias, but I see the opposite -- haskell libraries often are over engineeried and poorly designed and implemented. Fresh example: the only redis client library with working publish/subscribe I found on hackage is abandoned years ago. Most libraries pretend exceptions doesn't exist. Everyone introduces its own monad for no reason. I have no idea about Scala, but Haskell is much more ricky in production then C++ or Java. Though there are high quality libraries like aeson or attoparsec.
I only have a 3.2 GPA, one semester of research lab experience, and recommendation letters from two profs. I want to do theoretical computer science, but my degree is named "B Eng Software Engineering - Internship Program with Mathematics Minor" and is taking a total of six years to complete. I'm not very happy about my chances. 
&gt; I've chosen GHCJS so that I can share code between server and client. While I'm not questioning your choice of compiler, I'd like to humbly point out that this is quite possible with Haste as well.
I agree
Just made a look at it only to find out that it's like a 100 items long :) Here's the result of a fast grab from it: * [The best Monads and Applicatives tutorial](http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html) * [The best Free Monads tutorial](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) * [Reverse dependencies of packages](http://packdeps.haskellers.com/reverse)
Is there an annotation to tell the plugin to *not* fix some code?
In both cases you have a sequence of functions being applied to an input. Using a bunch of `$`s gives you a code structure where an input is shoved into the nearest function to get an input for the next function, repeat until you run out of functions and then you're done. Using a bunch of `.`s and one final `$` glues adjacent functions together into one bigger function, again until you run out functions, then you take the one remaining function and bludgeon the sole input into submission. One major argument in favor of using `.` is that any and every sequence of adjacent functions is a potentially meaningful operation that can be factored out with a simple cut-and-paste. Using only `$` the only things you can factor out are various half-transformed inputs, which are usually less meaningful. The comment you link to is basically me saying that I use `$` whenever the stuff that comes after it is meaningful in and of itself.
This is a nice breakdown. I agree with some of these points and I can definitely understand where you're coming from. One thing that was worked very well for me is drastically simplifying my methods with Haskell. I'm very meticulous now about the complexity I'm introducing in everything from library dependencies to monad transformers, language extensions and other abstractions. Keeping things minimal is significantly easier on me and also easier on team members who's experience with Haskell may vary greatly. I'm not actually saying "This is the way you should do it", but rather posing an alternate viewpoint for anyone new to working with Haskell (in production or not). Every language suffers from some set of problems. If I picked a language that solved the problems you mention, I would probably be introducing a new set of issues to deal with. When it comes down to actually using a typed FP language in production, I feel that Haskell is a great balance between abstraction, performance, and community.
I think the issues you had with monad stacks and fancy type trickery are mostly design pitfalls and antipatterns that we need to steer people around so they don't discover them the hard way, as you did, rather than inherent problems with the language or ecosystem itself. The other issues are... thornier.
What's your background in both languages? In my experience, most people who go to Scala from Java love it, and most people who go to Scala from Haskell don't.
I agree.
Check out [react-flux](https://hackage.haskell.org/package/react-flux). It has amazing documentation. The main difference between it and other haskell react libs is that it implements flux architectrue. The entire state of the app is in the Store. Html components do not own and maintain any state and are pure functions from arguments / properties to html output. Instead they fire actions that update the Store which in its turn executes global repaint.
A lot of non-us universities work in that way. 
thanks!
I don't know why anyone thought rewriting `maximumBy` using `foldr1` was a good idea.
Sorry, I assumed you were talking about US universities.
&gt; for Scala I don't think you can find equivalent in Haskell but maybe I'm wrong. &gt; Slick, Combination of these two: http://hackage.haskell.org/package/persistent http://hackage.haskell.org/package/esqueleto &gt; Spray, http://hackage.haskell.org/package/servant &gt; Json4s http://hackage.haskell.org/package/aeson &gt; Kafka, Cassandra http://hackage.haskell.org/package/haskakafka https://hackage.haskell.org/package/cassy I only found cassy after googling "haskell cassandra". It didn't come up in hackage when searching "cassandra". &gt; Spark, Don't see one for spark I believe the rest are equivalents though, let me know if not. 
&gt; Most libraries pretend exceptions doesn't exist. Everyone introduces its own monad for no reason. Can you give some examples? I know I've heard arguments from both sides for using/not using exceptions.
I'm not sure I understand the question; SML certainly has effects. (Now, whether they are reified into the type structure is an entirely separate question; this can be done in SML just as in Haskell, and is merely a question of what you decide upon for your basis library / prelude.)
By the way, SML's effects are "effective"/computational (they are explained in the evaluation semantics), whereas Haskell's effects have to be interpreted, either in code, as with most monads, or in the execution semantics, as with IO (just like ALGOL). EDIT: depending on your perspective, you may consider the above to be either an advantage or a disadvantage.
&gt; I can assure you that it is not how it is intended. Perhaps I should have said "a funding stream for Well-Typed's important work on the Haskell toolchain. &gt; We're well aware that there's an apparent conflict of interest and that's not where we want to be. I honestly don't think it's a problem, I was trying to give an example of where there's commercial financing and influence in the established tools. I was trying reductio ad absurdum on the conspiracy-creation by pointing out that if you think that way, you end up with seeing problems even with the tools you see as belonging to the community. &gt; Our interest (as we've said since 2008) is in finding ways to channel resources into improving the Haskell tools &amp; infrastructure -- whether or not that involves WT actually doing any of that work. The IHG model has been moderately successful at doing that. If we can change the model to make that work better then that's great, and if we can remove the apparent conflict of interest then all the better. It was all set up to help the community whilst also helping industry users. It's a good thing. &gt; As for the commercial users who put in money getting to call the shots on how that money is spent, well yes of course. It's the priority-setting table for the spending of the money those organisations have contributed. That doesn't give any extra privilege compared to anyone else who makes open source contributions. Organisations contributing financially get a say in how the money is spent. It's very above board and straightforward. &gt; The CHG is certainly a good thing in that it gets commercial users to talk about what they want, but it is not a system to get resources to do anything about it. True. It's not a funding stream for anything. They're different and they help in different ways. 
&gt; I think the issues you had with monad stacks and fancy type trickery are mostly design pitfalls and antipatterns that we need to steer people around so they don't discover them the hard way, as you did, rather than inherent problems with the language or ecosystem itself. Does anyone know if there's any existing guide, tutorial, or similar writing which shows examples of antipatterns/over-engineering then presents the best solution?
That's a feature I would like to implement, but haven't done so yet. Another potentially useful feature is to pass a flag to the plugin to make it identify all of the numeric instabilities andoutput them to stdout without actually performing the replacement.
Pretty much everything. Compute shaders are not fully supported – you can create them but you can do nothing with. The queries are not complete and the sync objects are not covered (yet?).
Indeed. I was trying to point out that if you go down the route of questioning commercial influence in haskell, you end up questioning everything in the fundamental Haskell tooling. I'm trying to point out how inconsistent and illogical this kind of criticism is. Open source and commerce fit fine together and have done for years. The person I was disagreeing with had managed to make FP Complete and the commercial haskell group sound scary and worrying, but I was pointing out that you can make the more established toolchain sound scary and worrying. It's all absurd. 
None of the samples run on windows system. They doesn't even create windows.
I've [just implemented](https://github.com/mikeizbicki/HerbiePlugin/commit/1c5f8aeb99ccfbe0d15e4e9033d87e15c6383bad) this feature. To disable the Herbie plugin for a specific binding, just add the annotation: ``` {-# ANN bindingName "NoHerbie" #-} ```
I would think that you'd want to stick to a single language rather than having multiple around, unless they offer compelling complementary advantages. Haskell and Scala are similar enough that I don't think you'd gain too much by having both around, and the impedance mismatch between them would be more trouble than it's worth. I've been impressed with Haskell's libraries and frameworks for web application development. Scala's Play and Lift both seem pretty useful too. Any advantage Haskell has as a language is likely matched by Scala being on the JVM and having a wider availability of libraries. Given that many successful apps are written in Ruby or JavaScript, I'd just pick whatever you're most familiar with and use that.
&gt; Name your price let million = 1000000 in 1 million ($) :: (Num a, Num (a -&gt; ((a1 -&gt; b) -&gt; a1 -&gt; b) -&gt; t)) =&gt; t
UW CSE has two different graduate programs, aimed at different populations. The [Professional Master's Program (PMP)](http://www.cs.washington.edu/prospective_students/pmp) is for people who are currently woring at at company. It's a part-time program that lasts a couple of years, taking one class at a time in the evening. It's intended to burnish your knowledge and skills and enable you to get a better job or more pay. It does [require](http://www.cs.washington.edu/prospective_students/pmp/prerequisites) work experience. [PhD program](https://www.cs.washington.edu/prospective_students/grad) is for full-time students who want to do research and earn a PhD. You do not have to have a Master's degree in order to [apply](https://www.cs.washington.edu/prospective_students/grad/application_info); some applicants do, and many applicants do not. 
&gt;Is it the monoidal nature of m that allows it to collapse the structure through join? That question is a bit above my pay grade, but I'll try to answer it. The question isn't so much whether you can get a join at all, but whether you can get a join that obeys the monad laws. join is defined as: join :: m (m a) -&gt; m a join x = x &gt;&gt;= id It has to obey [4 laws](https://en.wikibooks.org/wiki/Haskell/Category_theory#The_monad_laws_and_their_importance) The article goes through them all. The gist of it is that: yes, you do need `&gt;&gt;=`'s monoidal properties to fulfil all four.
Thank you, I'm glad that you found it helpful. I have been using SML exclusively for the development of my proof assistant [JonPRL](http://github.com/jonsterling/jonprl) since April. I can't honestly call it "Production", and my experience may be a bit tainted by the fact that I am using ML for precisely the purpose for which it was designed (building proof assistants), but I have loved every minute of it. SML certainly has a different set of problems than Haskell, but IME most of these issues amount to simply "wearing the hair shirt"; that is to say, certain things are a little annoying (like the value restriction, or having to use functors in order to quantify over higher types), but these can usually be pushed through with a bit of "grunt work". The most serious problem for SML is the lack of high quality libraries; but for the kind of thing I am using it for, this isn't a problem. OCaml is more expressive than SML and is a joy to write as well. I prefer SML for a number of reasons (both technical and political), but I would probably choose OCaml if I needed to write some serious production app. OCaml also has very high quality libraries and a strong engineering culture.
In that case, the class should define replace :: m a -&gt; b -&gt; m b with which one can define extend ma f = replace ma (f ma)
I'm also in the U.S. Central timezone. Why do you need to learn Haskell and can I steal that job from you? ;) I don't know how much time I have to give, but feel free to call/email/text/Google hangouts/etc. me and I'll try to answer what I can. All my contact methods are in my G+ profile: https://plus.google.com/u/0/117121643780343410072
Can confirm they don't work on Windows. It built fine (except the Common.hs file is missing from the package) but the sample executables don't do anything when ran. They just open the console window and immediately close without any message.
Well isn't this black magic for a noble purpose. 
In what way does ML give you streaming abstractions for free? I've written like ten lines of it.
Awesome tool! I just ran some of my functions on the web page and got a surprising response: http://herbie.uwplse.org/demo/943d47f80d472bf7a361b5c0118c843f/graph.html Can you explain what is going on here? Specifically the middle case. 
Go with the toolchain you are more comfortable with. Or if you have a team, go with what your team knows. However I would advocate for Scala. It's a great functional language and a pleasure to code in. Purity is sacrificed for practicality and JVM, which has great tools and a great ecosystem. And its waaay easier to hire a Scala dev, or train a functionally-minded Java dev.
That’s odd. I’ll inquire. Thanks for the feedback. I guess I know why. I’ll update the samples. EDIT: should be fixed in [luminance-samples-0.1.1](https://hackage.haskell.org/package/luminance-samples-0.1.1)
Thank you for your kind words. I have been pleasantly surprised by the response to my post. I agree... The 3+ monad transformer thing looked really nice in some paper about writing a modular typechecker, but it gets really uncomfortable very quickly in larger enterprises. As for monomorphic monad stacks, I worry about becoming too brittle, but I think that compared to the alternative (which is having the semantics of your code differ greatly by the order in which it is interpreted out of the monad stack, which is horrifying), I have come to agree with you.
I find Java to be a real pain in production actually. Not so much for software of our own but tools like Jenkins, Tomcat, SOLR,... always give off the impression that they are barely limping along between the exceptions you regularly find in the logs, the fact that log4j often screws up logging to the point where you don't have a log at all because of missing or misplaced plugins and libraries, version mismatches, uncaught exceptions for normal error situations, NullPointerException,...
Though I'd like to make clear, the tar package is not bit rotten :-) it just doesn't have direct support for the pax standard (though pax format can be read/written using the existing interface, but you need to know too many details of how pax works).
what's the reason for the latter?
Esqueleto isn't even on that front, it can't possibly be the best on it.
Sorry, I misinterpreted the tone. My apologies. Yes, I think we agree completely. I guess I'm a bit sensitive since some people really have been pushing this particular conspiracy (in semi-public forums) that WT somehow have a stranglehold on community infrastructure and are holding things back, which is obviously the complete opposite of our goals and philosophy.
The state of distributed Haskell is unfortunate. it is semi-abandoned. Nobody uses it. There are no developments on top of the basic infrastructure that is too low level. Nowhere near the level of mapReduce or akka, or quasar, while all the other major functional languages have distributed frameworks ready for industrial use. At the same time there are thousands of web frameworks and database interfaces. As if integrating a single box with the web browser and a single database were the solution of all the possible computing needs. This show what most of the haskell practitioners are: students with spare time that want to play with their laptops.
Did you mean to re-export the other modules in http://hackage.haskell.org/package/luminance-0.1/docs/Graphics-Luminance.html ? It seems rather blank
The above code can be run in MFlow with the compile-time guarantees that you mention: https://github.com/agocorona/MFlow
Yet more evidence that Floating point is the work of the devil. Good job.
In fact, I've found that the "extra" computation that happens is enormous. In my tests, we regularly need thousands of bits of intermediate accuracy for edge case inputs, with a slowdown of 100–1000× even in the average case.
I did watch it some time ago. I'm concentrating on the Reflex side though. I will check it again and see if there's more information on Reflex-DOM. Thanks!
This sadly mirrors my experience.
I agree with all the points except one: &gt; People love to complain so much about how in ML, you can't compose code because you'll end up traversing some datastructure a million times (that is, locally performant components may not compose into a local performant component), but honestly, this is a lot easier to predict, find, diagnose and fix than some bizarre resource usage bug that results in an "out of band" error (such as, running out of TCP connections, or something else). First off, you are conflating "laziness" (composable performance) with "lazy IO" (i.e. running out of TCP connections). I don't like lazy IO but I do like laziness and I will focus on arguing in favor of laziness. Contrary to what you say, strictness does not scale well to a large code base, performance-wise. This is an **extremely** common problem at Twitter where code is unnecessarily strict and requesting much more data than it actually needed. We feel this issue both in our services and in our analytics pipeline. All of our recent performance improvements have been making our entire pipeline more lazy. The part that's not obvious to most people is that it's easy to write performant strict code *if one person controls the entire data pipeline* but the moment you have multiple people or teams responsible for various stages of the pipeline you start to get these unnecessary wasteful strictness problems at the boundaries between people/teams. Laziness solves this problem quite cleanly and we have several diverse and practical examples of laziness giving huge performance wins.
&gt; The part that's not obvious to most people is that it's easy to write performant strict code if one person controls the entire data pipeline Ah, so NIH syndrome is a performance optimization! ;]
&gt; while all the other major functional languages have distributed frameworks ready for industrial use. With the exception of Erlang, I think all of the cloud programming frameworks out there are pretty bad. The main difference is that Haskell programmers are actually honest about it and don't oversell it.
I don't see how it would be possible to have a stable GHC API when they're constantly adding new features with each release.
Then I guess we will have to avoid IDEs at all costs
(you're right, I stopped the sentence before lol)
It is reasonable to design and implement a stable API for the needs of things like IDEs, including things like determining type/location information and name resolution, lookups, etc. The Clang project has done this to a pretty good amount of success, and it can mostly isolate the underlying compiler details. Most IDEs do not need the full power of the compiler in the common case, just some of its "smarts" and a little integration. Enforcing a stable API for things like compiler plugins or broadly speaking, the compiler internally at large, is unlikely to happen soon or probably ever. Compiler plugins especially are so rare and 'magical' it's tantamount to bending over backwards for an extremely small subset of use cases, and probably isn't worth it. That said, we're always open to cleanups drawn from experience with the tooling, and we could improve a lot of this I'm sure (especially the documentation).
No, it's much better!
[What Every Computer Scientist Should Know About Floating-Point Arithmetic](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)
Confluence, for one, is a big change for many scala devs
Does this help? http://neilmitchell.blogspot.it/2008/02/adding-data-files-using-cabal.html
Since you are using GHCJS, keep in mind that you could pick [some libraries](https://github.com/ghcjs/ghcjs#jsc-and-webkit) to make your application compilable into a native application as well!
I like to write practical Haskell libraries :(
Oh, yikes, the latter issue, I stand corrected, assumed Esqueleto was fully (and safely) composable. I still find Opaleye syntax challenging, though maybe that's just the norm with Haskell DSLs and operator conflicts. For example: nameAge :: Query (Column PGText, Column PGInt4) nameAge = proc () -&gt; do (name, age, _) &lt;- personQuery -&lt; () returnA -&lt; (name, age) `proc() -&gt;` and `-&lt; ()` stand out like a sore thumb Obviously there's a method to the madness, but this would be easily parsed by Haskellers of all levels: nameAge = do (name, age, _) &lt;- personQuery returnA &lt;- (name, age) In slick you can achieve the above with either of the following: `val nameAge = Person.map(p=&gt; (p.name, p.age))` `val nameAge = for{p &lt;- Person} yield(p.name, p.age)` These are trivial examples of course for both Opaleye and Slick. I'd love a Haskell query dsl that somehow could achieve above simplicity, but no implicits means you have to come up with unique operators for the dsl to work, and then readability tanks since desired operators are often already taken up elsewhere in global scope (module system...Haskell needs).
you probably want to use something like [file-embed](http://hackage.haskell.org/package/file-embed) Basically, you can add a file or a directory to your [cabal data-files](https://github.com/soupi/pureli/blob/ghc-7.10.2/pureli.cabal#L20) and then you can [embed it into a source file](https://github.com/soupi/pureli/blob/ghc-7.10.2/src/Pureli/Module.hs#L29) and just access it as if it was a Haskell List or ByteString.
Indeed, there are currently quite a few packages where the compatibility story isn't very good for Haste, mainly those that depend on bytestring or text. This is on track to be fixed for 0.6, and then we'll see what happens with regards to Template Haskell. 
Then what's the point of the first major version number?
I find a bit worrying that even experienced Haskell developers have to spend that much time and effort tracking down space leaks, and it seems to me that if we can write quickly elegant Haskell code only to spend painstaking hours fixing the space leaks afterwards, we haven't gained that much. Is it a fundamental issue with laziness? Or is it just that we're lacking the proper tools? Could HLint find space leaks? 
Nice! Thank you very much. It answers my question. ~~I'll try to use it later when I am home.~~ **Update** It's shame that I can't upvote you twice. Couldn't resist and tried it right away - works as nicely as I thought. Thanks again! 
Yeah it looks like what I want. But using `TH` for this looks like overhead. [Neil Mitchell](http://neilmitchell.blogspot.it/2008/02/adding-data-files-using-cabal.html) described `getDataFileName` function that doesn't require `TH` and also uses `data-files` cabal section. Anyway, thank you for links! I'll take a closer look. 
+1 more for file-embed. It embeds the file **in the executable** and not on the filesystem, which is an important distinction when you're using it to depoly web apps with templates like we were. 
Hmm, perhaps hlint could look for bang patterns applied to WHNF expressions and inform the user that this bang pattern will not actually reduce the values inside the expression. For example, `doStuff $! ( reallyLongListOfThunks, thingThatEvaluatesToUnboxedInteger )` could raise a message.
Spark is "just" Erlang-style (actors based) web programming as far as I understand it. Edit: was thinking of "Spray" ...
And then, the silence. Why MFlow is obviated despite that is objectively better than any other haskell framework in almost any consideration: web features, respect for functional principles, type safety, modularity, RESTfulness, composability, use of haskell abstractions, modular server and client programming? This is beyond my understanding.
Nah, it's an enhanced map reduce framework like hadoop only it has a better programming model. The biggest problem with it though is that you don't get checking of free variables in closures for serializability which can cause really annoying to debug issues, especially when they first happen to you.
https://ghc.haskell.org/trac/ghc/ticket/10804 is related as well.
 + 1
It already does :) Sample.hs:2:8: Error: Redundant $! Found: doStuff $! (reallyLongListOfThunks, thingThatEvaluatesToUnboxedInteger) Why not: doStuff (reallyLongListOfThunks, thingThatEvaluatesToUnboxedInteger)
Before [this technique](http://neilmitchell.blogspot.co.uk/2015/09/detecting-space-leaks.html), space leaks [took me months to discover and days to fix](http://neilmitchell.blogspot.co.uk/2013/02/chasing-space-leak-in-shake.html). Afterwards that technique, I found 4 space leaks in a matter of hours, and with the test suite changes, they're never coming back. I agree we need tooling, but I think we now have some tooling, and are in a much better place.
I guess in that case, thanks for nothing :P
I would like to see the committee standardize things that are already established practice enough that they are defacto standard, such as bang patterns. One way to figure out if something is a defacto standard is to ask yourself "if we published a report that significantly changed how this thing works, would people just ignore us?"
One approach is to write the formal specification in Coq and then [extract a program](https://coq.inria.fr/refman/Reference-Manual025.html) that implements the specification (currently Coq supports OCaml, Haskell and Scheme). For example, the [CompCert](http://compcert.inria.fr/) verified compiler was written this way.
Don't tell the Drupal community this.
 A . B . C . D ^ ^ ^ | | | /--/ | \----\ | | | | the point | | | \--the numbers ...but seriously, I have no idea.
We have a tool to implement, the core of which is going to require a good amount of assurance. A verified specification and implementation solves all the constraints we have, and I would like to understand better where we would be heading. That being said, thank you for your feedback, I will look into Coq more, as well as your other links.
Please do! Do you have a link to your latest iteration of recommendations. I was at your talk at CUFP Baltimore but that was a number of years ago now. 
Having this as an additional ghc warning might make it even more easier usable and especially findable.
I saw it in many places so far, mostly to hide your implementation details from the user of the API.
I wish I had time, but unfortunately I do not.
I've worked on a very large software verification project ([CerCo](http://cerco.cs.unibo.it/)), and right now I'm also working on the formal specification (and hopefully the eventual verification) of a large slab of systems software and infrastructure. Here's some advice and general points you may consider: * In CerCo we were building a verified C compiler. Getting the right number of passes, making sure optimisations did the right thing, and so on, before we started verifying was a key consideration. For this reason, we built an untrusted prototype in OCaml that we could use for testing and validation against external oracles (i.e. existing C compilers, the C standard, and so on). Only once we were happy that the prototype did the right thing and we had a nice design did we port this OCaml prototype into our proof assistant and start verifying. * In the specification project that I'm currently working on I have a fairly large model written but a large percentage of my time has been spent validating this model against external oracles (in fact, my machine is running a big validation run at the moment in the background). I'd estimate the amount of time that I have spent validating this model is at least as much as I spent building it in the first place, and the validation is still ongoing. The specification that I am formalising is an informal, English specification that at first looks complete, but only once you start validating do you realise how incomplete it actually is. I'm still (as of this morning) filling in big holes in the specification, and this has been going on for over 18 months. * /r/haskell is going to recommend Coq because dependent types are insanely popular on this subreddit and Coq is in vogue at the moment. Depending on what exactly you are specifying and verifying Coq and other dependently typed languages are not necessarily a good fit, and something like Isabelle or HOL4 may be much better for your needs. Further, depending on what it is you are trying to verify you may not even need an interactive proof assistant at all, and could instead reach the level of assurance that you are interested in using a model checker, or something like VCC, the verifying C compiler. Without knowing what you are verifying, and why, it's impossible to recommend anything.
I would be worried that the warnings would be too noisy. We cannot in general tell if the change we suggest is semantically correct, we can only say that it's likely.
I think that's the bare minimum expected from Haskell'. I would like to see a more radical approach than that. But even the bare minimum would be good. Five years is a long time.
Alloy might also be worth looking into (although I'd like to see something like a cross between Alloy and an SMT solver). /u/dmulligan makes some good points, among them that Coq might not be a good fit for your problem. I still think pretty highly of Software Foundations as an introduction to working with proofs / interactive theorem provers. If there are equivalent resources for Isabelle or other tools, I'd love to hear about them. My understanding is that some of them have a better story around extracting Haskell implementations than Coq currently does, for instance. 
They're going to have to pay *me* to browse a site in Comic Sans.
Awesome! Thanks heaps for that. 
&gt; Should not the committee also lead community efforts to improve language? You don't have to be on the committee to do that. The committee filters through all of those efforts and decides which ones are awesome and mature enough to become a standard extension or even on by default.
I will try not to get lost in details, but we are looking for safe data handling (no data loss or integrity loss), as well as strong segregation properties between defined domains (users must not be able to access data from another domain). [edit] I know we can get some of these properties through cryptography, but I want to make sure we can assert these for use in restricted environments.
Comic Sans is proprietary. I suggest using the free font, Comic Neue: &lt;http://www.comicneue.com/&gt;. It is also nicer than Comic Sans, in my opinion.
I am aware of the "effort" part, hence the careful planning; thanks for your answers!
I saw it :)
No problem. Good luck!
I had also thought Java libraries were typically over-engineered to the extreme, possibly the worst in the programming language communities. ...Until I found Haskell's. I had to reset my expectation on over-engineering to bear with the various over-complicated, over-generalized, but at the same time feature-lacking Haskell libraries. The good news is, now those Java libraries look pretty simple to me.
I consider this a difficult compiler issue, not a Haskell problem. Laziness is the future. 
IIRC, it gets covered in Real World Haskell. 
See e.g. [Ed Kmett's comment from a year ago](https://www.reddit.com/r/haskell/comments/1pjjy5/odersky_the_trouble_with_types_strange_loop_2013/cd3bgcu), [Runar's talk on why FP in Scala is terrible](https://www.youtube.com/watch?v=hzf3hTUKk8U), etc.
Isn't a strict pragma coming to next ghc? 
Super
I would also like to see the reverse process: identify failed extensions and remove them from GHC.
That's actually happened a few times, if memory serves me. Most GHC users wouldn't notice it, though, for nearly tautological reasons.
I think it is [fixed](https://ghc.haskell.org/trac/ghc/ticket/9605) in `ghc-7.10`
I recommend checking out - [toolz](https://pypi.python.org/pypi/toolz): general FP library with tools for iterators, functions, and dictionaries - [pyrsistent](https://pypi.python.org/pypi/pyrsistent): efficient immutable datastructures based on clojure's vector tries - [effect](https://pypi.python.org/pypi/effect): first-class effects - [attrs](https://pypi.python.org/pypi/attrs): nice decorator for defining classes which can be a bit safer/more sensible If you're using Python 3, [MyPy](http://mypy-lang.org/) seems to be coming along (and Python 3.5 added official support for its type annotations). Sadly the Python 2 support isn't (yet?) usable, which makes it useless to the large majority of production Python use. If you want to go wild, there's [hask](https://github.com/billpmurphy/hask) (not on PyPI), which implements something like Haskell's type system in Python. I don't think I'd use that in real code, though. Full disclosure: effect is mine. I've also made small contributions to pyrsistent and attrs. 
I'm pretty sure aeson and servant provide the same things (including type safety) as spray and json4s. Maybe I'll have time to write some sample code to compare them later.
There are some differences in behaviour for weak references attached to heap objects that do not have identity (considered fragile in the base library, see System.Mem.Weak), due to the way thunk updates are handled (by shallow overwrite rather than indirection). Weak references to objects with an identity (`mkWeakIORef` and `addMVarFinalizer`) should however work exactly the same as with GHC. Reflex uses these extensively and sodium also relies on these. If you find differences here it should be considered a bug, and I'd like to fix it. And if you really need weak refs for non-identity objects, perhaps it can be handled by some changes in the thunk update mechanism. Let me know if you want me to have a look at a specific use case.
Wow. Thank you. These are some intense tools! I think "coping" is starting to border on "choking" but I can see how these would be very nice if you simply can't escape the Python env.
_Really_ basic question: do you use Text/String as Either's Left, or do you use sum types?
&gt; Every large Haskell program almost inevitably contains space leaks Ugh :(
I just emailed you two ideas!
Hi. We want to work on free software, not work gratis on software. By free software, we mean free as in freedom, not free as in free of charge. Please see &lt;https://www.gnu.org/philosophy/free-sw.html&gt; for more information. Your ideas sound interesting though -- good luck with them!
At my previous position, we had to write an appender that would do the things that weren't possible by default with slf4j (or log4j) : * line break on long lines * repeat the hostname on all lines, not just the first line of the stack trace * escape special characters I don't know how people handle logs with java applications (log files ???) but syslog support is TERRIBLE despite it being the most common solution.
Aeson can generate marshallers and unmarshallers for new types. You don't need to define them. Servant has a nicer DSL than Spray, IMO.
Good point. I just hope it doesn't take until Haskell2022...
Thanks for chiming in! And thanks for working on GHCJS, as well. :-) I need weak references for reactive-banana as well. /u/ocharles has probably asked you about them already, the trouble is that GHC and GHCJS produce different results when running my library. For reference, here is how I use them: * [Helper functions to create `Weak`][1] * [Example where a weak reference is created (and discarded (!)) to introduce a dependency between one value and another][2] Note that the second use case relies on the full semantics of weak pointers as described in [the paper][3]. It's really quite subtle. For instance, the weak pointer can be discarded after establishing a dependency between key and value. PS: Stephen has [deprecated `sodium` in favor of `reactive-banana`][5] [1]: https://github.com/HeinrichApfelmus/reactive-banana/blob/master/reactive-banana/src/Reactive/Banana/Prim/Util.hs#L49 [2]: https://github.com/HeinrichApfelmus/reactive-banana/blob/master/reactive-banana/src/Reactive/Banana/Prim/Plumbing.hs#L94 [3]: http://research.microsoft.com/en-us/um/people/simonpj/Papers/weak.htm [5]: http://blog.reactiveprogramming.org/?p=258
Isn't `replace` just `flip (fmap . const)`?
Oh, behind the scenes, yes. Should've had a look at the source. But ideally I think it should be possible to specify the precision at the type level
We don't really care for open source. Please see &lt;https://www.gnu.org/philosophy/open-source-misses-the-point.html&gt; for why.
It can't help with (x + y,z) or return $ x + y 
Just few minutes ago I found [this](https://www.reddit.com/r/haskell/comments/1dhbjr/haskell_2014_committee_has_now_been_formed/c9qckhl): &gt; it originally had a much broader scope with tons of moving parts. That scope turned out to be so daunting that they eventually scaled it down to just cherry-picking the non-controversial extensions that everyone could agree on, and which wouldn't drown the committee in ink to document. So, historical reasons, OK. Thanks anyway.
Someone needs to write an article "Why free software misses the point of marketing."
Oh, I'm well aware, but confusing terms will kill your business. Just call it the most understood thing first and then clarify later.
It's [under development](https://ghc.haskell.org/trac/ghc/wiki/ImpredicativePolymorphism/Impredicative-2015).
Data Parallel Haskell (DPH) has been suggested for removal before.
If you could take the time to write a comprehensive guide that would be awesome. Kinda collecting all your works together. I've tried to collect a lot of your links as well as others here: https://delicious.com/lukehoersten/haskell,performance
If you want someone to pay you to do software, please let them decide of the "freeness" of the product. The way you are doing it looks like "please pay me to do an open source project show I can show people around how good I am and find more customer". In other words you want me to pay for your advertising. I don't think you'll lots of people to do so.
The truth is if you are looking for customer, the fact that you like your website is irrelevant. What is relevant is if your customer will like it or not. You are indeed showing that you are not good at front end, Which is fine but that's the only thing your are showing (as well as you can't be bothered to put on a good website or at least pay someone to do so). That's not the best way to get customers.
I suggested this once. It's quite neat. http://haskell.1045720.n5.nabble.com/Counter-proposal-Do-not-include-whenM-and-ifM-td5748169.html
&gt; In slick you can achieve the above with either of the following: &gt; &gt; `val nameAge = Person.map(p=&gt; (p.name, p.age))` `val nameAge = for{p &lt;- Person} yield(p.name, p.age)` &gt; &gt; I'd love a Haskell query dsl that somehow could achieve above simplicity Then you'll love Opaleye. It allows nameAge = fmap (\p -&gt; (name p, age p)) personQuery I should probably make more of this is the tutorials. It seems that people don't realise.
An interesting perspective that is not shared by many in the research community.
A dispute between "this is the more ethical option, and it has practical benefits" vs. "this is the more pragmatic option, and many of us find it ethically preferable". Basically, it turns out that saying "let's not mention ethical principles because that makes people *uncomfortable*" doesn't always sit well with people who strongly value those principles.
I wonder if something more automatic can be implemented in the compiler. A sort of heuristics for deflating space leaks via evaluation of thunks when the stack reach a limit.
rewriting the C++ to Haskell or just the Python?? also plz where do you work?
Sum types. We also used newtype wrappers often to avoid the use of undifferentiated basic types. You want as much information as you can get from the types, so long as it doesn't create an excess of work to manage that information.
If the RTS can reallocate stack space then this can be supported there with little overhead and for production workloads.
* Lazy deserialization of thrift (which is one of our persistent data storage formats -.-, don't ask) to avoid deserializing more fields than we need * Columnar data stores (i.e. `parquet`) with projection pushdown and predicate pushdown so we only read fields and blocks of data that we actually need. This matters a lot when you have data structures with thousands of fields introduced due to unchecked cruft (again, don't ask) * Query services powering dashboards that were querying way more data than they actually needed from the backend and then filtering the result down (when they could have just queried for less) * Lazy comparison functions and hash codes for sorting/shuffling data in map-reduce jobs Only two of these are properly laziness (i.e. lazy deserialization and comparison functions/hashcodes) and the two others are technically "lazy IO" (with all the concomitant problems), but they still give large performance improvements. There is also one other candidate for performance improvement on the horizon which is that our BUILD tool is very slow due to strictly loading all backends, potential tasks, and argument subparsers even when it doesn't use them.
wouldn't that change the behavior of the functions?
Not only that, but in most systems where it really matters (i.e. heavy data processing using the scientific stack) those libraries are increasingly finding ways to release the GIL. In one script I have I can hit 40% utilization of a hyperthreaded quad core without any concurrency, and with some simple parallelism through ipyparallel I easily use every core available. I'd say it's actually easier than in Haskell.
Ah yes I am, thanks !
Removing `CPP` immediately is unrealistic, so yes, we need a replacement for its use cases. Unfortunately it seems to be out of scope for the commitee.
I wrote a new CPP that doesn't have any licensing snags with linking directly into GHC. It is designed to make Haskell-compatibility a primary concern (eg turning off C-style line continuations). First version is on hackage, and a patch to GHC for in-process CPP'ing is nearly done. It'll be on github soon.
In my own work with Haskell I have a tendency to use 'domain' modules that are constructed with the space leaks for the module's typical application factored out. As a result I rarely encounter space leaks. This is not everybody's way of doing things, and is not possible for domains that don't have library support of the type needed.
Check out real industry experience called "correct by construction", e.g. as once presented by Praxis (one of first links in Google is http://www.anthonyhall.org/c_by_c_secure_system.pdf). Also the Overture Tool and VDM, http://overturetool.org/, the books section (http://overturetool.org/publications/books/) lists some very practical ones. Note that VDM is similar to Z, but uses almost Haskell syntax.
This is my advice too. Learn and get good at TDD. Tests are your new types. Embrace it and become the best python/ruby/java programmer you can be. Join a Software Craftsmanship community to get help with learning TDD - if you're in London the London Software Craftsmanship Community is really good and has hands on/pairing sessions multiple times per month. 
Yeah, it probably was a little bit of an exaggeration. Having a standard language is a very valuable thing.
* No classes, only lists and dictionaries. * Mostly pure functions * A comment immediately above each function definition with a type signature.
Work in python shop. In the evening when I get back home I work on some side projects using haskell. Learning servant at the moment. Hoping to learn lenses soon and some extensions. Especially polykinds, datakinds and typefamilies 
https://github.com/kazu-yamamoto/unit-test-example/blob/master/markdown/en/tutorial.md#internal-modules
Ah I think you might have stumbled onto one aspect where I deviated from the from the semantics in the paper, since this was more efficient to implement, and in the discussions I've had I haven't found a reason for why the behaviour should be as described in the paper. The situation is like this, right? - A weak reference is unreachable, but its key is - There are no strong references to the value remaining Why should we consider the value alive? There is no way to actually reach the value anymore throuh normal Haskell operations. The only reason for keeping the value alive is to prevent finalizers deeper in the structure from being run. Other than that, it seems that it's just some data needlessly being kept alive on the heap. I considered submitting a proposal to change the semantics in GHC, but held off since I was working on other things, and wasn't sure if I had missed anything in the design rationale. Perhaps you can shed some light onto this? The modified semantics are more efficient to implement in GHCJS, and that's what is currently in the RTS. The original semantics can be recovered from the modified semantics by keeping a reference to the value in the finalizer (using the touch primop for example) to explicitly keep it alive.
That surprises me. Do you have a link explaining why?
Right, since those types/functions are declared in modules that won't be compiled with `Strict`.
I don't understand your answer.
Ok, thanks.
http://i.imgur.com/2wkSPhh.jpg
It would, in the case that the computation is not needed but produces an error (or infinite loop, or other bottom "value").
Here arrow notation becomes very natural. Here's one way to achieve it. There are many equivalents. It corresponds closely to your pseudocode and your Slick(?). proc () -&gt; do u &lt;- user -&lt; () ur &lt;- user_role -&lt; () restrict -&lt; (id_ u .== userId ur) r &lt;- role -&lt; () restrict -&lt; (id_ r .== roleId ur) returnA -&lt; (u, ur, r) 
http://i.imgur.com/YQW4J9Y.jpg
Here's a plea to remove parts of it because it slows down the GHC build and it takes maintenance time to keep it working: [vectorisation code?](https://mail.haskell.org/pipermail/ghc-devs/2015-January/007986.html) And the DPH libraries have been disabled from the default GHC build. You need to set `BUILD_DPH=YES` in your mk/build.mk file explicitly to build them. Since nobody does this, they're slowly bitrotting. See also the thread [I'm going to disable DPH until someone starts maintaining it](https://mail.haskell.org/pipermail/ghc-devs/2014-August/005907.html).
&gt;You didn't use the words but your argument reminds me of all those people who claim that you can't know what you are doing until you have written projects with millions of lines of code, ignoring completely that, no matter the use case or environment (especially when it is important), no project should ever get to millions of lines of code or the state you describe. Here's the problem with your line of reasoning. You think that all application development is the same. You think that no application should grow large just because it wouldn't work for the type of development that you do. A lot of internal applications don't have to worry about scalability so there's no performance benefit to breaking it into pieces. Keeping it monolithic makes for easier debugging and development. That isn't a good idea for all application development but it does for some use cases. I'm not saying that you don't know how to develop an application because you've never developed a very large one. I'm saying that you don't know how to develop a large application because you've never developed one. Your one size fits all approach is ignorant.
&gt; I deviated from the from the semantics in the paper, since this was more efficient to implement Awww, please not in Haskell! :-) If I use the base module `Sytem.Mem.Weak`, then I expect the documented semantics, not something else. This is unfair... (;_;) &gt; The situation is like this, right? &gt; &gt; * A weak reference is unreachable, but its key is &gt; &gt; * There are no strong references to the value remaining &gt; &gt; Why should we consider the value alive? There is no way to actually reach the value anymore throuh normal Haskell operations. Well, you can still reach the value through weak references. In my case, the weak references to the values would certainly be happy if the values were dead, but I'm using the key to prevent this. I could make the value reachable from the key by adding it as part of the data structure, but I'm not sure if that works. If you have multiple values per key, then you would have to keep a list of values to keep a live, but I think that's not the case for me. &gt; I considered submitting a proposal to change the semantics in GHC, Please do. Chances are that someone will point out if you've missed anything in the design rationale or if it's fine. Unfortunately, I can't say off the top of my hat why it was a good idea that the key keeps the value alive. It made sense to me when I read it at one point. I would have to read the paper again. &gt; The original semantics can be recovered from the modified semantics by keeping a reference to the value in the finalizer (using the touch primop for example) to explicitly keep it alive. Ah, because the key still does keep the finalizer alive.
You can disable it during production. There's either an option or an environment variable you could set, I forgot what it was. Even if there isn't an option to turn it off, you could just replace `Contract` with a no-op. That being said: It might be useful to get contract exceptions in production for the corner cases you didn't test for.
&gt; Well, you can still reach the value through weak references. In my case, the weak references to the values would certainly be happy if the values were dead, but I'm using the key to prevent this. One of the preconditions is that all weak references with this value are unreachable (as you do in reactive-banana, throwing away the `Weak` immediately), so nothing can use `deRefWeak` to get to the value. Unless I missed something, there really is no way to actually do anything with the value in this scenario. But I agree that I should at least document the difference in behaviour, and probably implement GHC's variant as the default until this question has been resolved.
they want people to pay them to write open source software. and they want that software to be "free as in freedom". that's their constraint. they won't "please let them choose the freeness".
Most of the software we develop is open source, only the core of our business logic is closed. We save a lot on maintenance that way. If we were looking for Haskell developers it would quite likely be to develop open source. But that's just us.
&gt; My fear is that too conservative of a committee would [...] Well, fwiw, I'm self-nominating ;) &gt; Similarly, I fear that too liberal of a committee would [...] That I can't help with, though I like to think my liberalism isn't too out of line with community standards
&gt; So you think Why do you say that? I was just saying what I perceive to be the interests of the Haskell community, and how the community seems to be already quite able to come up with new ideas along with implementations for those ideas. The only opinion I'm offering is: based on this seemingly active development in the community, I don't see the need for a committee to add direction. If you think these are bad ideas, then you should get involved in those discussions! Is a bureaucracy really going to help? Maybe if you carefully select people for the committee who already align with your opinions... but if it's a fair election, then presumably they will just reflect what the community is already deciding to implement. edit: to be clear, I'm 100% behind a committee being formed to select extensions to make default in the next revision of Haskell. 
Yeah, or things that can be done better with another extension or set of.
I just spent a minute looking. I haven't read much but have a couple of suggestions: * Perhaps ```type Context = [(Name,Type)]``` could be ```Map Name Type```. * If you start your comments ```-- |```, they will appear in Haddock. I think that would be good. 
With much help from the Haskell community we've been developing our realtime backend game server in Haskell. After having initially given up on the language a couple years back, I revisited it at the beginning of this year and have had no regrets moving forward. There is still a large amount to learn, and I imagine a more seasoned Haskeller would grimace at most of our code base. However, we are incredibly satisfied with our results. The game itself is a realtime simulation where war machines (mechs) engage each other on a battlefield, controlled independently by AI which players have written in their language of choice interfacing with the game server API, executing inside containers within our infrastructure (due to fairness concerns). There's still an incredible amount of work to do, but due to how agile we've become with Haskell we're confident we may have a prototype ready sometime next year. Right now we are trying to gauge interest levels, and hopefully try to start a small initial community before release.
That sounds reasonable to me
Sounds interesting but the 3d client seems VERY ambitious
I completely agree. In fact, immersing my self in the modern Java community made me learn TDD, which I use in Haskell as well. Types and tests are a powerful combo. In fact, yesterday I was doing some type level programming in haskell and used TDD to write type level tests. It was awesome.
yes, I saw it. Greate work btw. But `CPP` has much more downsides except haskell-compatibility. IMO it was a bad idea from the very beginning. I'd prefer something embeded into AST, like TH but specialized. Though fixed haskell compatibility is better then nothing, and probably the best we can have if near future.
Amazing
&gt;In fact, yesterday I was doing some type level programming in haskell and used TDD to write type level tests. It was awesome. Blog post on this plz 
&gt; A comment immediately above each function definition with a type signature. This helps a *ton*. I do this in my PHP and Javascript as well.
I am saying that they should not grow large because they are all developed by humans. Humans have finite mental capacity. Large, monolithic code bases make it hard to debug one piece without taking all the other pieces into account because there are no clearly defined interface between pieces that allow you to see all the other pieces in a black box fashion. They also make it hard to replace pieces of the whole as the requirements change beyond the point where refactoring is feasible. If anything, large projects with changing requirements over the years and changing developers require more modularization than the projects that could be rewritten in a couple of person months or even years.
Looks very interesting. Good luck!
&gt; So I wrote Beethoven, a gem that makes Ruby look and compose much more like a functional language. Why would I want to write a rather verbose class definition instead of Add5 = -&gt; (x) { x + 5 } ? You can also add function composition on top of `Proc`s and don't need classes for that: http://blog.moertel.com/posts/2006-04-07-composing-functions-in-ruby.html
You mean "no space leaks by construction"?
Wow, OpenGL 4.5 requirement is kind of... well, a deal breaker to be honest. I can't really see myself developing a graphic application that has such high requirements from the user especially since I don't have control over those requirements. I understand it's a pain to support more than one version. I hope you do eventually though!
Nowadays pretty much all GPUs have that requirements. OpenGL 4.5 added DSA and a lot of nice features I don’t want to get away from. It’s very unlikely I support lower versions. Sorry!
Why a language standard? There has long been a good replacement available, and it doesn't require any special language support: Put the version-specific code into separate modules, and use the cabal flag to select extra source directories instead of to set CPP symbols. The problem is that there is still a huge amount of legacy code which still uses CPP macros for this. And of course, there are also plenty of other more complex uses of CPP macros out there, although it is less common.
This is me! I'm flattered that someone thought it was worth sharing. I'd love to hear people's thoughts and answer any questions people have.
&gt; Put the selectable chunks of code in separate modules and then select modules using cabal flags. Yes, I [know](http://blog.haskell-exists.com/yuras/posts/stop-abusing-cpp-in-haskell.html) &gt; For more complex uses of CPP macro functions, perhaps m4 would be a better fit. Can m4 be embeded into haskell AST? If not, then how will e.g. refactoring tools handle it? How m4 is better then CPP? I believe there is better solution for each macro preprocessor use case.
I appreciate the criticism, it was clearly a bit hand wavy. I have edited it down to "We are writing the realtime backend game server component in Haskell" and have slated a future blog post to cover the reasoning in more detail.
According to this statistic http://www.plane9.com/plane9/stats and this statistic http://feedback.wildfiregames.com/report/opengl/ (GL_EXT_direct_state_access), the "pretty much all" is 35%.
I talked to some designers/frontend devs. They think the website is mostly fine, and that this feedback sounds like hyperbole. So I'll trust them at that. :] They did agree the colours are a bit dark and that a better font could be used. FWIW I don't choose my dentist based on their web design ability. And this is what most r&amp;d people's websites look like: &lt;http://research.microsoft.com/en-us/people/simonpj/&gt;. :]
That would make perfect sense, I talked to you guys at scipy this year. Were you able to attend? By the way, great job on hosting most of that conference, especially getting the videos up on YouTube within a few hours of being recorded. I heard a lot of people commenting on the efficiency of enthought :) 
DSA is really powerful and very useful in combination with haskell. DSA + Haskell feels very natural, I can totally understand your decision to require it. And with dumping the "old clumsy global state setting" OpenGL, I would assume it will be easier to provide a Vulkan backend later on when Vulkan is ready. Sadly OSX 10.11 still does not support OpenGL 4.5 :( I hope Apple will join Vulkan.
Thanks! 
I've put up the actual slides up for you: [http://jmct.cc/Haskell2015-Slides.pdf](http://jmct.cc/Haskell2015-Slides.pdf)
Still, a native Haskell construct for doing the same thing without extra modules and extra logic in the cabal file would make more sense in my opinion.
&gt; If you can make TH simple enough, and not deeply intertwined with compiler internals, then it can replace lightweight macro processing. Yes, that is what I mean. I know I'm bad in expressing anything in English :) &gt; Otherwise, there will always be a need for non-TH macros. Haskell is not Lisp - for better or for worse. I agree.
Oh, I know Postgres will handle pretty much whatever you throw at it. Case in point, until recently Slick has generated sql you would never write by hand (i.e. huge amount of extraneous sub selects), and yet query plan is nearly the same as hand written version. For other databases, like MySQL, not at all the case, performance face plant. Anyway, that's not what I was getting at, more curious as to what your example query generated: implicit or explicit joins. And yes, obviously outer joins must be explicit, was just showing the different approaches in Slick-like DSLs wrt to implicit and explicit joins. Thanks for the Opaleye examples, by the way, am keeping an eye on it and Esqueleto.
 first :: Product arr tuple =&gt; arr a b -&gt; arr (tuple a c) (tuple b c) or something.
I deny it! ;) In particular, it's much easier to write an algorithm with the wrong time or space complexity when you have laziness. Quick, what's the time complexity of this function: insert :: Eq a =&gt; a -&gt; [a] -&gt; [a] insert x [] = [x] insert x (y:ys) | y &lt; x = y : insert x ys | otherwise = x : y : ys 
Your concerns are common among the client authors, and I really appreciate you pressing on this. I wasn't sure if it would take off, but it looks like we've stayed pretty faithful to the [spec](https://github.com/elastic/elasticsearch/tree/master/rest-api-spec/src/main/resources/rest-api-spec/api) that lives alongside ES. It's not as good as executable GADTs, but you can at least write harnesses to verify it, which is what the internal clients do. I pledge to try Bloodhound more instead of plain HTTP. That will help me provide better feedback and hopefully we can help on the commercial user side of things.
I do? I've implemented a bunch of category-related typeclasses in [category-syntax](https://github.com/gelisam/category-syntax/tree/master/src/Control/Category), but I'm definitely not claiming that my version is better than what's already in the [categories](https://hackage.haskell.org/package/categories-1.0.7) package. I consider the fact that category-syntax defines its own class hierarchy to be unfortunate, but I haven't yet determined whether the correct approach is to try to adapt category-syntax to the existing typeclasses or to try to convince everybody else to switch to my typeclasses. Probably the former!
Trivial functions are definitely easier to write as lambdas. More complex functions are significantly easier to write as classes in Ruby, and classes tend to be a lot easier to test and understand for Rubyists than a mess of lambdas. I was primarily implementing higher order functions for data filtering/aggregation where the parameters for processing could be stored as a nested JSON. The recursive nature of the data scheme would have made that rather tedious to do with lambdas, and I was able to make the code rather concise and clear with module mixins.
Yeah, forceably hiding things I actually find quite annoying. I use the `.Internal` trick myself.
Mesa seems to be at OpenGL 3.3 at this point, so on Linux you would have to use the proprietary NVIDIA or AMD drivers.
Very nice ! Suggestion: I think it would be nicer if you were made aware of things you had proven before. Perhaps the logic blocks could be numbered, as in theorem numbers, so you would know that a block is on the list on the left because it has been proven. As it is it seems a bit misterious why those blocks are there and not others. As a game this would be made obvious by the fact that once you prove a fact it would appear as a new block on the next exercise, on the list on the left. Also, it would be nice if exercises are numbered, so that you know the order in which to progress (i.e. following the theorem numbers). I've been doing simple propositional logic proofs as a hobby, and it totally feels like a game, it's very fun !
I think that this may be doable
I never claimed that it is a sound theorem prover :-).
Does it attempt to normalise the given expression? If so, then it would be hard to derive False but easy to crash the prover.
In that case, it should be easy to derive False just by using the term (λx. ¬ (x x)) (λx. ¬ (x x)), as it's equal to ¬ ((λx. ¬ (x x)) (λx. ¬ (x x))). But, I don't know how to write raw lambdas in this tool.
I go even further in `hask` and in the HEAD of `categories`. I don't yet think that that is a good idea to inflict on the rest of the community, because frankly, I'm not smart enough yet to do all the things I can do with the simpler versions that way.
To clarify, there are modules that provide functionality that has space leak avoidance as part of its design. If you use those modules many of the space leaks that might otherwise plague your application can be avoided. For example the conduit modules can be very useful when processing streams of data where one off hand-coded Haskell might otherwise result in space leaks.
That's a great article too. Thanks for linking.
Can you explain how minimum is O(N) due to laziness? That is eluding me.
If you want that to error, then you don't know what fmap means. `fmap` means "functor map." It's the only function in the Functor typeclass, so a functor is anything that can be mapped over, and therefore all lists are functors. For list instances, `fmap` = `map`. Applying `fmap sqrt` to a list of numbers returns a new list containing the square root of each number in the original list. Haskell: Prelude&gt; fmap sqrt [4] [2.0] Prelude&gt; fmap sqrt [4,9,16] [2.0,3.0,4.0] In Python you would just use `map` to do this. Your Python function is not wrong for the other use case, but it's not `fmap`, because it doesn't work for all "mappable" types and there is no such thing as a Functor typeclass in Python--unless you [implement it yourself](https://github.com/billpmurphy/hask/blob/master/hask/Data/Functor.py).
You can make as many out-wires from a given block as you want.
If we pretend every value in Python is a `Maybe` value (since it can be `None`), then the Haskell equivalent of Python `[4]` is `Just [4]`, and Haskell won't let you do `fmap sqrt (Just [4])` either. I'm fully aware of what fmap means.
Then you just have nested functor instances, and in Haskell you would do this: Prelude&gt; (fmap . fmap) sqrt (Just [4]) Just [2.0] Doesn't work for your Python function, so again, it's *not* `fmap`. You could call it `maybeMap` or `maybeApply` or something.
M For all chin there exists
It's assumed (and reasonably so) that due to the recursive structure of lists `sort` will be able to produce the beginning of the sorted list without having to sort the whole thing first. As such, by using `head` you get the minimum element as soon as it's found and then discard the rest of the computation. Whether this is actually O(N) depends on how `sort` is implemented but I'd expect it to be true for most of the sensible algorithms. As a trivial example, imagine that `sort` is suboptimally implemented by taking the minimum element from the list, then recursing on the remaining elements. `head` in this case obviously forces exactly that first scan for the minimum element and nothing else.
What happened with R6RS?
I disagree - if a compiler warning ever attempts to provide a 'solution' to some 'problem' it has 'identified', it need to always be 100% correct and preserve the semantics 100% if it suggests anything, or as close as humanly possible. GHC has been sloppy about this before, but it's a terrible thing to ever allow and unacceptable IMO. Lots of these warnings would be possibly-semantics-changing category. Warnings also need to be robust to 'code evolution', and warnings need to be predictable and fast. I imagine strictness analysis is too finnicky to rely on to generate reliable and predictable warnings 'at-large' for a Haskell compiler. I'm certain you could easily 'break' these warnings with small refactorings that would cause the tool to miss it, or worse yet, make a change that causes a cascade of demand-info changes, which could exacerbate or silence other reports. Such a tool would still be useful of course, just not as a compiler warning (or at least, not any kind of 'traditional' warning, and it should be emphasized it's more of a static analysis.) Of course, all compiler warnings are basically an approximate static analysis. Some are simply more approximate than others. :)
I meant specializes for function, instead of the more general Arrow.
MFlow abstract from all this stuff and it is more type safe than servant. Some explanation is somewhere else in this discussion
&gt; We've sought to keep the text factual, rather than imply that one option is "best" for any particular class of user, since opinions vary so widely on this point. While I appreciate this sentiment, and think it's a good way to keep the Haskell community from fighting, it does make the page pretty confusing for newcomers. As a newcomer reading this page, I'd have no idea which option to pick. That's not a problem with the explanations: I think there's just no good way to explain what the differences are to someone without experience. I'm not offering any solution to this; I don't know what it would be. This is just the impression I got from the gist, which otherwise looks good.
Think of it this way: b -&gt; b = true so a -&gt; b -&gt; b = true. a -&gt; b, true | a -&gt; c is clearly absurd. I believe the intended task is a -&gt; b, a -&gt; b -&gt; c | a -&gt; c
Yes, that might work – if only the parser would allow arbitrary terms in the function position. Currently it doesn’t (for no particular good reason), which makes this at least hard to construct.
For practical reasons, I admit that I deferred this discussion for later. I expect that the current system can be broken, and I hope that it can still be fixed without dumping or complicating too much of it :-)
Thanks! I look forward to looking at them. 
It looks like the doc generator [mangled a few lines](http://hackage.haskell.org/package/luminance-0.1.1/docs/Graphics-Luminance.html) of your example code. Like so: (program,uniformInterface) createProgram shaderStages $ uni - do
I wonder how difficult it would be to apply this technique to GHC itself, and how many space leaks would be found.
namedtuples have some significant... annoyances. Most notably: &gt;&gt;&gt; Foo = namedtuple('Foo', ['x', 'y']) &gt;&gt;&gt; Bar = namedtuple('Bar', ['a', 'b']) &gt;&gt;&gt; Foo(1,2) == Bar(1, 2) and Bar(1, 2) == (1, 2) True This is one reason why I prefer/recommend the [attrs](https://pypi.python.org/pypi/attrs) package and just define classes with fields and (usually) no methods.
You might want to take a look at how it's handled in Purescript http://pursuit.purescript.org/packages/purescript-arrows/0.6.0/docs/Control.Arrow#t:Arrow Basically the `Arrow` is just a composition of `Strong` and `Category`. All the functions that are considered "Arrow functions" (arr, first, second, (***), (&amp;&amp;&amp;)) in fact simply come from `Strong`, `Category`, `Semigroupoid`, `Profunctor`. Yes, Bifunctor operations in PureScript are called `lmap` and `rmap` instead of `first` and `second`. `first` and `second` are instead coming from `Strong`. I guess this is a practical implementation of what /u/edwardkmett is explaining here (correct me if I am wrong). 
Afaict UHC does support full Haskell2010 as of about eight months ago. [The documentation](http://foswiki.cs.uu.nl/foswiki/Ehc/UhcUserDocumentation#A_4_Haskell_compatibility) suggests there are a few missing holes, but if you follow the ticket links they all look to have been resolved. As far as production use, I've used UHC in the past —enough to be sure my code was portable— but I can't say I've really stress tested it. (Alas, since getting my new computer I haven't had the chance to install it, since I ran into a few issues when I first tried.) There are two things I'd like to see out of a new standard. First is exactly what you say: to canonize a number of language "extensions". In particular, many extensions like MPTCs and the associated FlexibleFoo extensions have been with us for ages and have been considered "standard Haskell" for a decade or more. The only reason these didn't make it into Haskell2010 is because of the dispute between fundeps and typefamilies. I think at this point everyone agrees that (a) type families are the way of the future, and (b) current type families are crippled by their inability to express everything fundeps can. We've been making leaps and bounds in correcting the second point, but the solution lands us in exactly the same place where people worried about fundeps being "too complicated". Ultimately the difference is, or should be, merely a question of surface syntax; but to make that official we need to define exactly what the behavior of the core language should be with respect to these sorts of relational constraints. Regardless of the details, extensions like MPTCs, FlexibleFoo, etc should be the official standard. The second thing I'd like to see out of a new standard is to properly deal with all the fiddly bits that show up from having slapped more plaster on the old language for so long. Things like hammering out [what exactly it is that typefamilies are supposed to represent](https://typesandkinds.wordpress.com/2015/09/09/what-are-type-families/) and giving them a proper and intelligible semantics, regardless of how GHC has cobbled things together. Similarly, when we go back to Haskell98 the language is blessedly free of all the declarational keywords of languages like ML. But because development process and the need for backwards compatibility, we've introduced a whole slew of ad hoc keywords. I'd like to see all those polished away again, getting us back to a syntactically clean language. Especially since we're now we're moving towards a full-blown dependently typed language, which makes all those keywords truly extraneous. I've long advocated for defining Haskell as having a dependently typed core/IR, and a surface syntax that (by default) hides the full power of that core in order to allow us to reap the benefits of type inference etc. And I'm not the only one who holds this position. Other than thinking it's the right way forward, I don't have any particular axe to grind about whether Haskell *should* be dependently typed or not. But I do have an axe to grind about how grody the syntax has become, and that's something that really should be addressed.
With that kind of reasoning, a huge chunk of software, which we rely on, would never have been developed. Google, for example, still creates huge monolothic projects, that they keep in giant repositories (which is the reason why they use perforce internally instead of git, perforce handles monolithic huge libraries much better, especially when you have binaries). Their reasoning? Its actually much easier to quickly debug problems and iterate. If you break your project up into many small parts, you often have lifecycle issues as you try to bump up the change you made to some small dependency down the line all the way. So in short, with this attitude &gt; I am saying that they should not grow large because they are all developed by humans. We probably wouldn't have operating systems, kernels, Windows/Linux etc etc that we have today. Although keeping things simple is a good principle in general, it doesn't apply all the time
I truly wish I had an answer for this, but no, nothing we can commit to at the moment. There are a lot of pieces involved (core server, execution containers, 3d client viewer, account web/db, API documentation &amp; tutorials, language specific starter SDKs, etc.). Incredibly basic prototype early next year if all goes well?
Heh, exactly the one I struggled with. The fix was released already though :D Found it out just before checking the comments here (didn't understand the first arrow / implication block at first). This stuff really is very cool as a game.
Are you hiring?
I feel like this is good personally but wonder if it will affect ease of use and because of that affect adoption.
I feel that page should solely exist for newcomers, though from there I'm not sure what the best option is. Perhaps we could take a page from marketing and split test the different options then ask users for feedback on the different methods. There is the downside of the page not looking the same for everyone.
Right, by "Term language" I should have said "proposition language". I'm aware of things like NuPRL that have subtle distinctions here.
sounds ambitious. I hope you manage to pull it off. The idea of programming bots that do battle sounds very exciting. good luck
Hrrm. My real function might actually be `f(x) if x is not None else x`... I don't remember whether the example you give made sense in the context. It might have. Not sure!
even if it's not a compiler warning, it would be good to have "compiler support". like being bundled as some option in GHC, or even just generally a better GHC API 
This thing is awesome! I just solved the `A -&gt; B |- (B -&gt; _|_) -&gt; (A -&gt; _|_)` one, which took a little bit of effort. Two requests: 1) a way to rearrange the things on screen to make the layout pretty after you finish (or perhaps before). 2) the ability to export the wired-up drawing to some kind of concrete syntax (LaTeX? MathML?) that's familiar so you can look at the proof you just made.
It's the cofree comonad.
And now, [combine it with automatic differentiation](http://jtobin.ca/blog/2014/07/06/automasymbolic-differentiation/) for maximum pleasure.
Thanks for noticing! Though I don’t understand what causes that…
I'm going to go out on a limb and claim "probably fairly easy" and "probably quite a lot"!
Please clarify. Is it not fairly uncontroversial that decent programming languages will need to provide access to efficient implementations of both strictness and laziness? EDIT: Hmm perhaps this clarifies. I interpret it as an agreement with my claim. https://www.reddit.com/r/haskell/comments/3m0gxq/neil_mitchells_haskell_blog_three_space_leaks/cvcn31w 
Sure, but it's hardly a problem when your algorithm executes *more* efficiently than expected.
Could we rename `return` to `inject` as well? Or will `return` be phased out and completely replaced by `pure`?
by J. Carette and C.-C. Shan Abstract. We transform probabilistic programs to run more efficiently and read more easily, by composing three semantics-preserving transformations: (1) apply the denotational semantics; (2) improve the resulting integral; then (3) invert the denotational semantics. Whereas step 1 is a straightforward transformation from monadic to continuation-passing style, the rest builds on computer algebra: step 2 reorders and performs integrals, and step 3 represents density functions as differential operators.
I wonder if I am fine if I say that lambda abstractions may be introduced * In in put terms, as arguments to constants (i.e. ∃, ∀), to implement bindings. * During unification, as instantiations to free variables. I guess I need to do some homework here :-)
Turn this into an edutainment video game, now. If presented properly, you could actually sell this, but it would at least make a fun phone app!
The proposal quite clearly keeps `return` around, as a top-level definition. There is no reason to start using `pure` instead of `return` in new code, and in fact, thanks to its new more general type, we'll be able to use `return` in Applicative code as well. Since the purpose of the top-level definition is clearly to allow (new and old) code to continue to use the name `return` instead of the name `pure`, renaming it to `inject` would defeat the purpose. If you prefer the name `inject` to the names `return` and `pure`, you can simply define a top level definition `inject = return`, both today and under the new proposal. I guess that if many people prefer that name, that top-level definition might get added to the standard as well, in addition to `return` instead of replacing it. But since this is the first time I hear that new name being proposed, and since there is a cost to introducing new definitions in base which might clash with existing code, I highly doubt you'll manage to piggy-back on this proposal. If I were you, I would instead create a package containing this new top-level definition, and see if the usage statistics of that package are high enough to justify a change to base.
That kind of answer I hoped to get, thank you. &gt; if you follow the ticket links they all look to have been resolved The links point to HaskellPrime trac, not UHC one. These extensions are part of Haskell2010, so the issues are closed. IIUC the missing holes are still missing from UHC. &gt; First is exactly what you say: to canonize a number of language "extensions" I agree with the whole section. &gt; The second thing I'd like to see out of a new standard is to properly deal with all the fiddly bits that show up from having slapped more plaster on the old language for so long. I strongly agree with that. I have a bit different list of fiddy bits to fix, but it is expected and normal. &gt; But because development process and the need for backwards compatibility, we've introduced a whole slew of ad hoc keywords. It will be hard to fix. It probably requires a major redesign of half of the language IIUIC. But it is definitely a good long-term goal.
In the evaluating expressions part, you probably wanted to write `(Char -&gt; Expr a) -&gt; …` instead of `(Char -&gt; Expr a) =&gt; …`.
Exactly! Some monads are easier to implement in terms of join, and it always bothered me that =&lt;&lt; is treated as a second-class citizen when its signature is much closer to fmap than &gt;&gt;='s. Having both mirrors the situation of &lt;* and *&gt; in Applicative nicely.
&gt; Finally it would be awesome if =&lt;&lt; and join could be moved into Monad! Well, `GeneralizedNewtypeDeriving` breaks if you move `join` into `Monad` because roles aren't expressive enough. I find it ridiculous that `join` can't be made a part of `Monad`, but that's how it is, currently.
Is it Columbian cofree?
Those designers aren't really doing you a favor. Hope you don't mind me giving a bit of advice: * Show a clear statement of what you do, what services you deliver. You want to catch the attention of your intended audience quickly, so a wall of some cryptic message isn't really a good idea. It also takes up valuable screen space for what really matters. * Your bear logo is partly obscured by the main content, depending on the width of the page. * Having a bear mascot is fine, but the image you choose is rather low quality. * The logo looks like a comic book logo, which isn't normally associated with a serious business (unless that business has to do with comic books or children). * Choose a matching color scheme for the site. If you like blue, stick to blue, don't mix it with bright red and yellow. * The main content block is rather narrow for todays standards. Most monitors have much more screen estate. * Typography and style does wonders! Use a different color for links. Sans-serif is generally better for web pages. Use a different font for headings than for text. * Get rid of the thick borders which make the page look too confined. * Use the same style for the mailing list page. * plaim ~~are~~ _is_ a [..] company. Company is singular. The content is fine, but I would make sure that you put your mission statement on the front page. The page from SPJ looks fine to me. It's plain, but professional, there are no design mistakes. SPJ is already very known, so he doesn't really need a fancy page. You page gives an impression of amateurs, which or course you guys aren't. If you intend to attract customers with your website, then you need to send out a clear message.
Could we go further, and pull out `pure` into its own class while we're at it? Something like class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b class Functor f =&gt; Pointed f where pure :: a -&gt; f a class Pointed f =&gt; Applicative f where (&lt;*&gt;) :: ... class Applicative f =&gt; Monad f where (&gt;&gt;=) :: ... I mean, pointed functors are a concept in their own right in category theory, so I feel like it makes sense in principle to factor them out into their own typeclass. OTOH, I'm not sure how useful it'd be in practice - off the top of my head, I can't think of any Haskell `Functor`s that are pointed-but-not-applicative.
It was not an oversight; I think [I even mentioned this specific case](https://wiki.haskell.org/Functor-Applicative-Monad_Proposal) as something beyond scope of the AMP in the proposal. Writing an Applicative for each Monad was something you could have done at any point before the AMP, and in fact most libraries did that. This way, things were future-proof before they even had to. Compare that to removing `return`: this breaks *all* Monad instances written before the AMP, unconditionally.
_People don't just read our papers to figure out what we're about._ Yeah, you have to bait them first :-)
Ah, this is not my work. :)
The Haskell community cares a lot about backward compatibility, but it cares more about correctness. This is, in fact, Haskell's unique selling point - just like C++ rarely sacrifices performance for convenience, Haskell rarely sacrifices correctness for convenience (a few blatant exceptions like the `Num` typeclass notwithstanding...)
The vispy talk was pretty impressive, and to be fair I haven't seen any other programming language that has that kind of plotting library. I need to see if there's been any updates that make it easier to use under Windows, last I tried the Cuda support wasn't working for me. 
True. And just for the record: I'm in favor of both the `return` and `&gt;&gt;` proposals.
We don't put redundant things in classes for aesthetical reasons. There is no flipped fmap in Functor, there is no flipped ap in Applicative, and there will be no flipped bind in Monad. Type classes should contain the minimum amount of operations to make an abstraction possible, and for efficiency, some redundant other definitions. Those serve a definite purpose that would not be possible if they were a top-level synonym. On the other hand, adding `=&lt;&lt;` to Monad offers no benefit whatsoever over having it as a top-level definition in `Control.Monad`. If you're still convinced this is a good idea, by all means, post it to the proposal discussion on the mailing lists.
Excellent, having you and Edward on board is key to success ;-D
&gt; We've sought to keep the text factual, rather than imply that one option is "best" for any particular class of user, since opinions vary so widely on this point. One objective element is that (IIRC) the Haskell Platform makes it hard to reliably build some libraries, such as `network`. (My apologies if this is no longer the case.) I think it would be fair to include that part in the Haskell Platform blurb.
I see what you're saying, and I agree that we don't need flipped versions of fmap or &lt;*&gt; or everything else under the sun. The reason I pick on bind is that Monad currently only has the flipped version, and not the unflipped one. In an ideal world I would prefer &gt;&gt;= to be thrown out in favour of =&lt;&lt;, but that would break such an amount of existing code. :)
A more interesting split would be class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b class Functor f =&gt; Apply f where (&lt;*&gt;) :: ... class Apply f =&gt; Applicative f where pure :: ... class Apply f =&gt; Bind f where (&gt;&gt;=) :: ... type Monad f = (Bind f, Applicative f) Just sayin'.
 I strongly #if __GLASGOW_HASKELL__ &gt;= 710 support this #elif __GLASGOW_HASKELL__ &gt;= 708 proposal. #endif Edit: just to be clear. This is meant to be sarcasm, lamenting that the new "nicer" spelling of the old thing will be lots of CPP.
Nice post! Small advice – use a `type Variable = Char` synonym or something similar (maybe even a newtype) to abstract over how exactly variable is represented.
To be fair, having Edward involved in anything is a sufficient precondition for success. I'm happy to go along for the ride.
Oh neat! If I drop the nil constructor that's it exactly.
I think the problem here is a level of abstraction: sure, if you run `blacklist_filtering` twice and check it's idempotent, you're *really* just testing `filter` and `Set` - *but that's not the point*! The point is that `blacklist_filtering` works. You could reimplement that function in a totally different way (using different data structures, a different filtering function, maybe even direct recursion) but you want the same property, and the same test would apply.
You're sure about that? Given the current environments where I use GHC, I could only read that message at work: GHC | Where | Message --|-|----------------- 7.6.3 | (Ubuntu 14.04 LTS)| "I strongly" 7.8.x |@ Home | "I strongly proposal" 7.10.x |@ Work|"I strongly support this" That's not really backward-compatible ;).
If you treat your functions as blackboxes operating on opaque types, then `blacklist_filtering_idempotent_prop` is a very good test. -- constructor not necessarily exported newtype Item = ... -- Might be a set actually newtype ItemList = ... deriving (Foldable) instance Arbitrary Item -- | Whether item is blacklisted isItemBlacklisted :: Item -&gt; Bool -- | Removes blacklisted items blacklist_filtering :: ItemList -&gt; ItemList And the actual properties: blacklist_filtering_idempotent_prop :: ItemList -&gt; Property blacklist_filtering_idempotent_prop il = let filteted = blacklist_filtering il in filtered === blacklist_filtering filtered blacklist_filtering_filters_blacklisted :: ItemList -&gt; Property blacklist_filtering_filters_blacklisted il = let filtered = blacklist_filtering il in all (not . isItemBlacklisted) filtered They speak on the right abstraction level! You are free to change the implementation of `blacklist_filtering` or `ItemList` or ... Tests aren't only to help you to get stuff right the first time, but also help you refactor your code later. For some implementations of business level functions, business level properties are obvious. This is good thing if it so. But sometimes business logic is so complicated that properties you care don't *follow directly from library definitions*, i.e. we don't *trivially see* our implementations are correct. Not the case in your example though.
Easier said than done. You can lose the "newtype is free (performance-wise)" guarantee that way.
Thanks for you suggestion. I really had this exclusive or idea about 'unit tests' (for lack of a better word) and property based tests. Too bad there is no such thing as a silver bullet in software design.
To me it's just another piece of evidence that roles is the wrong solution.
`+`, `abs` and `fromInteger` seem unrelated 
What do you mean with: &gt; including the implementation directly into its documentation I find your remark: &gt; if you make the claim that it's idempotent but don't prove it, that is indeed a property you should test. very interesting. I don't know if the following is what you meant, but it's how I interpreted it. In a language like python in which you can't guarantee that a function has no side effects use a test to show that it is indeed idempotent. In Haskell we can proof the function is idempotent, because filter is idempotent. This begs the question, is there a way in Haskell to describe properties like this? 
Java and C++ classes are far more similar to each other than `return` in Haskell compared to almost all imperative languages though.
I meant "class" in Haskell vs. "class" in Java or C++.
- `Num` is too broad: it includes 7 operations that are only partially related, and there are plenty of things where you would want to implement some of them, but not all. For example, for a `Nat` type, you'd want addition, multiplication and maybe subtraction (though it wouldn't be total), but negation wouldn't make sense. Another example would be vectors (the Math thing, not the dynamic array storage algorithm): these can be added, subtracted, and negated, but neither `abs` nor `signum` make sense, the `fromInteger` implementation would be kind of arbitrary, and multiplication is problematic because we'd have to decide between scalar multiplication (which wouldn't match the type that the typeclass demands, because its type would have to be `a -&gt; Vector3 a -&gt; Vector3 a`, not `Vector3 a -&gt; Vector3 a -&gt; Vector3 a`), dot product (which wouldn't match the type either, returning a scalar instead of a vector: `Vector3 a -&gt; Vector3 a -&gt; a`), or cross product (which is bad because it is only defined for 3-space and 7-space vectors). `Num`, however, is all-or-nothing - if you want one of the operations, you have to implement the others too. - `Float` and `Double` being instances of `Num` is wrong, because floating-point numbers violate a few laws, such as `a + (b + c) == (a + b) + c`. This isn't just academic musings, it turns out to be a real concern in real programs. - All `Num` instances form monoids over both addition and multiplication, but the typeclass doesn't reflect it. Part of the problem is that Haskell expresses Monoid as a typeclass rather than a type-level tuple (something like `(Int, +, 0)`), but still; there's a monoid that the code doesn't tell us about.
That looks cool, but it is way more than a couple lines, and it also uses Java.
Got a link to Hackage / Github? ;)
The end boss is `P = NP`
`abs` arguably makes sense as the norm of the vector, and you could reasonably define `signum` as the elementwise signum. Otherwise, very much agreed.
&gt; If you can make TH simple enough, and not deeply intertwined with compiler internals, then it can replace lightweight macro processing. I rather routinely use CPP for things like adding an extra deriving clause and the like. Those things would be inordinately painful with something less syntactic than CPP. CPP hasn't changed since the stone age. TH changes every version. Heck, a rather large percentage of my CPP lines are to work around differences in template-haskell! I'd love for TH to be able to fill that role, but it is inherently much too complicated and too much of a moving target and even if we could accept that, it can't do much of what is required, (edit export lists, toggle language extensions) and has the disadvantage of basically killing any implementation of Haskell but GHC, because nobody else has it.
&gt; I think at this point everyone agrees that (a) type families are the way of the future, and (b) current type families are crippled by their inability to express everything fundeps can. I'd say that I happen to agree that internally in core type families are the way to go, but that I have concerns in that I can't point to any non-ghc implementation of the idea, and that functional-dependencies in the surface language are something I'll fight long and hard to keep from seeing removed. We have a long history of embracing several ways to do a thing in the surface language wrapped around a rather simple bit of core functionality. As for the report, OutsideIn(X) is hard, and adding 80 pages to the standards document in order to describe TFs seems somewhat unpalatable. The nice thing about Haskell today is that the Haskell2010 language target is at a really good sweet spot. More or less every stage of implementing it is "obvious", most implementations strategies work. How to typecheck it basically writes itself. With type families this is significantly less true! If we can figure out a way to describe TFs in a concise, accurate, way then they'd add an enormous amount of value to the report, though. Getting that right would transform maintaining the report from a janitorial task to something of a grand undertaking.
I used to think I wanted this, but in practice the problem is with inference. If you split out both `pure` and `(&lt;*&gt;)` then you often will get situations that only infer as needing `Apply` (Semiapplicative) + `Pointed`, despite the fact that they really needed `Applicative` for the unit laws.
`join` was considered as part of the original AMP. The only reason it wasn't included in the end was that it conflicted with the `RoleAnnotations` machinery. With `join` in the class we couldn't use `GeneralizedNewtypeDeriving` to derive `Monad`, which was pretty damning.
Did he ever write part 4? That blog just blew my mind a little, and now I want to learn what it means to divide and subtract my types.
Hear hear.
...are there any types for which it would be sensible to have instances of `Apply` and `Pointed` that taken together don't obey the `Applicative` laws? Then again I'm still not sure what `Pointed` even means without also having an `Applicative` instance. What would you *do* with it?
You can also use my `ad` package with a symbolic number type, like the one from `traced` or `numbers` and get symbolic differentiation.
You would create an affine traversal.
OOOH! I never ran into `data-reify-cse` before!
Ok, but if that's the only use case it doesn't seem terribly persuasive. :T
The issue is more that `Pointed` carries no useful laws on its own. Folks abuse it for making singleton sets and doing all sorts of things. It is only with the presence of an additional class that it has no inheritance relationship with that it'd start to pick up laws in this setting. As a guideline I tend to allow classes to pick up laws when you refine them with a subclass, because that is a rule the person writing the subclass can follow. There is someone to blame. But when you start saying instances of class X should obey some extra conditions when it is also an instance of an unrelated class Y and class Y should obey extra conditions when it is also an instance of class X, then 'nobody is to blame' when the laws get violated.
Definitely not java.
Are there types that are both `Pointed` and `Apply` but do not obey the unit laws? Because in terms of practical problems with inference, I expect that using `ConstraintKinds` and type Applicative f = (Pointed f, Apply f) addresses them well enough. Thinking about it, I guess silly instances of `Pointed` like an instance for `[]` that puts _two_ copies violates the unit laws. Are there any instances we care about? It's not like we're shy to laws about interactions of type classes.
Can you give an example of when you would actually want an affine traversal?
Such an instance arguably should violate some sort of law for `Pointed` instances, but AFAIK there's no way to specify such a law except by way of otherwise unrelated classes.
As much as I'd love to have the Semiapplicative/Semimonad machinery in the main hierarchy I've resigned myself to it being a niche enough desire that I can't win over the bulk of the population.
One could argue that in this case the type system and the fact that Prelude functions this simple can usually be assumed to be free of simple bugs found in tests one can come up with off the top of one's head already solve your confidence problem in the code. You do not need to test things further until you compose functions large enough they are not obviously correct.
EDIT: here's a longer, better announcement: &lt;https://www.reddit.com/r/haskell/comments/3mvaib/haskellcafe_announce_megaparsec_an_improved_and/&gt;. --- The changelog is [here](http://hackage.haskell.org/package/megaparsec-4.0.0/changelog). I'll briefly list the main improvements: * It no longer clashes with `Control.Applicative` (i.e. you don't have to hide `&lt;|&gt;`, `many`, `optional`). * Parsers like `integer`, `hexadecimal`, `charLiteral`, etc are included by default (Parsec had them too, but using them was [rather cumbersome](http://hackage.haskell.org/package/parsec-3.1.9/docs/Text-Parsec-Token.html) and many people were just writing something like `read &lt;$&gt; many1 digit` instead). * Analyzing error messages is now easy (previously you actually had to render the error message and parse it back if you wanted to get something out of it). Actually, the error reporting logic was pretty much completely rewritten, and should produce better messages than the previous implementation. * You can add backtracking `State`, `Writer`, etc to your parsers (the latter can be useful if you want your parsers to produce warnings in addition to errors, for instance). Parsec had backtracking state as well, but it was baked into the `ParsecT` type and wasn't pleasant to work with; Megaparsec defines a `MonadParsec` class that lets you combine monad transformers with `Parser`, a la [parsers](http://hackage.haskell.org/package/parsers-0.12.3). * The annoying [silent bug](https://github.com/aslatter/parsec/issues/8) with being unable to write `notFollowedBy eof` was fixed. Oh, and it's also got a lot more tests and a benchmark suite. And the author promises not to ignore your pull requests. (I especially like this last thing.)
Pretty sure that's what he means, yes. `newtype`s may be free in terms of memory use and may not add additional indirections, but (without `coerce`) converting to and from them can require lots of superfluous traversals to apply what's effectively `id`.
Subtraction and division are a little weird. As far as I know, we haven’t really found a killer use-case for them yet. You can use them to model backward data flow and constraint propagation in a reversible computing system ([The Two Dualities of Computation: Negative and Fractional Types](http://www.cs.indiana.edu/~sabry/papers/rational.pdf)). David Barbour has suggested using them to model futures or continuations or something like that, but I don’t know if he’s worked out exactly how. I’ve thought of using fractional types to denote mutable borrowing of substructures, for example, if `a * b` is a pair, `lend_fst :: a * b -&gt; a * ((a * b) / a)` is a function that pulls out the `a` and gives you an `(a * b) / a`, denoting a pair where the first element is borrowed. `(a * b) / a` is obviously isomorphic to `b`. `repo_fst :: a * ((a * b) / a) -&gt; a * b` “repossesses” an `a` to give you back a regular old pair. Haven’t worked out the details, but I think there’s a paper to be had there, if anyone’s interested in chatting about it.
Context sensitive grammars are rare and generally unnecessary. In parsing theory, it's mostly accepted that context free grammars are the way to go. Any contextual things ought to be handled by semantic analysis The advantage of the Earley algorithm is that it can parse any CFG no matter what (unlike most other algorithms). And it can do most classes of CFG, including all LR(k) CFGs, in linear time. You would need to read up on the Earley algorithm. It really is brilliant stuff. Marpa is an example of an Earley parser generator. It is well documented and has loads of resources for learning about the algorithm. Marpa makes use of two major extensions to the Earley algorithm to fix common problems.
How does this compare to attoparsec?
I think it cares to much lol. if productive members of the community (who are maintaining dozens of libraries over multiple versions) say that a breaking change is too brutal (and they want a better plan, or better justification, or just some more time) you should listen. if some random corporation doesn't want to refactor their codebase, they should just consult someone. or if some hobbyist like myself can't be bothered to update their 2y package, then it's probably not that worth importing either. I think it's better to evaluate the scope of breakages. which is possible because (I'd guess) a lot of the haskell code that's written is in open source libraries and well-maintained. then we can run scripts against hackage to estimate the costs. I think that (open source and maintenance) is a nice social contract between the language and its community. at some point, violating backwards compatibility might become necessary, and at that point, the only way to make things better is to accept it.
&gt; does the claim that this is a better parsec come with a catch? No, that's the point of it. It's a better Parsec not because it does something cool that Parsec authors didn't think of (tho the rewritten error-handing module is neat), it's a better Parsec because it has been actively worked on for 2 months while the last commit to Parsec was 4 months ago.
And how many of those laws would `Float` and `Double` actually obey? ;]
I understand why you might want `inject` (η : 1 → T), but `return x` meaning “construct an action that returns `x`” feels more “programmatic” (operational?) than `inject x` meaning “inject `x` into the monad”, and notwithstanding the fact that `return` doesn’t short-circuit like in imperative-land, I think that’s a good thing pedagogically. Now, `simply` has a nice ring to it… 
Touché :)
The only disadvantage I've come across is that it seems like you can only return one parser (correct me if I'm wrong), as opposed to one parser per rule. However, this is the same limitation as `alex` / `happy`, too, but a disadvantage compared to the parser combinator libraries.
I've just spent 6 months at work (a windows shop) convincing sysops that including haskell in our standard desktop build will not lead to utter ruination. I now live in fear that 1. we are going to get the HP, or 2. they will add MinGHC and either announce the job done or revert to thinking that haskell is evil because they need to add MSYS to the path etc. Could we add a short caveat to each option: HP is for people who would like to try haskell out or are in a learning envoronment - hence 'with batteries included'. MinGHC is for tinkerers and developers who would need an exact setup. Stack is a build tool offering a one-stop, enterprise-ready environment. 
in Earley or Marpa? if in Earley, do you mean because the `production`s have local scope (in a do block), and only the `grammar` is a top-level definition? if so, I could clean up and upload the observed sharing interface I wrote against it. 
Except you'd think a newtype expects to be able to have whatever instances it wants. ;)
now the question becomes, where do we get a statistically meaningful population of new Haskell users? :D
Yeah but, uh. Something something `GeneralizedNewtypeDeriving` something something goofy fake syntax something &amp;c, QED.
they mean inline a definition into the haddocks. like if you wrote -- | @sum = foldr (+) 0@ (and its true) then you know sum works on empty lists. 
Sure, we can't clean up all those conflicts and we'll never have a perfect transition story. `class` was large in my mind when I was writing about `return`, too. But a Haskell class or instance definition is, at a glance, clearly not directly equivalent to the OOP concept. (The more likely point of confusion is thinking "oh, they just refer to interfaces as classes.") `return` is constantly used in a way that can be confused *semantically* with its unrelated imperative homonym, and operating under that misinterpretation *works correctly* often enough to make it even more confusing. And we have an alternative already in place! That's what really puts me over the edge in this particular case.
&gt; it's got a nice solid feel to it. you're returning a result. when you put in at the end of your do-block. I actually feel like this is problematic too. It maps well to `IO` but can give some wrong ideas about `Maybe` and dissolves into magic for `[]`. Beyond that, any number of other functions could be at the end of your `do` block and return their values directly. I can't tell you how many times I had an unnecessary `x &lt;-` followed by `return x` in my early Haskelling. It's better IMO to get the idea across ASAP that every Hakell function -- monadic or not -- returns a value, with no special syntax required to enable that. `pure` fits that goal nicely: it clearly doesn't describe an action, but a status. I feel like this leads to better intuition.
I've noticed several people comment that they had trouble installing Yi. I thought I might suggest an easy way to get Yi working for those people: All of Yi's dependencies are on Stackage. So, if you have [stack](https://github.com/commercialhaskell/stack), it's as easy as git clone http://github.com/yi-editor/yi cd yi stack init stack install 
While it's true in principle that applicative parsers can only describe context-free grammars, it's not actually the case, because your grammars need not be finite in size and since you have the whole of Haskell to write your rules. (I know a citation is needed, but I don't know where to find one quickly)
Clearly this is a reason to come up with a solution to deep class hierarchies in Haskell, so that the change is non-breaking. Then everyone will have no legitimate objection and in gratitude they'll accept the change.
Do you mean this? https://byorgey.wordpress.com/2012/01/05/parsing-context-sensitive-languages-with-applicative/
With `data-reify`, HOAS, PHOAS?
Footnote: the only vector operation that would fit `Num`'s `*` type would be the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)), or element-wise multiplication.
Though `(&lt;*)` and `(*&gt;)` aren't the same function with just the arguments flipped...
What a great timing! I was just looking into Haskell on RPi today. Will definitely check it out. EDIT. Did anybody manage to run Haskell binaries on RPi bare metal?
Yes, I'm firmly of the position that fundep syntax should be kept around just about forever. There are far too many situations where we have relations like `(_+_=_)` which support multiple modes and it's terrible to have to come up with unique names for each of the modes! The one and only complaint I have about fundeps is that the current implementation makes them incompatible with TFs. I could be mistaken, but I get the impression that the big issues with TFs aren't in the places where they overlap with fundeps, but rather it's how TFs interact with GADTs. Thus, so long as we don't add GADTs to the report, adding TFs shouldn't be much different than adding fundeps. (Closed TFs being the main new addition.) Is there something I'm missing?
This * 1 million. When we tackled AMP at skedge.me the breakage was so obvious it took 5 seconds to fix.
Many "beginners" most likely won't use it: no tutorial, no *examples* directory. Even though it's derived from Parsec, it could use those so that folks don't have to realize "oh, it's like Parsec, I'll just pickup one of the Parsec tutorials".
Why refer to the spec instead of what GHC does? I mean, GHC is basically *the* Haskell compiler nowadays, right?
For 1) I was sorta hoping there would also be some sort of magic auto-arrange. Like, throw this at dotty and see what sticks or somesuch.
The issue is more that once you add TFs then all sorts of different viable implementation strategies of Haskell are shut off. e.g. jhc uses a form of subtyping based polymorphism. It isn't clear how to implement their internals in a form suitable for TFs to me. We use a form of HMF as part of our typechecking engine for Ermine. With some minor changes we could massage that into a compiler for Haskell as it exists today, but I don't know how to modify that to add something like Fc coercions.
Two reasons: * GHC doesn't, to my knowledge, have any sort of language definition associated with it. The Haskell report is the best we have. (Relatedly, I don't think GHC makes any promises about the efficiency of newtypes, either.) * I will be really sad if the other Haskell compilers die off, and I'm a bit unhappy with a lot of the additions in GHC, so I'm not willing to just say that Haskell is defined by GHC.
Oops, fixed-- Thanks!
[It can get a bit busy](http://imgur.com/YdmEGdp)
good points 
Thank you
What about `produce` ? 
I understand you fell like you are testing `filter` but what you should test is not is `filter` works but if `filter` is actually the solution to your problem. First, I am never sure of which way `filter` works, are you keeping things which pass the test of keeping the one which doesn't. If you gold mining, filtering mean keeps what's inside the filter, if you are making apple juice, filtering means keep what go through the filter. Now, I've been telling you that are you still 100% filter is the answer to your problem ? Ok it's a prelude function so you are probably right, but what if you use my `filter` function, how are you sure it's doing what you think ? (My filter rejecting function will have the same signature as your filter function and my test will pass, because I wrote them thinking my way). Another example would be, you ask me to write your `blacklist_filtering` function, but in my mind I mixed black and white, and I wrote a `whitelist_filtering` ,i.e. blacklist_filtering = filter (not $ Set.null . Set.intersection blacklist) This function, should type check, doens't do blacklist filtering, and even though, in my mind I think I'm doing whitelist filtering it probably doesn't work either. In a word it doesn't work . If you come back to me (angry) and I tell you that I didn't write any test, because it's obvious what the function does and I would be only testing `filter` and `intersection`, would you take that as a good reason ? So what, I'm saying, what ever your implementation is, you test are there not check the implementation but also that your understanding of the problem is correct. The easiest way to do that, is example : input -&gt; desired output. For that type of small trivial function, I would just use `doctest`.
In case you missed it: http://incredible.nomeata.de/
As (correct) proofs have quite a regular structure, we should be able to do something here (https://github.com/nomeata/incredible/issues/21), it just needs doing :-)
&gt; The more likely point of confusion is thinking "oh, they just refer to interfaces as classes." Yet, classes aren't "interfaces" either. If anything, they loosely resemble C++ templates, but even those are a different beast. If typeclasses were called "interfaces" in Haskell, we'd have a situation similar to what we have with "return" right now: a superficial resemblance, and an overlap in real-world use cases, but in the end, we're talking about completely different things, which causes confusion. Other candidates for such confusion that come to mind: - Tuples in Haskell vs. Tuples in Python - `do` in Haskell vs. `do` in Lisp - `=` in a Haskell let binding vs. `=` assignment in imperative languages
I agree; A/B testing installation types could be a good idea; if I may throw my vote in, I haven't tried Stack yet, but HP under OSX has been an absolute p.i.t.a. as a beginner; only after hitting several dozens of walls I switched to the minimal installation, and lived happily ever after Edit: moreover, how to elicit user feedback regarding the installation process? from within haskell.org itself? from a prompt in the installer? 
+1 for the "intended audience" text
No, my brain took a shortcut via &gt;&gt; . As /u/quchen points out elsewhere in the thread, having both flipped and unflipped versions isn't a good thing anyway. My argument for =&lt;&lt; becomes a bit weaker as a result. I still think =&lt;&lt; is the operator that fits together with $ and &lt;$&gt; , whereas &gt;&gt;= fits with &amp; and &lt;&amp;&gt; (in other words &gt;&gt;= is flipped and =&lt;&lt; is unflipped), but we can't very well replace &gt;&gt;= with =&lt;&lt; . I would prefer a compromise that brings consistency at the cost of duplication, but there seems to be strong support for the oposite compromise.
That's what I meant by "point of confusion" -- they bear enough of a resemblance to interfaces that a lot of people (myself included!) grab on to that as a first familiar intuition to work from. I definitely wasn't suggesting it's a good name for them.
I thought about it some more after the last post and have come to the conclusion that the problem in the Java ecosystem that leads to these kinds of solutions is more related to the way Java is deployed. Almost every other language allows developers to rely on services provided by the underlying operating system. Cron (or the Windows equivalent) for a scheduler, syslog daemons or services for logging, logrotate, init systems to take care of restarting a crashing app. JVM applications on the other hand usually don't have that support at all since they are supposed to run on their own on a variety of platfoms so all kinds of stuff that really shouldn't be part of an application at all is built into them. Similar things are true about conventions. Unix or Windows have a huge number of conventions for e.g. filesystem locations for certain things, ways to lookup locales, timezones and all kinds of similar system dependent information. Since JVM applications can't rely on those conventions they do configure a lot more, e.g. in our logging example on Unix most daemons log to /var/log/&lt;name&gt;/&lt;name&gt;.log or something in a very similar location, put a logrotate config file into /etc/logrotate.d on installation and do not have to worry about anything but the most extreme logging cases any longer.
The next thing the android port needs is a cabal repository with patches for all the packages (like network) that don't build without tweaks. Having such a repository has been useful for the iOS port and would be for android too. I have a few thousand lines of patches with which everything up to yesod can be gotten working on android, but I don't have time to make such a cabal repository.
Firstly, all Prisms are affine traversals. Lots of traversals are in fact affine traversals. From the user code's p.o.v, an affine traversal is useful when you're using `^?` and you know you're not potentially throwing away information (throwing away information is a common source of bugs, IME, which is why warnings like "unused parameters" are useful). 
Perhaps I haven't been clear enough (sorry for that!). I didn't mean that Megaparsec is 2 months old, I meant that Megaparsec is Parsec + 2 months of intensive polishing. Look at the [commit graphs](https://github.com/mrkkrp/megaparsec/graphs/contributors) if you don't believe me (“mrkkrp” is the author of Megaparsec, “aslatter” is the maintainer of Parsec). It's just a fork, not a rewrite from scratch! &gt; GNU `cat` had its last commit 10 years ago Luckily it's not the case – see the commit history for `cat`: &lt;http://lingrok.org/history/coreutils/src/cat.c&gt;. Like all other GNU coreutils, `cat` is being maintained; the latest (admittedly, pretty minor) bug was [filed](http://bugs.gnu.org/18449) almost exactly a year ago and fixed *on the same day*. Moreover, even `cat` has 3 tests. `chmod` has 11. `rm` has [48](http://lingrok.org/xref/coreutils/tests/rm/). Would you care to guess how many tests Parsec has in its [testsuite](https://github.com/aslatter/parsec/tree/master/test/Bugs)? &gt; The test should be whether the software works, not whether it is being worked on. Does Parsec work? Sure. Can it benefit from more active maintenance? I claim it can (and has).
Same. I *think* this function would allow me to remove something hacky I did with my page templates, but I'm not sure I need to bother.
How do I input logic symbols to try to prove other things? Also, I've managed to remove the horizontal bar in the textarea under "Custom tasks". The Add button doesn't do anything after that (it doesn't even report an error).
&gt; Also, why do you need to rule out empty sums and products? Just simplify them to zero and one, respectively. What do you mean by this? (I'm learning Haskell)
Thanks for the info! The task sound like something that nix can handle very well by having specific hackage configuration for different flavors for adjustment. (https://github.com/NixOS/nixpkgs/tree/3f27be8e5d5861cd4b9487d6c5212d88bf24316d/pkgs/development/haskell-modules) Making a successful build of yesod sounds like a good milestone. Wil try. Thanks!!
He is refering to the code giving back an error when you try to get the sum or product of the elements of an empty list even though those things have a defined value. Also, Wikipedia articles to the rescue: [Empty Sum](https://en.wikipedia.org/wiki/Empty_sum) [Empty Product](https://en.wikipedia.org/wiki/Empty_product)
 type Applicative = (,) &lt;$&gt; Pointed &lt;*&gt; Apply Hey, let me dream.
Ah, that's the key insight that I was missing! That actually solves my use case I didn't care about being able to observe the sharing. I just wanted to be able to use the parsers independently
For building a cross compiler itself, template haskell is not a problem. But we cannot cross-compile a program/library that uses template haskell at this moment. This is because of how ghc handles TH code. GHCJS (this is a kind of cross compiler (host is binary, target is javascript)) solved this problem very elegantly by out-of-process-template-haskell and there have been some discussion about this for mainline GHC. (https://mail.haskell.org/pipermail/ghc-devs/2014-December/007555.html) Before realizing this, we need to restrict ourselves to the world of no TH (which I think still very okay). 
&gt; And do I need to mention what "isomorphic" apparently means in the JavaScript world? I hope not. Duckduckgoed and found some articles about running application on both client and server. Are you referring to this 'concept'? 
Found this from the blog of /u/joeyh http://joeyh.name/blog/entry/Template_Haskell_on_impossible_architectures/ This helped me better about the situation.
started ghc-android organization (http://github.com/ghc-android) please let me know if anyone is interested.
for nix build case, we can set up a hydra server for distributing binary. I will find some resource for this.
Great code and great ideas :) I do feel a bit bad for saying this, though, but I feel like this case is a classic example of an *abuse* of typeclasses. Typeclasses ended up turning out to be a bad idea in most cases because they made code a lot less maintainable and harder to reason about, and also turned "normal" haskell code on first-class data to compiler magic...and sometimes unpredictable compiler magic, too. It turns out that there are actually only a couple uses of typeclasses where people have agreed that the benefit outweighs the massive, massive headache and loss in maintainability...the main one is when your typeclass has laws that lets you reason about code in a polymorphic way that lets you write safer code. For example, in `Control.Monad`, there's a lot of code that works polymorphically over all `Monad m`, and the way it's written is much safer than if you had written `replicateM` over `IO`...because if you write something polymorphic over all `Monad m`, you're forcing yourself to only use things from `Monad` that you can reason about, and you don't introduce yourself to arbitrary effects. Writing code polymorphic over monads with laws helps you write safer code. Aside from that, the general rule of typeclasses is: "If you think you want to make a new typeclass, you probably don't." As it turns out, using typeclasses as a way of giving multiple types functions with the same names ends up making everything messier and it's a small benefit/convenience that is nowhere near worth the headache and all that you lose. Typeclasses as sort of generic interfaces often ends up in the same situation. Sometimes there's a benefit if you want to leave your class open for other people to write their own instances (a lot of libraries do this), but in your case, there will only ever be three instances ever...so you are in a good position to just use normal haskell data types in a normal way, instead of relying on compiler magic in typeclasses. A lot of us started using Haskell to avoid implicit compiler magic, right? So why jump back into it? :) How about just defining all of your methods to work only on `Isom`? You can probably have: transBy :: Trans -&gt; Isom -&gt; Isom rotBy :: Rotation -&gt; Isom -&gt; Isom reflectBy :: Reflection -&gt; Isom -&gt; Isom Which are pretty straightforward to write if you have `Num` instances for `Vec` and `Angle`. (or even `Trans` and `Rotation`)...which are definitely idomatic and great :) You can have apply :: Isom -&gt; Vec -&gt; Vec where you'd be writing the application logic for all types of basic transformations at once. If you really don't like this, then you can just write applyTrans :: Trans -&gt; Vec -&gt; Vec applyRot :: Rotation -&gt; Vec -&gt; Vec applyRefl :: Reflection -&gt; Vec -&gt; Vec and then use what you already use with composition in your definition of `transform`. This is honestly the exact same thing...you just don't get to use the same name (`apply`) for all three anymore, but that's okay/fine :) A marginal loss for a huge gain. The same logic applies to your new `compose`, which is your new `(.+)`. Sorry if my commentary sounded negative! I just wanted to warn you so you don't go through all of the headache that the Haskell community went through ever since typeclasses were invented. Back in the day, we *did* use typeclasses for things like this...we used them in very situation when we thought they might be useful, we used them like they were the coolest new things ever. We wrote them without a care in the world! They were our interfaces! But the code that ended up being written was awful and unmaintainable and very fragile, easily broken...and we also realized that the "benefits" (being able to use the same name for different types) were marginal and *superficial* at best: they weren't *real* benefits and didn't actually help us write cleaner code and better logic...it was more or less completely superficial. So, hopefully knowing that you're going through something that *everyone* in Haskell went through might be encouraging :) You're thinking like a Haskeller! On other notes, I think that `data Flip = Yes | No` is a great idea. It has a lot more semantic meaning than `Bool`, and it makes your code a lot more readable. People don't have to think "wait, what is True? What is False?" -- your code self-documents. It's a very commonly used pattern across Haskell :) One last thing...do you *really* think that transformTimes = (transformSeq .) . replicate is more readable than transformTimes n = transformSeq . replicate n or even transformTimes n t = transformSeq (replicate n t) ? I mean, really, look deep into your heart :) **EDIT:** It might be interesting to see that `(.+)` (with an identity transformation) alone does have meaningful laws...so you actually might be inclined to pull that out as a class: class Composable t where (.+) :: t -&gt; t -&gt; t cId :: t And your laws are: (x .+ y) .+ z = x .+ (y .+ z) x .+ cId = x cId .+ x = x And that would be a great idea for a typeclass. It has laws and everything, and it lets you reason about code polymorphically! You can write instances for `Isom`, `Trans`, `Reflection`, `Rotation`, and they can all obey those laws :) Luckily for you, this typeclass already exists in *base*...with these exact same laws. It's class Monoid m where mappend :: m -&gt; m -&gt; m mempty :: m So, this is a situation where a typeclass *would* be a good idea to write...only, it was such a good idea that someone already wrote the typeclass *for* you :) Now you can define `(.+)` as `mappend`, if you want! And you can also define `(.+)` for all for your types separately, if you wanted, too, instead of all for `Isom`. Then you get all the benefits of methods written for all `Monoids`...like, `mconcat`, which is `[Isom] -&gt; Isom`, for instance. You can also throw it into the Monoid instance for Maybe, or Writer...
You made a great bunch of points that'll take me a while to digest. &gt; I mean, really, look deep into your heart :) Oh, of course not, that's just a joke that I'm going to undo in the next commit. :P
Not saying this isn't a thoughtful post or anything, but... it's been a long time since I've seen a post on /r/haskell that I disagree with more than this one.
Am I reading your repo right that the interface is written all in JavaScript? Hmm. I was going to try to steal this to write a toy IDE, but not sure its worth it as I don't know JS very well.
It's not that I have any particular obsession or love for Haskell's rather limited notion of a typeclass, but rather, the underlying ideology that I support is writing code which is based on the exact traits needed instead of on some particular implementation, and in fact, avoiding an implementation for as long as possible. In Haskell, typeclasses are somewhat helpful in doing this. Any time you write on a type instead of something more generic, you're saying "my function needs *all* the properties of this type!" Why require your user (or, more importantly, yourself) to provide all those properties instead of just the ones which are actually needed in that local function scope? Just my two cents.
Attoparsec is specialized - and highly optimized - for parsing text, where the token type is `Char`. That distinguishes it from parsec and other parser libraries that allow more general token types, such as when you parse a higher-level representation after first applying a lexer. By specializing in this way, attoparsec is able to optimize heavily using internals of the text and bytestring libraries, and is therefore far faster than parsec. I recommend attoparsec over parsec for any parsing task where you parse text directly. There is nothing special about network protocols or anything else. Attoparsec is for general parsing of any kind of text. Use [binary](http://hackage.haskell.org) and the like for binary network protocols, not attoparsec.
As primarily an application programmer, AMP hardly affected us. Whereas this step will cause more pain. Nevertheless, I am +1, because it's the right thing to do, and because there is a reasonable deprecation cycle.
I didn't find it so negative. It claims certain improvements over parsec, which is fine. It's true though that if this aspires to be a complete replacement for parsec, the focus should be on core functionality ("this is a parser combinator library"), with perhaps a link to a description of how it compares to parsec.
Is this the actual solution? If so, can you explain it? I have no idea what's going on ;[
Yes, "simple enough" would have to include fixes for all of those things. What /u/Yuras has in mind, I think, is something much more similar to CPP or m4 than TH, but with some minimal awareness of Haskell surface syntax. So that, for example, you would be able to embed macro expressions in Haskell expressions without worrying about macro expansion spilling out of the Haskell expression. The problem is that Haskell syntax is quite a hairy beast.
Add a line consisting imof one or more hyphens and you are good to go.
Interesting, I'd not considered that convention, thanks!
Ditto.
So first, we transform `separatedLines` into `sortedWords`. And the final output we want is `lineLocations`. Speaking a bit more generally, what we have is a list of words. And what we want is a list of lists of occurrences of those words. So we'll want to do a transformation of some kind on the list of words. What would we do with a single word? wordOccurrences :: String -&gt; [Int] wordOccurrences word = ... Well, we want to see if it is a member of a line. If it is, then we add the index of the line into the result list. If we have an item, and we have a list of items, how can we tell if the item is in the list? isItemInList :: a -&gt; [a] -&gt; Bool is the type signature we'd want. So let's search that signature in [Hoogle](https://www.haskell.org/hoogle/): [`a -&gt; [a] -&gt; Bool`](https://www.haskell.org/hoogle/?hoogle=a+-%3E+%5Ba%5D+-%3E+Bool). The first thing that comes up is `elem`! So we can use: "word" `elem`["word", "asdf", "lol"] =&gt; True "wat" `elem` ["asdf", "wut", "no"] =&gt; False Back to the function. Let's go ahead and pass the array of lines in, to make it a bit more explicit And we'll use a few type synonyms to make it clearer. type Line = [String] wordOccurrences :: [Line] -&gt; String -&gt; [Int] wordOccurrences lines word = theAnswer where -- ... We have a list of lines, and a word. And we want a list of indices. Let's map some function over the list of lines. We know we can figure out if the word is in the line with the `elem` function, so let's try that: lineHasWord = map (\line -&gt; elem word line) lines Note that you could also write: lineHasWord' = map (elem word) lines Consider that `(\line -&gt; elem word line)` has the type `Line -&gt; Bool`. And `elem word` also has the type `Line -&gt; Bool` in this context. We don't need to write a lambda, we can just partially apply the function. So now we have converted our `[Line]` into a `[Bool]`. If the word was in the line, we have `True`. Otherwise, we have `False`. So we want to convert those `True` values into their indices in the list, and remove the `False` values. Let's `zip` the list of bools with `[1..]`: indexedBools :: [(Bool, Int)] indexedBools = zip lineHasWord [1..] Now we have a list of pairs of Bool and Int. We can filter the list, only keeping elements which have a `True` in their first element. We can access the first element with pattern matching: truePairs = filter (\(b, _) -&gt; b == True) indexedBools but we don't really need to compare with `True`, we can just use the bool itself: truePairs' = filter (\(b, _) -&gt; b) indexedBools We can also use the function `fst` to get the first item out of the tuple: truePairs'' = filter fst indexedBools Lastly, we need to extract the `Int` from the `(Bool, Int)`. Again, we can either pattern match or use the `snd` function. For brevity, let's just do `snd`: intFromPair = map snd truePairs Which... actually is our answer! theAnswer = intFromPair Now you just need to map `wordOccurrences` over the list of words. A more experienced Haskeller might instead write something a bit more compact: wordOccurrences ls w = map fst . filter snd . zip [1..] . map (elem w) $ ls 
Using your `separatedLines` variables, we first label each line with a line number. We have to do this now, otherwise the information is lost labelledLines = zip [1..] separatedLines We then label the individual words in that line with the line number and flatten the list, but instead of using only the line number, we make it into a singleton Set. labelledWords = concatMap (\(lineNumber, words) -&gt; map (, Set.singleton lineNumber) words) labelledLines Now we collect together all the line numbers that a word appears on by unioning together all the sets. lineMap = Map.fromListWith Set.union labelledWords Now we have a map with the words as keys and a set of line numbers as values. This has all the information you need in a very nice form. To get the list of line locations like you asked, we can do lineLocations = map Set.toList (Map.elems lineMap) 
For Ryan Newton's talk (day 3), he has you pull down a [multi-package GH repo](https://github.com/iu-parfunc/haskell_dsl_tour) and `cabal install` everything. Yeah... no thanks :( This `stack.yaml` worked for me: flags: {} packages: - front_end/directive_driven/ - front_end/overloading/ - middle_end/GADT_transforms/ - middle_end/multi-level_AST/ - middle_end/nanopass/course_example/ - middle_end/nanopass/exercise/ - middle_end/syntactic/ extra-deps: - syntactic-2.1 - data-hash-0.2.0.1 - wl-pprint-1.2 resolver: lts-2.22
First of all, you would never define a datatype like that. It makes more sense to have: data Car = Car Color Then if you want a list of cars, you just declare it: [Car Red, Car Blue] Now a simple function to get the color of the car. carColor :: Car -&gt; Color carColor (Car color) = color Now get the color of every car in a list. carColors :: [Car] -&gt; [Color] carColors cars = map carColor cars Disclaimer: typed this out on my phone and I'm not an expert by any means, so there might be some syntax errors above. Also, look into record syntax to avoid having to define the carColor function explicitly.
Dunno about the Haskell implementations, but all morphisms in a category must be invertible. Elements in a monoid don't have to have inverses. Edit: I was wrong, see below.
Shouldn't that just be `id`, or am I looking at that the wrong way?
[This is a place where the documentation is noticeably lacking...](https://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Category.html)
Well, once such a flexible system has been created and in use for a few years, then I'd be happy to consider starting to move code to it. I just admit to a level of pessimism about a sufficiently clean design existing.
I agree, this is a valuable point, but I am not sure that it necessarily has that much to do with type classes. For example, you could achieve the same by performing the typeclass desugaring (that the compiler would otherwise do) by yourself: classes become data declarations, and instances become values of that data type. Another slightly different approach is to use parametricity to hide irrelevant details.
Oops, I was thinking of groupoids. :(
I have to agree with /u/mstksg that typeclass here are not necessarily good, especially because not all of the functions in it seems to make sense. I don't necessarily agree with the fact that type class always needs law. A more pragmatical way to decide if type classes are needed is to answer the question : Am I planning to write useful generic functions which will work whatever the real type of type class is ? If so then it's a good candidate for a type class. In your case, AFAIK the real useful type is `Isom`. What you call `BasicIsom` are just contructors or a way to get the result of the decomposition to those basic elements. I'm not sure you'll ever use them as such (except in the compose code). Anyway, If I was writing a `BasicIsom` class I would just use `apply`. Last thing, about `data Flip = Yes|No` it would be better indeed to do `data Flip = Flip | Unflid`, but you can even go further and do `data Ref = Flip | DontFlip`. 
Yes, this is exactly the reason why I want to be able to test such a 'trivial' function. I made the error of writing union instead of intersect, effectively creating a tautology as predicate. How would you test whether you wrote the desired blacklist instead of a whitelist? If you test your function with an idempotent test then you won't be able to figure out whether you made a blacklist or a whitelist. 
&gt; Can you give some more information or examples about type classes making code more difficult to reason about and maintain? This example is not really about "reasoning" or "maintenance" but simply about writing code. Take, for example, postgresql-simple's query_ :: FromRow r =&gt; Connection -&gt; Query -&gt; IO [r] `FromRow r` is a constraint which literally just gives you a "default" value of type `RowParser r`. It is easy to use in simple situations (you just let the compiler infer the `FromRow` instance for your type) but frustratingly awkward to use in any case where you want the slightest bit of control over the `RowParser`. It doesn't allow you to supply `RowParser`s you've made yourself but forces you to use one magicked up by some instance definitions (if indeed one exists). The fully-general version of `query_` you want is queryWith_ :: RowParser r -&gt; Connection -&gt; Query -&gt; IO [r] and `query_` is just sugar over the top of that. Typeclass-constrained functionality becomes very awkward when you want to access that functionality dynamically. The reflection library helps a bit, but it's mind-boggling and I don't think it even works in all circumstances. Type-classes are useful when someone uses your library in an application, but they severly inhibit someone reusing your library in another library and abstracting over functionality you provide.
Do you mean something like being able to define things from a superclass in an `instance` declaration of a subclass? So you could write stuff like: instance Monad T where fmap = ... (&lt;*&gt;) = ... pure = ... (&gt;&gt;=) = ... I like the sound of that - it'd give us a lot more flexibility to do these sorts of reshuffles in future without breaking backward-compatibility.
So basically if you want to combine two things of the same type and get another thing of the same type you want a monoid, but if you want to combine two things of (potentially) different types and get something of a type that is (potentially) different from both of the inputs you have a category?
That's fair!
it is not auto generated, i paste code in the link, and the error i copy from ubuntu console
If you don't know how to solve it, this is not the best place, you should go to Math StackExchange and ask there about how to solve it, or a numerical algorithm. And after, when you know it, and how it can be implemented, you can ask again, but in haskell questions instead.
Sure, and that's why I opened my above post the way I did. Typeclasses in the Haskell sense are rather artificial anyway, in that when you move into the dependently-typed world, they are completely subsumed and only exist as syntactic sugar. IMO, Agda handles this almost perfectly -- it has the `instance` keyword which lets you retain an "obvious" implicit value for some type, which, provided it's the only thing of that type in scope, will be automatically used by the compiler for implicit function parameters. This allows you to (mostly) retain the syntactic cleanness of Haskell's typeclasses when you write, but yet still retain the full power of the mathematical reality that something can be a member of a typeclass in more than one way. And of course, in that world you can actually state your assumptions (laws) in ways that the compiler can understand and enforce.
I'm honestly not sure what you're asking here. `n -&gt; n` is not the type signature of anything monadic. Polynomials are not monadic. You can't automatically derive a monad except for on newtypes, in which case it is trivial and you don't need a template function. ... That's all I can parse from your question.
Unfortunately, solutions have been proposed for at least as long as I've been using Haskell (and probably longer), but all have had major shortcomings.
Really? That's a shame. Don't suppose you know where I'd find any of these proposals? I can't immediately think of any problems with the idea I posited above, so I'd be interested to see what shortcomings people have already identified.
I used ~~Edward Kmett's~~ /u/glguy's script to do it: https://github.com/ekmett/lens/blob/master/scripts/hackage-docs.sh 
Has anyone really been far as decided to use even go want to do look more like?
I don't understand what he's trying to say. Compared to the standard null it 1) forces you to check for null and 2) allows just to express that something can't be null.
This is just Erik exaggerating to get a reaction from people. 
Or a play on words, `Maybe` solves the problem of `Nothing` ;)
Some links, with no particular screening for quality or ordering: [1](https://mail.haskell.org/pipermail/haskell/2005-October/016614.html), [2](https://ghc.haskell.org/trac/ghc/wiki/IntrinsicSuperclasses), [3](https://wiki.haskell.org/Superclass_defaults), [4](https://wiki.haskell.org/Superclass_defaults), [5](https://wiki.haskell.org/Class_system_extension_proposal#Extensible_superclasses), [6](https://mail.haskell.org/pipermail/haskell-cafe/2009-April/060638.html), [7](https://wiki.haskell.org/Class_alias).
or maybe he doesn't understand?
Also the aplicative/monad and monoid instances let you easily chain or combine maybes in a way that is a super pain for nulls.
Due to [his background](https://en.wikipedia.org/wiki/Erik_Meijer_\(computer_scientist\)), I would have expected him to understand.
[**@headinthebox**](https://twitter.com/headinthebox/) &gt; [2015-09-27 16:53 UTC](https://twitter.com/headinthebox/status/648178721417904128) &gt; @GabrielG439 Why, the solution is easy, introduce \*non nul\* types http://research.microsoft.com/en-us/um/cambridge/projects/comega/ or http://research.microsoft.com/pubs/67461/non-null.pdf, or ... ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
Usual "don't feed the troll" rules apply
Erik Meijer has always talk of using Lists instead of Maybes. And I see your side, if you program thinking of 0 or 1 elements its better to program it expecting 0 or n elements.
[**@headinthebox**](https://twitter.com/headinthebox/) &gt; [2015-09-27 17:50 UTC](https://twitter.com/headinthebox/status/648192891609726976) &gt; @pepper\_chico @jacobmcarthur Worse since it \*adds\* yet another notion of null, instead of removing it. ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
And it's certainly not the first time he's done that...
Oh. I must offer you my apologies then, because I was so certain that you were a bot that I reported all of your posts as spam. Mea culpa! The reason I thought you were a bot was because you had a brand new account and you posted many posts in a row, all containing the same link. I assumed the bot was trying to get me to click on that link, so I didn't. The fact that the link was accompanied by some hard-to-read but clearly Haskell-related stuff made me think that reddit spam bots were now trying harder to look like they were posting valid contents, probably by searching for the name of the sub and pasting random excerpts from google results. If you received complaints by a moderator about the fact that your posts were reported, please tell me so that I can contact the moderator in question and clear up the confusion.
Sounds good. I'll give it a try. Edit: also correctly credited you on the link.
You could make exactly the same argument for `Maybe` and `fromJust`. The problem is not with the type, but with using partial functions. Maybes and lists of length less than two are interconvertible through `listToMaybe` and `maybe [] (:[])`.
Blasphemy!
But it **is** related to Maybe. Sum types inevitably lead to partial functions because sometimes you **know** that a particular constructor will be used. For example if I know that a Map contains a specific key `k`, then I know that `Map.lookup k m` will return `Just v`. So using `fromJust` (or `Map.!`) in that case is perfectly fine. The problem is that Haskell's type system is not expressive enough to convey these invariants. A more powerful type system like LiquidHaskell handles them just fine, and lets me **prove** that uses of partial functions are safe. http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1443379136_3773.hs (LH also catches the partial `case`, but I commented it out because it actually renders the rest of `main` inaccessible, so LH no longer flags `fromJust Nothing` as unsafe.)
But you do have to explicitly say 'I know this is not `Nothing`' rather than it being implicit. This prevents you from saying it accidentally.
The compiler forces you to check for nulls in the sense that you can't pass a value of type `Maybe Int` to a function expecting an `Int`. Since that doesn't typecheck, you are forced to deal with the type mismatch, in some way or another. Here you are handling the `Nothing` case by telling the compiler that you know that the value won't be `Nothing` at runtime. If that turns out not to be true, who's to blame, you or the compiler?
Certainly, `Maybe` is infinitely better than `null`. I'm just arguing that we can do even better :)
I really enjoy Ryan Newton's talks. He articulates his key messages very well and enthusiastically, and is an important performance ninja that the Haskell community so greatly benefits from. 
wat
Thank you! I'm glad you liked it :D
Awsome, it works! Thank you so much! Just one question, when I hit `SPC m h t` (`haskell-process-do-type`) on some identifiers, the mini buffer says they are `not in scope`. But, if instead I hit `M-x haskell-mode-show-type-at`, I get the correct type. Are they getting there type information from different sources? What could be causing `SPC m h t` not to work? Thanks!
To do specifically what you're asking, you'll need a `newtype` and then a function something like: putTell toPut toTell = NewtypeWrapperCons (put toPut &gt;&gt; tell toTell)
Nitpick: avoid naming things `id`. It makes code confusing to read, and will likely cause some pain to users of your API down the road. 
I will note that `these` hasn't had any builds recorded in over a month. Eventually I uploaded the docs myself, but AFAIK it's still not getting built.
My dynobud library will do the automatic differentiation internally and call ipopt https://github.com/ghorn/dynobud
where did you find that out? Is there a repo somewhere, or does it make a note on the site?
&gt; The government seems to finally close down floppy distribution at the beginning of 2016, forcing any remaining doctors over to newer electronic patient journals. The future may be here at last. That was the happiest ending I could have hoped for.
I had huge trouble getting manually-uploaded docs to work. The only way I could make it happen with all the proper hyperlinking to external packages was using `stack haddock` IIRC.
&gt; undefined isn't null More accurate to say that undefined really really really shouldn't be used as a null replacement, even though you technically could. {-# LANGUAGE PackageImports #-} import "spoon" Control.Spoon import "base" Data.Maybe (isNothing) isUndefined = isNothing . spoon x1 :: Int x1 = undefined x2 :: Int x2 = 1 main = do print (isUndefined x1) print (isUndefined x2) prints True False 
I already had to start hiding `id` from Prelude... was planning a refactor anyway. Thanks for the tip, though.
Is `NewtypeWrapperCons` the same as `Game` in my example? I just tried playing with declaring `newtype`, but apparently some other types need to be updated because the `process` function is still inferred to run in the `RWS` monad.
How would you fix the code in this case? I've never touched LH, but I tried this: let x = Nothing :: Maybe Int when (isJust x) $ print (fromJust x) and it didn't seem to like it.
GADTs require some type-level equality (and hence coercions), whereas the role of equalities in TFs seems more like an implementation detail. Given a type family `F`, whenever we see "`F a b c ~ z`" why must we think of it as an equality on types (`_~_`), why can't it simply be a relation (`F _ _ _ ~ _`)? The relational perspective is exactly what fundeps do, so why not say the desugaring of GADTs in terms of fundeps is trivial? While surely any sort of injectivity gives rise to equalities, somehow we managed to avoid them when we were only looking at fundeps. How? Why are they so much more important to the TF story? Was it simply that noone realized how to implement GADTs with fundeps, is that all?
I think I need some more explanation... Wouldn't making `Command` a monad cause the `Game` monad to lose its meaning? After all, I only need one monad to encapsulate all possible game actions. Which one is it going to be? *Or*, are you suggesting I create two monads because there are really two APIs, the one declared for users (currently implemented via `Command`s), and the one I've requested in this reddit post (consisting of things like an atomic `put`-`tell` or context-aware `exists`, etc.)? So the `Game` monad might support the low level operations by composing `RWS` functions, while the `Command` monad supports the user level operations by composing `Game` functions? Then making `Command` a free monad makes a lot of sense, as various user level commands may have different kinds of output (for example).
What do you mean? Like readline style?
Your Game is just a type, but if you change it to a newtype and add a constructor (probably Game) to the right hand side then, yes, you would use that constructor here.
[readline](http://hackage.haskell.org/package/readline)? [ncurses](http://hackage.haskell.org/package/ncurses)?
I'm in the middle of a large refactor, but I think I've managed to incorporate your suggestions. There are a few spots I couldn't figure out how to make things more concise, however: simplifyMultFlatten :: Simplifier simplifyMultFlatten (Mult es) = Mult (foldr (\e acc -&gt; case e of Mult ees -&gt; ees ++ acc; ee -&gt; ee:acc) [] es) simplifyMultFlatten e = e And, a bit more complicated: simplifyMultAlike :: Simplifier simplifyMultAlike (Mult es) = Mult ( foldr (\ees acc -&gt; case ees of [eee] -&gt; eee:acc eee:eees -&gt; (Exp eee (Coeff (fromIntegral (L.length eees)+1))):acc; [] -&gt; acc) [] (L.group es) ) simplifyMultAlike e = e The first just flattens nested `Mult` lists. The second groups identical expressions and uses the length of each group to create a `Exp` with the corresponding `Coeff`, e.g. x*x =&gt; x^2. Is there a clever way to solve either of these more concisely? Edit: Also, I've been wondering about package hierarchy conventions. Is `Numeric.Algebra.Elementary` appropriate, or `Math.Algebra.Elementary`, or perhaps something involving "symbolic" since really this is a symbolic algebra library? Edit#2: Reformatted for ease of legibility. Also, WRT `Numeric` vs `Math`, it looks like there's no particular convention on hackage, though I did find [this](https://wiki.haskell.org/Hierarchical_module_names) on the haskell wiki.
`newtype` creates, as you might have guessed, a new type. If your `process` function is still calling the old operations, it will still be running in `RWS`. What you need to do instead is to create new operations which work in your new `Game` monad, by implementing them in terms of the `RWS` monad. newtype Game = MkGame { runGame :: RWS GameDimensions [GameOperation] GameModel GameMessage } deleteObject :: Id -&gt; Game () deleteObject objectId = MkGame $ do modify (delete objectId) tell [Delete objectId] If you want some `RWS` operations to be available in the `Game` monad as well, you'll have to create a wrapper for each of them. getModel :: Game GameModel getModel = MkGame get If you only call your new `Game` operations instead of the originals from `RWS`, the type of `process` will be `Game`, not `RWS`, as desired. For extra safety, you should declare all this in a separate module and export everything except the `MkGame` constructor. This way, the code which uses your API won't be able to create their own invariant-violating `RWS` operations by wrapping them in `MkGame`. -- MkGame is *not* listed module GameAPI (Game, runGame, deleteObject, getModel) where newtype Game = MkGame { runGame :: ... } deleteObject = ... getModel = ...
The implementation of type level equality ~, then we can desugar the GADTs like GHC does. data a == b = (a ~ b) =&gt; Refl As for the non-TF story, with rank-n types we can encode Leibnizian equality, and you can use that for most situations you'd use (~), it is just vastly more tedious. newtype Leibniz a b = Leibniz { runLeibniz :: forall p. p a -&gt; p b } Without rank-n types, with FunctionalDependencies + ExistentialQuantification we could write class Equals a b | a -&gt; b, b -&gt; a where leibniz :: p a -&gt; p b instance Equals a a where leibniz = id data Equal a b = Equals a b =&gt; Refl a b The main thing that TFs give us over FDs is the ability to _not_ talk about one of the arguments in a type. As for whether desugaring GADTs in terms of fundeps is viable, I don't know if it is or not. I don't know how to do appreciably better than the above. When I'm done, I'm still left with an equality that I have to substitute with by hand I think. I do know that I dont know how to extend simple type inference approaches like HMF with all of the TF machinery, yet can use them to implement Haskell 2010 today.
It's vaugely similar to what you've implemented, but its underlying calculus is one of discrete-time reactive string processing.
I can't understand how the example "qsort" is not actually insertion sort. It looks like each step is inserting the new item in the correct spot (O(n)) and then does that once per item (another O(n)), resulting in O(n\^2).
Thanks for that! LH looks really cool.
Ah, that makes sense. Thanks for the explanation!
Right, sorry about the misleading title. I wasn't aware it was a complete consensus yet, as I figured there'd be an implementation already if that were the case! The FFI question is a good one. I haven't used that part of Haskell much, but what you say makes sense - if, for example, it's an FFI call to get the current PID, that can probably happen in `RT` not `IO`. I definitely realise such a monad could be implemented as a library, and I plan to do that to learn about how `Free` works (I don't know, but it sounds like that would be the best approach). I guess the 'unfeasible' part of the proposal is expecting widespread adoption, but hey, that's just how software works. 'Something like `STM`' is exactly what I had in mind.
&gt; I've certainly found cases where tilde-equality wasn't strong enough to make things typecheck whereas unification-equality worked just fine. I've no doubt that implementing "equality" via its substitution effects, à la Leibniz, turns out to be quite different from actual equality (whether tilde-equality or unification-equality). The only situation I'm aware of where these are considered different are in the new `DeriveFoldable`. Any other situation I'd very much consider a bug!
&gt; `apply` can be anything that matches the type signature It can't, because I like to assume that people who have written the code I'm using are neither stupid nor malevolent. So, when I see `apply`, I can bet that it does something more sensible than returning the null vector. In fact, when a function – *any* function – violates my expectations drastically, in almost all cases it's a documentation bug and in most cases it's a bug in code. P.S. I am quite sympathetic towards the `Default` typeclass, too. I should write a rant about it one day.
I haven't considered it too much, but I think I'd be opposed to moving `&gt;&gt;` out of `Monad`. Frankly,. `*&gt;` and `&lt;*` are some of the ugliest operators I can think of, and currently, one can largely ignore them. But there are often huge efficiency gains in specializing `&gt;&gt;` for monadic code. While it may be possible to do the same by specializing `&lt;*` or something like that, this requires looking up for the millionth time which is which, and cringing as another tiny part of your emotional well-being is lost...
Oops, that was a misunderstanding. I don't find prudent for the mexican economy to pay a norwegian salary for free software... so I'll have to pass. But good luck as well!
[haskeline](https://hackage.haskell.org/package/haskeline)
Aww, here I was hoping for some Babbage/Lovelace action. This is cool too tho. ;)
Could this be dealt with in a typeclasses way like in mtl? Say, class Monad m =&gt; MonadRunTime m where -- ... class MonadRunTime m =&gt; MonadIO m where -- ... Another nice side-effect of this is that you could happily substitute another implementation of IO, such as if you wanted to mock IO while testing. I'm not sure on what else this could mean, though, especially since to replicate IO in a typeclass would require a ridiculous number of primitives.
Well, a runtime type would include actions like [getNumCapabilities](https://hackage.haskell.org/package/base-4.8.1.0/docs/GHC-Conc.html#v:getNumCapabilities), I imagine, as well as the concurrency stuff like `MVar`.
Good point. I guess the question is how often these 'runtime' actions are interleaved with honest-to-goodness IO. Probably depends on the application, right?
What a thorough answer. I think it covers all the questions that I could possibly come up, at the moment, about this topic. Thank you. Hopefully, anyone else who confuses the two things will come across this thread.
While we are on the subject of IO-related spitballing: why not add linear types to Haskell, make GHC's definition standard: type IO = State RealWorld ... then define `RealWorld` as a record of linear values that give you access to various parts of the real world and runtime?
To me, it seems there's a clear enough separation between 'actual IO' and 'stuff that got put in IO because' that it could be useful, but I guess we won't know until somebody tries!
&gt; (f x y z) &gt; Expands to/from: &gt; λλλ(((f x) y) z) Don't you mean `(((f x) y) z)`?
Instead of defining alternative-to-IO types, another option is of course to add various operations to various specialized typeclasses. Then, in functions where we don't want the full range of `IO` operations to be available, we just "forget" that we're dealing with `IO` by using a type parameter and specifying only the typeclasses we're explicitly allowing. The `IO` type can carry on being what it is now - no breaking changes - and it's also the obvious way to write one function that can work for both `IO` and `ST`, using a typeclass with a polymorphic `newRef` etc rather than having to choose between `newIORef` and `newSTRef`. 
Yes, thank you!
I think intuitively it is pretty clear what is actual I/O and what isn't. It would be anything that has the potential to either make the program depend on the state of the world around it (beyond resource usage concerns like running out of memory or being slowed down by other processes running on the same system) or influence it in some way. So e.g. file, database or network I/O and IPC are actual I/O while spawning threads, allocating memory, running an STM transaction,... are not. It might be tricky to separate this out into individual calls though.
&gt; In fact, I'd hesitate to even call a stack overflow a bug, if your code has the correct semantics. I'll let you try to explain that one to the end user. &gt; Ideally you'd use segmented stacks and not get an overflow unless you filled up your entire computer's memory. Maybe, but the next idealist will be saying it's not a bug to run out of memory or even address space - the machine should just offload the excess to disk, or over the internet, and what is it with these insanely restrictive finite-bit address spaces? And if we ever get to the point where the limits of our universe are the issue, someone will be claiming that's not a bug in our code either - all we need is a compiler that offloads the work to other universes. 
Thanks - interesting stuff. Looks like #2 is the most in-depth/up-to-date discussion on the matter.
Also "spawning threads" can already be done without IO (see `parMap`) when it actually doesn't involve side effects.
I've been writing a lot more "low level" code than perhaps some others lately, by which I mean things where I can't just use really generic types and functions to help ensure everything is right, as it really does just all come down to manipulating and traversing data structures in fairly complex ways. So most of my errors recently have been logic errors. I guess they fit into your first subcategory, but calling them a "misunderstanding" doesn't seem quite right: I have a consistent model of the solution in my head, but at a higher level of abstraction, and the mistake comes from bridging that last gap in the actual code. (of course, it's also a sign that the code should almost certainly be refactored to be at a more comfortable level of abstraction)
One error I recently made while refactoring is that I forgot `reverse` from `Data.List` in a chain of compositions. Generally functions that have signatures like `a -&gt; a` might be more easily forgotten since the type system can't catch it. But fortunately a test case did. 
normally I would go this way, but in this case university strikes. We have to use a Language like Haskell or ML for the whole editor :D
I don't understand how cross-compiling works, but I've been using TH on an ARM based chromebook for several weeks (that means development and running the final binary). Maybe using a dedicated ARM based machine to compile for android can solve the TH problem?
Absolutely! This happened to be at a point in time just before I switched to stack. Now I have and there's no going back. I've even been [committing a bit to stack](https://github.com/commercialhaskell/stack/commits?author=rrnewton). Also, I added your stack.yaml to [the repo here](https://github.com/iu-parfunc/haskell_dsl_tour/commit/f75a7e492a1e5d219a77fb128f70441d54a706eb). 
Which state? STM goes to some lengths to specifically not allow IO actions to occur during the transaction, so that it doesn't have to deal with any notions of the 'state of the world' - it can live in its own STM bubble and happily rollback or retry as much as it likes.
Exactly. But my timing is perfectly off: it appears that Hackage just enabled 7.10.2, which solves my problem (it the change is permanent) and probably also explains why nothing (not even the scripts) were working for me.
It is a partial solution in some contexts. Having "notnull" constraints seems better is what I think his point is. If a function takes notnull types and you pass it null you get a diagnostic message. If you use Maybe you get silence and a different runtime error.
Check out Curry, and check out delimited continuations in Scheme and OCaml.
[Cont](https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-Cont.html)
I think that diagrams is certainly a useful package, but some of the typeclass usage makes the library for me, as a user, harder to understand. Signatures like this diameter :: (V a ~ v, N a ~ n, Enveloped a) =&gt; v n -&gt; a -&gt; n are tough to understand. It seems like it was important to the authors to be polymorphic in the dimensionality of the vector space and in the type of number, so I guess this was the only way to make that happen. But it seems like most actual uses of the library involvement a 2d vector space and the numeric type `Double`. There are also a lot of convenience typeclasses like: class (Floating s, Ord s) =&gt; OrderedField s and class Juxtaposable a where juxtapose :: Vn a -&gt; a -&gt; a -&gt; a And these aren't really what I would consider "good" typeclasses. Fortunately, the library is very well documented though.
Ruby does have call-cc, I believe. Otherwise, as far as I know it's only found in lisp and ML dialects. (And unlambda, but let's not go there.)
So, would a language with linear types and algebraic data types support defining data using both kinds of sums and both kinds of products? :]
&gt; I think system threads should probably count as externally observable, but not runtime threads. Which is exactly why you don't parallelize your code by having it run in IO. But these runtime threads have the power of doing IO, that's why forkIO or async run in IO: because they launch something that should have the capacity to do IO. 
My point was not that "oh haskell also has null", but that `Maybe`, in the face of non-values, adds more non-values than only `Nothing`, Here's scala, which *certainly* has `null` val x : Integer = null // as in, java.lang.Integer val x0 : Option[Integer] = null val x1 : Option[Integer] = Some(null) // not Option(null)! val x2 : Option[Integer] = None the haskell in the parent is only since this is /haskell/ and I thought it would be more comprehensible.
&gt; Otherwise, as far as I know it's only found in lisp and ML dialects. (And unlambda, but let's not go there.) Lua's coroutines are oneshot continuations. Generators are limited delimited continuations, which JavaScript, C# and Python now have.
Hm. The `~` stuff in the type of `diameter` is a pretty common pattern and probably even makes the function easier to use thanks to better type inference, but it does start to look ugly pretty quickly. Are you talking about looking at haddocks or at types in GHCi or whatever? If the former, would using constraint synonyms with a more descriptive name and haddocks explaining what it does be helpful, or just even more complexity?
It would be a fun experiment to write a library that tracked "list reversals" with a phantom type or similar trick. Does something like that already exist?
Right, but those are continuations restricted to specific use cases that aren't headache-inducing. Feels like there's a very big gap between those and having `call-cc` available to use at will.
It's sort of obvious, but the primary one is "only". In that other languages might have `IO + Except + Cont`, but Haskell has `Cont`, `Cont + Except`, `Except + Cont`, `IO + Except + Cont`, and any other interesting permutations I'm ignoring.
call/cc is a braindead given what we now know. Delimited continuations are much simpler, and if you [generalize generators you obtain delimited continuations](http://lambda-the-ultimate.org/node/4349) in a way that's not headache-inducing.
I think this is a subreddit simulator bot posting in the wrong place.
Huch? It's not about the Plankalkül?
so in that setup you have some default.nix file in . declaring your package dependencies don't you? how would you declare you depend on say all of lts-3.7 ? having some workflow on how you use stackage + nix would be interesting
I think `teaspoon` would be more appropriate there. `spoon` has an `NFData` constraint and effectively turns a data structure containing any "nulls" anywhere into just "null", whereas `teaspoon` gives you something that looks more like null references.
I'm certain he understands it all just fine. Think of his tweets less as a lecture, and more as... uh, performance art.
You say that Caramel is **bi-directional**. But how would you recover the `qsort_example` code from the `qsort_example.lam` expanded form? Do you inject comments or annotations or similar to help recover names like 'qsort'? (*Edit:* Could you add this as a command-line option?) A related, similar project is Paul Chiusano's [Unison](http://unisonweb.org/posts/). Unison targets lambda calculus, albeit a variation with extra syntax for binding/linking via secure hashes. David Barbour's [claw code](https://awelonblue.wordpress.com/2015/06/18/command-language-for-awelon/) also seems similar, albeit targets a purely functional concatenative bytecode instead of lambda calculus (resulting in a more Forth-like syntactic sugar).
I have a doubt about this recent method for detecting space leaks: http://neilmitchell.blogspot.com.es/2015/09/detecting-space-leaks.html Since it is based on using small stack sizes, it seems to me that it won't catch huge thunk buildups that never get evaluated and just exhaust the heap (in Haskell, [stack frames come from nesting thunks](http://stackoverflow.com/questions/13782222/haskell-recursion-and-memory-usage#comment18959991_13783864)). Is my impression correct?
&gt; What's a totality checker? Just like we have a type checker to make sure that our program obeys some set of rules governing types (a type system) a totality checker is a program that checks whether the input terminates. However, this turns out to be a hard problem given that the halting problem is undecidable. Instead what happens is the totality checker will check the program obeys some restriction it knows ensures termination. In Coq for example this requires checking that each recursive call in a function is on an argument which is known to be smaller than the input and all matches are exhaustive among other conditions. This is quite important for proof assistants where unrestricted nontermination renders proofs useless. It would also be nice to have in other languages because a program which loops forever and does nothing is not a program I wish to write.
How does `⊥ :: a` relate to the idea that "if it compiles, it (probably) works"? Does it not present a practical concern because in reality people aren't throwing `undefined` everywhere? Related, what goes into the "(probably)"? My idea was that it could be mixing up functions with the same type signature (like accidentally passing `reverse` instead of `id`) but is there more?
`default.nix` is a bog-standard `mkDerivation`, the kind of thing you get from cabal2nix. I didn't yet try it, but presumably it's a matter of a) getting hold of a nixpkgs, one way or the other, that contains the new lts stuff and then use the lts' `callPackage`.
Indeed, although for heap space leaks, there are many different profiling capabilities available. I'll have to defer which exactly to someone else, though :)
What's a good alternative to a "Types.hs" module in every project? It encourages me to introduce orphan instances a lot of the time.
In Haskell, tuples are fixed-size (i.e., a 4-tuple and a 5-tuple are not type-compatible), and heterogenous (i.e., elements can be of different types, although for two tuple types to be the same, all the element types must match). Lists, by contrast, are variable-length and homogenous, i.e., all the elements must be of the same type. Both are immutable. In Python, lists and tuples are the same thing, except that lists are mutable and tuples aren't, plus tuples support unpacking syntax. WRT isomorphism; I believe this is just a cringeworthy attempt to use a fancy math sounding word without knowing or understanding its established meaning. "iso", "same", plus "morphic", "shaped": yeah that could totally mean "use the same language for both". Except of course that it can't, because it's the wrong kind of same, and the wrong kind of shape.
Do not put much weight into the phrase "if it compiles, it (probably) works". You hear this phrase because people are often surprised by the fact that after they fight through some compiler errors their program runs as expected the first try. This is in contrast to some other type systems where the type system catches fewer issues and so it is common to run the program, get a runtime error, and then have to head back to the code. While it is great that people often feel this way, the type system in Haskell (and GHC) can only catch *certain sorts* of errors, and just as you pointed out, there is still the possibility of have runtime errors because of partial functions or the introduction of `error` or `undefined`. This phrase should never be thought of as any sort of guarantee, instead it should be thought of as a short way of saying "I'm often surprised by how many bugs the typechecker catches for me, I can focus on the logic of my program and not worry that I've done something silly like call `head` on an `Int`". 
Bottom is most certainly a concern in Haskell. But, as you say, people try to avoid partial functions and throwing error. Prevalence of error or pattern match failure is nowhere near as bad as NullPointerException in Java, say. Try to stick to total functions. Regarding the probably, there was a very recent thread here about that.
Learn You A Haskell is excellent if you have experience with imperative programming. Once you get to monads, I like these for extra reading: * http://blog.jle.im/entry/inside-my-world-ode-to-functor-and-monad * http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html * https://wiki.haskell.org/Monads_as_computation
What's the current state of the tooling story? Anyone using a Haskell IDE can comment? 
Put the types in separate Data.MyType modules. That's the cleanest way. I usually include all instance declarations in Types.hs or those Data. type modules.
Probably [resourcet](http://hackage.haskell.org/package/resourcet) is the most widely used (after `bracket`). Other approach is based on [dynamic regions](http://hackage.haskell.org/package/io-region), where region is a first class thing.
I may be atypical, but I just kept reading /r/Haskell until stuff started to make sense, and then I started writing short programs. 
The Haskell plugin for IntelliJ is pretty good, though slow-ish. I like the haskell-ide for atom. It provides scope based autocompletion, type signatures on hover, on-save-typecheck and inline error reporting. It's quite a nice tool and relatively small. It works with cabal and non-cabal projects, so it'll tell you if you forgot dependencies for example.
sigfpe's blog is always a good recommendation! Have an upvote :)
That's the point.
&gt; What's a totality checker? Half of a halting oracle. More seriously, it's a part of (usually) the type system that tries to prove that a function handles all its input and always returns real output, no infinite loops or `undefined`s, and if it can't, it screams loudly. In the same way that a type system often restricts you to working with only certain programs that make sense, disallowing others that wouldn't crash but it can't reason about (e.g. `f x = case x of { 0 -&gt; ""; _ -&gt; x }`, which is a bad idea but definitely not a run-time crash), a totality checker might restrict you in a similar way (others have mentioned structural recursion as an example).
Thanks! I didn't mean to offend - I figured that was the case. Also, I've got you beat, *[this](https://github.com/commercialhaskell/stack/commits?author=mitchellwrosen)* is committing "a bit" to stack ;)
I am an idiot ... sorry.
If you want from zero (not even assuming other programming experience), the Haskell Wikibook is superb. (It's in the sidebar). And while great already, you can *improve it while you go!*
https://github.com/bitemyapp/learnhaskell
Another way to look at it is that `⊥` is equivalent to writing a program that enters an infinite loop. In fact, writing an infinite loop is the easiest way to define `⊥` undefined :: a undefined = undefined
Are there jobs in Haskell outside finances and academia? Have been curious about FP for a long time, and Haskell looks very interesting - but it seems that most jobs there are in these two areas. Finances are, well, a very special area let's say, and academia has its own quirks as well; while I have lots of respect for researchers, I'm more of an engineer myself. So, are there (m)any non-financial technical companies using Haskell?
Most of us use command line tools from a terminal or an editor (typically Vim or Emacs). For me the unix environment is my IDE. It took a while to master the toolset, but now I prefer it over IDEs. I realize it's not for everyone, but it can be quite nice. As for more traditional IDEs, at various points in time all of these have been options, but may not currently be viable. I don't have personal experience with them, so maybe someone else can address that part of your question: * [ghc-mod:](https://hackage.haskell.org/package/ghc-mod) turns Vim/Emacs into more of a Haskell IDE. * [eclipse-fp:](http://eclipsefp.github.io/) Eclipse plugin to make eclipse better for Haskell dev. * [leksah:](http://leksah.org/) An IDE in Haskell for Haskell * [Visual Haskell:](http://research.microsoft.com/apps/pubs/default.aspx?id=67496) A third party plugin for Visual Studio for writing Haskell. * [FP Complete's IDE:](https://www.fpcomplete.com/business/haskell-center/overview/) Web based IDE for Haskell. If you're not interested in using the unix environment and you really want an IDE for a functional language, then I would recommend looking at F#. You can write F# with the mono environment or with Visual Studio.
&gt;For me the unix environment is my IDE. It took a while to master the toolset, but now I prefer it over IDEs. I realize it's not for everyone, but it can be quite nice. My issue with that is the high setup cost - getting a setup you like can take a while, and I'm juggling 5-6 languages so I need something that works well out of the box.
Definitely good for a start. Now it would be interesting to automatically map access between Haskell ADTs (e.g `Map String (Set (Map Int Char))`) and Rust's ADTs for extending the inter-op. I can't imagine anyone doing hand-writing data type conversions. Or, maybe come up with a language-neutral ADT specification that could bridge between those two?
How would one go about implementing a totality checker?
I have had problems with libraries that either contain non-total functions or ones that call error from pure code. Now when tacking on a dependency I actually spend time to check uses of error and other error producing functions like fromJust.
crazy. so who / what group is responsible for hackage? is there a ticket somewhere about migrating its GHC? 
But that's just restricting what you can do. It doesn't provide additional functionality.
For what it's worth: some people criticize the Gentle Intro for not being gentle, and for being unsuitable if you have no previous functional programming or PL-theory experience. All I can say is that wasn't the case for me. I read the Gentle Intro only knowing some C, C++, and the basics of Python. That, together with [Composing Contracts](http://www.cs.tufts.edu/~nr/cs257/archive/simon-peyton-jones/contracts.pdf), made such a strong impression on me that I went on to do a PhD in programming language theory. The Gentle Intro was _literally_ a life-changing read for me.
If I have something like: data Thing = Blah | BlahTwo | BlahThree | BlahFour | --many such Blahs that take no arguments BlahStr String | BlahInt Integer | -- a few that do take arguments Is there a way to do instance Show Thing where show (BlahStr x) = show x show (BlahInt x) = show x but have it fall back on the behavior provided by "deriving Show" for the type constructors that take no arguments? 
Not directly, but I'm pretty sure you could rig something up using either Generic (or one of the other generic programming frameworks) or TH; neither would be pretty. You could also hack around it with a newtype: data Thing_ = Blah | BlahTwo | ... | BlahStr String | BlahInt Integer deriving (Show) newtype Thing = Thing Thing_ instance Show Thing where show (Thing (BlahStr s)) = show s show (Thing (BlahInt i)) = show i show (Thing x) = show x
Let's say I have a function like this \x -&gt; E where `E` is a stand-in for some expression. Within `E`, I can say that I *have* a value `x`. It exists. Now let's go to bizarro world and write this in made-up syntax E &lt;- x/ which is a bizarro lambda which is supposed to indicate the idea that within the expression `E` I *owe* an `x`. I have to somehow generation some satisfactory `x` and give it up. I have a "negative" `x`, perhaps, or an `x`-shaped hole. The word for this second situation is often that I have a *continuation on `x`* in `E` in the same way that I had a `x` *directly* in the first example. Let's simulate this. One good way to "have a continuation" is to have a function which consumes a value. That might look a bit like \f -&gt; E where `f` has a type like `X -&gt; A` and `E` has a type like `R` (\f -&gt; E) :: (X -&gt; A) -&gt; R in practice this doesn't work so well, though. Since we potentially *know* what the type `A` is we can do more than merely continue on `x` like `f x` since we could examine the result and compute with values of type `A`. We also ultimately only need to return a value of `R`, so we could avoid continuing on `x` entirely. So let's rejigger it a bit (\f -&gt; E) :: forall r . (X -&gt; r) -&gt; r now we do not know anything we can do with the return of `f` *except* return it completely. In other words, we've been forced to truly *continue* with `x` once we pass it to `f`. This is very similar to `Cont` except that we don't bind the `r` with a `forall` but instead leave it free Cont r a ~ (a -&gt; r) -&gt; r this lets us talk about a larger space of `Cont`-shaped things while restricting what we know about each one. For instance, `(forall r . (a -&gt; r) -&gt; r)` is isomorphic to `a`, but `Cont r a` is not unless whoever wants to convert `Cont r a -&gt; a` has the right to constrain `r` right then.
The Haskell community seems to have a lot of these "it's been known for a while that XY is bad" yet I have no idea where a person is supposed to find these things out. For example, I haven't heard of delimited continuations until now, but I have heard about call/cc many times before.
Hmm. If you split Thing into data Thing = Thing0 Thing0 | Thing1 Thing1 data Thing0 = Blah | BlahTwo | ... data Thing1 = BlahStr String | BlahInt Integer | .. instance Show Thing where show (Thing0 t) = show t show (Thing1 t) = show t would do what you want. Probobly not worth the extra noise. The other option would be to define a: showThing (BlahStr s) = show s showThing (BlahInt i) = show i showThing t = show t and just make sure to use showThing instead of show, but that has issues too.
Lambda calculus is older.
Under exactly what circumstances are computed values memoized, and what is the scope/lifetime of these memoizations?
&gt; The Gentle Intro was literally a life-changing read for me. Me too, but I could only get through it together with the Haskell Report. Those two were about as far from Gentle as you can get. But the intense pain they caused was worth it.
Thanks! Checked your website a bit earlier. I see you're located in Portland - do you do/consider remote? ...hell, are you doing embedded? I started out with development for Atmel MCUs early in my career, C and ASM, tried to find connections to FP back then, the first article I found said "Embedded and functional programming are, of course, the opposite ends of the spectrum" or the like :)
[Monads as computation](https://wiki.haskell.org/Monads_as_computation) must be read in parellel with its companion, [Monads as containers](https://wiki.haskell.org/Monads_as_containers). Neither of those are what monads **are**. But when I read both of those together - plus the basic definition of course - a light bulb lit up in my brain. 
"exactly", I couldn't tell you, and probably the best you could do is to read the GHC code. More or less, however, when you bind a value to a variable, and then use the variable, that's when memoization occurs, and the lifetime is the lifetime of the parent value (e.g. a binding in the scope of a function lives as long as the function).
Is there a name for the kind of `Profunctor` that's like data P f g a b = P (f a) (g b) and the Profunctor instance is instance (Contravariant f, Functor g) =&gt; Profunctor (P f g) where dimap f g (P fa gb) = P (contramap f fa) (fmap g gb) I know it's not `Strong` or `Choice`, but it supports `Strong`-like and `Choice`-like operations: (***) :: (Divisible f, Applicative g) =&gt; P f g a b -&gt; P f g a' b' -&gt; P f g (a, a') (b, b') (P fa gb) *** (P fa' gb') = P (divide id fa fa') (liftA2 (,) gb gb') (+++) :: (Decidable f, Alternative g) =&gt; P f g a b -&gt; P f g a' b' -&gt; P f g (Either a a') (Either b b') (P fa gb) +++ (P fa' gb') = P (choose id fa fa') ((Left &lt;$&gt; gb) &lt;|&gt; (Right &lt;$&gt; gb')) I know there should be a wider typeclass for these operations, because they're supported by `Arrow` and `ArrowChoice` respectively, but `P` isn't an `Arrow`, because it has neither `id` nor `.`.
A couple of questions I asked parent, too: Just to clarify - are these contracts specifically for Haskell, or are you hired to solve a certain problem and choose to use Haskell yourselves, without requirements/discussions with clients? And, if you don't mind, what problem area are you active in? Glad to hear you enjoy it, and that options exist :)
Sounds extremely interesting! Also heard a lot of good things about FP-based DSLs before. I'm not American though, so that's a bit of a hurdle :) Thanks anyway, it's all very interesting stuff.
Hackage now builds through 7.10.2.
As for completion, I've been using `company-ghc` with `ghc-mod` to get some nice on-the-fly code completion. I'm still very new to it as well, however, and would love some pointers from anyone. For example, I still haven't successfully gotten a REPL session running, so I'd be interested in how you managed to get that working!
Generally, icu4c requires you to have its DLLs present to run applications that use it. In any case, the text-icu library is designed to work that way. It might be possible to create a Haskell application that links statically to icu4c and does not require the DLLs at runtime, but it sounds to me like that would be a significant research project.
Restriction and functionality are always tangoing. What restricts someone who is *constructing* certain values gives power to someone *examining* or *using* those very same values.
Is it possible to make 'ghc --make' use less memory? I was trying to compile a Hakyll site on a 768 MB VPS and I was getting OOM killed. I suspect there isn't but I thought I'd ask. I tried with '-O0' flag and it didn't help me.
&gt; The key difference is the presence of colons - with colons, it is a tuple, without them, it is an application. This should probably be "comma", not "colon".
This is true, but the main issue is the learning curve, not so much setup. And even something more similar to a traditional IDE will require much more of a learning curve for Haskell. It's not like moving from one imperative OO language to another in the same IDE, where you can basically just start working without blinking an eye.
No theoretical insights here, but recently I ended up setting up the state first, then used it to compute values to go in the reader monad, then running the writer monad several times inside that.
I don't know, I have spent weeks, almost months, at a time just coding and relying only on the compiler as a sanity check and it never took more than a few hours after that to actually get it running. That's not too bad actually.
[This might be interesting](https://www.fpcomplete.com/blog/2015/08/new-in-depth-guide-stack)
Heard bits here and there, glad to have some confirmation. Very relevant since I'm doing webdev (Py/JS) at the moment - thanks!
Are there any plans to introduce an ML-like module system? Also, when is Backpack coming out and is the specification finished?
For the contract work (which is mostly an on-the-side thing I've been doing with one client over the years), I am hired to solve specific problems, and I use haskell. The client doesn't really care what I use. For this client, I have built a promotional web application platform (where the users participate in play-to-win programs or sweepstakes draws) and a community-focused deals engine (where the clients will mostly be restaurants). You should probably be prepared to continue providing support for something for a while if you're going to do it in haskell, since there are so few devs that know it. I used to feel uneasy about using haskell for contract work, but I realized over time that people get burned by contracting developers all the time. I've seen more than my fair share of garbage apps that previous developers did in PHP or Ruby or Java. So, I feel like if you're building something well and aren't going to abandon the client, you're already offering more of a service to them than a lot of devs do.
As the other comment suggest, fork it on github and then point stack at your fork instead.
Sure, call/cc has been around for 40+ years IIRC, delimited continuations only began in the early 90s and were only properly formalized in the late 90s I believe. But call/cc has been known to be flawed for longer, in that it yields memory leaks and isn't sufficiently general to express certain types of exceptions IIRC. People hear about this sort of progress on Haskell mailing lists, or [LtU](http://lambda-the-ultimate.org/), or here on reddit nowadays (on /r/haskell for instance).
On the index page of hackage there is a "Reporting Problems" heading. Under that, there is text that reads: "For bugs with the site code or server/hosting issues, please report them in our issue tracker." The issue tracker in turn points here: https://github.com/haskell/hackage-server/issues In turn there are conversations there about this upgrade.
&gt; Why? Because, as is, the article is mostly fluff, with very little substance. I think completing the proof would provide additional substance. 
This is a lie, but a good rule of thumb is: a value is memoized when it is referred to by a variable with a monomorphic type, and accessed through that variable. It's a lie because GHC will fairly often make a different choice than this. It's a good rule of thumb because GHC will try to only make a different choice when it can't be much worse than that rule. In particular, it sometimes surprises people that `let` is not just syntactic sugar in Haskell, because a `let` can cause a result to be memoized. This is also related to the monomorphism restriction, which was created to try to make sure that top-level definitions that don't look like functions are memoized instead of recalculated at each use.
You don't need to set up a development environment for "screwing around with Haskell". Just use a standard text editor on your favorite platform (e.g., Notepad++ is fine on Windows), and spend a few minutes learning how to use the `cabal` or `stack` command, and GHCi. It's worth investing more time than that once you see that you are spending a significant part of your days on Haskell work, or when you start doing production-quality commercial development in Haskell.
I guess I'm spoiled, but these days I don't write any code if I don't have the equivalent of hoogle + red squigglies.
The only good trick I've found is to make sure GHC isn't splitting object files. To enable this you need to rebuild GHC and set the option at build time. I got this working once on a VPS with 1GB of memory, but I don't recall ever getting it working on less. I seem to recall 512MB is not enough. Not sure about 768MB. I recommend upgrading to the 1-2GB range. It's less hassle and enough ram for most builds. If that's not possible, then you might consider having a local VM (or computer) with the same OS / version as the VPS. Build locally and then transfer the binaries to run on the VPS.
Edward Yang would know, but I can't figure out his reddit username to page him. I looked at his blog and this was the most recent article about backpack: http://blog.ezyang.com/2014/08/a-taste-of-cabalized-backpack/
You can do broken functors that typecheck. For example : data Ook a = Ook Int a instance Functor Ook where fmap f (Ook i a) = Ook (i+1) f a doesn't satisfy functor laws. 
&gt; but I'd like to know how experienced haskellers go about this kind of situation. Some grow sufficiently frustrated to [post their own forks on Hackage](https://hackage.haskell.org/package/regex-tdfa-rc).
Why does this encourage you to create orphan instances? I usually have a `Types` module, or more commonly, several `Types.Foo`, `Types.Bar` modules. These include the types themselves, some simple functions (accessors, `isFoo`, etc) and instances.
No, I haven't met with a client that was asking for Haskell contracting, but I did meet with some nice folks who were looking for a full-time person. These are contracts where the client hasn't specified the language. I always have to explain my choice, but Haskell is an easy sell unless they've been talking to some guy at the golf course about ruby or something. These are projects with clear deliverables and timelines, and they aren't planning on building a team later. I just got off the phone with one client who wanted to use node.JS (I'm good at node too), and I attempted to convince him to use Haskell. I think he walked away convinced, but we'll see. He does want to build a team for version 2 or 3, but I'm personally convinced (rationalization?) that Haskell is a good choice for inheriting a codebase and building a good team after a skilled contractor has built the first version. I have had 2 projects in elm though, where the client was interested in it prior to talking to me. I'm active in mobile (iOS, where I always use swift), front-end web (using react or elm these days), and web services (using haskell + servant). 
Front-end web dev? What framework and/or compiler are you using? 
It's back-end and frond-end. I use yesod + persistent + esqueleto, but I've played with several other technologies. I used scotty once but I didn't like it. I used hakyll recently, and it's awesome. I've used GHCJS + reflex-dom for one front-end tool, and it's very cool, but the tooling is still very immature there.
Maybe? paging /u/ezyang? Can you answer this question about backpack?
I took the liberty to fill in issue reports in haskell-mode project: https://github.com/haskell/haskell-mode/issues/895 https://github.com/haskell/haskell-mode/issues/896
&gt; The practical way to fix the DLL popups is to copy the required DLLs into the same folder as the executable. In order to make my program to run, I was already doing that. However, as you have already posed, it isn't a great solution, not the one I was looking for. The point I want to understand is: 1. Am I using correct the `ghc-options` in both my `disaccent.cabal` and my `stack.yaml`? 2. Is it possible to statically link my program to the `icu4c` library?
MFlow is a web framework that has a Web navigation monad and a interactive form monad: https://themonadreader.files.wordpress.com/2014/04/mflow.pdf hplayground is a client-side framework that has a widget monad that handle events: http://github.com/agocorona/hplayground Workflow is a monad transformer that gives thread state persistence, so it can restart a monadic process at the point where it was interrupted: http://github.com/agocorona/workflow transient is an extensible effects monad that implement reactive, backtracking, logging, indeterminism, concurrency, streaming, parallelism and distributed computing among others and all the effects can be mixed. http://github.com/agocorona/transient transient uses continuations _within_ a monad, not over a monad, like Cont. So it has the power of delimited continuations without the akward syntax, And the effects are packed in concrete and intuitive primitives, instead of the unrestricted wilderness of Cont. EDIT: by the way, transient is a monad that can save the word as we know it, since it can undo launchMissiles: launchMissiles `onUndo` destroyMissiles before impact ;)
Here's another bug that I came across today: if you define a `val` in the wrong place in a Scala class it will get initialized at the wrong time by the Scala compiler, causing it to be `null` when you least expect it. This is often resolved by changing the `val` to a `def` instead. In Haskell you don't have these sorts of issues because: * You don't have `null` * You can't have "partially initialized" values/records * The order in which you evaluate or declare values does not matter (modulo making sure things are in scope) Everything in Haskell is sort of timeless and immutable, always present, always defined, so having a bug like this in Haskell wouldn't even make sense. I can't even articulate what the Haskell analog of this Scala bug would even be.
I agree it's unpleasant. But this is the way it works for most programs using ICU, not just Haskell programs. In practice, it usually isn't a problem. For Linux, you can assume that ICU shared libs exist without installing them. On Windows and Mac OS X, people expect to use an installer, and the installer can supply the required DLLs/dylibs.
It might help you with your decision if you understand that the "monad transformers" are nothing more than glorified newtypes with cute instances. Unwrapping the newtypes will give you insight onto what's actually happening. In particular: type ReaderT r m a = r -&gt; m a type StateT s m a = s -&gt; m (a, s) type WriterT w m a = m (a, w) And for the "non-transforming" versions, type Reader r a = r -&gt; a type State s a = s -&gt; (a, s) type Writer w a = (a, w) So we can see `VM1` for what it really is: type VM1 a = ReaderT r (WriterT w (State s)) a type VM1 a = r -&gt; WriterT w (State s) a type VM1 a = r -&gt; State s (a, w) type VM1 a = r -&gt; s -&gt; ((a, w), s)) And looking at `VM2`: type VM2 a = WriterT w (ReaderT r (State s)) a type VM2 a = ReaderT r (State s) (a, w) type VM2 a = r -&gt; State s (a, w) type VM2 a = r -&gt; s -&gt; ((a, w), s) so it looks like, for this case, they really are the same thing, so it doesn't matter. Because newtype wrappers exist only at compile time, both of your stacks actually have the same runtime representation, and are really the same thing to the Haskell runtime. They both: * Take 2 parameters, an environment and a starting state * Return a result, a "logged data" field, and a modified state. Generally "unrolling" the wrappers helps give you a good idea of what the transformers really "mean". type VM3 a = StateT s (ReaderT r (Writer w)) a type VM3 a = s -&gt; ReaderT r (Writer w) (a, s) type VM3 a = s -&gt; r -&gt; Writer w (a, s) type VM3 a = s -&gt; r -&gt; ((a, s), w) In this case, it seems like the actual thing is more or less the same...just the order of inputs and outputs are swapped a bit. Consider also: type MaybeT m a = m (Maybe a) type ExceptT e m a = m (Either e a) MaybeT (State s) a s -&gt; (Maybe a, s) StateT s Maybe a s -&gt; Maybe (a, s) The first is a computation that takes a state and returns a new state with a result that could have been a success or a failure. The second is a computation that takes an initial state and could possibly fail and return nothing at all (no new state or result), or return a success with a new state. 
Isn't this captured by GOTO in imperative languages?
Well, one popular rule is to check that all functions that operate on a (finite) recursive data type "descend" down the recursive part of the type. For example, a function like this is guaranteed to terminate for any finite list: last :: [a] -&gt; Maybe a last [] = Nothing last (a:[]) = Just a last (_:as) = last as But suppose we messed up and wrote it like this instead: last :: [a] -&gt; Maybe a last [] = Nothing last (a:[]) = Just a last as = last as -- WRONG!!! A popular style of termination checker would allow the first definition but not the second one. It would spot the bug by noticing that: 1. `last` operates on a recursive data type; 2. The third equation in the first definition recurses down a *subpart* of the list, but in the second definition it recurses on the whole input value. This "recursive descent" idea can be stated more carefully to get you a rule that guarantees that all functions you can write are terminating, assuming that the input is finite. It gets a bit more complicated when the input can be infinite. The way several languages work is by: * Requiring all definitions of recursive types to specify whether the values can be infinite or not (often called "data" vs. "codata"); * Using the aforementioned rule for the guaranteed-finite types; * Using a different rule (sometimes called "guarded recursion") for potentially-infinite types); * Specifying the interaction between these rules carefully. But the basic idea of guarded recursion goes like this. Take the `map` function: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map _ [] = [] map f (a:as) = f a : map f as Here the key is that in the second equation, the recursion on sublist `as` is being used inside of the `:` list constructor. It's a bit elaborate, but from this it follows that no matter how long the input list is, the `map` function always "makes progress"—with lazy evaluation you can *always* compute the value of the next element of the list, and the computation never gets stuck in a loop.
The parsers: Parsec and so on are also a good example of what monads can do when used in a language that does not define a particular way for sequencing statements (i.e define algorithms). Conal FRP defines a monad, but is inherently leaky since FRP tries to store all the history of events and label it as "first class time varying values" called behaviours. That is what I call first class space leaks. There are monad transformers for writing to a file, for reading a file, for logging, for streaming. If you try to use all of them you will end up with a stack of 10 or 12 monad transformers that is unmanageable. The limit for monadic constructions is the imagination. But the imagination is quite poor nowadays and many people use monads for trivial things that can be done better and faster using the IO monad and no monad stack at all. 
Also, you say &gt; oldest programming language in the world Lambda Calculus is not that. Early forms of the Combinator Calculus were defined by Schoenfinkel starting in 1914. I'm not sure at what point it became Turing complete, but I'm pretty sure that happened before Curry picked up Schoenfinkel's work in the mid 1920's. Going further back, the punched-card programming language that powered Babbage's proposed Analytical Engine has been shown to be Turing complete. The first design was completed in 1837. A number of mathematical papers were published during that period with algorithms written in that language. EDIT: Schoenfinkel *defined* the Combinator Calculus. He didn't *implement* it. There wasn't any hardware on which to implement it at that time. :)
I'm also just getting started with spacemacs. Is your configuration publicly available? I'd love to see how others have it set up. I also haven't yet figured out how to do things like the comma bindings you mention. I think that ideally I'd want to have them bound to some kind of `SPC` key combination, but comma would be better than nothing to start with. I've also had trouble with massive slowness every time I save a Haskell file when using the spacemacs haskell mode. I haven't had time to investigate more, but this issue makes spacemacs haskell mode pretty much unusable for me.
&gt; Or, maybe come up with a language-neutral ADT specification that could bridge between those two? That seems like the most expensive solution. Perhaps one step below would be to provide one language with a library for access to the others' basic types. My guess is that it would be easier to adapt Rust to Haskell's types, rather than vice-versa.
&gt;STM goes to some lengths to specifically not allow IO actions to occur during the transaction Does it? As far as I understand, STM specifically disallows nested transactions, and that's about it. It doesn't disallow general IO any more than regular Haskell code, so it "only" means the programmer shouldn't do any IO unless she or he is sure there's no harm in repeating them.
I pretty strongly disagree with this. "Delimited continuations" (in the sense of shift and reset) are good, but call/cc is not braindead at all. It is actually the easier abstraction to reason about (by a large margin) and has the benefit that it has a nice curry-howard interpretation and it interacts with typing well. Type systems for delimited control end up being type and effect systems (and so are evaluation order dependent). While type systems for classical control are just ordinary type systems with multiple returns (which are evaluation order independent).
Tweet mentioning said GHC bug, for reference: https://twitter.com/bos31337/status/116372971509121025?lang=en
[**@bos31337**](https://twitter.com/bos31337/) &gt; [2011-09-21 04:47 UTC](https://twitter.com/bos31337/status/116372971509121025) &gt; So glad you asked! The best ghc bug ever involved a dev version of the compiler deleting your source file if it contained a type error. ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
I think you're pretty much [alone in that opinion at this point](http://okmij.org/ftp/continuations/against-callcc.html). There are plenty of documentation littered with examples describing the failings of call/cc as useful abstraction. It's by no means easy to reason about. Edit: and the [libraries providing delimited control](http://okmij.org/ftp/continuations/implementations.html#caml-shift) have interfaces no more complicated than those providing undelimited control, and they have fewer actual pitfalls as the previous article elaborates.
should've avoided operators and used the handy isLessThanOrEqualTo jk 
I think the rule is slightly more complicated. If you write `fmap f x` such that (1) it compiles, and (2) `fmap id x = x` then it must be correct.
I think that still fits into the theme of the question though, because it's talking about how IO and exceptions are "special" in imperative languages wheras in Haskell they are reified. So `Cont` would be available in the way OP intended, if I understand his question. But not in Python and Java, I guess, neither of those have anything like goto. Also, continuations allow GOTO accross function bodies, which modern implangs tend to lack too.
Heya. Haskell is probably never going to get totally ML modules, but you can basically simulate ML modules with Backpack so it should be good. I'd like it if we could more Backpack stuff in for 8.0, but I've been fighting with the Cabal end of things quite a bit which has been quite a stopper. The specification as far as things go, is fairly clear, although perhaps not entirely written down yet.
Have a look at Data.Data.Lens
&gt; [numeric differentiation] works pretty well. We get good approximations as long as h is small enough. It's worth pointing out that numeric differentiation is actually more problematic than the mention here would lead one to believe (due to rounding error), and choosing an appropriate value for the increment `h` is tricky. 
Don't know of a name. Conceptually, it is just a pair of functors with opposite polarities... I'd be interested to know your use cases...
It would be cool to go one further... could you build typeclass &lt;-&gt; trait bindings, preferably as automatically as possible? There are a few clear problems with this (rust can't do any higher order types, haskell can't do lifetimes) but you might be able to work around at least some of them?
Restriction is often a primary goal in this community. As a library consumer, for instance, I *love* for things to be very restricted, because it allows me to easily understand what I can and cannot do.
Obviously not GP, but I can answer for my own experiences. &gt; Just to clarify - are these contracts specifically for Haskell, or are you hired to solve a certain problem and choose to use Haskell yourselves, without requirements/discussions with clients? I can say this is the case for me in most contracts. &gt; And, if you don't mind, what problem area are you active in? Web development, mostly REST api's. 
How were you planning on using your type? What would the `MonadReader` instance be...the outer `ReaderT` or the `RWST`? Also, check the type of `lift` for your instance to see why it won't work. (For future reference, posting the compiler error would be helpful :) ) lift :: m a -&gt; CommandT m a But your `lift` is lift :: ReaderT Tag (ExceptT String (ZoneT m)) a -&gt; CommandT m a See why the types don't match?
&gt; the programmer shouldn't do any IO unless she or he is sure there's no harm in repeating them. That doesn't sound like IO at all. Are you trying to say that the type system is not effective at separating IO from pure code (and by extension, at separating STM computations from IO computations) because `unsafePerformIO` allows you to bypass the type system's restrictions? *edit*: turns out that using `unsafePerformIO`, I can also trivially nest one `atomically` transaction inside another without triggering any failsafe mechanism, so I'm not sure what you mean by "STM specifically disallows nested transactions". I think it disallows nested `atomically` calls via the type system, in the same way in which it disallows IO. In *exactly* the same way: `atomically` is an IO call, and that's why you can't call it from STM. So if you don't consider the type system to be an effective measure against using IO inside an STM transaction, then you shouldn't consider it to be a good measure against nested transactions either.
Thank you! The way you went through your thought process helps a lot; I'm slowly learning how to think in Haskell instead of C or Java, haha.
The rule is that any `Functor` instance that satisfies the identity law (i.e. `fmap id = id`) automatically satisfies the second law: (i.e. `fmap (f . g) = fmap f . fmap g`. This is only true if parametricity holds which requires ignoring things like `seq` and `_|_`, etc.
Right, I think the point is not to let the name `Types` make you think it should _just_ be the types -- instances and other general-purpose machinery on them is perfectly fine in the same file.
Well shoot. I was completely stumped until I considered the fact that there are two `ReaderT` in the same monad stack.
I have to do this for work getting types between Haskell and Rust. What I do is use an intermediate C struct with one field an int for enumerating the ADT's constructors and another field a void pointer pointing to the payload of the constructor. You gotta define Storable instances for the intermediate type fairly mechanically using hsc2hs and tie it together with some type families and classes. type family CType (haskelltype :: *) class ToC a where toC :: a -&gt; IO (CType a) class FromC a where fromC :: CType a -&gt; IO a And similarly traits on the Rust side to translate to and from the intermediate C type.
call the list ls and then you can use concat, map (or better yet, concatMap) and list comprehension like so: concatMap (\x -&gt; [ (fst x, letter) | letter &lt;- snd x]) ls
That's awesome, thank you very much!
A language such as Rust? I think they only support one of each: structs (⊗) and enums (⊕). As for (&amp;), I think it should be relatively easy to implement it using an existential type / trait object: // note that the argument is "self", not "&amp;self", so calling one // of the two methods consumes the object and the other method // cannot be called. trait With&lt;A,B&gt; { fn left(self) -&gt; A; fn right(self) -&gt; B; } For the above to work, we'd need to have a value of type `With&lt;A,B&gt;`, but due to a missing Sized instance (I'm not very familiar with Rust yet), I've only been able to create a value of type `&amp;With&lt;A,B&gt;`, which defeats the purpose since neither method can be called. Oh well. Finally, for (⅋)... hmm, what's that one supposed to do again?
Well, thanks for taking a look. I can certainly try to encapsulate the `ExceptT` layer in a `newtype` myself. Is the issue you're imagining similar to what mstksg brought up below (two `ReaderT`s in the stack)?
Well, for a simple one, data ReadShow a b = ReadShow (a -&gt; ShowS) (ReadP b) Or basically any form of parser combined with a deparser.
I agree with all that, but it was just an example of the syntax.
Yes, but I just love pedanting, and this is one of my pointless pet peaves. 
This is what I enable in the .spacemacs file: dotspacemacs-configuration-layers '( ;; ---------------------------------------------------------------- ;; Example of useful layers you may want to use right away. ;; Uncomment some layer names and press &lt;SPC f e R&gt; (Vim style) or ;; &lt;M-m f e R&gt; (Emacs style) to install them. ;; ---------------------------------------------------------------- asciidoc auto-completion ;; better-defaults dash emacs-lisp extra-langs git haskell markdown ;; org (shell :variables shell-default-term-shell "/bin/zsh" shell-default-height 30 shell-default-position 'bottom) syntax-checking puppet version-control ) 
you get the input as a string and then try to parse it as an int. You can use the 'read' function to convert a string into a Haskell value, but it throws an exception when it can't parse the string correctly. A better solution is the 'readMay' function from the safe package (https://hackage.haskell.org/package/safe-0.3.9/docs/Safe.html#v:readMay) readInt :: IO (Maybe Int) readInt = do str &lt;- getLine readMay str main :: IO () main = do putStrLn "Enter an integer:" mbInt &lt;- readInt case mbInt of Nothing -&gt; putStrLn "That was not an integer" (Just i) -&gt; putStrLn ("Thank you, " ++ (show i)) 
Prezi uses Haskell for back-end and Elm for front-end IIRC
Not sure how new you are to Haskell, but if the question means what I think it does, you can't convert an `IO Int` to an `Int`! All you can do is convert it to an `IO Something`. A good way to think about this is: an `IO Int` is the specification for a program that produces an `Int` when executed by the Haskell runtime. The way to use an `IO Int` program specification is to build it into a larger program specification, which you then feed to the Haskell runtime by calling it `main`.
You might mean `return (readMay str)` on your first do block. Also, there is no need to import an extra package...`readMaybe` is already in *base*/the default package that comes with Haskell in the `Text.Read` module :)
Have you read the multiplate paper? It provides a rigorous explanation of how lenses correlate to the store comonad, and builds upon that to get to multiplate.
Forgot to mention a very useful feature. You can do "search and replace in spacemacs" following these guidelines: https://github.com/syl20bnr/spacemacs/blob/master/doc/DOCUMENTATION.org#replacing-text-in-several-files Try it. I take time to get used to all possible combinations (such as `S` for substitute) in iedit-state mode but it is awesome. 
Try this version of ghc-mod and see if it is any faster https://github.com/alanz/ghc-mod/tree/timing I just realised it needs cabal-helper from https://github.com/DanielG/cabal-helper to compile 
list comprehensions are already basically `concatMap`, so you can do it all in a list comprehension: [ (i, letter) | (i, letters) &lt;- ls; letter &lt;- letters ] or all in a `concatMap` concatMap (\(i, letters) -&gt; map (i,) letters ) ls You should never really have to mix both :) also, why use `fst`/`snd` when you can pattern match :O
Thanks ! 
The most straightforward way is to pass a list of integers read so far to your input reading function, then inside that function, append to the list and pass the new list to the recursive call.
&gt; GHC does this automatically as an optimization, but only when it can deduce that it is very likely to be helpful and not harmful. Really? Doesn't it always memoize except when it knows it can get away with out it? Memoization is, after all, explicitly part of the semantics of lazy evaluation, not an optimization.
a guess, try: {-# OPTIONS_GHC -O0 -fno-cse -fno-full-laziness #-} at the top of your module to debug it. GHC might perform common subexpression elimination on the "pure" value. so: do print$ unsafePerformIO newUnique print$ unsafePerformIO newUnique might be optimized to: do let purestOfThePure = unsafePerformIO newUnique print$ purestOfThePure print$ purestOfThePure the options pragma above would prevent that. but that's fragile. can you use an impure unit test? or avoid the unsafePerformIO? (in GHCi it might work, because each statement is executed separately.) btw, for any pure value `x`, (and any lawfully-reflexive `==`), the property `x == x` should always hold. 
Another thing that hasn't been mentioned: If you have the time and you really need to build it on the VPS, you could add a swap file to the machine to give it more (but slow!) memory: # dd if=/dev/zero of=/swap bs=1M count=2048 # mkswap /swap # swapon /swap # free -h I've had to resort to this a few times, and the extra swap memory is often only needed for the initial builds of some libraries, so you could even try disabling it after the initial build if you want to delete the file and reclaim the disk space.
Also depends what you want to do with consecutive non-letters, i.e., do you want to split on single characters or interleaved words/non-words: split "abc::def" = ["abc", "", "def"] -- or ["abc", "def"] -- ?
I suppose Prolog must be your best counterpart for List/Logic. (Or List/Logic + IO) GPU calculations are some kind of State, I suppose.
Try changing your definition of `uniques` in your testsuite to: uniques :: [U.Unique] uniques = US.unsafePerformIO $ do u1 &lt;- U.newUnique u2 &lt;- U.newUnique return [u1, u2] The reason why your current code doesn't work is given by /u/spirosboosalis. 
Real World Haskell is a bit more pragmatic a companion to Learn You a Haskell. From there if you want to continue, you can dive deeper with * Pearls of Functional Algorithm Design * Parallel and Concurrent Programming in Haskell * Purely Functional Data Structures * Types and Programming Languages Each of those dives deeper in some area. Pearls of Functional Algorithm Design teaches how to "think" like a Haskeller through a series of interesting puzzle like problems that the author shows how to optimize from a simple form into one that is fast and efficient by equational reasoning. Parallel and Concurrent Programming in Haskell shows you how to use Haskell as a powertool rather than a toy. Purely Functional Data Structures teaches how you can analyze the performance of algorithms in a lazy setting. Amortization, worst-case analysis, etc. and gives a very large set of building blocks for building good functional data structures. This is a must read from a formal computer science perspective if you are interested in algorithms. Types and Programming Languages teaches you about type systems themselves. If you want to understand more about type checking and type inference, this is a great place to go to learn how languages like Haskell are put together.
In theory I could make the `profunctors` package depend on `bifunctors` and add an instance for [`Biff`](http://hackage.haskell.org/package/bifunctors-5/docs/Data-Bifunctor-Biff.html). This was suggested recently on the #haskell-lens channel, but it hasn't happened yet.
&gt; What's the current state of the art behind custom sandboxed memory allocators in Haskell? We don't really have anything like that in truth. Laziness plays hell with object lifetimes.
That's... confusing. For some reason all the times I saw that package name I thought it was a member of `text`.
Thank you, thank you, thank you! The `-fno-cse` did it for the `Unique` test case in the test-suite. I am curious, though-- I changed my `mkVar` function in `Numeric.Algebra.Elementary.AST` to use the `INLINE` pragma, which fixed the other tests in the test-suite. Is this fragile, though? IOW, should it be `mkVar :: String -&gt; IO (Id)` rather than what I have now, which is `mkVar :: Sting -&gt; Id` and `mkId n = Id { name = n, unique = unsafePerformIO newUnique }`?
&gt; shouldn't do any IO I think not being able to do any IO except using `unsafePerformIO` is a little bit stronger than a 'shouldn't', but yes, it's no more safe in that regard than regular pure code. But we still call those functions pure despite the fact that they could open the escape hatch and perform arbitrary IO, so I think it's quite fair to say that `STM` also 'disallows' IO effects.
I'd expect a lot of the warnings in such a situation to be false positives and provoke "I told you so" from the other side, if anything (OTOH, the ones that aren't can be bad enough to stop it).
I am not an expert on GHC internals, but I have used bitboards (for word games, not for chess) and also a lot of Word64-transformations in general (like Bloom-filters and 64bit Simple9 encoding). When I find a way to use bit operations the performance skyrockets. Regarding the special bit operations I have noticed thath the popCount function was just recently added to Data.Bits, I therefore assume it was just recently mapped to its matching low-level primitive.
&gt; if you explicitly name something and use it more than once, GHC is likely to keep it in memory (however much of it has been computed so far) and not compute it again; and if you don't name it OK, that's fair. What I'm getting at is that the default translation to STG *always* keeps an evaluated let binding in memory, only freeing it when it is garbage collected. Any deviation from this would be considered an "optimization". 
&gt; What's up with exceptions everywhere? Good question.
Not as well known as Rust I'd say, and probably because the documentation is arguably not up to par, but I'd assert potentially better for system level things than Rust is ATS. It takes its nods from the likes of Haskell, Prolog, ML, and the like, but translates to C. I took a course with the prof who made it, but I never had the time to look into the generated C to vouch for it, but it would be fun. I do a fair deal of work on the system level so I really feel at home in C. That said, the reason I poke around here trying to learn Haskell is because I've seen how learning new paradigms helps me program C better. The biggest of these if probably trying to stay purely functional even in C followed second by (forgive me I can't recall the name) what I know as unique pointers and other responsible memory management techniques that higher level languages force. I don't foresee C getting knocked out of the System domain, but I can see the likes of Haskell or ATS having a positive influence in such an arena. Honestly, keeping functions as pure as possible, especially in multi-threaded environments, is a simple practice to enforce on oneself that saves thousands of headaches.
This is yet another reason why you need to split up the type classes from their associated data types. There is no reason why the `Profunctor` class should depend on the `Bifunctor` class
Re getting stuck in helm: That used to happen to me too, all you need to to is issue another helm command and it kills the extisting helm session. So if you do `SPC s S` (search for current word) and go off doing something else while the helm buffer is open, just hit the same keys again when you notice.
&gt; "Embedded and functional programming are, of course, the opposite ends of the spectrum" I'm also a low-level/system programmer, sort of like you. I like hearing people's opinions on things like this because it makes for good debate. What I quoted here from you, do you agree with it or no? I'd like to hear your perspective on it.
&gt; You cannot use atomically inside an unsafePerformIO or unsafeInterleaveIO Hmm! Well, I tried and it did not throw a runtime error. I'll try again, maybe I wasn't careful enough with strictness and the transactions did not actually overlap.
[I have a list of project ideas](http://beerendlauwers.be/project-ideas.html).
The untyped lambda calculus.
&gt; How does `⊥ :: a` relate to the idea that "if it compiles, it (probably) works"? It is one of many examples that show that the slogan is not to be taken too seriously.
Could you describe a bit the "mental linear type system" you mention? It sounds like there might be useful advice there.
Here's some good reading about it: http://blog.sigfpe.com/2007/07/data-and-codata.html I'd quibble slightly with what /u/beerdude26 answered below to say that codata is *possibly* infinite. These examples from Idris, which explicitly distinguishes data from codata, are illuminating I think. (remember `::` is cons): data List a = Nil | (::) a (List a) codata CoList a = Nil | (::) a (CoList a) codata Stream a = (::) a (Stream a) In Haskell data/codata are conflated, so there wouldn't be any observable difference between the first two definitions. But Idris does totality checking, so the distinction can be important since codata has a different notion of totality ("productivity"). For example, something like `repeat x = x :: repeat x` is marked as total when its defined as a Stream or CoList, but not when defined as a List.
Definitely in for support of automatic, semantic (as in, type-checker-verified) package versioning. The goal should be this: https://github.com/elm-lang/elm-package
Is this some FP class homework, by any chance...?
I just came up with that one too! The fact that you can't distribute ∀-Introduction over the two sides of ∨-Elimination confused me at first, but now I just go with it. Here's how I read your proof: Either ∃x.t(x)→∀y.t(y) or not. If the former, we're done. If the latter, consider an arbitrary t(c). Either t(c) or not. Consider the case where t(c)→⊥. Since anything follows from, ⊥, we can conclude t(c)→∀y.t(y), so ∃x.t(x)→∀y.t(y). But (∃x.t(x)→∀y.t(y))→⊥, so we have a contradiction. So it must be the case that t(c) for any arbitrary c. So we can state ∀y.t(y). Since A→True is always True, this means that t(b)→∀y.t(y), so ∃x.t(x)→∀y.t(y).
Well, ⊗ and ⊕ are the ones that work basically like standard product/sum types, so that's not terribly interesting. :P Plus you can do all four with Church encoding if you have linear lambdas, which is why I was talking about direct definitions. Anyway, ⅋ fits the pattern of the others: as a disjunction it only contains one value, and being multiplicative means you have to use both values! ...that is to say, assuming a single result not containing ⅋, you have to disprove one branch in order to use the other. Functions are the obvious example, where the input (in negative position) is "disproved" with a value of the appropriate type (via function application) so that you can make use of the output.
Search for "Functional systems in Haskell" (Stanford CS240h Spring 2014). I think it is a good introduction for people with a C/C++ systems programming background.
Cool thank you! My main question is where is the best place to put it? Is this something you would just paste into your project somewhere? I would love for it to have the chance to become more widely used, would putting it in a library make a difference?
I can't find positions opened by Facebook about Haskell
Also, for anyone who stumbles on this later: It's (|&gt;) I'm asking about here. (&lt;|) is synonymous with ($) in elm. 
Thanks! Working backwards? Can you explain? Is there more to this than personal taste? I personally often find that "do x, then y, then z" matches my mental process better than using ($) or (.). 
I have no idea. I guess it's not as fundamentally useful in Haskell as in Elm, where you'd like to be able to define an element and then modifying it by giving it "attributes" like color, style, position. That may be the reason why it's not widely used.
Fixed, thanks
Great, thanks 
Ha, sure, it's just slower than I would like. 
They are defined in the fsharp package: https://hackage.haskell.org/package/fsharp-0.0.4/docs/Control-FSharp-Syntax-Operators.html EDIT: Don't actually use this. The fixity is wrong for `&lt;|`.
This reminds me a bit of [this other blog post about unit testing IO](http://engineering.imvu.com/2015/06/20/testable-io-in-haskell-2/). Great minds think alike! ^(I wish I could think that cleverly)
You can also get it from [base-compat](https://hackage.haskell.org/package/base-compat-0.8.2/docs/Data-Function-Compat.html#v:-38-) if you're on an older version of GHC. 
Your package looks great! The debate around this is super interesting. It's funny, I found myself using the equivalent of (.) and ($) in my elm code, and the elm community said something like: "You should switch to using |&gt; and &gt;&gt;, they're easier to read and more idiomatic elm"
True, I've proposed [something similar](https://www.reddit.com/r/haskell/comments/34k6fn/a_good_symbol_for/cqvvtby) some time ago. This issue seems to come up periodically ;) Personally, I'm mostly fine with Unicode symbols, but I can understand that a lot of people don't want to use them. If one sets emacs' input mode to `TeX` one can simply type `\to` for `→`, but it is still a pain to search for those symbols, and one needs to know some TeX.
&gt; a variable Haskell doesn't have variables. ;) But, yeah, any monomorphic binding will, once evaluated, hold the value for future use though that binding until garbage collection.
Amusingly enough, I run the Montreal Haskell meetup, and was looking for someone to trade code reviews with a few months ago. By the sound of it, we're about at the same level -- think that could work for you?
I've asked for code reviews publicly before, and gotten some help. I don't think there's anything wrong with it. The issue is that it takes more effort than answering a stack overflow question and goes beyond what most people are motivated to do without a personal relationship. And I think that's what is so great about mentoring. When you mentor over a longer period you develop a personal relationship. You care about them. It's emotionally rewarding in a different, deeper way than stack overflow. For me at least, that makes me more willing to do things like a deeper code review. That said, I don't think code reviewing a larger project line by line is as important to learning as suggesting next steps to learn and answering questions. 
I'm curious what you mean by saying Haskell doesn't have variables. It certainly seems to have variables in the same way that mathematics has variables.
 import Control.Applicative main = do let loop list = do x &lt;- (read &lt;$&gt; getLine) :: IO Int if x == 0 then print list else loop (list ++ [x]) loop [] I just wrote this out in the reddit comment editor, so it might not be correct. But here, I define a function 'loop' that takes a list as an argument, reads a line from a user, converts it to an integer, and if the int is not zero, calls 'loop' with the modified list (list ++ [x] because we want to append the new number to the end of the list). Hopefully this works for you, and if you have any questions please ask.
&gt; how do I convert an IO string into an int You don't. You can't take an "action that may produce a `String` in the future" to an "`Int` value present at all times.". You can convert a `String` to an `Int`. You should try writing this function or type `String -&gt; Int`. I'll just use `read :: String -&gt; Int`, for now. Now, use can use that to convert an `IO String` into an `IO Int`. In particular we should look at the type of `fmap`: `(a -&gt; b) -&gt; f a -&gt; f b` then specialize it for `IO` to `(a -&gt; b) -&gt; IO a -&gt; IO b` and call it on the fuction your wrote `fmap (read :: String -&gt; Int)` -- this causes `a` to be unified with `String` and `b` to be unified with `Int` -- to get something with the type `IO String -&gt; IO Int`. HTH.
Sure, just make sure when you look at core that all your numeric things are getting unboxed to primitive hash-y things that look like `#0` and `#+`. You could also try the LLVM backend.
`elm-package` does not check (nor infer) version bounds of dependencies. It infers and checks version numbers of packages (which is cool). The two are orthogonal. `elm-package` fixes the problem where a new version includes breaking changes but does not have a major version bump. It does not solve the problem in the original post. What I want to say is that `elm-package` is a great intermediate goal but not the end goal. In Haskell the packaging problems are amplified by the fact that we not only have versions of packages but also variants of versions of packages. The interface and semantics of a package may depend on the platform but also on what versions of other packages are used (via `#if MIN_VERSION_transformers(0,2,0)`).
Great question! It becomes even more difficult once you mix in async an imprecise exceptions!
Another related project is [Lamdu](http://www.lamdu.org), where just the "sugaring" process is implemented and not the "de-sugaring" - the AST is stored de-sugared but presented sugared in the editor.
IIRC `|&gt;` and `&lt;|` are used in `Data.Sequence`, which is why most libraries don't use those symbols as operators. The actual functionality you describe is in `Data.Function` I think.
And neither (Haskell nor maths) version are variables, because they don't vary within their scope. They are bindings--(name,value) pairs. Sometimes the name variable seems a sloppy to me. Haskell variables, C variables, and Prolog variables are pretty different beasts. I prefer working with the terms "binding"--an (id, value) pair--and "reference cell" or just "cell"--a storage location for values; the value stored may vary over a context (possibly including time)--they are less ambiguous and seem to cover most, if not all, of the behavior we want. If we have to use the term "variable", it should clearly refer to something that varies, not a simple binding. In C, variables are special bindings where the values are all themselves cells. In Haskell that would only apply to a bound *Ref (e.g. IORef), not any bound name.
Yes, but the Control.Applicative imports the function (&lt;$&gt;) which lifts the 'read' function into the IO Applicative Functor. Besides all that weird lingo, just know that (&lt;$&gt;) allows a normal function that has type String -&gt; Int to operate on an IO String (the type of 'getLine') and return a IO Int (full type IO String -&gt; IO Int). However, an important caveat is that it doesn't just work like this for IO, but for all other data types with an Applicative instance. The code that doesn't need a Control.Applicative import would be: main = do let loop list = do x &lt;- getLine let xInt = (read x) :: Int if xInt == 0 then print list else loop (list ++ [xInt]) loop [] Note: just as before, when Haskell tries to 'read x', if x is not an integer (since we annotated the type of 'read x' as 'Int'), there will be a read, no parse error. Also, notice how Control.Applicative removes the necessity to add the let declaration: let xInt = (read x) :: Int
Instances for types in base would go in the profunctor-class package (and the goal is to merge that package into base anyway). All other instances (like Cokleisli) go with their respective data types.
The term "variable" has a clear and long-established use in mathematics. The fact that programmers have confused things since then is irrelevant. If you want to insist on non-sloppy terminology, let's call variables in Haskell what they are and rename whatever it is that C has.
How would you handle versions which build but have known bugs and should not be used in this scheme?
I don't know if Reddit notifies you of edits; I made a big one above. HTH.
There's a Montreal Haskell meetup? Any more details about when and where?
That's the approach Harper [takes](https://existentialtype.wordpress.com/2012/02/01/words-matter/).
I don't know enough about systems programming for my opinion to be of worth, but I have seen people saying that computer architecture has been changing so much that C no longer maps to modern architectures and I could see another language taking its place because of this. I saw some parallel Lisp machine that was a graduate students project where the compiler would figure out ahead of time which expressions could be computed in parallel
I don't think I could be your mentor but I am sure that if you want we could learn a lot by sharing code and reviewing it.
Authors: Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh Shan, Robert Zinkov Abstract. We present Hakaru, a new probabilistic programming system that allows composable reuse of distributions, queries, and inference algorithms, all expressed in a single language of measures. The system implements two automatic and semantics-preserving program transformations—disintegration, which calculates conditional distributions, and simplification, which subsumes exact inference by computer algebra. We show how these features work together by describing the ideal workflow of a Hakaru user on two small problems. We highlight our composition of transformations and types in design and implementation.
Explain that to the nodejs crowd, I dare you...
I do it directly in the contrib package of haskell ... which is bad enough but simple. I can't remember the specific but I had issue trying to set it to my .spacemacs (and had no time to lose trying to figure it out). I might actually propose a PR to spacemacs to include such bindings and see if they accept it (both bindings are not used). To get to the contrib files you can &lt;SPC f e c&gt; (for file emacs contrib). As an aside to open the .spacemacs file you can &lt;SPC f e d&gt;
To be clear, I have no problem whatsoever with overloading the jargon "variable". It's usually clear from context which definition is relevant, and there are plenty of ways to disambiguate when necessary. Harper is, as he often does, extrapolating a core of truth taken out of context into a needlessly inflammatory opinion. It's very silly and I have no interest in going there, but for anyone who *does* the fact of the matter is that Harper is correct.
Would you like to mentor me? I'm a Haskell/servant beginner: https://github.com/arealities/simplestore
If you mean composition as in `ComposeT` from `mmorph`, that's not composing monads; it's composing monad *transformers*. The difference between `ComposeT t1 t2 m a` and `t1 (t2 m) a` is the same as the difference between `(f . g) x` and `f (g x)`. If you mean composition as in functor composition, given two monads their composition isn't necessarily a monad at all, as explained on SO.
Well, the least complicated way to do that reliably is let GHC do it for you, e.g. via the GHC API. Anything else is probably going to require too much manual intervention to be helpful.
You would at least have to what amounts to full path coverage for all the flags and all CPP macro decisions in the code to even get to the types exposed by the dependency.
Sure, I doubt I'm that far ahead of you, but I'll give it a go. It sounds like there are several people at my level who are interested. Maybe we should create a study group instead. Which would interest you more?
Maybe we should make a study group or something? 
Awesome. It sounds like there are several people at my level who are interested in getting together. Maybe we should form some kind of peer study group? 
Sorry, useless comment. You can dump a lower-level representation of your code which ghc uses as part of the pipeline called "core" with e.g. the following flags `-ddump-simpl -dsuppress-all -ddump-to-file`. The core can be confusing, but you can usually peruse the dump and trace which bits of core corresponds to which bits of your code. You may want to read a little tutorial on core. The main bits here are you want to make sure that you see a lot of [primitive]( https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/primitives.html) numeric values (rather than things with a heap representation), which you can recognize by the `#` character in their name. If you've got those you know you're working with numbers in registers, primitive arithmetic ops, etc. If you are seeing boxed numeric types, you should play with inlining (transform your definitions so that they are syntactically "fully-applied" w/r/t their call sites, and possibly add an `INLINE` pragma), and possibly try some bang patterns to add strictness. I would suggest setting up a cabal project with criterion, and play with both dumping and inspecting core as well as benchmarking to get a sense of what's going on. You shouldn't trust what other people tell you about performance (*cough ↑)
At one point I made some notes on ideas for implementing a Haskell-ish language that used linear types from the ground up, and that was one of the things I planned on--in "pure" code, even types like `!a` would be subject to linearity (and not usable where just `a` is required) while in code that permitted nonlinearity as an effect the rules for exponentials could be applied (explicitly) to duplicate/discard such values or to extract individual values of type `a` to pass to "pure" linear functions. (However, in keeping with Wadler's law, the part I spent the most time on was probably figuring out the syntax for data types containing ⅋.)
Idioms are about custom and tradition as much as what is "correct." The idiom in Haskell is to use `(.)` and `($)`, and the idiom in Elm/Elixir is `|&gt;`. Neither are necessarily better than the other, but it's generally preferable to write idiomatic X when writing in X language. I don't have any issue reading either form, but it does take a moment to context-shift and read the idioms appropriately.
From libc::types: type c_int = i32; It was close enough for this exercise though, rust's i32 is a signed int of 32 bits which is represented by an signed int in C. OTOH, For proper bindings, I would probably create a Foreign.Rust.* with all the proper types.
As I’ve said, it’s about semantics i.e. what the types stand for. A C `int` does not stand for a 32-bit signed integer with 2's complement. There’s `int32_t` (from `&lt;stdint.h&gt;`) for that purpose.
Wow, that's pretty sad if you can't do that kind of customization in the .spacemacs file.
correct link: https://dl.acm.org/citation.cfm?id=2804317
Fewer monads indeed. I noticed while talking about the code with someone else that there are a lot of things I'm doing that I think could be decomposed further into a pure part and an IO part and taken out of their monadic context altogether. I did hit on the idea of a higher-order generic retry function for authentication because the auth token expires every 5 minutes and must be renewed. I handle that by doing something like the following, which has worked well. withTokenRefresh :: Traduisons a -&gt; StateT AppState (ExceptT TraduisonsError IO) (Maybe a) withTokenRefresh f = go (1 :: Int) where go n = do AppState _ _ _ _ tradState &lt;- get expired &lt;- liftIO $ authTokenIsExpired tradState when expired renewToken result &lt;- liftIO . runTraduisons tradState $ f case result of Left err@(TErr ArgumentException _) -&gt; if n &gt; 0 then renewToken &gt;&gt; go (n-1) else throwError err Left err -&gt; throwError err Right msg -&gt; return $ Just msg I'll have to look for other places to apply this idea.
A study group might be a good idea... What is the next step we should take to get this up &amp; running?
That is a third conflated use of the PVP bounds, and it really has nothing to do with whether *it builds*. *It builds and tests work* != *it builds* != *it is working correctly* != *I like this package version*. No matter what, meta-data about a package that can't reasonably be known at the time when the package is uploaded, or can't reasonably be calculated by the author should be handled independently of the package. Whether the package is buggy is such information - it should be published by some build bot (tests fail) or manually as an independent statement by some authority "I hereby declare foo-3.14 to be buggy. Signed Bar Baz"
I really appreciate your reaction blog post. I'm still very much a Haskell newbie and it's hard to get honest reactions to concepts sometimes. Every language has this sort of 'entrenchment', and it's helpful to see opposed views in order to make an honest opinion.
I think it can depend. But if I were to describe what the `totalPoints` function in the post does, I would say that it returns the sum of the points for each entry that was spoken. That seems natural to me and matches the standard compositional order. And you don't need to be a wizard to know that, say, an optimization problem is going to have `maximum` or `minimum` as the "last" step.
You could try ```(evil-leader/set-key-for-mode 'haskell-mode "mhq" 'ghc-show-type)``` then you can use ```comma h q``` 
Also see the discussion around that article. I was asking exactly the same questions you have - there should be a simpler way to write this boilerplate and/or there should be a standard "typeclassed IO" that came with mocks, spys etc. that everyone that is writing production (i.e. not hobby) code could use. https://www.reddit.com/r/haskell/comments/3ajy44/testable_io_in_haskell_at_imvu/
`INLINE` should be used only* for optimization, not correctness. *full disclosure, in a project that defines grammars with observed sharing, I am abusing `-fno-cse` for "correct behavior", as well as `NOINLINE`, but they are indeed fragile. I'm no expert on GHC, and sometimes there's more or less sharing than I expected from the program textually. (where `let x = 1+1 in x*x` would mean "more sharing" and `(1+1)*(1+1)` would mean "less sharing"). 
Yes this shouldn't be a problem.
Or is it maybe going to be `map snd` instead? Anyways, yes, you're right. When the composition is short enough, it can be read like a phrase. But in those cases it really doesn't matter which way you go. You could say that I should split up my pipeline so all the compositions are short but in my experience this leads to having to name thin that don't have meaningful names more often than not. It feels very comfortable to me writing longer pipelines in languages that provide left to right chaining. It's not that way in Haskell.
thanks a lot.
"Add 2 to the result of multiplying 3 with the result of subtracting 2 from 3" versus "Subtract 2 from 3, then multiply with 3, then add 2". Which one do you find more natural to do in your head? I know which one it is for me.
Cool. So if you wanted to multi-line it, and avoid point free style would you do this or something else? totalPoints entries = sum . map points . filter wasSpoken $ entries 
/u/jkarni has volunteered to mentor me and I couldn't be more happy. Thank you so much! In hindsight, I think it makes more sense to seek mentoring *after* having established some kind of relationship. I've interacted with Julian a couple of times on the servant github and here on reddit. I remember reading advice somewhere about seeking mentors that suggested choosing a specific potential mentor and asking them personally. I'm very fortunate that Julian offered to help, but I'm not sure whether he would have offered if we hadn't interacted previously. I think when we ask for code reviews or other in-depth help publicly, we run in to the [Bystander Effect](https://en.wikipedia.org/wiki/Bystander_effect). This is especially true when the help we need is specific to our application, and is of limited use when people google it later. Instead, I think it makes more sense to identify the right person, interact with them and get on their radar, then ask them specifically for help. 
What I'm kind of confused about is... at the bottom right of the proof we have... `t(c6) =&gt; [ ∀] =&gt; ∀x.t(x)` but at the bottom right we also have `t(c6)` and need `∀x.t(x)`, but I can't replace the implication-into-ex-nihilo with a `[ ∀]` block because it results in `t(c6) =&gt; [ ∀] =&gt; ∀x.t(c6)`instead. Why do the same inputs (in both cases I'm inputting `t(c6)`) into the same block (both cases `[ ∀]`) result in different outputs?
That can also be a build artifact: either it builds and passes tests with dependencies X and Y, or it doesn't. 
do you think other packages in the ecosystem could benefit from this kind of refactoring?
I'm sort of in the same boat as you - I've been learning Haskell by creating a web app using Yesod. So I can't necessarily mentor you, but maybe we could try mentoring each other :)
Why would you want to avoid point-free style in this example? I think the most legible and idiomatic choice here is this: -- Note the signature makes the `entries` variable name unnecessary. totalPoints :: [Entry] -&gt; Integer totalPoints = sum . map points . filter wasSpoken As a general rule, chaining one-argument functions with composition like this example is the "good," highly-legible, non-golf kind of point-free style. Values flow in a straightforward right-to-left order. The right-to-left bit is unusual to most people at first, and thus some languages' preference for a `|&gt;` operator. But to echo a lot of comments that have already been made, it's a minor thing, you get used to it, and the cost of changing the language's style is larger than the benefits. The only circumstance where I would avoid point-free style in this example would be if it was aimed at somebody who doesn't know Haskell and has zero intention to learn it. But then I'd write it with parens: totalPoints :: [Entry] -&gt; Integer totalPoints entries = sum (map points (filter wasSpoken entries)) 
Catching SomeException causes problems with asyncronous exceptions. See [catching all exceptions](https://www.fpcomplete.com/user/snoyberg/general-haskell/exceptions/catching-all-exceptions) and the [enclosed-exceptions](http://hackage.haskell.org/package/enclosed-exceptions) package.
If you're not evaluating what you're reading than you're not doing anything. Come on. You have to be manipulating some kind of state in your head and reading the last operation that's done is literally useless. I mean, this is blowing my mind. I'm having trouble convincing myself that it's even possible not to think what I think. Why would you read code if not to find out what it does? Do you somehow read the last part in the pipeline and say "Yeah, I get it now. It's `maximum`" or something? `maximum` of what? You need the whole thing to understand what it's doing. There's no other way around it. And is there some other secret method to understanding an algorithm than going through it step by step? Do you not have that voice in your head that says what "you currently have"? Hell, even mathematics does everything bottom up. You never start with a definition and then later define it's subparts. You never prove a stronger theorem and then prove lemmas required for it. It's all about incrementally building some knowledge that lets you do the next operation.
If it isn't too much to ask I would love to join in as well. Why don't we set up either a google hangout, irc chat room, or whatever you prefer. I am currently writing an api that takes same data in mySQL and serves it up as a JSON object. I have turned to PHP because I have found guides that walk me through most of the process, but I would much rather write it in Haskell. What database library did you use and what guide did you follow? Why did you use Servant instead of Snap or Yesod?
&gt; Neither of those is a transformer, though. I know, but that's how I see monad transformers. Anyway, you understood and answered my question, thanks ;-)
It depends the way you think. If you start from a data and transform it, then `|&gt;` make sense, (for example, I have `2` and I need to print it, I can do `2 |&gt; show |&gt; putStrLn`. But if you start from a type signature (which happend if you prototype your function using `undefined`) then it's easier to start from the type. In our case need a `IO` : `putStrLn`, now we need a `String` : `show` and now `Int` : `2` which give `putStrLn $ show 2`. If you prefer, the `$` way gives you the type of expression straight away `putStrLn ... ` is an `IO`, whereas with the `|&gt;` I have to read the full expression to know the type.
It can't necessarily be known by the author of the dependency but it can be known by the author of the dependent package which is why I dislike this simplified view that bounds should be set to whatever the package happens to build with or passes tests with,... There is a place for manually gathered information in the package bounds, especially the lower bounds.
This only works in cases where you care about the outermost type constructor. If you want to know the whole type, you have to parse the whole thing. In any case, I do consider this a valid point. There were cases where writing like this was helpful.
Why on earth would I need "some kind of state in my head" to understand a simple arithmetic expression? I'm not even sure what you're trying to say here. Replace the numbers with variables, so you can't evaluate it, then what? Hell, just write it normally, with infix notation. Do you read that left-to-right, counting parentheses and trying to keep a mental stack? Or do you scan it for high-level structure and look at the outermost or innermost operations, as appropriate? And even aside from all that, what if the last operation is "...and multiply the whole thing by zero"? Surely you don't need to understand (never mind compute the value of!) the rest of the expression to know everything that matters about the expression as a whole. &gt; Why would you read code if not to find out what it does? Do you somehow read the last part in the pipeline and say "Yeah, I get it now. It's maximum" or something? maximum of what? What should I do instead, read the first bit and say "Yeah, it does something with the individual lines of the input text". Does what? Who knows! Like, say we have `maximum . map length . lines`. It finds the maximum line length of the input text. `maximum . map length` finds the maximum length of a list of lists. What does `lines` have to do with understanding that? Nothing. &gt; You need the whole thing to understand what it's doing. There's no other way around it. I'm not sure how "the whole thing is necessary" supports your argument that it's only possible to start with one particular bit. &gt; Do you not have that voice in your head that says what "you currently have"? Sure. But it's not any louder or more insightful than the voice that says what "I currently need". Why would it be? In the end they meet in the middle, and which end is easier to work from depends on the problem.
also check out `wordsBy` in the same package (won't create empty words)
How about `git reset --hard` whenever the test suite fails :D
You can modularise the types and control their imports more tightly.
That is the point though isn't it? The compiler is supposed to find those errors. Or do you mean the runtime?
I'm the author of this announcement. (Just to reiterate: *not* the author of Megaparsec.) I wasn't going to submit it here, because Megaparsec has already been announced on Reddit, but since u/HoboBob1 decided to post it anyway, I shall consider myself blameless. You can read the announcement in HTML (and with working links) here: https://www.notehub.org/2015/9/29/announcing-megaparsec. I'm interested to know whether more or less announcements should be as long as this one (that is, pretty long). If you have an opinion, please tell me! 
I've written one, here you go: &lt;https://www.reddit.com/r/haskell/comments/3mvaib/haskellcafe_announce_megaparsec_an_improved_and/&gt;.
Ok, and `map length` just tells you you're turning your input into a list of numbers for some reason. `maximum` tells you what you're actually trying to accomplish. So? I'm also not sure I'd really consider long chains of application "readable", relatively speaking, if it means you have to mentally keep track of an "abstract object" and go step-by-step just to make sense of the code. That's extra cognitive overhead and probably obscures the high-level structure of the code.
This is actually one of the ways callback hell is handled in PureScript: http://pursuit.purescript.org/packages/purescript-transformers/0.7.1/docs/Control.Monad.Cont.Trans#t:ContT It ends up looking quite a lot like the JS shown here.
Both are migrating together into base in 8.0 upon which those dependencies invert and no functionality is lost.
What I mean is that the standard monad transformers are newtype wrappers around some composite type, and everything they do can be implemented directly using the same composite type (you just couldn't write a `Monad` instance), but that the order in which the transformers are nested doesn't always match the way the underlying type is put together. For example, `MaybeT` puts the `Maybe` on the inside (e.g., `IO (Maybe a)`) whereas `ReaderT e` puts the function on the outside (e.g., `e -&gt; IO a`). And `ContT r m a` applies `m` to the result type, not `a` (e.g. `(a -&gt; IO r) -&gt; IO r`).
&gt; (However, in keeping with Wadler's law, the part I spent the most time on was probably figuring out the syntax for data types containing ⅋.) This is more or less what killed my second thesis. ;)
You're right, but all practical purpose it just doesn't matter. Also again, I would not use haskell direct types or C types anyway for proper bindings, but would make something like: module Foreign.Rust where (new)type I32 = &lt;something&gt;
Oh dear, I don't know if I've heard *that* story before. Was it ⅋ specifically, or just syntax in general?
That was really more hyperbole. The actual problem was that I wasn't smart enough to really program in a programming language with a type system based on display logic. The syntax was a minor technical aside that I never resolved.
I'm curious about this high level structure you keep mentioning. What is it and how does it help me understand algorithms? It's one thing knowing what a function does. That's why we have top level functions and that's why we document them. It's another reading it's source. The high level structure is only as high as the atomic parts you're assembling in it's pipeline. To me, knowing that a function ultimately gets a maximum of something isn't any more important than any other piece of what it does. I feel that it doesn't make sense to talk about hierarchical terms like "high level" when the structure you're observing is linear. What I'm saying, is that in the end you WILL have to read the whole thing and this is precisely when one order of reading is easier than the other one. My whole premise is based on the fact that you'll do a full read, and not just start at one end and then stop. I agree that if the latter were the case, then obviously it's better to start from the end because then you at least get some idea what will ultimately happen, but I don't think that that's what you'll be doing the majority of time when you read code.
Yep, I meant as runtime errors. 
There is a Haskell meetup in SLC? Tell me more, I will come.
I like to use `--enable-library-profiling` and `--enable-executable-profiling` (use those with `cabal install` and/or `cabal configure`). If you don't mind the extra compile time you can enable the library profiling in your global cabal config, `~/.cabal/config`. For optimizations, I usually just put the optimization flags (-O or -O2) in my cabal files, even though it sometimes you not to. You probably don't want executable profiling enabled all the time as that would make all your programs slower, but having library profiling always on means that both the profiling and non-profiling versions of the libraries will be built and available. Otherwise, you will need to do a full rebuild when you want to start profiling your executables.
&gt; Here's a hint it's wrong: Unless you're some kind of a wizard and know what the last step in your algorithm is before you start writing it out, you're probably going to be writing the first step first. Using the following "backwards" example from another thread: totalPoints = sum . map points . filter wasSpoken You could say that this reads "The total number of points is the sum of the points of the entries that were spoken." How can that possibly be "wrong"? I just don't see that there is a big, principled debate to be had between these isomorphic alternatives: 1. Describing the *results* first, followed by what fed into that result. 2. Describing *inputs* first, followed by what results they led to. Some people will prefer #1, some #2, some will prefer a random mix, some will prefer a stylistic rule that chooses one or the other in different circumstances. Whatever.
awersome. I miss the power of some CAS made in functional programming such is MuMath of the 80s. It was made in LISP and all the simplification rules were visible as source code. It had a lot of them for doing all kinds of symbolic calculations. It was an aversome tool. I had a lot of fun playing with. The simplicity of the REPL and the direct access to the lisp rules was attractive. Later the same company did Derive, which was much more packaged and much less accessible for hacking. I intended to translate all of the code to haskell but that was a lot of work. I have a copy mumath by the way, if anyone want it, let me know.
How I can access profiling information from within the program that is running? I wan to to adjust the parameters of an algorithm at runtime so it maximizes the speed, the heap size, CPU usage etc...
`in -&gt; out` of course (!) Because it wouldn't be Wadler's Law without a healthy helping of opinions ;) Also because I'm a little curious to see what's going on here in more detail :)
Well, there is `Control.Arrow.(&gt;&gt;&gt;)` (or abusing `Control.Lens.(&lt;&amp;&gt;)`). A shorter operator would however indeed be nicer.
Sorry! I just really like the work you've done, and I wanted to make sure more people saw it :)
I agree with you, don't get me wrong. This really is a minor stylistic thing, but check out the comments that guy that made the Flow library got for his stylistic opinion. The problem here is that even though some may prefer the #2, the #1 is the standard so you don't really have a choice. Meh... Maybe it doesn't really matter. I just wonder how much flak one would get if he wrote a popular library in a non-standard style. It people indeed don't care one way or the other, I'm happy.
For a while I actually tried to abuse lambda notation and make something like `\x. y` a valid pattern that would bind a continuation `x` and a value `y`, which was cute, but perhaps not really ideal. And in any case, I never figured out how to keep the scope for the two values separated in more complicated bindings. Anyhow, I've since misplaced the notes, but they likely would've made little sense to anyone but myself regardless. I keep meaning to revisit the basic idea at some point, though...
Well, it can still happen. I have seen some of those errors as well. The OutUfBound is definitely a sore spot since `base` does not return `Maybe`s upon accessing elements by index or using `head`, but it is relatively easy to reimplement that or use a library like `extra`. NullPointer usually only occurs when using FFI, and same goes for Segfault. Haskell Allows you native interactions, which obviously have a price. 
Wrote [this](http://gilmi.xyz/post/2015/02/25/after-lyah) once. Hope it will help.
I think you'll find that flak will be received for writing code in a non-standard style in any language, regardless of the language, the style, or the relative merits of either. I mean, there's a long-time member of the Haskell community who insists on a non-standard naming convention that most people dislike so much they avoid his libraries entirely. In some languages people have deep emotional investment in the placement of curly braces. Tabs vs. spaces is a bitterly divisive argument even in languages where they're cosmetic. For that matter, you seem to be awfully worked up over not liking the standard style in Haskell. I'm *still* not sure why any of it's a big deal. Consistency is important, but why does everyone want to start holy wars over this stuff?
I think I understand now. In /u/dnaq's suggestion, the common subexpressions are still impure, so they won't be optimized to one expression, but because I made two pure expressions, CSE changed the semantics. Is this correct?
When I first came to haskell from F# I hated $. Now I just think its wrong but still use it. For a while I did use a |&gt; operator particularly when processing collections and building data flows. I can now easily read the code going in both directions. Still its a weirdness that can trip people who are new to the language up since it feels so unnatural. 
I think you might use [EKG](https://ocharles.org.uk/blog/posts/2012-12-11-24-day-of-hackage-ekg.html) for that. Never used it myself but I recalled the blog post
For what it's worth, Flow provides `&lt;.` and `.&gt;` for composing. They behave a lot like `.` does.
Works for me; depending on the details. I'm more interested in trading code reviews and asynchronously discussing certain topics than in anything "formal". But if I can make it work, I'll at least try to help.
IMO, the "true root" of the PVP debate is that package authors don't try very hard to ensure backward compatibility. When I try to compile someone's code from a year or more ago, I often find that I have to make changes just to get it to compile. This is not a good way to invite new users into the community. I don't mean to suggest that compatibility must always be maintained, but it's good to try to maintain it.
While I can't recall the specific example I recall a segfault without using the FFI (though this of course represents a bug in GHC rather than a deficiency in the language which is the point you're trying to make). 
In Brisbane, we have [BFPG](http://bfpg.org), which is run from a meetup group. We have half a dozen folks in the executive group plus some others who help out a lot, and we meet on the second Tuesday of every month at 5:45pm, usually for two talks, and we have a Hack Night on the Wednesday the week after. We have a few companies that have been sponsoring for quite a while now, although I don't know how they got involved. More recently, the company I'm working for now approached me to sort out getting involved as a sponsor. We try to get involved with / help promote the big developer conference - YOW - that happens every year in Australia (and hosts one off nights throughout the year), and they tend to mention the various meetup groups in Australia pretty often. That's great, although it'd be nice if there were ways we could help them more, because it's an awesome conference. What works really well? We typically have a pre-meeting vetting session for the talks, to provide feedback and to make sure no one is going to stand up and ramble for an hour - that has effects on future meetups. It mostly seems to provide feedback and to help people who are new to talking get into it. What do I wish worked better? Most of the regular speakers are really into Haskell, so we're now struggling a little to even things out and get more FP-but-not-Haskell talks onto the schedule. We've done better at that recently, but I think that trend might have caused a stall or a decline in attendees. Hopefully we keep mixing it up and people return.
Read this https://github.com/winterland1989/Action.js/wiki/Difference-from-Promise and you will find the difference. Promise solve the problem using state and a pending array.
Note that Liquid Haskell can eliminate off-by-one errors and out-of-bounds errors
You could use `zipWith` instead of `zip` to exchange the tuple constructor for the list constructor, thus resulting in a list of lists. Alternatively, you could convert the 3-tuples to lists using pattern matching: map (\(a, b, c) -&gt; [a, b, c]) lst 
I'm going to take this opportunity to give [fantasyland](https://github.com/fantasyland) a quick plug. This is a functional javascript standard set in place that has really caught a lot of traction with other really great libraries like [ramdajs](http://ramdajs.com/docs/) and [folktale](https://github.com/folktale). I've used these libraries with great success and highly recommend to anyone who has to use javascript on a regular basis. [Here is an example of what my javascript code looks like using these libraries.](https://gist.github.com/TrevorBasinger/3739eb71c7fe94914166) I'm sure that I have a lot of room to improve, but I feel that the overall structure is very DRY and expressive. 
This bumps up against something I brought up a few times several months ago. It was mostly met with skepticism that it would make any sort of difference, but I might as well take an opportunity to bring it up again since it's meant to address the same issues you're discussing here. At the very least, maybe it will help spark a better idea. What I argued for was a pair of non-breaking changes: one addition to the .cabal format, and an associated change in the preferred method of stating dependencies. 1. `breaks-dependents:` would be a new field (optional, but encouraged) that provides a concrete statement of what the PVP is supposed to encode. It is stated in the same way as a dependency range, e.g.: `version: 1.2.3.4` `breaks-dependents: &lt; 1.2.0.0 &amp;&amp; &gt;= 1.2.4.0` If either bound is omitted or invalid (i.e. would exclude `version`), it's set to `&lt; version` or `&gt; version` as appropriate. *This field is named poorly on purpose, to use the same format as dependency statements. Its actual meaning is that the inverse range is guaranteed NOT to break, not that the stated range necessarily will.* When resolving dependencies, first the dependent package's allowed range would checked; then, for all available versions, `breaks-depends` is checked to see if its range *does not exclude* any version number within the dependent's allowed range. 2. Following adoption of this change, dependencies should be stated as *single version numbers* which are known to build successfully. Then, `breaks-dependents` (by inversion) can decide whether a different version is also satisfactory. This serves the actual goal: enabling the demarcation of whether or not an API change is backwards-compatible by the *library author,* who actually has concrete knowledge (at least more than anyone else), rather than distributing it as educated guesswork to everyone downstream. Basically, this obviates the question of whether or not the author knows and understands and follows the PVP. If they do, good -- it's a well-thought-out system. But if for whatever reason they don't, that doesn't break things for everyone else because the important information the PVP conveys is now redundantly encoded in a way that Cabal actually understands internally. It's not foolproof by any means, but the failure cases (and their solutions) are no worse than in the current system; and it removes what I see as a fairly wide zone of human error potential by concentrating decisions in the same place as expertise (i.e. the upstream author). On top of that, the worst case is status-quo: because of the defaulting behavior of `breaks-dependents`, doing things exactly as they're currently done still works the same.
&gt;&gt; You don't know that your package won't work with text-1.3 &gt; So what. If you find out that it does, make a revision with bounds that allow 1.3. Try this when the official package maintainer hasn't been seen in 3 years and you're a newbie trying to follow a guide. 
In Orlando, Florida. Any haskellers want to meet up? I'm game.
The pattern matching works like a charm; thanks!
If anyone lives in Atlanta and would like to meet up (not like a meet up but just *meeting* each other), that'd be cool. I'd be interested in starting a haskell meetup in Atlanta if there were more haskell devs in the area.
I don't think its that black and white, obviously to do devops you need to have some development skills, otherwise you wouldn't be able to do your job at all. My point was that devops doesn't have such an intense focus on pure software developmnet (else it wouldn't be devops, you would be a developer instead) In fact its funny you mention google, because the language they created (Go) is basically turning into the goto language to use in Google for any devops related code. As I mention earlier, Go was specifically created to dramatically reduce the entry barrier in programming, with the main demographic being devops people (one of the primary complaints from devops people is being unable to work with code typically seen in C++/Java that has a huge amount of abstractions/cognitive overhead) I mean we can argue all day what the definition of "rudimentary" is, but there is a reason why the huge amount of code related to devops is written in Ruby/Python or Perl (in the old days). They are languages that are very easy to code in and very easy to pick up (Go has the advantage over these languages in the sense that it can be compiled to a binary, and it has much better support for concurrency)
You're right, it is really confusing, and maybe someone else can explain it better than I can, but I'll tell you how I understand it anyway. The difference is that the c6 in the bottom-left corner is secretly bound to a specific term, while the c6 outside the ∨ block is free and can refer to anything. The statement "t(c6) is true or t(c6) is false" is true for any c6. However, in order to apply it, I have to pick a *specific* c6, and then I can say, either t(c6) is true for *this* c6, or else t(c6) is false for *this* c6. Then if I can prove something about c6 either way, that will be true for any c6. What I can't say is that either t(c6) is true for *all* c6, or else t(c6) is false for *all* c6, and that's what you would be doing if you could add a ∀ quantifier to the t(c6) inside the or block.
I can't wait for the future of VR meetups.
I think this is exactly the reason operator was added to F# since it is a one-pass compiler
That's not working for us in Santa Monica - meetup.org for a couple of years, always about 8 people, sometimes a few more, and lots of turnover, despite a steady venue. I think it's not a very FP area.
We've had rambling talks :) For my last talk, I just wanted to show off how I was able to stumble onto a decent implementation of the MaybeIO monad transformer using hole-driven Haskell. I only had about a 20-minute demo planned, and the host was going to fill in with something else, but we ended up going for 2 hours. I did it as a live demo, and let everyone make guesses and chime in, and even lead me astray, working with everyone to get back on track when we did. Everyone was really engaged and talking the whole time - small group of around 8 folks, me at a desk up front. It was like a team building something together. I think most of what I did was type in what they were saying, and act more like a DM in a D&amp;D game, steering a bit. I didn't plan to do that, but once I saw people working to figure out each next step, I backed off a bit and let everyone have fun. I tried to make sure the more green members had a voice, too, asking if people understood, clarifying as much as I could where they didn't as I went, and demoing little side bits of confusion. At the end our host said "This is what I've been looking for. These talks don't have to be a big presentation with fancy slides. He told the group that it's fine to just bring a snippet of code that's somewhat interesting that we can all pick apart and learn from together. I like good, informative presentations, but also whatever this night was. It was really fun.
With [NYHUG](http://www.meetup.com/NY-Haskell/) in NYC we organized through meetup.com which at least in our area seems to be the de facto platform to use. I think the most important thing we did to build what is now the largest Haskell meetup group in the world (1200+ members right now) was to meet regularly and always make sure that we have two talks, one targeted at relative beginners and one open to any level. I think this is important because it creates an environment where beginners can rub shoulders with experienced haskellers and learn. This is a vitally important characteristic for a healthy and growing group because without it one of two things are likely to happen. If you schedule just a beginner talk, experienced users are much less likely to come because it's not worth their time. If you schedule just an advanced talk, beginner users are likely to be scared away. However, once you establish a pattern of having two talks and probably a time for socializing afterwards, then you can occasionally slip and have two advanced talks or two beginner talks and the people will still be pretty likely to come because you've established in their minds that your meetup is a valuable place to go for the socializing alone, regardless of what the talks are. I also think it's important to try to video your talks and post them on youtube, link them here, etc. This serves the dual purpose of increasing the worldwide publicity of Haskell and increasing the odds that people in your area who are interested will find out about your group.
To be fair, that's how many registered (i.e. paper) members there are -- our monthly attendance is between 70 and 110 or so, which is in fact rivaled by London. And while we're the biggest at the moment, we're far from the longest lived, so to keep something going as _long_ as the Sydney group is a different sort of achievement too. And to be honest, I think the most important thing that made us big is that we are lucky enough to be in new york, where tech meetups are _very_ in and tech people are abundant. That said, you're otherwise right on. :-) I'd also add that posting video also is a good way to share interesting talks with a wider audience than would otherwise hear them, which is worthwhile in itself. It also is a great way to attract and encourage speakers, because they know that they will have an eventually larger and wider audience. To me, one other thing is trying to constantly encourage and solicit _interesting_ talks on a range of topics -- finding out what everyone who comes to the group is involved in, and looking for any project or experience of theirs that is potential "talk fodder" or even soliciting talks from anybody at all on topics that you personally would be interested in learning more about, etc. One of the harder things to keep up is that sort of good variety, and establishing an "anything goes" attitude with regards to what constitutes content can be helpful as well.
Added for Ctrl-F people: Sweden.
There's a huge number of [potential Haskell devotees](http://www.meetup.com/OrlandoPHP/) there. Go preach!
if you can install GTK right ? that seems like an engineering feat on macos
inferring is comonadic. we are reaching out for the left part of the arrow in a computation. using monad to express the computation of retrieving the left part of the original computation sounds good ..
You could identify every version "somehow" and then have a massive build bot try every combination and then come up with a build plan based on these results.
I see, so the real order in which monad are wrapped don't necessarily matches the order in the stack. Does that mean that we could implement an *outside*`MaybeT'` equivalent to `Maybe (IO a)` ? (I'll try as an exercise but I'm still interested in your opinion).
If you're looking at the where clause, you're already in the "how does it do it" mode for the enclosing function. Pipelines are limear because function composition doesn't provide branching. As for why I think one way is better, the mental state I was talking about.
&gt; Your system as explained in your second point does not allow for explicit blacklisting of a known bad dependency version any more which is strictly worse than the status quo. Oof. Agreed, this is a definite technical flaw and I'm not sure there's a way to solve it without getting crufty. &gt; It also does not allow for the situation where someone uses only a small subset of a dependency's API and so a vast majority of breaking changes do not apply to them. ...but I'd argue that this is a cultural problem, of packages being too monolithic and/or breaking changes being made too freely. I'll also point out that here my suggestion does no damage: the solution goes back to what it currently is, i.e. downstream maintainer revises the dependency constraint.
Honestly i never tried it
Could you help me better understand this statement? Let's start with "inference means updating weights of a set of mutually exclusive states of belief after observation"; I mean, I don't want to start a holy war here, but I still cannot relate with the concept of a comonad and its practical implications; especially when this requires multiple conceptual leaps such as this case.
I'm quite excited for Megaparsec to be honest.
We've been organizing [Oslo Haskell](http://www.meetup.com/Oslo-Haskell/) for a while now with anywhere from 70 to 15 participants. We've had workshops and talks, mostly targeting newcomers. 
Sydney's FP-Syd is dominated by Haskell programmers, although many of them are forced to write Scala, and we have occasional talks about other languages. There's also a Scalasyd group that is pretty active but I don't know much about them. FP syd has very large attendance, good quality talks, and a yearly Coq fight. We meet every month at Atlassian.
This comment is missing the main point of the blog post: the way we do version bounds according to the PVP is a lossy translation from what we know (known versions it builds with) to version bounds. The argument around PVP bounds boils down to which piece of data we're willing to lose (distinction between known breakage vs unknown breakage, or whether we track known good builds at all). The fact that tracking the right information also happens to be much more amenable to automated tooling is just a really nice perk. I won't delve into the standard PVP back-and-forth debate here, it's simply not worth anyone's time.
Belgium doesn't have many: https://wiki.haskell.org/User_groups#Belgium Seeing as I live near the Dutch border, perhaps I should go to Dutch Haskell group meetups :)
And if you're a bit further North, [Trondheim Haskell Users' Group](http://www.meetup.com/Trondheim-Haskell-Users-Group/) might be more convenient for you. :] We are brand new, and have only had the introduction presentation so far. A beginners' workshoppe series inspired by the one in Oslo is starting up next week or so. «A Haskell users' group in the Trondheim area. For seasoned veterans, complete newbies who only heard of Haskell two minutes ago, and everyone in between and beyond. We meet and talk about Haskell and anything else members are interested in.»
There is a forum of Haskell developers from Turkey: https://groups.google.com/forum/#!forum/core-haskell There is also this society: http://bilfp.wikidot.com There are about 11 developers from Turkey who have an Haskell repository on GitHub: http://github-awards.com/users?utf8=✓&amp;type=country&amp;language=Haskell&amp;country=Turkey You can also find people on the #haskell IRC channel on freenode.
http://www.meetup.com/Intersections-KW/ is about the intersection between pure math and computer science, and often features talks and discussion on Haskell and FP topics.