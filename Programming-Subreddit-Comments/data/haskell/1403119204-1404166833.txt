I guess you could have that in a unit test anyway, unless I am misunderstanding something? But yeah this is all in the setting of writing tests. 
Thank you What do you mean by parameterization? sorry if its a stupid question. Yes i guess i should write it first and see after but since i wont have enough time for 10days, i thought i would try to think about it and ask a few questions before starting. And yes i was more thinking about dynamic add-on, like in firefox or notepad. 
Hmm. In a strict language, if eqEvenList computes the value "eqPair ea (eqOddList ea)" immediately and returns a function that uses that value, and eqOddList does the same, we get infinite recursion. And if eqEvenList returns a function that computes that value on each call, we get a lot of wasted computation. What strict implementation are you thinking of?
Thank you. I'm not sure if what you said answer to what i meant, by plugin i was more thinking about dynamic add-on, wrote by anyone, like firefox add-on or notepad. Sorry if i wasnt clear enough.
`eqEvenList` and `eqOddList` have to take three parameters before they call each other, but `eqPair ea (eqOddList ea)` only gives one. Perhaps eta-expanding the seemingly-recursive case to `eqPair ea (\a b -&gt; eqOddList ea a b)` makes things clearer? As an aside, I rather hope that GHC avoids actually building the partial application, which would probably be accomplished by inlining `eqPair` at each of its uses and then beta-reducing.
The [Hawk](http://github.com/gelisam/hawk) project (of which I am the main developer) uses the [hint](http://hackage.haskell.org/package/hint) library to evaluate files containing haskell code. We do this so that users can extend the environment into which their expressions are evaluated, but I can easily see how the same idea could be used for a plugin system. I can show you how it works if you like.
Thank you ! I'd be really interested yeah! Looking through your source, i dont understand a lot of what's going on(Newbie in Haskell still). Could you give me a little example? 
Ok, so far we have maybe half a dozen responses, most of which are along the lines of "don't use plugins" or "write it first, and then use plugins if you need it." Neither of these addresses the questions at hand, which is one I'd like the answer to as well. HOW do you program for plugins/user implementations in Haskell? Here's my scenario. I want to work with time-varying data in my program, but each organization has their own binary format for their time-varying data. Is there a way for me to design some kind of specification/interface for a data reader and have the user implement it themselves, and then dynamically include that user-defined implementation in my program? This eliminates the need for the program itself to support many many data formats. I feel like this scenario is similar in intent to the OP's original question but with a narrower scope. If plugins are a bad design decision, why? EDIT: Ok, I looked into this a little more for my particular scenario. I believe I can use the the GHC foreign function interface to define the required function signatures and then swap out the dynamically linked dll/so. I have used the SQLite bindings with the sqlite dll before so I will look into the implementation there to see how it's all set up.
&gt; Perhaps eta-expanding the seemingly-recursive case to eqPair ea (\a b -&gt; eqOddList ea a b) makes things clearer? Well, that's what I meant by recomputing the dictionary on each call. I complained that it's wasteful, though I don't actually know how that code would be optimized in a strict language, maybe I'm wrong and it will be fast. 
Why not fully leverage the type system? Something like: data Symbol = Dot | Dash data Sequence = Nil | Seq Symbol Sequence e :: Sequence e = Seq Dot (Nil) t :: Sequence t = Seq Dash (Nil) s :: Sequence s = Seq Dot (Seq Dot (Seq Dot (Nil))) six :: Sequence six = Seq Dash (Seq Dot (Seq Dot (Seq Dot (Seq Dot (Nil))))) etc. In fact, if you do it in the list/tree style like above, you can essentially derive the rest. Morse code characters were given their sequences by frequency, so since 'e' and 't' are used the most, they get shorter sequences than the rarely used 'z' or 'w'. Figure out the frequency mapping, and you should be able to generate the entire Morse code alphabet programatically. You probably also get the `toMorse :: Char -&gt; Sequence` function for free, too. I might re-visit this later for fun to give a full sample implementation for fun.
I'm a total newbie (I've just started to learn Haskell), and I'd also like to know the answer. Just thinking out aloud: how, for example, [Yi](http://www.haskell.org/haskellwiki/Yi) approaches extensibily? It's an Emacs-like editor, so it has to be extensible at its core. &gt; Hooks-Extensibility: &gt; All (good) apps allow users to extend, through, e.g., hooks --- a list of functions that are run before/after some event (like saving a file) I think that using polymorphic data types and various other more advanced techniques (that I don't know yet and of which I'm only vaguely aware) it should be possibile to define and encode the extension points in the types. Since a requirement of yours is also *dynamic loading* of plugins, I find the [hint](http://hackage.haskell.org/package/hint) library quite interesting (see gelisam's comment).
For some definition of _like_ :-)
I wasnt aware of this Yi editor, i'll try to look at the source when i'll be a bit more solid in Haskell! Thank for the input! 
If you want plugins to be written in Haskell, you can use the GHC API to load and link with Haskell modules at runtime, grabbing symbols with `dynCompileExpr` for example. You can also use [hint](http://hackage.haskell.org/package/hint) for this purpose. If you want plugins to be written in some arbitrary language with a C interface, you will have to call `dlopen` (or `LoadLibrary` on Win32) and grab the symbols that implement your plugin API. It all depends on how complex you want your plugins to be and what you want them to be able to do. Outside the Haskell world, you could even embed Lua with [hslua](http://hackage.haskell.org/package/hslua) or design your own scripting language. You have many options, and you can discover the tradeoffs best through experimentation. 
I'm not sure to understand how "loading and linking" Haskell modules would help me... Could you please expand on that a bit?
Thanks, that gist is a good staring point.
 type Sequence = [Symbol] would probably be better, no? *Possibly* a newtype, but still?
I did something similar a few months ago, but rather than making each letter a function, I made a tree of all of them: https://gist.github.com/CodeBlock/9637272 I don't have a practical use for this, but seeing that morse can be encoded as a binary tree was cool and I thought I'd give it a try.
Well, suppose you define a plugin to have an API like this: -- Start up or shut down an instance of the plugin. init :: IO () exit :: IO () -- Do something on the contents of the current buffer. act :: String -&gt; IO String And a corresponding data type for representing loaded plugins: data Plugin { plugin_init :: IO () , plugin_exit :: IO () , plugin_act :: String -&gt; IO String } Then in `hint` you might say: loadPlugin :: FilePath -&gt; IO (Either InterpreterError Plugin) loadPlugin name = runInterpreter $ do loadModules [pluginPath &lt;/&gt; name &lt;.&gt; "hs"] setTopLevelModules [name] i &lt;- interpret "init" (as :: IO ()) e &lt;- interpret "exit" (as :: IO ()) a &lt;- interpret "act" (as :: String -&gt; IO String) return $ Plugin i e a (Haven’t tested this.) Then you can `loadPlugin "SomePlugin"`, call `init` and `exit` as you need to, call `plugin_act` when the user asks for a particular plugin to act, and so on.
There's also a few extra flags to make pretty printed Core more like the source language so it requires a little less manual editing. -ddump-deriv -dsuppress-idinfo -dsuppress-coercions -dsuppress-type-applications -dsuppress-uniques -dsuppress-module-prefixes 
More than labels, the useful aspect of ML languages is the specific notation for type variables using quotes in front of them. I find it quite readable.
Interesting name: https://github.com/Daniel-Diaz/wavy haha However, the library I wrote does much more than parsing WAVE format. Also, mine is not in Hackage (yet) since it's an unfinished project.
Worth checking out http://apfelmus.nfshost.com/articles/fun-with-morse-code.html as well for various interesting ways of implementing morse-code.
As for me, usually, concise code is sometimes easier to read. The functions should always have descriptive names, but the variables can be abbreviated or even ignored. You can leave out variables entirely and let type inference fill them in. The function names and types are far more informative than the variable names. I do write descriptive variable names for functions that are more complicated which may take four or more parameters.
I wrote it in that format to *explicitly* show it in its tree-like structure, so that we could reason about it like /u/CodeBlock did in his gist. It's easy to miss that you can make a nice tree out of it all [0] if you write it out as `[Dash, Dot, Dot, Dot, Dot]` [0] - https://gist.github.com/CodeBlock/9637272
The fpcomplete install of hoogle: https://www.fpcomplete.com/hoogle has everything on hackage. That doesn't exactly answer your question, but I know I stopped trying to screw around with a local hoogle once I found out about it.
Haha, whoops, sorry that I swiped the name. We even started working on our projects at about the same time: https://bitbucket.org/robertmassaioli/wavy/commits/734bd33cb12ece4d88122ba79f81272fbd5a03cb Though I started 10 days earlier...so I don't feel so bad calling dibs. :D I originally intended to have libraries that parse more than just the WAVE file format too; so maybe we should work together. However, I thought that it would be more valuable to release my work as a bunch of smaller libraries so that I could get incremental fixes out of the door. My intention was to continually add more and more libraries to implement all of the various moving parts required for a really good audio library and then unify them in one overarching library. How does that sound to you?
Interesting. The proof was done as part of the work in my maths course (in which I coincidentally used Haskell to compute the function in a similar way to which you have done). I wouldn't bring it up otherwise, but I actually received full marks for that section of the course, so I can't believe that my proof was wrong... I'm fairly hesitant to post it here though as the material for the questions doesn't really change year on year. If I find it I can PM it to you if you'd like?
Defining a lens that actually does get just the next layer works pretty nicely, then you can define your own `zipWith` or whatever without too much trouble.
Sure! Here is a basic example of using hint: import Language.Haskell.Interpreter import Text.Printf -- | -- &gt;&gt;&gt; main -- result is 6 main :: IO () main = do result &lt;- runInterpreter $ do setImports ["Prelude"] x &lt;- interpret "2 + 3" (as :: Int) f &lt;- interpret "(+1)" (as :: Int -&gt; Int) return (f x) case result of Left err -&gt; print err Right y -&gt; printf "result is %d\n" y As you can see, you can use hint to evaluate arbitrary Haskell expressions, *including functions*. This is the key to allow communication between your program and its plugins. First, we need to define our application's extension points and document the format which the plugins should follow in order to hook into those extension points. To keep things simple, let's only define a single extension point, "transform", into which plugins can hook to transform a piece of text in some way. Since hint can also load Haskell modules, we'll say that the plugin format is a Haskell module named "Plugin" defining a function named "transform" of type `String -&gt; String`. Here are two plugin files which follow this format: -- PluginExcl.hs module Plugin where transform :: String -&gt; String transform = (++ "!") -- PluginCAPS.hs module Plugin where import Data.Char transform :: String -&gt; String transform = map toUpper With this convenient format, loading a plugin is simple: simply load the module file into hint and extract its "transform" function. loadPlugin :: FilePath -&gt; IO (String -&gt; String) loadPlugin filePath = do result &lt;- runInterpreter $ do loadModules [filePath] setImports ["Prelude", "Plugin"] interpret "Plugin.transform" (as :: String -&gt; String) case result of Left err -&gt; do print err error "could not load plugin" Right transform -&gt; return transform And now we can load our plugins, combine their hooks in some way, and trigger those hooks when appropriate. -- | -- &gt;&gt;&gt; main -- what is your name? -- Samuel -- hello, SAMUEL! main :: IO () main = do transforms &lt;- mapM loadPlugin ["PluginExcl.hs", "PluginCAPS.hs"] let transform = foldr (.) id transforms printf "what is your name?\n" name &lt;- getLine let transformedName = transform name printf "hello, %s" transformedName In a real application, of course, you would probably obtain the list of plugin files by [looking at the contents](http://hackage.haskell.org/package/directory-1.2.1.0/docs/System-Directory.html#v:getDirectoryContents) of a plugin folder or something. One last, slightly tricky part of using hint is that the *type* of the expression you are evaluating must be imported twice: once at the top of your program, and once in `setImports`. If you need a custom type (for example, a datatype with one field per extension point), you need to put that type in a library, write a .cabal file for that library, install the library to `~/.cabal`, and add your library's module name to the `setImports` call.
Certainly. Also, while Billingsley proves all the properties I had claimed, the proof for just a.e. differentiability is based on the common result that a weakly monotonic function on an interval is almost everywhere differentiable [see for example Theorem 14 at http://terrytao.wordpress.com/2010/10/16/245a-notes-5-differentiation-theorems/ ]. We are still left with the task of showing that the fuction is (at least weakly) monotonic, but this is plausible given its probabilistic interpretation. 
The approach used by xmonad is to have a config file which includes "main = xmonad myConfig". So your program is essentially structured as a library that the user uses to build the version of the program they want, including any additions or changes they desire. xmonad takes care of the compilation when the config file has changed, and provides some DSL-ish parts (for eg, composing window layouts). I'm using the same approach in propellor. For example, see https://propellor.branchable.com/haskell_newbie/ (However, the hint approach looks great for dynamic addons in a program targeting less adept users.)
Ya I was kind of joking, I just say that because pretty much all of the Haskell users I know either have PhDs or are working towards them
It's because of IO buffering. Add hSetBuffering stdout NoBuffering in the begin of main function, it will fix the issue. 
You want putStrLn, see http://www.haskell.org/pipermail/haskell/2006-September/018430.html . It's not due to haskell by the way, any unix program will act the same way if stdout is line buffered.
By parameterization I mean that you take existing code/data, find a part of it, and make it variable -- a parameter that the caller provides, even if it isn't, strictly speaking, a function parameter. So, instead of operating on a particular (single) data type, you operate on an arbitrary (or perhaps with some restriction) or parameterized data type. Instead of operating in a particular monad, you operate in some arbitrary monad or maybe something even more general like an applicative functor. Instead of using a particular way to combine intermediate values, you use a monoid abstration. Each time you generalize, you make an extension point. Now, not all generalizations / extension points are useful but, really I'm still learning that myself. HTH.
So embarrassingly simple that you think "why don't other statically typed languages have this?".
It's called mappend because a list of a's is isomorphic to the Kleene star of a monoid on a
There may be a wall between them, but not in practice, since GHC is built with a specific Cabal library, which makes it difficult to upgrade Cabal and still use the GHC API. There is a ticket for that: https://ghc.haskell.org/trac/ghc/ticket/8244
Take a look at Dyre.
Thank you for telling me advance info! That'll make me more comfortable. I'll check it in detail. 
why not just use hFlush stdout
I think someone was even working on this, but I forgot who. Anyone knows?
Duncan was doing work towards this, I believe. But I'm not sure where it stands on the radar, at the moment (it would be great to factor `PackageInfo` out of `ghc-pkg`, however).
That is a bad idea because it would break a lot of things that expect output to go to the standard output.
This is not because of lazy evaluation. The reason is that your terminal keeps collecting characters to be printed until it gets a newline or the program finishes. When it gets a newline or the program finishes, it prints everything. It does this for efficiency reasons. There are a few solutions: 1. If you change `putStr` to `putStrLn`, it will send a newline to the terminal after every message, so they will end up on separate lines but they will come when you expect them to. 2. If you don't want a newline, but want to tell the terminal "print what you've got already!" you can say `hFlush stdout` after every `putStr`. This will make the terminal print what it's got even if it hasn't received a newline yet. You can also make a function that first prints with `putStr` and then flushes stdout so it shows! 3. If you remove the buffering so the terminal does *not* wait for a newline, every character will be printed as the terminal receives it. This can be done by doing `hSetBuffering stdout NoBuffering` the first thing your program does.
First I want to say, welcome to Haskell, I think you will like it. The Yi editor may be a good starting point for you, although it is a bit large and complex: http://www.haskell.org/haskellwiki/Yi I would recommend first learning how to write an "Editor" data type and use it in a StateT monad transformer, of the type "StateT Editor IO a" and write every API function as a function of this type -- it is a good way to learn. **To address your question,** if you don't mind your that code will depend on the GHC compiler (making it less portable), you could use the "System.Plugins.Load" module in the "plugins" package: http://hackage.haskell.org/package/plugins-1.5.1.4 The API documentation is here: http://hackage.haskell.org/package/plugins-1.5.1.4/docs/System-Plugins-Load.html You can compile Haskell code to dynamically linkable objects and place them in a modules directory. Then you can load them with the "loadModule" function. import MyEditor import System.IO import System.FilePath import System.Plugins.Load -- Lets say you want a syntax coloring function to be a function of this type: type SyntaxHighlighter = String -&gt; [(MyEditor.Color, String)] -- Now lets load a function of type 'SyntaxHighlighter' from a dynamically loadable (plugin) library file. colorExample :: FilePath -&gt; IO [(MyEditor.Color, String)] colorExample sourceCode = do -- -- Get the module handle from a hypothetical module called "PythonSyntax.so" mod &lt;- loadModule (pathToModules &lt;/&gt; "PythonSyntax.so") -- -- Load a function called "colorSyntax" from this module. -- You must specify the exact type of the function that you want with -- the "::" operator. pyColorSyntax &lt;- loadFunction mod "colorSyntax" :: IO SyntaxHighlighter -- -- Now read a String from a file and map the string to the -- syntax coloring function you loaded from the module. fmap pySyntaxColor $ readFile sourceCode 
&gt; Also, if anyone wants to critique the rest of my code, it would be appreciated. Looks pretty good. Some suggestions: input &lt;- getLine let g = read input can be replaced by `readLn`, which does exactly this: g &lt;- readLn and the nested `if ... then ... else` could be replaced by `case` + `compare`: case compare g n of EQ -&gt; putStrLn "Correct!" LT -&gt; putStrLn "Too low; guess again:" &gt;&gt; loop n GT -&gt; putStrLn "Too high; guess again:" &gt;&gt; loop n 
fpcomplete's hoogle is really useful (it have definitions for lens and all the yesod stack). But I found it a bit slow compared to haskell.org's hoogle.
With the last version of Cabal you can build Hoogle .txt file alongside with Haddock documentation. I made a little script to combine them in a default.hoo database : https://gist.github.com/Piezoid/b4602e9d23f6888750ac
In the case of cryptography, it pays to understand what the intent of your code. Does your code need to run in a hostile environment where the attacker can look at the memory, directly time every functions, etc ? or are you trying to decrypt some of your files on disk, or doing some asym crypto signature verifications, etc ? I think in the overwhelming cases, the detailed implementations do *not* matter, providing they do their operations right (which in cryptography is hard to not have different implementations not behaving the same in term of input -&gt; output) "don't implement crypto yourself" is a misguided recommendation, that don't educate anyone about cryptography/security as a starter. There's many differents messages embedded in this: * don't invent a crypto algorithm yourself. absolutely true, the theorical part should remain the property of cryptographer that spend years of research on their algorithms. the level is insanely high here. * don't give an implementation to an existing algorithm. sometimes true, depending on the complexity of the algorithm. some ciphers are easy to implement (for example RC4, salsa, chacha), some others much harders if you don't know what you're doing (RSA, ECC, ...). In any case it really depends what you're going to do with your implementation. are you going to verify RSA signature ? then it doesn't matter if your code is not timing resistant, and swap the memory to disk etc., provided it doesn't answer the verification function wrongly. * don't invent a high level protocol. Usually true, it's hard to get things right, and you should try to base yourself on existing high level blocks (AES-GCM, chacha-poly1305, AES-CTR-HMAC, etc..). * don't implement a high level protocol yourself. There's certain things to looks for and know, but again it depends what your intent is. In general though, if you're not prepared to spend countless hours studying those details, you shouldn't even try to do anything else than toy re-implementations. 
zero padding is usually a bad idea, as it just give you the ability to do text. I think usually a more sensible and generic padding is 0x80 0 ... or NB NB NB NB (where nb is the number of padded bytes)
I'm not really convinced that that's the right position for the firewall. Idris, for example, does `cabal install` on its own, not just `ghc-pkg register`, which means that the package manager (if there were one) doesn't need to know one bit about how packages actually are implemented. Doing more than checking mere dependency integrity and managing packages on a one-by-one basis, locally to the local store, is what package managers should do.
Brand new release. Just wanted to thank everyone that contributed! See [the website](http://fvisser.nl/clay/) for more info on Clay. ---- Edit: Btw, if anyone is using Clay for their site and wants their site listed on the Clay site as an example please e-mail/message me.
I don't think any of his arguments hold up to scrutiny. 1. Seeing a spelling mistake is of no importance since that would be caught by the compiler. We're talking about readability, not writability. 2. Not seeing the shape of the function could make sense, but that is something the editor could help us with. However, the editor cannot help us deduce useful names for type variables or variables in general. So there is a loss of information here that cannot be regained.
The website is really nice. Perhaps a single example could be copied from it to the Clay module docs, so users that only look at the Hackage page can get a better idea of how to use the package?
Sounds like a good idea, I'll do that soon.
The same happens in C, right? If I'm remembering correctly, output doesn't get flushed until you pass 'endl'.
Correct. The actual behaviour depends a little on how your system is set up to work. For example, on my system, this program would print 1Guess a number between 1 and 10: 10Too low; guess again: 2Too high; guess again: Correct! because it would flush the stdout as soon as a newline is printed, even if it is just stdin echoing to stdout.
I tried to update a simple style sheet I wrote. I had a few issues, I switched: "border" -: "0 none" to borderWidth 0 borderStyle none Which is acceptable I suppose but I also had the same issue when I changed "transition" -: "all 0.2s ease-out". I had to add an extra property for the delay. Although I may have to change it back transition "all" (sec 0.2) easeOut (sec 0) give me incorrect css, I think, the output is -webkit-transition : all 0.2s easeOut 0.0s; -moz-transition : all 0.2s easeOut 0.0s; -ms-transition : all 0.2s easeOut 0.0s; -o-transition : all 0.2s easeOut 0.0s; transition : all 0.2s easeOut 0.0s; but shouldn't it be ease-out? Anyhow, is it a limitation of haskell that prevents clay from having multiple property definitions with different signatures? I have a few things that I wish I could add such as flexbox support and some more size types. For example "min-height" -: "100vh" "flex-direction" -: "column" I tried looking in the Transitions.hs to see if I could figure out how to write my own properties and also if I'm doing something stupid with my transition. Really all I learned though was that I know nothing about haskell. Thanks for a neat project. If someone happens to have a minimal example of adding a new property to show me that would be awesome. Maybe this weekend I can dig out my copy of Learn you a Haskell and take a crack at adding flexbox support.
You can define a border in one go with: border solid (px 1) red That `easeOut` is definitely a bug, sorry for that. It wil be fixed, thanks! Flexbox support isn't in there yet, but should be reasonably easy to add. I might add it soon, or maybe do a quick write up how to extend Clay with more properties.
Fixed the easing bug: http://hackage.haskell.org/package/clay-0.9.0.1
Wouldn't it make sense to use a [dlist](http://hackage.haskell.org/package/dlist) instead of a list, as is usual for the `Writer` monad?
Do you have any examples / would this be a good tool for learning the pi calculus? My knowledge of the topic is pretty bare and I would really like a great introduction to "that other calculus" I've put off for so long.
You may have heard that "`(List, ++)` is the free monoid": it is possible to explain the computation of a formula in a monoid, for example `(a * (b * c)) * d`, as first gathering all the elements in a list (by associativity) `[a, b, c, d]`, and then doing a `fold ( * )` of the monoid operation on the list. Said otherwise, the free monoid on the set `M` is recursively defined as `L = 1 + M*L` (this is a recursive definition of the type of lists). You may also have heard that "monads are just monoids on the category of endofunctors". This means that there is some setting in which you can apply the equation above to compute what a "free monad" is: the free monad over the functor `T` is recursively defined as `F(X) = 1(X) + (T*F)(X)` or, said otherwise, `F(X) = X + T(F(X))`. This corresponds to the usual definition of free monads: data Free f a = Pure a | Roll (f (Free f a)) In this paper, the same construction is done for Applicative Functors, which can also be seen as monoids in a slightly weirder sense, and this naturally gives a notion of "Free Applicative Functor" that is equivalent to the one [of previous work](http://arxiv.org/abs/1403.0749) by Capriotti and Kaposi. If you know about the Caley theorem for groups/monoids (you can see any group element as a bijection on the group set), the paper explain how to extend it to lists (this gives difference lists), monads (codensity monad), and applicative functors (something new).
Left append performance... If that's an issue.
Thanks for fixing that bug so fast. I personally would certainly appreciate a write up on extending Clay. I picked up Clay and Hakyll thinking it might be a good way to back into learning Haskell. That hasn't exactly worked out haskell is just too foreign for me to learn through immersion.
The `Monoid` instance for lists has `O(n^2)` complexity. Clay would presumably be used in web apps, where performance is an issue. Would the `[Rule]` AST for a typical large CSS file work out to be a deep forest with usually only a few elements in the list at each node? Or would it be more likely to include larger flat lists? I suspect the latter, in which case you might consider a `DList`. A `DList` is a monoid that collects pieces of a list with `O(1)` append; then at the end you get your entire list. It is typically used in place of the list monoid in the `Writer` monad. Also, a `DList` still satisfies the property of "no clever Haskell tricks" advertised on the site. Basically, a `DList a` is really just `[a] -&gt; [a]` inside.
There are a few code examples in the testing folder on github. I'm on my phone at the moment, but I'll annotate a couple when I'm home. 
The [Haskell Report 2010](https://www.haskell.org/onlinereport/haskell2010/haskellch1.html#x6-100001.1) defines: &gt; At the topmost level a Haskell program is a set of modules, [..]. Since GHC is a Haskell compiler I believe it should take a set modules and produce a compiled program. The purpose of something like "Cabal" should be to provide GHC with this concrete set of modules. What we now call "installation of a package" is caching of the compilation result of individual modules. Cabal allows us to describe the set of actual modules as a function of the environment (platform, user flags, what is already installed (cached), what is on hackage, ...).
Stylesheets are mostly static and will probably be pre-rendered. And yes, large lists are unlikely, because there is a lot of nesting.
_Classical_ Mechanics. :-P I'd also add that the question is if you are more interested in crunching numbers or doing more abstract work in establishing structures and theorems. For the latter, haskell is a gateway to looking at the world of modern theorem proving, using Agda, Coq, etc.
[sigfpe](http://blog.sigfpe.com/) used to write a lot of blog posts about math, using haskell as a notation and reasoning tool. Lately his focus seems to have switched a bit, but almost every post from August 2006 to April 2012 are filled with math, physics and haskell mixed together in a flavorful and informative soup.
&gt; There is a place for fancy techniques, but ye good ol’ abstraction and parametric polymorphism are often all that’s needed I like this way of thinking, parametric polymorphism can be surprisingly powerful. I think that we will discover quite a few more techniques in the future which rely nothing much more than that
No, it only use hoogle database generated by haddock. You have to enable documentation and hoogle database generation in your cabal config before building your packages. I know it's different from what you want, I should precised that earlier. I'm surprised that "hoogle data all" doesn't work for you. Have you tried to do "hoogle combine *.hoo -o default.hoo" in the databases directory ?
This is awesome. Nice work!
What is meant by "cabal install" here: does the Idris compiler know how to download packages from Hackage? That's the place of cabal-install in the (idealized) architecture.
No, it doesn't. It looks for a package description in the current directory, builds the package and installs it. `cabal install --no-dependencies` (or whatever is the right flag) in the current directory, that is. Should've expressed that more clearly, yes.
1. Not all typos in type signatures will be caught by the compiler, especially when we are giving a term a more narrow signature than could be inferred. For instance, we might annotate a parameter `f` as `a -&gt; a -&gt; r` when it could be any `a -&gt; b -&gt; r`, say. 2. You are assuming that 'useful information' is discarded by giving variables short names. That is not necessarily the case when we are talking about generic, higher-order functions--in fact it's the opposite. Using longer names, suggestive of a particular application, can be actively misleading. Personally, I tend use very short names in type signatures, and use parameter names to provide additional documentation if there is more to say about a parameter beyond what is implied by its type. (If there is nothing more to say about a parameter other than its type, I usually give it a one or two letter name, following certain conventions) One thing we perhaps agree about--a good editor could totally eliminate the need for these sorts of bikeshedding discussions. That is, the editor could store names and other documentation as metadata separate from the structure of the code, and we could if we like switch back and forth between different views of the same code and type signatures. But if we have to pick one representation or the other for the tools we have now, the conventions commonly used in Haskell are IMO the right tradeoff. Also, I am not presuming anything about your background here, but in general I've noticed that these objections about naming conventions are raised by people who new to functional programming. In non-FP, higher-order functions (and hence parametricity) is not the norm for most functions in a codebase. Thus, type signatures tend to involve concrete types, and people get accustomed to looking at these concrete types for documentation. When moving to FP, with prevalent use of higher-order functions, functions tend to be parametric, so there's much fewer concrete types. The newcomer used to seeing concrete types everywhere and extracting information from this now asks that quantified type variables be giving 'meaningful' names, often suggestive of a particular concrete application. This lets the newcomer continue reading signatures 'as before'. But what actually needs to happen is the newcomer needs to develop better tools for understanding and reasoning about abstraction and parametricity. Always mentally translating abstractions to some concrete is IMO a mental crutch that impedes the ability to write good functional code. For instance, one often sees newcomers giving overly specific type annotations (and type and parameter names) to much more general code, which harms modularity and tends to lead to code duplication. More generally, one sees newcomers not on the lookout for common, abstract patterns in their code, with code duplication and poor factoring as the result.
Good question -- I'm really a perl programmer and still getting my head round having a powerfu type system. So the honest answer was that I'd not even thought of using the type system. .s and -s seemed too trivial to merit a type... 
Wow Ollie, you are speaking! :)
Wow how did I miss "Functional Differential Geometry"? I knew about SICM but not this... exactly what I've been looking for! Thanks
I find it's a great first project for new language. It involves randomness, input, output, parsing text, error handling (which I don't have in my program yet). To top it off, it's a moderately useful program, especially for beginners.
Wow a big thank you!! I'll try to dig through your post in a few days to test is myself, it look great!
ok i think i've understood that, thank you for the explanation, i'll try this asap!
Thank, i'll take a look
Someone mentionned Yi in this thread before you, and i wasnt aware of his existence. But yeah the sources looks a bit hard for me to understand at the moment but when i'll be a better Haskeller i'll go look at it. Thanks for the tips, i'll try it asap 
I am posting this here because the talk is really about tracking side-effects with types - something I think Haskellers would be interested in.
The line "We developers talk semi colons and curly braces" reminds me of the Picasso quote "When art critics get together they talk about Form and Structure and Meaning. When artists get together they talk about where you can buy cheap turpentine.”
While some of this is true, I am especially "offended" by type signatures that involve 4+ different parameters, where the parameter names require a whole lot of context information. Types with say 6 parameters, called P a b c d e f can have lots of different structures and the same is true for functions. For functions with just a few arguments I agree that the type signature does say a lot. What does this type tell you: f :: X a =&gt; (forall x. a x -&gt; b x) -&gt; Y a c d e f g -&gt; Y b c d e f g I feel that in the Haskell community, a lot of the argumentation around naming comes from the people who write their own code and then reads it. I'm not presuming anything about their background, but that kind of thinking often comes from people who have not worked on software projects with millions of lines of code written by hundreds of engineers. Using examples from the Prelude is one indication that people arguing their position have not seen complex software. Another indication is that people argue based on whether they feel comfortable reading their own code after, say a year. That is completely irrelevant information. It is the time required to become familiar with other people's code that is interesting. It is readability, not writability that is interesting. So what is interesting to measure is the time required to learn libraries like lens, conduit etc. for people that are not familiar with those libraries. Requiring experience with the concepts sidesteps the whole point of readability for a programming language. Again, my main gripe is with types that have many moving parts, many free variables and types with many parameters.
I still don’t know what would be something *reactive*, according to what he said.
 head :: [list] -&gt; first_item This is just wrong. I can highlight a possible answer, though. With some friends, we have a – funny – idea of making a troll language: replacing all the syntax of common functional language with french words (I’m French). Instead of fmap :: (a -&gt; b) -&gt; f a -&gt; f b we’d write (translated to english) fmap is a function from stuff to thing to a contraption of stuff to a contraption of thing That was an interesting experience, because it’s actually the very true way we do read Haskell’s signatures! And it’s fair enough. Let’s fix your example: head :: [elem] -&gt; elem Is this clearer to you? It might be, but it’s actually useless. It’s like writing, in C++, the following: template &lt;class Elem&gt; … instead of template &lt;class T&gt; Since such types are polymorphic, they could be anything. Why would you give them a specific name then? We just want to be able to distinguish types, `a` and `b` are as good as `elem` and `newElem`, I’d rather say `a` and `b` is better because shorter.
the begginng: https://channel9.msdn.com/Events/Lang-NEXT/Lang-NEXT-2014/Keynote-Duality
But then you'd need a way for ghc to ask the package manager questions like "there's an `import Foo.Bar` here, do you know of something like that?", which would mean ghc depends on the package manager, now. Also, `-XPackageImports` should be part of the standard. It of course didn't make Haskell2010 because Haskell2010 was a conservative overhaul of Haskell98.
&gt; Another thing is that naming function 'head' does not automatically guarantee that returned value will come from a head position. Isn't that true of everything? The program can't possibly stop any but the most trivial definitions of things if you are worried about the name matching the implementation.
Unsattisfied with the existing answers, I put significant effort into answering this, so I figured I'd share it with a wider audience.
Your previous link has an extra "posts" in it.
Not bad so far! Sent some feedback on a few specific queries. There are also some spelling errors and such that need fixing.
Hi! Thanks! I'll take a look to the feedback, right now I think there might be still a problem with the db making some result to not appear in for some cases....and about the spelling errors :s, (well, that's embarrassing) I'll try as well to fix them.
Why is the quasi-quoter called qq? It should definitely be more meaningful than that, some like `b` would be more meaningful (somewhat equivalent to 0b100101 in some languages), but also `bits` or `ba` (bit array)
I don't see the point - if it's a wrapper around a base type which supports Num and Bits (e.g. Int8) why wouldn't I just use Int8 and it's Num and Bits instances? I kind of expected by the description to be arbitrary length bit arrays.
Thanks for your explanation. Personally, I prefer saying ???=computation. A monad doesn't abstract away everything. Specifically, operations in a monad are sequenced (before, after, concurrently? Depends on the monad!) according to the monad laws. Computations can be composed, or trivially constructed. The trivial monad is for trivial computation. The either monad is for computation that can halt early. Maybe is a special case where halting carries no additional semantics. The list monad is for ambiguous/simd computation. Where a deterministic computation might be represented as Maybe, a nondeterministic one as List. (Strangely, there's no nondeterministic analog of Either. I guess you'd just lift list into either.) The list goes on. 
So... you're saying that in the expression do x &lt;- foo bar it is wrong to try to relate `x` to `foo`, and that we should relate `x` to `bar` instead? I just call `x` the value "returned by" or "computed by" `foo`. Or sometimes just "the result".
&gt; we should relate `x` to `bar` instead? No, you should describe `x` in the context of `bar`, because that is the scope in which it is bound, and that is the expression in which it is used. Or you should describe it in terms of what `foo` actually is. "Returned by" and "computed by" work for most monad instances, but they're quite a stretch to use for the `List` monad. ---- `bar` in this example represents the "continuation" to `foo`, and it is `bar` which is parameterized by x. In other words, `x` only has meaning where it is used inside of `bar`.
&gt; "This site is still under development**,** please help report**ing** any issues or unsatisfactor**ial** queries" and "concretely querry" These aren't well-formed English statements. "This site is still under development. Please report any issues or unsatisfactory queries." And change the other to: "...related with this query." (I don't know what you meant by "concretely" in that context)
Free-to-access PDF link: http://homepages.inf.ed.ac.uk/wadler/papers/arrows-and-idioms/arrows-and-idioms.pdf Hmm, strange, I thought I've read this, but now that I've looked at it again, I see the secion on "static arrows" which are equivalent to applicative functors... Let me read it in detail and get back to you.
I still didn't get any intuition on what it is * that you can do, when given a regular arrow, that you can't do with a static arrow * that can only be exposed as a static arrow I am looking for intuition on the same level as how everyone knows applicative functors are more static than monads since the choice of side effects can't depend on the results of other side-effecting computations. This immediately gives you answers to both questions: * You cannot choose your side effects based on other results you get from the applicative functors, but you can with monads * You cannot expose something as a monad where you need to be able to analyze side effects without executing any side effects So what are the analogous operations on arrows vs static arrows?
Oops, I tried the type as shown in the header (* -&gt; [*]) and it says "something went wrong".
Looks like no class methods are included, only ordinary functions.
Ooh, this looks really promising. My first thought is that in addition to the module you should display the name of the package that it is from and a link to its hackage page.
No problem! The important thing for you right now is to learn about the StateT monad transformer in the "Control.Monad.State.Lazy" module in the "mtl" library. (MTL means "Monad Transformer Library"). The type of your monad should be one of these two: State Editor a StateT Editor IO a and you would define your own data type called "Editor" in this case. 
So if I understand it correctly, the transformation makes things more efficient because things inside a lambda are not evaluated until the lambda is applied, and by replacing the "regular way" of mapping over a data type with lambdas, we gain a lot more laziness? Is this the described efficiency? For the leafy trees, forcing the concrete version of a Node to normal form would cause evaluation of everything under that Node, correct? And by replacing it with the abstract version, lambdas are introduced, where are these defined? Is it just the newtype? Didn't actually make the exercise because I'm on mobile, but I'm very interested in this. Does GHC do this transformation whenever applicable?
Yea, I'm a haskell noob and just copied it assuming it was an example. :) 
Search `a -&gt; b -&gt; a` finds also `a -&gt; b -&gt; b`. Is it intended?
Of course. They're the same up to argument flipping!
&gt; I kind of expected by the description to be arbitrary length bit arrays. Doesn't `Bits Integer` already provide an arbitrary length bit array?
&gt; somewhat equivalent to `0b100101` in some languages Haskell2010 already supports `0x...` and `0o...` literals for base 16 and base 8 literals respectively. Why can't we have `0b...` as well?
Good functional code consists almost entirely of lots of trivial little functions, like what are in the Prelude. One rarely needs to use anything more complicated if the code is well factored. Writing crazy functions with lots of arguments and/or type parameters is generally considered bad style. If it is really truly necessary, I agree there needs to be good documentation for what things mean. &gt; I'm not presuming anything about their background, but that kind of thinking often comes from people who have not worked on software projects with millions of lines of code written by hundreds of engineers. I know this wasn't your main point, but IMO, there is no project in Haskell that requires millions of lines of code and hundreds of engineers. GHC itself is about 140k LOC, and was basically written by a handful of people, and it's a rather old codebase so I'm sure it could be condensed to fewer LOC if rewritten today. And most software is not nearly as complicated as GHC. If you really have a million line Haskell codebase chances are it's written like Fortran and poorly factored.
Yes, change in the arguments order is supposed to yield the same result. That's no the best thing when you are looking for things like fst or snd;so I plan to add the option to activate/deactivate this.
Promoted datatypes as well as most fancy things are not supported yet. Though it should have said "error while parsing" instead of "something went wrong". Thanks for reporting! I'll take a look.
I'm a newb with Haskell, and I'm *starting* to grok why this would be useful, but if anyone would like to further enlighten me... Is it just cool to search for things, or are there deep, practical reasons for this? I'm suspecting the latter. Fill me (and the other, quieter newbs) in!
It provides specialized instances for `Show` and `Read`. Which make experimenting with bitwise algorithms much more graphical. Since it's a `newtype`, the whole facade is free, since it compiles to just the base type without any wrapper. Also the `Num` and `Bits` instances of the base type are forwarded.
Yes, it does. However there's a problem with treating it like something foldable, since it has no definite `bitSize` and hence no definite length to limit the traversal over the bit positions by. So it actually requires maintaining length in a separate field if you want that kinda functionality. Wait. That's a good idea. I should definitely include such a wrapper in this library. )
You will go through a phase of remembering that a certain function exists, but not remembering what it's called or which module you must import it from. Searching by type signature is an effective way of finding these functions. This can also work for finding out if someone has already written the function you are about to write. (Caveat: just because something has the type you want, it is not necessarily the value you are looking for. But it is a good way of finding a short list of possibilities.)
It's very practical to have such a search, just imagine you have 2 numbers and Want to add them but you can't remember how that add function was named. So you search for what you know: a number and a number Form a third number, in Haskell terms that'd be `Num a =&gt; a -&gt; a -&gt; a`, then the search will give you a list of functions and while reading their descriptions or names you discover that (+) is the one you need. The same goes for even more complex types. I often found the correct function by using hayoo or hoogle this way. 
&gt; but retrieving just exact matches Is this just to keep the implementation simple or is it a feature?
Instead of using `f a -&gt; g (a, b)` for `Exp`, `f (b -&gt; a) -&gt; g a` also works, and matches better with the `Applicative` class.
Looks quite useful! Any examples, though? It's not clear what's the difference between `_`, `_1`, `$`, `$1`, `ANY` and `KEY`, whether parentheses can be escaped for grouping, or whether `--semantic` and `--regex` are incompatible. I did find the [test](https://github.com/awgn/cgrep/tree/master/test) folder on github, but it seems to contain examples of files to be searched, not example searches.
When I saw this, I thought: "Hey, it would be cool to do this in Haskell...". Well, it's already in Haskell /o/
I'm really happy to see a package from Nicola, I think his work on Github deserves more attention :)
&gt; but retrieving just exact matches approximate matches are the killer feature though. being able to generalize [a] to m a , for example.
This sounds amazing. I can't wait to try it out. Seriously, I've been in need of something like this for a pretty long time.
The efficiency gain has to do with associativity of the monoid binary operation. This is more clearly seen for lists: `(xs++ys)++zs` is more inefficient than `xs ++ (ys ++ zs)` because the former has to traverse `xs` twice, as list append `(++)` is linear on its first argument. The same happens in the case of monads. If you consider leaf trees data T a = E | L a | N (T a) (T a) the bind operation `(&gt;&gt;=) :: T a -&gt; (a -&gt; T b) -&gt; T b` replaces each leaf by some tree. In order to do that it has to traverse the tree. Hence `(t &gt;&gt;= f) &gt;&gt;=g` is more inefficient than `t &gt;&gt;= (\y -&gt; f y &gt;&gt;= g)` because the former traverses `t` twice. By changing the computation into the Cayley representation, computations are associated to the right and this inefficiency is avoided.
Indeed, this is also a presentation of the right exponential with respect to the Day convolution. 
Could such a transformation be done automatically by GHC? Seems like a great place to do some rewriting.
If with "bigger systems" you mean non-Haskell projects I agree, the result of compilation does not always have to be an executable, it could be a shared object file or a javascript file or even multiple files. But for Haskell systems I disagree. You never want to compile and install a library package for its own sake (except to make it visible in ghci). You always want to use it as part of a project that compiles to an end result. Using a package should be equivalent to running all preprocessors, copy pasting all (transitive) module files into your project and then running GHC on them. Caching of intermediate compilation results should be an invisible optimization.
Is there a language which does records *right* so I can see what it looks like? I'm aware of plenty of problems with records but not what is considered the "solution".
I think records are pretty much solved for the enterprise. Just use a separate module per record, and derive some lenses. It solves the namespacing issue, and the composition of updates issue. It's a bit more setup in advance and a bit more boilerplate, but that just makes it more enterprise.
Consider adding "difficulty of making a native GUI application" to the list.
Small typo on page 15 in summarizing the properties of coends defining [phi] the A should be a superscript on the integral.
strange cabal hell seems fine right now with sandboxes - and as a enterprise you can always consider using your own "hackage"
Or [travitch/hasksyn](https://github.com/travitch/hasksyn). I'm willing to give this new syntax plugin a try, but just wondering the high-level overview of how it differs from the other ones. (EDIT) just tried it out, and it seems to make better indentation decisions than hasksyn. Data records indent properly, the beginning of multiline strings do too. 
So, uh, not to sound ignorant or anything, but what do each of the items mean?
Can you give me a concrete example of "context-aware"? Maybe I'm just tired atm, but I don't get it.
I would add extensibility too. From what I've seen the final approach from Oleg (and Carrette and Shan) is the best way to address this but I don't see it brought up very often (in books, tutorials etc.). Mostly, I see comments and blog posts telling users to avoid type classes which presents a quandary if you are trying to solve for extensibility another way (is there any other way?).
It's not a technical problem, but the documentation problem also may need a bit of work before I can feel comfortable unleashing Haskell on my coworkers. Playing type tetris for an entire afternoon because a library doesn't provide a worked example is more tolerable when working on hobbyist code, not so much at work.
Cabal sandboxes don't work that well for large complex codebases. If I had one sandbox per project in our main source tree I would have almost 200 sandboxes. I am hoping nix may be able to replace our homegrown system built on shake. 
you cannot even combine some of this 200? (for example I like to use one sandbox for say snap projects)? But yes heard good thinks about shake (did not try it yet to be honest)
Row polymorphism?
I think most of the comments/blogs need to be taken with a certain context. It's very common to hear someone from an OO background translating OO ideas to Haskell by trying to force existentials/typeclasses on very simple types. The thrust of the usual "existential antipattern" advice is that you'll do better by just using initial encodings for simple stuff. At the larger levels of abstraction in a program, typeclasses and existentials begin to win the power/weight tradeoff. Existentials still have type inference issues, but you can ameliorate those in public APIs by CPS transforming them into universals. Essentially, it's not a bad idea to emulate ML modules. Haskell has pseudomodules as well in that you can create abstract data types such as Data.Map.Map, but these don't admit multiple implementations so it's best for library writing instead of plugin writing.
That's mentioned (sort of) in remark 5.5z
You may also be interested in theorem proving: http://www.haskell.org/haskellwiki/Applications_and_libraries/Theorem_provers As far as I know, Agda is pretty popular. http://wiki.portal.chalmers.se/agda/agda.php
&gt; Solving the representation of well-typed SQL Nope. Every language has some kind of ORM-solution which causes more headaches than they solve except for the most trivial of problems. The abstraction is leaky and as databases/caches/keyvaluestores continue to fragment in features and blend this is less and less a good idea. In order to take full advantage of a databases features we have to stop trying to abstract away from the data store, and should just accept that you are going to couple to a particular datastore. There is no such thing as "swapping out the database" for another in real life. Completely agree on the first two, however. Also I was really interested in the [Backpack](http://plv.mpi-sws.org/backpack/) idea but I don't hear a lot about it, perhaps we have another plan to solve some of the same modularity problems a different way?
true, but I don't really find myself doing that for business apps anymore. The web has replaced nearly every native UI.
I think you're making an objection to a claim that doesn't exist. Edward/Duncan did indeed say "well-typed SQL" and not "a Haskell ORM".
`hsqml` addresses this pretty well.
&gt; In order to take full advantage of a databases features Not everyone has that goal.
Who would ever want to disable "turbo mode"? (I'm excited about the direction you're taking this program; nice stuff.)
It's interesting that this wouldn't be my list at all, but that's just par for the anecdotal course. Cabal sandboxes and lenses have addressed the first two points. The issues I encounter when trying to coordinate with a larger development team are that just getting a Haskell environment off the ground somehow tends to be slightly more involved than other languages. If you want to stick to your package manager, you might only have access to an old GHC, so you get a binary of a new one, but then you need to get cabal-install, too. This minor friction rolls so many eyes. Then there can be some trouble with using Haskell code as a library callable from C or C++. This always seems to almost work, but then maybe it complicates the executable linking step because not everything is statically linked in the Haskell lib, or linking capabilities are very different on different platforms. I'm hopeful this will be fixed, if it isn't already, but I've never made it work for a big project. Then there's Windows. 
Is this Meijer's definition as well?
We're talking about enterprise scale, not webscale. :)
Meijer has somewhat rejected the notion of the term "reactive" in his talks lately (because everyone is so confused what it means) and likes instead to talk about what more granular behaviors a system is optimized for. In his Rx library he talks about his API for concurrency a lot; though compared to other systems that are held up as examples of "Reactive" it doesn't do everything those other systems can do - but he does strive to make the effects explicit and detail in the types exactly what the behaviors are. 
Ok, I understand the difference. I find it odd to include this on a list of things things missing from enterprise haskell; other languages seem to do okay in the enterprise without well-typed SQL. In other words, what would this really buy us, and remember we also likely throw out the specific features databases give us with this approach.
Me, for debugging purposes :)
Context are the filters for code, comments and literal enabled for different programming languages... 
Xhevahir has a cabal hell comment. Enterprise software is almost entirely data manipulation. Bring data in, send data out. However Haskell's choices for records puts some heavy restrictions on their use. Most notably there can only be one definition of say "name". Since records are the bread and butter of enterprise logic you either have to use weird names that make your code less intuitive or use some other method to store data. I should be able to say "Give me all the rows where the id is X" with the id being specified as the correct type. This is a very complex problem to solve and something people are working on but haven't fixed completely.
Even sandboxing does not help because big projects pull hundreds of libraries and there's always some clash. Only enterprise repository of snapshotted packages can help with this situation. FP Complete are already trying to roll out such a repository. 
Thank you for your response. If I understand correctly, the issue occurs in situations like the following? data Foo = { id :: Int, data1 :: String } data Bar = { id :: Int, data2 :: String } The 2 id fields collide, causing problems?
&gt; I find it odd to include this on a list of things things missing from enterprise haskell; other languages seem to do okay in the enterprise without well-typed SQL. This is a great point! &gt; remember we also likely throw out the specific features databases give us with this approach. Sure, in the narrow interpretation. If you widen the interpretation to mean "well-typed generation of database queries including those for Postgres, MySQL, Oracle, SQL Server ... not necessarily all with the same API" then it does indeed seem like something that would be very useful for "enterprise" Haskell.
Correct, I believe the compiler will complain that there is a duplicate definition of `id`, one `Foo -&gt; Int` and the other `Bar -&gt; Int`.
&gt; Most notably there can only be one definition of say "name". Enterprise developers are used to this restriction. Try to pull 2 columns with the same name from 2 different tables in one sql statement. You will get the same error. And the solution is exactly the same as in haskell: table aliases and fully qualified column names. In haskell it is module aliases and qualified record fields. 
Enterprise developers are used to dealing with the restriction within the bounds of the current working set. Haskell's case is more akin to removing the ability to use an unqualified name at all, not use an unqualified field in a join. I never said there wasn't a solution to the problem, I said it was an annoying problem. A better solution is one of the several proprosed addendums to the language that allow the ambiguous function to be defined as `a { id :: Int } -&gt; Int` i.e. some record type `a` with a field `id`.
And a common workaround is to do something like this? data Foo = { Foo_id :: Int, data1 :: String } data Bar = { Bar_id :: Int, data2 :: String } (There was also a comment about how modules could be used instead.)
I'm going to try this!
Thank you, that is exactly was I was looking for. The rest of my summer will now entail working through "Functional Differential Geometry".
 fooId :: Int, barId :: Int, But otherwise correct, which honestly is fine for `id`, it just gets harder with `lastName` or `zipCode`.
Your way works, but becomes more cumbersome as the number of types increases. The module way import qualified Module.Foo as F import qualified Module.Bar as B quux :: F.Foo -&gt; B.Bar -&gt; (Int, Int) quux f b = (F.id f, B.id b) allows for types to have multiple records with overlapping names, at the cost of more modules. But since you only have to import the modules that you need, you tend to not get overwhelmed in `Data1_id`, `Data2_id`... `DataN_id`.
I'll read more about monad before looking at that but thank you very much for your great input! :) 
&gt; Then there can be some trouble with using Haskell code as a library callable from C or C++. I'll nod to this as well. It can be done but a lot of the knowledge to make it happen exists only in Haskell folklore and it can be very brittle in my experience.
I'm also scratching my head at the claim that type-safe SQL is *necessary* to make Haskell more enterprisey. I mean, sure, it's a neat feature that could make Haskell *stand out from the crowd*, but it's not like other languages have great type-safe SQL support that we have to compete with in order to be web scale enterprise cloud-ready synergy.
* Solving cabal hell: Stackage * Solving records: lens * Solving well-typed SQL: esqueleto?
I've seen this "final style" thrown around a lot but never clarified, does it specifically refer to oleg's paper, or something more general? I think I understand the paper, but in what sense is one encoding "final" and the other "initial"? (what category are we talking about here?)
Yea, it would be great if the README started off by showing in 2-3 examples how you can use cgrep in the same way as grep, and then moved on in another 7 examples of more complicated things you can do. I would have loved to read about that.
I read the paper before I understood the problem and was left with the impression that it was neat but vaporware. Is there any effort to use backpack on a significant portion of the Haskell ecosystem? Can it coexist with normal Haskell "modules"? Is there an upgrade path?
You'd also have to prefix all the module names with the package name (Containers_2_1.Data.Map). Also, packages have a concept of exposed vs. hidden modules, which you can't easily do this way. Well you could simulate it by putting all the hidden modules under "Hidden." and make sure that nothing imported them, I suppose. At the GHC level, packages are really very simple, it's just the prefixing mechanism described above, module hiding, and stuff to find precompiled .hi files and libraries.
Thanks! Do you mind a question about your implementation? Did you write a parser for every language to produce an AST, or do you use language specific back-ends for this task? (e.g. Clang for C/C++) Or did you take a completely different route?
I always call 'return x' or any other value of type 'm a' a monadic value, implying x is just an 'ordinary' value, in whatever context the monad provides. I call 'return', or any other function of type a -&gt; m a, a monadic function, to remind myself that composing them is done a little differently than an ordinary function of type a -&gt; b. I try not to let the term monad or monadic imply much beyond that since they can be used to model so many effects that don't seem very similar to each other. I'm not claiming any authority, just sharing what has helped my brain adjust to Haskell more easily.
You almost certainly don't want Fay. Haste is okay if you can actually get it to install, but you won't be able to use a lot of libraries. PureScript "just works" and is optimized around this specific problem. Take it more seriously. Idris does a credible job but I don't know what raichoo is up to with the JS code-gen.
Why _almost certainly not Fay_?
&gt; Solving well-typed SQL: esqueleto? Sadly not. * https://github.com/meteficha/esqueleto/issues/40 * https://github.com/meteficha/esqueleto/issues/41 
What about Elm? It's a different language, but at the same time it's targeted specifically at this problem.
I talked to Scott at this years POPL and he made it sound as if there was no real implementation, and that he was only really interested in the theory of the system.
Section 4 gives an example of something one can do with an arrow but not an applicative -- the "freshName" function. I do agree the paper doesn't go the other way to illustrate what you can additionally analyze about applicatives and not arrows, but we can infer that arrow computations can depend on input, while applicative computations cannot, and so we are able to say "more" about arrow computations than applicitive ones -- i.e. we can analyze side effects _regardless_ of input in the applicative case, but must analyze them over the _range_ of input in the arrow case.
&gt; Not even really close until Haste can be installed by normal people that don't know what hsenv is. Don't cabal sandboxes solve this with barely zero user knowledge? I don't know about Haste, but I don't see what would complicate things.
Speaking from experience, cabal sandboxes were *NOT* enough and I had to create an hsenv to get Haste to build (and work. Sorta)
Yup, this prospect makes me very happy indeed
Elm does records pretty well. Actual, extensible records are primitives.
If you want to stay "close to JS" while still getting enough functional syntax and features to make it less painful (and especially if you are interested in generating JS to do rather complex things, as well as just in 'writing nicer JS') I would put in a word for jmacro: http://www.haskell.org/haskellwiki/Jmacro It has been given a pretty good workout in a number of situations. Unlike any of the above solutions, it requires no "extra" compiler, and lets you write your stuff that goes to JS directly inline with your haskell code, as it is a quasiquoted solution. so if you intend to write a "medium" amount of JS as opposed to such a quantity that you _really need_ a full type system (and especially if you intend to use many untyped [and perhaps nearly untypable] external JS libraries), then I think JMacro is a strong way to go.
For the last one: what about [Persistent](http://www.yesodweb.com/book/persistent)? It also mitigates the records issue.
I don't think Elm is really ready to completely take over Javascript (and I don't think that's it's purpose, either). Elm is good for making reactive animations and UIs, but it's not (yet) a viable full replacement for Javascript.
What's the workflow you guys ended up with? I'm less interested in PureScript because "it's not Haskell" but I am curious how the tooling feels. I might use it in another project.
In [the last Haskell Cast episode](http://www.reddit.com/r/haskell/comments/27sh2d/the_haskell_cast_7_chris_done_on_compiling_to/), Chris Done detailed the advantages and disadvantages of Fay, GHCJS, and Haste. My understanding is that there is no clear winner, you should look at the list of tradeoffs and decide which one fits your project. *edit*: Slightly-edited transcript of the tradeoffs listed in that episode: * UHC outputs really large code, and so did GHCJS at the time Chris Done first tried it. * The frontend of the FP Complete IDE (15 000 lines) and the login system and the running system for the tutorials are written in Fay. * Fay outputs very small code. * Fay's output is readable, it's basically Haskell with a bunch of thunks and forcing wrappers. * The Fay compiler itself is small, which is a feature, it allows users to contribute to the compiler with ease. * The Fay FFI is very convenient, which is a big deal because you need to interface with jQuery etc. all the time, and if it's not convenient you will feel like you should be using Javascript instead of Fay. * The Fay FFI makes it easy to use callbacks and native Javascript types like strings and lists. * Fay's FFI is based upon UHC's FFI. * Fay does not have type classes (but will in the future). * GHCJS has type classes. * GHCJS is full GHC Haskell, including all extensions. * GHCJS can compile any existing package, so you can use most existing Haskell libraries. * The GHCJS runtime implementation is quite nice now. * GHCJS supports the full GHC runtime: STM, threads, terminal stuff, blocking IO... * The GHCJS compiler is rather large (it used to be really huge, but now it's just pretty big). * GHCJS-compiled programs are faster than Fay. * Haste is in between Fay and GHCJS. * Fay compiles from the Haskell syntax, while GHCJS and Haste compile from the STG intermediate representation. * Haste doesn't try to be GHC Haskell. * Haste doesn't have threads. * Haste uses an asynchronous way of coding, like normal Javascript. * Haste has the FFI which Fay and UHC use, ~~which GHCJS does not have~~ ([discussion](http://www.reddit.com/r/haskell/comments/27sh2d/the_haskell_cast_7_chris_done_on_compiling_to/ci44oxs)). * Haste outputs small code. * Haste supports Haskell98 + GHC extensions. * Haste does not support GHC's runtime. * All three have debugging overhead compared to languages like Typescript, Purescript, and Roy. This means that if your bug is not in the Haskell code but in its interaction with the Javascript code, then debugging is similar as when using the Haskell FFI with C. * All three have performance penalties compared to languages like Typescript, Purescript, and Roy. If you're implementing an application, you probably don't need tight loops, so the slowdown probably doesn't matter because the slowdown is not that bad, it's not 5 times slower. * You can debug Fay via ghci. * It's quite difficult to use GHCJS right now. You have to get a VM. * Installing Fay is really simple, you just `cabal install` it from Hackage. * AJHC's output is pure C, which can be compiled to Javascript via emscripten, but it's a bit slow.
Actually, there is a way to take your original approach (just combine the naive serializer and deserializer) and still be able to pair them up to generate new derived combinations. This is the API: data Encoding a = Encoding (Get a) (a -&gt; Put ()) -- The trivial encoder unit :: Encoding () unit = Encoding get put mult :: Encoding a -&gt; Encoding b -&gt; Encoding (a, b) mult (Encoding getA putA) (Encoding getB putB) = Encoding (liftA2 (,) getA getB) (\(a, b) -&gt; putA a &gt;&gt; putB b) People who have read [the Applicative paper by Conor McBride and Ross Paterson](http://www.soi.city.ac.uk/~ross/papers/Applicative.pdf) might recognize this API: it's the lax monoidal functor interpretation of applicatives. However, this cannot be made a Haskell `Applicative` because it is not a Haskell `Functor`. You can make it a functor in a more general sense, though: fmap' :: Iso' a b -&gt; Encoding a -&gt; Encoding b fmap' (f . g) = fmap' f . fmap' g fmap' id = id I prefer this approach because it does not require advanced type system extensions.
Initial and final have a very specific, rigorous meaning in category theory, but there is also a colloquial usage of the terms that roughly corresponds to the rigorous meaning. Basically, an "initial encoding" (colloquially) is one that preserves as much of the original value's information as possible (it tends to be a syntactic encoding). A "final encoding" (colloquially) is one that only cares how the value is going to be used, not how it was originally represented (it tends to be a record of values/functions encoding). I'll use a simple example to illustrate the difference. Let's say that I have a `Point` data type with `x` and `y` fields: data Point = Point { x :: X, y :: Y} ... but you have a function that only needs the `x` value: printX :: X -&gt; IO () Now let's say that you and I communicate using some intermediate file on disk. The question is: what should I store in that file? The entire `Point` or just the `X` value? Well, **somebody** will have to extract the `x` value from my tuple for use within your function. If we agree to store the entire `Point` on disk, then you're responsible for getting the `x` out of it. If we agree to store only the `X` value on disk, then I'm responsible for getting the `x` out of the `Point`. If you think of my `Point` type as the beginning of the computation, and your `printX` function as the end of the computation, then a "more initial" encoding (colloquially) is one that is closer to the initial representation, and a "more final" representation (colloquially) is one that is closer to the end result. If we store the `Point` on disk then that is a "more initial" encoding and if we store the `X` value on disk that is a "more final" encoding. Which approach is more correct? Well, there is no right answer. For example, you might say "Obviously, you should store only `X` on disk to save disk space", whereas I might say "I prefer to store the entire `Point` because maybe somebody else down the road might want to extract the `Y` value from this file". You will see similar issues in other initial vs. final encoding scenarios: generally the final encoding is less flexible, but does more work up front (and is therefore more useful) whereas the initial encoding is more flexible since it preserves more information. Existential quantification and advanced type tricks tends to appear a lot in initial encodings, whereas pre-applied functions tend to appear a lot in final encodings. Also, the word "encoding" has nothing to do with the fact that the example was communicating using a hard disk. It's just the very first example that came to my mind. The term "encoding" has nothing to do with serialization or deserialization in this context.
As an Elm contributor, I'm curious: What are some use cases where you think Elm is poorly equipped? In the browser, at least. I don't think anybody is worried about having nothing with which to replace Javascript on the server.
I think the general consensus of learning State/StateT and monads is a good starting point, and I wanted to make an architectural suggestion once you're comfortable with programming with monads. Taking a page from XMonad, you could consider adapting the concept of a plugin being a thread that uses channel(s) to communicate with the main program (main thread). This way, the internal state of each addon/plugin is largely separate from the main program state. This suggestion is reminiscent of Erlang (and Smalltalk) more than Haskell, but it does save you the pain of having to work with GHC dependent libraries. On the other hand, You don't get the nice dynamic loading of libraries/plugins that some of the other suggestions have. You will have to recompile the entire binary every time you add a plugin.
I find that can be awkward to actually use though, because tuples are notoriously awkward to build up and tear down systematically. What you describe is a subset of the approach in the "Invertible Syntax Descriptions" paper, and the authors go further their `ProductFunctor` and `IsoFunctor` type classes, along with their own Template Haskell to derive the underlying isomorphisms. I want to reuse existing TH (hence prisms), so I like the `Category` + `Monoid` approach. I don't find anything particularly advanced in the types here. You can do what I do without poly-kinded categories or heteregeneous lists - that's what `stack-prism` is doing (entirely in Haskell '98), I just prefer (syntactically) the types that you get in the end. You say you prefer this approach, can you show how it would look with my `PairOfStrings`/`ThreeStrings` constructors on the same data type?
When I started learning Haskell I jumped from Vim to Emacs with `haskell-mode` (and of course [Evil](https://gitorious.org/evil/pages/Home) the Vim emulator for Emacs)
&gt; Playing type tetris A potent description of what I spend 90% of my time doing when learning a new library, even a simple one.
I got haste to install and compile... By using the global package db.
&gt; Is there any effort to use backpack on a significant portion of the Haskell ecosystem? Not that I know of. &gt; Can it coexist with normal Haskell "modules"? Is there an upgrade path? It exists on top of Haskell's regular module system, so I presume that it coexists/upgrades easily.
I've been fiddling with GHCJS for a bit. It compiles most Haskell libraries which is a big upside, but it's also very clearly rough around the edges. JavaScript interop feels like C interop instead of the more natural feeling you get with PureScript. I'm able to feel pretty confidently that I can just pretend that GHCJS is Haskell-with-a-weirder-IO though, which is killer. I imagine that GHCJS has the worst compiled size as it has the largest runtime. It has more maintainers than Haste, though, so I imagine at some point it might overtake. 
What was interesting to me is that you can do that with general monoidal products instead of just the product itself.
I've also been using Purescript heavily, and I've been very happy with it. I've used Fay and GHCJS for toy projects in the past. Fay is slow, buggy, and has no typeclasses. GHCJS is still very immature, and still produces huge executables. I'd definitely like to see GHCJS mature, and I haven't spent much time with Haste, but for now, I think Purescript is the most viable.
I can't answer for the OP, but the "usual" tooling comes from the JS side - grunt or gulp for build automation, grunt-init and yeoman for project templates, and bower for package management. On the editor side, there are packages on GitHub for working in vim and Sublime.
The state of the world is not such that we can simply have languages compile to JavaScript and forget about it. JavaScript has 15+ years worth of code floating around out there and it is what we see when we debug in the browser. In other words, the best we can hope for (for the time being) is a language that clears away the broken semantics of JavaScript yet integrates well with it, while producing JavaScript that is legible enough to debug and trace back to the original source. PureScript seems to be leading the way here over Coffee, Fay, Opal, Roy, Haste, and a bunch of others. I guess its just my luck that its also inspired by Haskell, my favorite language, because a lot of the reasons why I use it are based on what I mentioned above which has little to with Haskell, that's just a HUGE added bonus. Also, Purescript is like a foreigner who has learned the native language and customs well enough to blend in and go to parties and stuff. While written in Haskell, PureScript embraces the JavaScript ecosystem by using tools such as Bower for package management and grunt for task automation. Basically, it seems to me that PureScript is not trying to hide the fact that JavaScript exists and is very relevant; it just sucks as far as modern languages go and needs to be replaced as it should have been years ago: http://www.ecmascript.org/es4/spec/overview.pdf... but such is life. One thing I would like to see in PureScript it the ability to load JavaScript modules in PureScript source. This way I wouldn't have to find some alternative way to load JavaScript dependencies when FF interfacing to native JavaScript.
The other answers are great but perhaps I can give a more simple minded explanation. Final encodings are a generalization of this simple idea: you can represent a list by its fold. Take the data type for lists: data List a = Cons a (List a) | Nil And lets construct a list: let xs = Cons 1 (Cons 2 (Cons 3 Nil)) `fold f x xs` is: f 1 (f 2 (f 3 x)) Note how you can compute the fold by syntactically replacing each Cons with f and Nil with x. The idea is now that instead of working with xs, we can work with: let xsFinal = \f x -&gt; fold f x xs This represents the list by a function, which takes as arguments what the constructors should be replaced by. We can always recover the original list by doing `xsFinal Cons Nil`. This idea can be generalized. Take this data type: data Expr = Const Int | Plus Expr Expr | Minus Expr Expr let expr = Plus (Const 1) (Minus (Const 2) (Const 3)) We can define a fold for this by replacing each constructor by a function call: foldExpr constFn plusFn minusFn expr = plusFn (constFn 1) (minusFn (constFn 2) (constFn 3)) Similar to the list, instead of working with the data itself, we can work with its fold: let exprFinal = \constFn plusFn minusFn -&gt; plusFn (constFn 1) (minusFn (constFn 2) (constFn 3)) Edit: note that this is *not* representing each value by its deconstructor that allows you to destructure the outermost element, e.g. for lists you have the following encoding: cons x xs = \consFn nilFn -&gt; consFn x xs nil = \consFn nilFn -&gt; nilFn In contrast, a final encoding is representing the *whole* data structure by its fold: cons x xs = \consFn nilFn -&gt; consFn x (xs consFn nilFn) nil = \consFn nilFn -&gt; nilFn Note the difference! This means that it's not always easy to translate a function using pattern matching to a final encoding, because you have to somehow rewrite the function as a fold. 
How's Elm interop with third party Javascript libraries?
Now you can make a program that guess a number thought by the user.
I would be very happy to see Purescript gain wider adoption. Seems like the most sensible approach to me.
You can make `Encoding` a `Monoid`, too: instance Monoid (Encoding a) where mempty = Encoding mzero (pure (return ())) mappend (Encoding get1 put1) (Encoding get2 put2) = Encoding (get1 `mplus` get2) (liftA2 (&gt;&gt;) put1 put2) Also, you can weaken the argument of `fmap'` to a `Prism'` instead of an `Iso'`: fmap' :: Prism' a b -&gt; Encoding a -&gt; Encoding b fmap prism (Encoding getA putA) = Encoding getB putB where getB = do Just b &lt;- fmap (preview prism) getA return b putB = putA . review prism Then all you need is: _PairOfStrings :: Prism' Strings (String, String) _ThreeStrings :: Prism' Strings ((String, String), String) ... and you can write: fmap' _PairOfStrings (string `mult` string) &lt;&gt; fmap' _ThreeStrings (string `mult` string `mult` string) ... assuming that `mult` is defined to be left-associative.
i think you're overlooking type-safety issues here. Clearly, there will be far fewer type errors resulting from a library which isn't used because people can't work out how to use it[1]. :-P [1] Probably due to a lack of magic "Just follow the types!" pixie dust. &gt; type tetris This is an awesome phrase!
Although I think Haskell generally requires far less documentation because of the fact that you can use this technique and types are partial-documentation, the average Hackage library still falls short of the "download and have something working in 30 minutes" metric. I've definitely thrown my hands up in disgust after spending hours trying to get even the simplest things working from some libraries because the only way to understand it is to like reverse engineer it from unit tests.
Since the beginning of 2014 this has been an area of big progress. Primarily event based libraries - You can define ports at the boundary of your elm components and attach event handlers externally. Both incoming and outgoing event streams are accessible as Signals. Primarily render based libraries - You can wrap native modules to expose custom Element types. The more prominent examples are wrapping the d3 library and the WebGL API.
Can you explain what the javascript problem is exactly?
Note that `Codensity f a` is in some sense "bigger" than `f a` (Codensity of Reader is State, for instance). For more info, and a solution to the "smallest" such transform, see http://comonad.com/reader/2011/free-monads-for-less-2/.
What he's referring to exactly*: http://lmgtfy.com/?q=the+javascript+problem&amp;amp;l=1 Skipping one step for you: http://www.haskell.org/haskellwiki/The_JavaScript_Problem :) *EDIT: "exactly"
The only issue I had with hmatrix was the GPL license. Very exciting!
&gt; it is what we see when we debug in the browser. Not neccesarily. At least with Chrome you can supply a .map file and have the debugger show you the origian coffeescript or whatever. 
The 'a' in 'm a' is the result of the computation. Not all computations have results, some have multiple results, and some might even consume their results rather than produce them -- e.g. if the computation is modeling a flow of information backward through time, such as in the reverse state monad. 
How does this work with sandboxes? I've always held off from using GHCJS as I don't want to install packages globally. 
If you have programmed JS for any real project you know what the problem is. Basically current web/mobile applications can have a nice Haskell back end, but these days the frontend often is larger in code than the backend and will contain 100s of 1000s of lines of JS. Some projects I did lately were 70/30 frontend/backend and it is very frustrating to do that in 2 different languages, especially when one of them is as clunky as JS. 
That sounds scary. 
I have been very impressed by Purescript as well. It is strict and uses the JS primitive types which makes integration in the browser, debugging and FFI a breeze.
Could you hack it together with the FFI? foreign import data Example :: * foreign import example "var example = require('Example');" :: Example I haven't tried this; just thinking out loud.
Why not make it a `Profunctor`? data Encoding a b = Encoding (Get b) (a -&gt; Put ()) Having two type arguments rather than one is often a neat way to escape from invariance. Then you'll notice there are some additional useful combinators unit :: Encoding () () product :: Encoding a b -&gt; Encoding a' b' -&gt; Encoding (a, a') (b, b') void :: Encoding Void Void sum :: Encoding a b -&gt; Encoding a' b' -&gt; Encoding (Either a a') (Either b b') (The `Get Void` will always fail to parse). I've been calling the former `ProductProfunctor`, and the latter could be `SumProfunctor`. Because they are actually `Profunctor`s you no longer have to deal with *just* tuples. You can `lmap`/`rmap` the type arguments to anything you want. They are a bit more awkward to deal with than just `Applicative`s though.
Could you elaborate on its shortcomings in type safety and limitations? (truly interested)
&gt; PureScript seems to be leading the way here over Coffee, Fay, Opal, Roy, Haste, and a bunch of others Why? How would you compare it to Fay? It seems to me that both of them are almost equivalent, only that purescript allows for more type magic while Fay allows for code sharing. What are your experiences with building apps in purescript? Would you use it in production? What framework would you recommend to use it with? I only found [react bindings](https://github.com/purescript-contrib/purescript-react) which are alpha.
whenever you want non-trivial sql, e.g. cte's and afair even calling functions in postgres does not work when using named parameters.
Oh, that's great news! GHCJS looked like it was the most appropriate for me, but I was really turned down by the VM thing. I'll try it out!
So in summary, yes GUI programming is easiest if there's a lot of mutability under the hood, but `reactive-banana` and other FRP libraries give you a safe functional interface on top of that mutability.
And I would have expected interop with existing Java-libs/frameworks to be the #1 thing missing for enterprise acceptance... as in today's reality, the term enterprise seems to be a synonym of "runs on JVM".
While I am not familiar with gtk2hs itself, the situation is common whenever a library exposes bindings from C. In an imperative language, it is common to register a callback, and for this callback to maintain a state in between calls via some mutable variables. In a library whose role is to expose this existing API to Haskell, you will also need to register callbacks and to maintain a state. If you decide to use such a library, just do what we always do with IO: keep the imperative bits on the outside, and use them as a thin wrapper around a pure core. In addition to those raw bindings, there are often alternative libraries exposing a modified API which is more suitable for the language. In object-oriented languages, that would mean packaging a bunch of procedures inside a class, and in Haskell, it often means abstracting over similarly-named methods via typeclasses. The IO is usually kept as IO, though, because there is no formula for turning an imperative API into a pure API. In the case of GUI programming, you are lucky because many researchers worked on the problem of re-imagining how GUI programming could be expressed in a purely-functional way, first with [Fudgets](http://www.altocumulus.org/Fudgets/) and then [FRP](http://www.haskell.org/haskellwiki/Functional_Reactive_Programming). I think what makes this re-imagining work so hard is that once an imperative way to do something becomes entrenched, it's hard to think of any other way of doing it. For this reason, GUIs and games are often described as inherently stateful, even though FRP and initiatives like [PuzzleScript](http://lambda-the-ultimate.org/node/4869) show that by thinking outside the box, alternative formulations are possible.
It works horrible compared to vim2hs: http://imgur.com/pa4O8gk,9BPUKCi (first image is vim2hs, second is this one) I tried all three plugins and to me it looks like vim2hs is clearly better that the other two. EDIT: I didn't test them for indentations because I don't care about indentation support. Most Haskell projects have their own indentation rules (like do in newline vs. do in same line with `=`, where to put `where`, where to put `in` etc.) so as long as I'm automatically indented to same level as previous line when I move to newline, I'm fine putting extra indentations manually.
Note to self: Always provide examples, and use colors for them. People with love it. Note to author: The link to the examples has an extra slash.
I had not any problems whatsoever with installing haste 
The link to the code examples is broken (It's /~/alberto in the url instead of /~alberto). They look good though!
There are threads about this regularly. [Previous](http://www.reddit.com/r/haskell/comments/1zt8t6/front_end_haskell/) Also my vote goes to Haste. Full haskell (full = if it cannot install some package from hackage then either it's a bug or that package is system-dependent) and small js on output. But yes, it is not popular, a pity. 
I think his point is that the term is fairly meaningless, and that it makes more sense to talk about each implementation of "reactive" separately. This presentation is about his reactive library, Rx.
Hey, thanks for the extensive reply. This approach also seems very interesting. I went with FRP and reactive banana initially because the author made it extremely easy to understand how to use it (and you hadn't written this comment yet). I'll try to build a small MVC example to see if I can figure out how this works. Curiously those links of yours were already purple. I guess I hadn't realized when I read those pages the first time that it would be applicable.
Hm, I don't actually know, but it seems that it should work just fine. I am going to try using sandboxes with GHCJS 
Making illegal states unrepresentable is mostly an orthogonal thing, but my anecdotal experience is that final encoding requires fewer advanced type system extensions to avoid illegal states.
cartazio will be proud
Is GHC not on its way of solving this problem though with [overloaded record fields](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Design)? As far as I know this should be available with GHC 7.10. Maybe even as soon as next year?
Cool, are you planning on automating the process of creating GHCJS Hackage database in any way?
I would say that is a fair assessment of how I felt when I first started reading this sub. Sometimes still do :). Come for the Haskell, stay for the never ending mind expansion.
Yep, I already have the scripts to mirror hackage for http://hdiff.luite.com/ , they just need to be modified to check the yaml files in the shims repository ( https://github.com/ghcjs/shims ), to see what changes are needed. Then getting a package updated will just require sending a pull req with the yaml file there. The file format can already handle version ranges ([Gen2.Shims](https://github.com/ghcjs/ghcjs/blob/master/src/Gen2/Shim.hs#L161)). I think we also need something to apply patches, and a mechanism to alert admins when a patch fails or any of the `c-sources` files in a new version have been changed. But nothing's set in stone, I'd be happy to take suggestions if there are better ways to handle the transition. 
No, because Codensity isn't always a win. Such transformations in essence make the tree faster to *construct*, but you'll pay every time you use it. Consider a difference list, which is essentially the codensity transform of a list - appending is now fast at O(1) (it's just function application), but every time you need to *use* the dlist you have to 'flatten' it to get the resulting final list. So if you're going to reuse the result, it's not a win if you pay for it every time.
I'm pretty sure that /u/sigfpe [knows what a free monoid is](http://blog.sigfpe.com/2014/04/the-monad-called-free.html).
Don't worry, you don't have to understand everything on this reddit to work with Haskell. Though there are also pretty reasonable threads from time to time, like practical experiences or tools written in Haskell (see the cgrep one recently, or the hspec for attoparsec). But of course, making the effort to understand the more obscure threads can be hugely rewarding, bringing you ideas that will expand your mind and your skills as a programmer.
I'm still excited about my stuff though :-)
I remember being thoroughly confused about pointers when I first tried to learn C/C++.
Do you really think we're going to tell you to quit because you don't understand everything right away? Isn't it always sort of like this when you learn something new?
Ooooh kay, /r/vxjunkies is a joke or satire, correct? In that case, rest assured, Haskell is not. If you already have some programming or even better, CS background, learning FP through Haskell with its strong abstractions and idioms will alter the way you think about other programming languages. *[If you don't have a CS background and want to learn just another language, that'll probably work too. But I suggest that you take a more abstract approach and first learn about programming languages in general (e.g. Programming Language Pragmatics by Michael Scott)]*
yeah the world is waiting xD
I don't think he is concerned that haskell is a joke. The way /r/haskell and /r/vxjunkies is similar is that both uses a lot of fancy words like monomorphism restriction and Hans-Rodenheim Law of Vectoral Momentum which sometimes makes conversation sound like incomprehensible technobabble.
it is on github already mind you, just still prealpha! (once I finish a few more todos, i'll be declaring a hard to use public alpha :) )
You may also want to have a look at reactive-banana's sister project [threepenny-gui][1]. There, the FRP functionality is more tightly integrated with the GUI stuff. [1]: http://www.haskell.org/haskellwiki/Threepenny-gui 
I'm always surprised that [js_of_ocaml](http://ocsigen.org/js_of_ocaml/manual/) doesn't get more love. It's a very robust compiler for the whole language (I've never experienced a compiler bug), the bindings for the native Javascript libraries and types (Dom, WebSockets, etc.,) are very good and the FFI is a pleasure to use.
&gt; and it is what we see when we debug in the browser. What we see in the browser is minified js, indistinguishable from noise. So the web invented source maps, which should work for haskell as well.
Yep I'm quite happy seeing tools like this appear for GHCJS. I think this will eventually eventually make GHCJS a much better tool for bigger projects, in particular for things like FRP, where it's often hard to reason about memory consumption or easy to make mistakes. Thanks for the hard work so far, and for putting up with the sometimes messy or still undocumented parts of the compiler! Now for the second half we are going to need something to display the statistics in the browser. Is there a good JavaScript library that makes it easy to do interactive breakdown/drilldown of structured data? We start by determining the live set of all reachable heap objects. Then we have the following data for each (sampled) point in time: - heap object - allocation time - first use time - cost centre stack that produced the object - module that produced the object - closure descriptor - cost centre stack - cost centre (top of the stack) - parent (unless it's a root) - cumulative number of objects allocated (per closure descriptor) - cost centre - module - name / source location - closure descriptor - module - closure type (thunk, data, function, caf or not-caf) - type (for example `Either` for the descriptors for `Left`, `Right`) And there's a bit more information about primitives (`ArrayBuffer` objects used as storage for the primitives underlying `ByteString` and `Text` for example). It would be nice to allow users to quickly answer queries like "break down data of type T produced by cost centre stack CCS in the parts older/newer than t0" and open a live-updating plot showing the breakdown over time. Some data cannot be calculated from just the initial traversal of the heap (retainer profiling), other data can only be fully determined in retrospect (biographical profiling), so the system should have enough flexibility to allow this. In the future we might come up with more data in this category, perhaps data reachable by a particular group of threads, or by the CAFs in a particular module. Unfortunately we cannot yet write the GUI in Haskell and run it on the same runtime, since it would produce its own data and interfere with the measurements, but if there a good library that does most of the work, then having to write some JavaScript might not be so bad (and otherwise we can always modify the GHCJS runtime to make it possible). So suggestions for libraries and approaches (what kind of user interface, how to show hierarchical breakdown over time, how to let users enter queries) are very welcome!
I'd put "testing functions in the IO monad" first on the list. Unlike most other languages, haskell libraries hard-code use of IO which means that mocking a submodule that uses IO in a large system is impossible. In Java for example, a large system written using modern dependency injection can be tested by, for each module, mocking all its dependencies. Haskell libraries are similar to the Java 1.0 JRE where no interfaces were used.
Well, `($)` is really `id` with a different fixity!
The line: [d] &lt;- map read `fmap` getArgs is binding the value that `getArgs` will supply at run-time to a pattern, `[d]`, that will be in scope for the following line. That pattern says it matches a single-element list and binds that one element to the variable `d`. This is just a normal pattern match, there's nothing special about it aside from a bit of the `do` sugar. You're getting a match failure, though, because the pattern is explicit about wanting a *single-element* list, and as you've passed 3 of them, the match fails. Instead, you could use `[d:_]` as the pattern to allow extra parameters but ignore them. As for this bit: map read `fmap` getArgs First thing to do when you get something like this is figure out the associativity and binding strength of the operations going on. The `fmap` in backticks is a normal function transformed into an operator, and it's going to bind less-tightly than the function application in `map read`. So, first `read` will be applied to `map`, yielding a new function that expects a list and will apply `read` to each element. Next, `fmap` will be applied to that function and `getArgs`. It has the effect of *lifting* the mapping function into the IO monad (due to the fact that IO is also a Functor) so that it will be applied when the IO action executes to the command line arguments that `getArgs` reads from the environment. The end result, when you pass a single argument that can be read as a Double on the command line, is that you get a list of a single Double from the mapping of `read` over `getArgs` and it's available (sans list wrapping thanks to the pattern-matching) as the second argument to `mean` on the next line. The form that the `mean` function takes is what's known as *worker-wrapper* form, which ghc is especially good at optimizing. The recursive call needs an extra accumulator parameter; instead of making the user supply it, it's tacked on by the *wrapper* part when the first invocation of the worker (named `go` here, as is common) is made. So, when the program calls `mean`, the first parameter is always `1` and the second is the one you passed on the command line. That means that the worker `go` is always going to start with an invocation that looks like `go 0 0 1`. You'll notice that the parameter you pass in isn't even used in this first invocation! But that's okay; it remains constant through the recursive calls, and although the variable occurs free in the body of `go`, it's bound in `go`'s lexical environment, namely by the binding `m` from the parameter bindings to `mean`. So, just to reiterate: The two `Double` parameters in `mean`'s signature refer to the `n` and `m` bindings in `mean n m = go 0 0 n`. The `Double`, `Int`, and `Double` parameters in the signature to `go` refer to the `s`, `l`, and `x` parameters that get their initial values from the `go 0 0 n` part of the wrapper. Also, to clarify: There's no *fusion* going on here. This is just a straight recursive function. Fusion refers to eliminating intermediate forms when you apply multiple higher-order functions to a container-like object, such as a list or `Vector`.
Unless you work somewhere really awesome and fun, "Enterprise" and "CRUD" are synonyms. The benefits of using Haskell to write "Enterprise" software are somewhat underwhelming if all of the CRUDdy parts are as unsafe as every other language.
&gt;Instead, you could use `[d:_]` as the pattern to allow extra parameters but ignore them. Shouldn't that be `(d:_)`?
Why isn't it Data.BitArray?
Thanks for the response. If the program takes a single parameter, how do you call it in such a way that it actually calculates the mean of a collection of doubles? EDIT: I think I figured this one out, I assumed it was taking an arbitrary list of doubles from the command line when instead, it's just calculating over a range, which makes that part far less cool. Still some interesting stuff going on here though. . &gt; Also, to clarify: There's no fusion going on here. This is just a straight recursive function. Fusion refers to eliminating intermediate forms when you apply multiple higher-order functions to a container-like object, such as a list or Vector. I put that in the title because of this line from the article: This is a fusion transformation, instead of two separate loops, one for generation of the list, and one consuming it, we've fused them into a single loop with all operations interleaved. 
Whoops, you're right. That's what I get for posting code without checking it.
Ah; it's describing *manual* fusion from the previous versions that used lists. It's actually an optimization that can be done automatically in some circumstances, which can lead to clear and efficient code. Anyway, I think you've got it figured out now.
If you minify/closure compile your JavaScript during development (as opposed to production) then the noise you see in the browser during development is no other's fault than your own. ~~So your comment is indistinguishable from nonsense.~~^Apologies
I love Haskell. I think it is wonderfully simple. I do feel that the ill-chosen names are sometime very confusing. I say "ill-chosen" because the names of the patterns to which I refer are taken directly from the mathematical concepts from whence their implementations were inspired. This needn't be. F#, for example, has chosen the name "Workflows" for a concept similar to Monads and "Computation Expressions" for the "do-style" syntactic sugar on top of them. This serves to limit the chest-beating and endless mathematical pontificatation* that one finds when researching a solution for a simple problem in Haskell. It's funny how, for example, Monads, an interface with two methods and 3 guiding laws (of which we all became familiar with in elementary school), merits so much discussion. *Not that all of such material can be categorized as such... but yes, sometimes it goes way to far.
:V
&gt; limit the chest-beating and endless mathematical pontificatation I'm genuinely interested in why you perceive the use of names as implying these things. We use terminology specific to the domain, for example nobody criticizes doctors for using Latin names and I don't see why computer science should be different. The terms are chosen to disambiguate, not to intentionally confuse or exclude.
I really don't understand complaining about the use of precise names. Do you go to data structures people and complain that "trie" and "graph" are terrible names because they're either made up or don't match your existing intuition of what those terms should mean? No. You just learn what they mean and move on, happy to have learned something new. But then when it comes to Haskell, you reject the decision to use pre-existing well-defined names for patterns? Why is that?
I don't know about java, but in c#, mocking a static call that connects to the database is not trivial. You most certainly want to refactor your code to inject a dependency in order to test it. The haskell IO layer is not better than any other language in that respect. It should be as small as possible, and you should be careful about your dependencies. Haskell does not hard-code IO calls. You are the one doing the coding.
I get the opposite impression. Whenever I was on the other programming subs, everyone comes off as dry and half dead. This sub, personally, feels like it's filled with people willing to help with a smile on their faces, and legitimately want to help. That's not saying /r/learnprogramming or /r/learnpython aren't helpful, it's just that this sub seems to be full of happier people. I'm just stating to get serious at learning programming myself, and Haskell feels like the right fit. 
Now roundtrip please; it wouldn't be too hard to include a database backed backend with this kind of system, also Haskell ofcourse. So you can use the same lib to hook up the backend. Finally a killer application purpose for Haskell. Edit: Disclaimer: I never built Haskell web apps; does it exist? Haste does something like it minus the storage backend. If it does exist they sure kept it secret. 
I prefer non-natural primary keys in most situations. Natural and compound keys are hard to get perfectly right. SSN's get reused, postal codes (in my country) do not map 1-1 to a municipality, etc. But how does this destroy the ability to use lookup and reference tables?
Very interesting! Reminds me of attribute grammars like in UUAGC.
Haskell is worse because libraries dealing with IO do not expose a mockable typeclass. This includes base. This means there is tight coupling between libraries, unlike in enterprise java. Mocking an SQL backbend has never been a problem I've seen in java based systems. The database is typically an interface in java, not a concrete implementation. &lt;rant&gt; Edit: Just one example, but basically all haskell libraries are like this: The etcd package which is an interface to the etcd service https://coreos.com/using-coreos/etcd/ cannot run any tests without the etcd daemon running. The library uses http-conduit library. Again, that library has no abstract interface. Everything is hard-wired like 1990-style C. If you look at the tests for http-conduit, a real server is created in order to do tests. That means that the TESTS WILL RANDOMLY BREAK (the test tries to pick a "random port" that is available, but that is racy. I've done the same mistake and running the tests 1000 times on busy servers WILL BREAK). So http-conduit isn't testable, and etcd isn't testable. It is like a disease and the root cause is that no library is mockable. Thus they cannot be tested, or they can be tested with great pain, and the painful tests are fragile/non-working. Fragile tests will be disabled when run in large-scale settings. &lt;/rant&gt;
Very interesting, but am I the only one scared by the complexity in the example TODO app?
I have to agree. A nicer syntax would be welcome!
This reminded me of something I have been wanting to implement for some time in GHC: [Support for binary literal syntax](https://ghc.haskell.org/trac/ghc/ticket/9224) as known by Python/Ruby/Java et. al: GHCi, version 7.9.20140622: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Prelude&gt; 0b11001001 &lt;interactive&gt;:2:2: Not in scope: ‘b11001001’ Prelude&gt; :set -XBinaryLiterals Prelude&gt; 0b11001001 201 it :: Num a =&gt; a Prelude&gt; 
I think most Haskell programmers have felt that way at some point. Keep writing code and reading! A thing I've noticed, both about Haskell and other things - reading things that you don't entirely understand is actually more helpful than you'd think. Some of it sticks around in the back of your head and makes it easier the next time you run into the concept. A lot of my Haskell knowledge was soaked up slowly rather than learned from a single source.
Oh, I had trouble with unsigned and signed ints of various sizes. But I managed, in the end!
I don't even find it comfortable, but I know it's worth it in the end.
&gt; We use terminology specific to the domain, for example nobody criticizes doctors for using Latin names and I don't see why computer science should be different. Well, if my doctor talked to me about the bone in my upper arm and called it *humerus* I'd be confused as well. It's partly about adapting your language to the people you are talking to. That being said, it's useful to have a specialised vocabulary when talking between two people in the field, and you'll eventually have to learn the words.
Probably because when learning Haskell, people who have been programming before expect a certain familiarity, and they are met with strangeness. They do not only have to learn the syntax of the language, they also have to learn a strong, static type system, immutability, the functional paradigm *and* as a cherry on top, lots of new words for design patterns. I guess the words just become the scapegoat because language is so important for us to feel at home.
I started in gw-basic and Qbasic in '91, and found none of it confusing. I've worked in about 30 languages over the years, but Haskell is quite another thing. I recall taking to javascript and Flash Actionscript almost immediately back in '99/'00. It felt like I was made to think that way. I would make up things, and they would just work. I never felt like Perl was difficult (though looking at my old code later sure was!). Clojure has been amazing, and I've managed to guess at a bunch of things in there, too. Common Lisp before that was fairly straightforward and fun. I went start to finish through "Teach Yourself C++ in 21 Days" many years ago, and a bunch of Zed Shaw's C tutorials recently - both super easy and fun. I read through "Assembly Language: Step-by-Step" a decade ago like it was a novel, then closed the book, downloaded a compiler, and wrote out a little program that worked. I've used Python over the last 4 years, and it's also super simple. Bash was easy enough, though somewhat messy. Haskell has not been like anything I've played or worked with in 22 years of writing code. I'm not saying that's bad - Haskell may be loaded with powers and 'correctness' and 'reasonability' that I've never had before, and I think it's probably helping me make small inroads into the land of mathematics - but it's hard for me to compare all those other languages with Haskell. I agree with the OP - I rarely have the foggiest idea what in the world anyone is talking about in here. Granted, LYAH - which I'm only partway through - has felt pretty accessible, but Haskellers use *so much* language that I've never encountered anywhere else - functor, applicative, monad, monoid, "*the* Prelude" (why *"the"*), prisms, lenses, Kleisli arrow... The links are often completely opaque to me, and I usually have [no idea what the comments mean](http://www.reddit.com/r/haskell/comments/28ju9t/notions_of_computations_as_monoids/ciblbit). You guys don't talk about the git-'r-done work of programming day jobs. You converse over the merits of computational theories and the various mathematical truths of the universe. Admit it. You also all often say "It's not that hard, don't worry," and "Oh, that's just this - that's all there is to know," but it feels really weird that you give big, important-sounding names to things that are so simple they hardly deserve them. Then I find out something I thought I had a grasp on goes waaay further, so now I never believe anyone who says "Nope, that's all there is to that thing with a name that doesn't sound like it's that simple in reality." It's like today's /r/programming link on "The Lambda Calculus for Dummies"; it didn't seem all *that* simple, and it said "That's it, there's literally nothing else you can do," and then commenters were like "Yeah, that's not exactly correct. It actually goes a lot farther." Sigh. In short, where all the other languages have felt [this hard to learn](http://macappstorereview.com/wp-content/uploads/2011/06/Time-Mac-App-Store-Review.jpg), Haskell has felt [this hard to learn](http://37.media.tumblr.com/4d54bccaffff62b3b2a49ef3febab9f0/tumblr_meoggqRFdV1rndo89o1_1280.jpg).
&gt; sometimes Sometimes?
I can answer that. It's the sheer volume. I've explained tries to three different programming newbs, and in 2 minutes they've all said "Oh, okay." Meanwhile I've been trying to grok monads since last year. Questions always lead to more and more, ever deeper questions in Haskell, in the way that maths links on wikipedia lead to click after click, backward through ever-simpler concepts, and both suffer the same issue: I never make it back from the depths. And meanwhile, I have yet to feel like I've really grokked something in Haskell. Understanding of any kind always feels shaky at best. I always sense that there's much more beneath what I sort of maybe know. My programming friend who gets it has often said "And that's it. So you get it now," but I don't. He says "That's all there is to it. Go watch Beckman's monads link again and you'll see that's all he's saying," so I go watch it for the 4th time, and I see nothing at all of what we discussed in there, and still have no idea what's going on past the 30-minute mark. There's not a single common thread between any of the monads info out there. They're all talking about completely unrelated things, it seems.
&gt; the community isn't very advanced Sounds like a big advantage for me. May be these folks will finally build something useful out of FP :)
A friendly reminder: Always provide examples, and use colors for them.
Interesting. But this convention proposal is very debatable. And therefore is not used widely, at least it is not used in notable packages. And yes, personally I like Data.BitArray more than just BitArray. It's easier to find and to reason about the module just by looking at its name.
How long have you been reading this sub?
However, it kinda seems like a step backwards. We lacked hierarchical modules in H98, which forced to use flat module names such as `List`, and with Haskell2010 we finally had officially hierarchical module names, allowing for more structured module names such as `Data.List`.
&gt;It didn't seem all that simple, and it said "That's it, there's literally nothing else you can do," and then commenters were like "Yeah, that's not exactly correct. It actually goes a lot farther." Sigh. That's the basis for a sustained interest in Haskell. I thought I understood functors as much as they offered to be understood, but then I saw [F is for Functor](http://www.reddit.com/r/haskell/comments/2327e7/functional_pearl_f_for_functor/) (on reddit), and realized I was very wrong. I would have given a big groan six months ago to find out how superficial my understanding is, but now I'm not bothered at all. Experience showed me every time I learn something like this, my code gets better, I have more fun writing it, and other previously opaque topics get closer to being within reach. I didn't fully grok functors? Great news! Haskell's selling point is that there will always be new useful ideas, and the company of other people that like that. Keep reading LYAH and some of the words that sound exotic now will feel no weirder than the lingo around other abstract concepts ('market', 'integral', 'cause', etc.). And then you can find a lot of fun in listening to hackers &amp; PL researchers thrash out new ideas :)
... and a list of problems described in the post I linked to. There's no back step about this convention. It's dead simple: the root namespace of a package must accommodate to the package name. Under that namespace you can have whatever the hierarchical structure you want. Since the `bit-array` package is single-module, hierarchy is redundant, hence it's all placed in the root module. 
Me too. This begs the question: why deal with all that complexity, translators, deployment stuff to get even more complex code? I know i sound like old "asm vs c" oldfag here, and maybe there's only so much upfront cost to make a correct and scalable system but this isn't apparent from examples and it should be.
&gt; It's easier to find and to reason about the module just by looking at its name. Right! But how is finding `Data.BitArray` easier than finding `BitArray`? Is it `Data.BitArray` or `Control.BitArray`, or maybe `System.BitArray`? Yes, concerning clearly a data structure as in this case there's no ambiguity, but in most cases the entities don't clearly pertain to just a single category. Take a `Lens` for example - is it `Data.Lens` or `Control.Lens`? Edward Kmett released two packages, which reflect both ("data-lens" and "lens") - what is it if not a proof of the ambiguity of that convention? &gt; But this convention proposal is very debatable. No, it's hardly debatable. Throughout the whole thread you'll find only one valid point against it: migrating things into other packages becomes more complicated (though proven that not that much). There also are some suggestions concerning minor stylistic details of the proposal, but no principal criticism. &gt; And therefore is not used widely, at least it is not used in notable packages. It won't ever become used to any degree if nobody starts using it. If it wasn't for revolutionary decisions, we'd still be burning witches today, forget about quantum physics. This proposal is not something revolutionary either. Similar tendencies were already formed prior to it. And yes, there are notable projects that follow these tendencies: "pipes", "fay". &gt; And yes, personally I like Data.BitArray more than just BitArray. I think that it's more about being used to, than liking. It's kind of the things that grow on you once you start using them.
I wonder if GHC could detect a bunch of left-associated binds and `lower . lift` those bunches away.
&gt; Meanwhile I've been trying to grok monads since last year. Are you comfortable with functors, applicative functors and monoids? If not, forget everything about monads and start with those. Starting with monads is going the wrong way about things. That's like trying to understand, I don't know, Keynesian economics without knowing what money is.
The reason maths is like that is that words have a very specific meaning in a mathematical context. You rarely see that problem in other fields. When something is "closed" in engineering, it means it probably has hinges and you can swing it to reveal a gap of some sort. When a set is "closed" in mathematics, it means it contains all its limit points and it cannot contain all its limit points without being closed. In that definition, "contains", "all" and "limit point" also have very specific meanings. In this case the mathematicians have chosen a word with an intuitive meaning (i.e. "closed") but *despite this* you have to learn what the precise definition of "closed" is in the mathematical context. So if you're going to have to re-learn what the word means anyway, why choose common word instead of a unique one? When chosing the common word (such as "closed") you run into other kinds of intuitive problems, such as it being possible for a set to be both "closed" and "open" at the same time.
Author here. I'd be interested if anything stood out in particular as incidental complexity? Especially with respect to all the other Javascript implementations of TodoMVC. The line count is comparable as far as I have seen. (Documentation is surely needed and on the way of course.)
I wouldn't consider myself coming from a maths background. All the maths relevant to Haskell is stuff I've learned *because* people talked about it when I learned Haskell. The reason the language is that way is that there are no better words, at least that I know of, to convey the same thing. If I knew better words, I'd be the first to use them. What would you like to call a "monoid"? (The typeclass for things that can be combined with a specific concatenation operator.) What alternative word would you like to use instead of "isomorphic"? (Several things that carry the same information but possibly using different structures, in a way that makes it possible to translate from one to the other and then back again to get the original structure.)
Good. I don't want you to. I was just making a belabored point about how grokking a for loop is not anywhere near the scale of the things going on in the world of Haskell.
You may be interested in looking inside hoodle (http://github.com/wavewave/hoodle). hoodle is a pen notetaking program written in haskell (http://ianwookim.org/hoodle) and somewhat large-sized haskell GUI code. In hoodle, I wrapped gtk2hs IO layer by a simple coroutine (free monad per se) (which is separatedly packaged into coroutine-object). So by doing so, I made basic business logic inside State monad (called MainCoroutine in hoodle code) without referring explicit IORef variable. This design immediately paid me back, since functional state monad has persistent data structure so that I can keep old state as I want. This is very useful for implementing undo/redo feature very easily. I think my approach is in the same spirit as Tekmo's `pipes` families as shown in other comments. (I am planning to switch my coroutine-object infrastructure with `pipes` some day) Since you are interested in developing some GUI program, I think we can have some common interests. Just contact me if you have any questions about my code. 
&gt; instance Functor DumbAddition where &gt; fmap f (Num i) = Num i Not "fmap f (Num i) = f (Num i)"?
No, that wouldn't type check. The idea is that the `Functor` instance identifies all the positions where recursive term occurrences can go.
Thanks for the feedback! Right now, each DOM element returns an Async, which is left abstract so the same markup code can be interpreted differently on the server and the client. Async is Functorial right now. I think it could be moinoid-ish (with the "fireFirst" being mconcat) but I'm not sure the right way to express that with the higher kind of the abstract Async? I don't know if I can demand Applicative+Alternative but I'd be interested in looking more into this. But in general, your pipes/mvc is very much the same approach that I want to facilitate: all the Async values can be left pure and saved for a single, top-level effectful loop. I was thinking of looking into using the same approach for monoidally composing the various top-level effectful actions in the same way as your mvc "Controllers." I certainly need to work out more sample applications and build some clear demonstrations of that style -- the TodoMVC app right now clearly doesn't express that possibility yet.
Notably, the laziness attributed to `LazyFix` actually comes from the functor instance for `Iffy`; You can get the same lazy evaluation by doing essentially the same thing with the standard `Fix`. `LazyFix` would better be called `Fix2` or the like imo
Haste author chiming in, you can actually share code between front- and backend using Haste as well. You can even get your entire program, client and server, type checked as a whole using a trick described in this [paper for Haskell Symposium](http://haste-lang.org/haskell14.pdf). The trick requires no particular compiler support, so if you're into Fay or GHCJS you could easily port it to either of those. As for commercial adoption, judging by communication with Haste users there are some who do seem to use it commercially. We use it for teaching the [introductory Haskell course](http://www.cse.chalmers.se/edu/year/2013/course/TDA555/) at Chalmers, which might count as production use if you squint really hard and completely redefine the word "production".
I sympathize; the article isn't intended as an introduction I think. The "Basic Background" is more like a quick review helpful for people who already have at least a little familiarity with F-algebras. The second part is about extending the concept by using a monadic type as the carrier type of the algebra. Here is [a more gentle introduction to f-algebras in Haskell](https://www.fpcomplete.com/user/bartosz/understanding-algebras). 
You mean `Free f Void`: bottom isn't a type.
The syntax is not a real issue here, but the jump from hello world to ToDoMVC example is too much to handle for the first contact with the library.
Ah, thanks. This approach reminds me a lot of using `Free` to directly encode program flow using do notation, a la http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html
I'll try to wrap my head around that. In the meantime, I thought the type signature of fmap was (a -&gt; b) -&gt; f a -&gt; f b, isn't the type of this (a -&gt; b) -&gt; f a -&gt; f a?
Thanks. I've never used the empty type in Haskell but that's the way it's denoted in Agda's std-lib.
No, Num doesn't contain a type variable, because i has type Int. When we make something in the type Fix DumbAddiation we're actually making a tree of values where any constructor with a type variable is a interior node, and any constructor with no variables, or variables of concrete types (like Num), is a leaf. When writing fmap we want it to recurse down the tree, so we only apply f to nodes with type variables. 
I was going through your internals and I noticed that if `Async` were an `Alternative` then you wouldn't need `FireFirst`, since you could replace `fireFirst` with `Data.Foldable.asum`. You can also simplify your implementation by using [records of functions instead of type classes](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/). You may also be interested in [this post by Paolo Capriotti](http://paolocapriotti.com/blog/2012/06/04/continuation-based-relative-time-frp/) on a monad for registering and releasing handlers. That seems like a better fit for what you are trying to do. Applying, Paolo's advice to your code would change your `AsyncSource` type to: data AsyncSource a = AsyncSource { listen :: (a -&gt; IO ()) -&gt; IO () } ... and that's already a `Monad`, specifically the `ContT IO a` monad. By implementing `AsyncSource` directly in terms of `listen` instead of the `MVar`, you get more mathematical properties for free.
This is why I don't like /r/haskell. No humor.
The documentation component can be replaced by comments: stems = ARGF .read -- all text .split -- space delimited "words" .each_cons(2) -- successive word pairs .group_by {|e| e[0]} -- lists of word pairs by leading words However, this won't give you the logging features.
Is it possible to use something like ghc-mod or hdevtools with haste?
I joined reddit specifically for this sub, and my account is more than 2 years old. Must be comming up on three? I was already programming haskell for a few years before that.
Java/Enterprise developer here. Can confirm, most of my work consists in creating confusing and cumbersome CRUDs. If people ever learn to use Excel, or even worst, Access, I will be out of job. Also, if someone can make a type safe haskell-sql bridge, that will be awesome!
&gt; though you can be quite happy for a long time with only a partial understanding. This is a big peeve I have with Haskell, or perhaps the Haskell community. Haskellers are constantly telling people to be happy with a partial understanding of things. One of the big advantages of functional programming is supposed to be that it makes it easier to reason about things, but with only fuzzy ideas of how things work you can't really reason and you can't make reliable software. You're just a cargo-cult programmer at that point.
&gt;I really don't understand complaining about the use of precise names. Except the names aren't always actually that precise. For example, in Haskell a "monad" is a specific kind of monad (in the category Hask or a subcategory) and also has some extra "hair" on it (`fail`). Haskell programmers rarely actually mean a "monad" in the true mathematical sense. I think one thing that trips up a lot of programmers trying to understand Haskell nomenclature is that the things that are named in Haskell are often not the sort of things that are named in other languages. Monad is a good example of this. I believe most programmers learning Haskell at some point come to believe that a list value or a Maybe value or an IO value are all monads, but this is wrong. `[1,2,3]` is-a `List`, but it is not a `Monad`. `List` (ie: the type itself) is the monad. I know it took me months before I finally realized this, and I actually studied abstract algebra. Monads get picked on a lot for being confusing, but they aren't the only example of naming in Haskell being less than ideal. A few others I can think of of the top of my head: - State, where a value of the type does not actually represent a state. - fst, because i and r are just too hard to type? Really? - first, because let's use the full word for something completely unrelated and far more abstract. - return, because in really simple cases it kind of behaves like the keyword with the same name in many imperative languages. Wait till they find out it's actually completely different! And then there's the whole practice of using single letter variable names for both types and values virtually all of the time. The usual excuse is "this code is so abstract that there is no more specific name that fits" but from the code I've seen this seems to be true only about 5% of the time. It seems most Haskell programmers have convinced themselves that brevity is more important than everything else, ignoring the fact that the 3 seconds saved by making that function a few characters shorter is actually costing hours of wasted time for the people who have to decipher it later.
Why is there no Num instance?
As the author of unittyped: units is indeed the better choice. unittyped hasn't been updated in a long time, there were a couple of things that got a lot easier with 7.8 (so much that I didn't want to bother keeping it working with 7.6), but I never got around to making a new release with that. They seem to be using the same approach, so there shouldn't be anything you can do with unittyped, but not with units.
`Num` requires your addition to have the type: `(+) :: a -&gt; a -&gt; a` But that would not allow you to add some amount of hours to minutes. `(*)` is even worse: meters `*` meters should not give your something with type meters, but m^2 . `Num` can't express that.
I believe the author was aware, and just wanted the logging too. ```c``` is for comment?
&gt; I'm curious if anything here is either dumb, non-idiomatic, or both. Looks good! Small things: the parentheses in `IO(a)`, `(Message y)` and friends, `(y x)` and `(flip act chan)` are unnecessary and unidiomatic. `IVar()` is usually written `IVar ()`. Bigger thing: top-level definitions typically have a type signature. The signature isn't for the compiler, as it can infer the type on its own; it's for humans. At least, for humans like me who find type signatures useful :) &gt; I'm also interested if something similar exists in some library somewhere. [Cloud Haskell](https://hackage.haskell.org/package/distributed-process) is supposed to be inspired by Erlang, but from what I remember of the paper, Cloud Haskell is defining distributed versions of `newChan` and `forkIO`. So it's not really similar to what you wrote, it's more like an alternate base on which you might want to build instead, if you want your actors to be distributed instead of concurrent.
I feel like there's been a lot of people lately who post single comment flamebait and then sit back to watch the rest of the sub squabble over terminology and wring their hands about how awful the state of monad tutorials are. Well done, /u/unimpaired, artful and subtle trolling at it's best. (esp. linking to /r/VXJunkies)
Yep!
Yeah, there's a very wide spread of audience expectations. Typically comments on a link to an academic paper are correspondingly academic, but if it's a Functional Pearl or something written by one of the more general audience authors then they may not be!
My dog is functional. &gt; "What time is it?" &gt; "Now!"
You're aware that persistent provides uniqueness constraints which allow exactly the workflows you're describing, right? The only primary key constraints is that persistent requires that there be a primary key column, not the you only perform lookups with it. And yes, uniqueness constraints can span multiple columns, so you can have compound keys, just not primary keys.
This is fantastic! Both "cabal test" and "cabal run" have been unusably slow since they were changed to include an implicit "cabal build" (which calls "ghc --make").
Are you on Windows? I find typically Windows systems with virus scanners and lots of C Pre Processor modules will suffer a lot more from slow "ghc --make", but very interested to know if it's noticeable in other projects.
&gt; Small things: the parentheses in IO(a), (Message y) and friends, (y x) and (flip act chan) are unnecessary and unidiomatic. IVar() is usually written IVar (). most things like that are caught by hlint. some tips might be too advanced for a beginner (it might be ok not to eta-reduce everything or rewrite in a fancy style) though.
OS X. I am trying to make things faster for GHCJS packages built in Leksah, because once they are built they can run in a WebKitGtk window straight away (so any delay is annoying). I think cabal is cleaning up something needed by ghc-make. Here is a quick test I ran... http://lpaste.net/106158 As you can see the ghc-make runs slowly the first time I run it by itself and then runs really fast the second time.
i have not looked at the source, so my comment might not make sense. &gt; You can also simplify your implementation by using records of functions instead of type classes. isn't one point of them using typeclasses that they use final encoding? you might say that they should use free functors instead, but that might be a different discussion.
do (general) you know how to make cabal call `ghc-make` instead of `ghc --make`. apart from writing a `ghc` wrapper script, that is.
I'm confused, I thought [parallel-make was built into GHC](https://ghc.haskell.org/trac/ghc/ticket/910) some time ago?
Do you mean why isn't the 0m0.986s actually 0m0.335s, or why isn't the 0m0.335s actually 0m0.058s? Be careful that running the same operation multiple times may go faster due to caching. To track down the one that is going wrong, try passing --shake-V (or --shake-VV) to the step you expect to be quicker, which should give greater visibility over what it is doing.
Maybe it's just me, but compiling "Hello World" with ghc 7.8.2 fails with: machine:~/temp/Hello$ ghc-make Main.hs You must specify at least one -dep-suffix ghc-make: Error when running Shake build system: * .ghc-make.result * .ghc-make.makefile Development.Shake.cmd, system command failed Command: ghc -M -include-pkg-deps -dep-makefile .ghc-make.makefile Main.hs -odir. -hidir. -hisuf=_hi_ -osuf=_o_ Exit code: 1 Stderr: You must specify at least one -dep-suffix 
Yes, `-j` for use with `ghc --make` has been a [documented feature of GHC](http://www.haskell.org/ghc/docs/latest/html/users_guide/flag-reference.html#idp15539456) since 7.8.1.
check out [fixplate](http://hackage.haskell.org/package/fixplate-0.1.5/docs/Data-Generics-Fixplate.html) for a generic programming library for fixed point types, which was explicitly motivated by attribute grammars
This is a bit off-topic but the author had a great book on refactoring legacy code. It's good to see him still writing.
This interesting question on SO was mistakenly blocked by an over-zealous SO moderator. So I'm posting it here so that we can continue to give replies. (The OP *also* wanted a code review for a particular proposed solution, and so correctly posted that request on the codereview site. That doesn't detract from the question itself being interesting, and completely on-topic for SO. I think what triggered the moderator action was the OP's use of the word "cross-post", which anyway isn't really true.)
Is `cabal test` slower because of GHC checking if something needs to be rebuilt or because of unnecessary relinking? The latter should have been fixed in 1.20.
In before should've-used-a-free-monad.
I haven't tried it, but since the entirety of Haste's standard library is buildable on vanilla GHC (in fact, it's installed along with the compiler) and the parts that make sense outside of a browser are runnable as well, I'd say there's a fair chance.
A similar post by [@pbrisbin](http://twitter.com/patbrisbin): http://pbrisbin.com/posts/the_advent_of_io/
This is a regression in GHC 7.8 (http://www.haskell.org/pipermail/ghc-devs/2013-March/000677.html) which I've just worked around in ghc-make-0.2.1. Thanks for letting me know.
Great, I actually wrote the parallel feature over a year ago - I guess I should be less delayed with my release announcements! Mine has the advantage of working with older versions of GHC, and the fast rebuilds, but if you are on GHC 7.8 and after the parallelism, I would expect ghc --make -j to go much faster.
Before going into sequencing, I'd prefer to have an example of a short program using your primitives. Something like myMain = Put "Please enter your name!" ( Get (\name -&gt; Put ("Hello, " ++ name) (Return ()))) Just to familiarise the reader with how the data type can be used. Another thing I noticed is that you hint that this blog post explains how I/O can be modeled purely. The last two paragraphs deal with something completely different though – they explain how this technique can be used to create a safe I/O language using the existing I/O language to run it. You might want to conclude the post that "This model of I/O, as a sequence of commands, is actually a decent way of understanding the I/O system in Haskell. The `main` function returns such a pure tree of commands, and then the run-time system runs the appropriate actions, allowing even the I/O parts of a Haskell program to be pure" just to emphasise that this is actually a reasonable understanding of how the existing I/O system works! Other than that, great writeup!
It only happened after a huge amount of work, including: * The original work by Simon Marlow more than eight years ago * A successful GSoC last year * More work on top of that by several other people that finally turned it into an actual feature So while you're right that the native GHC support is probably faster, the fact that Shake could pull this off is a great testimony to the power of Shake.
How do you find it stability wise - I get the impression it's still early days and the language is still going through significant revisions and changes? 
It's time for another reddit thread about backpack; there was one [about a year ago](http://www.reddit.com/r/haskell/comments/1id0p7/backpack_retrofitting_haskell_with_interfaces/) and another [about half a year ago](http://www.reddit.com/r/haskell/comments/1uhbci/backpack_retrofitting_haskell_with_interfaces/). Backpack was also mentioned in the comments of a [recent reddit thread](http://www.reddit.com/r/haskell/comments/28n2kx/the_top_three_things_missing_from_enterprise/). Backpack is the work of /u/skilpat, in collaboration with his thesis advisor Derek Dreyer, SPJ, and Simon Marlow. As discussed in the original reddit thread and mentioned in the [final version of the paper](http://plv.mpi-sws.org/backpack/backpack-paper.pdf), the main piece missing from backpack that was preventing it from being implemented was how to handle type class instances. Has there been any progress on that? It would be great to have backpack!
The description is rather brief and doesn't address my main question: how can this be better than what GHC does already?
It's already a free monad, isn't it? 
Can someone explain clearly the precise problem that Backpack addresses? I've read the paper and I know the problem is "modularity" but I fear I am lacking the imagination to really grasp what the issue is. What's the problem with what we've currently got?
Persistent seems (if I'm reading the [source](http://hackage.haskell.org/package/persistent-1.3.1.1/docs/src/Database-Persist-Class-PersistUnique.html#insertUnique) correctly - insertUnique/replaceUnique) to have oddities like trying to detect unique key clashes by reading the row before writing. Which is surely a race condition? (Not to mention a perf issue). So if you end up having to try or catch anyway, why bother? (I thought it might be specialized per backend, but it seems not).
If I remember correctly, it would give "first-class" ground to packages like [`abstract-deque`](http://hackage.haskell.org/package/abstract-deque) allowing applications to depend upon APIs instead of actual package implementations.
I think this kind of merging (combining containers element-wise even though some elements might not match) is sufficiently common that there should be a typeclass for it. Who is with me? import Prelude hiding (zip, zipWith) import qualified Prelude import Control.Applicative import Data.Map -- minimal complete definition: mergeWith. class Zip f where -- return Nothing to remove an entry mergeWith :: (Maybe a -&gt; Maybe b -&gt; Maybe c) -&gt; f a -&gt; f b -&gt; f c -- if either input is missing, the entry is removed zipWith :: (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c zipWith = mergeWith . liftA2 zip :: f a -&gt; f b -&gt; f (a, b) zip = zipWith (,) instance Zip Maybe where mergeWith = id instance Zip [] where zip = Prelude.zip zipWith = Prelude.zipWith mergeWith f xs ys = case f (safeHead xs) (safeHead ys) of Just z -&gt; z:zs Nothing -&gt; [] where zs = mergeWith f (safeTail xs) (safeTail ys) safeHead :: [a] -&gt; Maybe a safeHead [] = Nothing safeHead (x:_) = Just x safeTail :: [a] -&gt; [a] safeTail = drop 1 instance Ord k =&gt; Zip (Map k) where zipWith = intersectionWith mergeWith f = mergeWithKey both leftOnly rightOnly where both _ x y = f (Just x) (Just y) leftOnly = mapMaybe (\x -&gt; f (Just x) Nothing) rightOnly = mapMaybe (\y -&gt; f Nothing (Just y)) 
If that were sorted, would backpack be likely to find its way into GHC/Cabal? Because I loved the idea but I didn’t think it’d gain enough traction.
You should definitely post this as an answer. You might also like the [data-ordlist](http://hackage.haskell.org/package/data-ordlist) package, which includes [`mergeBy`](http://hackage.haskell.org/package/data-ordlist-0.4.6.1/docs/Data-List-Ordered.html#v:mergeBy) and [`mergeAllBy`](http://hackage.haskell.org/package/data-ordlist-0.4.6.1/docs/Data-List-Ordered.html#v:mergeAllBy).
In my opinion, it would definitely gain traction if implemented. I think it would revolutionize Hackage and the Haskell ecosystem. But that's a big "if". Even after the major hurdle in the theory, type class instances, has been gotten past, it would still be quite a bit of work to implement backpack. The author of the paper, /u/skilpat, has stated many times that he has no delusions that he would be able to implement it on his own. In Edward Kmett's opinion (as stated in one of the previous threads), it's also well beyond the scope of a GSoC project.
Sorry to sound stupid, but what's the benefit of that? Presumably it's not *literally* to swap implementations in and out, because the occasions on which that's a sensible thing to do will be few and far between. Surely there's a related technical issue at play which modules ease.
Yeah, I see what you mean. Let’s hope though!
It's a free monad but not explicitly encoded a free monad over a functor.
Mix-in modules give equivalent functionality (and still work with recursive linking). Instead of: functor List(A : Elt) = struct ... end You write: package list where A :: ... Elt signature ... and then apply it by mixing in a package which defines a module named A.
Database backends seem like a perfect example where you'd want to use a library in your application that depends on an arbitrary backend.
A very simple example: you wrote an API that takes a `Map` as a parameter, but I only have a `HashMap`. Converting between the two takes O(n*log n) time. If your API doesn't really need a weight-balanced sorted map implementation, depending on a map interface module (or alternatively: a `MapLike` type class) would allow me to use my `HashMap`.
Interesting!
Here you go: http://i.imgur.com/QXSxoin.png So popping the minimum from the queue and looking up a the priority/value tuple for a given key have similar performance for OrdPSQ and PSQueue. However, insert and delete are consistently faster (roughly 15%-20%). We use a lot of inserts in our use case, although I admit "considerably faster" was perhaps a bit too strongly worded. :-)
Ahh, much better. Thanks Jasper! EDIT: Any way to make this more useful graph visible on the blog post, which seems to be the primary source of information about psqueues? Also - there is no link to the blog post from either github or hackage. Thanks!
Here's the CSV produced by criterion: &lt;http://jaspervdj.be/tmp/lru-bounded-map.csv&gt; Our simple implementation of psqueues seems to perform similar to your LBM_CustomHashedTrie for inserts, deletes and lookups. Lookup w/ LRU update is where our implementation is significantly slower than yours though. However, there are a few tricks we can use. For LRU caches, the most important trick is that when you're inserting an element (or bumping the priority after a lookup), the new priority is always going to be larger (i.e., less priority). As you probably know since you've implemented a few LRU caches yourself, this allows some optimizations such as not rebuilding parts of the tree in those cases. By exposing those "unsafe" primitives such as `unsafeInsertLargerThanMaxPrio` and using those we can get some great speedups for our LRU implementation. We are however still integrating this package into all our code, and these unsafe primitives are bound to change frequently for now (both in terms of types and behaviour), so we decided not to expose them in the first release. They are available on GitHub though, if you are interested. :-)
I'm really happy to see that for once a project hasn't been left into "hackaton-quality", but actually polished and published as a package. Well done guys!
You can do final encodings with records, too. I think you might mean extensible encodings.
I can see a world which is a lot more API-centric under this model. API maintainership being separated from package maintainership, a tendency to target common APIs (or combinations thereof—does backpack allow composition of API?), vastly improved concepts of the "public API" for managing the PVP. I agree that most of the time you won't be swapping whole packages, but that could even be useful for benchmarking. More likely, you'd be talking about swapping versions within-package. Presumably, if you depend upon a particular version of the API itself then all versions of the package which match the API would be compatible... and perhaps the latest one is the most bug-free!
When I first heard about backpack, I thought net effect would be for cabal to "catch up" with other language's package systems, because we keep hearing how broken haskell's package system is. But the more I learn about it, it seems like it is really treading in new waters. Would implementing this stop people from saying the package system in haskell is broken - or is it a feature that adds some flexibility for some edge cases? I am mostly interested in solutions that, when people come to haskell from another language, they don't feel like something is missing that their previous language's package manager gave them.
Cool, thanks.
This is pretty darn neat and I'm glad its getting more publicity. There is a certain relationship between this and ghc's "rapier" technique, although that is slightly more complex and solves a more complex problem. The emphasis on "local knowledge" and compositionality in solving naming applies to both though.
Direct link is here: http://byorgey.wordpress.com/catsters-guide-2/ . It's not quite done yet but I'm getting close, on schedule to complete it by the end of the summer.
Thanks for sharing those! The 'Lookup w/ LRU update' is by far the most performance sensitive part for my three use cases so far, though. It seems likely that the actual lookup operation dominates the runtime of most cache structures. I was planning on revisiting this data structure at some some point, wanting to try out how much better the performance would be with a mutable implementation. When I get around to do that, I'll be sure to check out psqueues and some of the optimizations you're suggesting!
Among other things, it's very useful for library code where you don't want to decide on a concrete implementation for the user. For example, the [functional graph library](http://hackage.haskell.org/package/fgl) does this by having a [Graph](http://hackage.haskell.org/package/fgl-5.5.0.1/docs/Data-Graph-Inductive-Graph.html#t:Graph) typeclass. This is important to support different usecases of the library. Unfortunately, the typeclass-based solution to this problem is *very* awkward in practice, and messes up type inference consistently. Using it felt like I was fighting the system more than taking advantage of it. For things like this, a real module system would be a significant improvement.
I've used this trick as well. It's really useful when transplanting entire sub-trees of ASTs, the naming will still be fine.
&gt; This way of naming has the added bonus of ensuring the productivity of the knot-tying program for picking a fresh variable in lam. We don’t need to descend into the body of a lambda to find a variable that is free. I don't understand this part. It's not avoiding descending into the lambda that ensures termination, it's because `maxBV` doesn't inspect the parameter of the `Var` constructor. The first version of the paper does indeed descend into the lambda. The one that avoids going under the lambda is presented as an optimization.
I think running "cabal help" will create a ~/.cabal/config
Here's a dumb suggestion, but it might be worth trying: just create `.cabal/config` with that line in it.
Having never implemented an EDSL, I wish I knew enough to understand why this is stunning.
You're right. I'll try to clarify the post. I would phrase it that naming lambda parameters based 1 + the depth of lambdas in the body makes it unnecessary to examine either the body of lambdas or the value inside of `Var` constructors (since it is the 'parent' that adds 1 to the depth) when building a new lambda. But the scheme will still terminate like you say if you merely avoid examining each `Var`.
run cabal update. delete entire contents of .cabal folder except config file. delete .ghc folder Edit the config file to point to stackage. run cabal update again. 
Precisely.
What issues did you have with it? What made you feel like you were fighting it?
I use the Oleg-style "Tagless Final" approach. I think the abstract types are a real benefit for this use-case so I don't think I'm committing an existential anti-pattern, but I am definitely open to the feedback.
Is the question "how does it rebuild faster than GHC itself?". If so, the README (https://github.com/ndmitchell/ghc-make#why-is-it-faster) says: &gt; When GHC does a compilation check it runs any preprocessors and parses the Haskell files, which can be slow. When ghc-make does a compilation check it reads a list of file names and modification times from a database and checks the times still match, and if they do, it does nothing. 
Ah thanks. But why can't GHC use that speedup too?
There is another post with a SkillsMatter presentation also from Simon Marlow about Haxl [here](http://www.reddit.com/r/haskell/comments/1o2mqe/the_haxl_project_at_facebook_concise_efficient/), but comments to that link are closed so I posted another link with a more recent one. 
I am working on implementing Backpack for my MSR project this summer. We're still scoping out the design, so it's not clear how much will actually get implemented. While it seems like type classes are a big deal, we are not really concerned about them at the moment: typeclass resolution, even when typeclass hiding is allowed at the module level, is still coherent (the reason being you can't "open" a data type that introduces extra instances into scope), and the other problem, that abstraction can be violated by orphan instances, is one that already exists for Haskell'98... so we'll solve it later.
Here's an example of where ML modules come in handy: http://hackage.haskell.org/package/tagstream-conduit-0.5.5.1/docs/Text-HTML-TagStream-Entities.html This module really exports an ML module functor. I used a data type because a type-class doesn't make sense here. I don't want overloading and I'm not defining a common abstraction that I want users to be able to extend. Using a type-class gives the wrong idea. Also in fancier type cases, e.g. multiple-parameter type-classes you have cumbersome inference problems and in the worst case a runtime hit because you have to pass the dictionary around (in the case of a generic library where instantiation isn't compiled together with the declaration of classes). In this example, the data type is instantiated by two modules of the same library, Text.HTML.TagStream.ByteString and Text.HTML.TagStream.Text. With a decent module system I would be able to import Text.HTML.TagStream.Entities from each of these modules with the few functions and types parametrized at compile-time. Instead, I used a data type, which is a bit awkard as you have to pass the dictionary around, and, although in this case there's probably no speed hit because GHC can inline the call sites, in general that's not true. At any rate, the ability to instantiate a whole parametrized module with types and names like in ML has typed benefit, clarity-of-intention benefits and speed benefits.
I believe I understand now. Thank you for the great explanation. The value of eliminating double-negation in the simplification step wasn't obvious to me until I thought about a really heavy computation like multiplying a matrix by its inverse.
I'm a bit sad that the fingertree-based implementation fared so badly, since it's quite an elegant datastructure. Is this just the price of generic-ness, or is there something else going on in fingertree-psqueue that makes it slower? 
If ghc dependencies on package management could be minimized in your implementation, please consider https://ghc.haskell.org/trac/ghc/ticket/8244. 
I'm assuming you teach Haskell to first year undergrads?
Mostly dealing with type inference, especially if I wanted my code to be generic over the graph implementation too. Ultimately, I just ended up using a concrete type in my code, which was unfortunate. I think there were some other minor annoyances, but it was a while ago so I don't remember exactly.
Thanks, I really appreciate this! It works flawlessly, at least after really unpacking the zip file into `C:\qt530-hp` (maybe you should emphasize that part in the description). 
That's what polymorphism is for.
GHC could use the same technique, and it probably should. GHC would need to introduce an additional file per project containing the timestamps as things were when they were last processed, and trigger its rebuilding off that. Not a trivial change, but not "type-checking GADT's" hard. Alternatively, GHC could make use of Shake and get that machinery ready made.
This is an excellent and well explained example. Thank you.
So can anyone here give a precise explanation why this is useful (not vague statements about it being better than type classes because 'its just better')?
What comes to mind is that you'd need to periodically give rank 2 types to ensure that the code is sufficiently polymorphic. A type like data Ex c where Ex :: c =&gt; a -&gt; Ex c might help.
It just seems like a bunch of Java programmers that want to add yet another layer of complexity for the purpose of reconciling their bad design choices. I've yet to see a single specific example of why this is better than typeclasses.
It's possible, but rare. See tikhonjelvis's comment below 
Is it possible to write a typeclass that would suit both Map and HashMap, without it having both the `Ord` and `Hashable` constraints ?
Yeah I only saw it after I wrote the comment. The points about inference are the only problems I could think of to begin with. I was hoping for more information.
He does
You wouldn't put that in the base typeclass, methinks, just as you wouldn't in the module.
Interesting! Clearly you using this for something far more demanding that I am. Mostly, I had something like 20-30 threads writing into the cache with one thread reading from it. My first idea after realizing this is a bottleneck was also to just update the LRU less frequently, maybe just randomly. Or perphaps just when the element is at the bottom and in danger of being retired.
I am not even so certain about the "worth at the end", but the journey is totally fun and interesting, so i'll take it.
This is definitely new waters. I haven't seen OCamlers use modularity in a way that is compelling or particularly better than what we have in Haskell. (In terms of multiple libraries/packages targeting a common API) We do a better job of that as it is with typeclasses (typeclassopedia, kmettoverse, etc) Let alone any other language community.
The problem is that the lambdas inside your representation are opaque... Ideally the computation for the next action would itself be a transparent composite so you can actually analyze the whole computation.
"Another important use case is the GHC IO Manager: we believe that it would profit by switching to `IntPSQ`." Great! Who's up for testing this hunch by forking ghc HEAD to make this change?
I've always thought that it was to help with dependency hell in cabal. (I believe) It would (mostly) remove the need for lower and upper bounds on versions for a package, just a single version that works. Essentially, if you change versions of package `x`, and you know the API of package `x`, and the part of the API that package `y` uses, you can use different versions of package `x` as long as that part of the API hasn't changed. With the further proviso that functions change name or type signature if they change functionality, you can now "calculate" the version of a package, and the version that another package depends on.
Hmm, your workload sounds interesting. What makes it it that write-heavy?
That's great to hear! I've updated the second paragraph in the [Windows section](http://www.gekkou.co.uk/software/hsqml/#windows) to make it clearer how to deal with the zip file. Is that better, do you think?
I've done the benchmarks quite some time ago: https://github.com/meiersi/psqueues/commit/4009123e0a9cb1ceec3bac9fd6432420cd00d47b I found that on my MacBook Air, I get a 25% speed increase on lookup and about a 3x speed increase on inserts. I don't know how these improvements translate to speed improvements for real-world programs. I'd expect that they have to do quite a few `threadDelay` calls to get a noticeable speedup. 
It's a very nice paper, but the fact that you can do it is somewhat part of the FP folklore. Which is why the paper is a pearl. 
This probably shouldn't be overlooked. There are some packages for JVM interop, I've never used so no idea how robust they are.
I feel like the question itself is wrong, or at least unclear. I'm reading his description of `Model` as an FRP-style signal with values that change at particular times; i.e. account 1 has a balance of 1 for a while, then 3, then 5, and account 2 has a balance of 1 for a while, then 2, then 4, then 5, and ending at 6. Which means that simply accumulating the balances in the style given gives the wrong result. I imagine something like this to make more sense: -- result values are never both Nothing group :: Ord t =&gt; [(t,a)] -&gt; [(t,a)] -&gt; [(t, (Maybe a, Maybe a))] group = group' Nothing Nothing group' :: Ord t =&gt; Maybe a -&gt; Maybe a -&gt; [(t,a)] -&gt; [(t,a)] -&gt; [(t, (Maybe a, Maybe a)] group' l0 r0 [] rs = map (\(t, r) -&gt; (t, (l0, Just r))) rs group' l0 r0 ls [] = map (\(t, l) -&gt; (t, (Just l, r0))) ls group' l0 r0 ls@((tl, l) : ls') rs@((tr, r) : rs') = case compare tl tr of GT -&gt; (tl, (Just l, r0)) : group' (Just l) r0 ls' rs LT -&gt; (tr, (l0, Just r)) : group' l0 (Just r) ls rs' EQ -&gt; (tl, (Just l, Just r)) : group' (Just l) (Just r) ls' rs' merge :: Ord t =&gt; (a -&gt; a -&gt; a) -&gt; [(t,a)] -&gt; [(t,a)] -&gt; [(t,a)] merge f ls rs = map (\(t, x) -&gt; (t, go x)) (group ls rs) where group Nothing Nothing = error "shouldn't happen" group (Just x) Nothing = x group Nothing (Just x) = x group (Just x) (Just y) = f x y This gives newtype Time = Time Int deriving (Show, Eq, Ord) test = merge (+) [(Time 0, 1), (Time 3, 3), (Time 6, 5)] [(Time 1, 1), (Time 2, 2), (Time 3, 4), (Time 4, 6)] -- test = -- [(Time 0, 1), (Time 1, 2), (Time 2, 3), (Time 3, 7), -- (Time 4, 9), (Time 6, 11)] (You could implement this more simply by replacing the stream-of-values with stream-of-deltas-of-values, but you'd end up with a less generic type)
Are these videos at all suitable for a beginner to the subject? Or would you recommend getting a solid foundation in category theory before tackling them?
I'm fairly certain that's not what Haskell programmers mean most of the time when thy talk of partial understanding. I frequently see Haskell programmers talking about just glossing over operators they don't understand or even pretending like the do-notation is imperative without even knowing what it desugars into. The abundance of just plain wrong monad tutorials is evidence of bad the "partial understanding" problem in the Haskell community is.
You should compare with `pipes-concurrency`, which is another actor-like library that I wrote. The main novel feature of `pipes-concurrency` is the use of the garbage collector to detect and avoid deadlocks.
Another common demand might be to serialize or persist an AST. The higher order representations have no persistent form.
Just to elaborate on my other response, your version is smushing together two separate concepts: * operate on a number in the form of its digits * the particular operation you want to perform on its digits With the list version, the digit conversion has been cleanly factored out and I can conceptualize "this is doing XYZ to the digits". I then glance again and see that it's asking for the number of trailing 0 digits. 
It seems to me they would work best side by side with some intro cat theory text of choice.
Because gtk is difficult to install on windows?
Ah Simon. I’m so pleased to have been a part of this project. :) If anyone’s using Haxl, even for toy projects, please make sure to post on /r/haskell about it! It’s available at https://github.com/facebook/Haxl and http://hackage.haskell.org/package/haxl.
I have had a bit more of a look and I think the problem is related to the fact that cabal invokes ghc -make (or ghc-make -make) twice. The first time it runs it with -no-link and does not specify an output. The second time it runs it without -no-link and specifies an output. First... /Users/hamish/Library/Haskell/bin/ghc-make --make -no-link -fbuilding-cabal-package -O -static -outputdir dist/build/hello/hello-tmp -odir dist/build/hello/hello-tmp -hidir dist/build/hello/hello-tmp -stubdir dist/build/hello/hello-tmp -i -idist/build/hello/hello-tmp -isrc -idist/build/autogen -Idist/build/autogen -Idist/build/hello/hello-tmp -optP-include -optPdist/build/autogen/cabal_macros.h -hide-all-packages -package-db dist/package.conf.inplace -package-id base-4.7.0.0-0683e72c2150a986a33a7436cf16e5ef -XHaskell98 src/hello.hs Second... /Users/hamish/Library/Haskell/bin/ghc-make --make -fbuilding-cabal-package -O -static -outputdir dist/build/hello/hello-tmp -odir dist/build/hello/hello-tmp -hidir dist/build/hello/hello-tmp -stubdir dist/build/hello/hello-tmp -i -idist/build/hello/hello-tmp -isrc -idist/build/autogen -Idist/build/autogen -Idist/build/hello/hello-tmp -optP-include -optPdist/build/autogen/cabal_macros.h -hide-all-packages -package-db dist/package.conf.inplace -package-id base-4.7.0.0-0683e72c2150a986a33a7436cf16e5ef -XHaskell98 src/hello.hs -o dist/build/hello/hello Take either one of those and run it twice and ghc-make is able to optimise the second run. But if you switch from one to the other it winds up being much slower (slower even than ghc -make). I imagine the problem is that .ghc-make.args does not match each time. Perhaps when checking ghc-make should consider it to match if the only difference is that -no-link is included and the -o is missing (since that will produce a strict subset of the output files). In that case it should not update the .ghc-make.args (unless something else has changed). Edit: Sent a [pull request](https://github.com/ndmitchell/ghc-make/pull/5) with a rough fix that seems to work for me. 
This only covers a very small part of what is usually called the actor model. I think the other parts are terrible ideas, so I'd rather use `pipes-concurrency` than an actual actors library. It is however a stretch to say this library is about actors whereas it's only about message exchange.
If you're going to ignore everything I say and keep calling something broken, there's not much of a discussion here.
Lots of threads reading data from disk / network, few uploading data to the GPU etc. In the end this clearly a concurrency scenario, not very parallel. We're talking about dozens of insertions per second with maybe a few 10s of thousands reads or so max. So it's not as difficult as what you described, despite a high number of green threads involved. So far 'stick in a TVar' has been good enough ;-)
Also please note I'm no expert on actor-based programming, which might explain why I have definitive opinion on the subject :)
Yes, that's better. One thing I noticed: Using a ApplicationWindow (like in [this](http://qt-project.org/doc/qt-5/qml-qtquick-controls-applicationwindow.html) example) leads to the program crashing/hanging on startup. 
I'll add two more links: * The [slides](http://www.haskell.org/wikiupload/c/cf/The_Haxl_Project_at_Facebook.pdf) * And the [paper](http://community.haskell.org/~simonmar/papers/haxl-icfp14.pdf)
I don't quite understand why there is an infinite loop in the first version (when using max_BV) and no infinite loop in the second version (when using max_BV+). Can someone explain it in simple terms? This is not the first time I am having trouble with this kind of trick. How can I make reasoning about laziness/circular definitions easier? 
Well, no, IO *is* a side effect.
Hi, I'm the blog post author. I wrote this about 18 months ago, when this was pretty new to me. It looks painfully naive now, but I guess it's nice to see it pop up again..?
There isn't an infinite loop in max_BV; it just has quadratic complexity for building an expression while max_BV+ has linear complexity.
Sorry, I meant the first circular version (infinite loop) in the paper vs. the second (no infinite loop).
Well maxV traverses the entire expression, and looks at each variable's name, and then picks a name that is 1 larger. For a problematic example, take `lam(\x -&gt; x)`. In order to compute a name for `x`, maxV is called on the body. But `x` already appears in the body, so we get that the name for `x` is 1 + the name for `x`. That can't work. The trick in maxBV and maxBV+ is to not look at the Var's at all, but only count the depth of nesting of the lambdas. That way there is no infinite loop. edit: The general strategy to reason about lazy/circular programs is to first identify what could possibly be problematic. In this case that's just the `Name`s. Then you ask yourself, in order to compute this possible problematic value, what other problematic values have to be computed? If the answer is that in order to compute the value, we first need to compute that very same value, then you've got an infinite loop. If not, you have a terminating program. In this case with maxV, in order to compute any given name, if that name appears in the body of the lambda, we have to compute that very same name first. So we have an infinite loop. With maxBV and maxBV+ to compute any given name, we don't need to compute any other name. So there isn't an infinite loop. This is also why I think that lazy programs are easier to read in a strict language. In a lazy language every value is possibly problematic. In a strict language all lazy values are in a `Lazy a` type. So you can immediately identify the set of possibly problematic values from the types. If you were writing this program in a strict language then only the Name = Integer would have to be wrapped Name = Lazy Integer. All the AST constructors could remain strict.
Yes, as I allude to in the comment. As others mentioned you need `ConstrainKinds` to do it well. Using a backpack-like module system is another way to achieve the same goal.
OK. Stick in a TVar works pretty well provided you ensure that you evaluate the thunk *before* comitting the transaction. Otherwise, you end up synchronizing on blackholes a lot.
Has this been posted already ? Looks like a dup from a month ago. For another typed time experiments, there's also [Hourglass](https://hackage.haskell.org/package/hourglass)
I made a PoC with a Haskell client I made for our distributed key-value store, Arakoon. The PoC uses Haxl's facilities to turn multiple 'get' requests into a single 'multi_get' request (as exposed by the Arakoon API). See https://gist.github.com/NicolasT/705e4a6e518daf2d5ee5
Perhaps you may reduce the complexity by using the monad-monoid instances of the little package haste-perch (100 lines of code) that target Haste.DOM directly. https://github.com/agocorona/haste-perch The HTML elements can be defined in a more intuitive way, similar to blaze-html directly in the DOM. The language allows also any custom tags and attributes.
Is your data on HDD or RAM? because 70ms is kind of slow…
Great talk. Thanks for sharing.
Can't remember. Please take those numbers with a huge grain of salt: the Haskell client I made isn't optimized at all, the node could be on HDD indeed, was most likely a debug build with tons of logging enabled,... If you want numbers, feel free to run the built-in benchmark on your systems (but keep in mind this isn't Memcache, Redis or whatnot, it's a product with a completely different goal &amp; scope).
Umm... no, I said nothing of the sort. I affirmed that there are currently limitations, stated that there are ways of working around those limitations, that the limitations aren't nearly as severe as you stated, and that there's work planned to overcome those limitations. So I think it's fair to say that you ignored everything I said.
I'm currently going through http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf. At a really slow rate though, mostly because I want to do the exercises, too. The chapters on the yoneda lemma were really hard to grasp for me, but when I watched the appropriate catster session, it became clearer. The problem I see with the book is that there aren't actually so many new definitions per chapter, rather than hard to grok examples thereof. They don't really motivate the defined concept, but are just examples to make your brain recognize the structure. I keep constantly asking myself why we would do this. Maybe it's easy for me to say in hindsight, but after I work through a chapter and end up rather confused, a catster video explaining it in much more concrete terms is what helps me getting a feeling for what I just read. TLDR; I use it to get another perspective on the definitions I went through in a book. Meanwhile, I want to try out http://math.mit.edu/~dspivak/CT4S.pdf, which seems to be much more concrete and referring to programming concepts. 
I don't understand the complexity analysis in the article. What's the definition of "time complexity" for a function in a lazy language, if the arguments might be infinite or expensive to evaluate, and the return value might be only partially used by the caller?
I think they mean constructing an expression takes time linear in the number of constructors in the representation. 
why the negative votes?
A very cool example is OCaml's [ocaml-bitcoin](https://github.com/darioteixeira/ocaml-bitcoin) library which abstracts over the choice of any particular http-client. It can be used with ocsigen, cohttp, ocamlnet and ocurl (and probably more), whichever you prefer. Another interesting bit is that choosing ocsigen or cohttp yields a monadic API based on LWT threads while ocamlnet does not. 
I'm not an expert on actor-based programming either! :) I know that actors are usually intended for distributed settings and have neat features like migration across machines, but I didn't see any of those features for this library so I thought the comparison was appropriate. I'm not sure what the correct name for this style of library is.
Hmm, good point. I think I'm going to take the info from the blogpost and reformat it into the cabal description so it appears directly on Hackage.
Thanks a lot, that helped! 
To me, you need : * actors that take input and return a new actor that will consume the next input * a way for actors to spawn other actors * some mechanism that would let actors monitor other actors lifecycle According to [wikipedia](http://en.wikipedia.org/wiki/Actor_model#Fundamental_concepts), that's not what people usually mean when they are talking about the actor pattern, which seems to prove I'm no expert ! All of this to say I was tempted to point out that the original post was not exactly what I would expect from an actor based library, but it had the "behavior changing" property (with `Message (a -&gt; a)`) ...
Just checked with cabal-install-1.20.0.2 - it does not.
1. Does not work 2. Indeed, cabal nicely separates the indeces so there's no interference. However, I do need to install two other packages (glpk-hs and HaXml), which are not on Stackage. What I did, was to install them manually from http like this: `cabal install http://hackage.haskell.org/package/HaXml-1.24/HaXml-1.24.tar.gz` However, it's like going back to `Make` since I loose the benefits of cabal. Is it possible to somehow have Stackage as primary and Hackage as secondary `remote-repo`?
Strange, I don't see any other lines in my `.cabal/config` that should be required.
The [Package Versioning Policy](http://www.haskell.org/haskellwiki/Package_versioning_policy) does work like this. It has two components for the major version, and there are some additional haskell specific rules. On hackage packages are free to specify the granularity of dependency ranges. In your example, if uno really depends on json == 1.3.6 and not json == 1.* it doesn't make sense to allow it to compile with json-1.4.12 since the author explicitly disallowed other versions. This is also important to be able to exclude known bad versions such as broken releases. One limitation that Cabal has is that you can't have multiple instances of the same package-version built against different versions of its dependencies installed. This is meant to be fixed. 
If I understand the example right, that example would be mapped to Cabal as follows: - `uno` has `build-depends: json &gt;= 1.3.6 &amp;&amp; &lt;2` - `dos` has `build-depends: json &gt;= 1.4.12 &amp;&amp; &lt;2` - `tres` has `build-depends: json &gt;= 2.1.0 &amp;&amp; &lt;3` So with Cabal you just have to be more explicit in formulating the dependency version constraints, whereas Cargo allows just one specific form of version-constraints. Otoh, this means that Cargo will run into the same dependency-hell issues at some point, and have the same problems we're fighting with.
Its associated with ICFP and its been happening for years, although only in recent years did it upgrade from "Haskell Workshop" to "Haskell Symposium" reflecting the quantity and quality of papers accepted, as well as the selectiveness of the pricess.
Actually, the `-XPackageImports` extension could provide something like that if its extended to also specify a version in addition to the package name, e.g.: {-# LANGUAGE PackageImports #-} import "network-2.4.2.2" Network.Socket 
Oh nice, I didn't know that.
&gt; One limitation that Cabal has is that you can't have multiple instances of the same package-version built against different versions of its dependencies installed Is this because imports would be ambiguous of which version to use? Or is there another reason for that?
Another question, which is late but I hope doesn't slip by the wayside: What are the implications for compilation? If you don't know at module compile time what concrete type you're acting on does this mean you need a second stage of compilation?
Sure, but changing a package manager to allow to install mutiple version is easy compared to the changes needed in the language itself to support it. You can see an example of that with Java and project jigsaw.
&gt; Simon is probably the only one in the world who could have ever pulled this off successfully Maybe that’s true—naturally he knows the RTS well—but the actual code implementing it is remarkably small and simple. Loading a shared object is not rocket science. My understanding is that the modification to the loader was really a bugfix: it wasn’t unloading code properly and nobody noticed because the feature was rarely used.
My biggest recommendation would be to get those packages into Stackage. It's just a pull request away, and then the problem's solved. Besides that: I don't remember the details, but you can set up a local repository in a cabal file and add in exactly the files you need. You can also create your own Stackage snapshots containing the packages you want, but I haven't (yet) documented the file format.
Yes, and I do that, but often I only need one more step, and writing 3 lines with intermediate state instead of one more surrounding thing feels unnecessary. It doesn't scale very well, but I'm happy up to around 80 characters. Thanks for the name, though. It's good to have something to look up.
Just did a small test. I added both `remote-repo`s at the same time and cabal works with both. `cabal update` reads both and `cabal install` finds the missing packages from Hackage automatically. I am not sure what the longer term consequences are but that's quite amazing. 
Note linear secret sharing much simpler and faster but less flexible (in the number of parties and their interactions). EDIT: I originally thought this post was in crypto, let me expand on my statement. Given a secret, `s`, you can share this secret with any `N` participants by generating `N-1` random values, `s_1 ... s_{N-1}` and computing `s_N = s - s_1 - s_2 - ... - s_{N-1}`. Distribute the `s_i` values as the shares and discard the original secret, `s`. In this manner you end up with information theoretically secure sharing and the security is painfully obvious - all but one of the values are just random while the Nth is encrypted via `N-1` one-time pads.
I'm actively seeking feedback, if anyone has any corrections, suggestions, or requests?
One nit: data Item = String ByteString | List [Item] Whoa! You just defined a constructor with the same name as one of the most common (sadly) types in Haskell. That's going to be confusing sometimes. I'd probably rename String to something else here. I really like this bit: &gt; Some of the bugs QuickCheck found would only show up on very specific examples — like a list containing a single string, which consists of a single byte with a value less than 128. Testing at that level of granularity is infeasible to do by hand, so the automation is more than welcome. The example here really shows how cool QuickCheck is :) All in all, I really like this writeup! Thanks for making it.
You might want to have a look at Data.ByteString.Builder in bytestring =&gt; 0.10.4.0. They provide you with the proper O(1) concatenation that you want when constructing sequences of bytes from multiple fragments. Both strict and lazy bytestrings have O(n) concatenation costs, which essentially means that you end up with O(n^2) serialization costs.
Thanks for reading! I figured I could get away with the constructor `String` because it's never used in type position, and the `RLP` module is always imported qualified (so you normally see `RLP.String` and `RLP.List`). Does that make it better, or is it still misleading? I've been trying to match the naming in the spec, but it may not be worth it here...
Great post/suggestions for a path to learning haskell! I started reading 3 books on haskell (beginning haskell, Algorithms: a functional approach, and haskell road to logic), I keep jumping from one to another (mostly for fun). I'll also try your recommended path now!
cis194 has rough edges but has worked better than everything else we've tried. Ping me if you need help. I'm bitemyapp, my email is on the github account.
Fair 'nuff - it's not a big deal at all, and given the usage seems fine - it just jumped out at me :) Now that /u/tel mentions it, it never bothered me in `aeson` either, so guess it's fine.
Nice write up! I'm glad to see that my post about setting up a Haskell project was useful.
If i remember correctly the GHC package db just doesn't support this, packages are uniquely identified by name and version. I blindly assume that fixing Cabal after that is pretty straight forward. 
Very much so! I only wish I'd come upon it a bit sooner...
I'm not sure what you mean, because we have package imports as extension, but the work to allow cabal+ghc-pkg+ghc to deal with multiple instances of the same version is still not finished.
Thanks for bringing it up, regardless... I was on the fence about it, so the discussion's been useful.
 $ ghci &gt; let fibs = 0 : 1 : zipWith (+) fibs (tail fibs) &gt; take 10 fibs [0,1,1,2,3,5,8,13,21,34] 
The classic.
 &gt; replicateM 3 (return "hello") ["hello","hello","hello"] "Okay, so Haskell's got a builtin which does the equivalent of `Array.new(n) {...}` in ruby. What's the big deal?" &gt; replicateM 3 "01" ["000","001","010","011","100","101","110","111"] "Whoa, what magic is this??"
Haskell's main strength doesn't just lie in little snippets of code golf. How about showing someone how easy it is to refactor existing haskell code using the type system. I.e. adding a case to some variant or changing a field in a record and then just following the compiler's errors. Usually if you show this to someone from a js/python/ruby background they'll be like "wtf you can't do that safely without 200% unit test coverage".
Often people get stuck on applicatives, not necessarily because they necessarily get stuck on the exercises but because cis194 doesn't give a "bird's eye view" very effectively. It does a good job of motivating applicatives, IMO, though. Other common sticking point is 1st or 2nd chapter, so don't feel bad if you do! cis194 leans too heavily on LYAH and RWH as supplement resources in the earliest chapters!
Freenode IRC channel #haskell-beginners has people going through this guide in order to learn Haskell. Experts are also present to help.
Correct. In fact we'd had several requests in the past to fix the linkers unloading facility, but it was never particularly high priority until recently. As for bus factors; well, I'm not sure. There is a bus factor here but I don't think it was increased substantially by this. I can say that the RTS's ELF linker is remarkably stable and well tested for a while now (ELF is a very stable format that's well documented), so relying on it isn't *too* much of a gamble, I'd say.
Hamming numbers, aka 5-smooth numbers. hamming = 1 : foldl1 merge [ map (n*) hamming | n &lt;- [2,3,5] ] merge (x:xs) (y:ys) = case compare x y of LT -&gt; x : merge xs (y:ys) GT -&gt; y : merge (x:xs) ys EQ -&gt; x : merge xs ys This generalizes to any prime very easily, and generating b-smooth numbers turns out to be a key step in the quadratic sieve, so it isn't just a puzzle for the sake of it... (the above implementation borrowed from here: http://conway.rutgers.edu/~ccshan/wiki/blog/posts/Hamming/)
I've always been fond of this sort of magic: data NumExp = FromInteger Integer | Add NumExp NumExp | Sub NumExp NumExp | Mul NumExp NumExp | Abs NumExp | Negate NumExp | Signum NumExp deriving Show instance Num NumExp where fromInteger = FromInteger (+) = Add (-) = Sub (*) = Mul abs = Abs negate = Negate signum = Signum main = print $ (sum [1, 2, 3, 4, 5] * 6 - 7 :: NumExp) You could write an interpreter and a pretty-printer as well, but that's more code.
or the even magicier fibs = fix $ scanl (+) 0 . (1:)
Replacing all elements of an int tree by their minimum in one traversal: repmin t = fst p where p = repmin' t (snd p) repmin' (Tip n) m = (Tip m, n) repmin' (Fork l r) m = (Fork t1 t2, min m1 m2) where (t1, m1) = repmin' l m (t2, m2) = repmin' r m
Thanks! I'll keep that in mind.
Except that'll make you run into the dreaded monomorphism restriction.
I believe that is the List Monad.
fibonacci: let fib = 0:1:zipWith (+) fib (tail fib)
damn you monomorphism restriction. Damn you
For some reason it actually took about 15 minutes to even understand what the hell it was doing, and haskell's terse syntax is odd enough for me that it took a while to understand entirely what I was doing with the "i", "fn", "01" etc. The part that was making me spaz out was that, once I understood it, I was able to successfully guess the next intuitive leap in logic and make the more complicated version without consulting anything. Yeah, it's really simple in retrospect, but the syntax from haskell is different enough from C++ that it throws me for a loop sometimes. It doesn't help that haskell can be about 50x more verbose, but that experienced haskell devs get into the habit of writing extremely concise and obfuscated code that makes sense only to others experienced in it. tl;dr I'm an idiot, but I'm working on it.
Strict bytestrings are simply a series of individual bytes, and they store their length separately, so they've got `O(1)` length lookup. It's lazy bytestrings and `Text` that have linear length calculation.
Good to know! It does make me curious, though. Is fix always user defined or is there a common library "most everyone" imports that contains the fix implementation/definition? It seems odd to have something like 'fix' have behavior that could vary from application to application. Or is fix just a random name chosen for the snippet and not a commonly used idiom like I'm guessing it is?
And it's a quick hop from there to automatic differentiation, which is awesome all on its own.
Dumb question. Why are there only links to some of the papers?
I figured there wouldn't be a lot of sources for it. People tend to like simple five second whiz bang "ooh shiny". (Of course, how hyprocritical of me to say that since I made the entire topic to basically ask for whiz bang...) Thanks for the article! I'm taking a look at it now
It's in [`Data.Function`](http://www.haskell.org/ghc/docs/latest/html/libraries/base-4.7.0.0/Data-Function.html).
What a a great set of papers! So glad I'm going this year.
Thanks for the close reading. That code takes advantage of a symmetry that's not explicit in the spec. Once you've serialized and concatenated the list contents, the only difference between the logic is a single bit in the first byte: the first two bits are always `10` for a string, and `11` for a list. (That corresponds to the `0x80` and `0xc0` 'offset'/'sentinel' values.) Subtracting out those high bits gives you `0xB7 - 0x80 = 55` and `0xF7 - 0xc0 = 55`. (And why 55? 63 is the largest number that can be represented in the remaining six bits, and we need to save eight values at the top for the length serialized as a 64-bit integer.) Does that make more sense? I think part of the confusion may be the constant switch between treating bytes as bit-flags and as small integers. I'll see if I can do another pass to make that aspect more obvious.
 &gt; replicateM 2 "abc" ["aa","ab","ac","ba","bb","bc","ca","cb","cc"] &gt; replicateM 2 (Just 4) Just [4,4] &gt; replicateM 2 Nothing Nothing &gt; replicateM 2 (Left 4) Left 4 &gt; replicateM 2 (Right 4) Right [4,4] &gt; replicateM 2 . map (+1) $ [1..4] [[2,2],[2,3],[2,4],[2,5],[3,2],[3,3],[3,4],[3,5],[4,2],[4,3],[4,4],[4,5],[5,2],[5,3],[5,4],[5,5]] &gt; replicateM 2 . fmap (+1) $ Just 4 &gt; Just [5,5]
Great job, looks clean. Ironic how similar the project I'm working on corresponds to what you wrote above. Only thing I could think of that might be advantageous is a way to tell the thread manager to stop forking threads (for when you might need to swap out the binary, won't lose queue messages this way). Maybe have a global data Status = Reading | Stopped that gets read by the thread manager before a thread is forked. Also, GCStats (GHC.Stats) has a currentBytes used field for watching your memory consumption, nice if your worker threads are I/O heavy. Really like your rate-limiting idea. As you mentioned, async is great for handling the workers propagated exceptions.
Well, novices coming across expert C++ code are likely to be pretty lost as well. When you are new to a language, the simplest and most obvious things can be baffling. Even when the names of identifiers are suggestive of what's going on, that can mislead you as often as it informs you. I'm not making any comparisons in "ease of learning" for syntaxes, not really interested in doing so, but just commenting on how we eventually become comfortable with new things to the point that we forget the original difficulty when we approach a new "new" thing.
only some preprints are online, or have been located.
Neat -- I'll do another pass and see if I can make that more obvious.
You might also want to implement some kind of supervision for workers, so that if a worker dies for whatever reason, it gets restarted. Sure, that shouldn't happen for your particular workers, unless a writer puts a partial value into the queue, but in a lot of real world code it's harder to make such a guarantee. And of course, the Erlang approach is to "code the happy path" and let exceptional conditions (e.g. a network timeout) raise an exception, terminate the worker, and then have the supervisor restart it. I've tried to adopt this approach as well into my haskell processes, with varying degrees of success. In my first production process, thread supervision is a total mess, but I'm working on replacing that process and the new supervision scheme is much much cleaner.
Great article, thanks! &gt;But what is this appl in our tree (haha)? This made me chortle the most ridiculous sounding laugh ever and I'm glad noone heard me
might I ask what you changed about your first production thread supervision code? 
I still think people should check out the Haskell Wikibook more (and work to improve any parts you think could use help, although thanks to others like myself doing that already, it's quite excellent now).
Seems to be a general sticking point for a lot of people. When Brent Yorgey was on the [Haskellcast](http://www.haskellcast.com/episode/005-brent-yorgey-on-diagrams-and-the-typeclassopedia) (around 47 min) he mentioned that a lot of people had issues with applicative functors. He mentions he later splits applicatives into two sessions to help the students digest the ideas better. I concur with your second point. A friend and I are working our way through CIS194 and we slacked off after #2. He related that #1 felt like a big hump to get over because of the reading. (He's never really done Haskell before) We did the lesson together and it clicked for him eventually. Another person told me at a Baltimore Haskell meeting that he got to the Tower of Hanoi problem and was completely stuck. He just had no idea where to start in Haskell. I wonder if the class would benefit from a couple of more sessions inserted before #1, or a slight restructuring of the intro. I feel like it takes a bit of time to get used to thinking in Haskell and thinking functionally vs imperatively, especially for those of us that were brought up on C-derived languages. It's almost like I've had to re-learn programming at times.
The restructuring of the intro and making ToH less of a speedbump for people that haven't done FP before is a priority, but probably won't happen via textual material. My current interest is recording screencasts/code-alongs. We'll see how things go with the videos. I might go back to textual material after that.
the problem is that you use `answer` in the where - either addd answer as an argument to `loop` or (IMO better) just add the definition for `loop` as another `let`-binding just before `loop 5`
Please tell me more :) I didn't know they were related.
Although the `where` binding is after the body, it is syntactically *outside* the `let` where you bind `answer`, so `answer` is not defined in the enclosing scope of `loop`. You could try something shaped more like this: guessGame gen low high guesses = let (answer, genl) = randomR (low, high) gen loop n = do ... in loop 5 or guessGame gen low high guesses = loop 5 where (answer, genl) = randomR (low, high) gen loop n = do ... 
&gt;Only thing I could think of that might be advantageous is a way to tell the thread manager to stop forking threads (for when you might need to swap out the binary, won't lose queue messages this way). The only thing I've seen that was remotely similar was with WAI, but that's not the same thing and was request level. It would be nice if we had something like that. Rate-limiting would be a good follow. I didn't even really just mean async for exceptions, it's just promises are how I've generally sent results upstream if promise instantiated not by worker. Maybe it's redundant with channels + green threads but old habit of mine.
`let` expressions bind values to identifiers in themselves, you can’t escape them. you have to pass `answer` and `gen1` to your `loop` function, or use some kind of `StateT`. I’d personnally turn the `let` expression into a `where` since you don’t rely on any local values nor monadic actions. I use `let` for this kind of stuff: foo a b = do x &lt;- bar a b let y = somethingWithX x …
Sounds good - but I think the experience of /u/mbcook demonstrates that this deserves a documentation comment in your code.
Not an expert in this field, but it seems to me that there is a serious flaw in this implementation. The poster uses rational polynomials, but the hidden message can likely be assumed to have a only a certain limited size. A subset of the participants might be able to use this to deduce some information about the secret. Wouldn't it be better to declare in advance to all participants that the secret is an integer in the range `[0..n-1]` where `n` is a power of a prime number, and then do the calculation in the ring of polynomials over the finite field of `n` elements? EDIT: Arithmetic in finite fields is difficult to implement efficiently for large `n`. But still, it seems that this problem can be decomposed: the secret can be a sequence of a fixed number of bytes, for example, and each participant would receive a sequence of key bytes of the same length. Arithmetic in the field of 256 elements is easy to implement efficiently using a log table. As usual in crypto, you would first apply a known compression algorithm to the secret text to eliminate most of the information from the higher occurrence of certain bytes and sequences of bytes in text.
 data I a = I a a Now write a Num instance where the first entry of the pair follows normal arithmetic and the second follows the laws of a derivative under those operators. Ie I x x' * I y y' = I (x * y) (x * y' + x' * y)
Excellent, thanks! Another thing I noticed - you did not include [pqueue](http://hackage.haskell.org/package/pqueue) in your benchmarks. That implementation hasn't been updated in a while, but given that it is written by Louis Wasserman, I'm guessing that it's pretty good.
I just want to say thanks for fact-checking before posting your comment. I'm not being sarcastic, there are too many people that don't bother making even a cursory check of their thoughts before posting them. So thanks. 
Why did I not know about this earlier? - Printf-like syntax ("I like my `%`"). Since names like `t` (short for `text`) pollute the namespace a bit, they're included in a separate module `ShortFormatters`. - Type safety - `Text` compatible - `String` incompatible (probably a plus) Also note the [`Examples` submodule (source)](http://hackage.haskell.org/package/formatting-5.0/docs/src/Formatting-Examples.html) to provide basic usage examples.
This may be a dumb question, but what about this simple protocol: Generate x^1, x^2, …, x^k-1 randomly (possibly negative) and set x^k = N - x^1 - x^2 - … - x^k-1 . Then all participants can together reconstruct N by summarizing their respective x values, but if one x is not known, N could be anything. This seems much simpler and I see no drawbacks.
I've impressed friends with this little snippet: reader &lt;- forkIO $ forever $ recvLn serial &gt;&gt;= writeChan incoming writer &lt;- forkIO $ forever $ send serial =&lt;&lt; readChan outgoing And [in context](https://github.com/eightyeight/serial-penny/blob/a989a9a208c9ab23622e3b6aa5fc787bb55a101c/main.hs#L39). This is part of a serial interaction program, and these two lines set up the communication between the serial port (using `recvLn` and `send`) via two channels `incoming` and `outgoing`. Those channels are written to/read from by other threads in the application. For example, if a thread puts something into `outgoing`, `writer` will `send` it to the serial port. It's that simple!
This is a nice library, based in turn on a the nice [text-format](http://hackage.haskell.org/package/text-format) library by /u/bos.
This is nice. Why not wrap it up as its own executable and release it on hackage? A nice feature to have would be prompting for options not provided on the command line, with sane defaults for author and email, as in `cabal init`. Another nice feature would be an option for a program instead of a library, or both a program and a library.
I believe GHC supports this now, since packages are identified by hash. Looks like it was built as a [2012 GSoC project](http://www.gwern.net/Haskell%20Summer%20of%20Code#section-6). What's lacking is cabal support, I think.
I thought dependent types (or Template Haskell) were needed for things like this.
The polynomial method generalises to giving *n* people their own keys, enabling a subset of any *k* of them (*k* &lt;= *n*) to reconstruct the secret.
&gt; I think the Erlang way works better when you have a live console, good aftermath logs, and live metrics. We've not yet caught up to that AFAIK and plugins are not sufficient. Logs and metrics we can do a good job of though. True, but what alternative works better in Haskell? There's pretty much always going to be things you want, but that doesn't necessarily change the overall relative merit of two or more approaches. &gt; I wouldn't really put it as "restarting" a worker so much as checking live worker population and repopulating as needed. I don't know how to safely validate worker liveness though. Well, for a simple "keep N workers running", my idea would be to use STM, something along these lines: superviseWorkers :: Int -&gt; IO () -&gt; IO () superviseWorkers target worker = do current &lt;- newTVar 0 let dec x = atomically $ modifyTVar' x (subtract 1) forever $ do atomically $ do n &lt;- readTVar current if n &gt;= target then retry else writeTVar current $! (n + 1) forkIO (worker `finally` dec current) Of course, you might consider making the target number of workers a TVar so that you have some control over size of the worker pool, although this wouldn't really offer the ability to shut down workers in a timely fashion without more modifications. (Rather, you'd have to wait for them to terminate.) You'd probably want to log any exceptions. This doesn't deal with workers that get stuck in a non-productive or otherwise inappropriate loop. You probably want to terminate all the workers if the supervisor dies. And of course this particular restart strategy might not be the best for your app either.
Good point.
I thought this was based on Martijn's Holey monoid.
That's a Priority Queue though, not a Priority Search Queue, as far as I see. The difference is that you can't have fast insert/lookup/delete by key in a regular PQ.
I broke this function down, but one thing still eludes me: Why does scanl work here but not scanr? I thought left folds didn't work on infinite lists, and scanr is doing what I expected from scanl.
Sure, the first version was loosely modeled after OTP design patterns, and I'd still say that aspect was pretty successful, but as I needed to get something running quickly and I didn't have a supervisor handy, the supervisor was pretty deeply integrated with each individual service. The new version actually has a proper supervisor process that's separated from the services, and uses STM and async. I certainly did find STM, async, and reading Concurrent and Parallel Programming in Haskell to be useful in disentangling the code. 
It is. Underneath that, the actual formatting is done by text-format.
Wow, tasty looks great! I especially like how you can constrain the test suite using patterns. 
I've made a few small changes that should make this more obvious... thank you all for the feedback.
These aren't the slides used in the linked video. For example, the error @ 24:50 (where the slides in the video say "Every Applicative is a Monad") doesn't show up in the slides you link.
Uhm, probably that's a link to a previous version of the slides. Sorry, I currently don't have a link to the newer slide deck.
&gt; It might not be a good idea to do it this way, because you could end up with a version of the Cabal library incompatible with the one bundled with GHC installed in your package database. Instead, you can do that in a cabal sandbox, and then copy the resulting cabal executable from .cabal-sandbox/bin/ to somewhere on your path. Interesting! I'm not sure I can do that here, though -- the reason I'm suggesting upgrading cabal-install is because many package managers are still stuck on 1.16 and have no sandboxes. Something of a chicken-or-egg problem, it seems... Incidentally, why is installing into a sandbox more likely to be compatible than installing to the global db? 
Ah, I see. OK. In the blog post you linked to the Wikipedia article about plain PQ's. I didn't realize that you are really only interested in implementations that also provide a finite map interface.
it would be so useful to have those examples in the README to show up in hackage
Also, the length calculation for lazy bytestrings/text is linear in the number of chunks, not the number of bytes. The number of chunks will typically be quite small since chunks are sized around several kilobytes.
&gt; What properties do a heuristic metric need to satisfy for the A* algorithm to guarantee an optimal solution? If I recall my AI class correctly, all that's necessary is that the heuristic be [admissible](http://en.wikipedia.org/wiki/Admissible_heuristic), which is to say, it never overestimates the cost.
I thought `Text.length` was always linear in the number of bytes/codepoints/characters (can't remember which), since a single Unicode character can require multiple 16-bit codepoints? The documentation has `length` being `O(n)` for strict Text values, which implies that it's `O(n)` in characters. I would assume `length` would then also be `O(n)` in characters for lazy Text values, unless the chunk lengths are calculated and stored on initialization?
I seem to recall discussion about Hackage2 supporting a README or README.md file but don't remember what came of it.
Yes. thanks. Since each move takes a single piece exactly one step horizontally or vertically, the sum of the L1 distances is indeed an admissible heuristic. It must take at least that many moves to solve the puzzle. And according to the [wikipeida page about A*](https://en.wikipedia.org/wiki/A*_search), at least, that is sufficient for A* to be guaranteed to produce an optimal solution.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**A* search**](https://en.wikipedia.org/wiki/A*%20search): [](#sfw) --- &gt;In [computer science](https://en.wikipedia.org/wiki/Computer_science), __A*__ (pronounced "A star" ) is a [computer algorithm](https://en.wikipedia.org/wiki/Computer_algorithm) that is widely used in [pathfinding](https://en.wikipedia.org/wiki/Pathfinding) and [graph traversal](https://en.wikipedia.org/wiki/Graph_traversal), the process of plotting an efficiently traversable path between points, called nodes. Noted for its [performance](https://en.wikipedia.org/wiki/Computer_performance) and accuracy, it enjoys widespread use. However, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, although other work has found A* to be superior to other approaches. &gt; --- ^Interesting: [^A* ^search ^algorithm](https://en.wikipedia.org/wiki/A*_search_algorithm) ^| [^A ^Search ^for ^Reason](https://en.wikipedia.org/wiki/A_Search_for_Reason) ^| [^Search ^for ^a ^Star](https://en.wikipedia.org/wiki/Search_for_a_Star) ^| [^Search ^for ^a ^Method](https://en.wikipedia.org/wiki/Search_for_a_Method) ^| [^Search ^for ^a ^Supermodel](https://en.wikipedia.org/wiki/Search_for_a_Supermodel) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cigqp9u) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cigqp9u)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
/u/rpglover64 is correct, admissibility is enough to prove optimality for A* when performing a tree search. I said in the article that manhattan distance was consistent which is what one needs to show a heuristic for A* is optimal in a graph search. Given that finding the optimal solution is NP-hard, I'm not surprised solving the puzzle yitz posted would take a lot of space and time. I'm sure there are more efficient algorithms out there - might be fun to expler.
Oooh, nice, it's clicking together now. I'm thinking I need to pick up a book or textbook or something to learn the underlying theory of all this stuff. I'm not sure it's enough to "just learn" Haskell, but I don't know if there's any accessible (being a upcoming Sophomore undergrad) material on this type of stuff. Would your "Given..." be set theory or group theory or something else?
It's not really useful, but I really like how knot-tying code can melt people's brain. The following snippet creates a circular doubly-linked list from a list: data DList a = DList { left :: DList a , val :: a , right :: DList a } fromList :: [a] -&gt; DList a fromList xs = start where (start, end) = go xs end go (x:[]) prev = (this, this) where this = DList prev x start go (x:xs) prev = (this , end) where (next, end) = go xs this this = DList prev x next go _ _ = error "empty list" also, a classic: powerset = filterM (const [False ..])
Dang... Now that's elegant, and impressive. I can't even imagine trying to write a one liner for that in something like C++
`["hello"]*3`
&gt; In Haskell we can't really go from a less sophisticated type into a more sophisticated one (well, type-families allow for some cleverness Lack of real pi-types in Haskell is a killer (and the inability to promote String and Char with DataKinds), unless there's some trick for rewriting `printf` below so that you can pass it a `String` rather than the `FormatS` singleton type that I'm not aware of (based on a few minutes of toying with GHCi): {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE ScopedTypeVariables #-} module Printf where data Format = FEnd | FInt Format | FString Format data FormatS (fmt :: Format) where FEndS :: FormatS FEnd FIntS :: FormatS fmt -&gt; FormatS (FInt fmt) FStringS :: FormatS fmt -&gt; FormatS (FString fmt) type family PrintfType (fmt :: Format) :: * where PrintfType FEnd = String PrintfType (FInt rest) = Int -&gt; PrintfType rest PrintfType (FString rest) = String -&gt; PrintfType rest go :: FormatS f -&gt; String -&gt; PrintfType f go FEndS acc = acc go (FIntS fmt) acc = \(i::Int) -&gt; go fmt (acc ++ show i) go (FStringS fmt) acc = \(s::String) -&gt; go fmt (acc ++ s) -- printf :: ? -&gt; PrintfType f -- printf fmt = go (? fmt) "" printf :: FormatS f -&gt; PrintfType f printf fmt = go fmt "" In GHCi: &gt; :t printf (FStringS (FIntS FEndS)) &gt; String -&gt; Int -&gt; String &gt; printf (FStringS (FIntS FEndS)) "Hello" 5 &gt; "Hello5"
This is pretty wicked. Great example of using laziness to the same effect as explicit pointer fiddling in other languages.
The easiest implementation is palindrome list = reverse list == list This is also more efficient than your implementation, since you're calling last on essentially a singly linked list.
For the doubly-linked list, the code snippet doesn't look like it handles insertions or deletions or other common linked-list tasks. If I'm thinking this through right, since functional programming is all about being stateless (e.g. input is 100% responsible for output), then you shouldn't have the double-linked list creator handling any of that, but you should instead modify the list itself and then redo the list generation (except it looks like the linked-list lazily generates every time you modify the list?)
Not quite; because of the `return` in the Haskell version, I needed the Ruby version to also produce each value via a computation.
You could write a quasiquoter for the format.
That can still be achieved linearly by using higher dimensional vectors, and a linear transformation instead of the sum. Choose a transformation represented by a matrix of dimension `k x n` such that every `k x k` square sub-matrix is invertible.
Yes, this is completely static; if I wanted to do modifications I'd have to keep record of the number of elements (in a wrapper datatype or something) or add a constructor signifying the node where we "tied" the list. But even then we'd have to redo the generation, so as you say we could just operate on regular lists, and then convert. Well, I said this is not very practical...
The "magic" in Haskell for me is mostly in our ability to derive programs algebraically and reason about them equationally, which is difficult to put into a pithy sound bite. [This Sudoku solution](http://www.cs.dartmouth.edu/~cs8/F2011/notes/11/Sudoku.lhs), originally by Richard Bird, is a reasonably short example of this.
&gt;True, but what alternative works better in Haskell? Honestly, I try to avoid exceptions save for truly exceptional (unforeseen, broken substrate) errors and stick with Either for anything that can be anticipated. I think I'd want a side-channel/command channel to the workers before I started making the supervisor boost the number beyond a static amount. One of the nice thing about static worker pools is it's something you can configure https://hackage.haskell.org/package/reflection and tune to your workload and more or less nail down. Getting the heuristics and dynamics of an automated system is much harder. Also for now, particularly with Haskell, I care more about getting details/diving into the guts of a failure (logging, metrics) than I do trying to write code that can recover from it. At the most I'd have a process level (not spark/worker level) supervisor. (Angel?)
An example of testing some simple laws. Cancellation laws: f = fst · (f △ g) g = snd · (f △ g) can be checked with: {-# LANGUAGE PatternSynonyms #-} import Control.Applicative import Test.QuickCheck import Test.QuickCheck.Function -- Function under test, (&amp;&amp;&amp;) from Control.Arrow (f △ g) a = (f a, g a) -- Will be included in next QuickCheck release pattern Fn f ← Fun _ f infix 4 ~= (~=) = liftA2 (==) prop_cancellation₁ (Fn f) (Fn g) = f ~= fst . (f △ g) prop_cancellation₂ (Fn f) (Fn g) = g ~= snd . (f △ g)
Another point-free version is palindrome = (==) &lt;$&gt; reverse &lt;*&gt; id I'd say this is not idiomatic Haskell, especially for a beginner though, although it's not entirely unreadable.
Thanks! I didn't think it needed to be its own package since it's a template for hi, which is already a package. Any additional features (like [not generating extra files](https://github.com/fujimura/hi/issues/26)) would be a part of hi. I agree that it would be nice to give the option between a program and a library. As it is, it generates both and you have to remove the one you don't want.
Disclaimer: me and some colleges from Portugal, UK and Germany have been working on static analysis for inferring space costs of lazy evaluation. Although this work is in an early stage and can't (yet) provide costs for real-world Haskell programs, it can give some pretty accurate bounds for some toy programs. You can try out a web version of the analysis here: http://kashmir.dcc.fc.up.pt/cgi/aalazy.cgi Note that you need to read at least some of the paper (draft version downloadable above) to make sense of the analysis output. 
More details available on the wiki as well: http://www.haskell.org/haskellwiki/Hac_Boston
Right. I am not sure I like that either, and it isn't the approach I took personally learning. But it is only a problem in a few cases: not understanding how do-notation desugars into binds and whatnot is going to result in pretty big problems, but not understanding that [1,2..] desugars to fromThen 1 2 is not really an issue.
Hac Boston will be amazing. You should visit us. Bring your Haskell-using friends. Better still, bring your non-Haskell-using friends and turn them into your Haskell-using friends.
&gt; Well, I said this is not very practical... Yeah, it looks like it... At least it's still cool as heck, though; that counts for something.
This answer should help you: http://stackoverflow.com/a/23733494/1651941
Ah, gotcha. Once again, I'm confusing cabal-the-library and cabal-install. I'm glad this area of the stack seems to be settling down... I'm on Arch Linux, so I get bleeding-edge versions of pretty much everything, but not everyone's quite so lucky.
Very cool. Coincidentally, I've listed some interesting papers on the topic here: https://ghc.haskell.org/trac/ghc/ticket/9207#comment:10 Sorry for the lack of PDF links; I've got them locally stored (and IIRC I had to buy some of them).
burrito meat
You have to expand out the definition a bit more to see the magic :P (scanl (+) 0 . (1:)) (scanl (+) 0 . (1:)) ... Let's ignore the rest for now. What are the guaranteed outputs from the right scanl used as inputs into the left one? Well, before anything is taken into the right scanl, 1 is appended to it. That helps us get started scanl (+) 0 [1:] The output of this function is [0,1], and if we want to calculate further, we need another "layer" of fix unwrapped. So let's ignore this for now, and take this output and put it into our left scanl scanl (+) 0 [1,0,1] -- 1 is appended at the beginning of every fix output = [0,1,1,2] If you notice, this is the output of take 4 $ fix $ scanl (+) . (1:). We gain 2 extra fibonacci numbers for each run of scanl. Also by prepending the result list with 1 every time, we are ensuring that scanl cannot remove any elements from the list: it merely uses 1 in the accumulator sum and then throws it away, only to gain it again on the next iteration.
This is literally in the main lobby of the building that I work in. Looking forward to it! 
The point of Haskell's IO type is that it represents effects as first-class values, instead of *side* effects.
"Introduction to FP using Haskell" by Bird covers a lot of what you want. It's also a very good book overall, so it's worth getting. Note that while there is a chapter on laziness, I wouldn't jump straight there - there's a lot of related material from chapter 1 onwards, so it's definitely worth working through it from front to back.
I think it's a weird, unnecessary attempt to pretend like IO is pure.
This book is very up to date: http://www.amazon.com/Beginning-Haskell-A-Project-Based-Approach/dp/1430262508
Note that the answer isn't complete, at least from my point of view. On the other hand, that's why I made it a community answer, so feel free to add new information.
There's no pretending: it *is* purely functional. To be more specific, the IO type retains referential transparency and equational reasoning, neither of which hold in the presence of side effects.
Give it the right name, at least. :) The algorithm it implements is treesort, not quicksort.
Thank you so much! I'm out right now, but I'll read this later this evening when I'm back on my desktop!
how does this one work?
What about LYAH book, does it also have a community post like /u/qZeta one? What parts of LYAH are outdated?
So you cannot even combine formatters?
Can you point me to where you're getting this from? Each `ByteString` [stores its length inside itself](http://hackage.haskell.org/package/bytestring-0.10.4.0/docs/src/Data-ByteString.html#length), so finding the length really is constant time. Although lazy `ByteString`s might delay prior computations until you call `length`, it's still O(1) in the grand scheme of things. In `Text`, [the `length` is eventually performed on a `Stream Char`](http://hackage.haskell.org/package/text-1.1.1.3/docs/src/Data-Text.html#length), which [counts Char by Char](http://hackage.haskell.org/package/text-1.1.1.3/docs/src/Data-Text-Internal-Fusion-Common.html#lengthI), and doesn't seem to involve any chunks. Perhaps you're thinking of a different type? What am I missing here?
Also source: https://github.com/mankyKitty/leesp . And another of the speaker's projects [targetting llvm](https://github.com/mankyKitty/haskell-to-llvm-compiler). 
I recently read through (or typed through) most of the book LYAH. I only remember having problems with chapter 13 (online version). For instance, writer and state monads have different type signatures now and should be created using 'state/writer' instead of 'State/Writer'. 
This is old, but I'll link this for posterity: http://www.randomhacks.net/2007/03/10/haskell-8-ways-to-report-errors/ The author seems to be a fan of use `Monad` and `fail`.
Thanks! I'll have special care with Chapter 13 then o&gt;
MANKYKITTY I"M SO PROUD OF YOU although up to now i totally thought you were a girl
What if restarting that executable was really expensive (setting up new connections or loading non-trivial amounts of data into process memory) and you still wanted to update quickly?
Is that llvm project based on Stephen Diehl's lovely [tutorial](http://www.stephendiehl.com/llvm/)? Or vice-versa perhaps?
I was wrong about `Text`, but I know I'm correct about lazy `ByteString`s. First, the documentation says that the time complexity of `Data.ByteString.Lazy.length` is `O(n/c)`, where `n` is the length in bytes and `c` is the chunk size. The implementation confirms this since it is a fold over the chunks and therefore linear in the number of chunks.
Thanks for the reply! Still a little bit confused ... &gt; Well, before anything is taken into the right scanl, 1 is appended to it. That helps us get started So it just magically starts with an empty list? or the list [[]]? I can't get that to run in ghci: $ ghci * snip * Prelude&gt; scanl (+) 0 [1:] &lt;interactive&gt;:3:14: A section must be enclosed in parentheses thus: (1 :) Prelude&gt; scanl (+) 0 [1:[]] &lt;interactive&gt;:4:7: No instance for (Num [a0]) arising from a use of `+' Possible fix: add an instance declaration for (Num [a0]) In the first argument of `scanl', namely `(+)' In the expression: scanl (+) 0 [1 : []] In an equation for `it': it = scanl (+) 0 [1 : []] &lt;interactive&gt;:4:14: No instance for (Num a0) arising from the literal `1' The type variable `a0' is ambiguous Possible fix: add a type signature that fixes these type variable(s) Note: there are several potential instances: instance Num Double -- Defined in `GHC.Float' instance Num Float -- Defined in `GHC.Float' instance Integral a =&gt; Num (GHC.Real.Ratio a) -- Defined in `GHC.Real' ...plus three others In the first argument of `(:)', namely `1' In the expression: 1 : [] In the third argument of `scanl', namely `[1 : []]' however calling f with [] works Prelude&gt; let f = scanl (+) 0 . (1:) Prelude&gt; f [] [0,1] Prelude&gt; scanl (+) 0 . (1:) $ [] [0,1] so is there just a magical [] put wherever you need it when calling fix (i don't see it in the definition or implementation)? is there some logic as to why that would be and does it differ if fix is called with something other then a list? 
http://apfelmus.nfshost.com/blog/2013/08/21-space-invariants.html This is a good writeup. When I read it, I thought to myself "hey, this is the first person who's written down what I do."
Updating packages to the latest is the biggest trouble.
This. Also the ability to detect and automatically rebuild broken packages would be incredible
Somehow the ideas I'm coming up with aren't the most practical ones, but hey, just a few thoughts... One interesting idea would be automatically figuring out true version bounds. I have no idea if this is possible, but it'd be very cool to try a grid search over packages to figure out which versions work! I'd be happy to leave a tool that does that running overnight or on a server somewhere to tell me what the *true* version bounds for my package should be (in order to have it compile). Another interesting idea would be *suggesting* package boundaries. You could maybe do some sort of graph clustering algorithm on the module interdependencies, and suggest which data types and functions should go where. Something I've run into - given several packages that I want to have installed at once, verify that the package boundaries on all of them do not allow for a configuration such that the packages are installed with different versions. Weirder question, though. Excited to see what comes out of your internship :)
Regarding the Ubuntu instructions, why not just install haskell-platform?
How about working directly on Cabal? There's some nice things we could do if we had a cabal file "splicer" -- that is, something that can append properties to cabal files without ruining indentation or comments. One idea I ran by the cabal team a day or two ago was a "--save" flag for installations, that automatically appends things you install to a sandbox (while using the `--save` flag) to the dependency list, so you don't have to open the file.
It would also be nice if `cabal init` produced a `src` directory and set `hs-source-dir` accordingly. I keep having to do this manually, and *nobody* nowadays doesn't put their project in a `src` directory!
(and, a third post, so people can comment on the suggestions separately) a way to use predefined templates with `cabal init` would be cool. i.e. `cabal init yesod` for a yesod project with pre-made yesod boilerplate. These templates could be defined by packages when you `cabal install` them, so you could ie `cabal install pipes` and then `cabal init pipes` for a pipes playground or even `cabal install pipes-server-backbone` and `cabal init pipes-server-backbone` where `pipes-server-backbone` is a package just to provide the init template!
I seriously want my build system divorced from my package manager. 
But, why?
Syntax, the comma-requirements in multiline `build-depends` field are irritating (while `exposed-modules` and `default-extensions` have no such requirements).
I'd want a way to manage dependencies. Often after developing (in GHCi) you have to manually add all the dependencies one after the other. Then, after refactoring there are unneeded ones, but no way to find out which ones. This would probably have to be done in cabal itself, but I think that's a good idea in general. Features in cabal will be exposed to many more people. 
de-installing specific packages and listing installed packages which don't correspond to the latest available version
At my first day I was told about signatures: They showed me the signatures of head, fst, snd, map, zip and foldl and I had to write them myself. Then we tested those in ghci. It was great. 
I think the template packages should get their own name though. So that you can `cabal install pipes-example` to install it and try it out without creating a new package. Then when you know you like it `cabal init pipes-example`. Leksah supports something like this now and hopefully [hi will too](https://github.com/fujimura/hi/issues/30). When you create a new package in Leksah it gives you the option to choose an "Existing package to copy". It even suggests a few (hello, gtk2hs-hello, ghcjs-dom-hello and jsaddle-hello). It would be nice if we had a `category` for template packages so it could easily list more as they are added to Hackage. 
you can use [hi](https://github.com/fujimura/hi).
i'd like a simple command-line ui to update version bounds and manage dependencies. the following might bump package version 1.2.3 to 1.2.4. &gt; cabal version +0.0.1 &gt; cabal dependency add text # will add text as dependency &gt; cabal dependency version text &lt; 1.2 # set upper bound 1.2 for text &gt; cabal dependency version text latest # accept latest text according to pvp (of course it should have switches for which target is wanted (testsuite a, executable b, etc).
Same here! I'm in chapter 5, Higher Order Functions. You may have saved me some confusion. Thanks!
%. will feed one formatter into another.
Unfortunately the C parts of packages will always be sketchy with Haskell, but that's C's fault. It would be better for everyone if people use the FFI more in their packages to call system stuff directly (the kind of stuff that just needs linking, not building) and use Haskell, rather than C, as the glue. For example, the new IO manager is written in pure Haskell! With some heavy modification (Haskell, after all, can use the C pre-processor to detect platforms), `network` could be too -- the one package I dread building on Windows, ever.
I chickened out of Haskell on Windows, bought a macbook and never looked back.
(=&lt;&lt;) often reads better: palindrome = (==) =&lt;&lt; reverse
The simplest solution, in my opinion, would be to simply pass `answer` as an argument to `loop`.
That's proper weird. Do you have anything at all in your .ghci?
Yeah, I set the prompt and defined `:hlint` and `:hoogle` commands. I moved my entire `.ghc` out of the way and tried again, same output. This is on 7.8.2. Not that it really bothers me, I didn't know about `:sprint`.
Something like `$(printf "%s")` to generate `printf (FStringS FEndS)`, or [fmt|%s|] to generate `FStringS FEndS`. Like [this](https://hackage.haskell.org/package/Printf-TH-0.1.1/docs/Text-Printf-TH.html). But I don't think many people tend to use it. People shy away from TH.
Or using `free-functors`: {-# LANGUAGE TemplateHaskell, TypeFamilies, DeriveFunctor, DeriveFoldable, DeriveTraversable, FlexibleInstances #-} import Data.Functor.Free deriveInstances ''Num main = print $ (sum [1, 2, 3, 4, 5] * 6 - 7 :: Free Num Int) which prints (((((fromInteger 0 + fromInteger 1) + fromInteger 2) + fromInteger 3) + fromInteger 4) + fromInteger 5) * fromInteger 6 - fromInteger 7
Ah, OK. I was thinking you could use a dependent pair to write a function that lifts a `String` into a `Sigma (FormatS f)`, or something like that, but having played with trying to do it briefly I can't get GHC to like the idea (plus, I'm not sure it would work now anyway as the dependent pair will mess up the return type of `printf`).
The monad section is a little bit outdated, since the AMP passed and `Applicative` _will be_ a constraint for `Monad`. Also, monad transformers, for the same reason as in RWH. Other than that, LYAH is more or less an appetizer than a full meal.
I tend to use :sprint for the sake of trying to gain an intuitive knowledge of what's evaluated when. (In fact, that's precisely what the example is supposed to be about), so not having it work properly would be annoying. If you do want to understand this, you'll need someone more experienced than I to help. :(
It's kinda already there. `cabal` is a build system, `ghc-pkg` is a package manager.
I think so too. Perhaps we could make it not required. I wonder if it's there for a reason. I guess dcoutts would know.
Nobody doing it is perhaps a good reason not doing it by default. Perhaps they don't want a src directory. :)
I would recommend hacking on cabal itself and try to get your features in there. It increases the number of users who would benefit enormously. For example, once sandboxing was added to cabal itself basically all the competing ones stopped being used, because using a shared, available to all solution is very valuable.
&gt; Other than that, LYAH is more or less an appetizer than a full meal. Mind you, it's a delicious appetiser that made me crave the rest of the meal.
I can see the issues with comments, so they most likely should be part of the internal cabal representation, but I never quite understood the issues with the formating of the cabal file. Like every programmer ;), I have strong opinions regrading the formatting of source files, but I don't care that much about the cabal file formatting, as long as it isn't completely out of place. I don't read all day cabal files and they're even not that complicated, so caring that much about the formatting seems illogical. 
If you want to operate with the Cabal library, then using lenses and traversals is quite nice and keeps you sane, if you've to consider the conditionals inside of the cabal file. For my own two cabal tools I've started the small library [cabal-lenses](https://hackage.haskell.org/package/cabal-lenses-0.2). 
I think medium term we should just enforce one kind of formatting. There's already a `cabal format` command that does a reasonable job, but we need to polish it until it's good enough. Once we have a normalizing format, we can much more easily do automatic rewrites of .cabal files.
This is turning into a silly discussion. The conventional definition of a [side effect](https://en.wikipedia.org/wiki/Side_effect_\(computer_science\)) is when the evaluation of some function or expression observably modifies some state or interacts with the outside world: that's what makes it a **side** effect. By definition, side effects are tied to the order of evaluation: the way you invoke a side effect is by evaluating its expression, and the way you avoid invoking it is by *not* evaluating its expression. This is the foundation of how imperative programming languages work. [`unsafePerformIO`](http://hackage.haskell.org/package/base-4.7.0.0/docs/System-IO-Unsafe.html#v:unsafePerformIO) is a Haskell example of a function with side effects. It is "unsafe" and not part of pure Haskell because Haskell does not specify any particular order of evaluation, and hence makes the evaluation and ordering of side-effecting functions like `unsafePerformIO` undefined in general. Pure `IO` , on the other hand, involves no side effects at all: you can evaluate an arbitrarily complex composition in absolutely any possible order, and you'll always end up with exactly the same compound `IO` result value. It's impossible (modulo Haskell's `unsafePerformIO` et al.) for any part of the program to change, affect, or alter the resulting `IO` actions by introducing side effects as part of their evaluation. You will always get *precisely* the same compound `IO` action out, and you are guaranteed that no part of your program can introduce or remove any further `IO` actions at run-time (for example, in response to user input), because the `main` IO action is entirely fixed at compile-time. That, by definition, is what makes `IO` pure. There is nothing specific to Haskell about this. You can define the same `IO` type in any imperative language, too, and compose it in a pure subset of the language: the result will still be a pure `IO` action that's immune to any side effects from the pure subset of the language. (You might object that you may need side effects to implement the IO executor in your imperative language, but that's irrelevant to the pure part of the program: the whole point of pure IO is that any side effects inside the executor remain purely internal, and cannot leak into or out of the pure part of the program. By analogy, it would be no different to the fact that Haskell compiles to imperative machine code, even though it is a purely functional language.)
As I said in the bug tracker. If people think this is what most people want, it's easy to provide data for that claim by downloading the Hackage index and checking how many .cabal files set hs-source-dir(s) in the library section. Please don't spam the issue tracker with "me too"s.
At least half set the directory: kvanb at entropy in ~/Downloads/index $ grep -l -r -i "hs-source-dirs" . | grep -c . 22336 kvanb at entropy in ~/Downloads/index $ grep -l -r -i "name" . | grep -c . 41814 I still don't think that's a good way to survey it. Lots of the packages on Hackage were not even intended to be used or modified by others. There's a lot of good stuff on Hackage, but there's also a lot of not-so-perfect examples of a good package. It's definitely a lot easier to undo the configuration (delete the line and folder) than to add it to every new package manually.
Unless they explicitly mention laziness, it's probablys safe to assume that's the time if the result is fully used and the function is not charged for the arguments (maybe one unit each time a value is forced, or they are supplied fully evaluated - yes, even the infinite ones).
I think this is because in 7.8+ `XNoMonomorphismRestriction` is enabled by default. GHCi, version 7.8.2: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Prelude&gt; let a = [1..] Prelude&gt; :t a a :: (Num t, Enum t) =&gt; [t] Prelude&gt; a !! 4 5 Prelude&gt; :sprint a a = _ Prelude&gt; let b = [1..] :: [Integer] Prelude&gt; b !! 4 5 Prelude&gt; :sprint b b = 1 : 2 : 3 : 4 : 5 : _ Prelude&gt; --- GHCi, version 7.6.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Prelude&gt; let a = [1..] Prelude&gt; :t a a :: [Integer] Prelude&gt; :set -XNoMonomorphismRestriction Prelude&gt; let b = [1..] Prelude&gt; :t b b :: (Enum t, Num t) =&gt; [t] Prelude&gt; 
The monomorphism restriction might actually have helped here. Start instead with. let a = [1..] :: [Int] alternately, you could blame it on `:sprint` also printing evaulated functions as _. If the output was like λ: let a = [1..] λ: let b = map (+1) a λ: :sprint a a = &lt;fun&gt; it would have been less confusing. The type information from `:print` might help: λ: :print a a = (_t18::(Num t, Enum t) =&gt; [t]) 
1. Signed packages. It's really a giant security disaster that cabal exists and doesn't support this. 2. As far as automatic management of .cabal files: there should be a system that automatically generates correct maximum version constraints when it is known that compilation will fail. Actually, that should run on hackage (where the newest version is always available), but the first step is to implement such a thing in cabal. The biggest frustration with cabal, in my experience, is the absence of these constraints. And it's all-the-more frustrating to see that the errors are practically always caught at compile-time, so that they *could* have been avoided completely by smarter tools.
To give my own reason... cabal packages have dependencies on software that isn't managed with cabal at all, and isn't written in Haskell. The Debian package manager can pull in those dependencies, but cabal can't.
I can't help but feel that Template Haskell is best suited for this task: one can parse the same old "printf" syntax to generate the formatters at compile-time and with compile-time checking of the formatter's correctness. E.g., [__th-printf__](http://hackage.haskell.org/package/th-printf) approaches this with quasiquotes: -- String interpolation [s|Hello, %s!|] "Jeff" -- "Hello, Jeff!" -- Width specifiers [s|%010d|] 1977 -- "0000001977" -- Different radices [s|%d, %x, %o, %#x, %#o|] 100 100 100 100 100 -- "100, 64, 144, 0x64, 0144" -- Variable-width formatting [s|%0*d|] 5 10 -- "00010" 
Well then, my workaround is to always place the `base` dependency the last. Since it is a dependency of absolutely any project, it is consistently the only dependency without a trailing comma. E.g.: build-depends: -- debugging: loch-th == 0.2.*, placeholders == 0.1.*, -- general: focus &gt; 0.1.0 &amp;&amp; &lt; 0.2, hashable &lt; 1.3, primitive == 0.5.*, base &gt;= 4.5 &amp;&amp; &lt; 4.8 
I have [good news for you](http://www.reddit.com/r/haskell/comments/295lbr/announce_stmcontainers_a_hash_map_and_hash_set/).
Haskell Platform causes problems for beginners that beginners don't really know how to cope with. I steer everyone away from it and will probably talk to the Platform team about making some changes in the near future.
&gt; It provides very efficient implementations of Map and Set data structures, which are slightly slower than their counterparts from "unordered-containers", but scale very well on concurrent access patterns. Any benchmarks to support this?
Adding an extra unnecessary parameter to loop is hardly simpler than moving the call to randomR to a where clause. Why re-bind it on every iteration when it's never going to change?
Not the best statistic. Many Haskeller's no little of cabal in general, and do not even realize that hs-source-dir exists, however still like have a src directory.
Is there any downside to always installing dependencies with profiling support?
It's the same debate between regular expressions and parsec. One is short and used in other languages and not extensible, the other is longer but first-class and extensible.
It takes twice as long and twice as much disk space. Otherwise, no.
No reason to not have both. TH for quick convenient use and proper functions where you would build a more complex composable tools. 
Posted because Hashable and unordered-containers has been on my brain lately.
Yeah, the audio is terrible, I'm afraid.
Sure. [The source](https://github.com/nikita-volkov/stm-containers) comes with two benchmarks, which you can run with `cabal bench insertion-bench` and `cabal bench concurrent-insertion-bench`. I'm writing a post with analysis of results, which I plan to publish in the coming days.
Umm... I'm not sure for universality of `Maybe`. Given some `BigProgram` you get a "big" `Nothing` at runtime if something is wrong, exactly as exceptions (moreover, exception report source and error kind). Of course `Maybe` is a better way to catch these wrong results but: f &lt;$&gt; a &lt;*&gt; b &lt;*&gt; ... not inform about what `a`, `b`, ... fail (one exception do it). A better approach could be `Either`, you get the same "big" `Nothing` and one message about the source problem. I think the matter is very close to `Maybe` vs `Exception` problem. Then, how well will scale a "universal" `Maybe` usage? I'm not sure... (when you get a "big" `Nothing`).
Very nice, thanks! I'm looking forward to your analysis. :)
I definitely was not trying to say people should use Maybe for all error handling and I miscommunicated if that's the impression you got. I thought I typed out an example saying I preferred Either. The point was that Maybe is preferable to things like "head []" -&gt; error "Prelude.head ..."
Would someone mind writing a short abstract?
oh. it's nice to have cabal format. i will use that as a git hook i guess.
&gt; for all error handling and I miscommunicated if that's the impression you got ouch! i'm sorry (yes was my impression) 
It was my fault for not being more clear.
Hi, Author here. So there's been about a two month delay since the last part, but just wanted to wrap it up. I wanted to make this last part more or less a pipes tutorial; there are some lens concepts sprinkled in. My understanding of lens is still shaky, and I'm also fairly new to pipes, so any corrections or help would be greatly appreciated, for the good of future readers and of the greater mankind :)
Thanks, I'll take a proper look through your patch in the next few days. I think your analysis is good, but it might need to be more selective about which args go where. Fortunately, we only really have to get the cabal case right and can be conservative everywhere else.
Hmm ... These should probably live under `Data.Map.STM` and `Data.Set.STM` respectively, shouldn't they?
This is all correct, but there are also cases where `foldr` works on an infinite list and `foldl` doesn't. Actually, you can implement `foldl` in terms of `foldr` but not visa-versa. Scans are different, but there are also cases where `scanr` can work on an infinite list (not here though, you need `scanl`).
Ugh, fuck packt. Can we just pirate the book and paypal you the money?
Seems to be a great book by the cute Chapters Summary. Very cool to have a little image/code as a "quote" for each chapter.
Seems like a great book, congrats! btw, what's my best option of buying a hardcopy outside the US?
I really liked the screencast, and I recommend watching. But for those that want a little tl;dr (I'm a beginner but I'll give a try): * **Prelude is dangerous** : Prelude comes with several functions that do not have correct type signatures. Head should be [a]-&gt; Maybe a , and exploding on empty list is plain wrong. Another problem is that some functions treat List as a finite sequence (like length), when a list can be infinite. length will crash your program on an infinite list. As such, one should 1- avoid partial functions on prelude (like head), using pattern matching (x:xs and []); 2- Use correct signatures ([a]-&gt;Maybe a); 3- having length only for sequences that are guaranteed to be finite, like arrays/vectors. * **String and Ints on type signatures are code smell**: Most of the times, one does not want to mean that our function receives any possible strings. Also String is a terrible type, being a synonym to [Char] and having the same problem of conflating finite vs infinite (induction vs conduction). Does the function really deals with receiving infinite strings? As for Int, does the function really expects all the 18446744073709551615 possible values of an Int? Take as an Example, a Webserver Host Port : there's only a limited number of strings viable as a host, and an even shorter range for Ports. For start, its best to create newtypes for Host(String) and Port(Int). Then, an abstract data type (like data Webserver = Webserver Host Port), following one can hide the constructor (by not exporting it), then create helper constructor functions (mkHost :: String -&gt; Maybe Host), finally use applicative when using a Constructor that does not expect the type to be wrapped in a Maybe: server = Webserver &lt;$&gt; mkHost "google.com" &lt;*&gt; mkPort 80 * **Record syntax is dangerous**: an abstract type that can have values where only some are Records is incorrect, and bad typing. It messes type signatures, and make them fail: data BLah = Woot | Alt {access :: Int } access :: Blah -&gt; Int access Woot -- Explodes One should Split the record type, separate from an Abstract Data Type (when its values are a Sum of Products?). Like this: newtype AltProxy = AltProxy {access::Int} data Blah = Woot | Alt AltProxy access :: AltProxy -&gt; Int blah Woot - type error: correct typing! * **Granular Types** : quoting the author here "Splitting out more granular types allows you to circumnscribe value inhabitants of types in a way reminiscent of what dependently typed languages will allow. (On a next episode of the screencast if the audiance wants)"
How much Haskell knowledge is assumed?
Are there any sample chapters available? Also, the book/buttons wiggling on this page feels pretty weird to me
You might mention how much I emphasized inductive vs. coinductive types. For granular typing, this article I wrote should demonstrate: http://bitemyapp.com/posts/2014-04-05-grokking-sums-and-constructors.html
Please humor the ignorant. What's so bad about packt?
http://www.packtpub.com/sites/default/files/9781783286331_Chapter_01.pdf?utm_source=packtpub&amp;utm_medium=free&amp;utm_campaign=pdf
Good knowledge it seems: "A good understanding of data sets and functional programming is assumed." From: http://www.packtpub.com/haskell-data-analysis-cookbook/book - there's also a table of contents, and the book seems to be incredible fun by it. Edited: Just bought it, see what is assumed on my other post on this same thread.
Alright, just bought the book. For those wondering, here's the "Who's the book for" section: * Those who have begun tinkering with Haskell but desire stimulating examples to kick-start a new project will find this book indispensable. * Data analysts new to Haskell should use this as a reference for functional approaches to data-modeling problems. * A dedicated beginner to both the Haskell language and data analysis is blessed with the maximal potential for learning new topics covered in this book. [I'm a beginner on both haskell and data analysis, I bought for reading a bit everyday - like a kata book]
Chris Okasaki's Purely Functional Data Structures is more or less the first and last reference on how to reason about asymptotics in a lazy functional programming language. http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504 The original version of his thesis, which forms the bones of the book is available online: https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf
I'm so excited! Exactly what I wanted!!! No google play? I bought "learn you a Haskell" which was ok, but this appears to have actual examples.
Thanks! All examples can be found on [GitHub](https://github.com/BinRoot/Haskell-Data-Analysis-Cookbook), where they are always kept up to date.
website is down for me at the moment.
hm... restarted the server, should be back up :) thanks :) 
 turtles = fix Turtle
Nah, we want to statically enforce turtles all the way down.
When the prerelease for this was announced, there was a link for O'Reilly as well. Do you know if it will be available there? 
Austin Haskell meetup? Where could I find out about future ones?
I've had a pretty bad experience applying newtypes extensively in algorithmic (mostly number crunching) code. They might be good for specifying nice APIs (and it's really nice that they basically come without any overhead in GHC), but I'm afraid that when used internally (not only in the interfaces of your modules), they just add too much boilerplate and distraction to the code. I believe dependent types are the future here: semantics of a type alias, still type-safe. What I'd really like to see is some form of well-integrated unit system. I know there are more libraries that try to do this, but I find it hard to rely on one-man projects with outdated documentation for my work (especially for something this crucial). Does anyone have similar problems?
You can use the `ghc-prof-options` field in your library/executable section to store the ghc options used when you `--enable-library-profiling` or `--enable-executable-profiling`.
Well yeah, as a rank beginner in Haskell one sure has the *potential* for learning Haskell. What resources that has the highest likelyhood to realise that potential is another question that I haven't found a conclusive answer to. It looks like a both fun and valuable book though. Thanks for the additional info.
Doesn't work. What is wrong? import Data.Word import Data.Int jumpConsistentHash :: Word64 -&gt; Int -&gt; Int jumpConsistentHash key buckets = go key ((-1)::Int64) 0 where go :: Word64 -&gt; Int64 -&gt; Int64 -&gt; Int go _ b j | j &lt; fromIntegral buckets = fromIntegral b go key b j = let new_b = j :: Int64 new_key = key * 2862933555777941757 + 1 :: Word64 d1 = fromIntegral (2^31) :: Double d2 = fromIntegral (2^33 + 1) :: Double new_j = (truncate $ fromIntegral (b + 1) * (d1 / d2)) :: Int64 in new_key `seq` new_b `seq` new_j `seq` go new_key new_b new_j 
Woops! I missed that. Then you might want to see Jim Apple's excellent summary of "new" functional data structures since Okasaki: http://cstheory.stackexchange.com/a/1550 I've personally been off working on ways to integrate Okasaki's methods with the tools of cache oblivious data structures. An older talk I gave at Mozilla on the topic is online here: https://www.youtube.com/watch?v=P3pLDpbzqCw but the slides from my more uplifting current results are here: https://github.com/meiersi/HaskellerZ/blob/master/meetups/20140606-ZuriHac_Edward_Kmett-Functionally_Oblivious/Functionally%20Oblivious.pdf?raw=true I'm not aware of any general purpose 'how to do it for everything you can write in a lazy language' tools. My recommendation is to never create a thunk without an understanding of when it'll be forced or that you're willing to leak the space until it will.
We have an open ticket for building normal/prof libs in parallel in Cabal. Once that's done it should be less painful to turn on profiling by default in ~/.cabal/config, if you have the cores that is.
Nice suggestion. Thank you.
Wow, that StackExchange answer is epic. And your own work also sounds really important, because cache obliviousness is obviously the right thing. You have nerd-sniped me, now I'll have to learn all that stuff. Thanks!
`Identity` is a newtype, so I don't think it's the same.
The blog post also hints at this (that this was already done by Church in the 30s). Do you have some references to (early) papers that use it? It would be great to try to track down who first came up with it. (Hej!)
It's turtles all the way down. http://en.wikipedia.org/wiki/Turtles_all_the_way_down
A dedicated "profiling" target, like "benchmark" and "test-suite".
Ah, that explains it.
Yes, plus it also just showed up at Amazon.de!
How come not `Control.Concurrent.STM.Map`? After all, `TVar` and `TChan` are there. See? It's a terribly ambiguous and hence an inconsistent convention, which I consider to be a historical mistake. That is why I follow [another convention](http://www.reddit.com/r/haskell/comments/1qrilm/packages_and_namespaces_naming_convention/), which solves the mentioned issues amongst others.
Sorry, I don't have any references. But when came up similar trick I had this feeling that I'd seen it before. Very unhelpful, I know.
That's not a joke, that's philosophy.
&gt; Usage: &gt; `./hs2lhs myprog.hs &gt;&gt; myprog.lhs` I can see that the monad-fu is strong in this one, but I think for normal shell redirection one `&gt;` is sufficient. :) 
What's the inhabitant of `Fix Identity`?
oops, thanks I'll fix it.
I made a screencast recently and found iMovie pretty good for this. You can increase the volume of a clip and reduce background noise. I needed to turn my volume up alot to hear your voice, but that also introduced a really annoying background noise (a loud fan or something).
http://www.meetup.com/ATX-Haskell/
Why not d | t == T.empty = (Code, "") | isPrefixOf "{-#" t &amp;&amp; isSuffixOf "#-}" t = (Code, "&gt; " `T.append` t) | otherwise = (Code, s `T.append` t) Saves you one extension I guess.
Thank you for naming your library something fairly self-descriptive instead of a cute play on words or something else entirely unrelated.
if you do (1 :: Int) &lt; 2.0, it will fail. It works because the literal 1 is polymorphic accross all Num until it is used in an expression that forces it to be monomorphic. 1 &lt; 2.0 forces the 1 to take the same form as 2.0, which is Fractional a, but 1 is not guarenteed to be Fractional, only Num. Try instantiating your own Num class, and use 1 :: YourDataType to see that it will work. Alternately drop into ghci and try ":t 1." You'll see it has the signature Num a =&gt; a, indicating that 1 is some value of type a where a is an instance of Num. Similarly ":t 2.0" will give you Fractional a =&gt; a. I encourage you to drop into #haskell on freenode where there are always people willing to help answer these sorts (and others) of questions quickly and accurately
Thanks for the detailed answer. 
Yes, that is better but then i don't get to use multiway-if :)
Shouldn't that be `| j &gt; fromIntegral buckets`?
https://github.com/goldfirere/units https://github.com/goldfirere/units-defs http://hackage.haskell.org/package/HoleyMonoid http://hackage.haskell.org/package/formatting 
I can't fight past iMovie's interface. Too infuriating.
`d2` is also completely wrong. It is supposed to be `fromIntegral (shiftR new_key 33 + 1)`
To be a little bit more detailed, here's what happens in the mind of the compiler: - Oh, I see a `1`. That's a number... I'll give it the type: `Num a =&gt; a` - Oh, I see a `2.0`. That's a number, but it has an explicit decimal. I'll give it the type: `Fractional a =&gt; a` - Oh, I see a `&lt;`. That has the type: `Ord a =&gt; a -&gt; a -&gt; Bool` - The top-level expression was `&lt;`, so let's see if I can match the type of `&lt;` to the types of its arguments... - I see that the two arguments to `&lt;` need to be the same type - let's call it `t` - and furthermore, `t` should match `Ord a =&gt; a`. Furthermore, `t` needs to match the types `Num a =&gt; a` *and* `Fractional a =&gt; a`, since those are the types of the subexpressions. The heads of those types are all the same, so that works, and I just need to merge the constraints. - So, `t` should be `(Ord a, Num a, Fractional a) =&gt; a`. - Oh, `Fractional` is a subclass of `Num`, so the `Num` was redundant. Now `t` is just `(Ord a, Fractional a) =&gt; a`. - There are no more constraints, since the `&lt;` ocurred at the top level, so I'm done! - Wait... this type `t` doesn't actually occur in the type of the final expression, which is just `Bool`. So I need to choose a specific monomorphic type, so I can actually pick an implementation of those operations... - Time to apply defaulting rules! They tell me that if I have a `Fractional` constraint, and `Double` works, then I should choose `Double`. - Good! `Double` works, so I'll choose `t = Double`, and now I'm really done. Crucially, the defaulting, which chooses `Double` for the types, happens *after* type inference when all those constraints are unified to reach a most general type for `t`. So while on its own `1` would have defaulted to `Integer`, in *context*, it keeps its polymorphic type until type inference is done. By that time, the compiler knows that it must also have a `Fractional` constraint because it's an argument to `&lt;` alongside another argument with a `Fractional` constraint. So the compiler is able to decide that `1` is a `Double` instead. This is a *global* decision, rather than the *local* decision you were expecting.
As promised: http://www.reddit.com/r/haskell/comments/29a2vy/a_post_on_motivation_behind_stmcontainers_and/
Thank you for mentioning defaulting rules! I wasn't aware of them. I really wonder why Haskell has overloaded numeric literals, though. It's not needed for overloading arithmetic operators, because ordinary type classes would suffice for that. What's the rationale here?
I wanna stress on the following: &gt; From the benchmark results analyzed here, you can see that 6 cores is really not enough to test all the potential of the data structure. That's why I'm asking those of you with huge amounts of processors to run the benchmarks and share the results. [Details here](http://nikita-volkov.github.io/stm-containers/#ben-i-need-help).
thanks, now it works import Data.Word import Data.Int import Data.Bits jumpConsistentHash :: Word64 -&gt; Int -&gt; Int jumpConsistentHash key buckets = go key ((-1)::Int64) 0 where go :: Word64 -&gt; Int64 -&gt; Int64 -&gt; Int go _ b j | j &gt;= fromIntegral buckets = fromIntegral b go key b j = let new_b = j new_key = key * 2862933555777941757 + 1 d1 = fromIntegral (shiftL (1::Word32) 31) :: Double d2 = fromIntegral (shiftR new_key 33 + 1) :: Double new_j = (round $ fromIntegral (b + 1) * (d1 / d2)) :: Int64 in go new_key new_b new_j 
I meant that the non-haskeller character who reads the first example and its output would not think that `replicateM 3 (return "hello")` is a clumsy way of writing `["hello"]*3`. That character is not trying to show off their knowledge of Ruby, but to understand Haskell. So, in an attempt to understand why the example contains a `return`, they would guess that the stuff in the parentheses is a piece of code, and thus that `replicateM` must be a way to construct a list by running a piece of code for each value. To show that they understand the first snippet, the character then gives an example in another language whose behaviour is as close as possible to his understanding of the original. 
What type would you suggest for numeric literals? Whatever you pick, there will be a lot of explicit type conversion if you ever use other numeric types. The overloaded literals was a clever way out of a real problem (invented by John Hughes). 
I included the request for people with big machines to run the benchmark in my twitter.
In the meantime, have you considered pre-compiling a binary for Linux and running the bench on the biggest machine AWS has, paying only for the hour of compute time?
Even if it exists, which I am sure it does, it is a non-linear process, so the order would be unclear to the reader. Unless you specified exactly which literal or function you wanted to track through the process I guess, which could be useful.
The token "1" or "2.0" in your source code is an overloaded literal in Haskell source. For instance: λ: :type 1 1 :: Num a =&gt; a λ: :type 2.0 2.0 :: Fractional a =&gt; a These expressions desugar in the core language into a ``fromInteger`` or ``fromRataional`` call which results in a polymorphic type. fromInteger $dNum (__integer 1)) fromRational $dFractional (:% (__integer 2) (__integer 1)) When you invoke ``(&lt;)`` over two overloaded literals it unifies them, and adds a type constraint that they must be of the same type. (&lt;) :: Ord a =&gt; a -&gt; a -&gt; Bool Since ``Num`` is a superclass of ``Fractional``, both types are then inferred as ``(Fractional a =&gt; a)``. If you execute this in GHCi or a module with a module with the MonomorphismRestriction enabled ( it's on by default in GHCi in 7.6.3, off in 7.8 ) then the defaulting rules will be applied to top level expressions and the expression (``1 &lt; 2.0``) defaults the two inferred overloaded (``Fractional a =&gt; a``) into the equivalent ``(1.0 :: Double) &lt; (2.0 :: Double)``.
In principle the mechanism exists in Haskell, basically every step in compilation has a ``-ddump-$pass`` flag that outputs a lot of the internal state used to perform the pass . A lot of the core-to-core transformation passes give really readable output if you know what you're looking for and can read Core moderately well.
Last I heard the new release was slated for *next week*. (Yay!) EDIT: Argh, did I really state it that absolutely? I'm worried because I cannot find where I read it (it was a comment by one of the developers). Well, it's pretty close to ready, anyway.
&gt; This is mainly a limitation of lens-family-core, which uses a custom type instead of Identity (and I've petitioned Russel to fix this). This is now more or less fixed. 
`(== empty)` is `null` `append` is `&lt;&gt;` "{-#" `isPrefixOf` t
I had to hold off asking about this until *now*.. But that's great news! I can't wait to see what's in the new platform!
Is there, though? For instance: unTurtle :: Turtles -&gt; Turtles unTurtle (Turtle turtle) = turtle isTurtle :: Turtles -&gt; Bool isTurtle (Turtle _) = True turtles :: Turtles turtles = fix Turtle turtleKaboom :: Turtles turtleKaboom = Turtle (Turtle undefined) threeTurtlesDown :: Turtles -&gt; Turtles threeTurtlesDown = unTurtle . unTurtle . unTurtle isTurtle (threeTurtlesDown turtles) == True isTurtle (threeTurtlesDown turtleKaboom) == undefined Any of the levels of `Turtles` each might be `undefined`, which means that you can distinguish (by termination behavior at least) between different levels at which `undefined` appears. If you reason about the total subset of Haskell, then this isn't an issue... but then, you're reasoning about the total subset of Haskell, and does that include infinite datatypes? Probably you need to start reasoning about the data/codata distinction at this point, which Haskell doesn't make.
The one inhabitant (ignoring bottoms and various inhabitants who partially include bottoms) of `Fix Identity` is (and this was elsewhere in the thread as well): `fix (Fix . Identity)`.
Would be nice if they included a decent GUI library. It's a nightmare trying to get haskell to work with gtk or wxwidgets on a Mac
I am currently using http://ghcformacosx.github.io/ since I found ghc and cabal are the things I only need (thanks to new cabal sandbox)
It's not dead, but it's really only useful for windows. Cabal sandboxes are a huge step forward towards avoiding Cabal hell, which was one of the big reasons to use the Haskell Platform on other platforms.
`fix (Fix . Identity)` is bottom.
I mean that if you want things to work like this: &gt; 1 + 2 3 &gt; 1.0 + 2.0 3.0 then you definitely need this: (+) :: Num a =&gt; a -&gt; a -&gt; a instance Num Integer where ... instance Num Double where ... but you don't, strictly speaking, need this: 1 :: Num a =&gt; a 1.0 :: Fractional a =&gt; a because this would also work: 1 :: Integer 1.0 :: Double Though if you did that, you wouldn't be able to do this: 1 + 1.0 so the question is whether want this last thing to be legal or not. I guess it's a matter of taste.
Woah, that clock is awesome! Perhaps you can remake Space War for your last game.
[Thanks!](https://i.imgur.com/8S30SIC.jpg) :)
First let me say this is awesome work! But I disagree fairly strongly about the naming scheme. Edward Kmett pointed out the problem in the old thread you linked to, which is that we need the freedom to move modules between packages without changing their names. We want the ability to have several packages that implement the same API, and packages that implement a super/subset of an API, split existing packages into multiple sub packages, and so on. We want to be able to move modules out of `base` without breaking code. A secondary reason is that module names are supposed to help people find things. They've never been very good at this, but non-zero information is better than nothing. So I suggest choosing between `Control.Concurrent.STM.Map` and `Data.Map.STM`, I don't particularly mind which, but both are better than `STMContainers`. 
Int and Double won't work since they would overflow and loose precision. Haskell's choice is Integer and Rational, and then implicit conversions for the literals. It might be surprising if you try to use the language without trying to learn a bit about it first. 
Sadly we need a decent GUI library first :/
How would this work? If a package `foo` provides instances (for it's types) for type classes from package `bar`, and thus `foo` depends on `bar == 1`, I cannot use `foo` and `bar == 2` in my application.
Minor extension but great to have :)
Fwiw, I wanted to implement this already for GHC 7.8 (in which the somewhat related `NumDecimals` and `NegativeLiterals` extensions were added) but then forgot about it again. If you're used to `0b`-literals from the (growing number of) languages that support this very syntax (C/C++, D, Perl, Python, Ruby, and even Java), it's something you miss in Haskell. Maybe this could even be suggested for the `Haskell2014` revision (this way Haskell may have something in common with `C++` which starts supporting this syntax [officially since `C++14`](http://en.wikipedia.org/wiki/C%2B%2B14#Binary_literals), even though Clang and GCC have been supported the `0b`-literal syntax a bit longer already)
Well, every possible useful thing you could do with literal 2 would be also useful with 2 as a result of "length list". They're both equally convertible to any Num. That's why it's surprising to me that the former has implicit conversions but the latter doesn't. I mean, it's not surprising from a Haskell POV, but it is surprising from a language design POV. It would be easy to remove the distinction by making them both Integers. But I wonder if there's some possible language design that removes that distinction in the other way? Could we have all Integers implicitly convertible to any Num, instead of only literal Integers? I think Haskell typeclasses can't do that: x :: Integer f :: Rational -&gt; Rational f x -- error, no typeclasses in sight to help the conversion! Scala's implicit conversions might help, but I think they're overpowered. Here's an [LtU discussion](http://lambda-the-ultimate.org/node/4977) I started about that. Maybe there's some middle ground?
&gt; First let me say this is awesome work! Thank you! It's nice to see appreciation from a figure like you. &gt; Edward Kmett pointed out the problem in the old thread you linked to, which is that we need the freedom to move modules between packages without changing their names. ... We want to be able to move modules out of base without breaking code. I know. It was me arguing with him (/u/nyvo is my older account). And it is the only valid argument against this proposal that I've seen so far. As [I already mentioned](http://www.reddit.com/r/haskell/comments/1qrilm/packages_and_namespaces_naming_convention/cdg9zwd), it's not that much of a problem to migrate a package from one namespace to another. All it requires is just three trivial actions: * replace `import OldNameSpace` with `import NewNameSpace` throughout all project files * replace `import qualified OldNameSpace` with `import qualified NewNameSpace` throughout all project files * rename the `OldNameSpace` source folder to `NewNameSpace` So, sure it is a matter of a compromise. However I find that the mentioned argument can hardly outweigh the problems that the standard convention brings, which were described in the original thread. &gt; We want the ability to have several packages that implement the same API, and packages that implement a super/subset of an API, split existing packages into multiple sub packages, and so on. I don't see how a different namespacing convention could bother that. &gt; A secondary reason is that module names are supposed to help people find things. Let's compare. What does a user have to do to come up with an import statement? ### The proposed convention 1. Transform the package name from the spinal-case to the UpperCamelCase. 2. Follow it with the name of a module he wants to import. ### The standard convention 1. Choose a single most important property of the module. In our case, one has to decide, whether it's more of a thing pertaining to "STM" or to "data". 2. Remember or look up the namespace pertaining to that property. While in case of "data" it's trivial, in case of "STM" `Control.Concurrent.STM` is not something everyone would want to carry in his mind, so some users will have to look up. 3. Follow the namespace with the name of a module he wants to import. 4. Tada! Module not found. Go to step 1 and do again. If still not helping, probably you (or the package maintainer) have chosen the most important property wrong, just give up and consult the docs. Notice that besides the standard convention being a generally more complex algorithm, each of its steps is way more mentally demanding. To my perception it simply is a pain, that is why I always just open the docs from the start, which to me simply dissolves all the arguments about hierarchy somehow helping me find things. &gt; non-zero information is better than nothing. Hierarchical structure is awful for describing things with multiple properties. Just imagine StackOverflow using hierarchy instead of tags to describe questions: one would have to categorize the question as either "data" or "stm", but not both. We're dealing with the same kind of a problem here. Unfortunately tagging is not an option (though this could be explored). I find that half-assing the problem (pardon my French) with hierarchies only makes things worse. I am a strong proponent of a belief that an ambiguous solution to a problem is both not a solution and an extra problem. --- There's another problem, which the current convention simply can have no solution to, and which I brought up in the discussed post: the name collisions. E.g., packages `classy-parallel` and `monad-parallel` both export `Control.Monad.Parallel`. Suppose a user wants to use `monad-parallel` in his package, but his package also depends on another one, which creates a transitive dependency to `classy-parallel`. What to do? Only the `PackageImports` pragma to work around this, but I stress that it's a workaround, not a solution. &gt; So I suggest choosing between Control.Concurrent.STM.Map and Data.Map.STM, I don't particularly mind which, but both are better than STMContainers. I'm sorry, but I am not convinced. I am open to discussion and criticism of the proposal, but it needs to be established that there are some major issues with the standard namespacing convention, and they need to be addressed. My proposal does address them at a price of slightly complicating the process of cross-package migrations, but I find it a price well worth paying, considering the amount and the volume of the problems it solves. 
Maybe you read it in the ghc-dev mailing list.
The pensive sheep is my favourite. Update: Just messed around in the IDE, and wow, this is totally awesome! :O
Only in the same way that `fix (1 :)` is. That is, any finite prefix is fully-defined. edit: Not that I can think of a _useful_ application of `Fix Identity` offhand, like for infinite lists. But still. :p
that's a bit harsh. gtk2hs isn't that bad.
For this one case, the function you're looking for is `Data.List.genericLength`. Why the return type of `Prelude.length` _isn't_ overloaded, I don't know. Probably for Hysterical Raisins.
Some tools like SublimeHaskell and the IntelliJ haskell plugin say they "require haskell platform". When not using the platform, do you just need to install a couple packages globally to get them working?
The `length` function started out as overloaded, but it was changed to `Int` for performance reasons (misguided, IMO). I'm not a fan of implicit conversions, but I think `genericLength` and numeric literals are fine, because their types promise that they can return type you ask for (as long as it's in the right type class). Yes, you can argue that `1` should have type `Integer` and that would have been fine, but now it has type `(Num a) =&gt; a` and that's even better. :)
I always liked looking at which packages seemed good enough to make it in.
Only trouble is that it's not only Haskell - it needs the front end to be written in JavaScript, right?
I learned Haskell as a teenager, and this is my advice: Let them play around in gchi! Not only is it a fast and fun introduction to Haskell, but it can help them with homework as well. Most problems in High School math (especially geometry) can be solved with one-liners in ghci.
Working on a [binding](http://github.com/deech/fltkhs) to [FLTK](http://fltk.org) . For now it's mostly still low-level bindings. It's a good project for getting your feet wet with the Haskell FFI. Help is welcome. 
Ah, that's good to know. I'll have to give it another look soon.
Great blog - I'm looking forward to hearing more! More data points on the subject of teaching Haskell as a first language are always good to have. Also, hi from a fellow Sydneysider :).
In a similar boat, but man is this cool. I've introduced a lot of my friends who write JS, Java or Python to Haskell. Most give up after a few weeks because the difference between "math one-liner" and "cool visual app" is freaking huge in Haskell! Tools like ghcjs and this one are greatly lowering that gap, it certainly won't hurt.
You can also replace the `&amp;&amp;` with a colon
I also really like the project (I was working on a similar thing, though with somewhat different goals when I came to the conclusion that running all Haskell on the server wasn't going to be satisfactory, so then I started working on GHCJS). I hope that the language/library will stay close enough to regular Haskell to make the transition easy for those interested in writing bigger programs. Perhaps some basic/advanced mode setting would be necessary for this? Unfortunately I can't offer much help in terms of code at the moment since I have more than enough on my plate getting the first GHCJS release (on hackage) out of the door, but if anyone wants to work on related parts of the implementation, come to `#ghcjs` on freenode and I'll try to answer any questions. Dan's last year [interactive-diagrams](https://github.com/co-dan/interactive-diagrams) demonstrates how to use ghcjs as a library and run it in a sandbox. Preloading libraries in memory also makes compilation much faster. 
It would be useful to know what tree u use 
That's so short I can't believe it 
Nope, it's actually bottom. Prelude&gt; newtype Identity x = Identity x Prelude&gt; newtype Fix f = Fix (f (Fix f)) Prelude&gt; let fix f = let x = f x in x Prelude&gt; let i = fix (Fix . Identity) Prelude&gt; i `seq` () ... ... never returns ...
Do the distro-specific teams (e.g. Debian Haskell Group) agree on this?
But GTK+ itself is.
Can you comment on [Oliveira and Cook](https://www.cs.utexas.edu/~wcook/Drafts/2012/graphs.pdf) with respect to this?
Never ever got it to compile on OS X. Tried for practically 2 days solid. Also, it's a very imperative solution (you're basically writing C in haskell). I'd love a more functional approach (although this is turning out to be quite hard - lots of active research though, so I remain hopeful).
After taking a look at the code samples, I have noticed that you are using HaTeX to generate the LaTeX table. Nice to see my work being used in a book! I am curious about how you explained that part. Also, seeing this has motivated me to implement a [new function](http://hackage.haskell.org/package/HaTeX-3.13.1.0/docs/Text-LaTeX-Base-Commands.html#v:matrixTabular) to make easier the creation of tables. With this function you don't have to deal with `(&amp;)` or `lnbk`. You just supply a matrix with the data, and the table is automatically formatted. Of course, at the price of flexibility. You might find this useful.
This sounds rather like the approach Malcolm Wallace and co-authors took for their "Scene graph" that's actually a tree in "Huge Data but Small Programs..." presented at PADL '09. The paper seems to be online at http://www.researchgate.net/ 
It's not dead, and we are planning a release shortly after GHC 7.8.3 comes out. I hope to have alpha Mac installers, for the intrepid (well not that intrepid), in a day or so.
A similar nice feature is the ability to separate digits with underscores. Eg in ruby you can enter `1_000_000` for a million. Makes it much nicer to read and less error prone with larger numbers. Also, I wish literals like `1.4e6 :: Integer` would work - currently they only work on `Fractional` numeric types.
Your other wish is already fulfilled: "The language extension -XNumDecimals allows you to also use the floating literal syntax for instances of Integral, and have values like (1.2e6 :: Num a =&gt; a)" http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#num-decimals
I'd love to test on Win and Mac. Is there going to be a new version of cabal? The current one still suffers from some "permission denied" problems. That's why on Win I always keep cabal 1.18 to install packages when 1.20 fails.
Nice, didn't know about that extension. Thanks! (for anyone wanting to try it, it was implemented in ghc 7.8.1, which is newer than the latest haskell platform ghc of 7.6.3, so you will have to install another ghc manually if you want to try it) 
Consider a microservice over HTTP. Expose the Haskell functions via a RESTful interface. I use [Network.Shed.Httpd](https://hackage.haskell.org/package/httpd-shed) for this sort of thing. Have your Haskell code respond with something easy for Python to consume, like JSON. This approach decouples the different languages completely. Need to call the same Haskell from Scala or Ruby? No change needed.
I have found compiling gtk2hs on OSX can be a bit trying. More reason to include it in the platform. 
Me too. Got it going on a Linux box but never succeeded to get it running on Mac. Tried on and off for about a week. Having a similar problem with wxwidgets too. Getting an error compiling some of the wxhaskell stuff that others have reported ages ago but it's still broke. Where's VCL for Haskell?
If you remove the ``$GHC_RUNTIME_LINKER_FLAG`` from the Makefile in that repo that might fix the Linux build problem you were having, don't know why that's hardcoded but everything else looks right.
Yeah I had to replace it with the version of my ghc for some reason they got rid of the .so GHC_RUNTIME_LINKER_FLAG=-lHSrts-ghc7.6.3
I boosted the volume of the original video and posted a copy here: https://www.youtube.com/watch?v=kyzK6aNgyV0
Its entirely straightforward to deduce from the repmin' function
No, as this won't work: &gt; show 100#000 &lt;interactive&gt;:165:9: No instance for (Num String) arising from a use of `#' Possible fix: add an instance declaration for (Num String) In the expression: show 100 # 0 In an equation for `it': it = show 100 # 0 You would have to have `#` have a higher operator precedence than function application, and that isn't possible currently. Sure you could wrap it in brackets or use `$` everywhere, but that is starting to add additional unneeded complexity. Also, having `10#00` equal 10,000 would be very confusing behaviour. 
See you there!
Hi! I'm the maintainer of HaPy. I've been working on an update to make it usable on linux that I'm planning to release tomorrow or Monday. This update also makes it easy to use with a cabal project instead of being limited to standalone scripts. I hope you'll give it another look when I do! **Edit**: Updated! Please let me know if you run into any issues.
@kanzenryu, use `MVars` and/or `TVars` is not (probably) a good idea; you can use, share and modify immutable estructures efficiently in Haskell; look for tree, zippers, maps, hash, ... A simple example could be http://hackage.haskell.org/package/KdTree-0.2.1 (k-d tree nearest neighbor with O(log n) cost)
Thanks :)
&gt; Also, having 10#00 equal 10,000 would be very confusing behaviour. I think that whole "consenting adults"/"do the right thing" attitude is still lingering from my Python intuition. I'm not sure what I'd expect 10#00 to be though, other than perhaps an error in much the same way as writing "10,00" is an error. Interesting point about the Show instance, though. I guess when I was writing that abomination I was mostly concerned with being able to replace commas while typing the numbers, not `show` them afterward, though it's clearly a valid use case.
I think either having an error/warning, or just returning 1000 would be acceptable behavior for `10_00` or `10#00` (I would prefer a warning while returning 1,000). Returning 10,000 is obviously wrong though, and would be very difficult to debug. Also, it isn't just an issue with show, but any time you use a literal in a function. eg `f 10#000 "hello"` would be equivalent to `((f 10)#000) "hello"`. So except for just assigning the value to a constant (eg `let x = 10#000`), it would be very annoying to use.
The problem I see with `26#xyz#` is that it departs too much from the currently supported `0x`/`0o` prefix style. So adding support via `0b` was a rather natural and obvious incremental enhancement. Which decimal bases beyond 2, 8, 10, and 16 are used commonly enough to justify a generalized baked-in syntax?
MVars and TVars are meant for use in shared-state concurrency, you have other options too. STRef/Array, IORef/Array, various types of immutable &amp; mutable arrays/vectors with different trade-offs and usually come in both boxed and un-boxed flavors (array of pointers vs array of values). Are you trying to implement a (data-)parallel algorithm? 
I guess it won't be parallel at first, but if it is easy to make it run on multiple cores later then great. But I would prefer to focus on getting the basic algorithm to do what I want first.
Should be new_j = (floor $ fromIntegral (new_b + 1) * (d1 / d2)) :: Int64
The idiomatic way to do this in Haskell is to internalize and be ok with that you can't change the value at all. Whenever you want to change the value, you generate another value with slightly different structure. That allows you to set a dirty flag, if needed, actually.
Ideally I would like to get rid of the Pattern type and replace it with something cleaner (in the same way lenses were cleaned up with by van Laarhoeven), but I don't know what that would be. Also, there is clearly some kind of sub-isomorphism going on here in the Kleisli category for `Maybe`: instantiate p (match p x) &lt;= Just x match p (instantiate p b) &lt;= b I'm not quite sure what this structure is, but it seems like something just shy of an isomorphism.
Ok, so this would be most likely like solution nr. 3. I have a function `alter :: (a -&gt; a) -&gt; Old a -&gt; New a` and for figuring out what has changed I need to compare `Old a` with `New a`. Isn't this bad for performance?
ViewPatterns is a bit of weird one, once it clicks as you say, it's quite simple, but putting the pieces together can be hard at first. reallyUnsafePtrEquality / System.Mem.StableName basically answers the 'I need to compare Old a with New a. Isn't this bad for performance?' question you asked in the other thread. In C/C++ you could compare pointers to see if you got the same object back, this is the Haskell version of that. Data.HashMap is an example of how that can be used to optimize a data structure. Sorry, but I haven't really answered your general question, just pointed you at a few tools. I'd forget as much as possible about the way you'd do things in C++ and focus on finding a functional / idiomatic Haskell solution. In a lazy language with memorization for computations with a named result there are often very different ways of avoiding work, and many things that seem complex can be implemented in very little code in reality.
thanks! I've made check, to find out distribution of hashes map (\x -&gt; (head x, length x)) $ group $ sort $ map (flip jumpConsistentHash 128) [0..100000] and my program was VERY biased. (2992 hashes ended in 4, but only 249 ended in 127) with this change problem was solved! PS. My conclusion is - I am very distrait =) EDIT:code (order of arguments to jumpConsistentHash is flipped) {-# LANGUAGE BangPatterns #-} import Data.Word import Data.Int import Data.Bits jumpConsistentHash :: Int -&gt; Word64 -&gt; Int jumpConsistentHash buckets key = go key ((-1)::Int64) 0 where go :: Word64 -&gt; Int64 -&gt; Int64 -&gt; Int go !_ !b !j | j &gt;= limit = fromIntegral b go !key !b !j = let new_key = key * 2862933555777941757 + 1 d2 = double (shiftR new_key 33 + 1) new_j = (floor $ double (j + 1) * (d1 / d2)) :: Int64 in go new_key j new_j double x = fromIntegral x :: Double limit = fromIntegral buckets d1 = double 0x80000000 
Think of it more like making top level bindings to meshes, materials, etc. Sure you probably wind up building up an environment you look up free variables in, then do a global pass over the syntax tree to make sure its closed. Rather than think of how to reference a mesh from outside of the scene in code you can't reason about within the abstraction to change the mesh, think of how you might let the mesh change itself or how you might allow a combinator to walk the syntax tree of something that represents a mesh and get it to manipulate it.
I came upon a similiar problem some time ago. My solution was to track the effects instead of trying to extract the changes after the fact. I did this by keeping a *journal* of the changes. This journal was implemented using *free monads*, basically a data type that encompasses all actions that may happen on your datatype. In this case the free monad allows you to treat the operations as data and therefore enable you to track these changes. For more on I'd recommend Gabriels blogpost: http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html
A possible solution would be using a combination of state (to track the map itself), and a writer (to track the changed keys), without exposing the state &amp; writer functionality to an API user as-is (otherwise one could perform behind-your-back modifications). You can find an example at https://gist.github.com/NicolasT/a4316331e711be5df46a
Perhaps have a tree structure, modeling "A and B depend on C" as A and B being children of C. Then, you could define the final properties of a node in the tree as a function Node -&gt; Node, the argument being the parent. Assume the root node R to have the attribute position (0,0,0). Node C then becomes a child of R and defines its position as an offset of its parent node: (5,5,5), say. Children A and B of C do the same. Traversing the tree results in calculating the value of each node, because you apply the chain of parent attributes. data Node = Node [Node] Attrs data Attrs = Attrs (Int,Int,Int) update (Attrs (x,y,z)) (Attrs (x',y',z')) = ... root upd = update upd (Attrs (0,0,0)) traverse parent (Node children attrs) = Node (traverse newparent children) newattrs where newattrs = (parent attrs) newparent = \upd -&gt; update upd newattrs Wrote this in the train real quick, so apologies for any typos or compiler errors. You can also switch around parameters for eta reductions if you want. Also didn't use a functor instance. If anyone recognizes an abstraction I missed or knows how to actually execute these values (can you do effectful operations while traversing a tree like this?), I'd be very glad if you could respond, it's been a while since I've done Haskell.
awesome!
It uses pointers heavily, so only the new parts get generated. The old parts are reused. 
No, Haskell Platform is still the default way to install Haskell for all platforms. It's the easiest way to install Haskell on any platform, and it starts you with the basic set of standard Haskell libraries and the standard working environment for Haskell. Due to the unfortunately very unusually long delay between releases this time (for various specific reasons that ought not to repeat themselves), it became necessary for quite a few people to need to install a later version of GHC as an alternative to the one that came with the last version of HP. We very much hope that won't happen again. (EDIT: Not that it's a problem to do that; not at all. It's pretty easy, and it's great for people to try out bleeding-edge GHC releases. Just that we hope it won't be *necessary* for people to get their work done.) In fact, the new machinery set up by /u/MtnViewMark will make it easier to track GHC releases with a more automated process of producing HP releases. That should also allow us to spend more of our mind-share on getting more packages vetted, debugged, and approved more quickly.
Have you looked at [hsqml](http://hackage.haskell.org/package/hsqml) and [threepenny-gui](http://hackage.haskell.org/package/threepenny-gui)? Truth is, nowadays GUIs often start out as Web UIs, and only later become mobile apps and then finally native desktop GUIs. That's not the best fit for every application, but it happens often enough that Haskell's weakness in (relatively) easy to use native desktop GUI frameworks is not nearly as much of a problem now as it used to be.
Maybe a [partial isomorphism](http://hackage.haskell.org/package/partial-isomorphisms)? I once implemented [ordinary pattern matching](https://github.com/gelisam/objc2java/blob/master/src/Data/Pattern/Prim.hs#L19) using those.
Which can be done because the compiler knows that nothing's going to change. 
In fact, as long as the data isn't evaluated, that's how the runtime handles it too. I'm building a slitherlink solver with alternating depth first search and simple inference, and breadth first search. I just keep a list (sequence) in the data structure of all the positions that were altered during inference and check them during breadth first search. That works. 
I just include them, you can take a look :).
Just changed, now it should work as expected.
Implement the standard library monads ( List, Maybe, Cont, Error, Reader, Writer, State ) for yourself to understand them better. Then maybe write an monadic interpreter for a small expression language using [Monad Transformers Step by Step](http://www.cs.virginia.edu/~wh5a/personal/Transformers.pdf) paper.
I fiexed those typos, thanks for pointing them...some times I can not even recognised what I've written.
Fixed, well now it says there was a problem while parsing.... 
Thanks! Fixed, now the error code is a bit more usefull. The querry is still not showing up results cause it can not interpret "(a++b)" as a type.
If you're concerned with speed, look into STUArray and friends http://www.haskell.org/ghc/docs/latest/html/libraries/array/Data-Array-ST.html . It's basically a c array in Haskell.
Great idea! I just add them as you said, the links are not working yet but they are showed and for the next patch (once I have time) I hope I'll be able to implement them. You can check them :)
 joinPS = join . lmap Var Done. ;)
It adapts cleanly to the profunctor HOAS style. `Mu` is a form of binder. I personally don't like standard weak HOAS, as the mixture of both positive and negative constraints on the variable type makes it very hard to use. If nothing else it is a fairly simple example of how to turn a form of infinite graph or list into an explicit syntax tree.
This becomes more powerful if you have a typed AST to talk about these sorts of things. e.g. the jmcompos combinator in sclv's excellent jmacro library lets him do safe replacements. I've also built higher order versions of bound that can do type preservation.
Does the book really add anything to the gitub code ? I'm tempted to buy it, because the topic is exactly what I need at the moment. However, looking at the sample chapters and the code on github, it seems that the book is a succession of 1 page code examples with a few comments which have been removed before being put on github. I'm not denying the work needed to put everything together and I'm sure there is some interesting information but it doesn't seem to be deep be what you expect from a book. For example, I've been investigating D3JS as a way to visualize data and find it sparsely documented. This book mentions D3Js, so I would expect from a book to explain properly how to use D3JS. Instead , (according to the sample code) I assume the book doesn't give any more information that the README of the D3JS package itself. If it's that the case, the only real information given is "there is D3JS package, I managed to run the example given in the package documentation, therefore I recommend it". Am I wrong ?
Can you summarize more specifically what problem you are trying to solve?
Perhaps this isn't fair, but... Applicative f =&gt; (a -&gt; b) -&gt; f a -&gt; f b Results: liftA :: Applicative f =&gt; (a -&gt; b) -&gt; f a -&gt; f b What the real answer is: fmap Here's another unfair one: Applicative f =&gt; (a -&gt; f b) -&gt; [a] -&gt; f [b] Results: None. What the real answer is: traverse
This looks nice. I haven't seen it before, and it doesn't seem to be on Hackage. Do you have a link to more information about it? What are the differences between this service and Hoogle and Hayoo?
The problem is pattern matching and structural subtyping on heterogeneous records/lists!
Prisms are first class pattern matches. I remember somebody earlier showed me how you can even ensure exhaustiveness in the types, but I forgot where I saw it and I don't remember the trick.
I ran the benchmarks. [here](http://www.christopherbiscardi.com/2014/06/29/stm-containers-benchmarks/) is where I published them, or [here](http://christopherbiscardi.github.io/stm-containers-benchmarks/) for just the html pages. Hope it helps.
It would be nice to have some examples that simulate common situations where Monads are used
I guess you wouldn't be able to pass certain datatypes, arrays for instance?
I might be doing something completely wrong -- I've only been using Haskell for two weeks now but this is roughly what I have, and it allows me to use arrays. import System.Environment main = do args &lt;- getArgs let stock_prices = read (head args) :: [(Integer, Float)] print . do_something_with $ stock_prices
Yeah -- I'm just trying to figure out what that reason is!
This is great! So, according to the "concurrent-transactions" benchmark, it reaches the factor of 10x on 12 threads and 17x on 64 threads. This is a very good scaling. Thank you for your input!
Ah this is very useful information thanks
FWIW it should be simple enough to FFI wrap the original implementation and use QuickCheck to compare against the haskell translation; both ought to be providing _identical_ results. ...I wonder if this one would be faster, according to Criterion it seems the first haskell version is about 15 times slower than the original.
What about [this one](http://youtu.be/jLj1QV11o9g)?
Design-wise, I think you should make it clearer which function belongs to which package. In your "mapMaybe" example it confused me a bit to see that `mapFilter` is included in `Data.Maybe`, until I realized the packages are *above* the names. Maybe a separator or alternating backgrounds?
Many thanks!
&gt; to modifyVector does not work. When I tried your program, I saw that "does not work" means that there is a type ambiguity error. More precisely, when ghc sees your `let cvVec` definition, it needs to decide on a type for `cvVec`. It is clear that it must have the form `ContainsVector m` for some `m`, but it is not clear to ghc that this `m` must be the same as the `m` in the type signature of `modifyVector`. &gt; the compiler can't seem to deduce by itself that I want the record's monad type to be the same as my function The reason for this is a bit complicated, and has to do with the fact that `PrimState` is a type function instead of a type constructor like `ContainsVector`. Basically, it could be that `PrimState m ~ PrimState m2`, in which case `cvVec` could just as easily have type `ContainsVector m` as `ContainsVector m2`. &gt; In Haskell you can't reference the types from your function type declaration inside the function itself Yes you can, via the `ScopedTypeVariables` extension: {-# LANGUAGE DatatypeContexts, ScopedTypeVariables #-} import Control.Monad.Primitive import Data.Vector.Mutable as VUM data PrimMonad m =&gt; ContainsVector m = ContainsVector { cvVector :: VUM.MVector (PrimState m) Int } modifyVector :: forall m. PrimMonad m =&gt; VUM.MVector (PrimState m) Int -&gt; m () modifyVector vec = do VUM.write vec 0 100 newVec &lt;- VUM.replicate 100 (0 :: Int) let cvVec = ContainsVector newVec :: ContainsVector m return () That being said, why do you parameterize `ContainsVector` on `m`? If you only need `PrimState m`, then you can simply parameterize over that state instead: data ContainsVector s = ContainsVector { cvVector :: VUM.MVector s Int } modifyVector :: PrimMonad m =&gt; ContainsVector (PrimState m) -&gt; m () modifyVector (ContainsVector vec) = do VUM.write vec 0 100 newVec &lt;- VUM.replicate 100 (0 :: Int) return () There is also the issue of the now-deprecated `DatatypeContexts` extension, but I don't understand that part enough to comment on it. 
&gt; In Haskell you can't reference the types from your function type declaration inside the function itself (unfortunately?), That's true, but you can in GHC with the [ScopedTypeVariables](http://www.haskell.org/haskellwiki/Scoped_type_variables) extension. HTH
Thank you! The ScopedTypeVariables extension makes perfect sense, I used it before, don't know why I couldn't remember it! I'm still not really sure why the type checker can't just infer that the m type is the same in both places, as there only seems to be a single constraint acting on it. As usual, I'm a bit confused about the need for the 'forall'. I found this SO answer, though: http://stackoverflow.com/a/15800879/1898360 I guess that makes sense. I don't have much intuition about this part of Haskell. I know about the reasons against having type class constraints on data types, it seems in this case I can simply get rid of it. I've had a different case where it seems to make sense, though: data TextureAtlas = forall texel. Storable texel =&gt; TextureAtlas { ... , taBackground :: !texel -- Single texel for background filling ... } Is this consider a sound usage? Here I want an arbitrarily piece of data that I can access through Storable. I have yet to do much with GADTs beyond some basic examples, so I didn't consider them here.
And the `MagicHash` extension, too.
Your TextureAtlas datatype is an existential datatype, made possible by ExistentialQuantification extension. It's different usage than constraining data declaration. In this example, if are OK with losing all type information about texel other than it has a Storable instance, it's fine. Keyword *forall* can be confusing because it's used by 3 extensions - RankNTypes, ExistentialQuantification and ScopedTypeVariables. Enabling each one gives forall yet another meaning.
What makes this service awesome is the fact that you actually try to get feedback from your users. In the long run that beats all other features.
There is a middle ground which is to provide "did you look for ..." when other significant results exists
&gt; Basically that's what this line in the error message refers to: &gt; NB: `PrimState' is a type function, and may not be injective &gt; , right? yes.
You misunderstand the question. The issue isn't about preserving sharing or handling mutation as such. The issue, as I understand it, is about tracking the changes _for interface with_ an external library, where handling updates is effectful and costly, and so to be avoided.
Purely functional programming is about reasoning about things at compile time, not run time. Contrast these two definitions: example1 :: String example1 = "Hello" example2 :: IO String example2 = do str1 &lt;- getLine str2 &lt;- getLine return (str1 ++ str2) `example1` is a bona-fide string, meaning that *at compile time* I can deduce the exact sequence of characters the string represents. `example2`, on the other hand, is not a string; it's a subroutine that returns a string. This means that *at compile time* I can deduce the exact sequence of instructions the subroutine represents.
I think this makes sense. Thanks for the response as I think my confusion relates to compile time vs run-time. So if my factorial function took a number n as input from the user, it would be impure since the output is not known at compile time. On the other hand, if I hard-coded my factorial function to output 5!, it would be pure? 
That's correct. To be more explicit, we can specify it exactly like this: factorial :: Int -&gt; Int factorial 0 = 1 factorial n = n * factorial (n - 1) exampleIO :: IO Int exampleIO = do n &lt;- readLn return (factorial n) examplePure :: Int examplePure = factorial 5 `examplePure` is an `Int`, meaning that I can deduce at compile time the specific `Int` it represents (120, in this case). `exampleIO` is a subroutine that returns an int, so I can deduce that it will read one line of input from the user, and I can also deduce that it will transform that input using the factorial function, but I can't deduce the specific number it returns.
&gt; Given parallel on IO takes two additional threads, and parallel on C takes only one, it's not too surprising that converting IO to C requires an additional thread. I feel like Kan extensions should just be called IOUs. `Yoneda` lets you act like something is a functor so long as you "pay it back" later. `Codensity` lets you act like something is a monad so long as you pay it back later. `ContT`, as used here, lets you minimize thread usage as long as you pay it back later.
&gt; arguments swaping should s/swaping/swapping/ s/arguments/argument/ sidebar: s/mising/missing/
The side effect is that you have no idea what you will get. A keyboard is a hard example since very little can go wrong with it (as far as I know), so I will instead use a database as an example instead. Suppose you wanted to get some value from mysql or some other database. You call the database for your value and you might get it or the database may be down or you get a -1 error, or a million other things that can go wrong when calling a database. That's the side effect. If my connection to the database is always on and correctly configured to never fail, then I will always get "Kelly" when I get I call it, but the thing about the real world is that it is unpredictable (for now). So the possibility of a side effect is always there. I don't know in what ways a keyboard might fail but I'm sure they can, somehow.
When we talk about pure functions as always producing the same output for the same input, "input" means the parameters to the function, and "output" means the return value of the function. That's just the way a function is defined. It's a mapping from parameters to return values. If the function depends on *other* things besides its parameters, or produces *other* results besides a return value, then those are side effects, and the function is not pure. You might argue that you could think of the user's input as sort of like a parameter... but definitions are definitions, and it's still *not* a parameter, and functions are defined as mapping parameters to return values. You could then say that really, you're doing some kind of a pure function except for the part about getting input from the user. And you'd be right! Indeed, if you can write your function as: do x &lt;- getLine return (f x) (also can be written as `fmap f getLine`) then `f` is a pure function, which encapsulates most of the behavior of your code... it's just the `getLine` part that's impure. And this way of breaking things down into the parts that are pure and the parts that are not pure is *precisely* the point of the whole thing. If you write `f` on its own, without the `getLine`, then you have a reusable component that you can compose more easily with the rest of the system. You can use it when the input comes from somewhere else. You can use it to speculatively decide what the answer would be for future possible inputs. You couldn't do any of those things when your code was caught up with reading input from the user.
1. Write a function `twice :: [a] -&gt; [(a,a)]` that, given a list, computes all possible pairs built from elements of the list. Use a list comprehension. λ twice [1,2,3] [(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)] 2. Modify `twice` to use do notation 3. Without changing the definition, change the type of `twice` to `twice :: Monad m =&gt; m a -&gt; m (a,a)`. Try it out for different monads: λ twice [1,2,3] [(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)] λ twice Nothing Nothing λ twice Just 1 (Just 1,Just 1) λ twice getLine foo bar ("foo","bar") λ twice (putStrLn "hi") hi hi ((),()) `twice` is a function that works on all Monads. Make sure you understand how it works. Usually, though, I don't write a lot of monadic code that's *that* general. Apart from a helper function here or there, usually all the monad-independent functions I need are already defined for me in `Control.Monad`. More often, I'm writing monadic code that's reusing monadic values *specific* to the monad I'm working in, like the IO monad: main :: IO () main = do putStrLn "Tell me something" line &lt;- getLine putStrLn (reverse line) Or the List Monad: subseqs :: [a] -&gt; [[a]] subseqs xs = [] : do (y:xs') &lt;- tails xs ys &lt;- subseqs xs' return (y:ys) Or some custom monad I'm using in my code: -- given a Monad "Action a" with predefined function &amp; values putOutput :: String -&gt; Action () getInput :: Action String fireMissles :: Action () -- I can write code building upon it askFor :: String -&gt; Action () askFor request = do putOutput $ request ++ "? [y/n]" ok &lt;- getInput if ok == "y" then return () else askFor request wargames :: Action () wargames = do putOutput "Enter backdoor password:" attempt &lt;- getInput if attempt /= "Joshua" then return () else do askFor "Global Thermonuclear War" fireMissles 
Worth mentioning also that "factorial" is pure. The only impure thing there is "exampleIO"
https://www.dropbox.com/sh/w55js7yft9s2m5w/AABAXURa2seM7MwESJHEmVsXa sorry for the delay - that's on a 40 core machine.
The course [here](https://github.com/NICTA/course) has a bunch of good "build it yourself" exercises that allow you to build an intuition for the abstractions themselves, as well as some practice for coding according to the types. 
Assuming I use an IO/State monad, then no, each active rule needs its own thread, since it runs arbitrary user code, which will leave stuff on the stack. Note that active rules doesn't include rules which haven't started, or have finished - they don't use a stack. There is a more thread-efficient way to implement it, which is to use the technique in the post :)
I think you should calculate the length of the list and do it that way, so you don't have to walk the list as many times.
Note that you only have to "pay it back" once per time you move from C to IO. If you nest lots of C calls, and only go back to IO once at the top, then you only have to pay back a single thread despite avoiding many threads.
You don't want to create a new subprocess for each function call or piece of data being passed. 
Shake already uses a thread pool, with a small constant number of threads (basically your -j flag), and a queue. However, you're assuming an applicative build system, not monadic. With applicative the scheme you are describing works great, but with monadic arbitrary user code gets to run before asking for dependencies. You have to keep that user code in a half-executed state before resuming it, and that takes a blocked thread (or a continuation based Monad). The Shake build system does keep track of how long each build rule took, but I've found that picking the next rule to run randomly does better than most other strategies. The "long tail" problem doesn't affect most typical build systems (since even slow rules are typically &lt; 1 min), but resource contention (linkers using disk, compilers using CPU) does. A random strategy can give a 25% speed boost.
Are you accumulating a value using every other element starting from the back of the list? You could do something like this: everyOtherRev :: [a] -&gt; b everyOtherRev = go . reverse where go [] = undefined go [x] = undefined go (_:x:xs) = undefined Replace instances of 'undefined' with your logic. **edit** You could also filter the list, reverse it and then apply your function (in this case you would have a list of every other element starting from the back of the list). processEveryOtherRev :: [a] -&gt; b processEveryOtherRev = yourFunction . everyOther . reverse Here we reverse the list, then apply the everyOther function (which would then return every-other element from the reversed list in this case) which finally will be processed by yourFunction which has type [a] -&gt; b. Note i didn't write the everyOther function i used above but its simple enough and is very close to the definition of 'go' in my definition of everyOtherRev above.
You could compute both simultaneously and eventually choose the right one. Laziness will make only the right one actually be computed, but may stack overflow and will take O(N) memory. If your operation is cheap, you can make it strict, instead, so both are computed and then you can be in constant memory and avoid stack overflows. f z op = go z z where go a _ [] = a go _ b [y] = op b y go a b (x:y:xs) = go (op a x) (op b y) xs
I love when this kind of thing happens :)
So this is more like *dynamic programming* instead of difference between datastructures. In that case I suggest you to have a look at [http://www.reddit.com/user/edwardkmett](/u/edwardkmett)s http://hackage.haskell.org/package/recursion-schemes package. Especially the *dynamorphism* (use `ghylo`, `distHisto` and `distAna` to build it) should allow you to thread your "already altered list" through your computation. I found that using these abstractions give a nice increase in speed too (shameless selfplug): http://stackoverflow.com/questions/23664197/are-ana-catamorphisms-just-slower
Writing many interpreters by just changing the monad to change the semantics is definitely the way to go. 
f op xs = snd $ foldr (\x (a,xs) -&gt; (not a, (if a then op x else x):xs)) (True,[]) xs would apply op to the last element, but not to the second-to-last element, etc. To make it apply to the second-to-last but not the last, just start the accumulator value on 'false' instead.
There is layout bug on the site (Google Chrome) http://i.imgur.com/56qKguh.png
Wow, your machine demonstrates a very different behaviour from mine and of /u/biscarch. According to the "concurrent-insertion" benchmark, `TVar (HashMap TVar)` ("Unordered Containers") performs better than "STM Containers" on a single thread and it does scale (though poorly), reaching a factor of 2.2x at 32 threads. For comparison, "STM Containers/Focus-based" reaches a factor of 13.4x at 32 threads. According to the "concurrent-transactions" benchmark, "STM Containers" reached a factor of 14.6x at 32 threads and a maximum factor of 16.7x at 64 threads. The results of the single-threaded "insertion" benchmark however turned out to be overall comparable. Summarizing, for some reason concurrent benchmarks demonstrate a poorer scalability of STM Containers (/u/biscarch 's 12-core machine reached a factor of 17x on 64 threads). Unfortunately the benchmarks did not include 40-thread subjects, so they didn't quite cover all the potential of your system. For information, I've added a 40-thread, a 52-thread and an 80-thread subject to the benchmarks and increased the number of transactions in the "concurrent-transactions" benchmark to `400 000`. Thank you for your input!
Curious: is there a reason you're using a list and not a vector? 
Perfect, that's the explanation I was looking for. Thanks!
&gt; (setq haskell-program-name "nix-repl --pure --command "ghci") Isn't it `nix-shell` instead of `nix-repl` ? There is still something that I find quite unclear. If I try `nix-shell --pure --command "cabal build"` it is telling me that it needs cabal . Do I really need to add cabal to the list value of `buildDepends` ? In `default.nix` cabal is sometimes written `cabal` or `Cabal` and I don't see why it is so.
async provides an API for writing the first version nicely, but every call to async spawns a thread, so it will have the same problems as before. (I think, I've gone through the async code just now, and that's how I read it)
How about "terms"? The most correct thing I can think of to call them is expressions, but we need a slightly different word to indicate that we are talking about expressions in a functor context. Terms work well. "Objects" might also work but that might confuse people with an object oriented background and in any event is more suitable to things "contained" with a Category rather than with Functor/Monad.
Thanks for this very informative post. It's great to see NixOS progressing. I think I'm going to do this some day...
I don't think reasoning about the programs in compile time has any bearing in this; you could have easily written a pure (non IO) function for which no-one knows the dynamic behaviour (e.g. whether the Collatz sequence terminates for all inputs). To use your examples, the issue is that `example1 :: String` is always the same (immutable) string, whereas `example2 :: IO String` is an action that may yield different strings on consecutive runs.
Being playing around with this for a day or so and looks really cool! Wish everyone/everything used this. 
Thanks all for clarifying this issue. I'm sure I'll have more questions as I continue my journey. Is this the proper place to post such questions or is there somewhere else people would recommend? For what it's worth, I'm documenting my Haskell progress and thoughts [here](http://pieceofcaketech.com/my-haskell-journey/)
That works. I actually needed to use `buildTools = [haskellPackages.cabalInstall_1_18_0_3];` It actually does not work with the latest cabalInstall for some reasons. Thanks for your help
`example2` is always the same (immutable) subroutine.
Note: the allowUnfree is now fixed with an allowUnfreePredicate which lets you choose exactly which unfree package to enable.
Thanks! Nice contribution from Silk Engineering, they certainly didn't have to cover as many supplementary libraries as they did. Certainly don't see industry doing that very often in any form. I certainly applaud you and cannot wait to check out what you've come up with. 
Thanks for the answers guys, I'll look into them. 
if squinting makes the parentheses in `m a -&gt; m (a, a)` into square brackets :)
Or slightly more concisely, using mapAccumR from Data.List: f op = snd . mapAccumR step True where step acc x = (not acc, if acc then op x else x)
This looks amazing. Thanks for contributing this to the community. I always thought that Haskell may hold the holy grail for typesafe APIs that generate documentation that doesn't lie.
Oh, I thought it was an improvement on top of MIO. I wish I had known that before reading it.
Really nice work! I've been working on a client for one particular REST API. I look forward to reading how rest solves this problem generically and comparing.
This is a great place to ask longer-form questions. The #haskell IRC channel on freenode is also great for shorter questions. This particular one could have worked well in either. I recommend watching that IRC channel occasionally, when you're learning. People talk about huge ranges of subjects, from the very simplest to the most complex. You don't need to understand everything there, but if you ask a couple questions, you'll often find someone willing to explain stuff you're curious about. (No one understands everything about every topic discussed there.)
I like this explanation of the difference between commands and datastructures (http://blog.jle.im/entry/the-compromiseless-reconciliation-of-i-o-and-purity), which I think is needed for clarification, otherwise one can just say that the bytecode for a java function is the same immutable bytecode.
This distro is dying and has no chance of going anywhere, here's why: Every single package that isn't meant to be managed by another package manager (npm, cabal, etc.) needs to have a custom expression which often requires the maintainers to know the internals of the configuration and build processes for the package. This is not an issue on other distros because most software relies on a standard unix filesystem layout which nixos doesn't have. This results is **almost all** packages being outdated and many being broken while many are simply unavailable. Non-opensource software is absolute hell to install. Not even worth wasting the time trying because the next update will almost certainly break the software. Terrible documentation and total mess in expression tree. Terrible community. Basically there is just way too much overhead with maintaining the packages for NixOS, the features it has are not because of some ingenious new tech, but due to the huge amount of hours devoted to maintaining the expression tree. The main point isLots of packages are unavailable, and even these are almost all outdated. I don't see any good reason to use NixOS or Nix. 
To be precise, in a total purely functional programming language I can determine what an expression evaluates to at compile time via equational reasoning.
That's why I think it's such a shame 'papers' don't carry a publication (or rendering, whatever) date, except maybe somewhere hidden in the copyright section.
The key part is where the `=` is in the program. When I say `example1 = "Hello"`, I am saying that it is equal to that `"Hello"` string. When I say `example1 = do ...`, I am saying that it is equal to the subroutine itself, not to the string bound inside of it. See [this post](http://www.haskellforall.com/2013/01/introduction-to-haskell-io.html), which makes the distinction precise.
Another aspect of pure expressions is that they can be reduced in any order consistent with their data dependencies. For example, in the expression (a+b) / (c*d) we may choose to perform the addition first and then the multiplication. Or, we can evaluate the multiplication first and then the addition. In any case, the division has to happen last. Now suppose that we have a C function which reads the next number from stdin: double readNum(); and consider the expression: double x = readNum() / readNum(); Clearly the result depends on whether we evaluate the numerator first or denominator first. Haskell's type system prevents you from writing such indeterminate expressions - you will have to specify which operand to read first. C allows such expressions but specifies that the result is undefined and puts the onus on the programmer to avoid writing them. 
Sweet. Where can I track progress on a the fay generator (generating JS and/or Fay code to parse the JSON)? =3
I deleted a branch that PR was sent against and github closed it and won't let me reopen :-( it's: https://github.com/silkapp/rest/pull/11 (it works!) and more generally: https://github.com/silkapp/rest/issues/20 
I'm a beginner (at the point where I sort of vaguely kinda understand a Monad) and have been looking for a quality book with actual examples. So I bought the ePub book, figuring it would be perfect until I started actually trying to run the examples. Got to the CSV chapter and spend a half hour wondering why I was getting the error Prelude.read: no parse Finally figured out that the line age [a,b] = toInt a should be age [a,b] = toInt b Serious lack of quality control here. You should at least check that the code is correct. It's hard enough already to get up to speed with Haskell. This doesn't help!
Why? It sucks. Though I guess you won't realise this until you try to use it for something serious for a few weeks.
Could you link a description of the task at hand please?
&gt; Sorry, but I haven't really answered your general question No need to excuse, you offered me already a lot of helpful advice, ideas and new insights. I appreciate that! :) I try to change my OO mindset as well, but it's kinda hard.
That's amazing, you coded up an example! Thank you very much for your effort! I'm going to study it to see how and if I can use it to solve my problem. :)
You're going to have to qualify why. 
That's funny, because I've been dabbling around and playing with a Free monad for a few days now to solve this problem. I even incorporated it into my transformer stack already. Now I'm just looking for a way of how to make the best of it. I think I could pass the result of the free monad (the stacked up operations, the effects) alongside the model to my views which then could use it for deciding what parts of the view need to be updated. Every view would then be free to use the effects or just redraw the whole model.
I was just wondering how I would get automatic service level documentation using snap, and here's a package that does that very thing! Well, for happstack, but it doesn't sound like it will be long.
Awesome, this looks great. It's frustrating that the docs didn't build on hackage though.
What's the advantage of using this over exposing routes through vanilla snap? It seems to offer more structure, is there any other advantages?
This is awesome! I think it would be interesting to have an Angular client-side code generation option. The code would generate Angular services for each API or API endpoint or something like this.
Hope this isn't a dumb question: does this framework include authentication solutions?
Hey dhjdhj, sorry about the typo! Please check out the code on GitHub, as it will always be up to date https://github.com/BinRoot/Haskell-Data-Analysis-Cookbook/blob/master/Ch01/Code03_csv/Main.hs I'm really sorry about this mistake. I'll tell the publisher to modify it for future prints. Thank you so much for bringing this to my attention!