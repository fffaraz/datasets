The main problem is the 2-parameter version of indexed state is what you want here, but it is the less expandable version than the 1-parameter version, so alas, it isn't a clear cut win.
I'm +0 on deprecating them, but +1 on the discussion thereof.
Perhaps GHC should warn about mixing tabs and spaces instead then? All my cabal files have to disable that warning now :|
`Lazy.ByteString` is used in various places in `warp`, correct? Was there a design choice for this? Some impact on low-level / socket stuff? (asking mostly out of curiosity).
I'm actually against deprecating them. There are several use cases where you don't need the extra guarantees that streaming libraries provide, and in those cases the simple lazy versions greatly simplify your program.
IMO, `Text` and `ByteString` should just be strict ropes of chunks. This covers most of the use cases of both strict and lazy variants - often people use the lazy versions just to avoid copying things when slicing, dicing, and building them up. For the remaining stuff, people can just use `[Text]` explicitly.
I believe that Haskell statically links all Haskell libraries by default (and dynamically links C libraries). Have you tried what you're suggesting? It should just work. If you're curious about what's being linked, see http://stackoverflow.com/questions/6115459/small-haskell-program-compiled-with-ghc-into-huge-binary
For the unaware, there is also http://nikita-volkov.github.io/record/
What about \`snoc\` from [DList](http://haddock.stackage.org/lts-2.17/dlist-0.7.1.1/Data-DList.html)?
But is that extra functionality worth the pitfalls of when they are used wrong or the confusing error messages they cause?
Wait, really? *goes to check* Not for [anything useful](https://github.com/yesodweb/wai/commit/cbe73f9bf3eb573c51b45962c4db73ce6d950827). There was an unused import/identifier, and in one place it used `responseLBS` when `responseBuilder` was more appropriate anyway. I'd completely forgotten about that code, thanks for reminding me!
Because it has completely different asymptotics, thats why. Strict Text is an array without sharing. You'll get memory bloat and burning CPU cycles by using the strong structure. Builders don't do the same thing as Lazy Text even, you simply can't write the same programs with them. Keeping Lazy Text is pretty much an algorithmic requirement. You could advance it to be a rope or something, but you need a chunked version.
What benefit does the strictness bring? It is "morally" out of keeping with how we do data structures otherwise... Note this is distinct from a discussion over lazyIO.
It's true; someone on the internet says so: [“Considered Harmful” Essays Considered Harmful](http://meyerweb.com/eric/comment/chech.html).
Apparently I don't know the first thing about strict vs. lazy text. Do you have any reading material about their implementation?
That's true, but they've got to make sure manually that the mapping function is pure/threadsafe.
Your comment reminded me of a PhD thesis that I partially went through a few months ago. You may find it a good starting point in your explorations: http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-130.pdf
I work in a large C++ codebase that has void returns _everywhere_. It's a nightmare. Even if the method logically doesn't return something, it'd be nice to be able to chain setters so I don't have to repeat the object name on every line!
You can map to any `Num` type *from* `Integer`s, so a way to get rid of your `Enum c` constraint is zipWith (*) (map fromInteger [1..]) :: Num c =&gt; [c] -&gt; [c] or with some pointfree shuffling: zipWith ((*) . fromInteger) [1..] :: Num c =&gt; [c] -&gt; [c] 
The cases for `Data.Text.Lazy` and `Data.ByteString.Lazy` are quite different, and much stronger for `text`. The special optimization scheme that `Text` uses is 'pure', but it is indifferent between lazy and strict text. There is a pair of `stream` and `unstream` functions for each case, and in the (absurdly idealized) case where all the text functions fuse, no text is created or destroyed; everything happens at the level of the stream. It would not make any difference whether your module said `import Data.Text` or `import Data.Text.Lazy`. Since the stream operations in https://github.com/bos/text/blob/master/Data/Text/Internal/Fusion/Common.hs *do* admit application to a sort of 'lazy text' or 'pure text producer,' it would be too bad if they weren't used for it. In the case of lazy bytestring, I cannot really see that anything but confusion is served. It is `[ByteString]` pure and simple, and this disfigures everything including the builder machinery. The correct concept is that of a bytestring producer, something equivalent to data B r = B {-#UNPACK#-} !ByteString r type ByteString m r = FreeT B m r -- or one of a zillion variants splitAt :: Monad m =&gt; ByteString m r -&gt; ByteString m (ByteString m r) fromHandle :: Handle -&gt; ByteString IO () lines :: ByteString m r -&gt; FreeT (ByteString m) m r and so on. The types could be specialized to the case at hand - just as `Data.ByteString.Lazy` doesn't use `[ByteString]`, but its own specialized list concept. The 'prelude' for such a 'ByteString' idea would perfectly pure, in the sense of monad-indifferent. As soon as one sees that lazy `ByteString` is `ByteString Identity ()` one sees that what we have in `Data.ByteString.Lazy` are things like fromHandle :: Handle -&gt; IO (ByteString Identity ()) which is an out-and-out conceptual monster. The trouble is that as things stand the only way to get an equivalent of `ByteString m ()` is to use one of the fancy streaming IO frameworks. which do indeed "bring in extra notions."
Thanks very much for your help! I have tested several my own little projects to learn cabal, now I want to clean them up. Basically, if I am working without a `sandbox`, my workflow is: 1. run `cabal init` 2. edit `src/Mylib.hs`, and then edit `mylibname.cabal` file 3. run `cabal build` 4. run `cabal repl` and test my code 5. run `cabal install` Now, I see my own project: 1. installed into `~/.cabal/lib/x86-64-linux-ghc-7.10.1` 2. registered in `~/.ghc/package.conf.d` I can write `import Mylib` in my other haskell source code, so I think the package is successfully installed. Then I want to uninstall the package, as the package itself is just meaningless experiment code. I read [this](https://wiki.haskell.org/Cabal-Install) article, who says that: &gt; There is no "cabal uninstall" command. You can only unregister &gt; packages with ghc-pkg: &gt; &gt; ghc-pkg unregister so I run ghc-pkg unregister mylibname Now, it seems that the package is unregistered in `~/ghc/package.conf.d`, however, there is still a compiled library in `~/.cabal/lib/x86-64-linux-ghc-7.10.1`. So, how could I completly remove my project, could I just `rm -rf` the library in `~/.cabal`? 
what's a strict rope ~~or~~ of chunks?
I'm not sure what you are putting under the heading of 'asymptotics'. If you mean for example, that we can cut and paste and crop without copying and making giant new actual arrays, then there is absolutely no difference between lazy bytestring and e.g. the pipes-bytestring `Producer ByteString m r` or some perhaps much more specialized equivalent. There couldn't be, since lazy bytestring *is* `Producer ByteString Identity ()`; there's just no way around it. If there are special optimizations available for operations only in the Identity monad, we can use them in module `Data.ByteString.Producer.Identity`. In fact though, there aren't any. I was amazed too, but it goes on and on like this.
I've always been irritated by the docker insistence that development and production share the same container. A consequence of this was the restriction of only one Dockerfile per folder which has been annoying to deal with. What if I have two projects in two subfolders, which share some configuration in the parent directory? As great as docker is for providing a package format for an application I can't wait to see competition like rocket try their solutions to the same problem.
*eats popcorn*
My point there was that the optimization framework is indifferent between realizing the streams as strict or lazy text. This is a genuine 'asymptotic' advantage that has no equivalent in `Data.ByteString.Lazy`. 
&gt; I meant that to talk about undefined not returning is to think in terms of what happens at runtime, but that's not actually information available to the typechecker. Actually, that information IS available to the type checker in my mental model of how Void works. By virtue of declaring a function Void, the type checkers can deduce that the function will not return, because there is no valid value the function could create that would be of the appropriate type. &gt; But TBH I'm not totally sure if I follow your meaning, since that kind of "subtype" does not exist in Haskell type signatures. This might actually be one of the compelling reasons Haskellers don't like the "Void has zero values" mental model, since it conflicts with the "There are no subtypes" mental model. &gt; If undefined is the only value that has a type that's a subtype of other types, then I guess it's just another way of describing the same thing. So "typeclasses" in Haskell are not "types" in terms of the Haskell language spec (or am I mistaken about this?) But typeclasses are indeed "types" in the sense of defining a set of operations which, if they exist on a value, shows that that value is of that type. This is in the same sense that a Java interface defines a type, or that in the category of Haskell types (traditionally called Hask), it seems we could define an object corresponding to all values of a given typeclass, and all the category laws still hold. Again, maybe this whole mental model is self-consistent, and makes all the correct predictions about what the typechecker would do in all possible Haskell programs, and yet this mental model does not catch on in the Haskell community because it's trying to shove a concept of subtyping that Haskell programmers do not readily accept.
There are two schools of thought around `_|_`. The one you are fishing for is a lot closer to the ML approach, where `_|_` isn't an inhabitant of the values, but rather happens outside as you go to compute them. You don't need subtyping for those semantics to make sense in a strict setting, so I think throwing subtyping into the mix drags us off the path much the same way you are trying to avoid caring about usabilty. Whether you can adapt that reasoning in a satisfying way to a lazy setting is a question I don't know the proper answer to.
I'm also interested in daemonless competitors like [vagga](https://github.com/tailhook/vagga). On the whole it's an exciting area which is being held back by the (necessary) pace of development and ecosystem growth.
Can you elaborate? I know others have, but I'd also like to know your specific reasons why/how lazy bytestrings can be very useful.
Coincidentally, there's a recent [branch of stack](https://github.com/commercialhaskell/stack/compare/401-docker-images) which automates creating a docker image with your project's executables. This makes creating a docker image as easy as running "stack image container". As discussed in this post, it can base this image off a different one than your development image.
I think a very important thing missing from this discussion is: what do we mean by "deprecate"? I think that term can be finessed in a variety of ways. One that I think sees some support from the discussion here is this: 1. As /u/sclv mentions, it's important to distinguish between the lazy types and the lazy IO actions. The case for deprecation may be reasonably narrowed to the latter. 2. /u/Tekmo mentions the case where you don't need streaming libraries, and can make do with (what I read as) lazy IO. But perhaps a finesse on that is this: use lazy IO only in contexts where *you* are the one who will have to live with any negative consequences thereof. So for example, if you're writing a library that you would like third parties to use, avoid lazy IO. If you're writing a throwaway standalone program for your own use, feel free.
I don't know. It's just the Visitor pattern, except that I modified it so that visitors return values. I'm certainly not the first person to think of it—the ANTLR4 parser generator generates visitor classes with a similar pattern.
Ah I see the argument. But you're missing a beat. Assume we use `FreeT` -- that's: data FreeT f m r = FreeT { runFreeT :: m (FreeF f r (FreeT f m r)) } Where `FreeF` is our skeleton of a free monad. So even if Identity is a newtype, `FreeT` and `FreeF` are not -- so we have more constructors than we otherwise would for the same thing. You're right that in this construction the asymptotics match up, because we "morally" just have a list. But it is a heavier-weight way to encode it, because it is a list with interleaved potential effects, even if we now specialize those effects to identity. And furthermore, as I've been arguing, lazy bytestrings have a very rich API, which is a different sort of rich API than you get from the construction you give. Just because a simpler thing is a special case of a complicated thing doesn't mean there's any reason to then get rid of the simpler thing in favor of the more complicated one. Your same argument would extend to never using lists at all, but instead always using interleaved lists with effects. I hope you'll agree that's... odd at the very least.
I'm not really sure they "fill different niches". They just approach problems differently. Pipes prefers layering abstractions on top of Pipe in order to accomplish parsing and finalization, while Conduit builds some of that into its underlying Pipe type. Lately I use mostly Conduit, mostly because its ecosystem is interwoven into the other packages I use. As for writing an updated post, I'm not really sure what to write about. Also old but less out of date is my [Coroutines for streaming](https://www.fpcomplete.com/user/DanBurton/coroutines-for-streaming) series, which attempts to provide some insight related to Pipe's Proxy type.
&gt; GHC has... not the greatest tab handling That's not a fault of GHC, but rather a result from following the Haskell 2010 specification! Quoting [HR2010 / 10.3 Layout](https://www.haskell.org/onlinereport/haskell2010/haskellch10.html#x17-17800010.3): &gt; * Tab stops are 8 characters apart. &gt; &gt; * A tab character causes the insertion of enough spaces to align the current position with the next tab stop. If you can't configure your editor to follow that for `.hs` files, I'd rather argue your editor doesn't have "the greatest tab handling" ;-)
I actually find #haskell *so* large that basically everything is noise. I think most people in there are not actively participating, either — something that seems true of most IRC channels. Having said that, a positive attitude goes a long way towards getting what you want. Asking questions like "why are people so unfriendly" is a negative question, bound to get negative responses. Do you ask questions with a similar negative tone in #haskell? If so, try to formulate the question in a way that will draw people to you and make them want to help you. "Where can I go for friendly help?" or "Who are the most helpful people on #haskell?" There are a lot of different Haskell communities you can try. Maybe a different one would be a better match for you. If you are interested in a particular tool or library, there are other IRC channels like #yesod, #snowdrift,#haskell-lens, #haskell-stack, and a bunch of other #haskell-\*. There's also /r/haskellquestions, Stack Overflow, mailing lists, ...
This is why we have the add-source command. Your sandbox can track the library and build/install it in the sandbox as nessecary.
That's really cool! I've only used stack briefly but it's very slick, and this would be a great addition.
A few interesting things are mentioned in the history section of the read me.
What makes you think Shake provides a pure-Haskell grep-like function? It has a few utilities for command line stuff, but little else outside the build system world.
&gt;Actually, that information IS available to the type checker in my mental model of how Void works. By virtue of declaring a function Void, the type checkers can deduce that the function will not return, because there is no valid value the function could create that would be of the appropriate type. The Haskell type checker doesn't know that ``Void`` is uninhabited.* The type checker isn't in the business of deducing whether a function can return. It just decides whether the types unify. If they do it will compile the code, which may or may not return. Anything** can be made to unify using ``undefined``. The haskell typechecker will gladly compile this, and it will even run without errors: let v :: Void; f :: Void -&gt; Int; v = undefined; f = const 1 in f v (You could also put ``v`` into a separate module and export it, and it would still compile. The type checker isn't doing some kind of analysis where it knows that ``v`` isn't used.) You can evaluate that ``v`` at run-time, by various means, in which case you'll get a run-time exception. The compiler doesn't try to predict that. The type checker has no information of that kind. --------------- [\*] ``Void`` *isn't* uninhabited, because ``undefined`` inhabits all types. But the typechecker doesn't even know that ``Void`` cannot be constructed without ``undefined``. [\**] Well, some things, like infinite types, still can't. But Void sure can. ---- &gt; So "typeclasses" in Haskell are not "types" in terms of the Haskell language spec (or am I mistaken about this?) But typeclasses are indeed "types" in the sense of defining a set of operations which, if they exist on a value, shows that that value is of that type. This is in the same sense that a Java interface defines a type, or that in the category of Haskell types (traditionally called Hask), it seems we could define an object corresponding to all values of a given typeclass, and all the category laws still hold. Typeclasses may justifiably be called types (they're involved in type checking in a similar way), but they still aren't subtyping. That's because an ordinary type can't be a typeclass. For example you can put ``Bool`` in a typeclass, and you can put ``X`` and ``Y`` into that same typeclass. But you can't put anything into ``Bool`` or ``X`` or ``Y`` -- they can't be typeclasses, because they're types. Types and typeclasses are in distinct universes; within the universe of types, none is ever a subtype of another. (Also, typeclasses don't necessarily define sets of operations. You can have a typeclass with no operations.) &gt; it seems we could define an object corresponding to all values of a given typeclass, and all the category laws still hold. I'm not sure what you mean by this. Typeclasses don't have values; they have instances. Each instance is a specification of a type to be put into that class, sometimes also with some definitions for the instance at that type.
Cool...Thanks for your help, I think I now have some basic understanding about cabal!
Great answer. If only this was posted on StackExchange :)
We built https://github.com/factisresearch/dockercook to automated and solve these issues: * cookfiles and file dependency tracking for fast rebuilds * possibility to COOKCOPY files from one image to another It's currently even written in haskell ;-) 
I'm not really sure if deprecating lazy variants is a good idea, but one thing that confuses me very often is the proliferation of string types. For instance, when using the `aeson` package, I often find myself importing lazy `ByteString`, some `Builder` and lazy `Text`, just for the purpose of converting between these types. So, I'm in favor of consolidating the string types in a single import, but I have not yet formed an opinion on which it should be.
+1 for the madoka reference
&gt; What benefit does the strictness bring? It is "morally" out of keeping with how we do data structures otherwise... The impression I have of my own code (at home and at work) is that in practice we rarely use lazy data structures, as is echoed in this comment by [Johan](https://www.reddit.com/r/haskell/comments/36s0ii/how_do_we_all_feel_about_laziness/crh7v6c). These days I start with: what benefit does the laziness bring? 
Interesting - looks quite handy for when you're dealing with languages that have runtimes and need all their source in the container. That said, I still prefer putting my dev environment in a throwaway container, instead of rebuilding the app container each time I make changes.
Well, yeah, it's obviously better that the difficulty of writing parallel programs in Haskell lies in selecting the right Strategy, or perhaps trying something like [lvish](https://hackage.haskell.org/package/lvish), instead of dabbling in threads and mutexes.
It avoids having to allocate a single large chunk of memory and copy all of the bytes in there. I've never once tested what the performance hit of doing that would be. Arguably, a single strict chunk is a better default in that case since you should never be using httpLbs for a large response body.
 I saw that Go 1.5 added a low pause GC(under 10 ms max I think), and that Java is planning to also switch to a low pause GC. Is there any chance Haskell gets a low pause GC? I'd love to use haskell for games, but it seems to not care about pause times from what I've read.
I’d like to hear from you guys, as I’m looking for ideas and opinions here. :)
It's great that something like this appeared, directly serializing thunks, without the strong limitations of Cloud Haskell (limited to what you can make a "closure" from) or a bit [less restricting](http://stackoverflow.com/questions/17785916/can-haskell-functions-be-serialized#comment45194091_17791889) ones of HdpH (allowing additionally the composition of closures). Some people like me [have been looking for such a solution](http://stackoverflow.com/q/22272453/94687) for communicating partially evaluated states to/from a computational agent (RESTfully, i.e., the agent is statelessly doing steps of pure computations; and the agent might be restarted). ### Here is a shorter summary: &gt; ...this serialisation is orthogonal to evaluation: the argument is &gt; serialised **in its current state of evaluation**, it might be &gt; entirely unevaluated (a thunk) or only partially evaluated (containing &gt; thunks). &gt; &gt; ...The library enables sending and receiving data between different nodes &gt; of a distributed Haskell system. This is where the code originated: &gt; the Eden runtime system. &gt; &gt; ...Apart from this obvious application, the functionality can be used to &gt; optimise programs by memoisation (across different program runs), and &gt; to checkpoint program execution in selected places. Both uses are &gt; exemplified in the slide set linked above. &gt; &gt; ...Another limitation is that serialised data **can only be used by the &gt; very same binary**. This is however common for many approaches to &gt; distributed programming using functional languages. &gt; &gt; ... 
The Atom mode for Idris is also quite good. Emacs is still the best, but Atom is catching up quickly.
Thanks for the link, I will take a look at it.
Others have covered it pretty well, but yeah, partial processing, splitting/joining, there are operations for which laziness is useful. If laziness was all about I/O, we shouldn't need it at all ;). This is all especially true when producing a ByteString, which is at least as common as consuming them.
Have you seen [mvc](http://hackage.haskell.org/package/mvc) ?
You know, I haven't thought much about whether the rope should be strict or lazy and don't have strong opinions on that. What I do think is overkill is having separate types for 'a single strict chunk' and 'a sequence of chunks'. If you use a good sequence type there is no need to distinguish these at the type level and we can have less plumbing code. Note: I am not saying one can never conjure up a use case where you want to distinguish these concepts at the type level. But those cases are a minority.
That is really great. I love how fast the development has been moving.
I saw! It’s in the `Proxy` type! That’s amazing!
That would seem to suggest that `concat` is enough.
Thanks for the answer.
Seriously. You don't need docker in production. Haskell produces static binaries. The only thing you need to pay attention to is the libc and libgmp version. As those are dynamically linked. The rest of your executable is fully self contained. I just rsync a binary for deployment. Its fast and does the job just fine. Docker is great for sharing a development environment. Not a deployment environment when working with non-dynamic languages. So I firmly agree with the author here
This discussion has gotten completely derailed. It's somehow become about whether `[Text.Strict]` and `[ByteString.Strict]` are useful concepts or not. Of course they are. That doesn't mean the status quo (which is a usability catastrophe and results in plumbing code which is annoying for experts and confusing for newbies) should continue! What is wrong with the following definitions: import Data.Sequence data Text' = Text' (Sequence Text.Strict) data ByteString' = ByteString' (Sequence ByteString.Strict) We then get rid of separate `Text` and `Text.Lazy`, likewise for `ByteString`. That is, `Text'` *is* the only public type, and the strict chunk type inside is an implementation detail. A `Sequence` of chunks covers 99% of the use cases, is just as efficient or more efficient (asymptotically anyway, and I'm sure constant factors could be optimized). And if you really want to work with `[Text']` where the segments happen to be single-chunk sequences, go right ahead. We don't need to create a separate type for that though (but if you really want to, well go ahead and make a separate library).
The really nice thing about mvc is that it isn't bidirectional. The computations flow strictly one way: controller -&gt; model -&gt; view So you dont have to handle communication between each of the three parts - just events rocking in (from the controllers), and events rolling out (to the views). This is very different to the sort of javascript MVC out there, where controllers can produce effects and views can talk back to the model. The other nice design feature is that the model part is pure. You work out what's your state, how that state can change due to inputs, and can refactor aggressively at any time without having to think about IO effects. Asynchronous bidirectional comms quickly overwhelms my ability to work out what's going on. You should take a closer look at mvc's one-way flow with the pure computation in the middle - I get the sense that it's what you might be working towards.
Yes.
You still have `FreeF` and the base bytestring functor. And I don' t know why `FreeT` isn't a newtype there -- i just pulled the direction straight from the pipes library. Perhaps it was an old version?
Every time we use lists, we use lazy data structures. :-) Maybe you don't use lists. I happen to like them.
But the problem is the APIs are totally different -- the append operation on "chunk" makes a new chunk, the append operation on a sequence is like list append, etc. With bytestring, but not text, there is also the important difference that the strict variant works directly with the FFI, which is not unimportant :-)
&gt; rarely use lazy data structures Indeed, we use things like lazy lists for control structures not data structures. One obvious case is serialising structures, like json/binary serialisation. Yes one can also do that with continuations or pipes, but lazy sequences work perfectly well there with none of the issues of lazy I/O for input, and many would argue with lower cognitive overhead than continuations or pipes.
Hah! I could almost get on board with this for text. But I suspect there would be a riot :-P. (Also we'd need to use lists and not sequences, because the asymptotics are not what you would hope). For bytestring, as I discuss elsewhere, this wouldn't make sense, because one purpose it serves is as something that works well with the FFI, as a wrapper for "foreign array of bytes," and that only works properly with the strict variant.
I'm also pained by the number of string types, but I don't think you really can get away with just one of strict and lazy bytestring/text. Usually in each case only one of them makes sense (usually for performance reasons), and so I usually think trying to parametrise over them all is a mistake.
 (•_•) ( •_•)&gt;⌐■-■ (⌐■_■)
This appears to work directly with GHC. So very neat!
&gt; I'll level with you, man: I hate ligatures. I hate them very, very much. Down with W!
I also have a branch on Github for `mvc` generalized for bidirectional communication between the `Model` and `View`. I wrote it up for somebody else and kept it on a branch in case somebody else was interested in it. Edit: Forgot the link: https://github.com/Gabriel439/Haskell-MVC-Library/tree/bidirectional?files=1
I wouldn't complain either. Incidentally, the I/O actions in Data.ByteString ought to move to their own module, like they're split out for Data.Text.
By 'performance reasons', you are talking about constant factors. With an efficient sequence type, you get the same or better asymptotics as the strict 1-chunk `Text` we have now. And the type becomes usable for many more cases. Seems like a clear win to me. I also believe that the constant factors can be made very small with good engineering. But in any case, why would we settle for a worse API just to improve constant factors? This is Haskell, not C. Again, this isn't a discussion about whether my proposal satisfies every use case, I'm sure there are cases where working with a lazy list of chunks is exactly what you want. Fine, someone can create a separate library for that (or just use `[Text]` explicitly). It's a question of what the default type should be that everyone uses 95% of the time. The situation right now is pretty bad IMO.
I may be a bit presumptuous but I think I am the author of this PR :-) see this SO question, I might post an issue to docker : http://stackoverflow.com/questions/30874184/why-is-etc-hosts-file-empty-in-my-docker-container
I didn't, and I think my comment says that? Sorry if I wasn't clear. 
That's certainly true.
Indeed. Nixops is great, I am literally in the midst of a `nixops deploy` run at the moment. Even though I develop under and deploy from OS X, it can still deploy to a Linux EC2. But even when doing so, it will build on local Virtualbox instance so that the dev tools do not need to be installed on the EC2 instance.
There are some issues, such as when exceptions get kept inside the renderer, instead at the point of construction, so you only get an exception half way through writing to a Handle. Laziness is leaky. Sure, that's just the typical laziness problem (I think I have a banana, but really there's a gorilla holding onto the banana waiting to hit me on the head), but it's the most problematic when combined with I/O.
Hah, there you go. It's _very_ difficult to align names across sites. (I still get emails sent to "Mr. Snoyberg" because of that...)
&gt; Incidentally, the I/O actions in Data.ByteString ought to move to their own module, like they're split out for Data.Text. I'm interested in the reasoning behind that. I always found it a bit of a buggerance to have to import `Data.Text.IO` to do a `T.putStrLn`. Although I'd be totally onboard with it if there's a compelling reason for it.
But a strict bytestring is morally just a list of bytestrings. So it fits neatly in the exception :-P
But the marshalling needs to go via _some type_. That type is, most naturally, `ByteString`. You may say "that is a niche case" and yes it is. So that tells me that strict bytestring is useful mainly for this niche case :-)
erm, not asymptotics, but rather constant factors I meant. i mentioned this elsewhere -- the constant factors in sequences are bad enough that lists can beat them in certain very common cases.
Certainly that should be newtype. However there are a thousand alternatives before us even for the general case. In the present discussion we considering the possibilities where the so-called base functor is specialized as data T r = T {-#UNPACK#-} !Text r data B r = B {-#UNPACK#-} !ByteString r whatever you want to call them. The real trouble with the newtype formulation is that the compiler has nothing to specialize. Constructor counting by itself does not have the significance it might usually be thought to have, at least as far as I have been able to tell, because we are not envisaging the whole structure ever present in memory. That's because we are only considering sequences of things, not e.g. branching trees, implementations of maps to which refer again and again, and so on. Also, we are attempting to optimize `Stream Word8` and `Stream Char` and have already made a spectacular win by packing the individual units of interest in chunks of genuine ByteString and Text chunks. 
But that's not a problem with lazyness, you have the exact same problem with a continuation/pipe style: you can get an exception thrown at any stage when you ask for the next chunk of output. This is fundamental to an incremental approach, and I don't think you're advocating abandoning incremental approaches.
&gt; In both Java and Python, concatenating strings in a loop is quadratic In some implementations of Python, it is not. Current versions of CPython can concatenate a string in-place if the reference count is 1. Not sure about Java, both `javac` and Hotspot optimize string concatenation in very different ways.
I sympathise. These days we do at least provide semi-convenient conversion functions. The concern with making them too invisible though is what I said above, about hidden performance bugs. But it's worth looking at whether we can make the other common conversions clearer / more regular to help with the "but I don't even care yet" case. Suggestions welcome. But I should say pre-emptively that I don't like a general `convert :: Convertable a b =&gt; a -&gt; b` approach because again I think it hides those expensive operations too much (`toLazy` is O(1) time and space but `toStrict` is O(n) time and space).
If your code can take incremental input you may as well support a stream producer and not just a pure stream of chunks?
Constant factors are often more important than asymptotics. We care about performance, and asymptotics are only half of the problem, and I disagree they're necessarily the more important half.
I would really welcome someone writing a tree-style multi-chunk strict bytestring/text and demonstrating what the constant factors are. When Don and I were writing the new bytestring we did think a bit about a rope/sequence of chunks style. As I said above, it's not the same use case as a lazy list because a balanced tree can't represent a lazily produced sequence. But as you're suggesting it could in principle replace the strict bytestring as the default (leaving the "guaranteed to be one chunk" for the special case of FFI). The problem is it's a lot of hard work to do really well, and our intuition was that you have to do really well to stay close to the constant factors of the single chunk (or even lazy multi-chunk) representations. If you or anyone else wants to have a go, here's where I'd start (I've got an old tech report on bytestring and its variants somewhre if you're interested): keep the first chunk always easily available to minimise indirections to the head, something like: data ByteString = BS {-# UNPACK #-} !SingleChunkByteString !(FingerTree SingleChunkByteString) With the invariant that the first chunk is never empty except when the entire tree is empty. You also need to pick your heuristics for when to merge chunks carefully and tune and benchmark them. Good luck!
I'd like to hear your response to: https://www.reddit.com/r/haskell/comments/3cqlua/should_bytestringlazy_textlazy_be_deprecated/csyvvgg In short, I'd like to see a use case in the wild where `[Text]` and `Text.concat` is insufficient. Not including lazy I/O. Such cases have to be a minority, since they are doing alternating between building the text and observing it, but they can only observe the front of the sequence efficiently. I mean, I'm not saying there are no such cases, but they seem like a minority. (I have never personally encountered one, while I certainly have encountered uses of `[Text]`.)
Right, an incremental writer must fundamentally run arbitrary code on request. I agree they are the same.
see http://stackoverflow.com/questions/7460809/can-someone-explain-the-traverse-function-in-haskell traverse is like fmap "with effects". in Python, an impure language, they are the same, just map. in Haskell, a pure language, "adding one to a list of numbers" is pure: fmap (+1) [1,2,3] :: [Int] but "printing out each number in a list" is impure: traverse print [1,2,3] :: IO [Int] that's the intuition. I'd now take a look at the type signatures and compare and contrast them. the "f" in the traverse make it different from fmap, and give you the "effectfulness" 
My point with taking lazy input was that anything can be converted to that - either strict or produced. I would expect that having a function from lazy text to lazy text can be trivially lifted to a pipe-like thing consuming/producing strict chunks by the user if needed. Can it not? 
You can't really convert an IO stream of bytestrings/text into a lazy bytestring/text. You'd have to read it all or use lazy I/O, both of which are bad options. I don't think you can lift an incremental processor like: `[a] -&gt; [a]` to a `Pipe m a a` that incrementally consumes (without lazy I/O). Pure consumption of the next list element should trigger the next `m` action to generate the next chunk, but that may require lazy I/O.
then how does it make cross compilation easier? oh, I guess I misread that as "compiling on (not for) different systems".
The main annoyance I have is that the types have the same name, so if you have both in a module, one needs to be qualified. It would take some of the annoyance and confusion away if the lazy variant were called `LByteString` or `LazyByteString` and `LText` or `LazyText`. As for "deprecation", I'd say as an exploratory first step, identify major libraries that use these lazy variants, and see if you can get them to stop using them voluntarily. If a widespread consensus emerges that the Lazy stuff isn't really necessary, *then* we can talk about official deprecation. And I think that's a possibility, it's just something that should be approached gradually and discussed a *lot* more before we do anything drastic.
&gt; My point with taking lazy input was that anything can be converted to that - either strict or produced. Effectful streams generally cannot be converted to "lazy input" without performing the entire effect. For example, reading from a file. On the other hand, lazy input can easily be turned into an effectful stream. So I think that it is more accurate to say that "anything can be converted to effectful streams", and that should be the preferred universal input type.
I don't know C++, but there's some software I'd like to try to write bindings for, Windows only. what are some of the nontrivial features I would need to take care of?
There are three forms of "parameterization" folks use The McBride version has one parameter. type a ~&gt; b = forall i. a i -&gt; b i class IMonad m where ibind :: (a ~&gt; m b) -&gt; m a ~&gt; m b ireturn :: a ~&gt; m a There is an `i` in there if you squint at the ~&gt;'s closely. The Atkey version has two parameters: class PMonad m where preturn :: a -&gt; m i i a pjoin :: m i j (m j k a) -&gt; m i k a and then there is a true 0-parameter version where we absorb 'i' into the monad 'm' itself, and you get stuff like the code in my [2007 post](http://comonad.com/reader/2007/parameterized-monads-in-haskell/) which was rather completely unmotivated by any theoretical concerns and was just slapped together. This third approach has been adapted in the [polymonads](http://research.microsoft.com/en-us/um/people/nswamy/papers/polymonads.pdf) paper from Microsoft Research, but inference really only flows in "one direction" there. Here I'm comparing the McBride and Atkey versions. With some type-level jiggery-pokery we could make the McBride version and the normal Monad [into one class](https://github.com/ekmett/categories/blob/b98a100946ad442197c82a23dc12f2f6e27129af/src/Math/Monad.hs#L19). But for the application here we need to actually have a pair of states -- a start and a stop state. The Atkey version is a more natural fit for this problem. An Atkey style parameterized monad could be slotted in directly and all things would work. Now, indeed we can derive the Atkey version from the McBride formulation, but everything isn't roses. We pay for an extra box in the conversion at the very least. And while monads work very nicely in both the McBride and Atkey settings, monad transformers kind of break down. The short version of it is, we could get to a world where we had McBride-style monads as just monads in a more general setting, but not everything we like to do and use lifts into that world cleanly. So, ultimately, we can't really make a nice thing happen where you get to use the same combinators for indexed and unindexed state.
Perhaps lining things up visually will help illustrate what's going on? getAge (Person _ a _ ) = a ---------------------------------------------- getAge (Person "Stan" 94 Cabbage ) = 94 getAge (Person "Brent" 31 SealingWax) = 31 The underscores indicate that the data at that position is ignored. The `a` indicates that whatever data is at that position will be referred to as "a" on the right-hand-side of the equation.
Is your idea that the 'strict' text modules should be understood as fairly low-level? Then the user could be faced with two types, a genuine streaming text type, and a genuine 'data' type -- some sort of structure made of (what we now call) strict texts -- into which to accumulate these streams for repeated inspection etc. The latter then could be given a first approximation implementation by putting a bang on the second field of Cons constructor; the former by putting a 'm' in front of it? I could probably be convinced by that, but in the streaming case the `Empty` constructor should be changed to `Empty r`, for streaming division of text and the like. 
Please, please don't do this. Our tooling is barely holding together as is. Throwing yet another curve ball is yet another failure case we all get to worry about and experience.
I guess the thing that's missing here is a "use cases" section in the ticket that describes why this is desired
What's the motivation for this feature? The GH issue describes how this might work, but not why we would want/need this support.
Very good point. The more idiomatic approach I've seen would be long the lines of import matplotlib.pyplot as plt from scipy.misc import imread #Load an image to give context to method calls image = imread("foo.png",flatten=True) plt.figure(figsize=(10,10)) plt.imshow(image,interpolation='nearest',cmap='gray') For those not familiar with matplotlib, the original code was setting a global variable that would change the colour map and pixel interpolation for every plot in the rest of the program. That's the biggest reason why it burned to read that code. The above version only changes the figure being drawn and does not touch the colour of any future plots. We've also moved to having proper function arguments, instead of dictionary keys, so python will throw an error if you type **fgisize** instead of **figsize**. Now, the fact that we're using a string to declare out interpolation method is a travesty and I'm not going to pretend otherwise. The colour map is a bit more complicated. I've worked on a program that required that the user could define and name new colour maps at runtime, so it's not as simply as being able to enumerate via a data type. I'm not saying that strings are the right choice here, but the solution isn't as trivial as it might first appear.
That's... I have no comeback to that. That is some ugly, ugly code write there in a high profile tutorial. There are no excuses here.
Copied from updated ticket: 1. For Backpack, we absolutely need the ability for a single Cabal file to result in the installation of multiple "packages" in the installed package database, because these packages are how you do modular development in Backpack. I took a detour to implement this feature, because it will serve as a good blueprint for how to make it easier for Cabal to support this use-case. 2. There are many packages which have been split it a wide constellation of packages in order to make it easier for users to install useful subsets of functionality without pulling in the rest of the dependencies they don't want. However, maintaining N different Cabal files can be a bit of a pain for tightly coupled packages. With scoped packages, all of these packages could be placed in one Cabal file. (We have to make sure components get depsolved separately, but @edsko has put us most of the way there.) 3. This change presents a really good opportunity to substantially simplify Cabal's handling of components. Currently, benchmarks, testsuites, executables and libraries are all separately special cased in Cabal, and anything that, e.g. mucks about the BuildInfos has to be implemented FOUR times for each of these cases. Here's a simpler model: every Cabal package has some number of components, which may be one of a few types. 
I mentioned this in the uses cases, but I think this change will allow us to greatly simplify Cabal's internal code, by having us treat components more uniformly. So I think this will make the tooling situation better.
By all means, clean up the internals of Cabal. But claiming that this will make the tooling situation better is ignoring the enormity of the situation. What you're proposing will be breaking changes for: * GHC * Cabal (the library) * cabal-install * stack * Stackage * Editor integration * IDEs And who knows what else. Lobbing about these kinds of changes because they make a library a bit easier to clean up, while forcing breakage across a widely distributed system of components, is not a good trade-off.
Apart from the case for or against this due to the current state of tooling, what is the theoretical argument for or against? 
This proposal is fully backwards compatible with GHC (since each sub-library gets its own package in the installed package database, so it just looks like you installed multiple packages), and only requires minor API updates for cabal-install/stack if you decide to simply ignore packages with multiple libraries. Honestly, that's what I expect to happen for a few years. But if you want to a feature to eventually enter into circulation, you have to put it into Cabal some time. (Edit: BTW, you should see the original proposal SPJ pushed me towards https://ghc.haskell.org/trac/ghc/ticket/10622 , which was even more breaking! I spent a day cracking my head against it and decided to ignore it. This is the more tame version. )
Oh, now I understand your remark. The idea is to avoid tools like grep, using simple readFile/writeFile instead, rather than make grep better. 
When you do `plt.figure` what does that apply to? 
There a reason one library was chosen in the first place: it comes with a very logical "this package provides a library of the same name." It's breaking that abstraction. Should we have initially adopted a totally different mindset about how library packaging happens to allow for this 1-to-many relationship instead of 1-to-(0/1) relationship? Perhaps, but I'm not convinced it would have been worth it even then. Trying to hoist it in now is a totally different ballgame.
&gt; Isn't this an absolute 100% requirement if we ever want proper package management and real modules? I think I disagree with every phrase in that sentence :). 1. No, it's certainly not a "100% requirement." 2. Who said proper package management requires real modules? I'm sure *someone* did, but it certainly wasn't me, and it certainly wasn't "the entire Haskell community." 3. Do I want real modules? Sure, that'd be great. I never said I'd be willing to pay a limitless cost to get them. &gt; It always seems like the haskell community is complaining about how we want these things, but when people do the work to get us there, now it is bad? Ask anyone at the Haskell symposium who spoke to me after Edward's talk about Backpack (including Edward). I've been terrified of what this is going to do to *break* package management ever since. The fact that a major change to how we do library packaging is apparently now a requirement for this project has me even more terrified than I was previously.
I hadn't worked out this part of the issue, so thanks for forcing it. You are right: ghc-pkg will not accept a slash/period separator in package names. Thus, we'll have to go for hyphens. So any sub-library name is also a valid package name; the hyphenation scheme is a "convention" that cabal-install can use to find a Cabal package which exists and defines the file. So, no papercuts! A Cabal file with multiple libraries is now is /exactly/ equivalent to multiple Cabal files. (I've updated the proposal)
&gt; we absolutely need the ability for a single Cabal file to result in the installation of multiple "packages" in the installed package database, because these packages are how you do modular development in Backpack This needs to be elucidated.
I agree the situation is quite fragile. But a lot of improvements are being made recently (thanks FPC). Slowly I dare to foresee a version of LTS Haskell that contains GHCJS and full-blown editor/IDE support, all available through Stack.
No, that doesn't make the problem better, that just makes the problem *different*. It's not some "convention," we've now completely broken invariants. Where is snap-server, or yesod-core, or pipes-bytestring, going to be located? Probably a dozen tools have hard-coded into them that they'll be located in snap-server.cabal, yesod-core.cabal, etc, located in the 00-index.tar file at a specific location. That invariant's gone. And this is the crux of the matter: every tool out there has been going on the fact that library X is in package X. You're removing that rule. Whether you're removing it by saying "libraries can now be X.Y," or "library X-Y can exist in either package X or package X-Y," or any one of the untold variations we could come up with, all of those tools will need to be changed. (And this doesn't even get to the level of *documentation*, and teaching all Haskellers and all new people "ignore what we've said until now, we have a brand new rule in place.") On top of all of that, there is a very mysterious argument from authority going on here: Backpack requires it. I think [Tom's comment](https://www.reddit.com/r/haskell/comments/3cu5nu/feedback_requested_for_supporting_multiple/csz2xfc) needs to be addressed: where does this requirement come from? Is there no other way of achieving this? Is the benefit we get from this implementation worth the cost we're all going to have to pay to get it?
We can simplify Cabal's internal code to treat components uniformly, but keep cabal file parser so it accepts at most one library. Even internal structure supports some feature, it doesn't need to be publicly exposed. Cabal-the-library GenericPackageDescription could then have multiple library components. But you could construct such entities only programmatically, or using different parser (for Backpack need?). 
Yikes! Cabal files/the cabal format is already hugely complicated as it is. Please don't do this. -- (former) ide plugin writer
The way of formulating a general `FreeT` type that corresponds to what `Pipes` does is data List f m r = Return r | List (f (List f m r) | Wrap (m (List f m r) For its complicated Proxy type, pipes just specializes this to the 'base functor' `Sum (Compose ((,)a) ((-&gt;) a')) ((Compose ((,)b) ((-&gt;) b')))`. All this needs to be handled with some care, since the type one intends is a sort of quotient of this. The scheme works well, but there are a million others. The implementation that would be strictly parallel to this, in the present case would be data ByteString m r = Empty r | Chunk {-#UNPACK#-} !S.ByteString (ByteString m r) | Wrap (m (ByteString m r)) this gives the compiler plenty of things to specialize before it gets to the unspecific 'm' in the `Wrap` constructor. This is the existing implementation, except that `Empty` can carry a return value and a possibility of 'monadic' sequencing is given. The return value is need to expressing pairing: splitAt :: Monad m =&gt; Int -&gt; ByteString m r -&gt; ByteString m (ByteString m r) in place of the non-streaming splitAt :: ByteString Identity () -&gt; (ByteString Identity (), ByteString Identity ()) And, you still need something like the more general `List` or `FreeT` type to express things like lines :: ByteString -&gt; [ByteString] as lines :: Monad m =&gt; ByteString m r -&gt; List (ByteString m) m r The only other deep differences between the associated prelude of functions are that splittings are expressed like so and that folds are monadic length :: Monad m =&gt; ByteString m r -&gt; m Int Thus procedures for composing various folds are desirable, since of course we can't get the past back to do a second run; for that you need to stop streaming and accumulate into another type. 
OK, this is a kind of long story, but here goes. The point about modularity is that you can take a module and switch it out for something else, without needing any source level changes. So if in Haskell today you write: module RNG where data RNGState = ... module Crypto where import RNG data SessionState = ... RNGState ... The goal of a project like Backpack is to make it possible to compile Crypto with different versions of RNG in a relatively user-friendly way, and furthermore, perhaps use the resulting Crypto modules in the same program. Now, the fact that RNG can define types, and Crypto can define types based on those types results in an interesting problem: if I have RNG.HmacDrbg and RNG.CtrDrbg, their RNGStates are probably different; and furthermore, the SessionState I get from Crypto should have a different *type-identity* depending on which RNG I picked. OK, so now for a tangent: how does GHC decide if two types are equal or not? You might naively think that it's something like doing equality over the module it's defined in plus the name of the type. But in GHC today I am allowed to write two packages with a module having the same name, and if they define the same types these SHOULD NOT be the same. So, GHC defines the "Name" of a type to be the module name, the type's name, AND some sort of "package key" (more on this shortly as well). In pre-GHC 7.10, this package key was usually just a string like "transformers-1.0", which solved your problem if the conflicting name was in "mtl-1.0". Now, there is one more piece of the puzzle I have to describe to you, which is how GHC does separate compilation. When you build and install a library, GHC bundles up all the types and unfoldings into interface files, so that when you type-check some code that depends on one of those types, GHC can slurp in the interface file and find out what the actual darn type of the thing is. Now, there are a lot of interface files, and GHC tries its hardest not to load them all in because that would make GHC very slow. Instead, GHC uses the package key (which we just talked about) to figure out what "installed package" contains the interface file for any type in question. It does this by consulting what is called the "installed package database", which is essentially a big mapping from package keys to directories holding interface files among other things.) To summarize: the identity of a type is a package key ("foo-0.1"), a module name ("Data.Foo") and an occurrence name ("FooTy"). When GHC comes across one of these references, it looks up the package key in the installed package database to find the interface describing what the type actually is. Following along? So let's go back to the original Crypto example. We want our Crypto.SessionState to be different based on which RNG we filled in with. The identity of this type is the package key (unspecified), the module name Crypto (not changeable) and the occurrence name SessionState (not changeable). So the ONLY place we can stuff in the information we need is in the package key. The implication of this is that each *instance* of Crypto (compiled against RNG) needs to be installed separately in the installed package database, because we still need to be able to lookup these interfaces. (BTW, you could try just adding a new field to our concept of a 'Name'. We decided not to do this because, (1) it would slow down GHC, and (2) SPJ was quite insistent, early on, that type identity be computed by looking at the dependencies of a package as a whole, rather than just a module: it makes some problems like how to link your programs and UX easier. This, by the way, was NOT how the Backpack paper used to work.) So, where are we at? If you want to write a module and instantiate it multiple times, you need to give it different package keys, which means it needs multiple entries in the installed package database. On the other hand, when you distribute this package to users, you are only going to have one Cabal file and one source distribution. Thus, you have a one to many relationship between Cabal packages, and Backpack units of modularity. BTW, I am not that invested in the scoping bits; we could live with an implementation of Backpack where there was only one package you could access externally. But regardless of whether or not cabal-install/stack know about the internal private packages, they DO have to be installed. And it seems the best way to do this is to have Cabal treat each instance of a package which it is going to install as another library. 
Actually, within the last few months we've gone a lot back further having Backpack be embedded in the Haskell language. For example, in my HiW talk I described a Cabal-esque format for defining Backpack packages: we've gone back to a proper language like in the Backpack paper which GHC will parse and compiler. However, I've needed to circle back to Cabal, because I need to answer the question, "So you've written a Backpack file which uses the new module system features: *how do you distribute it with Cabal*?" And the biggest barrier is dealing with Cabal's assumption that when it calls GHC, it is going to get a blob of code that can be registered as a single entry in the installed package database. But this is simply not true when you're compiling Backpack files.
Oh, I know your pain. Back during my thesis, I wrote a concurrent program with Matplotlib and handling the threads took quite a bit of discipline and care that Haskell would have handled for me. As for just using a dedicate type for colours, there's two bits to it. First, on the pedantic side, the parameter doesn't take a colour, but a colour map. The reason that I bring this up is the second part, that even in Haskell, you wouldn't want this to just be a data type. Rather, you'd want something like (Num a) =&gt; a -&gt; Colour And then have Data.Cmap.grey just be a function. That would be the proper way to handle it in either language. It'd still kill some neat tricks I pulled in the past (e.g. using python reflection to make a labelled list of all defined colour maps), but I think it's better than just a data type or a stupid string. Finally, best of luck with your GUI library. Honestly, even with its problems, matplotlib is the biggest reason I keep coming back to Haskell. It's amazing how many plotting utilities don't have something as fundamental as imshow. Actually, given my explorations into other plotting packages, it's disturbing how many utilities can't even handle errorbars or a log axes.
Thanks a lot, I didn't notice my post made it onto reddit :) I mean to make each extension linkable (current semi-limitation of system I'm using) and have a bunch more to add, so I'll update it gradually as I find time/learn more! **Edit** Linkability added!
I write a lot of my code in c indeed. Don't force me to write even more of my Haskell code in c :)
Hey, there is one about [Pokemon](https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/type-families-and-pokemon). Granted, it's much more pointless than dependently typed lists, and it kind of starts with rather awkwardly defined types just to show that they're awkward, but still.
Largely self taught developer here.Looking to better understand the mathematical basis for a lot of the stuff in Haskell. And what it's good for. What are some good places to start? Discrete mathematics textbooks? Proofs? The math part of all this seems incredibly interesting but I'm a little overwhelmed as to where to start.
I would have to disagree, docker is not just for development, though it may seem that way in for a simple application. Containers provide isolation, resource sharing policies, simple automated scaling, defining interrelations and dependencies in system with several parts, etc. For example, I use docker and docker-compose to easily deploy and manage multiple instances of my application alongside other applications such as nginx, redis, etc. You can do this without docker but docker makes it much more convenient and simple as well as being able to deploy and test the entire system on other machines easily. There are other tools for doing similar things such as google's kubernetes (http://kubernetes.io/), but I mostly use docker-compose and docker - maybe someone who has experience can way in with a comparison.
"Can't" is not the issue. "8 character tabs are an abomination I refuse to endure" is more the crux of the matter. :) You're right that GHC shouldn't be blamed for an upstream spec, though on the other hand a tab size override option seems trivial to provide. However, I'm of the opinion that "don't use tabs" is actually the better answer considering both the semantic rules and stylistic tendencies of indentation in Haskell.
OK, that's a standpoint I can understand. And I agree that "this breaks things" shouldn't be a *veto* against a proposal. But it would be a bad idea to ignore breakage. It's yet another cost (of many other costs, like how difficult is it to implement, maintainability of the feature, etc) that needs to be weighed. I don't want us to be in a world where Haskell *ever* puts the same weight on the cost of breakage that, say, Java does. However, I probably do put more weight in that direction than others, probably yourself included. I used to not care about that kind of breakage. But as the user base of my open source packages grew, I got feedback about how much people care about stability, and have grown quite sensitive to those needs.
&gt; the cost of an extra pointer indirection This is exactly the sort of thing that the `bytestring` and `text` library authors worry about so that you don't have to. It's partly why Haskell went from being considered slow to being considered fast. We got these libraries and a bunch of others that take performance seriously with minimal compromises on "Haskelly niceness" (e.g. pure, lazy, high level).
I didn't realize rails was *that* bad. Jesus.
&gt; In H98, there is no function that you can write where the type cannot be fully inferred. (With the usual caveats about polymorphic recursion.)
How is it possible to use vectors with sizes known at compile time without hardcoding these sizes at least somewhere, perhaps near the IO? The only thing that comes to my mind, that when one would like to convert a regular list / vector to a sized one, perhaps something like this would work: class GetSize v n where type Sized v :: Nat -&gt; * getSize :: (n &lt;= m) =&gt; v -&gt; sing n -&gt; Sized m instance GetSize Vector (n+1) =&gt; GetSize Vector n where type Sized Vector = ... getSize v n = if length v == natVal n then ... else getSize v (undefined :: sing (n+1)) With an exponential instead of a linear search it could be feasible, I guess, but I have no idea how to write this code so that it actually has some base case (other than artificially constraining it to some hardcoded number). 
mkfifo / named pipe sounds overkill. Try specifying /dev/stdin as the input file. Maybe ghc also allows the usuall '-' as shorthand for this (Not at computer to try this)
As far as I understand, sharing is done automatically. It's probably more difficult to avoid it than to have it. For example, if you get the tail of a list and cons a new element, the whole tail won't be copied. Any algebraic data type you make will behave like this. Say you have a record of 3 lists, if you "update" one of them, the only thing that needs to be recreated is the record itself so it points to the two old lists, and the new one you created, and that new list will also share as much as possible with the old one. The more you deconstruct the original object before updating it, the less is shared. That's why, for example, reversing a list breaks sharing and creates a new one. Also, there's no complicated mechanism in place that lets you do this, no explicit storing of "changes needed to produce the new one". It's just how pointers work. A list [1,2,3] is basically 1 -&gt; 2 -&gt; 3 if you cons the number 4 to the tail of that list you get 4 -&gt; 2 -&gt; 3 but the original 1 isn't gone, it still points to the same 2 so you have this kind of implicit tree that looks like 1 -&gt; 2 -&gt; 3 ^ | 4 and depending on which head you're currently observing you get a different traversal of the same tree.
I'm a little confused about what the question is exactly asking... mutation is extremely explicit in Haskell, and requires using things such as IORef or STRef -- you should be very much aware when you're doing destructive updates. If you're not using those things, then any "updates" you're doing to structures are actually just constructing new structures. If you're not using anything like writeIORef, writeSTRef, putMVar, or the like, and your update operations don't require IO or ST, then the data structures you're creating are almost certainly immutable, and any "updates" to them are really constructing new structures. There are rare exceptions to this like DiffArray, which does a bunch of sneaky mutation behind the scenes (using unsafePerformIO) when you update, and stores a list of changes that get used on accessing the *old* versions of the array. The implementation is pretty subtle in the face of concurrency, and had some pretty serious concurrency-related bugs in the past, if I recall correctly (but those hopefully did get ironed out somewhere along the way, given the present implementation is using MVars in careful-looking ways). However, I can't think of anything too popular of that sort. DiffArray never really took off, because the cases where it out-performed other less-evil structures were pretty rare. While there's no simple description of how to design immutable data structures such that various operations perform well, the key is typically to try to ensure that as much as possible of the "old" structure is kept when forming the "new" structure, which generally happens when you bind part of the old structure to a variable (through pattern matching) that then gets used as an argument to a constructor of the new structure. This just amounts to copying a pointer. It's very common when manipulating tree structures that an insertion into the tree will only involve rebuilding a path between the newly inserted element and the root (and perhaps a constant-width swath around that to maintain balance) such that only O(log n) new constructors get built.
Thanks for the reply. But that doesn't actually do what I want. I'm specifically interested in managing things when the JSON and haskell don't auto-magically match up. See my other comments for more detail.
I'd do what /u/mjmrotek suggested and pass the 'boundingBox' value along with the 'Polygon'. One nifty thing about Haskell is that due to laziness, the 'boundingBox' value won't be calculated until it's needed, so you'll be able to achieve the effect you're looking for without any explicit memoization logic. Example code: module MyModule ( Point (..) , BoundingBox (..) , Polygon -- Constructors NOT exported, meaning you can't create a , mkPolygon -- 'Polygon' unless constructed with 'mkPolygon' , points , boundingBox ) where import Data.Vector (Vector) import qualified Data.Vector as V import System.IO.Unsafe data Point = Point { x :: Int , y :: Int } deriving (Show) data BoundingBox = BoundingBox { topLeft :: Point , bottomRight :: Point } deriving (Show) data Polygon = Polygon { points :: Vector Point , boundingBox :: BoundingBox } deriving (Show) -- This prints "EVALUATED!" each time 'boundingBox' is evaluated. {-# NOINLINE calculateBoundingBox #-} calculateBoundingBox :: Vector Point -&gt; BoundingBox calculateBoundingBox vec = unsafePerformIO $ do putStrLn "EVALUATED!" return $ actualCalculateBoundingBox vec -- This is just a proof of concept and can be further optimized to collect -- all the necessary values in just one pass over 'vec'. actualCalculateBoundingBox :: Vector Point -&gt; BoundingBox actualCalculateBoundingBox vec = BoundingBox { topLeft = Point { x = V.minimum $ V.map x vec , y = V.maximum $ V.map y vec } , bottomRight = Point { x = V.maximum $ V.map x vec , y = V.minimum $ V.map y vec } } mkPolygon :: Vector Point -&gt; Polygon mkPolygon vec = Polygon { points = vec , boundingBox = calculateBoundingBox vec } -- main :: IO () -- main = do -- let p = mkPolygon $ V.fromList [Point 0 0, Point 0 1, Point 1 0] -- print $ points p -- putStrLn "Bounding box has not been evaluated" -- print $ boundingBox p -- putStrLn "Now it has" -- print $ boundingBox p -- putStrLn "It is not evaluated again" -- -- =&gt; fromList [Point {x = 0, y = 0},Point {x = 0, y = 1},Point {x = 1, y = 0}] -- =&gt; Bounding box has not been evaluated -- =&gt; EVALUATED! -- =&gt; BoundingBox {topLeft = Point {x = 0, y = 1}, bottomRight = Point {x = 1, y = 0}} -- =&gt; Now it has -- =&gt; BoundingBox {topLeft = Point {x = 0, y = 1}, bottomRight = Point {x = 1, y = 0}} -- =&gt; It is not evaluated again
Okay, forgive my bluntness, but what if I don't really want to prove theorems about natural numbers, but do some actual programming instead? Let's say I have functions like add :: Vector n a -&gt; Vector n a -&gt; Vector n a append :: Vector n a -&gt; Vector m a -&gt; Vector (m+n) a That's all nice of course, but then I want to read a vector from a file, without any idea what its length could be: read :: Handle -&gt; IO (Vector' a) So how to connect (Vector n) functions to Vector' ones? I guess something like this would be useful: withSize :: Vector' a -&gt; (forall (n :: Nat) . Vector n a -&gt; b) -&gt; b but I have no idea how to implement it...
I think you're selling this solution a little short. I used `deriving (Show)` because it was the quickest way to demonstrate how things get evaluated, not because I expect you use it in production code. If you wanted to use `Show` in production, you'd want to write a custom `Read` and `Show` instance that hid the `boundingBox` construction/value so that you couldn't use `read` to create an invalid instance, and it wouldn't evaluate `calculateBoundingBox` when shown. As for the relationship between `boundingBox` and `points`, that's encapsulated entirely within the `Polygon` data type. As long as you don't export the constructors for `Polygon`, there's no way for the caller to know its internal representation, so they wouldn't be able to tell whether the `BoundingBox` returned from `boundingBox myPolygon` is stored as a part of the `Polygon` or calculated each time.
&gt; As for the relationship between `boundingBox` and `points`, that's encapsulated entirely within the `Polygon` data type. The data type only states that there *is* a vector of points and some bounding box. To see their relation one also needs to look at `mkPolygon`. It may be that I am selling the solution a little short, but it has some disadvantages that may seem minor, but that I think could be more problematic than one might think at first. &gt; If you wanted to use Show in production, you'd want to write a custom Read and Show instance [...] And that is exactly the problem, that one cannot/should not longer use derived instance but has to write custom ones. The problem is *not* that we have to *write* more code, but that one has to *read* more code afterwards. Maybe `Show` is not significant, but what if we want to translate the `Polygon` to Json or xml or serialize to binary? Libraries for doing such things may provide generic functions (or template) to automatically derive functions for the translation, but if we include things in the data type that we don't want to appear in the translation we have to do all the work ourselves. And that leads to more code to maintain and reason about. So, adding properties as fields to the data type works, but it feels like the language does not really want this. We have to *write* our way out of adding those fields many times later by adding more and more code to ignore what is not the real data.
The reflection package uses some tricks for efficiency, but reifyNat can be written without such tricks. 
This is great!
OP didn't state, but I imagine s/he perhaps wanted practice designing such a structure thenselves (even if it is redundant in Haskell proper)
&gt; Null pointers are impossible This point isn't clear. The Haskell type system doesn't have much to do with pointers. But the type system prevents the use of e.g. integer as a pointer.
The "null pointer" case is just an example of bad language attached to a legitimate concept, the real thing it's trying to say is "no implicit nulls", but Java's NullPointerException's infamy appeals to the imagination of developers better than "implicit null". I always add the caveat that Haskell has implicit exceptions (any value can just explode when you try to use it, breaking abstraction barriers), which is actually worse than nulls per se. The bad news can be qualified with the fact that people don't use exceptions as a form of data like `null` is used, so it's much rarer.
Maybe start a Wiki page now or after the discussion has taken off?
&gt; And there is ofcourse the problem with lack of clarity as to the relation between boundingBox and points. It is unfortunate to loose that. Well you can always make a "smart lens" if you want to make it look like Polygon is still a Vector Points and an unrelated function: import Control.Lens data Polygon = Polygon {_points :: Vector Point, boundingBox :: BoundingBox} points :: Iso' Polygon (Vector Point) points = iso _points $ \p -&gt; Polygon p (calculateBoundingBox p) ( ... ) p = from points $ someVector p &amp; points %~ Vector.map someTransformation Now whether you like stacking abstractions like this or not is another matter.
At the risk of sounding like a broken record, [these](https://www.reddit.com/r/haskell/comments/3cmc6l/thinking_differently_about_imperative_code_after/csxfatx?context=3), for example. Longs story short, wanton cruelty to the common memory. &gt; "What are the legal values of this parameter" is usually trivial to answer Unfortunately, ⊥ is always one of the answers.
Yeah, I think non-monadic exceptions are an evil part of Haskell, and I'm glad people tend not to use them. 
1.2 ≤ 1.0 + 0.6/2
There are two systems at work here - Haskell's type system, and the language semantics. You can think of these roughly correlating to the rules governing types and terms. Haskell's type system only catches type errors, but what makes it useful is the ability to encode information in the types in such a way that many programming errors will present themselves as type errors at compile time (wrong argument order/count, using Maybe to encode the possibility of failure, type-safe vectors, pure mutation with ST (mutable cells cannot escape ST, so runST is pure), homogenous lists, etc). The other system is the semantics of the rest of Haskell, which is pure, so functions cannot keep state (except for those in IO), and also doesn't know anything about pointers (so null pointers are impossible), memory management (it does it all for you so you can never double free, use after free, etc), or allow access to arbitrary memory, so things like segmentation faults should be impossible without a bug in the compiler itself. That all being said, the guarantees that the the type/runtime system give you are not absolute - there is the possibility of bugs, which might invalidate some guarantees (an error in the garbage collector could give you a runtime error, things of that nature), but also there are some holes in the type system: usage of "unsafe" things such as unsafeCoerce and unsafePerformIO basically means all bets are off, and certain kinds of recursion* can mean that the typechecker cannot always catch all kinds of type errors. * ie: data False false :: False false = false will not result in a type error, even though it is obvious that it is impossible to produce a term of type False.
Something like that was the plan, yes.
[`purescript-thermite`](https://github.com/paf31/purescript-thermite) is probably the closest thing to Halogen for React in PureScript-land. Halogen uses virtual-dom, but supports different backends in theory, so a React backend should be possible. The separation of state updates from the component itself is a little more marked in Halogen, though: it uses a purely functional model, with components emitting new states, whereas React supports a direct `setState` operation on the component context.
&gt; what this effectively leads to This "effectively" is very hard to define. A typechecking program can and will be able to crash. We still have deadlocks, accidential non-termination, `error`, and so on. I think we should be very careful with the "if it compiles then it works" statement, and all its siblings.
Bottom is different from null though. It may lurk inside any type, but we do not use it to structure program flow (and we couldn't even if we wanted to). This situation is the same across all popular programming languages, so I don't think we should separately mention it.
Why 0.6/2 ?
"The term 'crash' here has a formal definition that includes hard crashes like 'segmentation fault', but not things like pattern-matching failure. The non-crash guarantee can be subverted by using certain unsafe language features, such as the Foreign Function Interface."
Well Haskell is a bit different, as the bottoms may lurk in unevaluated values, while in a strict language they would at the very least have the decency to crash/hang a program immediately, I remember reading [Robert Harper](https://existentialtype.wordpress.com)'s posts about this. Though I admit I never cared about it that much, the last time I had to hunt an exploding variable throughout a program it was royally broken anyway and I had to "fix" it by just scrapping the offending part and starting again ;) EDIT: Sorry, that would be "strict functional language", I think. Mutable pointers and NullPointers in particular could be considered to have the same effect as lazy evaluation here.
I guess people are less curious about reflex and more so about reflex-dom. When I looked into Halogen, for example, I compared the code to what's found in the [try-reflex](https://github.com/ryantrinkle/try-reflex) tutorial. From a quick glance,Halogen would seem to have more in common with [haste-perch](https://github.com/agocorona/haste-perch), in particular in the way event handlers are set up.
It's easier to just list deficiencies in other languages, which could go on indefinitely - not because Haskell is perfect, but because there are so many lowest common denominator languages like Javascript. * No more switches with missing cases (especially when enums get updated and you have to hunt down code) * Java / runtime type checking - no more using Object and "instanceof" checks in an attempt to fake sum types. Missing checks if classes get added. * Javascript / duck typing - if you want to do the equivalent of a typeclass between several objects, you just have to hope you remembered to implement them all. * Javascript - since all arguments to a function are optional no way to force calling a function "correctly". Have to throw a runtime error instead.
Well okay, I think I misunderstood the question. I thought it meant "what can I expect from inside a function", when apparently it was supposed to mean "what can I pass to the function".
Well just like I wrote in that post, I still remember one case of debugging just as heroic as ultimately unnecessary. Other than that, well, in Haskell I'm obviously more eager to use infinite data structures, which can cause this, though it could be considered a side effect of more power. 
Where did you get stuck? The datatype above, `EitherT e (StateT s (ReaderT h IO)) a`, is a composite, obtained by "stapling" additional functionality on top of a basic datatype that assumes I/O. This stack of types is made to (in outward order): 1) record values of a type `h` from a shared environment 2) modifying some state, parametrized by a type `s` 3) testing for exceptions and in case return one of type `e`. 
The Enum instance for Double works a bit different: it can produce values higher than the max bound. The Haskell report says &gt; the list terminates when the elements become greater than e3 + i/2 for positive increment i, or when they become less than e3 + i/2 for negative i. Source: https://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1310006.3.4
If you look at the [specification](https://www.haskell.org/onlinereport/basic.html#sect6.3.4), it says &gt; The sequence `enumFromThenTo e1 e2 e3` is the list `[e1,e1+i,e1+2i,...e3]`, where the increment, `i`, is `e2-e1`. If the increment is positive or zero, the list terminates when the next element would be greater than `e3`; the list is empty if `e1 &gt; e3`. If the increment is negative, the list terminates when the next element would be less than `e3`; the list is empty if `e1 &lt; e3`. &gt; For `Float` and `Double`, the semantics of the `enumFrom` family is given by the rules for `Int` above, except that the list terminates when the elements become greater than `e3+i/2` for positive increment `i`, or when they become less than `e3+i/2` for negative `i`. Whether that's a "wat" or not, I leave up to you to decide.
Having bottom not crash the program immediately is a feature. Then you can write code like (not (null x) &amp;&amp; foo (tail x)) without it exploding.
&gt; Null pointers are impossible Same applies to the untyped lambda calculus, so it is not a type system feature. &gt; "What are the legal values of this parameter" is usually trivial to answer Uhm? &gt; Wrong argument order is likely to cause type errors That's the point, IMO. Although wrong argument order is identified in a second on the untyped lambda calculus if you are doing it right. &gt; Wrong argument count will cause type errors Same as above. &gt; Calling a convenience function never changes that function (e.g. a "validate" function that resets some counters, which I've recently seen) Same for the untyped lambda calculus. 
`[0,x .. 1.0]` is simply syntactic sugar for `enumFromThenTo 0 x 1.0`. This function is defined over all types in the `Enum` typeclass. Its default definition (that is, if one isn't provided when a type is instantiated) relies on the type in question's definitions of `fromEnum` and `toEnum`. λ&gt; fromEnum 1.2 1 λ&gt; fromEnum 1.0 1 As far as the `Enum` typeclass is concerned, 1.2 *is* 1.0.
To be honest I don't think I ever wrote code like this. There are pretty specific circumstances at work here, that &amp;&amp; will always short-circuit on the first argument, and that apparently you need to apply some ([a] -&gt; Bool) function. Both in this and in any other case I can imagine right now, a pattern match would be more readable anyway, perhaps in a list or monad comprehension to let the MonadPlus instance short-circuit on failed matches in a more, in my opinion, principled way. EDIT: Okay, sorry, apparently I'm so used to lazy evaluation that I forgot that &gt;&gt;=, &lt;|&gt;, and the like also depend on it. So nevermind then ;)
Can't misspell a variable, function, or field. The last one's the worst, especially when assigning, where in for example javascript I won't get any sort of error except that things are mysteriously not working. Or, worse yet, they will work for awhile and then stop in the future, making it all the more difficult to find the origin.
&gt; I KNOW that a reference (`Foo&amp;`) will never be null int main() { Foo* p = 0; // null DoTheThing(*p); // this line doesn't crash? return 0; } void DoTheThing(Foo&amp; you) { you.DoThatThingYouDo(); // crash doesn't happen here either } class Foo { public: int mValue; void DoThatThingYouDo() { mValue++; // crash happens here?!?!?! } }; WHERE IS YOUR GOD NOW?
You can use the type system to eliminate IO when you want to enforce that code has no side effects
But &gt; Coming next: what about getting Arrow from Applicative? Are the Applicative axioms powerful enough? (Hint: no) 
I dunno, I think it meets the qualifications put out - "...kinds of bugs people found in other languages that would have been impossible in reasonably written Haskell code." Sure, it might also be impossible in other classes of languages as well, but the same can be said for the other points given.
Not really; you could have a dynamically typed language where assigning to a property that hasn't been declared on an object causes a loud failure, instead of silently adding the property.
Automatic type conversion problems. For instance: object.SetValue(3445); But unfortunately for the class of which object is an instance, SetValue is overloaded and only has string and bool implementations. C++ converts your number to a bool and parties on. 
I actually recently hunted down a bug at work that couldn't have existed in Haskell. I work in Go, and we had a global level struct that we intended to never change. It mostly just contained some special values in its fields to make looking up a specific thing easy. The type of this struct is used extensively in our codebase, and is often mutated to track changes in things. Long story short, it ended up getting passed into a function somewhere that ended up modifying the values of the struct, and breaking a lot of things. In Haskell, this global value would have been immutable, thus preventing it from being modified in unexpected ways.
haha, editing above for correctness! Btw, *please* don't think my god is anywhere near C++ ;)
Psh, you oughtta be using the "composite pattern" to fake sum types in Java. For example, write an interface representing the top-level "datatype" (e.g. List) and classes representing the constructors (e.g. Cons, Nil), and then put all of the functions that should just be a successful pattern match on Cons onto the Cons class, etc. You end up having to put all of the functions related to each constructor right on the class involving it (if you want to truly avoid all the "instanceofs") but according to some proponents of OOP, this would be an organizational benefit somehow. (The user knows all of the useful List functionality is right in the List class and doesn't need to concern themselves with other functions about lists that the original implementer - you, in this case - didn't think of). Anyway, I'm not actually being facetious about this pattern being a good way to fake sum types - I use it all the time if I have to use Java, and the lack of explicit runtime type checks makes the control flow much simpler.
&gt; No more switches with missing cases (especially when enums get updated and you have to hunt down code) Haskell doesn't have exhaustive pattern matching. Missing cases won't produce compiler errors. You can work around this issue by turning on compiler warning, but the same thing can be done with C compilers.
[stable-memo](http://hackage.haskell.org/package/stable-memo) author here. I think stable-memo is a solution to your problem, as long as you are okay with the memoization being by identity rather than by value. That is, if you generate the same vector of points in two places, it won't work; they have to originate from the same allocation. Also, apologies for the unnecessary `tagged` dependency which is now forcing to use an older version of tagged. I will remove the dependency altogether soon. I just haven't checked on this package for a while.
The first thing I add to a new .cabal file is ghc_options: -Wall -Werror
What IDE do you want showcased?
I have an idea for a project: Social organization / "countdowns": It's not about small talk, but actual information, the time being the most important. People would have their own countdowns and would be able to follow the countdowns of the others. This project would contain: - web development - database management - internet communication BUT: - needs server (for more comfort) - no complex Haskell algorithms, however: interesting IO mechanisms ("real world Haskell")
Perhaps ironically, it'd also be immutable in c..
It's indeed the empty type forall a. a, except it's not empty in Haskell because of * undefined * let x = x in x * error "boom" * etc.
* Haskell has no null type or value. An integer must be an actual integer, a string must be an array of zero or more characters. There are plenty of types for representing non-data, such as `Nothing` or `Left "&lt;error message&gt;"`, but no Haskell function can be passed a null value. * The programmer never has to guess at types. Haskell has especially strong type signatures, even typing for side effects like I/O and random number generation. You get a solid sense of how a function behaves based on the input / output signature alone. * Again, strongly typed languages catch many kinds of invalid function parameter errors. Something Erlang, a dynamic language similar to Haskell, cannot do very well. Erlang only valides the number of arguments, not the types. * As a pure functional language, Haskell is quite excellent at caching/memoization/idempotence. Simon Peyton Jones likes to say that in Haskell, functions can't accidentally fire a missile too many times due to multiple function calls. Read [Learn You A Haskell](http://learnyouahaskell.com/) and [Real World Haskell](http://book.realworldhaskell.org/), two excellent free ebooks on introductory Haskell programming.
`forall a. a` isn't the empty type, though. It's the type of dependent functions `Π(a : *), a` (which happens to be uninhabited, however). The "empty" type in Haskell is `Void`. But I know about Haskell having terms of type `Void` or `forall a. a`; what I'm asking is whether, when one refers to "⊥" or "bottom" in the context of Haskell's type system, are they referring to the empty type, or a term of that type.
In the notation `[a, b .. c]` you're meant to put the last value you want in the list as `c`. Your example goes wrong partly because you're not using it as intended. You're supposed to say `[0, 0.6 .. 1.2]` or `[0, 0.6 .. 1.8]` etc, depending on which list you want. It's just rounding to the nearest multiple of the increment. This is to deal with the inherent inaccuracy of floating point arithmetic, which could have bitten you whilst using it as intended but with very large final values, if it didn't round to the nearest half increment. 
[hawk](https://github.com/gelisam/hawk) can interpret haskell in a terminal. (might help with your original problem)
The explanation/motivation you wrote here should probably be copied to the hackage package introduction.
To understand the motivation for the current design, try takeWhile (&lt;= 0.3) [0.0,0.1 ..] and [0.0,0.1 .. 0.3] 
⊥ is a *value*, not a type. The empty type is usually called `Void` in Haskell, defined via `data Void` without any constructors. The only value of type `Void` is `⊥` of course.
Robert Harper's posts about haskell may be constructed with a great deal of thought and a good deal of knowledge, but objective, fair, balanced and unbiased are adjectives I'm increasingly unable to use for them. He does seem to find many creative ways to conclude that haskell is Wrong and Broken. 
&gt; not matching on Nothing is pretty much the same thing as java.lang.NullPointerException. You are much more likely to forget to check for null than to forget to pattern-match on `Nothing`! The type system forces you to distinguish between `Maybe a` and `a`, so when you receive a `Maybe a` but need an `a`, there is a conversion step at which you need to decide what to do if your input is `Nothing`. If I decide to only pattern-match on `Just`, it means I have decided that due to reasons I am aware of but the compiler isn't, the input can never be `Nothing`. I may be wrong about that, but at least I was forced to think about it.
if you know the index is less than the length, you can often just use a V.map and V.fold, right?
This is all a question of orthography, so let's be clear: 1) In Haskell, there is no ⊥. There are various things that mean roughly the same thing as that, but there's no symbol that means ⊥. At best, `undefined`, but that's just one particular terminating flavor of ⊥. 2) ⊥ is used two different ways in the literature, either as a computational value or as a type, as follows: ⊥-the-type is the empty type (roughly equivalent to Haskell's `Void` type), which has no elements. It's the nullary disjunction. ⊥-the-value is the value corresponding to non-termination, implicit errors, undefinedness, etc. This reuse of the symbol is actually quite reasonable, if you look at the history. ⊥-the-value comes from the mathematical community, where it's often used as the symbol for the least element of a lattice (hence why ⊥ is pronounced "bottom"). In languages like Haskell, the semantics of the language has to be given in terms of lattices, so that each type `A` corresponds to a lattice of values of varying amounts of definedness (see [here](http://blog.ezyang.com/2010/12/hussling-haskell-types-into-hasse-diagrams/)). The usage of ⊥-as-type comes from the logic community. You see, one particularly important lattice is the lattice of propositions, and in that setting, the least value is the proposition of falsity. So in a logic setting, ⊥ means false, and as a type of course false is `Void`, since there are no proofs (ie no elements). When Haskellers write ⊥ when talking about Haskell, they almost certainly mean ⊥-the-value.
`forall a. a` and `Void` are equivalent. But see my reply to quchen below.
Good, it looks promising. Memoization by identity should be allright -- that would be the same as for adding a field to the data type anyway. I will certainly try it. :) Would `Data.StableMemo.Weak.memo` be the memoization function to use in order to achieve a similar effect as to putting the value in the data type? If a polygon object is garbage collected and no value references it's memoized bounding box I would like that to be garbage collected as well. Are there any known drawbacks that one should know about when using stable-memo?
To simplify, let's ignore the details of Agda and focus on how a language could simultaneously be Turing-complete and have no partial functions. All Haskell functions are pure, even those which return an IO computation, right? Because a value of type `IO a` is not a function which causes side-effects when it is called, but rather a value which represents a computation. At the same time, at runtime the program does perform side-effects, because our `main` is a pure expression whose value is a computation describing all the side-effects which we want to see at runtime. This way, we can have our cake and eat it too: the language is pure, but the runtime is effectful. One important way in which the language separates the two worlds is by preventing pure functions from depending on the result of an `IO` computation. Only other `IO` computations can depend on that. (except if you use `unsafePerformIO`) Similarly, in our hypothetical Agda-like language, all functions are total (terminating on all inputs, without ever throwing an exception), even those which describe an infinite `IO` computation. Because a value of type `IO a` is not a function which might not terminate when it is called, but rather a value which represents a possibly non-terminating computation. At the same time, at runtime a program simulating a Turing machine might loop forever, because our `main` is a total expression whose value is a possibly non-terminating computation. This way, we can have our cake and eat it too: the language is total, but the runtime may be partial. One important way in which the language separates the two worlds is by preventing total functions from depending on the result of a possibly non-terminating `IO` computation. Only other `IO` computations can do that. (except if you disable the termination checker)
Are you saying that we should only point out classes of bugs that only haskell prevents?
Yes. It's a bug in the spec, imo. 
I think it is called method-chaining or fluent interface, see: https://en.wikipedia.org/wiki/Fluent_interface
This is the `operational` monad. You've discovered a form of the Yoneda lemma with `Wrapped`.
Both of those look excellent. It didn't occur to me that there'd be a phrase like Automatic Differentiation - which seems rather obvious now. Thanks!
Calculating derivatives is rather pleasant. There is an algorithm available there. Forward and reverse mode AD have different comparatively minor trade-offs, etc. The process of calculating integrals comparatively sucks. We get stuck with [numerical quadrature schemes](http://hackage.haskell.org/package/integration) typically for one dimension, or playing around with [Padé approximants](http://mathworld.wolfram.com/PadeApproximant.html) and [Taylor models](http://bt.pa.msu.edu/index_TaylorModels.htm). As the dimension of the problem starts to rise [Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) and [quasi-Monte Carlo techniques](https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method) dominate in terms of just crunching numerical results.
This is a pretty old blog post of mine, but I ran through one way of doing symbolic differentiation in Haskell here: http://kovach.me/posts/2013-05-01-symbolic-calculus.html It's nowhere near as fancy as automatic differentiation, though :)
The situations in which we "iterate over every value in that range" can probably be rephrased in terms of maps and folds, yes. I'm talking about the situations in which `(!)` is used anyway, for example in the implementation of those mapping and folding functions, when implementing binary search, etc.
Okay I'm just pointing those are not a type system feature, it is not fair to advertise them as such. &gt; How? Creating only small functions, one at a time, and testing them incrementally. Never creating a new line if the predecessors don't pass all tests. This way, you never have more than one line that could go wrong and fixing it is trivial. In Haskell, you can write a hundred of lines with all sorts of mistakes and let the type system miraculously guide you to fixing them. That's cool, of course. You can't do that without types. But that doesn't mean you can't write quality programs.
&gt; I don't think we're trying to convince users of the untyped lambda calculus to switch over to Haskell :) Now I feel like nobody wants me :'(
Swift enforces exhaustive switch statements - surprised it's not in Haskell
Yup, I just thought the OP might not know about the CA techniques for antidifferentiation (as opposed to approximation techniques), which always seemed a bit magical to me. 
Nice article! Though with this formulation, it looks like you lose the ability to define GADT constructors that would terminate the computation (what "Why free monads matter" calls "abort" semantics).
Well, for example, if you want to call a C++ member function directly from Haskell, you have to know that `this` is passed implicitly as the first argument. If you want to call a `virtual` member function, you need to know how the compiler implements virtual dispatch, or write a wrapper function on the C++ side that does the call for you. If your Haskell code wants to create or destroy C++ objects directly, it’ll need to manually call constructors and destructors. If your Haskell code wants to use a C++ template, you have to ensure that that template has actually been instantiated in C++-land. Using C++ data structures such as `string` and `vector` compounds the problem because these things depend not on the compiler, but on the *standard library* implementation, of which there are several. It typically turns out to be easiest to wrap the C++ stuff in a C-like interface, and export that to Haskell. 
&gt; University of Miyazaki Didn't know that was a place :3
Standards-specified undefined behaviors are not useful when they can (1) be caused by code far away from the site of said behavior and (2) no major compiler contains code to insert runtime checks for that behavior. See my [other comment](https://www.reddit.com/r/haskell/comments/3cwv5i/haskells_type_system_eliminates_entire_classes_of/ct0b47p) about this guarantee. Real code contains bugs, including bugs like this which, according to the standard, can cause demons to fly out of your nose. I've had to debug a "null pointer inside a reference" many many times. I'm tired of arguing with C++ official standards pedantics about this issue when every major compiler treats code with this problem exactly the same way. This discussion is about common bugs in code which Haskell prevents, and "assigned a pointer that I thought wasn't null to a reference when it really is" is exactly the kind of bug that C++ encourages you to shoot yourself in the foot with. Basically everywhere the standard specifies "undefined behavior" they mean "this will cause your code to segfault or trash memory in unknown ways", and this is one of the ways that can happen. **TL;DR: C++ doesn't help you write "standard-conforming program code", so nobody actually does. Most buggy C++ programs are buggy because they contain undefined behavior.**
Looks nice! I like that you made it based on the looks of haskell.org.
I'm pretty new here, but I assume if we get enough people wanting it, maybe the mods will change it? I'm predicting that some tweaks might need to be made before switching though. 
Ok, so there is a performance price to pay. Still could be a nice thing to try as a first step to improve performance by memoization, given a data type and a function.
&gt; Same applies to the untyped lambda calculus, so it's not a type system feature. The point here isn't "haskell has a type system, so here are the benefits" - java has a type system too. The point is that haskell has a great type system that stops you at compile time from doing something you'd only detect was a problem with good unit testing in a lot of other languages. You might not need a type system to have these benefits, but haskell has an advanced type system that does have these features. I've not come across anyone using the untyped lambda calculus for actual programming, so comparing haskell to it in a section that talks about benefits of haskell over other languages seems irrelevant. 
[Similar discussion](https://www.reddit.com/r/haskell/comments/2zts44/wither_the_platform/) 3 months ago, and [4 months ago](https://www.reddit.com/r/haskell/comments/2w3php/venerable_haskell_platform_necessity_or_product/).
Ironically that bit looks good until some Javascripty thing kicks in and wrecks it!
Looks good, it's got my vote!
They are not identical. Look closely at the type constraints. It's important that the GADT need not be a functor. But I don't think classes for Interpreter / WrappedInterpreter are warranted. There should be more than only one sensible instance given any f and m.
Ooh, this looks nice. My only nitpick would be that the "Save" button on the comment page doesn't have a shadow on the left when scrolled over. I also prefer slightly darker vertical bars indicating indented comment levels since I find light ones hard to read, but that's just personal preference, not an actual issue.
I'd suggest changing the title to `Haskell :: SubReddit`, with the case as you see fit. Subreddit seems like a better type than just Reddit. 
Sum types can be only used to handle expected failure. When the program has to bail out because of a failed pattern match, asynchronous exception, some condition that indicates that the program has been miswritten and there's no way to recover from it, etc, it has to use an "all-encompassing" value like bottom.
To me it's an actual issue. Peoples monitors are different and on some monitors those bars will be completely invisible. Definitely make the bars more contrasty.
Lazy IO with `[Char]` is Just Fine - in some circumstances. But I doubt anyone would disagree with moving the lazy IO functions out of the Prelude.
1. Added missing lifting functions. 2. I personally use this library to drastically cut the amount of lifting instances in [Ether](https://int-index.github.io/ether/). Not only it simplifies my code, now if you want to make your monad transformer Ether-compatible, you don't need to depend on it, just depend on the minimalistic `transformers-lift` package. Other libraries of monad classes (such as mtl, monads-tf) could use `transformers-lift` in a similar way, though I don't think I can convince mtl authors to depend on my lib. 3. You can abstract over any transformer that is able to lift operations you're insterested in. 4. Not doing so is code smell. Like using `addInteger`, `addFloat`, `addDouble`, `addInt` etc. instead of polymorphic `+`.
Be careful not to fall into the trap of advocating type systems on the basis of what they are against. Remember what they are for: communicating exploitable structure to other humans and to the computer.
That's true, but for most of the integrals you'll find in calculus text book the algorithm is likely to work. :)
&gt; They are exactly equivalent No, not exactly. Applicative is a bit more general than Arrow. In [part IIa](https://cdsmith.wordpress.com/2011/08/13/arrow-category-applicative-part-iia/) of the post you linked, /u/cdsmith postulated some additional laws for Applicative which he conjectures would allow you to derive the Arrow laws. In that post he derives all except one. Ross Paterson earlier proposed a similar but different [set of laws](https://mail.haskell.org/pipermail/libraries/2011-January/015556.html) for the same purpose. I don't know if anyone has actually completed this kind of construction, but it seems that it should be possible. It appears that the required extra Applicative laws will be reasonable and intuitive, and satisfied by any garden-variety Applicative that you encounter in practice. Anyway, I have not yet seen any Arrow-based library that could not have been implemented easily, and more simply in my opinion, with an Applicative interface instead.
Automatic differentiation is pure magic when you first discover it.
This sort of question arises from time to time. You might like to look at a previous discussion: https://www.reddit.com/r/haskell/comments/1xwbb8/what_is_the_type_of_the_derivative_operator/ Another approach is to use Chebyshev polynomials such as is done in Matlab http://www.chebfun.org and Julia https://github.com/ApproxFun/ApproxFun.jl. I think laziness could help in Haskell as the representations could be infinite lists of coefficients; in both the Matlab and Julia implementations, I believe there is an algorithm to decide the length of the representation (although I also believe there is some part of the Julia implementation that allows for infinite representations).
I like the visual distinction because when I'm following my orangereds I don't always keep track of where I've ended up.
I see! So which words should we use to convey the advantage of `Maybe` over nulls, then? How about this: &gt; Haskell uses a different type for values which can be null and values which cannot. This has two advantages: &gt; &gt; 1. The type-checker will stop you if you should have done a null check but didn't, and &gt; 1. You no longer have to pollute your code with defensive null checks everywhere. For the record, here is what I would reply to the skeptic who thinks Haskell renamed null to bottom: &gt; That is true, any value can evaluate to bottom, and evaluating bottom does raise an exception. But the exception it raises is closer in spirit to a StackOverflowError than to a NullPointerException. Haskell doesn't claim to solve the halting problem -- any expression can still get stuck in an infinite loop! In Java, both kinds of exceptions could be thrown by any code, but in the case of nulls, you're able to check whether the expression is null, and a NullPointerException is what happens when you forget to do that. In Haskell, you can't forget, because the compiler will remind you. With infinite loops, whether you're in Java or Haskell, there is just no way to check whether a given call will loop forever or not, you just have to hope it won't.
Yes, it is better to err at runtime than to continue. Still a bug. If it fails at compile-time, it is not a bug. 
Haskell does check for exhaustiveness, you just have to enable warnings. I don't know why warnings are not enabled by default, I think they should be. Here is a program in which the False case is missing: module Main where boolToInt :: Bool -&gt; Int boolToInt b = case b of True -&gt; 1 -- missing case for False! main :: IO () main = print $ boolToInt False By default, we don't get any warning, so we only get an error at runtime: $ ghc Main.hs $ ./Main Main: Main.hs:(4,15)-(5,13): Non-exhaustive patterns in case The sane thing to do is to turn on at least the default warnings: $ ghc -W Main.hs Main.hs:4:15: Warning: Pattern match(es) are non-exhaustive In a case alternative: Patterns not matched: False It's a good idea to enable "all" the warnings (which in reality is more warnings than the default but still not everything), and maybe even to turn warnings into errors: $ ghc -Wall -Werror Main.hs Main.hs:4:15: Warning: Pattern match(es) are non-exhaustive In a case alternative: Patterns not matched: False Failing due to -Werror.
Disagree. Checking in code that doesn't compile is a bug. Checking in code that fails automated tests is a bug. Checking in code that produces incorrect results is also a bug. class Foo { int bar; }; def foo = new Foo(); foo.baz = 23; // &lt;- THIS IS A BUG. That last line is a bug one way or another; the difference is when you are going to catch it - at compile time, when the code itself runs, or maybe much later when it turns out that you've been saving wrong values to the database for months, and have lost large amounts of mission critical data, and then your manager comes to your desk and asks how this is possible and you don't really have a good explanation and neither does your team lead, and you all get fired and then you spend your days building Drupal websites for people who shouldn't be allowed anywhere near electronic devices or useful information of any kind, or teaching Microsoft Word and Outlook Express to unmotivated long-term unemployed fellow citizens who only take your class because it's free and they'd be cut on their welfare if they didn't, and it doesn't really matter whose fault it was anyway because that information you lost was so critical that the whole company goes down the drains and everyone is laid off except for the boss who gets fired just in time to give him what's left of the company's value as a golden handshake and to cash his equity shares and book a flight to the Bahamas. Anyway, wrong code is a bug one way or another, the question is when and how you find out that it's wrong, and in that regard, finding out earlier is strictly better.
What is the category-theoretical concept behind the "pairing" method?
Good call. I think where my mind went was that the way you use them makes the `WrappedInterpreter` redundant; it seems like its just a class used to make an `Interpreter` instance for any type `f :: * -&gt; *` using `Wrapped`.
To quote /u/chrisdoner: `ᕕ( ᐛ )ᕗ` __EDIT__ Just to clarify, Chris introduced me to this Unicode-art a few months ago, and I use it at any opportunity I can find since I like it so much ;)
We at [Snowdrift.coop](https://snowdrift.coop) are a volunteer-based community cooperative non-profit, the site is basically entirely in Haskell, and we aim to be the most beginner- and volunteer-friendly project we can possibly be. We have thoroughly documented stuff, lots of tiny ways to help, and everyone is really encouraging and helpful. We've even taught Haskell basics to people who support our mission and started Haskell just to help us. We mostly chat at #snowdrift on freenode, our code docs are at https://git.gnu.io/snowdrift/snowdrift#tab-readme (which has links to appropriate pages on the site, tickets, etc.)
Does this include/preserve the "idealized" tutorial Windows (it's not an issue for Linux) experience where you can click next a couple times and have all the batteries installed, including network and maybe something that lets you draw shapes in a window?
&gt; the wiki page on errors and exceptions Thanks for the link, I had not encountered this page before. To clarify, what I call an "unexpected error" corresponds to what this wiki page calls an "error", while an "expected error" corresponds to what this wiki page calls an "exception". What I call an "exception" includes anything which bypasses the type system to wind up the stack and release resources via `bracket`, and therefore include both `error` and `throw`. I think this corresponds to what the wiki page calls an "IO exception" and to what you call "non-monadic exceptions". What I call "failure monads" includes things like `Maybe` and `Either`, which implement the propagation of failure as a monadic effect. I think this corresponds to what the wiki page calls "exception monads" and to what you call "monadic exceptions". The wiki page claims, if I understand correctly, that errors and exceptions may be propagated using any mechanism including error codes, IO exceptions, and exception monads. The difference is that exceptions should be documented and callers should handle them, while errors should not be documented nor handled. What I am claiming instead is that errors should be propagated using IO exceptions, while exceptions should be propagated using exception monads. I do agree that errors should not be documented nor handled. I personally find the wiki page's nomenclature more confusing than either mine or yours, but if it's standard, I'll try to adopt it. Do you know of any document advocating it which was not written by Henning Thielemann?
True. You can definitely work your way through Spivak with it and pass with an A if nobody checks your work. =) The main issue I have with Risch's algorithm is that folks trot it out like it works in a meaningful fraction of the situations that arise in practice, and it just doesn't. It is great when it does work, but my experience is that there are vanishingly few situations that arise in the "real world" where Risch's algorithm applies in a way you could implement it in code rather than just offer it to the programmer as an aid to help them write the program in the first place. In that setting it isn't an algorithm so much as a search tool, and plays approximately the same role in my programming process as googling for the answer, or asking Wolfram Alpha or Mathematica to see if there is a nice closed form for something. Since the latter exists, has had way more effort piled into it than any Risch variant I can code, and I can't turn it into a usable algorithm that can solve more than a tiny fraction of the cases that arise anyways, I just let Mathematica fill that niche for me.
Gershom brought up the point of binary packages for Windows, which [I answered on the mailing list](https://mail.haskell.org/pipermail/cabal-devel/2015-July/010212.html). The tl;dr is: 1. This plan as stated would _not_ include those binary builds 2. I'm in favor of augmenting this going forward to allow for binary packages, though I don't want that to become a blocker for other improvements 3. I think even on Windows the situation won't be too bad, since once we add in msys, installing things like network will become much easier on Windows (we already have that experience from MinGHC)
You may also be interested in this old draft I worked on regarding a first-class representation of (approximate) differential equations: http://gbaz.github.io/slides/ode-draft-2009.pdf
👍 
`pair` looks kind of like [`zapWithAdjunction`](https://hackage.haskell.org/package/adjunctions-4.2.1/docs/Data-Functor-Adjunction.html#v:zapWithAdjunction) to me. I'm not sure whether `zapWithAdjunction` is as general though.
&gt; Can you please elaborate what you mean with that? From your statement there I can see no issue with calling baz on that. Is baz a partial function or what exactly is the issue? I think he means data Foo = Bar {foo :: Int, bar :: String} | Baz {baz :: Double} then &gt; baz $ Bar 0 "" *** Exception: No match in record selector baz. Not only this is legal Haskell, there's actually code like this [in GHC itself](https://downloads.haskell.org/~ghc/7.10.1/docs/html/libraries/ghc/src/TyCon.html).
You may find this blog post about forward mode automatic differentiation (similar to yours `DxDt t x`) useful: https://vandreev.wordpress.com/2006/12/04/non-standard-analysis-and-automatic-differentiation/
Will there still be reasons to use Stack after Nix-like features added to cabal? I'm talking about this Google Summer of Code project https://gist.github.com/fugyk/37510958b52589737274 https://www.google-melange.com/gsoc/project/details/google/gsoc2015/vishal4556/5634387206995968
He has a truly remarkable collection of such smileys.
Animated fade removed!
Can't wait until ghcjs works with stack. Then the way to download ghcjs will be "just download it from haskell.org and do `stack build`".
[/r/haskell has `#ddf`](http://i.imgur.com/YAlYGdq.png)
Hopefully when [stack-ide](https://github.com/commercialhaskell/stack-ide) becomes available IDE support will be as easy as: 1) Install haskell platform 2) Make stack project 3) Install plugin for ide of choice that just uses `stack ide` under the covers [GHCJS should be supported by stack](https://github.com/commercialhaskell/stack/issues/337) at some point.
`#DDF` is the color of the background when you click on a comment or move to it with arrow keys. That's why it turns a shade of light blue instead of the normal gray. 
No, the image I linked to specifically says the border-left (i.e. the tree line) is #ddf. [Ready to believe me?](http://i.imgur.com/4hRMYeb.png)
Oh, yeah, that's a huge difference! I had forgot how much RES does. I should probably install it again...
I make the lines a lot darker. [Is this better?](https://www.reddit.com/r/evanrelf/comments/3czuut/i_think_the_fade_in_on_load_may_be_unnecessary/) I can also make the line solid or dashed if it's still not bold enough (right now it's dotted).
Looks good to me! Nice work. :)
can we please rename `stack` to something more sensible while it's still cheap to do so? like eg `cabal2` 
Or 0.3/0.1==3
This is a summary of my journey with Haskell tooling: * Downloaded Haskell Platform * Learned some Haskell * Tried a bunch of editors / IDEs, gave up and went with SublimeText * Got into trouble with dependencies in Cabal * Learned the situation is so bad that we've coined the phrase Cabal Hell * Reconsidered continuing with Haskell * Nuked everything Haskell related on my machine and installed GHC on its own * Learned about sandboxes * Found MinGHC - nuked Haskell again and tried that to simplify things * Found that various packages won't install with the latest GHC (including ghc-mod) * Decided to use Stackage - working out how to use Stackage was surprisingly confusing * Hated having to rebuild packages like lens in every new sandbox * Occasionally I'd install a package globally by accident (forgot to initialise a sandbox) and would have to nuke everything again * Tried Stack - seemed cool but was confused about how to configure it * Realised I needed to get SublimeText to build using Stack instead of Cabal - and went back to `cabal init` and sandboxes Though to be fair my experience with learning Java and Maven wasn't necessarily much better. Thought it more than made up for it on the IDE front. And dependency problems on Ruby gems (working with SASS for web) was pretty painful. So it's not like Haskell is alone in this. But the handful of times I've helped someone who was newer than me - my first advice was "uninstall HP and get MinGHC". I think that's a pretty sad state of affairs. TL;DR - how soon can we get this? :)
Thanks for the advice. I will try `linear`, and look into proper package management instead of cobbling junk together.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/evanrelf] [Click here to go back to my proposal post on /r/haskell](https://np.reddit.com/r/evanrelf/comments/3d2no4/click_here_to_go_back_to_my_proposal_post_on/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger/wiki/) ^/ ^[Contact](/message/compose/?to=\/r\/TotesMessenger))* [](#bot)
Maybe try making the logo a bit bigger? Looks amazing though! :)
No, facebook's typed php superset is called hack. At the very least calling it haskell-stack would be an improvement.
If you want to try writing some bindings yourself to mp4v2, here are some resources: http://book.realworldhaskell.org/read/interfacing-with-c-the-ffi.html https://en.wikibooks.org/wiki/Haskell/FFI The only part that I didn't find immediately obvious was the need to sometimes block the Haskell RTS. [example in mysql library](https://github.com/bos/mysql/blob/master/cbits/mysql_signals.c#L28) https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Signals Here's the effect of not blocking the Haskell RTS in FFI code: http://stackoverflow.com/questions/15226806/importing-c-delay-function-into-haskell-using-ffi Some more resources on Haskell/FFI: https://wiki.haskell.org/IO_inside#Interfacing_with_C.2FC.2B.2B_and_foreign_libraries_.28under_development.29 [Haskell ffi basics presentation](https://speakerdeck.com/frasertweedale/haskell-ffi-basics) 
Yes, as it happens Greg [just wrote a response](https://groups.google.com/d/msgid/haskell-stack/CAKRanNC605RTO4kP%2BBm0WiAr6c4-sj9DeHQv4xD8PhprwuDRzA%40mail.gmail.com?utm_medium=email&amp;utm_source=footer) to this thread mentioning two features that are in stack and not likely to arrive in cabal-install in the near term (first-class multi-package projects and curated package sets). That's not to say this Nix-based stuff isn't exciting work. I spoke with Ryan Trinkle (who's mentoring the project) about it a week ago and how it might relate to stack, and it certainly looks like this will address some important pain points with cabal-install.
Thanks. I've been reading up on FFI a bit. I'll give it a shot if I don't another option.
`boop` **edit:** `whee` `yow` `plz` `gosh` `mo` 
You might have fun ;) It only took me about 10 minutes to do the sin example from [wikibooks] ( https://en.m.wikibooks.org/wiki/Haskell/FFI).
I pronounce ask with long a vowel "aask" (London accent), and hask with a short a. For that reason I say no.
Theres still lots to do, but I think this version is a big step up from 0.15.0.0, so I am posting it here. A big thanks to all those who contributed (directly or indirectly)! As well as some new contributors to the code, we had some good feedback from users on features to add and some really excellent bug reports. I have switched on the threaded RTS. This makes Leksah faster, but in the past has contributed to some stability problems relating to using Gtk+ on multiple OS threads by mistake. I think most of these have been sorted, but if Leksah crashes and gives you a stack trace, that trace is like gold dust to us (please file an issue). If you can reproduce the crash you can be a hero!
First-class multi-package projects are a rather minor thing and I don't think a strong reason for including a very young package in a repository of things the community has deemed its most fundamental and stable. This is not something that affects the "get haskell experience". People who need that feature are likely to be savvy enough to get the right tools anyway. Curated package sets are definitely planned for cabal-install from what I understand. It sounds like the nix cabal work is coming along nicely, and frankly I think that could solve the vast majority of user cabal hell complaints. All said, I think it's too early to put such a new tool into a place of this much significance and fragmenting the ecosystem when we have several promising developments coming up in the standard tools. I'm not opposed to putting stack in in the future if the situation warrants it, but let's not make that decision now.
A little bit of explanation what NsName, QName, ns1, ln1, etc. are might have helped with understanding what "contains" actually does and what it can be used for. Other than that, pointing out the similarities of Haskell and C++ Metaprogramming is great (for me), since I am familiar with one, and want to learn the other. Ah and btw.: Compile-time validated XML writer? Awesome!
I agree, stack is a terrible name, because it is so generic and widely used in programming to mean different things. I have no good suggestions, but `haystack` jumped out to me as an okay meld of `haskell` and `stack` that's distinct enough.
I don't see the point. If you google "haskell stack", you already get stack.
Although if it was going to be renamed this would be the best time. The smallest number of people would be confused.
I propose kcats ps: [surprisingly common](https://www.google.com/search?q=kcats)
Likewise. Also I am *deeply* offended that none of my fantastic suggestions [here](https://np.reddit.com/r/haskell/comments/3d0vb0//ct1b9bb) made it into this post! The nerve!
That's... not bad, heh.
It's a collision that doesn't need to exist. There are numerous examples of where things could go wrong - What if I wanted to get search results for how haskell manages the function call stack? Or how about searching for the ideal web development stack using haskell? What if searching for "transformer stack" starts getting stack-the-tool references. Or what if the user wants to search for a stack config file to work with the "transformers library", and gets only monad-transformers references? 
Ha ha I do like "plz". "plz build", "plz install", is what actually goes through my mind when I am trying to compile.
I agree in theory. I was going to be working on something called hake which was very much like stack when Michael told me about stack and we dropped it. For me the name was not the right thing to optimize for.
"st4ck" is way better and is wicked leet. feckin right buddy, those google searches will be so relevant my bitcoin startup will write itself.
or skat ~_~;
Stack is indeed too generic to my taste. Name it to something category theoretic? 
I'm not convinced it's a good idea including `stack` in the HP distribution unless `cabal` is being officially deprecated. Why? Because then new users gets faced with the decision of whether to use `stack` or `cabal`, and if we can tell new users with a good conscience that `cabal` is the legacy option (maybe `cabal` could even print a "please use `stack` instead" on startup?), and `stack` is the future, then it's all fine. On the other hand, if `cabal` is still being actively developed and gaining features from `stack` together with other features exclusive to `cabal`, then `stack` would merely constitute a tech-preview, which may ultimately become a legacy tool. Is it sensible to include an alternative tool as a temporary kludge in the HP which is supposed to provide a single recommended mature API/tooling for each task, when we already know it may be removed again?
From [How we might abolish Cabal Hell, part 2](http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/), the last sentence of the conclusion: "Nix-style package management and curated package collections are mostly complementary and we want both." I've also seen core devs mention on IRC and elsewhere that they specifically want to enable support for curated collections in a general way so anyone can point cabal to whatever custom curated set they might want to use.
costack?
I think you mean it will rite itself, don't you? 
Stacks are a categorical concept, and a pretty fancy one at that! Sort of a higher sheaf, or generalized scheme...
so... A stack is just a pile in the category of endodependencies?
It should rather be called "needle".
https://github.com/rnikoopour/Python.HS There's the link. Right now I have no git ignore so pardon the compilation related files. Also, the only thing implemented is some primitive arithmetic types.
That ship has sailed.
But a lot to type out. EDIT: I just realised that you don't have to name the thing after it's command-line alias :p.
Hastack.
I'm not totally clear on what the idea is, maybe you can clarify? I can see some situations in which countdowns would be cool in an environment like twitter, but I can't really picture them as the core datatype of social media network. It sounds like what you're describing is isomorphic to a calendaring service that allows subscriptions to other users calendars, is that right? If you only mean to write this thing to practice your Haskell, that's definitely cool and I support you. I just don't understand the end-user experience from a product perspective, if you're serious about taking it live eventually.
So `hackage` is the `haystack` and `stack` should rather be called `needle`, and then the invocation could be `needle find yesod`...
absolutely yes. I look forward to the day when trying out Haskell on your box won't be harder than surfing to ghc.io 
This metaphor is rapidly becoming untenable.
&gt; Swapping out a live server for a new one just seems plain tricky. That one is not a Haskell related fear. Yeah that sounds tricky in general, especially with a managed environment like Heroku. I suppose, if you have a domain name for the app, that you could keep running the current application, deploy a new version that operates on the same database etc., then point the domain name at the new one, and users would gradually switch over as their DNS resolution caught up... ...I might be making that up, but it makes sense in my head.
I've actually been thinking about something like this recently. The way I've been phrasing it is as a matchmaking system like for video games, or a non-regular meetup.com. The motivation is that without a matchmaking system video gamers that want to play but don't know about each other still cannot play. The matchmaking system enables collaborative activities that otherwise would not have occurred. These systems exist for video games but not for most other activities. The key use case I had in mind for my own use was, in your words, a countdown to watching a movie or reading a book, so that other people could join in and we'd have people to discuss the activity with. In short, trying to enable the [network effect](https://en.wikipedia.org/wiki/Network_effect). If you're creating some sort of mailing list or other online meeting ground feel free to send me a link. I can't promise I'll code but I'd be happy to help discuss ideas.
That's kind of a cool idea. Best of luck! Have you considered countdowns which might have geographical variation? Like anything related to satellites/astronomy?
Edit: Ok, sorry, I apologize. This was inappropriate.
Yes, it is like a calendaring service. I think the difference is that, like leperLlama pointed out, it is not really for personal organisation, but for group events. It isn't really oriented on making money, it's just a fun project... for now, at least. We'll see what the future brings.
In the internet, no one knows you are white. So its not actually racist.
Sounds nice. Do you have any frameworks or libraries in mind? I don't have much experience with Haskell, but I'd love to contribute if it is going to be open source. 
I want this *so* much.
great idea!. I think that it could be great oportunity for increase popularity of purescript and web solutions in haskell. 
Please don't. [Haystack](http://haystacksearch.org/) is already well-known in the python community (indeed, we use it here at work). Even if I think the name is a bit generic, a) it's as many letters as cabal, so just as easy to type, and b) searching for "haskell stack" yields the first 4 results as relevant (as of7/13/15).
You don't have to be white to be racist.
to summarise the first link: /u/jhinkle wrote this differential geometry library, with a notion of approximate identity for floats, https://bitbucket.org/jhinkle/haskell-diffgeom ; it includes a module for gradient-based optimization ( https://bitbucket.org/jhinkle/haskell-diffgeom/src/c9f0b09f7c4f2a382ad987ed19a1d54598b6bd7f/src/DiffGeom/Algorithms/Optim.hs?at=master ) that employs this notion of differentiable function: `newtype (Smooth r) =&gt; DiffableFn r = DiffableFn (r -&gt; (Double, CoTangentType r))` However there are no tests covering this and from the comments there seem to be some gaps in the guarantees offered by the library.
Firs of all, try to think of it as a gradual change. If you have several subdomains for your service, migrate one at a time for example. I'd code a functionality, deploy it in a heroku thing (and check how halcyon manages that), redirect the domain and test it. Then repeat for other parts. I also agree that 1500 lines is not a lot, it'd be a great exercise to learn real world haskell. About the hash lib, you probably have the same one already in a hackage library, if not coding it yourself is not that hard. Remember that this things take into account some global variables, like a *salt* maybe. And you can always test this yourself. Regarding your last point, I've rewritten CLI tools in haskell (previously written in python) and always ended up very happy about my choice (which as a haskell fan may not be a great deal anyway).
Can you elaborate?
Heroku looks like a solved problem at least: https://haskellonheroku.com/
1500 lines of Python is not a lot of code to rewrite, especially if done by the person who originally authored the project. 1000 lines of code is usually the sweet spot for rewriting a Python program, because that's roughly the point where the code starts to become difficult to maintain. The longer you wait to rewrite the more difficult it will be to fix later. I've rewritten a similar amount of Python code to Haskell so I'm speaking from experience.
I clarified the searchability point in my post above. Haystack has no haskell connotations and does not collide with any known haskell concepts. The command line tool may be shorter (say "hstk"), but that's a matter of personal preference. 
He thinks it's too late to change it now. Which is pretty unlikely if there were buy-in from the stack folks themselves. edit: typo
+10 for changing the name. +1 for `hastack` - seems most unique to me. -1 for `haystack` But actually, I really liked the name `stackage` and the `stk` short for the executable was very convenient.
&gt; Some African-Americans only consider nigga offensive when used by Americans of other races,[8] its use outside a defined social group being an unwelcome cultural appropriation. 
Interesting: I just found out Leksah depends on some packages of the Yi editor :) I did not know that... Question: what does Leksah currently use integrating with the GHC? (what many Emacs users accomplish with `ghc-mod`)
Thanks a lot. I had some problems with the refreshing speed on Windows and some task in the background that slow down everything after some time running it. But still is the best IDE. I hope that some of these problems have been alleviated now.
I've been in a similar situation, migrating a cronjob script built with PHP to Haskell. Although using a selfmanaged VPS instead of Heroku, and the app was not a realtime service like yours. For building and deployment I'm using Docker. Using a separate container to build the binaries with cabal, then copying the resulting binaries to a clean Docker debian image, with just a few libs installed with apt-get (mysqlclient, etc), to reduce imagesize. Probably could be optimized by using a base image with all haskell libaries, and a small dynamically linked image. For now I'm just optimizing for build time and ease of use. As for the codebase, it took about a year time to build the Haskell version due to being a complete newbie, and working a few days on it, then getting stuck and needing to learn more about haskell and the differences in building software compared to PHP (which is based on OOP, variables, loops, dynamic typing, using printf to check if your function is called with the right values, etc). There was no pressure/deadline so it wasn't a real big problem, it's just part of the process I guess. My takeaway from the PHP -&gt; Haskell conversion: * In the process I've discovered a few bugs in the original PHP codebase while comparing output from both versions. Most of them having to do with unexpected side-effects and mutability. * The typesafety of Haskell makes me feel more confident about the quality of the new version. It has been running for a few months without any problem. I've only had a bug that could probably have been avoided by using more strong typing (just plain lists were used and got mixed up, instead of giving each their own newtype to avoid). * The Haskell version runs faster with less memory usage. The PHP version uses the Doctrine2 ORM and so it starts executing DB queries whenever you access an unloaded property on an entity. In Haskell all necessary data is loaded once with Esqueleto/Persistent to be passed around later, and then the processing is done in pure functions. (Probably should be using the Reader Monad here, still learning about that :-)) * Refactoring / adapting existing code is more easy. When making changes the typechecker will let you know if you forgot to adapt some parts of your code, or what doesn't make sense. Some background: The original PHP script came in two variants, each working on a different dataset, but with a big overlap in processing steps. It was easy to adapt the Haskell version to support both variants of the data. It was a matter of changing the Esqueleto/Persistent entities, and then fixing all type errors. When it compiled it just worked. So refactoring isn't a suicide mission as it is in big PHP projects. (One exception though is a few places where I just used strings to create some database queries, and haven't got around yet to using/understanding the right types/helpers that Persistent provides). * Using Haskell for a real world project lets you experience the piece of mind you get from strong typing, purity, immutability and some other Haskell features that are foreign concepts when coming from PHP/Ruby/Python. Feels good knowing that all kinds of weird bugs can just not be there, because the program typechecks and your (pure) code can't have side-effects. On the long term I think it's good to learn Haskell because most languages are taking on FP concepts, and I think FP will become more popular over the years, because users of those languages will see benefits. So why not start now :D Last point, not really a plus for Haskell, but by using Haskell with Docker it's easy to start from scratch when you made mistakes with cabal, it's like doing a fresh install of your OS everytime you rebuild your container. And it'll be easy to switch to using stack or whatever, as you just deploy a docker container which exposes an HTTP interface to nginx. Hope this helps! [edit: list formatting]
It can also be pronounced similar to "chief", which is not too bad.
It's important to consider that the name of the executable does not need to be the name of the tool. [leiningen](http://leiningen.org/)'s executable is `lein`. [Bundler](http://bundler.io/)'s executable is `bundle`. If the name of the tool/project is easy to google and unique, then the name of the executable doesn't have to be as much
Not yet, will do! Thanks
LOVE love love this idea edit: but I would be so damned depressed if I started seeing Haskell nonsense in the results when I searched about sheaves.
It seem more aligned with FRP or something like that, as they are dealing with user interface. I'm more interested with "backend" code, using arrows to create a REST-like web service, no user-interface involved.
I currently have a pull request open for the ffmpeg-light library, which seems to do what you want: https://github.com/acowley/ffmpeg-light/pull/13
[I'm Black](https://github.com/aspidites/) My point was more so that his potentially being White was not what made the comment racist. I personally avoid the word most of the time (including amongst family members), because I think it only perpetuates ignorance in its continued use, and offends my ancestors who, more than I, had to deal with the deragatory use of the word. Actually, it just occurred to me that it's been about a year since I was called that by *anyone*. Perhaps America *is* finally learning. Or at least the subcultures I find myself immersed in.
Thank you!
&gt; Tried a bunch of editors / IDEs, gave up and went with SublimeText I would love to know how you got on with Leksah. Were there any particular things you think we should be working on? 
&gt; Does Leksah support plugins? No. Leksah is written in Haskell, so for plugins also written in Haskell, a plugin interface might just get in the way. It would be possible to add one though using something like hint or the JavaScriptCore engine included in Leksah and GHCJS. &gt; Also - does it provide a vim-mode? Only via Yi and the Yi integration is currently broken and needs someone to rescue it.
Thank you :)
&gt; Interesting: I just found out Leksah depends on some packages of the Yi editor :) I did not know that... Sadly the Yi integration just a proof of concept. It would be great if someone had time to make it usable. &gt; Question: what does Leksah currently use integrating with the GHC? (what many Emacs users accomplish with ghc-mod) It has its own system (leksah-server) that uses Haddock and GHC-API. Before it starts it `cabal unpack`s the source for everything in you your `ghc-pkg list` then indexes it.
Thanks for this!
&gt; I had some problems with the refreshing speed on Windows and some task in the background that slow down everything after some time running it. Yeah I think we might have a space leak somewhere. I suspect it might be related to the background build. I find if I leave Leksah running over night it sometimes seems broken in the morning. I'm off to bed now so I will repeat that test now.
With MSYS2 and the appropriate packages, you can even build the GTK bindings from source on Windows without difficulty. (Once [an issue with gtk2hs gets resolved](https://github.com/gtk2hs/gtk2hs/issues/110).) We're pretty close to eliminating the myth that Windows *needs* binaries except for the same reason as on other platforms, i.e. saving cycles and time.
I like `stack`, it's sounds quite good IMO. I had not problem Googling it, but as suggested that might not be always the case. Alternative name: `sinc` (accronym of sinc is not cabal :) Why not staying in the same (or opposite) connotation than `cabal`? - `myst` - `conspire` - `light` - `illum` (or better `hylum`) BTW, I'm interested in knowing where does the name `cabal` comes from :) 
It's lovely! Great job!
You realize this discussion is about [stack (the tool)](https://www.fpcomplete.com/blog/2015/06/announcing-first-public-beta-stack) and not stack (the data structure), right? :-)
should be fixed, looks like some unicode got mangled during my copy-pasting
This is great! It's a subject that is quite fascinating and that I am only recently getting familiar with. I started writing a Lens tutorial (https://github.com/ajnsit/haskell-tutorials/blob/master/lenses-from-scratch.md) after watching Edward's talk (http://www.youtube.com/watch?v=cefnmjtAolY) which follows a similar course. I hope you won't mind if I borrow some bits from your writeup (with attribution of course). 
Awesome! And I really like that the "Constructive" label has been removed from the upvote, because many things can be constructuve without an upvote.
It's likely that you can use `openssl` at the terminal and just play around with the hashing algorithms to make sure you can get the same hash. In my experience, I've found that it's easier to do this first and then play with haskell's crypto libraries.
Pretty sure he's one of the stack folks.
IMO the collision 'issue' is not actually an issue in this case. I think 'stack' is fine.
Looks great, but a quick request -- can we turn off the "new messages" css popup in the bottom left? I like to keep somethings in my inbox as "reminders" and so now on every page that thing bounces up.
Here is my Stack Overflow post: http://stackoverflow.com/questions/31359894/catching-an-exception-from-rundb. I was not sure if I put my link on my original post, whether it will be get flagged or not. 
Don't forget that the code that modifies that "read-only" struct was committed a year ago, so you can't simply fix it without breaking some other piece of the software that was relying on the buggy behavior.
Any chance that the amount of vertical spacing could be reduced a bit?
Is there any way to reduce the vertical spacing a bit?
Beautiful! This needs to get much better advertising than its getting!
Huh... actually, your font's different. Looks like the same typeface, but everything's just a hair thicker in your screenshot, which helps immensely -- I can see why you wouldn't have noticed a problem. That being said, yeah, I think the underlines are great. **Edit:** definitely a tiny bit larger, you've got better definition on the permalink/embed/etc. link text too. For reference I'm on Chrome-Win64, all default settings.
I only understood lenses after Simons Talk for Humans .. :) https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation
Ok, now the next step is to integrate this with Hoogle and Djinn. Then Haskell will be unstoppable! :D
At a high level, lenses allow you to write APL style functions where they bind at the appropriate sub-object of the data type?
Yes it does solve the issue. Actually the page was zoomed in a bit (I did it in the past and the browser retain the zoom factor per domain). So I guess it's pretty ok for me (although the text is a bit smaller than I would like), thanks. B.t.w. the theme is pretty neat.
yeah, I've tried and failed to install both GHCJS and Leksah (and ghc-mod) multiple times and they wouldn't build. I try again every few months ;)
'plz' is still my favorite.
Thanks! So much better!
Looks good!
I am using `try` from the lifted base package (Control.Exception.Lifted). I don't really understand what the error message is telling me. 
PureScript is always looking for contributors, and we have some [beginner issues](https://github.com/purescript/purescript/issues?utf8=%E2%9C%93&amp;q=is%3Aopen+is%3Aissue+label%3Aeasy+) marked on the issue tracker.
The use of Coyoneda to work around Coq's strict positivity requirement for transcoding the Proxy type was the most useful result. I've written a separate Functional Pearl on this, which has been submitted to the Journal of Functional Programming. Another positive outcome is the ability to play with experimental transformations of the core Proxy type (for example, using a Church encoding to avoid poor asymptotics of the recursive nature of the `Proxy_bind` function), and prove that such transformations are correct with respect to the Pipes laws.
*applauds*
Sorry for the very delayed reply, I have been travelling and missed your comment. Here are a couple reasons off the top of my head: 1. The burrito analogy strongly implies that a value of type `m a` somehow "contains" a value (or values) of type `a`. But that is not true for all monads (e.g. there is no sense in which a value of type `IO String` contains a `String`). 2. It is not actually very easy to take a burrito containing a burrito and merge it into a single-level burrito. At least this is not in any sense a natural operation on burritos. Perhaps you could argue that it is always easy to remove outer tortilla layers (but not the innermost one since the food will all fall out), but this is a bad analogy since in general `join` does not just "remove" an outer layer, but somehow merges the effects of two layers into one.
Something went wrong with the generated Haddocks; there are no type links except to `base` and within the package.
This look amazing, great job!
I don't think the meaning of the word matters. There is apt, yum, and even cabal out there, names I have no clue how they came about, which is no problem whatsoever. Just pick something more unique with a good two or three letter shortcut. 
&gt; For most companies this means building your own build systems and processes for maintaining packages sets I don't know which "most companies" you are speaking for. I would say we are one of the more mature commercial Haskell development shops around. We have three active major products based on Haskell. All of them are evolving very dynamically, and all are gaining significant traction in their targeted enterprise markets. Besides that, we complete enterprise-scale projects in Haskell on a regular basis. `cabal-install` works great for us. Period. Not that we're not interested in `stack`. We are very interested. It looks like a great tool, and we're watching it closely. But frankly, we are too busy with real work to waste time on switching to a new and much less proven build tool. I sincerely hope that the next HP release will not force us to do so prematurely. &gt; Cabal had its bite at the apple and until it was clear that a competitor would emerge frankly did little to improve the situation. That is totally unfair to the stellar Cabal team. Build management for a Haskell compiler that is a package-based separate-module compilation system turned out to be far more complex than anyone imagined, and cabal has stepped up to the task. With help from the community, the cabal team has worked tirelessly to pound out release after release and add feature after feature continuously for the past several years. Newbie hell is not yet solved, but calling it "cabal hell" at this point is wrong. While there is still plenty more we can do to improve our tools, the build problems commonly experienced by newcomers are most often no longer caused by lack of capability of cabal. They are caused by outdated and wrong information about cabal. Or by well-meaning "helpful" members of our community who advise them to change the way Haskell build tools are installed on their computer and eventually get them into an inconsistent state, instead of just showing them the right cabal commands.
No way it has sailed. The project is still very young and not that widespread. Scattering some permanent redirects should help the transition.
I like the flashy upvote animation, and like that you don't get it with a downvote. A little psychological push towards more positive rather than negative reinforcement :-)
The low contrast text is really hard to read, especially with non-ideal screens or lighting conditions. I've been really confused by the trend away from high contrast text (see: Solarized or Zenburn) in recent years.
I am not the author of the parent comment, but can give my answer to your question. I was turned off by Leksah because, if I remember correctly (I may have this mixed up with another editor I have tried over the years), Leksah was one of those "you must start a project before you can write any code" IDEs. In fairness, I wanted an "editor" and Leksah is an "IDE". Ultimately, I never found a way to start writing code in Leksah, though I did not look very long. I know I didn't give Leksah much effort, but this was my initial turn-off, since you asked.
It'll get there eventually. By the way, this is exactly why I stopped using DDG in general: it was always far behind the times when it comes to relevance. Stack is not unique here. Even if you compare it to un-personalized google.
I'm a fan of opt-in rather than opt-out, perhaps we can advertise on the sidebar a Stylish extension for the 'directors cut' version of the style?
Excellent, I'll add it to the OP.
Hmmm, I'll take a look when I get a chance. Thanks for the heads-up.
Well, until that stuff gets worked out I'd say it remains a fact rather than a myth :-P
That is good to know :). Thanks
sorry/u/sccrstud92 that was a typo (the issue of manually typing it here to simplify things). I am getting `liftIO` from here: `Control.Monad.Reader` All the stuff inside `runMaybeT` are Persistent statements specific to my model. If any of them returns a `Nothing` I want to throw an exception so `runDb` will rollback the transaction. This code works fine without exception handling. Of course, it will not send any meaningful error message back to the client as to why it failed (could be a form validation failure or db unique constraint failure). I am trying to distinguish between these two failures. If there is an exception, I want to check for the validation errors list and if it is empty, I know it is a db failure (these types of errors should be rare and will happen only when someone tried bypassing the client interface and post to the api directly I have added import specific to the functions used. 
You're getting `liftIO` from `Control.Monad.Reader`? I don't see than anywhere. EDIT: Oh I see, it's a re-export from `Control.Monad.Trans`, and from there it's from `Control.Monad.IO.Class`.
also can the plz use the narwhal logo? http://images.uncyclomedia.co/uncyclopedia/en/8/85/NarleyYeeaaahh.jpg 
Okay, I think the problem is the top-most `liftIO`. liftIO :: (MonadIO m) =&gt; IO a -&gt; m a `liftIO` can only be applied to `IO` actions. You are applying it to an action that uses `runDb`. `runDb` obviously runs in something more complicated than `IO`. The error is saying that `IO` is not an instance of `MonadReader Config`. It thinks you are in `IO` because of `liftIO` (and probably your type signature), and it needs an instance of `MonadReader Config` because of `asks` in `runDb`. My proposed solution to fix this specific error is to not use `liftIO` and get rid of your `IO (Either MyException Auth` signature. If you need the `Either MyException Auth` bit for some reason, you can stick that in the `case` expression.
I would be interested in helping. Experienced C#/web dev. Quite new to Haskell but keen to learn. Feel I have learned enough about Haskell to write some lines of production code. I will send you a PM with my contact details.
Thanks, /u/sccrstud92. I will try removing the `liftIO` and let you know what happens. I added the type annotation because GHC was complaining the Exception was ambiguous. I did not realize I could do `case (eauth :: (Either MyException Auth)) of` so I will try it there to see if it helps. 
&gt; My problem was performance. It would freeze, crash and lag. I stuck with 12.something for ages because v13 and v14 pretty much wouldn't work. Within a few minutes of using it (with auto-build turned off) there would be a massive delay between typing and the text appearing on screen. With an i7 and 16gb of ram this was pretty bad. Also - Eclipse FP was having exactly the same problem. It would be great to track this down (if it still happens). Quite a few performance issues have been fixed so it might not happen any more. If it does though please let us know. &gt; I only abandoned Leksah v12 because my need for sandboxes outweighed my want for an IDE. Leksah has some support for sandboxes. It foes not currently index packages only installed in the sandbox though. &gt; Will the latest Leksah work with the latest GHC? Yes 7.10 is supported.
cool, thanks. so a "C-like interface" mean no objects, no C++ types, etc.?
Here’s one plan of attack. Start by understanding the [Curry–Howard correspondence](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence) and how types are related to logic—[constructive logic](https://en.wikipedia.org/wiki/Intuitionistic_logic), specifically. Read [The Algebra of Algebraic Data Types](http://chris-taylor.github.io/blog/2013/02/10/the-algebra-of-algebraic-data-types/) to understand what “algebraic” means, and how algebraic manipulations can give us useful results. Then get into [abstract algebra](https://en.wikipedia.org/wiki/Abstract_algebra) and learn about the various algebraic data structures that are used in Haskell—monoids and so forth. Then you’ll be prepared to understand [category theory](https://en.wikipedia.org/wiki/Category_theory) and how we (and particularly Ed Kmett) use category-theoretical tricks to write reusable code. And then you will find it disappointingly easy to understand how a monad is just a monoid in the category of endofunctors. ;)
I really don't understand your criticism of them. Is it just that they're not subtle enough?
happy cake day!
Oh my god I can't believe I just found out about PatternSynonyms.
Additionally, `liftIO` only works for `IO`.
Sure. I'm just saying that renaming the tool is not impossible if they were to decide that they want that. You just do it and people will get used to the new name because they will want new features. It's not like we're talking about a mission critical piece of software that's been in production for years in systems no one understands.
Reading the subject, I envisioned a p2p social network configured by compiling Haskell (like xmonad), and was intrigued. Disappointed on reading the fuller post, though I'm not sure I have any right to be. I certainly wish y'all the best of luck :)
Wasn't this issue closed some time ago? Hasklig is also nice, thanks.
I also had a similar experience to you once I felt I understood the basics. What I found useful was picking a library or GHC extension and trying to understand the problem it was trying to solve and what possibilities it opened up for me. I would recommend reading 24 days of Hackage and 24 days of GHC Extensions: https://ocharles.org.uk/blog/
^ this This is #1 reason why I turn off most subreddit styles. Not everyone has a massive screen which can accommodate for extra vertical spacing whilst still displaying a good number of links. I get 60% extra links by turning the style off... 
Here are some links: * http://dev.stephendiehl.com/hask - What I Wish I Knew When Learning Haskell * http://www.haskellforall.com - Gabriel Gonzalez's blog (the author of pipes) * http://bartoszmilewski.com - Bartosz Milewski's blog. Contains a big section about [category theory](http://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface) if you're interested in that. * https://www.fpcomplete.com/school - FP Complete School of Haskell edit: &gt;I was also wondering if there are any noob friendly Haskell open source projects. Well you can start by sending fixes or upgrades to whatever project you feel needs them, worst thing that can happen is that they won't merge your changes. You can also try releasing your own packages that extend existing libraries.
Good point.
&gt; I was also wondering if there are any noob friendly Haskell open source projects. Sure! I'm the maintainer for [Hawk](https://github.com/gelisam/hawk#readme), a command-line tool for manipulating text via Haskell expressions. As my [CONTRIBUTING.md](https://github.com/gelisam/hawk/blob/master/CONTRIBUTING.md) file explains, I have tagged some of the issues with "easy", "medium" and "hard", specifically to allow contributors of different levels to contribute. Here are the definitions I give for the three difficulty tags: &gt; "Easy" means you should be able to find a solution just by finding the appropriate place in the code (presumably with our help) and making a small local change. &gt; &gt; "Medium" issues will require you to link a few pieces together, either by getting familiar with an outside library, an unusual Haskell feature, or some other part of Hawk's codebase. &gt; &gt; "Hard" issues may require the code to be refactored in order to create a place where the new feature can be added. Ideally, the refactoring and the addition of the feature should be in separate commits. So if you want to contribute to our project in a way which will make you learn new things, you should probably take a look at the [medium](https://github.com/gelisam/hawk/issues?q=is%3Aopen+is%3Aissue+label%3Amedium) issues. There are only two right now, but that's mostly because the contributions I receive are always about new issues contributors have encountered themselves, never about fixing existing bugs, so I haven't been super motivated to properly tag the existing bugs.
What I noticed about my own learning curve is that the actual advances came after time spent practicing, and not necessarily gobbling up facts (read: language extensions or libraries). Unfortunately, learning this language requires doing quite a fair bit of reading, just to understand what even the tutorials are talking about. You have to understand the actual cases when a repetitive piece of algorithm can be abstracted out, before being able to tell which library could be dropped in as a shortcut. I don't have any quick advice on noob-friendly projects to collaborate on, though
You can have a look at https://github.com/jeffreyrosenbluth/static-canvas/blob/master/src/Graphics/Static/Interpreter.hs#L44 it's pretty simple. 
Why not make something? Make a simple game (lens!), or a REST API for some weird niche (aeson!), or a CLI tool for processing .. I dunno, shp/shx files (pipes?).
This might help with that: http://issuestats.com/?language=Haskell Sorting by the different columns will give you some information about how popular or active a project is. As for complexity, my advice, which has served me well in the past, is to pick a project that you find interesting enough to put the time in to figure out. Then fix a bug or add a feature whichever way you can, and usually the projects will either be happy to take your code or to tell you how you can improve it to make it acceptable. (The latter obviously being the more preferred outcome, since this is the kind of feedback you should be looking for.) Point is, choose based more on personal interest than "newbie friendliness". Interest will carry you over whatever tough spots you may come across.
Write one about using `PatternSynonyms` to make "Datatypes a la Carte" less intrusive!
Thanks for the links :) I'll read through them.
The two most important things at your stage are: 1. Work on a more substantial and longer term project that you have strong intrinsic motivation for. 2. Work with other people who have more Haskell experience than you. Learning to program in a new language well is a very deep endeavor. Because of this, it can be quite difficult to stick with it, so point #1 is really important. Secondly, no matter how smart you are, there are guaranteed to be areas of the domain that you just haven't thought of. That is why it's crucial to work with other people so you can cover more of the search space by virtue of their experience.
Yesod takes about 15 seconds per reload even if you only modified a template, honestly I think haskell is not suited for agile development.
Yes, exactly. Which is why I wanted to contribute to an open source project - for the more experienced devs who can help me learn. Plus, I enjoy programming so win-win :)
Also, does Haskell have the Some(x) and None like in SML and OCAML? I tried Googling and the closest I could find was "Just" and "Nothing" Is that the same? It doesn't seem to be "built in" in that it needs to be defined as a datatype first.
I wrote this a while back to help us make the examples in our diagrams library have more interesting colors. Check out the haddocks for example for http://hackage.haskell.org/package/palette-0.1.0.2/docs/Data-Colour-Palette-BrewerSet.html
&gt; Like, I don't even know what, say &gt; &gt; forall a. a -&gt; a &gt; &gt; means That's quite basic and important. I would advise to learn how to decipher these kinds of type signatures before trying to learn more complicated things like lens and category theory. The keyword to search for is "parametric polymorphism".
Lens and pipes would probably be good. I'd say when I was around that point I started very closely looking at the api for both, as well as `hslogger` and the implementation of `reflection` and `recursion-schemes`. Look at other people's code and write your own.
It seems you want to pattern match or reflect on terms of the free construction. Perhaps you can use an alternative to Free in the line of [reflection without remorse](http://wwwhome.cs.utwente.nl/~jankuper/fp-dag/pref.pdf) or [smart views on datatypes](http://www.fceia.unr.edu.ar/~mauro/pubs/smartviews/smartviews.pdf) which are meant to solve this problem you find with the Church encoding.
I know what it means as a standalone. But, I don't know how to interpret it when it is part of a function parameter to _another function_ `f :: forall a. a -&gt; a` Just introduces a type variable a. What trips me is something like `g :: a -&gt; b -&gt; (forall c. c -&gt; c)` So now, what is the `forall` inside the brackets? Why can't it be "pulled out" ?
While I'm not an expert on Yesod, I'd guess that there is some recompiling going on. Even if you just modify templates, since they are also compiled. I can relate to your impression, that this feels slow and "unproductive" if you come from a more dynamic language. Rest assured though, that this comes with benefits. * The type system gives you more guarantees than most other languages, meaning you will spend less time debugging. In my experience, you'll be much faster overall if you care about correctness and/or security of your code. * You get a native binary that runs much faster than what implementations of languages like Perl, Python, Ruby, JavaScript, etc. are able to provide. Also once compiled Yesod typically runs crazy fast, even compared to other statically compiled languages.
&gt; Libraries like lens and pipes and all that? Yes! Those are what you'll use to create a real application so much faster later on, so well worth learning. Keep in mind a lot of the things you might traditionally think of as almost "language features" in other languages are distributed as third-party libraries in Haskell.
Sorry, coudl you elaborate on &gt; One of the neat things about Haskell is that very little is actually built into it How is that a neat thing exactly? Genuinely curious. For example, I like coding in Python a lot because there's a lot built into it, whereas I don't really like C as it feels like (for school assignments at least) I need to implement a lot of things from the ground up
Yes, yesod devel is too slow. There's a gsoc project to rewrite it using ide-backend, which should speed things up considerably.
That is not so surprising, considering that one of Go's design goals was fast compilation :)
I think they meant that [`evalScript`](https://github.com/jeffreyrosenbluth/static-canvas/blob/7fac1cc0c6aca96dcdfe08da57f609b0f2f3aaa4/src/Graphics/Static/Interpreter.hs#L30-31) converts `F` into `Free` so that `eval` doesn't use the Church encoding.
Wait so its power comes from the fact that it leaves you free to implement it yourself or you can just use it from the library? Why not just overload a function (not sure if I'm using the term correctly) as in Python? Sorry, could you elaborate further? I'm still not really seeing how it's necessarily a good thing. Edit: Perhaps I'm confusing the difference between a standard library and having something built into the language itself....
Haskell as-a-language is pretty small and simple. The standard libraries are pretty nice, and Hackage has a quite good library selection, so there's a lot of code you can bring in. A lot of what seems built in to the language is actually just regular old function application with functions that anyone can define in Haskell. Syntax like `Alternative` and the lens operators comes to mind. 
I'm not sure where overloading comes into play here. The nice thing is that the language is powerful enough that you can define things like this, which is why it doesn't need to be builtin to the language. The point isn't that specifically that you can define `Maybe` or any of the other standard library things, but that Haskell allows you to define other more powerful things that would need to be builtin in many other languages. For one example of this, take the Swift language. The equivalent of `Maybe` (optional) is actually built into the language. It also has builtin language constructs for dealing with optionals (`if let`, `?` and `!`). The Haskell standard library has a set of constructs that are used, among other things, to deal with `Maybe`s (`Functor`, `Applicative` and `Monad`). They are extensible though, so they are more general than the Swift approach. In fact, even in the standard library, there is support for using those constructs with things other than `Maybe`s as well as the possibility for the programmer to add support for new types. In this particular case, it allows us to easily write general code that works for optionals but that can also work for other sorts of types. This also means that `Maybe`s are first class values that can be manipulated the same as any other algebraic datatype value: those constructs I mentioned above are implemented as ordinary functions.
Does your Go framework compile and type-check your templates? (Not saying that should take particularly long, just curious if it is that advanced.)
I migrated to SPA (single page apps) where html and javascript are static files and completely eliminated yesod templates. Although this was not the reason i moved away from yesod templates it definitely helped me with recompile time of yesod devel. 
As a yesod user, i can assure you that once you have around 10 templates the compile cycle becomes unbearable. I completely eliminated all yesod templates from my projects. 
Thank you, RES, you can disable those jumping and animations
It is best understood as a game. Your adversary can call your function g with any type a and b. You are responsible passing back a function that can be called with any type c and must basically give that thing back. A more interesting situation arises when the forall thingy is an argument. Then g has the right to choose the c type.
hackage-server welcomes contributors, as does cabal! There has been ongoing work to try to distinguish "easy" from "hard" issues. There's a lot of code to get up and running on, but there certainly are some easier issues to tackle, and it gives you experience in some architectural choices of large-scale haskell programs. Also, the #hackage channel is a good place to ask questions about development there, as is the cabal-devel list.
With GHCi, Yesod development is fast. It's yesod-devel that's currently a sluggish dev cycle. I recorded a video (just for you!) demonstrating using Yesod with GHCi for haskell.org's codebase: https://www.youtube.com/watch?v=1akOBGihKNs I used this same technique on stackage.org, fpcomplete.com, all my other sites. See [this file in the scaffold](https://github.com/yesodweb/yesod-scaffold/blob/simple/app/DevelMain.hs), which you should have in your own. Also /u/eegreg is a user of this too, last I checked.
*endocodependencies
&gt; how did GHC being package-based complicate things? It is the interrelationship of separate-module compilation with pervasive inlining and static typing that is tricky. Especially in the presence of a very dependency-rich ecosystem.
It definitely sounds like the threaded flag isn't getting passed properly.
Which it failed at. The go compiler is much slower than ocaml's compiler, despite ocaml being a significantly more advanced language.
Thank you! Though that seems to be a type error - Could not deduce (r ~ F f a) from the context (Functor f) bound by the type signature for matchF :: Functor f =&gt; (a -&gt; r) -&gt; (f (F f a) -&gt; r) -&gt; F f a -&gt; r at src/Types.hs:185:11-67 ‘r’ is a rigid type variable bound by the type signature for matchF :: Functor f =&gt; (a -&gt; r) -&gt; (f (F f a) -&gt; r) -&gt; F f a -&gt; r at src/Types.hs:185:11 Expected type: r -&gt; F f a Actual type: F f a -&gt; r Relevant bindings include f :: F f a (bound at src/Types.hs:186:28) go :: F f a -&gt; r (bound at src/Types.hs:186:25) fr :: f (F f a) -&gt; r (bound at src/Types.hs:186:11) ar :: a -&gt; r (bound at src/Types.hs:186:8) matchF :: (a -&gt; r) -&gt; (f (F f a) -&gt; r) -&gt; F f a -&gt; r (bound at src/Types.hs:186:1) In the first argument of ‘fmap’, namely ‘go’ In the second argument of ‘(.)’, namely ‘fmap go’ 
Dev mode in snap still gets unreasonably slow by the time your app is anything beyond trivial. There's just not really any way to work around the fact that GHC is incredibly slow.
Looks nice I'll give it a look
Yes, but as I mentioned above, Snap is in a much better position to do well here because you don't have to compile templates with GHC.
is there a write up or anything on how to use yesod devel with non scaffolded projects? I'm talking about the minimal yesod startup code like [helloworld](http://www.yesodweb.com/book/basics) example. 
Thanks! I'll have a look. Our scaffolding is *very* old, but we do try hard to keep Yesod itself quite fresh. We just finished a painful few days of work to get past the `system-filepath` barrier, company-wide.
Actually, the work of /u/chrisdoner with GHCi shows that this is not an inherent limitation of compile-time type-safe templates, but rather just an implementation detail of `devel` mode. /u/snoyberg posted elsewhere in this thread that he looking into fixing it.
Even before Stack gains more traction, Google will soon learn that you really do already know about the stack data structure, and that your are currently not interested in changing your Haskell web stack.
Yeah, I think the issue you're having isn't so much with the name as with the lack of material on the net. There's pretty much just the GitHub wiki and a smattering of blog posts. I notice that some people are using the `haskell-stack` tag on StackOverflow, which sounds like a good idea. http://stackoverflow.com/?tags=haskell-stack
No, but the goals of the new yesod devel include making it work for any Haskell web project, not just a Yesod (or even Warp) application, so we'll necessarily be giving instructions on how to do so.
I'm not really following what is happening with `matches`. It refers to `exactMatches xs y` but that doesn't appear to be defined? For `countColors`... First we want to find an element in a list. I did a [hoogle](https://www.haskell.org/hoogle/?hoogle=a+-%3E+%5Ba%5D+-%3E+Maybe+a) search for `a -&gt; [a] -&gt; Maybe a`, and the top result took me to [`Data.List`](https://hackage.haskell.org/package/base-4.8.0.0/docs/Data-List.html#v:elemIndex), which has all the list functions. `elemIndex` looks like we can use it to figure out how to determine the count of colors, but `elemIndices` (right below it) looks like precisely what you need. It returns a list of ints where each int is an index where the element occurred. The function `countColors` takes a `Code` which is a `[Peg]`. We have a term `colors :: [Peg]` which has all of the colors in order. And we have the following pieces of the puzzle: colors :: [Peg] elemIndices :: (Ord a) =&gt; a -&gt; [a] -&gt; [Int] countColors :: [Peg] -&gt; [Int] If we have a list of indices that an element occurs at in an array, then the size of that array is how many times the element occurs. `length (elemIndices Red xs)` will tell you how many times `Red` occurs in `xs`. Since we're trying to transform a list of one type into a different list, we probably want to use `map`. And since the `Code` passed as a parameter to `countColors` won't be a complete list of pegs, we'll start by mapping over the `colors` list: countColors pegs = map f colors where f color = ... And you should be able to figure out the rest out for that function. Now, for `matches`, you're given two lists. Usually, when you've got two lists, you want to zip them together, so that's what I'll start off with: matches xs ys = undefined where zipped :: [(Peg, Peg)] zipped = zip xs ys So we have a list of pairs, and we want to identify which ones are equivalent. Haskell has the `uncurry` function, which is useful for applying a function that takes two arguments to a function that takes a single pair of elements: add x y = x + y uncurry add (3, 5) == 8 So we can map over a list of pairs with `map (uncurry f) listOfPairs`. Now, we want to check to see if the pairs are equal. So `map` is just `map (uncurry (==)) zipped`, and we can put that in: zipped = zip xs ys pairsEqual :: [(Peg, Peg)] -&gt; [Bool] pairsEqual = map (uncurry (==)) pegsEqual = pairsEqual zipped So how many of the pegs were equal? We can filter out all the false ones with `filter id` (`id` is a function `a -&gt; a` which always returns its argument), and then we can just take the length of that list. onlyTrue = filter id numberOfTrue = length (onlyTrue (pegsEqual)) Wrapping that all up and omitting some of the intermediate steps, you've got: matches xs ys = length (filter id (map (uncurry (==)) (zip xs ys))) But I want to clean that up a bit! So `Data.List` provides `zipWith :: (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c]`. `zipWith` lets you specify a function to use to zip two lists together. You could write `zip` as: zip xs = zipWith (\a b -&gt; (a, b) ) xs We can replace the `map f (zip xs ys)` with a single `zipWith f xs ys`. matches xs ys = length (filter id (zipWith (==) xs ys)) Much nicer :) All of those parentheses make me feel like I'm in a lisp or something, so if you're feeling ambitious, you could remove them and use `$` instead: matches xs ys = length $ filter id $ zipWith (==) xs ys When you have a pattern like `f x = h $ g x`, you can change the `$` to a `.` and remove the variable name. Haskellers *love* doing this, so you can also express the above as: matches xs = length . filter id . zipWith (==) xs A more experienced Haskeller would see `length . filter id` and want to turn that into a single `foldr`. Give that a shot
OMG I'm such a derp I'm sorry. I meant matches in the place of exact matches. If you look at the code it should be updated now. Also thank you so much for taking the time! I really appreciate it. I'll read through it in a bit when I'm free
I agree, it is a good idea. I've [added a comment about it](https://github.com/commercialhaskell/stack/commit/e3a024a27a6e7e47b0a43012442725a8deeedc27) to the README.
Somehow I don't feel like this is a satisfactory solution. Maybe it's because I do also use DDG quite a bit.
Found it! It's the `-O` flag (same happens with `-O2` and `-O3`). Now I need to track it down. Thanks for your help, you saved me a lot of time! :)
This is not about `devel` mode. It's a fundamental fact that reloading compiled templates will always be slower than concatenative templates that just have to read bytes and fill in holes. As I said, I've found that for large apps, GHCI reload time is also unbearable. It might not be visible with smaller apps, but it's still there and in a large enough app the issue is simply unavoidable. EDIT: The project in /u/chrisdoner's video is 2300 lines of code. And because the templates are compiled, that includes the markup. The app in my video is 30,000 lines of code *not including* the templates. And I can tell you that GHCI is nowhere near that responsive on an app of that size.
Now *that* would be a good name.
The name of the project should be distinctive and the name of the executable should be convenient. For example, the `stk` name for the `stackage` executable was very very nice. And the name of the project could be `haskell-stack`, `hastack`, whatever. I see nothing wrong with `haskell stack` with or without the hyphen and it is natural for people to write and Google will work out the associations. Now, for Hackage, the name `stack` can remain unchanged but the docs should use a qualified name with `haskell`.
It might be fast for small sites, but you're still going to see significant slowdowns as projects get larger. The example in your video is 2300 lines of code. That is not at all a "substantial codebase" in the grand scheme of things. I'd like to see what that reload cycle is like on a 30,000 line app.
Who exactly will be helped by this? When you say "the project," what does that mean? You mean the stack repository name should be changed to haskell-stack, because the `commercialhaskell` organization it belongs to isn't sufficient to indicate "Haskell?" Or if it's about search results, how will using "haskell-stack" throughout the docs force people in blog posts and such to use "haskell-stack", and how will it convince people doing a search "oh, I better make sure I search for haskell-stack, even though the executable is called stack?" As I see it: the project is "the Haskell tool stack." The repo name is stack. The package name is stack. The executable name is stack. And it's a Haskell tool.
No, `liftCallCC'` in my library is what `mtl` could have used to implement some of their instances for `callCC`. Instead they write the instances by hand (see here: https://hackage.haskell.org/package/mtl-2.0.1.0/docs/src/Control-Monad-Cont-Class.html)
Thanks, fixed!
I'm not sure if it'll help in this situation, but I just want to give a +1 for ghcid being awesome.
The mix of "color" and "colour" in `Data.Colour.Palette.ColorSet` is killing me.
But you have to implement [`LiftCallCC`](https://hackage.haskell.org/package/transformers-lift-0.1.0.0/docs/src/Control-Monad-Trans-Lift-CallCC.html#LiftCallCC) You do provide `defaultLiftCallCC`, but the instances are quite trivial to write by hand (unfortunately ghc-mod doesn't have any proof-search integration - djinn - to fill the terms, given the types). I still don't get it. Maybe tomorrow (that happens to me).
I've gotten quite far without really understanding RankNTypes. htebalaka's answer above was super helpful though. @Bollu, if you're into web dev http://snowdrift.coop/ is written in haskell (with Yesod), actively looking for volunteers, and explicitly trying to be noob friendly. Edit: Oh, I see @wolftune replied below and you already talked to them, nice:)
Any advice on game making in Haskell? (graphics-wise)
To be fair the idea of Colemak is that regular words *should* roll of the fingers. ;)
Please check out "Intermediate Haskell Documentation" https://github.com/commercialhaskell/haskelldocumentation It's more like a list of topics than content right now, but it might be you'll find this list nice, and maybe even contribute to it.
&gt; For example, the `stk` name for the `stackage` executable was very very nice. Haha, it just hit me how nicely `stkg` would rhyme with `dpkg` on Debian-based systems. This is *not* a serious suggestion. It's just the kind of cute pun that I assume we're not going for.
the docs are very good (an enum of images!), but I wanted to point out that documenting "synonym for Colour Double" above "type Kolor = Colour Double" is annoyingly redundant. if it's to maintain coverage, I'd just explain why this type is common enough to deserve an alias (why Doubles over Ints or something).
I suppose only search engines will be helped by it. Having the phrase "haskell stack" appear frequently in plain text documentation will give it more weight and make it distinct from "stack overflow" and others. That's it. I am not suggesting github repo name change, nor hackage name change, nor executable name change. For people it's distinguishable enough from the context.
I always recommend starting with [gloss](http://hackage.haskell.org/package/gloss) because it's easy. It's limited (you can't even center text...), so you'll eventually outgrow it, but for sprite-based 2D games such as [mine](https://github.com/gelisam/ludum-dare-31#readme), it's quite sufficient.
I'm assuming in the same cases where spoon breaks pure code. I didn't look to much into the actual mechanism that catches the exceptions. Just extending it to functions.
&gt; However, if you use `ghc -O main`, it won't write out anything, and you have to kill the process through Task Manager. Try using [`yield`](https://hackage.haskell.org/package/base-4.8.0.0/docs/Control-Concurrent.html#v:yield) instead of `return ()`. As the documentation for [`forkIO`](https://hackage.haskell.org/package/base-4.8.0.0/docs/Control-Concurrent.html#v:forkIO) explains, this forks a lightweight thread, not a system thread, so I guess you need some kind of collaboration between threads in order to ensure that they all make progress. I think the yield usually happens when the stack gets checked for overflow or something like that, which happens at every function call, but that your `runForever $ return ()` has been optimized to a busy loop which does not call anything and therefore never checks the stack nor ever calls yield.
If it ain't broke, don't fix it! The old theme was just fine.
Don't leave us Workman people out!
&gt; As I said, I've found that for large apps, GHCI reload time is also unbearable. For this see [Making GHCi scale better and faster](http://chrisdone.com/posts/making-ghci-fast). &gt; The project in /u/chrisdoner[1] 's video is 2300 lines of code. It's also a recognizable open source project that I am allowed to demonstrate. &gt; And because the templates are compiled, that includes the markup. The app in my video is 30,000 lines of code not including the templates. And I can tell you that GHCI is nowhere near that responsive on an app of that size. The fpcomplete.com codebase is 25K~ of Haskell files (excluding templates) and I can reload any module in it in a second (which includes loading hamlet dependencies). I develop all my codebases like this (loading the whole thing in GHCi and using :load to type check modules I'm working on). &gt; It's a fundamental fact that reloading compiled templates will always be slower than concatenative templates that just have to read bytes and fill in holes. The question is how slow reloading a compiled template is on a real industry-sized project, and the fact is that it's not a problem (under or at about a second) with the right configuration. (Actually, part of the delay is that I use ghci-ng, which generates a complete database of all types of all spans in your code every time you load a module; it's much faster on regular GHCi).
Oh, sorry, I didn't mean to say that at all. I wasn't trying to say that that contrasts with OCaml, just that there are some things you can implement in the standard library (rather than as a language feature) in Haskell that you can't in some other languages, not specifically OCaml. I wasn't really trying to say anything about OCaml at all.
Go (Golang) had/has the same issue with its name, in that when you search for Go, it took a very long time for it to appear anywhere near the top of the page. With time, content, issues, questions, etc it started to rise. Also as a minor point, when people would refer to Go in publications, there was always a suggestion to use the term Golang, as some of the suggestions have pointed out, when searching use Haskell Stack. It will sort it self out, Go's users were new once, and the language survived; Stack is new, and it will (hopefully) survive too :) Personally I like the name.
Thanks for the good read. 
This might be of interest: https://stackoverflow.com/questions/14710928/is-spoon-unsafe-in-haskell
Okay, and the two Workman people! And that strange guy in the corner who uses Norman!
oh, my mistake! `matches` is probably best approached by breaking the problem down into a smaller question: given a list and an element, how many times does that element appear in the list? countMatchesIn :: (Eq a) =&gt; [a] -&gt; a -&gt; Int Once you have that, you can do: map (\x -&gt; countMatchesIn ys x) xs Which gives you a list of how many times each element in xs is in ys. Then you just have to add them up ~
What IDE? You need a way to send "DevelMain.update" to the ghci context that has your project loaded.
This is great, well done! Looking forward to try it myself and for your next post. Front end dev with Haskell is still a rough territory and posts like this will make it a reality
Maybe yesod init's scaffolded project supporting this by default would be a good idea. Actually I'm curious, do you use this for other stuff at all? I've always been interested in being able to patch changes to ghci for more interactive development like this.
I'm going to have to checkout ocaml sooner than I thought if this is true.
That is great news! Do you know if JavaScript works in it? I have been using the mingw64 WebKitGTK+ package from Fedora 22 and I have not been able to get JavaScript to work in it.
This is very nice. So what is right adjoint to `Flapp b`?
Wow, this is great. I didn't realize you could do so much with so little code with GHCJS and React-Haskell. I've been incredibly hesitant about using Haskell on the frontend, but I'll definitely give it a try at some point after seeing this.
With the `DataKinds` extension you can pass a list of types as a parameter. See [vinyl](https://hackage.haskell.org/package/vinyl) for example.
I'd recommend being flexible with the model/update/view pattern. Although it can be a useful model, there are often better models that you can build depending on your specific data model. It's hard to say definitively without taking a look at your code, but it seems reasonable in your situation to keep some of that state local to the individual UI elements, update it in-place, and then collect it up into a model later. In my experience, FRP is great at expressing a wide variety of data flow models, so there's no need to try to fit everything into a predetermined flow like MVC/MVVM/etc. - just follow your appplication's needs. One last thing: reflex-dom will not rebuild elements of your list unless their keys (presumably, in this case, list indexes) have changed. If you'd like to talk in more depth about your specific situation, feel free to hit me up on #reflex-frp on irc.freenode.net, or send me a PM here on reddit.
Haskell tooling is a wasteland. A lot of people swear by Atom + plugins, or highly customized Vim environments. Good luck! 
&gt; data type that can take any number of type parameters &gt; type constructor can take any number of values There is two questions, depending on what exactly you need: * **data** constructor taking any number of **value** parameters; * **type** constructor taking any number of **type** parameters; The first one can be easily modeled in Haskell. Try looking into e.g. this [link](https://wiki.haskell.org/Varargs). The latter one involves some more sophisticated approaches, but I think you don't need it for your task.
I think your question is worded a bit poorly. Haskell doesn't really have dependent types, so you can't have a type constructor which accepts values as arguments. I am assuming you mean something else. If you are asking about *type constructors* which can take any number of *types* as arguments, then mjmrotek's answer is probably going to point you in the right direction. If you are asking about *data constructors* which can take any number of *values* as arguments, you could look at the way [Text.Printf](https://hackage.haskell.org/package/base-4.8.0.0/docs/Text-Printf.html) handles overloading, or you could use type families. Either way, you will need a smart constructor for your vectors.
I just found the video of that presentation on youtube: https://www.youtube.com/watch?v=TP0ApVPLc24
stack does all the sandboxing automatically so you don't need to worry about it at any point. as long as you are working on separate projects things will not get into their way. It doesn't use the standard cabal sandboxes for this afaik so you can't mix them.
I use [spacemacs](https://github.com/syl20bnr/spacemacs) and haskell-mode works after adding it to your layers in your .spacemacs file.
Does this mean Stack has its own sandbox implementation?
Awesome! I'll definitely have to watch it tomorrow.
I want to mention that a few libraries are easier to install on unix-like systems
I'm using atom
Yeah, I once saw someone asking "What do the separators . and $ mean?" "separators"??!!
Yes. This is called an "n + k" patterns. It is deprecated. The idea behind it was that it matched the way induction is done over the natural numbers. (`data Nat = Z | S Nat`). Writing functions in this way is, in principle, good practice. Something like `factorial` becomes: factorial :: Nat -&gt; Nat factorial Z = 1 factorial (S n) = (S n) * factorial n Notably, you do not need to perform subtraction on the argument, and so it's more obvious that the function is structurally recursive. The function can't get stuck in an infinite loop for the simple reason that each recursive call strips off an `S` constructor, and each `Nat` can only have so many before they run out. That said, they were removed from Haskell (or deprecated?) because they are essentially a hack to make it *look* like that's what's going on. It is the only time, as far as I know, where a (non-constructor) function can appear in a pattern. They also fail to serve their purpose related to "obvious termination", as the above definition will still get into an infinite loop when you call `fac (-1)`.
Won't `product [1..n]` lead to a space leak?
I admit to feeling like I'm in a minority group for using haskell on windows. 
I started with a very involved vim setup, then moved to a very involved emacs setup, and I moved to spacemacs the other day. It's fantastic. Great documentation, very sensible defaults, a nice help system. I already had the various tools installed with cabal, so adding haskell support involved: - pressing SPC f e d - adding "haskell" to the list of layers - pressing SPC f e R I've since added a variable to customize hindent, but that's about it. I do Scala at work, that layer worked out of the box with something similar, as did git (magit is great), org (also great) and markdown. Now I just need to write a layer for Proof General and company-coq...
I'm definitely looking forward to use that with [servant](http://haskell-servant.github.io/) too.
ping /u/joelburget
But is it not the future.
This is wrong in so many levels. Please, use Vim instead. http://yannesposito.com/Scratch/en/blog/Learn-Vim-Progressively/ http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/
`:set -fobject-code` helps a lot! I wonder why haskell-mode does not set it by default.
I've used it for running xmonad in ghci. I tried to use it with SDL/GLUT, but neither are able to run in GHCi anymore on my Linux. Weird linker errors beyond my motivation to diagnose. Otherwise we could have live update of renderers on the fly; just reload the code and update an IORef to the rendering function.
It's just `stack test`. It will reconfigure the project, install testing deps and run the thing.
There's a hackage bug which prevents me from uploading my version here: https://github.com/chrisdone/ghci-ng The current one on Hackage is rather old and doesn't support this type info. It was maybe-gonna be merged into GHCi, but I'm more likely to redouble my efforts on [ide-backend](https://github.com/fpco/ide-backend).
Due to this caveat: &gt; After that, you may notice that loading some modules gives less type information and general metadata than before.
&gt; highly customized Vim environments. tbh, I don't really use haskell specific plugins, the only thing I use for haskell is syntastic (not haskell specific error/warning highlighting), which uses hdevtools to do on the fly checking of compilation.
&gt; It is deprecated. No, it is not: Prelude&gt; :set -XDatatypeContexts &lt;no location info&gt;: Warning: -XDatatypeContexts is deprecated: It was widely considered a misfeature, and has been removed from the Haskell language Prelude&gt; :set -XNPlusKPatterns -- everything is fine
Can the mods put links to earlier discussions of this frequently asked question in the sidebar please?
Atom + plugins is pretty easy to set up. That's my preferred way of doing things as someone who isn't hardcore enough for vim/emacs (yet) :P
You can express foldl in terms of foldr. 
No, why would it?
Huh. I get the same thing, but [sources](https://www.fpcomplete.com/user/PthariensFlame/guide-to-ghc-extensions/pattern-and-guard-extensions#npluskpatterns), along with other comments, seem to indicate that they really are deprecated. A "bug" in GHC's warnings, perhaps?
But... but.. That's the perfect name! Why can't we have nice and fun things? :(
1. Yes. 2. I start new/continue on old C project, where I bang out a ton of code without much thinking, enjoy fast compilation times and curse crashes/hangs/memory corruption. Few days after I get fed up with this and return back to Haskell with love levels at 11. 3. Compiling is slower but debugging is nearly non existent. They cancel each other. 4. See #2. Also SSDs, few gigs of RAM and fast CPU. 
Still it uses almost 100% CPU after using leksah for some time. After some compilation (no background build active). That also happened with the previous version. It slow down everithing. It is not the leksah-server but the leksah IDE. I use Windows ghc 7.8.3
For anything which can be reflected in types: no, it has near instantaneous iteration speeds. If you can take advantage of this well then it will change everything.
Highschool nickname as a reference to Dr. Zoidberg.
1. My Haskell iteration cycles are faster than my Python iteration cycles, at least. I don't know whether Python counts as fast or slow. Compilation sometimes takes longer with Haskell (obviously, since Python barely gets compiled), but run times and test times are so much quicker. A full, optimised build is significantly slower, but I generally don't do those when I'm working. 2. I work on parts of applications in isolation, either as executables or in the REPL. Since I design my application as just a thin wrapper around "library style" code, I can make many of those thin wrappers for different parts of my application as I'm writing it. After a few days of working on an application I might have made 15 different executables, each one for a very specific view in my application. I throw them away when I'm done, but they're useful for testing things in isolation. 3. Yes, and it's not even a linear relationship with the non-interactive parts. A non-interactive cycle that takes twice as long might make me lose four times the productivity, because when I know it's going to take a while I tab over to check my email or whatever while it does its thing. Bringing down the waiting time is essential for me. 4. Isn't this just question #2 with different words?
I use emacs with this .emacs.d file as my starting config because it includes everything I need for Haskell (and Clojure and Ruby and Python and Javascript), and I want to write as little emacs Lisp as possible: https://github.com/purcell/emacs.d
&gt; Do you have slow iteration (compile+test+run) times? The compile times are OK, especially when compared to Scala. Dear Christ, Scala... my gut tenses up every time I think about it. It used to take 30 seconds on a Linux machine to compile a single, 5-module package - with the zinc server and everything. The experience's frustrating nature was only compounded by the atrocious type-checker's proclivity to spit out spurious errors. Debugging is another issue. ghci's CLI is crude an inconvenient when used for interactive debugging. IDE-style graphical inspection and breakpoints in the code would be a significant productivity boost. &gt; How do you deal with that? I write many modules and many small functions that are individually testable. &gt; Do you find it negatively affects your productivity when the compile+test+execute cycle take longer? The unavailability of graphical debugging does, but its manageable. &gt; What are your solutions? Testing functions in the REPL.
Stack is Stack sTack stAck staCk stacK.
I'm really surprised by how few people seem to work with GHCi based on responses to this post.
Also, can I get you to clarify to part where you talked about reflex-dom not rebuilding the elements of the list? If I `sample` a `Behavior` and then use `forM`/`mapM` to iterate over the results and build the DOM from that, I can't see how reflex-dom could be smart enough to caching, even if we're only considering the situation of updating an item at a particular index. If you have `[Behavior t a]`, then it seems possible, but with `Behavior t [a]`, it seems like caching couldn't be happening. Were you describing the former situation, or does your statement apply to both?
I think you got it all backwards... :)
Since the situation changes so fast the information of discussion tend to go off the expiry date really quick.
That's the default implementation for Foldable, not the actual implementation for lists. The [actual one](https://hackage.haskell.org/package/base-4.8.0.0/docs/src/GHC-List.html) is in terms of `foldl`, which is in turn implemented via `foldr`, to enable `foldr/build` fusion.
Good question! Honestly, I'm a bit unsure. I do not know what the exact model is of reflex, or if there is something you can do with this and not with reflex (or vice versa). Does anyone know what (if any) the denotational model of reflex is (Ryan Trinkle mentions a denotational model in one of his talks, but I do not know if it is done yet)? This would make a comparison easier... Currently the only thing I can say is that the interface is different :) Reflex makes a distinction between push-based and pull-based computations in the interface, whereas with frpnow this is left to the implementation (currently pull-based with sharing, but a push-based implementation should also be possible). It is not possible to be notified when a behavior changes in reflex, this is possible in frpnow. 
The [Haskell Wikibook](https://en.wikibooks.org/wiki/Haskell) already starts from absolute zero, no programming experience expected.
I have learned vim and used it for 8+ years. What's wrong with spacemacs?
Not really saying anything new here, but...GHCi. It's great. It takes a few sec to start, but after that it's surprisingly fast for what it is and most of the time it just works. My iteration times are on par or perhaps even better than when scripting Lua (:r vs. lua -opts file), which I'd say is pretty damn good. Disclaimer: I've never had any huge projects, so it might be anywhere from a little slower to a lot slower to load something big in GHCi, but either way I'm sure it's a big improvement over an entire recompilation.
It's not deprecated in the same way DatatypeContexts is. Rather, n+k patterns were formerly *standard* Haskell; see [section 3 of the Haskell 98 Report](https://www.haskell.org/onlinereport/exps.html). They're now downgraded to a language extension that must be explicitly turned on. They're informally deprecated in that when you use them people will make comments on Reddit saying that they're old syntax and shouldn't be used.
Amazing, thank you.
Note that the insistence on `Colour` comes from the `colour` library, which is written by someone else. Some libraries expose synonyms for en-uk and en-us sellings. I'm not sure whether that's an improvement or not.
Agreed, but I was just thinking of linking to the most recent discussions on tools, what to do after you learned the basics, etc. 
Great! Also nice that you have made gloss and gtk interop packages. Have not tried them yet, but looking forward to it.
If somebody would be kind enough to contribute a reflex implementation and an FRPNow implementation to the [frp-zoo](http://github.com/gelisam/frp-zoo#readme), we'd be able to answer that question for a large number of pairs of FRP frameworks. There are so many, I haven't been able to keep up and to add either of them to the zoo myself!
Only operators. Otherwise this could be actually usable, not only for documentation purposes.
You may be interested in [`base-prelude`](http://hackage.haskell.org/package/base-prelude), which attempts to export as many things from `base` as possible (without introducing conflicts).
Because I didn't know about it. ;-) Thanks for the pointer!
It's called ExceptT. I think that name was a giant mistake exactly because you're asking about it and obviously are expecting EitherT. But unfortunately that's the situation we're stuck with. I would say that the definitive answer to this question will always be found in the [errors package](http://hackage.haskell.org/package/errors). It just made the switch to ExceptT in errors-2.0. :(
Because it is named `ExceptT` and not `EitherT`. Edit: I would not expect `MaybeT` to be named `PerhapsT` or something other than `MaybeT`, and the same goes for `Either`.
The errors package is not the answer. It may be semanticaly similar, but I mean we should have an `EitherT` in transformers as a companion to `Either`. The name is half the point.
Huh. How is it different from the sieve? I'm new to Haskell. I do notice that in his description of the algorithm, he says that one starts with "an infinite list". :-)
What 'special code' are you talking about? And if, for anybody, build times are big enough of an issue to switch languages I'm sure they'd stumble upon whatever they could be doing to speed up build times after some googling. As for Chris's work (I'm guessing you mean /u/chrisdoner), most of that should be common knowledge, or at least for relatively new Haskellers coming from Real World Haskell/Learn You a Haskell. Whatever the case, with such a radical "fix" (switching languages entirely), whether or not you were joking, that's a lot of work and I'm confident learning a little about GHCi is a better solution, even if it's a bit tedious to get it working smoothly.
There's a concise, pure implementation of the Reflex semantics available here: https://github.com/ryantrinkle/reflex/blob/develop/test/Reflex/Pure.hs While it's not quite a denotational semantics, it's a step in that direction. There's a test suite that verifies that this implementation operates equivalently to the primary implementation ("Spider"). With respect to detecting when Behaviors change, a common pattern in Reflex is to use an Event and a Behavior together - called a Dynamic - to achieve this.
With all these FRP libs in Haskell the Zoo is a necessity. I would not mind if there was also a bigger example in the Zoo, a bit like what TodoMVC has become for JS frameworks.
unfortunately it does not cover how to (use stack) to create, build and test programs and libs, and how to install and configure an editor.
If you get it into Stackage, then stackage.org will build and host the docs for you too.
`(print . toEnum) 0` works.
I think it's partly because the Yesod website still recommends using yesod devel: http://www.yesodweb.com/page/quickstart I understand from /u/snoyberg that work is being done to make yesod devel faster, but in the meantime since reading [this](https://www.reddit.com/r/haskell/comments/3d9i9q/why_is_yesod_so_slow/ct3495s) I've switched to using ghci and DevelMain.hs in my yesod project and my reload times immediately dropped from ~20 seconds to ~2 seconds.
Emacs has the best support for all the editor tools. Vim is currently lacking IDE backend, but I hope to fix that soon 
I don't have a `stack.yaml` file, but I am receiving [bug reports](https://github.com/gelisam/hawk/issues/150) from users who are apparently able to build my code with stack anyway (the bug report is about what happens at runtime afterwards). Does stack use the `.cabal` file if it cannot find a `.yaml` file?
I don't have slow iteration times. I use eclipseFP and compile as I go. I get instant feed back for type errors so I feel like the cycle time is probably faster in Haskell than any other language once you get going. 
stack is only weeks old! and it's a WIKIbook… we can quickly go ahead and add that information to it! Let's do it!
Not everyone is familiar with Linux tooling and the "beauty" of this name is lost on them, let alone how it is "easily pronounceable".
Yes, Stack uses both the .cabal file (for the project information including dependency list and build options) and writes its own configuration to the `.yaml` file (which includes which stackage snapshot to fetch packages from). When you first run Stack on a non-stackized-but-cabalized project it will generate the `.yaml` file based on the `.cabal` file.
Good point. I just have so many of these operators imported manually for GHCi as it is, I don't want to have to import `Control.Monad`, `Control.Applicative`, and `Control.Arrow` every time I start up an interactive session. This would have just made my .ghci file shorter =p
You should turn this into a full-blown blog post...
Thanks, that was very helpful :) 
Stack is a Totally Awesome Cabal-Install Killer
Neat !
Apart from the name I am not very comfortable with the Monoid constraint on `MonadPlus` and `Alternative` for `ExceptT` because `mappend` does not make sense for error types with a single error value.
Literal haskell source and slides at : https://github.com/gwils/next-level-mtl-with-classy-optics
&gt; How hard would it be to add a GLFW-b backend? Since gloss is supported and gloss has a GLFW-b backend, I guess GLFW-b is already indirectly supported?
I agree with your comments. I'm not sure the different FRP libraries are liable to have particularly similar implementations, though; it would tend to expose the libraries' (and authors') concepts of division of view and model, for instance. *Edit*: I didn't mean to suggest that the FRP zoo should use TodoMVC; they're different projects with different goals, as you note. I do like that various FRP and non-FRP projects have been doing TodoMVC implementations.
This might seem like a silly question, but is there a reason that we don't have a `MonadMaybe` and `MonadEither` class in `mtl`? It would be nice if for example we could have `lookup :: (MonadMaybe m) =&gt; k -&gt; Map k v -&gt; m v`. That way you wouldn't need to lift `Maybe a` into a `MaybeT m a`. And similar for things that work with `Either`.
It's on OS X 10.10.x I'll check once I'm home.
Just use ghci and live reloading: http://www.reddit.com/r/haskell/comments/3ddcrw/does_haskell_have_slow_iteration_times_are_those/ct420d5 I was wondering the same thing before this post. 
That's because `MonadMaybe` is just `MonadError ()`, and `MonadEither` *is* `MonadError` (the one from `Control.Monad.Except`). Overall I find the `MaybeT` transformer quite useless because it's equivalent to `ExceptT ()` up to isomorphism but its name is clearer and you also can replace `()` with an actual error type with less refactoring. We could indeed add many more classes to `mtl`, but this would come at a great cost (remember, O(n^2 ) instances, and we can't really have less for fundamental reasons), so if you can emulate one class with another (like `MonadMaybe` with `MonadExcept ()`) -- that's a better solution.
This might be relevant: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/win32-dlls.html
I want to suggest a few renames while we're at it 1. `ReaderT` to `FunctionT` 2. `WriterT` to `TupleT` 3. `StateT` to `FunctionTupleT` Seems to fit the idea of naming monad transformers after their implementation details.
Without sarcasm, I would prefer to see `MaybeT` renamed to `FailureT`.
The use of `Zero` in the talk is not to denote partiality. It's used as the canonical *empty* type, which we can use to talk about whether other (more complicated) types are inhabited. If you have a function of type `A -&gt; Zero`, where `A` is some other type, then that function is evidence that *there is no thing of type `A`*. To see this, suppose there was a thing of type `A`, then we could use that function to get a thing of type `Zero`. But we know that there is no thing of type `Zero`, since we defined `Zero` that way, hence we conclude that there can be no thing of type `A`. Sorry about the delayed response! For some reason, I only just saw your comment.
Thanks :) Does hackage not already do this? Also, I've just realised that this is actually a bit of a lie: if your bower dependencies are not set up properly, when you run the tool to generate the data, you can end up with links not working for types in your dependencies. The tool does warn you when this happens, though.
I guess it does. I could have sworn I checked a package and only internal types were linking properly. I just checked pipes though, and the signatures seem to be properly linked. update: Yep, I'm just crazy. On another note, I do appreciate the more modern styling, so kudos there as well.
[My project](http://hackage.haskell.org/package/fltkhs-0.1.0.2/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html#g:4) is a binding to C++ from Haskell and I have it building on Windows using [CMake](https://github.com/deech/fltkhs/blob/master/CMakeLists.txt) . Not sure if this helps because it's Haskell calling C++ and not the other way around.
Sort of the wrong level of abstraction at that point.
O(n²) when?
Let's think about the operations of `MonadMaybe`. You need `return` for `Just`, and `empty` or `mplus` for `Nothing`. There isn't anything missing. lookup :: MonadPlus m =&gt; k -&gt; Map k v -&gt; m v works just fine. On the other hand, `EitherT` and the like could benefit from having a thing for emitting the left hand side and to handle it. class Monad m =&gt; MonadError e m | m -&gt; e where throwError :: e -&gt; m a catchError :: m a -&gt; (e -&gt; m a) -&gt; m a is precisely that... and the instance already exists. The naming of stuff is goofy, historical, not terribly symmetric or nice, but its all there when you think it through.
Given n different "features" and m different monad transformers we wind up writing up to n instances for how to lift each of the features over each of the m transformers, so that is O(nm) chunks of code. If each transformer is associated with a feature then that is O( n^2 ). We have to write all these data points because occasionally the features can't lift over particular transformers, so the absence of an instance often tells us as much as its presence.
As a sample-size-of-one anecdote, I've been trying to use a minimal todo app to try things out with different FRP libraries. I've been focusing on event-and-behaviour based FRP - so reactive-banana and sodium - and so far the todo use case has lead to interesting intersections of dynamic switching and event handler registration. Normally that part is pretty painful for me relative to everything else in the app - to the point that I'm not sure I could sell it to others. It's entirely possible that I'm just massively misusing those libraries so far. I've got FRPNow in my queue, and am optimistic - it looks like it might have an API that removes that pain point.
It works for you? I'm having compile errors with ghc 7.8.4 and 7.10. Are you using the code from https://github.com/chrisdone/ghci-reload-demo? Are you using specific versions of wai/warp?
That did the trick. Thank you!
Sorry about that! I recently rewrote react-haskell and those examples are the one thing I haven't gone back to update. Will try to fix within a couple days.
Author here. Some updates from the six months since publishing this: * I no longer recommend doing animation work in Haskell. The potential performance problems scare me off. In fact, I've moved most of my layout work to javascript so I can take advantage of tools like [react-hot-loader](https://github.com/gaearon/react-hot-loader) to tweak styles in realtime. The javascript community is much larger than ours and they've built a lot of great tools. I'm trying to take advantage of as many as I can. I now tend to do logic in Haskell and layout in JavaScript. * This is possible to do because react-haskell can now import foreign react classes ([example](https://github.com/joelburget/react-haskell/blob/master/example/foreign/Main.hs#L37-38)). I'm not happy with the API yet - suggestions welcome. * In fact, I've been doing quite a lot of react-haskell work. You now define *real* react classes, which improves performance and debugging (hello react-devtools!). Animations are no longer built in to classes and transitions, though there's nothing stopping you from using them. I gave up on the `React` monad and decided to use `RebindableSyntax`. My stance is now, if it isn't a monad it shouldn't pretend to be one. Additionally the code is much simplified and clarified (much more suited to external contributions, I'd say).
Forgot that I promised another post until I saw this comment. Oops!
I went from Scheme to Haskell, not CL. &gt; Right off the bat I want to know if Haskell has generic setters, structs, multiple return values and good macro system. Basically no. Haskell's functions are all unary, but you can return a tuple. Lenses feel like the Haskell answer to generic setters. Our macro system (template haskell) sucks quite a bit, mostly just due to lack of homoiconicity, but we don't need it for as many things as you might think, seeing as we have lazy evaluation to help us define our own control operators. Monads are also helpful here as the distinction between evaluating a monadic value and executing its effects allows you to introduce the same sort of phase distinction you want with CL macros. We also have some lightweight syntax and overloading features for DSLs that make life a bit easier in that respect.
&gt; Haskell's functions are all unary, but you can return a tuple. What's the difference? I don't think I've ever used a language with 'real' multiple return values.
It's not clear to me how to use setup/teardown helpers with ghci. A link to an example project where this workflow is used would make it much more clear.
You can catch and process multireturns in CL with value bind. Implicitly only first return value matters and it doesn't die if nothing is done to arbirtary amount of following values. For example hashes return value first and t/nil second to indicate it was/wasn't found. Then you can explicitly bind to that boolean and do something like it. Handy in some cases.
When is the object system useful? Prominent OCaml users basically tell you to ignore it. Value restriction makes function composition monomorphize unnecessarily. Modules are very useful when programming in the large. But type classes are very useful in the small. I can't program in the large if I can't program in the small..
&gt; Many Haskellers come from C++ and “stop program, edit file, re-run compiler, re-run whole program” cycles and don’t have much awareness or interest in it. If you are such a person, the above probably won’t come naturally, but try it out. Actually, after learning Haskell and embracing the REPL development style, I started to dread the "C++" style of development. The pace at which I do my iteration feels much slower.
Haskell tends to solve this kind of use case by ensuring via the type system that you can not access the value unless there really is a value. Something like the lookup function on a hash would have a type lookup :: Hash key value -&gt; key -&gt; Maybe value with data Maybe a = Nothing | Just a This way you only get a Nothing which does not contain a value at all if there wasn't one in the hash for the given key. You can use pattern matching to extract the value or even just make the boolean distinction between Just or Nothing if you don't care about the value, just its existence. Of course there are utility predicates for the latter case to turn that into a boolean value.
After reading this paper, I'm wondering about the following: is it true that their API excludes a function like Elm's foldp: foldp :: a -&gt; (a -&gt; b -&gt; a) -&gt; EventStream b -&gt; Behavior a I get the impression that this would be impossible to construct and the closest you could come is foldp' :: a -&gt; (a -&gt; b -&gt; a) -&gt; EventStream b -&gt; Behavior (Behavior a) Can someone tell me if this impression is correct?
I did several years ago, though I'd never written anything serious in CL, just toy projects. I remember at first it was quite annoying (for example that lists can only contain values of the same type, or having to declare data types instead of just using a symbol), but I came to like it. &gt; generic setters Setters in a Lisp sense technically don't even exist in Haskell because of its pure and immutable nature, but you may want to have a look at lenses - [lens-family-core](https://hackage.haskell.org/package/lens-family-core), or the bigger library [lens](https://hackage.haskell.org/package/lens). All in all they can be used like [this](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html), so I think it's not only quite close, but actually more capable. &gt; structs Abstract data types are kind of the most basic thing in Haskell. And you can not only have structs, or product types, but also sum types (multiple variations of the same type), like: data Foo a = Foo {foo :: Int, bar :: a} data Bar = None | Bar Double I know it may not sound like much when coming from a dynamically typed language, but I do think that documenting what forms can a given variable take greatly helps in refactoring or even just understanding the code later. Though Haskell does have an (almost) all-encompassing [Dynamic](https://hackage.haskell.org/package/base-4.8.0.0/docs/Data-Dynamic.html) type in the standard library, if you'd ever need it. &gt; multiple return values Only tuples or composite data types. &gt; good macro system There is Template Haskell, and I think it is just as capable as CL macros, but its not used as often. Haskell's syntax is fairly complicated when compared to s-expressions, and the resulting macros are IMHO quite ugly. Though there are libraries that use TH a lot, like [yesod](https://hackage.haskell.org/package/yesod). Haskell has also a different method of code generation, namely functions that are polymorphic in the return value, or just polymorphic values. Then the type, and consequently the value of an expression may depend on the context of its use. There are also [Generics](https://hackage.haskell.org/package/base-4.8.0.0/docs/GHC-Generics.html) that allow one to break up a data type into simple pieces, that allows to write data type agnostic functions. With libraries like [vinyl](https://hackage.haskell.org/package/vinyl) or [hlist](https://hackage.haskell.org/package/HList) one can recurse over the types to make functions that work over an `Rec` or `HList`, regardless of the actual types. I think that static typing helps a lot in this respect. &gt; defparameter I think the best alternative to dynamically scoped variables is the [reader monad](https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-Reader.html): foo :: Num a =&gt; Reader a a foo = return.(+1) =&lt;&lt; ask bar :: Num a =&gt; Reader a a bar = do a &lt;- foo b &lt;- local (+a) foo return $ a + b main = print $ runReader bar 3 + runReader bar 0 If you want the called functions to be able to modify the value seen by the calling function, then there's the [state monad](https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-State-Strict.html): increment :: Num a =&gt; State a a increment = do a &lt;- get put $ a + 1 return a foo :: Num a =&gt; State a [a] foo = do a &lt;- increment b &lt;- increment c &lt;- increment -- there are functions in the standard library to loop like this, of course d &lt;- get return [a,b,c,d] 
Yes you are correct :) Indeed frpnow has this fold over event streams: foldES :: (a -&gt; b -&gt; a) -&gt; a -&gt; EvStream b -&gt; Behavior (Behavior a) The change from Behavior a to Behavior (Behavior a) is necessary to prevent space leaks: you get the fold since you requested it, not since the start of time :) Elm has limitations (no behaviors of behaviors) to prevent this kind of leak, in this paper we solve the space leak without these limitations.
I have. Multiple return values: Haskell supports patten matching of the return value, so you get something like `destructuring-bind` on the return value, and you can easily ignore the return value you don't like. `let (value, _) = someHashLookup table key` if the second return value is not interesting. Types are very light weight in Haskell. Ignoring parts of the return value is frowned upon in haskell-land, OTOH, lots of the Prelude consists of non-total (i.e. unsafe) functions. The Haskell community is moving towards more use of total functions but it takes a long time to change the Prelude. Lenses are generic setters. Structs are similar. Your defparameter is often the bottom layer in a monad stack, a Reader. This reader gives access to global configuration information. You'd often create some sort of hierarchy in the data structure that represents the set of global configuration information in order to modularize this. 
it is not directly linked in the post but I found [this live reloading demo](https://github.com/chrisdone/ghci-reload-demo) project in [this post](http://chrisdone.com/posts/ghci-reload), which was linked in the post above.
That was a comprehensive reply, thanks. You said you came to like it ultimately? For what reasons? More than CL? How long did the transition process last? Just the thought of lists where only one datatype is allowed makes me shiver lol. 
foreign-store can help a little with that.
I was just yesterday wondering about the opposite. For someone who has written almost exclusively Haskell for the last few years, and doesn't have any plans to stop using the language, would learning Lisp be worthwhile? I'm a little interested in macros (though mostly think Haskell removes the need. Mostly I'm interested in the more powerful REPL features in lisp (the story of debugging spacecrafts after a crash was interesting), though I can't seem to find if that limits me to common lisp, or if another variant would be better to try. Unfortunately it seems like anything being advertised as a functional language is never written for people who are already familiar with one.
I dimly remember. The problem should be fixed in version 0.8. If it's not, then I'd like to hear about it again. :-)
:) I do numerical PDE work too (in the context of optimization in particular), so I'm quite aware of preconditioning. What I was trying to say is that Hs typeclasses allow a very formal representation of the "grammar" of a problem, but in this case a continuous parameter gives a very different meaning to a linear operator (and the set of operations that it can represent), so how do we represent this behaviour? Cool that we have similar thoughts btw :) 
Well first of all Haskell doesn't require you to pull in the whole compiler to just write a "hello world", and developing stand-alone applications was a concern to me. Of course it does make it less flexible (no `eval`), but there are some programs written in Haskell that try to have the cake and eat it too (for example [xmonad](http://xmonad.org) is configured by writing a replacement main module that imports `XMonad` as a library, which is then recompiled) &gt; You said you came to like it ultimately? For what reasons? More than CL? Yes, I really just came to like writing code that stays where I leave it, where I can put a lot of assumptions into the types which are checked by the compiler, instead of keeping elaborate structures in my head or having to write them down in comments. &gt; How long did the transition process last? I don't really remember now. I didn't use Haskell all that often at first, sometimes bouncing off some difficulties that I couldn't overcome. I ended up using Haskell in a program for my thesis (I'm not studying computer science, it was a data analysis program), and having deadlines on my head apparently helped me make the transition from still struggling with the language itself (though I'm not sure whether things mentioned in the [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia) count as "language" or "library") and some more advanced libraries (like [lenses](https://hackage.haskell.org/package/lens) or [pipes](https://hackage.haskell.org/package/pipes)) to releasing some first packages to Hackage. &gt; Just the thought of lists where only one datatype Comfortable programming style in Haskell is way different than in Lisp. If you want a "different lisp" you might be better off trying [Racket](http://racket-lang.org) (I don't have much experience in it, I'm just namechecking because I saw some people mentioning it here, and it does look nice ;)). It's not that Haskell prevents you from writing code in a particular way, you can do this if you want: foo :: [Dynamic] -&gt; Dynamic foo [] = toDynamic "end" foo (d:ds) = case fromDynamic ds of Just lst -&gt; toDynamic $ fst : lst Nothing -&gt; case fromDynamics ds of Just ... -&gt; ... where fst = case fromDynamic d of ... it's just pretty awkward. I don't really use lists all that much in Haskell, for example whenever I'd use a list with complicated structure in Lisp I'd use a data type in Haskell, especially that I can have constrained sum types, instead of just throwing everything in the `Dynamic` bucket implicitly. I honestly think that static typing is more flexible than dynamic typing (`Dynamic` is perfectly legal Haskell, but I can't dispatch on return value in a dynamic language, for example) in the end.
Indeed, that's the hard part.
Yes, the hack for that is like this: $ stack ghci λ&gt; import Foreign.Store λ&gt; :!echo 'x = 123' &gt; X.hs λ&gt; :l X [1 of 1] Compiling Main ( X.hs, X.o ) Ok, modules loaded: Main. λ&gt; chars &lt;- readFile "X.hs" λ&gt; chars "x = 123\n" λ&gt; newStore chars Store 0 λ&gt; :!echo 'x = 456' &gt; X.hs λ&gt; :r [1 of 1] Compiling Main ( X.hs, X.o ) Ok, modules loaded: Main. λ&gt; x 456 λ&gt; chars &lt;- readStore (Store 0) :: IO String λ&gt; chars "x = 123\n" As opposed to updating function definitions in place, you could alternatively implement "pinning" which would pin a structure generated from an IO action and keep it around inbetween loads like this. With IDE support this could be transparent. Although it gets funky when types change. Segfaults are possible.
Thanks, this helps me to understand the precise compromise that you make. My impression of your approach is that you provide a kind of switch construct, but you avoid the space leaks that it normally implies by some kind of general restriction on how state can be kept in behaviors (as demonstrated by the discussed lack of support for a foldp as mentioned above). If you have any additional insights to offer about this, I'd much appreciate them...
I think you raise a good point, there should be an easy bridge from your REPL to your test file.
Yep, I prefer writing actual unit tests to test the feature. But in actually exploring the code that will make the unit test pass, I find the repl to be super useful. It allows you to quickly see what functions you have available and their types. Maybe if there were better ide support this would not be a problem.
&gt; Just the thought of lists where only one datatype is allowed makes me shiver lol. I'm really curious as to what you do with lists containing multiple types! Where do they occur in your code, and how do you use them? I mean, I know in Lisp everything is a list, but ignoring the obvious cases that might be replaced by syntax in another language.
&gt; Many Haskellers come from C++ While it's important not to miss an opportunity to bag on C++, keep in mind C++ does offer an advanced interactive experience: Gdb. Can your REPL trap system calls? Single-step? Peek and poke registers? Disassemble machine code? Jump between stack frames? Reanimate a crashed program? [Cling](https://root.cern.ch/drupal/content/cling). Here is almost your hot-code-reloading, interactive C++ experience. If only she could escape pre-alpha after 5 years. Don't get me wrong I love GHCI and racket.
I shell out to SymPy for symbolics. I recommend reading McIlroy's [Music of Streams](http://www.cs.dartmouth.edu/~doug/music.ps.gz) and his Power Series Power Serious articles. 
I like to think of a doctest as a REPL session stored in the code. It would be fantastic to be able to open a REPL session from a doctest. This way you can preserve all the setup you have. UPDATE: submitted a [new feature for doctest](https://github.com/sol/doctest/issues/111).
repl allows you to "WRITE" code faster. comparing this to unit test is irrelevant. here is why: let us assume you write a function that supposed to match some pattern through regular expression and it does not work. your unit test will catch it. you fix and run unit test again. but it does not work again. and you fix it again and run it again and wait again. and so on... in repl loop you do not exit you interpreter. you just source and run your code again and again until it work. After that you test it using your testing framework... More important example - let us assume your app is working this way: some slow taks-&gt;data in memory-&gt;new code-&gt;other tasks... in this case - testing will be at as slow as your slow task. in repl loop you can do your slow task in interpreter -&gt; and write/check your code again and again until it work...
Learning Common Lisp is worthwhile for the same reason Smalltalk is, because they have impeccably good live development environments and inspection/debugging facilities, and knowing that they can be this good lends to a healthy dissatisfaction with the state of Haskell's. Restarts are also cool and quite more advanced than Haskell's exception system, they're used for development control flow and everything is ran in the Lisp image, including the package manager. Also writing a non-trivial project in Lisp is worthwhile to know what it's like to work in a language whose syntax doesn't actively work against being easy to edit. Anyone who has done substantial development with Paredit will tell you that it's heavenly. People who tried to edit Lisp in editors that don't do what Paredit does will tell you that s-expressions aren't that great. These people swam in a puddle. You have to dive into the pool to know what swimming is like. Don't underestimate homoiconic macros in Lisp, they solve problems that you can't in Haskell, such as defining a lambda-case or multi-way if; this is something you could define in a few lines in Lisp and move on. In Haskell it took years and discussion and compiler modification and tooling update just to support it. [Related this-plt-life GIF](http://this-plt-life.tumblr.com/post/39920361990/when-someone-is-enamored-with-a-languages-petty). If you wanted unification or pattern synonyms in your pattern matching, you could do yourself.
Good job! I was able to make this work with ghc 7.10 by changing "gloss &gt;=1.8 &lt; 1.9" to "gloss &gt;=1.8" in cabal and switching makeColor8 -&gt; makeColorI, rawColor -&gt; makeColor. I'm not an expert in AFRP yet, so cannot say anything conceptually interesting about code, sorry. But I'd like to notice performance characteristics: it's pretty nice and smooth except every N seconds there is a little lag. Typical GC thing. Would be interesting to know, have you tried anything to improve memory consumption, and/or do you any ideas what could be improved here.
It's great in video game development, for example. While the game is running you can change your 'update' function a bit and see the new logic.
When you learn a language with a very different paradigm, you should try dropping how you think for a bit and rebuilding it in a new way. Then after some experience with the new form you will subconsciously mix the two modes of thought in an interesting way. So try haskell while forgetting about setters/structs/macros in the CL way for a bit, and start thinking in applicatives/monads or whatever. haskell is closer to scheme than CL too, I'd say. By trying to recreate CL-like programming in haskell, you will simply add more accidental complexity rather than remove it. The parts of CL-like development I'd like to see recreated in haskell would actually be the live coding type stuff (changing behavior and types while your program is running).
Would you say Common Lisp is preferable to Scheme/Clojure/Racket in the teach-a-Haskeller-something-different sense (assuming you know enough about the others to compare)? I don't really have a pressing *need* to learn another language right now, which makes the paradox of choice feel a little stronger.
[No True Scotsman](https://www.youtube.com/watch?v=5zzSqL--d_I)
Or perhaps like in Matlab, where a function can check how many return values are actually consumed, and change its behavior accordingly. Kind of like combined lazy evaluation / dispatch on the return value, but without all the guarantees that Haskell provides.
Yeah I meant more in the Pythons and Lisps as sasquatch007 was saying above. In Lisps (I've only done CL, Clojure) it basically works out of the box. It'd be sweet to have something of the sort in haskell but it could get messy if you change types (add/remove a record field for example). CLOS handles it pretty well.
 readsMaybe r = fmap fst . listToMaybe . r
The reasons for learning each are different IMO: * Scheme is a tiny toy language, it's pretty much only worth learning as an example of a minimal language. It doesn't teach anything particularly interesting for a Haskeller. * Clojure is probably the cleanest designed Lisp and closest to functional programming, and practical because it can use Java libraries. I don't think it has restarts or a particularly good development environment, though. Its answer to SLIME, nrepl is kind of new and less established. * Racket is Scheme with extra stuff that tries to be everything with different dialects. I don't know that there's anything new in it, probably not its development environment. (One of the Racket authors is likely to read this and come reprimand me, but I'll live.) * Common Lisp has a mature development environment and mature compilers, along with a culture of using them, with restarts and spitting out assembly and all that. SBCL is a fascinating compiler to me. I think you get a genuine "Lisp" experience with CL. It's also the oldest of the non-toy Lisps and as such has a lot of cruft in the standard library. * Emacs Lisp is another alternative you didn't mention. It comes with a native development environment and self-documenting standard library. It's not a great Lisp but it at least gives you the image-based development model and you can use it to enhance your development environment. But I've only learned Scheme (although my Scheme of choice was PLT Scheme, the predecessor to Racket) and CL.
I haven't benchmarked it. I assumed the parsing functions from `text` would be respectable. I'd certainly be open to a pull request switching if it was accompanied with a benchmark showing the performance difference.
&gt; All the time, that's why it is used all the time. According to "Real World OCaml": &gt; In fact, many seasoned OCaml programmers rarely use classes and objects, if at all. And according to people I've talked to IRL, "use OCaml but ignore the object system". The open-recursion it is used for seems very rarely useful, and may as well use ordinary fix-points for that. &gt; What problem does this cause as a programmer? You have to eta expand everything. &gt; Nobody else has a problem programming in the small. How can you tell? Many of us who had a problem programming in the small just use Haskell.
&gt; Anyone who has done substantial development with Paredit will tell you that it's heavenly. Oh yes, jumping around the syntax tree is one things I really miss in Haskell.
It's the same as (apple:apples). The rest of the apples. What continues to irk me is what to call the entire thing when using the "as pattern". For example xz@(x:xs). Is there an accepted way to name the xz? 
What if you have a list of lists? `as:ass` is quite a lot shorter than `list:listlist`. What if you have two lists? `a:as` and `b:bs` is quite a lot shorter than `element1:list1` and `element2:list2`.
&gt; Is there an accepted way to name the xz? I usually go for `xxs`. In Agda I tend to use `x∷xs` but some find it confusing (and it would not be a valid identifier in Haskell anyway).
Shorter does not always mean better. Longer symbol names could be easier to read.
This blog post didn't comment on the *main* use of `Default`. Specifically, I almost always see `Default` used in complex settings initialization cases, like this: data Settings = Settings { settingsFoo :: Foo, settingsBar :: Bar, settingsBaz :: Baz } instance Default Settings where ... runFloopWith :: Settings -&gt; Floop a -&gt; IO a runFloopWith settings floop = ... runFloop :: Floop a -&gt; IO a runFloop = runFloopWith def Then user code can use it like this: main = runFloopWith def { settingsBar = newBar } I'm not saying that the blog post is wrong or even that I disagree with it – I just think that in order to make this point, you *have* to address the most common use of `Default`. (Or, at least, I think that's its most common use.) One answer to this is to just suck it up and write full variable names, e.g. main = runFloopWith defaultFloopSettings { settingsBar = newBar } And I actually think that is preferable, because then `defaultFloopSettings` is a documented identifier that's easy to inspect and find the source of via Hackage; in comparison, Hackage docs for instances tend to be a little bit less accessible in my experience. The loss of convenience is not very large – you just need to find the right identifier to replace `def`.
cool! when might this version be released?
It's just an argument name. I think it doesn't hurt for arguments and local variables to have short names, as their scope is small and there are types forming a safety net outside.
I had seen those instructions and I assumed they were out of date. (They mention linking in the “Whatever_stub.o” file, which GHC &gt;= 7.2 [doesn’t even generate](http://stackoverflow.com/a/10983869/371228).) But I tried anyway: ghc -c SpectrumMath.hs ghc -c dll.c ghc -shared -o SpectrumMath.dll SpectrumMath.o dll.o where “dll.c” is exactly the same as the “StartEnd.c” shown in the linked section of the manual. This gave me a whole bunch of linker errors—apparently GHC couldn’t find the “vector” package, among other stuff. In order to pass the correct linker flags to GHC, I followed a process like the one I mentioned in the Stack Overflow question I linked: ghc -no-hs-main -v SpectrumMath.hs &gt; ghc_output 2&gt;&amp;1 I then found the linker command in that file, extracted all of the `-L` and `-l` flags, and gave them to GHC: ghc -c SpectrumMath.hs ghc -c dll.c ghc -shared -o SpectrumMath.dll SpectrumMath.o dll.o "-LC:\Users\bdesham\AppData\Roaming\cabal\i386-windows-ghc-7.10.1\vecto_6aaU0nnmvuo8Fm695YhjzM" "-lHSvector-0.10.12.3-6aaU0nnmvuo8Fm695YhjzM" [many more lines snipped] This successfully (?) created a DLL and a .dll.a file, but when I tried to invoke them from my application I get the error described in [this other SO question I asked](http://stackoverflow.com/q/31416185/371228). To summarize, when I try to open the application I get &gt; The procedure entry point LVSpecData_GetGlob could not be located in the dynamic link library SpectrumMath.dll. SpectrumMath.dll is the name of the Haskell DLL I built, but weirdly, LVSpecData_GetGlob isn’t in that DLL—it’s in another DLL I link against, and nothing about *that* DLL or my application’s use of it changed when I added the Haskell DLL. Moreover, this problem only crops up when my application was built in Release mode. If it was built in Debug mode everything seems to work! I don’t know what the hell is going on here.
Thanks, but those seem to be two different beasts.
You can do this with recent versions of Cabal. You'll want to be familiar with [this GHC tutorial](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/win32-dlls.html). * As GHC's tutorial shows, you need a `StartEnd.c` * Add `-shared` and `StartEnd.c` to `ghc-options`. Cabal 1.22 and prior will issue a warning when building, but you will still end up with `HSdll.dll` and `HSdll.dll.a` in the same directory as your cabal project. You will find a `&lt;MyProject&gt;_stub.h` file in cabal's build tree. [It's still quite crude, as you can't easily rename the DLL via cabal (you can, but it's messy) and you can't have it place the output in any other folder.] In your C++ code, include the `_stub.h` file and link in the `.dll.a` like a `.lib` file. I have done this with both cabal-install and stack, on GHC 7.8.4 and GHC 7.10.1, both 32 and 64-bit compilers. ~~Since you ask, I might try to write up a blog post about it actually.~~ I answered your SO question instead.
Yes, I agree that's why despite having written "better" originally, I edited my message to "shorter" instead. But the idea is to have short variable names carrying a lot of information: * the degree of nesting (for lists it's going to be multiple `s` but for a functor, you may write `fa` or `ffa`, for a function `aTob`, etc.) * the relationship in terms of types (an `a` can be pushed onto an `as`, an `aTob` can be `fmap`ped on an `fa`, an `as` and a `bs` cannot be interleaved because `a /= b`, etc.) It's a matter of a taste, I guess.
Maybe I am a bit biased, coming from Java, where you can have a ConnectionFactoryAwareAspectInstanceFactoryBeanInfoAssembler. :-) But I still find more clear to read the version with element and elements.
There's a culture of making code as succinct as possible in the Haskell community. In the beginning I thought x:xs and y:ys were confusing too, but overtime they became very natural and I actually carried the idiom to other languages.
Builder pattern works just fine in Haskell.
&gt; It's laziness that's the questionable part. Why, if you don't mind me asking?
I'd imagined `Default` as being more like a default constructible type in C++, than a default argument to a function.
It's easy, you just define an existential or algebraic data type, which itself can hold values of multiple types, and you put those in a list. http://chrisdone.com/posts/existentials
I might be biased, but I'm going to argue that it's not "just taste", and there is a best choice for particular cases. In languages like java, expressive names are preferred because they give additional information about the expected behavior of the arguments. In Haskell, it's not so much needed because the types strongly constrain the behavior already. Particularly, in the case of parametric functions on containers, there's really nothing more to say about the arguments other than what's already provided by the types and the shape of the constructor. Using short names in this case makes the structural where you can think of the names (x,xs, etc.) as shorthand for "wires" and the whole function as a compact notation for a (structurally determined) "circuit". On the other hand, in particular cases where the "real type" of the data is not fully specified by it's type annotation, an expressive name can be useful. Another issue that's not a matter of haskell vs java or whatever, is structural vs nominal for inductive vs coinductive types. Inductive types (data) are defined structurally and deconstructed by pattern matching. On the otherhand, coinductive types (codata, of which OO Objects are a special case) are defined by pattern matching and deconstructed by views, which can be thought of as labeled fields. Since everything in haskell acts like both data and codata (with Bottom discriminating the true difference), the choice should be made based on if you intend it to be used as data or codata. 
If you want laziness, you need purity too. If you have lazy functions with side effects you will have a really hard time reasoning about when your side effects will happen. This is a huge pain point for Clojure developers because that language is impure but makes extensive use of lazy data structures.
`LambdaCase` sometimes is a nice way of *not* having to name the `xz`.
Yeah, I like this.
I use that for `((x:xs):xss)`.
What about for protocol buffers, thrift, etc, where there isn't an obvious monoidal instance for a struct but there _is_ a semantically meaningful default value? 
In a dynamically typed language heterogeneous lists are often used where a homogeneous list of a sum type is really what is sought.
Thanks for the [blog post](http://chrisdone.com/posts/haskell-repl) and video. I was able to achieve fast project reloading as you demonstrated after only a few minutes of fiddling with an existing project.
It is 1s for 10 lines of _Haskell code_ without comments. Until now we write all the liquidHaskell annotations in special comments of the form `{-@ liquidHaskell annotation @-}`. These kind of special comments are interpreted from LiquidHaskell. So, we can count the Haskell code and the required annotations separately. Btw, I am a "she":) 
Personally, I'd love something like this: map(function, List.Empty) = List.Empty map(function, List.Node(value, tail)) = List.Node(function(value), map(function, tail))
Holy shit! How in the world did I not know about -fobject-code? I'll have to give it a try.
Ah, you're right. My mistake. 
Standard chartered. Git annex. Shake. Pandoc. Xmonad. Haxl @ Facebook. Yesod/happstack/snap/etc power more web sites than OCaml does. Tons more in house projects you rarely hear about. I use: Our company uses buildsome (my own build system in Haskell), and I heavily rely on resolve trivial conflicts (another tool I wrote in Haskell). I'm implementing Lamdu in Haskell. 
I'm getting: Prelude Foundation&gt; :r [2 of 2] Compiling DevelMain ( src/DevelMain.hs, src/DevelMain.o ) src/DevelMain.hs:25:29: Couldn't match type ‘IO b0’ with ‘(wai-3.0.3.0:Network.Wai.Internal.Response -&gt; IO wai-3.0.3.0:Network.Wai.Internal.ResponseReceived) -&gt; IO wai-3.0.3.0:Network.Wai.Internal.ResponseReceived’ Expected type: IO Application -&gt; (Application -&gt; IO b0) -&gt; (wai-3.0.3.0:Network.Wai.Internal.Response -&gt; IO wai-3.0.3.0:Network.Wai.Internal.ResponseReceived) -&gt; IO wai-3.0.3.0:Network.Wai.Internal.ResponseReceived Actual type: IO Application -&gt; (Application -&gt; IO b0) -&gt; IO b0 In a stmt of a 'do' block: handler &lt;- readIORef ref In the expression: do { handler &lt;- readIORef ref; handler req } In the second argument of ‘runSettings’, namely ‘(\ req -&gt; do { handler &lt;- readIORef ref; handler req })’ src/DevelMain.hs:26:29: Couldn't match expected type ‘IO b0’ with actual type ‘(wai-3.0.3.0:Network.Wai.Internal.Response -&gt; IO wai-3.0.3.0:Network.Wai.Internal.ResponseReceived) -&gt; IO wai-3.0.3.0:Network.Wai.Internal.ResponseReceived’ Probable cause: ‘handler’ is applied to too few arguments In a stmt of a 'do' block: handler req In the expression: do { handler &lt;- readIORef ref; handler req } Failed, modules loaded: Foundation. Prelude Foundation&gt; Trying to run the code from the linked repo: https://github.com/chrisdone/ghci-reload-demo
I've also tried creating a new site from scratch with Yesod which runs the site, but doesn't seem to update on any changes :S Here is a link to it: https://github.com/codygman/spawncamping-octo-cyril EDIT: Does this method not reload templates? EDIT: I must have forgotten to do something haskell-mode's haskell-process-reload-devel-main works fine.
Seems to me, what you're really critizising, is Haskell's type class mechanism, or more precise: One type can only have one instance.
Thank you. But don't you still need laziness if you want to choose what to evaluate and when? Unless you have macros.
I don't have any suggestions cos I don't grok FRP, but thanks! What's with the double arrowhead here? game :: a -&gt;&gt; Gloss.Picture FWIW even using a sandbox I had to install netwire separately, or cabal couldn't work out what to do (ghc 7.8.3, cabal 1.22). 
Laziness is a (pretty obvious) requirement
1.) Not really. 2.) I hack up cabal a little bit to disable the building of profiling builds and tell it not to build documentation by default. This rather dramatically cuts my build times. After that builds are typically on the order of seconds for me. Factoring my code so that I don't have things that change a lot in big common modules upstream that would force lots of rebuilding helps a great deal as well, but that is just basic software engineering. 3.) Definitely.
I like `hd:tl`. Works great no matter what your type parameters are called! If you've got two lists, `hd1:tl1` and `hd2:tl2`.
Can you put this on this Haskell wiki?
You forgot [try.jupyter.org](https://try.jupyter.org) :)
Real World Haskell
I was going to buy it, but some of the reviews make it sound really... Intimidating
I personally found [Beginning Haskell](http://www.amazon.com/Beginning-Haskell-A-Project-Based-Approach/dp/1430262508) to be a very good and comprehensive book.
Your answer is my favorite here as a beginner. Also I don't like specifically "hd" and "tl" as much as "head" and "tail" but I guess those aren't options
This is one thing I feel like the Haskell community is missing. We really need a book that covers design principles and some more advanced concepts in Haskell. For instance, I don't think there is a single book that covers free monads, which is a popular design pattern for implementing DSL's. How many books talk about comonads? I think a book about good Haskell practices is sorely needed.
[This is a repo that has worked well for me.](https://github.com/bitemyapp/learnhaskell) The author is in the process of writing a [book](http://haskellbook.com/). I got the early access version, and it is really good so far. At the moment, they've just covered up to folds, so it isn't comprehensive quite yet, but I found it to be much more useful/helpful than eg Learn You A Haskell.
Did you read it as a beginner?
Some of LYAH but yes it's a bit too light after a while, then the wikibook ( https://en.wikibooks.org/wiki/Haskell ), then all FP101x videolectures by E. Meijer (w00t!), "the essence of functional programming" by P. Wadler, as many Functional Pearls as you can, then RWH as suggested above, then Marlow's "Parallel and Concurrent Programming in Haskell", then the stars ! :) enjoy the ride
[These](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) [two](http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html) links are decent introductions to free monads. They essentially take a functor (like a list) and make it recursive: Free [] a &lt;=&gt; data Tree a = Tree [Tree a] | Leaf a Free ((-&gt;) e) a &lt;=&gt; data PolyArgFn e a = Arg (e -&gt; PolyArgFn e a) | Done a Free IO a &lt;=&gt; data PausableIO a = DoMoreIO (IO (PausableIO a)) | Finished a --general form data Free f a = Free (f (Free f a)) | Pure a Provided the first argument to Free (the `[]`, `(-&gt;) e`, or `IO`) is a Functor then the Free version of it is a monad. It's possible to create a functor which represents a set of instructions, and then the Free version of that Functor can be used to represent (a peculiar type of) abstract syntax tree, which those links explain in more detail. Comonads are probably best left for later. They seem a lot less common than free monads. EDIT: Though if you're just starting out it's not terribly important until you've already got monads (and monad transformers) under your belt I think.
Agreed. Lot's of the examples have trouble working. It uses an outdated version of `parsec` to teach parsing, which is part of the problem.
You mean an `empty` value? Well, `empty` is already very different than `def`. :)
Well, you could hide `head` and `tail` in your prelude import. There are arguments for this already, since they're icky. :)
Don't be afraid to ask questions on IRC (and also don't be afraid to ask people to slow down if it's over your head). There's a very wide range of expertise on IRC.
...huh. That animated GIF is playing backwards.
Oh, we have tons of gurus. If anything, we have too many! Most Haskell stuff out there is for an advanced Haskell audience. Academic papers, blog posts with more category theory than code, etc. And I love that this stuff is out there -- but there's a lack of material that's accessible for beginners.
I use a bunch of Haskell programs in my daily routine. What OCaml programs do I use? I'm pretty sure I use 0 OCaml programs. I haven't heard of "mirage" before, I'm pretty sure Yesod, git-annex, and others are far more popular.
&gt; So everyone here just "hacked it out" to get to their level? Um. Yes?
&gt; Oh, there are definitely gurus. They just haven't written the great haskell tome yet. Because books about programming topics are basically not worth writing (except maybe for the money). Get outdated way too fast.
&gt; What are these things??? It's important to note that you can get *lots* (everything the working programmer needs) done without learning any of these "advanced topics". I mean, I love them, but I know people who freeze up on the "I'll never learn arrows and free monads and FRP and fancy new technique mark 67", and the truth is that to get things done you don't need to. Each topic has value, but only in some particular design space.
I do not. -fobject-code trades a linear factor for a constant (ish) factor. It'll be slower on small projects where it's so fast that it won't make a difference, and faster on large projects where it's slow enough to make a difference.
I tile a narrow ghcid window beside my editor, it's not a very fancy setup. The point is, forall editors x, there are people who do not like x. ghcid doesn't actually integrate with any editor at all, and so can be used with *every* local editor. Convincing people to switch editors for a feature is distilled frustration. Saying "this will work with basically anything you want it to, and make your life easier without any additional cognitive overhead" is much easier.
About 10 years ago I was doing a lot of Common Lisp programming. I started checking out Haskell because I had a class that *made* me use it. I hated it. So many type errors. Not only that, but the type errors kept pointing out when I had done something dumb. I already knew how to do functional programming, why did I have to relearn so much? Why did I need to memorize a new standard library? **Why??** Fortunately that class didn't last forever and I was able to go back to Common Lisp. That's when I realized a few upsetting things: * First, **I** continued to write programs that didn't make sense, but now the language did nothing to help me figure that out **statically**. * Second, instead of having things evaluate as needed, I was back to inserting `quote` to delay computation as needed. This was brittle and I often get it wrong. I hate finding out about my stupid mistakes at run-time. Basically, **Haskell ruined lisp** for me. So I gave Haskell a second try with an open mind and that time it stuck. I had a period of time in grad school where I was doing Haskell for fun and lisp for my professor. Now I'm about 10 years into this Haskell-for-the-second-time experiment and the only lisp I write is when I'm configuring emacs. I don't miss CL at all, but I still have tons of respect for it. You won't need macros as much as you do in lisp because laziness allows normal run of the mill data structures to serve as control flow structures. No macros needed for that class of problems. Haskell has a template system and generic programming for the other cases. The lack of side-effects is great. I had to retrain myself a bit to write code correctly in that style, but it forces you to separate concerns early and that always pays off long term, as far as I can tell. One thing to be warned about, in Haskell you probably don't want to convert everything to tail-call form all the time. I'm referring to the transformation where you add a parameter to a function so that you can pass in an accumulator value (the result of the computation, thus far) and when you're done you just return it. That form still makes sense in Haskell (sometimes), but due to laziness you usually don't need it (you can think of every function call in Haskell as already being a tail call), and there are documented cases where it's actually worse than not doing it. Haskell doesn't need a normal function call stack, but in GHC some computations are reduced using a stack (as an optimization). When you do the above tail call optimization you can sometimes put too much pressure on that stack and cause performance problems. Overall, keep an open mind and I think you'll be happy you gave Haskell a shot even if you don't stick with it ultimately.
http://www.catb.org/jargon/html/S/SMOP.html &gt; 1. A piece of code, not yet written, whose anticipated length is significantly greater than its complexity. Used to refer to a program that could obviously be written, but is not worth the trouble. Also used ironically to imply that a difficult problem can be easily solved because a program can be written to do it; the irony is that it is very clear that writing such a program will be a great deal of work. “It's easy to enhance a FORTRAN compiler to compile COBOL as well; it's just a SMOP.” &gt; 2. Often used ironically by the intended victim when a suggestion for a program is made which seems easy to the suggester, but is obviously (to the victim) a lot of work. Compare minor detail.
Not at all! Also, it's available for free online! http://book.realworldhaskell.org/
Comonads are actually simpler than monads, or Free, in my opinion. A comonad is like fmap, except you can use the whole structure to determine the result, and still be guaranteed that the structure doesn't change. Generally you only inspect the neighbourhood of the element you're working on, but you don't have to restrict yourself like that. An example would be something like `data Grid k a = Grid k (Map k a)`, where the Map *must* contain the key k. You then have a Map with one specific element singled out. If the key type k can be navigated by a `left` and `right` function, then a function to add up the local neighbourhood: localSum (Grid k m) = sum &lt;$&gt; mapM (`M.lookup` m) [left k, k, right k] can be applied to a whole grid by `grid =&gt;&gt; localSum` and the result will be every element replaced by `Just` the sum of itself and the elements to its left and right should both such elements exist, and otherwise `Nothing`. The keys and number of elements will be totally unchanged. There are other things you can do with them, but personally, I started out just thinking of them as mapping but with access to more information.
It seema odd that you think you should know these things but are looking for a beginner haskell book.
Thank you, rather. And no worries. :-)
The thing about haskell is the tight scope of variables. So you can call something x and know that all references to x will probably be withing view in your editor at the same time. In Java, C# etc. where you have code 'more spread out' calling something x (unless it is a coordinate) is probably bad for future maintainers. Also in Haskell the x:xs thing is idiomatic, you will see it everywhere and so it will not surprise other coders. They will be used to scanning it when reading your code. Of course record naming in Haskell is different and needs super-verbose names to get around lack of namespacing plus needing to come up with a descriptive name anyway.
Price is rather steep, and especially so for early access. It's going for $59 dollars from what I can tell. Richard Bird's "Thinking Functionally with Haskell" is only $39.99, and he's well established in the computer science community. 
Hbc did this, and our Mu interactive top level also keeps the state after reloading (it also doesn't require you to reload, it just does it). Or at least as much of the state as is type correct. It's easy to implement. 
&gt; Haskell should has them too I can has macros?
My belief in Haskell superiority over OCaml is not based on popularity (which Haskell has over OCaml), you brought that one up. Type-classes are IMO far more important than modules. Purity is far more important than row polymorphism (though Haskell has that too now, with QQ). Nicer ecosystem (with amazing work like kmett's, e.g: `lens` library). Much better designed syntax. OCaml having faster compilation is a huge point for OCaml but it's nowhere near cancelling out all the above.
I'm surprised that you'd call Emacs Lisp an image based development model, but I've always suspected that I never understood the whole image system properly. As I understood it, I would have though Emacs Lisp is a file based system, since only what is physically written in my .emacs file is preserved between sessions, while all of the remaining state is cleared. Have I misunderstood emacs or have I completely failed in comprehending images?
The simplest example I can think of is `Maybe` vs `nil`. In scheme you might want to represent a list of strings that exist and don't exist. You'd do this with a heterogeneous list of `string` and `nil`. (list "bar" #nil " "foo") In Haskell you'd use a homogenous list of the sum type `Maybe String`: [Just "bar", Nothing, Just "foo"] 
Well, despite what people say, I’m still waiting for a good use of that package. I think there’s none. It’s way more rigorous and practical to reason about `Monoid` or `Zero` instead of `Default`. The `empty` case – `Data.Vector.empty`, `Data.Map.empty` – might be **the** exception, but in that case, `def` is not a good name, and we’re talking about something very different. :)
It's probably just mincing words. An image indeed tends to mean image-based persistence. Rather, I meant the environment, the thing that you update. Usually that's a running image in Lisp parlance, but indeed, Emacs has no way to persist the image to disk.
two things 1. the project says IHaskell is &gt; a kernel for IPython that means it depends on IPython to run, no? I'd hope that the official ghci would be pure Haskell. 2. it looks great :) I need to give this a try. I've used IPython for Python and forgot what the standard interpreter even was. do you talk about how you picked this "architecture" somewhere? (rather than say an Emacs mode). thanks!
I also did https://www.edx.org/course/introduction-functional-programming-delftx-fp101x-0 . I am not really in "instruction videos" but at 1.5 speed it was actually fun to watch. The book, Programming in Haskell, it follows is also very good, IMO. If you want an academic(-ish) approach pick either "Programming in Haskell" or "Thinking Functionally with Haskell". If you want a lighter read, "Learn You a Haskell for Great Good". I have all three (and some more).
What's the rationale for this? https://github.com/mightybyte/readable/blob/master/src/Data/Readable.hs#L73-L85
The https://www.haskell.org/documentation page has attempted to assemble a list of suggested resources. None of them are like "effective c++". That book is not a first book, but a second book. One that presumes you know the language, and now teaches you best practices for working with it. The issue is that the "best practices" for Haskell have been under quite a bit of evolution. If you want a short dense read that spares no detail, something like Yet Another Haskell Tutorial or the Gentle Introduction is a classic to way to go through the core language. But as always it depends what you want. Do you want to write webapps? Do you want to write number-crunching? Data-centric algos? Services? Do you want deep results in how to _think_ about even basic functional programs? Depending on your "angle" there are better and worse resources for any given approach.
Generally I try to avoid type synonyms because they don't survive type inference and error messages. Also, the specific goal of the `errors` library is error-handling so `ExceptT` is actually a more appropriate name in that particular context. If there were an `EitherT` type synonym it should belong in another library (or `transformers`)
That's been the biggest problem for me. The books go so far but they just don't give me some of the real knowledge I need to do anything really significant. I still think the biggest inhibitor is the lack of help/examples in the packages stuff. There are some great libraries available but I find it far too difficult to figure out how to use them because you just need to have a much deeper understanding of Haskell than essentially any other language with which I've ever worked to do anything. 
I must have misunderstood that part of the discussion. I realize they use ocaml, but I think that's for internal tools. If that's the case, the Haskell part has quite a few more direct users. Anyway, we will probably just have to agree to disagree on many of the other issues except that GHC could definitely use some improvements to its compile times.
I'm a newb, and I have all three (Learn You a Haskell; Haskell, the Craft of Functional Programming; and Thinking Functionally with Haskell). LYAH is great for being an approachable text, but also has more detail on monads than the other two (although it includes no exercises). HCFP is more mathematical and thorough, and the exercises are very good. TFH is even more mathematical, slightly less thorough, but has exercises INCLUDING answers. My method has been as follows: I read LYAH twice to get the lay of the land, then read HCFP once and now I'm working through the exercises. Once I do that I'll do TFH (which by then I will mostly breeze through). It's worth noting that I personally looked through bitemyapp's recommended links and even spent a lot of time on the Yorgey CIS194 course -- for whatever reason, that stuff felt like it was moving too fast for me. With the books, I've had no problems and am making good progress. Everyone is different. Good luck! (BTW, I also first got LYAH and HCFP from the library for free before buying, I strongly recommend that -- I'm guessing that a major city library should have them.) 
Hmm, okay. I think I've just become too familiar with implicit nullability - to me thinking about a list of strings and nils isn't a heterogeneously typed list, just a list of pointers.
I prefer `defaultFloopSettings` over `def` from the `Default` type class, because the type class has no laws and it's also one less dependency and one less import.
You can simplify your presentation by explaining a couple of things: * What are the core type(s) of your library? * What algebraic operations do those types obey? If you have that then it's easier to fit this into a larger context.
I think it depends what you want to get out of Haskell. If like me you want to write simple programs that are maintainable then readability and sometimes performance will be the main concerns. So I would go for fac n = product [1..n] My philosophy is keep it simple and avoid explicit recursion unless you really have to do it. Use built ins like product, or failing that, foldl' or foldr. Any answer &gt; 1 line for this I wouldn't do but may look at for curiosity or educational interest.
What, specifically, are you referring to?
Just pointing you can use `x:xs` as the name of the whole: foo (x:xs) = 3 : x:xs Which is obvious so I don't know why I posted
It's defined in the [safe](https://hackage.haskell.org/package/safe) package, which gets re-exported from the [errors](https://hackage.haskell.org/package/errors) package. I've also got code up in the [repository](https://github.com/dalaing/cofun), but that's changing and evolving as I write the blog posts. 
Edit: also, I plan to have a few posts on the zipper and some related fun once I get the networking post done.
Not sure... GHC is particularly bad at CSE so I'd guess it would reconstruct it.e
Just that you can type `def` instead of one of the arguments, probably. 
The logic that you define in your paper really reminds me of my intuition about Big-Oh notation. Where also functions are equal Big Oh when after a certain point they grow similarly. It has a very same feel in thinking.
I think this is partly because all those things have been invented/put to practical use relatively recently. There just hasn't been time for interest/practices to build up enough to make books on those topics viable. 
&gt; The Default typeclass is evil. It’s shipped with default instances, like one for [a], which defaults to the empty list – []. It might be clear for you but why would I want to default to the empty list? Why not to [0]? Because `[]` is the *only* value that *all* `[a]` can default to. `[0]` is not of type `[a]`, but `[0] :: Num t =&gt; [t]`. Just like `Nothing :: Maybe a` is the only possible default values that works for all `a`s.
I've been interested in the series of blog posts that appeard on School of Haskell about this technology, but I'm afraid I can't make head of tail of it.
Yeah, but this reasoning breaks for datatypes with fields of concrete types.
You’ve forgotten a point. If you have `Default [a]`, how can I have my `def = [0]` with `(Num a) =&gt; a`? I cannot. So, it’s insane stating that all lists should default to `[]`. It depends.
that's not the name of the whole tho. that's a pattern match on the left, with a reconstruction of the cons on the right. in-memory, it's building a whole new cons node on the right. the point of using `@` is so that you can write this: foo xxs@(x:xs) = 3 : xxs and it will reuse the exist cons node in memory rather than creating a new one.
I am the author and in some way I'm the first one that is trying to understand it. For haskellers [this paragrap](https://www.fpcomplete.com/user/agocorona/the-hardworking-programmer-ii-practical-backtracking-to-undo-actions#a-word-on-continuations) may be the best summary
The problem with monoids is most configs in practice end up being difficult to define as monoids and I often see ones that don't satisfy the laws. It's cleaner to just have the `defaultConfig` value.
hastor, thanks for the suggestions. Unchecked exceptions kill the thread where the exception appears and all its child threads, unless `freeThreads` is used. In this case the threads do not die when the parent finishes. Most of the real processing is done in IO computations, since in the Transient monad only the skeleton of the program is defined. The transient primitives have IO computations as parameters, so how you deal with your blocking concurrency inside your IO actions is up to you. For example, `async`, mentioned in another example here, will de-block your blocking IO. async :: IO a -&gt; TransientIO a To implement asynchronous keyboard IO, I have a single getLine that updates a TVar. Then many IO processes watch the TVar. By lifting each blocking IO with `async` to the Transient monad, all IO computations execute in parallel when combined with (&lt;|&gt;) or (&lt;*&gt;) so the one that find what it is waiting for, executes the rest of the monadic sequence: waitInput :: TransientIO () waitInput= do r &lt;- async (blockedExpecting "hello") &lt;|&gt; async (blockedExpecting "world") liftIO $ putStrLn $ "input entered: "`++ r `blockedExpecting` represent any IO computation that may use any kind of blocking interprocess communication. But it may be a long download or anything that block for a certain time. `async` executes the IO expression a single time. `waitEvents` execute it forever in a loop. Both are variants of `parallel`. They finish the current thread after forking a new thread, that executes the IO computation and then executes the (closure&gt;&gt;= continuation).
The problem with that approach is that you often have very abstract functions in Haskell and x:xs is about as meaningful as element:list when you are familiar with the idiom. Often in abstract code you can not find a better description than a generic name like that. That is a problem Java, with its almost completely lack of abstractions, does not really have when naming variables.
&gt;The Default typeclass is evil. It’s shipped with default instances, like one for [a], which defaults to the empty list – []. It might be clear for you but why would I want to default to the empty list? This is a bit much really. Typeclass libraries are expected to ship with instances for relevant types from base. I don't know that these types are relevant but at least having them here prevents using orphans - which would be worse. I think default is fine but should only be used with application-specific or library-specific types - there is just no sensible "default" for float or [].
I felt exactly the same way six years ago. Turns out, this crazy stuff is all about some very deep patterns important both for implementation of programming languages and for reasoning about programs. And while it's highly unlikely you'll ever write anything quite as strange as that in production code, one day you might find yourself reading through that joke page -- or the papers that inspired it -- simply because you'll want to know more, because you'll want to better understand the fundamental math underlying our trade. That's been my experience so far, anyway, and I'm quite glad it worked out that way.
&gt; doesn't require you to reload, it just does it when? 
&gt;That last line is a bug one way or another; the difference is when you are going to catch it - at compile time, when the code itself runs, or maybe much later when it turns out that you've been saving wrong values to the database for months, and have lost large amounts of mission critical data, and then your manager comes to your desk and asks how this is possible and you don't really have a good explanation and neither does your team lead, and you all get fired and then you spend your days building Drupal websites for people who shouldn't be allowed anywhere near electronic devices or useful information of any kind, or teaching Microsoft Word and Outlook Express to unmotivated long-term unemployed fellow citizens who only take your class because it's free and they'd be cut on their welfare if they didn't, and it doesn't really matter whose fault it was anyway because that information you lost was so critical that the whole company goes down the drains and everyone is laid off except for the boss who gets fired just in time to give him what's left of the company's value as a golden handshake and to cash his equity shares and book a flight to the Bahamas. Please, no! Not Drupal! I have kids!
This might be true for "Potato.js for Dummies", but the basics of Hs deserve a textbook. A "live" format such as the online version of RWH, with slots for (moderated) reader annotations, would be quite optimal, I think ..
Speaking precisely, you're thinking of Big-Theta; it's true (though misleading) to say that mergesort is O(e^(n)). I have to say, though, that it's a surprising connection that I hadn't noticed. Thanks for making it.
I personally care about the syntax that I'll stare at hours every day.
Trying get a feeling for these objects; from the article, we need a set M and two monoids over M with multiplicative identity and distributive product (see below) (M, ⊗, e, ⊕, z) where both (M, ⊗, e) and (M, ⊕, z) are monoids for the same set M; moreover, the following laws relate both structures: z ⊗ a = z (a ⊕ b) ⊗ c = (a ⊗ c) ⊕ (b ⊗ c) Here ⊗ is the multiplication of the near-semiring, ⊕ is the addition, e is the unit, and z is the zero. Why the "near-" label? Because of the absence of a commutative operation? Also, the implications for search strategies seem intriguing, I'd like to understand more
As /u/NiftyIon said, the common use of default is for a _library_ to provide it for a custom type, usually a settings record of some sort, and for the user to override some (but usually not all) fields to tweak the configuration. One example of using this to great effect is the `diagrams` library. Now, it's true that `Default` is a type class with no laws. This means that, as the user of the `def` method, your only guarantee about the value is that it is of the correct type (and it's reasonable to assume that it doesn't contain bottoms). This means that you should only use `def` when you want someone else to pick the value you receive without having to fully understand it; i.e. you want someone else to pick a sensible default for their library. Using a `Monoid` doesn't work for many configurations because it requires every optional field to be a `Maybe` type (or similar) and for functions using the configurations to have the default built in (I'm reminded of the Python convention of setting `None` as the default, but that's to work around its particular mutability semantics). The builder pattern (or the related pattern of relying on the `Endo` monoid) require the library writer to provide functions to modify the configuration and doesn't seem to me to provide any advantage over just using a record without a `Default` instance. In many libraries, I could reasonably see using a plain record like that, but in bigger libraries with many configuration types, it begins to feel redundant, more like a design pattern than an abstraction: e.g. doThingNumberOneWith thingNumberOneDefaults{ fizz = 1, buzz = "hello" } "more" "args" "here" doThingNumberTwoWith thingNumberTwoDefaults{ foo = "something", bar = Nothing } "yet" "more" "args" as opposed to doThingNumberOneWith def{ fizz = 1, buzz = "hello" } "more" "args" "here" doThingNumberTwoWith def{ foo = "something", bar = Nothing } "yet" "more" "args" I guess the recommendations I would give: * End users: `Default` is a way for someone (usually not you) to choose an arbitrary value; don't use it except when the library recommends it. * Library authors: consider having a default arguments record that isn't an instance of `Default`, especially if you only have one configuration type.
A near-semiring is a semiring, except that the 2nd operation doesn't have an identity. Maybe it's because `Alternative` doesn't have a unit?
The `onUndo` operation looks similar to the `orElse` operation of `STM`. Is there a similarity between `TransientIO` and `STM`?
Not much, since `onUndo` add his second parameter to a list of computations that will be executed in reverse order when `undo`is called. Each `onUndo` can stop the execution of further `onUndo` and resume the execution forward from this point on. it is intended that a `onUndo` can either perform a compensation (to undo a transaction) or to fix some problem, so that the computation can restore execution forward from that point on. When a backtracking point call `retry`, then the backtracking stop and the execution resumes forward from the backtracking point onwards. This `retry` is not the STM retry, that re-executes the STM action. A web navigation can use onUndo to check if the user has pressed the back button and has sent a form corresponding to a previous page. if a page does not match, it can trigger an undo, so the previous pages can verify the response. the one that match will restore the flow forward. It is even possible to use the backtracking mechanism to close resources when the computation has finished It is possible to create publish-subscribe variables: EVars (from Event vars). I have the module EVars, but currently does not work with the last changes. it is something more close to STM. Modifying an Evar with `writeEVar` trigger the execution of all the continuations that are executing `readEVar`. 
Whee! Thank you :-)
The problem is that roughly half of the instances we have for `MonadPlus` and `Alternative` today do not satisfy the oidification of the right-seminearring laws mentioned here. There are really two or more families of `MonadPlus` instances. There are those satisfying "left catch" and those with "left distribution" -- some satisfy both. `Maybe`, `IO`, and almost anything that doesn't use a list or other source of non-determinism fails these law and fails them badly. Most monad transformers preserve the "sense" of what `MonadPlus` law they inherit from the base monad, but then things like `EitherT e`, or variations on `ListT` change the meaning of `(&lt;|&gt;)` It gets worse when you move to `Applicative` vs. `Alternative`, because then things like [`Backwards`](http://hackage.haskell.org/package/transformers-0.4.3.0/docs/Control-Applicative-Backwards.html) have a right unit instead of a left one, and the extra structure afforded by `Applicative` makes the type enforced left unit law we had in `MonadPlus` becomes an arbitrary imposition, even less likely to hold as we can see with `Backwards`. =( So this is definitely the essence of something nice to have, it just isn't the essence of the `MonadPlus` or `Alternative` classes we have today.
ex (as in "export") - ez (as in "fez")
I'd ideally like an `EitherT` in `transformers` and an `ExceptT` in `errors`. Naming-wise that's the most logical. As Doug pointed out, it would avoid the confusion that caused this thread in the first place.
As I mentioned elsewhere, I'd ideally like an `EitherT` in `transformers` and an `ExceptT` in errors. Naming-wise that's the most logical. As Doug pointed out, it would avoid the confusion that caused this thread in the first place. 
Alas, that isn't the naming convention that Ross elected to go with. Now there really isn't a transition plan or a will to change that that would get us there, so we're stuck with what we've got.
I hope the linked exposition on MonadPlus / Alternative doesn't become a widely spread factoid, like Arrows = Categories + Applicative.
Why do you want a QUIC implementation? Are you hoping to use it in conjunction with a haskell web server library/framework?
Dude, OO is just the glorification of the first parameter of functions. Polymorphism is done by matching the first parameter, aka, the object. In Haskell, we typecheck and match all the parameters, not just the first one. It is like comparing a stone hammer to a gravity gun. It is not very fair to compare them. OO paradigm kinda stopped in the early 90's. Functional programming has been evolving non stop year after year.
Is this a spam post? Why linking to a blog summary instead of the original paper?
With "instruction videos" I mean YouTube videos that have someone show how to do something. I don't mean that in a belittling way, and my apologies if it comes over like that; English is my second language, I am Dutch, like Erik. I often have a hard time watching such videos; in general I prefer to read about something, probably also because I can control both speed and direction. So I was very pleasantly surprised how much I enjoyed Erik's fp101x videos. What helped me was the ability to speed videos up to 1.25x or 1.5x. And it's funny to hear someone talk English somewhat like how I do it :-). I am aware of who Erik is in the Haskell community, and most of his work I can't read yet. The course was a great experience, and I sincerely hope a Category Theory 101 is going to happen.
I thought GHC could optimize these
Really cool paper, thanks! So, did you submit your annotations/patches to the upstream packages? for example, knowing that bytestring is safe seems like it would be very valuable! BTW, in the interface with external interfaces (FFI, input, etc), are runtime tests generated for precondition about input, as in the recent work on gradual verification in coq?
I think the point is to not "overname" something. At least to me, calling it `x` and `xs` acts like a reminder that they are not important, and helps you focus on the "code pattern", e.g. compare: map f (x:xs) = f x : map f xs map function (currentItem:restOfList) = function currentItem : map function restOfList Some people tend to use short names when they don't, e.g. on class type variables: `Lens s t a b`. At least I think this very confusing when you are trying to learn those stuff.
The objection I took comes from an early statement in the paper: &gt; This paper provides a new algebraic understanding of the operations of the MonadPlus type class. &gt; We establish that both MonadPlus (Section 4) and Alternative (Section 5) are instances of this generalised notion, and we specialise the constructions for both cases. Both of these claims seem to be stating something very broad about the actual class we have today. It isn't until more or less a footnote at the end of section 6 that the paper even alludes to the fact that this isn't connection isn't about "the class we have today" but rather that &gt; A MonadPlus instance satisfying equations 6, 7, 8, 11, and 12 is an instance of a generalised near-semiring, and we call it a non-determinism monad. and then the old `MonadPlus` reform proposal is mentioned and the dirty laundry is aired at all. By then they've been talking about `MonadPlus` and `Alternative` unambiguously for several pages. Perhaps my issue is more with the order of the presentation. If you read it with sufficient care you may be able to tease out truth from the contents, but its written in such a way that I find it very hard to see that they are claiming this about a new class rather than describing the existing state of affairs in the existing class.
The terse style common in Haskell may start to make more sense to you as time goes on. (Or maybe not; people are different.) As I see it, when `x` is polymorphic, calling it `x` rather than giving it a real "name" like `head` helps to emphasize or clarify that there's no meaning to `x`; it's just some thing, and `xs` is the rest of the things. This style makes the most sense when the scope of `x` or whatever is very short, e.g. in the [implementations of the function in `Data.List`](http://hackage.haskell.org/package/base-4.8.0.0/docs/src/GHC-List.html). There, the only function of the variable names is to link the left-hand and right-hand sides of the equation, and you can see everything at once. In longer or more complicated definitions, where a variable is used several lines away from its definition, descriptive names serve a more important role reducing cognitive overhead. Likewise when the variable really has some extrinsic meaning, like `firstName` or what have you.
For me, the first step from translating from a recursive definition to a recursion scheme is writing separately what a single step of the recursion is doing. From there you can try to match that against an existing pattern.
That's usually my approach too. I just got stuck on the `local` part; I'm saddened that as far as I could come up with, the solution is to not use `Reader` there.
Yes, but in these cases where there is an obvious default value, there's also an obvious combining operation, in which case the `Alternative` or `Monoid` type class is more appropriate with `empty` or `mempty` as the default value. For the two examples you gave the `Alternative` type class suffices: class Alternative [] where empty = [] (&lt;|&gt;) = (++) class Alternative Maybe where empty = Nothing Nothing &lt;|&gt; x = x x &lt;|&gt; _ = x
JVM.
They get to it very clearly in the conclusion. As you well know, the correct order to read a paper is intro, citations, conclusion, related work, and then maybe the middle bits :-P
You mean JVM+JAVA Libraries ?
JVM+JAVA Libraries
I work for a startup where we use Scala. The reason is two-fold: - All the programmers came from the same company and already had experience with Scala. It was possible to introduce Scala in that company due to its great JVM interopability and its "learn as you go" transition from Java. - Scala has a much better record system than Haskell, and we tend to use a lot of records when programming. Most of all, Scala is a compromise that lets Java programmers be somewhat productive while they learn the full language. With Haskell, it's a much harder sell: "Can we have a few month of zero productivity, while we take some time to learn a language you haven't heard of, and which won't work with any of our existing code?"
I understand, but I think that it would be more fruitful to link directly to the original paper instead of redirecting the traffic to a summary blog.
It's quite unfortunate that virtually all examples of OCaml code google comes up with on top are repl code rather than OCaml code. Do you have some example code to look at that is representative of the syntax?
Now that we have a socket library that supports UDP I guess it would be possible to implement this protocol.
Scala is similar to the latest versions of Java, C#, and C++ in that they're imperative and mostly object oriented languages with constructs that fascilitate a somewhat functional style. I suspect answers are similar as to why Java is preferred over Haskell.
what is IRL ?
Here are two new ways to do it, in `main2` and `main3`. {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE TupleSections #-} import Data.Maybe (fromMaybe) import Control.Monad.Reader import Data.Functor.Compose import Data.Functor.Foldable gl = [ (0, [0, 1, 4]) , (1, [0, 2]) , (2, [3, 4]) , (3, [2, 1]) , (4, []) ] g x = fromMaybe [] $ lookup x gl echo x y = liftIO $ putStrLn $ x ++ show y dfs graph start = asks (elem start) &gt;&gt;= \b -&gt; if b then echo "Cycle detected involving: " start else do echo "Visiting: " start -- Use of `local` below -- vvvvv local (start:) $ do -- Explicit recursion below -- vvv mapM_ (dfs graph) $ graph start echo "Done with: " start main = runReaderT (dfs g 0) [] g2 :: ([Int], Int) -&gt; (Int, Maybe [([Int], Int)]) g2 (path, s) = if elem s path then (s, Nothing) else (s, Just $ map (s:path,) $ g s) f2 :: (Int, Maybe [IO ()]) -&gt; IO () f2 (start, foo) = case foo of Nothing -&gt; echo "Cycle detected involving: " start Just actions -&gt; do echo "Visiting: " start sequence_ actions echo "Done with: " start main2 = hylo (f2 . getCompose . getCompose) (Compose . Compose . g2) ([], 0) g3 :: Int -&gt; (Int, [Int]) g3 s = (s, g s) f3 :: (Int, [ReaderT [Int] IO ()]) -&gt; ReaderT [Int] IO () f3 (start, actions) = asks (elem start) &gt;&gt;= \b -&gt; if b then echo "Cycle detected involving: " start else do echo "Visiting: " start local (start:) $ sequence_ actions echo "Done with: " start main3 = runReaderT (hylo (f3 . getCompose) (Compose . g3) 0) [] I think `main2` is like close to a solution using `State`, while `main3` is closer to your solution, using `Reader`. As far as I can tell, they all produce the same output.
Yep, it is a bit surprising, but think that the expression is "overloaded" with two threads executing two different things. In each thread one side of the expression succeed and the other do not. Since there are two threads running, the second statement is executed two times with the two different results. Maybe this can be used for the simulation of quantum computing
For me there are several reasons: 1. The JVM has excellent *tooling*: IDEs, test automation, profiler, debugger, coverage analysis, static code analysis, several good build tools 2. You can find a *Java library* for nearly everything 3. Scala *mixes functional with imperative*: You can go back to stateful programming for performance reasons or for clarity if needed 4. The JVM is usually quite *fast* and you can tune it when needed (e.g. choose between different implementations for the garbage collector or even the whole VM) 5. For several important tasks there are proven and *well supported Scala solutions*: GUI programming, database access, actor framework, data crunching, machine learning 6. There is a *backup plan*: You can go back to plain Java when needed
Hopefully I'll get the joke in some time.. years maybe ;)
it's like readability-performance tradeoff is underlying every language.. one cannot remove it as though it was quantum uncertainty in position and momentum (I am generalizing though, I just know some python)
Think of the problem recursively. What are all the codes of length 1? Given all the codes of n-1, how do you create all the codes of n? I don't want to give too much away, but if this is too vague, I'll try to give a more detailed hint.
&gt; Scala mixes functional with imperative Don't want to spoil the party, but so does Haskell, and IMO Haskell does it better. Other points are perfectly valid though.
Okay, I guessed it must be something like that... similar to the `[]`-monad nondeterminism. Cool!
JVM interop. Businesses have a lot invested, and frankly there's more people that know it. We use clojure at work for that reason.
And many-many-many use Scala just as *a little nicer* Java. And that’s still counted as Scala!
In a Haskell sense, imperative programming is all about mixing implicit state changes. This cannot be done in Haskell because its type system forces you to separate various kinds of operations into various kinds of types. You need to lift things up and down. This is not necessarily a bad thing, but is also never a typical imperative programming. Using imperative style in Haskell is just as ugly as using functional style in imperative languages.
Maybe because they haven't seriously tried Haskell? The devs I know that have used both, Scala and Haskell prefer Haskell hands-down and mostly complain about Scala being so incoherent and unecessarily complex...
Your comment confuses me. 'network' has had UDP support, via rather raw bindings to traditional berkley sockets, for many many years, dating back to the beginning the packaging in Haskell ~2007.
So would it be any good to have something like `ghcjs` but insteadof targetting JS engines, targetting the JVM?
why then not Frege?
&gt; Today however, Haskell is the better choice. For some definitions of "better."
Because most people have no idea it exists and, accordingly, it lacks the necessary community for a lot of companies to take it seriously. 
Scala has better marketing. And it's similar enough to existing languages that it's not seen as a risk. The same can't be said for Haskell.
&gt; You don't need a special library to do that in every other language. I think you've missed the point there -- note that the line contains more than the just record field accessors. You cannot express with other languages' record accessors. Not to mention the fact that `lens` lets you do all this as *functional* updates (i.e: `Game -&gt; Game`, and not `Game -&gt; IO ()` as in other languages). Functional updates let you preserve referential transparency, get easy atomicity/transactionality of updates, parallelize easily, and avoid aliasing issues (easing reasoning). I think if you try to translate the examples I wrote (which highlight some of the simpler use cases of lens) you'll see these short *easy* snippets translate to multiple (often tedious) lines of code without lens (in OCaml as well).
OK... For the preponderance of definitions of better, Haskell is more likely than not, the correct choice, today, then Scala.
Go back to the hint. Think about how to approach the problem if you restrict the type of concatMap to this: concatMap :: (Code -&gt; [Code]) -&gt; [Code] -&gt; [Code]
Phil Wadler's presentation was also good: https://www.youtube.com/watch?v=FiflFiZ6pPI
http://www.urbandictionary.com/define.php?term=IRL
To stir up the discussion: I liked the part where he suggests rephrasing some common FP terms to things that have a more direct meaning pure function --&gt; stateless function easy to reason about --&gt; easy to refactor safe --&gt; reliable XYZ monad --&gt; use concrete names that don't mention the monad abstraction upfront
&gt; pure function --&gt; stateless function This at least doesn't make much sense. "Pure function" is a well-known term and is used even outside FP. Heck, you can even use it in C as gcc has \_\_attribute\_\_((pure)), and THAT language is certainly not functional! Trying to change something so established is bound to just confuse people.
Now on Stackage too: https://www.stackage.org/package/transformers-lift
I'm honored to find that this was posted by someone other than me :o
maybe! but that doesn't make `x:xs` the name of the whole
I don't think this point is really a big deal. But "stateless" reinforces the reason we care about pure functions to begin with: state is hard to reason about. When discussing Haskell with a non-funcitonal programmer, there's a tendency to bring up the phrase 'mathematical function'... as if that was any more enlightening to someone who wasn't already immersed in the culture. But the point that the functions are *stateless* is meaningful to programmers, even if they haven't developed a hesitation towards statefulness yet.
One module per record or prefix field names with the type name
&gt; stack Stack made it a pleasure. On Mac OS X (usually a challenge for stuff like this), I just had to make sure that it was GHC 7.8.4 that was in my path. Also, I think that last line should be: stack exec spanout
http://conal.net/blog/posts/the-c-language-is-purely-functional
Just give all your records' fields globally unique names
See point 2 in chrisdone's comment, the link explains things.
Examples? I feel like we could learn a lot more by presenting/dissecting examples and maybe even come to a consensus! :)
I'll also say that [Typesafe Activator](https://www.typesafe.com/get-started) is really awesome.
I don't think your assertion that state is *the reason* we care about purity is correct, either historically or in practice. As I understand it, the historical origins have more to do with laziness, which I suppose is "state" in a sort of abstract sense, but not really what you're talking about. And in practice, my concern at least is usually not about state per se but rather the unbounded range of "side effects" or rather *untracked semantic relationships* implied by `IO ()` or anything in a (straw-man) non-pure language. For instance, I've just written an installer in `nsis`, which is a perfectly admirable binding to a pretty ridiculous language. It's a thin-ish binding, though, so most of the semantics of the program are not captured in the types at all: you define the order of events in one place, and then define the various events somewhere else; in one place you say "make an uninstaller" and somewhere else you write the uninstaller. It's confusing not because it's stateful--which it isn't, actually--but because the pieces fit together in a manner completely divorced from their types.
&gt; If you are using a language that has the type safety property (sadly, many dialects of Haskell do not qualify for this distinction; ML and some intersections of Haskell features do), then anything you write in it is inherently type safe, so this is not a very strong claim. I'm kind of surprised to see you of all people write this. Of course you can internally to a module use non type safe parts of the language and still be observationally type safe. Indeed, by Rice's theorem, for every sound decidable type system there are modules which behave as if they were type safe but which do not type check. Also, Haskell is pretty close to type safe. Specifically, the newtype problem has finally been fixed.
&gt; what it means to "reason about" something What does it mean?
It's a bit strange to see Philip promoting quotation. Unless F#'s quotation is orders of magnitude better than Template Haskell that's just going to mess with composability and type safety. On the other hand, SQL fits neatly into the well-known `Applicative` abstraction (and [can be extended neatly to an `Arrow`](http://hackage.haskell.org/package/opaleye)) which really lets you manipulate your queries in a generic and first-class way. Promoting "it only issues one query if you check a few properties" also seems a bit of a stretch. Why not just define an `Expr` that can only issue one query?! Then you've got nothing to check.
I was planning on submitting one, but in the off chance I don't get time I figured I'd post it here.
&gt; Arrows = Categories + Applicative What's wrong with that?
Mostly just lacks the necessary marketing. If it had a Rich Hickey going to conferences it'd probably start catching on.
I agree that tutorials shouldn't attack readers with jargon from the beginning, but giving it up entirely seems to me like asking OO programmers to give up the word "object".
&gt; I often see an API described as "type safe" (which is nonsense!) Really? According to [the definition on Wikipedia](https://en.wikipedia.org/wiki/Type_safety#Definitions) type safety is relative to some semantics. Thus it seems perfectly reasonable to say that `head :: [a] -&gt; a` is not type safe with respect to a total semantics despite it being type safe with respect to the usual semantics which contains `_|_`. Correspondingly, I claim my [Opaleye](http://hackage.haskell.org/package/opaleye) API for composable Postgres queries is type safe in the sense that well-typed expressions built up from the Opaleye combinators always generate well-formed SQL. This is a much stronger claim than just saying it's "type safe" in the usual Haskell sense and I don't think it's a nonsense claim at all.
To make monads less scary, I like the idea of referring to them as "computational contexts", which is how they're called in much of the Idris documentation and seems like a good choice. Of course that's mostly orthogonal to Evan's main point that you should avoid presenting the general concept at first, but focus on the specific example at hand.
There's one using Scotty &amp; Persistent: https://github.com/jhedev/todobackend-scotty-persistent
You don't spoil the party, I am a big Haskell fan. In fact I have learnt Haskell before Scala even existed. But regarding state, Scala is easier (although much dirtier) than Haskell. In Scala I just change `val` to `var` and can work stateful. In Haskell I would need a state monad and probably some monad transformers (usually you can take IO as given in any reasonable complex program). Especially when you have a team with not so experienced developers, the Scala approach can help you to let your developers handle state better (you start to structure imperative code with functional constructs). In Haskell such a team would not be able to deliver anything at all.
I think the point is that you can use the same syntax as the host language to write SQL queries, as if the database was an ordinary data structure in your heap.
Spotting it is not so much of a problem... But deciding if it is a good idea to eliminate it is much more so !! There's plenty of case where you don't want CSE and GHC often has to assume you didn't do it yourself intentionally because doing otherwise might wreck the performance in a common case in your application !
It has been mentioned in a thread somewhere, but I want to have it on the top level as well. There is a [mostly-Haskell language on the JVM called Frege](https://github.com/Frege/frege). I think it should be part of this discussion as the JVM is mostly quoted as the main reason for using Scala over Haskell.
Then again, the function is called mappend.
I thing you're hugely overestimating how many people are familiar with the term.
There's also [Hell](https://github.com/chrisdone/hell), for an example of extremely thin embedding of shell into Haskell.
Not global, only in the module.
&gt; But the very fact that you can replace something with either Reader or State implies their connection. There are other ways to replace global variables in question, for example [reflection](http://hackage.haskell.org/package/reflection), or, well, `unsafePerformIO` + `IORefs`, which don't have much in common with Reader or State. I don't bring in every monad that there is by just mentioning the concept, just like I don't bring every single class with overloaded operators by saying "you can use the `&lt;&lt;` operator to send text to `cout` or some fstream". It's not like OO got worldwide acceptance by shying away from their jargon. It's perhaps unfortunate that Haskell-specific terms often do make newcomers think that they should learn category theory to understand them, while they're not really different from "method" or "object". 
In F# you have for loops to iterate over collections. This work translates those loops into SQL queries. Using applicative in F# would be very different. The analogous Haskell would be list comprehensions that translate into database queries.
Well, it should pass the tests provided by the todobackend guys, which you can find [here](http://www.todobackend.com/specs/index.html?http://todobackend-scotty.herokuapp.com/todos). I just wanted to add some Haskell specific tests on my own :)
the scala api for spark is much better than the python or java ones. 
&gt; The problem is that the beginner just wants to implement a web-server, a game or something fun and doesn't care about [abstract concept] X at all Not only beginners. People who are not (and don't want to be) Computer Scientists are much more interested in getting stuff done than in exploring the nethermost implications of a school of programming. At least in the first instance. Unfortunately, far too much introductory haskell material ends up seeming inside out and upside down and back to front for the working programmer who'd like to see if it's true that they can do better work in haskell. This is because that material tries to lead the reader down a path to enlightenment (with the agreeable side-effect that they might be able to write a useful program) rather than explaining how to *get stuff done*. I've found with many of the libraries that I've tried to use in my haskell experiments that the code I eventually end up with really is simple and clean and clear and all that—which is great!—but getting to that point is very frustrating. And this is because the tutorials (if there are any) and especially the reference material for the library leave me utterly bewildered and make library *seem* very hard to use. They create this impression by devoting far, far too much space to an exegesis of the intellectual adventure and subtle Apollonian beauty of the *implementation* and sort of assume that anyone who *gets it* will then see clearly how to apply the library to their problem. That's not the way it happens for us working stiffs. If anyone really wanted haskell to be “mainstream” they'd have to become comfortable with programmers who love the way that code written using applicative functors (for example) ends up but will learn how that works and why and what the relationship to monads is later. Maybe much later. Maybe never. For “mainstream” success, haskell advocates need to get comfortable with that. *Edit: typos* 
And that is the problem with `Monoid`, not the name of the type class.
&gt; The former one is already listed on the todobackend site. I wonder why there is no Haskell icon nor filter? I assume their absence it the reason the poster assumed there was no Haskell implementation.
What is the easiest way to build this version ? I would like to use it to build ghcjs improved base.
You'd better! We need those display forms to look nice! :)
I learned a lot of insight (as a beginner) from these lectures : https://www.youtube.com/watch?v=oLO34_5SMqU it really explains the most **fundamental** parts in great detail. (Only part of the lecture series is available on youtube, the rest has to be downloaded from the original website, given in the comments.) I personally think, that these lectures are really underrated. 
Someone asked regarding Elm on server-side, but I would be more interested to aks on Elm coordination with server-side Haskell: are there ways to share data-structures? Functions? What is the current state of interoperability? Thanks.
I think the general spirit of "let's not make things needlessly complex or obtuse for newcomers" is really good. OTOH, when I look at Elm, which is the result of Evan's philosophy, I'm more skeptical. Elm is just way way too limited a language, and those limitations are deliberate. Maybe that will help make Elm more mainstream. But then again, Javascript is pretty mainstream... so maybe that shouldn't be the goal. :)
This relativeness of the meaning of "type safe" is precisely why Evan thinks its preferable to use more direct expressions, such as "easier to refactor"
Why the fuck am I using Go?
It's not true.
Why is anyone?
That is a problem related with the use of a stack of monad transformers. I think that mt's are not a good idea for general programming scenarios and there are better alternatives
These are mere word play tricks in the wider situation which is that we are a community of programmers, and programmers on the whole have little history, reputation or incentive for being interested in thinking pedagogically or empathetically (as in many other fields). 
I disagree. Its more about replacing those technical terms in the introductory materials in order to make the language have a more intuitive user interface. Whenever you needlessly use terminology that the user doesn't know about, you tax their limited attention span (humans can't pay attention to more than one thing at once) and you present an opportunity for the user to give up and change his mind on what he was doing. For example, the "monad" term makes you ask yourself if you have to learn category theory before learning Haskell. Of course you shouldn't but by this point you already confused the user more than you need.
This is one of the reasons why I think Unicode designers will be the first against the wall when the revolution comes.
I've always wondered if that was what the `-XTransformListComp` and `-XMonadComprehension` extensions were all about, e.g. see https://ocharles.org.uk/blog/guest-posts/2014-12-07-list-comprehensions.html
&gt; `IO a` is essentially an effectful function (closure) with no arguments (or equivalently, an argument of type `()`, if the idea of a function with no arguments bothers you) I think this is getting off on the wrong foot, though, because what you're saying isn't actually even true at all. A value of type `IO a` is not a function. And it very clearly doesn't take any parameters of type `()`. Sure, if you ignore bottoms, there is an isomorphism from `IO a` to `() -&gt; IO a`, but that doesn't make them the same type. Better to say flat-out that a value of type `IO a` is an action that produces an `a`, and it's *not* a function.
Why not?
&gt; Once there, there is no reliable way download the latest executable. Do you mean that while it is easy for a human to click on the binary for the platform of their choice, you wouldn't be able to tell curl or wget to download the latest release because the filename is `v0.1.2.0/stack-0.1.2.0-x86_64-linux.gz`, not `latest/stack-latest-x86_64-linux.gz`? &gt; It'd be nice for it to be like homebrew, where you issue a single command and it figures it out for you. That command is a bash+ruby script. In your script, you can easily include a bit of logic to find the URL of the latest release using the [github api](https://developer.github.com/v3/repos/releases/#get-the-latest-release): $ curl -i "https://api.github.com/repos/commercialhaskell/stack/releases/latest" | grep '"browser_download_url": .*-x86_64-linux.gz"' | sed 's/^ *"browser_download_url": "\([^"]*\)".*$/\1/g' https://github.com/commercialhaskell/stack/releases/download/v0.1.2.0/stack-0.1.2.0-x86_64-linux.gz
There's an [issue on Github](https://github.com/commercialhaskell/stack/issues/532) related to this.
RWH is pretty good, a lot of the time they do just launch into showing how to do things with an illuminating example. But there is still that zealous tendency to say “and we can do $TASK that much more easily using a FooBar. Here's how FooBar is defined … and here's the rules that FooBar follows … and here's the standard implementation of FooBar … and having learned all that now we can easily do $TASK in a few fewer lines of code … but then we can abstract like this! … and then apply that new abstraction in these other cases! …” and $TASK gets a bit lost.
Also my concern, but i am not totally sure about the main differences between Elm and Haskell. Off the top of my head the main differences are: - lazy versus strict evaluation - typeclasses Is this all?
Type safety is a technical property of a language, *relative to that language's semantics*. Usually it is defined in terms of progress and preservation, but there are other options. I don't really agree that "type safe" is the term you want for describing this query thing. "Precise" or "fine-grained" or "correct by construction" all sound a bit better to my mind, but I'm not here to tell you what you can and cannot say. In the past, I had written a fine-grained very typed library for DynamoDB (not released), and I had been calling it "type safe" for reasons similar to those you bring up... But now, I regret it.
I hope it is not *too* surprising that I feel this way :) I think the Rice theorem corollary that you mention is a pretty strong indictment of decidable type systems... which partly explains my preference for semantic type systems like Nuprl's, which are based on a computational meaning explanation of Brouwerian truth, rather than on proof-theoretic derivability. My perspective is that a decidable type system is only adequate for *mere matters of grammar* (well, to be precise, it is exactly what you want for matters of grammar). For instance, Epigram's type system was a very elegant "grammar of derivations". Using `unsafeCoerce` as a kind of benign effect is of course possible, as you mention—in a semantic type system, it is not even necessary (another way to look at it is, this is what you *always* do when programming in Nuprl, since there is no type checker to subvert—you must always demonstrate well-typedness). Haskell is indeed "pretty close to type safe", whatever that means. In practice, you will never accidentally implement `unsafeCoerce`—that's not really the point though, since type safety is not a measure of your likelihood of getting into trouble, but rather a technical property which may hold (or not). One of the disadvantages of the proliferation of extensions that has made Haskell so much fun to play with is that statements about Haskell must be made relative to some slice of extensions. I think there is still some case where Haskell+typeable is type safe only in case there are no collision in MD5 (I can't remember the ticket, but it was an interesting thread!) So my perspective is that for "practical purposes", Haskell is basically type safe. But in an adversarial scenario, there have been numerous ways to implement `unsafeCoerce` over the years, using combinations of "locally reasonable" extensions. Anyway, I don't want to argue about Haskell being type safe—I just brought it up in my previous comment, because I'm being me.
Not having typeclasses is a pretty huge difference since they're so pervasive in Haskell. Add "no higher-kinded types" and "no higher-rank polymorphism" and you see that you can't abstract over Functors/Monads etc, which means every time you come up with a special purpose Monad you need to reimplement everything from Control.Applicative/Control.Monad etc that you need.
- Elm has extensible records with pretty nice notation - Haskell has do notation - Elm has FRP built-in - as /u/maxlepoo_ said, Elm doesn't have higher-kinded types (yet) - Haskell has all these fancy extensions - imports in Elm are qualified by default TBH, I'm much more excited about PureScript at the moment than Elm.
This has been my experience as well.
All we know, is that he's called ~~The Stig~~ PHP.
The word Monad is not fundamental to Haskell programming in the same way that objects are to object-oriented programming. Just read my [turtle tutorial](https://hackage.haskell.org/package/turtle-1.2.0/docs/Turtle-Tutorial.html) which teaches new Haskell programmers how to use IO without using the word Monad.
It *is* the same thing as a zero-argument or `()`-argument closure in any language that doesn't track side effects: `std::function&lt;T ()&gt;` in C++, for instance, or `unit -&gt; 'a` in ML; `IO ()` is also the same thing as the `Runnable` class that some languages have. In those languages you could write all of the same combinators for the zero-argument closure type as which Haskell provides for `IO a`. Sure, `IO a` is not *literally* a function... if by that we mean something like that the Haskell type `IO a` doesn't unify with the Haskell type `b -&gt; c`. But that's so obvious that it's scarcely worth mentioning. (And even then: `IO a` *is actually* implemented as an abstract newtype wrapper over a function in GHC.) But it *does* behave just like a zero-argument (or `()`-argument, again, whatever) effectful closure in every way except for being directly callable. &gt; Better to say flat-out that a value of type `IO a` is an action that produces an `a`, and it's *not* a function. Ah, but the challenge, when trying to explain a new thing to someone, is that you do, eventually, need to tie it back into something which they already know. (The brain is like a purely functional data structure, in this sense.) This is why the "a monad is just a monoid in the category of endofunctors, what's the problem?" joke has some bite. Here you've just generated a fresh variable: now you need to explain what "an action" is. I frequently encounter this sense that drawing analogies between things which are not precisely the same is dangerous, because the person on the receiving end might be lead astray by the difference. But it's rather seldom the case that a new thing is *precisely* the same as an old one. Establishing a way in which two things are the same, even if they are not the same in *all* ways, is the whole point of an analogy or a metaphor. In this instance, `IO a` in Haskell and zero-argument effectful closures in other languages are the same in the ways in which they can be manipulated, combined, and transformed, and different in that `IO a` in Haskell can't be called directly. (Which is also an *imprecise* claim that nonetheless gives a useful intuition, if we were to notice the existence of `unsafePerformIO`.)
Finally you reply to the code and not your wrong preconception of it. Now try to convert the code above to use map and filter and see for yourself.
I agree with you that Haskell's approach to IO is not a good one. But my perspective is that the solution you offer is not desirable (however, it may be all we can afford!). Turning stuff into "functional programming" by reducing the activity to generating codes (which will be interpreted by some backend into effects) is kind of a cop out, since it is only FP in the most boring sense (i.e. functionality/extensionality is preserved trivially because codes have the discrete/uninteresting equivalence). I wrote a bit about this in my post on functionality and non-determinism in type theory (I'd link, but I'm using a frustrating phone to type this). EDIT: [link here](http://www.jonmsterling.com/posts/2015-06-14-functionality-mutability-non-determinism.html) I spoke with Conal a while back about what IO should look like, and we both agreed that it is unclear---but one thing we know is that it should be direct and denotative. Writing out interaction trees that simulate imperative programs is neither of these things... In my opinion we should start from the standard tools of type theory and meaning explanations: computational justification and PER semantics. I think one thing to be inspired by is Conal's unamb operator, which is a very nice example of how direct use of a computational effect can be sensible: it is also a very clear demonstration that benignity of an effect can only be understood with respect to a type, since the equivalence differs at different types. What can we do to give a similar treatment to other use cases of IO?
I think Richard Bird's textbooks and Functional Pearls will be a great introduction for you. Also check out automatic differentiation and lazy power series. :-) You will probably have smooth sailing with Haskell, if my experience is any indication. The more "computer things" might come less natural to you -- installing libraries, building executables, writing files, working with the current directory, etc.
Both #haskell and #haskell-beginners are great resources. People there tend to be [very patient](https://gist.github.com/quchen/5280339), and the channels are very active compared to much of IRC. I'm consistently amazed at the patience and devotedness of bitemyapp, monochrom, and a few other users.
Much of the advice you're citing is right: you really don't need to know what those things are to make Haskell programs! Your mathematical background will help *a lot*, and you'll be much faster at recognizing the overarching patterns, but in the end, you might as well just go through the same resources recommended to everyone else. Skimp on the stuff that waxes lyrical about what a monoid is *abstractly*, and just read on about how to use them in *code*.
Can't help you here ...
&gt; Here you've just generated a fresh variable: now you need to explain what "an action" is. Indeed, that's absolutely necessary. Fortunately, an action is something most people - programmers or not - already have a good intuition for. An action is just something that can be *done*: reading a file, sending an email, creating a window, etc. Of course computers need to perform actions. Nothing confusing or scary about that. &gt; you do, eventually, need to tie it back into something which they already know. I'm deliberately rejecting "function" as the thing to relate it back to, for a reason. In some sense the *most* important thing to really learn about Haskell's computational model is that actions and functions are both types of first-class values, but they are not the same thing. Functions are not actions - which is a very important idea to internalize in a lazy language, unless you want to spend your life in a constant battle over evaluation order. The other side of the coin is that actions are not functions. If someone doesn't get that point, then they will forever be a bit uncomfortable with the whole model, and feel that it's unnecessarily complicated and arbitrary. Sort of like the elderly person who gets a smartphone, only uses it to make phone calls, and wonders what idiot made a phone that needs navigating through menus just to get to the buttons to make a call. If `IO b` should be thought of as a "function", what about Kliesli arrows, like `a -&gt; IO b`, which are now a mix of multiple kinds of so-called "functions"? I've seen people try to say it's some kind of "impure function" that's separate from normal "pure functions". So that's just awkward, and we're now telling people that there's some whole different parallel set of rules for understanding `IO` types. Yuck. When really, it's a very simple thing: a function whose result is an action, and you can see that by looking at the domain and range, and noting that the range is an `IO` type, so it's an action. I have noticed you seem to be referring to "closures" a lot. I'd like to understand what you're saying there, but your notion of a closure seems to be different from mine. As far as I can tell, closures (i.e., runtime data structures generated by the compiler to implement static nested scope) don't have much to do with `IO` types in particular. In applicative order languages (i.e., not Haskell), there's is a strong connection between closures and functions; but in Haskell, the analogous situation is that closures are associated with expressions, regardless their type. In any case, they are an implementation technique for compilers, and don't have much to do with the semantics of the language. Do you mean something different there?
I don't think it is possible to tackle both denotation and type safety at the same layer. I believe that denoation should be the sole responsibility of the backend, and type safety should be the sole responsibility of the front-end. When you conflate the two within the same layer you invariably end up incorporating unsafe operations into your language. Don't take that to mean that I disagree with you. I agree that the denotational half (i.e. the backend) is incredibly valuable, but I don't think you should mix the denotational layer with the type safety layer. I am just frustrated that it is 2015 and we don't have a single secure programming language because every single language makes this mistake and ends up with some sort of escape hatch that defeats any safety guarantees.
I've only seen this with Ruby's 'rvm' tool. In fact, I was just searching for something like this, because Debian Jessie comes with GHC 7.6, sadly enough.
QUIC is TCP, TLS and (currently SPDY but soon) HTTP/2 baked into one. I would very much advice against a pure Haskell implementation not only due to the large undertaking to make a correct implementation, but also since the spec keeps changing very quickly. Every 6 weeks you've got a new version of Chrome using a new version of QUIC. If you're interested, you can force Chrome to use QUIC whenever possible (some Google services) by enabling it here: chrome://flags/#enable-quic You can then see active connections here: chrome://net-internals/#quic
Hi, I was in your position a couple years ago. I was a mathematics PhD with very little programming experience. All I can say is that there are not a whole lot of resources for people with our kind of background. The most common assumption for beginning Haskell programmers is that they have experience with programming but will be terrified by monads. I started with Learn You a Haskell for Great Good which I loved because of the writing style and fun illustrations. I'd say I probably learned the most from reading blog posts and papers. Check out blogs by Edward Yang and Gabriel Gonzalez. But while these resources helped me to get "advanced" in some areas, I'm still pretty novice at things most programmers take for granted, the "computer things" as /u/apfelmus called it, and honestly it's extremely difficult to find good resources to learn that stuff. There are very good communities to participate in to learn Haskell including here on Reddit, on IRC and on Facebook.
Hi! I'm an applied math grad student at Caltech, and I've used Haskell for several years. I really like it. Dan Piponi has a blog post (http://blog.sigfpe.com/2006/01/eleven-reasons-to-use-haskell-as.html) which I highly recommend. Happy hacking!
[**@aisamanra**](https://twitter.com/aisamanra/) &gt; [2015-03-20 22:02 UTC](https://twitter.com/aisamanra/status/579040253169668096) &gt; I have made a wonderful discovery about the conjunction of Haskell and Unicode. &gt;[[Attached pic]](http://pbs.twimg.com/media/CAkqCsNVEAI5Hbg.png) [[Imgur rehost]](http://i.imgur.com/tQWKpq7.png) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
When I was first learning Haskell I had the exact opposite problem: everything seemed like it was written for a mathematical audience. For example, the first hit for free monad used to be [this article](https://wiki.haskell.org/Free_monad) which pointed to this article on [free structures](https://wiki.haskell.org/Free_structure).
I definitely understand why most learning materials are pointed the other way! Probably a good sign for the language that I'm having this problem rather than the other way. Your blog has been really helpful too. Thanks!
You're welcome! :)
How would this be harder then calling C from Haskell? The data moving over language boundries needs to be properly typed in both cases. I think there is some work by Frege to make it easy in simple cases.
Calling Java from Haskell like calling C from Haskell would be a big burden. It would be just like having to call into a C++ library through exported C interface -- you're losing functionality. With Java you have the OOP type system, with subtypes and runtime dispatching. Then you have the Java generics stuff, which (I don't really know, but would presume) probably does not cleanly map onto Haskell polymorphism. You need to somehow map all that onto Haskell to make working with Java convenient and accessible. That to me seems very hard to do (but NB. I'm not claiming it's impossible). But Scala can do it much more easily because it was designed to work with Java from the ground up.
The combining operation can just be record update syntax though. I guess it could also be lens operations.
I am very suspicious of the idea that Hask should be a boolean topos. Even in an idealized setting, functions to `Bool` have to be computable. But there are many subtypes whose membership is not computable. It is easy to imagine a dependent type theory that is really a topos (indeed, I am pretty sure at one point Coquand constructed a dependnet type theory which, when viewed as a category, is initial in the category of topoi). But then, your subobject classifier should look more like Coq's `Prop` rather than `Bool`.
Now that's a real mystery.
I see that solution as fairly hacky. 
&gt; I think that mt's are not a good idea for general programming scenarios and there are better alternatives Can you go into detail on this or present some examples? It was my understanding that monad transformers are the best way to use Haskell most of the time. Also, what are the better alternatives? Do you mean like Purescript's effects?
Hummm, I'm not 100% clean on what you're asking. I'm afraid I'm also not the best person to ask about performance in Haskell but I'll try and give some sort of answer. Personally, I would advise against rolling your own date representation because dates can be pretty complicated (leap years, leap seconds, etc, etc) and you surely have better things to do than spend time solving a hard problem that has already been solved. The most common way to represent dates and times in Haskell would probably be UTCTime. You can see the source code [here](https://hackage.haskell.org/package/time-1.4/docs/src/Data-Time-Clock-UTC.html#UTCTime). It's basically a tuple of day and time, which makes sense. If you're sure you just want the day part then we can see that [Day](https://hackage.haskell.org/package/time-1.4/docs/src/Data-Time-Calendar-Days.html#Day) is just an Integer under the hood. Integer is implemented with the GNU's GMP and I understand that GHC is pretty good at interacting with GMP performance wise. Performance wise, it would appear that Haskell's representation is a little more safe than the more typical single Int32 or Int64 unixtime, as there is a very high degree of precision available and dates will never roll over, but *might* be ever so slightly less performant because of integer overflow checking. I don't see it being a huge difference though. As for your question, which I'm not 100% clear on, I'm assuming that what you want to avoid is this: ``` struct { int day; int month; int year; } ``` In C that would be packed and would probably all end up loaded on the same CPU cache line. Everything would be contiguous if put in a Array of Vector&lt;&gt;. In Java, of course, each individual field is a primitive and so there is no pointer chasing but if you put that kind of object in a collection then it would be impossible to avoid pointer chasing to get the instance. But well, you can't put an int in an array, only an Integer, so you can't get away from pointer chasing even encoded as an int32. I wonder aloud what language was your library in. 
Sort of; you have to run in batch mode and specify a file to load and that always gave a segfault for me. Far and away from CL or Smalltalk where this is part of the development process and you can just take your current image and persist it safely and resume later. 
Sure easier with Scala, but then you also have the burden of a multi-paradigm language. To be honest, from what I know about Scala land, they don't like to break out into using Java much. They prefer to move the Java interactions to the "edges" of the application, much like in Haskell land we like to move IO and FFI to the edges.
Usually optimizing space is optimizing for speed, because memory accesses (cache misses) are much more expensive than most kinds of computation.
Sure, typeclasses make a big difference, but Evan says in the video that they are going to be added eventually. At that point, the two languages might become very similar
Are you particularly set on Vinyl? It's worth pointing out the extensiblity [isn't completely free](http://code.haskell.org/~aavogt/HList-benchmark/a.html), if you're not already aware of the associated costs. If you're just trying to solve more straight-forward record woe, perhaps the [records](http://hackage.haskell.org/package/record-preprocessor) library might be more suitable. You can find the introduction [here](https://nikita-volkov.github.io/record/).
I had a look at Purescript again, it is indeed very interesting! Too bad for the many small differences from Haskell. I guess that all those small things might drive a programmer mad, if he/she tries to go back and forth from Haskell to Purescript on a daily basis
I'm trying to move away from ``record``, or at least investigate the alternatives. I like Vinyl because it is very powerful, and seems like it will actually stick around. Already I have been burned by the change from ``record`` 0.3 to 0.4. What is the current state of Haskell libraries which solve the overloaded record fields' issue? Maybe that would have been a better question...
I tried it briefly and could not get the preprocessor to work as a simple dependency. Maybe I didn't try hard enough, but I don't want to have to tell the users of my library to first install another library before using mine -- I would like it if Cabal could manage all this itself.
Have you reported the issue?
Still: get it correct first. Optimize (on a low level) only if needed.
Why is this not all included in the library? That is, I want to use Vinyl to write applications, not to roll my own records' solutions. 
If you are worried about API churn, I would urge caution when using Vinyl (speaking as the author of Vinyl). I don't know of any big API change that we intend, but if you look at what we have done in the past, we had to make two very disruptive changes to the API before we arrived at the current one. Vinyl is intended to support use cases that are a bit more advanced than the standard "I need proper ML-style records in Haskell" requirement. I'm not familiar enough with Nikita's library to comment on it... With regard to your request for a simple example, I am sorry about the state of our tutorial. I do not currently have time devoted to working on my Haskell projects, since I am no longer being paid to do so—and I am in the middle of a very large endeavor (the [JonPRL proof assistant](http://jonprl.org/)) which demands all my free time. Vinyl will continue to be maintained for the foreseeable future (as long as anybody is using it) unless it is obviated.
Thanks! That approach is much more future-proof than my current strategy of guessing which kind of sandbox was used based on the installation path given by cabal's autogenerated Paths file. I wish cabal supported this as well.
That's a joy to read, thanks. I've always found the Haskell community on SO more patient and helpful than average, and certainly here on reddit too. 
I'll believe it when I see it.
What do you mean by unsafe? Do you mean non-benign?
By unsafe I mean anything other than pure evaluation. I view the sole purpose of the front-end language as type-checking and normalization and the sole purpose of the backend is interpretation. In other words, there needs to be a language separation between type checking and interpretation.
I name variables a, b, etc. and elements x, y, etc. This leads to this a@(x:xs)
This is surely what haskell feels like for beginners. I recommend watching the BBC's _In the Night Garden_ if you want to know what it's like to not understand many nouns. 
(Those links are to the hmatrix and hmatrix-gsl libraries.) 
I'm sure this is the most powerful and general way to do it. That's okay. The burden here is a problem which most of the most powerful libraries Haskell have: where is the simple example in the documentation which says "If you want to be immediately productive with this library, here's how to do it"? 
Thanks, Jon. I'll go back to the tutorial and see if I can whittle it down to something obvious. I appreciate your point about generality. 
When I notice haskell community supports emacs, https://www.youtube.com/watch?v=siwpn14IE7E
I don't follow. What are you trying to say?
The `vinyl` benchmarks seem to be using version 0.4.* (with `Universe`), which has a rather different implementation than the current series. In particular, `rLens` is now much easier for GHC to inline and monomorphize at compile-time. If you're using `vinyl` primarily for API purposes, e.g. as a substitue for tuples when returning or passing multiple values, the new typeclass-based lens accessors *may* allow GHC to inline away intermediate records entirely. *Edit*: Monomorphic `vinyl` records should be isomorphic to nested strict pairs, which in turn should be isomorphic (under optimization) to strict flat tuples. When I was last experimenting, back in 7.6, neither of those was quite true. I don't know if GHC has yet gotten wiser about this kind of GADT.
I definitely agree that blogs and papers were the most helpful; once I was past the very beginning stages of learning Haskell, I found dedicated resources like tutorials and Haskell books much less useful than seeing innovative ways that people used Haskell to solve problems in practice. The first blog you mention I can recommend as well, the second I do not know.
I had a strong mathematical background before I first used Haskell (although not grad school; I only knew the basics of category theory at the time). While I found the style of Haskell very mathematical, I don't feel that the mathematical knowledge I had benefited me when it came to learning Haskell. In comparison, my experience in programming in other languages was massively helpful. You might get some use out of looking for resources targeted at functional programming in general, instead of Haskell in specific. It sounds like your concern is less "what does this command do in Haskell" and more "how do I design my code well". Chris Okasaki's book Purely Functional Data Structures (written in ML, I think?) is superb, and while it's not exactly the kind of material you are looking for, it provides many examples of how to approach problems (especially data structure related problems...) in a "functional" way.
These days we can make use of lenses to hide the bit-twiddling in one place where we can afford the effort to get it right. That said, avoiding the effort until profiling shows it's worth it is the way to go, and Haskell is pretty friendly to refactoring when you need to.
The derivative of regular bookshelves is bookshelves of one-hole contexts.
Seems more like design issues.
I get your joke, but not the original one...
Don't tell them that Haskell is older than Java.
Oh, thanks! I totally missed the text on the shelf itself :)
What the actual fuck.
If anyone wants to submit a quick Pull Request to get that Servant+Persistent implementation listed I'd be very happy to merge it in. Doesn't need to be the original implementor; you just need to [make an edit to a YAML file](http://www.todobackend.com/contribute.html).
Even if your date structure fits in a cache line, bloating it means less of it fits. More useful stuff has to be evicted to make room for it. Loading multiple adjacent cache lines is usually much faster than random access (though the processor doesn't always have the ability to predict adjacent access), but it's still noticeably slower than one cache access. Bit operations needed to access bit fields are absolutely negligible compared to the downside on cache behavior.
Interesting, thanks for the info. I have worked on a lot of really tiny systems, video game consoles and Atmegas for example, where you constantly squish multiple datas into into bitfields to save space and time, so I do agree with you. However, on a modern computer I save my optimizations until there is an obvious issue, and I don't optimize every little thing for space at first writing. Edit: Trying to layout data contiguously in memory is somewhat more important to me these days than compacting stuff into bitfields.
I agree as well :) In a language like c, though doing these kinds of "premature optimizations" is so easy and cheap you just say why not and err on the side of more optimizations rather than less.
This was a helpful post; I wish I had it on hand when using conduit for a small project a few months back. The conduit/conduit-combinators distinction was definitely confusing.
Erlang's in there too -- and both are alphabetical with the other titles (ignoring the misplaced Camel book) so this was probably done by an employee.
It makes it easier, but in practice it doesn't happen very often. To reduce the chance of accidental bottoms, reflex-dom is structured so that it is *always* safe to use an FRP value recursively - Events, Behaviors, and Dynamics are always read lazily by the primitive reflex-dom widgets. Since these are the values that most frequently need to be used in a cyclic fashion, the end user generally won't run into trouble if they just write idiomatic code. Decoupling layout from binding is a bit tricky, because it makes it difficult to ensure that elements are placed once and only once. I'll be very interested to see what other approaches are possible as people explore this design space.
What don't you like about aeson and/or what are your inspirations for making this library?
I *really* like how straightforward this is, but I'm curious what the benchmarks look like if you only want some fraction of the source data. I have existing Aeson code where this is the case; the JSON coming from an API call (Algolia search) contains tons of info that's irrelevant to what I'm doing, and I probably discard 75% of it or more by simply ignoring it in `parseJSON`. Here I'd actually have to parse it all, and pick out the useful bits once it's native (though of course I could do that immediately, so the rest of my code doesn't have to change). Presumably the actual parsing operation is strict, to get raw JSON out of memory ASAP, right? So I guess it comes down to whether the compiler sees that the intermediate form is never used? And of course it might be negligible anyway. Plus I don't really care that much about performance in my application. It's just the **only** concern that comes to mind. ;) I hope to see this un-WIP'd soon!
Ah, OK. That's a pretty nonstandard use of the term "unsafe", but I see what you are saying. I don't really agree with your perspective on this, but thanks for explaining it!