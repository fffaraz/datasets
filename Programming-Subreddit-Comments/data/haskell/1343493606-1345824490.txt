I'm all for making Haskell more intuitive, but its complexity may be necessary. I believe that imperative style programming is more easily understood than functional and declarative programming.
The one linked from the word [This](http://i.imgur.com/1xWmB.png).
Oh, thanks! =)
I agree that partial functions should be avoided (maybe even banned). Then you have eliminated the source of crashes in pure code. And you can write large, useful programs that won't crash. However, other classes of programs must deal with exceptions: for these stack traces are a necessity, and for these programs "I can't think of when stack traces would have helped" is like "I can't think of when types would have helped". Shelly (scripting) operates in this realm of programs, and the lack of stack traces made Shelly a bad idea until I came up with detailed logging as a replacement for stack traces.
Almost makes me want to use emacs. Almost.
how does static vs dynamic linking make a difference?
&gt; Did the author even listen to it before uploading? he excuses for it in the description.. so yes. :)
Any chance you can share the source for fay's website? I'm sure all those jQuery and DOM manipulation helpers I see defined there would be very helpful for lots of people.
Golf and Haskell: both Scottish.
Maybe she doesn't want to learn any programming language? Would telneting to port 80 of a server teach her what she wants to know?
It's [in the repo.](https://github.com/chrisdone/fay/blob/master/docs/home.hs) :-)
Beautiful!
That's a good point, IO-based code can of course crash in lots of creative ways. I can usually figure out where it did (especially since the IO code is pretty high up in the call stack) but I can imagine that stack traces could be useful for heavy shell code. I seem to recall that Simon Marlow figured out how to do good stack traces. Did that lead to anything?
One-off tools and fast prototype development of math-y/test-y stuff. For example, I wrote a quick-n-dirty polynomial library and used it to verify that a couple different implementations of quaternion rotation were actually identical, so that we could safely use the more efficient one. In fact, my analysis showed they *weren't* identical, but the error term (the difference between the two) was zero if the quaternion was normalized, which it always was in our case.
I have a [patch](https://github.com/eli-frey/Cairo-Image-Surface-For-Data-for-Haskell) for Cairo to allow it to be hooked up to SDL (or any other interface that exposes a pixel buffer). It's been accepted upstream, but no word on when they're planning to release it yet. In any case, I was just getting ready to sit down today and see if I could use it to add a repl/UI^1 friendly backend to Diagrams. I'll talk more with Brent Yorgey about this at [Hack Phi](http://www.haskell.org/haskellwiki/Hac_%CF%86) I believe. ^1 I am aware of the GTK backend, but that just feels so heavy weight to me. I am fond of SDL.
I'm sure there's a laziness joke in there somewhere.
Have you used any of the bindings floating around like hquantlib? or hs-monte-carlo? I was toying around with a (simple) MC pricer and found it to be unbelievably easy to make something nice and quite fast, but I'd like to know what kind of rats nest you end up in when you start getting to more exotic contracts.
On OSX, SDL's Haskell bindings do not build well. GLFW-b and its Haskell bindings work well on Linux, OS X. As far as I know, they also work well on Windows although I've never tested that.
Cool, thanks for the tip. I'll have to look into GLFW-b some more. No audio support, but looks like quite a handy lib. The interface certainly looks cleaner than SDL (esp. for keyboard input). Allows access to the raw pixel data, so I should be able to hook it up to Cairo. [EDIT] or just hook cairo up to opengl, but I don't know how to do that in Haskell yet.
No, I haven't. First, it is questionable if these libraries are "production quality"; indeed skimming the docs they don't look very appealing (not like my code is, but at least I have control over it). Also I need much more than these packages provide. Second, "quite fast" is relative. I haven't even considered a Haskell solution for the performance critical part; instead I want to use Haskell as a meta-language and compiler, and scripting environment. I suspect even hand-written assembly by Intel engineers wouldn't be "fast enough" in the given situation. Pricing is one thing, but calibration is a whole other. So the goal is "as fast as possible with a reasonable effort", not just "quite fast" (I suspect there are orders of magnitudes between the two). Third, even the random number generators from GSL or Matlab are not really suitable: One thing is that the performance is not fully optimal; but a theoretical issue is that the resulting streams are not stable under continuous change of the parameters of the distributions (except simple distributions like normal and exponential). So I spent weeks just writing random number generators. But they behave better, and are also faster than Matlab (ok, I'm cheating since I generate floats and Matlab generates doubles, but still). I haven't compared with GSL. Fourth, you want automatic differentiation for efficient calibration. If you justs want simple MC pricers, even with moderately exotic contracts, Matlab works quite well, no need to go to uncharted territories.
I don't know how GLFW draws things, but the problem with SDL is mostly that it is extremely slow, since it draws everything in software. Using cairo with opengl is possible, but not straightforwardly so; there are various ways you can approach that, depending on what kind of features you need. cairo had various opengl rendering backends for a while now, but I don't think any of them are considered stable/official yet. I used glitz a while back, but I think that's considered deprecated now. Using cairo with an opengl backend is probably the best way to go for most cases.
You're missing the point. Processing does not run in the browser, and when OP said "processing style library" he certainly wasn't thinking about having it run in the browser. Processing also has nothing that either resembles or replaces HTML, AJAX, JSON, or CSS. It also does not have any kind of concept of a client/server, and it does not have a way to serialize or structure/create "documents" (of any shape or form) that are then displayed.
I already showed her the output of curl, but I think that is a great idea.
In pipes, the finalizer a frame begins with is reachable: you just close upstream before you await anything from it. The initial finalizer being empty (i.e. pass) reflects the fact that until you await from a pipe for the first time it has nothing to finalize because it has never gained control and had the opportunity to acquire anything.
I'm just pressing `S-j` switch window and `F5` in xmonad. Actually I did do [a code-reloading thing for CSS](http://github.com/chrisdone/bdo) which works from Emacs. It wouldn't be hard to do the same for JS. There is at least one Emacs library to do that in a generic way actually. [Here](http://www.emacswiki.org/emacs/SaveAndReloadBrowser) and [here](http://www.emacswiki.org/emacs/MozRepl).
I have not been using SDL for drawing, just getting a pixel buffer on the screan and then drawing directly to the pixel buffer with Cairo. Not sure how efficient that is either but I havn't had any problems yet.
That's fairly inefficient if you want to draw in realtime at good FPS, you probably want to keep the actual drawing part as much on the GPU as possible. Try it out on higher resolutions (like 1900x1200 or so) while using things like alpha transparency and such, if it works at decent FPS -- great.
Check again, because processing is nothing like HTML/JS/CSS. In your scheme of things, it could fit in as a drawing api on the low-end of the stack.
Come to think of it, all those Kenyan runners I know use C.
I use Haskell for econometric/statistical analysis and business planning/risk management.
Gloss already has interaction, it's very simple to use. See `Graphics.Gloss.{IO,Pure}.Game` (from `gloss`), and `Graphics.Gloss.Raster.{Array,Field}` (from `gloss-raster`).
My problem with existing RNGs was not just the speed but the instability in the parameters. Basically all nontrivial distributions use rejection sampling, and the rejection can depend on the parameters (it's not a problem for the normal distribution, but a problem for say gamma). You perturb the parameter, you get a completely different stream (with the same seed). This is acceptable for some purposes (after all, it is random :), but I'm not so happy about it. But speed can be improved too, using SSE (or even AVX). I use mostly the same algorithms as GSL (or Matlab) except vectorized, and the underlying uniform generator is also well established (but vectorized), so quality should be similar (also in tests it looks ok). &gt; You're planning on doing the AD part in haskell, then I assume? It's certainly been well established there. &gt; &gt; Or are you really trying to rewrite -fvia-c with SSE support and do the whole project that way? No*, and no. I'm planning to write my own (very specialized, and simple) compiler (in Haskell), which would include the support for AD. Less performance critical parts may be implemented in pure Haskell though. (\* well, it depends in the interpretation of the words "in haskell")
I recently generalized the interface that's used for gtk-toy, which is what I made to mess around with doing interactive diagrams https://github.com/mgsloan/toy-interface/blob/master/src/Graphics/UI/Toy.hs An SDL implementation of this would be great. See you at hac-phi!
Have you tried recently? After switching to 7.4.1 it all built out of the box on the first try.
Great! I'll queu this up on my list of "things to do."
Contemporary linguistic theory treats names as predicates indistinguishable from other noun phrases except perhaps by pragmatic restriction. Just FYI.
Here's a simple Client &amp; Server written using network-conduit. I used Conduit idioms in this code rather than Pipe idioms. [NetworkExtras](http://hpaste.org/72332) (to clear up the import cruft) [Client](http://hpaste.org/72334) (uses command line input/output) [Server](http://hpaste.org/72333) (hard-coded responses to certain messages) Put these all in the same directory, and open up two terminals. Compile and run the server in one terminal, and the Client in another. This code illustrates a few basic principles about client/server networking: * Whether you are a Client or a Server, you are dealing with a stream of incoming bytes and a stream of outgoing bytes. Clients typically send a request first and then receive a response, while Servers typically receive a request first and then send a response. Complicated networking tasks, such as an online videogame, are still nothing more than dealing with requests and responses, but do not necessarily adhere to this one-to-one request/response model. * The Client and Server must agree on a uniform encoding of messages. In this example, both requests and responses use utf8-encoded text. * There must be some way of knowing where one message ends and the next begins. In this example, both requests and responses use the newline character "\n" to signal the end of a message. * Once you get past encoding details, a server is typically nothing more than a direct mapping from request to response. I like this particular example because it takes practically zero programming experience to enhance the server. Try editing the Server code so that when it receives the request "Who's your favorite brother?" it responds with "Gabriel, of course!" If you are interested in this example, then let me know, and after I'm finished with my current series of blog posts, I'll write up a better tutorial that properly explains the code.
&gt; In your scheme of things, it could fit in as a drawing api on the low-end of the stack. Which is what I have been describing. The end goal of `[HTML, JS, CSS, BROWSER]` is the end goal of Processing, hence the quoted paragraph: &gt; 'an open source programming language and environment for people who want to create images, animations, and interactions'.
Can always compile Fay with GHCJS and run it directly in the browser. ;)
What would be cool is if Fay would use haskell-src-exts' XmlSyntax extension so you could write: do li &lt;- &lt;li/&gt; toc &lt;- &lt;div class="table-of-contents"&gt; &lt;p&gt;Table of Contents&lt;/p&gt; &lt;/div&gt; etc. This has the benefit of being syntax-checked at compile-time, and also makes syntax highlighting possible (I have that for Vim in my vim2hs project). The `trhsx` preprocessor from the `hsx` package could be used for making it valid GHC Haskell for sake of the type checking phase (just make the Fay monad an instance of the XMLGen class).
Thanks a lot! This is really helpful!
Indeed, I like it too. That's the approach I want to use for a process-monitoring tool ([sentry](https://github.com/noteed/sentry)).
I'm a long-time (well, as long as it gets with kindof new software!) luakit user, definitely interested in this as my new browser of choice though ...
Glancing at some of the code, I see an interesting comment scheme -- {{{ Imports import Foo ... import Bar -- ]}} Is there any specific reason for using triple curly braces? Documentation, or IDE support, perhaps?
[Code folding in Emacs](http://www.emacswiki.org/emacs/FoldingMode) can work like this. Also Vim, if I recall. It's a language-generic way of doing it.
That means pkg-config, and combined with Windows, that means pain.
Admittedly the first post was flippant, but you it's like you have deliberately misconstrued everything I said. You're flatscreen analogy is actually terrible (software/software vs software/hardware). SVG. Canvas. XML. DOM. Browser. JSON. Server. TCP. Gecko. Webkit. None of those live in isolation, and contribute together to achieving bit patterns on a screen. Which is essentially what Processing does too. I'll repeat: **IN NO WAY HAVE I SAID THAT THERE IS A ONE-TO-ONE MAPPING BETWEEN HTML AND A COMPETENT VISUAL PROGRAMMING LANGUAGE**. Reread that last sentence. A Haskell DSL for visual programming would be a marvellous step towards a renderer, then a renderer for common data types, then a client/server for common datatypes, then HTML disappears and you have type checked styling and data. Then you have the DSL as a fall back for canvas and SVG when the default visual constructs for common datatypes are not enough. Is this making sense yet? &gt; HTML/CSS/DOM aim to create a portable document format specification A minor correction (and remember I listed JS amongst others), they have been (ab)used way beyond that. Also see 'web' apps on various mobile devices. The 'browser' paradigm happened backwards. What was a limited feature set that allowed for easy visualisation of common data structures (strings, lists of strings, lists, two dimensional data structures (ie the table)), is being kludged into a fully animated interaction. It's all a bit silly. It would be nice for something a little better, haskell has the opportunity to do that integrating and introducing and designing a few things. Beyond the command line Haskell suffers because it hasn't had the opportunity to piggy back on another technology (for Javascript that was the 'browser'). The next programming language to be widely adopted will be whatever bigger backs on whatever To answer the condescension, no to SDL, 10 years of JS, HTML, browser quicks, xml shittery, god awful flash for a while, some Swing.
Cairo is basically impossible to compile on some widely used operating systems (like OSX. I suspect it is similar for Windows). SDL is probably a bit better, but I haven't tried. GTK is again impossible, though sometimes you can find binary builds. I would go for a light-weight approach. OpenGL + GLFW/GLUT/whatever to open a window and handle keyboard/mouse is quite portable.
I haven't looked at the code much, but she might check out http://hackage.haskell.org/package/acme-http. It's a toy web server that stepcut wrote specificaly as a hack of the pong benchmark. As I'm sure you're aware, modern robust production-level web servers like Snap/Warp are going to be much more complicated under the hood and probably not the type of code she's going to want to be looking at just yet.
Not nearly as much pain as e.g. calling Qt (or something else written in C++) from any other language.
My statistical needs are pretty modest. I'm using 'levmar' for non-linear regressions (typically polynomial regressions), 'statistics' for descriptive statistics, 'random-fu' for Bayesian stuff. 'levmar' + 'statistics' together are capable of doing parametric regressions to common probability distributions, but I haven't needed that yet.
Got to be honest most of the solutions on that haskell wiki are beyond my understanding right now. I have started looking at them and woosh right over my head. I've had a look at the illustration that eske has posted and I can see why it take's a long time to run for larger numbers. Is there a way of stepping through a the code so that I can see what is happening at each stage of code execution? Sorry if this is a really basic question.
That was a really good diagram, and I can see how the tree can rapidly expand and get to quite silly proportions with higher numbers. I was also thinking that with higher numbers that there maybe a limit to the value on the Int type so that when the GHC interpreter hits a certain value it just hangs? I am not sure if that is correct or not however?
At least from testing with Word32 it looks like Haskell treats integers correctly. (Int does some other things other the hood to enable unboxing, a technique in Haskell to reduce computational overhead, but other than that I think Int overflows are handled as wrapping around just like in C by default.)
That's good to know. At least it really is down to the computation of the list then rather than the type 'failing' to deal with the large number. 
Our system is built on type of easyVision (https://github.com/albertoruiz/easyVision), so much of it is already open source. We added arrow syntax this year, for example. At some point we will write a paper and release more of parsing code. Probably the majority of our code will be open source at some point.
That is brilliant thanks, and thanks for taking the time to provide such a response. I actually learnt from this. So building up the result of this 'backwards' as it were will result in not creating that massive tree view list as in my code but will just concentrate on creating a series of additions. hmm. It actually makes sense logically as well. If I was going to actually use a pen and paper to calculate fib 5 then I would start with 0 then add 1 then 0+1 then 0+1+1 and so on. I think because I was so hung up on recursive functions (they just seemed really powerful) I didn't think so much about what was actually happening with the function. Note to self, just because I can, and it appears simple doesn't mean that it's necessarily the best way of doing something. I've not come across the operator '!!', don't worry googled it as 'index of' in a list and it made sense. Looking at it as a list was also fairly easy. I like the way you can prove the original solution through looking at the creation of the list. It really is nice. 
I'm glad you liked it. :) I tried to provide as much of a calculational explanation as possible. It's not always possible to give such nice calculations, but sometimes it is, and I feel that it helps understanding a great deal. There's a whole literature on program calculation, actually. From what I hear, it can be incredibly useful as a teaching tool, and I take this as evidence that that's at least partially true. Also useful is giving explicit examples of evaluations, I feel. That's not as common as it should be. Also, I think the tree recursive version you gave can actually be transformed into this version with similar calculational techniques. :)
Why? I studied a lot of math in college. The first language I learned and used professionally was Perl. And I struggled for years to recover mathematical reasoning techniques Haskell makes dead simple. Computer science would be a lot different if it engineers hadn't taken over it from the mathematicians and logicians in the 60s and 70s.
If all you need is a comparison, just tell me what features you need and I will hand-write a Pipe implementation for you. The ony reason I haven't released it yet is that I'm coming up with an extensible framework for composing features.
My favorite memoized version for the infinite sequence: fibs :: [Integer] fibs = 1 : scanl (+) 1 fibs
This is great, thanks for creating these - I know it's never easy to do these and it's really helpful for folk that are new to haskell However, I found it a little disconcerting that, while there was an emphasis on 'correct', you still allowed partial functions to exist. This, in my opinion, leans fairly close to 'incorrect'.
Thanks for pointing that out. How would you make it better? I'd love to correct it next episode.
Great job. Since you're using youtube for video, have you considered allowing others to upload their videos to HaskellLive to create a one-stop spot for live haskell coding?
That makes a lot of sense. Thanks for clarifying. I agree that leaving readPiece as a partial function is dangerous and incorrect. It seems like the complexity of adding another Maybe Piece would bubble up all the way through readSquare into readBoard, however, where we would lose the pretty inversion of `(readBoard . showBoard) = id`. I would prefer either failing fast when trying to read an invalid piece or being maximally accepting of input (by using a sane default), but clearly letting Haskell error out on the pattern match is not the correct way to go about it. Edit: Now I see `readSquareDef_`, which coerces Nothing to Empty, making the behavior from `readSquareDef_` on up to readBoard the same as adding `readPiece _ = Nothing` but with the added value of offering a richer set of type constructors for Square as well. I think that's what I'll go with. That would also differentiate between Nothing the empty piece and Nothing the error in reading a piece character, which will be necessary later on so that we can fail during IO. Thanks very much!
The problem isn't so much the libraries except for the fact that GUI toolkits have large APIs. The problem is the non standardized name mangling in the C++ FFI. This means maintaining a binding in another language is a huge amount of busy work in addition to fixing the mismatch in programming styles on top of the low level bindings.
Upvoted for actually providing an explanation and an alternative algorithm.
After skimming through HDBC-mysql code on github I discovered that it uses both mysql_stmt_prepare and mysql_stmt_execute as means to implementing HDBC's Connection "prepare" function and Statement's "execute" function.. I wanted to contribute but it seems like it's already resolved.. Please correct me if I'm wrong. (newStatement using native mysql_stmt_init: https://github.com/bos/hdbc-mysql/blob/master/Database/HDBC/MySQL/Connection.hsc#L260 ) ( execute using native mysql_stmt_execute: https://github.com/bos/hdbc-mysql/blob/master/Database/HDBC/MySQL/Connection.hsc#L332 )
Awesome video, looking forward for more. Is there any chance, that you add RSS feed to your site?
fromJust . readBoard . showBoard = id
Seems like: linux + xmonad + vim + ghci in xterm
I don't suppose you're planning an `mtl-free` package yourself? Are your reasons for avoiding mtl here the same as always, or are there specific concerns for Free monads relating to mtl?
Could you share your configuration of guard? I thought the feedback was very interesting. Also would be nice to have a quick explanation about whats happening in QuickCheck and Hunit
I'd rather say: os x + iTerm + tmux + vim + ghci
I think that this video has done more to make me finally migrate from emacs than any other I've seen so far.
Yeah, some of those bindings are really incredible @.@ I totally agree with this. I feel like it would have taken me much longer to get this done in a different text editor. Very cool.
Vi is a great editor, but there is nothing ReinH did in this video that stock emacs 23 or 24 does not do out of the box. The only thing that is slightly more natural in vim at default is rectangle editing, and that's mostly because the default emacs key binding is a bit obscure. Tmux was fancier than vim here. It's funny/sad that Mac OSX window management is so hostile to developers now that many are opting out of it entirely.
Could you give a quick tutorial on how to set up your workspace like you did?
Emacs has some wonderful tools for Haskell. I'm just a vim guy. :) I use OS X as a nice GUI and a way to run Photoshop etc. I spend most of my dev time in the terminal, just as I did on linux. Edit: Actually, you would need emacs haskell minor mode to do some of the things I did, like automatic type annotations. That said, there isn't anything vim + vim haskell mode can do that the equivalent emacs + emacs haskell mode can't do. In fact, there are some things (like a REPL buffer with auto-loading) that emacs can do that vim can't do. I'm just way better at vim than I am at emacs.
fromJust brings us back to partial function land though and is generally a bad idea.
Yes.
Yes.
Yes.
Hm, I thought the issues with control-monad-free were specific to that package, not specific to free monads with functional dependencies?
You could also just change your spec (for this lesson) such that any character that is not explicitly a piece is a space. This would be good anyways, since I think " " is a lousy character for an empty board space as opposed to ".". In any case, this seems like a perfect place to seque later to discussing Either, and how using Either as a monad allows for error reporting in a flexible way.
I dug back through my comment history to find the specific post where I originally raised the issue here: http://www.reddit.com/r/haskell/comments/sarkz/four_tips_for_new_haskell_programmers/c4cltmy Apparently I mistakenly attributed it to functional dependencies but now I realize from your comment that `control-monad-free` does not use functional dependencies, so perhaps somebody else can shed light on what caused that issue.
I'll do a video on this but: OS X + iTerm + tmux + vim + vim haskell mode + ghci + guard. Guard config is in the source or look around for my other comment or wait a couple days for the video. :)
I see. Most of critical generated code for me is just dealing with memory layout... the bulk of the math code is in C and is not (currently) vectorized. I'm hoping to try out the autovectorizer work that is going in to LLVM 3.2 at some point soon. We're using LLVM 3.0, which does work with a few small patches to the llvm package. Some of these may be merged into bos's tree. There are a few different branches being maintained by various people on Github. I have a somewhat complicated type system as well and did end up introducing something akin to unsafeCoerce to make it simpler to emit getelementptr instructions for types that are defined at runtime. It's not strictly required, but does remove a few hoops from our compiler. The patch is available here: https://github.com/alphaHeavy/llvm/commit/b7a95a6b83528478e09817bff5dc62965765fef6 Sounds interesting though, what sort modeling are you doing?
I don't like Maybe too much unless the error you're "reporting" is strictly "there is nothing like that to be found". I'm happy with it in map lookups (for example). For anything more complex (like this), I'm much happier with an Either, or a custom Maybe-like type with better names (like Square' above).
I also liked it a lot. I would, however, suggest using a larger font size. There is a lot of unused, "dead" space in the video and you could fill this using a bigger font, making the video readable in the "large player" view of youtube. You wouldn't be able to fit as many lines of code but I don't think that should be a problem.
As a vim user who manually runs his code in a separate terminal, I thank you.
explained here : https://github.com/k0ral/hbro/blob/master/README.rst
Your colorscheme is an eyesore. Solarized's light scheme is world's better than its dark counterpart IMO. Also take a look at the [Tomorrow-Night-Eighties](https://github.com/chriskempson/tomorrow-theme) theme for something better that even matches the colors on your web site and text transitions.
&gt; A Python-style generator is a lot about interleaving of effects. So "yield" needs to interleave the effects of the recipient of the yield value at that position. All `FreeT`-based data types interleave effects, on both ends. The example I gave interleaves both `print` statements on the recipient's end with `getLine` statements on the generator's end. I think it would help more if you gave me a concrete example of something you believe cannot be done with the generator example I gave.
I do think that Reddy is unfairly painting functional semanticists by confusing the notion of "value" as "the result of evaluating an expression" (i.e. What Conal calls the "in-system denotation") with the notion of "value" as (just, or ultimately) a reference for the abstract (i.e. Platonic, or mathematical) denotation. I don't think that any functional semanticist would have a problem, if pressed, with making this distinction precise, while I suspect they'll denounce it as unnecessarily prolix.
On a tangent, I was thinking about this and the behavior of readPiece is semantically the same as looking it up from a Map Char Piece (a map from a finite non-duplicated set of Chars to Pieces). Would you be happier if it were simply renamed to lookupPiece instead?
I've known for a while that the definition people typically give for referential transparency is wrong. Wikipedia is wrong about it. Etc. But I don't have the energy to go around correcting it everywhere. How many people that _do research_ on semantics of functional languages make the same mistake? Lots of people who are interested in functional programming as a hobby or living make the mistake (and they're the ones who wrote the wikipedia article, probably), but the corresponding population of imperative programmers probably haven't even heard the term 'referential transparency' at all.
I agree higher contrast would help for video (while I do prefer lower contrast for prolonged work). I'm revamping the presentation to be easier to read and etc so I'll experiment with this very pretty colorscheme. Thanks!
I used that tool to make a qr code to my blog with the haskell logo embedded which i used as my avatar for a while
But this is just to describe a property, not to put into production code.
That looks great. I picked the Bright version for maximum contrast but I had to add a few customizations specific to Haskell syntax tokens. I'll contribute those back upstream ASAP. Looking forward to showing my new sexy theme off next video!
I like the cut of your jib. I'll be introducing QuickCheck in the next video or two so this is very timely as well. Cheers!
&gt; Thus I find it hard to understand what he means by 'Functional programmers seem to believe that these "values" exist within the programming language, not outside.'. There is a fundamental (yes, fundamental) difference between what Reddy and other semanticists (including the theoretical computer scientists he cites) mean by "value" and what programmers mean by "value". To avoid confusion herein, let's switch to using the technical term common in philosophy/semantics: denotatum. With few exceptions denotata are, of necessity, things which exist outside of language. If I talk about Bob, I can't say Bob. I can utter the word "Bob", but I cannot utter Bob himself. Bob himself is not a sound wave and so cannot be vocalized. Bob himself is not a linguistic object and so cannot occur within a sentence. Bob himself is the denotatum. This same thing occurs in programming languages. We **cannot** write a *function*. Functions are not writable things; they are mathematical objects, which in turn are wholly abstract and exist outside of linguistic media. We can certainly write down *expressions* which are understood to denote a given function (or a given algorithm); but functions themselves are unwritable, just as the formal concept of unity is entirely divorced from the symbol "1" often used to denote it. The expression `\s -&gt; (f s, g s)` is very well called a "value". It's an expression in head normal form, which is what "value" is typically taken to mean in lambda-calculus. In particular, look into proofs of strong normalization and progress, which are often quite explicit about "value" meaning a particular subclass of expressions. That expression, however, is not a denotatum of lambda-calculus. The denotatum of that expression is a function--- which function exactly depends on what `f` and `g` denote. The only way in which that expression could be a denotatum is if we move outside of Haskell and move to a new language which we use to manipulate the linguistic expressions of Haskell. In this new language, the syntax of Haskell is taken as the domain of discourse, and so an expression in this new language is allowed to denote a syntactic expression of Haskell. Now, it's certainly possible to create meta-interpreters and therefore to seemingly use a language A in order to manipulate the syntactic expressions of the language A. However, there are two things to note here. First is that this is not the normal use of language. Therefore, this is not what people are normally talking about when discussing RT or functional programming or whathaveyou. Second is that once we start doing this sort of thing then we enter into a world where we have to worry about paradoxes of various sorts. This is why, in practice, it is exceptional to actually have a language that talks about itself. Instead, we usually have a ramified hierarchy where version 1 of language A is used in order to talk about version 2 of language A (which in turn may be used to talk about version 3...). Values and denotata are very different notions (even if the denotata of "values" and "denotata" happen to coincide in some particular instance). It is unfortunate that Reddy used the former term to discuss the latter concept, as this causes a great deal of confusion. And Reddy's misunderstanding of the significance of monads is just as deep as others' confusions between values and denotata. Indeed, I'd say that it is much the same variety of confusion, since the point of monads is to distinguish between talking about the names of things vs talking about the things named.
As a linguist, I'm well aware of that. However, we're not discussing linguistics here; we're discussing analytic philosophy, which has a rather different take on the matter as it is tackling different issues.
THE MORE YOU KNOW ░░░░░░░░░░░░░▒▒☆ 
[How does this look](https://img.skitch.com/20120801-ca639dcpxf7u4waggbjj1dgbpx.jpg)?
The mtl maintainers will likely pull in whichever implementation makes it into transformers. After the dust settles with this push to integrate with transformers, we should end up with just one blessed Free transformer that "does it all".
Link to where you've explained it before?
Monomorpishm, yo.
This is `fib`, not `genericFib`. :p
Yeah, it appears that all expressions are RT according to his definition. It seems this is [Conal's interpretation](https://twitter.com/conal/status/230477624231743488) also. Maybe some of the disconnect could be resolved by not allowing arbitrary denotations which can float free of the types of the program, which honestly, I think is rather strange anyway. For instance, in `int y = x++`, we cannot assign `x++` a denotation of a state transition function... since the context assumes it to denote an integer. With this constraint, I wonder if the two definitions end up being equivalent.
You had the right cause, but in reverse. The problem is precisely that they _don't_ use functional dependencies. Their m doesn't determine f, so you wind up in the MPTC refinement hell that fundeps were invented to help solve. If you want a case study in how MPTCs without fundeps really ruin your day, try to use the classy prelude.
As the mtl maintainer, I confess, I doubt free will ever merge into transformers, it doesn't seem like the kind of type that Ross has an intuition for, there are a ton of bikesheddable points in its design, and you need rank-2 types for the improve combinator that motivates the class in the first place. This is why it is sitting off to the side in 'free'.
That makes sense. Thanks for the explanation.
As a programmer, I care about functions to not have hidden effects so that I can read the transitive closure of a piece of code part of a large code base and be confident it does what it claims it does. In a pure functional language the result of a function does not depend on the context, so taking the transitive closure is possible, whereas in an imperative language the result of a function depends on the context, so taking the transitive closure of the function is mathematically incorrect. Uday attempts to wave away this critical distinction by claiming that an imperative language equipped with a complete denotational semantics includes, by necessity, the context in the function denotation. That is simply not true. No imperative program/programmer explicitly expresses the full denotational invariants including the context, mainly because that would be an onerous task of gargantuan proportions. Therefore I cannot read the transitive closure of an imperative function in confidence, therefore the problem of incrementally understanding a large imperative code base is fundamentally intractable. Whether we call the property of having the denotation of a function completely described by the transitive closure of its code "referential transparency" or something else it's a matter of nit-picking. 
###Monad Transformers is innocent! I'll be showing myself out.
just now: "I did not claim that we have a priori names for everything" a few posts ago: "in natural language, the names of things are in principle already given" I'll concede there are differences between thinking about natural language and thinking about logics and programming languages. But I think the traditions are far more intertwined than you (apparently) suspect, and the lineage Reddy gives on the SO thread is right on. Carnap and Quine spent more time with natural language, I'll grant, and I'm less familiar with. But when we're doing PL theory, we're *constantly* going back to Frege and Russell, if not directly, then through those that followed them. And I'd contest the idea that Quine (as Reddy presents, at least, I'm still not claiming deep knowledge and careful reading myself) was talking about "how it is that names denote" not because he wasn't doing "how" instead of "what" but because he wasn't doing "names" (as given things) but expressions, and perhaps because even more than "how" he was still stuck on what it "means" to denote at all. You can read these guys just on natural language, or you can read them on semantics, and depending on what you want to get out of them and what sections of the text you focus on, you'll come away with two very different sets of ideas. This is a broader thing I run into all the time -- the notions of disciplinary boundaries and concerns that we have today don't map onto the past in a neat fashion, and the further back we go, the more we are compelled to reimagine what it meant to be a scholar in a different context of knowledge (and in a world where people imagined a different shape of knowledge). But on the plus side, this process of going back and re-envisioning disciplinary boundaries can inspire us as well, and get us asking questions that the current set-up doesn't typically provide spaces for.
As I understood them, Uday's remarks are mainly about the way many functional programmers apply the notion of RT to *imperative* languages (not to Haskell) where what we nowadays typically call a "value" is only a tiny part of the denotation. As he pointed out, giving referentially transparent semantics to imperative programming (previously lacking) was Strachey's main interest.
"It is a tautological property of any compositional semantics." Precisely! But when Strachey was first spelling out how to give a compositional semantics, he leaned on referential transparency to get there. Now, it's so ingrained in us that semantics must possess RT that it's hard to see how this wasn't always something that people understood, or even understood how to say. The latter half of your post, I think, does an injustice to how much we actually *can* understand fragments of imperative programs. Remember, that's been the focus of much of the U.S. side of PLT for years now. We just hear about it less in the FP world.
Yes, that makes sense. I'm familiar with that 'encoding'. I think Reddy's notion of RT can be reconciled with the more colloquial FP definition. Let me try again. Consider the `++` context. Is `++ ?` a referentially transparent context? By Reddy's definition, it is referentially transparent if we can substitute `?` for something that refers to / denotes the same object and obtain something with equivalent meaning. Given `int x = 42`, assume `42` and `x` have the same denotation. Since `++x` and `++42` have different meanings, we conclude `++ ?` is not RT. Now, you might argue with the statement that `x` and `42` ought to have the same denotation. It is certainly possible to "define away" this problem - we can "lift" everything to the level of state transition functions and state accessors and claim that statements like `int x = blah` are actually sequencing state transitions rather than saying anything about the relative denotations of `x` and `blah`, etc, but one can argue this is rather silly, and it throws out a very useful distinction. Anyway, I don't really care about how the term RT is defined, or whether we use some other term, but I think the colloquial meaning makes a useful distinction, it is precise enough and commonly accepted. Taking a step back, I honestly feel these sorts of discussions are just pointless wanking that do little to advance the field. Granted, it can be fun sometimes, but let's call it what it is - arguing over definitions. There are lots of real problems to solve, so honestly, who cares what we call these things? Does it really matter if we've co-opted a term from analytic philosophy and are using it in a slightly different way than when it was originally introduced? (And I have not read through all the literature to even know whether I agree with Reddy's interpretations) As long as we agree on the definitions for the terms being used, that's what's important!
The monad laws holding are very useful (The ability to factor out IO subprograms into their own "do" blocks, the ability to use "fmap" and "liftM" in place of binds without worrying about changing semantics). Just not as useful as the properties we can get with more denotative programming.
Close examination of the difference between x and 42 gives rise to L- and R-values. 
Yes, you can make that distinction... however doesn't this feel like a rather formal distinction that has little to do with programmers actually reason about their programs? I don't find it very satisfying. When a programmer sees `int x = 42`, he generally expects `x` and `42` to mean the same thing, hence the `=`, and reasoning in terms of the substitution model is pervasive and practically unconscious. Even in imperative languages, people will locally treat the `=` as equality without even thinking about it. If we lift everything up to the level of state transitions and state accessors, talk about L and R-values, etc, it's "cheating" in a way - you are resolving the apparent difference in meaning (which would violate RT) by basically getting rid all equations one might use for reasoning about programs. (Except for some simple, not very useful equations like what Conal mentioned earlier.)
Will do. We can always handle the resulting pollution later if and when the problem gets bad enough. Lazy is not a dirty word around here :) Note that until there is a way to hide those, I'm sure I will still be self conscious about this pollution and will probably restrict the number of such libraries. Do you think this should be explained in [Checking and uploading packages](http://hackage.haskell.org/packages/upload.html) ? Also that document says "A Maintainer value of none indicates that the package is not supported." Should I use none in this case ? 
You know, I think Uday's talk of values actually gets him to the linguists and philosophers notion of referential transparency. Programmers don't just think that say expression can be replaced by its normalized form (i.e. value), but that a normalized form can be replaced by anything that normalizes to it. That means that any expression can be replaced with any other expression as long as both normalize to the same thing, thus we get the philosophical/linguistic notion of linguistic transparency. The only real difference is that philosophers and linguists don't imagine there being normal forms that govern the substitutions, but rather denotata.
I think that means more that the package has been abandoned. However there are no hard and fast rules for how to use those fields. A lot of them exist only to allow automated tools to parse them.
Even given `int x = 42` (in an imperative language), it is not the case that `x` and `42` have the same denotation. If you think it does, then you'll land at the common (mis)understanding of RT that Uday was describing. 
I note that you don't see value in "these sorts of discussions", so you might be puzzled about what value others do see in them. I imagine you're not alone in this puzzlement, so I'll offer my personal answer for what it's worth. I've witnessed and participated in many fruitless discussions comparing the strengths of different programming paradigms. AFAICT, a consistent obstacle to understanding &amp; resolving perspectives has been sloppy language, including use of terms like "referential transparency". Since we think in language, especially when thinking collectively, sloppy language not only reflects but also reinforces sloppy thinking. For some questions sloppy thinking/language suffice, but for some it does not, including the programming language discussions I mentioned. In those cases, I'm motivated to work harder and encourage others to work harder to dispel the confusions that hide out in common casual use of language. Not everyone hits this point of motivation at the same time, and those who haven't yet are puzzled (at best) about why someone suddenly wants to take care with language that had commonly gone unchallenged. Sadly, what I often see is not even puzzlement over a point but assertions that there is no point (and hence it's not worth asking). David R. MacIver mentions this dynamic of negative reaction to increasing carefulness in his post [*A problem of language*](http://www.drmaciver.com/2009/05/a-problem-of-language/): &gt; Of course, once you start defining the term people will start arguing about the definitions. This is pretty tedious, I know. But as tedious as arguing about definitions is, it can’t hold a candle to arguing without definitions. 
Of course I agree with your central point about the importance of clarity in our definitions for any productive discussion. On the other hand I can also see why some people would be of the view that the RT understood by FPers has been established as the 'canonical' one for computer science and attempts to destabilize it by going back to the 'classical' literature should be viewed with suspicion. I remain open minded on the issue; the definition that has been used by sharp FPers is based on a great deal of 'classical' material (the lambda calculus, etc.) and can't just be dismissed as the casual confusions of some FP hackers that couldn't be bothered to look up a few definitions. But I accept it might be too narrow and I am interested to see what could be done with a wider definition.
Thanks for the feedback, Paul. You're right that I was placing you in the happy-sloppy bin (again, apparently). I think I'd be much happier to drop the quibbling if indeed there was a single precise definition that we could refer to here. I haven't seen yet, and I suspect there are instead a few different murky ones. I'm seconding Uday's remarks below in inviting a rigorous definition.
Here' my personal motivation for challenging Haskellers' use of RT: I keep hearing the claim that RT is a property that Haskell has and other languages don't, even for Haskell IO, which lacks a (nontrivial) denotational semantics. By tilting the discussion away from the fuzzily used "referential transparency" to something precise, accurate and useful, I hope to clarify where we are in the journey from imperative to "genuinely functional"/"denotative" programming (both Peter Landin's terms). I hope to dispel the notion that we're already there for Haskell, exactly so that we can get back on task. I've written a bit on this topic on my blog, e.g., [*Is Haskell a purely functional language?*](http://conal.net/blog/posts/is-haskell-a-purely-functional-language). 
is there a tool to see documentations from command line too?
If you refer to the complete documentation of a module, I'm not aware of any. But with hoogle from command line I am very pleased to see just the doc of any function/type.
Can we have some more explanation of what you mean here? What do you mean by the "transitive closure" of a piece of code? By the way, somebody commented on the StackOverflow page that the naive functional programmer seems to use syntactic reasoning while the naive imperative programmer seems to use operational reasoning. I think there is a great deal of truth to that, and I don't think that is all that naive either. Imperative programmers, by and large, don't shuffle code around, either on paper or mentally. They let the code stay fixed and trace through it. So, the mental denotations they build up are in terms of traces of actions.
Not exactly. Take the number 3. What is its type? Is it an Int or a Real? Perhaps a complex number? None of this is clear by the value alone. Don't be mistaken by the fact that just typing "3" at a ghci prompt provides a type. "3" is an expression in Haskell.
"You are sad", the Knight said in an anxious tone: "let me sing you a song to comfort you." "Is it very long?" Alice asked, for she had heard a good deal of poetry that day. "It's long," said the Knight, "but it's very, very beautiful. Everybody that hears me sing it - either it brings the tears into their eyes, or else -" "Or else what?" said Alice, for the Knight had made a sudden pause. "Or else it doesn't, you know. The name of the song is called 'Haddocks' Eyes'." "Oh, that's the name of the song, is it?" Alice said, trying to feel interested. "No, you don't understand," the Knight said, looking a little vexed. "That is what the name is called. The name really is 'The Aged Aged Man'." "Then I ought to have said 'That's what the song is called?' " Alice corrected herself. "No, you oughtn't: that's quite another thing! The song is called `Ways And Means': but that's only what it's called, you know!" "Well, what is the song, then?" said Alice, who was by this time completely bewildered. "I was coming to that," the Knight said. "The song really is `A-sitting On A Gate': and the tune's my own invention." (*The White Knight's Song* by Lewis Carroll) 
I'd be interested in what you think of my attempt, above, to distinguish two notions of RT as applying to two different classes of things.
What you've said doesn't conflict with my understanding at all. I think either I have misstated or you have just misunderstood me. By "terminology" I meant that i was previously equating value with term, in this comment thread, but I should have been thinking of denotation instead.
I used to use Cygwin when still on windows. Perhaps that is easier? There is also [this post](http://neilmitchell.blogspot.nl/2010/12/installing-haskell-network-library-on.html). But yeah, Windows doesn't have a great development setup by default, so building C-dependent libraries like network is going to be tricky.
Since I'm the maintainer of network, let me chime in. First, I'd love to have more contributors that use Windows. I no longer has a Windows license (or three, as we probably need to test the package on a few different versions). Every so often when we have some Windows related issue and I go out to try to find someone willing to maintain the package on Windows and/or provide an always-on Windows build bot. I've had no luck so far. Despite Windows being the largest source of problems in the network package and Windows having the largest user base (if I remember the Haskell Platform user numbers correctly), I get almost no contributions from Windows users. One thing that makes the network package particularly messy is the variance in the underlying OS support (e.g. some functions/flags only exists on certain platforms). Also, winSock is worse than its unix counterparts. I believe GHC already ships with msys on Windows and you can use that msys installation to compile network.
Sadly I'm using the Haskell Platform - easiest way to get up and running with Haskell. It also leaves me utterly unprepared to actually *contribute* to Haskell, which is a problem.
Classic dialog, yes. But I wanted an explanation regarding monads.
Perhaps this is your way to contribute to Haskell: by developing a guide/distribution of MinGW/MSYS to make building Haskell libraries on Windows easier.
Windows is bad for open source development, being closed source and using it's own (often badly designed) APIs, and thus the support for doing open source development on that OS is bad. You will have a harder time if that's the platform you decide to develop on. That doesn't mean that you are wrong for trying or that we won't help you were we can, but it means that you will have to climb obstacles that most people avoid by not using Windows. &gt; So why do I need to trawl the web for answers on how to build a package so simple as "Network.URI" - which shouldn't have any dependencies at all upon MSYS or MinGW or build tools? As you said: `Network.URI` is hard to build because the whole package is hard to build. In theory it could be broken off into a separate package. &gt; Internalized knowledge is a problem for any project. Everything that makes it more difficult to participate without grokking something else is going to prevent people from participating. I have the same problem as you trying to build network on Windows and I have to figure out how to do it every once in a while. We're not withholding information from you. Since there's no Windows maintainer for network and haven't been one for years and years, the Windows parts (code and docs) are in a bad state. There's an opportunity for you to fix them though if you care. &gt; Let me give you an example of a project that fought this and completely conquered it: GitHub for Windows. It brought together many different packages together in a seamless way that lowered the cost of using git or GitHub on Windows to, basically, nothing. I assume GitHub has paid developers. Getting people to do this in their spare time is hard.
Language is important, especially for mathematicians and scientists. One could argue that programmers are neither, but when speaking about mathematical issues, that's irrelevant. Math based on faulty language leads to faulty reasoning. Spreading the use of faulty language spreads the use of faulty reasoning. Language doesn't just help us communicate problems, but work them out and solve them too!
Installing gtk2hs on Macs is also a pain. It is really only easy on Linux, I think.
&gt; in an imperative language the result of a function depends on the context, so taking the transitive closure of the function is mathematically incorrect. constexpr int f( int x ) { return x + 5; } That is not always true, like in the above example. 
&gt; By the way, somebody commented on the StackOverflow page that the naive functional programmer seems to use syntactic reasoning while the naive imperative programmer seems to use operational reasoning. Interesting. I have a theory that functional programmers use more concrete (denotative, indicative, elemental) thought whereas imperitive programmers use more abstract (connotative, analogous) thought. However, sequential and random thinking styles can also be very different. On the [Meyers-Briggs](http://en.wikipedia.org/wiki/Myers-Briggs_Type_Indicator) functional and imperitive programmers seem to differ on both axies of the four main personality groups.
That was my post. To clarify one thing, I've been using "naive" there and here not it a pejorative way like "unlearned" but instead in the typically philosophical way, where it means something like "straightforward" or "direct" or "ignoring potential confounding details" or perhaps "first approximation". In that sense, "naive" is often a good thing. Also, I've been speaking about "naive" _approaches_ and not "naive" _people_ for the same reason.
Here's a blog post describing how to build the network package on Windows using cygwin: http://neilmitchell.blogspot.com/2010/12/installing-haskell-network-library-on.html I had more luck with MSYS but your milage might vary.
At work we have a huge code base and formal reviews. The above represents one attempt to make more precise my feelings about the code I write/review. I'm going to try to clarify a bit more why I find difficult to work with code that has plenty of side effects. Each unit of abstraction (function) has a definition and multiple uses. Each function has 3 information carrying snippets associated to it: * The in-language function signature, including the human readable name and the argument names. I rely on this to understand 90% of the function uses. * The human language specification describing what the function is supposed to do. I rely on this to understand 9% of the function uses. * The in-language implementation of the function body, which depends on other functions to achieve its goal. I rely on this to understand 1% of the function uses. When I write/review a new function, I have the following concerns: * Does the function signature reasonably abstract the function specification? This breaks because people don't name functions updateCountAndLaunchNukesAndFlushToilet(), but rather updateCount(). * Does the function body reasonably implement the function specification? This breaks because people don't faithfully write all the side-effects in the function specification, especially side effects caused by dependencies. * Does the function serve any useful purpose? I cannot answer this question unless I check all the callers and figure out how the function is actually working in the given context. But the callers may not even exist yet, as we thrive to develop incrementally. This is what my initial reference to "transitive closure" of a piece of code refers to, which is the function and transitively all its callees, but not its callers. * Does the function replicate functionality elsewhere? Functions with side effects are usually non-reusable, so I usually waste my time. The other kind of review is when there is a bug fix and I try to figure out: * That the bug is fixed. Since the function is tightly coupled with its callers, I can't tell until I carefully understand the callers. Again, a failure of needing just the transitive closure of the function under fixing. * That no other bugs are introduced. This again requires understanding all the callers and/or carefully preventing new side effects from being introduces. If the code heavily relies on side effects, it is hard to preserve them untouched. The situation gets worse as the code evolves: * If a new side effect is introduced in a deep function, nobody ever updates the specifications all the way up to the caller roots to document the new side effect. In the long term, this causes function specs to be regarded with suspicion when it comes to their accurate depiction of the side effects. Forget about the function signatures, I've resigned myself for these to be wildly side-effect inaccurate eons ago. * If the code of a mid-level function is reorganized, people usually do not carefully check that all the side effects of the callees are still making 100% sense for the new implementation. This leads to the side effect part of the function specification being no longer accurately used by the code, which again weakens the credibility of the function specification, which leads to the need of carefully checking callers for all reviews. Nasty example: { ref.setValue(mock); updateCount/AndLaunchNukes/(ref); } becomes { updateCount/AndLaunchNukes/(ref); ref.setValue(mock); } and I catch it in production. The company has been successfully developed large code bases and amazing products. The commons sense constants of these efforts are: * World class engineers. * Formal code reviews. * Limit the number of side effects in a function. Ideally 0. * Limit the scope of side effects. It's OK to use a mutable map, but do not "re-use" it all over the place. * Abstract I/O behind RPCs, such that at system level everything looks like a tree of function calls, it just happens that some cross process boundaries. Edit: Sorry for the reply length, it got a bit out of hand. Obviously you his a nerve. I hope it makes some sense.
You have a weird definition of orthogonal. One that doesn't mean orthogonal.
GHC comes with the mingw tools (gcc, ld etc.) but not MSYS, you have to install that separately. I agree the mingw site is completely unusable, but msys itself is quite easy to install these days (it used to be a lot harder).
&gt; So maybe we can distinguish between two notions of referential transparency in PL. First is what you give, the naive functional one, which is basically a syntactic property under the image of a given semantics. Second is the one that Strachey developed for imperative languages, which is a property of a semantics -- i.e. of a translation from syntax to denotation. This second one strikes me, quite simply, as equivocation over which language is the object language. "Language *L* is compositional because this other thing is!" Though I do think that compositionality is a ultimately a bit of a trivial property, because you can always give more complex denotations to your object language's atoms or use very complex valuation rules for your language's expression types.
&gt; The [...] difference is that philosophers and linguists don't imagine there being normal forms that govern the substitutions, but rather denotata. Certainly. Though I'm not sure whether that's truly the "only real" difference. The problem is, there's a difference between normalization and evaluation. Normalization is a special variety of evaluation which is nice, clean, and transparent; so we can replace terms by unnormalized versions of themselves, just as you suggest. However, there are other forms of evaluation which are more complicated than normalization because they have computational side-effects, involve non-local state, manipulate things which cannot be expressed directly in the language, etc. If all we had to worry about was normalization, then we may very well be able to get away without worrying about extralinguistic denotata. But that only works for toy languages. For real languages I don't think we can get away from the fact that the language is very often *about* things outside of the language. Philosophers and early semanticists did worry a small bit about "normal forms" in natural language. This is where the idea comes from that the active and passive versions of a sentence can be compiled down to the same logical statement, or that we can compile away differences in case marking which 'don't matter' (e.g., "I gave the book to her" vs "I gave her the book"). Often these discussions were phrased as being about "fragments" of a natural language, or about a "regimented" subset of a natural language. Albeit, this form of normalization was about trying to cut away the complications of doing linguistics, as opposed to being something they thought very deeply about.
 C:\Neil&gt;hoogle search --info map Prelude map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f xs is the list obtained by applying f to each element of xs, i.e., &gt; map f [x1, x2, ..., xn] == [f x1, f x2, ..., f xn] &gt; map f [x1, x2, ...] == [f x1, f x2, ...] From package base map :: (a -&gt; b) -&gt; [a] -&gt; [b] So --info does it, however fmap has no documentation.
I'm not sure how much you know about monads, but... the thing that makes monads "pure" is that, essentially, what we're doing is just building up expressions. Those expressions, when run, may have side-effects; but the expressions themselves are just expressions. And since they're just expressions, we can pass them around, glue them together, and all that--- without worrying about what they "mean". So, for the pure part of monads, we're just creating a big complicated expression or a name for the computation we'll eventually want to run. And then after we're done coming up with this name/expression, we hand it off to the special function for running/interpreting expressions for our monad. That is, the monad's run function is an interpretation function--- in the sense of semantic theory: we give this interpretation function a name (expression), and it gives back the denotatum (computation) which is the "meaning" of that name. So there are two key reasons why we care about monads. First is that they allow us to separate the idea of building up an expression vs evaluating that expression. This is the key point for the purity argument, and is the well-known reason to care about monads. But the second reason ---which hits on why I think Reddy misses their point--- is that, by using monads, we can take some fragment of the language's semantics and internalize it. With traditional imperative languages there's just one big monad, and the only way to run it is to hand your program to the compiler/interpreter for the language. Thus, all the magic is in the compiler. But with monads, we can take some of that magic and explain it directly in the language itself. Internalization is a very important thing in logic. And when it comes to RT, I think the internalizability of the semantics of monads gives them a sort of transparency lacking in the non-internalizable semantics given by a compiler. Reddy seems quite upset that functional programmers mislike calling imperative languages RT when they clearly can be (according to the original definition of RT, assuming the language in question is sufficiently formalizable, etc). Surely the additional transparency given by being able to internalize their semantics deserves its own special name, but I believe our misapprehensions about calling imperative languages RT stems from these issues about internalization.
Thanks sclv. I thought that was a very insightful point. I also believe that naivety is a good thing. Programs are very complicated beasts. Whatever we can do with them using naive reasoning should be highly prized.
&gt; and perhaps because even more than "how" he was still stuck on what it "means" to denote at all. Certainly. I meant for this to be included in the exploration of "how"; that is, the thing these philosophers were keen on is the phenomenon (phenomena?) of denotation--- what it is, how it works, how it relates to the idea of "language", how it relates to "meaning", etc. This being in opposition to the more specific project of discovering what the mapping is between a particular language and its denotata. Certainly there's a bit of the latter in each of their writings, but it doesn't seem to be the real goal of their work. Some authors, like Strawson, were much more interested in this latter project; but others, like Russell, considered it something of a bother--- being primarily interested in formalizing mathematical language rather than natural languages. Quine himself is notoriously hard to pin down on this sort of thing. Personally, in spite of the less mathematical topics he discussed, I think the style of his work belies more of a mathematical bent than a linguistic one; though that may have more to do with which of his works I've read, than with the entirety of his opus.
Yup, that is also a quite insightful point. But we also need to keep in mind the training aspect. Functional programming is taught in a semi-rigorous way, using the knowledge of set theory and mathematics that we all know. Imperative programming is taught in an intuitive way, because there is no well-understood mathematics for it yet. In fact, let us say that imperative programming is not really taught at all. Whatever the imperative programmers learn, they learn through experience. They write programs, see how they behave, and build up a mental picture of how they work and how to reason about them. It is an art rather than a science. So, they won't be able to articulate very well how they think. However, we shouldn't underrate what they are able to think and reason. I have seen Tony Hoare remark somewhere that, if they are able to write programs that work, they must have very good reasoning techniques in their heads. Once again, inarticulacy doesn't mean a lack of understanding.
It actually is not. You can use Cygwin for example. Or write your own external library. The fact is there is an FFI wrapper that requires some native library that implements the external symbols it depends upon. This does not have to be MinGW (although it is definitely the option I prefer).
Re your edit, Usually I'm a fan of reducing jargon and explaining things plainly. But for this particular topic, I think it's helpful to stick with terms which are obviously jargon. When you throw around a word like "denotatum", it's clear that you mean something very particular, even if it's entirely unclear what on earth you do mean. Whereas words like "value" are woefully ambiguous, even when you strip away their colloquial meaning. IME, PL theorists use "value" to mean expressions in normal form just as often as they use it to mean the semantic interpretation of expressions. Authors differ in their preferences, naturally, but both traditions are well-established. This no doubt accounts for some of the confusion in people's minds as well as the miscommunications in discussing the topic; which is why I break from my usual attempts to minimize jargon.
Thanks, pacala. That gives me a lot to think about. I understand that what you mean by "transitive closure" is a form of *compositional understanding* of the code, which is of course highly prized. That is basically what we semanticists are after. But we want more. True compositionality means that you should only need to look at the code of the function, not that of the functions it calls. So, no "transitive" bit. A few simple points that I can make, before an in-depth analysis. 1. I think Hudak and Wadler wrote somewhere that Haskell was an exercise in language design "without compromises". Basically, what you see is what you get. Other languages have rarely had such a goal. There, what you see may be quite *less than* what you get. The rest has to be inferred or documented separately outside the programming language. So, I do worry when you say that you rely on types 90% of the time, and the specifications only 9% of the time. Specifications should play a much larger role than that in understanding programs in weakly typed languages. In my mind, types are just bare bones specifications. There is no dividing line between types and specifications. 2. It is useful to make a distinction between "effects" and "side-effects". The "effect" is what you can see visibly as the purpose of a function. A "side-effect" is what it might do surreptitiously, *in addition* to its main purpose. A side-effect in a value-returning function is always a "side-effect" because the returning of the value is what we take to be its main purpose, not changing the state. (In the old days, languages like Fortran, Algol, Pascal and Ada used to provide two separate abstractions "function" and "procedure" so that value-returning functions and legitimately effectful computations could be distinguished. It is a pity that modern languages don't do that.) But side-effects might also be present in legitimately effectful computations (procedures), when they change things that they are not supposed to change. That is **foul behaviour**. If I was reviewing code, I would throw the code back at the programmer and get him/her to fix it. If I was maintaining the code, I would fix it myself. To decide the line between the "effect" and "side-effect", we have to figure out the purpose of the function. It should be clear either from the name of the function or the specification. Once again, specifications are an indispensable part of the software. 3. Specifications must document the pre-conditions and post-conditions of the function. This is true even if you are programming functionally. Imagine, for instance, the code for a balanced search tree. If there is a *rebalance* function, you have to know to what extent the input tree is balanced, viz., the pre-condition. Without that, you can't tell whether *rebalance* is doing its job. Functional/imperative distinction is immaterial for the necessity of pre-conditions. The only difference is that in imperative code, pre-conditions become a lot more important. They determine the interface between the function and its callers. Without pre-conditions, it is simply not possible to get a compositional understanding of the program. 4. Modularity of state is a key principle of imperative programming. (Also called "information hiding", "data abstraction", "object-oriented design" and so on). A good imperative program would make clear how the state is modularised, and which parts of the code affect which parts of the state. Any piece of code that changes the parts of state that it isn't supposed to be changing is a "side-effect". Once again, it is a candidate for throwing the code back at the original programmer. In my spare time, I maintain the [VM mail client](https://launchpad.net/vm), which was written by a superior Lisp hacker ages ago. Note that (Emacs) Lisp has all the faults that you can possibly imagine. No types. No procedures. No object-orientation. The code I inherited had virtually no comments. Yet, once I started working with it and understood enough, I found that all the structure that I could possibly want is really there. Pretty much everything gets done at the right place at the right time. I do document the functions as I go along. In fact, I do that first before I decide to work on some piece of code, because it gives me a clear understanding of what is going on and throws up any warts that might have been there in the original code. Then, when I make the changes I want, the code just works, modulo a few clerical errors here and there which could have been caught by a type checker if there was one. Of course, this doesn't prove anything, other than to point out there exists good imperative code out there. I am sure that there is also plenty of bad code. I think object-orientation has been able to minimize the potential badness of imperative code, which is part of the reason why software has been able to expand so much in the last decade or two.
Code extraction from CIC.
Could you explain it better, please?
Suppose we postulate a term, say "functional transparency", for this idea that functional programmers are after. What could it be? No matter what it is, it should be something that makes sense for *all* programming languages, not only functional languages. Because the purpose is to come up with a property that is applicable to all languages so that we can then say how functional languages differ from the rest. The Quine-Strachey definition of "referential transparency" is something that makes sense to all programming languages. It says that you can replace an expression by another expression that is equal in value. We are left with having to define what is meant by "equal in value". Strachey did that for imperative programming languages. Once we plug in the Strachey definition, the imperative languages become referentially transparent. But, now we find that this idea of "referential transparency" says essentially nothing. We can presumably always come up with a suitable notion of "equal in value" for all languages. (That hasn't been proven, but let us assume that for the moment.) Then essentially all languages become referentially transparent. So, the Quine-Strachey definition fits the bill, but it is essentially pointless. Let us try the Wikipedia definition. A language is "functionally transparent" if every expression can be replaced by its "value". So, now, we are left with having to define what is meant by "value". One candidate is to say that the "value" is what is obtained by "evaluating" the expression. All we have done is to move the burden of definition from "value" to "evaluation". What does "evaluation" mean in a way that makes sense for *all* programming languages? I believe that there isn't a single such notion of "evaluation" that is uniformly applicable to all programming languages. In fact, there may not be a single such notion even within a single language. Take an expression like "x++". You say, if it evaluates to 42, then you should regard 42 as the "value" of "x++". Then, the C programmer will turn around and ask you, "what is the value of "x++" when you write it in Haskell?" You say "x++" in Haskell essentially evaluates to itself. It is a state transformer. It is only when you *run* "x++" as a state transformer and pipe its value to another function that 42 flows from "x++" to the other function. Ooops! Did you say "value"? What kind of "value" is that? Why don't you replace "x++" by its "value" just like you tried to do it for C? Aren't you being shifty, by using two notions of "value" for your own language, while you maintain there should be only one notion of "value" for my language? So you are stuck. Unstick it please, if you can.
How does this work with regards to the Haskell Standard? Would this need to be made into an extension? If so, it is probably overkill.
Why not extend it to newtypes and data types? Could be useful to make phantom types more explicit: newtype Const a _ = Const a
Let me clarify/refine that a bit - of course no one says this explicitly, but in practice, when reasoning about an imperative program, say a function `foo` that takes two arguments, a programmer will almost invariably understand program fragments by examining a scope that is _too narrow_, and within this scope treat variable binding as equality so long as there is no mutation _apparent in that scope_. That is, he/she will look within the body of the function, note that nothing in the function modifies the arguments passed in, and then treat variable binding as equality to reason about its behavior. This is practically subconscious. Something like this is necessary to make programming mentally tractable - people expect to be able to state equalities in their code and reason equationally, and will frequently reason in this way even if it isn't sound (say, because an unrelated part of code references the same values and mutates them).
The signature for `map` is: (a -&gt; b) -&gt; [a] -&gt; [b] What you gave does not match. Also, even if you fix the one you gave, it's not clear how you would distinguish the `a` and `b` type variables using nothing but underscores. Also, underscores typically imply the variable is not used, but `map` uses both of them.
re including it, I already have msys (via msys-git) and mingw so that would be annoying... this is just something you need to learn to wrangle.
How does it not match? I think he's asking for the type to be "inferred", as they are in Agda.
I think gergoerdi wants something similar to http://hackage.haskell.org/trac/haskell-prime/wiki/PartialTypeAnnotations which would be nice to have.
I don't think inferring the type is the idea the OP had in mind. Just a wildcard variable that gets bound to some unique variable internally.
Oh, now I see. The first underscore matches the function, the second one matches the list type variable, and the third one matches the final list. I'm not used to Agda so it was so confusing for me at first.
How is that different? As far as I can tell, he wants to be able to give partial hints to the typechecker about the types, without having to provide the entire type. He wants to tell the typechecker that he's using the `[]` `Functor` instance, without writing out the boring stuff, so he provided the `[_]` hint in his type.
See also http://www.cs.uu.nl/wiki/bin/view/Ehc/UhcUserDocumentation#3_4_Partial_type_signature
&gt; Math based on faulty language leads to faulty reasoning. I can't agree with that. When I write a semantic function I don't care if that property I want will be called "referential transparency" or "baboon's principle" or "elephant's law". I'll just write down what proprety I want exactly (e.g. "forall contexts C with hole of type T and two terms A and B of type T such A === B (for some ===) . C[A] and C[B] evaluate to the same value (for some 'same value')") and then prove a fact or ten about it. For mathematicians phrases don't matter as much, what is needed are precise definitions. 
It makes a little sense when you think about it. Python is the obvious alternative first language these days and the comparison is interesting. In Python effort is made on making the syntax simple and not intimidating (according to programmers). Concepts like side effects are implicit, not explicit, and classes are relatively large 'units' to take in at once. A class can't be broken down much into smaller and friendlier chunks then the class definition + the __init__ function with the unusual 'self' parameter. It's open and friendly to a programmer but to a non-programmer it relies on a large amount of implicit knowledge. In haskell, side effects are explicit, things like print aren't keywords and things like for would be constructed. There's a ridiculous amount of tutorials for monads and storing everything as lists is relatively intuitive. In general all the concepts are explicit, and you wont accidentally be trying to learn something without having learned the necessary pre-reqs. If only there weren't so many other problems with haskell like installing it, running it, compiling it, debugging freaky typererrors and learning why all the defaults are bad (strings, arrays, hashes, etc), to name a few.
Forgive me for being a day late on this one. I had to take some extra time once I ran into the discrepancy between the two implementations.
Right. My point is you shouldn't have to find this information. I'm not sure why building network requires these tools, but if Gtk can build without anythig except the normal windows installer of Gtk and the haskell platform, why can't Network work similarly?
The "defaults" aren't always bad, they're just not always optimal. String is a pretty heavyweight representation, and probably the closest thing to being "bad", but it's convenient and very commonly the inefficiency of it just doesn't matter. I'm not quite sure why you're saying the array library is bad. Perhaps in comparison to the new vector library? Data.HashTable was pretty bad last time I checked, but we never recommend those to anyone, you're more likely to be introduced to Data.Map, which is probably what you ought to use in most cases for key/value mappings. Also, the type error messages can be a bit unclear if you're a beginner sometimes, but remember that they always give you a line number. Having a line number to go to and a description of what's wrong (even if you don't completely understand it) is in my experience a lot better than having a program that runs and fails and being completely in the dark about why it doesn't work. Think about all the hours you might spend in a debugger otherwise. It would be nice to hear more about the problems you've had installing GHC. I know that cabal dependencies can get quite hairy in many of the larger cases. I might be able to offer some advice on the best way to get GHC installed though, if you've had trouble. For Linux, I usually recommend just grabbing the generic binary from the [GHC website](http://www.haskell.org/ghc/download), and then getting the cabal-install tarball from [Hackage](http://hackage.haskell.org/package/cabal-install) and running the bootstrap.sh script from that. I don't like how most of the Linux distributions package Haskell stuff, and a lot of the time the stuff that they do package seems to be out of date. The Haskell Platform is supposed to help with that process, but in the case of Linux, I wasn't terribly happy with it when I tried it. (However, my opinion may be out of date, I only tried it back when the HP was just starting out.)
Agreed about the typeerrors. I only mentioned it because everyone else does, I didn't notice anything overly complicated about them when I was learning. I may have gotten confused as to which typer it was saying I actually was but just pointing to the line number and saying there is a typerror there is often enough. And best of luck to modular/classy prelude. A language's supporting libraries are more important then its ecosystem, so these things really shouldn't we swept under the rug.
Yes, it's not so much that the defaults aren't usable, but that if you ever intend to make something real of your code you'll have to port to the more sensible alternatives at some stage. Strings as char lists is definitely elegant to work with.
As I said earlier, "There is no single, universal definition of any interesting programming term, all the way down to the lowliest of the low. A 32-bit signed integer still has the option of an encoding that permits -0 and one that does not." What you've got going on there is basically [equivocation](http://en.wikipedia.org/wiki/Equivocation). There's no such thing as a "programming language value". There's a value^Haskell and a value^C++ (and depending on how you slice, possibly more than one of each), but no (useful) generalization for those two, let alone across all programming language semantics. It isn't "shifty" to use two different definitions, it's _necessary_. Until you let go the idea that you can create useful universal definitions, you will remain stuck. (Note you provided a "universal" definition, but then immediately observed it was useless.)
Edit: see my sibling response regarding type errors. Tldr I agree with you and mentioned them as a proxy for others. Regarding the install of the Haskell platform it was definitely easy. The problem was it was over 100mb to download and around 7 times that on disk. That isn't a practical problem so much as an ugly impression to others. No other language is anything like that in size. Real Edit: oops, this isn't an edit, it's a Phone Phail.
Oh wow! I think this can be integrated into haskell-mode, so we get docs in terminal mode too :) 
&gt;After I had used Haskell for a couple of months I don't think you can do the language justice in that little time.
Perfect, just what I was looking for, thanks!
A few months certainly isn't enough time to get comfortable with the Applicative &amp; Monad abstractions, but it's plenty of time to learn how to distinguish pure computations from impure ones, become proficient in pattern matching using lists, Maybe, and Either, and maybe even folds.
Allowing the type checker to specialize the type is different from providing syntax for anonymous type variables (in analogy to anonymous pattern variables.) The latter is provided, for example, by [Curry](http://www-ps.informatik.uni-kiel.de/currywiki/) and only valid if a corresponding signature (with all underscores replaced by unused variable names) is valid. With this approach, your `map` signature would mean map :: a -&gt; [b] -&gt; c which is invalid (too general) and, obviously, not what you intended.
I'm really confused about the time lines of this --- he was playing Diablo 2 which came out in 2000 and then after a few months dabbling in Haskell he moved on to Clojure which only came out in 2007. Were those few months in fact a few years? Was he just playing a really old game in 2007? When did Clojure actually get its community together?
&gt; thanks for agreeing with me I am aware of the fact that you thought that. My point was that you should roll with that fact because there's no way out of it, not that you thought otherwise. I am also aware that "providing" a definition is not the same as being the one who created it. That's why I said "provided". It was also your own observation that it is pointless in this context, so it's not like I made something up.
I don't really understand your question - on Windows, GHC bundles gcc and the binutils so GHC "targets" the version of gcc it bundles. You are free to compile C libraries with any gcc, GHC plus its "private" compiler tools should be able to link Haskell bindings to them.
A java JDK is of comparable size. If an equivalent set of tools and libraries for C++ were packaged together I don't think it would be much smaller; it's about normal for a mature platform for a compiled language. Though the dynamic typed interpreted languages: python, ruby, etc., do come out somewhat smaller.
Diablo 2 was played for a very long time. Enough time for an entire hacker community to develop around it hacking on it for fun. I've actually seen a few programmers who admit coming from that community.
Diablo 2 even received an patch last October, so it still continues to have a substantial player base. Also the author is pretty young, which leads me to believe he was playing the game more recently.
Coq is rather popular for proving things with dependent types. One of the nice things about Coq is that you can "extract" programs from your proofs: essentially you just erase everything in `Prop` and use a contraction mapping for whatever dependent types remain. pCIC ---the predicative calculus of constructions with co/inductive types--- is the type theory of Coq. Though how this extension would assist in extracting programs from Coq, I have no idea.
I think one of the problems is Haskell's jargon, and I don't just mean the monad. Transformers, functors, lenses, pipes, conduits, DSLs, lazy evaluation, currying, applicative, types, synonyms, type classes, abstract types, concrete types, monoids, at first the list seems endless. The other problem is the jargon creep from other specialities as Haskell attracts physicists, mathematicians, language designers, so it can be a bit much at the beginning. 
I'm not sure that the OP is claiming any sort of expertise in Haskell. On my reading, after bouncing off of other languages (namely OOP ones) Haskell is what finally gave him some grounding to figure out what the heck programming is all about. Getting your sea legs is a profound step in anyone's career, but once it's happened there isn't really a whole lot of stuff going on there. It's completely believable that he could have come to grips with the notion of programming after a few months exposure to Haskell. His skill in programming itself was surely learned in the Clojure world, if he only spent a few months with Haskell; but learning what programming *is* is quite different from learning how to program (which is different again from learning how to program well).
There's a reluctance to accept unimplemented features into the standard, so yes, it would help if it were an extension first. Then it could be proposed through Haskell Prime.
Yep, bumping the font size and switching to native 720p for the next episode. Hopefully that'll help. Thanks.
&gt; Undoubtedly there are many subtleties of the philosophy and mathematics which Uday understands and I do not. But he has not grasped the qualitative difference between pure and non-pure programming languages. I think "pure" and "non-pure" are just as ill-defined terms as "referentially transparent". The best I can make out is that "pure" languages try to make do with static things. The so-called "non-pure" languages make use of dynamic things. Personally, I find dynamics a lot more interesting than statics. But, as far as, qualitative difference goes, the difference between "static" and "dynamic" is quite clear.
I forgot Template Haskell and quasi-quoting. I was wondering how much do I have to learn before I can use this properly. I have a list of the known unknowns, how long are the unknown unknowns?
I was trying to contribute a bug fix to it. I submitted the issue, the maintainer asked me to submit a patch and run the tests. Now I could have said "I can't, here are the test cases that fail" and waited - but I went the extra mile to try to do that. So I forked it, tried to build it and failed. And I basically said, after over an hour of messing with it, I don't care about building the whole package. I don't want to build the whole package. I want to submit a fix for a URI parsing bug and that's all I want to do. Now, most people I wager would have given up at this point. But I made the alterations to Network.URI that I needed to be able to build and test it myself, I patched it, and I submitted my patch along with additional test cases (that I wasn't even able to run, because again, I couldn't compile it.) If you can't see the problem with this, then I don't know what to tell you.
I'm supposed to give a talk on Haskell to a bunch of .NET programmers at the end of the month. For this reason, I've been pondering this very question for the past few weeks. The best I can come up with is this: Functional programming is clearly becoming more relevant to mainstream programming. Haskell is uniquely focused on being a functional programming language first and foremost. Therefore learning Haskell is rather like language immersion for functional programming.
You might want to consider something of what [spj](http://research.microsoft.com/en-us/people/simonpj/) had a video of recently. He put a chart up on one axis having 'safe' and 'unsafe' and on the other, having 'useful' and 'useless'. C++ was on the useful and unsafe quadrant, with Haskell in the safe but useless quadrant. He basically was talking about how we need to move to the safe and useful quadrant and stuff like LINQ is the sort of thing. The idea of controlling the side effects. Just found the [reference to the video](http://lambda-the-ultimate.org/node/2356). 
http://www.youtube.com/watch?v=ECODePT6VHM
Yeah, it'd be interesting to see that in place of constraints! foo :: _ =&gt; Double -&gt; m a Could make classy-prelude-like things more palatable (implicit duck typing).
The same can be said of other languages as well: arrays, strings (ask a non-programmer friend what "string" means in computer programming), pointers, bit shifting, loops, recursion, templates, static methods, private members, switches, classes, actors, etc. The only difference is that tons of languages use these words, while Haskell is sort of in a world of its own, making it harder for people to migrate *from their other language* to Haskell.
"Why not?" Jokes aside, I like it because it's a very clean language. Lazy, high-level and functional. It has everything I want.
Haskell gives me powers of abstraction that are practically impossible to achieve in other languages. Haskell is the only language I can confidently claim that I could do *anything* with. I heard it said once of Racket macros that "you only have to write ugly code once". Haskell has a similar ability to provide beautiful library bindings for almost anything, via the FFI if necessary. Haskell encourages you to solve problems The Right Way.
I like that I can write software that does complicated things without feeling like I'm boiling the ocean. I like that the compiler can tell if I've told it to do something nonsensical. I like that I can use data structures in complicated ways and not worry about whether I'm accidentally dererencing a pointer to the wrong type. I like that when I solve a problem in Haskell, I feel like the problem is definitively solved. I like that the generated code is fast enough that I don't feel like I need to write the performance-critical parts in some other language.
Please consider in future videos showing the following please. I started trying to learn haskell as a concise languaje that would allow me to do a fast data filtering, manipulation, calculations and charting. I haven't gotten there yet. And I haven't seen interactive charting either. But since I already invested a lot of time, and I don't want to stop now and start over learning R languaje that I recently found it to be a better alternative. I don't what to give up. I just want to see if someone can do with haskell this in a fast and interactive way.
&gt; Haskell is the only language (AFAIK) that allows the use of entirely equasional reasoning to solve problems Prolog is pretty cool... you should check it out.
In my case, the one of the biggest reasons we _are_ using Haskell is that it allows us to stay close to the formal spec that we are writing concurrently, and the clear correspondence makes it easy to understand. We have found errors in the spec by looking at the code and errors in the code by looking at the spec, and it would be much more difficult to do so in e.g. Java.
Thanks, that is what I've been doing but just wanted to make sure there wasn't something a step or two simpler.
Knowing that a language can do anything doesn't help you figure out what a particular piece of code does. 
Hello Uday, thanks for your reply. You are right, they are equally ill-defined! In fact I think I would use them to mean the same thing. But although I can't formally define purity or referential transparency as I understand it to apply to Haskell and not to C, say, I do have a strong intuitive feel for the distinction between the two languages. Forgive me if I am mistaken, but I interpreted the goals of your stackoverflow comments to be to blur the distinction, rather than sharpen it.
One thing which I don't think has been mentioned is how well Haskell lends itself to tooling, and how energetically the tools around it are developing. GHCI is a wonderland of productivity. Its tab completion, illuminating tye information, and flexibility derive from Haskell's strong and powerful typing. I have Hoogle hooked into GHCI, and since Haskell types are awesome, searching by type saves a lot of googling and fruitless documentation poring.
&gt;Code is easier to read. (Put java up there). I agree. To add another perspective, Clojure is also a good alternative if you're targeting the JVM.
1) *Fallowing compiling errors is not refactoring. In fact, Haskell should eliminate the need to refactor by forcing you to write small correct functions in the first place.* -- I have been corrected. 4) Highly subjective and relative to what alternative are being considered. For example, i'd say that Python is easier to read than Haskell. 5) No, your math training does not become useless. No one reading this should dissagree that if x was equal to 4, after x = x + 1 it equals 5. It's your thinking style that needs to change. 6) Hell yes. (as in agreement)
LINQ is nice, I use it all the time. But it does nothing for safety. It is pretty much impossible to get safety out of C# or any language of that style.
Not directly, but I understood it as the primary goals of the language, being a crucible for research at its inception, not real-world oriented. This is changing however, Haskell moves to the safe and useful quadrant.
I like this as a short but sweet example - functional fizz buzz! http://www.free-variable.org/2009/07/were-not-in-kansas-anymore-toto/ I did a longer write-up of this here: http://pragprog.com/magazines/2012-08/thinking-functionally-with-haskell
Is this the new Daft Punk song?
I see. So, you think abstract thinkers find concrete thinking hard. Is the converse true as well? Concrete thinkers find abstract thinking hard? If so, does it mean that Haskell and C are made for different classes of people and they will find each other's trade challenging? That is a radical thought! I will need to research into that. That will be quite useful for figuring out how to teach things. In any case, what I was originally saying about articulation is that imperative programmers are not normally taught how to articulate how programs work. So, if they do it at all, it is because they figure it out on their own. Functional programmers, on the other hand, are taught how to articulate how programs work. But, I am also concerned about the extent to which functional programmers seem to think *syntactically*. Abstract thinking is need to scale up to larger systems.
I don't think good programmers think about code in natural language. Dijkstra on the [foolishness of natural language programming](http://www.cs.utexas.edu/~EWD/transcriptions/EWD06xx/EWD667.html)
A theory yes, but I'm starting to wonder if Turing complete is actually a shakey foundation to build a language on top of. I.e, lack of totality or a guarantee of termination.
Occurs check: Cannot construct infinite type, `a = b -&gt; Maybe a`. Lines 12-135.
It is extremely hard to sell languages to a Haskell user.
Oh, I am sure that a clear distinction exists between functional programming and imperative programming. And, I think the *static vs dynamic* axis captures it as best it can possibly be. (The point of my stackoverflow comments is to point out that the ideas of "referential transparency" don't do justice to the real a distinction.) Static vs dynamic distinction also allows us to relate our state of affairs to the larger body of knowledge in science. Physics knew statics for a couple of millennia before it began to understand dynamics (started by Galileo and Newton). In Mathematics, we have set theory which represents "statics". But the rest of mathematics deals a lot with dynamics, even though it is rarely presented that way. In fact, I think of set theory not as mathematics, but rather *meta-mathematics*, the language in which mathematics is expressed. The distinction between meta-mathematics and mathematics is quite clear too. For instance, you find cartesian closed categories in meta-mathematics, but they are incredibly rare in the rest of mathematics. So, when we think we are being "mathematical", we are actually being meta-mathematical, trying to use the same language and tools that mathematicians use. But we are not being "mathematical", i.e., we are not applying real mathematics, say, the way the physicists do. The state of affairs we have at the moment is like that of Physics before Galileo and Newton. We understand statics well, but not dynamics. But we need to progress. My own active research is into speeding up that progression, from statics to dynamics. So, I won't accept a value judgement that suggests that "statics is good, dynamics is bad." There is no value judgement to be made here, from my point of view. I would invite you to read a short position statement that I wrote for an ACM Future Directions conference in 1996: [Imperative Functional Programming](http://www.cs.bham.ac.uk/~udr/papers/imperative-functional.pdf). I don't seem to have known at that time that Simon PJ and Phil Wadler had a paper with the exact same title (but different content of course). I was at fault for not publicizing this paper more. But, fortunately for me, Haskell has progressed precisely along the lines that I was recommending. Haskell is an imperative functional language. So, I don't see why we should be shooting down imperative programming in any way. We don't understand imperative programming well enough yet. But we need to.
I usually tell people about a project that I've done that I know will be of interest to them, and that it has X, Y, and Z interesting features, and that it was just a weekend (or two) project in hundreds of lines of Haskell.
Well, I'm currently selling Clojure to myself... at least until Frege becomes as usable.
&gt; To my understanding, an expression will never denote an L(eft)-value. Rather, the L-value may denote an R-value as the value of the expression. Unfortunately, that observations get stuck at the terminology issue. "Expression" is used in imperative languages typically for the kind of thing that goes on the right hand side of assignment statements. But the things that go on the left hand sides are expressions too, e.g., a subscripted variable like a[i]. They would denote L-values. In ML and Haskell, there is no constraint on expressions denoting L-values (i.e., what you call "references"). So, quite arbitrary expressions including function calls etc might denote L-values. An R-value is a state-reader, the kind of thing expressed by !x in ML or (readST x) in Haskell. ML doesn't distinguish state-readers from state-transformers. I think Haskell doesn't either. So, they miss the kind of "R-value referential trasnparency" that Strachey talks about.
No, recursion is not a loop. It's certainly possible to describe recursive function evaluation *operationally* in terms of loops and explicit stacks, but (a) it's rather involved, and it's actually a lot more straight-forward to describe loops in terms of recursion, suggesting you have the wrong primitive there, and (b) recursion is a far more general and widely applicable idea.
Obvious troll is obvious.
Starting your post by "renaming" monads to state transformers is a bit jarring for someone who seems so attached to the precise meanings of words... If you want to speak of state transformers that's fine, but not all monads belong to this category. Your trace view also seems eminently internalizable in Haskell, since that's exactly what the sequence function consume. Even if I misunderstand this particular view, I fail to see why the internalization under the monad form of state transformers would rob you of your freedom to think about them in another way, compared to languages who don't internalize them at all ?
It's called the Turing tarpit. &gt; Beware of the Turing tar-pit in which everything is possible but nothing of interest is easy. —Alan Perlis, Epigrams on Programming
Natural language can communicate what we haven't formalized yet, and so it definitely has a place in text books. As you said, it has little place in proofs. I don't think people follow math in an "unquestioning way", but they do want to take the powerful reasoning tools that maths can offer so they can be applied to programming, why wouldn't they? 
4) I see. But what i was responding to is the claim that Haskell **is** easier and my point is just it's **subjective**. &gt; 5) Math doesn't work that way. In Math, if you assume x = x + 1, you can prove 1 = 0, Now, we're having a symantics debate because in some programming languages, == denotes equality while = denotes assignment and in Haskell, = donotes equality. No mathematician should be surprised it you reworded that "assign x to x+1". In fact, that's how i often see the Euclidean Algorithm written, such as at [Proof Wiki](http://www.proofwiki.org/wiki/Euclidean_Algorithm). 6) Neither do i. I said "hell yeah!" in agreement. 
It is one of the few languages that allows refining a high level specification into a practical program: 1. Write a high level executable specification of your problem. This is possible due to the declarative nature of the language. Using the type system, equational reasoning and basic testing, you can get a very high confidence level that your high level specification/naive first implementation is correct. But it will probably be slow. 2. Rewrite the slow parts in a more efficient way (but that might be harder to reason about), using either more advanced algorithms, more imperative constructs provided by the language or ffi. But you can still use your reference implementation of part 1 to test against. An even stronger version of this approach is illustrated in Pearls of Functional Algorithm Design by Richard Bird. In the book he focuses mostly on purely functional solutions to some tricky problems. He starts with a high level naive solution that he proves correct, then he refines his solution it while still proving that each transformation was correct, and works his way to the faster but less obvious solution. 
It might help to consider the more symmetric definition of Applicative: unit :: () -&gt; f () mult :: (f a, f b) -&gt; f (a, b) This formulation is equivalent and is the basis of the Applicative laws (and I highly recommend reading the original paper). Also, keep in mind that what you are looking for might not be an Arrow. It could be something else.
It might make more sense to do something like this: class (Category (~&gt;)) =&gt; Monoidal (~&gt;) i p where pair :: (a ~&gt; b) -&gt; (c ~&gt; d) -&gt; (p a c ~&gt; p b d) ileft :: a ~&gt; p i a iright :: a ~&gt; p a i assoc :: p (p a b) c ~&gt; p a (p b c) This is basically a category with some structure that is weaker than arrows, but still powerful enough to contain stuff like monoids, categories, etc.. Then we define a functor between those kinds of categories like this: class (Monoidal (~&gt;) iA pA, Monoidal (&gt;~) iB pB) =&gt; MonFunctor f (~&gt;) iA pA (&gt;~) iB pB where fmap :: (a ~&gt; b) -&gt; (f a &gt;~ f b) unit :: iB ~&gt; f iA mult :: pB (f a) (f b) ~&gt; f (pA a b) If we define instance Monoidal (-&gt;) () (,) where pair f g (a, b) = (f a, g b) ileft x = ((), x) iright x = (x, ()) assoc ((a, b), c) = (a, (b, c)) then an instance of `MonFunctor F (-&gt;) () (,) (-&gt;) () (,)` is basically the same as an `Applicative F`.
&gt; Is the converse true as well? Concrete thinkers find abstract thinking hard? You want an example of concrete thinkers having trouble with abstract thought? "x=x+1". Since x cannot equal x+1, a truely concrete thinker would write "x'=x+1", like in Haskell. Haskell also encourages random thought over sequential (since functions can be defined in any order and you don't implement order of operations yourself). The sequencial thinkers might look at x differently before and after the operation, but a random thinker might prefer to look at x as the same entity, no matter where it is. But, it is worth noting that aptitude and preference are two separate things. When i say completely concrete thinker, i mean little abstract aptitude and vice versa. While someone may prefer one thinking style, they may be equally adept in each. &gt; I will need to research into that. For a short while, i was trying to get various people from different programming communities to take the Meyer-Briggs to confirm the theory. &gt; That will be quite useful for figuring out how to teach things. The MB uses four main personality types with each having four sub-types so it identifies sixteen personalities. I've been reading "Please Understand Me II", which contains a copy of the test and 342 pages of information covering each one. Other than that i'm completely ignorant on the subject. Still, let me paraphrase some of the content. **SPs and SJs** (Sensory-concrete thinkers) Both most concerned with the right-here, right-now. It notes that SPs have no use for theories, but facts and first-hand experiences and can be impulsive. SJs are more conservative, detailled, "everything in its place and a place for every thing"-type people. **NFs and NTs** (iNtuitive-abstract thinkers) NTs are most likely your low-level systems programmers, working on theories, research, and technical things. The F in NF stands for Feeling; they seem to be described as more social and caring people. I took the test once in December and once a month or so ago. The results were respectively SP and NT. It's not uncommon for one to jump personalities frequently. &gt; But, I am also concerned about the extent to which functional programmers seem to think syntactically. Abstract thinking is need to scale up to larger systems. This could very well be a concrete trait--syntax being concerned with mostly what you can see and demonstrate. At the same time, concrete thinking seems necesary to make large systems smaller since concrete logic is almost always more concice. I like to use qsort as an example because it can be a 10+ line function in C++, but when written in a Haskell-like syntax, i can condense it down like in the fallowing psuedo code. vector&lt;int&gt; qsort( vector&lt;int&gt; v ) { return sorted(v) ? v : concat( qsort( filter(less_than (v.front()), v) ), qsort( filter(equal_to (v.front()), v) ), qsort( filter(greater_than(v.front()), v) ) ); } I'm not aware that you need abstract thinking to scale up (has scaling ever been a problem in FP?) but i know we also need concrete thought in order to scale down. 
The issue I have with ClassyPrelude and similar projects is not so much the use of type classes for overloading, but the fact that they seem to solve the wrong problem. As Michael stated several times, the idea of ClassyPrelude is not to allow people to write polymorphic code, but simply to offer an alternative to qualified imports. I think that the question of how to manage imports and how to navigate through code easily does not pertain to libraries, but to development tools and IDEs. Instead, what I think is really needed is a set of abstractions to be able to write code that is polymophic in some "related" family of types (classical examples being ByteString, Text and String). ClassyPrelude fails at that, since it doesn't provide any law describing the semantics of the functionality it abstracts, and how the various methods interact with one another. For the most part, we already have a lot of the abstractions needed to write fully generic code: `Functor`, `Applicative`, `Alternative`, `Foldable` for parameterized collections, and `Monoid`, `Reducer`, `Generator` for monomorphic ones. Things like `length`, for example, can already be made mode general: length :: Generator g =&gt; g -&gt; Int length = getCount . reduce although one needs to be careful to ensure that specialized versions are used when possible. The remaining APIs can be abstracted piece by piece by coming up with a reasonable set of laws that describe their behavior, and that can be used to reason about code containing them. For example, here's my attempt at defining a set of laws for "splitting" functions like `take`, `takeWhile` and similar: https://github.com/pcapriotti/pipes-extra/blob/text/Data/Monoid/MonoidSplit.hs My hope is that someday we'll get to a point where we'll have type classes abstracting and generalizing all or most of the functionality of the current Prelude and container packages, without taking away any of the reasoning power that monomorphic code currently provides. 
I'm tempted to suggest that it would be better for each package to have its own Prelude replacement. I think you basically do that in the Yesod scaffold with the Imports module or something like that. I used to do this myself, and put GHC extensions in the cabal file. However I've found that this leads to modules that can't be fully understood in isolation, code that is harder to reuse, and is highly dependent on your cabal file to compile (which makes it harder to use `runhaskell` or packages like `plugins`). There's also the PVP problem that if you have implicit imports you can't really depend on any wide ranges of versions; the PVP even recommends depending on the *minor* version series in such cases. I'm unlikely to be using the classy prelude myself, but I'll share some thoughts on it regardless. &gt; Similarly, I wish that ++ was just mappend. So in classy-prelude, I've done just that. In base 4.5 it has an operator variant called `&lt;&gt;`. If anything, I'd favor using the same name (although you could implement it yourself to stay compatible with base &lt;4.5). &gt; One change I didn't make, but was considering, was using . and id from Control.Category. I'm not sure how much of a practical benefit that would provide people, while making error messages likely much more complex. But if people want that change, let's discuss it. I think on the contrary, when you need Category but have the Prelude variants, the error messages tend to suck really hard. There seem to be a ton of uses of Category, and it's compatible with the Prelude ones via the `(-&gt;)` instance. Exporting a shorter variant of `&gt;&gt;&gt;` might be nice too, perhaps called `&amp;`. Moreover, I find myself using Foldable a lot lately. There's a problem here though that many of the names there clash, in particular with those in Control.Monad (some of which are re-exported by Prelude as well). I'm not sure how similar in effect the conflicting names are, and if the Foldable ones are strictly more powerful versions; if that's the case they could be nice to export in place of the lesser versions. Another powerful and generically useful package to consider re-exporting from is Kmett's new `lens` package. &gt; In other words, if there's something I'd want to cut out first from classy-prelude, it's the gymnastics which it pulls to accomodate conduit instances. It was definitely a fun part of the experiment, and I'm glad to have tested it. Sure! I do this all the time in my own code. Introduce some crazy type class hackery just to see if I can, how it would be done, and how well it works. I usually end up removing these experiments in the end, but it's all good fun and educational.
I know of one other pure language (called Pure), but it doesn't have static types. &gt; When I discover that a language is missing one of these, it becomes pretty much an automatic deal breaker for me. Are you saying that no other language is good enough for you?
&gt; In Math, if you assume x = x + 1, you can prove 1 = 0, which means your formal system breaks in a non-interesting way. The [trivial ring](http://en.wikipedia.org/wiki/Trivial_ring) may be non-interesting, but mathematics does not break down just because 1=0 :)
;-]
Hardly visible to the end user (at least for now), yet so important in the resulting code and the development of GHC. I'm glad to see it' ll simplify your work on GHC.
as a crude proxy, I Control-F'd Simon M's google+, and he hasn't talked about this since November: https://plus.google.com/107890464054636586545/posts/EPnW2WvDrWt
Not to mention stack overflows...
We have a fairly large Haskell codebase (&gt;75klocs) and being able to approximate stack traces (by inspecting the evaluation stack) using an experimental GHC branch with native debugging symbols has saved a lot of time. Is this something I need regularly? No. But when it is useful, it is massively useful.
Using fromJust defeats the purpose of Maybe type. Why on earth would you do that in first place ? Do you not anticipate that you MAY get Nothing ? And how this is haskell's fault ? &gt;The above is just horrible horrible code to debug. Actually it's horrible code to write, the debugging part is just the consequence. 
https://singpolyma.net/2012/03/haskell-for-rubyists/
fromJust is useful if you get to a point in the code where you've filtered all the Nothing out and you happen to know everything is a Just.
&gt; Natural language can communicate what we haven't formalized yet, and so it definitely has a place in text books. As you said, it has little place in proofs. I don't know what you mean. Text book mathematical proofs are full of natural language. The proofs in research papers are the same. &gt; I don't think people follow math in an "unquestioning way", but they do want to take the powerful reasoning tools that maths can offer so they can be applied to programming, why wouldn't they? Because they might not be applicable. Mathematicians never programmed an autopilot for an airplane. They have never done anything to model state-manipulation. So, we can't assume *a priori* that mathematics has all the techniques we need. 
Could be something in a library that he's using.
I would like to ask something the other way round. Why is it still there? Why is all the other braking changes not done? Isn't the "avoid success at all costs" thing supposed to mean were never to big to change things to the correct way?
Simon will talk about stack traces at this year's Haskell Implementors' Workshop: http://www.haskell.org/haskellwiki/HaskellImplementorsWorkshop/2012 With any luck, this talk will be taped.
I think it's unfair to say that the community does not fix things. We just prioritize the things we fix. For example, lazy IO was once widely regarded as a huge game-breaker for writing production-ready code and virtually overnight several iteratee libraries sprung up to solve the problem. I guess what I'm trying to say is that if something doesn't get fixed, it's typically because it's not as big of a problem as people make it out to be. For example, as much as I don't like `fail`, its existence has never really stopped me from writing good code. Sure it is annoying and ugly, but I don't think it has ever been a blocking issue for anybody so it sort of slides by and people tolerate it. Also, one thing I like about the Haskell community is that we solve our own problems rather than waiting for some company or person to do it for us. If somebody really cared enough to fix this, they would submit patches to all the packages in the Haskell platform to remove all uses of `fail` (including pattern matches on binds) so that it could be safely deprecated.
I'm keen to get this in some form, and I've been talking to Peter about ways to improve the annotation strategy he's using to track the source information through GHC. It's similar, but not the same as the profiler, because the goal is to have flat profiling - just map object code to source code, and not interfere with any optimisations, whereas the profiler's goal is to produce more information about the call stack. Peter also has a really cool Core profiler in his branch of ThreadScope, I hope he'll be able to demo it at the Haskell Implementor's Workshop.
Yes, I should look at that. Thanks for the reminder!
Yes and no. GHC maintains static, dynamic, and profiling versions of every library. It is about twice the size it needs to be to just run.
Yes, that's a nasty one. Basically, it means that you mismatched a type with a variant of it and the compiler is trying to infer a recursive type. The compiler can't be more specific than that, because of the nature of recursion. The type you constructed is chasing its tail, and the compiler is following along. Use a binary search strategy to comment out code until you find the minimal type expression which produces it.
That depends. What are the known unknowns? A quasi-quoter is a monad which constructs Haskell (or other, but lets ignore those) expressions. You don't have to define the monad, but you will need to learn how the API (i.e., the names of the monad actions that generate the kind of Haskell syntax you want). You can construct new monad actions by chaining previously constructed monad actions, just like any other monad. (I.e., using (&gt;&gt;=) and (&gt;&gt;), or do-notation) You "splice" the code generated by TH, typically with the special syntax: $( quasiQuoterName ) There are two gotchas: You can't use a value in the same module which spliced it (so you'll need "internal") modules. And, if you have the TemplateHaskell extension turned on, it is possible that the order in which you define things matters, so that: foo = bar bar = "bar" might lead to an error. What's missing?
 let Just x = mb in ... Gives you source location without TH.
That was a decent ad. I'd suggest the "Gentle Introduction to Haskell 98" for any interested beginners (but, probably not inexperienced programmers). www.haskell.org/tutorial/
Perhaps I should have explicitly prefaced the rhetorical question with 'as a novice'.
To expand on this idea, consider a datatype like: data Tree a = Leaf a | Branch (Tree a) (Tree a) Now, suppose you have a tree of Ints. How would one add 1 to each element of the tree? Basically, you would use recursion: addToTree :: Int -&gt; Tree Int -&gt; Tree Int addToTree m (Tree n) = Tree (m + n) addToTree m (Branch l r) = Branch (addToTree m l) (addToTree m r) We add one by calling (addToTree 1) on a Tree. In fact, there is a structural pattern here which is usually factored out into the "Functor" type class: instance Functor Tree where fmap f (Tree a) = Tree (f a) fmap f (Branch l r) = Branch (fmap f l) (fmap f r) so that addToTree can be defined as: addToTree n = fmap (+ n) The (+ n) is basically some syntactic sugar, but it does what you expect it to. This code is so trivial that I am 100% confident it will work, without even testing it. How would you use loops to traverse a tree? How confident would you be in your implementation? How much time would you need to test it? How well would your solution scale up to changes -- for example, having a type of branch which contains an unbounded list of Trees? (I.e., a modified rose tree) I would have to change the Tree definition to: data Tree a = Leaf a | Branch (Tree a) (Tree a) | RoseBranch [Tree a] and add a case to the Functor instance: instance Functor Tree where fmap f (Tree a) = Tree (f a) fmap f (Branch l r) = Branch (fmap f l) (fmap f r) fmap f (RoseBranch ts) = RoseBranch (fmap (fmap f) ts) Easy. But unless you are very skilled at structuring loopy-code correctly, the first time, you will struggle. Also, the GHC compiler has some nice pragmas for warning when a case is missing. So if you add a type of branch to your tree, but forget to change its Functor instance, you will get a nice big warning (or even error).
Of course we're talking about semantics. Gasp! We're talking about languages and what they mean! That is "semantics" by definition. Also, while no mathematician would be surprised to see "let x = x + 1" in an amateur's forum, they would be disappointed, because it is a non-standard and ambiguous usage. Typically, a mathematician would use induction/recursion to deal with proofs about sequences (e.g., "let x_(n+1) = f(x_n)", where "_n" denotes the subscript n.) That's not as nice to type as it is to write by hand, but it avoids the complex issue of interpreting what "let x = x + 1" should mean. In particular, there is a branch of mathematics called "model theory" which formalizes the notion of meaning for mathematics. Dealing with assignment and other referentially opaque contexts makes the models *much* more complicated for no gain (in fact a loss) in clarity. That might all sound esoteric, but it is important to remember that there are extremely deep connections between logic and programming (like the Howard-Curry isomorphism theorem, the fact that "denotational semantics" is *nothing but* model theory, etc), which are obscured by complex models for computation for dealing with things like assignment and other referentially opaque contexts.
Fair enough. I tend to get a little defensive about Haskell, at times. The lack of context in your post brought those issues up for me. Haskell is not that hard, and the "hard" parts make it easier. :)
Correct. As of today, I know of no other language that fits these three criteria and that I would use for real work. I hear that Coq, Agda, and Clean fit all my criteria (although I haven't personally looked at them), but they're not really viable for production development today.
Rather than commenting out code, my typical strategy is to add more type signatures inline where I suspect problems, especially in where clauses. It's a great sanity check: does the compiler think that this expression has the same type I think it does?
&gt; First, Haskell does the basic things right -- and by basic I mean the things from Scheme. Yup. Basically, Haskell is probably the most popular language that's comparably good to Scheme. Arguably an improvement over it, too; it's definitely more powerful, but on the other hand, Scheme has a mild learning curve, Haskell has a learning **cliff**.
If you really know that then you won't get that error.
&gt; they seem to solve the wrong problem .. I think that the question of how to manage imports and how to navigate through code easily does not pertain to libraries, but to development tools and IDEs. Fair enough, but the fact is that the problem remains unsolved. It's tiresome to use qualified imports, and I for one think its high time that Haskellers are provided a new environment that makes using the good stuff easy and intuitive. Until we get *both* TNDR *and* Eclipse/Java-like support for automagical imports via CTRL+SHFT+O, I think ClassyPrelude is just about the best we're going to get. This is why I latched onto the local-imports-via-records idea and made ModularPrelude, so that instead of being forced to deal with ClassyPrelude's polymorphic junk everywhere, you could locally scope a specific set of monomorphic bindings. &gt; what I think is really needed is a set of abstractions to be able to write code that is polymophic in some "related" family of types .. My hope is that someday we'll get to a point where we'll have type classes abstracting and generalizing all or most of the functionality of the current Prelude and container packages, without taking away any of the reasoning power that monomorphic code currently provides. I'm not so sure this is a good idea / will ever happen. Mostly what scares me about this is that the Haskell standard libraries will reach the same level of incomprehensibility as the Scala ones. The idea is a good one, but I don't believe any language has "successfully" gone there before, which is why I hesitate to believe it is possible.
Note that Michael has already patched the "dropped finalizer" bug in conduit-0.5.2.3. We're still discussing the associativity bug, which is trickier than it first appears. I submitted a patch attempting to solve this, but Michael pointed out a weakness. Gabriel claims to have solved the problem in his new (as yet unreleased) non-indexed formulation of pipes; I'm very curious to see what he's cooked up, though I think I have a good idea. This is all very exciting; I'm very glad I took the plunge and joined the pipes conversation in a meaningful way.
&gt; Of course we're talking about semantics. Gasp! We're talking about languages and what they mean! That is "semantics" by definition. You missunderstand. &gt; In Math, if you assume x *=* x + 1, you can prove 1 *=* 0... This is a [semantic argument](http://en.wikipedia.org/wiki/Semantic_dispute) (not an argument about semantics) because in one instance, you use *=* in the context of assignment, and in the next you use it in the context of equality. It hinges on you correctly viewing it as equality and me incorrectly viewing it as assignment. However, when used as an equality operator, the assertion that "x=x+1" fails and therefore 0!=1. When used as an assignment operator, it implies a transformation and not that 0=1. Other languages could have chosen to use a more appropriate syntax, like &lt;- or :=, but they didn't. Just because they used the = sign doesn't mean it means the same thing. I do not dissagree about using subscript notation, nor anything else you said. I only dissagree that 0=1.
Look at this awful hack in [Data/Maybe.hs](http://hackage.haskell.org/packages/archive/base/latest/doc/html/src/Data-Maybe.html#Maybe): instance Monad Maybe where (Just x) &gt;&gt;= k = k x Nothing &gt;&gt;= _ = Nothing (Just _) &gt;&gt; k = k Nothing &gt;&gt; _ = Nothing return = Just fail _ = Nothing Look at that! `fail` has type `String -&gt; m a`, but the `Maybe` monad just discards the string! What if that string had important information? There is no reason at all for `fail` to take a `String` input if you are working with the `Maybe` monad. This is one of the many reasons that `fail` in its current incarnation is a dreadful, awful, *horrible* hack. Here's what `fail` *should* look like (using multi-param typeclasses): {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE TypeSynonymInstances #-} {-# LANGUAGE FlexibleInstances #-} import Prelude hiding (fail) class Monad m =&gt; MonadFail e m where fail :: e -&gt; m a instance MonadFail () Maybe where fail () = Nothing instance MonadFail () [] where fail () = [] instance MonadFail IOError IO where fail = ioError instance MonadFail e (Either e) where fail = Left instance Monad (Either e) where return = Right Left e &gt;&gt;= f = Left e Right a &gt;&gt;= f = f a 
&gt; The problem is all monads have to support fail, and most leave the default which is just an alias for error. It would have been nice if you had to use a constraint like MonadFail m in polymorphic code, or a monad with a MonadFail instance in monomorphic code, if you have refutable patterns in do-notation, and that you got a type error otherwise. 1000x this. The *only* reason that `fail` is hacked into `Monad` is for determining how to handle pattern match failures, e.g. Just x &lt;- foo 
This is excellent! Lots of the "old Haskell pros" use emacs or vim, but if we want to entice newcomers we definitely need to make sure that commonly used IDEs are well equipped to help their first Haskell experiences be as painless as possible.
Oops, I forgot to reply to this. It does seem like you're right, and since it was in a version from 2009 it is not something that was added recently. *Mea culpa.* I actually went through the source code for it at one point a long time ago and came to an incorrect conclusion.
Right, that explains it quite nicely for me. I was musing about how `unit` and `mult` could be defined without the `Functor` pre-requirement, and that makes sense that when you add functor powers to it, you can build `pure` in terms of `fmap` and `unit`.
The compiler-generated ones (e.g. inexhaustive pattern match) can give you a line number, but there's no way for a library function like `head` or `fromJust` to get its caller's line/file. That's where stack traces would help. 
You're talking about the "syntactic rt", right? I don't know whether that definition can be made precise enough to evaluate and language-independent to objectively determine how two languages (with semantics) compare.
Call-by-name/need makes code far cleaner to reason about, no doubt about it. But there are multiple ways to go about separating expressions from their denotations. You can wrap everything in an extra abstraction, which is the ML solution--- but that's far from the most obvious option IMO. The obvious solution is to define an explicit data type for the expressions you wish to manipulate (or normal forms thereof). You can even embed functions into this syntax by using HOAS or similar tricks. Now, having this explicit representation of syntax on hand, the fact that "running" these expressions is a semantic valuation function is obvious; as is the fact that you can choose different valuation functions. And by using free monads to define your syntax of encoded expressions, the fact that such syntax forms a monad becomes trivial. As with Jedai, I fail to see the advantage of renaming monads into "state transformers". Not all monads transform extralinguistic state. Indeed, I'd wager that the majority of them do not. Fail not to remember that monads include such simplicities as `Maybe _`, `[_]`, `Either a _`, `Either _ a`, `a-&gt;_`, `a-&gt; (a,_)`,... and that these monads are used to *remove* the need for extralinguistic state for explaining the semantics of exceptions, global state, nondeterminism, etc. That's a small part of why the internalization of monadic semantics is significant. We do not need to postulate extralinguistic state in order to explain the semantics of programs using these features. Therefore, we can get by with simpler denotata, and therefore the referential transparency we get is more transparent (as it were). The semantics of exceptions, global state, nondeterminism, etc can all be reduced to the much simpler semantics of algebraic data types. And, more than merely reducing the set of primitives we must explain, we additionally make it possible to discuss subprograms which *do not* use these features. That purity, that ability to exclude the use of "side effects", is what functional programmers find so important. Thus, every monad defines a separate sublanguage, based on what computational effects the monad encodes. And the important thing from a semantical point of view is that we can compose the semantics of these sublanguages in just the same ways we can compose programs written in these sublanguages.
It's still not clear how to desugar inexhaustive pattern matches in `do` notation. Do you require a `MonadFail String m` instance to supply the error message to? If there isn't one, is it an error (inconsistent with other uses of inexhaustive patterns, which are only warnings) or does it default to an exception like in pure code (causing behavior to change if an instance is added later)? Requiring inexhaustive patterns in monads without fail to be made irrefutable and continuing to desugar pattern failures to `fail errorMessage` wouldn't be that bad, I guess. You'd still be able to use more interesting arguments elsewhere, since there's no fundep. 
Another one could be: fromMaybeOr :: String -&gt; Maybe a -&gt; a fromMaybeOr = fromMaybe . error Of course, the best solution is to write total code.
&gt; Refactoring is incredibly easy. (Just follow the compiler errors) With good tools, refactorings should never cause a compiler error. You apply the refactoring, the new program compiles and is guaranteed to be functionally equivalent to the previous version. The fact that Haskell still doesn't have such tools is one of the main reasons why it's not more popular than it is. 
They are tracked somehow for profiling. I don't use the profiling all that much anymore so I didn't spend much time looking into it. Peter's code reuses some of the profiling information to rebuild the DWARF symbols. The regular (non-profiling) bits of the compiler drop the source annotations during Cmm or STG lowering... simonmar or someone more well versed in GHC's internals might be able to give some insight into why it's still like this.
Golang has [gofix](http://blog.golang.org/2011/04/introducing-gofix.html) for doing this sort of automated transformation. I haven't used it, but it seems like a great idea. The difficulty with doing something similar for Haskell is that you really want to write transformations on an explicitly typed AST with all overloaded identifiers resolved. Then adding a constraint to a value and all the values that depend on it whose types quantify over a variable mentioned in the new constraint shouldn't be *too* bad... * Haskell-src-exts and the current GSoC project haskell-type-exts seem like a good foundation to build such a tool on top of, but it would still be a lot of work. \* Dealing with higher-rank functions whose arguments need to have constraints added to them or trying to do constraint simplification on the new contexts would still be a mess. 
Keeping track of source location information may not be feasible for many optimisations at the Core-level, as the transformations may be so aggressive that the resulting code bears little resemblance to the original.
If you need failure handling, use `Either`. Or `ErrorT Maybe` or `ErrorT []`. "No result" does not mean "failure", in both cases, it just means "no result". ...that is, there just shouldn't be a fail for `Maybe` and `[]`.
On the value level that's certainly true: functions and algebraic data are too simple to have hidden surprises or exceptions to the rules, and it's easy to desugar extra syntax into what it really means. On the type level I'm not so sure. The type rules of System FC-pro are probably about as complicated as (say) the operational semantics of a Smalltalk, and the typechecking system that gets you there from the source types (fundeps, GADTs, type families, datakinds and all) is a pretty enormous beast. Still, I much prefer complications that make it harder to tell whether a program will compile to complications that make it harder to tell what a program will do.
'length' along with other functions like head or last can (inefficiently) be written using Foldable. Although maybe it's convenient, I don't think it's worthwhile to bloat the Foldable class with more functions. Also, for some instances of Foldable ie Maybe it doesn't make sense to do 'length'. Instead, maybe use a Container typeclass or something, like {-# LANGUAGE MultiParamTypeClasses, FlexibleInstances, FlexibleContexts #-} import qualified Prelude import qualified Data.Sequence as S import Prelude hiding ( length , (!!) , head , last ) class Container c i e where length :: c e -&gt; i (!!) :: c e -&gt; i -&gt; e instance Integral i =&gt; Container [] i e where length = fromIntegral . Prelude.length x !! i = x Prelude.!! fromIntegral i instance Integral i =&gt; Container S.Seq i e where length = fromIntegral . S.length x !! i = S.index x (fromIntegral i) head :: Container c Int e =&gt; c e -&gt; e head x = x !! (0 :: Int) last :: Container c Int e =&gt; c e -&gt; e last x = x !! (length x - 1 :: Int) 
Why doesn't 'length' make sense for Maybe? It might be that 'length' should actually be called 'size', but getting either 0 or 1 for Maybe might be useful in some circumstances.
Buh?
The answer is: because you can fold things with no sensible (i.e., uniquely well-defined) notion of length. For example, you can fold trees of ints to compute a sum. What is a tree's length? The number of nodes? The number of branches? The maximum width or depth?
I meant the above generally, not just for maybe. The idea is make non-totality explicit with case expressions and helpful error messages.
[Here](http://www.catb.org/jargon/html/H/hysterical-reasons.html).
Indeed. FWIW F# includes such a function, called [Option.count](http://msdn.microsoft.com/en-us/library/ee353455.aspx)
you can let the semantics of length be getSum . foldMap (const $ Sum 1) for every foldable type
I'd hope rewrite/specialization rules could take care of optimizations, since the set of methods that could be made more optimal is an open one. Or at least, an extra type-class.
(5) refers to the use of equational reasoning (Leibinz's Rule) in studying your code. Since "=" is used as assignment in imperative languages this property of the source code is "lost" or at least hidden. Haskell gives you this tool back.
Hopefully, once he's landed this he can work on that :-)
To me, though I like Haskell, it's hard to imagine a concise yet convincing answer to that. Haskell has a very different style to other languages. In learning to use it, you go through a number of "paradigm shifts". I think what that means is examples that illustrate the benefits to the already-converted aren't convincing to the uninitiated - they can easily end up looking like cryptic nonsense. And the explanations full of functional programming jargon don't help. Of course the same issue exists when teaching any kind of programming, mathematics, or anything sufficiently abstract - the "why" often sounds like nonsense because the context for understanding it isn't there. Sometimes, you can't really understand the why until you've got sufficient practice with the how. Analogy - Laplace transforms as a possible motivation for partial fraction decomposition can't make much sense to people who've never heard of Laplace transforms. The same for other applications of partial fractions. So how do you motivate learning partial fractions? The answer I remember from school was "you'll do badly on the partial fractions part of your test if you don't". 
That's not uniquely well-defined. You might say "size" is a better name for it. And I say, "fine". But "length" is not named "size". It's named "length", a word that has implied semantics many Foldable data structures don't satisfy. It does not belong in a "Foldable" class. It belongs in a "length" class, and even then, I would stay away for not having unique, well-defined semantics.
Yeah, but I reject the Foldable instance implying length semantics on the face of it. There are many semantics for length, and **choosing** the Foldable semantic is a **choice** I don't want foisted on me.
&gt; We know the right solution: Functor should be a superclass of Applicative, which should be a superclass of Monad. EEK! * Monad should be a superclass of Bind should be a superclass of Apply should be a superclass of Functor * Applicative should be a superclass of Apply should be a superclass of Functor [The Semigroupoid Way of Thinking](http://hackage.haskell.org/package/semigroupoids) is so so much more amenable to neat code in practice (the fail issue aside). 
Thank you, this Functor class was what I was looking for.
It probably should be called "size" and be part of another class as there might be things that have a size but aren't Foldable, but in general, yes. Oh, and it wasn't necessary silly of the authors *back then*, but only in hindsight. Hopefully we'll once write up a new Haskell standard that dares to be backwards-incompatible.
If refactoring is understood narrowly, then we can generalize the point 1 above to "incremental changes are increadibly easy". Many changes cannot be expressed as realistically reusable refactorings.
That's something he has been saying for a long time... but Haskell has definitely been sliding towards the useful and safe quadrant for the last ~10 years.
Yea, I always have mixed feelings about downvotes. I made a similar mash up project for getting scheduling feedback from the community in a project I work on, and I'm always asked to add a 'wontfix' button. My view has always been that if you want to add negative (though constructive) criticism that it's much more valuable to be able to expand on that with an actual rationale. Maybe I can collaborate with Chris on his HES, and so that if a user clicks 'downvote' or whatever, they are prompted to provide feedback?
I hope the Haskell community is mature enough for that, on Hackage. I'm not suggesting it for general social sites. However, I also hope that a mandatory field is a kind of trigger for thinking a bit more about what exactly is wrong with the given package.
One could easily argue that a downvote *never* provides constructive feedback, so even if you have 90% fake comments, at least you've gained 10% useful feedback.
I don't like downvotes. They are used too often to silence people with non popular but legit opinions.
Why would downvotes be useful? Upvotes suffice to make the good things visible and the bad things stay on bottom.
I would personally like to see Slashdot-esque downvotes: the ones that come with an obligatory categorization. For example, "-1 abuse of partial functions". That way, people that aren't bothered by certain categories that are popularly considered "negative" could "turn off" their view of those downvotes, or even turn them into upvotes.
&gt; Register a Raskell Account Noooooooooooooooooo :( Lately I refuse to create "new" accounts, *especially* for trying out services like this that I may never use again. Please please *please* let me use my Google account instead. Would it help if [authenticate](http://hackage.haskell.org/package/authenticate) had a specialized snap integration package?
I did the same thing for hpaste. I made title and author fields mandatory, because [80% of the titles were useless](http://hpaste.org/browse/page/906) blank “-”, ungoogleable or browseable. Now, with required fields, [if you look](http://hpaste.org/browse), maybe 10-20% are useless. Some people complained about having to type, but not much.
This already exists in Chris' script in the form of "stars".
Downvotes are the difference between "niche" and "considered harmful". You want to know *P(liked it | tried it)*, which is hard to estimate from *#(upvoted it)* alone. Comments of course are far more informative, but I'll take whatever bits I can get. 
No relation to the scooters.
I can't seem to get it working on chrome, it complains that "extensions apps and user scripts can only be added from the chrome web store".
I actually have exactly this implementation for 'lengthOf' in 'lens'
As Tekmo said above, the goal is not only filtering, but constructive feedback. Maybe the "bad" package is actually not bad, just not mature yet.
This experiment supports the "thinking trigger" theory then, working even in a less serious context!
Could be nice if you used a cookie to feed the author automatically in repeated pastes?
Done.
Awesome! thanks :)
Have you taken a look at the new behaviour of `+RTS -xc` in 7.4.1?
"Building up expressions," which was Jedai's point I thought, is a feature of call-by-name. It has nothing to do with monads. Before monads, Haskell used "continuations" to encode effects, which worked out the same way. Before all these things, Algol 60, a *call by name* imperative language, used *commands* (which form a *monoid*) to encode effects in exactly the same way. If you talk to any Algol programmers or Simula 67 programmers, they will tell you how it was done. As I said, you can do exactly the same kind of "building up expressions" in call-by-value languages *under the lambdas*. So, the effect may be a bit harder to get, a bit less intuitive to see, but *it is there*. It is there even in C and Java. For instance, there is a technique that programmers call "call backs". You store an expression in some slot of an object, and let the object invoke it whenever it needs to. That is exactly the "building up expressions" idea that Jedai referred to. [I thought Jedai was talking about state transformers because he mentioned "side effects". When you use the List monad for instance, there is no state or side effects involved. And, list comprehensions have been used for ages before Wadler figured out that they were an instance of monads. So, monads systemized a lot of what we knew. But they haven't changed the world, not in any real sense.]
I have not, this looks very interesting! I'll be sure to give that a try
I got the same message, you can install it by downloading the extension and then dragging and dropping it into the chrome extensions window (tools-&gt;extensions).
Maybe use Boost libraries for cross platform
Slightly relevant: to implement a typesafe API, I recently added this line, among others: (Just (Refl, Refl), Just (Refl, Refl), Just Refl) -&gt; Just (Refl, Refl)
Hmm, I don't know. I think that it's because there's no good way of updating the core libraries without breaking everyone's monad instance. This is related to the issues brought up in "Scrap your type classes", and resolved by Default Superclass Instances / my Instance Templates proposal.
What difference does it make if you have no prior programming experience? Such a person isn't going to immediately know what either set of jargon means. Quite a few of the listed pieces of Haskell jargon are not Haskell specific, others are not hard to understand at a cursory level (as long as you can get a definition), and others are highly specific to libraries that I haven't even used before (so I'm sure someone can learn without encountering them; I managed to almost a decade ago when the jargon didn't even exist). This is the same problem, as far as I can tell, as lamenting that Haskell requires different execution intuition than the majority of other languages. People learn a bunch of jargon when they start out programming, and eventually get to a state where they know most of the jargon used around most languages. Then they encounter Haskell, which uses a lot of different jargon, and being reset to square one makes it seem a lot worse. But I'm skeptical that it's significantly worse for a complete neophyte.
exciting! I have two Raspberry Pi's on order. Hope to get one by Christmas :) Running Haskell on them is definitely priority #1. In the meantime I am suffering with C and arduino :)
I would hazard a guess that most people that want FP on the .NET platform are content to export Haskell functions via the FFI and then import them on the .NET side or else use F#. Also, it's a lot of work to write a performant Haskell compiler and nobody on the GHC development team is interested enough in it to write a backend.
Some one to do it? Does that mean you are volunteering?
I wonder if a coq-like "eauto" effect could be accomplished with clever use of typeclasses.
My apologies, I wasn't trying to impose on the community by asking a dumb question, or by implying that I am entitled to a .net implementation, I was just wondering if there are any specific CLR issues that may be difficult or insurmountable, that may be preventing a .net version of Haskell
When I see this: updatePost updatedPost = do meadow ← get put meadow { meadowPosts = updateIx (postId updatedPost) updatedPost (meadowPosts meadow) } I can't help but think that it could be written more beautifully if `meadowPosts` and `updateIx` were lenses (also with named field puns): updatePost post@Post{postId} = meadowPosts . updateIx postId ^= post 
My wild guess is that there are CLR annoyances that would make it difficult (though not impossible) to optimize in the same way GHC does. Note that [Frege](http://code.google.com/p/frege/) exists as a JVM solution, although it claims to be "in the spirit of Haskell" rather than 100% Haskell, and chose to differ from Haskell on subtle points.
Yea, I use lenses a lot with acid-state. IxSet works less well with lenses directly (I tried it with `data-lens-ixset` but it's not a legal lens) although with these new lens packages you can skip the laws by separating getters and setters.
Disregard my comment really, I probably could have phrased it better. No need to apologise either.
I have a hard time seeing the use case for acid-state. How do you backup your data ? How do you restore ? How do you do incremental backups ? How do you access the data from other languages ? from Reporting and analysis tools ? 
In the short term you just use rsync to backup the log files. In the medium term, you can run a fulltime live mirror. To access data from other languages you would typically create a web service and export the data via JSON or something. Not sure what sort of reporting and analysis tools your are talking about... Obviously acid-state is not the right tool for every job. 
Hm.. i thought there was only one... 21.0.1180.60
I just use LastPass to generate unique passwords for all the throwaway sites. No problems.
"types /= documentation", "citation /= documentation", ...
Or agda-like "auto" through clever use of emacs?
What about Mono? Or Microsoft Research's contributions to Haskell? As long as you aren't developing a .net application using WPF and code behind, or using features which aren't supported in Mono yet, for example Entity Frameworks, your generic C# or F# code should run fine on the Mono VM. 
I actually just wrote about using type classes to infer proof terms... http://www.jonmsterling.com/posts/2012-08-05-static-and-dynamic-proof-inference-in-haskell.html
F# is the main reason, and should be highlighted. Yeah, it's not Haskell, but for people who want FP on .NET, you'd be hard pressed to find a better solution. &gt; export Haskell functions via the FFI and then import them on the .NET side I'm not sure if I understand why you'd want to do this. In my experience, the .NET ecosystem is larger than the haskell ecosystem. Most Haskell modules have similar .NET libraries. What you'd gain from using the Haskell modules instead would probably be lost in the FFI.
I doubt things have changed much in which case GHC isn't a true cross-compiler but you can make a build of GHC for one platform and make it target it another platform.
I wouldn't get your hopes up too high, you really don't want to be building non trivial apps in any language on the raspberry pi. I wouldn't even want to develop on it if I wanted to target it.
This was exactly my experience trying to get it rolling on Arch on my Raspberry Pi. The version of GHC hanging around in the Arch arm repositories is a version 6-something-or-other, and again, interactive mode isn't supported. Silly arm. Great to see it's on its way though!
&gt;Are there any proposals or patches that have made it far enough to be excited about in this regard? I have literally no idea how relevant this is, but I recall these two things existing: http://hackage.haskell.org/trac/haskell-prime/wiki/TypeDirectedNameResolution (page is 3 years old) http://hackage.haskell.org/trac/ghc/wiki/Records/SyntaxDirectedNameResolution (page is 5 months old)
Yeah, I simplified the daily programmer post code and added it to the repository as [Control/Monad/Tardis/Example.hs](https://github.com/DanBurton/tardis/blob/master/Control/Monad/Tardis/Example.hs). First I'll work on haddocks for the package, and then later you can expect a post on my blog explaining the Monad instance and how to use it. The name "tardis" is perhaps deceptive because it does not involve time *travel* per se, rather, it's just two streams of communication: one goes "forwards in time", and the other goes "backwards in time".
The [2012 State of Clojure survey](http://cemerick.com/2012/08/06/results-of-the-2012-state-of-clojure-survey/) contains a few interesting thoughts regarding the CLR. Compared to the JS, C, Python, and Scheme backends, there was practically zero interest in the .NET backend.
I hadn't done it, someone else would have... 
ok for the syntax and semantics, but I'm wanting to look at the wider picture - hence this series of articles. It won't just stick to Haskell
The way I do this in my projects is to separate types that have similarly named fields into separate modules and use namespacing to distinguish which one I mean. As you very well know, you can also use module dictionaries to not have to separate them into separate files. However, I wish there were a way to derive lenses from fields that did not rely on Template Haskell.
hey, the real fun is happening over there: http://www.reddit.com/r/programming/comments/xmd4x/thinking_functionally_with_haskell/
There was an effort to come up with a solution earlier this year. Unfortunately this problem is proving impossible to solve with the language features as is (and support all of Haskell's complex type machinery). We need to change the language (possibly in a minor way), but that is a paralyzing thought for the community. 
This posting looks via a recruiting agency. Folks may want to cut out the middleman and follow up directly. Neil Mitchell has a blog post describing how to do so: http://neilmitchell.blogspot.com/2012/08/standard-chartered-bank-where-i-work-is.html
I'm not saying that I'd want to do so, but the typical scenario is wrapping a Haskell program with a GUI, in this case a WPF framework. Also it's a good way to Trojan horse some Haskell into a company.
I love the take away message of this talk: "Seemingly obvious things take a long time to discover".
&gt; The way that acid-state records events allows you to rollback back through your events one at a time. So, you don't lose anything. Doesn't this also imply that you never free up space from large data you had in your data store in the past?
To be fair, "trojan horsing" any new language into a company is probably a bad idea. You won't work there forever and you may be the only person who knows how it works. In general, it's best to work with what your team knows well.
All Haskell? Come on, sadness is even above 1...
&gt; We need to change the language [...] but that is a paralyzing thought for the community. That's hardly a description I would've expected to hear of Haskell. I think, rather, the philosophical issues mentioned by godofpumpkins are the more significant impediment.
The use case is, more or less, "I have this haskell program with some internal state that I manipulate using pure functions, and I want that state to be reliably persisted to disk so that it's still there when I restart the program." In a lot of cases, you'd use a database for that, but that adds a lot of extra complexity that might or might not be worth it. The way acid-state works (or at least, the way happstack-state worked, it's been a couple years since I used it) is that you provide the functions that you'll use to manipulate your state, and acid-state invokes some template-haskell magic to turn those into IO actions which do the same state manipulation, but also, in the case of a mutating function, they write out to a log on disk what function you called and what it's arguments are. It can also write out the current state, to serve as a checkpoint. If you kill and restart you program, it will restore from the most recent checkpoint and then replay all the functions with the arguments given in the log. I think acid-state is useful in cases where you want something that just works and doesn't depend on some other program you have to manage separately (i.e. a database) and you don't need to directly access the data from other programs. For instance, I could see this being useful if one wanted to write a distributed hash table using cloud haskell, and one wanted to be able to shut down nodes and bring them back up without forgetting all the data they had stored.
Paid recruitment people never disclose the name of the company, since they get paid commission, and if you go direct to the company they wouldn't get that. If you talk to them they will give you the company name though.
It's upstream now: https://github.com/travis-ci/travis-build/pull/28
I just love the abstract at the end \^\^: &gt; Dr Paul Callaghan rides a big, fast motorbike, and suggests that this informs his programming language opinions.
I would say "composability". 
I tried really hard to get this going: https://github.com/mwotton/heroku-buildpack-haskell it turns out to be more difficult than expected - 200Mb cache space is not much when you need cabal &amp; ghc, and downloading it each time is slow even off S3. I'd love to hear of a better approach. You can always just build a binary and upload to heroku, but yesod binary file sizes make this a not-incredibly-fast approach.
I would really love this. It should spit out line numbers and names.
This is a good description. A framework with opt-out.
I also tried to create a buildpack. But the fact is I didn't find a way to compile a Haskell program without downloading at least 100MB of stuff and using a lot of cache resources during compilation. May be I'm wrong, but won't a part of the problem could be resolved if we had an apt like tool for Haskell. I mean a cabal which would download binaries directly instead of getting the source and compiling them. For the anecdote: the first time I compile a new yesod website the "cabal-dev install" commands takes about 15 to 20 minutes on my very slow VM. Download binaries directly will save me time and computer consumption. Each time I do this, I am under the impression that I kill a polar bear. Also, here is the size I use for ghc on my ubuntu VM: ubuntu /usr/lib/ghc % du -k . | sort -n | tail -n 4 58088 ./Cabal-1.14.0 64892 ./base-4.5.0.0 166200 ./ghc-7.4.1 402436 . About 400MB and about 230MB just for ghc and base, this is very Large. And we can compare to the only 16MB needed for ruby 1.9.1. (without the gems), and 28MB for all gems I use. Haskell need about 10x more space. 
The "you are probably the only person who knows how it works" bit is true no matter which language you use in my experience and it is a lot harder to figure out if a modification in a program you don't know broke something if the language is dynamically or less potent statically typed languages.
That's actually pretty simple: * `iterate (*2) 1 == map (2^) [0..]` * Given a positive integer `x`, `length (show x) == ceiling (log (x+1) / log 10)` * This means the `i`th element in the printed list has a length of `ceiling (log (2^(i-1) + 1) / log 10) + 1`, or approximately `log (2^i)/log 10`, or `i * (log 2 / log 10)`. * The length of every "increment" (modulo the screen width) is proportional to its position in the list, so the actual position of the comma is proportional to the square of the position in the list.
 function fib(n : int) : div int { if (n &lt;= 0) then 0 elif (n == 1) then 1 else fib(n-1) + fib(n-2) } &gt; Note that the type inference engine is currently not poweful enough to prove that this recursive function always terminates, which leads to inclusion of the divergence effect div in the result type. I wonder if there are plans to make it powerful enough for that. One option would be to allow the programmer to explicitly mark 'n' as a termination metric. ATS for example uses that strategy to allow for more complex proof functions. http://www.cs.bu.edu/~hwxi/ATS/TUTORIAL/contents/termination-metrics.html 
Perhaps something to add to `ghc-mod` which is already integrated into Emacs and Vim.
My favorite abuse for monadic pattern matching (which doesn't rely on fail exactly, but does rely on case-desugaring) data TEq a b where Refl :: TEq a a -- witness to equality on types data T a where -- universe of types TInt :: T Int TBool :: T Bool TFun :: T a -&gt; T b -&gt; T (a -&gt; b) TList :: T a -&gt; T [a] -- etc eqTyp :: T a -&gt; T b -&gt; Maybe (TEq a b) eqType TInt TInt = return Refl eqType TBool TBool = return Refl eqType (TFun la ra) (TFun lb rb) = do Refl &lt;- eqType la lb Refl &lt;- eqType ra rb return Refl eqType (TList ta) (TList tb) = do Refl &lt;- eqType ta tb return Refl -- etc eqType _ _ = Nothing 
I wish the explanation would have been easier to understand, than figuring it out for yourself… ;)
There is no *single* algorithm that can solve termination for *any arbitrary* program of course. But we have several strategies to solve that for *particular* programs. One such strategy, applicable to the presented version of fib, is to use a termination metric. For a function f(x) we choose a termination metric m(x) such that * for each recursive call f(x') within f(x), we have m(x') &lt; m(x) and * the function f contains a condition k(x) for terminating recursion, where k(x) &lt;=&gt; m(x) &lt; c for some predefined constant c. This guarantees that each recursive call will, in a finite amount of steps, lead to the condition for terminating recursion.
 length (show 100) == ceiling (log 100 / log 10) False
I never understood why someone would want to avoid using language extensions which have been in GHC for at least some time. The only reason I can think of is: compatibility with other compilers. But is anyone ever going to compile/run a yesod-routes based application using something other than GHC?!
OtherLicense?
To anyone else trying this, you need to compile with profiling to use this ("-prof" and optionally "-auto-all"). You can read about it in the [GHC docs](http://www.haskell.org/ghc/docs/7.0.1/html/users_guide/runtime-control.html).
I'd like to join that workshop (living in CPH), but I cannot find any registration information. It's colocated with ICFP, but does it require ICFP registration (quite expensive)?
I sent this to the web-devel list in response to a similar question. Somebody claiming to be yi huang wrote: &gt; Why prefer code generation over template Haskell? Isn't them essentially &gt; the same thing, and template haskell is performed automatically. Also, from Reddit (nicolast): &gt; I never understood why someone would want to avoid using language &gt; extensions which have been in GHC for at least some time. The only reason &gt; I can think of is: compatibility with other compilers. But is anyone ever &gt; going to compile/run a yesod-routes based application using something &gt; other than GHC?! First off, yes, Template Haskell is very similar to code generation. There are a few reasons I would like to avoid it. It's a language extension. I try to avoid those in general, in every stanardized language I code in (C89, R5RS, Haskell98) for several reasons. As nicolast said, compatibility with other compilers is a big one. When I get a piece of code from someone who assumed that "what MSVC does" or "what Racket does" is the same as "anyone can run this", it makes it quite difficult to use my favourite implementations of those languages. I don't want to make assumptions about other people's environments, or what will be useful in the future. Maybe someone writes a Haskell interpreter that makes use in some context I haven't even imagined much nicer. Who knows. Additionally, any other static analysis/code processing tools (like, say, hlint) *also* needs to support whatever syntax extensions you're using (semantics exceptions may or may not apply here, dependings on the nature of the tool). Requiring that every tool author support all my favourite extensions limits my tool options, and makes life harder for tool authors (since they cannot just look in one place for the spec and write to that anymore if they need to look up compilers' extensions as well). Will anyone ever compile/run/analyze a yesod-routes based application using something other than GHC/hlint? (Actually, does hlint support TH. It might.) What specifically about yesod-routes makes this less likely? What drew me to Yesod.Routes.Dispatch was its relative purity in terms of extensions/dependencies, etc. Additionally, I find Template Haskell specifically (and some other language extensions, like Overlapping Instances) can make code harder to read (for me) and possibly harder to reason about. A code generator makes a file that I can read for comprehension, edit if I want to, etc. Ok, that's a bit of a long answer to a short question, but it sort of sums up my motivation vis extensions in general and TH in particular.
I chose a text input because it seemed easy to build and is similar to how many other routers do it. A useful DSL that works in a similar way might be interesting.
I think people usually pick BSD3 in that case, since its the closest of the options given. Anyone know what it would take to get a BSD2 option in there, which is essentially the same as MIT and ISC?
Another research language with full effect inference! It seems to be quite basic at the moment, I wish there were draft papers or source code to look at. (I would like to take this opportunity to plug in [Disciple](http://disciple.ouroborus.net/), a very nicely designed [dialect of Haskell with region typing + effects](http://www.cse.unsw.edu.au/~benl/papers/thesis/lippmeier-impure-world.pdf) /end of plug)
Well, I use `vi`, but without any plugins or configuration, for two reasons. First, this keeps my code simple and clear (by necessity). Second, one of the advantages of `vi` over `emacs` is that it is installed everywhere, so if you stick to defaults then you are automatically ready to program anywhere without any setup.
I just use my normal vim setup, nothing Haskell specific. I have the "show me whitespace errors" stuff on, but that's useful in any language.
There's [Haskell mode for vim](http://projects.haskell.org/haskellmode-vim/) and donri's [vim2hs](https://github.com/dag/vim2hs). I prefer the second one but my main editor is Sublime Text anyways.
Well vi doesn't support plugins, so without plugins is pretty much a given. How do you deal with collaboration though? I currently use vi as well, and dealing with other people's code is quite an irritant, given the widespread using of arbitrary alignment in haskell code. Do you really use no configuration at all? Don't other haskell people get annoyed at you for having tabs in your code?
I don't use tabs. I use s/^.../.../ to change indentation. As far as other people's code goes, I just resize my terminal if they go over the 80 character line.
I also use ghcmod-vim/neco-ghc. Most 'vim' like of all the Haskell plugin options imo.
he must mean vim wich does support plugins imho.
Entirely agreed - I also don't like it.
Here's what I use: http://haskelllive.com/environment.html You can see it in action in my videos. It's quite nice. TL;DR: haskellmode-vim, ghc-mod-vim, neco-ghc, syntastic.
With a new 720p format with a bigger font and higher contrast colors for better legibility. I fixed a bunch of stuff from the first episode and only made a few errors in this one (I hope). I also explained composition really poorly -- I should have just said that f . g x is f(g(x)) -- but I think overall it'll be a fun watch. Please tell me what you think!
I remember your video, it was a sweet setup. I'll probably try this first because it looked so nice. Unrelated, but your videos are awesome and you should definitely keep doing them.
Thanks! I just released episode 2, which is actually even better than episode 1 (episode 1 just had haskellmode-vim, which might be all you need, although HLint and HCheck are very nice).
I guess you mean `(f . g) x`! 
Then don’t. They should feel like bad people for pulling a fraud scheme on you, by using the imaginary property delusion and a epic failure of a business model, and then insulting you when you don’t buy into it. If they wanted money, they should have taken it when they did their *service* work of programming the software. You can’t just go and tell people to pay real money for virtual copies. The money took work to make. The copies just took “cp”.
Really good and fluent presentation! I like the new screen layout, very pleasant and much easier to read.
For 1 or 2 indents I just type in the spaces manually (4 spaces per indent). Anything more than that I just type `Ni&lt;space&gt;&lt;esc&gt;` to insert N spaces. For the indentation thing you are talking about, I just duplicate a line (i.e. `yyp`) and then overwrite the part that differs using the `R` command. This trick is also useful for indentation relative to the previous line. Just duplicate the previous line, seek to the first non-whitespace character (`^`) and delete everything to the right (`d$`).
That doesn't drive you nuts? I ended up just writing my haskell code like I would in any sane language because doing what you describe is so irritating, and switching to another development environment is even worse. But this has the problem of people not wanting patches from me since my code is formatted sanely.
Well, we have six or so corporate licenses and I was the one who decided to buy them so yeah, I'd say it's worth it :-) There's a nice Haskell plugin as well which also adds a colour scheme that looks like Hackage's source code view :]
That doesn't really make any sense. They are taking money for the service work, it's just delayed from when the service actually occurred. Plus, they're still actively developing it, so you're paying for a service that's occurring right now as well. Whenever I pay for a piece of software, I understand that I'm paying for the work the developers did, not for the work the cp command did. I think of it like buying a car (or any other physical product). You pay for the cost of manufacturing the car, but also plenty of extra for whatever other costs the car company has, including R&amp;D. This isn't fraud; it should be obvious that no company can survive while charging only the manufacturing cost for their products. In the same way, I pay for the manufacturing cost of software ($0), plus the other costs of the business; development, servers, advertising, whatever. 
&gt; That doesn't drive you nuts? Nah, he's just a vi user with a proficiency at least ten times mine. And I'm already beyond sanity. Editing paragraph text efficiently is for starters, efficient ASCII-art capable editing skills are a thing for masters. There's certain things I'd miss in such a pure setting, like automagically inserting type signatures, the language extension popup, navigation via tags, jump-to-error and similar things. Not having haskellmode's rather brokenish tab-expansion is at the very end of that list, if at all.
&gt; Your statements regarding Framework vs Library are very debatable; it's far from "fact" that frameworks are Bad. The biggest way in which a framework is bad is that it takes control of your mainloop and composes poorly with anything else that wants to take control of your mainloop. Haskell nearly makes this irrelevant as almost any control structure can be inverted, the "real" mainloop is in the runtime and nothing stops you from running something outside the Yesod loop if you like, etc. Almost none of the usual "frameworks are bad" problems apply in Haskell.
I use haskellmode-vim, vim2hs, haskellconceal, and tabular (not strictly haskell, but useful and integrates into the others). That said, I'm looking forward to trying some of the other ones mentioned here.
I'm just now seeing this, don't know how I missed it. Thank you *so much* for the time and effort you put into this write-up. I've been very curious about Monad Transformers for a while, and I feel like reading my original code in contrast to the changes you made for me will give me a solid feel for what is actually happening (seeing as I'm reading my own program in a different fashion). Honestly, I'm really baffled by the strength of Haskell. I feel like I grasp many of the basic concepts pretty well after roughly 9 months of using it, but there's always a nearly infinite stream of new concepts to learn about. Thanks again for the review. :D
You're welcome! I'm always happy to review other people's code.
the cool thing is that this applies for many functions, not just this one. [Here](http://tunes.org/~nef/logs/haskell/12.05.30) is the #haskell irc log of when I discovered [it](http://img687.imageshack.us/img687/6310/fibs.png) with the fibonacci numbers. (Search for "I tried to zip" in the log)
&gt; and delete everything to the right (d$). or (C) to save a keystroke and be in insert mode right away. Although this could be vim only...
Does anybody use [scion](https://github.com/nominolo/scion/) in conjunction with vim? There was an attemp to maintain it [here](https://github.com/MarcWeber/scion-backend-vim) but it seems like it died down.
I love these, hurry up and finish the next parts! :P
I got to say, that looks beautiful. Pity it's a research language I can't really afford to get aboard ATM.
Remember to keep function application and function composition as two different subjects. And maybe mention that Haskell makes the space character (aka, function application) high precedence.
You have good taste in music! Make sure to check out [SOUNDSHOCK 2: FM FUNK TERRROR!!](http://ubiktune.org/releases/ubi044/) as well :)
I published my Vim configuration, which I use (among other things) for Haskell development, here [1]. It contains several Haskell-specific plugins as submodules. Check the screenshot for some of the functionalities. I obviously introduced some errors for demonstration purposes ;-) [1] https://github.com/NicolasT/dotvim
I don't quite follow. Are you saying a “good monitor” will not show me the palette #B40, #044 and #000, but a well-designed one, and that now that you have a “bad monitor”, such as what I apparently have, you will be able to see the “bad monitor version” of your palette and “fix” it? I am genuinely confused.
I've been using vi for everything for 15 years, I would say I am quite proficient. It still drives me nuts to have to do inefficient things like that.
You’re kidding, right? Don’t drag that sleazy pseudo-political false dichotomy bullshit parade into serious topics with actual meaning. End of thread.
Haskell is #yolo
&gt; package up operations to be performed with the datatype Note that this is the heart of "true" object oriented programming. "objects" are data bundled with operations you can perform on that data.
Well, let's play his game. His conclusion: haskell - extremely conservative, dynamic languages - extremely liberal. He completely missed one of the defining features of conservatism - ignorance. In real life conservatives cluster in heavily religious and uneducated areas. By that measure haskell/ml are the bastions of liberalism whereas java/c# are the most conservative, and python/ruby fall in the middle (road to liberalism). Another measure is a social net. Liberalism is all about not letting people die/suffer from deceases/poverty. Heck they even let people recover from financial mistakes (housing bubble). Again this directly correlates with haskell/ml providing the strongest and widest social net (strong type system) and dynamic languages being a "wild west" of programming industry. And Rubyists are of course the mormons of programming. He got it ALL wrong :)) 
Except saying he jumped the shark implies he ever said anything insightful or relevant. As far as I know, he is just (in)famous for being able to use so many words to say so little.
it's painfully legible
Nice article! Haskell seems to sit at the apex of a triangle whose base is the continuum from liberal to conservative in Yegge's sense. I guess many programmers don't even know what's achievable. *Lisp macros: Why use macros when you can do it properly in Template Haskell!* In many cases lazy evaluation and clever combinators can do exactly what you'd want to do with macros, but in a type-safe way! There's generally no need to go as far as template Haskell. 
hahaha. Man, this is one of the best pieces of satire I have read in a long time :P
By the number of downvotes i'd say people take it quite seriously and are mostly offended. :) 
Yegge also admits to drinking copious amounts of wine before his posts. His post on why legalizing marijuana is a Hard Problem is really good, though. People oversimplify things they don't understand.
Woah, step off, Edward is one of the GHC compiler hackers, if he thinks it's worthy of discussion, there might be something to it. That said, I generally agree that Yegge gets a little bit too into analogy. There's a core idea that's worthy of discussion about risk aversion versus novel solutions that I'd like to see people actually talk about with regards to language. edit: Why do I keep thinking Edward's name is Ezra, I do not know.
Do you have examples where lazy evaluation helps to reduce the need for macros?
It's a convenience function from my `errors` package: http://hackage.haskell.org/package/errors Hoogle only searches a limited set of packages by default. If you want to search all packages on Hackage, use [Hayoo](http://holumbus.fh-wedel.de/hayoo/hayoo.html).
A discussion about risk aversion is useful, but couching it in political terms is an absolutely terrible way of trying to have that discussion. Acknowledging Yegge's fundamentally flawed hypothesis as a valid way to frame the discussion doesn't help.
At the core, I would go with Haskell as Conservative or maybe libertarian. In America, young people will associate liberal with the good guys and conservatives as the racist old white guys. I look at it differently. If you look at the core political philosophy, 1940s - 1980s? conservatism is supposed to be rooted in limited government inline with early American and traditionalist approach to governing. It is "conservative/traditional" ideas. Liberalism 1940s-2000s seems to be rooted in spending on social issues (trying to address poverty and wealth gaps), attacking issues that have arose out of racism, influx of new immigrants. Liberal/progressive are not conservative. ... As it relates to software. C/C++/Java are conservative. By design, they share old ideas from 1970s C style coding. Haskell, not the C paradigm, is almost a bit conservative. Static type checks at compile time. Rigid syntax. Lisp/Python/Ruby are liberal and progressive. Screw it, just put some code in there and run it. Don't worry about compile time checks or a bunch of ugly type definitions next to variables. Is conservative bad? Is haskell bad because it is conservative. No.
Sure, but Python 2.7.3 (default, Aug 1 2012, 05:14:39) [GCC 4.6.3] on linux2 Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; if False: ... 1/0 ... else: ... 1 ... 1 That's the default behavior of the if condition in python. I can't think of a language which has semantics ite e1 e2 e3 ---------- e1-&gt;v1 e2-&gt;v2 e3-&gt;v3 Edit: I think I understand the source of my confusion. I thought you were referring to the if construction in a programming language, but you were referring to the function evaluation semantics. I think all languages have lazy semantics for if, but you're absolutely correct that almost only Haskell has lazy semantics for application. 
Please please please let's stop taking this trash seriously. It does not deserve a response. Yegge's typically dishonest drivel does not belong here. The dressing-up with entertaining writing style is not a satisfactory apology for talkin' crap imo.
Yegge's primary motive in blogging is to build a better shark. So he can jump it.
I'll make it easier by linking ~directly to an example reimplementation of [if in Io](http://www.iolanguage.com/scm/io/docs/IoGuide.html#Objects-Blocks)
FWIW I had [a short discussion](http://hpaste.org/raw/72938) with a non-Haskeller (but Lisper) about the benefits of built-in laziness (including in contrast to macros).
Thanks for doing these! I'm learning a few tips from these screencasts and I'd love to see more of them!
That's not true. Look up Church encoding.
Of course you can do whatever you want in Python, since its a turing complete language (heck, you could write a Haskell Interpreter in Python and go nuts). The point is that it is a pain in the ass to create lazily-evaluated constructions in Python and when you come up with one its often unwieldy or doesn't play well with existing parts of the language.
Thank you for not only explaining some fundamentals of haskell, but also giving us a little detail about your vim/guard setup. That's really helpful!
A simplified but more-or-less correct story of what happened is that the mtl used to be monolithic and in a single package, but was split into transformers and mtl for 2.0. transformers has the benefit of not requiring extensions, while the mtl does. mtl re-exports the transformers types at the locations they used to live in mtl for backwards compatibility. The full details of how the split happened are more convoluted, and involves a painful tale of how a separate monads-tf and monads-fd that were originally built on top of transformers, but which were incompatible with the mtl, and this splintered the community support between different libraries, before ultimately Ross was able to just replace the mtl with the contents of monads-fd plus some a few compatibility tricks. I'm sure there is a made for TV movie in there somewhere. *edit: monads-fd
In any language with effects (including non-totality), in order to be of any use, the conditional construction must necessarily be lazy in the branches. The point is: in an eager/cbv language you cannot define a conditional construction without (a) using macros, or (b) reinventing laziness. Whereas, by definition, lazy languages get (b) for free. Thus, in lazy languages you can define the conditional construction yourself, without macros, and without it being built-in.
Well at least the type checker gets the trains running on time! (jumps out window)
Don't get caught up in the politicization aspect of this; risk aversion/management vs novel programming solutions are 100% worth talking about. I don't know what your background is, but for me, this represents some of the fundamental hurdles I face in my work when trying to push forward on things. I want mental tools to discuss this with superiors and coworkers. We all want to write better code, framing things so people can understand how both "conservative" and "progressive" approaches can lead to this goal is useful.
Yes, **fraud scheme**. The below explanation is exactly the same for software companies and programmers, as it is for the media industry and creative people: ___ **[TL;DR] “intellectual propery” can in practice be treated like a physically impossible concept.** The people that invented it, are people who deliberately created a delusion of “owning” information, even though such a things is complete nonsense… like saying “let’s go north” on *north pole*. ;) In reality, ownership is defined as control over something. That works for your house and clothes only, because there is someone stronger (your government) enforcing it. But all your “ownership” goes out of the window without that enforceability of control. And that’s the thing: Try enforcing control over information. I *really* thought about that one. And you can trust me when I say: Without a DRM/TCPA chip in everyone’s head and every information processing device, this is literally physically impossible to ever achieve. For the simple fact that if you give somebody some information, you will never be able to tell if he passed it on. And if only one person passes it on, *everyone* has access. So the only reasonable approach, is to throw out the assumption of ever having control. And with that, “intellectual property” becomes a (nearly physically) impossible concept. ____ **[TL;DR] The content industry are the *actual* pirates / organized crime, harming the exact people they say they protect and are: The creative industry.** The content “industry” takes the works of creative industry, acts like it’s a tangible good that can be “owned” by employing the above delusion, and “sells” us this fantasy. (Yet behind our backs goes: No, all you get is a usage license. You don’t own anything. Because they know exactly, that it’s not a sale, and never can be.). But the problem is: All they give us, are copies. Which cost exactly *zero* to make. and take exactly *zero* work. So we pay money for nothing. Which is fraud. And they justify this, by saying “We made that ‘thing’, so we want money for it”. Then why didn’t they demand it for doing their *service*? I tell you why: Because that way they couldn’t make money way beyond what the work is worth. Because that way, they couldn’t say “life + 70 years” for the time span to uphold the delusion of a mere copy being worth the same as somebody doing some actual work. And the worst part: It’s not even their work! It’s the work of creative people. Which they *also* defraud, by giving them as little as possible for doing *all* of the actual work. * Musicians get 3.5% of the end user price, *and* they still have to pay the studio from that. The content Mafia takes 96.5% *plus* the studio pay. For taking the file from the musician, doing a bit of EQ work that would cost a *fixed* price in the area of $1000 if you hire somebody (because it’s also a service!), doing a bit of marketing that also has a fixed price, and uploading it to iTunes and Amazon. * With book authors it’s similar. * Movie/TV authors and actors get paid a *percentage* of the final *profit*. Which just happens to be *zero* or even negative, because of creative “Hollywood accounting”. * And game/software developers basically get treated like chicken laying batteries, working their asses off with unpaid overtime, having no free time, and getting a comparatively extortionary shit pay for it. So in essence, they get tons of money for sitting on their fat asses, and handing out copies of the works of others they defrauded. And then they have the audacity, to call *us* thieves, *and* launch a racketeering scheme against everyone who doesn’t play their game!! *Fuck them!* ____ **[TL;DR] By spreading their mindset, you harm artists, freedom creativity all around, and support the schemes of the actual organized crime.** In what kind of delusional world do you live, where it isn’t??
Thanks for confirming that you're a troll.
You were making a joke perhaps, but the automatization routines for Paris's metro line 14 (and now also line 1, which is still in its test phases) were all formally verified using Coq. So in a way, yes, static type checkers *do* get the trains running on time. :)
I haven't used it yet, but http://hackage.haskell.org/package/errors intrigues me
Use my `errors` [library](http://hackage.haskell.org/package/errors), which simplifies using `Either`/`EitherT`/`Maybe`/`MaybeT` and was designed to [solve precisely the problem](http://www.haskellforall.com/2012/07/errors-10-simplified-error-handling.html) that the post you linked described by standardizing on `Either(T)` and `Maybe(T)` as the correct way to handle errors and make it easier for libraries to wean themselves off of `fail` and `IOException`. Unfortunately, Hackage is down right now for some reason so you won't be able to check it out, yet, but I'll still describe how to use it and I will see if I can host the package somewhere so you can download it. Edit: You can download it from here: http://www.filehosting.org/file/details/366035/errors-1.2.1.tar.gz Then just extract it and do `cabal install` in the extracted directory to try it out. You can also check out the documentation by doing `cabal haddock` in the same directory then navigate with your browser to the `dist/doc/html/errors/index.html` within that directory. You import `Control.Error`, which re-exports several other modules that provide convenience functions for error handling. You want to use the `EitherT e IO` monad, and if `e` is a `String` then the `Control.Error.Script` module provides a convenient newtype for that monad which I call `Script type Script = EitherT String IO However, if you prefer to use another error type (such as `[GLEnum]`), then feel free to define your own type synonym, such as: type GL = EitherT [GLEnum] IO Rather than use `fail`, you instead use `throwT`, which accomplishes the same thing: check' = do e &lt;- tryIO $ checkError unless (null e) $ throwT (formattedErrorCodes e) tryGL m = do r &lt;- tryIO m check' return r -- or: tryGL m = tryIO m &lt;* check' Then you wrap every gl command in `tryGL` if there are errors to be checked, or `tryIO` otherwise, and then you can use `catchT` to catch and handle errors. Then, for your main function you can use `runScript`, which provides a top-level error handler that converts `EitherT String IO` back to `IO`: totalGLCode :: Script r main :: IO r main = runScript totalGLCode Of course, you don't have to use `runScript` as your top level error handler. You can write your own, and it's not hard: main = do x &lt;- runEitherT totalGLCode :: IO (Either String r) case x of Left e -&gt; ... handle the error Right r -&gt; ... successful termination
Where is throwT defined? Is it equivalent to the fail definition in MaybeT? (Not sure if EitherT has a fail definition, though it should). I guess it's more polymorphic? (Since fail requires String)
This library needs TH and QQ to be able to generate all the boilerplate required for safe routing, so without QQ there isn't much benefit to using it. You are right about the example in the README file. However, the full example in the source code compiles and works fine. 
It's defined in the `Data.EitherR` module (which is because it is equivalent to the `return` of the flipped `Either`). My [blog post](http://www.haskellforall.com/2012/07/errors-10-simplified-error-handling.html) goes into more detail about the relationship between `throwT`/`catchT` and `return`/`(&gt;&gt;=)`. Also, any time you are not sure what module a function comes from, you can always use either [Hayoo](http://holumbus.fh-wedel.de/hayoo/hayoo.html) (which searches every Hackage package by default, unlike Hoogle), or you can also check in `ghci` like so: Prelude&gt; import Control.Error Prelude Control.Error&gt; :info throwT throwT :: Monad m =&gt; e -&gt; EitherT e m r -- Defined in `Data.EitherR` Or you can use (`:i`) for short. Also, `EitherT` is in fact too polymorphic for fail, whereas `Maybe(T)` discards the string. This is one of many reasons why `fail` is a bad fit for error handling. `transformers` uses the `ErrorT` class which constrains the error value to be something convertable to a string, but my post describes some of the disadvantages of doing that.
I think the last time I checked the basic status was that Hackage 2.0 is complete and ready to be deployed, but the only person that is willing to take on the responsibility of maintaining Hackage is Ross and he has Hackage 1.0 set up in a way that keeps the maintenance light for him so he is reluctant to upgrade to 2.0 where he would have to relearn the new quirks of Hackage 2.0. So basically, the only thing missing is a maintainer who is willing to host it and take on that responsibility.
Can't someone the thing in parallel with 1.0?
http://hdiff.luite.com/packages/archive/ mirrors the actual packages, fwiw.
If you add the line remote-repo: hdiff.luite.com:http://hdiff.luite.com/packages/archive to your cabal config file ( ~/.cabal/config usually ), it will install the packages from there
Yeah, I don't blame him either. He does a lot for the community already.
Me too! I just couldn't find a way to make this episode shorter.
&gt; Other languages could have chosen to use a more appropriate syntax, like &lt;- or :=, but they didn't. Just because they used the = sign doesn't mean it means the same thing. Some languages do use `:=` for exactly this reason.
Very cool! I'm also using Haskell in computational biology, and have had a PDB parser on my todo list for a while. Might you put your code up on github so the community could extend it with attribution? Shameless self-promotion: I'll be presenting our Experience Report on using Haskell for a computational biology project at ICFP in a month: http://www.eecs.tufts.edu/~ndaniels/Noah_files/mrfy_experience_report.pdf
I would donate.
I heard a rumor the site is down because the network connection is being upgraded and will be down most of the day. No idea if this is true. Hackage 2.0 supports mirroring, but there are few things blocking its release right now. Perhaps the most significant is that the data structures used to store the package index are insanely inefficient. The data takes 20MB uncompressed (and 5MB compressed) on disk, but when loaded into RAM it is taking gigabytes. Clearly that is crazy. If it took 10x as much RAM.. we could overlook that.. but 100x ? Unfortunately, the profiling tools don't really give us a clear picture as to why so much RAM is being used. For example, it says that all the RAM is being allocated by the cereal library. Well, of course it is, that is the library that reads the serialized data from disk and loads it into RAM .. we knew that already. What we need is a tool that somehow gives us an idea of where we are using too much RAM. I am currently working on a little template haskell code that makes a 'du' like program for data structures. That way you can inspect and hunt down usage hogs like you would if you were trying to free up disk space. No idea if that will prove useful or not, but it seems worth a shot. The package index also contains many opportunities for data sharing that are not being exploited. For example, the version number 1.0 could be shared by every package that has a version 1.0. And, the synopsis and description fields often do not change for a particular package from version to version. Exploiting sharing, though, is not the place to start -- that is what we do after we get the memory usage down to something sensible. The question right now is, where is it currently all going. acid-state/safecopy does not add any overhead to the size of the data structures, so it is something about the Haskell data-structures themselves that is the problem.
It is on github and there is a public mailing list and a chat room. So, those things will not help. Switching to sqlite would be a significant amount of work, and would require major design changes. Fixing the current code is a lot less work, IMO. The project is not stalled due to technology, but because nobody is willing to dedicate their time to finishing it. People get interested for about a week, and then they decide they have other things they would rather do with their life. That said, Ian Lynagh has been doing some work recently. Not sure what the current status is as everything is currently offline due to the network migration (or whatever is going on). Here is the mail he sent: http://www.mail-archive.com/cabal-devel@haskell.org/msg08777.html 
I followed up, and it sounds like the coding portion is largely done for now. The next step is some sysadmin and testing stuff. Turns out a bunch of the unexplained memory usage can be explained due to some really poor interactions between ByteStrings and the garbage collector. And, to make things worse, the heap profiler doesn't really know what is going on and makes the problem hard to discover and hard to figure out the extent of the issue. But it sounds like things are under control now.
That sounds interesting... I'd be interested in learning some details. Regarding administration, I'd be willing to help out just as long as there are a couple others also helping out. I've been doing something similar as Ross has been doing, at least with respect to approving new accounts, on LtU for quite a few years now.
Is there an existing, ready-to go code-base somewhere we could use to keep our own mirrors up like this? (Not that writing a mini crawler would be too hard)
I look forward to your blog post about this :) It would be interesting to generalize this in a way akin to algebraic data types: algebraic data FIELDS, which have a non-standard (and more general) form that is not byte-offset-delimited, allowing flexibility, and some way to (perhaps monadically, since it won't always be possible) reduce down to the standard byte-offset format. This would involve properties of the whole file, certainly. 
We've joined SPI, and are setting up to be able to accept donations and provide better accountability for such now.
Physically hosting it isn't the problem so much as finding someone who is willing to indenture themselves as its maintainer.
I usually install stuff with `--global` as root but perhaps that is dumb.
I can't see how Yegge's post is about risk aversion as opposed to novel programming solutions. Of his 9 points, I think only #8 is related to novel programming solutions.
So, I'm using this now. I understand why it's nice to have something more polymorphic, etc, but who do the EitherR and Either monads exported by your library use (error) to define fail? That seems like even worse behaviour...
Entirely possible.
Thanks! I never actually thought that it's lazyness that makes stuff like foo (print 123) possible. I realize now that I've written a lot of code that relies on this -- I was just taking it for granted that stuff like this just works. :)
I use xmonad but have only dabbled in haskell programming. It's certainly possible and I doubt it would be particularly difficult. (Edit: looks like it's as easy as I thought but you only get a limited type of communication back and forth, you can send in predefined commands and you can get events from the loghooks but you can't just query an api for state and send it complex commands) I don't have any specific advice for you unfortunately but it seems like you would want to look into the main function in your xmonad.hs file and start your webserver up there and then look into the http://xmonad.org/xmonad-docs/xmonad-contrib/XMonad-Hooks-ServerMode.html hook to allow your webserver process to communicate with xmonad. You could also use something like http://www.semicomplete.com/projects/xdotool/ to let your webserver just pretend to be a keyboard/mouse. It's hacky but maybe it's good enough for what you want to do. Whatever you figure out I would be interested in how it turns out. Here is an interesting old discussion I found that helped me understand why this is harder than it should be: http://www.digipedia.pl/usenet/thread/14422/9850/ If you are ambitious then you should find some way to allow xmonad to listen for messages from other sources instead of blocking in the X event loop and then this would be much much easier. That's alluded to at the end of the thread I just linked. edit: all of this info is from 2008 so if things are different now please correct me. Now i'm thinking of some cool things I could do with this. 
The main reason is that there is no reasonable implementation for fail without constraining the Left return value. For a long time Haskell had no Either monad because there was no correct way to implement fail. A lot of people complained because Either actually is a valid monad and fail has nothing to do with the mathematical definition of a monad, so they introduced it as a monad using fail = error because having an Either monad is much more important than having a valid fail. In fact, the only monad that fail actually works for is IO. Every other monad that tries to implement fail swallows the string. A better alternative to fail is mzero from the MonadZero class. This is why library discourages the use of fail and tries to standardize on using Either\Maybe.
What is magical and new - are there release notes somewhere? *quick edit*: I found [this](http://hackage.haskell.org/trac/ghc/browser/docs/users_guide/7.6.1-notes.xml).
Looking forward to TH support for ConstraintKinds.
LambdaCase &amp; MultiWayIf, huzzah!
It would be nice if you could include examples like that or a minitutorial into the documentation of the library on hackage.
It would be nice if you could include examples like that or a minitutorial into the documentation of the library on hackage.
That's a good idea. It could use some additional documentation.
heres a link to an os-x pkg installer ~~https://letscrate.com/f/carter/ghc-7-6-rc1/GHC-7.6.0.20120810-x86_64.pkg~~ no longer works, i'll figure out another mechanism
link no longer works, i'll update it later if anyone wants
How does replication work in Hackage 2.0?
More information and discussion here: http://www.haskell.org/pipermail/glasgow-haskell-users/2012-January/021611.html
I noticed this post from following him on Google+, which I did after the services affair. I thought that was a well-written and interesting rant. It's disappointing to see something so comparatively shallow as the next public post he makes.
This is really cool! Sending to my friend who's getting a phd in microbiology and isn't nearly using enough coding to do her stuff :)
This analogy is pretty stupid. I would absolutely use python and agile methods to develop a web app for a typical corporate client who isn't exactly sure what he wants. Yeah, I wouldn't even use Haskell. Haven't used it in web development much yet, and just how hard it is / how hard it is to learn is an unknown unknown to me. Here, being "radical" in using a less safe language is the "conservative" thing to do! Whereas if I worked in a real time/embedded environment, I'd certainly use something designed for it, like Ada/SPARK or RT Java - depending on the security demands. I have more experience with plain Ada and RT Java, but SPARK is probably the right choice sometimes. All hyperconservative by Yegge standards. If I worked somewhere with stuff that _had_ to be correct, and also developed as fast as possible (like maybe high frequency trading), I'd first be very scared, and then probably look at Haskell (everyone who has ever expressed any interest in FP in a public forum has probably seen ads from Jane Street Capital by now...)
While we're at it, why not also allow type signatures in export lists? People tend to add them anyway as comments, just because it's useful documentation. Checking for consistency would be nice. 
Isn't this something like newStdGen &gt;&gt;= print . zipWith (+) [0 :: Int .. 10] . randomRs (0, 10) ?
For this particular example, it would be exactly like that. But in the real world I'm using this to do string-based random mutations. I couldn't find a good way to distill that to a nice example.
How is this different from just using guards on a let?
Couldn't you also use a random-numbers Monad and mapM ?
&gt;Package database flags have been renamed from -package-conf* -package-db*. This is a bit of a pain if you have to support several versions of GHC. Either you have to write code to identifies the version of GHC you're using and then pass the correct flag or you'll get warning spam. I ran into this on my build bot (which builds all my packages with the 3 latest versions of GHC).
You should use the `State` monad to thread your generator through. It's identical to what you would write by hand: (`evalState` gen) $ forM xs $ \x -&gt; do g &lt;- get let (a, g') = randomR ... put g' return (x + a) Or you could use `MonadRandom` which does what I just did, except in a more convenient way.
Huh? `mapAccumL` is extremely simple: It's just a state machine applied to a stream of inputs. As it often happens, the type signature tells all you need to know: mapAccumL :: (acc -&gt; x -&gt; (acc, y)) -&gt; acc -&gt; [x] -&gt; (acc, [y]) Let me change it a little bit, with uncurrying and renaming `acc` to `s`: mapAccumL :: ((s,x) -&gt; (s,y)) -&gt; (s,[x]) -&gt; (s,[y]) That's it. (small rant: I miss is the `mapAccumL_ f s0 xs = snd (mapAccumL f s0 xs)` function from the standard library. Also for the right version, of course)
Sounds good to me. I've done a bit of research and created [trac ticket #7140: Allow type signature in export list](http://hackage.haskell.org/trac/ghc/ticket/7140). Don't expect this to make it into 7.6, though; maybe 7.8 if it catches someone's fancy. (The [ticket for InstanceSigs](http://hackage.haskell.org/trac/ghc/ticket/5676) appealed to SPJ who said "I've wanted this myself, so I've done it in a spare moment.")
Where are the list functions?!? Also, if I understand correctly that this should be some enhanced Prelude, I have some feature requests (function names are negotiable): * when you add the list functions, also add (the new function) `mapAccumL_` :) * add `fst3`, `snd3`, `thd3` (possibly also the quadruple versions) * `swap :: (a,b) -&gt; (b,a)` * add some Ord-based `nub` (for example the one which builds a set) * `equating :: Eq a =&gt; (b -&gt; a) -&gt; b -&gt; b -&gt; Bool` (so that you can say `groupBy (equating f) . sortBy (comparing f)` * `sortOn`, `groupOn`, `groupSortOn`, `mapGroupSortOn` (something like, `mapGroupSortOn :: Ord b =&gt; (a -&gt; b) -&gt; (a -&gt; c) -&gt; [a] -&gt; [(b,[c])]`) * strict sum/product * semantic editor combinators a la Conal * optionally re-export `MVar`, and add something like `adjustMVar :: MVar a -&gt; (a -&gt; a) -&gt; IO a` and also `adjustMVar_`.
It's simply that threading state is more convenient via the state monad. Though I have to admit that I'm lazy and often write explicit state-passing style code instead.
To have a personal spin of this, I'm feeling more and more conservative when coding in Haskell. I don't like language extensions, even though I most often like the ideas behind them. The best example is probably type families: I really don't like type families, even though the idea of type families is very appealing... Recently, I even wrote a somewhat big library which turned out to be Haskell98 (with some tricky parts)
by the way, I also tend to prefer the monadic `mapAccumM` instead of using the state monad transformer. Unfortunately, that is not part of the standard library either.
Like some others I immediately thought of hiding the state using the State g monad to make the calculation more elegant. Come to think of it, it that's its main purpose: *hiding* state. Here's my take... {-# LANGUAGE RankNTypes #-} import System.Random import Control.Monad.State -- end result ... nice! main = run $ mapM addRand [1,1,1,1] -- λ&gt; main -- [12,3,9,11] type R a = State StdGen a -- State monad hides state addRand :: Int -&gt; R Int addRand x = do i &lt;- randomNumber return $ x + i run :: R b -&gt; IO b run calc = do gen &lt;- newStdGen return $ evalState calc gen randomNumber :: R Int randomNumber = do gen &lt;- get let (i, gen') = randomR (0,20) gen put gen' return i
Yay, thanks!
Argh! I kinda liked that.. At once, it's strange to have infix operators for things that you can't define fixity for, and we can still use backtick infix syntax. So, that's alright. Since type constructors are the common case, and are actually exported, it's better to have those be able to be undecorated infix. It's tubad that a ":" isn't used for type operator variables. Special casing those would make more sense, but switching the convention now would be confusing and break too much code.
Did they accept the . prefix for lowercase proposal? Otherwise, is there no way at all to have infix type variables?
Type signatures in export and import lists could make it easier to do precise dependency matching rather than version-number based matching. It could be nice to auto-generate them, though. It is also a bit of a problem when higher ranks are involved.
I'm not sure if it is a good idea to move away from lists. List is definitely the data structure with the lowest cost of entry (both mental and boilerplate), and performance is also good if you use keep in mind that it is actually a singly linked list.
The error you linked to looks like your package database has been corrupted. Looks like you may have updated the directory package in the global package db, which has rendered your Cabal package unusable. I would reinstall ghc/haskell platform, and make sure to either always use cabal-dev, or only ever install into your user package database. I believe that cabal will install to the user package db by default, so leaving off --global for new installs should suffice.
Not really. We are not talking about GHC / language extensions /standard libraries development. And the company involved (FP Complete) already provides a haskell support. So bringing in Michael on board does not change any of that. But Yesod now gets a full time paid developer. 
Oh, OK. If even Edward Kmett thought this was a good idea even though he is probably the one who had to update most packages, then I guess it must be. And his packages seem to already have been fixed.
Very cool. Yesod could end up doing for Haskell what Rails did for Ruby.
Every SQL injection and cross site scripting breach ever? Noting that the type system / module system *can* be used to prevent this, but you have to put some effort into it.
wouldn't xmonad be blocked on the X socket in between X events? How would you get the information from the forkIO'd thread back into xmonad core?
Hmm, that's a good point. I assumed you could elevate the forked IO to X for some reason, but I see now that that makes little sense.
I started my professional life as a Perl programmer, after earning a mathematics degree. The real argument isn't that it makes bugs harder to write, but that it makes successful refactoring trivially easy. The steps are: 1) Encode invariants you want maintained as types. Use smart constructors (total functions into the type) and explicit exports to ensure that the invariants are maintained by all values of the type. (This is the design phase) 2) Write client code using the smart constructors. 3) Make changes to the host types (the redesign phase). 4) Chase down type errors in the client code. 5) You are done. The type system is just one piece of this puzzle. Haskell's referential transparency means that one can use "equational reasoning", type homomorphisms, type quotients, etc., to embed semantics into types, while satisfying natural mathematical laws. Consider the following Haskell: newtype Name = Name { unName :: String } newtype Address = Address { unAddress :: String } data Person = Person { name :: Name, address :: Address } people :: [Person] people = [ Person (Name "Me") (Address "Somewhere, US") , Person (Name "You") (Address "Europe") ] names :: [Name] names = join . intercalate " " . fmap name $ people Now, you *can* do the same thing with Perl. But if you are using a stateful, object based representation for a Person, you will have to check the invariants yourself. This seems like a thin example, until you consider that ANY line in the Perl version program can change the semantics of a Person's name. (This has been a huge problem for Ruby, for example, since the community used to actively encourage injecting behaviors to objects "generically" by directly manipulating the class hierarchy) This means you need to check (potentially many) stacks of *callers* to a function to make sure the function will work as expected. Even unit testing will not suffice, unless it treats each stack of callers as a unit. Typing (i.e., hitting the keys on your keyboard) the thing is easy in either language, but refactoring Haskell is significantly easier, because you have the type checker to catch any incomplete refactorings, and because values never change. The example breaks down if you explicitly use state or IO. But there are plenty of "pure" computations you can factor out of State or IO, even in something like system administration or a web app. To paraphrase a Perlism, you should make the easy things TRIVIAL, and the hard things easier.
Found the reference: http://markmail.org/thread/nurcqm4yotgkmhbr "... Unlike every other language I've used, large portions of my haskell code are turning out to be general-purpose, reusable code. Fully 20% of the haskell code I've written for git-annex is general purpose. Now, I came out of a decade of perl with maybe 1% reusable code. So I'm sure this is a credit to haskell, and not to me. ..."
Yeah, it's almost certainly possible to do in Haskell. Even when writing Haskell, if I encounter a bug, if I think about it for a minute I can usually figure out a way to use types to eliminate it permanently (modulo certain conditions like nontermination). In Agda, that's of course always possible - any bug found can be eliminated via types. (Sometimes, though, I decide that while it's *possible*, it's probably *not a good idea* simply because it would complicate everything beyond reason).
&gt; Except yesod needs to compete with rails. As far as I'm aware all rails had to compete with was php and. Net or Java. Back then, LAMP was a buzzword. The competition was already strong between the many python, perl and php frameworks and libraries. But we expected more and more of webapps, and RoR was just more up to date. We're still expecting ever more of webapps, RoR is hardly the bleeding edge anymore...
This is brilliant. Thank you.
[Yesod](http://www.yesodweb.com/) does this. Every URL is a type and thus you prevent malformed URLs and other bugs. There was a [webcast](http://event.on24.com/eventRegistration/EventLobbyServlet?target=lobby.jsp&amp;eventid=488007&amp;sessionid=1&amp;key=0EFA6BF4DB7BDB31ABA706AD34BD24DF&amp;eventuserid=67702258) about this at O'Reilly, recently.
Congrats!
Smart constructors usually use runtime errors (or Nothing) to prevent violation of invariants, so while they promote errors to be discovered earlier *at runtime*, they still defer errors to runtime.
Damn. Guess the work on the solver wasn't completed? :-(
The edge seems to have moved to client side javascript frameworks. The server side, be it rails, django, or yesod, doesn't appear to have much room to improve anymore. As long as you can RESTfully spit out html, css, js and json while interacting with a DB of your choosing, whether SQL or any of the production friendly noSQL types (mongo, riak, couch, neo4j, cassandra...) there isn't much left for the server framework to do besides stay up to date on security. That is, these days a good server framework is one that doesn't get in your way. [Meteor](http://meteor.com/) seems to be the "cutting edge" (time will tell) in server frameworks and what it does differently is basically eliminate the distance between the database and the client.
I see *a lot* that is missing on the server side. We have not solved migration, distribution, and performance: - Migration from one database schema to another - Migration of a database from machine A to machine B. - Migration of RPC APIs from API A to API B. - In general, migration of multiple distributed servers from code-base A to code-base B. - Upgrading a fleet of servers with simple stateful sessions. - Reverting a partially done migration. - Simulating performance Doing most of the above is lore. If you have worked on systems with millions of users, you know how it works. If you have not, you don't. I see a great future in formalizing this. We can have a meta-language that describes the deployment, dependencies, and APIs, and we can check whether an action will break anything or not. We can search for the best migration path etc. We regularly see downtime on LAMP platforms.
While this somehow circumvents the original question, I really have to second this. A type system à la Haskell is a good way to *organize* code. It's not so much about static guarantees per se; it's rather about the useful and well-organized information conveyed by the types: they help a lot when you try to get a hang of some codework.
If Harper read the original extensible exceptions stuff, he'd know that this was always well known. If he read the even older stuff on dynamics, he'd know you don't need extensions, just hand-written instances of typeable. (in fact, he points this out in passing, but still posts as though *exceptions* are the issue). If he read the safe haskell stuff, he'd know that that's easy enough to statically prevent, and we can. Instead we get this presented in a naive "Hai guyz, look wut I found!?" voice, which I find ridiculous. I'm tired of Harper deliberately going out of his way to troll us :-)
Like most of Harper's articles about Haskell, this is both mostly true and an elaborately constructed troll where it's impossible to tell whether he's getting some lulz or merely piqued that nobody uses ML.
The problem is not with exceptions, it is with Typeable and in particular with hand written instances of Typeable. Once you write such an instance you don't need exceptions to make things explode. The obvious solution is to disallow these hand-written instances, and I believe Safe Haskell does exactly that. My impression is that O'Caml exception declarations implicitly include something like a typeable instance, which allows the runtime to distinguish the types. The only difference is that this is not exposed to the programmer. Harper says that: &gt; There is no need in ML for any form of type casting to implement exceptions, because there is exactly one type of exception values, albeit one with many classes. Haskell, on the other hand, tries to have exceptions with different types. But typeable is used precisely because it allows you to write a single existential type data Dynamic where Dyn :: Typeable a =&gt; a -&gt; Dynamic And exceptions have such an existential type.
After reading the article I've got a genius insight: Agda safety is a hoax. You can get an out of memory exception for the verified code. Went to write a lengthy paper.
Here is an actual "happened to me" -example: inline double maxAbove(IplImage *src, int width, int row, int *cursor, int halfWidth) { ... } void dynMap(IplImage *src, int halfWidthStep) { CvSize size = cvGetSize(src); for(int row=1; row&lt;size.height; row++) for(int column=0; column&lt;size.width; column++) { int c=column; int sum = DGET(src,column,row) +maxAbove(src,size.width,row-1,&amp;c,halfWidthStep); // some other stuff that was commented out.. DGET(src,column,row) = sum; } } Now, arguably, that's just about me being sloppy and not reading code carefully enough after editing it. But my test images were black and white, and the result was normalized for viewing, so this went through my tests. It took me hours to debug this, since "dynMap is tested, so it can't be there". Seriously, I shouldn't be able to write code that works just fine with black-and-white images but borks when there is a single gray pixel in the image. (Even if there is a flag that would have warned me if I had been careful enough to enable it.) 
(of course not everything has to be on Planet Haskell)
Bring in a ton of PHPtards who don't want to learn the language but want to tell the people who wrote it how wrong and dumb they are? I sure hope not. The ruby community is still trying to recover from rails, I'd hate to see the haskell community go any further in that direction.
Bob is working in a curious universe that is quite unfamiliar to Haskellers, and his post makes a lot more sense if you've read the chapter 34 he points to on his post, which clarifies what he means by "dynamic" in this post. I *believe* he moffs the implementation in Haskell, which obscures the issues being discussed, but I think you can pull off "dynamic classification" as he defines it with some global state and a helper library. Code incoming...
A less anecdotal study is here: http://evanfarrer.blogspot.co.uk/2012/06/unit-testing-isnt-enough-you-need.html
Have you had a look at eegreg's [cabal-meta](https://github.com/yesodweb/cabal-meta)?
Must have missed the announcement somehow. Looks very handy. I will have a play. Thanks!
Precisely the same impression I got. "Haskell is exceptionally unsafe"... *if* you happen to be doing this exceptionally dumb thing.
I wrote up a very brief example a few weeks ago, based on a real bug I created at work. The take-away is that `newtype`s are really cool! http://eseidel.org/posts/strong-type-systems
Well, the use-case Harper has in mind is "sealing" the types in an existential, so you can *only* ever look at them if you have the destructor that also got created. So, in that sense, this behavior is intentional (better have unique IDs for each run though!)
You can use cabal unpack to download and unpack a package.
Oh, cabal-meta now has a flag to make it use cabal-dev instead of cabal. Nice! Time to look at it more carefully. I might actually use this to simplify, or even replace, my own local collection of bash scripts. The only thing is - does the "`sources.txt`" format support multiple [yackage](http://hackage.haskell.org/package/yackage) servers? In other words, when I organize my packages in various ways using multiple light-weight package servers, can I specify which of the servers to get a package from in "`sources.txt`"?
Fun fact: writing this tutorial helped me find a bug in `modifyBackwards`. The implementation used to be modifyBackwards f = getFuture &gt;&gt;= sendPast . f Which is obviously a paradox or "time loop" or something bad. ;) The implementation is now modifyBackwards f = do rec sendPast (f x) x &lt;- getFuture return () Much better. :) The more I wrote about the Tardis, the more I realized it really is *nothing* more than State and Reverse State combined, which makes me want to publish a Reverse State package (I couldn't find one on hackage despite my best ctrl+f efforts).
Bob's not actually interested in programming in Haskell. He's been using ML in his research since the days when *ML was the closest thing going to a successful statically typed functional language. He seems a bit miffed by Haskell's recent rise to be nearly mainstream, and has reacted by pointing out all of Haskell's blatant design flaws in comparison to his own one true strict, impure way. Fortunately for him, commercial language selection has always been about respecting the evidence and opinions coming from academic programming language research, so the tide of announcements from startups and commercial users switching to Haskell has halted while they all assess which ML compiler will sound best when they go for venture capital. I mean otherwise he'd come across as a bitter character whistling hopelessly in the wind while others reap the benefits of a functional language with an advanced type system and an active community, known warts and all.
Weird. Why would you want to do this?
this is basically the open witness stuff, I think? I endorse ekmett's comments on it: http://semantic.org/stuff/Open-Witnesses.pdf
It's not harper's originally. I think the origin may be lost in folklore :-)
I know there's been a bit written about this over the last few weeks, but one thing I've yet to wrap my head around is when I'd want to use this -- what kind of problems does this model? When is it appropriate to use this (or the reverse state monad)? When are more conventional solutions more appropriate? So far my best take is that the reverse state monad is for when regular state does what you want, but backwards. It also seems like there's some abstraction I'm missing here.
Fun, obviously! There are other ways to accomplish the task, but one example where I've found a Tardis to be useful is in determining the score of a bowling game, since the score for a given frame depends both on the score of the *previous* frame as well as sometimes depending on *future* throws. https://github.com/DanBurton/tardis/blob/master/Control/Monad/Tardis/Example.hs I'll be writing up a blog post later about Seers, how they can be implemented via Tardis, and why they are useful.
Can't help directly since my head almost exploded when i tried to read the perl 6 spec (I hadn't thought to wrap my skull with duct tape) But I think the dynamically typed mind runs this voice perl | python | ruby are, or, *could possibly be*, running the same checks at runtime that GHC does at compile time. What's the big deal? They're all Turing complete.. yadda yadda ---------- You could talk about formal methods (roughly: higher order logic, dependent types, model checking) and what's been accomplished in haskell and *ML variants, including theorem provers built therein http://axisofeval.blogspot.com/2010/10/notes-on-dependent-types.html -------------- Or you could talk about rewrite rules. This is a pretty good page to start http://www.haskell.org/haskellwiki/Playing_by_the_rules and this is the firehose: http://www.haskell.org/haskellwiki/GHC/Using_rules ------------------------- relevant: http://stackoverflow.com/questions/3220246/using-haskells-types-to-replace-assert-statements-or-if-checks-in-other-languag this guy just published a Clojure book http://s-expressions.com/2012/08/05/rathores-10th-rule-of-programming/ 
Yes, witnesses look about like the way I would implement it more safely (I was planning on taking a whack at it with GADTs tomorrow (China time) but I'm always pleased when someone else has done the work for me!)
&gt; what kind of problems does this model? At the risk of sounding timey-wimey, I offer this explanation: A Tardis is useful when you want to communicate with both the past and the future. The regular State monad lets you send messages into the future, and receive messages from the past. We think of this as sort of the "natural" forwards flow of information through code, and the usefulness of the State monad is rather obvious. The reverse State monad lets you reverse the direction that state travels through your code, allowing you to send messages to the past, and receive messages from the future. The usefulness of the Reverse state monad is much less obvious, since it behaves in such a strange way, but use cases still exist. A Tardis just bundles these two channels of communication (forwards-traveling and backwards-traveling) into one monad with clearly labelled primitives to let you know which way you are sending or receiving information. Now, I'm not saying a Tardis is going to be *generally* useful; no, not at all. However, there are very specific sorts of problems where you say to yourself "I wish I could just retrieve this information from the future... wouldn't that be nice"; that is an indication that a Tardis might be able to help.
Also, if you put `--dev` in `$HOME/.cabal-meta/opts, it just uses it automatically. This is handy if you work with people for whom cabal-dev would be overkill and you don't want to confuse them by cutting and pasting commands with the --dev flag in it.
Exceptions are iffy in every language I have seen, functional or not. For example, the Google C++ Style Guide, http://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Exceptions, says "We do not use C++ exceptions". The reasons given there and here: http://www.lighterra.com/papers/exceptionsharmful/ resonate with me. Possibility of corrupted state, non-local, hard-to-analyze control flow and a bad fit for high level parallelization methods. I don't have a solution, though. Sometimes it is nice to just pretend that a memory allocation will always work or that a disk read will never fail and add code to catch the exceptions when you really need to, instead of having to handle a "Maybe" every time you are doing something in IO.
Typo in the final code sample: flip execTardis (10, "Dan") $ do modifyForwards (++ "Burton") modifyBackwards (+ 1) should be flip execTardis (10, "Dan") $ do modifyForwards (++ " Burton") modifyBackwards (+ 1) (note extra space in `" Burton"`)
I haven't really looked into this, but here's one possible case where this might be of use. Markdown allows links like `[this]`, where the reference for `[this]`, [this]: /url "title" can occur anywhere in the document -- either before or after the occurrence of `[this]`. So, if you had tardis in a markdown parser, when you parse `[this]` you could check both the past and future state to see if a reference for `[this]` is defined. This approach seems cleaner than my old approach in pandoc (doing an initial parsing pass just looking for link and footnote references) or my current approach (returning values in a reader monad that gets run at the end against the final list of link and footnote references). [edited for clarity] 
&gt; I'm tired of Harper **deliberately going out of his way** to troll us :-) As opposed to passively trolling?
A smart constructor must be a total function, or else it isn't so smart. I'll edit the list of steps. That is an important point.
Thanks for pointing that out. I've fixed it in the repo, but I'll probably wait until I make more substantial changes before pushing it to hackage.
Isn't it from Guy Steele? http://people.csail.mit.edu/gregs/ll1-discuss-archive-html/msg01134.html
Correct, `MonadFix` is necessary in order to write the Monad instance for Reverse State and, consequently, Tardis. The `Identity` instance of MonadFix unsurprisingly makes use of `fix` instance MonadFix Identity where mfix f = Identity (fix (runIdentity . f)) If you remove the newtype coersions, then it is simply mfix f = fix f For other monads, it's more involved. Fortunately, State is an instance of MonadFix. instance (MonadFix m) =&gt; MonadFix (StateT s m) where mfix f = StateT $ \s -&gt; mfix $ \ ~(a, _) -&gt; runStateT (f a) s Reverse state is a MonadFix too, which means that Tardis can be built as either a reverse state transformer on top of state, or as a state transformer on top of reverse state, and Tardis is also a MonadFix, which greatly facilitates writing timey-wimey code.
Yup. In the case of XSS, you can point to Yesod's template languages as a concrete, well-documented example. Type-safe URLs are another cool example while you're at it.
The air of hostility towards Robert Harper in these comments is wholly unnecessary. Criticism is of course always valuable when it is correct, and there's nothing wrong with finding it enjoyable.
Except in Meteor you're writing in javascript, which likely isn't what readers of this subreddit will consider "cutting edge". I'll grant that reducing the distance between database and client seems compelling, but not at the expense of type safety because you end up writing *more* code in a less safe language. If you look at the problem from the other direction there's a ton still to be done with server frameworks.
Not sure about that, this method is used to add kind inference to a typechecker that already does kind checking.
Have you tried MaybeT/EitherT? Or even just the base Maybe/Either Monads? They give you this feeling locally, while causing the upstream to either need to handle the possibility of error, or to use of of these Transformers/Monads itself. I highly reccomend the Control.Error (errors package) set of utilites for working with this.
Probably, though why would you want backtracking in Haskell? We have lazyness ;)
There is nothing wrong with the criticism being leveled here. Typeable shouldn't allow user instances.
I'm just trying to get a better sense for what problems Tardis solves.
cabal-dev didn't work out for me either. We chose to write our own program that reads a list of package names and versions from a file and puts them in a sandbox for cabal. This ensures that our entire build process uses those and only those packages. The program also knows to reuse an installed package if all of its dependencies (and the dependencies' dependencies, recursively) are the same. It's a total hack, but we're quite happy with it. 
That wasn't the criticism. And if it were the criticism, it would not be a criticism so much as a restatement of widely known fact, regarding which there are initiatives underway to remedy.
The classic backtracking monad, `StateT []`, works by repeating a calculation with different values of the state. The repetitions are delayed by laziness until you need them. All possible future repetitions are represented by a single thunk, because they are held in a lazy list. The `MonadFix` requirement of the reverse state monad worries me. I suspect that it delays all calculations in a big pile of thunks until it gets to the state value it needs in the future. That could lead to a gigantic space leak.
Runtime-wise, for each OCaml exception declared with `exception`, there is a singleton value (I'll call this the exn value). Essentially, the address of this value in memory is an unique id. OCaml exceptions are represented as a tuple (or rather, tuples and exceptions (and arrays and other things) have the same runtime representation), with the first element being the exn value (and the rest being the arguments to the exception). So, when catching an exception, you can only catch those for which you know the runtime representation of the exn value, which means the 'exception ...' declaration should be visible. (One thing the runtime can do that the language doesn't allow is to determine if two anonymous exceptions have the same exn value, although that can be done using OCaml's `unsafePerformIO`, the `Obj` module.)
That sort of XSS prevention is trivially done in Perl though. I think the same goes for SQL injection prevention. Type safe routing would be hard to do with weaker/more dynamic types, though. I think the closest thing you get is something like "traversal" as seen in the Python framework "pyramid", which in some ways is even more type safe in that for example you can't trivially make a link to a blog post with a non-existent ID, because the routing hooks into your data[base].
No. In Python, and I imagine it could be done similarly in Perl, the convention is to escape by default unless an object implements a `__html__()` method which is then called instead, and the result left un-escaped. This isn't really less safe than deciding what goes unescaped with type classes, nor is it more repetitive.
I just read it as a pun, "unsafe exceptions" --&gt; "exceptionally unsafe". Admittedly it's written in usual Robert Harper style, but I think it just boils down to just an existence proof of the exception mechanism not meeting his standard for type-safety, not a claim that this is a fair representative of typical code.
&gt; The more I wrote about the Tardis, the more I realized it really is nothing more than State and Reverse State combined When I first saw Tardis, I couldn't see how it wasn't different from the product of the state monad and the reverse state monad.
Oh, it has been written. But before you look into it, play around and try to write it yourself! Once you've done that... http://mergeconflict.com/tying-the-knot-redux This blog post from mergeconflict originated from a StackOverflow question he asked: http://stackoverflow.com/questions/11060565/tying-the-knot-with-a-state-monad Which was the original motivation for and caused the birth of TardisT as the backend implementation for SeerT. Then I realized that SeerT could be written as a Reader + Writer instead. I will soon blog about the two ways SeerT can be implemented, mostly focusing on the Tardis implementation. (The Reader + Writer implementation is fairly straightforward. However, pretty much everything with a Tardis involved gets... rather interesting.)
Huh, `SeerT`. I was thinking of the name "God's Eye Monad," but that would be insufficiently secular... **EDIT:** I ain't gonna look yet, but I sure hope you used `prophesize` and `fulfill` as the operation names instead of `ask` and `tell`...
Somewhat more seriously, I _think_ this is the same thing Damian Conway goes into in some depth in [this video](http://blip.tv/oreilly-open-source-convention/oscon-2008-damian-conway-thoughtstream-temporally-quaquaversal-virtual-nanomachine-programming-in-multiple-t-1151669), but I'm not sure. If not, they are certainly related. 
Hmm, right, but I don't see usage of anything other than kind *, so kind checking this variety of generated code would be unnecessary.
I'd wager that it's useful in the same sort of places as tying-the-knot is. Namely when you want to leave some holes and then backfill them (that is, lazily *defining* what goes in the hole, rather than just lazily evaluating).
How is that even.. Who would invent a game that requires you to know the future to know the current score? :|
My people used to prevent paradoxes from causing bugs... they're gone now.
These are not exclusive alternatives...
To look to Homotopy Type Theory is to look to a relatively new type theorist (albeit a fields medallist) who has not yet changed his mind about equality.
This. I can't emphasize this aspect of strong static typing enough. Haskell allows me to refactor large code bases fearlessly. And this means that the type system becomes a tool to help me explore the problem space and reveal structure and dependencies that I hadn't noticed.
Can someone explain this to me? Is this people doing bad things knowing they are bad, and sad that bad things are possible? Or is this something that you could program unknowingly and be bitten with?
&gt; But before you look into it, play around and try to write it yourself! The links to your `ReaderT w (WriterT w m)` solution in your Stack Overflow answer and comments are broken. No matter, I figured it out: {-# LANGUAGE DoRec #-} import Control.Monad.Trans import Control.Monad.Trans.Reader import Control.Monad.Trans.Writer import Control.Monad.Identity import Data.Monoid type SeerT w m a = ReaderT w (WriterT w m) a runSeerT :: MonadFix m =&gt; SeerT w m a -&gt; m a runSeerT rwa = do rec (a, w) &lt;- runWriterT (runReaderT rwa w) return a send :: (Monoid w, Monad m) =&gt; w -&gt; SeerT w m () send = lift . tell see :: (Monoid w, Monad m) =&gt; SeerT w m w see = ask type Seer w a = SeerT w Identity a runSeer :: Seer w a -&gt; a runSeer ma = runIdentity (runSeerT ma) Now an interesting toy example. A "program" is a mixed sequence whose elements are either definitions of identifiers, or uses of identifiers: type Program a = [Statement a] data Statement a = Define Identifier a | Use Identifier type Identifier = String We want to eliminate all definitions and replace all uses of an identifier with its defined value. But definitions may occur either before or after their uses. No problem: import Data.Maybe (catMaybes) elimDefine :: Program a -&gt; [a] elimDefine = catMaybes . runSeer . mapM elimStep elimStep :: Statement a -&gt; Seer [(Identifier,a)] (Maybe a) elimStep (Define v x) = send [(v,x)] &gt;&gt; return Nothing elimStep (Use v) = liftM (lookup v) see example = [Use "y", Use "x", Define "x" 5, Use "x", Define "y" 7] -- elimDefine example =&gt; [7,5,5]
So you learn tomorrow's lottery numbers, then you party forever? I might suggest doing something in between those two actions...
I considered adding those points but I don't think they actually provide enough of a bonus. They are fast, and in ways that rails can't compete with (html compiled into the binary, etc), but as long as your server is stateless and RESTful, you can just create more instances to service more customers, making the database, not the server, the bottleneck. A lighter strain on the CPU would only be beneficial if the server was doing intensive computations, in which case algorithm improvements will have bigger benefits then language choice anyway.
I see, I'm actually a little disappointed by that. I'm no expert on the matter but I'm of the opinion that working with a library is preferable to working with a compiler, where possible. Would it be possible/worthwhile to make it so that the Fay monad actually had a 'toJS :: Fay a -&gt; Text' function? I presume optimizations, where they to be done, would have to operate on values in the Fay monad.
I wrote a small article about that. Turns out you can use a sorting algorithm and adapt it to return a random permutation instead. http://apfelmus.nfshost.com/articles/random-permutations.html
:D Writing snaplets turned out to be easy. And so is working with Fay.
See: http://okmij.org/ftp/Haskell/perfect-shuffle.txt
I feel like the entire deriving mechanism could use some work. It'd be very nice if we could add our own deriving mechanisms based on a reified generic representations of the types in terms of products and sums and so on, but I'm sure someone (pigworker?) knows why that's a bad idea.
If the post is made out of ignorance, is it still trolling?
...and the [shuffleM](http://hackage.haskell.org/packages/archive/random-shuffle/0.0.4/doc/html/System-Random-Shuffle.html) function implements this post.
&gt; I'm not just using Oleg as a magical oracle Not that you couldn't do so.
The mile high view is that this is a case of the GHC authors putting a few features into GHC which had an unforeseen bad interactions with each other; we worry a bit about this sort of thing (since this kind of unsoundness means there is something not quite right about the design), but usually users don’t run into it.
you already can! well, something close enough. You just use the new default signature mechanism to build your instance for you. see http://www.haskell.org/haskellwiki/GHC.Generics
That "Google has a bunch of exception-unsafe C++ code" shows a problem with the concept of exceptions. Just like having a bunch of thread-unsafe code shows a problem with the concept of threads. The language does not help you enough in making your code safe in these regards. And the compiler does not help you, in the way that e.g strong typing helps you, when you try to integrate exceptions in your code. Python is already an "unsafe" language, where unit testing plays a more important role than static guarantees, so using exceptions is less of a loss. Also idiomatic Python code tends to use exceptions a lot, so you cannot ban it. I completely agree with you that error codes are not superior. The problem with error handling is that some errors (key not found in a lookup, out of bounds index access) will be frequent enough that you want the compiler to whine at you if you do not handle them, while others (network, database) are common enough that you _probably_ want to be warned if you do not handle them. And yet others are so rare (out of memory, disk corruption) and would have to be handled everywhere, that it is often "safe enough" to pretend that they will never happen.
Eh, it's not that hard to fix. I'll probably just use infix `arr` (with backticks) instead of `~&gt;`. I'll get around to changing it soonish, or of course anyone else is welcome to make the changes as well.
? Do you also refuse to code in C because gcc exists? I just compile with -XHaskell98 and the demons go away.
&gt; "Research" is not an excuse. Nobody is smart enough to see how things turn out in advance. Putatively crippling flaws may turn out to instead be irrelevant corner cases in practice, putative benefits may or may not actually manifest, putative minor flaws may turn out to be crippling. You haven't got a recipe for quality, you've got a recipe for stagnation at the first halfway decent local optima. Things _must_ be tried.
This is one of the more well-reasoned arguments against using Haskell that I've seen in a long time.
If I had to hazard a guess, it would be because things that he might have originally wanted to make Stringable might include types from the Prelude or other places, which were not already instances of Monoid. If he had done so, then he would have had to make his own orphan Monoid for the type. It is interesting that all of the types that sclv ultimately *did* make Stringable already have the appropriate Monoid instances, and that of course, any other such orphans would not be orphans of Stringable defined by the end user. So in the end, there really isn't an advantage given the way this class is actually instantiated.
What are other examples? 
Yeah, I remember for me the big revelation about Haskell was that, as strict as the compiler was, it ONLY caught mistakes. The compiler wasn't really being overly strict, rather I was just a really error-prone programmer and no other language had really forced me to confront that fact because they would just gloss over my mistakes.
Ah, sorry about that. I had glossed over some other comments about --global, and for some reason assumed that it was yours :)
GHC's rewrite rules seem like an extremely dangerous feature to me. Optimization via series of rules that anyone can add which aren't guaranteed to be confluent is an inherently bad idea. I believe certain interactions with overlapping instances are unsafe as well although the issues there seem to be generally well-known. Template Haskell can be used to break encapsulation. Robert Harper just recently highlighted the problems with Typeable. A number of extensions allow you to hang the compiler. I dislike the number of overlapping features as well (e.g. functional dependencies versus associated types). There are several extensions that I think are just "bad design" (e.g. I can't write a closed type function), but I'll spare you my opinions on those. I've also managed to panic GHC several times when using extensions over the years, and I'm not exactly a heavy user. The recent addition of -XSafe is nice, but I wouldn't be surprised if there are problematic interactions that remain possible even with -XSafe enabled. In any case, I find the continuous stream of type system extensions to be somewhat frustrating when basic things—like parameterized modules, records that don't suck, or support for abstract types without requiring newtypes and manual wrapping/unwrapping everywhere—go unaddressed. OCaml lacks some of GHC's type extensions, but the basic things that I consider necessary for effective software engineering are (mostly) present at least.
Experimental features you don't need to use, or which only break if you go out of your way don't trouble me much. But on the other hand, the *lack* of features you want, I find a much more valid complaint. :-) (And those things you list are things that I think we eventually should and must do).
newtype deriving was sound until we introduced equality constraints. equality constraints were the new experimental thing. now we have -XSafe and the decision was made that equality constraints are better to have than newtype deriving, so newtype deriving was disallowed rather than equality constraints. eventually, there will be a fix that lets these two interact properly. in the meantime, rather than yank the feature, there's just a warning sign over it (although as you point out, perhaps not a loud enough one). but seriously, you have to *really want* to abuse these features to actually abuse them.
&gt; Experimental features you don't need to use, or which only break if you go out of your way don't trouble me much. I don't disagree. What I'd like are more giant flashing warning signs when using things that may be unsafe (and certainly when things are known to be unsafe as is the case here). When I started using Haskell, I made heavy use of newtype deriving with GADTs without being aware of these potential issues as it definitely seemed like it was just a simple extension that avoided a bit of boilerplate. I see these sort of things as potential traps for users that don't—and shouldn't need to—know better.
Nope. Because that module would still have been compiled with the package, so the package would still have broken when the upstream library was fixed.
I wonder if a haskeller switched back to another language, would he be less error prone now or would he tend to revert to old habits?
Point granted. Though I did say 'sound' which I considered a modest claim :-) Also I'm fairly sure the invariant issue was well known, for some definition of 'well'.
That's a nice tip. Thanks!
I suspect the reason standardization (while ostensibly everyone wants it) is going to slowly is due to a lack of real demand. Sure, having a more modern standard would be nice (and so would a pony), but no one really *needs* it enough to put effort or resources into it. Which is (again, I suspect) because we have a single dominant compiler, GHC, which serves as a de facto standard. No one has to care very much about using language extensions in a way that is compatible with other compilers, because very few (if any) other compilers even support the extensions. If we saw the rise of at least one other major compiler supporting the major extensions and people were regularly getting bitten by differences in their implementation, I suspect we would see the pace of standardization increase.
A MonadPlus is just a Rg (edit; originally Ring) in the category of endofunctors. EDIT: Whoops, I meant that a MonadPlus is a Near-semi-ring ("Rg") in the category of endofunctors. I think. EDIT2: Okay, with the "good" law set (good defined as the one I like), it's a right seminearring.
Thank you, that makes complete sense.
He means a sum constructor. Values of type `exn` are classified into disjoint sets ("classes") by which constructor was used to build them. Lacking sum types, OOP languages often use a hierarchy of types (LogicGate, AndGate, OrGate, NotGate) where a single type with multiple constructors would be more appropriate. His claim is that Haskell's exceptions are in an analogous situation, and should be built on top of extensible sum types like ML's rather than on the (currently unsafe) Data.Typeable framework. 
Cool. I can't figure out how to download it. Where can I get it? (Actually I'm mostly just curious to see what kind of sandboxing it can use)
I was going to answer your previous comment with a description of the Haskell' process, but I guess you're already aware of it. Anyway, for completeness' sake, you asked: "if this stuff is 'standard enough', is there any effort to actually standardize it?" The answer is: "Yes, as you well know, that effort of rolling 'approved' extensions into the spec on an ongoing annual or biannual basis is known as Haskell Prime, and it does not move as fast as many of us would like, but it is also done by volunteers with other things to do as well." Furthermore, with regards to resolving fundeps vs. type funs, this has been a research program for years! with papers and emails and blogposts and more papers! The conscious language design effort is there, but that doesn't mean the nut is fully cracked. 
It's not hard to make a sound GeneralizedNewtypeDeriving, it's just less powerful. I've implemented it twice in different compilers and both times I did it by actually generating the instances with code. If that passes type checking you know it's safe. 
I'm still a little bit skeptical of the ring part. Usually I only assume the monoid laws. I cannot remember right now, but I believe there were several times in the past where I encountered something that was only a monoid and not a ring. Perhaps the rings should be part of a distinguished sub-class of the one with the monoid laws.
It's actually worse than this. When people say that monads are monoids in the category of endofunctors, that refers to the monoidal category with composition as tensor product and identity functor as unit. But the operations of `MonadPlus` are not monoid structure in the same monoidal category, otherwise their types would be: mplus :: m (m a) -&gt; m a mzero :: a -&gt; m a That is, it'd be a second monad for `m`. `MonadPlus` describes (I haven't checked that it works out, but it should) a monoid with respect to induced structure from the underlying category, where (F (x) G) A = F A * G A, and I A = (). But this is not usually the way that (semi-)rings are generalized in category theory (as far as I can tell).
&gt; or support for abstract types without requiring newtypes and manual wrapping/unwrapping everywhere Can't you just define a type and not export it's constructors? I don't see why you need newtypes for abstract types.
Much to my surprise, I learned recently on `#haskell` that kind errors can be produced while typechecking *terms*. newtype Mu f = Mu { unMu :: forall a. (f a -&gt; a) -&gt; a } newtype Fix f = Fix { unFix :: f (Fix f) } blah (Mu x) = x unFix Couldn't match kind `*' against `* -&gt; *' Kind incompatibility when matching types: t0 :: * -&gt; * Fix :: (* -&gt; *) -&gt; * In the first argument of `x', namely `unFix' In the expression: x unFix I don't think the (very clever) demotion scheme produces these sorts of terms, but it's something to be wary of when extending this sort of system to a setting with polymorphic kinds.
&gt; a **MonoidPlus** combines monadic structure with monoid structure. Did u mean MonadPlus as in typo? Or do u really mean [MonoidPlus](http://hackage.haskell.org/package/monoidplus)
I'd suspect that, to the extent `MonadPlus` has any laws, what it hopes to capture is a semiringoid--- just as `Monad` (or, rather, the Kleisli category thereof) captures a monoidoid (i.e., a category). At least, that's what *I'd* like it to capture, since I deal with semiringoids in other areas as well. Of course, it is well known that the "laws" governing `MonadPlus` are disagreed upon by the extant instances. So it cannot be settled without invalidating some instances or other.
Absolutely not.
Whoops. It should be fixed now for people who require the good definition of MonadPlus. I think.
&gt; If you can't prove that that your type system is sound in the presence of some extension, it should have giant flashing warning lights all around it and should require magical incantations to enable. Has OCaml's type system been proven sound? I couldn't locate a reference for that. IIRC the core system has, but not the full system. Is this correct?
Not really, because instances imported anywhere are visible everywhere.
I believe that is the state of things, yes. My general impression though is that OCaml isn't perceived as a type laboratory by its developers. Things that are added are generally done so for clear practical reasons, and there is a rather conservative approach to its development; too conservative, in fact, when it comes to the toolchain and runtime.
No, he means that sclv splits the instances into a separate package, so that if upstream fixes the problem, then downstream users can remove the package from their protects.
SOMEBODY has to keep Maybe gainfully employed.
I have found that [The Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia) is basically a perfect summary of the things to keep in the back of your head which may be useful later.
I use the Either(T) monad more than any other monad except IO, of course), because I do a lot of error checking. My favorite category (other than functions) is the Pipe category (shameless plug). My favorite comonad is the ((,) a) comonad, and I'm going to write up a post on some neat tricks I do with it. My favorite monoid is the list monoid, hands down.
In practice, GHC is the standard.
Shameless plug, I like the Tardis monad :) although its general utility is questionable at best.
There is something I don't get (as somebody who has not used O'Caml). There are several major use cases of newtyping in Haskell, and I don't know how that addresses them. For example, one reason we use newtypes is because we may want multiple instances of a typeclass for the same type. In that case we don't want it to automatically inherit all instances. Second, we sometimes use them for encapsulation and if the user can punch through the newtype and pattern match as if the newtype wasn't there, I don't see how that encapsulates anything. However, I can see it being useful for rearranging type variables. I guess maybe those different functions of newtype should be distinguished rather than conflated into a single concept.
I guess my question is what the former use you described is used for? It is like a Haskell type synonym? I'm speaking as a guy who knows nothing about OCaml other than what you've just described. My later sentence was just referring to what you described about Ocaml. It seemed like no encapsulation was going on since you were working with the underlying type directly. Do you have a good reference link describing these concepts in OCaml?
&gt; I guess my question is what the former use you described is used for? It is like a Haskell type synonym? Internally, it is. Externally, this fact is hidden. The basic idea of ML modules is that you have implementations (structures) and interfaces (signatures). If I just write a structure, all of its members and their types are fully exposed. I can then "ascribe" a signature to the structure in order to hide some information. For example, a value in the structure may not appear in the signature ascribed to it, thus hiding it. Or, a type synonym defined in the structure may simply be given as an abstract type in the signature thus making it abstract. Going back to the example I gave, the idea is that the module signature morally says "there exists some some type `t`". It also exposes a function whose type refers to `t`. That's all any client of the module will know. The module structure gives the "value" for `t` and also for `next`. When the signature is ascribed to the structure, it makes the value of the type synonym `t` abstract. &gt; It seemed like no encapsulation was going on since you were working with the underlying type directly. That's the whole point. Within the module implementation, `t` is equal to `int`. As such, I can use it as an integer without any wrapping or unwrapping. Clients of the module, however, do not know that `t` is equal to `int`. As such, they treat it as an abstract type that is only equal to itself during type checking. &gt; Do you have a good reference link describing these concepts in OCaml? [This part of the Wikipedia page](http://en.wikipedia.org/wiki/Standard_ML#Module_system) seems good enough. ML modules also get much more interesting than what I've described here because you can have higher order modules (which ML refers to as "functors"). OCaml even allows modules to be packed into first class values and passed around at runtime. I'd check out the [OCaml module system documentation](http://caml.inria.fr/pub/docs/manual-ocaml/manual004.html) which has a number of examples. I think all of this stuff is really broken in Haskell. For example, if I have a value `x` of type `t` in Haskell and export `x` but not `t`, haddock will generate documentation that says `x :: t` but won't tell you what `t` is. This doesn't give you an abstract ype though; the compiler will still treat `t` as if it were equal to its definition when type checking other modules. At the very least, Haskell should require that you export `t` so that documentation tools can present a sensible module interface. It wouldn't give you abstraction, but it would at least make sense.
They can both implement Seer equally well; I suspect that the Reader+Writer approach is more efficient, but I don't have benchmarks to prove it. The Tardis approach allows an additional "run" function, which I will cover whenever I get around to writing a blog post about all of this.
I find 'traverse' really handy for applying IO to Maybe values - specialising the types, we can have (a -&gt; IO b) -&gt; Maybe a -&gt; IO (Maybe b) -- surprisingly common! 
Ocaml has its fair share of warts, too. Mutable string literals and polymorphic equality, to name just two.
I like []. A lot.
Ah, that makes sense.
When I write programs in other languages, I make tons of mistakes. I am so used to having a wonderful co-programmer (ie, GHC) that catches all my stupid mistakes that I don't think too hard about things that it will catch if I do them wrong. The other way that writing haskell code has made me "worse programmer" is that I get frustrated easily by dealing with things that GHC could catch. I feel like I am wasting my time when I am debugging _type_ errors. The upside to all of that is that, provided I stay with haskell (or another language with a similar type system), all the brain cycles that used to deal with crap that GHC can deal with for me are freed up to think about more important things - ie, the actual problem at hand. 
(.)
&gt; My favorite monoid is the list monoid, hands down. Why that? mappend has a bad time complexity for lists.
It's not super amazing, but I use it all the time to define extensible data structures, except I don't use pair directly (I actually define a record with a polymorphic field) and most of the neat stuff occurs with the functor instance.
It's a free monoid.
I propose Tekmo
I found this : http://www.cl.cam.ac.uk/~dao29/drafts/codo-notation-orchard12.pdf is your technique similar to what the paper describes in page 10? Very interesting and widening my eyes.
its about time mongo had every operation formally verified
I think you should be in /r/agda
Tekmo will close 10gen and wipe out their github repo to free the world from such dynamic and typeless abomination as mongo :) 
Way to spoil my plan! :) But in all seriousness I actually enjoy programming in other languages even more after learning Haskell. Before I learned Haskell I would always find design patterns that I couldn't quite articulate and that would nag me constantly. I would never be satisfied with anything I wrote because there was always a constant "what if?" about how I could have better organized or abstracted my code. After learning Haskell and some rudimentary category theory I find programming in lower languages much more blissful because now I can either: a) articulate clearly the design pattern I always wanted, or b) realize the limitations of the lower language and move on, content with the knowledge that it's not realistically feasible This ability to decisively understand the design pattern space of a language keeps me from getting distracted with the desire to over-architect code in search of that perfect abstraction and I find I am a much happier programmer as a result.
Cool, didn't realize you're a grad student (I'm starting a PhD at UCSD next month). What's your thesis topic?
Homology-based interactive protein design
runST with mutable arrays and swap is O(n) and provably pure. Sometimes you gotta go with the imperative solution when it works best, but that doesn't mean we can't take advantage of purity!
only if you promise to make agda #1 in every category in the debian shootout
Surely they also offer indexing that doesn't refine. That alone can be quite useful.
Well so you can index lists by their length and produce an indexed family of types that is essentially isomorphic to the original type of lists. You haven't refined it, you've merely indexed. But maybe I'm using a different notion of refinement.
So you are saying types like -- unary natural numbers data Z data S n -- indexed lists data Vec n a where Nil :: Vec Z a Cons :: a -&gt; Vec n a -&gt; Vec (S n) a What happens when you pattern match on these? test :: Vec n a -&gt; ... test Nil = ... -- type evironment A test (Cons x xs) = ... -- type environment B In type environment A, we have the constraint `(n ~ Z)`. In environment B, we have a new type variable `n'`, the constraint `(n ~ S n')`, and the two values `x :: a` and `xs :: Vec n' a`. "Refinement" means the new information we have about the type variables in these environments; that's exactly the power of GADTs. However, we can easily encode these type environments using just equality constraints and existentials again. data VecADT n a = (n ~ Z) =&gt; NilADT | forall n'. (n ~ S n') =&gt; ConsADT a (Vec n' a) In this case, to construct a `VecADT n a` with the constructor `NilADT`, we have to provide the constraint `(n ~ Z)`; that is, `NilADT :: forall n a. (n ~ Z) =&gt; Vec n a`, or, simplified using the obvious type substitution, `NilADT :: Vec Z a`. Similarly, to construct a `VecADT n a` with the constructor `ConsADT`, we have an existentially quantified type variable `n'`, the constraint `(n ~ S n')` an object of type `a`, and another object of type `Vec n' a`. Putting that together we get ConsADT :: forall n' n a. (n ~ S n') =&gt; a -&gt; Vec n' a -&gt; Vec n a Again we can simplify by substituting `(S n')` for `n`: ConstADT :: forall n' a. a -&gt; Vec n' a -&gt; Vec (S n') a and alpha rename and remove the implicit foralls to get ConsADT :: a -&gt; Vec n a -&gt; Vec (S n) a which is the same type as we specified for the GADT version of this constructor.
I'd like to see a more concrete example of what you are saying so I can understand it better; the posted link says exactly this: &gt; This pattern matching with simultaneous type refinement seems to be one of the key features of GADTs. which is exactly what you get by augmenting the type environment with new type variables and constraints as a result of pattern matching.
http://progmod.org/forum/sujet/87/haskell-list-shuffle/ Comments are in french, but the code is in english!
How about when you need to split by commas or something else (other than a space)? That is one function that really *should* have been in the prelude.
I usually call (.) . (.) `oo`
We could also define type equality as {-# LANGUAGE RankNTypes -#} data TEq a b = TEq (forall f. f a -&gt; f b) which means that we don't need type families: {-# LANGUAGE ExistentialQuantification #-} data T a = TBool (TEq Bool a) | TInt (TEq Int a)
I wholehartedly agree. Assuming that important semantic changes is the norm, and that every package update should individually be reviewed by N authors is wrong. The semantics of a package should be expressed in types or in test. If it compiles and all tests pass, then it should be shippable. If the package has no tests, then upper bounds might make sense, but it is really a band-aid. 
This problem has been solved already, for decades. See dynamic libraries on unix platforms. Major version means API change, minor version is compatible. If I link to libfoo.2.0, then libfoo.2.1 or libfoo.2.2 is fine, libfoo.3 is not. Simple, effective, proven.
The argument for upper bounds on packages is that if a dependency DOES make breaking changes that you have time to fix your library without your users breathing down your neck that your package doesn't compile. For example, I know Michael depends on his `yesod` suite of libraries for his work, and he certainly would like to know that he won't wake up one day to find his tool-chain breaks when he needs it the most. However, that said, I agree that upper bounds still need to be replaced with a better system. To be honest, I think the whole versioning system is just flawed. For example, for a while `data-lens` had an upper bound of `containers &lt; 0.5`, but I really wanted to use the newest version of `containers`. This was very odd because as far as I can tell `data-lens` only uses `containers` internally and doesn't reference it in the API, so it seems weird that I can't use simultaneously use a different version of `containers` in my own project. Perhaps we need to distinguish between dependencies required for the package to compile and dependencies that matter for downstream users. Also, I don't see why we version by the entire package. Why can't we just version by individual functions, so that if a package makes a change that doesn't affect a function's individual version number then the upgrade satisfies the dependency. Obviously this is not an ideal solution because then cabal's dependency solver would probably take way too much time, but I guess what I'm trying to say is that maybe the version system itself is flawed and we need to come up with something else entirely.
We already have a system for non-breaking changes, which is to change the sub-minor number. The problem is that Haskell packages change their API all the time.
I think it's more common than people realize that something which builds fine would fail to build with no upper bounds. Since upper bounds are commonly used, you won't really notice it when they're helping. For example when blaze-html 0.5 came out many packages without upper bounds on that dependency started to fail to build, and if they'd had upper bounds Cabal would have been able to select an earlier version and the build would have succeeded if all dependencies worked with the older version. What could be nice though is if cabal-install's `--constraint` flag allowed bounds outside what the packages specify. Currently that flag only lets you make a constraint *more specific*.
This is supposed to be how PVP works, and yet the Haskell ecosystem fails terribly at it. The true problem isn't that people use upper bounds, it is that packages don't adhere to PVP.
That's exactly what the PVP and upper bounds do. What are you proposing?
As long as people use upper bounds like &lt;= 2 instead of &lt;=2.4.3.1, upper bounds shouldn't cause any problems right? Is that what the PVP currently recommends doing?
I'm not sure what the PVP says specifically, I am just "proposing" that people do it properly. Don't set upper bounds that include minor release numbers, as that will break things. And make sure you increment version numbers correctly based on whether or not the API changed, instead of "yeah I think this number sounds good".
It does cause problem anyway. For example, every release of base typically causes every package to fail to compile due to an upper bound on base. base is a huge library so the chance that the few breaking changes that were made will affect your package is often tiny.
You can still add upper bounds if newer versions of a depedency are known to fail.
Yes, although in PVP, the first *two* numbers are the "major" version, and the third is the "minor" version; but yes, the rest of the idea is the same.
But you can't know that until after the newer version appears. At that point, *you* must make a new version release of the old version of *your* package in order to correct the issue. Few maintainers will be so thorough as to manage all old versions of their packages in this way.
By the PVP the major version in `A.B.C` is actually `A.B`, so upper bounds on a minor version (the `C`) are rare. However the PVP recommends constraint on the minor version if you use open imports, because a release that only adds names to a module without breaking existing ones is supposed to only warrant an increase in the minor version. Thus if you use open imports you might get ambiguous names that break builds. The reason people dislike these upper bounds though is probably because Haskell packages tend to release often, with breaking changes, and we tend to have a lot of dependencies, where the dependencies in turn may also depend on some of the same packages. So when you want to try the latest version of some dependency A, you might have a dependency B that also depends on A but with the older version. I think these issues are far less common in C programming, where you usually depend on just a few libraries who in turn have very few, if any, dependencies. And in dynamic languages the solution is basically to defer issues with API incompatibilities to runtime.
&gt; You will be responsible for supporting MongoDB in the Haskell ecosystem through driver development, integration with open source tools, community support, and developer advocacy. Presumably, this means you will make sure that haskell libraries make interfacing with MongoDB pleasant, fast, and bug-free. From a quick ctrl-f of hackage: * [mongoDB](http://hackage.haskell.org/package/mongoDB) * [persistent-mongoDB](http://hackage.haskell.org/package/persistent-mongoDB) * [structured-mongoDB](http://hackage.haskell.org/package/structured-mongoDB) * [snaplet-mongoDB](http://hackage.haskell.org/package/snaplet-mongoDB) 
Extending an API *can* break things, and that's exactly where the problem lies (because it can introduce name clashes). So every extension of the API is a major change, and thus package upper bounds exclude compatible changes.
&gt; I think the whole versioning system is just flawed.... Perhaps we need to distinguish between dependencies required for the package to compile and dependencies that matter for downstream users... Why can't we just version by individual functions... I wholeheartedly agree with all of these points. &gt; this is not an ideal solution because then cabal's dependency solver would probably take way too much time I think the typical Haskell philosophy applies here: build it *right* first, and *then* figure out how you can make it fast. &gt; maybe the version system itself is flawed and we need to come up with something else entirely. I concur.
No, I think you are taking it as insulting when it isn't. There's lots of issues where lots of people have thought about it a lot, and there's still no consensus on what to do about it. Dependency management isn't unique to haskell, which is really the point I was making. It seems pretty bizarre that haskell has so much trouble with this area given that nobody else does.
&gt; But you can't know that until after the newer version appears. Personally, I think this is the core of the problem, but in a way you didn't mean. The problem is that you _really_ can't know what versions are going to fail in the future, until the future arrives. That's actually a fundamental constraint, and the system simply has to be built to deal with that fact. That's not an objection, that's a _ground rule_. There's no theoretically pure perfect solution here. It isn't that we're not smart enough to find it, there _isn't_ one. At the time that a package is generated, there is no way to correctly write the upper bounds, where by "correct" I mean to specify the exact future versions this will and will not work with. Given that, it is merely a question of what practical policy causes the least pain, not what policy causes zero pain. I'd also observe that there's very little specific to Haskell in this problem, if indeed there is anything specific to Haskell at all, and the solution the Haskell community is currently using is simply worse than the ones used by other communities. I can see the initial appeal to being strict about upper bounds, but reality has spoken and that theory doesn't seem to pan out.
What i would love is for cabal to have a compile option to try the latest package and in case of compilation failure automatically switch to suggested by upper bounds. This way we could control the behavior on a project to project basis. 
The Logic monad from the logict package is pretty neat. It offers a more efficient alternative to the List monad for nondeterministic computations, along with additional functionality related to logic programming. Also, it has a monad transformer variant.
I'll reiterate my view here. Upper bounds are not the problem. It's the absence of a way to circumvent them that's the problem. An upper bound is essentially an annotation saying that you're less confident that your package will build outside a certain set of constraints. The existence of this information is a GOOD THING. And it is an absolute necessity if you want a particular version of your package to survive through time. If hackage packages don't supply upper bounds, then hackage will become unusable for production systems where it's not prudent to constantly upgrade, and running very old versions is common. I know this because I have production code in use today that I can't build any more because I wrote it before I understood the importance of putting version bounds in my cabal file. It would have been very easy for me to supply the information back then, but it is very difficult for me to reverse engineer that information today since it's been several years since I touched the code. One thing that would be interesting is a hackage+cabal-install arrangement that would allow hackage to track what dependencies were used for successful and unsuccessful builds along with other information such as OS, architecture, etc. This might allow us to gather the upper bound information without imposing a burden on the package maintainer. EDIT: Hmmm, on further thought, what about this idea? Everyone always specifies a closed upper bound (as in '&lt;=' instead of '&lt;') and if you don't, then cabal/hackage will do it for you. Then, every time a package is updated, hackage computes the delta of the exported type signatures, then looks at all the package's reverse dependencies and sees if the changes affected any of them. If no, then it automatically updates the reverse dependency upper bound to include the new version. If yes, then it doesn't do anything, leaving it up to humans to bump the bound if necessary. You wouldn't have to do any sophisticated analysis...just see if the types match up. At first glance, this seems easier to me than nominolo's proposal of versioning every symbol and mgsloan's proposal of compatibility modules. That isn't to say it's easy--it's definitely not. But it seems more tractable to me. And when combined with the ability to retroactively modify bounds, it seems like it might be effective.
Exactly. In practice, most of the packages are not maintained actively. So there will be nobody to add an upper bound when needed. Not sure if this is a better picture.
&gt; there's very little specific to Haskell in this problem As a small footnote: the Haskell-specific manner of compilation and optimization adds to "the problem". The compiled version of MyLib-0.2.1 cannot act as a drop-in replacement for the compiled version of MyLib-0.2.0. I completely agree with your assessment. The question now is this: should we be strictly defensive by using upper bounds, or lazily defensive by adding upper bounds only when an issue arises? I believe the lazy approach will make it easier to use the latest versions of things, but harder to use old versions, while the strict approach will provide the reverse effect. There are two questions that follow: 1) is this assessment correct? and 2) if so, then which is more important: ease of using the latest, or ease of using old versions? While I'd love to unilaterally support the former, the unfortunate truth is that most serious programming projects demand the latter.
&gt; Also, I don't see why we version by the entire package. Why can't we just version by individual functions, so that if a package makes a change that doesn't affect a function's individual version number then the upgrade satisfies the dependency. Yup. That's basically what I'm proposing [here](http://nominolo.blogspot.co.uk/2012/08/beyond-package-version-policies.html)
To do reactive upper bounds you release a new patchlevel version of your lib A when you know that one of its deps needs a upper bound. cabal-install we automatically prefer the new patchlevel version in favor of the one with an open upper bound.
That's awesome! I get it now. I was actually doing something like this yesterday with a comparison function, and I was wondering how to make it work like this. Great tip!
That's a reasonable guess, but I've played around with the combination of tying the knot and monads bit, such as [control-monad-queue](http://hackage.haskell.org/package/control-monad-queue) and [Fun with the Lazy State Monad](http://blog.melding-monads.com/2009/12/30/fun-with-the-lazy-state-monad/), the former which seems to depend on the continuation monad, and the latter seems to depend on the lazy state monad. Both of these dependencies seem to be essential, and also fragile. I've not been able to apply the reverse state monad to either of these problems, and not for the lack of trying. Also, ryani's application doesn't tie the knot at all.
That just feels like the wrong tradeoff to me. Not everyone can afford to be on the train of never-ending upgrades. Why sacrifice that when the addition of a simple command-line flag can give us the best of both worlds?
I also very strongly agree that cabal needs to be able to distinguish between internal and external dependencies. dcoutts has done some some work on this. My gut instinct is that the reactionary versus proactive approaches to upper bounds has all the makings of a religious war, and one where allegiances could rapidly shift by making some much-needed improvements to the cabal/hackage infrastructure. My belief is that the top two improvements would be 1. wiki-like editing of dependency ranges after the fact available to people with valid Hackage accounts. 2. separation of internal and external dependencies Other concepts may help too: more integration of cabal's new testing infrastructure with hackage, having both proactive and reactive ranges and able to specify (or leave out) any combination thereof, etc. But my guess is that those two points are the most important ones. Edit: Also, I don't know whether dcoutts would agree with this or not, but I meant to say that I think dependency hell is likely one of those persistent problems that will always be with us, especially as software grows ever more complex, but that there are ways to improve the situation.
Well, I think the first step towards that vision is to be able to modify version constraints after-the-fact, ideally with wiki-like editing available to those who have a hackage account.
Yes, I'd agree with all this. Add to that a system like nominolo has been suggesting recently to track dependencies more carefully than can be captured with a version number and use this as an automated way to suggest that dependencies can be relaxed (or simply automatically building and running tests would probably do too, though it's more work). The first step towards that is a tool to extract the package API. Simon Marlow and I have suggested this as a GSoC project a couple times but it didn't make it. Perhaps next year since this issue is at the front of people's minds.
Well, I think it'll be less of a problem than you think; people have a lot invested into hackage and tend to be proud of their contributions and hackage as a whole. People understand hackage etiquette. People want to be known and have a good reputation within the community. And that's why we haven't needed to enforce more stringent security criteria. (yet?) I mean, I serve as a bouncer for new accounts on Lambda the Ultimate, and we have not had a single spam message for many years ever since we've gone to "accounts must be approved before they can post" model. And the bar for getting approved is very low, all that is needed is that you convince me that you are a real person with at least a passing interest in computer programming. I do this typically by looking at the email address, which by itself is sometimes enough to tell, and usually enough to get a good idea. I then consider the username and any comments added to the account request. As a last resort and/or confirmation of my belief, quite often I google the email address, though I rarely click through to any of the results. I've found this to be quite effective at seperating the spammers from the rest. The other advantage to this model is, much like a physical-world bouncer and patrons, you can be suspended or thrown out if you start causing problems. So I think that people should be able to subscribe to notifications that the version constraints have been changed for a given package. Also, in keeping with wikis, we would need to keep track of who changed what, and when. Under these circumstances, I really don't think this will be a problem even if hackage grows in popularity by another order of magnitude in the coming years.
Wait, why is your former approach not suitable? What is wrong with: data T a = TBool (TEq Bool a) | TInt (TEq Int a)
Thanks for the reference! I didn't have a reference earlier that Luke Palmer's blog post on the Reverse State Monad. And I know I've looked at Wadler's paper in the past, though that was probably before I was comfortable with monads.
Not if you want to encode this GADT: data Stuff a where Unit :: Stuff () Product :: Stuff a -&gt; Stuff b -&gt; Stuff (a, b)
Actually, I believe you can still do that, but it requires existential quantification, and I don't have time atm.. I'll return soon to show how.
Oh yeah, you'd still need `ExistentialQuantification`. I'm guessing it would go like: data Stuff a = Stuff (TEq a ()) | forall x y . Stuff x y (TEq (x, y) a) Still, not bad.
The interesting thing is that pretty much anything which doesn't use direct dependent types can be encoded in simply typed lambda calculus plus universal and existential quantification (edit: and type constructors). The things we did just turned Haskell into a less "pure" version of that.
Doable - there are many typed attribute systems out there. Nesting could be done via debruijn types. Essentially a deep embedding in Haskell, for a simple expression language, and a whole ton of attributes.
Is it possible to detect old dependencies and upgrade to the newer api, or at least in certain cases?
`Control.Concurrent.Spawn` has a similar function, [`parMapIO`](http://hackage.haskell.org/packages/archive/spawn/latest/doc/html/Control-Concurrent-Spawn.html#v:parMapIO). You can also implement `concSeq` directly as `mapM spawn &gt;=&gt; sequence` (or, with `Control.Concurrent.Async`, `mapM async &gt;=&gt; mapM wait`).
Haskell and the Haskell community makes me feel so stupid. Thank you!
If you change one of 50 API functions, you may break none to some unknown percentage of your users.
People should import qualified or name lists. Adding names should not break things.
Another approach (that could be used with something like Hamlet) is to run HTML built at compile time through a validity check (libtidy perhaps) (also at compile time).
I completely agree with you on this point. I've been wanting wikified version bounds for some time now. I think a feature like this would also effectively eliminate the difference between those for upper bounds and those against.
That ought to be changed then! Add the upper versions by changing the cabal/hackage database and not the package itself. 
[Haskell is an advanced purely-functional programming language. An open-source product of more than twenty years of cutting-edge research, it allows rapid development of robust, concise, correct software. With strong support for integration with other languages, built-in concurrency and parallelism, debuggers, profilers, rich libraries and an active community, Haskell makes it easier to produce flexible, maintainable, high-quality software.](http://www.haskell.org/haskellwiki/Haskell)
I have done something like this, which I use internally. I'm extending it in terms of my needs as I come to use each tag. I am often torn between sticking to the element/attribute concrete representation, or going for a more abstract representation which is less intuitive for those who've internalized the html language, but which more accurately describes the legal html documents. I guess I should solve that tension by have two distinct layers, but that's even more work, and may impact performance further. I also refer to a custom, parsed, URL type, instead of allowing any string where a URL is needed, but others may want to use a different URL type. The model plug into CSS, SVG which I've do to some extent, but there are even more design choices there. I'm doing it so I think it is worthwhile, but it seems hard to do something that would be widely applicable.
Here's a similar idea: mapSpawn :: [IO a] -&gt; IO [IO a] mapSpawn = mapM spawn Then the resultant list will be full of IO actions for retrieving the result of each computation. As shachaf mentioned, you can then perform `sequence` on that.
That's how you know you are a true Haskell, being made to feel stupid only spurs you on. 
Yep, WASH did this, see HTMLPrelude.hs in http://www.informatik.uni-freiburg.de/~thiemann/WASH/WASP-HTML.tgz where there are hundreds of AddTo and AddAttr instances, one per line. :)
I'm not fond of the idea of removing upper bounds, because the semantics of a function may change, but its type and interface may not. This means that formerly correct code may become incorrect just based on the packages it's built with. It might build, but it will do *the wrong thing*. This irks me. We should, as a community, have more focus on correctness.
Well, as for (3), we don't really have a solution. Current version numbering scheme would fail just as miserable, so I think it's safe to ignore this issue. I confess I don't really understand (1) — could you give an example? As for (2) — it could be a real issue. I have to think about that a little, but it seems possible for resolver to detect such renamings. For example: if the package foo imports type Bar from the package baz explicitly, by specifying it in some type signature, and this type gets renamed — there is no hope for it ever compiling, so, resolver must fail. But if foo doesn't explicitly specify the type Bar, it just gets inferred, and the new version of baz package replaces it with some type like Bar1 -&gt; Bar2 Bar3, the resolver might detect this change, make the same correction in all other imported functions, and verify that everything is still good. And yes, I think it's OK for a manifest to depend on whether or not explicit type signatures are used — after all, succesful compilation does. There is another side of this problem though. What if baz-1.0 provides some value of type forall a. T a, and baz-2.0 changes it to forall a. C Int a =&gt; T a? This value is less generic then the previous one, and there is no way to know for sure if there is an instance of C Int Fuux before attempting to compile foo. My proposition would be "ignore the typeclass constraints you can't check, try the last possibly-good version, if it fails — try the default one, if it fails too — complain loudly". Now, your adapter idea seems cool, but I doubt anybody would do it. Unless it's enforced somehow that the new version provides exactly the same interface — but that would prevent some useful changes also, forcing packages to drag along lots of unnecessary code, or even old bugs. Suppose for example, that the developer wants to provide a function calculateLineHeight :: Widget -&gt; Integer. Xe doesn't want to bother with it at the start, so xe just writes a stub like this: calculateLineHeight _ = 12. After uploading it to Hackage xe realizes xer mistake and tries to fix it. Unfortunately, the new function looks like calculateLineHeight :: Font -&gt; Widget -&gt; Integer, as there is no way to calculate line height without knowing the font used. And it breaks compatibility with an old (buggy) version.
Yes, but (unfortunately?) the language allows it and it's convenient so people do it.
Right, but touting that is kind of making newcomers believe that they will achieve same speed in a near future (i.e. whilst knowing only a little of Haskell), which is wrong. It's not that hard, but it takes some learning. So I prefer to stay circonspect on that respect instead of blatantly claiming "yes, same speed" or "speedier even".
I used the Dimensional package from the link in my thesis' piece of physical simulations software. The library is very simple in the inside, easy to use and to add new units. 10/10 would use again :)
I guess you're right. It doesn't seem like I can do this without TypeFamilies or similar extensions...
There's also [parallel-io](http://hackage.haskell.org/package/parallel-io), providing a function parallel :: [IO a] -&gt; IO [a] which only runs N threads at one time, which might be preferable in some situations, e.g. if your 'input list' is very long and/or your computations need a lot of memory for intermediate results.
I come from a C background myself, and I can tell you that the best way to find out the benefits of Haskell is to use it for as many things as possible, so you can appreciate what it excels at it, which is almost everything. The first big advantage you will notice is how easy is it to create data structures on the fly. C has very brittle data structures because of the need for manual memory management, maintaining header files, and the lack of reusability of helper functions on these structures. This means that a lot of the time in C you will often choose a poor and inefficient algorithm because getting the data structures right can be an absolute chore for even the simplest of algorithms. I often find that I get much better performance from just choosing the right algorithm (because data structures are cheap in Haskell) than I do from working so "close to the metal" in C. Also, Haskell makes great use of tagged unions (they call them "sum types") and pattern matching to do things you wouldn't ordinarily think of using them for in C. This is something you can't really appreciate until you try it. This lets you write code where you can very clearly express all the alternative code paths a function might take and guarantee that you handle all of them. Once you get used to it you will really begin to miss it in other languages. Haskell is a compiled language and it produces very fast code with only a small amount of effort, much less effort than you would need to write the equivalent C program. There are two main reasons Haskell is fast: a) The compiler developers are first-class computer scientists that incorporate state-of-the-art optimizations that dramatically reduce the impedance mismatch between the functional paradigm and CPU architectures and provide a host of sophisticated program analyses and transformations. b) The standard libraries use rewrite rules that generate efficient transformations that remove intermediate data structures before the compiler even begins to optimize anything. This lets you program at a very high-level with very little hit in performance. Even if mainstream languages had these kinds of rewrite rules, they couldn't take advantage of them as much because they are less functional and consequently there is less code reuse since it is much more difficult to build up programs from a small set of reusable primitives. Haskell has one major weakness, which is the dreaded space leak. Right now it is still something of an art to remove space leaks in a program and it takes an intermediate level of proficiency in Haskell before you get good at it. This is probably Haskell's biggest weakness compared to C, because C gives you predictable memory usage with very little effort. However, I would venture to say that the amount of effort it takes to fix a space leak in Haskell is about the same amount of time it takes to debug a segmentation fault in C. Haskell also has several things C programmers really miss, such as a much more fully featured standard library and generic programming. Notice that in all this discussion I haven't even mentioned any of Haskell's high level features. You can do really amazing things in Haskell that simply are not even feasible in C, but the only way you can really appreciate how powerful Haskell is is to take the leap and try it out.
[Link](http://hackage.haskell.org/package/dimensional) to the `dimensional` package.
As long as Haskell lacks an IDE with an "organize imports" feature like eclipse, I predict that people will not do this. Thinking back to my java years, on projects where I used Eclipse, my import lists ended up entirely as explicit imports. But when I used vim, I always reverted to importing package.*.
I use emacs without any sort of smart import management (except a bound key to sort-lines for the import lines). It does take me a few extra seconds of work here and there, but all my imports are qualified or name lists. The source of names is clear.
Thanks! Fixed.
If you have any suggestions for how to explain it better I will update the post to expand upon it. I assumed most readers would be familiar with `(=&lt;&lt;)` because it is just `(&gt;&gt;=)` with the arguments flipped. The way I like to think of it is the `Monad` equivalent to function application: ($) :: (a -&gt; b) -&gt; a -&gt; b (=&lt;&lt;) :: (a -&gt; m b) -&gt; m a -&gt; m b The implementation for `(&lt;=&lt;)` sort of hints at that by parallel to the implementation for function composition, but I never did actually spell that out.
I recommend just calling them 'Alternative's ;) 
I think there may be some burden on the author of the function to avoid creating this situation.
This might suffer of the "too fine-grained parallelism" issue, depending on the job to execute.
You're welcome! Always glad to clarify!
 data Attr a = Attr a | Data Name Text
Where "type Name = Text"? Whilst the string-form should fulfill certain rules to be a valid attribute name. Any way to tackle this? (Except for smart constructors which push potential errors to runtime). My point was: it's possible to enumerate valid attribute names for a given element at the type-level, except for data-* stuff. The sum you mention won't enforce any validity constraints, so it's rather useless IMHO.
Nice. We use Category for composition of the [boomerang](http://www.happstack.com/docs/crashcourse/WebRoutes.html#web-routes-boomerang) PrinterParser combinators (combinators which define a pretty-printer and a parser at the same time). It works out very nicely and looks a lot cleaner than applicative functors, IMO. With applicative functors you have to use lots of *&gt; and &lt;* when you have combinators that parse value, but just return a () that you don't care about. With Category, you can instead define those parsers as an identity function. A bit contrived but: (,) &lt;$&gt; ("foo" *&gt; "bar" *&gt; int) &lt;*&gt; ("baz" *&gt; char &lt;* "biff") tuple . "foo" . "bar" . int . "baz" . char . "biff" There are two annoyances I have found though: 1. it is annoying to have to hide ((.), id) from Prelude and import the versions from Control.Category instead. Mostly because it is extra typing (which I don't like), plus I always forget to do it and then I get weird type error messages until I realize what is happening. 2. My other annoyance is that we have the syntactically pretty (.) that we can use instead of (&lt;&lt;&lt;). But there is nothing pretty for (&gt;&gt;&gt;). While it may not seem like a big deal, I find that the &gt;&gt;&gt; just looks a touch overwhelming. Compare: "foo" ␣ anyChar ␣ int / "bar" "foo" &gt;&gt;&gt; anyChar &gt;&gt;&gt; int / "bar" Not a huge difference, but I find the first a bit easy to read. Alas, ␣, is too hard to type to be practical. Long Live Category! 
Big thanks for writing this up. Been hoping people would start talking about Kleisli categories in a way that's useful to beginners; it seems to me that monads would be quite less mysterious to people if it were clear to them that their laws and signatures are just an instantiation of those of Category, which is conceptually easier to understand.
Nicely done! Still waiting for your new formulation of pipes.
Very good and understandable arguments. I think I'll reuse them. &gt; Even if mainstream languages had these kinds of rewrite rules, they couldn't take advantage of them If it's not clear for OP, the main reason here is that you can't apply rewrite rules to code that may perform side effects. So it's all about purity: when a function is pure (i.e. cannot do any side effect and reason only from its arguments), then rewrite rules and the compiler can do whatever they please with them, which gives much room for static optimizations. But mainstream languages don't provide the guarantee that some part of the code won't perform side effects. &gt; Haskell has one major weakness, which is the dreaded space leak. I would venture to say that it is the burden of every language with automatic memory management, just that it's surely even more true for Haskell. As long as you leave that task to the compiler, you'll gain much time during development but profiling is more likely to be required at the end. As you said, it's quick, if we have the right tools (and GHC give us the right tools).
It will be ready soon.
Claiming any operator will be bug-free by following the laws is a bit over the top, IMO: * Leaks and other operational behavior bugs are still possible * In some cases there are multiple possible operators for the same types that would be compositional -- so it could still be considered buggy if you get one instead of the other 
Yes, if I were to go into more detail I would point out that category theory also does not define what = means, and that I implicitly assume that equality does not equate performance or space efficiency. In principle one could assume that more stringent notion of equality, but then we would need considerably more sophisticated tools to reason about such equality. The latter point is more a flaw of overloading rather than a bug in composition. In principle you can define distinguished names for each type of composition to avoid confusion. Nothing about category theory says you must overload the composition notation.
The problem with flat out ignoring upper bounds is that they are not created equally. A single package can have both hard upper bounds for some packages and soft upper bounds for other packages. It is necessary to be able to ignore the latter constraints without ignoring the former. And there is no way to effect that distinction in compilation behavior without effecting the distinction in the semantics of the version constraints themselves.
Well, one example would be to consider backfilling. I haven't hacked it up in Haskell, though it seems like it should be straightforward to do so with reverse state. For those not familiar with the backfilling technique: Consider that you're compiling a language down to assembly code. In particular, consider what is involved with compiling conditionals. Assembly has conditional jump commands, so this should be easy, right? But the problem is, assembly is strictly linear. You can't just say "jump to the next expression"--- expressions are a hierarchical notion, not a sequential notion. When you need to make the jump, you don't yet know how far to jump; and once you do know where to jump to, you've already made a bunch of other commands. Rather than making place holders and going back to patch things up once you're done, one nice clean way of dealing with this is to leave holes in the assembly program you're generating. Whenever you need to make a jump command, rather than giving the command a label of where to jump to, it gives *you* a hole that you can fill with the correct label once you figure that out. Then you just chug along, generate the positive branch, generate a label for the negative branch based on the current position in the instruction stream, fill the hole so the conditional knows where to jump to, generate the negative branch... It should be pretty obvious that: * These holes are really just continuations. * We could fill the holes "immediately" by asking the future what should go there. * We're relying on a certain kind of laziness, making promises and then fulfilling them later.
Very nice! IF Raspberry Pi really get's that pervasive than porting the HaLVM as suggested https://plus.google.com/115274377971493973150/posts/2NjotTvUFLp could really be worth it (and it makes for a "good" platform where due to the limited RAM a smaller kernel really pays off).
Also glad to see our new distributed-process and related packages are proving easy to install. Credit to my colleague Edsko. I'll be [giving a talk](http://www.haskell.org/haskellwiki/HaskellImplementorsWorkshop/2012/deVries) on this new Cloud Haskell implementation at HIW next month.
Alright, I will update the language to reflect the more precise meaning I have in mind in an upcoming post.
Well, I don't really know a lot about Lisp, so I'm not the best person to ask. However, I think I can point out some differences between the two languages. For example, Lisp is a multi-paradigm languages that encourages many different programming styles (i.e. imperative, functional, and object-oriented). Haskell, on the other hand, is very strongly functional and frames all other programming styles as subsets of functional programming (including imperative programming, which it implements using functions under the hood). Also, Haskell has a static type system, meaning that all type errors are caught at compile time. Lisp (I think) has optional type annotations (somebody correct me if I'm wrong). However, in Haskell, unlike C, you don't have to add type annotations to most of your code. The compiler can figure out the types of your functions automatically, which means that functions are very cheap to declare and use, even cheaper than C functions. For example, if you wanted to write a function to increment a number in C, you would write: // in "increment.c" int increment(int x) = { return (x + 1); } ... and then you would have to modify your header file to include the function signature: // in "increment.h" int increment(int x); On the other hand, in Haskell you would write: increment = (1 +) ... and that's it! No type signatures or anything. So you get all the benefits of compile-time error-checking without the overhead of declaring type signatures. Another difference is that in Lisp everything is oriented around lists and list-processing, so lists are the central abstraction for Lisp. For Haskell, the function is the central abstraction. If I had to guess what the main philosophical difference between Haskell and Lisp is, I might guess that Lisp emphasizes flexibility, whereas Haskell emphasizes correctness. That also pervades the community attitude a bit, too, as you will see that many people here are very rigorous and exact and will always correct you. If you appreciate being corrected (by the compiler or by the community), then you will enjoy Haskell.
But... but... they're directory entries! You need them for navigation. I kid not, `..` might actually differ from chopping an element off the end of the path. They're also perfectly harmless, as you should be filtering by extension or similar if you're doing anything else but displaying the contents. And when displaying, you should not just filter out `.` and `..`, but `.*`
http://hackage.haskell.org/packages/archive/system-fileio/0.2/doc/html/System-Directory.html Not in the standard library edit: the listDirectory function
&gt; does / have a .. entry of course: $ ls -lai / total 104 2 drwxr-xr-x 23 root root 4096 Aug 5 23:53 . 2 drwxr-xr-x 23 root root 4096 Aug 5 23:53 .. [...] ...both point to '`/`', as the inode number shows: $ ls -laid / 2 drwxr-xr-x 23 root root 4096 Aug 5 23:53 / Also, foo &lt;/&gt; "bar" &lt;/&gt; ".." is *not* the same as foo if `bar` is a symlink. That's because '`..`' is a directory entry, and not some fancy special case.
I'd also be interested in your thoughts on the 'api-epoch' idea I floated. It's purely opt-in with sensible defaults, while providing a way (independent of pvp versions) for authors to specify when a package change is potentially so significant as to break all clients.
 saneGetDirectoryContents :: FilePath -&gt; IO [FilePath] saneGetDirectoryContents = fmap (drop 2) . getDirectoryContents Because under normal conditions "." and ".." get listed first right?
I can't tell if you're being serious.
[direct link to listDirectory](http://hackage.haskell.org/packages/archive/system-fileio/0.2/doc/html/System-Directory.html#v:listDirectory)
[Hayoo](http://holumbus.fh-wedel.de/hayoo/hayoo.html)?
&gt; I've lost count of the number of bugs in my code introduced by the stupid design of getDirectoryContents. For example?
(Which is another function that should be in the standard library, I think -- I've implemented this too many times to remember.)
Hey, can someone explain the process of getting something into the standard library? Email? Ticket system? There are other obvious omissions that really annoy me and which I have to write all the freaking time. For instance, `reject = filter . (not.)`.
Yep, wxdirect.
Right. Even if another easy way to get space leaks is through sharing (A huge data type that is kept in memory instead of being simply garbage collected and recomputed when needed), and this is due to the compiler being too smart. It would be one of very few the things I'm disturbed by: the fact that you have sometimes to use a little bit of "arcane" tricks to outsmart GHC when it comes to memory handling.
A steep learning curve means you learn over a shorter number of attempts, i.e. more quickly :-)
That is an interesting aspect I hadn't really thought of: you cannot fix a too loose upper bound. If you later release a package with a stricter upper bound, Cabal will happily choose the old version. This really shows that no upper bound is a lie: it says your package will work with any future version of its dependency.
That would be like editing the source code of a package version, without changing the version number. A version is supposed to be a snapshot, so that I can say: build my package with *that* version of a package. If versions can change after the fact, version numbers become kind of meaningless.
I don't think that's true. Say I have executable E depending on libraries A == 0.1.\* and B == 0.1.\*. B depends on A as well, no upper bound. Now, A 0.2 is released, breaking B (and also E, because B has no upper bound). A new version of B is released, with fixed code, and depending on A == 0.2.\*. Now B is fine, if you ask cabal to just install B. But installing E is still broken: cabal will try the latest version, find that it cannot resolve the dependencies (because E depends on A-0.1 and B depends on A-0.2), and will try the old version, which will fail to compile. This goes the other way as well: if the new B doesn't change any code and just adds an upper bound of A == 0.1.*, then everything is fine until E wants to use A-0.2. Cabal will then still find an install plan using the old B without an upper bound, but code will break. This can happen even more subtly if E works with both A-0.1 and A-0.2, and has dependencies that upgrade from one to the other. Cabal will suddenly select older, unconstrained versions and again, you'll get build errors. My point is, this will lead to a lot of dependency hell, something we're all trying to avoid. And this dependency hell might be worst, since it can manifest itself as build errors outside your own code, even in libraries you're not even using.
Can you stop writing this? I've seen you contribute real Haskell-related content on reddit and it'd be nice if that started again.
In addition to those two you might want to build with a full set of exact versions too for production systems in a similar way to Ruby bundler Gemfile.lock.
http://pastie.org/4557033 ?
Can we not record the reference configuration for a package -- then at least the reference configuration is availabe. Of course the ideal case is that it should build straight from the cabal file and this should generally happen with well-maintained packages (which should also have reasonably up-to-date reference configurations).
I find Haskell is *more* forgiving when I'm hacking, because it will tell me when I'm wrong (often).
nice! I think you just need a lens to access the fields of the tuple (as far as I can tell you are only modifying the x value)
Make a map from GLUT.Keyxxx to triples, i.e., GLUT.Down = (0,-1,0) etc. Multiply that one by three and add (x,y,z) to it. No more boilerplate. Needs some tuple map function, though. I don't think tuples can be functors, since the type of the values can be different. 
Yes, more information is better. The right solution is to have more information not less, and to use automation to help collect and update it. I agree that distinguishing upper bounds that are conservative approximations from definite upper bounds would be useful information. The tricky bit is to find a good way to allow this to be expressed in the .cabal file (and/or separate metadata). What I mean by automation is something like this: suppose you got an email from Hackage saying "Heads up! Package P has just released a new version X. Your package Q depends on P, and the good news is that we've checked and Q compiles fine against P-X. Please press this button to update Q's dependencies to allow building against P-X." 
Of course we don't need metaprogramming for the example I pasted. I was referring to it when I said, "But not all cases can be chalked up to the lack of metaprogramming ability." &gt; Haskell does impose more discipline upfront. I am not sure what you mean requires a mini-theory? But Haskell is less forgiving if you try to hack things without knowing what you are doing. I think that is plus. I see it as both a negative and a positive. It is a negative for the reason I gave: often a full understanding emerges only later on. Patterns appear naturally from the code, and we don't know what they are until we have something semi-working. By mini-theory I just mean the thing you need to know before factoring. But you may not know it upfront, and so you can't effectively factor upfront. And having unfactored code makes it harder to discover what you need to know.
I dealt with this and a number of other fine points in my old [directory-tree](http://hackage.haskell.org/package/directory-tree) module, which you might find a more "sane" interface (at least that was the goal). Very OT, but if anyone here uses this lib, what do you think of this change?: "build" returns an initial tree with "file" field filled with FilePaths relative to the one stored in AnchoredDirTree. Then the "free" function would change to: free :: AnchoredDirTree FilePath -&gt; DirTree FilePath ...and would actually map over the DirTree, prepending the path stored in the Anchored wrapper to all the File constructors. This new "free" would be equivalent to: fmap fst . zipPaths
It would give you one set of packages to build your project, so I guess in the weakest sense, yes. But it also might be a bit limiting by not allowing any deviation from the one set of packages that happened to be used for the reference. I'm not sure whether this would be a significant issue in practice or not.
You're not changing the version number. You're changing bounds on dependencies. I don't think there's anything wrong with that in theory. It's essentially an annotation to help the Cabal solver figure out what set of packages will build successfully so it doesn't have to speculatively try to build them all.
It's relevant for the reason I gave: I encounter "bad code" like this more often in Haskell than I do in other languages. This post was my guess about the reason. How on earth did this code get published in a paper called "Beautiful Code"? I could write it off as an anomaly only if it weren't so common.
With [lens](http://hackage.haskell.org/package/lens) you can do _2 %~ (+1) which creates a function that maps `+1` onto the second part of a tuple. `_2` is typeclassed so it works on all tuples. &gt; A Setter a b c d is a generalization of fmap from Functor. It allows you to map into a structure and change out the contents. ~ http://hackage.haskell.org/packages/archive/lens/2.4/doc/html/Control-Lens-Setter.html
&gt; I seriously doubt that. Well I've told you my experience from reading Haskell code. And I gave an example. I didn't cite Joe Newbie's code; the example was from a paper called Beautiful Code recommended on the Haskell wiki. I guessed the repetition phenomenon was related to the extra effort involved in refactoring. Maybe you have a better explanation.
I presume by that you mean you would prefer a key on the guides page? I was unsure but I can certainly do that. If it is preferred. I am looking for feedback so if anyone has any thoughts on what to improve within the tutorials please let me know.
Put in a real TOC. The magic JavaScript circle is just unintuitive.
Not a problem, I will update the guides page on the morrow. 
I see the weird circle thing, it changes colors when I hover, and the headings appear in the center circle, but clicking around does nothing.
The job will be a combination of improving (developing database bindings/fixing bugs) and promoting (blogs/conferences/reddit/etc..). We want some really cool Haskell/MongoDB use cases, so you need to rally the community for both MongoDB and Haskell. 
If you use Chrome, you can add a search engine so that you can use the URL bar. For Hoogle, just set the keyword to 'hoogle' and the URL to http://www.haskell.org/hoogle/?hoogle=%s. Then you can type hoogle, hit tab, and enter what you want to search.
Maybe it's time to push?
I suppose it's fruitless asking about something that happened ten years ago, but did you actually declare types? Declaring types is optional in Lisp. Perhaps we are back to the old theological dispute about liberty. Deity: "Types are henceforth optional. You are free, my son." Programmer: "Hooray! I always wanted to compile new types and new functions at run-time! No more awkward greenspunning! And now I can develop without leaving the REPL!" [Time passes] Programmer: "Hey I made a type error!" Deity: "Well did you add type information to that code?" Programmer: "Nope." Deity: "I see."
Ah, I haven't used it in a while so I didn't know. Pretty neat feature.
Not sure if that's much better than having `render :: HTML -&gt; Maybe Text`?
How is this less information than you would be getting from upper bounds? You suggested that the difficulty in getting older packages built had made them difficult to use, but here you would have the exact place that you left of. You can now insert the upper bounds with full knowledge of how the packages have evolved.
Let me praise your effort first. Awesome! :-)
I think it is, now you won't even be able to create an invalid HTML data structure. This means that rendering always succeeds, independent of whatever operation you might have performed on the DOM-tree. 
I guess. Could also (at least as an option) provide a QQ that fails at compile-time for invalid names but gives you an Attr without the Maybe wrapper for valid names.
You have your parentheses and brackets backward `[this](http://www.example.com)` is how you link.
I did, however, perhaps I didn't declare enough of them. I also put all my source into a file and reloaded it every time I edited it, basically only putting expressions, not declarations, into the REPL. I did however make decent use of higher order functions (which I'll add was a bit more awkward than I'd have liked, but that's a separate thing). If you would, I'd like to see what a similarly typesafe equivalent of the following Haskell code looks like: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f [] = [] map f (x:xs) = f x : map f xs As an example, the analogous code to this: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f [] = [] map f (x:xs) = x : map f xs where the application of f has been forgotten, should not pass compilation. I consider this a sort of "pons asinorum for type checkers", a good simple test to see if the type system in a language is serious or not.
You might want to clarify what you want to use isTruthy for, especially what types you would like to see implement it.
Oh the ability to set different search engines has definitely existed for a while. What's new is that it's integrated into the "magic bar" thing so you can just type "hoog"+tab, without having to choose from a drop-down or anything like that. I have something similar set up for Reddit so that it goes to a subreddit instead of searching. On the other hand, maybe they did have that and I was just that tech-unsavvy.
Thanks, fixed!
Cool! I think it needs a better README. Currently it doesn't say what the project is about, I had to search through the blog to understand it.
This seems awesome. Looking forward to the day when I can get haskell-platform and never have to wish I had Mathematica or Maple again.
last line: tell (x:y:xs) = "Long list: " ++ show x ++ " y " ++ show y ++ " whatever " ++ show xs 
I'll split this into two parts: printing the prefix and printing the list. import Data.List printList xs = printPrefix xs ++ printElems xs printPrefix [] = "Empty" printPrefix [_] = "One element: " printPrefix [_, _] = "Two elements: " printPrefix _ = "Long list: " printElems xs = intercalate " " (map show xs)
I won't comment on the first (I think it's a bad idea), but for the second, it's well worth checking out "pretty printing libraries", which do just that, but in a much more flexible and composable way.
You would use this function if you don't care about the order they happen.
For isTruthy, maybe what you really want is mzero (from MonadPlus) or mempty (from Monoid). For the second, the typeclass you've just defined seems fine, instances shouldn't be very hard to write (look at Data.Text.Encoding). edit: oops, I meant mempty from Monoid, not mzero.
Next time you might want to choose a more descriptive title. That might result in more replies.
Yes, but "hoogle" is much too long. I just use "h". =)
That would be nice as well, but it will only work for value known at compile time.
Added in a second menu to both remove some mystery and allow navigation on touch devices. I hope this is an improvement? 
I think you mean `mempty` from `Monoid`.
Hmm... there aren't instances for a number of useful things, though (like Data.Text ... or even Bool). Will think about it.
Why would people put -Werror in the cabal file?
Can you point me at one of them?
I've been trying to use GHCI as a replacement for Mathematica, but every time I use fractions and exponentiation together in short queries it complains about the type. Sometimes the interpreter seems to autocast integers to doubles, but sometimes it doesn't. How do you deal with this? 
My main motivation for `isTruthy` is for DSLs and code-generators that use weak-type-alike boolean semantics in their conditionals. So, instances for numerics (0 as false) lists and Text and other Monoids (mempty as false) Boolean (False as false) Maybe (Nothing as false) and, ideally, whatever other stuff users of the DSL/generator would find useful.
How is this different from [Try Haskell](http://http://tryhaskell.org)?
Generally you're weakening the type system by doing that and making it easier to accidentally slip something into some boolean clause somewhere you didn't intend. In practice, how dangerous this is seem to very much depend on how careful the language is to define "truthiness". I've never really had much trouble in Python, because despite having truthiness defined for a lot of types, the definitions tend to be very clean, and despite programming with it quite a bit this virtually never bites me. (Not zero, but less than once a year. Generally what bites is more the usual "anything can be None".) Perl and PHP, by contrast, have very complicated rules for what is and is not truthy and it's a lot easier to slip errors into your code, and despite programming in Perl for over a decade, this still bites with some frequency. For instance, in perl, the string "0e0" is numerically 0 (`"0e0" == 0`) but true. This is actually (ab)used by the common database library DBI to return that a query completed successfully ("true"), but had zero rows. Ack! Haskell's philosophy is of course that if you have a thing with two cases, make a data structure with precisely two cases (+ bottom, but we speak not of that, for in that case ye have already lost) and don't make any assumptions about how any other data structures map into that. Thus, by the type system, you _must_ explicitly state exactly how to map whatever else you are working with into the Bool type and _only_ the Bool type (+ bottom, ack). Given how hard it seems to be for languages to get truthiness right I'm definitely coming around to that camp as my default position (with Python grandfathered in). (Another nasty example is Erlang. Erlang has "atoms", which you may also know as "symbols". The true and false are simply the atoms `true` and `false`, so, despite rather rigidly requiring those to appear where booleans are required, there is also no type system support for ensuring that happens at all. It's sort of the worst of both worlds, neither strongly typed nor accepting truthiness.) My initial negative reaction to the typeclass was tempered when I realized that Haskell won't honor it, and its own `if` clauses will still need true Bools. What you do in your own DSL is your own business. If you are going to go this route, copy Python, not PHP or Perl.
Maybe is a bit of a dangerous one. If `Nothing` is false, what about `Just 0`, or `Just` any other falsy value? (I do say "a bit" though, because there are answers to this question. Just be sure to pick one deliberately and with thought, not accidentally.)
Do I need to take LSD to share your sentiment?
Now if only Kmett could visualize all the morphism (in 3-d), or we could have a wonderful visualization of the Tardis monad.... or the Arrows, ArrowTransforms, Kleisli... Let's do the TypeClassopedia! Or maybe TardisT (ListT (MaybeT IO) or something?
Consider using Monoid instead of default, where mempty is the False value. This actually gives rise to two Monoids for Bool and also requires newtypes, but it is just food for thought and will perhaps give you ideas for how to implement your own class by hand.
By that I presume you mean a complete code sample? I can certainly do that and thank you for checking it out. 
True. The hope is that when people make changes to the bounds, it will not be a regression.
I actually like that approach much better. Then you don't have to worry about a mismatch between the effects and the order of your function's arguments. In Applicative style, the solution is ugly and requires a lambda to rearrange arguments, whereas this proposed style is very syntactically sweet. I agree with the author that this syntax matches the nature of the monoidal pattern better.
Simplest example is lists, since they have a concrete notion of shape (their length). length . fmap f = length On the other hand, bind for lists does not necessarily preserve length: length . (&gt;&gt;= f) /= length
Perhaps this will help you visualize the Tardis monad: http://www.youtube.com/watch?v=vY_Ry8J_jdw
I'll update the README, try to explain the project better.
It doesn't quite, if you use a weird Ord instance. There is a package, [set-monad](http://hackage.haskell.org/package/set-monad), which provides some extra bookkeeping to obey the laws, but it doesn't quite behave like Set due to this extra bookkeeping. See the mailing list archives for an old discussion about it: http://www.haskell.org/pipermail/haskell/2012-June/023384.html
&gt; integer literals are polymorphic, so their type is chosen based on the context in which they are used. Isn't that super weird? It picks the tightest fit. So when I declare a function in ghci, I can't use it for general purposes anymore. I have to inline it myself, or fix the types. Both very ugly when you're entering code in the terminal. I understand this behavior for parsing .hs files. But I was hoping that the ghci had an option to say that all number literals are doubles, or whatever the loosest fit might be. That would make Haskell a lot more useful as a quick math scripting language.
Currently, no, but I if there was a low cost way to set it up, I'm sure shapr would take patches. ;)
Well, I meant to a first approximation. :) To be a true implementation, you'd have to disallow composing certain Unix pipes based on that type meta-information.
The pipeline could always just crash :)
Actually, I used to think that, but now I think that mzero is inseparable from mplus, for the same reason mempty is inseparable from mappend. The reason is that without the monoid laws, the notion of zero is poorly defined and arbitrary. However, framing mzero as being the identity of some mplus, whatever it is, gives good semantics for what a default is. Also, almost everything that has a mzero has a corresponding mplus, and I would argue that the exceptions (like IO) don't really have a semantically sound mzero.
The poor-man's type system! Fair enough. :)
Oh, true point. For example, for numbers, you can have 0 and +, or you can have 1 and *. Using separate typeclasses would obscure the difference between 0 and 1 as "zero"; in fact you could newtype any number to be the "zero" number and come up with some weird reason to justify it.
The disadvantage is that there are two separate monoids for `Bool`s. There is: mempty = False, mappend = (||) mempty = True, mappend = (&amp;&amp;) Most people expect `False`-ness to only produce `False` for `Bool`s. This is why the concept of truthiness is fundamentally flawed.
both of those can easily be made monoids
This was a great illustration of Monads, but I wish `join` was mentioned. Some of the explanations about 'unboxing' and 'reboxing' could have been more accurate if stated in terms of `join`.
Is `filter` a monad? Because: length . (filter f) /= length I've got the feeling that it would have to be a generalized form to be a monad, and that I'm calling the wrong thing a monad in the first place. (Is the list the monad?)
That definitely would have required US citizenship then.
Since they intend to release all related code as open source, it's probably very basic (ie. unclassified) research, and unlikely to require US citizenship.
If you desugar the type `[a]` to `[] a`, then `[]` is the monad. A monad is defined by a functor, not by a specific function. To answer your question about `filter`, you could implement filter using the list monad, but you wouldn't say that `filter` is a monad. filter p xs = xs &gt;&gt;= \x -&gt; if p x then [x] else [] Here I've used `(&gt;&gt;=)` from the list monad (which is equivalent to `concatMap`) to implement `filter`.
Damn, this is such a perfect job for me, it ticks all the boxes, Haskell work, embedded programming, high assurance programming, RTOS' and I assume scheduling. Personally I'd love it if they were using a DSL that produced Ada (or even better, SPARK Ada), because the infrastructure out there for verifying Ada program's is very advanced and has been specifically designed for exactly this kind of work from the outset, unlike C/C++. Shame there's no way my other half would let me move to the US for three months, my three months at Tsuru in Tokyo were hard enough. Good luck to whoever gets the position, I'd love to hear more about the project as it goes on.
They did some work with NASA a while ago on some monitoring systems for control systems for some autonomous aircraft I believe, I'm pretty sure it all civilian work, not military.
Thanks! I think I'm starting to understand it.
The ones that pop into my mind are [prettyclass](http://hackage.haskell.org/package/prettyclass) and [wl-pprint](http://hackage.haskell.org/packages/archive/wl-pprint/1.1/doc/html/Text-PrettyPrint-Leijen.html#g:10).
If you have an `f a -&gt; f b -&gt; f (a, b)` function, you'd still have to re-arrange the tuple or use a lambda to apply a function to the composed tuple?
Correct, but the proposed syntax sweetens the lambda part away using the lets.
But one can hold up any arbitrary example and play the same game. Can Haskell detect range violations at compile time? (let ((x -1)) (declare (type (integer 0 10) x)) x) ; Constant -1 conflicts with its asserted type (MOD 11). ; See also: ; The SBCL Manual, Node "Handling of Types" debugger invoked on a SIMPLE-TYPE-ERROR in thread Raising up an ad hoc example as the arbiter of "seriousness" is bit like saying that basketball sucks because you can't steal second base.
Well, whether it's what you want or not is subject to debate, IMO. In a strict language (like F# I assume?) the standard implementation of n-tuples for n&gt;2 is to use nested pairs. Whereas, since Haskell has non-strict tuples, we need to distinguish between `(x,y,z)` vs `(x,(y,z))` vs `((x,y),z)` since they have different partially defined values. But, if you're ignoring partially defined values, then there isn't any difference. The "alternative" definition is, in fact, the category theoretical definition. And in category theory you need to prove that `(x,(y,z))` and `((x,y),z)` (i.e., `X⊗(Y⊗Z) ≅ (X⊗Y)⊗Z`) are naturally isomorphic. Ditto for `(x,())` and `x` (i.e., `X⊗1 ≅ X`) and for `((),y)` and `y` (i.e., `1⊗Y ≅ Y`). Together, these requirements essentially mean that the "alternative" definition is only correct for strict tuples (i.e., smash products in domain theory).
nice article, thank you Tekmo. [french translation available here](http://blog.demotera.com/published/2012-08-22-Programmation-Fonctionnelle-et-Theorie-des-Categories.html)
Well, yes you can, though you can't use the standard Integer type and support isn't built in. Currently you'd have to invent your own type-level numbers (or I guess pull a package off of Hackage), but it's a bit ugly, which is why nobody does it. Soon it'll look prettier with type-level numeric literals coming in GHC 7.6.1 (there's a release candidate out for that). The machinery is already present for doing a fairly reasonable amount of type level computation. Here's an example, just using unary-encoded nats at the type level: {-# LANGUAGE DataKinds, KindSignatures, GADTs #-} data Nat = Z | S Nat -- lists of length bounded by a natural data Vec :: Nat -&gt; * -&gt; * where Nil :: Vec n a Cons :: a -&gt; Vec n a -&gt; Vec (S n) a xs :: Vec (S (S (S Z))) Integer xs = Cons 1 (Cons 2 (Cons 3 Nil)) -- This wouldn't typecheck: -- ys :: Vec (S Z) Integer -- ys = Cons 1 (Cons 2 Nil) Type families can be used to do type-level computation, though you can't (yet?) just lift functions to the type level like you can with simple data types. So this sort of thing can be done, but it's not something you're likely to do as an everyday matter. I'm a little suspicious of this range checking though. How much computation is the compiler willing to do to ensure that values stay in range? Does it internally do arithmetic on ranges to statically prove that the constraints hold when values pass through a bunch of computations, or can it only be used to ensure that literal constants lie in a given range? In any case, the reason that I hold up map as a litmus test for type checkers is that it tests a few simple properties of a type system which result in a lot of expressiveness: 1) Can the type system express constraints on polymorphic functions? After all, writing things over and over again at different types is no fun, and if the type checker can't reason about polymorphic values, then we're likely to either not check types (if possible) or view the type checker as antagonistic to getting programs written. This program is a particularly good test of the type checker's ability to reason about polymorphism, because there are two type variables involved which must remain separate. 2) Does the type checker keep track of the element types of data structures such as lists? A list of elements whose type we don't know is, from a strongly typed standpoint, essentially a natural number (its length), because there's absolutely nothing we can safely do with a value whose type we don't know. So it's important to keep track of the types of elements of lists, so that we can later pull them out and safely do computations with them. 3) Can the type checker reason about higher order functions? After all, these are the lifeblood of functional programming. If I can't factor functional bits of a repetitious program out into a parameter in order to save writing the same thing over and over, well, I'd rather use another language. If I can do this, but the type checker's not going to help me any longer when I do it, then all of a sudden I have to sacrifice potential correctness checking for convenience, which is awkward. That's why I think map is a good test for type checkers in general: all of those things (polymorphism, container data structures, higher order functions) show up in any setting, with essentially any sort of program I might want to write or maintain. Bounds checking is a reasonable thing for many sorts of programs to want to have, but there are some difficulties with it. It ranges from hard to uncomputable to statically reason about bounds in general (after all, the Goldbach conjecture is a special case), and so every system I'm aware of either fails to check most cases, to the extent that it's rarely helpful, or requires programmer-supplied proofs of bounds (which can be in many cases too much work on the programmer's part for the system to be used in practice), with very little in-between the two extremes. On the other hand, the machinery required to type check map to the extent that I mentioned is all very much statically and efficiently computable stuff which can happen without any need for the programmer to step in and write extra proofs. Thus, if present, it gets used constantly.
Pardon the tangent, but would you be able to direct me to some materials on Ada? I know almost nothing about it.
I think this is actually a good observation I hadn't quite considered myself. And also, in my view, a strong argument of why we should be able to change version constraints after the fact *without* also changing version numbers.
Check out r/Ada and the AdaCore blog. The wikibook also has some great stuff in it, but it's kinda patchy (like most wikibooks). Things to look for are precise control of resources, concurrency built in from the outset (since the mid 80's, and it's tools are still far more advanced than what both C and C++ have today). It also has mechanisms to disallow language features to make things like verification possible (see the ravenscar profile). 
I don't think there is a cmm debugger, but there is a lot of documentation on [trac](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution) about how Haskell is executed. I'd also recommend you read SPJ's paper about the spineless tagless G-machine. Edit: You may also be interested in some command line options [for debugging the compiler](http://www.haskell.org/ghc/docs/7.2.1/html/users_guide/flag-reference.html) which let you dump all intermediate representations of a Haskell program to the standard output or a file. 
Off topic, but I'm glad I have the readability function on my phone. This page is barely readable.
I did similiar thing with C++ templates, but in Haskell it's waaaaay nicer.
&gt; Kind polymorphism would be neat, but I wonder how much use it would have. Heh, I think Chris spoke a bit too soon ;)
White backgrounds are super bright, which always hurts my eyes. This is why the orange in my background used to be darker, but that admittedly had problems. So I worked quite a bit to lighten it without putting it in so-bright-it-hurts like white or straight pumpkin orange. Some people elsewhere have suggested a different foreground colour might work, but I haven't found anything other than black and near-black that even show up well on orange.
Ok, I've changed the background colour again. Let me know if that's working better for you :)
Hey, Zac from DuckDuckGo here. We do have a hayoo goodie but feel free to hack up a new one :) Trigger: "hayoo" https://duckduckgo.com/?q=hayoo+fmap
OT, but love the typography. Very readable.
Typo: `End`**)** (Superfluous paren-closer after "End" in the type operators first example).
Yes! Just make the PVP a bit subtler: use the first versioning digit to mean "your code will probably break" or the second to mean "your code will probably not break". Then set first-digit constraints. Possibly this could be augmented by adding into the documentation indication of "these functions are going to stick around" and "these functions are subject to change". That way, using only the "stable" parts of the API would guarantee you against all but first-digit version bumps. (note that this approach requires zero new technology!)
Not sure what you're saying. I'm suggesting you make it act on text everywhere.
Like, my return value is a Blaze Builder. I can construct a Blaze Builder from Text, but it just unpacks it anyway. That said, I looked at the package again and it looks like it's probably better supported and such anyway, so that may be an advantage by itself.
For the record, `data-lens` defines some lenses ([mapLens](http://hackage.haskell.org/packages/archive/data-lens/2.10.2/doc/html/Data-Lens-Common.html#v:mapLens)) for containers, so in fact does expose them in its API.
Ah you're right about blaze-builder's `fromText`. Weird. But why not use the `Text` builder instead? I'm not sure why people use bytestrings for things that are naturally text. I guess it is faster if you're rendering to exactly the one encoding you decided to default to?
Hmm... I was using the Blaze Builder because I'm using this with WAI, but I should look into the `Text` Builder. Thanks :)
Aha! Thanks. I was wondering where that dependency came from
I think it's strange how this debate has turned into "upper bounds or no upper bounds". I think it would be far more sensible just to have more *sensible* – looser – upper bounds. The PVP provides the right information, but gives the wrong advice – often, particularly with packages like bytestring, it's *obvious* that you're using features like `readFile` or `uncons` or `foldr` that, if changed, would demand a major version bump. So you don't need to specify an x.y version, just a single-digit version, and those change much less often. Please, let's not overcomplicate the technology here. Non-invasive, simple changes are the way forward, IMO.
That. Is. Awesome.
By the way, you don't have to use the `newtype` wrappers if you don't want to. The newtype wrappers in `Data.Monoid` are there so that you can choose which instance to use for the unwrapped type. So if you prefer the `Any` and `Sum` instances, then just define those for the unwrapped types like so: instance Monoid Bool where mempty = False mappend = (||) instance Monoid Int where mempty = 0 mappend = (+) However, you can't do: instance (Num a) =&gt; Monoid a where ... ... otherwise you get overlapping instances. It sucks, I know. You'd have to define one instance for each numeric type you want to make a monoid. Also, I only recommend this short-cut for your internal code. Don't release a library that has the above instances otherwise people will complain.
The problem is that it is red, which doesn't make a good background color. If you change it to a more neutral color like grey (and still lighten it a bit more) it would be easier to read.
My main problem is not so much with the operators, but the fact that they aren't clearly associated with the operations. Arithmetic operators like *, /, etc look like the operators they represent. The C++ structure dereference operator (-&gt;), for example, looks like what it does in my head. It's taking a pointer and following it to some member so an arrow fits. &lt;$&gt; and &lt;\*&gt; are just gibberish to me. Maybe it's clear to other people, and -&gt; doesn't make sens to them, but it's not clear to me which operations &lt;$&gt; should do and which &lt;*&gt; should do just by looking at the symbols.
Looks great! Nice to have two competing JavaScript generators competing :) nitpick: one place in the code uses == Play, and then right next to it uses case analysis on the state. Would be nicer, imo, to do one case analysis for both. Another: why not return true in the display function itself rather than lift const from outside? Is layout going to be added to elm? 
 fourArgFn `fmap` arg1 `ap` arg2 `ap` arg3 `ap` arg4 versus fourArgFn &lt;$&gt; arg1 &lt;*&gt; arg2 &lt;*&gt; arg3 &lt;*&gt; arg4 To say the former is more readable than the latter is just silly. When you read the &lt;$&gt; and &lt;*&gt; you may not know what they mean or do - but you know they're operators. Scanning multiple argument functions for the more meaningful bits of data (`arg1` to `arg4`) is made vastly easier with the operators. Reading the former gives me a headache because it's just a barrage of text. I can't scan and instantly identify which things are identifiers and which things are operator-like things.
Since these operators are so common, it's a small investment with a large payoff. Some Haskell libs define dozens of operators for rarely used stuff, and that is distasteful. 
Thank you! :D Is Fay the other one you are talking about? There are a number of hs-&gt;js projects, so I am curious which is the perceived leader :) Good point on the use of `(state == Play)`. I am not really sure what I was going for, especially when I did it the nicer way in the very next definition. `display` needs to return an element so that something actually shows up on screen when we set `main = view`. The value exported to JavaScript could be anything (the only info that matters is *when* the event occurs, not what it holds), so at some point I decided that it should export `True`. Layout: As in better ways to position stuff? Yes. I think the next release will focus on nicer graphics stuff like this. I am planning to improve the `canvas` API, so I'll look into doing this at the same time. Are there any libraries you know that do a particularly good job with layout?
Layout in syntax, as in indent based rather than braces and semicolons. Understood that you want true to be returned, I wonder why you add the true result from the outside rather than in the inner definition. 
I am very interested in that "SKI combinator calculus" thing. Does anyone know some good resources to start with?
The [unlambda](http://www.madore.org/~david/programs/unlambda/#howto_loop) language...
Ah, yes. I would really like to add that! Not sure when it will happen though. Sorry, I think I see now. The `(foreign export ...)` code will only allow a variable name in that position. I guess this restriction could be relaxed, but it opens the door to making things kind of messy. I am not sure if I'd want to do that. Hopefully that's what you were asking about.
I completely disagree. It's not silly at all to say it's more readable. My mind associates words with concepts much better, and I think the first example is much more readable and more easily understood to someone who doesn't use the cryptic symbols every day. I can easily scan and identify due to backticks. But, hey, if you like it than use it. We have different opinions and that's ok with me.
&gt; The easy answer here is that I'm not a language designer, so I don't have to make that decision :) There's the rub. &lt;$&gt;, &lt;\*&gt;, and &gt;&gt;= are not part of the Haskell language (well, &gt;&gt;= is *kinda*, but not exactly). All those are symbolic notation chosen at one point or another by library writers. *They* thought it would be useful, not the language committee.
It is a good point to make. Perhaps if it had been part of the core language there would have been different decisions that I agreed with more. But I don't think anyone would argue that it's become effectively part of the language in that it's distributed with the platform and talked about in literature designed to teach the language.
Wikipedia. SKI is a simple calculus tho: Sxyz = xz(yz) Kxy = x Ix = x There are other conventionalized combinator definitions, but all of them are definable in terms of SKI. In fact, even I is definable in terms of SKI: SKKx = Kx(Kx) = x therefore `I = SKK` There's a book by Raymond Smulyan called [*To Mock a Mocking Bird*](http://en.wikipedia.org/wiki/To_Mock_a_Mockingbird) that's about this sort of thing. It's the origin of many of the names for the combinators (`S` is the starling, `K` is the kestrel, `B` is the bluebird, etc.)
Elsewhere you said that "My main problem is not so much with the operators, but the fact that they aren't clearly associated with the operations. Arithmetic operators like *, /, etc look like the operators they represent." The thing is, is that these are just notation. There's nothing about * or + that really look at all like what they do, but we've all seen them enough that they're intuitive by now - they can't mean much else. The human mind (or at least programmer's minds ;) ) is as well adapted at consuming symbols as it is words. I agree that it's nice to be able to phoneticalize an operation in order to ask a question about it, but for notation that you use a lot, the conciseness far outweighs that aspect. So, don't get the impression that all Haskell libraries declare a profusion of operators - most do not. Ideally, they'd only be provided / used as notation for operations (or a class of related operators) which are anticipated to be used frequently. As mightybyte points out, for a given function in an API, you'll have to find out what it means, whether or not it's an identifier or an operator.
Instead of convincing yourself that there must be a better solution, sit down and spend a few hours becoming comfortable with the symbols. They are used very often in Haskell. Every language feels foreign until you have used it a bit, there is no getting around it. &lt;$&gt; &gt;&gt;= are just as natural to Haskell programmer as +,-,*,/ are to a grade school student. 
You are majorly missing the point. I'm not at all claiming they are hard to use. I'm saying there is a better way to do things. I am comfortable with the symbols, but they could be better.
I guess I would like to illustrate my point with an example. People do associate concepts with words, it's how we communicate. When you're talking about code with fellow programmers, do you say things like: * "I think here you could simplify by using a function composition" * "Why did you use the if else structure instead of a ternary operator?" **or** * "I think here you could simplify by using a dot" * "Why did you use the if else instead of a question mark colon?" We think about things in words, and associate symbols with the word concepts. If you truly think about things in straight up symbols, that's remarkable.
Don't forget, words themselves are composed of symbols. &lt;$&gt; and &lt;\*&gt; could also be called words. They are composed of graphemes just like the words you're used to, why eliminate their use? The only major difference between them I suppose would be that some of these symbols have sounds associated with them, and likewise with their compositions. But programming languages are written languages, they aren't really designed with diction in mind. You can't pronounce &lt;*&gt; in a standard way, but don't let that stop you from making a sound for it.
You indicated that the preference for symbols over names is a mistake of such proportion that you are surprised that the language designers could possibly have made it. That does not sound like a position that is open to differences of opinion. If that's not what you meant, then fine. But you shouldn't be surprised that you have attracted such a response.
I'm not impressed that you wrote a hello-world SKI program. I **am** impressed that you got it to typecheck.
Finally at a PC instead of a tablet, so I can be more specific :-) display ... = layers [...] view = lift2 display Win.dimensions gameState done = lift (\_ -&gt; castBoolToJSBool True) view Why not: display ... = layers [...] *&gt; pure (castBoolToJSBool True) done = lift2 display Win.dimensions gameState 
This is getting more and more interesting, but only from a sociological point of view which unfortunately is off-topic here.
I find white backgrounds too bright sometimes as well. I use a green background in vim (#ddeedd) and like it quite a bit.
The reason there's such a big discussion, is because Haskellers (at least the vocal ones) tend to really *care* about language design, and what others think of it. Usually these aren't very heated discussions, but when coming from another community I can see how it'd look like a bikeshedding flamewar. I think that this is an interesting discussion, and is good stuff to think about / be challenged about. I haven't thought hard about this issue in a long while. Your reasoning makes sense, but I think it makes more sense in languages other than Haskell. With Haskell, we tend to give names to more things - our code is more partitioned. We have the generic concept of "sequencing actions" (&gt;&gt;), and can use that all over the place. Other languages just have ";". Since we give more names to things, and some of them occur very frequently, then, we think that they really deserve a shorthand. I will say, though, Haskell's operator precedence system is one of the uglier, less thought out parts of the language.
What is the better way to do things?
Oh they certainly did. I meant the origin of the bird names, not the symbols. :)
Hmm, I may still be missing it. The intermediate value `view` is needed for the very last line of the program: main = view Without `view`, there is nothing to show to the user. The `display` definition is not actually putting anything on screen. Otherwise your approach makes sense to me (although Elm does not have an Applicative library yet). Hopefully I have not mixed things up a third time.
Probably. But why does print &lt;$&gt; readInt &lt;$&gt; getLine compile, but not (seemingly) execute print?
&gt; I'd be concerned about whether this sort of stuff confuse might confuse an RDBMS's query planner. Could we get bad plans out of this equivalent query? See the section [about query planning here](http://chrisdone.com/posts/2011-11-06-haskelldb-tutorial.html#speed-and-optimisation) for simple queries. The answer is, it's fine for PostgreSQL. I've done a lot of joining among half a dozen tables in HaskellDB. With MySQL, it will be very expensive.
Switching to `OverloadedStrings` and using `fromText` had no noticible effect at all on performance. Using `fromLazyText` actually made it slower. I'll keep playing with it :)
You can use join to make it work: import Control.Monad main = join $ print &lt;$&gt; readInt &lt;$&gt; getLine Magic! Not recommended, though.
The text package itself claims it's for OverloadedStrings. Not sure.
A good reason to give your main function a type signature of `IO ()`.
&gt; is a particularly ugly spot in Haskell's syntax Going out on a tangent here, but technically the symbols (such as `&lt;$&gt;` and `&lt;*&gt;`) are not part of Haskell's "syntax"; rather, they are part of its libraries. &gt; I don't need to have a solution in order to identify a problem. I was merely suggesting that if you are trying to make the argument that Haskell's use of symbols is a Bad Thing, then the burden of proof is on you. &gt; It wasn't my intention to convince you. Then what was your intention? To voice your opinion into the ether for no discernible reason? Generally people voice their opinions in order to guide other people to take the same opinion. I apologize if I and others sound hostile in our responses. I assure you that I mean no ill will (and the others almost certainly likewise).
In the same vein, `I` is `ask` for `(-&gt;) r`, the untyped reader monad.
Sorry, I just skimmed the article pictures. Allow me to rephrase as: "the author should develop a syntax for building SQL ASTs" ;) This should be fairly straightforward. I'd inspire from LISP and essentially type node_ctor arg arg arg. Another source of inspiration could be HTML combinators which build an HTML AST. Replace the syntax / semantics, i.e. the combinator names and the AST -&gt; String evaluator and you're done.
Extremely useful. Where were you last week? :-)
Awesome! How easy is it to deal with different delimiters, like tab-delimited text files?
I haven't used postgresql nearly at all, but I would assume that it's part of the schema. If so o it should be part of the schema generation, right?
There are versions of `encode` and `decode`, called `encodeWith` and `decodeWith`, that take an options record that lets you specify the delimiter. The options records are quite barebones for now but everything is in place for adding new options as we need them.
&lt;$&gt; is an operator that can be used on any Functor. Functor does not give you any way to combine effects. If x :: m a for a Monad m then x and f &lt;$&gt; x have the same m-effects. (If you don't like thinking about arbitrary Monads in terms of effects, just set m = IO.) So right away you can see that print &lt;$&gt; readInt &lt;$&gt; getLine cannot do what you want. In this case you want to produce an action which combines the effects of getLine and print. In particular the action of printing depends on the result of the action getLine. To do this you need to use some Monad function like join or &gt;&gt;= (or do notation which translates to &gt;&gt;=). Applicative is not enough either, it only lets you sequence actions and combine their results, but not select an action to execute based on the result of a previous action.
Ah, that explains it. You want it with and without the `Bool` result. Sorry :)
The `Homepage` field in the .cabal file contains the wrong link.
AFAIK it's valid monoid and its not in standard library. You may also want to read tis blog post: http://byorgey.wordpress.com/2011/04/18/monoids-for-maybe/
I'm confused as to how this is "in the style of aeson"? Especially since aeson is essentially the same core structure as the galois "json" library (but, of course, with some representation improvements and some very significant speed/efficiency improvements). The tuple-based decoding does remind my of mysql-simple. But in fact the basic tuple-based stuff there was done prior in Takusen (although mixed up with all the enumeratee stuff there as well). Sorry, I'm just sort of geeky on issues of api design and lineage :-)
Good speech, but Carmack here does give only problems and no solutions. (though I haven't looked at the 3 hour-long keynote)
Very nice. I've got two questions: * Are there plans for streaming parsing? Especially for CSVs with a header, which cannot just be broken up and parsed independently. (And most of the CSVs I've seen have headers.) * For named records, you first put them into a ByteString-Map, and FromNamedRecord then allows pulling them out. But most of the time the interesting named records are known ahead of time, so why not do the lookup on the header, and use the position index from then on? Just a suggestion, I don't know how much speed advantage this would give. :)
What version of GHC? For me with 7.4.1: [1 of 1] Compiling Main ( io.hs, io.o ) io.hs:7:8: Couldn't match expected type `()' with actual type `IO ()' Expected type: a0 -&gt; () Actual type: a0 -&gt; IO () In the first argument of `(&lt;$&gt;)', namely `print' In the first argument of `(&lt;$&gt;)', namely `print &lt;$&gt; readInt' 
And this then combines with the fact that [monoidal functors preserve monoids](http://ncatlab.org/nlab/show/monoidal+functor#properties_17) to fully explain OP’s monoid instance! I came to Haskell via category theory, and I love it when things that are familiar mathematically appear here in a whole new light…
It is. I didn't check mconcat though
How do `*` and `/` and `+` look like the operations they represent? What about `+` says "addition" to you? /curious
&gt; I see all the other aspects of Haskell as bucking the bad traditions of previous programming languages. Just to be clear, maybe you know this, but Haskell is pretty old. 1.0 was 1990 IIRC.
&gt; Are there plans for streaming parsing? Especially for CSVs with a header, which cannot just be broken up and parsed independently. (And most of the CSVs I've seen have headers.) This is pretty important, I think. You can't always just load the whole thing into memory. I think it would also make sense to have failure (optionally?) be per-row rather than all or nothing as well. Sometimes you have to deal with CSVs that have malformed or excess rows at the top or bottom. *edit*: This [Stackoverflow question](http://stackoverflow.com/questions/6360963/attoparsec-iteratee) about attoparsec-iteratee and this [pipes-attoparsec-streaming](http://hackage.haskell.org/package/pipes-attoparsec-streaming-0.1.0.0) might be useful. Looking at the source of the latter, it seems pretty simple and could likely be implemented without a pipes dependency.
Attoparsec already handles streaming input. pipes-attoparsec provides a Pipe interface to that streaming behavior.