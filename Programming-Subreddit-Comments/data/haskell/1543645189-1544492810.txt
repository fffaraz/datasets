Your 'nasty alternative' is not a nasty alternative. It's a perfectly sane way of doing something that you *almost never ever* need to do in an immutable language. If you want, you can tidy it up by writing it as: `(a:b:c:d:e:f:g:h:_) = [42,42..]`
Whether or not it's optimized away, it's a constant (near zero) cost, as long as the terms are declared as `Int`. (If they're `Num a =&gt; a`, they will each be recomputed every time)
But why do you want to avoid the list though? What does introducing a specialized rare use syntax gain when an *almost identical* piece of syntax gives you the same thing with no additional performance cost?
Probably a stupid question - what do the numbers mean? In the "Private leaderboard" for glguy, it gives me "13) 141 *" what is the 141? I thought the leader should be close to 200 but that doesn't seem to be the case and since I'm near 13, I thought I'd be closer to 200 - 13 * 2 ~ 174, but I'm at 141.
The spaces are also not actually necessary.
Everything he says from 8:15 to 9:00 is just so cringe worthy, that I think I'm going to stop watching. The only way `foo :: x -&gt; y` to `foo :: Maybe x -&gt; y` is an 'easing of requirements' is if you have inherently nullable pointers and just throwing exceptions on null, which he has already said are bad by this point. Otherwise `Maybe x` is strictly more information than `x` and so is an *expansion* of requirements, as more data is being communicated to `foo` and via the pigeonhole principle it cannot function correctly as it was without increased generality.
I think it is an advantage that he has a financial stake in clojure. So, he is getting rewarded and responsible for the long term existence of Clojure. It is extremely narrow minded to think that someone trying to cover up something because he has financial stake. It does not work in open source because the audience get to make an informed decision. They know what they are getting into, if the solution did not resonate with them, they will just walk away. For example DHH is doing whatever he is doing with RoR is just because he has financial interest in it is totally naive argument. On the contrary many open source projects are shutdown because we expect them to work for free and deliver what we want. It‚Äôs okay to have people sold on certain ideas and fixated on it and make decisions based on it apart from solely monetary reasons. May the best ideas win and improve our lives.
Why do you think it's non composable? Could you provide a small example? I tried hard to design it in a manner which allows normal composition and abstraction. The functions can get some funky constraints, but besides that it should work just fine.
The score goes that each time you answer a part, you get awarded between 0 to 100 points based on how you rank (100 for first, 99 for second, etc.) So if you have 141, it might mean that scored 30th on the first part and 29th on the second part (getting 70 + 71 points total). You might get bumped down in points because scores are all historically re-computed when a new player joins, taking into account the new player's completion times. You can also go down in rank once someone completes a new challenge and get new points on a new challenge. Yeah, it's sort of unfair based on time zone. I don't know if there is any way for them to really deal with it this sort of time zone problem :/
His financial stake in Clojure is a consequence of his strong views (based on experience), not the other way around, so it's wrong to suggest he is being disingenuous. I say this as one who admires Clojure and Haskell in equal measure.
I think a little hammock time could've eliminated that ugly testing by adding an abstraction at your entries and exits. For example, individual impls thread an opaque handle from entry to exits. Entries declare their reads as additional arguments and exits declare their writes as additional returns. The abstraction selects the reads, merges the writes. An orchestration layer validates all components wire up correctly.
That's a very good idea and a nice job! 
&gt; These are contracts, OK, yet in practice you need to write them, maintain them, etc, they can break just like type declarations, and they are not statically checkable It may be so in Clojure (I have never used it), but it does not have to be so always. There are for example [Object types](https://caml.inria.fr/pub/docs/manual-ocaml/types.html#sec111) in ocaml, where an expression which consimes an object with method `foo` with type `bar` has type `&lt; foo : a; ... &gt;` which can be satisfied with any object whcih provides the method. It is statically checked, and it may be written explicitely or inferred.
Two packages that differ by a dash? Oh woe. Hackage should probably forbid that. It's almost as bad as two packages that differ by capitalisation.
The problem could be managed much more cleanly and reliably by adding a language feature. Maybe something like {-# whenstuck (Rep x) 'ShowType x :&lt;&gt;: 'Text " is not an instance of " :&lt;&gt;: 'ShowType Generic #-}
I'd describe this as an awesome hack in the best possible sense. Constrained type families still feel like a much cleaner solution but this is a big improvement on the status quo. I've noticed that using some type family techniques (defunctionalization, trees that grow) leads to amazingly awful compile times. Do you know whether that's the case for this? Also, what happens if multiple TypeError's are in the same ast? Do they get concatenated or does ghc only show the first?
I found that parsing combinators libraries where tremendously useful most days in advent of code. I used trifecta last year, but I'll take a look at megaparsec this year. &amp;#x200B; Any other libraries that you found really nice for this?
There is zalando. They bundle their Haskell job posts with other languages, e.g. Pascal :D (Am unaffiliated, never worked there!!)
Clojure's USP is immutable data structures
Thanks for the explanation! Makes sense now.
My bad :/ I was actually using the Prelude package. In that package the unexplained colouring happens. It doesn't happen with spacemacs.
i totally agree with you; i am annoyed by the first [popular] version, because it contains a mismatch between the code structure and the syntax tree; and such mismatches cause painful cognitive dissonance to me which is worse than any potential practical advantage
For me it wasn't Haskell per se, but finding some new FP idea, trying to implement it in another language, and realising that mostly it was better off using them in Haskell itself, rather than bending other languages. &amp;#x200B; Lots of nice ideas work well in Haskell.
For day1 I do not need a parser combinator all I used was map and fmap 
In the latter stages of 2017, I used Vector inside the State Monad quite often for performance reasons.
That all makes sense. I appreciate you taking the time! If you have any more concrete refutations of things he said, I'd gladly hear them!
Scala has structural types that do exactly that, this feature is notoriously underused however
You might like to use `RecordWildCards` and `DuplicateRecordFields` in Haskell to avoid some of the boilerplate aspects of what you're talking about, I think. Lets you name semantically equivalent fields in different types the same, and implicitly "copy" them from one data type to another. 
This is the first time I see this project but I spotted in readme that you use -threaded and -N by default just like stack. This is a bad default, for GHC this makes programs run a lot worse in 99% of cases: if you need threaded RTS and multiple capabilities, you know to enable it. Recently we were hiring interns and nearly all applications had this default: in most cases, just turning this off easily HALVED or more the performance. Please reconsider.
&gt; Inspired by non-empty-containers library, except attempting a more faithful port (with under-the-hood optimizations) of the full containers API. _Please_ benchmarks against vanilla containers! As someone else mentioned, you're not using coercions and assuming the implementation is correct, we still know nothing about the difference in performance. There is a GitHub project benchmarking collections &amp;c, maybe add your library in there too? As a point of reference, we saw real slowdowns when using correct-by-construction NonEmpty list vs [] even though the cost seems like it'd be so little but alas, benchmarks showed otherwise. So please :)
To add to FineSherbert's comment, this called "parametricity". https://www.google.com/amp/s/bartoszmilewski.com/2014/09/22/parametricity-money-for-nothing-and-theorems-for-free/amp/
Awesome! Thanks! Looking forward to reading more.
You'd have a point if he was just talking about Clojure and Spec. But he's not. He is making a case for union types *in strongly typed languages* also. And, well, if you goal is to attempt to school typed FP people about type systems, then you should care *a lot* about type inference, since it is a critical point in the design space of any serious typed FP language. Instead, he rants about sum types in Haskell and Scala, and compares them with union types in Kotlin and Dotty, claiming they are different solutions to the same problem, and that the latter is the "right" one. At some point he literally says "do not get lectured to by people about Maybe and Either, they are not the best answers **in type system worlds**". That part of the talk is an annoyingly shallow and dishonest way to treat about a very complex design space, dismissing the myriad of tradeoffs involved.
cough cough Cabal versus cabal cough cough Though at least one is clearly marked as deprecated in favour of the other. Do cabal/stack warn you if you specify a deprecated dependency?
Do you have any examples and benchmarks that can show this behavior? We're open to all suggestions and improvements! But it's usually better to be sure in the outcome of things you're doing.
I joined MIRI back in August, and have been talking a bit about the fact that i'm working there with folks at ICFP and on my twitch stream here and there. At the moment I'm balancing working on my current research project with outreach to the functional programming community for MIRI. If you want some sense of what the heck "AI Risk" even is, [this talk by Eliezer Yudkowsky is worth watching](https://www.youtube.com/watch?v=YicCAgjsky8).
Yes, please!
The word could also be ‚Äúinformed.‚Äù I‚Äôm persuaded against paying attention to Hickey because of the disingenuousness of the arguments I have paid attention to. 
Congrats! Which project are they referring to here? &gt; building a new language and infrastructure to make it easier for people to write highly complex computer programs with known desirable properties
Congrats Edward! Really glad a bright mind like yourself is working in research and that you've found something you're interested in!
One other thing, if you're game: What is the difference between `isDataConWorkId_maybe` and `isDataConId_maybe` 
I'm working on a programming language project called `coda`. I want a language that is better at type classes than Haskell. If it is going to take me five years to fight each battle for refactoring any class hierarchy in Haskell, it behooves me to give equal time to designing a language in which I do not have to have the fight -- not just because there are no users of the language, but because I hope to fundamentally address the issue that allows users to have to care about how fine grained the class hierarchy is. If as a mathematician, I want 600 fine-grained super-classes above the concept of Field, I don't want to be flogged by users when I realize that it is 601, or when the users realize that they need an abstraction in the middle that the standard library didn't consider. To this end, I'm exploring an extensional type theory (a la NuPRL) on top of a typed (System F'ish) base theory extended with type classes. (Unlike NuPRL, which starts from an untyped lambda calculus.) Why extensional? This gives users access to quotient types and refinement types, but moves the proofs of those side-conditions out of band, so you can actually see your code, and the typed base theory gives you a place to hang proper type class dispatch. It is also necessary for me to prove some side conditons about the coherence of instance resolution. If I look at a piece of code I'm writing I'm doing 3 things interleaved at the same time. 1.) I'm writing down the computational content of my code. 2.) I'm proving to the type checker that it is well typed and 3.) I'm proving that it terminates. In Haskell we skip the third step, have type inference providing us most of the second, and we have typeclass dispatch performing the boring plumbing half of the first. In Agda, I tend to do all 3 steps myself. In a language with better tactic support like Coq or lean, I'm usually doing steps 2 and 3, and relying on tactics to fill in as much of 1 as possible, but what those languages call typeclasses don't really share many features with Haskell's typeclasses. They are some extra rules bolted into a pattern unifier with no real coherence guarantees. When everybody is taking the same side of a bet, it seems to be to be worth putting some effort into exploring the other side. Having an extensional type theory means that the theory doesn't have to be entirely syntax directed. This opens up techniques like those used by Nadia Polikarpova in [`Synquid`](https://bitbucket.org/nadiapolikarpova/synquid) to help produce typing derivations from specifications, but I want a heavier focus on tactics rather than just using Z3 to do the work. At the moment, I'm working on a subgoal, which is currently named `guanxi`, a logic programming framework a la minkanren in Haskell, which I'm hoping to leverage abstract interpretation techniques inside for constraint programming, with an eye toward using it for program synthesis and to use it internally in the implementation of the extensional type checker. The focus there is on moving more of the "natural domain" SMT techniques directly into the kanren-alike rather than trying to rely on external solvers, which don't play well with the funny biased evaluation model that Oleg came up with for Kanren way back in the day which isn't truly depth-first. The end result may well be a little propagator-based SMT engine in Haskell with a funny focus on nominal sets. I've been doing a fair bit of the `guanxi` and nominal sets work live on twitch lately: Sessions 15-19 so far on https://www.twitch.tv/videos/317285051
This is the first year I've heard of this, I'm excited to participate! I can already tell this is going to be a great way to push myself into becoming more familiar with the standard Haskell libraries. I might also do a couple of these problem in Rust while I'm at it. By the way, is there a convention for how long to wait before uploading solutions? I made a public repo for my solutions but I know people use AoC as a competition, I wouldn't want to help anyone cheat.
If the input type is finite or at least enumerable then functions and sets are isomorphic, right? Though you are totally right that in practice their usage is different.
First, "anamorphism" is a fancy name for an unfold, albeit generalized to apply to more types than a list. It takes a "coalgebra" and a "seed" and generates are larger structure (in this case a list). The "coalgebra" is a function that generates one layer and seed(s) for the next layer. Second, "futumorphism" is similar to an anamorphism, but the "coalgebra" it uses can "predict the future" (futu ~ future). Specifically, while it is required to generate one layer, it is allowed to generate more than one layer. It is still required to produce seed(s) for the next steps. For the list case, you can think of `ana` as `unfoldr`, and you can think of `futu` as a combination of `unfoldr` (though for lists of lists) and `concat`. While there's some type "noise" going on, you can think of `ana psCoAlg xs` as generating a segment of the result that all have the same first element (e.g. `[(1,2),(1,3),(1,4)]`); `psCoAlg` is either done, or generating one of those pairs. The `futu pairsCoAlg` is stitching those segments together to the final result. The reason *morphisms can work on structures other than lists is because we can take any (uniformly) recursive type, and consider it the fixed-point of a function. We call that functor a/the "base functor" for a type. You can define a type in terms of it's base functor, or you can "mechanically" derive a base functor from a (uniformly) recursive type -- you simply replace all the recursive locations with a new type parameter. So, for `List a = Nil | Cons a (List a)` a base functor is `ListF a b = NilF | ConsF a b`. Note that the base functor is not recursive. For binary trees with values in the nodes `Tree a = Leaf | Node (Tree a) a (Tree a)` we get `TreeF a b = Leaf | NodeF b a b`. `List a` ~= `ListF a (ListF a (ListF a ...))` (infinitely deep) and similar for our trees. Now, it might be more clear what I was talking about when I mentioned the "layers" of a structure. You might even guess how we use the "seed(s)". `ana` works on any (uniformly) recursive structure, but the coalgebra you give it is tied to a particular base functor, if you give it `seed -&gt; ListF value seed` then is will unfold from a `seed` to a `List value`, if you give it `seed -&gt; TreeF value seed` then it will unfold from a `seed` to a `Tree value`. I gave `ana` something more exotic, but not really any more complex. `futu` is based on the same ideas, but the coalgebra it uses needs to be able to produce 1+ layers, not just a single one. This post is already long enough, so I'll just say that it uses the "free monad" on the base functor to represent the "or more" part of "1 or more" layers. Also, free monads always have this (uniformly) recursive structure (and, of course, a base functor). Because they are, I can and do use `ana` to generate that free monad value. --- Isolating recursion to schemes like this can be valuable. If the coalgebra is total, `ana` and `futu` (and the other unfold *morphisms) are guaranteed to be productive. If the coalgebra is total, and there's a reachability relation between the seeds, they are guaranteed to be total. If the algebra is total, and the structure is data (not codata), the fold *morphisms are guaranteed to be total. (Folding codata depends on how well the specific codata value matches the laziness of the algebra, and it might be worth excluding from a language, or at least making much harder than the other things mentioned in this paragraph.)
Solved on the *#haskell* IRC channel. `v &lt;- optionMaybe . try $ vendor` does the trick here.
 scanr' (+) 0 [1..10] = scanr' (+) 0 (1:[2..10]) = (+) 1 q : qs where qs@(q:_) = scanr' (+) 0 [2..10] --- scanr' (+) 0 [2..10] = scanr' (+) 0 (2:[3..10]) = (+) 2 q : qs where qs@(q:_) = scanr' (+) 0 [3..10] ... scanr' (+) 0 [10] = scanr' (+) 0 (10:[]) = (+) 10 q : qs where qs@(q:_) = scanr' (+) 0 [] --- scanr' (+) 0 [] = [0] --- (+) 10 q : qs where qs@(q:_) = scanr' (+) 0 [] = (+) 10 q : qs where qs@(q:_) = [0] = (+) 10 0 : [0] = 10 : [0] = [10, 0] ... (+) 1 q : qs where qs@(q:_) = scanr' (+) 0 [2..10] = (+) 1 q : qs where qs@(q:_) = [54,52,49,45,40,34,27,19,10] = (+) 1 54 : [54,52,49,45,40,34,27,19,10] = 55 : [54,52,49,45,40,34,27,19,10] = [55,54,52,49,45,40,34,27,19,10]
Even if the input type is small and finite, it doesn't actually guarantee that the function will terminate, or even return in a small amount of time, though I do agree that it is a good indicator it will.
My solution for part 1:
 My solution for day 1, part 1 main::IO() main = do text &lt;- readFile "input.txt" print $ sum $ readNumbers $ lines text readNumbers::[String]-&gt;[Int] readNumbers list = [readNumber x | x&lt;-list] readNumber::String-&gt;Int readNumber number@('-':xs) = read number :: Int readNumber ('+':xs) = read xs :: Int 
i mean, thats true, otoh, its using the host type checker, so i consider it shallow in that regard :) 
Ghc definitely does return x evaluated. The only reason a function is called is that you need the value, so returning something unevaluated would be silly, and it would mean the caller would have to check if the returned thing is a value or not. The evaluation rules can, e.g., be found at https://www.cs.tufts.edu/~nr/cs257/archive/simon-peyton-jones/eval-apply-jfp.pdf
The "official" policy is to wait until the top 100 global have all submitted. Other than that, the competition isn't too serious because of time zones disrupting things, so we all usually just upload it whenever we're done. It's mostly an honor system after that :)
Will the MIRI non-disclosure announcement affect how openly you‚Äôve been working?
Congrats to both Edward and MIRI!
that owns! i feel slightly safer (not sarcasm). i had read about AI Risk stuff in high school, and tried not to think about it too much afterwards. btw, is there any Haskell at MIRI (/ or if not, might there be?
&gt; is there any Haskell at MIRI Indeed there is. A rather significant cross section of the folks here know their way around Haskell, Agda, Lean, etc.
I *think* packages that differ only in capitalisation have finally been banned in Hackage (although I'm not sure how to verify that).
They are. I tried to upload a package named 'quickcheck' and it failed 
Thanks, that makes sense. Does the top 100 always fill up so quickly, or just for the earlier problems? I finished the first problem in 5 minutes and I think 900 people had already beat me!
I'm rather consciously on the outside of that. I'll be trying to do as much as I can in the open, as while it is a capability-growing story, the capabilities I'm trying to grow are the ones that you pretty much need in any world where you can reason about the systems being built. Growing _those_ capabilities can be seen as an unabashed good. MIRI has been remarkably supportive of my research direction, wanting me to get involved in what their doing, but not turn what I'm doing into a clone of their existing approach. I do admit, though, there is a sense of being the only one doing heavily public-facing research, in an otherwise now closed-by-default organization, which places a rather large focus on me to get it right, lest it color impression folks have of the work they are doing. I would not have taken the position if I had felt that I wouldn't be able to do what I'm trying to do in the open. Nate Soares came out to Boston personally, and made a very compelling argument for me going off and doing the work I'd been trying to complete solely in my evening hours full time.
&gt; [...] REST/JSON using Servant and let an external framework handle the server mesh, discovery, health and welfare parts (e.g. Kubernetes) This is exactly what I‚Äôd do as well. I‚Äôd love to hear from someone with a better solution. &gt; With this approach you can create true language-neutral schemas that can be versioned, and those versions can be articulated in a way that is not dependent on the binary representation of a particular type, yet it can be used to deterministic-ally generate those types in Haskell and many other languages. Can you expand on what you mean by this, exactly? The greatest challenge for me in software development is versioning. Ie. depending on some serialized data in some format somewhere (e.g. tables in database, or a Haskell data type stored somewhere as JSON), and making sure updates are somehow aware of the format of existing data. For example, let‚Äôs say I update some packages and build my application, and a Generic serialization format for my data type changes. This will compile fine, but fail in production as it‚Äôs not compatible with the existing data. I haven‚Äôt found a solid solution for this problem yet. Manually versioning is alright, but it‚Äôs error-prone as I often don‚Äôt want to bother with serialization formats (I want them generated by some Generic instance).
Ed that‚Äôs really reassuring to hear. Thanks!
Some lines of code exceed the length of 120 characters. It's too much imho. And according to [haskell style guide maximum is 80](https://github.com/tibbe/haskell-style-guide/blob/master/haskell-style.md#line-length).
That's great to here. I've been following the Haskell and Miri/AI safety communities for some time now and it's lovely to see them work together more. Good luck Ed
This is a great idea! Just had some fun solving the first day, thanks for setting up the lil leader board
Not entirely incidental, that how custom type errors were designed to work. 
Your recent work and Twitch streams have been fascinating. I've really enjoyed guanxi and succinct-binary. If all these pieces end up working well (monoidal parsing, type class advancements, type-directed term synthesis, SIMD codegen, etc), Coda looks to represent a serious advancement in functional programming. As for MIRI, despite enjoying EY's writings, I think the basilisk-style AI risk is silly (though I do think there is AI risk -- it's already here, just look at the shameful ways we use the AI we have), but hey, if the organization is supporting this kind of work, then credit where credit is due: this is great!
This is nice and I understand the whole patreon thing, but why not open a PR to improve the Lens library documentation? This would help everyone looking for the lens library on hackage/stackage.
&gt; Even if you don't buy any of the AI risk stuff If I can get a little dark and OT: thinking about the talk you posted above, it seems to me AI risk is not different from the risks of technology generally, under capitalism and democratic systems of governance. [Cars](https://www.wnycstudios.org/story/on-the-media-2018-11-23), Facebook, fossil fuels, atomic weapons, etc: there is no "stop" button, and these systems have outstripped most real signs of human intentionality (if there ever was any to begin with), and the room is already filling with water ([literally](http://sealevel.climatecentral.org/)). It seems like without care AI should be expected to be at least as calamitous as the systems powered by human intelligence are proving themselves to be, only Much Faster‚Ñ¢. Ironically maybe an [intentional](http://amishamerica.com/do-amish-use-technology/) AI theory and practice like the kind MIRI is researching promises a solution to technology generally. Or maybe not.
Not sure docs are really the place for long form prose like this sort of thing (IMHO) though I definitely agree that the breadth of examples there could be improved. It takes a significant investment from me to write these guides and I have some longer term plans to structure these posts into a book which I plan to sell in order to make the efforts more sustainable for me (with special consideration for patrons who've helped along the way of course üòâ). If I could garner significant support towards contributing back to the docs that'd be great! In my personal experience it tends to be a pretty slow process and I'd like to get these things out as soon as possible. All that said, I definitely appreciate the feedback and will think on it! Thanks for the comment üòÑ
Thank you for style guide! I think 80 characters limitation is near to outdated today. That's why I have such lines. But I'll try to use it in future. Thanks again.
Awesome! I'm going to use this from now on. Thanks for the link.
There are two things I was thinking of here. The first is simply the issue of using `Binary` as the wire format for messages. Version compatibility is sort of built in to protocol buffers, you can always add new fields and still deserialize older objects. With Aeson if you add a new field with a Maybe then it can also deserialize older json messages. In Cloud Haskell that is more or less impossible, the new code wouldn't not be able to deserialize an older message, even if in the new type the new field is a Maybe. The other concept is dispatching to versioned services. Let's say you make incompatible changes in a Servant service. You can build versioning into your URIs and module namespaces and support multiple versions at the same time in a way that is quite straightforward, although maybe its a bit tedious, you just have totally modules that export different API types mounted under different versions that you chain together. You can do similar things with GRPC endpoints as well. In any case you can easily export a swagger spec or your .proto files and clients can reliably generate the correct types to consume those. In Cloud Haskell the client and server are typically a single binary - you really just have a cluster of services all running the same binary, some of which may take on different roles.
I have just published the first working version of a native Haskell runtime to run Haskell code on AWS lambdas inspired on the rust and cpp ones. The code is still very hacky and I'll be updating it during the next days but it seems to work. https://github.com/jesuspc/aws-lambda-haskell There are instructions on the README in case you want to try it out.
I do think the main thing that differentiates it is the "much faster" part. You get a lot of armchair folks saying, well, if we ever get an "AGI", to whatever arbitrary standard that would require, we'll just keep it in a box and ask it questions, so it isn't a problem, but to be honest, we're literally giving far more primitive systems the keys to the car, and the short-term benefits mostly come with increased agency.
It would be great to have tools that allows us to reason about high level concepts in machine learning models. Today engineering ML models involves a lot of trial and error and intuitive guessing into the properties of parts of the model. Things like "I guess if this part of the model is strictly positive, and this output is a monotonic function of this parameter, the training will go smooth". But if it was possible to deal with even more high-level concepts like bias, model capacity, etc, that would be awesome. I don't if this is possible though. Anyway, it's great to have someone like you working on how to improve our ability to reason about artificial intelligence code in any way.
Awesome work - I did more or less the same thing, but I've created a custom layer that would invoke any uploaded binary. I was wondering how bad would lambda work if we would make stack build the thing on their side before running first event (that would probably suffer from the cold start issue, but still interesting).
I don't want to go into detail on this topic here, but I do want to clarify something quickly. (Also, disclosure: I'm a MIRI donor.) &gt;As for MIRI, despite enjoying EY's writings, I think the basilisk-style AI risk is silly Yes, so does EY. That is not what he's worried about.
This is awesome! I've been back-of-the-mind excited about `coda` since you showed it to me at ICFP in Boston. You also might be the only person for whom, when they say "I'm going to build a better Haskell", my response isn't one of: * That's a pretty ambitious project; I expect it to come out half-baked and unusable if it doesn't get completely abandoned first. * That's great, but you think that local type inference is good enough, and you don't care about coherent type classes or laziness... so I'm probably not going to want to use it. I look forward to seeing your progress.
That makes a lot of sense. I wonder why Amazon has released language-specific runtimes when they could take your approach instead.
To be fair my response to myself is usually along the lines of the former. ;) I'm mostly hoping to get good artifacts out of the discovery process. =)
It gets harder over time (generally), but usually the public leaderboards fill up within the first hour. This post has a history of how long it took every challenge last year! https://www.reddit.com/r/adventofcode/comments/7lzsg6/2017_final_leaderboard_chart/
What exactly do you mean by abstraction? Could you maybe walk through an example?
Have you ever encountered the any of the work on Basil Johnston? He made the point that a lot of institutions, through the restrictions they place on those in them, form structures that function as optimization algorithms independent of the motivations of their members, and that there are already wendigoes (paperclip-maximizers) in the wild already, just not filly transistorized at the moment. One such example is the board of directors of a public corporation which is legally required to maximize profit for shareholders and replaces members who do not serve that purpose, functioning as a sort of genetic algorithm. Under this kind of analysis, a lot of current issues are symptoms of the world getting slowly paperclipped, and any kind of development that can speed these systems up (which can include a lot of non-general AI components such as hardware replacement is not all-or-nothing) is a threat to humanity until we can contain them and shut them down.
Could you give an example of how you would use this? I'm not quite following. Cabal uses a 100% from source model, which is different from e.g. NPM because people often use build tools (dev dependencies) and upload the build output to NPM, rather than having NPM build that from source when the user downloads the package. With cabal-install and stack, the whole build is performed from scratch, so there's basically no difference between runtime dependencies and dev dependencies. Nix can use binary caches to download build outputs instead, and this filters out compile-time-only deps automatically because of Nix's powerful dependency tracking. So ultimately I'm not seeing what we're lacking here.
If you never put a Gemfile or package.json in a project that doesn‚Äôt even use Ruby or Node.js, then you may not understand the benefit of this option.
You're right, I don't. Please enlighten me.
Cool! I really like GitLab. Its issue tracking and project management stuff in particular is better than pretty much every other tool I've tried of its scale (Trello, GitHub, ZenHub)
Starting with traversals is an auspicious choice. Between the `Traversable` connection and their position right in the middle of the *lens* subtyping hierarchy, they are a very good entry point for understanding optics.
If you, like me, were curious about why GHC is moving to GitLab after having moved to Phabricator somewhat recently, here is the thread: https://mail.haskell.org/pipermail/ghc-devs/2018-October/016425.html
thanks for the reply, and that's great to hear!
I'm using Lucid, and an extension *lucid-from-html*, to generate html in an app using Spock. Everything was going fine until I wanted to add some jquery. Below is the closest lucid code I have gotten to what I think it should look like. script_ [ type_ "text/javascript", src_ "https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js" ] $ "" script_ [ type_ "text/javascript", src_ "https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" ] $ "" Here is a screenshot of the error I'm getting. https://imgur.com/3EkKriq Looking at the documentation for Lucid. The difference between *script* and most other html attributes is in the type. Div for example has type: div_ :: Term arg result =&gt; arg -&gt; result While script has type: script_ :: TermRaw arg result =&gt; arg -&gt; result Looking at TermRaw vs Term the difference seems to be in the instances Instances for term: https://imgur.com/Zzr6nXc Instances for termRaw: https://imgur.com/gjUtz1r Documentation for [TermRaw and term.](http://chrisdone.github.io/lucid/Lucid-Base.html#t:TermRaw) I'm not very experienced with reading Haskell documentation, but from what I can understand, I need to modify my usage of script with some explicit reference of "toHtml" or something along those lines. Any help would be welcome. Thanks!
Believe it or not, it's been over four years since GHC's Phabricator instance was introduced. Time flies!
While there are certainly some facets of Trac that we will miss (metadata fields, ), I agree that GitLab's issue tracking is a significant step up from most other recent free tools around.
Those are explained in DataCon.hs. Read the comments starting with "Data constructor representation".
&gt; the board of directors of a public corporation which is legally required to maximize profit for shareholders They're not actually legally required to do this! This is a common myth. https://www.nytimes.com/roomfordebate/2015/04/16/what-are-corporations-obligations-to-shareholders/corporations-dont-have-to-maximize-profits However, it is nonetheless usually considered their "job" to do this, and the market itself incentivizes (even mandates) such behavior and the myth of a legal requirement is very nice way to pawn off responsibility for, say, mass layoffs or use of harmful chemicals or any other behavior that meets social approbation.
[removed]
This might be related. https://github.com/haskell/cabal/issues/3708
How would this relate to the existing build-tool-depends and pkg-config-depends fields? https://cabal.readthedocs.io/en/latest/developing-packages.html#pkg-field-build-tool-depends https://cabal.readthedocs.io/en/latest/developing-packages.html#pkg-field-pkgconfig-depends
If I understand their view correctly, it would be useful to have (for example) Brittany and HLint marked as dev-dependencies but they are not build-dependencies, so they aren't forced onto package users. Now HLint and Brittany are Haskell deps but maybe we want to allow non-Haskell tools too to be dev-dependencies.
This is how I use dev dependencies as well. The entire toolchain from start to finish is the dev dependencies. Linter configs, the linting programs, formatters and their configs, etc. Also, any code transformations become part of the dev dependency chain. This is very relevant for JavaScript where you transpile your code down to "old" JavaScript (or even from jsx to or typescript to JavaScript) but there's no reason something like source plugins couldn't be considered a "dev dependency" with the rendered output being considered part of the library code; same for template Haskell. Backwards compatability helpers are a huge reason for dev dependencies in JavaScript; it'd be interesting to see what Haskell could do there (although it generally has much less need for that in the first place) Essentially, any tool the library writer uses to make writing the library easier should ideally be tracked in the same way that traditional library dependencies are.
For extra fun, it's possible to define a composition operator that allows you to pass two arguments through. (Anything of the form `(f .) . g)` can be written as `f ... g` where `...` is the operator) I believe it's called the owl operator or the Blackbird operator? It's pointfree definition will explain the name :)
&gt;Haskell changed the way I saw programming. Same here. My first exposure to a real functional programming was Clean and soon after I discovered Haskell for a mind blowing experience. I have used Clojure for a good four years. I have struggled to choose between static and dynamic languages. I now do a lot of Elm programming (a much simpler typing system than Haskell) and the kinds of bugs that are caught by Elm compiler make refactoring a joy. I have now convinced myself that a strong type system is essential to programming large systems. &amp;#x200B; About Rich's talk, I give him the credit for popularizing Lisp and functional programming. He did a great job of Clojure's concurrency story. Same thing with Datomic. However, his talks, while very entertaining and thoughtful, are usually centered around ridiculing an idea (monkey patching code, unit testing or Haskell's type system) and coming up with a variant as the answer. The wholesale discounting of use of Category Theory in programming using superficial arguments was not something I expected from a language designer. IMHO, type signatures don't stand in for full-blown contracts nor do I want them to be. There are a number of benefits in being able to write "reverse :: List a -&gt; List a" and all the arguments Rich made about it are very shallow and biased. Anyway, as with most difficult subjects, time will settle this argument. If he does come up with a version of 'spec' that hits the sweet spot, it will be a great accomplishment. If not, this is just another experiment albeit one with a self-serving bias. 
&gt; but there's no reason something like source plugins couldn't be considered a "dev dependency" with the rendered output being considered part of the library code; same for template Haskell No and no. These are both things that we expect to happen *when the package is built*, not before it is uploaded to Hackage. GHC's model fundamentally treats these as compile time actions, not cacheable preprocessing steps. Even if it could do that, it would drastically hurts the ability to declare dependencies on Git checkouts of a package and whatnot, because those repos will certainly not have those things checked in. Now, stuff like brittany and hlint make more sense. They are things that have absolutely *no* practical effect on building the package. With `cabal-install` at least, I *think* you can just list such packages in `extra-packages` or `optional-packages` in `cabal.project`, and `cabal-install` should let you do something like `cabal new-run brittany`. Haven't tested that; just seems like how it would work.
Glad to see this, only a good change for GHC and Haskell. Every time I've deal with Phabricator, I haven't found it to have the friendliest or nicest interface. Reducing friction in the contribution process is key to getting contributors, so it's nice to see another project I'm a fan of take a step in that direction.
Why does Data.Set have no combined query and insert function? It feels rather inefficient to do that separately, I can't imagine the compiler can optimize that away. Map at least has insertLookupWithKey. Though no variant without the "with".
It sounds to me that this is something where partial types in Typescript can help.
Quite similar. Except, some tools are not strictly required to build a project, but are good to have anyway, such as ShellCheck!
This looks more relevant: * https://github.com/haskell/cabal/issues/5588
For Map, insertLookupWithKey is beneficial because you are operating at _that one key_ only, so having a pointer to the key in the tree structure is useful. You can't insert a mapping for any other key. For Set, it is most likely that you'll be modifying the tree somewhere else, so it is not really clear if that can benefit from such an operation.
The default implementations of [`sconcat`](https://www.stackage.org/haddock/lts-12.20/base-4.11.1.0/src/GHC-Base.html#sconcat) and `mconcat` in base seem odd. Anyone know why this is the case? sconcat (a :| as) = go a as where go b (c:cs) = b &lt;&gt; go c cs go b [] = b mconcat = foldr mappend mempty Instead, it seems more reasonable to have the following. sconcat (a :| as) = foldr (&lt;&gt;) a as mconcat [] = [] mconcat (x:xs) = sconcat (x :| xs) This way, `sconcat` benefits from `foldr`'s rewrite rules, and if someone writes a specialized implementation for `sconcat`, that gets reused in `mconcat`.
Are we hosting it ourselves or using GitLab‚Äôs infrastructure?
From Twitter : https://twitter.com/bgamari/status/1069049870391095296?s=19 &gt; Thirdly, the timing corresponds well with unrelated changes in GHC's (and, more broadly, haskell.org's infrastructure): We are happily moving our infrastructure to packet.net, who have graciously offered us their excellent hosting services. Thanks packet!
Hmm, not familiar with Packet. Will check them out.
Congratulations Edward! You make great things that are fun to use ‚Äì thanks for making the Haskell ecosystem better ‚Äì I can't wait to see what you make at MIRI. Best wishes!
&gt; A Traversal can be treated as a Prisms by focusing on the first element of the traversal Is this correct? It seems that you would still miss the "can be turned around and used as a constructor" part.
That is very cool! üôåüèº
Not a GHC dev, but I'm surprised that no one mentions the easy upgrade path you have with Phabricator with 0 issues so far. It's just a few trivial steps (depends how you count; I have an ansible scroll for upgrading the entire virtual host). I tried to install GitLab twice now. It was a horror, both times. Even the second time every dependency has been already installed. Since it's difficult to install, I don't even imagine how difficult upgrades would be. Note that I don't object to anything, I would like to hear some opinion of people who run GitLab and how much stress they have with it, if any.
I was just wondering how long the first two challenges (Day 1 &amp; Day 2) took you guys to complete? I'm asking because, while I was able to solve both challenges (part a and b) it took me like 40min for Day 1 and maybe 1.5 hours for today's challenge. I'm reasonably experienced in programming (~5y) but I don't often solve problems like these (e.g. Euler problems or CodeGolf stuff).
&gt;it's a huge part of why language devotees keep getting stuck in stupid religious wars about who has the more better technique instead of actually sitting down and thinking critically about which situations benefit from which approaches. I agree with this, but (some) proponents of static typing are constantly bashing dynamic typing, calling it's users anti-intellectual, lazy, 'the programming equivalent of climate change deniers', etc. Most programming language research focusses on static typing, so I'm sure he takes constant flack as the designer of a 'serious' programming language. &amp;#x200B; I think this is where the snark comes from - it is directed at static proponents that froth at the mouth at the mere thought that a 'serious' language designer dare suggest a dynamic language is a better way to develop certain types of programs. &amp;#x200B; I agree that the snark does not help, especially if this is the only talk you've seen. In the past he has clearly stated that static typing is a better fit for certain types of programs, but (he feels) not the type of systems he works on. 
Have you tried using Gitlab [package](https://nixos.org/nixos/packages.html#gitlab+edition) from nixpkgs or [service](https://nixos.org/nixos/options.html#gitlab.enable) in NixOS? It seems to be working pretty OK
I don't have any Linux installations at the moment. I had FreeBSD packages installed with all their deps last time. I followed a step by step howto especially for FreeBSD. The initial steps preparing GitLab failed for some reason and I don't know why. I don't understand the error messages that do not give any hints how to continue. 
Great stuff Chris!
Book can be read from here: https://plfa.github.io/
https://skeptics.stackexchange.com/questions/8146/are-u-s-companies-legally-obligated-to-maximize-profits-for-shareholders?rq=1
Each of them took me about an hour so far. I'll typically get a solution quickly and then spend some time golfing it down and/or cleaning it up. 
The two `sconcat` declarations are not equivalent unless the Semigroup is commutative. In particular: -- (base) sconcat1 (x :| [y, z]) = x &lt;&gt; (y &lt;&gt; z) -- (yours) sconcat2 (x :| [y, z]) = y &lt;&gt; (z &lt;&gt; x) If you wrote `sconcat` with a left fold instead, you'd get: sconcat3 (x :| [y, z]) = (x &lt;&gt; y) &lt;&gt; z which _is_ guaranteed to be equivalent for lawful Semigroups, but has different laziness properties‚Äîit won't shortcircuit, whereas `sconcat1` will.
&gt;I personally want a language that is better at type classes than Haskell. If it is going to take me five years to fight each battle for refactoring any class hierarchy in Haskell, it behooves me to give equal time to designing a language in which I do not have to have the fight -- not just because there are no users of the language, but because I hope to fundamentally address the issue that forces users to have to care about how fine grained the class hierarchy actually is. If as a mathematician, I want 600 fine-grained super-classes above the concept of Field, I don't want to be flogged by users when I realize that it is 601, or when the users realize that they need an abstraction in the middle that the standard library didn't consider. I have an approach to handle this that I've been working on in the back of my brain for the last few years, and this seems like a likely place to leverage it. &amp;#x200B; I have re-started working on exactly this problem (with some of the fundamental technology going back to joint work with /u/roconnor). My solution uses combinators to build up hierarchies, so that large ones are not such a pain; and flattening to deal with the issue that you don't want end-users to be able to see which 'path' you took to go from the empty theory to Field. That allows refactoring of the hierarchy without affecting end-users. We've built up a library of slightly over 1000 theories, as a test case. \[Makes both the Typeclassopedia and the Lens hierarchy seem rather small in comparison...\] It ought to be more like 5000 theories, but we never implemented diagram-level combinators. The underlying theory is actually quite simple - type theory and category theory already have all you need worked out. Email me if you're interested in the details.
Is there an easier way to obtain the book? Compiling from source looks intimidating, but the only alternative is the website. 
Maybe this does deserve its own thread but here we go. I'm doing advent of code. For each day, I want to write a module. The module should export a record containing its name and a run function. My first attempt was type Solution = Text data DayProg = DayProg { _name :: Text, _run :: Text -&gt; Either ErrMsg Solution } This works but `Solution` annoyed me. What I really want is for `_run` to have `Show a =&gt; Text -&gt; Either ErrMsg a`. In `Main` I then create a map `days = Map.fromList [("1", Day1)]` with `Day1` being an instance of `DayProg`. I tried using GADTs like so data DayProg a where DayProg :: Show a =&gt; { _name :: Text, _run :: Text -&gt; Either ErrMsg a } -&gt; DayProg a but I ran into `a is rigid type variable` issues everywhere. For example if in `Day1` I want `_run` to return something like `data Day1Solution = Day1Solution Int Int` then the program would ultimately fail (with the above error) when it came to creating a map. The map signature was now days :: Show a =&gt; Map Text (DayProg a) days = Map.fromList [("1", Day1)] but that didn't work because when I gave the `Day1` module a concrete solution (which derived `Show`) it complained about rigid type variables. *What I'm trying to do in plain English* Have a map of `Text` to `Thing`. `Thing` has a `name` and a `function`. `function` takes `Text` and returns something "showable" (let's just drop the `Either` for the purpose of this question). That showable thing could be a vector, a record, an integer, etc. I know way less about Rust than about Haskell but I feel like in Rust I'd easily achieve this with dynamic dispatch and a boxed trait.
As mostly an outsider to GHC development besides fixing some ticket metadata. I've liked Trac, but the interface to Phabricator can be a bit overwhelming.
Ah okay, makes sense. Didn't think this one through üòÖ.
Overwhelming is a good way of describing it.
Looks like we might need to do the same with this book as was done to Bartosz's book :)
&gt; No and no. These are both things that we expect to happen when the package is built, not before it is uploaded to Hackage. Right, and that's absolutely fair; Haskell doesn't really have a split in the ecosystem where you might want to have a bunch of pre-processing steps, like JSX or using the latest JavaScript syntax. However you perhaps could have (just spitballing her something like a little TH helper that writes out mechanical instances that's run as a build step without incurring a dependency on TH for your users? &gt; Even if it could do that, it would drastically hurts the ability to declare dependencies on Git checkouts of a package and whatnot, because those repos will certainly not have those things checked in. Yes, this is an unfortunate side effect. It's never been a large enough issue for the JavaScript community, but I expect it to be addressed at some point because it's always been a gaping security hole that has been exploited before (final code artifact isn't guaranteed to be from the actual source code in the repository)
It would be so bad ass if some future idris or coq book requires you to run it in idris to generate its output.
I'm confused by the discussion of structural vs nominal types in Haskell. I thought Haskell has nominal typing, because it has type aliases? If I do type Foo = String, then a function that expects Foo will reject plain String inputs, right? Isn't that nominal typing? And that has benefits in protecting me from mixing up input parameter ordering if I was just using raw String and numeric types, but it does remove the advantages of structural typing. Or am I making one or more fundamental errors in my reasoning? I'm a novice on these topics.
I should have described it better. With "combined query and insert" I mean an insert function that also tells me if the value was already in the set before the insertion. The type would be insertLookup :: Key -&gt; Set -&gt; (Bool, Set) For Map, insertLookupWithKey can do that because it also returns a pair.
Oh, I see, yes that makes sense. I guess you could open an issue on the containers GitHub for this. Did this come up for the Dec 1 Advent of Code problem ;)?
Whoops! You're right, that's definitely incorrect! You can use traversals with \`preview\` and friends of course, but I got that assertion mixed around, every \`Prism\` is a traversal, but not the other way 'round! Thanks for catching that :) 
I think [*Software Foundations, Vol. 1: Logical Foundations*](https://softwarefoundations.cis.upenn.edu/lf-current/index.html) might be like that... It's literate programming, anyway. I don't know what you need to do to render it as html/pdf.
I mostly chose to start on this topic because so far as I can tell there are barely any resources on actually writing your own Traversals, and there's not much help on it in the docs, but I definitely agree! Most optics are built on the \`traverse\` signature so it's an essential part of understanding how lenses actually work!
&gt; Did this come up for the Dec 1 Advent of Code problem Yep. Solving the problem of finding the first duplicate in a sequence. So many insertions that build a new map followed by only one insertion that returns the old map unmodified. The function should be appropriate in general when one expects much more real insertions than duplicates.
Here is another complete set of solutions for 2017 (in Scala, Kotlin, Haskell and Eta) and a (very simple/naive) benchmark to compare all implementations - [https://medium.com/@rolandtritsch/advent-of-code-final-cut-and-first-cut-88a351f69e6c](https://medium.com/@rolandtritsch/advent-of-code-final-cut-and-first-cut-88a351f69e6c) &amp;#x200B; The current effort (Haskell and Eta) can be found here :) - [https://github.com/rolandtritsch/haskell-aoc-2018](https://github.com/rolandtritsch/haskell-aoc-2018) &amp;#x200B;
FWIW there is also a [typechecker[(https://github.com/ElementsProject/simplicity/blob/master/Haskell/Simplicity/Inference.hs#L353) that coverts untyped Simplicity syntax into (maybe) well-typed tagless final Simplicity terms.
I think it will be the same amount of work because you need to traverse the tree structure anyways, not sure why laziness matters here?
[Already closed issue](https://github.com/haskell/containers/issues/31).
60 mins for Day01 and 90 mins for Day02. It normally takes me 45-60 mins to get my head around a solution and then another 20-30 mins to refactor it and polish it (data structures, algorithms, code structure, naming things :), ...). - [https://github.com/rolandtritsch/haskell-aoc-2018](https://github.com/rolandtritsch/haskell-aoc-2018)
I got 15 mins for day 1 and 30 mins for day 2 (both parts).
As another maintainer of `non-empty-containers` I also wouldn't have been opposed to this.
It's not so much that it's not composable, but rather that normal intuitions around things I can do don't always hold; for example, any time I might put elements, attributes, etc., into a list or some other data structure in order to make use of familiar syntax... Usually can't do that. Not sure what the correct wording for that is as I do agree with you on the library being composable in general (it's *really* nice how easily larger stuff just glues together, honestly). The `withAttrs` function is a good concrete example. Normally if I were to write something where I might be chaining a lot of things together, I'll put it into a list and then map that into the structure I need, so if I were to write something with a lot of attrs like: &lt;img id="thing" class="lazy-load" data-src="https://picsum.photos/400/300/" alt="Cute meaningless stock picture retrieved for a card" aria-hidden=true src="./low-res-fallback.jpg" /&gt; I'd write it in the library currently like so: image = img_A ( A.alt_ "" # A.id_ "thing" # A.class_ "lazy-load" # A.custom_ "src" "https://picsum.photos/400/300/" -- Currently not possible in the library? # A.alt_ "Cute meaningless stock picture retrieved for a card" # A.ariaHidden_ "true" # A.src "./low-res-fallback.jpg" ) This is... not really an insane amount of typing, but it's noticeably odd to me since I'm not using lists even though I'm giving a list of attributes. Like, even though it's the same exact amount of typing as a list, it feels like an inferior UX somehow; funny how that works. In a library like jQuery, you'd write this like: image = $('&lt;img&gt;', { id: "thing", class: "lazy-load", dataSrc: "https://picsum.photos/400/300/", ... etc }); Similar levels of typing, but it feels better, somehow? I think it has to do with the object literal syntax, which has a lot less mental overhead due to being so pervasive, much like the list syntax for Haskell. (Side note: Really starting to wish we had object literal syntax or something like that for this sort of thing. [(,)] is pretty good, but still feels a bit too heavy and of course the entire approach currently falls apart for heterogeneous collections like strongly typed HTML attributes :) 9 times out of 10, for example, I'd rather write `fromList [(1, "foo"), (2, "bar"), (3, "baz")] :: Map Int String` than something like `(1 # "foo" &lt;#&gt; 2 # "bar" &lt;#&gt; 3 # baz &lt;#&gt;)` (made up syntax). Is it less typing? Perhaps not, but it uses lists and tuples which I'm already familiar with vs introducing "lists and tuples but done the Mappy Way‚Ñ¢". If this were a looser typed library where every attr had the same type, I'd just define a quick withAttrs function and write image = img_A (withAttrs [ (A.id_, "thing"), (A.class_, "lazy-load"), ... etc ]) Doesn't really save me a lot of typing, but it feels more natural to write it out like that since now I can use lists and tuples and all of their assorted infrastructure. Of course, this is a mildly contrived example, I could always just use the `( # # # )` syntax, pull common attrs out into local bindings, and so on, but I did get bit by this once purely out of a desire to use the nice list syntax versus `( # # # )`. That being said, even if I were to be stubborn and try to write a `withAttrs` function, I can't figure out how I would write that sort of thing; it doesn't seem as if the types allow it. So, essentially anything where I might place elements/attributes/etc in a data structure and then do something to them... Can't do that. I can place strings in a list and map the element/attr/etc., over them, but not the other way around. It's an interesting difference for me, because I'm used to being able to glue things together by abusing lists/tuples as a way to get nicer syntax, DRYing things out a bit, and being able to glue stuff together using functions I already know. But, again, I'm not really sure what to call this limitation because it doesn't really feel fair to say it's not composable, since it is, it's just not as... ergonomic? Is that the word I'm looking for? And of course, since these can't be placed in lists, any of the patterns you'd use on lists don't really carry over nicely. Other interesting things I've noticed (that are largely irrelevant to the above): * I defined a function `nil :: String; nil = ""` whose sole purpose is to be typed explicitly. * Figuring out how to use Type Applications is a lot less obvious than I thought it would be. * Occasionally I end up having to put the inferred type of a function or two so that the rest of the program will get inferred. Could just be my inexperience, though. Currently, a solid third of the file that I'm using to play around with the library is just type signatures I wrote down to get GHC to stop complaining about ambiguous types (turning OverloadedStrings on/off makes no difference). Seems to be 100% caused by string literals lol. * Abstracting at the component level and higher works great. It really is quite composable and fun to use once the smaller building blocks are done :) * Small functions to save typing sane defaults help a lot, eg: css url = if null url then Nothing else Just . link_A $ A.rel_ "stylesheet" # A.href_ url # A.type_ "text/css" * The document ends up looking like lisp because the entire AST is in parens * Brittany doesn't like formatting `( # # # )` the same way it would `[ , , , ]` so stuff looks unlike I would do it by hand; especially annoying since the code can easily look very spaghetti like if I don't indent everything to look more AST like. Ah well. * I might end up writing a small component library for this to make writing out large amounts of HTML by hand easier. There's no standard function for the HTML5 boilerplate, for example; lots of HTML elements require certain properties that are always the same, so it's easy to write small wrapper functions that make writing those more ergonomic. Sorry for the wall of text; I realized 'composability' might not have been quite the word I was looking for halfway through and tried to clean it up some
https://gitlab.staging.haskell.org/ghc/ghc/-/jobs/337 and we still have validate failures on not linux :( 
how do i merge the trac_carter and carter accounts?
Lookup is tail-recursive, insert is not. Insert needs to remember the subtrees that won't change to stitch the tree back together. Though the implementation of Data.Set is optimized to avoid building a new tree. I suppose the performance difference would be rather small though. I guess you are right that laziness doesn't matter here.
Thanks for digging that up. I'm rather surprised foxik thinks it's not worth it. 
one concerning thing i've noticed is how synchronous / slow diff rendering is with gitlab... eg it took 2 seconds for my browser to load just the json in https://gitlab.staging.haskell.org/ghc/ghc/merge_requests/4/diffs this might be due to really really slow hosting box, but it looks like it takes 14 seconds for the diff to start being delivered to my browser, then its downloaded instantly ...
My biggest issue with comprehension syntax is it encourages people to abuse it for things better done with common functions, such as `[f x | x &lt;- xs]` instead of `map f xs`
Funny thing is my current approach started from a discussion I had with you at McMaster back in 2011 about your old classical type theory with pushouts of theories relative to a base theory. It really stuck with me! Having a flat representation of the theory is part of my story as well, at least from a user presentation perspective, though I think I have a bit of a different story from a software engineering perspective. Rather than blasting out whole hierarchies with combinators, I am picking up a bunch of small obligations to discharge to prove that things like fmap = liftM = fmapDefault = liftW (up to functional extensionality, but then being extensional makes that easier.) when you go to instantiate Monad, Comonad and Traversable with the proofs constrained by the common superclass constraints. (To use a ridiculously simple example.) The idea being you locally wind up proving a handful of coherence properties between immediate superclasses and get transitive superclasses instead, then I can rely on the fact that in coda I don't have orphans, to ensure that there is always a unique module to discharge each of those proof obligations. Your approach sounds similar to one that Aaron Vargo has been playing with. I'll shoot you that email!
Cool. Looking forward to reading 
I'm generally happy to take patches for `lens`, FWIW. Especially ones that add more documentation.
&gt; If I do type Foo = String, then a function that expects Foo will reject plain String inputs, right? It won't. `type` is mostly for convenience, i.e. "I don't want to type this long type signature over and over". To do what you described, you want `newtype Foo = MkFoo String` ‚Äì if a function expects a `Foo` and you want to pass in a string, you have to explicitly wrap it in a `MkFoo`.
Sounds great! I'll try to distill some things down a bit when I have a chance üòÑ
Do you by chance have `OverloadedStrings` on? The error message says that the compiler can't determine a concrete type for something that must have a `ToHtml` constraint. Looking at the instances of `TermRaw` you supplied, what needs to have the `ToHtml` constraint are the contents of the script `(Monad m, ToHtml f, a ~ ()) =&gt; TermRaw [Attribute] (f -&gt; HtmlT m a)` ("Given attributes, expect more child input."). You are passing "" as the contents. Does it work if you try giving them an explicit type like `(""::String)` or `(""::Text)`?
These are great fun to try. They do slightly encourage you to take shortcuts (the inputs are all correct, so it is tempting to cut checking, and you can assume that they have certain properties, which means that you can take algorithmic shortcuts). However, Haskell is a good language for trying to tackle the problems; more thinking is required than typing. The first couple of problems this year have involved a good deal of list manipulation, a Haskell forte. It is good to try to refactor a correct first attempt to make it more efficient / shorter / clearer. 
I expect that if you came up with a benchmark where it actually mattered, the issue would be reconsidered. I also expect that if you _really_ need it, you can make it happen using the `.Internal` module.
I'm sorry but I don't buy your argument, because ultimately there are two aspects at play: the types and the "behaviour" (the values contained by those types). While it's true a type signature tells the input and output type, it tells you nothing about the behaviour, which is the focus of attention here. One could think of type signature as "Value Erasure" i.e losing information about the behaviour of a function. Eg we could have an infinite number of functions: * Uppercase * Lowercase * Propercase * ReplacesSpaceWithDash Etc But all of them would still have exact same type signature: String -&gt; String And yet the signature alone does tell us enough in terms of "behaviour"
Sorry if I wasn't clear in my original post, but I am not arguing that you only need types, or that types completely specify the behavior. I do mention testing in the last sentence. I was just pointing out a place where Rich Hickey was wrong and they types do tell you more than a programmer would expect if they were not familiar with Haskell. Additional, as I mentioned in the previous post a more general the type the more you know about the type, while the less general the type, the less you know about the function. So, it isn't surprising that, as you say, String -&gt; String has an infinite number of implementations. On the other hand, a haskeller might have a better guess about what the function (Eq a) =&gt; [a] -&gt; a -&gt; a -&gt; [a] does since it's type is more general. 
In [the typing rule for `M ‚Üë`](https://plfa.github.io/Inference/#15441) the book uses an explicit equality: ‚ä¢‚Üë : ‚àÄ {Œì M A B} ‚Üí Œì ‚ä¢ M ‚Üë A ‚Üí A ‚â° B ------------- ‚Üí Œì ‚ä¢ (M ‚Üë) ‚Üì B Does anyone have any insight on why this is written this way, and not the seemingly more straightforward ‚ä¢‚Üë : ‚àÄ {Œì M A} ‚Üí Œì ‚ä¢ M ‚Üë A ------------- ‚Üí Œì ‚ä¢ (M ‚Üë) ‚Üì A ?
He clearly says there is a trade off, he is pointing out what it is. He means, in clojure, you can use a map as a function. He makes no judgement about Haskell having map. He is pointing out this is fine and explaining why. Try this, go to a Java developer and explain your just storing your data in a map, there probably going to argue with you. He is defending and explaining good choices. I don't recall the bit about pace oriented programs... Types making you do something isn't a value proposition, I'm not saying types aren't helpful, but your not proving they are by saying there mandatory. I found his bit on maybe informative, but from what I could gather it's best said as such. "Optionality is contextual" he is saying that the absence of something doesn't describe it, it tells you that it might not be there... But *when* might it not be there you ask? That's the context. Like if I say I'm giving you 5 maybe dollars it makes no sense. But if I say I'm giving you 5 dollars if you paint my sense it does, currently people are doing the former and he is saying we can do better by letting people describe the later. This might be what classy lens are, I have no idea. Not sure about the a to a thing... Haskell proves it's a subset? How? Seems like it could be a superset it different set, etc... He isn't taking away from haskell by not mentioning quick check etc... I'm fairly sure that work is mentioned on the spec docs. Like, I think he could be a bit more political, but honestly, if you heard the things people throw out about how there type system is going to save the world and feed the hungry you would probably cultivate a bit of an attitude over time as well. 
I mean, there are really important differences between schema and spec. Same with deps, lein and boot. Good news, your free to use which one you want. 
Congratulations to MIRI!
I mean, it's up to you want you consider evidence. But by your logic, I feel I can assume python to be "superior" to Haskell because it's more widely used. I would want to see studies the show types significantly improving the time to model your domain, lower the cost of maintenance, etc... So far, I haven't seen studies that show that. Pair programming seems to have a much greater effect for instance. Like, that he is spending so much time on spec means he believes there is a value proposition, but it send like he is struggling m arguing we need types/specs to do more then say javas, before there worth the cost. 
&gt; But by your logic, I feel I can assume python to be "superior" to Haskell because it's more widely used. To be clear, my logic is not that widely used ==&gt; "superior" (also I did not use the word "superior"). The point I was making is that beyond a certain scale (codebase size), the fraction of languages without types seems to be very small. &gt; improving the time to model your domain I didn't make any such claims. Of course, if you mentioned that just as a passing remark, I understand why you might be interested in such a study. &gt; lower the cost of maintenance, etc... So far, I haven't seen studies that show that. I do not have studies for you but I am presuming that companies that are writing type checkers for dynamic languages probably didn't decide to fund teams on Bay Area salaries without convincing reasons. Facebook has multiple type checkers and VMs -- I presume they're keeping track of the cost/benefit ratio, and yet they continue development on these. Heck, they just created a language and practically threw it away (Skip). Fwiw, Dropbox have written about how types helped their migration from Python 2 to 3. &gt;Like, that he is spending so much time on spec means he believes there is a value proposition, but it send like he is struggling m arguing we need types/specs to do more then say javas, before there worth the cost. I personally do not understand his position of clojure.spec as an alternative to type systems. My view is that spec is a contract system, and both are complementary, not supplementary, mechanisms to enforce correctness.
Honest question, what blew your mind. Feel free to link. I think of tired types as a program to communicate the intent of the actual program I need. If the former mirrors the later it's like a map which is the terrian ... useless. So it must abstract the necessary program in a way that's useful. 
What was shallow about the argument? I think he is just saying that it's not really descriptive enough. 
Thanks! I had `OverloadedStrings` on but the issue was the explicit type. I gave all the string attributes explicit types of `("" :: T.Text)` and it compiled and ran perfectly!
I'm saying that we should be cautious in mistaking correlation with causation. if they are keeping track of something, I would love to see that data! Sorry to put words in your mouth, I like to poke at this topic once a year and see if anyone drops something mind blowing on my lap. 
&gt; I'm saying that we should be cautious in mistaking correlation with causation. Correlation and causation aren't the same thing, certainly. However, if we do have a reasonable theory alongside some evidence (e.g. refactoring tools developed by these companies, which rely on static analysis/type information), I don't think drawing conclusions is an unjustified mental leap. &gt; if they are keeping track of something, I would love to see that data! I'm with you on that :). &gt; Sorry to put words in your mouth, I like to poke at this topic once a year and see if anyone drops something mind blowing on my lap. No worries, it wasn't clear to me what you were trying to get at earlier.
I have seen people make bad arguments for static typing. I don't try to defend those arguments. You shouldn't feel the need to defend bad arguments for dynamic typing. 
One major problem with Trac is that it's extremely vulnerable to spammers and apparently doesn't provide good tools to deal with that. It's also a dead project, I believe.
Hmm, ok? This doesn't feel like it's a continuation of my comment, so not sure how to reply meaningfully.
I did some digging with the help of some friends : looks like gitlab doesn‚Äôt cache the diff between two commits after it‚Äôs requested the first time. Which means it recomputes a merge request diff every page load. And it takes time proportional to size / length of the Change set. It does seem that both client and server side caching are possible for this and perhaps would improve responsiveness. 
&gt; twice the namespace on hackage Well it's not gonna be twice, because many packages still won't need to expose the internal details. But anyway, what's wrong about taking more namespace on Hackage? In what way can this be bad? &gt; twice the version maintenance Actually, not necessarily. Considering the two mentioned packages: "containers" and "containers-core", - if "containers" has a dependency "containers-core &gt;=0.1 &amp;&amp; &lt;0.2", as long as you don't introduce API-breaking changes in "containers-core", you won't have to change anything about the "containers" package. And if you do, well, then imagine what that would have caused to your users if the code was distributed using the "Internals" convention. &gt; twice the headaches I believe things like this mostly depend on the perception. And definitely things like this can be solved by tooling. &gt; users will bend over backwards to figure out how to use just containers-core to get one fewer dependency The point is that the majority of users don't need the internal details of packages, and such users would instead only need to depend on "containers", which would serve the current interface of "containers" sans the internals. As for those few users, building hardcore stuff around that library, will likely only have to depend on the internals package. 
Interesting! so you believe he really doesn't understand this? I often see a lot of counter arguments to richs ideas in r/Haskell, but more often then naught, their is a lengthy argument about the merits of one approach over the other it's rare to see a him straight up get something wrong. it's hard to imagine he is ignorant of the fact as much he slipped up and didnt qualify the statement. 
If you didn't have `OverloadedStrings` on, `""` could only have type `String` and there would be no ambiguity. But the extension it's too useful otherwise, so it pays having to write some annotations here and there.
If the code is highly volatile all the 7 layers of abstractions could be distributed in a single lower-level abstraction library ("lens-core"). An important property of this distinction is that "lens-core" will have a much smaller audience, and hence the API-breaking changes to it will cause a much smaller amount of suffering to the user-base. Also that user-base will itself be more prepared to such changes, since it commits to a low-level internals-like library. All that will give you as the author more freedom to experiment in "lens-core", and will come with the benefits of proper versioning on all levels.
To be honest I thought you were a troll who just wanted to start an argument over something provocative, but your reply seems genuinely confused so I will give a sincere reply instead of a flippant one. My comment wasn't intended to argue for or against static or dynamic types. Instead it was me venting about how Hickey argued, not what he was arguing for. I do like static typing so that comes through in the comment, but I think I could rewrite it from the perspective of a fan of Clojure and still make the same arguments. To me, it seemed like you were trying to argue that dynamic typing should be preferred over static typing while I am trying to argue the Hickey is arguing for dynamic typing badly. That's also why I don't find your point by point response to my comment convincing. You are trying to argue against a point I am not trying to make. To quote another person in the comments "it's many small misrepresentations". If someone messes up once, then I will give them the benefit of the doubt. In not just this video, but other videos as well, he consistently slightly wrong about things. Eventually I stop thinking it is an accident. You asked in a different comment if I really believe he doesn't understand. I don't know for sure. While I don't think he is wrong on purpose, I also think that he does not care about being right. That is also why I brought up the thing about maps and testing. If you say, "doing it like Y in Haskell is bad, instead use X in Clojure" it implies that you can't do X Haskell. Sure, it isn't saying that explicitly, but it does leave the audience with that impression. Any single time it happens it forgivable, but when repeated it is annoying. We also use maps and tests in Haskell, and for the same reason they are used in Clojure, so just saying Clojure has those things isn't an argument for dynamic typing. However, in the talk it comes across that way. Again, I am saying his argument is bad, not that dynamic typing is bad. 
amateur question, but why can't we just check global coherence? Couldn't we just write down all the (used) instances when compiling and then, as the last step, check whether there's a problem? Not having orphans seems to be really restrictive. While they are not ideal, I think they are quite necessary for some situations.
My guess would be that the latter is judgmental equality and the former is propositional equality.
Thanks, I appreciate you taking the time to write this. I'm trying to learn about software, Haskell, etc and I have a modest understanding of clojure so Im trying to frame things from that perspective. I agree his talks/actions have become too defensive, but I'm trying to keep his tone separate from his works and what I can learn from them. I don't want to be fanboy, yet it's nearly impossible to not self identify with your choices. I think he might be saying that in clojure, he has made maps not just the thing you can do ( one of many choices) but really the easiest. I understand your impression better now, but as a "clojure dev", I didn't take away that Haskell couldn't do this, just that it was the idiom defacto way? Like, mostly his talk was about the idea of maybe and his work on making spec more like reusable, and I felt like his explanation and ideas were sound. I think, for me, the tough part of types is that I they're useful for mathematical concepts, but I'm not sure if they help with abstract business logic. Like I leaned that f : [a] -&gt; [a] can't mean sorted because it only can deal with the list. That's really cool, like, that's a good abstraction. Along with the name reverse, I'm fairly confident what it does. Now what about making a hmtl page, does the type sig for a HTML page tell us anything interesting? I really don't know, I can't see how it could, I feel I'm going to have to compile and look at it in the browser. I'm not even sure how to ask the damn question really :). It's like, I keep hearing about how types guarantee something about my program, but I'm struggling to understand what they could be. Specs, to me, are more compelling as a discovery tool then as a type system. Like, spec your system edges and run gen tests to see how your assumptions hold. Putting them a function seems Overkill when the function adds 3 numbers together. I mean, just read the code, its more information then int to int or even describe that the input should be less then the output. Generative testing would point out that ur type/schema is wrong if the number is negative. That's useful, that's like having someone double check your work 
This looks fascinating, but i find videos of talks tiring; is there perhaps a PDF of the talk content?
Yes sure, but my question is why propositional equality is used here specifically.
Ah. I misunderstood you. Sorry
unsolicited tip: watch talks at 1.5x speed -- for me it keeps me on my toes/listening intently and you finish the talk faster. Most talks are ~30-45 mins, and the longer ones are usually the more involved/content-heavy ones (that you want to slow down and listen to).
Yep. And if you miss something, you can just rewind and play that bit at 1x.
I think they get more difficult and time consuming as days progress. Like day3 today.
Thank you for the suggestion. :-) The issue for me, however, is that my sensory system often finds it significantly more challenging to process information aurally than visually, and thus requires more cognitive resources to do so. Combine this with the fact that i have Chronic Fatigue, and it's easy for things like videos and podcasts of talks to overwhelm me when played at normal speed, let alone faster ....
Oooh got it, that would certainly make things hard -- just for my own curiosity, do you find that closed captioning helps or hurts?
Ah ok, I think in that case we're saying the same thing, perhaps then the difference is our interpretation of what Rich is saying: I've interpreted Rich as saying the type signatures don't tell you enough (but not that they don't you anything at all). I'm guessing you've interpreted Rich as saying signatures don't tell you anything?
So is someone working on the Haskell wrapper yet?
Thanks for the correction.
Unless both you and the person you listen to are both right on the things you've made your minds about.
I don't have exact metrics to offer but our work Gitlab instance is resource-hungry. We have about 20 active users and an ongoing continuous integration cycle hitting the server, and a dedicated VM with 2GB of RAM couldn't keep up. We bumped the VM to 8GB of RAM and its performance and uptime have been flawless. &amp;#x200B;
This trick is usually used to work around problems with the unification algorithm that underlies pattern matching. It sometimes gets confused when the indexes of a constructor are too complicated (I don't know the details, if you couldn't tell), and then the trick allows you to do unification 'by hand', manipulating the equality between indexes explicitly. With that said, the index in the example is just a variable, so I don't see why you'd need this trick here.
In a sense of true freedom (without something), you aren't free to pick what you want. Spec is bundled with Clojure, which makes Schema forcefully obsolete.
Essentially the same thing, without the payment (yet.)
Good question! It tends to help, i think, in that my brain switches to working on parsing the visual stream, rather than the aural stream. (i've had this happen when watching things that have people speaking with accents presumed to be difficult for viewers to understand, such that subtitles are provided: i can understand the accent just fine, but i find myself reading the subtitles regardless.)
I'm abroad at the moment with a bad wi-fi connection so I can't ascertain from the video what the meaning of "successor" is in this title - I assume Idris is not deprecated?
Types (especially nominal like in Haskell) let you guarantee behaviors by assigning complicated semantics to names. One common way to do this in Haskell is via the smart constructors pattern. Basically you have a type, e.g. newtype Sorted a = MkSorted [a], and a function, e.g. Ord a =&gt; [a] -&gt; Sorted a, that sorts the input and wraps it. Then you hide the MkSorted constructor and only expose the function. This means a developer most go through the function to create a sorted list and can't construct or modify it any other way. Effectively this guarantees that any place you have a Sorted type you have a sorted list.
We briefly discussed this, my understanding is that in the final migration this will be fixed and you'll get an account with your trac email address so you can request a password reset and use it.
&gt; I assume Idris is not deprecated? It should be backwards compatible IIUC because Blodwen is implemented in current Idris.
[https://github.com/edwinb/Blodwen](https://github.com/edwinb/Blodwen)
Two people upthread have already posted links to two different approaches :) it would be great to get these battle-tested quick though!
I hope Edwin is going to rethinn the Idris records syntax / implementation. It would be awesome if Blodwen had support proper row polymorphism 
The code inference mechanism is similar to the one from theorems for free? 
Thanks for this! I'd been having problems with ligature widths (they were rendering a few pixels wider than they should have been), but by comparing my .emacs with hasklig-mode I managed to fix that. (I'd been composing with non-breaking spaces `#X00a0`, but regular spaces `?\s` work fine.)
FWIW they're implemented using reflection. Structural types introduce major performance issues in Scala. I don't think structural types are underused because they're not useful. I would rather have row polymorphism but they're similarly useful.
&gt;As a point of reference, we saw real slowdowns when using correct-by-construction NonEmpty list vs \[\] even though the cost seems like it'd be so little but alas, benchmarks showed otherwise. That *is* quite surprising! Do you know the root cause of the slowdown?
Well your outlook isn't true for me, I'm not using deps and I'm using lein, for example. If, by forced, you mean he bundled spec with clojure, then sure, but that still doesn't force you to require it or not use schema. I'm actually confused, are you saying you would rather spec not exist and he just come out and say how he things schema is a good idea? If you think there equivalent, then I think your mistaken.
Same. I find people talking to be difficult to understand and tiring to listen to, and even auto-generated Youtube subtitles help some.
&gt; because Blodwen is implemented in current Idris. Being implemented in Idris doesn't make it compatible. I could write a C compiler in Idris; wouldn't make C compatible with Idris :P
Not that `(&gt;&gt;=)` for the list monad is `flip concatMap` and `concatMap f = concat . map f`. So, if you *wanted* to write yours monadically, you get (`pairHead` is unchanged): pairs = tails &gt;=&gt; pairHead (Using `(&gt;=&gt;)` from Control.Monad) (Or, with `do` notation) pairs xs = do ts &lt;- tails xs pairHead ts
so neat! Great project.
I just used a parser on day-3 using \`trifecta\`. My first one apart from the exercises in haskell book. I would have gotten away with using a regex or something, but where's the fun in that? :)
# How do I get started using Haskell with cabal-install? I last used Haskell (with Stack) a couple of years ago or more. I've recently heard that many of the problems that Stack solves for have been addressed in newer versions of `cabal-install` (specifically with the introduction of the `freeze` and `new-` commands.) I'd like to get comfortable using vanilla `cabal-install` (and potentially bump into its limitations) before layering anything else on top. I have three questions: 1. How should I install GHC and `cabal-install` on OSX? 2. What does my typical developer workflow look like with `cabal-install`? What commands am I going to be running to create, build and test my program? 3. What do I need to consider when adding a dependency from Hackage that, unlike on Stackage, won't necessarily be compatible with my other dependencies? I originally posted this to /r/haskell at [https://www.reddit.com/r/haskell/comments/a2pseu/how\_do\_i\_get\_started\_using\_haskell\_with/](https://www.reddit.com/r/haskell/comments/a2pseu/how_do_i_get_started_using_haskell_with/) but it doesn't appear to be showing up.
Naming it Idris 2 implies that Idris 1 is reached the end of the road, no? 
A non GADT way to do this is to parameterize the types: &amp;#x200B; data DayProg a = DayProg { _name :: Text, _run :: Text -&gt; Either ErrMsg a } then in whatever module is running the solution, you just have a function like run :: Show a =&gt; DayProg a -&gt; EitherErrorMsg a
To be self-hosted you'd have to write a third compiler for Blodwen in Blodwen though. That'd be a PITA.
That's my only concern with participating. Once the complexity hits a certain level that, at my skill level, it takes me more than an hour to solve, I can't keep up. I don't have more than a couple hours of free time in my typical day! That's usually when I drop off and give up. I was very much a beginner in Haskell when I decided to do Advent last year but I think that was a mistake in hindsight. The complexity of learning enough of the language to solve the challenges at the same time did teach me things but I couldn't keep up with the pace. This year I'm much more familiar with Haskell and I expect to get a bit further but we'll see. My advice: Advent is the kind of challenge that rewards creative thinking and quick hacks. Nobody is evaluating your code or how you arrived at the answer. If a quick awk script or some regex-fu gets you the answer, whether by an inch or a mile, winning is winning.
I didn't say I wasn't replacing them with something else. =) I just find them to be the wrong tool for the job. The main issue folks have with such a global path is it requires whole-program optimization, so the developer workflow is limited, and it is even more antithetical to things like plugin systems and hot code swapping than the existing Haskell approach.
Thanks for the answer. I am intrigued üòâ 
PLFA is literate Agda.
Have you ever read [Steve Yegge's post](https://plus.google.com/110981030061712822816/posts/KaSKeg4vQtz) about the two different worldviews in software engineering? The longer I'm in this career, the more I find myself agreeing with him. 
Are you still hiring for this position?
Why would it be a PITA?
It's a lot of work. Even more so for a one man operation.
Took more than 10 minutes, but I enjoyed every moment. Thank you for writing it and sharing!
Also in my experience automated refactoring with the type checker is much better. I would hate to have dynamic tests be automatically changed due to refactors in most cases.
He added dependent record updates: https://twitter.com/edwinbrady/status/1063162746164916224 Maybe Twitter would be a good place to suggest row polymorphism, if not a PR to Blodwen's repo :)
In addition to what other's have written, GHC is not keen on inlining list comprehensions. I read this in the GHC source, though I don't recall where.
#haskell is a sanctuary and deserves nothing but solemnity. #haskell-beginners is there for the rest. There's a moment at the steepest point in the learning curve when you stop needing to crack burrito jokes in order to deal with frustration and finally realize how disfunctional that behaviour is. This is the point at which all your dreams become strictly typed and other people lose the ability to comprehend you when you try to explain what the M word means. You now realize that one needs not understand Haskell in order to start using it, but one understands Haskell only when the time for Haskell to be understood comes, which is to say lazily, for in order to understand Haskell, you must first understand Haskell, otherwise = [].
This conversation paraphrased: &gt;It‚Äôs a pain in the ass. - &gt;Why‚Äôs it a pain in the ass? - &gt;It‚Äôs a pain in the ass.
Ok?..
Of course, a lazy map (like Data.HashMap) can also have contain a value that results in an infinite loop when you try to evaluate it.
Would you look at that, I tried again using the Hasklig font and `hasklig-mode` and everything Just Worked (TM)! I remember last time I tried the ligature widths were slightly off which messed up alignment but it looks like something fixed that. I've updated the post as well in case comes across it.
What was the point of this comment? My point is that you haven't answered /u/dwijnand's question at all; in fact, you just repeated the statement that prompted the question in the first place *as the answer to the question*. I don't understand your response to my point. You're saying ‚Äúok?‚Äù as if I've said something obvious. I guess it is obvious, but I thought perhaps you'd missed the fact that your response to /u/dwijnand was contentless, otherwise why would you have made that response?
You can [look at the implementation](https://github.com/edwinb/Blodwen/blob/master/src/Core/AutoSearch.idr). AFAIU it doesn't use parametricity but rather does a bruteforce search based on constructors, pattern-matching, recursive calls &amp; appeals to bound variables. Parametricity however does influence the search: when something is a type variable as opposed to a concrete type you *know* there's no way to pattern-match on it and that cuts down the search space quite a lot.
You can [look at the implementation](https://github.com/edwinb/Blodwen/blob/master/src/TTImp/ExprSearch.idr). It doesn't use parametricity results but if you write a parametric type, that does influence the search: when something is a type variable as opposed to a concrete type you *know* there's no way to pattern-match on it and that cuts down the search space quite a lot.
How is it contentless? Writing twice the same compiler in 2 slightly different language when you're the sole (main) contributor to the language and you have a somewhat independent day job is a herculean task and it is therefore a huge motivation to keep things backwards compatible. I don't get what's hard to understand about that. &gt; I don't understand your response to my point. You posted a crappy meme-like comment contributing nothing to the conversation, I don't see why I'd make any effort answering.
I‚Äôve asked him before and his response was things weren‚Äôt really going to change, which is a shame. Annoying records is one of the reasons I don‚Äôt use Haskell. Note that Row polymorphism is a bit harder in the context of dependent records, fwiw. I‚Äôd be happy just with structural dependent records though.
&gt; Writing twice the same compiler in 2 slightly different languages when you're the sole (main) contributor to the language and you have a somewhat independent day job is a herculean task and it is therefore a huge motivation to keep things backwards compatible. I don't get what's hard to understand about that. Nothing difficult to understand about that at all. But that wasn't what you said. &gt;You posted a crappy [‚Ä¶] comment I admit to that. It's late, and I hadn't appreciated the distinction between ‚Äúpain in the ass‚Äù and ‚Äúhard‚Äù and conflated them, which is why your conversation seemed circular to me when it wasn't. &gt; meme-like comment What, because I used blockquotes? &gt;I don't see why I'd make any effort answering. Yeah, I realise that now. I initially thought your comment contributed nothing to the conversation and explained why I thought so. In reality, it was my comment that didn't contribute, but instead of being kind enough to point that out to me (‚Äúyou've conflated a pain in the ass and a lot of work; something can be a pain in the ass because it's a lot of work, or because it's tedious, etc.‚Äù) like I did to you, you gave a brusque response that could have well been a non-response. I'm going to bed.
&gt; But that wasn't what you said. Ok... let's see: &gt; you'd have to write a third compiler for Blodwen in Blodwen (...) &gt; It's a lot of work. Even more so for a one man operation. And "you gave a brusque response" is quite the ironic statement. Anyway, let's leave it at that.
cool :) 
The only one I haven't tried yet personally is pivotal tracker (partially brought to you by Elm)
Sounds like a hackerrank problem in the making. So bored of the rambling "story" build up on hackerrank
[removed]
Makes perfect sense. Thanks!
I'm deaf, so I just turn the audio off, set it to x2 speed, and turn the captions on. Google's auto generated captions have improved enough in the last year or two that I can finally start watching talks now. Not sure if that'll help or not? It's a shame Google doesn't have the option to auto generate the captions into transcript form; I'd find that much more useful since x2 speed is far too slow for me normally.
I dunno, have you heard of this new language, "Python 3"?
How do you know what the length of the list should be? 
Given a partial function, you can't know what values it is not defined for, without using try-catch (which is probably what you're not looking for). If you know that it is total (i.e. defined for all Ints in this case), then you can get the result using `map f [minBound .. maxBound]` but that will give you an extremely large result, so that is probably not what you're looking for either.
I am trying to serve static (mostly js) files on a Spock application and my js files are not being found when they are requested. I am using [Wai.Middleware.Static](https://hackage.haskell.org/package/wai-middleware-static) and this is how I am calling it in my application. app :: SpockM () MySession MyAppState () app = do middleware (staticPolicy (addBase "static")) get root $ lucid $ do I am not getting a compile time error, however when I access a page that I have a script included, I get the following error in my browser console: Loading failed for the &lt;script&gt; with source ‚Äúhttp://localhost:8080/static/reportGenerate.js‚Äù Here is what my html script tag looks like in lucid: script_ [ src_ ("static/reportGenerate.js" :: T.Text)] $ ("" :: T.Text) And in the browser: &lt;script src="static/reportGenerate.js"&gt;&lt;/script&gt; My undestanding on WAI middleware is that it is supposed to intercept requests to static files and serve them. However this is not happening in my case as per the error in my browser console. /u/roman882 had a [similar question](https://www.reddit.com/r/haskell/comments/698r1n/serving_static_files_css_js_images_in_spock/) and I was not able to get anywhere with the solutions posted in that thread. Any help would be welcome. Thanks!
Anyone try solving Advent of Code #4 using some kind of data frames? My hunch is that using those would be lot more convenient compared to bunch of hashmaps (at least it would be in Python with Pandas). I had a look at the [frames](https://hackage.haskell.org/package/Frames) library but that seemed way too complex and also doesn't seem to have facilities for statistics built in...
Adding to this, I'd say that if you changed the function to be `Int -&gt; Maybe Int` then that would be another way of making it work for all inputs. Then you could implement `function_to_list :: (Int -&gt; Maybe Int) -&gt; [Int]` by starting with `f 0` and going until you hit `Nothing`, building your list on the way (or on the way back).
Even in that case, the type signature cannot guarantee that the input function is total. f :: Int -&gt; Maybe Int f 1 = Nothing
[removed]
Sorry I wasn't clear, I didn't mean that that type on its own would guarantee totality. I just meant that it's possible to write a function with that type signature that is total, unlike the original type signature. This is what I had in mind: import Data.Maybe list_to_function :: [a] -&gt; Int -&gt; Maybe a list_to_function xs = \n -&gt; listToMaybe $ drop n xs function_to_list :: (Int -&gt; Maybe a) -&gt; [a] function_to_list f = go 0 where go n = case f n of Nothing -&gt; [] Just x -&gt; x : go (n + 1) 
All it is missing is a definition of *mapped*.
Yup, and it can be generalized to a composition operator for N-args functions, and that you could write as `.. [...] ..` (2N - 1 dots), its name will also come from the pointfree def ;) Though at this point it has certainly no practical application, a non pointfree expression would be clearer
I do not think you can compare a language no one uses or even heard of to the language so rooted in linux scripting that it took almost a decade to convince people move off it. 
Obviously there are many different functions with signature (Int -&gt; Int) -&gt; [Int] One obvious example is a `const`: function_to_list f = [42, 42, 42] It returns the same list of Int given any input. Another option is to apply the input function to some integer constant, and build some list using the resulting integer: function_to_list f = repeat (f 3) 42 or function_to_list f = repeat 42 (f 5) or function_to_list f = repeat (f 8) (f 13) You _probably_ want a list of "results of applying f to all integers". The way to do it is to use an infinite list: function_to_list f = [f(x) | x &lt;- [0..]] If you run it "as is" it will eventually run out of memory: Prelude&gt; function_to_list f = [f(x) | x &lt;- [0..]] Prelude&gt; function_to_list (*2) [0,2,4,6,8,10,12,14,16,18, .... but you can take arbitrary number of elements, and it will work just fine: Prelude&gt; take 100 $ function_to_list (*2) [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198] 
It takes me at least 7 hours to solve even day 1 stuff which is pretty demoralizing when people solve these things in minutes... and I also just get demotivated as I cant keep up. Last year I think I got up to day 7 and quit due to this. Sometimes its due to the wording not being clear at all in what it wants which gets annoying. I agree that these challenges just require hacks and lots of time spent parsing files and mangling data before getting on to solving the thing. I've reviewed a few peoples haskell solutions this year and I am amazed at the creativity in them. I could never come up with the solutions myself but am inspired. I don't mind solving 1 or 2 out of the 25 days but its mentally exhausting when you cant solve them quickly due to just not 'getting it' and having to learn a new language. 
\&gt; There are other benefits as well: with a 2D universe, benchmarks indicate a speed increase of up to 2x! Could you provide the code, please? \&gt; Although that's probably because newtypes can now be used Not sure what you meant by this. The two implementations you provided are inherently different (most `shiftLeft` calls do not require calculating the length, while `peek` does).
Well for one thing, both `peek` and the functions users pass to `evolve` have to be total in `p`, which seems like a significant obstacle. Your code, for instance, has a *lot* of bottoms with `p`s outside the happy range. This was true in your first version as well, but it's avoided by using the `data Strip = Strip [a] a [a]` encoding.
And to throw one more: http://hackage.haskell.org/package/aws-lambda-runtime Running in "production" for few hours now, seems to work :)
I think it's useful to leave a heads up for legibility as your code is very confusing to read and reason about. I understand it's probably due to being an example, but still: `list_to_function x = \n -&gt; x!!n` is the same as `list_to_function x n = x!!n` is the same as `list_to_function = (!!)` So by "list to function" you are really saying "morphism from list (of Ints to Int in your example)". to clarify, if `f x = \y -&gt; g x y` then `f = g` So what you've got in the general case is :: t a -&gt; a as a morphism and you want the inverse so that would have to be :: a -&gt; t a and hopefully you can see where the problem is if your function is not bijective. Hope that helps.
Don't feel bad about yourself. I was in the same place as you are not two years ago. Eventually you do a bunch of exercises and challenges and start seeing repeating patterns. It's obviously a lot easier to glue a couple of familiar patterns together than it is to come up with a solution all by yourself on your first try. That being said, I'm more used to writing imperative code, so my Haskell is still very noobish. I'm using list comprehensions and recursion where other people use applicative style or else do some crazy stuff that I can't even comprehend. But it's a great way to improve: I write my own super na√Øve solution, and then check other people's solutions to see how a more experienced Haskeller would approach the same problem.
[removed]
[removed]
I think this is actually quite complicated with dependent records. Dependent records function more like "telescopes" so that later fields can refer to earlier ones in their types. This is good for defining, say, an algebraic structure: the first field is the carrier (a type), then some operations on that type, then some equalities which refer to those operations. But when things are no longer conceptually just a finite map of names to types row polymorphism is more complex.
Hi rebel, thanks for the link! I hadn't ready that particular post, now reading. A non technical person once asked why we have so many different programming languages, I wasn't really prepared to answer that, and to be honest I don't think I have a good answer, reflecting on it now, I think its for various reasons but mostly its a "thought shoe that fits our mind the closest"
A few hours from where I live, of all places in the world. Thanks for posting this, I'll probably go! Also, I used to study there as a cadet. Was an interesting time. 
Watching the video made me think it is very similar to [Polymorphic Variants in OCaml](http://dev.realworldocaml.org/variants.html#scrollNav-4). Help me out here. Is there any connection?
&gt; Nowadays, the standard way of simulating cellular automata in Haskell uses comonads: Yea, if you're writing a blog post! Your type class is a `(p -&gt;)`-relative comonad: https://arxiv.org/abs/1412.7148
I think you already know what the catch is, don't you? You were careful to use a _toroidal_ strip as your example, because if your 1D universe is bounded at the ends, it's no longer always possible to lookup `p` positions away from the current position. In a comonad, you don't just have access to your position's value and to the value of your neighbours, you can also seethe shape of the world around you, so if you're at the left boundary, you'd be able to see that you don't have any neighbours to your left. A function from `p` to `a` doesn't allow you to express this subtlety.
The laws of Comonad are: extend extract = id extract . extend f = f extend f . extend g = extend (f . extend g) So I'm guessing the CA are as follows? evolve peek = id peek p . evolve f = f p evolve f . evolve g = evolve (\p -&gt; f p . evolve g)
I am merely the poster, Matt Campbell is the author: https://www.stackbuilders.com/news/author/matt-campbell
its a tad more involved than just "cache" diffs, cause of how that whole subsystem works, but sure :) 
&amp;#x200B;
We could have non-dependent but tail-polymorphic rows, and then dependent records. (And their intersection would be non-dependent records.)
Because Haskell wasn't confusing enough
&gt;A few hours from where I live, of all places in the world. Thanks for posting this, I'll probably go! FYI, the NL FP day has been an annual tradition for years now :) It's a nice meet up to network and meet people around the NL area. 
Is the Amsterdam meetup still happening? I used to go there during the 4 years I lived in the NL.
Luckily the program will be in English, the speakers will keep their impossible to pronounce names though!
I haven't gone to the fp meet up in Amsterdam in ages, but I saw there's a CT study group affiliated with it, so I guess so. The NL-FP day generally draws a bigger crowd and is a bit more academic in terms of content. Also slightly more focus on Haskell (and Clean, when their group shows up). Whereas the Amsterdam FP meet up has (had?) a bit more Scala/Erlang/Elixir and similar slightly more commercially popular languages. 
FWIW, it was still active a couple of years ago, when I was living there.
You are so fixed in defending static types and Haskell that most of you don't even care about the underlying problem of managing data and making the program flexible enough to cope with the changes in the data definition. It's been a long time since relational key-indirected organization won the match against the direct pointed, hierarchically organized not only in the disk drive but in processing memory. Not only in the case of more or less ordinary processing but also for scientific purposes. And yet the Haskell world has never noticed and still tries to model his data as trees using nested records. That is an anti-pattern. This is because most of the Haskellers are hobbysts or comes from the Academic world and cares little or nothing about the representation of their data and his evolvability or never had the need to think hard about it. program aesthetic is the only thing that matters.
You are so fixed in defending static types and Haskell that most of you don't even realize the underlying problem of managing data and making the program flexible enough to cope in the medium-long term with the changes in the data definition. It's been a long time since relational key-indirected organization of the data won the match against the direct pointed, hierarchically organized. Not only in the disk drive but in processing memory. Not only in the case of more or less ordinary processing but also for scientific purposes. And yet the Haskell world has never noticed. Still tries to model his data as trees using nested records. That is an anti-pattern. This is because most of the Haskellers are either hobbysts or they come from the academic world and cares little or nothing about the representation of their data and his evolvability or never had the need to think hard about it. program aesthetic is the only thing that matters.
`peek n (Strip as) = as !! (n \`mod\` (length as - 1))` Why do you subtract one? That's off-by-one error, maybe?
Spoilers and all. &gt; I also thought that declaring a data structure with a strict field would make the constructor strict as well. It isn‚Äôt! But the constructor considered as a function is strict, isn't it? When you tried to evaluate the applied constructor to WHNF, it tried to evaluate the strict field as well. If you put some similar strict function there it would behave the same. 
Seems my interlocutor has deleted their contribution to this comment chain (including one more response to the parent of this comment). I'm going to keep mine up as I don't delete my mistakes. I was not 100% wrong, but I was 100% *in* the wrong and my opponent was 100% right. I'm gonna try to avoid commenting late at night, as I'm obviously more irritable then (on top of having a less accurate perception of reality while being more certain of that perception). None of these changes are large, but they add up.
Here's the output (spoilers): &gt;! Log line: 1 Log line: 2 evaluating b Log line: 3 Log line: 4 evaluating a 3 Log line: 5 3 Log line: 6 evaluating strict Log line: 7 evaluating lazy 21
I think there's a typo - print a logLine print a the second one should be `b`. Also, prompting someone with "can you get it right? (I didn't)" makes it more likely that the unaware reader might think there's something surprising going on :P.
I think it's less tricky if you remember that "when the record is constructed" doesn't make any sense as a time. The only thing that has a specific temporal location is when the record is evaluated. A strict field in a record says "if you ever resolve the constructor, evaluate me as well". Like mentioned earlier, this is the same as any function with "strict parameters". If `myFunction x y` is strict on `x`, it doesn't mean to evaluate x whenever `myFunction a b` is given a name. It means that, if you ever evaluate the result of `myFunction a b`, also evaluate `a` as well.
You can indicate spoilers using `&gt;!!&lt;`.
I think that's fair. I've had similar experiences. It's kind of interesting though because I when I use Haskell at work or on my real-world projects I find it is an indispensable tool. It helps me get to a reasonably correct working first version much faster than I could in a dynamic language. And yet it seems to me to be much harder to write quick-and-dirty Haskell, for the beginner-to-intermediate Haskell programmer, than it is to write robust, maintainable Haskell if these Advent challenges are any indication of that. Which is probably exactly what you would want in a good programming language. Keep at it and enjoy the challenge in whichever way works best. Avoid comparing yourself to others... it is very demotivating, I agree. Just work on the ones that interest you and who cares if you don't solve them all in record time.
Interestingly, I got foo right, but b wrong. I thought b wouldn't get evaluated at all. Why did it? I guess I don't understand the semantics of bang in a let. For foo the semantics I thought of were "strings are attached between the 'strict' field an the Foo constructor", therefore strict gets evaluated as soon as Foo is. I guess for bang-lets, it attaches the defined value to the in-expression? Seemed confusing to not really have one. Hmm this was more of a written out thought process than a comment, maybe its helpful for someone.
Thought that's what I did and it's showing up correctly for me, is it not for you?
Ooh, this is fun! Here's my guess, before reading any further on the post or reading any of the comments here Log line: 1 Log line: 2 evaluating b Log line: 3 Log line: 4 evaluating a 3 Log line: 5 3 Log line: 6 evaluating strict Log line: 7 evaluating lazy 21 
Will it use ‚Äúlinearity on the arrow‚Äù, as is being implemented in GHC, or ‚Äúlinearity on the kind‚Äù?
Huh, it shows up as quoted text for me. Have you used the closing !&lt;? On old reddit, your thing displays as a spoiler but not on new reddit.
Indeed. `let a = exp in body` *always* evaluates `body` first regardless of what `exp` is!
Seems not to work for code blocks :( &gt;! Hello &lt;!
Right: let !x = blah in bleep bloop is the same as: let x = blah in x `seq` bleep boop
&gt; It means that, if you ever evaluate the result of `myFunction a b`, also evaluate `a` as well. Doesn't it mean that, in order to evaluate `myFunction a b`, `a` must _first_ be evaluated to WHNF? The way you've worded it, it sounds like a `seq`-like guarantee ("if `myFunction a b` is evaluated to WHNF, then `a` will be also", which is not as strong), rather than a `pseq`-like one.
Even more surprising is that when you compile it with `-O2`, GHC doesn't get rid of that call despite `b` being an unused variable. Seems very odd...
Thanks, you've made me rethink things a bit. AFAIK saying that `f x` is strict on `x` means only that f _|_ = _|_ so this does imply that x must be whnf before f is whnf. Thanks for the clarification.
No - the index of the first element is `0`, so the index of the last element is `length as - 1`. It's the same principle as implementing something like `last xs = xs !! (length xs - 1)`.
Well, it could be explained by the comment. In that case its used as the first parameter to seq
Ah, I should've reloaded the page before I started writing my comment :-/.
[removed]
Oh, interesting. I think if "f x is strict in x" means only that f _|_ = _|_ then your first statement sounds correct to me. So now I wonder if operationally, f !x = unsafePerformIO launchMissiles is actually required not to launch missiles if `x = _|_`.
I thought that was a little odd, but wondered if it might be intentional to showcase that `a` was still only evaluated once.
[removed]
Looks like linearity on the kind, from the demo. Although, the guarantee is closely related to the guarantee provided by GHCs linearity on the arrow.
Hm, yeah. I guess if you think of `seq` as the function: seq _|_ x = _|_ then saying f is strict on x is basically a re-statement of `seq`'s promises, right?
[removed]
Missing rewrite rules?
I thought so too initially but there was no discussion of a or b later, so I thought maybe it's a mistake?
&gt; Is there a way to somehow traverse this function and know the domain? If you have a real function[1], then the domain is determined by the type: `Int -&gt; a` has a domain of `Int`; `(Int -&gt; Int) -&gt; Int` has a domain of `Int -&gt; Int`. Most Haskell types are not `Bounded` AND `Enum`erable, so it's not possible to generate all the values. That said you might look at some of the hackage packages around memoization. [1] A function is a process or a relation that associates each element x of a set X, the domain of the function, to a single element y of another set Y (possibly the same set), the codomain of the function. -- https://en.wikipedia.org/wiki/Function_(mathematics)
In case anyone else was worried, this doesn't impact "x is unused" warnings.
I counted, took me nearly an hour to make sure I understood it :D
Yeah, I'm using old reddit - /u/tomejaguar was right, doesn't work for code blocks :( fixed now I think, thanks
They do work, although IMO they are an extremely weak approximation of what you can do with real row/record/variant types.
&gt; It's been a long time since relational key-indirected organization of the data won the match against the direct pointed, hierarchically organized. Not even remotely true, how little real world experience do you have to have to unironically believe that?
I need to put a comment that `print a` is intentional there! You're the third to think that it is a mistake on my part :)
I wondered the same thing. Also, is there a breaking typo in the use of `&lt;&gt;`, or is this ghci version-dependent? My ghci (8.2.2) gets a type error on `&lt;&gt;`; I first assumed I was just out-of-date or something, but the only results Hoogle (online version) returns for `&lt;&gt;` are of type `Doc -&gt; Doc -&gt; Doc`, which doesn‚Äôt match up with the types Hoogle returns for `putStrLn`. In any case, changing `&lt;&gt;` to `++` fixed the type error for me, but I‚Äôm intrigued to know if the version with `&lt;&gt;` does work for newer `ghci` versions (and if so, how this squares with the Hoogle results).
&gt; Not even remotely true Have you ever lately take a look at real world languages like Java/Spring for example? &gt; how little real world experience do you have to have to unironically believe that? I have more than 30 years of experience working in private companies developing software for almost every sector. And you?
`(&lt;&gt;)` used to be `mappend` for Monoids, though now `mappend` is `(&lt;&gt;)` which is part of Semigroup (now a superclass of Monoid). I want to say that `(&lt;&gt;)` as an alias for `mappend` was in by 8.0, at the latest, and the Semigroup machinery came in with base-4.11, which would be 8.4.
Follow-up riddle: Does `{-# LANGUAGE NoMonomorphismRestriction #-}` change the output in any way? If so, how and why?
In the base alongside GHC 8.4.x, Semigroup is re-exported from the Prelude, so [`(&lt;&gt;)`](https://www.stackage.org/haddock/lts-12.21/base-4.11.1.0/Prelude.html#v:-60--62-) is automatically available. In the base alongside GHC 8.2.x, [`(&lt;&gt;) = mappend`](https://www.stackage.org/haddock/lts-11.22/base-4.10.1.0/Data-Monoid.html#v:-60--62-) is present in Data.Monoid but not re-exported from Prelude.
 % ghci Quiz.hs GHCi, version 8.0.1: http://www.haskell.org/ghc/ :? for help [1 of 1] Compiling Quiz ( Quiz.hs, interpreted ) Quiz.hs:27:29: error: ‚Ä¢ Variable not in scope: (&lt;&gt;) :: IO () -&gt; String -&gt; IO () ‚Ä¢ Perhaps you meant one of these: ‚Äò&lt;*&gt;‚Äô (imported from Prelude), ‚Äò*&gt;‚Äô (imported from Prelude), ‚Äò&gt;&gt;‚Äô (imported from Prelude) Failed, modules loaded: none.
&gt; Interestingly, I got foo right, but b wrong. I thought b wouldn't get evaluated at all. Why did it? I guess I don't understand the semantics of bang in a let. Same here. Got everything right except `b`, and I figured it wouldn't be evaluated. I'm betting the desugaring is to a strict lambda form, and we know that lambda gets evaluated (and thus `b` does) if execution gets to the right of the let.
Another helpful way to anticipate this: think of "code motion" as in "GHC is free to move that let around". E.g. in this example, it should inline/unfold `foo`, especially since there's only occurrence. That'd be a good exercise in a book/tutorial!
This works with ghci version 8.4.3. Try import Data.Monoid for older versions.
What the heck?!? I was feeling pretty good about getting the original quiz right, but this one is stumping me.
Playing a little golf, maybe this: &gt; Prelude&gt; [[a,b] | a &lt;- [1..4], b &lt;- [a+1..4]] &gt; [[1,2],[1,3],[1,4],[2,3],[2,4],[3,4]] ? 
Disabling the monomorphism restriction makes `a` and `b` have type `Num a =&gt; a`, which at the end of the day is *actually* a function `NumDict a -&gt; a` let a = \ndict -&gt; trace "evaluating a" $ fromInteger 1 + fromInteger 2 ... let !b = \ndict -&gt; trace "evaluating b" $ fromInteger 2 + fromInteger 4 So evaluating `a` or `b` to WHNF actually just evaluates a lambda to WHNF, not any actual number. Thus the trace is applied to a new thunk each time the lambda is called, but not the lambda itself. Since `b` is never used, it is never called, so it never has the changes to make a `trace` thunk. Since `a` is used twice, its lambda is called twice, creating two `trace` thunks.
&gt; Disabling the monomorphism restriction makes `a` and `b` have type `Num a =&gt; a`, which at the end of the day is actually a function `NumDict a -&gt; a` Hmm. Thanks for the explanation, and that makes sense. I wish I knew an explanation that didn't make reference to implementation details, though. I can't shake the feeling that one shouldn't have to think about how GHC implements type classes in order to understand observable strictness behaviors.
The intuition without implementation details would be "you can't evaluate `b` without picking a type for it, so you have to reevaluate it every time you use it; caching on a per-type basis would be arbitrary and nonsensical, so it should be every time."
This is good for incrementing lists of numbers but wouldn't work for any thing else I don't think
Haskell is nonstrict not lazy. Laziness is an implementation detail. There's no reason a conforming implementation couldn't eagerly evaluate stuff ahead of time as long as it makes sure not to choke on infinite loops or other errors when a nonstrict implementation shouldn't. Basically any implementation *could* automatically insert `par` where-ever except that'd be slow in practise.
&gt; Haskell is nonstrict not lazy. More relevantly, though, GHC _is_ lazy. If you want to check what other compilers do (both historical, like Hugs and JHC, and weird modern ones, like GHCJS and Eta), that could be a cool comparison.
I used a slightly different trick when writing the [third entry](https://www.schoolofhaskell.com/user/edwardk/cellular-automata/part-3) in the cellular automata series on the School of Haskell. There I used a comonad for 'relative' reads, and embedded it into the larger store. This allowed me to know that the rules of the universe don't change based on where you are, merely the manifold you're in might be warped. This seems largely orthogonal to your indexless encoding here. This here reminds me a lot of "plugging" one hole contexts, or indexed containers. You're just slightly less dependent than the latter would require.
I‚Äôve posted this ad a few times before. Big updates this time: we just got our first three customers, and we‚Äôre also offering a 10K referral bonus for intros that lead to full-time hires.
New to haskell. Trying to understand why div is not defined as Maybe Integral or something to cover the 1 ‚Äòdiv‚Äô 0 case. Wouldn‚Äôt it be more correct to be defined as a Maybe Integral leading to compile time type checking instead of leading to runtime errors?
I did look at your tutorial - it was really helpful, so thank you! I particularly found the first part very useful - the original version of my program used the `Store` comonad. The reason I didn't use your relative reading comonad was that I *wanted* to be as general as possible, including absolute positioning if it is required (e.g. for [block CAs](https://en.wikipedia.org/wiki/Block_cellular_automaton)). &gt; This here reminds me a lot of "plugging" one hole contexts, or indexed containers. You're just slightly less dependent than the latter would require. What's an indexed container? Does this have anything to do with [`Control.Lens.Indexed.Indexable`](http://hackage.haskell.org/package/lens-4.17/docs/Control-Lens-Indexed.html)?
A block cellular automaton or partitioning cellular automaton is a special kind of cellular automaton in which the lattice of cells is divided into non-overlapping blocks (with different partitions at different time steps) and the transition rule is applied to a whole block at a time rather than a single cell. Block cellular automata are useful for simulations of physical quantities, because it is straightforward to choose transition rules that obey physical constraints such as reversibility and conservation laws.
Well, the behavior of `trace` is also an implementation detail. AFAIK, according to the Haskell report, there should be no observable difference in the strictness behavior of any program when the monomorphism restriction is disabled. `trace` breaks that rule. It exposes the internal evaluation strategy of GHC, so you can expect it to highlight other implementation specific quirks.
I like the [explanation given in the Report](https://www.haskell.org/onlinereport/haskell2010/haskellch4.html#x10-680004.2). It says that strict fields are not a property of the data type, but of the constructor. data T = T !Int Has the same underlying runtime representation as (assuming no unpacking) data TL = TL Int except every occurrence of `T x` is automatically replaced with ``x `seq` T x``. You can then reason about ``x `seq` T x`` as usual: it is an expression with the value of `T x`, except evaluating it also evaluates `x`. Similarly, the "`newtype`-ness" of a type is attached to the constructor as well, and not the type.
My intuition with optimizations was [this](https://gist.github.com/Tarmean/9b89eaf4d380af61a6bd78ab0ce900e6) which matches the behavior. I wasn't sure whether ghci would show the same behavior, though. In retrospective this makes sense - even if not inlined foo is a thunk that looks like foo_ru8 foo_ru8 = case trace (unpackCString# "evaluating strict"#) (+ $fNumInt (+ $fNumInt (+ $fNumInt (I# 1#) (I# 2#)) (I# 3#)) (I# 4#)) of dt_X1jt { I# ipv_s1Sx -&gt; Foo dt_X1jt (trace (unpackCString# "evaluating lazy"#) (+ $fNumInt (I# 9#) (I# 12#))) } 
And let !a = exp in body only guarantees that a will be evaluated once body is evaluated.
No, you were right. Think of: bar :: Bool -&gt; Int -&gt; Int bar !a !b = b In core this looks like -- RHS size: {terms: 6, types: 3, coercions: 0, joins: 0/0} bar bar = \ a_a1kV b_a1kW -&gt; case a_a1kV of { __DEFAULT -&gt; b_a1kW } because if the result is evaluated b is evaluated we don't have to force b ourselves. So only a is evaluated first.
Is not required, this is called [imprecise exceptions](https://www.microsoft.com/en-us/research/publication/a-semantics-for-imprecise-exceptions/)!
 let y = { x: x is not in x } I don‚Äôt quite follow what‚Äôs wrong with this. Do you mean ‚Äúx is not in y‚Äù? For example, for x={1,2,3} x would be in y since the set x isn‚Äôt contained within x (x just contains integers).
Watching the video made me think it is very similar to [Polymorphic Variants in OCaml](http://dev.realworldocaml.org/variants.html#scrollNav-4). Help me out here. Is there any connection?
Hello, Relocation is possible? Contractor also?
Is that position expected to cover all those areas? If not, are there frontend positions?
Rick is mostly right IMHO. Most answers here are so concentrated in defending the Haskell stronghold against the evils of the world that they don't even realize the underlying problem of managing data and making the program flexible enough to cope in the medium-long term with the changes in the data definition. It's been a long time since relational key-indirected organization of the data won the match against the direct pointed, hierarchically organized. Not only in the disk drive but in processing memory too. Not only in the case of more or less ordinary processing but also for scientific purposes. And yet the Haskell world has never noticed. Still tries to model his data as trees using nested records. That is an anti-pattern. This is because most of the Haskellers are either hobbysts or they come from the academic world and cares little or nothing about the representation of their data and his evolvability or never had the need to think hard about it. The only things that matters is program aesthetics or speed.
I didn't do the quiz because, you know, lazy.
I guess having written about 4 Haskell compilers puts me at an unfair advantage, but that quiz wasn‚Äôt difficult. :)
Different notion of 'indexed' (Don't worry, there are like 5 such notions in common use.) =) Indexed containers involve breaking apart the 'shape' of your container from the data that it stores. Basically you pick a 'shape' and then for any path that actually fits in that shape, you assign a value. The difference between what they do and what you have here is that they use a dependent type so that only legal positions inside the actual shape that is present can be used to index. https://github.com/agda/agda-stdlib/blob/master/src/Data/Container/Core.agda#L19
I seem to be in the small camp of people who love Pivotal Tracker. I feel it does a great job of making you honest about what is realistically going to be fixed, and the pace of your work - but it does require some estimation work.
I see what you're saying. My only concern is that distinguishing dependent from non-dependent records is actually a little fidgety (the syntactic check is brittle). I'm sure it's fixable but it would require some careful thought.
On the trend of increased monitor size &amp; resolution, say over the last 15 years, it gives us coding space for more opened windows rather than wider lines. IMHO, the established 80 chars is not a technical limit but a human one.
On the trend of increased monitor size &amp; resolution, say over the last 15 years, it gives us coding space for more opened windows rather than wider lines. IMHO, the established 80 chars is not a technical limit but a human one.
I clearly understand your current view on this, as I had the same in the past, and it took about 5 years of coding to come to the established conclusion.
AIUI, nothing changes. `foo` becomes a lambda of type `NumDict a -&gt; Foo a`, but it's only evaluated once, as are its fields.
An in-memory updatable map of key-flat records which can have fields like this: type Key= FastString data RecordRef a= RecordRef Key (IORef (Maybe (WeakPointer a))) may be the best way to have pointer as well as key indirection. if there is no such pointer, the program can look for the key in the map and update the pointer, so the next time the dereferencing may be trough the pointer. If the record is deleted from the map, te weak pointer eliminates the pointer reference. 
I think 15-20 minutes for each of the first two days. Day 4 took me 22min for the first star, 27 min for the second - that's the only day I've started within minutes of the problem opening. All other days were 15hrs or more before I started them. Unlike Euler, I don't go for elegant and efficient for the first pass of an AoC problem. The input sizes are rarely so big that a quadratic but simple solution fails. I just tackled day 5 - seven minutes for first star, and a further three minutes for second star. I think this is the easiest problem of the set so far. Previous years have had some stumpers that took more thought and time.
In general Haskell does not guarantee totality. In practice though using only total functions/values is usually a good idea. The standard libraries include many partial functions for historical reasons. You can of roll your own safe division operators like this: [http://hackage.haskell.org/package/ideas-1.0/src/src/Domain/Math/Safe.hs](http://hackage.haskell.org/package/ideas-1.0/src/src/Domain/Math/Safe.hs) &amp;#x200B; See also: [https://en.wikipedia.org/wiki/Total\_functional\_programming](https://en.wikipedia.org/wiki/Total_functional_programming) [https://adamdrake.com/are-your-functions-total.html](https://adamdrake.com/are-your-functions-total.html) [https://wiki.haskell.org/Avoiding\_partial\_functions](https://wiki.haskell.org/Avoiding_partial_functions)
You're right that it would be more correct. (The same applies to `/`.) I suspect it's partial for reasons of convenience, and because when Haskell was initially designed the designers weren't trying all that hard to avoid partial functions.
&gt; I could never come up with the solutions myself but am inspired. No-one was born knowing how to make those solutions!
Regex is slower than slapping together a Parsec parser for me now.
This sort of operation makes sense with a mutable data structure, where you can walk the structure, find the key isn't there and then insert at whichever node you've found. But for an immutable data structure lookup and insert traverse the data structure in different ways (insert creating new nodes down the spine). So I don't think you'd get much performance benefit over lookup + insert.
Hoogle4 (at [http://www.haskell.org/hoogle](http://www.haskell.org/hoogle)) works off of a fairly old package index, iirc. Hoogle5 (at [http://hoogle.haskell.org](http://hoogle.haskell.org)) keeps an up-to-date index: [https://hoogle.haskell.org/?hoogle=%3C%3E](https://hoogle.haskell.org/?hoogle=%3C%3E) It used to be that type search did not work for Hoogle5, but I've been trying to hack on it recently and it is improving. Still not as good as Hoogle4 in some dimensions, but I'm chipping away at it.
First program I wrote (I was child) using PL/I. It was written for very old computer with punchcards. So I had very hard limit. Then, I still was a kid, tried to write something on PDP-11 like big mainframe (it was in USSR, so it was copy from US computers). And it still had limits for number of characters in line. A lot later I was keeping this limit in my code, because sometimes you was need, you know, to change something right on production server. Through primitive VT100 terminal. But I haven't change code on production servers for several years. And I hope, I don't need it. And, as you know, now you can have more then 80 characters even in terminal. So if you have one logic sentence and it's slightly longer than 80 characters, it's still better not to break it. But for this project, I know, I have lines that are too long even for me. :) So I'm agree I need to fix it when I have time.
I got `a` wrong: forgot that GHC does a bit of memoization (so `a` doesn't get evaluated twice) Also got `Foo` wrong, mostly because I forgot that even though the field is strict, the variable that was holding it `let foo` is still lazy. And also, case matching evaluates strict fields (which to this day is something that confuses me). 
They ARE implemented with reflection, which is not necessarily a bad thig, but their performance problems are actually bogus: http://www.csc.kth.se/~phaller/doc/karlsson-haller18-scala.pdf benchmarks here show Scala's native structural types as generally the fastest compared to other takes on row polymorphism in Scala, esp. shapeless' list records. &gt; I don't think structural types are underused because they're not useful. I think the `scala.language.reflectiveCalls` flag was a major mistake that shot them in the foot, they're not even nearly as slow as the flag implies them to be and it makes people try to avoid them at all cost, unnecessarily.
It's not the Comonad class itself which allows this, it's the extra operations provided by each individual Comonad instance. For example, you wrote `shiftLeft` and `shiftRight`, which are operations specific to toroidal lists; similarly, in a `ListZipper [a] a [a]` you can see whether the left and right lists are empty, indicating that you are at the left or the right end of the list. I said that "a function from `p` to `a` doesn't allow you to express this subtlety", but I now realize that this was not a very useful criticism. Those shape-observing functions can't be expressed in terms of the Comonad typeclass, and so I shouldn't fault CA for not being able to express those shape-observing functions either. I now think your CA typeclass is fine (it has laws, so it can't be that bad!), it's just not as general as Comonad since it can only have non-trivial instances for infinite or toroidal Comonads. So, it's a subclass of Comonad, and that sounds useful!
If it's toroidal list of length `n`, then I don't think the constraint should be that `p` must be in bounds, but that the type of `p` must be "the quotient type Nat modulo `n`", that is, a type for which functions are not allowed to distinguish between `p` and `p + n`. Unfortunately Haskell doesn't support quotient types (and I don't know any language which does).
Unfortunately, the interaction between `IO` (and `ST`) and strictness analysis in the GHC optimizer is a bit unprincipled. One generally expects that `m &gt;&gt;= f` is strict in `m` but lazy in `f`. But GHC will sometimes pretend that it's strict in `f`, which can sometimes lead to things being forced earlier than one would expect.
It would be an fruitful project to understand how what you did is an instance of that paper's concept.
I think he said it uses "quantative type theory" which if I remember right is a linearity on the arrows scheme.
This reminds me of my organic chemistry class -- everyone complained about unfair exams (the test average was 45/100), and the professor responded: &gt; I don't know what y'all are complaining about -- I took it and I only missed one question!
I feel like the trend is to focus on zero friction, maximum UI familiarity for largest audience, rather than features of an issue tracker. I am not sure where I stand personally, not sure if I care enough to have a stance.
Well, we could shift the responsibility to the programmer, at least until we had a good check on the compiler side. Have all row literals / types have a `|` near the end to signify the polymorphic tail or something like that. Once we figure out the best place to determine is a record type/literal is dependent, then we can figure out how to unify non-dependent records and their row types.
IMO, what you really want is a != 0 constraint on the divisor. You can get that with Liquid Haskell or with dependent types, but you can't really get that with "just" System F_c. Using a Maybe for the return value would encourage programmers to `fromJust` (or equivalent) when *they* know that a divisor can't be zero, even if the type system doesn't let them tell the compiler that. Then later, when the surrounding context changes, suddenly the divisor *can* be zero, but the compiler doesn't know it was supposed to check that, and the human programmer made a common mistake.
Can we get an estimate before submitting of about how many of the presenters this time are going to be fascists and/or white supremacists? That influences my decision about whether or not to submit a paper. Thanks! (I was banned from /r/functionalprogramming for asking this question there, but given lambdaconf's troubling history it's a serious question and deserves a serious answer.)
Yes to relocation, but not contractors at this time
You don't have to cover all those roles! We have two engineers doing just Typescript right now, 2 who are flexible with front/backend, and 2 who are just backend. We do appreciate people who can or are interested in working across the stack, but you certainly don't have to start out that way
Oops, got so excited at seeing the input/output code that I skipped the prose. Never mind!
Probably Approximately Correct (PAC) learning is not far from what you're describing, though it's probably too general a framework, as there are some pretty bad impossibility theorems about what kinds of models are PAC learnable (IIRC there's some restriction on the VC dimension of your model space). I suspect this result doesn't really have teeth, for the same reason that Rice's theorem doesn't stop people from writing program analyzers, but it does mean that there's plenty of theoretical work left to be done in verifying neural networks.
Wow. I will always remember the lecture he held where he started tearing apart a phonebook to describe searching in a sorted list via recursion. Fantastic professor and well deserved!
I'm away from computer -- but I was thinking defaulting to `Integer` would render this moot. Am I missing something?
Note that, in the meantime, I've switched to working in the opposite category. Then the 'theory morphisms' are just display maps (up to permutation), which are very well behaved indeed. The common base then just becomes 'initial segment'. Of course, if you work extensionally, you can use extensional equality for checking that the initial segments are equal, which could indeed help. Then theory combination is done via pullback. What you get from this is that what syntactically looks like a theory extension is just a display map in the opposite category. And because the category of contexts for nice enough lambda calculi is fibered (via the codomain fibration), all the pullbacks based on display maps exist, trivially. As a bonus, you get that combinations of extensions with arbitrary morphisms exist, also for free. Only when you try to combine two arbitrary morphisms do you actually get real proof obligations to discharge. So it really pays off to distinguish certain 'extensions' from others.
I was sticking to the original category sot hat I used pushouts, mainly because it seemed to correspond nicely to the whole adjunction between syntax and semantics. On the left I have a syntactic theory and an adjunction to my category of models, which takes everything there to (morallly) Set^op, so that each refinement on the left corresponds to a forgetful mapping between inhabitants on the right. This let this feel like the usual story, and helped motivate why I wanted a thin category of constraints. (So that it was a proper galois connection and not just an arbitrary adjunction.)
Man seeing term synthesis all wired up and integrated like that is so awesome. The short-term future of functional programming is looking really bright right now.
Very curious about the details. I was only convinced to switch to the opposite category because of the tight link with the category of contexts, which is already present in PLs, unlike the category of theories, which isn't. In your case, if you switched things around, you might end up with an adjunction on the other side to Set; wouldn't that be nicer? I do understand that mirroring the syntax-semantics adjunction is very important. But it already involves some contravariance, so maybe there is some wiggle room in there to make things even more elegant.
Not sure either way. I'm kind of used to seeing my model category as an opposite category and I already use Set^op to talk about complete atomic boolean algebras, so it isn't that strange a place to me. ;)
Honestly, most of that works in Idris, albeit sometimes with more steps. The linearity is new, and nice. Between ["macros" written using reflective elaboration](http://docs.idris-lang.org/en/latest/reference/elaborator-reflection.html) and [custom "interactive editor commands"](https://icfp18.sigplan.org/event/tyde-2018-extensible-type-directed-editing), you can probably even get some of the "generate this function" stuff down to a single keystroke with today's Idris. If you are interested in how type can make better programs, I encourage you to go those TDD w/ Idris. It starts with the basics, so it should be approachable even if you've never written Scala/Haskell/Idris/Adga/Coq (etc., etc.). If you know your way around an expressive type system, you'll be able to go through the book quickly, but the last two chapters might still give you some new tools or perspectives. (There are at least 3 versions of the exercises on-line to check yourself, too; an official version, mine, and at least one other that was posted to /r/idris.)
Ya the dependent type system isn‚Äôt new to me (I have around 10k lines of Agda), but term synthesis in Agda is super crappy, so watching it work like in the video is... inspiring. It also appears that a large part of why synthesis works so well in the video is the quantitative annotations‚ÄîI think without the linearity, it won‚Äôt work as well. In any case, I still spend the majority of my time writing Haskell, but hopefully even with just polymorphism + the new linear arrows coming soon, we‚Äôll be able to get decent term synthesis, too.
why can't you compile the haskell runtime into webassembly?
If I didn't hear back after the phone screen, I suppose I'm rejected :( Anyway, this seems to be a great company. Wish you the best of luck. I want more Haskell firms like this in the bay area.
&gt; It's not the Comonad class itself which allows this, it's the extra operations provided by each individual Comonad instance. For example, you wrote shiftLeft and shiftRight, which are operations specific to toroidal lists; similarly, in a ListZipper [a] a [a] you can see whether the left and right lists are empty, indicating that you are at the left or the right end of the list. The idea behind my `CA` class is that instead of operating on a whole universe (e.g. `shiftLeft :: Strip a -&gt; Strip a`), you would operate on points (e.g. `moveLeft :: Point -&gt; Point`), so instead of doing e.g. `extract . shiftLeft` - operating on the universe then getting the new center point - you would instead do `peek . moveLeft`, which operates on the current point then gets the value at the transformed point. Personally I think I find the second style a bit clearer, but then again, I am biased... &gt; I said that "a function from p to a doesn't allow you to express this subtlety", but I now realize that this was not a very useful criticism. No - it was very useful! Even if the criticism itself didn't really go anywhere, it was still useful to me as it led to a key realization about the usage of `Comonad` vs `CA` (above). &gt; I now think your CA typeclass is fine (it has laws, so it can't be that bad!), it's just not as general as Comonad This was my intention! I actually based this class (in part) on the [`ComonadStore`](https://hackage.haskell.org/package/comonad-5.0.4/docs/Control-Comonad-Store-Class.html) class, so your criticisms aren't really new - they apply equally to that class. &gt; it can only have non-trivial instances for infinite or toroidal Comonads. So are you saying that it can't have instances for finite universes? Why?
Being pedantic, there is no "Haskell runtime". If you're asking about the GHC runtime then I suspect you can compile much of that into webassembly with a C-&gt;WASM compiler. All of it? Almost certainly not since it uses various OS-specific or platform specific features such as handling signals, threading, allocation, etc.
So is there any way that this law can be saved? Even though it will require some sort of qualification, I believe it still expresses a very useful property - perhaps the most important out of the three laws - and it would be nice to have it.
Hey Kkweon, I‚Äôm really sorry about this. It‚Äôs kind of awkward to say no to people right after or during a phone call, so I usually try to wait a day, but I must have forgot to follow up, which is rude.
I should stop commenting, I keep accidentally saying incorrect things! I was still in the mindset that writing comonadic code meant you're examining your neighbors, that is, positions at a relative position from you. And so my complaint was that if you can _always_ describe your neighborhood using a type `p` of relative positions, then I can pick any inhabitant of `p`, say `f`, and use it to shift in one direction, and then shift again, and again, and again. The type says that this always succeeds, so the space must be infinite in that direction, or it must loop around. So you can't express, say, a list with 5 positions unless you interpret it as looping around But I now realize that you want `p` to be an absolute position, not a relative position! And you can definitely have a type `p` with exactly 5 inhabitants.
http://homepages.inf.ed.ac.uk/wadler/papers/sbmf/sbmf.pdf
&gt; It also appears that a large part of why synthesis works so well in the video is the quantitative annotations‚ÄîI think without the linearity, it won‚Äôt work as well. Well, both the zip / zipWith and traverse examples work today in Idris, without quantitative annotations. The door-DSL stuff did get better with quantitative annotations, since without them proof search would often try and re-use and already used door. I actually found term synthesis in Agda to be almost as good as Idris though. I'm admittedly less experienced than you, though; with only about 1k Agda https://gitlab.com/boyd.stephen.smith.jr/agda-tapl/blob/master/LNB.agda. There are definitely "tricks" to getting term synthesis to work better, in both languages.
I believe there's work on that ongoing.
Hey bor0s, I read parts of the paper, but I've always been lost when trying to understand the whole point of Dependent Types -- and in this case I'm still a little lost. I've even read Eisenberg's dissertation: [http://www.cis.upenn.edu/\~sweirich/papers/eisenberg-thesis.pdf](http://www.cis.upenn.edu/~sweirich/papers/eisenberg-thesis.pdf) yet I am still missing the point, since he for the most part only glosses over what dependent types are, and what they are used for: &gt;Functional programmers use dependent types in two primary ways, broadly speaking: in order to prevent erroneous programs from being accepted, and in order to write programs that a simply typed language cannot accept. In the paper you linked, here's the quoted part that explains what Dependent Types are and what they are used for: &gt;*The dependent function space* ‚Äò‚àÄ‚Äô generalizes the usual function space ‚Äò‚Üí‚Äô by allowing the range to *depend* on the domain. The parametric polymorphism known from Haskell can be seen as a special case of a dependent function, motivating our use of the symbol ‚Äò‚àÄ‚Äô.4 In contrast to polymorphism, the dependent function space can abstract over more than just types. The Vec type above is a valid dependent type. &gt; &gt;It is important to note that the dependent function space is a generalization of the usual function space. We can, for instance, type the identity function on vectors as follows: &gt; &gt;‚àÄŒ± :: ‚àó.‚àÄn :: Nat.‚àÄv :: Vec Œ± n.Vec Œ± n &gt; &gt;Note that the type v does not occur in the range: this is simply the non-dependent function space already familiar to Haskell programmers. Rather than introduce unnecessary variables, such as v, we use the ordinary function arrow for the non-dependent case. The identity on vectors then has the following, equivalent, type: &gt; &gt;‚àÄŒ± :: ‚àó.‚àÄn :: Nat.Vec Œ± n ‚Üí Vec Œ± n &gt; &gt;In Haskell, one can sometimes ‚Äòfake‚Äô the dependent function space \[11\], for instance by defining natural numbers on the type level (i.e., by defining data types Zero and Succ). Since the type-level numbers are different from the value level natural numbers, one then ends up duplicating a lot of concepts on both levels. Furthermore, even though one can lift certain values to the type level in this fashion, additional effort ‚Äì in the form of advanced type class programming ‚Äì is required to perform any computation on such types. Using dependent types, we can parameterize our types by values, and as we will shortly see, the normal evaluation rules apply. &gt; &gt;**Everything is a term** Allowing values to appear freely in types breaks the separation of terms, types, and kinds we mentioned in the introduction. There is no longer a syntactic distinction between these different levels: everything is a term. In Haskell, the symbol ‚Äò::‚Äô relates entities on different syntactic levels: In 0 :: Nat, the 0 is syntactically a value and Nat a type, in Nat :: ‚àó, the Nat is a type and ‚àó is a kind. Now, ‚àó, Nat and 0 are all terms. While 0 :: Nat and Nat :: ‚àó still hold, the symbol ‚Äò::‚Äô relates two terms. We will still use the word ‚Äútype‚Äù to refer to terms œÅ with œÅ :: ‚àó, and still call ‚àó a ‚Äúkind‚Äù, but all these entities now reside on the same syntactic level. As a consequence, all language constructs are now available everywhere. In particular, we have abstraction and application of types and kinds. Even in Eisenberg's paper, he mentions that dependent types remove the differences between types and kinds, and that that distinction was alway artificial (though I think if you understand the concept of Rank -N- Types that's easier to accept -- though is probably no one else sees it this way...). Though beyond that, still lost as to what Dependent Types are doing... Thanks, if you can shed any light on this.
I've begun learning to use `Network.Websockets`, and I have basically no prior experience with networking. One thing that I'm having an odd amount of trouble with is exiting the server side program. I tried putting `return ()` at the end of `main` and at the end of my application, neither worked. I tried returning after receiving a close request from the client, that didn't work either. What should I be doing?
`return` isn't a keyword or really even a control-flow operator in Haskell. It's the same thing as `pure`, which you might think of as creating a singleton container. You might want to [exit successfully](http://hackage.haskell.org/package/base-4.12.0.0/docs/System-Exit.html#v:exitSuccess) or just [die](http://hackage.haskell.org/package/base-4.12.0.0/docs/System-Exit.html#v:die)
Hm, that's odd. I also tried `exitSuccess` upon getting a close request from the client. It didn't work either, so I just assumed that I didn't know how the `System.Exit` module is supposed to be used.
&gt; I've always been lost when trying to understand the whole point of Dependent Types For *me* it's the ability to have the compiler verify any logical proposition about my code. I no longer have to find / write static analysis tools separate from the compiler and tack them on to our CI pipeline in the right place(s). Instead, in the same language as the code, we write and prove (not just test, but prove[1]) properties about the code. In addition, I enjoy seeing term synthesis (or as I call it "program inference") and it seems much more useful when you can very precisely define the type of the term you want. TDD w/ Idris gives small examples of big ideas about what dependent types can do for your programs. [1] This is part of the reason I'm a bit "meh" on Dependent Haskell. It's type system remains in correspondence with an inconsistent logic, so you can prove everything (including the things that contradict the other things).
Avoiding axiom K and allowing for higher inductive types with "richer" propositional equality? Just guessing.
Could you share (parts of) your code? It might help to see where/how you were using `exitSuccess`.
Sure thing! rcvAndProcessMsg :: W.Connection -&gt; IO () rcvAndProcessMsg con = do msg &lt;- W.receive con case msg of W.ControlMessage m -&gt; case m of W.Close _ _ -&gt; putStrLn "Bye Bye" &gt;&gt; exitSuccess _ -&gt; putStrLn "Control message received." W.DataMessage _ _ _ m -&gt; case m of W.Binary b -&gt; case runParser parser $ toStrict b of Left err -&gt; putStrLn err Right str -&gt; putStrLn str W.Text b _ -&gt; putStrLn ("Client said " ++ (show b)) application :: W.PendingConnection -&gt; IO () application pending = do con &lt;- W.acceptRequest pending rcvAndProcessMsg con rcvAndProcessMsg con return () main = do let port = 1024 address &lt;- readProcess "hostname" ["-I"] "" putStrLn $ "IP: " ++ address putStrLn $ "port: " ++ (show port) W.runServer address port application
I've kept on reading that many language communities want to wait until wasm can use the javascript GC, but I just don't understand why languages that have a GC model can't just write their own in WASM.
I've built a few systems with AKKA during my years of using Scala, and a few systems with Erlang. I have not taken a good look at Cloud Haskell, but this looks very cool at first glance. 
&gt; But I now realize that you want `p` to be an absolute position, not a relative position! And you can definitely have a type `p` with exactly 5 inhabitants. Yes. If you're interested in this topic, you'll like Edward Kmett's [comonadic cellular automaton tutorial](https://www.schoolofhaskell.com/user/edwardk/cellular-automata) - he covers similar concepts in Part 3. (As an aside, if you want a type with exactly *n* parameters, I've been finding the [`finite-typelits`](https://hackage.haskell.org/package/finite-typelits-0.1.4.2/docs/Data-Finite.html) to be very useful.) &gt; I should stop commenting, I keep accidentally saying incorrect things! No - you've been making very good points! --- I've actually been wondering: how *would* I go about making a finite CA? For instance, take the following definition: data Finite a = Finite [a] a deriving (Functor) Where the second field is a default value for all points outside the range. I can then *try* to make a `CA` instance: instance CA Int Strip where peek n (Finite as def) | 0 &lt;= n &amp;&amp; n &lt; length as = as !! n | otherwise = def evolve f s@(Finite as def) = Finite (flip f s &lt;$&gt; [0 .. (length as - 1)]) def' where def' = _ So there is a problem here, just not the one you suggested. The problem is that you need some way to convert an arbitrary `a` into an arbitrary `b`, and you just can't do that without using `unsafeCoerce`. The obvious solution would be to change `evolve` to have type `CA p c =&gt; (p -&gt; c a -&gt; a) -&gt; c a -&gt; c a` - that is, just remove `b` to make it a function between `a`s only. This would solve the problem - and honestly, I can't even think of a situation where you would *want* to make a type-changing CA - but it also makes the types a bit less expressive. What do you think? 
Can't you just use `Maybe Int` instead of `Int` as your `p`, so you have a position pointing at `def`?
`runServer` does have anote that it "never returns" and it should NOT be used in production. I checked the source code, and it spawns a new thread for each connection, using `forkIO`. `exitSuccess` is a special form of `exitWith`, which has this note on Hackage: &gt; Note: in GHC, exitWith should be called from the main program thread in order to exit the process. When called from another thread, exitWith will throw an ExitException as normal, but the exception will not cause the process itself to exit. --- If you are on UNIX / Linux, try [`exitImmediately`](https://hackage.haskell.org/package/unix-2.7.2.2/docs/System-Posix-Process.html#v:exitImmediately) instead of `exitSuccess`. --- The "simplest" thing that is cross-platform would be to capture `myThreadId` in main before calling `runServer` and have your application `throwTo` that thread the `ExitCode`. It's not a nice design, but it should get the job done. Something like this: main = do mainThread &lt;- myThreadId let port = 1024 address &lt;- readProcess "hostname" ["-I"] "" putStrLn $ "IP: " ++ address putStrLn $ "port: " ++ (show port) W.runServer address port (application mainThread) application :: ThreadId -&gt; W.PendingConnection -&gt; IO () application mainThread pending = do con &lt;- W.acceptRequest pending let conAct = rcvAndProcessMsg mainThread conAct con conAct con rcvAndProcessMsg :: ThreadId -&gt; W.Connection -&gt; IO () rcvAndProcessMsg mainThread con = do msg &lt;- W.receive con case msg of W.ControlMessage m -&gt; case m of W.Close _ _ -&gt; putStrLn "Bye Bye" &gt;&gt; throwTo mainThread ExitSuccess {- ...rest of rcvAndProcessMsg -} You could also use the C FFI to import `exit()` and call that. HTH
Sweet! Thanks for the help!
It's probably possible, but there's also a likelihood of generating memory leaks. If there's a liveness / reachability cycle that involves objects managed by both GCs, unless special care is taken to break those cycles, those objects will never get collected. To break the cycle, the language GC would need to release some objects to the JS GC. To even detect the cycle, the language GC would need to be able to walk the JS heap.
The use case for this operation would be when you expect an insertion to follow most lookups, for example when searching for rare duplicates in a list.
Anyone?
As you are probably aware, LambdaConf has a double-blind review process and selects proposals based purely on technical merit. Thanks to [PaperCall.io](https://PaperCall.io)'s anonymized submission process, not even organizers or volunteers are aware of the identity of presenters until *after* a proposal is accepted. &amp;#x200B; On the pro side, this double-blind process results in consistently high-quality selections and reduces the potential for discrimination based on a person's name recognition, gender-orientation, gender-expresison, gender-identity, skin color, sexual-orientation, or other attributes not relevant to a proposal's technical merit. On the con side, it means I can't estimate if any presenters will satisfy your definition of "fascist" or not. &amp;#x200B; As much as I love participating in safe spaces where everyone shares my own liberal values, and no one has any prejudice against those who exist or who think differently than themselves‚Äîand here I don't mean just the obvious forms of prejudice, but also the implicit biases that are unfortunately held by most of the population, [including probably yourself](https://implicit.harvard.edu/implicit/takeatest.html)‚Äîthat's not the purpose of LambdaConf. LambdaConf exists to promote functional programming by offering the highest-quality selection of talks submitted to the conference each year, in a professional environment that ensures everyone is treated well, with a clearly defined and enforced code of conduct. No more, no less. &amp;#x200B; I'm well aware this criteria doesn't work for everyone, and I'm really disappointed by that. Fortunately, if it doesn't work for you, there are other groups you can look into, including [MoonConf](http://moonconf.org), a functional programming meeting space that *does* consider more than the technical merit of proposals. Additionally, conferences that don't implement double-blind review have much more capability to screen speakers for political, ideological, and religious views prior to selection, so you might be happier presenting at one of those.
&gt; The aim of the Enecuum Node project is to provide a set of tools for building of network acting nodes and blockchain protocols. Nodes are capable to handle concurrent state (with STM), work with KV database, manage any types of graph (which is also concurrent). The code of the nodes should be testable, safe and well-maintainable, and writing of blockchain protocols should be as easy as possible. It allows you to create TCP, UDP and JSON-RPC APIs (both client and server side), intractable command line applications, or any kind of stateful applications. This looks really nice! &gt; Under some circumstances, it can be a replacement for Cloud Haskell! Recently there was proposal to redesign Cloud Haskell: * https://www.reddit.com/r/haskell/comments/a0u3kw/cloud_haskell_redesign_proposal/ Maybe it's a good idea to collaborate effort :)
Professor Hughes, Your "Why functional programming matters?" matters a lot to us.
Thanks! Collaboration is definitely a good idea! I've read the proposal and I'm clearly sure the Enecuum Node framework addresses many of the problems of CH described, and make possible to solve the rest of the problems. Looks like our architecture and model has a bit potential to be a tagline for the CH redesign. I probably need to re-read the proposal and highlight the points from Node framework point of view. I could do it these days.
That's really clever! Although I would use a custom type: `data WithOutside a = Inside a | Outside`
I don't know of any library that does this, but it shouldn't be difficult writing it yourself with TemplateHaskell. The problem I can see with this is that the JSON doesn't contain enough information to guess appropriate types; you can't tell which fields should be Maybes, for example, or what to do with heterogenous lists. In practice, you will be able to decide on a per-project basis, but generalizing it into a library isn't straightforward. Gee, now I'm interested... and I blame you!
Well, abstract algebra will teach you about [monoids](https://en.wikipedia.org/wiki/Monoid), which [appear throughout computer science](https://en.wikipedia.org/wiki/Monoid#Monoids_in_computer_science); and that in turn will help you understand what a [monoidal category](https://ncatlab.org/nlab/show/monoidal+category) is.
&gt; It's type system remains in correspondence with an inconsistent logic, so you can prove everything (including the things that contradict the other things). But you can run your proof and it terminates then we're back in the game!
Please make one pleaseeeee. But, what I want is a simple convert application (`f :: String -&gt; String`) just like JSON-to-GO. So, it could be done even without TH, right? I actually think it's easier without TH? &amp;#x200B; With that said, it doesn't have to be so friendly. I expect users to provide every element they want to generate or it could just put default values like `[Int]`. The behavior is clear as there are other services like JSON-to-GO. I expect to work the same. &amp;#x200B; What I expected to be challenging was handling nested part. But probably, not that bad.
Is [json-autotype](http://hackage.haskell.org/package/json-autotype) what you're looking for?
Yes, looks like it. Now I want a few more features lol I will look into it. Thank you!
Types in general are important for writing correct programs. In Scheme e.g. passing a number to a function that expects a string is totally valid for the compiler, but will fail at runtime. Per BHK interpretation, dependent types allow for encoding quantifiers. Most typed programming languages can express logic up to propositional logic, but expressing higher-order logic is not possible. Dependent types thus increase the power of expressiveness. It is about mathematical proofs, but not only. You get more expression. Compare these two type definitions: append : List a -&gt; List a -&gt; List a append' : Vect n a -&gt; Vect m a -&gt; Vect (n + m) a Since as we mentioned the expresiveness with dependent types is more powerful, we as programmers get much more info from the type definition of append' compared to append.
I'm surprised it took that long!
I've thrown something together for a project, but it only shows strings: https://github.com/prikhi/crypto-portfolio/blob/master/src/Table.hs In use: https://github.com/prikhi/crypto-portfolio/blob/master/src/GainsTable.hs#L224-L308
Tell me if any of what follows is confusing. --- intOrChar : Bool -&gt; Type intOrChar True = Int intOrChar False = Char `intOrChar` is a function that takes a `Bool` and returns a `Type`. In a dependently typed language, types like `Int` or `Char` are values just as much as `True` or `10` are values. The type of `True` is `Bool`. The type of `10` is `Int`. The type of `Char` is `Type`. Whatever goes on the right side of `:` in a type signature must be a *value of type `Type`*. If I run it in my imaginary repl: &gt;&gt;&gt; intOrChar True Int &gt;&gt;&gt; intOrChar False Char Here's a function that uses `intOrChar`: returnsIntOrChar : (b : Bool) -&gt; intOrChar b returnsIntOrChar b = ... `returnsIntOrChar` is a dependent function, which means the type can change depending on its input. When we pass a value to a dependent function, the type is updated with that value. &gt;&gt;&gt; :type returnsIntOrChar True Int &gt;&gt;&gt; :type returnsIntOrChar False Char The type of `returnsIntOrChar True` becomes `intOrChar True`, which simplifies to `Int`. Similarly, `returnsIntOrChar False` has the type `intOrChar False`, which simplifies to `Char`. Calling a function with a literal will change its type 'from the outside', but we also need a way to change its type 'from the inside' while we're defining it. For this reason, *pattern matching* on an argument can also update the type. Let's implement `returnsIntOrChar`: returnsIntOrChar : (b : Bool) -&gt; intOrChar b returnsIntOrChar b = case b of True =&gt; ?trueHole False =&gt; ?falseHole In the `True` branch, *b must be true* (because we just pattern matched it). Which means the compiler replaces `b` with `True` when it type-checks the right hand side of the `True` branch. This means the type of `?trueHole` is `intOrChar True`, which simplifies to `Int`. Our program will only compile if we put an `Int` there. Similar reasoning applies to the `False` branch, which lets us put a `Char` on the right hand side of the `False` branch. returnsIntOrChar : (b : Bool) -&gt; intOrChar b returnsIntOrChar b = case b of True =&gt; 0 False =&gt; 'A' --- Why is this useful? A big benefit is that we can check more more mistakes at compile time. Because applying literals to a function can change its type, we could write a `divide` function that causes a type error if you pass `0` as its second argument. Another common example is the [`printf`](https://en.wikipedia.org/wiki/Printf_format_string) function. What should its type be? `String -&gt; [Any] -&gt; String`? If you give it a non-dependent type, you might call it with arguments that lead to a runtime error. `printf`'s type *depends* on the format string it receives. For example, `printf "%d"` should have type `Int -&gt; String`, and `printf "%s %s"` should have type `String -&gt; String -&gt; String`. We can write a dependent type for `printf` that causes a type error if you pass arguments that are inconsistent with the format string.
In python you can write the following: def myFunk(b): if b: return 5 else: return "hello" In Haskell and Java we can't: myFunk :: Bool -&gt; ??? -- what should go here? myFunk b = if b then 5 else "hello" In Idris we can: myFunk : (b : Bool) -&gt; if b then Int else String myFunk b = case b of True =&gt; 5 False =&gt; "hello" The type of the result depends on the _value_ of the input.
Great read! I really like seeing uses for groups, they're one of my favourite maths abstractions!
You can use `Either` in Haskell for `myFunk`, as the example you showed is essentially a sum type.
Since [1993](https://clean.cs.ru.nl/NL-FP_dag_2018#Past_NL-FP_Days)! This will be the 26th NL FP day.
I don't see how the order of aggregating/cleaning doesn't matter. If i take the example from part 2 I get different end results if i reduce first or remove the C/c first. Am i missing something?
All these libraries can only generate the best-guess code. Go generally has less facilities for data modeling than Haskell does so while it's definitely possible to do the same thing in Haskell, I'd guess Haskellers would be less satisfied with the solution.
&gt; But, what I want is a simple convert application (f :: String -&gt; String) just like JSON-to-GO. Once you know enough TH to do this, I'm convinced the TH route (basically `Value -&gt; Q [Dec]`) isn't going to be harder than the `String -&gt; String` route - you simply map JSON values to suitable TH data structures, so you don't need to worry about syntax rendering and all that. &gt; With that said, it doesn't have to be so friendly. I expect users to provide every element they want to generate or it could just put default values like [Int]. The behavior is clear as there are other services like JSON-to-GO. I expect to work the same. JSON-to-Go actually gives up quite easily. Heterogenous lists, anything with `null` in it, and many other edge cases, basically just make it go `type AutoGenerated interface{}`. Not very useful, and there isn't even a good equivalent of that in Haskell. More generally, the problem is of course that a single example value isn't enough to fully describe a type. The type of `Nothing` is `Maybe a`, but we don't know which `a` to pick. A JSON string might deserialize as a `String`, `Text` (lazy or strict), or any enum-like type that has a matching constructor. We could even generate our own custom types as needed. In the string case, we could simply say that we always default to `Text`, but in the `null` case, this won't work, because there simply isn't a suitable default. `Maybe Void` would be "correct" in the sense that its only inhabitant is `Nothing` - but that's not very useful, because then the resulting code will never accept any `Just` values. But any other choice of `a` risks being wrong, except `Maybe Value`, which however is useless, because we already have a `Value`. Or possibly `Maybe Dynamic`, but that's kind of a cop-out too. Similar problems occur with lists. `[1, "hello"]` might reasonably decode into `(Int, Text)`, but it's equally reasonable to say it should be a list of whatever type captures all the provided elements. The first case pins down the number of elements and their types, while the second one merely pins down the possible types each element may have. Empty lists are even worse: `[]` could mean `()`, or `[a]` for any `a`. A possible way out would be to not actually pin things down to monomorphic types. For example, `[1, "hello"]` might render as `:: (IsString a, Num a) =&gt; [a]`. This would put the caller in charge of picking suitable types, and maybe that's exactly the right design. It would take quite some effort though to find just the right constraints to enforce here, and to express more complex situations accurately at the type level.
`Either` doesn't tell you at compile time that if `b` is true than the result is always `Left 5`, which is what we want to achieve here with a dependent type.
Note that we aren't "removing" all C/c. We're instead generating the group homomorphism where C/c is replaced with the identity element. This doesn't just remove C/c's, but actually has the potential to drastically change the internal structure and "length" of our free group.
Ah, right. That's correct.
This program performs `react; remove all C/c; react` process. It's different from what the question asks: `remove all C/c; react`, but it doesn't matter from the reason the author described.
Nice, I'll try this in Sage tomorrow.
Neat idea! I'll try to implement it this holiday season and publish it on Hackage. I'll update you if I succeed at using my free time productively. 
Let me know if you get any interesting results! :)
[http://hackage.haskell.org/package/friendly-time-0.4.1](http://hackage.haskell.org/package/friendly-time-0.4.1)
If your goal is to learn Haskell then you'll probably be better off just learning Haskell. You'll pick up the abstract algebra and category theory you need to during that process.
Haskell, abstract algebra and category theory are all cool to learn! My approach has been learn Haskell, and slowly approach learning the other two as I go along. You'll pick it up as you go and can then relate it back to your Haskell programming and gain some occasional deeper understanding. Just keep on learning! :D
i consider category theory to be a part of abstract algebra, as category theory is about categories which are a variety of abstract algebra some very basic abstract algebra is necessary to know for a programmer [most notably : monoid]; i wrote a book about programming, including the [abstract algebra recommendable to learn for any programmer](https://libeako.github.io/c/ID_380850480.html)
&gt; We are not trying to impose these guidelines on members of the Haskell community generally. Rather, we are adopting them for ourselves, as a signal that we seek high standards of discourse in the Haskell community, and are willing to publicly hold ourselves to that standard, in the hope that others may choose to follow suit. I hereby pledge to hold myself to that standard as well. Since I am not part of the GHC steering committee, please contact me directly if I accidentally stray from those guidelines.
&gt;Bonus question: With NoMonomorphismRestriction turned on, does anything else change if you define Foo this way? &gt; data Foo a = Foo { strict: !a, lazy :: a } &gt; Why or why not? I expected it to never print "evaluating strict", but nothing changed. I suspect that the printing of lazy determines the type inside the case expression? I also tested the version data Foo a b = Foo { strict :: !a, lazy :: b } but that still gave the same result. Why is that?
I pledge the same. Thank you to the committee for leading by example.
I have objected to having a CoC multiple times in different circles for multiple reasons, not that my opinion matters. But since this is not a code of conduct and is put forward in an unintrusive way I quite like it. However, can anyone enlighten me why proactively seeking: &gt;**Diversity** and inclusion. We recognize that the Haskell community, echoing the technology industry more generally, skews white and male. We see it as our duty and honour to spread the joy of Haskell widely and to broaden the patterns of participation, in the hopes that, one day, we will no longer be askew. &amp;#x200B; is automatically viewed as desirable, just right and whatnot? &amp;#x200B;
ifyou have a JEON schema, aeson-schema does this, I'll welcome anyone wanting to take it over, it's already and hand-me-down
My thought would be that diverse backgrounds, personalities, etc. make for a stronger community as you can have more diverse input, reach a wider audience, and allows for anyone to feel like they can take part rather than thinking it wouldn't include them. We can be better as a larger, stronger community :) 
The toList does the actual reacting, right?
There is another story as well: https://www.reddit.com/r/haskell/comments/a3dh72/building_network_actors_with_enecuum_node/ That story points to: https://gist.github.com/graninas/9beb8df5d88dda5fa21c47ce9bcb0e16
This doesn't really answer my question I am afraid. It may be true that artificially increasing diversity has positive feedback loop and draws more people and widens the audience more and more. But I haven't seen it sufficiently substantiated why that would mean *stronger* community for any definition of stronger other then *stronger is exactly the same as diverse*. Trying to alter a natural progression draws resources, resources that could be used elsewhere in another way. Diversity seems to me, at least in part, a converse to cohesion. I wouldn't advocate for prioritizing cohesion over diversity, but not because I think cohesion is of lesser value, but because I believe these two things get balanced best in a natural way. People that get along well, can communicate well, understand each other and can agree on a particular plan of their join project's future have equal potential of success to diverse, large and strong community whatever that means, in my humble opinion. Of course we all have different opinions, and while I do not provide evidence that would support my viewpoint, I haven't seen any evidence to the contrary, namely that striving for diversity doesn't actually hurt.
Here's a version in SageMath: https://github.com/octonion/adventofcode/blob/master/2018/5/free_group.sage
For Haskell? Neither is of primary importance, though each is interesting on its own. 
&gt;In my mind, \[Integer\] -&gt; \[Integer\] could mean I'm adding 10 to each integer, meaning the result isn't a subset of the input &amp;#x200B; Unlike `Integer`, we don't know anything about the type `a` so there's nothing we can do with the input except give it or a subset of it (or an error) back. In the same way, there is exactly one thing that `fst` could do, based on its type.
This appears to be the best recommendation. JSON -&gt; JSON Schema -&gt; Template Haskell There is a number of tools to perform the first step, you can search online. After that, have a good look at the generated JSON schema and adjust it manually if necessary. Finally use aeson-schema to generate the Template Haskell code. 
`fmt` library has `diffF` formatter: * http://hackage.haskell.org/package/fmt-0.6.1.1/docs/Fmt-Time.html#v:diffF
&gt; artificially increasing diversity &gt; Trying to alter a natural progression When you say "artificially", you mean *mindfully*, or *deliberately*. In one sense, humans in communities are always natural (it is our nature to exist in communities), and in another sense, they are always artificial (communities exist because of human --artificial-- actions). When you say "naturally", you mean something like *implicitly*. I think you're attaching value judgments by talking about it in the framing of "artificial" and "natural" (these words cannot be divorced from their connotations). I think it is unproductive to engage in a discussion about diversity and inclusion such a framing. Anyway, instead of focusing on the goal as stated, diversity, consider the process of attaining that goal. In that process, one would have to understand the implicit factors that make the community the way it is now, and one would have to make those implicit factors explicit, and think about them deliberately. There is absolutely no way that doing this would be detrimental to the community or to the individual. Unless you think it's a waste of time. But you started this conversation, so you must not think it's a waste of time. So, if you personally want to take the diversity and inclusion directive in the best way, then I would say you should take it as a suggestion to be more mindful and deliberate about the forces shaping your milieu and your personal influence on the community. It is always an interesting exercise to consider why another person would take the position that they do. Simon Peyton Jones' view and influence over the community is very, very different from yours or mine. He has come to the conclusion he has, that diversity and inclusion should be valued and sought after. As an educator and community leader, what experiences has he had that led him to this conclusion? And as for me, I support it for personal, selfish reasons :)
I greatly approve of this lead-by-example approach, rather than an enforced CoC. It has far less potential to cause strife, which it is intended to prevent.
I think you've expressed this better than I ever could have :D The above was just my opinion, and my selfish reasons are that I think it's nice to be nice so including more people stems from that. Plus the fact that I love Haskell so much that I want everyone to use it &lt;3
&gt;you're attaching value judgments by talking about it in the framing of "artificial" and "natural" Fair point, I chose those words for not knowing any better, I am not a native speaker and these words came to me first. I didn't realize there were better alternatives. That being said, I am not happy with the alternatives you offer. To me deliberate and mindful don't really cover going out of your way to bring different people in simply because you think there is something wrong about being almost exclusively a bunch of white dudes passionate about programming rather than anything else. Show me a pattern that has repeatedly driven people away, people that themselves invested effort into joining the community and contributing to it (even by just discussing, not necessarily by hacking on ghc or open sourcing some fancy piece of software or anything like that) and then you can be sure I will support a reasonable decision to fix that. I have my doubts about such patterns being tied to diversity. What I intended to asked in my original comment in this thread was: assuming a community does not draw a specific demographic as much as others, what assessment have we done to be sure that it is worth to put extra requirements on us and most importantly others that are happy in the community to draw that demographic that may or may not be interested in joining the community whatever you do. There are members of minorities that are extremely huge assets to this community and I admire them, their work and I could not compete with them, but still that does not prove that we should now revamp the composition of the community to include more specimens from that minority. Maybe we are chasing the wrong goal. And while we are doing that, we develop self-regulation that is bordering unhealthy. Diversity in my opinion should never be the goal. This doesn't mean diversity is bad.
I think being nice is nice as well? What does that have to do with striving for diversity? You can be as nice as you can but if you are still white guys you failed. Having a diverse community doesn't mean people are nice to each other either... so the question stands unanswered... what about diversity in itself is it that makes it a trait to be pursued with higher priority than others?
Unless I want performance, too, in which case I add REWRITE rules to turn all my proofs into Axioms.
&gt;Having a diverse community doesn't mean people are nice to each other either That's true, but having a variety of perspectives can help us collectively understand how better to *be* better toward people who are different from us. &gt;what about diversity in itself is it that makes it a trait to be pursued with higher priority than others? I feel like this assumes that we have some amount of energy that we're going to direct away from being productive in other areas to focus on diversity. How would this demand more of us?
one amoral/pragmatic reason is that you can bring in new contributors that are interested in a such anti-bigotry pro-diversity statements, because they've dealt with enough bigoted communities and don't want to waste their time again. there are higher-effort things that can be done, but writing one paragraph is a high-value low-effort signal. it's desirable because of how open-source labor works. at a job, a male boss that harasses you, a coworker that obliviously says ridiculously bigoted things, and so on, are (more likely to be) suffered by the employee. because they want to keep their salary, their healthcare, etc. open-source communities like Haskell (most of the research for Haskell-the-language, and most of the packages written in Haskell, are done by volunteers) are built off (mostly) free and voluntary labor. this is great. but it means if someone is being harassed, feeling excluded, and so on, they have the freedom to just stop volunteering. so why should someone who is competent, who is volunteering for the Haskell Community without getting paid, deal with b.s.? they have choices: there are other communities that need help, other communities that has more people who understand them; and there are tech jobs that pay a lot of money. thus, things you think are "small" (like microaggressions; and like racial/gender homogeneity of your collaborators, of your co-authors, in conferences, in hackathons, etc) might still be large enough to push away someone who **will** spend 10h per week for a year on something (and that thing won't be Haskell, when it would've been). Note that Haskell is already a relatively small community, so each new person can have a large impact. the more important reason that is since it's the right thing to do. programming is a high salary job, so even if (for example) women got paid the same as their male coworkers at any particular position, by there being fewer female programmers, the median female salary will still be lower. and Haskell makes programming more ergonomic and enjoyable. (Note, this is a sensitive topic. I dictated this into my phone while drinking coffee, and didn't edit it much, so please interpret typos charitably.) 
Well if you passively watch how diversity evolves and celebrate when it rises then I suppose where little. But that was not the premise of my original question. The document stated that diversity is a motivation... meaning there will be decisions made and actions taken to further that. Those may or may not get in conflict with other concerns. All I wanted to know was why do we commit to this goal. In my opinion a fair question. If it is so obvious (judging by the reaction of others here on reddit in this and other threads) I would believe there were no problem to conjure evidence in favor of putting diversity among the main goals of a community. So far I have seen only well intended fuzzy explanations in spirit of "let's all be nice to each other" but nothing that would convince me we are missing out on something measurable by being at a state that we are at right now... having this many white males and only that few members of some minority.
Folks, isn't it a bit unseemly to be downvoting jd823592, who is communicating respectfully, in a thread about respectful communication? Are the downvotes respectful? (At the time I'm posting, jd823592 is at negative points on most comments in this thread.)
Disclaimer: I'm a white male and don't think I've ever been marginalized by this community or any other. It's taken me 15 minutes to decide to respond to this. Your user history doesn't have any glaring red flags, so I'm setting aside my initial suspicion of https://en.wikipedia.org/wiki/Sealioning -- I anticipate that suspicion is why you're being downvoted. Here's my distilled line of thinking specifically for you. I hope it helps. My observations: O1 It's very difficult for a person inexperienced with marginalization to appreciate the perspective of a person marginalized by a community. O2 Relatedly, as /u/madelinja eloquenty pointed out, "natural progression" is nothing but a glorified description of "what happens when we don't stop and think about the less obvious (to us) consequences of our actions". Such phrases are often intentionally coded language for prejudice -- it's easy to find primary examples in history books (and everywhere else unfortunately). O3 In other words, we're "drawing resources", as you say, in order to ask ourselves: "Look at these skews in our former and current demographics. Why is that?" O4 All the "answers" for this question I can think of do not seem fair to me. Just age old, consequences of foolish, cowardly, and insidiously self-reinforcing behavior stretching back for centuries. This is deeply disappointing and infuriating as a humanist. O5 In still other words, it seems like we're on a trend of being white and male-dominated. *Regardless of why*, is that skew something that we want? (No.) If not, what can we do about it? O6 This tweet and the referenced thread helped me a lot with that recently, how I should answer that question. https://twitter.com/sbarolo/status/1047116117028823041?s=17 So let's be part of the solution, and actively steer our ship towards less polluted waters. Let's listen to those who feel marginalized and learn how we can help. Great work, steering committee! I don't won't be responding to you any more -- this is a snap judgement and I apologise for that. Amd I'm about to be uncharacteristically brutally honest: I think you need more assistance than I can provide. Find it elsewhere, there's lots of resources, like the Twitter link above. Maybe a kind soul here. But it's on you. My intuition says that you need some perspective on self and the privilege you were randomly born I to while others weren't. I'm not saying that you haven't had hard times, but try to see the hard times others have had that you have never even had to consider. Good luck! (I'm happy to help answer your Haskell questions, FYI.)
**Sealioning** Sealioning (also spelled sea-lioning and sea lioning) is a type of trolling or harassment which consists of pursuing people with persistent requests for evidence or repeated questions, while maintaining a pretense of civility. The troll pretends ignorance and feigns politeness, so that if the target is provoked into making an angry response, the troll can then act as the aggrieved party. Sealioning can be performed by a single troll or by multiple ones acting in concert. The technique of sealioning has been compared to the Gish gallop and metaphorically described as a denial-of-service attack targeted at human beings. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Numerous responses to you have listed numerous concrete reasons to support those hypothetical decisions and actions without appealing to a belief that it is "obvious" why we should do so. &gt;nothing that would convince me we are missing out on something measurable by being at a state that we are at right now Others miss out by feeling unwelcome in the community, insofar as there is such a thing, and we miss out on their potential contributions.
I agree with most of what you write. But I have trouble to see how all of this boils down to increasing diversity rather than stop being a--holes (and mind you, i think most people in this community are very nice people). Basically what I am saying is, that if your actions increase diversity, good for you, it probably won't hurt, at least given that your actions were made based on something really meaningful. If you identify a problem (unnecessary verbal slur, harassment) and you deal with them, you are in the right. If however you just run your numbers and conclude there are very few women and you start acting on that, then I think you got it backwards. And believe me I would have no problem with having more female coworkers as long as they left me alone the same way the male coworkers do :) just joking, there can be any number of people from any minority in any community I am member of for all I care ... If someone wants to join in, good...
&gt; Numerous responses to you have listed numerous concrete reasons Please list some of those not opinionated and measurable ones.
Does it have to be measurable to be valuable? I think diversity is essential for creativity, and creativity is essential for improvement by the discovery new approaches. I'm not sure how you'd measure that, though I'm sure someone has tried. But to me it's just plain obvious. If your input is constant, your output will be constant as well. But if you put a bit of chaos in there, interesting things will start to happen and some of those interesting things will likely turn out better than the old constant.
All the reasons do not have to be measurable. I would just like one or two so that I can change my mind based on something that I can verify, else the only other thing I know I can do is just to accept the moral superiority of others and never doubt anything myself as long as others asure me its a great idea.
Here's a concrete example for you. A few years ago at a Functional Programming meetup in my city, there was a guy who was sexually harassing women. The organizers of all the FP meetups in the area banned him from all their meetups to make sure he couldn't do that in our communities anymore. You're approaching this with the presumption that the status quo arises from a neutral state of affairs. However, if there are people out there who make these communities actively hostile to minorities, then the question boils down to this: If people are being mistreated in our community, do you think we should make an effort to stop that mistreatment? Now, that's just one example of a way the community's demographics could skew one way or another in a way that most people would consider bad for the community. There are others, but you can search those out yourself. For example, do you think it's possible for someone to mistreat another person without *intending* to mistreat them?
Politeness isn't the same thing as respect.
Thank you for your response. I never said that minorities should not be supported. I merely said that diversity is a bad measure in my opinion because it disregards all the reasons of any possible skewing and just simply tries to forcefully undo it. I will sure ponder this way more, thank you for the food for thought. I would just like you to know that I think very little of your decision to assume the amount of privilege of others over the internet.
How is this a concrete example of striving for diversity. Of course I condemn such appaling behaviour. But the action of banning that guy was in the direction of not having sexual harassment tolerated, not trying to invite more women. I am afraid you think I try (unknowingly) protect these kind of people, I don‚Äôt. I think you can derive the same action in this scenario from a completely different motivation than increasing diversity.
Where does the moral superiority come into it? Do you not believe in anything that is not measurable?
Ha, good luck getting Phil Wadler to communicate respectfully...
Oh, sorry, that's confusing. I thought my post was deleted by some reason (I saw it as deleted), so I've posted a link only. &amp;#x200B; Thank you for pointing this! I think, I'll leave both.
Neat abstraction. But the result from a naive `Text` based approach with fold is more than a hundred time faster than this one :/
&gt; I think you can derive the same action in this scenario from a completely different motivation than increasing diversity. Ok, but you can arrive at it from the motivation of increasing diversity. One might not consider the possibility that their male-dominated community is the way it is in part because of a high rate of sexual harassment. So it has non-zero value. Are you asking if pursuing diversity has a *unique* value you can't obtain via a different motivation? If so, that's a pointless question. You can dismiss any motivation in the same way. So the question is, why are you so skeptical of pursuing diversity, moreso than any other motivation listed in that code of conduct?
No, but it happens from time to time that I believe something even after I have been exposed to alternative ideas that did not offer any convincing proof. Most of the responses under my question repeat the same thing and mostly come down to (1) unsubstantiated claim that diversity offers improvement (regardless of the selection of individuals as long as they are diverse - diversity is the key) or (2) morals (basically that minorities are under represented because they are driven away by the representatives of my privileged demographic and it is our obligation to revert that). Both of these have something to them but (2) can be adressed without the goal of increasing the diversity (hey maybe a certain demographic just simply doesn‚Äôt care about haskell, have you made a study to refute that? Good that would be enough for me)
But, why arrive at it from diversity? What real value does diversity have for anybody? It sure isn‚Äôt the feeling of being included just because you were needed for the numbers. Giving people chances, supporting them and treating them respectfully are some of the means to achieve diversity but if your motivation is to feel better about yourself because you now have roughly equal number of friends from all sort of demographics than you are not addressing the real problem just the symptom...
+1 to this, and +1 to the committee's commitment to community comity.
Good work! I think it was probably also a good call to not ban the suggestion that less type-safe languages than Haskell are useful in some contexts in the spirit of following the GNU Kind Communications Guidelines \&gt; By contrast, to suggest that others use nonfree software opposes the basic principles of GNU, so \*it is not allowed in GNU Project discussions\*.
It only requires the assumption that people are [biased to their ingroup](https://en.wikipedia.org/wiki/In-group_favoritism). If they are, then a repeated competitive game around resource swill end up with people clustering. If you accept this premise, then you can just run your numbers and *assume* there is a bias. Of course, actually calculating the size of the bias opens a can of worms, but the idea isn't hard to comprehend.
So you give explanation why there is lower diversity than possible, disregarding the very questionable concept of ingroup. What does that prove about the effect of diversity on anything (productivity, creativity, ...). I am not saying diversity should be abbolished, just that I wouldn‚Äôt see it as the ultimate goal but merely a possible outcome of a pursuit of some more elementary better founded goals.
Would you at least recognise the desirability of, and urgent need for, eliminating gender/racial/etc. bias? I feel like this is the practical upshot of that section, that we typically have such biases even despite ourselves, and this can damage community participation and human flourishing generally.
&gt; unsubstantiated claim that diversity offers improvement (regardless of the selection of individuals as long as they are diverse - diversity is the key) What's unsubstantiated about it? Diversity IS the key. It's the differences in themselves that causes interesting interactions and lead to new ideas. &gt; hey maybe a certain demographic just simply doesn‚Äôt care about haskell, have you made a study to refute that? Good that would be enough for me Why don't YOU make that study? Most people seem to be fine with spending a bit of effort to try to increase diversity. Why would they instead spend effort trying to convince you? But if YOU think it's important enough to spend the effort, then I'm sure it would be welcomed.
I'd love to see a video of that. Do you know if there is one?
 &gt; So you give explanation why there is lower diversity than possible Not only lower than possible, but also lower than optimal, if ingroup bias leads to sub-optimal choices. I have no in-depth knowledge about this, but my naive understanding is that ingroup bias is a) efficient for the brain as a quick friend/foe classification. you don't have to spend a lot of sugar on conscious thought and make fast decisions. b) historically a safe default, as detecting a foe that will kill you with high false positive rate is still good for survival. So, again as a non-scholar in these matters, it seems *likely* that ingroup bias might be sub-optimal, and it's not hard for me to intuitively accept this. &gt; I am not saying diversity should be abbolished, just that I wouldn‚Äôt see it as the ultimate goal but merely a possible outcome of a pursuit of some more elementary better founded goals. I am also not saying anything about where diversity should be in a list of priorities. However, I think it's reasonable to have it in an *unordered* list of goals. The guidelines do not put this as the ultimate goal, and I think it's pretty clear that no order is assumed between this goal and for example technical goals. So then one can just assume that it is *a goal*, and for me, if ingroup bias is sub-optimal, it should be a goal as using rational thought (with the extra sugar involved) leads to better outcomes. 
&gt; you can bring in new contributors that are interested in a such anti-bigotry pro-diversity statements Well, would they be more interested those topics than Haskell? If so, maybe it's not a good idea to bring them into the community, since they may spend more time on their political crusades than on Haskell. I think most of us can agree that it's sometimes appropriate to take time to discuss political/ethical issues within the community, and sometimes it's not. Whether or not the discussions are appropriate, they have a high chance of upsetting some people. The biggest problem I see, is that I doubt we could come to any reasonable agreement on a definition of appropriateness in this context, so given that assumption I would err on the side of minimizing such discussions.
What part was disrespectful?
Would be awesome if hackage could let users link to this package from elsewhere on hackage.
I do recognize desirability of eliminating anything harmful that causes unnecessary skewing but I would not support zealotic pursuit of increasing diversity at all costs without being sure what the real causes are. So abusive behaviour for example needs to stop. But if that still leaves us with under-represented group of people and no other reason internal to the community is identified, so be it... but that in my view is not a pursuit of diversity, because no matter the diversity at the end the goal is achieved when harmful stuff was removed based on the harmfulness.
You have some state A, and you turn it to state B with greater diversity... where in this transition lies the inherent improvement? Suppose that then you change A to some C while keeping the internal diversity of its constituents, or maybe even lower it... what makes the ordering C &lt;= A &lt;= B so special and so obvious to you? When you breed animals sometimes you improve the outcome by reinforcing one thing and sometimes you need diversity of genes else you go down a degenerative road... but to say that diversity is THE KEY is in so far an unsubstantiated claim (in general) in this discussion.
With Haskell it's not straightforward, but the actual logic in the reaction comes from `foldMap` and `foldMapFree` (which use the `Monoid` and `Group` instances). But `toList` does drive the evaluation and trigger all of it to happen.
a few years ago some (respected) professor, one of the developers of ghc iirc, loudly made an sexist joke about (ironically) the inclusion of women in his field. a (respected) haskell developer sided with the professor and asked the person who brought this up to apologize. another academic, afaict a grad student at the time, listed it as one of their reasons for distancing themselves from the Haskell community (i won't name names, I don't even know their real name, but i used to read every post on r/haskell when learning the language, and that caught my eye). this person is very smart, they've done a lot of good work on haskell. it costs little to nothing for an old white tenured professor to keep their mouth shut, but does cost people with less power or status a lot of alienation. even if that's the only person the community lost, it would be a big loss to a small community. they're still involved, afaik, so now we've asked them to choose between doing what they love while being spit in the face, or not. and why should they have to choose? in a field/job where a lot of communication can be via text, a lot of work can be done alone, where a lot of times people don't always know the age or gender or race of the person they're collaborating with until they meet in person... those are social properties that should make it easier for us to be inclusive, since things are "more" impersonal. but when someone starts saying bigoted things, even if (by ignorance), they're not being bigoted at any particular person, they're now introducing an (accidental or intentional) personal attack into an otherwise impersonal channel or discussion). this is just one anecdote. I'm giving more details than I would prefer, because people always demand them, but I don't want to dredge up s*** for those involved. if you define "don't be an a******" as "don't make sexist jokes that can make some people feel uncomfortable or even threatened", well, that's not too incoherent, since they probably wouldn't consider it sexist, or they wouldn't consider sexism as existing irrelevant. so, we have to be more specific than "be nice", like "be nice to _". also, framing it as "justifying X backwards" rather than "deducing X forwards" sounds disingenuous to me. from my perspective (and many others), I am to deducing forwards from "why aren't there more women in high-salary jobs like programming, or in intellectually satisfying fields like that of programming languages". and since i don't think the status quo is "natural" or "justified" (you mentioned "artificially raising"), I don't need to make excuses to explain that under-representation, I'm just answering the question. the underrepresented demographics are, in my opinion (and many others) not any worse at programming. even if we you think that women are "worse at spatial visualization" than men or something (I don't), you can't say the same about (say) black men. saying "we value diversity and inclusivity" is such a small thing to say for the people in the mailing thread who are correctly saying it. and yet, it seems like such a big thing for other people to say, like we're pumping their stomach. people with power always feels as they don't have power, or that said power is right. here, they're saying "ok, as white men who have been given morr power by the world, like the opportunity to be in our position (which is probably a job we really like), we acknowledge that." I feel like it should be very minor, because there's a lot more we can do, and that we're not doing. but then conversations like this start inflating it into seeming like someone's taking a stance, rather than just publicly mentioning where they've (and many others) already been standing. 
Defaulting happens either way, the question is: when? Defaulting applies when an expression is required to become monomorphic. With MMR it happens at the let; with NoMMR the let-binding is allowed to be polymorphic, so defaulting happens at (each) print. 
Alright. I do not see how to judge suboptimal but I think it is reasonable to assume it. And I got carried away when using ‚Äúultimate‚Äù when I meant that diversity is not a goal for me, even if it may be the outcome in the end. Doing the right thing for the wrong reasons is not appealing to me.
as a guy, sure, it sounds nice to "keep things separate". but, when people use slurs (this is one of like a hundred examples of what's called a microaggression), when they say "ghetto parallelism" or "emacs virgin" (not making these up) they're introducing the personal into the otherwise intellectual or professional. I would err on the side of minimizing microaggressions like bigoted language. 
If "diversity and inclusion" is achieved by striving for equality of opportunity I have no problem with it at all. However, I think its a really bad idea to try to achieve or enforce equality of outcomes.
It was back in 01 but he was still giving the class when I left Chalmers in 06. Be aware that if there is such a video he held the class in Swedish so it might be tricky to follow :P
Thank you for sharing this, I had no idea such things ever occurred, I only hear people saying bad things happen but never back their claims up. I searched and found nothing on my own. Of course this is an unfortunate outcome. It is desirable to avoid these, but I believe it is desirable regardless of the identity of the person on the receiving end...
Damn, that is what i tried to say, and i ended up spamming this thread.
also, how much someone cares about "politics" (sometimes they're actually generally a political, but in a particular situation, they're being personally attacked) and how much they care about "technical stuff" are not mutually exclusive. we don't maximize the time we spend on stuff every single day. like, even someone who calls out bigotry literally every single time they see it on a mailing list or a forum or a meeting or whatever, that's not actually cutting into their time they're spending working. like, you can work on Haskell for a few hours; then take a break, or submit a pull request, (or something) and see someone not being inclusive, mention it, then get back to work. different parts of your brain. I'm not being hypothetical or rhetorical. like, this is how I see someone who contributes to both (for example) haskell and emacs. they're completely different, but given your knowledge and skill set, you might not have the ability or the interest to spend 8h on one versus 4h either. I think this is true with most intellectual work (including once as seemingly similar as programming in one language versus another), and I think this is especially true on intellectual tasks that are way more different from each other (like talking vs coding). 
Hm, are you using the implementation of FreeGroupL without the O(n^2) bug, the one from the PR?
I think it is desirable regardless of the persons identity. I do think white males need to be a bit more sensitive to other backgrounds though. No other classification has it so easy, and because of this white males may find it harder to sympathise with those difficulties. Speaking as a white male.
You say it yourself. Improvement can be made by working hard on something, but only to a limit. To go past that limit you need novelty. And to get novelty you need diversity. Hence diversity is the key (which is your choice of words to represent my position, remember). &gt; Also I politely asked if somebody could convince me, you dont have to... so maybe calm down with those ‚ÄúYOU make the study‚Äù! It's just emphasis, to underline that it's on _you_ to make the effort if it's that important to you. We're doing you a courtesy in trying to reason with you, but you're in no position to make demands about producing studies so you can have measurable evidence. &gt; To me it seems as if you are trying to prove the opposite of what you are championing by getting carried away. How so? And what am I championing, in your mind?
Would you be less dissatisfied with the CoC wording if this were concerned with the root causes of systemic inequality in society (which explains the demographic skew in PL communities) rather than addressing diversity per se? "Broadening the patterns of participation" is quite vague and can be implemented in many different ways, but I don't think participation in any form of society is a zero-sum game (in particular in open source, which is predicated on freely sharing intellectual labor). I believe in the open-endedness of the human spirit, and that showing consideration, inclusiveness, curiosity towards different walks of life can only attract fresh energies to a community and potentially "lift the tide" for everyone. On the other hand, such vocal opposition to a simple call for diversity is a pretty clear proxy for certain opposite cultural trends, which I will refrain from discussing further. 
[removed]
Did you compile it with all optimizations? I speculate that would be significantly faster than a naive GHCi run.
Sorry, I might have gotten the ‚Äútone‚Äù wrong, but I don‚Äôt see any courtesy in what you said. I posted a question about what makes diversity desirable and you come back at me with no more than turning my question around in a way that to me sounded quite aggressive? I sure can do a study but where did that urge to comment in such a hurt way come from? How does it further the discussion? The ‚Äúkey‚Äù was your word not mine. None of the implications between improvement, novelty, diversity is universaly valid, how so that improvement and novelty require diversity? It doesn‚Äôt hurt in that respect, i grant you that, but my example showed that some improvements are made while doing the opposite of diversification. So it seems to me that ‚Äúdiversity is under all circumstances desirable and leads to improvement‚Äù is not true and that is my way of paraphasing what I think you came to defend.
I do not see how diversity is *necessary* for improvement, less so how *I* said that. My example indicates that while *sometimes* it can help, other times doing the opposite leads to improvement (reinforcing some trait instead of diversification)
Ah, didn't see this PR. I'll try tomorrow to use your version.
I don't know of any packages that do this, and the built-in \`List\` type is definitely not a good choice for this. I suspect the best route to go is to build something that is designed specifically for this purpose along the lines of what /u/ComradeRikhi mentioned.
&gt; I expected it to never print "evaluating strict" Ah, because forcing a polymorphic value does nothing, right? This is a curious question, similar to nifr's one below: When does defaulting apply? Before answering that, let me stress that strictness and defaulting/monomorphism are unrelated constructs, afaict. You can force polymorphic values to WHNF (which does as much as forcing functions - nothing). Similarly monomorphizing (passing a dictionary) does not force evaluation in any way. And in practice, undesired strictness/laziness causes much more problems than defaulting, in my experience. So original article might be more important to get right than the details we are discussing here. Back on the topic: Things get a bit murky because pattern-matching can do both (forcing and monomorphization) at the same time (especially when using `case`). I cannot find a good reference for this stuff, but my intuition boils down to "`case` always monomorphizes, while `let` permits polymorphism (at least with NoMMR)". For example this works: let x = 13 in print (x :: Int) &gt;&gt; print (x :: Float) while this does not: case 13 of x -&gt; print (x :: Int) &gt;&gt; print (x :: Float) (one might think: `case` monomorphizes only because it deconstructs and thereby forces to WHNF, but I think this example shows that it does more - binding to `x` alone should not force anything.) So for your definition, the type of `foo` would be `(Num a, Num b) =&gt; Foo a b`. Pattern-matching with `case` monomorphizes the value, so the usual defaulting applies at the point of `case`. So the compiler translates this roughly to the following code with explicit dictionary-passing: -- foo :: Dict (Num a, Num b) -&gt; Foo a b case foo (Dict :: Dict (Num Integer, Num Integer)) of -- defaulting specializes `a` and `b` Foo { strict, lazy } -&gt; do -- this is a `Foo Integer Integer` logLine print Dict lazy -- we pass some Show Dict here, too 
This was fun and informative, Matt. Thanks for posting! One thing I think worth mentioning is that you can force the strict record fields of a record to be evaluated immediately by using a bang pattern in the let binding. For example, to get the behavior you describe in the second half of the post, you can change: let foo = Foo { strict = trace "evaluating strict" $ 1 + 2 + 3 + 4 , lazy = trace "evaluating lazy" $ 9 + 12 } to let !foo = Foo { strict = trace "evaluating strict" $ 1 + 2 + 3 + 4 , lazy = trace "evaluating lazy" $ 9 + 12 } and it will print `evaluating strict` when it reaches that line since the record is now forced.
I feel a bit conflicted on microaggressions. I don't want to offend, but I also don't want to spend much time policing my speech (although inevitably I do, anyways). &gt; they're introducing the personal into the otherwise intellectual or professional You're placing the burden on the speaker here, not the interpreter of the speech. Communication has to be an act of compromise between the speaker and the listener. The listener has a responsibility to assume the speaker isn't trying to offend, just as much as the speaker has a responsibility to try not to offend. When a listener takes a statement personally, it can be an error of the listener just as easily as it can be an error of the speaker. It's not clear to me where microaggressions begin and end, so I'm not sure I can support trying to minimize them.
I don't recognize the syntax in the day05b function. How does it work? Multiple equals signs and using foldMap inside the argument list?
The part where this topic has been addressed thousands of times in great depth online, and one could easily find those discussions if they so chose. But instead, a question was asked here. Note that I am not leveling any accusations in this next part. I'm describing a behavior that occurs sometime with harmful motives, but do not intend to ascribe those motives to anyone. A common attack on attempts to improve community behavior is to meet it with lots repeated requests to explain "dogmatic" assumptions and why they seen as good or bad. There is a careful art to this form of trolling. It must be performed with a maximum of politeness and it must seek as sincere as possible. The goal is to ensure that those who care get worn down. And it's exceptionally effective, because it takes advantage of the strongest impulses from those who are most eager to help. The same behavior does, in fact, often come from people with legitimate questions. The only way to combat the attacks while remaining polite is to reverse the burden. It is not the community's obligation to interactively respond to such questions. It is the questioner's responsibility to do the research themselves. There are a lot of people out there who have spent a lot of time thinking about issues like these, and have recorded very detailed thoughts. The knowledge is out there. There are lots of tools to direct one to that knowledge. The maximally respectful approach is to utilize those tools to find those resources. Repeatedly coming back with "but why?" is disrespectful in this situation. In the very best case it's an imposition that is difficult to distinguish from hostile behavior. And if it's sincere, it's something that needs far more space to answer well than an off-the-cuff reddit comment. It's better to find well-expressed arguments from people who wish to directly engage with the question than it is to ask random people to explain. 
&gt; The part where this topic has been addressed thousands of times in great depth online, and one could easily find those discussions if they so chose. In many other communities, common questions like that are answered with links to resources. I scanned through this thread, and rather than "hey, here's a link where this is addressed better than I could", I'm seeing long posts about how people are being disrespectful (or worse) by asking about some of the guidelines linked by OP. This doesn't strike me as a situation where a lot of people are simply misinformed and asking the same question too much. It _seems_ like there's legitimate disagreement over the issue, and it's being shut down with downvotes and this line about how that's just trolling. Maybe the truth is somewhere in between? Do you know of a link to a discussion where this has been discussed _without_ that assumption that people with different viewpoints are trolling? I'd be interested in such a read.
Awesome solution! Been learning Haskell for the past 2 years off and on, and currently studying abstract algebra; this post really hits home right now. Hopefully you find more mathematical abstractions in the future problems, would love to read about them.
&gt;*a* *to* *a**‚Ä¶ List of* *a* *to list of* *a**‚Ä¶ It means nothing! It tells you nothing!* ‚Äî Rich Hickey, [Effective Programs](https://youtu.be/2V1FtfBDsLU?t=4020). &amp;#x200B; Can't be clearer than that really.
It's a good guideline and one I generally take as a given. I'm not on the steering committee by a long shot but I don't expect I'd ever have a problem following it. I appreciate the positive tone. While I do everything I can to raise awareness about diversity and share Haskell with everyone who is interested, I am aware that there are people who like the language but avoid this community. They avoid the community because we seem to capitulate to those vocal members who are offended over imagined slights on their privilege to say whatever they want. Why does this guideline avoid enforcement or outline an explicit resolution policy? I have no problem telling someone to stuff it. However what should someone without my privilege or power do if, while interacting with someone on the committee, they are the recipient of hateful remarks or signals?
‚Ä¶And? &amp;#x200B; Haskell predates Clojure by 11 years.
&gt; [..] it costs little to nothing for an old white tenured professor to keep their mouth shut, but does cost people with less power or status a lot of alienation. It's also quite sexist and racist to generalize such behavior to a specific race and gender. You might of course have redefined sexism and racism as is often the case in modern feminist ideology or intersectionalism. But then we are talking about different things. &gt; "why aren't there more women in high-salary jobs like programming, or in intellectually satisfying fields like that of programming languages" Probably for the same reason that more women are in jobs that have to do with people. &gt; and since i don't think the status quo is "natural" or "justified" (you mentioned "artificially raising"), I don't need to make excuses to explain that under-representation, I'm just answering the question. Well, now you're assuming there is something wrong. Under-representation is not necessarily discrimination. It can be. &gt; even if we you think that women are "worse at spatial visualization" than men or something (I don't), you can't say the same about (say) black men. I would argue that anti-intellectualism is much more prevalent in poor and black communities (and unfortunately those are correlated to an extent). &gt; saying "we value diversity and inclusivity" is such a small thing to say for the people in the mailing thread who are correctly saying it. and yet, it seems like such a big thing for other people to say, like we're pumping their stomach. It depends on what it means. Diversity at all costs? Then you quickly get undesired results. The issue I see here is that some people try to smuggle their ideology into such code of conducts. &gt; people with power always feels as they don't have power, or that said power is right. here, they're saying "ok, as white men who have been given morr power by the world, like the opportunity to be in our position (which is probably a job we really like), we acknowledge that." See and here is the problem. You're unpacking your ideology and forcing other people to adhere to it. Every criticism is just from a white old male that is in power.
You could take a look at the Haskell-Tools AST([Link](http://hackage.haskell.org/package/haskell-tools-ast-1.1.0.2)). I'm not sure if it is any better but could be worth exploring. I don't quite like the way it is laid out but that is a personal preference. I've also found haskell-src-exts to make for a long compile time(quite recently I might add) so you can rest easy knowing you are not the only one experiencing the torture.
Precision of language is something I really try to strive for :).
This blows my mind. Demo starts around minute 14.
But the `Either` data type is *not* right-biased. The intent you refer to is not "baked in", it's a matter of how the type is used. The `Bifunctor`, `Bifoldable`, `Bitraversable` instances for `Either` have no bias, and the use of `Either` in `Choice` (from `Data.Profunctor`) and `ArrowChoice` is similarly unbiased. At its core, `Either` is literally just a data type which can hold *either* a `Left` value or a `Right` value; either or both of those values (or none) could represent a valid result.
Thanks for the catch; the multiple equals sign is actually a typo. Using `foldMap` inside the argument list is the `ViewPatterns` extension, but I removed it and rewrote it using `where` to be a little more generally accessible :)
&gt; We do not tolerate any form of discriminatory language or behaviour towards any minority [..] But do we tolerate discriminatory language against majority groups? Isn't any form of arbitrary discrimination or unfair generalization bad? &gt; Where we disagree with someone, we avoid forms of expression that might make our dialogue partner feel attacked, humiliated, demeaned, or marginalised. Our critique should always be of specific statements and claims, never of people. Sometimes people will feel attacked and there is nothing you can do about it.
eh? Wadler isn't on the ghc steering committee. https://github.com/ghc-proposals/ghc-proposals/#who-is-the-committee
I think I can answer this. First I'll note that while your interpretation is as valid as any, the official text doesn't mention anything about being proactive, nor does it prescribe any course of action. I'm on the board of a small cooperative affordable housing non-profit. Four (of five) board members, including me, are cis white men. By observation, our organization overall is a bit more diverse (in several ways) than the surrounding city, but our board is significantly more homogeneous than the membership which democratically elected us. And it's been like this for years, even as directors take and leave office. Our basic conclusion is, clearly we aren't meeting the needs of people from different backgrounds. Why are people from marginalized groups either not finding out about us, or if they are, why do they not stop by and maybe join? Given that we know invisible barriers permeate society and put many people at a disadvantage, it's likely that we have a few around our organization, and others still internally. I interpreted this part of the CoC as acknowledging simply that, for an open and transparent organization, it *is* curious that only white guys participate. Actively trying to dismantle those barriers shouldn't be equated to seeking diversity quotas (which I agree are harmful and problematic).
I'm not saying that either. I'm saying that diversity is necessary for _novelty_, and that novelty is sometimes necessary to improve beyond a certain point. You can't get anything new out of doing the same thing over and over. You have to try different ideas. And the more ideas you try, the more likely it is that you'll come across more novel ideas and more optimal solutions. This seems consistent with that you're saying. But then, as you also say, there might be instances where less diversity lead to faster improvement, which I understand to basically mean focusing on the ideas you already have instead of generating new ones. But we can't very well just scale the diversity of a community up and down at will to try to find the optimal efficiency at any given time. It seems better then to just increase diversity to get the novelty and raise the ceiling (as well as other benefits), and instead use other means to select a smaller set of ideas to focus on. 
/u/nometa would it be worth explicitly stating that the goal is striving for Equality of Opportunity and that Equality of Outcomes is *not* the goal.
Referring to it as either an "easing of requirements" or an "expansion of requirements" is missing half the picture. What is actually happening is that requirements are moving from one place to another. Going from `foo :: x -&gt; y` to `foo :: Maybe x -&gt; y` means that `foo` now has to accept `Nothing` as an input, true, and this expands the requirements for `foo`. *However*, it also means that the *caller* of `foo` is now allowed to pass `Nothing` in place of `x`, which eases the requirements on the caller. The speaker was looking at this from the perspective of the caller, not the function itself. Both perspectives are equally valid; they're two sides of the same coin. Of course, implicitly treating `Maybe x` as a superset of `x` creates its own issues. Given `foo :: x -&gt; y`, a call like `foo (Just 3)` must give `x = Just 3`. However, if you change that to `foo :: Maybe x -&gt; y`, does the same call still give `x = Just 3` as before, or does it change to `x = 3`?
It is biased in the sense that the type parameters have an order, from left to right. This means that it can only be used in one way as a `Functor`, `Monad` etc. as only `Either a` has the appropriate kind (`* -&gt; *`).
&gt; You're placing the burden on the speaker here, not the interpreter of the speech. I'm actually fine with this. It's for the same reason it's up to an email author to follow the formatting and reply-to guidelines for a mailing-list: One author / speaker, many readers / interpreters; a small onus on each reader / interpreter can easily overwhelm even a significant cost on the author / speaker, from any global cost metric.
If we want to be precise with language, then I‚Äôd say, neither is the goal. The goal is, if we have to put it in one sentence, ‚Äúto treat every person with respect.‚Äù There are multiple motivations to have that goal, and diversity and inclusion is one of them. But even if we did not have a problem with diversity and inclusion in this community (which unfortunately we do), respectful treatment still be a worthwhile goal.
Tiny bit of self-plug, but while I was toying with those myself, I also built a couple examples to exercise my understanding. They are publicly available here: [https://github.com/Ptival/recursion-schemes-examples/tree/master/lib](https://github.com/Ptival/recursion-schemes-examples/tree/master/lib) &amp;#x200B; Note that the histoparamorphisms and parahistomorphisms do not work the way \*I\* expected them to, so be wary of those combinations, cf. the discussion here: [https://github.com/ekmett/recursion-schemes/issues/57](https://github.com/ekmett/recursion-schemes/issues/57)
In that case, they should contact one of the two chairs of the committee: &gt; If one of us fails to meet these standards, the ideal course of action is to write to that person privately, gently drawing attention to their lapse. If you're not comfortable with that, please contact the chair of the committee, or (if the chair is the problem) the vice-chair or co-chair. It is true that these guidelines do not outline a precise process about what follows from them. I am optimistic that a person like Simon (either of them) will have the authority to make a misbehaving committee members understand and correct their behavior. We did not want to infest the document with too much negative sentences, but it is the understood that in order to be on the committee, you have to subscribe to these standards. This implies that if one of us continuously fails to meet them, they will have to leave the committee. This is implied by &gt; The committee members have committed to adhere to the Haskell committee guidelines for respectful communication. in the section [Who is the committee](https://github.com/ghc-proposals/ghc-proposals#who-is-the-committee).
I agree. Strict evaluation is useful in some contexts. In particular in evaluation contexs!
Then it may be worthwhile saying that this is not a CoC, but a guide for respectful conduct and civil discourse. Unfortunately, the political division between Left and Right has become increasingly large and heated over the last decade and politics is encroaching on fields like software engineering. Like you say we do have a problem with diversity (I am less sure of any problems with inclusion) in this community and explicitly addressing the Equality of Opportunity vs Equality of Outcomes issue now may avoid future problems.
&gt; But I have trouble to see how all of this boils down to increasing diversity rather than stop being a--holes If you damage someone else's property, is it enough to stop doing more damage?
&gt; Unfortunately, the political division between Left and Right has become increasingly large and heated over the last decade and politics is encroaching on fields like software engineering. The extremely conservative reactions in many comments to this post, including those from you, illustrate extremely well that "let's just keep ~politics~ out of it" is itself, as it always has been, a political position, and one that tends to comfort the comfortable and afflict the afflicted.
You might be referring to https://www.reddit.com/r/haskell/comments/zxmzv/ and https://www.reddit.com/r/haskell/comments/17jy0e.
&gt; Then it may be worthwhile saying that this is not a CoC, but a guide for respectful conduct and civil discourse. Indeed! That is why this document is intentionally not called a CoC. 
Break up haskell-src-exts into smaller packages and have downstream packages default to depending on the smallest subset of those packages going forward? This isn't based on in-depth analysis of the situation, just noting an idea in case someone else sees potential in it.
1. i'm generically answering a question about privilege (being about power). that's it. i didn't write the code of conduct, and even if I did, the clause being debate would be independent anyways. ideology is "what people who disagree with me believe in or care about". what you believe is either "ambient", "fact", or "opinion". (sorry, you brought it up). Simon Peyton-Jones is (i think) the author of this Code Of Conduct. he's involved in a campaign for programming education. afaik, they use Python. (just an example of how non-idealogical he is). I've never worked with him, but from what I've seen, he really listens to his collaborators and to the community (c.f. other leaders of programming languages). also, the man is a genius (not everyone with his intelligence and influence are that nice of a person). again, he is the one who advocates diversity. 2. &gt; Every criticism is just from a white old male that is in power. no it's not. it's the **committee** (mostly white guys, afaik, as they literally said "skew white and male") that added that clause about "inclusion/ diversity". **you're** the one arguing with an old white male (i.e. SPJ). 3. caregiving jobs are often unpaid caregiving tasks. being a homemaker is unpaid labor. nurses and teachers get paid less than programmers and engineers. i.e. high-value work (that a society requires), which is done more by women than by men, is de-valued by patriarchy, and thus earns a lower wage (/ costs a lower price). 4. this isn't about me, I didn't write the code of conduct. I'm just answering someone's question because, while I disagree with their premises, they asked in good faith. I can just provide some background that I've learned from others, and then if someone with the actual experience feels like joining the conversation (they probably don't feel like doing this for the thousandth time), i might have accelerated the conversation past the easy stuff. 5. no one said diversity at all costs. Simon Peyton-Jones didn't say "I fire all the whites, then I resign". no one is optimizing the compiler's development against the constraint of maximal diversity. 6. I wasn't generalizing any behavior to any demographic. I referenced something you seem ignorant of; a *haskell compiler author* made a sexist remark, which alienated a lot of people, some of whom are/were important contributors.
[Here is a preview](http://gelisam.com/files/comonadic-recursion-schemes/Data-Functor-Foldable.html) of the documentation (with examples!) I plan to add to [recursion-schemes](http://hackage.haskell.org/package/recursion-schemes) once I finally get a chance to review it.
Think you need more coffee. You wrote inOrderCata twice instead of the only left example üòÖ
Lol, thanks! I'll fix that, I wrote my own article build pipeline! Probably have the wrong annotation on one code block üòÑ
Please show me anything I wrote here in the comments that justifies you calling those comments "extremely conservative".
[removed]
The reddit voting system is not about respect, but polite communication and relevance. You're being intolerant towards somebody of an opinion you do not agree with. So, ironically, you are against diversity (of ideas). You can have a civil discussion and disagree with him, instead you choose to down vote and silence him. My problem with discussions like these is also that they conflate diversity of ideas with superficial diversity (gender, skin color) . They ignore just how diverse in background and opinion people who are superficially similar (e.g. "straight white males") can be. Although this happens out of good intentions it leads to real racism and bigotry. "Straight white males", e.g. one from France, one from South Africa, one from Russia, one from the US are virtually guaranteed to be more diverse in ideas than an African-American lesbian woman and Asian-American straight woman who grew up in the US. I hope you will consider jd's comments more seriously.
You do realize that what you've done there is precisely indistinguishable from trolling, right? Like, if you were going to troll, how would you change it? 
That's a fair point, and a point for codes of conduct in general. But this only really works if there already exists a shared world view within the community, and it's just being made explicit.
[removed]
I don't think this is possible or relevant.
Does the set y contain itself?
There is a good post explaining this on stackoverflow somewhere. I'll try and find it.
Your original version can also be expressed as memFib = let fibList = map fib' [0..] in (fibList !!) where fib' 0 = 0 fib' 1 = 1 fib' n = memFib (n - 1) + memFib (n - 2) Which is the same as memFib = let fibList = map fib' [0..] in \m -&gt; fibList !! m where fib' 0 = 0 fib' 1 = 1 fib' n = memFib (n - 1) + memFib (n - 2) Where the second version is equivalent to memFib m = let fibList = map fib' [0..] in fibList !! m where fib' 0 = 0 fib' 1 = 1 fib' n = memFib (n - 1) + memFib (n - 2) Which is the same as memFib = \m -&gt; let fibList = map fib' [0..] in fibList !! m where fib' 0 = 0 fib' 1 = 1 fib' n = memFib (n - 1) + memFib (n - 2) So it comes down to comparing: - `let fibList = map fib' [0..] in \m -&gt; fibList !! m` and - `\m -&gt; let fibList = map fib' [0..] in fibList !! m` In the first case `fibList` is computed once and reused for every call of the returned function. In the second case `fibList` is recomputed for every value of `m`.
yeah, ... aka phil wadler
&gt; in your example it looked like tax had become a separate variable that I would have had to plumb through the rest of my code. Indeed it was. Now that I understand your question better, I think extensible records (also called "row types") would be a better match for what you want. Haskell doesn't support row types, but there are similar languages like Elm and PureScript which do. In those languages, I think you'd be able to write something like this: addTax :: {total :: Float | a} -&gt; {total :: Float, tax :: Float | a} addTax basket = basket { tax = 0.10 * basket.total } addDelivery :: {address :: String | b} -&gt; {address :: String, delivery :: String | b} addDelivery basket = basket { delivery = "deliver to " ++ basket.address } addTaxAndDelivery :: {total :: Float, address :: String | c} -&gt; {total :: Float, address :: String, tax :: Float, delivery :: String | c} addTaxAndDelivery = addDelivery . addTax The `| a` means "and some other fields", so `addTax`'s type says that given a record which contains a `total` field of type `Float` and some extra fields, `addTax` will produce a record containing those extra fields, the `total` field, and a new field `tax`, also of type `Float. Similarly for `addDelivery`. Now, when type-checking `addTaxAndDelivery`, the compiler knows what those extra fields are, so it instantiates `a` to `{address :: String | c}`, so `addTax` returns a value of type `{total :: Float, tax :: Float, address :: String | c`}, and then it can instantiate `b` to `{total :: Float, tax :: Float | c}`, so `addDelivery` returns a value of type `{address :: String, delivery :: String, total :: Float, tax :: Float | c}`. If we try to add a function which uses the `tax` field, it will type check if we put it after `addTax` executes, but if won't if we put if we put it before. Which I think is what you're after? Since Haskell doesn't support row types, using them is definitely not idiomatic, but there are a few libraries which approximate them. Here is an implementation using the [`composite`](https://hackage.haskell.org/package/composite-base) library: {-# LANGUAGE DataKinds, FlexibleContexts, TypeApplications, TypeOperators #-} import Control.Lens ((^.)) import Composite.Record import Data.Proxy (Proxy(..)) import Data.Vinyl (type (‚àà)) addTax :: ("total" :-&gt; Double) ‚àà basket =&gt; Record basket -&gt; Record (("tax" :-&gt; Double) ': basket) addTax basket = (val @"tax" tax) :&amp; basket where total :: Double total = basket ^. rlens (Proxy @("total" :-&gt; Double)) tax :: Double tax = 0.10 * total RoadyMyRoad St"} addDelivery :: ("address" :-&gt; String) ‚àà basket =&gt; Record basket -&gt; Record (("delivery" :-&gt; String) ': basket) addDelivery basket = (val @"delivery" delivery) :&amp; basket where address :: String address = basket ^. rlens (Proxy @("address" :-&gt; String)) delivery :: String delivery = "deliver to " ++ address addTaxAndDelivery :: ( ("total" :-&gt; Double) ‚àà basket , ("address" :-&gt; String) ‚àà basket ) =&gt; Record basket -&gt; Record ( ("tax" :-&gt; Double) ': ("delivery" :-&gt; String) ': basket ) addTaxAndDelivery = addTax . addDelivery And here are some examples of using those functions: &gt; addTax ((val @"total" 10.95) :&amp; RNil) (val @tax 1.095) &amp;: (val @total :-&gt; 10.95) :&amp; RNil &gt; addTax ((val @"items" ["sugar","spice"]) :&amp; (val @"total" 10.95) :&amp; RNil) (val @"tax" 1.095) :&amp; (val @"items" ["sugar","spice"]) :&amp; (val @"total" 10.95) :&amp; RNil :&amp; RNil &gt; addTaxAndDelivery ((val @"items" ["sugar","spice"]) :&amp; (val @"total" 10.95) :&amp; (val @"address" "1234 RoadyMyRoad St") :&amp; RNil) (val @"tax" 1.095) :&amp; (val @"delivery" "deliver to 1234 RoadyMyRoad St") :&amp; (val @"items" ["sugar","spice"]) :&amp; (val @"total" 10.95) :&amp; (val @"address" "1234 RoadyMyRoad St") :&amp; RNil Now, that syntax is a lot hairier (e.g. to access the "total" field, we have to write `basket ^. rlens (Proxy @("total" :-&gt; Double))` instead of simply `basket.total`!), but it's the same idea: `addTax` states that it needs a field named `"total"` of type `Double` and that it provides a field name `"tax"` of type `Double`, and the program won't type check if you call a function which asks for a field that isn't provided by a previous function.
That wasn't wadler, carter.
Higher rank types are frighteningly expressive, especially with constraints and GADTs. In my opinion that style of programming is under-explored, perhaps because dependent types are very hot(t) at the moment. As an example, one can try to count the rank of the types in the machines library :\^) http://hackage.haskell.org/package/machines-0.6.4/docs/Data-Machine-Type.html System F is very strong.
I'm a regular r/haskell lurker, so it's nice to find my blog here! The content in this article was not really aimed at Haskellers, most of whom are already quite familiar with these higher-order forms of abstracting types from things. Re-reading it now, my main regret is describing parametric polymorphism with no mention of parametricity. I feel like I need to make up for this by writing an article about deriving free theorems...
Firstly, this isn't a code of conduct. But also, I would like to believe that the Haskell community already follows these guidelines except for rare cases that we'd like to make even more rare (or eliminate).
To add to your point about the power of System F, I was amazed when I first learned that you could define a sorting function in pure System F after first defining numbers, lists, and bools (via relatively straightforward Church encodings). And since System F is strongly normalizing, the sorting function is necessarily total by construction.
&gt; You're placing the burden on the speaker here, not the interpreter of the speech. I think the majority of the burden should be on the speaker, but the interpreter should assume that the speaker is acting in good faith. If the speaker is acting in bad faith they are cowardly if they don't make that bad faith obvious.
Wires crossed somehow. In my head Clojure has only one sale proposition - immutability... Whereas Haskell brings so much to the table, immutability gets lost a litt.e Btw, I think Erlang is better at immutability than Clojure, and it predates many languages by decades : )
The STLC is strongly normalizing, I don't think that is true of System F because you can give a type to self-application...
As I said, and probably should have stressed out more, I do like the document. I just noticed a theme that has been reoccuring lately in the haskell community and many others, that being praise of diversity and I felt like maybe people are attributing things to diversity without much thought. I only wanted to know what it was that after careful consideration it was that made diversity so highly desirable even in absence of evaluation of other indicators.
&gt;I see now how they can lead to performance decrease for applications that don't require multithreading capabilities They pretty much always do, unless you're getting nice gains from concurrency, it's rarely worth to have -N. &gt;But it looks like there are more chances that a person who wants to write a web-application in Haskell won't know about these options (and these options should be enabled for web-application) and thus the default values are ok rather than a person who wants to write high-performance single-threaded algorithm. I think a lot fewer programs are written in concurrent fashion by default. Even then, `-N` is usually a _bad_ default still: unless you have very high scaling, on a machine with high core counts, you usually see the performance _degrade_ after `-N4` (or whatever small number) and on modern machines `-N` can translate to `-N12` or more. I think this should be the conscious choice by the user or at least very clearly indicated and switchable off in TUI/whatever. 
Or they could just dig up your twitter feed, call you racist, and kick you off the project?
While you show that the cata version is more powerful than just using Foldable, I don't understand what its advantages are over explicit recursion. For example, your cata version of the infix ordering: inOrderCata :: BinTree a -&gt; [a] inOrderCata = cata algebra where algebra :: BinTreeF a [a] -&gt; [a] algebra EmptyF = [] algebra (BranchF a l r) = l ++ [a] ++ r could just be written as: inOrderExpl :: BinTree a -&gt; [a] inOrderExpl Empty = [] inOrderExpl (Branch a l r) = (inOrderExpl l) ++ [a] ++ (inOrderExpl r) Personally, I find the second one clearer.
That is true, though that's more of a limitation of the typeclass system which isn't flexible enough to bind whichever type parameter you want (missing type-level lambda), as opposed to a bias in the `Either` type itself. Even that can be worked around by defining a wrapper like `newtype Flip e b a = Flip (e a b)` which can then have instances for `Flip Either b` which work on the `Left` constructor instead of `Right`.
As u/rampion pointed out it is related on how the expression is reduced which allows compiler to not compute the expression for every value of `m`. I would like to add that you could find a great explanation in [Haskell's book Programming from the first principles](http://haskellbook.com/progress.html#laziness), in particular in Nonstrictness chapter where authors explain in great detail how Sharing mechanism works in Haskell. Basically it cores fundamentals sustain that when a computation is named, resulting evaluations of the former are shared between all references to the same named computation. I would strongly recommend this reading if you want to get deeper on how this works. There are several of practical examples. Best, 
For one, catamorphisms are primitive recursive, so you can be sure they terminate. It‚Äôs also easier to fuse successive operations due to the clean separation of the algebra from the recursion.
Very nice! Another interesting dimension in the same general space is [Session types in Cloud Haskell ](https://dspace.library.uu.nl/handle/1874/355676).
[removed]
That's true, but I think it's clear from the post that they're aiming to set a high standard internally in order to inspire better behaviour more generally in the community. Wadler isn't on the committee, but he's one of the more prominent faces of functional programming, and also one of the least respectful. My comment was more that although I think this is a great move, I think there are deeper rooted issues in the community, including with some of the most prominent people.
I love the closing remarks üòç this is how I feel every time I notice cool abstractions in Haskell!
One of the annoyances of Haskell is how you get a lot of differently-named functions to handle what would be optional arguments in say Python. E.g. `sortBy`/`sortOn`/`sort`. Allow me to present a cure which is worse than the disease. {-# LANGUAGE AllowAmbiguousTypes #-} module Awful where import Data.List (sort, sortBy, sortOn) class AwfulSort t (a :: *) (b :: *) where type AwfulSortT t a b awfulSort :: AwfulSortT t a b instance Ord a =&gt; AwfulSort "sort" a b where type AwfulSortT "sort" a b = [a] -&gt; [a] awfulSort = sort instance AwfulSort "sortBy" a b where type AwfulSortT "sortBy" a b = (a -&gt; a -&gt; Ordering) -&gt; [a] -&gt; [a] awfulSort = sortBy instance Ord b =&gt; AwfulSort "sortOn" a b where type AwfulSortT "sortOn" a b = (a -&gt; b) -&gt; [a] -&gt; [a] awfulSort = sortOn Now you can do `awfulSort @"sort" [1,2,3]` but also `awfulSort @"sortOn" negate [1,2,3]`. And only one function needs to be exposed. But please don't.
With default arguments, you can get rid of sortOn by using id. How would you get rid of sortBy though? My gut instinct is the problem is arising from the inability to assign a maximally flexible type to a combined sortBy+sort function (because one has an Ord restriction and the other doesn't), not from the lack of default arguments.
I've been posting in good faith, and I believe others have as well. If you want to avoid redundant discussions, please link some good summary resources. Crying "troll" is suppressing of other viewpoints and is antithetical to respectful communication and, indeed, diversity of thought. 
The second is clearer to you because you are familiar with explicit recursion but new to recursion schemes. The promise of recursion schemes is that once you're familiar with the names, seeing a word like "cata" immediately tells you "aha! this is a bottom up algorithm. There is going to be a base case and a recursive step". Then you can focus your attention to one or both or neither of those sub-steps, depending on what you're trying to understand about this function. Whereas if you see "para", you instead think "aha! this is like Map.insert, we're going to update the spine and leave most of the data structure alone". With explicit recursion, you have to read and understand the whole definition, and only then do you get the big picture. If you're already familiar with what the function does, then maybe you remember the big picture and so you can more easily focus your attention to the pieces you need; but if this is the first time you look at the code, it's helpful when the code tells you what the big picture is. Personally I think the names "cata", "para" etc. are terribly unintuitive, but I know many beginners say the same thing about Monad, so I guess I'm just not used to them yet!
Hi, I am looking for a type-safe way of allowing a user specify a field of a record, to then perform a lookup from an external source. data User = User { username :: String, bio :: String } For example I would like the user to write: viewField User{} @"username" And then be able to access the string `"username"` in the definition of `viewField`. Something like: viewField :: forall f s a. (HasField' f s a, Typeable s, Key s) =&gt; s -&gt; f -&gt; IO (Maybe String) viewField s f = do -- Can use Typeable to get key of external table. let tableKey = show $ Typeable.typeOf s -- Can use Key typeclass to get row key in external table. rowKey = keyOf s columnKey = show f -- ? 
Thanks! Just spend a few hours reading over a number of your recent writings, very clear explanations in each one - awesome work
Do you know of any ongoing work that might make this a bit "smarter"?
Heads up: you give the code for `AddressL` but then display a picture of `CustomerL` instead.
My understanding is that the compiler is better at optimizing recursion schemes than explicit recursion.
Thank you for the clarification! The community can only be improved by having more and better guidelines.
If you're using nix, you could consider building it once on a beefy machine and pushing it to the nix binary cache. This is assuming you're not working on haskell-src-exts itself or one of its dependencies.
It is desire-able because altruism is often seen as a positive characteristic worthy of emulating for its own sake. Helping more people participate in the Haskell community and join the steering committee is seen as an altruistic endeavor. Therefore it is desire-able. Affirmative action policies seem to work. Guido van Rossum, the former Python lead, has taken action to specifically include more women in the core CPython developer team and it has worked. There are now more women involved in the core CPython development team than there were before. The Python core team never directly excluded or discriminated against women. Yet despite trying to make the core development team open to women members there never were any (or few). You can hand wave all you want about meritocracy but without affirmative action the inherent biases of the group will almost certain exclude others even if it is not their intent or desire to do so. Only when they actively sought to on-board women and give them coaching and encouragement and actively recruit people did they finally start increasing the number of women on the core team. That's a big deal. While I see this guideline as diversity and inclusiveness level 0... it's a good start. However simply saying, "anyone can join," doesn't mean every can or will. Does that answer your question?
Well spotted, thanks! Fix is going out
&gt; This is an issue when, for example, we parse some text file, obtain a tagless final AST, and want to interpret it twice: to evaluate and to pretty print. Maybe I'm missing something, but this seems like a severe limitation that makes debugging much harder than it should be. Nice to see someone discuss both pros and cons of the approach. We need more balanced articles like this üòâ.
Nice overview of build tools! I've written the blog post with the description of basic workflows for `cabal` and `stack`: * https://kowainik.github.io/posts/2018-06-21-haskell-build-tools I think these two blog posts can nicely complete each other :)
I am guessing there are some unspoken assumptions: using stack, solution does not involve major feature additions to stack, solution can be easily used in various hosted CI's such as Travis.
I am really sorry but I must say it doesn‚Äôt. All I read is, people say it is desirable because they deem it desirable. Which is valid viewpoint I guess but not very satisfactory and convincing one. To your account of python team: what was wrong about *WOMEN* not being represented in the team in high numbers? I presume it might have been that there was something that discriminated against women and then it got fixed / worked around and the situation changed. Let me go with that. But do I think the motivation should have been ‚Äúlet‚Äôs have more women in here‚Äù? No! Would the story change if members of the same demographic as was already well represented in the team felt they did not have fair chance to compete over membership in the team? I guess it wouldn‚Äôt really matter to someone looking for diversity above all. What were the factors that held women back? Did those really only apply to women? Did those get discovered only because of the lack of diversity? Was there no other grounds for removing those factors? I mean couldn‚Äôt we focus on what drives people away regardless of who they identify with or how we categorize them? You will never appeal to everyone, why is it so important that you appeal to representative of every group imaginable? Your remark about altruism: since when is altruism bound to minorities? As I see it people conflate reasons with consequences. Is a patient really healed once you make the symptoms go away? Maybe maybe not. Is everyone with runny nose necessarily ill? Again, no. So while looking for symptoms is helpful setting on a crusade to abolish the symptom may not be as productive as digging even deeper for better understanding... so I will reiterate one last time and then I will stop responding to stop the outrage: if diversity is the outcome of otherwise reasonable change I am all for it, I even agree lack of diversity might be a red flag (which then could prove benign), but to act based on one oversimplified metric like that to me seems actually rather lazy and doing disservice to the community because while we achieve better rating in form of the simplified indicator we have not really addressed the problems that affect people regardless of their race, gender, ... So here is where I stop. I hoped to get a convincing answer to what maked diversity provably better worthwhile and unharmful to include it in a document that should help form interactions inside a community. I think I only made people angry by challenging (the prevailing) opinion on a very sensitive topic. This was meant as an honest inquiry and I meant no harm so I will stop furthering it now since we have really gone through the same thing multiple times.
Is there any case where you wouldn't want to float the lambda as far right as possible? Assuming the function is pure, could the optimizer perform this optimization in a safe way?
A Prism perhaps? All Prisms are Reviews and Folds but not all of them are Isos.
I understand that Prisms can only supply at most one value of type a for each value of type s, or is there something I'm missing?
How about `Prism' s (NonEmpty a)`? Discard the tail during construction `NonEmpty a -&gt; s`. Not sure if it would be lawful though.
It is certainly sometimes that ought to be be improved on the upstream and/or GHC side. I have had trouble with it when building Debian packages on weaker architectures... I vaguely remember that some gcc flags improves matters. But someone needs to invest the time to investigate where exactly the high memory consumption comes from, and come up with ideas to fix it.
It doesn't matter if you've been posting in good faith, if your behavior is indistinguishable from trolling. It is *your* responsibility to ensure your behavior is distinguishable from trolling. That's the difference between polite and respectful. 
This is cool, I C# for a living and have definitely missed lenses when working with nested immutable structures. For the really pervasive domain objects I've just added some "map over" methods but couldn't come up with anything generic I was happy with.
Woah there! Is everything ultimately political, then?
if you‚Äôre not going to reuse the let-variable, then it becomes unnecessary bloat to hold it in memory, and may prevent optimizing it out. 
At last! A clear answer about what the "tags" in "tagless final" are, and what would be a "tagless initial" encoding. I had wondered about that. Is "tagless initial" the same thing as what is sometimes called a "intrinsically typed interpreter", like [Stitch](https://cs.brynmawr.edu/~rae/papers/2018/stitch/stitch.pdf)?
You're getting upset over something that you're imagining. It's not on me to protect you from that. Hang on, I can hear the reply coming... _I'm_ the one trolling here because I'm calling you out for being frustrated, right? No, that whole line of thinking is bullshit. It's a trap that lets you say whatever you like and be however frustrated you like while shutting down anyone who disagrees because of course they're trying to provoke you like the good trolls they are. So, here, the politeness is coming off: Fuck off with that toxicity. People disagree with you. They did so politely _and_ respectfully _and_ in good faith. Get over it and grow up.
jd823592, with all due respect over 20% of the responses to this post, which is about respectful communication, are from you, on the topic of why you think diversity should not be a core value. Frankly, this seems inappropriate. Not only because it's almost a complete non-sequitur focusing on a single word in the guidelines, but because you're dominating the conversation here, and I see no indication that you're actually open to the idea that maybe the fact that nearly everyone in this discussion disagrees with you means you should revisit your beliefs. You can believe whatever you want to believe, but I do want to ask you to consider that your approach here might not be as respectful as you think.
I completely agree. The threaded RTS is definitely something you don't want unless you *need* it. I would consider this an undesirable default, too.
In the example of parsers, do you imagine the `s -&gt; [a]` mapping to be some form of nondeterministic parser ("this string can mean any of these things"), or is it about representing sequences (e.g., parsing lists in Haskell)? When talking about the `lens` library, I think it is important to distinguish a couple of ideas. This might give you some sense of where you want to go: - `lens` is a library of bidirectional transformations, a particular family called "optics", - related in a hierarchy (the diagram in the package's description), - they can all be implemented as functions of the form `p a b -&gt; p s t` for well-chosen `p`, so that the aforementioned hierarchy can be made to coincide with a subtyping relation naturally induced by a careful choice of type classes. For users of the lens library, the last point should be an implementation detail that would ideally be irrelevant to using the library (but in the current state of things, that detail unfortunately leaks in error messages). Now back to your question, it depends on which points we frame it under. First, as a bidirectional transformation, the basic ingredients are two or more directions (`s -&gt; [a]` and `a -&gt; s` that you already have), and some spec relating them, and now you've got a product to sell (possibly for free); you just need to find buyers. -- Equiv s a describes an equivalence relation, by assigning an element of s to -- equivalence classes of a data Equiv s a = Equiv { reprs :: s -&gt; [a], classify :: a -&gt; s } -- 1. map classify (reprs s) = map (const s) (reprs s) -- 2. every a should be an element of reprs (classify a) Looking at the points above, the other two more theoretical questions are whether it fits somewhere in the hierarchy of optics, and whether it can be implemented as a profunctor optic `p a b -&gt; p s t` (note that they're separate questions). I have no idea.
If there's a power structure, there's politics.
Thanks for the great writeup! &gt; cabal once had a single repository for all applications and libraries installed by a user. This meant that everything had to be compatible. This sounds exactly like `cabal v2-*`'s "nix like" store except you don't need everything to be compatible with each other. &gt; But the package set doesn‚Äôt have to be produced by manually solving the constraints, we can use Stackage‚Äôs freeze files! What do you mean by "manually solving"? Doesn't cabal solve automatically by default? &gt; **Of course, the easiest way to achieve reproducible builds is to ban network lookups during the build and to have everything locally available, which is the industry standard in military contracting and investment banking.** 100% this! &gt; Unfortunately, cabal can lag behind stack for features, simply because it is currently the second-most popular build tool. According to whom? What features? 
I think this is wrong too: &gt; new ValueL().Set(new KeyValuePair&lt;string, string&gt;("foo", "bar"), 3) should return a KeyValuePair&lt;string, int&gt; ‚Äî that is, a new KeyValuePair with a different type to the original. int should be string.
Thank you for the insight. So if I understand correctly, recursion schemes are to explicit recursion as map/fold/filter are to just for looping over a list. The advantage being that you can express more underlying structure of the algorithm by using more specialized primitives. Or am I wrong in this assessment?
No
Do you guys hire new grads/interns?
Yes to new grads. We currently have 2 interns for the summer and we can‚Äôt support any more at our current team size 
I dislike the term "tagless final", because it's not final in any commonly used sense, and fails to connect to wider theory. The proper name would be "Church encoding", and it is actually *weakly initial* in type theory and with some handwaving, in Haskell. 
If you update the `Value` to be an `int`, you should get back a `KeyValuePair&lt;string, int&gt;`.
How does this limitation affect debugging?
[How diversity makes us smarter](https://www.scientificamerican.com/article/how-diversity-makes-us-smarter/)
You might want to both evaluate and pretty print the same thing and seems like you can't do that here...
my bad
its a bug fix release with a present for those on recent ghcs, injective `Mutable`, plus boxed vectors will play nice with compact heaps now!
&gt; Unfortunately, cabal can lag behind stack for features, simply because it is currently the second-most popular build tool. I would have agreed with this a few years ago, but today (with "Cabal 2.0" being mature), I prefer `cabal` (though I try to make sure my projects build with both). it's possible I'm missing some newer features, since I've been tracking cabals development more than stack's recently. but since cabal supports project development (i.e. several interdependent packages that depend on local versions of each other), non-hackage dependencies, and recently added a stanza for arbitrary tarballs and git repos. it also supports `signature`s, which are a very interesting and powerful new language feature that's mostly been implemented on the package manager level. if you haven't tried it out I really would. btw, the `cabal.project` file is the `cabal new-build` version of a `stack.yaml` (i.e. define settings for a project with several packages). see &lt;https://www.haskell.org/cabal/users-guide/nix-local-build.html#configuring-builds-with-cabal-project&gt;.
Are you sure you're looking for an operating system? How would this be different from Android OS, for example?
&gt; If we try to add a function which uses the tax field, it will type check if we put it after addTax executes, but if won't if we put if we put it before. Which I think is what you're after? Yes, that‚Äôs exactly right. Thank you very much! I had heard the term ‚Äúrow type‚Äù before but I didn‚Äôt really understand. This sounds very promising. 
Yes, exactly!
I don't want it to be connected to a particular store. 
Which stores do you want to work with it?
&gt; Thanks for the great writeup! :-) &gt; What do you mean by "manually solving"? Doesn't cabal solve automatically by default? This is in the case where we use a freeze file obtained from Stackage: the "solver" is basically just a verifier at that point. Which we can disable further by using allow-older and allow-newer. I don't personally use Hackage freeze files in the few projects I've released to Hackage but it's essential for my (huge) work application. &gt; According to whom? What features? The features that came to mind were downstream tooling, not core, e.g.: - docker support (I created the linked script to fix that) - hoogle project wide integration (Emily is fixing that) - HIE / LSP integration - some editor things like flycheck (e.g. detecting language extensions) / intero - weeder But as I wrote, I think these things are easy to fix with just a weekend of hacking, so let's get started :-)
Maybe you would like something like https://www.lineageos.org/ with https://f-droid.org/
&gt; btw, the cabal.project file is the cabal new-build version of a stack.yaml Yes absolutely. Did you get to the bit where we convert a stack.yaml to a cabal.project using the various free software tools that are available to do that? I'm using it quite successfully at work to use cabal in a stack project.
Nothing. Ideally it will have preinstalled apps. Apps would have to be built too I guess, but they are not going to be many. 
no, that's pretty cool. relatedly, i was writing a `cabalproject2nix` tool myself, but never finished it.
You are probably going to need a development team...not just one developer..
Thank you. We are looking at other options as well, but I'm wondering if anyone has a Haskell-oriented advice. The basic idea behind this is to throttle plethora of app-downloads. Is Haskell not a good road to take with mobile systems?
I can't believe I'm saying this, but OCaml might be a better idea. It's strict, so there's less chance of unpredictable long delays, and Jane Street Capital has been using it on embedded systems without an OS.
[removed]
Yay injectivity! My family won't be able to top that present.
That paragraph lets you know that you can use a newtype wrapper to use a value twice. That just prevents you from extending that term for free. It's not a severe limitation, it's an annoyance.
&gt; (with "Cabal 2.0" being mature) Given that the last stable release still has bugs that break workflows enabled with new-* features I can't really agree with mature. The ones I know about are fixed in HEAD, so at least for this it's only a matter of time. But I'm sure I will more bugs in still. It's certainly already useful, and it's getting better still! But there are still a lot of oversights to be fixed before we can call it mature.
Using a newtype is a simple solution at a small scale and seems like a mild annoyance. If the AST fragment is (say) inside another record, and you want to use the newtype there, then you might have to change more code... I see your point but I'm not 100% convinced. I should try this out sometime.
Thanks ! This was long needed. We can argue on technicalities but your message to find a solution is truly the most important one here, IMHO. Personally, I dabble on Haskell a lot but the tooling although much better year after year makes it hard to promote it to my teammates or other hobby programmer. Haskell in itself requires at first much more resilience than many other languages simply because it is much different from the norm. However the real issue to adoption is tooling. It‚Äôs really cool to see that it is now a real point of attention in the community. 
I've been thinking about this issue a lot, and I think I've come up with a solution. If you run `evolve const c`, then each element of `c` is replaced with its location (so `evolve const :: CA p c =&gt; c a -&gt; c p`). Now, once you have this list of locations, running `peek p` should just give you the same value `p` back - but *only* if `p` is in bounds. Thus it should be possible to define `inBounds p c = peek p (evolve const c) == p`, which lets you define an updated version of Law 2: &gt; For all `p`, if `inBounds p c`, then `peek p (evolve f c) = f p c`. Do you think this rule would be good /u/gelisam? Certainly QuickCheck seems to confirm that it applies, but it may be debatable whether it is a good law...
i haven't run into any bugs myself since ~2y ago when I switched, but that's good to know. (I don't develop Haskell professionally, but I have a lot of projects with various requirements w.r.t FFI, number of dependencies, etc). 
To be fair there still is a good chance that you won't run into any of these bugs. And the big issues I had will be gone with the next release as well. So if you want to avoid running into the occasional edge case I would wait for another release or two. But if that's acceptable then I wouldn't worry.
Why not \*n\* was to build your software? At least then most of the time one of the \*n\* options should work. I test Stack every year, and I got random bugs in Stack, so then I wait another year.
But then by definition of the set y, the set y contains itself, thus the contradiction
No. TH gives you actual code generation. Dependent types allow you to encode more information at the type level, but this is a completely orthogonal concept to code generation. Code generation is also different from higher order functions. Super simple example; The `shakespeare` library has an [`I18N`](http://hackage.haskell.org/package/shakespeare-2.0.20/docs/Text-Shakespeare-I18N.html) module that lets you automatically generate text translation functions from a simple DSL, so it can turn an input file like this: en.msg: ``` Greeting: Hello World! Purchase count@Int: You bought #{count} items! ``` de.msg ``` Greeting: Hallo Welt! Purchase count@Int: Sie haben #{count} Dinge gekauft! ``` into something like this (slighlty simplified): ``` data Language = English | German data Message = Greeting | Purchase Int translate :: Language -&gt; Message -&gt; String translate English = translateEnglish where translateEnglish Greeting = "Hello World!" translateEnglish (Purchase count) = "You bought " ++ show count ++ " items!" translate German = translateGerman where translateGerman Greeting = "Hallo Welt!" translateGerman (Purchase count) "Sie haben " ++ show count " Dinge gekauft!" ``` No amount of dependent typing is going to let you create a DSL quite like that. There are lots of other interesting things you can do with TH. Dependent typing is never going to replace it, and TH is never going to be a substitute for dependent typing. The two simply aren't very closely related at all.
Dependent types alone won't really solve the code generation issue. You still need a way to convert from a small number of statements into a bunch of data types with varying structures plus a bunch of definitions. A lot of TemplateHaskell could be obviated by proper structural typing support such as rows, records and variants. For example lens TH would be replaced by building lenses off of the underlying row/record/variant, and should not require any TH. It could also potentially help with TH like Apecs or Persistent, as the structural types can be generated on the fly without any TH required, although some TH or (far less than usual) boilerplate might still be needed for creating the nominal types (newtypes) on top of that structure.
i haven't no
I don't know that the "conservative" label is useful here. But it's definitely extremely *something* that this very weak and nonconfrontational statement of ideals has brought out so much nitpicking prom people selling concessions or hedges against the possibility that anyone might want to seek, even just in their own personal conduct, to be inclusive to people from different backgrounds. It speaks volumes that this is a point of contention. And they are not good volumes.
We need a better macro system than TH, but the current work on dep. types isn't targeted to include one. (It *might* be of some assistance in writing the replace for TH, though.)
&gt; extremely something Extremely what? I was labelled "extremely conservative" and I didn't think that fully supporting Equality of Opportunity was a particularly conservative point of view. 
You can reach your own conclusions about the reasons for what happened. No doubt the details are different for each person involved, but the result is that the conversation as a whole has become a poster child for what the Haskell community should not be (but too often is). I have no doubt that wasn't anyone's intention. Still, it happened, and it keeps happening.
&gt;For example lens TH would be replaced by dynamically creating lenses off of the underlying row/record/variant, and would not require any TH. I thought the reason lenses are as fast as vanilla record syntax is because of inlining and specialization. If you're generating lenses at runtime (dynamically), then you'd lose out on performance... or do you mean something else by "dynamically"?
Dependent types aren't orthogonal to code generation, analogously to how parametric polymorphism is not orthogonal to code generation, as users of the Go language are painfully aware. Your `I8NÀô example can be written fine with dependent types. What's missing though, is a way to partially evaluate some type-generic programs at compile time, so code will be as efficient as equivalent externally generated TH code. However, I view this more a question of optimization, like how in parametric polymorphism it is a question of optimization to increase efficiency and code size by monomorphizing definitions (which is *exactly* a special case of partial evaluation with dependent types).
Ah, I see :) Because this means that, as per the set comprehension, for x=y x contains itself, otherwise it would be in the set y.
&gt; people seeking concessions or hedges against the possibility that anyone might want to seek, even just in their own personal conduct, to be inclusive to people from different backgrounds You are imputing internal motivations to discussants that you cannot possibly know. I haven't seen anyone in this thread express unwillingness to act inclusively (If you've seen that then please reply with a link! I'd like to know). What I have seen is concern that these guidelines will be used to push equality of outcome. If that's not the case then some people's fears could easily be assuaged with a simple reassurance. If it is the case then please be open and clear about it! (For what it's worth I personally think it's a rather overblow concern in response to the fairly innocuous phrase "in the hopes that, one day, we will no longer be askew")
You linked to a bug that's fixed in the latest stable release.
Perhaps the something that is extreme, or at least remarkable, is that so few question what is not clearly motivated (yet ideologically popular). In addition, when someone do, rather than without friction just getting the motivation clearly stated, they are met with comments of a different kind.
We actually already have such a lens library: `generic-lens`.
Sorry yes by dynamic what I mean is that they are not statically specified ahead of time, and are instead resolved at their use site via typeclasses and overloaded labels and so on. I am thinking something on the lines of: ``` {-# LANGUAGE DataKinds, FlexibleInstances, GADTs, OverloadedLabels #-} import GHC.OverloadedLabels (IsLabel, fromLabel) data FooBarBaz = FooBarBaz { foo :: Int , bar :: String , baz :: Bool } deriving (Eq, Show) data Optic p f s t a b = Optic { optic :: p a (f b) -&gt; p s (f t) } instance (p ~ (-&gt;), Functor f, t ~ FooBarBaz, a ~ Int, b ~ Int) =&gt; IsLabel "foo" (Optic p f FooBarBaz t a b) where fromLabel = Optic (\f x -&gt; (\y -&gt; x { foo = y }) &lt;$&gt; f (foo x)) instance (p ~ (-&gt;), Functor f, t ~ FooBarBaz, a ~ Int, b ~ Int) =&gt; IsLabel "bar" (Optic p f FooBarBaz t a b) where fromLabel = Optic (\f x -&gt; (\y -&gt; x { bar = y }) &lt;$&gt; f (bar x)) instance (p ~ (-&gt;), Functor f, t ~ FooBarBaz, a ~ Int, b ~ Int) =&gt; IsLabel "baz" (Optic p f FooBarBaz t a b) where fromLabel = Optic (\f x -&gt; (\y -&gt; x { baz = y }) &lt;$&gt; f (baz x)) ``` Except the key thing is that these instances I defined manually above would be defined for `Record` and `Variant` which reify any generic row of types into a concrete type of the appropriate structure. So then the user would simply define a newtype (or regular type synonym) over the `Record`/`Variant` and derive (or get for free) the lens instances.
&gt; ... even though I feel it takes a markedly adversarial interpretation to see the GHC guidelines, as currently written, as green-lighting discriminatory language against majorities. It doesn't logically follow from that. However, I've seen this kind of language often especially by feminists and intersectionalists to harass and unfairly discriminate individuals that are assumed to be part of, what they call, a privileged group. This way their ideology is smuggled in and the public accepts it because they have seen it repeated so often. &gt; &gt; Sometimes people will feel attacked and there is nothing you can do about it. &gt; &gt; Such is the nature of human communication. However, that needs not stop us from making a serious effort at steering clear of that (that is, "avoid[ing] forms of expression that might make our dialogue partner feel attacked, humiliated, demeaned, or marginalised). I'd say such an expectation is an entirely unsurprising part of what "respectful communication" might mean, in any context whatsoever. All I'm saying is that we shouldn't put the obligation to not offend solely on the speaker. That doesn't mean that unacceptable opinions don't exist.
TH is important for doing very important things that can not be done with dependent types like: `You bought #{count} items!` instead of `"you bought "++ count ++ "items"` or `sally ^. name` instead of `name sally`
So compilers do not apply a reduction to normal form at compile time? Does it not provide enough efficiency to justify the trouble of this optimization?
That gist shows building LLVM with [a patch that adds support for the GHC calling convention for RISC-V](https://github.com/bgamari/llvm/commit/8e50bcef2cd58509210f976907b9bb672fd99f31) + [corresponding GHC patches](https://github.com/bgamari/ghc/commits/riscv). That work hasn't been updated since late 2016. You can try picking it up :) ARMv8 (AArch64) works very well btw
And that's a sensible trade-off, isn't it?
That's essentially what the full laziness pass does. It usually beneficial but can cause space leaks, The motivation section of https://www.well-typed.com/blog/2016/09/sharing-conduit/ has a good example.
Compilers absolutely do not reduce to normal forms at compile time. That would result in massive code explosion and loss of shared computation.
Hello, everything is possible if the resources are there. How is your funding situation? Which hardware shall this run on, and does it have publicly accessible documentation and open firmware? If you have ~50 M$ funding secured and give me a team of 30 of my own choosing, I'll build a nice mobile OS for you in Haskell + Rust inclusive core GUI apps that look good.
I do not see why that could not be done with dependent types. The example seems very similar to what is done in other languages, for instance in python (with the format method) or in C (with the fscanf), where we have special functions that parse a template string and take inputs (depending on the template) to produce a string. Implememting this kind of function in dependent types should be possible, and I believe that Edwin Brady has provided an example for Idris. Generalizing these functions to get out other kind of values instead of just string should be possible, or at least I see no reason why it should not be. So i fail to see the problem here. Could you provide a different example which makes more clear what TH can do that could not be done with parsing functions for template strings? 
``` &gt; let nth lst n = lst !! (n \`mod\` (length lst - 1))` &gt; take 10 $ map (nth [1,2,3,4]) [0..]` [1,2,3,1,2,3,1,2,3,1] ``` &amp;#x200B;
&gt; analogously to how parametric polymorphism is not orthogonal to code generation, as users of the Go language are painfully aware Well yes they are, and that's exactly the problem with Go? Code generation and parametric polymorphism are _orthogonal_ despite some overlap, meaning they're _independently useful_. The problem with Go is they have only one of the two and suffer for it.
That is interesting. Could you provide a reference on the subject that highlights why does this happen (especially the loss of shared computation part)?
Could we not use just some functions taking an input template (codified via string or maybe a more complex datatype) that return the function that correspond to the template? This is basically a runtime-compilation though I guess one could use some compiler-optimizazion that automatically reduce the function(template) expressions to their code. Since functions in dependent typed languages can return types we clearly can generate type as well in this way. Am I missing something?
&gt; You can reach your own conclusions about the reasons for what happened. I can answer that from my own point of view and I will explicitly state what I think is going on. Elsewhere in this thread, there are strong hints of a radical left that is pushing an agenda that I cannot agree with. I am also well aware of other cases outside the Haskell bubble where if anyone dares question the far left's dogma, the far left shows its authoritarian colors and ties to shut down discussion. This makes many people nervous to discuss any topic that may possibly be contentious. I think that it is important that the Haskell community values and desires diversity but I would hope that also includes diversity of opinion and polite open discussion. &gt; Still, it happened, and it keeps happening. I know what I think "it" is, but I have no idea what you think "it" is.
For example, the normal form of `(\x -&gt; let y = x * 100 in replicate 1000000 y)` performs a million multiplications with a million machine instructions sitting in a hypothetical bloated executable, while the original code performs just one multiplication. 
&gt; [they] are orthogonal despite some overlap I'll be rather pedantic, but doesn't that mean that they're _not_ orthogonal?
I‚Äôll have a gcc built osx 8.6.3 ready later this weekend 
Isn't the closed source hardware a great obstacle to custom mobile os-es? You'd have to reverse engineer the chips to write drivers and the kernel. I think if open hardware was present in todays many mobile phones, many more os-es would be available... Please correct me if I'm wrong.
Yes, I was using the word in the colloquial sense rather than mathematical. In computing it's rare to have two methods that are _that_ distinct from each other that they don't have any common ground at all.
I'm really not imputing anything. I've stated pretty clearly what's happened in this thread. The document posted here did not "push" anything. It simply stated "that we seek high standards of discourse in the Haskell community, and are willing to publicly hold ourselves to that standard, in the hope that others may voluntarily follow suit." The response has been to nitpick the use of the word "diversity" in other people explaining their motivations for their own actions, and to explicitly ask the authors to revise their statement to add hedge language. They were asked to add assurances that the authors will only assess their inclusiveness by looking for overt biases and never by looking at outcomes. And that's exactly what I pointed out.
&gt; there are strong hints of a radical left that is pushing an agenda Okay, I was wrong. Conservative does seem to be the right word.
I doubt there are many people in the Haskell community that are members of either the radical left or the radical right. I do think, however, there are many members of a more moderate left/right that are extremely sensitive to potential signals of opposite-sided radicals. Unfortunately, quibbling about exact terminology of inclusiveness (which I think can be helpful) is easy to see as a sign. Although I might share your biases, let's just assume that signaling pro-diversity is to show the radical right they aren't welcome. Showing the radical left they aren't welcome is a longer-term battle and will have to be done gently.
I agree, our seeking of a pun stretched things too far with respect to 'final'. We were misled by the remark that in the usual initial encoding of a PL as an inductive type things are done as a sum-of-products, while as a typeclass, you have a product as the top-level type. This apparent 'duality' is not in the same 'dimension' as the initial/final. Again, blame our pun-seeking for that. As for tagless, I would strongly stand by that. The whole point of the paper was to show that you can do partial evaluation without the overhead of tagging/untagging all the time. It is only later that people forgot all about that and saw our paper as being another means of encoding a language in a way to make writing interpreters for embedded DSLs much easier. That's just a side-effect. Our principal point was that several papers used very fancy types to do partial evaluation and claimed their fancy types were necessary -- that was plain false. And, in PE, the tagging/untagging of the usual encoding with an inductive type causes extreme efficiency problems. Where of course the 'tags' are the discriminators for the sum-of-product. Lastly, it's not "Church encoding", it's Scott-Mogensen encoding [and we say so clearly in the JFP paper]. They are related, but the variant matters quite a lot for encoding lam and fix.
I don't understand. How is a link to the motivation of Backpack supposed to answer my question?
The Dhall configuration language does this, although it's not done for optimization reasons
I realize this post is 4 days old but still... I'm using Rust and Haskell to improve in both languages. For day 8 (today) my Rust implementation was going over the input token by token. I was keeping track of the state of my parser and it had like 5 match arms for the different cases. In the end I thought it was actually a relatively readable solution. Then I look at **(SPOILERS!)** [glguy's solution](https://github.com/glguy/advent2018/blob/master/execs/Day08.hs) which could more or less be ported to Rust but is just insanely elegant and I have to try really hard to convince myself that he probably has a lot more experience in programming in general and Haskell specifically so I shouldn't feel bad. I still totally understand that it can be frustrating as soon as you start looking at the solutions of others (which is great for learning though).
&gt; However, when build tools are mandated by management or legacy decisions, it is no longer a choice and we can become resentful of a tool that is not a good fit for our personal workflow. I'm having a hard time accepting the premise of your blogpost. You seem to talk about a scenario of non-conformist employees refusing to use the standard tooling conscientiously approved by management. By insisting on using different tooling than their colleagues these employees often cause unnecessary interoperability problems or might even result in opening up the company to security risks by use of non-vetted tooling. Besides annoying your coworkers you might even get fired if your employer pays you to do things in a certain way and expects you to be using certain tools and you defy those orders. You should reconsider whether you really want to encourage such unethical employee conduct which can harm.a companies by hampering team productivity or by causing unwarranted extra costs.
Does it? It doesn't typecheck for me at least
If you're worried about the price, you can look at Learn You A Haskell For Great Good online for free. If you don't like it, no money wasted.
&gt; **major issues** Stack supposedly suffers from No support for mixin packages (backpack) or any other means for ml-like modularity on package level &gt; how Cabal solves em. By adding support for mixin packages 
I'll do it for $45m
This is really not to sound condescending and I really hope it doesn‚Äôt come out that way. But haskellbook is probably the most thorough publication out there, regarding Haskell - there is a reason it‚Äôs over a thousand pages long. So I‚Äôm not sure any other books will be better, and certainly not learn you a Haskell, because that thing is the complete opposite in my opinion. Can you maybe explain more about what you don‚Äôt feel comfortable with? If it‚Äôs just the syntax, then I would go though each exercise again, from the start - maybe skim the chapters before, so you get a better feel for it.
The applicative implementation of lists can do the job
If you know Scala, maybe you could try https://leanpub.com/fpmortals It finishes with a quick intro to Haskell in terms of Scalaz concepts you will already know by that point.
I find stack to be the easiest way to get up and running on both macOS and Linux. 
On Archlinux I use https://github.com/haskell/ghcup which automates the Minimal Installer. You can also use the ghc in Archlinux but since they arbitrarilly update the version, it'll break everything one day. It's a shame they don't do what the ubuntu / debian / centos repos do and offer every version of the compiler.
If you don‚Äôt want to install the latest version of ghc (8.6.3) or multiple ghc versions, just use brew to install ghc (8.4.4) and cabal-install. And you can find the answer of other question in cabal‚Äôs manual.
Just to add to this, stack is by far the easiest. 
It is indeed a well-written and enjoyable read
you should include a link to the job description and mention the intended salary level at least broadly
I don't think Haskell Platform is discontinued? I just used it the other day (although I think it was on my other machine that doesn't have Mojave yet...). https://www.haskell.org/platform/#osx
You just made my day
&gt; I still do not feel comfortable even reading Haskell code A rant: Took me a while, too. A lot gets packed into very terse code. Implicit recursion abounds, and even the most fundamental tools like folds require some greater conceptual understanding than you may be used to in imperative-style programming. On the upside, it turns out ideas can be expressed much more clearly this way once you've become accustomed to it. Haskellers try to lure in unsuspecting imperative programmers by saying that Haskell's perceived difficulty is a product of the education system‚Äîthat it would be just as easy to learn as other languages were its approach to problem solving given the same consideration from the outset as more traditional languages'. I think this is a half truth, and that functional programming is inherently less intuitive. I'd venture that this is attested in the fact that most languages, including those in the beginning, are heavily imperative. Likewise, I suspect C-style syntax proliferated in no small part because it's significantly more accessible for beginner and intermediate learners, even if it feels horrendously deficient with respect to expressivity, concision, and ergonomics once you've become used to Haskell-style syntax. &gt; Could you suggest what I should do? Isolate code samples and investigate them thoroughly. One of the interesting things about Haskell is that concepts tend to be more tightly interwoven than other languages. If you understand one concept well, you start seeing it crop up all over the place. There's an overarching theoretical design at play, and the language's most discrete ideas tend to be repurposed in all sorts of ways to achieve solutions. If Python feels like it was thoughtfully designed, Haskell feels like it sprung to life from some deeper, universal truth (and, in a sense, it did). That, anyway, was the tack I picked up after floundering around a bit trying to understand the breadth of the language. I feel that with Haskell you're rewarded more by diving deep into a particular part of the language, because what you discover in one part of the language will help you understand other parts of the language‚Äîsometimes quite unexpectedly. Again, overarching design. This is not true of most languages, and particularly unhelpful in, say, JavaScript, where most every useful feature is literally an afterthought that may or may not bear resemblance to other parts of the language.
&gt; I doubt there are many people in the Haskell community that are members of either the radical left or the radical right. I agree completely with that. &gt; Showing the radical left they aren't welcome is a longer-term battle and will have to be done gently. The radical left are just as small as the radical right, but I see no reason why they cannot be resisted with the same vigor.
Since when is hoping for "diversity of opinion and polite open discussion" a conservative idea?. Does anyone that opposes the far left *have* be conservative? Do you not think it is possible to be somewhere in the political center and oppose both far left and far right? The far right are easy to spot and their agenda (ethno-nationalism) is obviously misguided and easy to argue against. The far left is far harder to pin down but parts of their agenda is just as misguided.
Well the latest version is only for 8.4.3. I‚Äôm sure it will work, but I seem to recall that it is really quite an invasive installation. 
After seeing the video it looks and sounds like what‚Äôs being implemented in GHC. He starts by saying that the meaning of the quantity annotation is that if ‚Äúx‚Äù is linear in ‚Äúf x‚Äù, then ‚Äúx‚Äù is consumed exactly once if ‚Äúf x‚Äù is consumed exactly once. This sounds just like the definition of linearity that I‚Äôve heard from the GHC implementation. The syntax difference is that function arguments are annotated, rather than annotating the arrow that follows the argument. Ie. f : (1 x : Nat) -&gt; String rather than f : Nat -o String f x
For 8.6.x via homebrew you can use `brew install ghc --head`
What makes you say that? As far as I can see, he uses the quantity notation to describe how many times a function consumes an argument. This sounds like linearity on the arrow to me. Although the syntax is different (see my comment above).
&gt; The response has been to nitpick the use of the word "diversity" People nitpick that word because "diversity" by itself is an admirable goal, but I think it is a mistake to use it without saying anything about how it to be achieved. &gt; They were asked to add assurances that the authors will only assess their inclusiveness by looking for overt biases and never by looking at outcomes. I think that actively seeking Equality of Outcomes will hurt this community. I fully support Equality of Opportunity.
I have answers to your questions! 1. You should install GHC and `cabal-install` using `ghcup`. `Ghcup` is a relatively new addition to the Haskell ecosystem, allowing you to juggle and manage multiple GHC versions locally, without the headaches. The instructions are simple, and its use is simpler. You can read more about it [here](https://github.com/haskell/ghcup). This is the preferred way to handle multiple GHC installations for my workflow if I'm not using Stack to bring it in. You can also install `cabal-install` from it. 2. Because the `v2` (i.e. `new-*` which is the same as `v2-*`) commands have a nix-style workflow, you really never need `cabal v2-install` anywhere but your projects. No more global dependencies! The new Cabal allows you to manage all the dependencies you could possibly need within the bounds of the project directory. The new workflow is basically just to use `cabal v2-(build|configure|install|test|repl)` for all your needs. 3. You still have versioning concerns, as you do in Stackage as well. The solver will attempt to reconcile if a solution exists, however, you still need to take care that your version bounds are correct, which is the same for Stackage - if a solution exists, it _will_ be found for you.
Thanks. Looking through the Ruby ghc installation code, it seems to have a lot of work-arounds for old ghc bugs. Do they really apply to HEAD . 8.6.3?
Does it provide 8.6.3?
&gt; it's Scott-Mogensen encoding Hmm, looking at the paper, it does not seem to be Scott-Mogensen to me. Both the HOAS and de Bruijn examples look Church; they're typable in System F-omega, while Scott-encodings aren't. I would expect recursive types and case-splitting constructors with Scott-encoding but I don't see them here. There is a slight complication with the HOAS example: since it's not strictly positive, there is no weakly initial algebra, i.e. there are no `lam` and `fix` constructors in a total setting. However, we can still build expressions by abstracting over an arbitrary algebra, and interpret such expressions by simply giving an algebra - this seems to me the usual intended usage for finally tagless representation. So, it's not technically Church-encoding; I'd call it "Church HOAS", but I wouldn't call it Scott-encoded. For the de Bruijn example, which is strictly positive, there is a weakly initial algebra given by Church-encoding, and finally tagless-style embedded expressions are exactly those which can be given by Church-encoded constructors. 
A combination of LYAH, ghci, and blog posts + haddocks goes a long way. Parallel and Concurrent Haskell is also great. You don't have to work through the entire book, but doing a project with concurrency using Haskell's concurrency libraries is a great jump-start.
I understand that you hold these opinions about equality. I don't think it's a problem that you hold these opinions, and I'm not trying to convince you to change your opinions. That's entirely between you and the people you care about and interact with. What I am pointing out is that this statement didn't ask you, or anyone else, to actively seek ANY kind of equality at all. It just said that a certain group of other leaders is committing to hold themselves to a high standard of respect and civility in conversation. There's absolutely no reason for you to be calling for those other people to amend their statement to incorporate your opinions about what their motivations should be. It's unreasonable, and it's completely derailing the intent of their actions. Do you care so little about their goals of promoting civil and respectful communication?
This is a reproducable method: * Install Nix `curl https://nixos.org/nix/install | sh` * Execute nix-shell declaring that we need ghc with packages \`wreq\` and \`lens\`. `$ export NIX_PATH=nixpkgs=https://github.com/NixOS/nixpkgs/archive/135a7f9604c482a27caa2a6fff32c6a11a4d9035.tar.gz` `$ nix-shell -p "haskellPackages.ghcWithPackages(pkgs: with pkgs; [ wreq lens ])"` `[nix-shell:~]$ ghci --version` `The Glorious Glasgow Haskell Compilation System, version 8.4.4` Nix will fetch all the dependencies from a cache since a continuous-integration server already compiled and tested all the packages for linux and macos, it put all the results into a cache called binary cache. We're reusing that work by providing a special NIX\_PATH. 
Indeed. It is not discontinued. There have been no 8.6 series releases yet because until 8.6.3 (yesterday) it was too buggy. Ghc wouldn‚Äôt even run on a vanilla os x system with devtools unless extra libs from brew were installed!
1. Use a syntax reference, something like [reading simple haskell](https://soupi.github.io/rfc/reading_simple_haskell/). 2. Simulate, Haskell is expression oriented so you should learn how to reduce expressions. This will give you some insight to what functions do. Haskell expression basically reduce just like math expressions. [Some more info about that](https://github.com/Gabriel439/slides/blob/master/bigtechday/slides.md). Try to simulate what `result` reduces to. Use pen and paper to write each step! result = factorial 5 where factorial n = case n of 0 -&gt; 1 n -&gt; n * factorial (n - 1) 3. Practice, Start reading simple programs. Start writing simple programs. [Here are a few exercises you can do](https://github.com/soupi/haskell-study-plan/#exercises). Now just keep going.
Actually, I misremembered - just Mogensen. [reread those bits of the paper just now]. Mogensen is close to Church indeed. So I guess I'd be fine with "Mogensen HOAS".
learning a language with a book is like learning Bicycling with a book. 
I think the key thing dependent types don't handle is _name binding_. So you can't use dependent types as such to declare top level names of functions, or constructors of datatypes, etc.
You didn't say whether or not you had been doing the exercises. Most of my trouble learning Haskell at first was not doing exercises because the books I was learning from either didn't have them or involved way too much other domain knowledge to tackle the problems. So I would read along thinking I understood everything until I tried writing some code. Doing the exercises for CIS194 helped to get a good foothold on the language. I don't know if the exercises are the problem you have been facing or not, but it's a major thing some people may skip. Here is some other advice. Try to treat Haskell like you are learning programming for the first time. Pay attention to type signatures, especially when you are dealing with type variables; give thought to why the signature is what it is. Don't try to write anything practical for a while until you understand the functor, applicative and monad typeclasses as most library functions you need will assume knowledge of those typeclasses at least. Don't be afraid to ask questions, IRC on #haskell-beginners at Freenode is a great place for that. I would end up at least asking one question for every week of CIS194, so don't feel bad if you have a lot of questions.
The *only* reason I brought up the opportunity vs outcomes distinction was because of [this comment](https://old.reddit.com/r/haskell/comments/a3mz2a/guidelines_for_respectful_communication/eb7mxac/) where the post was initially *heavily* downvoted because he raised concerns that he was not able to precisely define. My response *clarified* their thoughts. &gt; Do you care so little about their goals of promoting civil and respectful communication? You may think that, but I cannot fathom how you could come to that conclusion from what I have written in this thread. 
Nix is quickly becoming the best way to manage packages on any platform (except windows?)
As I said above, this already exists in the `generic-lens` library. Thanks to inlining, it's as performant as "plain old" record accesses, unless there's been a regression.
Yes -- https://www.stackage.org/lts-12.6
[removed]
Perhaps you could try [Haskell: The Craft of Functional Programming](http://www.haskellcraft.com/craft3e/Home.html) \- more in depth in some parts, and focused on the conceptual understanding, or [Learn You a Haskell for Great Good](http://learnyouahaskell.com/) \- more lighthearted, in some parts possibly not as deep (although this is mostly something I've been told by other people, I only read the former book myself).
`stack ‚Äîresolver ghc-8.6.3` should work or just choose a recent nightly version
I don't agree.
If someone can't afford the book, they should [reach out to me](http://haskellbook.com/support.html). Everyone gets a book regardless of circumstances.
&gt; What makes you say that? The annotation is attached to the binding rather than the arrow.
I have a nixOS VM running, just to see what the fuss is about, but I don't think that I have a working version of ghc on it. And if 8.4.4 is the latest version, then I may as well use Haskell for macOS, which is 8.0.2
Does that give me a ghci that I can use?
Yes 
I strongly recommend doing the exercises included in any resource but this goes double for the [Haskell Book](http://haskellbook.com).
I just download from GHCHQ (https://haskell.org/ghc) simple and immediate.
Thompson's book Haskell: The Craft of Functional Programming was what I recommended to people before we got [the Haskell Book](http://haskellbook.com) caught up in topic coverage. I talked about this a bit in my [article reviewing educational resources of Haskell](http://bitemyapp.com/posts/2014-12-31-functional-education.html).
Another Advent of Code inspired question! I've just solved [day 8](https://adventofcode.com/2018/day/8) using recursion schemes (mainly by following along with the exposition [here](https://recursion-schemes-by-example.chrispenner.ca/articles/recursive/basic-catamorphisms)). [Here](https://github.com/onthestairs/advent-of-code-2018/blob/2fa259c7bd9ec91a8af57c3f23452e99e1dc5cc5/src/Day08.hs#L24-L31) are the data types and instances I wrote: data Tree a = Tree a [Tree a] deriving (Show, Functor, Foldable) -- recursion schemes instances data TreeF a r = TreeF a [r] deriving (Show, Eq, Functor) type instance Base (Tree a) = TreeF a instance Recursive (Tree a) where project (Tree x cs) = TreeF x cs instance Corecursive (Tree a) where embed (TreeF x cs) = Tree x cs When I was writing the `TreeF` datatype and the `Recursive`/`Corecursive` instances, they felt quite 'mechanical'. There seemed to be only one 'obvious' way to write it, much in the same way that there is typically only one obvious way to write a functor instance. There is a language extension that allows automatic derivation of Functors. Is there an analogous way to automatically derive these datatype/instances for use in recursion schemes?
And a business model, hiring plan/policy, dev ops, etc...
What non-trivial dependencies does the binary distribution require? 
8.6.3 was literally released yesterday. Nobody is packaging it yet. If you want to use it, just download the binary installer from ghchq yourself.
You don't need a NixOS VM, those commands are also working for MacOS. 8.4.4 is the default ghc version as defined by nixpkgs. You can easily get 8.6.2 or HEAD (probably nix will build HEAD from source) by depending on a specific compiler version when entering nix-shell: `$ nix-shell -p haskell.compiler.ghc862` OR `$ nix-shell -p haskell.compiler.ghcHEAD`
Well. python3 and, perhaps, some gnu libraries and, presumably, some hackage libraries, I assume.
Oops! Thank you for catching this for me!
download binary distribution archive file from https://www.haskell.org/ghc/download_ghc_8_6_3.html extract the archive to &lt;some directory&gt; cd &lt;some directory&gt; ./configure --prefix=&lt;path to you want to install ghc&gt; make install this worked for me flawlessly in macOS 
I honestly wouldn't consider python3 or some core gnu utilities as "non-trivial dependencies". You are hardly going to do anything programming-related without those. But anyway, according to [GHC's website](https://www.haskell.org/ghc/download_ghc_8_6_3.html#binaries), you only need the Xcode toolchain, which you should have installed already if you have used homebrew. Also GHC comes with its own core libraries, so it doesn't even have to know what hackage or cabal is. I would be very surprised if it required anything more than the very basic stuff to just install a precompiled binary. Have you actually tried and encountered some difficulty?
Dependent types are also no help in syntactic abstraction, which is something that meta programming can give you (to a limited extent)
Yeah. Programing languages are tools, the best way to learn how to use them is to use them. When you understand more about the theory of programing languages a book might be useful, but definitely not before then 
I installes stack and the ghc from the official packages. Stack lets me use what ever version I need for projects and having a global version gives me the use a repl whenever I need it (where I rarely care about version). 
The recursion-schemes library has a Template Haskell script to do all that: [makeBaseFunctor](https://hackage.haskell.org/package/recursion-schemes-5.0.3/docs/Data-Functor-Foldable-TH.html) data Tree a = ... $(makeBaseFunctor ''Tree) I also wrote a GHC Generics version, but it can hardly compete in usability: https://github.com/Lysxia/generic-recursion-schemes
&gt; My first thought was that this is impossible without some super fancy dependent types, because there's no way to know what types a given user function will accept Indeed. GADTs allow you to define a precise type which make ill-typed expressions unrepresentable, but they are not a good match when you do need to represent possibly-ill-typed expressions. If you will only know at runtime e.g. by looking up that `String` into a table whether the arguments turn out to be of the type which the function expects or not, then it might turn out at runtime that the expression what not well-typed after all. So GADTs are not a good match in this case.
&gt; most freelancer marketplaces are flooded with OOP languages. Don't those marketplaces allow you to filter by language? I remember watching a talk in which the presenter explained that he found gigs in an obscure language by adding that language to his profile in such a marketplace and then waiting for companies who use this language to find his profile by searching for that obscure language.
I use ghcup (https://github.com/haskell/ghcup)
Installing from scratch is pretty simple. I am using the instructions from [mafia](https://github.com/haskell-mafia/mafia/blob/master/docs/installing-ghc.md) just substituting the version of ghc you need and the matching cabal version [instructions ](https://github.com/haskell-mafia/mafia/blob/master/docs/installing-cabal.md) So far I‚Äôm using Mojave at work and personally and I‚Äôve seen zero issues related to ghc. The terminal seems pretty laggy in Mojave under heavy load compiling things. 
&gt;I think this is a half truth, and that functional programming is inherently less intuitive. I'd venture that this is attested in the fact that most languages, including those in the beginning, are heavily imperative. As someone who has done a lot of teaching Haskell to children with no previous experience, I strongly agree with this. People who understand that sequences of mutations to shared state are inherently more **complex** often infer from this that they are also more confusing to beginners, and rationalize beginners' use of mainly imperative languages as a mostly historical anomaly of the time when high-level languages grew as wrappers around machine code. But the truth is more complicated. It seems that the human brain is somehow *innately* comfortable with reasoning about imperative sequences of actions. That makes these kinds of tasks quite intuitive. So even in places where teaching sequences of actions is inherently *wrong*, such as in mathematics, early teachers know they have to fall back on it. Hence, in elementary school, we don't teach children about operator precedence as the way that arithmetic expressions break down into subexpressions; we teach them about "order of operations" - the order in which they should do the arithmetic. It's confusing, mostly incoherent, and sabotages them when they get to algebra later and *cannot* multiply before they add because one of the multiplicands is a variable. But still, it's done that way because it's what gets the results. The issue is that this intuitive sense we have of reasoning about sequences of operations on mutable state is easy for easy problems, but it doesn't scale to *hard* problems. So at first, it seems that declarative thinking - whether it's Haskell, or predicate calculus, or algebra - is pointless and confusing, and it takes some exposure to get over the hump and familiar enough to start finding that problems that seemed quite difficult in the past are now much more accessible because of this new tool.
Well, not exactly. If you want a quasiquoter, you can just write something that turns a string into a well-typed value and use it directly. You just can't introduce _name bindings_ in such a construction.
 Œª&gt; :set -XFlexibleContexts Œª&gt; let a = (\x -&gt; x) + 1 Œª&gt; :t a a :: Num (p -&gt; p) =&gt; p -&gt; p This is saying that _if_ there is a `Num` instance for `p -&gt; p`, then `a` is well-typed and has type `p -&gt; p`. There aren't any instances for any `p`, but that's not sufficient to reject the definition, since an instance may be added later on. In fact, let's add one right now: Œª&gt; :{ Œª| instance Num b =&gt; Num (a -&gt; b) where Œª| f + g = \x -&gt; f x + g x Œª| f * g = \x -&gt; f x * g x Œª| negate f = \x -&gt; negate (f x) Œª| abs f = \x -&gt; abs (f x) Œª| signum f = \x -&gt; abs (f x) Œª| fromInteger n = \_ -&gt; fromInteger n Œª| :} It's quite a useful definition, too: it lifts numeric operations to functions by applying them pointwise. So I can now express a function which computes the squared magnitude of a `Complex` number as `realPart*realPart + imagPart*imagPart` instead of `\c -&gt; realPart c * realPart c + imagPart c * imagPart c`. Similarly, I can express a function which shifts a `Complex` number by one unit to the right as `id + 1` instead of `\c -&gt; c + 1`; and that's exactly what your `a` function does!
Don't use Haskell for mac. It is extremely busted and deprecated. Most of the Haskell world is still on the 8.4 series, but that is _streets_ ahead of 8.0.2. (there's roughly a two year gap between the 8.0 series and the 8.4 series!)
&gt; I still do not feel comfortable even reading Haskell code, almost halfway through the book. Try writing code instead, or try opening the code you're reading in a repl and interacting with it. Reading dense Haskell code can be quite an art, since it is so concise and may make use of so many idioms. It is better to get your hands dirty to get a "feel" for the language, and learn idioms as you need them than just try reading stuff.
&gt; Hence, in elementary school, we don't teach children about operator precedence as the way that arithmetic expressions break down into subexpressions; we teach them about "order of operations" - the order in which they should do the arithmetic. It's confusing, mostly incoherent, and sabotages them when they get to algebra later and cannot multiply before they add because one of the multiplicands is a variable. But still, it's done that way because it's what gets the results. Are you sure that is why it is done that way? I mean... what if it was done the other way? Are there studies that show that it fails?
I can't think of basically any reason why equality of opportunity wouldn't lead to equality of outcomes, outside of some absolutely discredited nonsense about innate differences between groups. The one thing you might be gesturing at is the so-called "pipeline" problem. But even in that case, one would imagine that equality of outcomes in terms of proportions within the Haskell world would be _at least_ at parity with that of say, software development or math as a whole, which it assuredly is not!
What about encouraging it through affirmative effort to support people from underrepresented groups, though? That sounds pretty good to me.
I do something similar, but I just install stack as a static binary and then it's in charge of updating itself. After that, I do everything in stack as it's the most stable and breaks the least things for me. For a global repl I go dirty and just stick the latest version of ghc (from stack) in my PATH.
You can just use `stack ghci` for a repl. It will drop you into a default GHCi version from your stack settings. This way you don't have to go through the hell of GHC updates on Arch.
I think its a dog-whistle against a nonextant threat, and I'm really pretty surprised to see you fall for it :-/
Very cool! Generics give us a lot of what we can get with structural types, the main thing fundamentally missing from Generic is the ability to generate new types on the fly with new structures, such as what is needed by DB libraries to convert DB specifications into one or many Haskell types. Regardless in the case of lenses it does look like generic-lens is not missing any desired power as no new types need to be created.
I'm proud of you fam :)
I cannot point to any specific studies on this question. I can say that I've had variations of this discussion with a lot of people involved in early computer science and math education, and my sense is that it's a pretty universal experience. In early elementary, John Mason is the researcher who most comes to mind in terms of trying to hook into that intuitive sense of the right action, and redirect it toward more algebraic modes of problem solving. But even his work mostly boils down to letting the kids solve the problem imperatively first, then redoing the work with reflection on the declarative interpretation. Later on, there's a metric ton of work on building the idea of functions. Again, I can't think of anyone who has published something specifically saying "we tried the declarative approach first, and it was a disaster." (It is, I think, a general problem that education research tends not to publish results of the form "we tried this, and it was a disaster".) But there's widespread acknowledgement (among researchers -- less so among teachers) of the importance of having a flexible understanding of functions as a mathematical object independent of the computational steps. And yet, no one has reported succeeding with the approach of just teaching the declarative notion first. (I suppose you could view the whole failure of the 70s New Math movement as an anecdote of failure in teaching the abstract objects first before the processes that manipulate them; but that would be far too broad to be actual research.)
I‚Äôve been learning from the same book. I‚Äôd been programming in C and C-like languages for decades. I‚Äôve had some prior exposure to Haskell from LYAH and failed attempts to understand what the fuss was about through blog tutorials. However I‚Äôve stuck with the book and I do recommend it for some people. The hardest part for people with prior, strong experience with other languages is to stop comparing Haskell to what you already know about programming. Little to nothing of your prior experience will help you learn Haskell. It may later on but when you‚Äôre learning disavow yourself of that knowledge and approach Haskell (and the book) with a fresh, beginner mind. Do the exercises regardless of how trivial they feel. Ask embarrassing questions on /r/haskell. And most of all: take a break to work work on something practical and see if you can apply some of the techniques and knowledge you acquire. If you have a day job find a small, low priority thing you could work on for an hour or two during the week when you have some spare time from your main tasks and write a tool to help you answer a question or automate some task... and write it in Haskell. Once you‚Äôve written some practical code using what you learned start teaching someone else. See if you can explain what a Functor is to them, even if you don‚Äôt use Haskell, and see if they get it or want to know more. Once you can communicate the idea to others in a more or less lossless way you know you‚Äôve mastered the fundamental concept. Then read a little bit more of the book... wash, rinse, repeat. 
I just want to chime in to say that ghcup is great! It is super lightweight compared to stack, but provides similar functionality (it is still lacking windows support though). Installation of `ghc-8.6.3` boils down to: $ ( mkdir -p ~/.ghcup/bin &amp;&amp; curl https://raw.githubusercontent.com/haskell/ghcup/master/ghcup &gt; ~/.ghcup/bin/ghcup &amp;&amp; chmod +x ~/.ghcup/bin/ghcup) &amp;&amp; echo "Success" $ PATH="$HOME/.cabal/bin:$HOME/.ghcup/bin:$PATH" $ ghcup install 8.6.3 $ ghcup set 8.6.3 `ghcup` also lets you painlessly download a `cabal-install` binary too. 
*Je n'ai fait celle-ci plus longue que parce que je n'ai pas eu le loisir de la faire plus courte.*
I would actually include "affirmative effort to support people from underrepresented groups" in the Equality of Opportunity bucket, especially if that support is in the form of things like removing barriers or extra training/mentoring. "Affirmative effort to support" is very different to "enforcing equality outcomes" which for example would be the introduction of quotas (which I would have supported in the past, but now think is misguided). For an example of quotas, one only has to look at Canadian Prime Minister Trudeau's decision [to include 15 men and 15 women](https://www.theguardian.com/world/2015/nov/04/canada-cabinet-gender-diversity-justin-trudeau) in his cabinet. However, those 30 men and women were chosen from a larger pool of 184 members of Trudeau's party elected to parliament, and that larger pool is heavily weighted towards men. If we assume that competence to operate as a member of the cabinet is normally distributed around some mean, and the the mean and variance of competence for men and women is the same, then the choice of a 50/50 quota means that some more competent men were passed over for less competent women (even if on average the women were just as competent as the men). The argument against Trudeau's decision is that just measured on competence alone, this 50/50 gendered cabinet is lower than it would be without the 50/50 quota. I am perfectly happy to accept that this negative consequence may be heavily outweighed by other factors, like having cabinet reflect gender distribution of the wider public the cabinet is supposed to be working for. However much sense the 50/50 gender quota might make sense for Trudeau's cabinet, having a 50/50 gender quota for the Haskell Committee would make as much sense as only allowing one person with a first name of Simon on the committee. 
\&gt; I just noticed a theme that has been reoccuring lately in the haskell community and many others, that being praise of diversity and I felt like maybe people are attributing things to diversity without much thought. In the end, diversity and inclusion are not abstract goals at all. They are words that describe more people, including those of many backgrounds and different experiences, being able to use Haskell and find it valuable in their lives. This isn't a technique that may or may not be effective at achieving our goals. It *is* our goal. At least mine, and I believe it to be the goal of the majority of other Haskell community members, as well. If social aspects of the community make Haskell less desirable or suitable for a large number of people, that's every bit as much a bug in Haskell as GHC crashes or missing tools and documentation. Just as the goal is not abstract, the problem is also not abstract. There is no reasonable debate over whether the Haskell community has a problem with inclusion. I personally know new Haskell programmers whose experience in the community has been atrocious. In one case, it's involved long campaigns of character assassination when advocacy for tools or technical decisions got way out of hand. In another, it has involved an interpersonal relationship problem that led to long-term harassment and spreading of lies and rumors by a high-profile member of the community. It's very hard right now to feel proud of our community. So in case you're thinking, as you seem to be in some of your comments here, that this is motivated by a general trend toward codes of conduct because of political correctness and politics... know that for many of us, we're not doing this because of politics or trendiness at all. We're doing it to try to salvage a community we can enjoy and be proud of again, when many of our experiences today are just heartbreaking.
You might be interested in https://github.com/tweag/asterius
&gt; outside of some absolutely discredited nonsense about innate differences between groups. I am nowhere near an expert, but just looking at men and women, it seems that the mean IQ (cognitive abilities) for men and women is nearly identical. However, men show a larger variance than women (both at the top and bottom ends of the scale). Since coding Haskell is something that does require relatively high cognitive abilities it is not surprising that more men than women code Haskell. However, this is definitely *not* the only factor. I have read the whole of the wikipedia articile [https://en.wikipedia.org/wiki/Sex_differences_in_intelligence#Current_research_on_general_intelligence]. I agree, that mean differences in IQ between men and women have been discredited, but the higher variance of men in comparison to women has not. In fact in the criticisms section of the above article, this greater variance in male IQ is used as an argument to suggest that this variance skewed the the mean results away from being identical. 
Brew tends to do stuff like that when there isn‚Äôt a maintainer keeping their ghc builds honest. And the feelow I know there has stepped away from brew for a break. I favor my own bin dist on Mac. But that‚Äôs just cause gcc built ghc is 15% faster for multithreaded programs on OS X :)
... the python dep should only be for building ghc ...
Well I'd challenge that IQ is a meaningful construct, or that anything reasonable about differences at the level we can measure can even be disentangled from the enormous and pervasive structures that prevent equal development of individuals with different backgrounds or gender characteristics, etc. I'd also suggest that Haskell does _not_ require "relatively high cognitive abilities" relative to many many other fields with _vastly_ different gender ratios. (like, say, med school! https://news.aamc.org/press-releases/article/applicant-enrollment-2017/) So whatever we can faff around with some 2% here and there in studies that are not determinative, or we can recognize that whatever is going on in the concrete is entirely a social question, and one that should be acknowledged as such and also one that it would be positive to take steps to remedy.
Ok, given the high disparity between the numbers of men and women in this community, I would like to know what factors you think drive this and their relative importance. I won't even try to argue them. I would just like your opinion. 
This is a _very_ widely studied question, and I don't think I have anything special to add beyond the widely available, but still somewhat inconclusive research. Here's some links from just the first page of google results: http://newsroom.ucla.edu/stories/cracking-the-code:-why-aren-t-more-women-majoring-in-computer-science and https://www.npr.org/sections/money/2014/10/21/357629765/when-women-stopped-coding and https://www.insidehighered.com/news/2015/04/21/study-measures-causes-gender-gap-computer-science for example. The further question that emerges is why is Haskell (or fp generally) typically even worse off than CS more broadly, and there I have even less in the way of answers, except to suggest that it is a specialized subfield which has in the past required people to invest time above and beyond just their day jobs and standard education, and for a variety of reasons, this tends to filter out still further a range of people. Also, being very mathy in outlook, the "math isn't for X" and "CS isn't for X" stereotypes which can act as barriers may well have a sort of squaring interaction effect. So I don't think the primary _causes_ of the disparities in Haskell users can be found within particularly bad characteristics of Haskell devs, bloggers, etc. But that doesn't mean we can't try our hardest to look for _remedies_ or at least steps to ameliorate or improve things nonetheless. And since the very _presence_ of a community that lacks certain sorts of diversity may lead people to conclude that it isn't for them, since they don't fit in, then A) it makes sense to establish things like CoCs as positive gestures to indicate that at least people aren't _happy_ about the imbalances, and B) special encouragement of people from more diverse backgrounds as they get involved, and even special outreach may in turn lead, in the future to a diminution in that "not for me" effect.
https://github.com/erkmos/haskell-companies
thanks!
Pinging u/isovector ; but also shameless plug: https://chrispenner.ca/posts/type-tac-toe
&gt; Nobody is packaging it yet [Well...](https://github.com/NixOS/nixpkgs/commit/c37ae1dcf669b5b262350ddbb75a40be166e7e88) :P Nixpkgs is pretty on top of GHC releases.
Nix has been the most reliable way to install GHC for me. These days I usually start a project by getting GHC from Nixpkgs and using `cabal new-build` to manage Haskell deps. Once I need even slightly non-trivial dependencies I switch to building with Nix, which makes deployment a breeze later on.
Ive never heard of that as im new to haskell, can you give me an example?
You can get the flattened version quite easily using: zip (cycle xs) ys &gt; [(1,1),(2,3),(3,5),(1,7),(2,9),(3,10),(1,13),(2,15),(3,16)] With the requirement that the first list is always shorter than the second; if you want them nested like that it gets trickier, may I ask why you want this? Regardless probably the easiest way is with unfoldr: nestedZip :: [a] -&gt; [b] -&gt; [[(a, b)]] nestedZip xs ys = unfoldr go ys where go [] = Nothing go bs = let (some, rest) = splitAt (length xs) bs in Just (zip xs some, rest) Or if you implement or import chunksOf from Data.List.Split you could do (untested): nestedZip xs ys = zip xs &lt;$&gt; chunksOf (length xs) ys 
Can somebody with deep knowledge of bang patterns explain this behavior? `let !2 = 3 in 2 -- runtime error` `let ((), !2) = ((), 3) in 2 -- no runtime error` &amp;#x200B;
This is a really interesting idea! I don't believe this is possible with Prisms; It seems from my experiments that you can only use `review` with a `Prism' s a` and not a `Prism s t a b`, which seem strange to me, since review should in theory just be able to use the `b -&gt; t` function and shouldn't care about the `s` or `t`... You can write the prisms themselves, but they won't type-check if you try to `review` on them... :'( Something like this: render :: Int -&gt; String render = show -- Just for an example we'll parse both the positive and negative vals parse :: String -&gt; [Int] parse s = let n = read s in [n, -n] parsePrism :: Prism String [String] [Int] [Int] parsePrism = prism constructor traverser where constructor :: [Int] -&gt; [String] constructor = fmap render traverser :: String -&gt; Either [String] [Int] traverser = Right . parse You CAN however write something which matches the type signature for a `Traversal`: -- parseTraversal :: Applicative f =&gt; ([Int] -&gt; f [Int]) -&gt; String -&gt; f [String] parseTraversal :: Traversal String [String] [Int] [Int] parseTraversal f s = fmap (fmap render) . f $ parse s And you can pass it a function which operates over a list of ints, give it a string, and get a list of strings back: Œª&gt; "15" &amp; parseTraversal %~ fmap (*100) ["1500","-1500"] Which I'm not sure is terribly useful, but it's something ü§∑‚Äç‚ôÇÔ∏è I wrote a guide to writing Traversals here: https://lens-by-example.chrispenner.ca/articles/traversals/writing-traversals 
Sorry, I was in the bus when I read your post, and I didn't understand your expectations correctly.Long story short, Applicative is a class which provide a useful operator `&lt;*&gt;`. Then, you can do [\a b -&gt; (a, b)] &lt;*&gt; [1, 2, 3] &lt;*&gt; [1, 3, 5, 7, 9, 10, 13, 15, 16] -- [(1,1),(1,3),(1,5),(1,7),(1,9),(1,10),(1,13),(1,15),(1,16),(2,1),(2,3),(2,5),(2,7),(2,9),(2,10),(2,13),(2,15),(2,16),(3,1),(3,3),(3,5),(3,7),(3,9),(3,10),(3,13),(3,15),(3,16)] But of course, it doesn't do what you want, so take a look at u/ChrisPenner answer.
I apologise. I misread what you wrote. I thought that you were saying that some commenters were indicating that they wouldn't make an effort to be inclusive. 
Love the pun üòú I used: ique = fmap fst . filter ((&gt; Sum 1) . snd) . M.toList . M.unionsWith mappend . fmap (,Sum 1) Probably not any faster, but seemed elegant to me ü§∑‚Äç‚ôÇÔ∏è
&gt; I can't think of basically any reason why equality of opportunity wouldn't lead to equality of outcomes, Then isn't explicitly aiming for equality of opportunity a perfect solution to satisfy all reasonable parties?
You could do this quite easily if Data.Map had a newtype wrapper for the other Map monoid, but unfortunately it doesn't üò• I usually rebuild it by using unionsWith mappend (using TupleSections): count = M.unionsWith mappend . fmap (,Sum 1) It's not foldMap, but if you want foldMap you can write the newtype around Map to do so.
&gt; Nobody is packaging it yet. ;-) - As hinted by [this comment](https://old.reddit.com/r/haskell/comments/a4d9fi/haskell_for_macos_mohave/ebem73u/) GhcUp supports it already (if you look at the [commit](https://github.com/haskell/ghcup/commit/0b67aeb23fb05b460bf424c6eec0bfb804050c16) adding 8.6.3 support it happened shortly after ghc 8.6.3 was officially announced) - Moreover, at the time you made that comment GHC 8.6.3 had already been packaged and uploaded into the respective Apt package repositories for [Debian](http://downloads.haskell.org/debian/) and [Ubuntu](https://launchpad.net/~hvr/+archive/ubuntu/ghc)
I was wondering if you think this comment adheres to the Haskell committee guidelines for respectful communication (hypothetically, since I'm not aware whether you hold yourself to them or not)? In particular * We treat everyone with courtesy, aware that their diverse backgrounds, experiences, goals, and perspectives may be very different to ours. * Where we disagree with someone, we avoid forms of expression that might make our dialogue partner feel attacked, humiliated, demeaned, or marginalised. Our critique should always be of specific statements and claims, never of people. and * Where we disagree, we try to be curious about the perspective, goals, motivation, and priorities of the other person. 
Personally, I prefer using `ghc` and the `haskell-*` packages from the respository. These are for the dependencies that you can fulfill and the others you can install by invoking `cabal` within the source directory of your main package. It will install all other dependencies that are not available. Before you use cabal you should [read this and change your settings accordingly](https://wiki.archlinux.org/index.php/Haskell#Problems_with_linking). Of course, using `stack` is much easier for just a single package but it will use a lot of space and compile most of it.
I wish in this thread there had been more comments like this and fewer from the sentiment "diversity is good because the word has a private meaning to me which is obviously a good thing and I don't even need to explain that".
Thank you, you are warmhearted.
&gt; I don't think the primary &gt; causes &gt; of the disparities in Haskell users can be found within particularly bad characteristics of Haskell devs, bloggers, etc. But that doesn't mean we can't try our hardest to look for &gt; remedies Is the idea, then, that the Haskell community has the resposibility to correct the damage done by the bad characteristics of others?
I'm looking for resources on the theoretical foundations of Haskell, I've been reading random articles and blogs and was wondering if there was a book or something with a more cohesive approach. I'm familiar with category theory and logic, but just have cursory knowledge of type theory, "propositions as types" and in general how they're used in Haskell (and functional programming in general).
Thank you, this is exactly what i was looking for.
Crazy, why are people downvoting this?
All meaningful interaction (be it an exchange on social media or collaborating on an open source project) begins with good faith and trust.
Available in `nixpkgs` master: `nix run -f https://github.com/NixOS/nixpkgs/archive/master.tar.gz haskell.compiler.ghc863`
I'm the author of `PyF`, a formatting library which helps you do stuffs like `[format|You bought {count} items for {untaxedPrice * 1.2:.2} (including 20% VAT)!|]` and dependent type may improve a lot the programming and user experience, but I'll still need template haskell for the code generation. For example, here `untaxedPrice * 1.2` needs to be parsed and transformed to haskell code.
Sorry, forgot to mention: &gt;:set -XFlexibleContexts &amp;#x200B;
Thanks for this! &amp;#x200B; I got a similar reply from Edward Kmett on Twitter: [https://twitter.com/kmett/status/1071176870039875586](https://twitter.com/kmett/status/1071176870039875586) &amp;#x200B; Really grateful to have access to such smart ppl : ) This def was a brain teaser for me..
the first list will be equals or smaller than the second list and the length of the second list will be a multiple of the first list. The first function does this perfectly but does not separate them into individual lists per cycle. Ive tried to tweak the code in order to get that but cant seem to get it to work 
plug: http://hackage.haskell.org/package/monoidal-containers
Thanks for your question, I have never heard of Tidal but decided to give it a try, it looks like fun. Here is how I built Tidal on archlinux using stack. This method does not use any arch provided haskell/ghc packages. It uses stack's nightly-2018-12-06 collection of packages. ## stack setup Install stack: curl -sSL https://get.haskellstack.org/ | sh That installs the stack executable into /usr/local/bin. Optionally you might want to move it into \~/.local/bin, make sure it is in your PATH. From now on, everything stack downloads, builds will go into \~/.stack/ Update the package index: stack update To prevent automatic installation of ghc edit \~/.stack/config.yaml and include the line: install-ghc: false To trigger installing the most recent ghc and base libraries and to set the resolver in \~/.stack/global-project/stack.yaml for implicit global projects run the stack command below in a directory which is not a stack project. stack setup --resolver nightly-2018-12-06 ## Build the Tidal library Here we use Tidal's sources directly from its git repo on github. git clone https://github.com/tidalcycles/Tidal Change into Tidal/ and modify the satck.yaml file. cd Tidal/ git checkout -b local-build Edit stack.yaml and set the resolver and extra-deps, it should look like this below: resolver: nightly-2018-12-06 packages: - '.' extra-deps: - hosc-0.16 Build Tidal. That will probably take ages and consume quite a bit of memory. Depending on your machine, you might one to restrict the parallel jobs running to avoid trashing: nice stack --jobs 2 install Apart from some compiler warnings that worked for me. ## Install dependency packages pacman -S supercollider sc3-plugins For the rest, installing editors, plugins etc. follow the instructions in Tidal's Getting Started document. As far as I know all packages needed are available on Arch. I use neovim, so I still need to figure out how to use the repl plugin with that. I hope that helps. However, it looks to me that you would still need some Haskell knowledge to use it in practice. Well, this can be as good a reason as any to learn the language.
Good faith should not be a shield against criticism of what is being said. It is possible to be wrong in good faith. It is possible to derail discussions in good faith. It is possible to be obnoxious in good faith. Consider the comment just above that I'm replying to, in which you so very performatively tell /u/c_wraith to fuck off. Ideally, I would reported the comment to the mods because of that insult. However, I feel discouraged to do so, as the framing of the surrounding discussion suggests that the removal of the comment might serve as fuel to the narrative that dissenting views are being suppressed here. Such a situation is, for the lack of better words, acutely irritating.
This whole discussion makes me think of https://en.wikipedia.org/wiki/Wikipedia:Assume_the_assumption_of_assuming_good_faith
If you allow me to take the liberty, here is what I think /u/cdsmith and /u/sclv are trying to get at here: 1. Taken at face value, "diversity of opinion" sounds like a good thing that non-authoritarians shouldn't object to. However, it is possible to use this notion in an equivocating manner, by implying diversity of opinion is entirely analogous to, say diversity of gender or race, when it is clearly something of a different nature. Such equivocation is often used to push for a certain kind of agenda while appearing to say something uncontroversial. There is one example of that in a comment by someone else elsewhere in this thread. 2. It feels over the top to talk about "far left" views being pushed in this thread.
wow that's pretty complicated to set up lol is it possible to install Tidal via cabal by any chance? cabal's packages are usually more up to date right? while the repository for stack is usually more curated and stable? Are there any easier method by any chance? :P Basically, I should avoid using pacman because through pacman things are dynamically linked and tend to lead to dependency hell down the road correct? I see these in the AUR, but not sure if it's of any use: cabal-static, and cabal2arch. Or could i perhaps follow the manual method here described here in the lower portion of the page: https://www.haskell.org/downloads/linux 
Book reading is a pain. Quite often good explanations are found in stackoverflow or blog posts.
How about "Types and Programming Languages" (TaPL)?
`let` is lazy, so that `let ((), !2)` actually means `let ~((), !2)`, and the inner bang only fires when the outer tuple is needed, which in this case is never. Note that following that rule, `let !x` would naively translate to `let ~ ! x`, which is equivalent to `let ~x`, but since that is quite counterintuitive, there is in fact a special case for desugaring strict `let !x`.
Seems interesting, thanks!
&gt; But even his work mostly boils down to letting the kids solve the problem imperatively first, then redoing the work with reflection on the declarative interpretation. This sounds a lot like what Piaget described as reflective abstraction. I think he was pretty spot on with his analysis. People tend to think of him nowadays just as a child psychologist, but he was genuinely interested on how mathematical thinking develops. Ed Dubinsky [investigated](http://www.math.wisc.edu/~wilson/Courses/Math903/ReflectiveAbstraction.pdf) how functions and induction may arise as a result from reflective abstraction. 
Thanks!
Windows already has chocolatey which is just as simple `choco install ghc` and that's it. 
So use the second or third function I provide üòú
If you check the source of the `integer` parser, you will see that it is wrapped in a `token` parser. This means that it will try to parse some digits and then any remaining space. So in your third example the `integer` parser will _consume_ the whole input string "123 ", and then the `eof` parser will succeed
Haskell programs on arch usually take up way more space because the libraries are dynamically linked and can't be discarded. Plus you end up with constant version churn.
It is common practice for token parsers to consume the whitespace which comes after the token. As the megaparsec documentation [explains](http://hackage.haskell.org/package/megaparsec-7.0.4/docs/Text-Megaparsec-Char-Lexer.html): &gt; Parsing of white space is an important part of any parser. We propose a convention where every lexeme parser assumes no spaces before the lexeme and consumes all spaces after the lexeme; this is what the `lexeme` combinator does, and so it's enough to wrap every `lexeme` parser with lexeme to achieve this. I can see that [the `token` combinator](https://www.stackage.org/haddock/lts-12.22/parsers-0.12.9/src/Text.Parser.Token.html#token), which `integer` uses, uses the same convention.
If you want something "painless" you can use `ghcup` instead of Stack: Follow the simple instructions from https://github.com/haskell/ghcup to setup `ghcup` (i.e. download `ghcup` and set `$PATH` accordingly), and then ghcup install-cabal ghcup install ghc-8.4.4 ghcup set 8.4.4 then just cabal update cabal new-repl --buiild-depends tidal and cabal will go ahead, figure out the build-plan and compile everything, and you'll be thrown into a GHCi repl with the `tidal` package in scope. It doesn't get easier than that! :-) PS: I've tried this in a `archlinux/base` Docker image, and I had to install some baseline libraries GHC depends upon via pacman -Sy tar awk grep gcc make
Not the "Haskell community," but rather, decent people in general, in may respects. Except I would not say "bad characteristics of others." I'd say, we want to seek _good outcomes_, even if the reasons there are sometimes _bad outcomes_ are not directly attributable to things we have caused ourselves. This seems a very elementary principle. What's the alternative? "I didn't cause the economic crisis, so I don't care if people starve." "I didn't declare war, so I don't care if people are bombed." "I didn't personally cause global warming, so what concern is it of mine if oceans boil."
Yes, I think it does. And also, no I'm not a big fan of the guidelines in that I'd prefer something more standard like the contributor covenant. Also I won't be "logic-errored" into my head exploding from this absurd line of questioning like a cartoon robot.
Hey all OP here -- I took some time to write about other things so this post was a bit delayed but I'm finally through with it! As always, any feedback is welcome -- if you see anything wrong or weird or dumb, please let me know so I can update the article and let others know. I should see it in &lt;10 hours (I'm way past my bedtime). The post is only lightly edited so it is likely full of grammatical bugs. Please don't try to compile the code snippets directly, but rather refer to the full code listings and commits littered throughout the article. Most of the type stuff is the same, but I think the more interesting part (apart from actually getting the API up and running) are the steps where we see how earlier choices like making the `Task (state :: TaskState) f` type make things difficult -- after getting things to compile I actually go back and simplify quite a bit.
I was just referring to the idea of any of this being "far left" which it is manifestly not. 
I don‚Äôt see any mention of Haskell on the page. My browser says no matches, too. Could you quote the relevant section, please?
Rock! ‚úä I lose
The charts are lazily loaded, so the text won't appear until you scroll down to it. Searching for "Which languages do developers prefer by age?" will take you to the chart mentioned by the OP. * 18-24: -14.8% * 25-34: 10.4% * 35-44: 16.6% * 45-54: 38.2% * 55+: -20% Explanation: &gt; The language preference graph is based on a Love-Dislike Index, which takes the % of developers who love a language and subtracts the % of developers who dislike the same language. This helps us determine the positive or negative sentiment of a given programming language or framework. Where a score of 100%=most loved and a score of -100%=most disliked.
Good to know. I assume that you have Python3 installed, as well us any other required dependencies.
If you allow me to take the liberty, here is what I think /u/cdsmith and /u/sclv are trying to get at here: 1. Taken at face value, "diversity of opinion" sounds like a good thing that non-authoritarians shouldn't object to. However, it is possible to use this notion in an equivocating manner, by implying diversity of opinion is entirely analogous to, say diversity of gender or race, when it is clearly something of a different nature. Such equivocation is often used to push for a certain kind of agenda while appearing to say something uncontroversial. There is one example of that in a comment by someone else elsewhere in this thread. 2. It feels over the top to talk about "far left" views being pushed in this thread.
I replied to the wrong comment, sorry -- this was meant as a reply to erikd.
Interesting to see the group with the least experience being burned disliking it the most.
&gt; Good faith should not be a shield against criticism of what is being said. I agree. What's frustrating is that there's an explicit accusation of bad faith, and that's being used as justification to downvote rather than engaging with what's being said. &gt; Such a situation is, for the lack of better words, acutely irritating. I get that. It's irritating that if I write a polite response, I'm to be ignored because politeness is part of some trolling strategy. I think we can agree that this isn't a great dynamic.
Anyone try using multiple types of bound variables using the, erm, [bound](https://www.stackage.org/haddock/lts-12.17/bound-2.0.1/Bound.html) library? If yes, I posted a [question](https://stackoverflow.com/q/53689413/2682729) on SO :).
You mean the second? The 18-24s are more keen than that 25-32s
(-14.8) ftfy
Besides /u/Syrak's excellent answer, you get `Base`, `Recursive`, and `Corecursive` instances "for free" if you write the base functor type first, then define the type using `Fix`, `Mu`, or `Nu`. data TreeF a r = TreeF a [r] deriving (Show, Eq, Functor) type Tree a = Fix (TreeF a) (`type` or `newtype` depending on what you want to do.)
Thanks! Well, just using the binary distribution worked fine for me. (I must have tried to install the src distribution for it to require a newer Python than comes with MacOS, and the extraneous gnu dependencies seem to be an 8.6.2 bug.)
Haskell, for when you are old enough to know better, and not old enough to have stopped caring. ;)
I had to search for "Haskell" (3 hits) and then the age-specific graphs have to be manually chosen, so that you could see how the Haskell bar changed.
Not Haskell related, but they lump together AngularJS and Angular2+ (which are hugely different) and .NET and .NETCore (which are less different, but still different enough to warrant their own statistics) It's interesting that favor towards FP languages correlates with age (and therefore experience?)
No, 18-24 is 14% **lower** than all-age average, and 25-34 is 10% **higher** than all-age average.
Also, the 18-24 group had an above-average interest in PHP of all things, and they are the *only* age group that does.
Is it really that surprising, though? Almost all of the kid-targeted programming content is imperative.
I don't know whether I'd completely endorse cdsmith's comment, but it is certainly true that learning FP after learning imperative is quite difficult. My experience learning and teaching is that habitual patterns of thought from imperative programming point you in the wrong direction, and the more you have to suppress those reflexes as you learn, the more uncomfortable it is at first. (Aside: It reminds me of the discomfort I felt learning to type properly, and having to override all those reflexes felt awful.) As to OP's problem, the key is learning how to get toe-holds into new source files, even when they appear impenetrable at first. This is a learned skill, but is very seldom taught because I think most people build up an intuitive understanding through practice, and never distill the knowledge to pass along. Here are some questions that may help slice things up a different way. You might not be able to answer them all at first glance, but they could point you in the right direction: * What is the module's objective? * What are the types in play? What do they mean? * What are the smallest functions in this module? How are they combined to build up an "entry point"? * What are the largest functions in this module? What are the main subproblems they need to solve? * What are the abstractions and typeclasses in play? How well do you understand them? On that last point, I don't just mean "Oh, a `Monoid` is `mempty` and `mappend` with laws blah blah blah". I mean, if you look inside yourself and ask what `Monoid` means, is there a part of you that comes up with an intuitive response? I find I need to work with things for a while before I gain an intuitive understanding of them, and I can only build out so far away from things I intuitively understand.
Thanks for the in-depth explanation. That really helps me build a fundamental intuition on this.
Thanks for digging this gem out. Very helpful :)
Cheers! I'm writing a post on prisms ATM so I'll try to cover some of this stuff! You can subscribe at https://lens-by-example.chrispenner.ca or follow me on patreon to hear about it when it comes out!
 shakespeare : String -&gt; String -&gt; { language : Type, message : Type | language -&gt; message -&gt; String } I think it could be done but it would be awkward and less usable.
This is a great introduction to a confusing world! I offer another alternative which partially avoids the n^2 problem by using parameterizable Phantom Data Kinds here (see the last section): https://chrispenner.ca/posts/mock-effects-with-data-kinds
That doesn't really do the same thing though. Of course you can write an actual translation function even without any sort of TH, and without dependent types for that matter. What this doesn't do is generate the right enumeration type for you, implement the right classes for you etc.
As a Haskell newbie, I really wish that Haskell was more popular among my peers--it may be my specific circumstances, but having that to have discussion with people I interact with would be amazing. But not through internships, university, my work, friends, or acquaintances have I encountered Haskell or FP, only through the internet (and perhaps a stroke of luck).
`generic-lens` is useful because the lenses are calculated at compile time, and *not* dynamically generated at runtime.
Are the answers anywhere? I *really* want to see if I was right (and see the ones I missed)! Enjoyed the puzzle though; these kinds of challenges are fun tests. Anyway, here's what I've got. Spoilers etc. etc. (it's a fun exercise, I'd recommend doing it before looking at other people's answers) &gt;! apple: reverse ($) grapes: infinite loop? nutmeg: const albatross: map (over lists) Cat: [] (with arguments to (:) swapped) jaguar: infinite loop? (type says id) Lion: non-empty [] opossum: foldr Vulture: Functor fox: fmap alpaca: const [] Vultures: Monad without return lynx: =&lt;&lt; (or flip (&gt;&gt;=)) Artichoke: ? (I think it's some sort of algebra relating to nori) Fennel: Fix nori: some form of catamorphism? These Haskell ones were really confusing with the names :P True: Maybe putStrLn: maybe False: semigroup (.): No idea if it relates to something common. Think I know what it does though: take a function, and a converter, and a single argument; apply the converter to one copy of the argument, and pass the converted and non-converted arguments to the function iterate: id reverse: infinite loop? And without clicking the link, I'm guessing that bananas link comes with some lenses and barbed wire too. !&lt;
Do they give absolute numbers? Perhaps, in the older groups, many of the people who would dislike Haskell are not programmers at all? E.g., more non-geeks become programmers these days? &amp;#x200B; Another hypothesis: kids these days know a lot of random things from the net, so in older groups many of the people that would dislike Haskell don't know about it at all.
Thanks for playing :) I originally intended the post to be open-ended, which is why I left out answers. But I can see why it would be nice to have some reference to compare your answers to at the end, so here are mine (which might still contain errors!): https://gist.github.com/Lysxia/fe1ffd54ecb1daef0998c6c46c8851d7 Regarding Lion, I changed the definition after posting, but your answer is the correct one for the initial version.
I agree with most of yours, but here is where we differ: * I think that Lion is a non-empty tree, not non-empty list because it has an element and a list of Lions, not a list of elements. * I think Artichoke is Coyoneda * I also think False is semigroup, but it's hard to tell with no laws =) * (.) appears to be the S-rule from https://en.wikipedia.org/wiki/SKI_combinator_calculus, which I just found out is equivalent to &lt;*&gt; for (a -&gt;) functors
**SKI combinator calculus** The SKI combinator calculus is a combinatory logic, a computational system that may be perceived as a reduced version of the untyped lambda calculus. It can be thought of as a computer programming language, though it is not convenient for writing software. Instead, it is important in the mathematical theory of algorithms because it is an extremely simple Turing complete language. It was introduced by Moses Sch√∂nfinkel and Haskell Curry.All operations in lambda calculus can be encoded via abstraction elimination into the SKI calculus as binary trees whose leaves are one of the three symbols S, K, and I (called combinators). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; Showing the radical left they aren't welcome is a longer-term battle and will have to be done gently. I'm not for a political purge of anyone, which I think is a grotesque idea. I would like people not to be bigots in our main venues of communication. I don't think that one has to imply the other.
Great, thanks. I got some of the ones which I thought were infinite loops/undefined wrong. \`grapes\` is a really cool function (for anyone wanting to test it, try running \`grapes (1:) (2:)\`: it will return the same list as \`cycle \[1,2\]\` (an infinite list \[1,2,1,2,1,2...\]). I thought it would infinite loop like jaguar, but it actually works (in the same way \`fix\` does). I really need to look into \`fix\` more, because I don't really have the intuition for how you can apply a function without a starting value. I guess it kind of makes sense considering infinite data structures (like \`fix (1:)\`). I think \`reverse = iterate . reverse\` is an infinite loop, not \`fix\` as you suggested. As \`iterate\` is \`id\`, it's the same as saying \`reverse = reverse\`. Not sure which version I saw for \`Lion\` (but based on the edit time on the GitLab repo I think it was the updated one); I think I just didn't look at it too carefully and didn't see the nested \`Lion\`.
It's sad. I only know 1 other student at my University who actually likes Haskell. Everyone else has either never heard of it or despises it
I think I did my answer for \`Lion\` after the change too, I just didn't see the nested \`Lion\`. Hmm, haven't come across \`Coyoneda\` as a datatype before; I've read a bit about the Yoneda lemma before but don't really remember what it is. I've just found another rabbit hole to jump into I think (or maybe I should just keep reading [https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/)). Good spot on the \`S\`; I didn't notice that. I think we should rename SKI to the \`(.)-nutmeg-iterate\` calculus.
Makes sense why though. Haskell has a learning cliff and (afaik) doesn't have a solid way to write a real, meaningful program. (I've run through learnyouahaskell but if you asked me to write a simple web server in Haskell I'd be so lost)
I want that on a t-shirt.
Obviously, you can't draw many conclusions from one random survey. I wonder if this points to more principal engineers and architects allowing Haskell on the short list of languages to consider for their shops. I suspect the remaining hurdles will be breadth of ready to use functionality in libraries, and getting past the cultural problems that powerful languages like Scala, Haskell can sometimes be prone to over how advanced abstractions to use in production code. 
my build didn't have that issue :) 
I often wonder if Haskell would have been easier to learn as a first language than it would have to have become sunken into the imperative ways, and then tried to learn FP
http://www.haskellbook.com is the book for you, especially if you're looking for depth.
I'd say `alpaca` isn't `const []`, but rather `const (const [])`, and even that's not true since `alpaca` forces the list structure of the 2nd argument, so if the second argument is an infinite list (or the evaluation of the list structure is an infinite loop), then `alpaca a b` is an infinite loop as well, while `const (const []) a b` is *always* `[]`
It seems weird to me that Haskell has so much more mindshare than Rust.
I think it more than when you are starting programming, immediate feedback feels more valuable than good design or even understanding what you are doing. PHP gives the feedback new programmer want (some in my web browser) *immediately*. Haskell, much less so. But, it certainly could be problems in the sample.
It's been around longer. Rust is definitely worth learning. It's not as nice as Haskell, but satisfying the borrow checker makes you think about object lifetimes, and that can be valuable even when you have a GC, and essential with you don't.
If you parse to an intermediate 'token' type, you can write a function that transforms the indentation tokens into left and right parens. While traversing the sequence of tokens, if you notice that the indentation level has increased, drop in a left paren. If the indentation has decreased, drop in a right paren. Then you can parse the output of this as if it were a regular lisp program.
Nice blog post series! I've enjoyed reading previous parts. Here's some feedback from about this part :) &gt; A slightly more nuanced explanation might be that it‚Äôs the way to distinguish Int from Maybe Int I'd rather say that _kinds_ is the way to distinguish `Maybe` from `Maybe Int`. Because both `Int` and `Maybe Int` have the same kind. &gt; the syntax for ‚Äúthing that is a type‚Äù is actually * ‚Äì this means that Int is * Since GHC 8.6.1 there's a process to remove `*` as a kind annotation. It's recommended to use `Type` instead of `*` for kind annotations. Would be really good if new blog posts could follow this convention to help moving things forward! Though, I see in the code that you're using explicit kind signatures in a form like `Type -&gt; Type`, so maybe it's okay to use `*` in the description. But I guess you can mention `Type` as well. * https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Kind.html#t:Type Nice job on writing `FromRow` instances for GADTs! I've written such instances on my job just couple days ago, and it was like deja vu to see something similar described in the blog post :) Also, I see that you're adding constructor to your GADT to cover unknown case. Like here: data WithID (ident :: Identifier) a where WUUID :: UUID -&gt; a -&gt; WithID 'UUIDID a WINT64 :: Int64 -&gt; a -&gt; WithID 'INT64ID a WID :: Either UUID Int64 -&gt; a -&gt; WithID ident a I'm not sure about this approach because it breaks the ability to use type-directed pattern-matching. For example, you can't write function: fromUUID :: WithID 'UUIDID a -&gt; a fromUUID (WUUID a) = a This function is not total, because case with `WID` is not covered and GHC will complain about that. Usually, what I do in my code, is that I create extra existential wrapper around GADT. This also helps me to work with lists of different constructors. Here's is how it looks like: data WithID (ident :: Identifier) a where WUUID :: UUID -&gt; a -&gt; WithID 'UUIDID a WINT64 :: Int64 -&gt; a -&gt; WithID 'INT64ID a data WithAnyID a = forall (ident :: Identifier) . WithAnyID (WithID ident a) So if you don't know which ID you have, you should use `WithAnyID` data type and then pattern-match on it.
Thanks so much for the feedback! sorry you have to suffer through my writings and ruminations :) &gt; I'd rather say that kinds is the way to distinguish Maybe from Maybe Int. Because both Int and Maybe Int have the same kind. Hey you're absolutely right -- that's what I was talking about. I will try and rewrite that section. You're right of course that `Maybe Int` *is* the same kind as `Int`, since it's fully "resolved', I should have used `Maybe a` (or just `Maybe`) for that example, not `Maybe Int`. &gt; Since GHC 8.6.1 there's a process to remove * as a kind annotation. You're right i should have called this out more explicitly. I meant to also include a link to the proposal (I think I saw one a while back), to make `*` more readable as `Type`. I will go back and make sure it's a little clearer, maybe break up that section a tiny bit to make it more digestable. &gt; Also, I see that you're adding constructor to your GADT to cover unknown case. Like here: &gt; I'm not sure about this approach because it breaks the ability to use type-directed pattern-matching. For example, you can't write function: &gt; Usually, what I do in my code, is that I create extra existential wrapper around GADT. This also helps me to work with lists of different constructors. Here's is how it looks like: Yeah it is super unfortunate what I had to do there. I actually optimize it out all together since I never ended up using the `WINT64` constructor anywhere when simplifying. Thanks so much for the alternate suggestion, that existential wrapper looks *so much better*. I'm going to update the post and add this approach there if you don't mind me crediting your reddit username (if there's a better site or something let me know). I'll try and update the post by EOD today (&lt;8 hours?)! Thanks again for the feedback -- honestly I'm a little worried that all the type antics are going to turn people away -- it can be really simple and easy to write APIs with Haskell (thanks to the hard work of the `servant` contributors and WAI and Warp and all the other great libs out there), and I'm finding it hard to balance the "easy breezy" attitude with the masochistic "let's punish ourselves and the compiler with more types" tones. 
Besides syntax stuff, i use ghc-mod and stylish-haskell. Also this command to help open current file in ghci `command! Ghci :!ghci -threaded %:p` 
Ohhh
Thanks. Yup, got the numbers from a comment.
What is the most general setting for a `filter`-like function? It seems like a structure that can be both folded and unfolded would fit the bill, but using `toList` and `fromList` seems inefficient.
This is somewhat off topic, but I looked at the [qualifications portion](https://research.hackerrank.com/developer-skills/2018#qualifications) of that report and it seems that "problem solving skills" are very much in demand, but "system design" is not. I read this as: "Please intelligently solve my problems, but don't bother me with your bigger 'system design' ideas please." I am both irritated by that attitude and can understand it. I've had to maintain systems where overarching problems limit my ability to permanently fix any problem, and I've seen developers complaining and asking for a huge rewrite rather than doing their work (and may have been one of these myself once). Just something I noticed.
I don't do anything special. Just \`hlint\` and \`brittany\` through \`ALE\`. I do all of my interactive development through \`ghci\` or just a simple \`stack build . --fast --file-watch\` in another \`tmux\` pane.
The approach looks nice! The only problem with filtering out `import` and `module` statements is that these statements can be spawned over multiple lines (multiple export entries or multiline import in case of imports with explicit lists). So this approach is not precise enough... I'm using `cloc` utility to calculate line numbers which filters out blank lines and this is already good enough to me.
Those surveys don't seem to have any controlling variables. It's not clear what conclusion to draw from them
Lots of things I don't yet about haskell but with the replies I will how far I will get.
It would obviously be good if the haskell community could solve the economic crisis. Does it follow from this that our "Guidelines for respectful communication" should have "to solve the economic crisis" as an axiomatic motivation? To answer your question, the alternative is for our guidelines to instead have as exact focus whatever serves the haskell community best. There is a wide spectrum between a) "have as axiomatic motivation" and b) I don't care rejecting a) does not take you to b). &amp;#x200B;
I care
It is well within our power and not hard to try to not have people be bigoted jerks and treat one another decently within certain limited realms of discourse. And even to bend over backwards a bit on this account. So it seems at about the right scope of things that people can do as participants in the Haskell world, and it also seems like it would further the entirely reasonable aim of enabling all the wonderful things about FP to be spread more widely to a broader audience. It is beyond me why this seems to stir up such strange reactions.
Grapes is `fix (f.g)`. The `(.)` is `&lt;*&gt;` specialized to functions. Not super useful knowledge outside of code golfing. I thinj nori is basically cata :: (f a -&gt; a) -&gt; Fix f a -&gt; a cata alg = alg . fmap (nori alg) . unFix
The [`megaparsec`](https://www.stackage.org/lts-12.22/package/megaparsec-6.5.0) parser combinator library allows indentation-sensitive parsing (see [the relevant tutorial](https://markkarpov.com/megaparsec/indentation-sensitive-parsing.html), and more generally the [`megaparsec` tutorial series](https://markkarpov.com/learn-haskell.html#megaparsec-tutorials), for more information). I've used `megaparsec` regularly, and it's a great library! (I haven't tried the indentation-sensitive parsing yet, though.)
Awesome! I'm currently going through part 2 and really like how you explain the thinking behind the design. I also have a suggestion for another blogpost, I'm curious what your IDE setup is :)
That's possible. And more people start programming early. And many of those are taught at school in a stress-free fashion and are triggered heavily by the unforgiving and uncaring type system. :)
As someone in the 35-44 bracket I might add that at some points you just don't hate languages that much. At the end they are all tools for a job to be done. Some tools are better suited for some jobs, and you love them, some are 'meh' and you just don't care. You use what you have to and that's it. When a tool is not suited for the job you are using you don't hate the tool - he tool has its uses after all - you hate whoever makes you use an inadequate tool.
Thank you for the kind words! The design stuff is what I've found to be the trend across codebases, none of them in Haskell. It seems like most languages with frameworks for web services work gravitate to the setup I'm trying to build -- at some point you have to organize your monolith! My IDE setup is emacs haha. It's even worse than the usual flow, because I'm not using and evaluating stuff inside emacs with the in-process GHCi, I'm actually using GHCi in a tmux pane right next to the code (so I switch tabs and `:r` a lot). I can't remember what went wrong when I was using in-process GHCi but I stopped a while ago and haven't looked back. I am living in the stone ages in some sense :). All ears for your blog post suggestion! 
&gt; where we have special functions that parse a template string and take inputs (depending on the template) to produce a string You can write such a function without dependent types with just regular Haskell. What TH gives you is the ability to generate top level definitions from a text description automatically, meaning you can use the result of parsing this template in a type safe manner. This isn't the case for either C or Python.
Is there any way to measure the time taken by GHC in the linking step (as part of building an executable)?
For the last few months I've been using [Haskell IDE Engine](https://github.com/haskell/haskell-ide-engine), which supports the language server protocol. There are some plugins that enable vim to act as an LSP client. I use [LanguageClient-neovim](https://github.com/autozimu/LanguageClient-neovim) together with [deoplete](https://github.com/Shougo/deoplete.nvim) for completion support. To enable HIE in `LanguageClient-neovim` you need to add this line to your vimrc: `let g:LanguageClient_serverCommands = { 'haskell': ['hie-wrapper'] }` However, this setup is not as lightweight as some of the others that have been posted here. If you are just getting started with Haskell, a second terminal or tmux pane with ghci is just fine.
I was worried for a moment that `reverse` might actually loop unproductively but if I load the file in ghci it does look like `fix`: &gt; reverse (1 :) [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1...
I agree. I'll update my answer, thanks!
* [Control.Monad.mfilter](http://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Monad.html#v:mfilter) for MonadPlus * [witherable](http://hackage.haskell.org/package/witherable) for non-Monad container (Vector, Map, etc)
I spent some time thinking about valid use cases of `foldl` and accidentally wrote an in-depth analysis on the use cases of the different list folds: https://gist.github.com/cjay/c15813c911609d355f8e280ee64b3e8f Is it fully correct? Did I miss something? And if it's free of errors, does it have value for others? Should I put it on wiki or blog? :)
I was so confused at that working until I remembered that you redefined `(.)`... Redefining the Haskell names was both evil and brilliant at the same time.
I'm currently using * [haskell-vim](https://github.com/neovimhaskell/haskell-vim) for syntax highlighting * [ghcid's neovim plugin](https://github.com/ndmitchell/ghcid/tree/master/plugins/nvim) to get in editor jump-to-error support and a baseline IDE experience * [fast-tags](https://github.com/elaforge/fast-tags/) to get tag jumping to work. The combination is remarkably effective. I've yet to make the jump to a full LSP setup.
Heheh. ~~Sorry~~ You're welcome!
what's the difference between cabal new-repl --build-depends tidal vs. cabal install tidal? i guess this would be cabal new-install tidal ? but anyway what's the difference exactly? 
I don't know how much weight I'd give to this survey. Scala is way over represented, and I closed the page already but the vim mindshare was through the roof compared to all other editors, which is not representative.
&gt; VIM or Emacs? They should put this on the first slide! I had to go all the way to the last slide and see the outlandish results there before I realized this whole thing must be humbug :)
`foldl'` is more efficient than plain `foldr` when the operation is sufficiently strict, but otherwise I don't think there is a clear winner even if the operation is associative. For example, `concat` is asymptotically more efficient using `foldr (++)` than either `foldl` or `foldl'`, which at worst have quadratic complexity, and at best still require walking through the whole list of lists regardless of what is needed. Using `foldr` you can implement all three other variants. More generally, the "call stack" buildup can be avoided either with a little continuation-passing or fine-grained control of laziness, and `foldr` can also ignore "payloads on the left" based on the computation "on the right".
I know it‚Äôs probably a long time away, but do you have a timeframe on your book? I love your blog, so a book from you would be amazing!
Thanks!
&gt; vim mindshare was through the roof compared to all other editors, which is not representative. Yeah, this is this only survey I've seen with vim *so high*. I've seen it come out on top before, there seems like *something* skewing the data, at least on that question.
Not sure if there's a proper way, but my hacky instinct is to write a linker wrapper script that just calls the actual linker and also outputs it's running time somewhere. Then you can just use GHC's command line flags to use the wrapper. If your worried about linker times, LLD seems to be the fastest of the bunch by far
Huh, looks really good, but $60 for an incomplete E-book is just way too high. Don't know what they are thinking there.
At this point it's too far out to commit, it'll probably be a year or two haha, the plan is to keep releasing content and then eventually consolidate it down into the book üòÑ Thanks for the kind words!
[https://hackage.haskell.org/package/relevant-time](https://hackage.haskell.org/package/relevant-time) is good, which uses the wonderful time library [https://hackage.haskell.org/package/chronos](https://hackage.haskell.org/package/chronos) :)
You also have to be old enough to have enough area for the text. 
\&gt; I'd like for working programmers in the West to pay full price If you said just +- "I want those who can spare the money to give me some incentive", it would have been commendable. As you wrote it, you just expressed a prejudice against people based on nothing else than group membership. Social Justice is poisonous brew, it destroys whatever it touches (under the guise of "helping"), it twist love into hate, tolerance into oppression. &amp;#x200B;
It's just a way to say it's based on ability to pay.
You need to relax.
Are there any estimations for when the book is going to be finished? I started looking at it back in 2016 but figured I'd wait to purchase until it was completed + printed. I would love to buy a printed copy when I have the cash.
It's content complete now and has been for some time. This is really last stretch editing I'm doing right now. I made one content change (purging CoArbitrary) for the RC3 release yesterday. It's mostly a matter of whether you want to get the ebook now or buy the print version later. I can't promise on timing, I have more work to do at this point on the e-commerce backend to http://lorepub.com than I do the book in my estimation. I've streamed myself working on that before: http://youtube.com/bitemyapp
Where do you live? There may be more haskell users nearby than you expect. I had a similar experience with enjoying the language but feeling really isolated from other devs, but I've managed to interact with other enthusiasts at conferences and even in Atlanta (where I live).
`sloccount` is a nice powerful tool for this: https://dwheeler.com/sloccount/
For most students, it is the most (only?) non-algol general language they will ever touch. It's easy to think that programming just *is* imperative c-like statement writing. I can see Haskell leaving a bad taste in your mouth in that sense. It's like discovering Native American music with semitones and such after ten years of thinking that Western pop is all there is.
ee.. what?
I pretty much learnt it all alone, certainly took longer than it should ;)
learnyouahaskell is just a bad resource to learn haskell IMO.
I've found those people who 'despise' it tend to not really know anything about it. And when questioned say silly things like "it doesn't have semicolons"
Is `v2` a synonym for `new`? e.g., is `$ cabal v2-install ...` the same as `$ cabal new-install ...`. I can't find any documentation of `v2`.
&gt; It is beyond me why this seems to stir up such strange reactions. 1. Motivation/Goal formulations matters. Badly formulated or justified goals cause real harm. 2. One strange reaction feeds the next, and before long we get to the level of absurdity where commenters feel the need to speak about "decent people" and "people starving" and generally fighting imagined monsters. The Principle of Charity is quickly abandoned when terms that have ideological signal value are mentioned. &gt; ...it also seems like it would further the entirely reasonable aim... Given a carefully chosen main goal/aim anything that furthers it is good - _exactly because, and only to the extent_ it is aligned with the main goal. Different goals are never perfectly aligned, so it is very important to distinguish between primary goals and subordinate goals.
It's essentially complete, I don't know what might still be missing at this point but it never feels incomplete as you go through it. What you get for your money is a very high quality textbook, better than any that I remember having to buy in university (for much more than $60 in some cases), and it has tons of content that will keep teaching you new things for months. For me at least, it was well worth it.
I don‚Äôt see what‚Äôs absurd about the word ‚Äúdecent‚Äù?
What is `Capture` for? Desugaring a little bit, it looks to me like you just have type Observable f a r = ContT r f a
Yes, it's a forwards compatible alias (and `v1-` is a backwards compatible alias for the old build commands afaik). If you run `cabal --help` on a recent version of cabal you'll see both sets of commands listed :)
What editor is that in the top screenshot?
Very good points, thanks. So continuations can be used to invert information flow. This makes it possible to base the decision to ignore input list elements on information from both left and right. And the call stack buildup is avoided by the nested accumulator-call returning a continuation right away, which then gets tail-called by the outer call? &gt; `foldr` can also ignore "payloads on the left" based on the computation "on the right". That was already in there though, I think you meant to write 'ignore "payloads on the right" based on computation "on the left"'.
At the age I am, I've spread out enough that there is room for that much text on my t-shirts.
I did mean the same as for `foldl`, since it can be used to implement `foldl` but there may be other computations lazy on the left side that are still more natural to express as `foldr` than `foldl`.
How does this compare to other stream libraries and frameworks?
I'm mildly disappointed to not see a single `Burrito` type class ...
Half life 3 confirmed
- Does you layer provide the shared objects (`.so`) files used by the most GHC compiled apps? - How to make own layer, if I need weird set of own libraries (in fact I do). - In `aws-lambda-runtime` (no redundant `haskell` in the name :) I package `.so` in the zip: http://hackage.haskell.org/package/aws-lambda-runtime-0/docs/AWS-Lambda-RuntimeAPI-Package.html - What does `configureLambda` does concretely. Does it look in the scope for right things and generates the boilerplate? How that boilerplate looks like? Is TH really mandatory? - :+1: for using Docker (I use Docker to build executables too). But where's the `Dockerfile` for the image? (I'm not ok with running arbitrary docker images).
Cool idea! I have a question about `zipMatchWith` though. zipMatchWith :: (a -&gt; b -&gt; Maybe c) -&gt; t a -&gt; t b -&gt; Maybe (t c) &gt; [...] the function [`(a -&gt; b -&gt; Maybe c)`] can make whole match fail by returning Nothing. Do you have any examples which use this "fail the match" mechanism? Also, I feel like the function could just be `(a -&gt; b -&gt; c)`, and if the user wants the option to "fail the match", they could use a Maybe-returning function and then `sequenceA` the result. Am I missing something?
Looks like [Haskell for Mac](http://haskellformac.com/).
[All concepts are Kan extensions.](http://www.math.harvard.edu/theses/senior/lehner/lehner.pdf)
Try using a tuple of `(Integer, Bool)` as an accumulator for your fold operation, starting with `(0,False)`.
Yup, you aren't missing anything. Your approach would work. Why I did this way is a design decision. I wanted a composition of `Matchable` functors are also `Matchable`. To do this, there were two approaches: class (Eq1 t, Functor t) =&gt; Matchable t where zipMatchWith :: (a -&gt; b -&gt; c) -&gt; t a -&gt; t b -&gt; Maybe (t c) instance (Traversable t, Matchable t, Matchable u) =&gt; Matchable (Compose t u) where zipMatchWith f (Compose tua) (Compose tub) = fmap Compose $ sequenceA $ zipMatchWith (zipMatchWith f) tua tub And class (Eq1 t, Functor t) =&gt; Matchable t where zipMatchWith :: (a -&gt; b -&gt; Maybe c) -&gt; t a -&gt; t b -&gt; Maybe (t c) instance (Matchable t, Matchable u) =&gt; Matchable (Compose t u) where zipMatchWith f (Compose tua) (Compose tub) = fmap Compose $ zipMatchWith (zipMatchWith f) tua tub The former (your approach) has a more generic method but requires `Traversable` to be able to compose. The latter (the way I used) has a narrower method but doesn't require `Traversable` to be able to compose. I took the latter because I think it's good to require weaker constraint. Also, Generics is easier this way.
The best thing is that the command below installs not only the haskell `curl` library but also the c library that it depends on called `libcurl`. Most of the package managers require it to be preinstalled by the user. `$ nix-shell --command ghci -p "haskellPackages.ghcWithPackages (p: with p; [ curl ])"` `GHCi, version 8.4.3: http://www.haskell.org/ghc/ :? for help` `Prelude Network.Curl&gt; take 100 . snd &lt;$&gt; curlGetString "https://www.google.com" []` `"&lt;!doctype html&gt;&lt;html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"uk\"&gt;&lt;head&gt;&lt;meta content"` You can also ask for more packages: `$ nix-shell --command ghci -p "haskellPackages.ghcWithPackages (p: with p; [ curl ])" -p python35Packages.requests -p letsencrypt` `GHCi, version 8.4.3: http://www.haskell.org/ghc/ :? for help` `Prelude&gt; :!certbot --version` `certbot 0.24.0` Note that `letsencrypt` depends on `python27` but we also ask for `python35` with `requests` package, which works flawlessly with Nix. 
If you haven't seen it already, there's a similar question with lots of answers here: https://cstheory.stackexchange.com/questions/1539/whats-new-in-purely-functional-data-structures-since-okasaki
Do you know how to write a linker wrapper script? Any pointers / links where I can read more about this? 
Thank you, that's exactly the kind of explanation I was hoping for :) 