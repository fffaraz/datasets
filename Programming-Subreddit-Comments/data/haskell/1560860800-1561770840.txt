Very good advice.
Highly recommend emacs for Haskell Dev, it is in a class of its own compared to other environments I've tried. If you're a vim user like myself, or like easy config and things working out of the box, check out spacemacs with the Haskell layer installed.
Thanks
Good post! I'm a bit confused though. If I understand correctly `takeUniqueFromSupply` just calls `genSym`, but I don't understand why we need two references `UniqSupply`s in `MkSplitUniqSupply`. Couldn't we just do data MkSplitUniqSupply { currentUniq :: {-# UNPACK #-} !Int , mask :: {-# UNPACK #-} !Int } mkSplitUniqSupply c = MkSplitUniqSupply { currentUniq = unsafeInterleaveIO genSym , mask = ord c `shiftL` uNIQUE_BITS } takeUniqFromSupply us = ( mkUniqueGrimly (currentUniq us .|. mask u) , us { currentUniq = unsafeInterleaveIO genSym } ) splitUniqSupply us = (us, us) Comparing with the current code I don't see why this wouldn't work. Am I missing anything? I'm also wondering why you thought the current implementation is as fast as it will get. I think one way to improve it might be to implement an inline `genSym` primop in the compiler. In the threaded runtime it'd be an `atomic_inc`, in non-threaded just an increment. This would eliminate FFI overhead. Of course adding a primop just to use in the compiler is not great, but it'd be an improvement.
Thank you very much for mentioning `relude`! BTW, Travis CI config used there is documented in details in a separate blog post: * https://chshersh.github.io/posts/2019-02-25-haskell-travis
Welcome to Haskell! Already being mentioned that you should probably look into a recursive approach, here's how you do it with list-comprehensions (I'm renaming `lst` to `xs` as this is more idiomatic and `l` to `i` for index, but you do you ): The issue you have is that your list-comprehension will iterate `(x,i)` for every `x` in `xs` and `i` in `[0..length xs]` (note that there is no need for parens), think of it like two nested loops. What you want is to pair the elements of `xs` and `[0..length xs]` by their indices, you can accomplish this with `zip`: f xs = [x | (x, l) &lt;- zip xs [0..], odd l] As you can see I changed the `mod`-part to `odd` since this is in base and more readable, I also removed the `length xs` part: `zip` will stop once one list is empty. If you're in favour of language extensions, you could make use of `ParallelListComp`, the `zip xs [0..]` part is equivalent to `[(x,i) | x &lt;- xs | i &lt;- [0..]]`: {-# LANGUAGE ParallelListComp #-} f xs = [x | (x,i) &lt;- [(x,i) | x &lt;- xs | i &lt;- [0..]], odd i]
``` -- Expression data type (extended to include closure, a pair of Lam String Expr to Env): data Expr = Var String | Lam String Expr | App Expr Expr | Clo Expr Env | V Expr deriving (Eq,Show,Read) -- Mapping type (pair of a string variable and closure) [type Mapping = (Var String, (Lam String Expr, Env))]: type Mapping = (Expr, Expr) -- Environment data type (which is a mapping list): type Env = [Mapping] -- Frame data type, which can be a closure and a hole, or a hole, expression and environment -- [data Frame = (Lam String Expr, Env) Hole | Hole Expr Env]: data Frame = F1 Expr | F2 Expr Env deriving Show -- Continuation data type (which is a frame list): type Kont = [Frame] -- Configuration type (triple of an expression, environment and continuation): data Config = Conf Expr Env Kont deriving Show -- Lookup function that takes a string variable expression and environment, and returns the closure mapped to that expression: lookupAux :: Expr -&gt; Env -&gt; Expr lookupAux (Var string) [] = Clo (V (Var string)) [] lookupAux (Var string) (mapping:env) | fst mapping == (Var string) = snd mapping | otherwise = lookupAux (Var string) env -- Function that executing one step of the CEK machine: eval1cbv :: Config -&gt; Config eval1cbv (Conf (Var string) env1 k) = Conf lamExpr env2 k where Clo lamExpr env2 = lookupAux (Var string) env1 eval1cbv (Conf (App exp1 exp2) env k) = Conf exp1 env ((F2 exp2 env):k) eval1cbv (Conf (Lam string exp) env k) = Conf (Clo (Lam string exp) env) env k eval1cbv (Conf (Clo lamExpr cloEnv) env ((F2 exp env2):k)) = Conf exp env2 ((F1 (Clo lamExpr cloEnv)):k) eval1cbv (Conf (Clo lamExpr cloEnv) env1 ((F1 (Clo (Lam string exp2) env2)):k)) = Conf exp2 ((Var string, (Clo lamExpr cloEnv)):env2) k eval1cbv (Conf (Clo lamExpr cloEnv) env1 []) = Conf (V lamExpr) [] [] -- Function that takes a lambda expression and evaluates it using a Call-by-Value strategy until termination, returning the terminated -- expression (if it terminates): eval :: Expr -&gt; Expr eval exp = recursiveEval (Conf exp [] []) recursiveEval :: Config -&gt; Expr recursiveEval (Conf (V exp) env k) = exp recursiveEval (Conf expr env k) = recursiveEval (eval1cbv (Conf expr env k)) testExpr :: Expr testExpr = App (Lam "v" (App (App (Var "v") (Lam "z" (Var "z"))) (App (Lam "v" (App (App (Var "v") (Lam "x" (Lam "y" (Var "x")))) (Lam "z" (Var "z")))) (Lam "x" (Lam "y" (Var "x")))))) (Lam "x" (Lam "y" (Var "y"))) ``` Also while I'm at it, here's a little word guessing game - I used IO String and IO () but don't know if there's a better/more efficient way (but don't think so without having console output interaction): ``` import System.IO import Control.Monad.Random main = hangman hangman :: IO () hangman = do putStrLn "Think of a word:" word &lt;- getRandomWord let ds = replicate (length word) '-' putStrLn ds putStrLn "Try to guess it!" play word ds 0 getRandomWord :: IO String getRandomWord = do x &lt;- (readFile "dictionary.txt") let dictionary = words x i &lt;- getRandomR (0, (length dictionary)-1) return (dictionary !! i) play :: String -&gt; String -&gt; Int -&gt; IO () play word answerSoFar guesses | guesses == (length word) = do putStrLn "Too many guesses! Sorry, you lose!" replay | answerSoFar == word = do putStrLn "Well done, you guessed the word!" replay | otherwise = do putStr "Remaining incorrect guesses: " print (length word - guesses) putStrLn "Enter a single character:" guess &lt;- getLine if length guess == 1 then do updatedAnswer &lt;- putUpdate (updateMatch word answerSoFar guess) if answerSoFar == updatedAnswer then play word updatedAnswer (guesses+1) else play word updatedAnswer guesses else play word answerSoFar guesses replay :: IO () replay = do putStrLn "Play again? (Y/N)" answer &lt;- getLine if answer == "Y" || answer == "y" then do main else if answer == "N" || answer == "n" then do putStrLn "Bye-bye!" else do putStrLn "Enter Y or N:" replay putUpdate :: String -&gt; IO String putUpdate s = do putStr "Your answer so far is: " putStrLn s return s updateMatch :: String -&gt; String -&gt; [Char] -&gt; String updateMatch [] [] c = [] updateMatch (x:xs) (y:ys) [c] | x == y = x : updateMatch xs ys [c] | x == c = x : updateMatch xs ys [c] | otherwise = '-' : updateMatch xs ys [c] ```
&gt; Is there a reason for the order of ReaderT’s type arguments (from the transformers package)? So that `ReaderT r` is a monad transformer, and an instance of `MonadTrans`. It's also a monad morphism (see the [mmorph](https://hackage.haskell.org/package/mmorph-1.1.3/docs/Control-Monad-Morph.html) library) &gt; Seemingly, if they were rearranged from r m a to m r a it would admit an arrow and pro-functor instances as well as the existing monad ones There is `Kleisli` in base for that. transformers also predates profunctors. I'm not sure about arrow, but the origins of `ReaderT` as a monad transformer make the connection to either profunctors or arrows far from obvious.
For the concrete `MyInst` version, maybe type MyInst' a = HasCallStack =&gt; MyInst a &gt; but then I need to remember to use that (and import it), so it's not ideal. Is that different from having to import `MyMonad`? Other very different alternatives: - Plugins allow you to define your own code transformations - A callstack can also be built by a program compiled for profiling (I don't remember offhand whether it's the same stack as `HasCallStack`). Compile with `-prof -fprof-auto` and run with `+RTS -xc` to print callstacks when exceptions are thrown. More details are in the GHC manual: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime_control.html#rts-flag--xc
You probably need a PackageDescription: [https://hackage.haskell.org/package/Cabal-2.4.1.0/docs/Distribution-Types-PackageDescription.html#t:PackageDescription](https://hackage.haskell.org/package/Cabal-2.4.1.0/docs/Distribution-Types-PackageDescription.html#t:PackageDescription)
Can be obtained with \`readGenericPackageDescription\`
 import Data.Version ( showVersion ) import Paths_giant_squid ( version ) `Paths_giant_squid` is specific to the name of my package "giant-squid"; you should have a similarly named import available. Just run `showVersion version` to get a `String` version number specified in your .cabal file. &amp;#x200B; At least, that works for me in my stack project!
Thank you, I thought there would be a reason. I'm using a newtype now to get all the instances I'm after - Kleisli lacks a Monad m =&gt; Monad (Kleisli m a) instance (I assume because there are other possible instances than a ReaderT style one). &amp;#x200B; The relationship between the ReaderT transformer and arrows/profunctors is interesting, turns out that withReaderT is just lmap and ask is the identity arrow etc. I've yet to work out what mapReaderT is.
&gt; is there no way to stuff the concept of "matchability" into GHC Indeed there is a way; that's what this proposal is (partly) about! The error messages should not be any worse than they currently are, and might even improve.
Thank you very much. I learned so much today. This language is literally plowing new neural connections inside my brain.
The [paper](https://www.microsoft.com/en-us/research/uploads/prod/2019/03/ho-haskell-5c8bb4918a4de.pdf) is worth a read, and appart from section 4, very approachable for the working programmer.
Was this with ADVANCED optimizations?
I've been trying to follow the tutorial, but I still find it confusing. Fundamentally, dealing with types that takes a function is kind of unusual and I am not sure what I can or cannot do with it.
You're welcome, btw. if you're still in need for more brain-teasers: Consider the following piece of code (feel free to just ignore it): f = concat . zipWith ($) fs where fs = mempty : pure : fs
There is only one law you'd want relating fmap and pure: `fmap f . pure = pure . f`. This is a free theorem, meaning it's fulfilled no matter how you implement pure.
Sorry, what I mean is matchability seems like a wonky concept that should live inside ghc and be a concern for GHC devs not their users. This smells like complexity leaking from the compiler into the language. I think I'd rather see a more limited proposal that was elegant (if one exists). Or a horrible hack in GHC that results in a simpler language (type families behave more like types) rather than a more complex one
&gt; Kleisli lacks a Monad m =&gt; Monad (Kleisli m a) instance There is a proposal to add it: https://mail.haskell.org/pipermail/libraries/2019-April/029478.html Another isomorphic type is [`Star`](https://hackage.haskell.org/package/profunctors-5.4/docs/Data-Profunctor.html#t:Star) in profunctors, which has the `Monad` instance but not `Arrow`. The difference here is also because these types were developed in a different context with different concepts in mind.
Distinct, not unique.
I wrote [this](https://vaibhavsagar.com/blog/2018/02/04/revisiting-monadic-parsing-haskell/) a while ago based on [Monadic Parsing in Haskell](http://www.cs.nott.ac.uk/~pszgmh/pearl.pdf), which is pretty approachable.
Haskell will do that to you! It's definitely a new way to think about problems. It's sort of more like designing a regexp than programming sometimes - you think about how data flows and gets transformed rather than writing a step by step list. Here are two more approaches to solving the problem: https://gist.github.com/KerfuffleV2/f7e58da8ec16b4eabecfebde87031991 Since your code doesn't touch the data, you can make it work for any type of list rather than just `Int` - which I did in my versions. If you want a little exercise, try modifying one of the versions to allow passing in the constraint so you could make it work with even indexes instead and so on. Hint: You'll want your function to take a function as an argument of type `Int -&gt; Bool`
Pretty sure the intended meaning was that optimizing these functions further would have effectively 0 impact on total compiler performance.
&gt; It's not enough to learn basic Haskell and get it installed Sure it is. Everything else you talk about can come in later, or from outside contributors. You don't even need to be comfortable using Monads other than IO; just repeatedly pattern match on `Either String a` values, and you'll be doing Go-style error handling!
Nice one, definitely helpful! A small suggestion for `f2` (or maybe just another `f3` alternative): f3 (_:x:xs) = x : f3 xs f3 _ = [] But it's not very general, so if they want to generalise it for any index-filtering function `Int -&gt; Bool` this won't do anymore.
Looks like you may be interested in [safe-money](https://ren.zone/articles/safe-money)
I think that makes an excellent `f3` since like you said, it does have different limitations.
&gt; and you'll be doing Go-style error handling! LOL, exactly why I'm **not** using Go until 2.0 comes out with revamped error handling.
I agree that this looks easier to understand, and doesn’t make me ask questions like yours (why are we generating two uniq supplies at each step just to chain a call to gensym in them? Why not just call gensym?
Yes there is already an issue on github, I will fix it today or tomorrow.
Cool! I am looking forward to examining this closely.
Why does it make sense to do that?
You make great points which for the most part I also did think about when looking at the code. I don’t think it’s worth to try to optimize **the current approach** further (and did say so in the post). But that wasn't meant as the general idea can't have a faster implementation. Handwriting the whole thing by hand in Cmm, custom primops or other trickery is always an option. But it does not seem to be worth it. I did look at the generated Cmm code and it was fairly decent so I think not much would be gained there. I think having a synchronized global counter available in the Haskell rts would be useful even outside the compiler and could be used here. So I do invite anyone to implement this! I will happily review any such merge request. It's probably a nice project for someone. But if the goal is improving compiler performance it does seem like a bad allocation of resources. &gt; Couldn't we just do ... At the very least your types don't match up, which in this case is not a minor detail as it has subtile implications on the behavior. There are two ways to fix your code: Approach one: Each allocated US calls genSym just once. * You generate an us which will give the unique X when demanded. * You split the us by duplicating the *reference* to this us. * You pass both into different functions, expecting after split to get two different uniques. * Both take an unique from their supply. * Both get X * Chaos ensues. So you decide to fix this by having each call of `takeUniqFromSupply` on the same UniqSupply generate a different unique. * You generate an us. * You pass the same us to two different code paths, expecting them to generate the same unique anyway. * `takeUniqFromSupply` returns different uniques along the code paths. * Chaos ensues. One way or the other your approach violates referential transparency, which is a REALLY nice thing to have.
IIRC his 2015 OPLSS track was way more focused on more standard Martin-Lof Type Theory. Things have come a long way in a short time!
Nice, thanks!
Cool! Awesome to see polysemy is getting some traction. Thanks for the post! OOC, would you be interested in a code review on what I consider to be the "best practices" for polysemy?
There's something screwy going on with your home and archive pages; homepage has headers/footers and archive page seems to be duplicating posts
&gt; Cool! Awesome to see polysemy is getting some traction. Thanks for the post! Thanks for making it! &gt; OOC, would you be interested in a code review on what I consider to be the "best practices" for polysemy? Absolutely, I'd love that! It was only done in a couple hours and some of the choices I made were to use Polysemy features to increase my understanding rather than writing the most idiomatic or performant program so I'm sure a lot of improvement is possible. By the way, if any part of this is useful for your documentation feel free to use it without restriction.
I thought, "if `Functor` is a container you can map over, then shouldn't `pure` be able to create `Functors`". Obviously this is pretty loose thinking. I figured there was a good reason, so I asked and got lots of good answers here.
Thanks I will check that later!
Cool! One quick note: There is a polysemy Random effect in the polysemy-zoo package. So you could use that as well if you wanted to.
https://github.com/KerfuffleV2/haskell-polysemy-test/pull/1
That's a good point; when you expect people to write ASTs themselves, intrinsic typing gives some nice guidance (assuming that the types aren't too complex). I somehow didn't consider this angle, despite commenting under an article about eDSLs.
It's definitely possible, it only seems a bit of a chore for something (`Read`) that's barely used. My 2 cents as the author of [generic-data](https://github.com/Lysxia/generic-data) (a library for actually deriving the standard classes with GHC.Generics (generic-deriving defines its own classes, mostly as an example)) and someone who's never needed `Read`. I'm not really opposed to adding a generic `Read` though: https://github.com/Lysxia/generic-data/issues/1
I use stack install --copy-compiler-tool profiteur
See my [other comment](https://www.reddit.com/r/haskell/comments/c1zqbo/taking_a_look_at_how_ghc_creates_unique_ids/erhkgdz?utm_source=share&amp;utm_medium=web2x) as for why we use two supplies. It's (sadly) required for referential transparency.
Thank you for taking the time. That's definitely helpful for me and I'm sure others as well! Hopefully it's not hubris for me to say that I don't think I'd consider it a direct replacement for the existing example though. It seems like it teaches different things. At least for me, when I'm first starting out I find a concrete example is easier to understand than one which is more general and abstract like your version. One other thing I noticed is how you changed the pure function to take an infinite list of input. Right now it's statistically likely terminate because it won't answer "yes" to the play again question. However, if the input list is `[""]` or `["y"]` or probably even `[]` it will run forever and consume all memory without producing output or reaching the IO part.
His biggest issue, as far as I understand it, is that there is not a formal nature of Haskell. SML has a report, which specifies the language _formally_. Haskell has informal language "reports", which don't look anything like what a mathematician would want (there is also the fact that bottom can hide everywhere, which he doesn't like, so you don't ever have actual values...).
note that you'll probably need to add the `Paths_giant_squid` module to your `other-modules` in the `.cabal` file if you use a `.cabal` file. `package.yaml` already does this for you.
&gt; Almost every article or guide is going to expect lens Most stuff I read on Haskell doesn't use much lens ([the one exception](http://www.serpentine.com/wreq/tutorial.html) I can recall explained all the lens I needed in the first paragraphs). Maybe the advice should be to avoid any article/guide that expects you already know lens – it's a timesink and you don't need it to get your work done. (Unless you actually want to, then you should.)
I meant "any article or guide (on lenses)". I'll update my post to clarify.
[removed]
I wouldn't use list comprehensions at all when starting to learn haskell. Sure they are really handy but its not going to teach you much. Write your own recursive functions or build something from scratch using a fold. Once you've solved it this way then if you wanted to try the list comprehension
Well the video in question is him discussing the theory and contrasting his "computational" approach with the formalist (in the theoretical, not political sense) approach. He may well have issues with the politics around Haskell's standardization too, but the video is about the theoretical concerns.
I think my first attempt would be to write it using standard library functions. filterEven = map snd . filter fst . zip $ cycle [false, true] main = mapM_ (putStrLn . show) $ filterEven inputList I don't think this `filterEven` is very clean though, and besides you said you want to write code more direct code to get a better understanding of the core language. So, let's rewrite that with raw recursion. Actually, I don't want to have to number/tag the list nodes to remember if I'm currently at an odd or even node, so let's write a corecursive _pair_ of functions. filterEven [] = [] filterEven h::t = h :: filterNext t where filterNext [] = [] filterNext _::t = filterEven t Of course the `where` clause isn't necessary, but I'd rather not expose `filterNext` since it is not useful outside its usage in `filterEven`. I imagine this is the simplest solution, and it doesn't use any library functions whatsoever. I realize my answer was much more "show", than "tell". Sorry about that. I think if you look over this second code snippet those, it should start to make sense, so long as you are comfortable with (co)recursion.
I think my first attempt would be to write it using standard library functions. rmOdd = map snd . filter fst . zip $ cycle [false, true] main = mapM_ (putStrLn . show) $ rmOdd inputList I don't think this `rmOdd` is very clean though, and besides you said you want to write more direct code to get a better understanding of the core language. So, let's rewrite that with raw recursion. Actually, I don't want to have to number/tag the list nodes to remember if I'm currently at an odd or even node, so let's write a corecursive _pair_ of functions. rmOdd [] = [] rmOdd _::t = rmEven t rmEven [] = [] rmEven h::t = h :: rmOdd t I imagine this is the simplest solution, and it doesn't use any library functions whatsoever. I realize my answer was much more "show" than "tell". Sorry about that. I think if you look over this second code snippet though, it should start to make sense, so long as you are comfortable with (co)recursion.
Thanks for the suggestion! I think `safe-money` could be a great complement when used together with the exchange rates API. For the API itself I don't see how it could help.
&gt; readGenericPackageDescription Exactly what I was looking for. Thank you!
Given that this isn't going to change anytime soon, if ever, or when Haskell 202x arrives, you obviously have a couple of options, stop or keep going and learn the language, its techniques, its extensions, etc. It's a pretty huge playground for type theory, constructive mathematics, and cool programs.
 genR :: Pixel -&gt; Gen s -&gt; ST s Ray genR px gen = do r &lt;- rndP gen r' &lt;- rndP gen return $ toRay r r' px `toRay :: Float -&gt; Float -&gt; Pixel -&gt; Ray` &amp;#x200B; How would you write this?
Another way to skin the cat: `f xs = map snd . filter (odd . fst) . zip [0..] $ xs` Or even: `f = map snd . filter (odd . fst) . zip [0..]` This also avoids running over the list more times than you need, because Haskell is lazy by default. Rather than allocating three lists (one for zip, one for filter, and one for map), each function 'pulls' from the list to it's right as it's being allocated, so the intermediary lists are consumed before they are even constructed, with the results of their consumption being used to construct the output list. However, you do end up allocating a bunch of intermediary values for each step... Unless [list fusion](https://stackoverflow.com/questions/38905369/what-is-fusion-in-haskell) saves you, which it does for many simple cases, such as this one. Understanding all about list fusion isn't super important, so don't worry about committing all those details to memory just yet, but understanding about laziness and how it relates to data structures is pretty important once you start dealing with bigger programs. It's ok if the latter part doesn't quite make sense yet either, but it's a good concept to mull over as you get more experience playing around.
```haskell data Ctx m = Ctx { getLogger :: Logger m , getCache :: Cache m , getForexClient :: ForexClient m } class HasLogger ctx where loggerL :: Monad m =&gt; Lens' ctx (Logger m) instance Monad m =&gt; HasLogger (Ctx m) where loggerL = lens getLogger (\x y -&gt; x { getLogger = y }) ``` How can I write such instance? I get this error: ``` Couldn't match type ‘m1’ with ‘m’ ‘m1’ is a rigid type variable bound by the type signature for: loggerL :: forall (m1 :: * -&gt; *). Monad m1 =&gt; Lens' (Ctx m) (Logger m1) at src/Context.hs:27:3-9 ``` It is my guess that is related to higher rank polymorphism (`forall m`)? What I found so far only talks about it in plain functions but not in records / classes / instances.
Sooo many of the CLI utilities I have written throughout the years were mere String -&gt; String transformations (whether from stdin to stdout or from one file to another), that my typical CLI program (whether made in perl, python and even C) has been "load the input in memory, then call a pure function to do the actual work, then go back to IO to emit the output" even before I knew what the pure/impure dichotomy was.
The problem is the compiler has no way of knowing that the `m` in `Ctx m` is the same as the `m` in `Logger m` so the type of `loggerL` ends up being `Lens' (Ctx m) (Logger m1)`. You can't resolve this issue by just changing the instance. However, you can solve this by changing the type class. For example for could have `ctx` take a parameter. class HasLogger ctx where loggerL :: Monad m =&gt; Lens' (ctx m) (Logger m) instance HasLogger Ctx where loggerL = lens getLogger (\x y -&gt; x { getLogger = y }) Notice that `m` appears in both `ctx m` and `Logger m`. This tells the compiler that the `m`'s must be the same type. A potential disadvantage of this method is that I had to move the Monad constraint into the type class. Another way would be to use the `MultiParamTypeClasses` and `FunctionalDependencies` language extensions. class HasLogger ctx m | ctx -&gt; m where loggerL :: Lens' ctx (Logger m) instance Monad m =&gt; HasLogger (Ctx m) m where loggerL = lens getLogger (\x y -&gt; x { getLogger = y }) This is more flexible, but has the potential disadvantage that you need to depend on language extensions.
Are there any frameworks for asterius?
To chose a package, I often check its popularity using [reverse dependencies](https://packdeps.haskellers.com/reverse)
WebAssembly doesn't have a GC and the last time Asterius was posted here, it didn't free any memory so I'm afraid you're out of luck there unless things have changed since. Consider using Purescript or ReasonML/OCaml or Elm if GHCJS is not an option.
In one of the other threads their dev said it's implemented, along with something to do with recursion.
Pretty much in the same way. Alternatively, once you recognize that the two calls to `rndP` are independent of each other, you can use the Applicative instance of ST : `genR px gen = toRay &lt;$&gt; rndP gen &lt;*&gt; rndP gen &lt;*&gt; pure px`
What exactly do you mean with generating javascript? Do you have an example for this ? Or you "just" want to splice in some js in your page header?
Sure. I would imitate `Map`s with lists of pairs sorted by the key. After you have an (inefficient) `Map`, you can construct graphs, and then doing something like a depth-first search should be relatively easy.
Thanks! Yes, at the moment, knowledge about HTML and CSS is still unavoidable for using Threepenny. However, the Websocket parts are abstracted away, and presented as a JavaScript foreign function interface.
It would be better (and more interesting) to define an own datatype for `Map`. Probably you misread `Integer` as `Integral`, but the OP's question needs to be answered with *no* since the signature `Integer a =&gt; [(a,a)] -&gt; [(a,a)]` is never valid ;P
Is Functional Core, Imperative Shell an inevitability if we ruthlessly pursue extracting pure functions? Is there any more to it than this? I liked the article. I’m just left thinking I’m missing something important because it seems so... obvious.
Well. newtype Map k a = Map [(k,a)] is an own datatype for Map. It's just not very efficient :)
What things? Computational type theory is not exactly new, and I'm pretty sure neither is Bob's preference for it over swedish-style type theory.
Asterius is not yet ready, but you are encouraged to try to build your code and help out with missing features. It will support the existing frameworks in the future.
You managed to upload a short snippet of Richard Eisenberg instead of Ryan Trinkle's keynote. Not that I mind seeing what Richard is up to!
lol, it's using all kinds of random languages.
&gt;dealing with types that takes a function is kind of unusual Yeah I get that. Here is a minimal, self-contained example of doing a single parse step. Put it in a single source file and it should just run (no dependencies). newtype Err = Err String deriving Show data Duck = Duck deriving Show newtype Parser a = Parser { runParser :: String -&gt; Either Err (String, a) } parseDuck = Parser $ \s -&gt; if length s &lt; 4 then Left (Err "Ran out of chars") else do let some = take 4 s let rest = drop 4 s if some == "duck" then Right (rest, Duck) else Left (Err "Not a duck") main = do case runParser parseDuck "duck goat" of Left (Err err) -&gt; error err Right (s, Duck) -&gt; do putStrLn "Found a duck" putStrLn $ "Remainder of input was: '" ++ s ++ "'" Note: This is just so you can play with an individual parser at a fairly low level, so you can see why it's a function and what purpose the String has. It's not yet 'parser combinators' because I'm not combining two parsers together.
Your formatting has failed, badly.
truly amazing
Are you on old Reddit? Seems to be incompatibility issues - I'll fix later at home
I don't think so. For example, when dealing with something like a database, it can be the case that the "core logic" involves a lot of impure functions. Of course this depends on where you draw the line between purity and impurity, but it can be the case that your functional logic has a very imperative central component, in which case your whole thing will end up being imperative by nature, unless you choose a different pattern.
This looks awesome! How familiar were you with Yampa (or FRP in general) when you applied it to a game? Had you already tried an ECS and if so how would you compare the two approaches? Lastly, a marketing observation - using a Serif font (and generally really different presentation style) in your video kept yanking me out of your game promo, I think you should use more of your game's style in it :)
That would appear to be the case. Now they have yet another way to try to drag me kicking and screaming into the new, terrible design.
I just bought the game. Is the source anywhere? I want to build a statically linked version of this. Also, it seems you compiled the haskell executable for linux without stripping, at least for linux. running \`strip Peoplemon\` took of 7 MB.
While the above is (hopefully) useful, I don't think I really addressed *why* Parser has a function type, i.e.: Parser { runParser :: input -&gt; output} Crap analogy: You can connect garden hoses together because there is one side for the water to go in, and other side for the water to go out. If a garden hose only had an output but no input, it would be difficult to combine them together. &amp;#x200B; A parser takes *its string input* and returns the domain object you were looking for, as well as the *rest of the string*. Just focus on the strings and forget the domain object for now. &amp;#x200B; If you view DuckParser as a function: "duckgoat" -&gt; "goat" and GoatParser as a function from: "goat" -&gt; "" then you could combine these two parsers together into DuckGoatParser as a function from "duckgoat" -&gt; "" by feeding the remainder string of DuckParser as the input string into GoatParser.
Don't worry, it's horribly broken on "new mobile" too.
 {-# LANGUAGE NoImplicitPrelude #-} import qualified Prelude as P class Integer a instance Integer P.Integer f :: Integer a =&gt; [(a,a)] -&gt; [(a,a)] f = P.id main = P.print P.$ f [(1, 2 :: P.Integer)]
That's where effect systems come in. You can write your core logic purely, then provide an impure interpreter to actually get the work done.
Thank you. I read the original Yampa papers—[The Yampa Arcade](http://haskell.cs.yale.edu/wp-content/uploads/2011/01/yampa-arcade.pdf) by Courtney et al., and [Functional Reactive Programming, Continued](http://haskell.cs.yale.edu/wp-content/uploads/2011/02/workshop-02.pdf) by Nilsson et al. Then I experimented with Yampa for about a year before starting a bigger game project. At first I tried to write Peoplemon using a more traditional approach in C++. It resembled an ECS structure, which I have since worked with elsewhere. The current version is much better, but I don't think the FRP and ECS concepts need to compete. You could probably organize a game in Yampa using the ECS structure.
Thank you so much! The source code is here: [https://hub.darcs.net/linearity/pplmonad](https://hub.darcs.net/linearity/pplmonad) The code up to date, but the README file is a little out of date.
You’re comparing yourself to other people, and commenting about it on reddit.
I guess it’s more about partial application being super useful, and bringing this to the type level.
But (my claim is that) there exist problems where there's no sensible abstraction of the core logic, and all of the meaningful manipulation happens in order, with non-functional control flow.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/haskellgamedev] [Peoplemon: an all-Haskell role-playing game](https://www.reddit.com/r/haskellgamedev/comments/c2i6m6/peoplemon_an_allhaskell_roleplaying_game/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I agree that there is a whole class of programs that exist as `IO ()`, not any pure function. For example a simple CLI audio player (I'm imagining aplay from ALSA). There are ways to find a functional core (e.g. `ByteString -&gt; [Either String ByteString]` maybe?), but they are fundamentally about doing the "impure" IO -- even if their "pure" functional core happens to be interesting (e.g., resampling, reframing, scaling, endian-conversion, alaw/ulaw transform, etc.) they'd still be a useful program if the pure core was the first solution that Djinn proposed / typechecked; `([]:) . Right` or similar. They would not be a useful program if they didn't do discovery, respond to timing (and other) issues, and deal with error conditions reported by the hardware, firmware, or driver. --- None of this means that pure functional core, impure imperative shell can't still be an architecture guideline for many programs, and might still be a useful mindset for all programs.
For inspiration, the following libraries provide ways to derive `Read` (or things close to it): * `deriving-compat` provides a Template Haskell function for deriving `Read` (as well as `Read1` and `Read2`) [here](http://hackage.haskell.org/package/deriving-compat-0.5.6/docs/Text-Read-Deriving.html#v:deriveRead). * `transformers-compat` provides a `Generic1`-based default implementation of `Read1` [here](http://hackage.haskell.org/package/transformers-compat-0.6.5/docs/Data-Functor-Classes-Generic.html#v:liftReadsPrecDefault).
It was 18MB pre-strip, 11MB post. Thanks for the source! Going to enjoy looking over this/getting a statically linked version.
* Link to previous discussion in [r/ProgrammingLanguages](https://www.reddit.com/r/ProgrammingLanguages/comments/c1899u/steady_typing/) (reddit doesn't normalise youtube links so it misses cross-posts) * `defer-type-errors` [doesn't quite change](https://www.reddit.com/r/ProgrammingLanguages/comments/c1899u/steady_typing/erbii7n/) the claim about inability to run without errors in Haskell * Haskell's UX for "propagating API changes" is in fact better than Python's, but the red-yellow-green resolution for cells in the table didn't allow to capture this difference
Thanks a lot for those links, the arcade paper looks pretty accessible.
Mutual recursion (indexes start at 0, an even number) everyEven :: [a] -&gt; [a] everyEven [] = [] everyEven (h:t) = h : everyOdd t everyOdd :: [a] -&gt; [a] everyOdd [] = [] everyOdd (h:t) = everyEven t --- Look, ma, no explicit recursion. evenOdd :: [a] -&gt; ([a], [a]) evenOdd = foldr eoAlg ([], []) where eoAlg h (et, ot) = (h:ot, et) everyEven = fst evenOdd everyOdd = snd evenOdd --- `foldr` is trivial to write yourself, if needed: foldr alg seed = foldr' where foldr' [] = seed foldr' (h:t) = h `alg` foldr' t
Out of interest did you test drive any other FRP frameworks (Haskell or other) in your journey?
It's not worth it. I tried new reddit last month because of this issue, and really tried to like it. I switched back. I've gotten spoiled by RES, I guess, but I *really* want the new-comments-since-last-time-your-visited-count display. Without that discussion subreddits like this one are next to unusable.
Reformatting for old reddit. -- Expression data type (extended to include closure, a pair of Lam String Expr to Env): data Expr = Var String | Lam String Expr | App Expr Expr | Clo Expr Env | V Expr deriving (Eq,Show,Read) -- Mapping type (pair of a string variable and closure) [type Mapping = (Var String, (Lam String Expr, Env))]: type Mapping = (Expr, Expr) -- Environment data type (which is a mapping list): type Env = [Mapping] -- Frame data type, which can be a closure and a hole, or a hole, expression and environment -- [data Frame = (Lam String Expr, Env) Hole | Hole Expr Env]: data Frame = F1 Expr | F2 Expr Env deriving Show -- Continuation data type (which is a frame list): type Kont = [Frame] -- Configuration type (triple of an expression, environment and continuation): data Config = Conf Expr Env Kont deriving Show -- Lookup function that takes a string variable expression and environment, and returns the closure mapped to that expression: lookupAux :: Expr -&gt; Env -&gt; Expr lookupAux (Var string) [] = Clo (V (Var string)) [] lookupAux (Var string) (mapping:env) | fst mapping == (Var string) = snd mapping | otherwise = lookupAux (Var string) env -- Function that executing one step of the CEK machine: eval1cbv :: Config -&gt; Config eval1cbv (Conf (Var string) env1 k) = Conf lamExpr env2 k where Clo lamExpr env2 = lookupAux (Var string) env1 eval1cbv (Conf (App exp1 exp2) env k) = Conf exp1 env ((F2 exp2 env):k) eval1cbv (Conf (Lam string exp) env k) = Conf (Clo (Lam string exp) env) env k eval1cbv (Conf (Clo lamExpr cloEnv) env ((F2 exp env2):k)) = Conf exp env2 ((F1 (Clo lamExpr cloEnv)):k) eval1cbv (Conf (Clo lamExpr cloEnv) env1 ((F1 (Clo (Lam string exp2) env2)):k)) = Conf exp2 ((Var string, (Clo lamExpr cloEnv)):env2) k eval1cbv (Conf (Clo lamExpr cloEnv) env1 []) = Conf (V lamExpr) [] [] -- Function that takes a lambda expression and evaluates it using a Call-by-Value strategy until termination, returning the terminated -- expression (if it terminates): eval :: Expr -&gt; Expr eval exp = recursiveEval (Conf exp [] []) recursiveEval :: Config -&gt; Expr recursiveEval (Conf (V exp) env k) = exp recursiveEval (Conf expr env k) = recursiveEval (eval1cbv (Conf expr env k)) testExpr :: Expr testExpr = App (Lam "v" (App (App (Var "v") (Lam "z" (Var "z"))) (App (Lam "v" (App (App (Var "v") (Lam "x" (Lam "y" (Var "x")))) (Lam "z" (Var "z")))) (Lam "x" (Lam "y" (Var "x")))))) (Lam "x" (Lam "y" (Var "y"))) We should write a /r/haskell bot that does this automatically. It "just" needs to find triple-backtick blocks, and indent them 4 spaces to cover 90% of the formatting changes we need.
triple-backtick formatting doesn't work on old reddit.
I read about several, but I haven't used them. Yampa seemed to reflect some of the ideas I had earlier about how the game should work. The others never seemed as closely related, so I just kept working with Yampa.
From poking around a few random files, this looks really really well organized! I look forward to reading through the source more.
These were not meant to go public and are not the final versions. Feel free to watch them, but better versions with slides etc. will be uploaded &amp; announced later.
Awesome work! And a really nice promo video as well. I have to say that the concept of catching certain creatures and making them fight has bothered me for a long time. This fixes that! How long have you been working on this if I may ask?
So I've learned; fixing now
Fixed; if you dislike it, don't succumb to the new design over little issues like this lol (I might actually write a quick Python script using PRAW that converts all one's comments utilizing backtick formatting to old-style 4-space indentation formatting).
Thanks for posting this (I've edited my post+comment too). Never written a Reddit bot but can try sometime, that seems a good idea. I've done Python PRAW script so can at least write a little script people can run to convert all their posts/comments containing triple-backtick blocks into 4-space indentation blocks.
Thank you. I came up with the idea in 2011, so in truth I worked on it for 8 years off and on. It took me a year and a half to make this final version in Haskell. I reused only a few artifacts from previous versions: a few music compositions and a few sprites.
I wanted to add javascript to the pages I return from the server. Thanks you for helping ! If you have a good source I can read from I would really appreciate it.(about the issue, or scotty framework)
Does anyone have a reference for the assumed idea of "abstract syntax with binding and scope". I think he also mentioned Higher-Order Abstract Syntax in the same breath -- would Pfenning &amp; Elliott be an OK ref?
Thanks, I will add a link to summoner shortly.
Thanks for the suggestion, I will add a recommendation for this.
I implemented the feature to play the replay automatically after game over (a bit faster). Starting the executable from anywhere is now also possible (used file-embed, as you suggested). Thank you!
I have a noob question about lambda calculus and order-of-application: I'm basing my notation as well as question on this well-known pdf: https://www.inf.fu-berlin.de/lehre/WS03/alpi/lambda.pdf I'm having trouble understanding in what order i should perform substitutions, and whether that even matters. λz.( λwyx.y(wyx) ((λwyx.y(wyx)) z) ) 3 (the 3 is a church numeral) What happens first and why? Replacing the z with the 3? Or applying the term ((λwyx.y(wyx)) z) to the function λwyx.y(wyx)? The pdf does not seem to explain this. The reason I'm asking: I want to build a function that takes a number and adds 2 to it. I have a function that gives the successor of a church numeral (namely λwyx.y(wyx) ) but how do I represent function composition in lambda calculus?
Afaict, "abstract syntax with binding and scope" is just a complicated way to say "binders and substitution". So he assumes that you have an intuition for how beta-reduction works in the untyped lambda calculus, nothing more. I'm also not sure why he mentions HOAS (from watching the first video).
This looks cool! I'd love to see it tie into the broader IDE ecosystem by having, say, a driver that an emacs environment could utilize (for example).
Are there specific externalities to this pair of extensions? My understanding was that these were benign
I don't think the differences between computational and "formalist" type theory matter in the Haskell vs ML wars. Haskell2010 has complete inference afaik, so you could just as well view it as a computational type theory where the relation `G |- t : T` is deterministic. The differences only become relevant when terms can have different (in the sense of beta-eta-...-equality) types, so you need computation at the type level. (I'm not 100% sure about this; please disagree if I'm wrong.)
Thank you very much, that was a great explanation! I went with the more flexible option as I think these language extensions are recommended by many but I don't fully understand the signature of the class, would you mind expanding on it? I get the first `ctx m` but not really the `ctx -&gt; m` part.
I don't mean to sound ungrateful, but do you have versions with better audio? And thank you so much for uploading this, it's really, really nice.
Interesting post, a few combinators I had not seen before that look interesting. While I see the value in data types a la carte, I personally don't really like the kind of code that style generates. I find that it usually takes some mental gymnastics to fit every single function into some sort of combinator instead of just pattern matching for every case. Folding/Unfolding is usually OK but as soon as you have to do a transformation on only certain constructors, or remove certain constructors, you get a huge spike in complexity. I tried to use it in a compiler, where I wanted to gradually remove constructors and add/change tags (typing information). Simple ADTs were annoying because I just needed too many of them. Data type a la carte made creating and consuming (interpreting) dead-easy but certain transformations were just hell. What I found to be a great alternative was a GADT with a couple of 'selector' type parameters like: data HasSugar = Sugar | NoSugar data HasAnnotation = Annotation Type | NoAnnotation data Expr (s :: HasSugar) (ann :: HasAnnotation) where Add :: AST 'Sugar a -&gt; AST 'Sugar a -&gt; AST 'Sugar a Identifier :: Text -&gt; AST s 'NoAnnotation TaggedIdentifier :: Text -&gt; LanguageType -&gt; AST s ('Annotation LanguageType) ... Then I could just have functions like `AST 'Sugar a -&gt; AST 'NoSugar a` and a (admittedly long) list of cases instead of a mess of instances of different typeclasses and then an even more complicated instance to tie them all together. Tbh the issue might have been that I tried to use the [compdata](https://hackage.haskell.org/package/compdata) library which is just terribly documented and sort of unmaintained.
Thanks! One of the primary goals of this project is to explore the possibilities of projectional as opposed to text editing. "Projectional" means you're not directly editing the code, but a projection of the code, which is edited structurally. For example, a feature not displayed in the video, but that is already implemented, is internationalization. You can edit the same code in Hebrew, English, German. The grammar and names and code direction all swap when switching languages, but you're collaborating on the exact same code. It's hard for me to imagine how emacs, or other text-based environments, could work with this environment.
I have a homework and there's a part of the code that defines a data like this : data Value = VInt Int | VPrim (Value -&gt; Value) What is the point of having VPrim if I can't access the function that is in. &amp;#x200B; Ex : x = VPrim (\\x -&gt; VInt (x + 1)) y = x + 1.
Thank you very much for mentioning summoner as well! You called your guide opinionated but I think it contains a lot of common best practices and a huge number of useful suggestions. It's super useful, and I like almost everything about it, so I'm just glad there is a document now that I can share with Haskell beginners.
Recent thread: https://www.reddit.com/r/haskell/comments/bnoxy5/what_the_better_library_for_create_guis
GTK with gi-gtk-declarative.
This is great, like a much more fleshed out version of the [Functor-oriented programming post](http://r6.ca/blog/20171010T001746Z.html). In my own code, I found a practical application for `Day` and `Lift`: [defining an Applicative](http://hackage.haskell.org/package/process-streaming-0.9.3.0/docs/src/System.Process.Streaming.html#Streams) for handling the stdin, stdout, stderr and exit code of an external process.
As I said, I think these were accidentally made public by HSR (the university that we organize this with) and among other things, the audio still needs to be mixed &amp; edited. Expect the final versions to be released next week somewhere.
The `ctx -&gt; m` part is what I needed [functional dependencies](https://wiki.haskell.org/Functional_dependencies) for. It says that the type of `ctx` uniquely determines the type of `m`. It's not absolutely necessary, but it better models the intentions of the type class by constraining the parameters and helps with type inference. It means we can't create another instance that has the same `ctx` but different `m`. -- not allowed newtype Foo a = Foo a instance Monad m1 =&gt; HasLogger (Ctx m1) Foo where loggerL = undefined This helps with type inference because if the compiler knows the type of `ctx` it can immediately infer the type of `m` without any additional information.
I think the distinction between Harper's CTT world and Haskell is still substantial: to use one of his examples, he has something like `if b then "hello" else 5`, which in the context where `b : Bool` is a program with well-defined behavior, and thus a sensible object to talk about in the CTT world - and indeed, he makes statements like when b is true, `if b then "hello" else 3` has type `String`. That kind of reasoning really is quite different from what you have in the continental formalisms of type theory, where a term is introduced together with its type and cannot be discussed independently of that type. You cannot say `(\b -&gt; if b then "hello" else 3) True` in Haskell, despite the fact that that really should be a syntactically valid program with well-defined computational semantics. One advantage of the CTT approach is that you don't have to know what type a program has in order to talk about it or use it. You can just write the y-combinator and not bother trying to explain what type it has, then when you use it, you can say "ok, I can prove that when I apply the y-combinator to this term, the result has type blah." To my intuition, though admittedly I'm no expert, this really does sound like a superior approach to studying computation from a mathematical perspective. That said, also to my intuition, a lot of what I like about languages like Haskell is the formal arbitration power I have: what is the actual basis between distinguishing between `data B = T | F` and `data C = Y | N`? Ultimately, they both *are* represented equivalently computationally, so distinguishing between the two is an arbitrary property of my formalism, not a property of the computation itself. And yet arbitrarily distinguishing between those two for "no good reason" is exactly what I want when I make real-world software!
Crystal clear, thank you very much!
Absolutely!
Oh yeah they are benign, I would have considered any extensions to be a potential disadvantage. My main concern is that extensions create a dependence on the features of a particular compiler. So for example, it would prevent you from using Frege to compile your code to JVM, because last time I checked Frege doesn't support multi-parameter type classes. I don't have a problem using extensions, but I think it's worth considering if you actually need to use them, which is why I made sure to include a solution without using extensions.
Just want to say thanks a bunch for releasing the code, it's an interesting game and sure to be helpful to study the source!
Awesome! I'd love to know how you built this for distribution
If you are looking for another example of a projection editor with ideas you can still and aren't aware of it you should check out [Leo](https://leoeditor.com/), which structures an entire program as an outline. The idea is simple: everything is a node in the outline and nodes can have arbitrary child nodes so, which includes clones (all peers, rather than symbolic links) so you can organize your file nodes however you want, and when a file node is written all of its children are combined and written to a flat file with sentinels so that the outline can be reconstructed; furthermore, you can put named sections which cause the correspondingly labeled subnode to be substituted in that spot so that you can write code that looks like: &lt;&lt;First do A&gt;&gt; &lt;&lt; Now do B&gt;&gt; and break out the code for each into a separate node. The clones mechanism is in particular pretty cool because it means that you can have, say, a WORKING ON node where you put clones of all the parts of the files you are working on so you can keep them in one compact space. I used Leo for a while but at some point it just felt too lacking as a text editor so I moved away from it, but I would love to see the ideas in it spread more widely.
This is largely unchanged from when it was described in ["On generating unique names"](https://www.cambridge.org/core/journals/journal-of-functional-programming/article/functional-pearl-on-generating-unique-names/763DE73EB4761FDF681A613BE0E98443) by /u/augustss.
Thank you. I used [Stack](https://docs.haskellstack.org/en/stable/README/), both for building incremental changes during development and for building the final distribution files. I built the Mac version natively on my Mac development system. I installed Stack on virtual Windows and Linux systems on AWS, and I built versions for those systems natively as well. Linux was pretty easy. Windows was a little painful, but not bad in the scheme of things. On Mac OS I used [cabal-osx](https://hackage.haskell.org/package/cabal-macosx) to generate an app bundle, and I assembled the files manually for the other versions.
Every function in Haskell is what we call "curried." That means a function can only ever have one argument. We can *imitate* multi-argument functions by making our functions return other functions, since Haskell is a higher-order language. When you read type signatures like ` addThree`'s, read them as `addThree :: Int -&gt; (Int -&gt; (Int -&gt; Int))`. So on line 2, `addThree` is bound to the function which takes x, and returns (the function which takes y, and returns (the function which takes z, and returns x + y + z)). The types match; `addThree` is a function that takes one `Int` and returns a function which (eventually) expects two more. This is useful because then we can use "partial application." Some functions, like `map :: (a -&gt; b) -&gt; [a] -&gt; [b]` for example, expect a function as an argument. Because of partial application, `map (+ 1) [1, 2, 3, 4] == [2, 3, 4, 5]`. The function `(+)` is partially applied to the argument `1`, but isn't given the second argument. So initially we had `(+) :: Int -&gt; Int -&gt; Int`, and after partial application the first `Int` has been supplied, so now `(+ 1) :: Int -&gt; Int`.
Have you (or anyone) found a solution to this yet or is this definitely a problem with the new Stack version? I've just started up a new side project and am having the exact same issue, not that I strictly need intero's features for it (yet).
The type signatures in the `GI.Gtk.Declarative` module are... quite large, to say the least.
Thank you for sharing your experiences :) I think one distinction from this style vs "full" data types a la carte is that the emphasis is on the actual data types and constructors, so pattern match on the actual data structures we would be normally using. You would only rely on folds if you really wanted a fold, or use a typeclass method if you wanted the abstraction; otherwise, normal pattern matching would be fine. I do wonder if this would still get complicated in a truly large project, however.
Ah, thank you for the link! I remember reading this when it came out and I found it very enlightening. Thank you also for sharing your practical application. Short/sweet examples like this really help show places where these things fit in very well!
&gt;import Math.NumberTheory.Primes main = print (toEnum (10 \^ 6) :: Prime Word) In GHCi `arithmoi` is 0.02 and my new one 1.47. But with `ghc -O2` and `+RTS -2` mine is faster (0.062 v 0.078 totals) . arithmoi uses almost half the memory of mine. Both are for 10\^6.
I posted after the first video - the double arrow notation (vs pi types) is new to me. Unfortunately the end of the lecture series is a bit rushed so there aren't many specifics in the new formulations and implementations of CTT that have come out of CMU in the last few years.
Not as far as I'm aware. For now I'm just disabling `intero-mode` and using [`hasky-stack`](https://github.com/hasky-mode/hasky-stack) to 'manually' recompile when I make a change, but this problem has to be solved at some point.
Heh, seems like such a Scala flavored feature.
I think you have to be a moderator to edit the sidebar -- /u/taylorfausak , /u/Iceland_jack ?
This is part of the reason I don't like the functors-as-containers analogy. It breaks down too quickly, IMO.
Consider using redux. Your code will be able to compile to native code on all major platforms (web, Android, Windows, iOS, MacOS, Linux). Rendered with the DOM. You can write platform specific code as well. It doesn't prevent you from writing platform specific code if that's what you need to access some api (e.g. for notifications).
&gt; Every function in Haskell is what we call "curried." That means a function can only ever have one argument. GHCi, version 8.4.4: http://www.haskell.org/ghc/ :? for help Loaded GHCi configuration from /home/bss/.ghc/ghci.conf GHCi&gt; :t \x y -&gt; "foo" \x y -&gt; "foo" :: p1 -&gt; p2 -&gt; [Char] One (1) lambda, one (1) function, two (2) arguments. :)
No, it's still a function of one argument that returns another function of one argument. Specifically; `(\x y -&gt; "foo") () == (\y -&gt; "foo")`. In fact, during part of compilation, that code will get "effectively rewritten" (hiding ugly, gory details) into `\x -&gt; \y -&gt; "foo"`.
`addThree = ...` top level binding, binds the expression on the right hand side to the name (`addThree`) on the left hand side. `... \x -&gt; ...` a lambda ("function") expression, introducing the variable `x` that can be used in the body, which extends as far to the right as possible. When the function is applied, `x` will be bound to the argument. `... \y -&gt; ...`, `... \z -&gt; ...` similar. `x + y + z`, expression representing the sum of `x`, `y`, and `z`, all variables that has been introduced earlier on the line. [There's type class magic, and precedence/associativity (fixity) magic, too, but I think we can ignore them.] So, we get a function, that returns a function, that returns a function, that returns the sum. If you take into account the associativity of the `-&gt;` type operator [`a -&gt; b -&gt; c` ~ `a -&gt; (b -&gt; c)`], that matches (can be unified with) the type on the line above.
I'd be happy to update it if there is consensus. Maybe the whole section should be replaced with a link to &lt;https://www.haskell.org/documentation/&gt;?
My hope is that you could take a region of text as input, Lamdu it to satisfaction, then have Lamdu serialize the final state back to flat text. Does that sound plausible, or is the representation necessarily more exotic than that?
Yes and no. Yes, there's some desugaring going on. But no, not all functions have a single argument GHC / STG treats functions differently depending on their arity--even after inlining; that information never goes away completely, being implicitly encoded in the generated binary. It's done this way primarily for performance, and it barely matters for the theory, but it is incorrect to say that all functions in GHC Haskell are unary.
Based on recent threads, I'd say there seems to be a consensus that HPfFP should be under Learning material. (Though, it also feels like a lot of people miss the sidebar entirely.) How do you propose we go about showing consensus? A small gitlab repo, with no open issues for X amount of time, an iterative series of thread on the subreddit, or something else?
Did you just make Bumfights: The Game?
Best of luck! Keep up the great work.
My comment's point is primarily about semantics. I'm fairly aware of how the compiler works (I've done a little bit of hacking on the source, although nothing that deep into the optimizer). *Semantically*, every function is a function of one argument. And this is what a newcomer to the language should be concerned with. &amp;#x200B; Is it only implicitly encoded in the binary? I thought the binary contained an actual arity table.
Ha, that was a friend's first reaction years ago. I'd say it turned out more like Yuppiefights.
The serialized text would contain a lot of rich information that would make it too verbose to be useful, I think. What is the purpose of text'ing it back?
I think the rules are: * If an expression can be reduced to normal form, then the normal-order reduction strategy - leftmost, outermost substitutions first - will always end up in said normal form. * If an expression is successfully reduced to normal form, then it does not matter which evaluation strategy got it there - the answer will be the same (Church-Rosser) An admittedly not-very-convincing example: \`(λabcd.ab)(λy.y)3(λx. xx)(λx. xx)\` If you reduce from the left, you see that \`ab\` becomes \`(λy.y)3\` becomes \`3\`(\`cd\` gets discarded) But if I decided to start by reducing \`(λx. xx)(λx. xx)\` I'm stuck there forever because it loops.
The purpose would be the ability to use Lamdu editing in smaller scopes without needing the Lamdu GUI to pick up the billion lifestyle features editors and IDEs provide. To steer clear of an XY pit, I suspect a bunch of Haskellers want to use Lamdu and are trying to figure out what that could look like.
\[GHC.Generics\]( [http://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Generics.html#t:Datatype](http://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Generics.html#t:Datatype) ) \[Metadata\]( [http://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Generics.html#t:Meta](http://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Generics.html#t:Meta) ) includes the package name. Perhaps you could extract the metadata of a dummy type.
&gt; Partially implement layouting class instances: Layouts children, but falls back on ghc-exactprint for the instance head Nice! Excited to try this out on all my Haskell code and have it start formatting my instances better!
I understand: the ecosystem we all program in has reached closely to a local optimum. Abandoning it for a higher place requires first climbing down a large slope, and losing many features we've gotten used to. If we knew how to reach the destination incrementally, we'd definitely use that route. But it seems to us that we must make that leap - and temporarily wear some hair shirts, to reach a new and better place.
Interesting post! Not sure if this is a typo, but `f :+: Proxy` in the identity section of the Product combibator might actually read `f :*: Proxy`?
Not sure if I'm misunderstanding but in the first `HBifunctor` typeclass declaration, `hbimap` type shouldn't be? `hbimap :: (forall x. f x -&gt; h x) -&gt; (forall x. g x -&gt; j x) -&gt; t f g a -&gt; t h j a`?
Thank you! And, you're right, that is definitely a typo :) Thanks for the catch!
Thanks! And, yes, that is definitely a typo; i appreciate you helping me find it :)
&gt; My first attempt failed, because I couldn't get it to see the internal package used for test discovery. Perhaps you could show what your attempt actually looks like. To start, did you add `hedge-trimmer` (and not only `hedge`) to the `packages` lists of your other stack projects where you want to use it?
In the `HFunctor` class declaration, shouldn't the method be `hbimap :: (f ~&gt; g) -&gt; t f ~&gt; t g`? The code in the article seems to be missing the argument.
What is your use case?
I had the same experience. Reverting to all those type classes simply in order to share as much of the (G)ADT representing my core language with the (G)ADT representing the full, sugar enriched user language.
hehe, yeah... beware of PTSD. The Haskell feature predates Scala by about 10 years, and the language feature by several more decades.
Neat, learning something new every day. That was a great blog series leading up to this one btw~
&gt; Is it only implicitly encoded in the binary? I thought the binary contained an actual arity table. I didn't think that was necessary, but could be.
Totality checking definitely sounds like something you can bring up directly on the GHC issue tracker. Regarding the last point about quantified constraints, there is a common misconception that `forall` in Haskell means just "for all", but it actually means something stronger, which is a notion of parametricity. The rule of thumb of `forall` is that specialization should not affect runtime behavior. A `SingI b` instance, at runtime, is a boolean value, `STrue` or `SFalse` (at runtime, forget about types, and `=&gt;` becomes `-&gt;`). If we had `forall (b :: Bool). SingI b`, then it could be specialized to `SingI 'True`, which must be `STrue`, and also to `SingI 'False`, which must be `SFalse`. These are different runtime values, and `forall` doesn't allow that. A slightly different way to look at it is that having `SingI 'False` and `SingI 'True` does not imply `forall (b :: Bool). SingI b`, because `b`, being `forall` quantified, cannot be used to decide whether to pick `SingI 'False` or `SingI 'True`. In contrast, `forall a. a -&gt; a` is inhabited by `\x -&gt; x`, which can indeed be specialized at will. This is a subtle point, as evidenced by the fact that existing languages with dependent types use `forall` differently from each other in this regard. In Haskell, this restriction is what motivates the addition of a new quantifier (usually named `pi`) for Dependent Haskell (of which `forall` might be a special case). See also Richard Eisenberg's thesis, [Dependent Types in Haskell: Theory and Practice](https://repository.brynmawr.edu/cgi/viewcontent.cgi?article=1074&amp;context=compsci_pubs).
&gt; My comment's point is primarily about semantics. In that case, I think you'll find *every* language only has unary functions. There are few type systems where `curry` and `uncurry` either don't exist or don't form an isomorphism up to undefined behavior. --- Haskell isn't even unique in that it's application syntax only has one argument expression; that's something in common with the rest of the MLs.
I can't help but wonder if this is related to the wierdness where `Void` and other uninhabited types defined by GADTs (`STrue :~: SFalse`) or type families actually do have values in Haskell. But, it could just be that no one has done the work to "lift" the coverage analysis done at the term level to the type level -- it took long enough to get accurate term level analysis in the face of GADTs; I can't imagine doing it at the type level is easier.
I'm using fltkhs.
&gt; They seemed a bit too opinionated without giving me escape hatches to wire in my own functionality. If they're working for you, then great! But they weren't working for me, so that's where SitePipe came from. Yet another case of “I looked at some else’s solution. It was too complicated (mature) for me to understand in 5 minutes, so I gave up and wrote my own poor clone of it”. Seriously people, we need to move beyond reinventing the wheel over and over again.
Having alternative projects relieves pressure from original projects. There's nothing wrong with having more Haskell projects out there, exploring alternative ideas.
The alternative project was a Haskell project (Hakyll) which itself is a clone of another project Jekyll.
Lol :) so everyone like stops doing shit, right?
I have just been recently moving to a statically generated site for my blog and found Hakyll way harder to understand than it seems it needs to be. I stuck with it so I could customise my site in Haskell, but I’m not sure it’s going to be worth it, considering it’ll probably be just as hard to customise as it was to set up (which I’m already experiencing while trying to customise it to be the same as my old blog). That said, I’m not sure how much easier this looks at first glance. I’ve been using Haskell on the side for a few years now, comfortable with the basic typeclasses, monads, monad transformers, `mtl`, web programming with Scotty, `postgresql-simple`, lenses, `amazonka`, extensions, etc. and I found Hakyll hard to get my head around.
&gt; usually named `pi` The current planned keyword is `foreach`, IIRC. There is a GHC proposal to reserve all the syntax that will be needed for dependent Haskell where you could check this.
I'm not sure whether it really makes sense to compare CTT and Haskell. The use cases are very different. In CTT, you trade a bit more expressivity with the ability for the type checker to perform automatic static analysis. I agree that CTT's approach is appealing from the POV of formalization of mathematics, but IMO it's hideous for programming.
Happening in order doesn't have any impact on the ability to model it in pure functions with an effect system.
I'm interested in Haskell benchmark performance between different Haskell versions 8.0, 8.2, 8.4, 8.6 and with different optimization switches O2, O2 etc. It would be interesting to see how Haskell runtime performance has improved over the years... Are there some resources / links out there for something like this?
i installed hakyll (cabal new-install hakyll) but its weird the watch option doesnt work :( and i dont know how to fix
is this right ? " GHCJS uses a lot of memory during compilation. 16GB of memory is recommended, with 8GB being pretty close to bare minimum. "
Thanks! I'm having a slight issue that I really don't understand. I have a function: toFormat :: ImageFormat a =&gt; FilePath -&gt; a toFormat ".jpg" = JPG toFormat ".png" = PNG toFormat x = error $ "Unexpected format " ++ x But it won't compile. I'm getting ```Couldn't match expected type ‘a’ with actual type ‘JPG’```. Could you please shed some light on this?
Mature =/=&gt; complicated. There are many well-documented Haskell libraries that are mature but easy to follow, so it is clear what you can and can't do.
Ahhh... this is a very helpful insight.
Shouldn't this be called PokéPeep? Pocket Monsters → Pokémon, so Pocket People → Poképeeps.
”Hakyll specifically got me really bogged down; what was the Compiler monad? How does an Item work? How do I add a custom field?” Totally agree. I use Hakyll and probably will keep using it, but I welcome simpler approaches. I guess my problem tweaking my Hakyll blog is that I rarely need to touch the blog Haskell code. But when I *do* need to touch it, I have forgotten all about how it works and need to re-learn the abstractions like Compiler and Item. For some other non-blog content (manuals) I’ve simply gone for just writing my own Haskell code on top of Pandoc.
I like the graphic's modern art reference
I haven't used either, but it might be worth checking out [Slick](https://github.com/ChrisPenner/Slick) since it's the spiritual successor to SitePipe.
Yup. I've Never been able to reliably compile it from scratch on my laptop (only has 8GB).
I asked Richard about it and he said it's "Certainly not set in stone.", I find `foreach` grim even though I understand the motivation
You can try using nix on aarch64 and tell stack to use nix. I had success using that approach (you definitely need to be patient and use swap as you will run out of RAM). I also had success compiling Haskell from source by using provided stage2 builds from haskell.org as bootstrap compiler (worked best using arch linux). I put a version of ghc for aarch64 built for aarch64 at [1] [1] https://github.com/atopuzov/pinebook/releases/tag/ghc-8.6.5-1
Serving static content shouldn't require complexity. It's static content. That's not maturity, it's a leaky abstraction.
Ah, that looks fantastic, especially as it uses Shake.
Thanks, the `type ... = HasCallStack =&gt; ...` seems to work. I typically import `MyMonad(..)`, so the type alias there gives an extra import line, and no way to apply it to the whole codebase without touching each file that uses it. The type alias on the instance is better in that regard, since I don't usually import the instance constructors; so if I rename `MyInst` to `MyInst'` I can do `type MyInst = HasCallStack =&gt; MyInst'` and I expect that'll work. It does mean it wouldn't affect the places where I explicitly do `MyMonad m =&gt; ...`. There's probably just nothing here that does 100% exactly what I want, which isn't too awful or surprising. The others feel a bit heavyweight, but I appreciate the suggestions anyway.
Oh dear. I have a few comments. First, why are are futzing with the file type manually? That isn't needed ``` toFormat :: ImageFormat a =&gt; FilePath -&gt; a ``` This says that no matter what string I provide `toFormat` can give me a value of any type at all (`a`). It does not say that `toFormat` will give me a particular type dependent on the first parameter - dependent types like that are not supported in Haskell. The function you're trying to write, `toFormat`, isn't possible. You can't provide a value of any type at all. I see you're trying to overcome the issue that `JPG` and `PNG` are different types, luckily the library has helpers that make this issue disappear. **The solution** HIP has read and write operations that will infer the correct file type and save the file, which is a bit like `toFormatAndSave` in that it will detect the format but instead of returning (and thus needing a single type) it will write the file out in that format and return a unit, as in `IO ()`. ``` module Main where import Graphics.Image (readImageRGBA, writeImage, VU(VU)) import Graphics.Image.Processing (topToBottom) import System.Environment (getArgs) main :: IO () main = do [iname1, iname2, out] &lt;- getArgs [i1, i2] &lt;- traverse (readImageRGBA VU) [iname1,iname2] writeImage out (topToBottom i1 i2) ```
I don't believe the other peak is actually higher, yet.
The problem is that the images aren't on my drive, I'm downloading a bytestring from the internet and want to manipulate that.
That's what we're out to prove. We have a lot of work :) Do you see the value of the presented features made possible by projectional editing, but do not believe they outweigh the disadvantages? Or are you unconvinced by the advantages in the first place? And also, did you watch both of our published videos?
&gt; Do you see the value of the presented features made possible by projectional editing, but do not believe they outweigh the disadvantages? I don't see why you can do projectional editing and still save the files as normal text. Yes, I do see advantages in structured/projectional editing, but there's a LOT of convenience in existing tools. Also, there's certianly been times when it was faster (and no more error prone) for me to make a change by (temporarily) breaking the structure, and just dealing with Unicode characters for a bit, before restoring the structure and making the commit / saving the file.
Ah thank you, that's actually something pretty important :) I wonder what happened there ...
Do this! https://github.com/nh2/static-haskell-nix (see the `*-example` dir) Should be able to cut down the size by quite a lot.
&gt; And also, did you watch both of our published videos? I have, and I don't even find the statement of problems you are trying to solve convincing. Refactoring in Haskell is *way* better than in Java or Python, and in fact nearly ideal. Part of that is because Haskell lets me put type annotations anywhere I need in order to direct the type checker to the real arror (assuming it attaches to the "wrong" side to begin with, which I think is just a coin-flip). I do believe errors can be made better in C++ and Haskell without throwing out the text format, or altering the language at all. IIRC, UHC has much better error messages than GHC, and clang forced gcc to output much better error messages. Then we have the whole Rust ecosystem where an error you don't understand being output from the compiler is considered a compiler bug -- quality of error messages is a first-class feature. I do like how your editor is... closer to how I write code than how that Agda editor works. I'm still not 100% convinced I want to use another editor, since I currently use the same editor for Java, Python, Haskell, HTML, CSS, README.MD, man pages, etc., etc. and it's nice not to have to change my flow or lose features when moving between them.
&gt; I don't see why you can do projectional editing and still save the files as normal text. I see a could of places where the editor behavior is based on temporal properties of the code, which is hard to pull from plain text. However, it seems like you could still save the files as plain text, and use an external history, either generated by the editor scraped from the local DVCS repository, or some combination.
Not at the moment.
I don't think any of this is true. &amp;#x200B; There are 3 concepts at hand: \- strong normalization: for all reduction sequences, you will reach the normal form \- weak normalization: there exists a reduction sequence such that you will reach the normal form \- confluence: at a very abstract level, it means that there are no "bad" reductions (in practice, it means that for two starting sequences of reductions, there exist two suffixes such that the two paths meet again, up to some equivalences) &amp;#x200B; It seems your article is about the untyped lambda-calculus, which is confluent, but \*\*not\*\* strongly-normalizing and \*\*not\*\* weakly-normalizing. In fact it's very easy to make bad reduction strategies for some terms, say: if-then-else true zero infinite-loop This term has a normal form, but attempting to reduce the infinite loop, you will never reach it. &amp;#x200B; Now things are different the moments you add types. The simply-typed lambda calculus \*\*is\*\* strongly-normalizing, in which case any reduction strategy is valid (not necessarily efficient).
Very nice! Looks really slick! I like that it's dimmed. It's cool to relive (and study) the moment of failure. Now I want that to be prolonged/emphasised somehow - it disappears too quickly. Maybe you could drop to slow motion/bullet time in the last seconds, and/or leave it on screen until confirmed. Incidentally I think now it would be harmless to let SPACE work for selecting buttons, the replay creates a breathing space. Pro tip: for added verticality, turn your monitor to portrait mode!
Yeah, i was going to use that. I already use that for a project at work. I wish momentum on static-haskell-nix would pick back up.
At work we needed to statically link against librdkafka.
dang... lol. I guess thats the thing.. I find haskell really interesting and fun but how do people afford do use it in work environment? Besides GHCJS i've found compiling a lot of the stuff i'm trying to do in haskell taking quite a long time, and this is on 12core laptop 32gb mem, nice nvme etc.. Honestly I guess the good side is that while doing haskell dev I get a ton of book reading in while I wait so i guess thats a positive!
Good point. I've create a repository here: [https://gitlab.com/everythingfunctional/fizzbuzzhedge](https://gitlab.com/everythingfunctional/fizzbuzzhedge) I just created a project with `stack new` and then tried to add hedge to both the `extra-deps` in the `stack.yaml` file, and to the test dependencies in the `package.yaml` file. It complained about the version number for quickcheck-with-counterexamples, so I added that to `extra-deps` as well. No I get the following error, and I'm not sure what to do about it. ``` Error: While constructing the build plan, the following exceptions were encountered: In the dependencies for fizzbuzzhedge-0.1.0.0: hedge-trimmer needed, but the stack configuration has no specified version (no package with that name found, perhaps there is a typo in a package's build-depends or an omission from the stack.yaml packages list?) needed since fizzbuzzhedge is a build target. Some different approaches to resolving this: * Consider trying 'stack solver', which uses the cabal-install solver to attempt to find some working build configuration. This can be convenient when dealing with many complicated constraint errors, but results may be unpredictable. Plan construction failed. ```
`&gt;&gt;=` only pushes a single argument, but that's enough foundation to build `do`-notation that lets us pass multiple: f args = do let o1 = length args == 1 let o2 = True o3 &lt;- getO3 check o1 o2 o3 -- f :: [a] -&gt; IO Bool -- inferred by GHC
First, I reformatted your post: ``` doIt cmd args = do let o1 = length args == 1 -- Bool let o2 = True -- bool (getO3 &gt;&gt;= \o3 -&gt; check o1 o2 o3) -- IO Bool getO3 :: IO Bool getO3 = return $ True check :: Bool -&gt; Bool -&gt; Bool -&gt; IO Bool check x y z = return $ x &amp;&amp; y &amp;&amp; z ``` Then, you are asking if you can make: ``` doIt cmd args = do let o1 = length args == 1 -- Bool (getO2 &gt;&gt;= getO3 &gt;&gt;= \o2 o3 -&gt; check o1 o2 o3) -- IO Bool ``` I think you are missing the point of `do` notation here, I'd review more of that. `&gt;&gt;=` inside of a do block should instead be `x &lt;- exp`, as a syntactical sugar, where `x` is the name of the variable you want to give as a result of evaluating `exp`.
It's a bit hacky, but you can just use the readImage code minus the actual file read. If I were you I'd clean it up and submit this as a patch, adding a function `imageFromByteString` or some such. The code for readImage is: ``` loadImageRGBA :: ByteString -- ^ image -&gt; IO (Image VU RGBA Double) loadImageRGBA imgstr = do let maybeFormat = Nothing :: Maybe InputFormat formats = enumFrom . toEnum $ 0 orderedFormats = maybe formats (\f -&gt; f:P.filter (/=f) formats) maybeFormat reader :: Either String (Image VS cs e) -&gt; InputFormat -&gt; IO (Either String (Image VS cs e)) reader (Left err) format = return $ either (Left . ((err++"\n")++)) Right (decode format imgstr) reader img _ = return img imgE &lt;- M.foldM reader (Left "") orderedFormats either error pure $ fmap (exchange (undefined :: VU)) imgE ``` Meaning the code for loading the image is:
I agree that refactoring in Haskell is way nicer. Putting type annotations is a lot more work than a single key press - and in my talk about "friendly static types" (prior name before "steady typing") I showed how even with those, the compiler still complains about the wrong position in the code. Lamdu has grown to be quite a large Haskell project. It's not rare that debugging a type error involving some optics, classes and type families is not at all trivial. Adding type annotations requires enabling `ScopedTypeVariables`, adding `forall` annotations. Using `undefined` generates "ambiguous type variable" errors. There's a lot of friction. When fixing type errors after a refactoring, sometimes more type changes are required, and the simultaneous change of many types can become quite overwhelming. For me it generates actual stress about the refactoring, and it's not always easy to follow these errors through a large codebase. But all of this is really a discussion about one of many features. Some more features include: * Live value annotations (continuous evaluation and subexpression annotations) * Smart completions, jump to definition, ... that *always* work * Type mismatches make editing guidance better, not worse * Projectional syntactic sugar * Continuous auto-layout * Internationalization (edit same code in many human languages), type-based auto naming * Reliable refactorings * Future: Reduced merge conflicts: we believe most conflicts in practice as artifacts of text encoding, and not editing of the same code entities. * And many many more...
We can save the files as json text. But there's more information in there than a typical code file has. The names are not text, but unique ids with translations in many human languages. Frozen definition types are stored. There are more useful things we could store that would further enhance the dev UX, but make the textual encoding cluttered. &gt; but there's a LOT of convenience in existing tools Agreed! We strive to give the best of both worlds, and for us, the editing experience is even more convenient than text editors we've used for decades. There are quirks and inconvenient parts, but we see those as temporary bugs -- there's nothing inherently less convenient about the structured editing. &gt; when it was faster (and no more error prone) for me to make a change by (temporarily) breaking the structure, and just dealing with Unicode characters for a bit, before restoring the structure and making the commit / saving the file Can you give an example?
Sure, but the text you store is going to be cluttered if it is to have the data you need for the extra features, and then it's not really a plus in any meaningful way. You can do a json export and get this, essentially.
Yes. I'm saying that, if you have, for example, 6 shell commands that are run sequentially, with no data transfer or meaningful interaction, there's nothing gained by performing any abstraction, and no purity to encapsulate.
You have my bind
&gt; Can you give an example? Not a good solid one. Sometimes swapping out explicit recursion for a recursion-scheme feels that way if there's a lot of constructors. Both ends type check, and they share a lot of text, but all the in-between steps might not even be good syntax, though it's more likely it "just" doesn't type check.
I would rather have cluttered text I can edit over a dumb terminal than be required to have a dedicated editor for a single language.
&gt; I showed how even with those, the compiler still complains about the wrong position in the code. Again, that's a flaw in GHC, not all Haskell compilers. UHC can/will show you both/all places that contribute to the type disagreement. And, I don't think it's a problem that's reasonable to solve by making a language-specific editor.
Of course a language-specific editor isn't reasonable to solve any single particular problem that it solves (except maybe live evaluation, that one is awesome! :-) ). It's an accumulation of many features. Still, refragmenting the type error to other places is less friction than an error text pointing you at it. You immediately get correct auto-completions based on your choice, can continue working with all editor facilities, etc.
If that language is where you spend almost all of your programming time, why?
Lamdu lets you go thru intermediate structures that don't type-check (even demod in the video). These kinds of refactorings are going to be much nicer in Lamdu, as changing the type will lead you to the places that become fragmented as mismatches, and the first completion result will be the one you need - so a single key to translate each subexpr.
Sounds like a good idea. I think I will implement it. The renderer scales, so there should be no difference / advantage by changing screen properties (resolution, etc.)
A lot of the time a `DEPRECATED` pragma gets added to what is being phased out and this will result in a warning while compiling anytime you use it in your code. (e.g. `DEPRECATED: "Please use this other function x"`) While code with the `DEPRECATED` pragma should still work as expected, it is a warning to change to the new function/style a.s.a.p. https://downloads.haskell.org/ghc/8.4.4/docs/html/users_guide/glasgow_exts.html#warning-and-deprecated-pragmas
I want my application to read a configuration file if one exists, and give the user the option of creating such file for later use. I'd like to place the configuration file in `~/.config/MY_APP_NAME/MY_APP_NAME.dhall`. I can hard-code the value of `MY_APP_NAME`, but if there's a way to grab it programmatically, I'd prefer that. I'll update my post to make the use-case clear. Thanks for asking!
Ah, I see and agree; initially I interpreted what your statements to mean that you were suggesting no sensible abstraction existed that could encapsulate that scenario rather than that there was nothing to be gained by doing it.
I'm starting to use Hakyll - haven't had to touch Haskell code so far - just CSS, HTML and Markdown editing; I might try out yours. &amp;#x200B; Just in case, how easy/hard would it be to port HTML templates or CSS from Hakyll to sitepipe, should I want to do so in the future?
(I wouldn't even mind having to write a conversion script in such a case)
Yes, GHCJS has its problems (the JS it generates is quite big and slow, barely acceptable even after minifying it with Google Closure), but Reflex does not require GHCJS - you can use it with normal GHC to generate native code; you can even use Reflex-DOM in a native app - using a web rendering engine to display the UI, while still having a native app.
\`DaTwinkDaddy\` is the ultimate username.
Got it as a nickname back in high school.
Because I only have dumb terminal access to prod, sometimes. Also, I've not been writing in a single language for years. I usually have at least two going on -- one compiled one scriptable, and some extras for experimentation / fit to purpose.
Super excited for this, personally. This'll be the fourth consecutive year of tackling the contest with Haskell. (At least, something very unexpected would have to happen for me to not pick Haskell again this year!)
The problem with `readImage` is that it looks at the filename to determine the filetype. The way I implemented mine is by looking at `readImageExact` which is defined as readImageExact :: Readable img format =&gt; format -- ^ A file format that an image should be read as. See -- &lt;#g:4 Supported Image Formats&gt; -&gt; FilePath -- ^ Location of an image. -&gt; IO (Either String img) readImageExact format path = fmap (decode format) (B.readFile path) So I have the bytestring that I want to decode, but I need to somehow convert the extension I have (which is either ".png" or ".jpg") to a format. I copied their implementation for `guessFormat` and it compiled but I'm again getting errors when trying to use its result as an argument to `decode`.
But that only works for back-end, right? If UI and back-end are separate, we still need to build the UI.
&gt; `&gt;&gt;=` inside of a do block should instead be `x &lt;- exp` Not necessarily... I often use `&gt;&gt;=` in a `do` block when it makes the code clearer.
This is true (although perhaps pre-Java 8 a definiton/use of curry would've been untenable). On the other hand, at least in Kotlin, ```kotlin fun sum(x: Int, y: Int): Int = x + y println((1 .. 10).map(sum(1))) ``` doesn't compile. It's certainly possible to define curry. But this `sum` function is not *semantically unary* in the way that every Haskell function is. Getting curried functions for free is one of the really powerful features of functional languages and definitely one that a beginner should know is going on. In Haskell, the equivalent to the above ```haskell sum = (+) main = print $ map (sum 1) [1..10] ``` compiles and executes just fine. In fact I think the above contrast is a good example for the OP as to why `addThree` can be defined as such in Haskell.
Looking forward to the cases-in-wild post.
There is no single "author" of Haskell. Haskell was designed by committee, which released a the language specification in 1991. Since then there are probably more than 100 people who have contributed in some way. When Haskell was first developed Dependent Types were still almost purely a topic of research. That is probably why they weren't implemented. However, over the last 6-8 major releases, GHC has implemented a number of language features which implement parts of Dependent Types.
Sorry for the unformatted code, that wasn't expected. So what is the actual difference between `&gt;&gt;=` and the "left arrow &lt;-"? Just a synonym if it is located in a do block? And is it possible to combine a Bool with an IO Bool in a function?
So a Bool and an IO Bool could both be passed to a function, without problems?
Au contraire, I'm talking front-end UI: compile Haskell to native code that uses a browser engine (i.e. Webkit) as a portable GUI API - using web standards (DOM) as your UI API, but with a native app. The [jsaddle](http://hackage.haskell.org/package/jsaddle) library can provide you with the necessary bindings to drive the DOM from Haskell - with backends to do so both GHCJS and GHC - web and native. Of course, if we require the front-end to run on a browser, we are stuck with GHCJS, at least until Webassembly support for Haskell matures...
You might be able to use call stacks for this. [http://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Stack.html](http://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Stack.html)
Yeah, looks like this does the trick. ``` import GHC.Stack (HasCallStack, callStack, getCallStack, srcLocModule) moduleName :: HasCallStack =&gt; String moduleName = srcLocModule . snd . head . getCallStack $ callStack ```
`Bool -&gt; IO Bool -&gt; IO ()` is an inhabited type with non-trivial inhabitants.
Smells like Reader
Interesting, thanks.
Here's [the contest website](https://icfpcontest2019.github.io/) ([source](https://www.icfpconference.org/contest.html)). What criteria do they use for judging "the programming language of choice for discriminating hackers"?
The whole point of `do` notation is that it lets you write `x &lt;- foo bar; ...` instead of `(foo bar) &gt;&gt;= (\x -&gt; ...)`. `do` notation is just syntax sugar for (i.e. a mechanical translation of) `(&gt;&gt;=)` and `(&gt;&gt;)` that often is easier to read. About whether you can combine a `Bool` and `IO Bool`, the pedantic answer is no - `(&amp;&amp;)` can only combine `Bool` and `Bool`. BUT you can easily extract the `Bool` out of the `IO` using `do` notation: doIt cmd args = do let o1 = length args == 1 -- Bool let o2 = True -- bool o3 &lt;- getO3 check o1 o2 o3 getO3 :: IO Bool getO3 = return True check :: Bool -&gt; Bool -&gt; Bool -&gt; IO Bool check x y z = return $ x &amp;&amp; y &amp;&amp; z
Yes, but as soon as one of the arguments to a function is in `IO`, chances are good that the return type of the function will be forced to be in `IO` as well.
Scoring methods will be announced as part of the problem description, plus there's a separate Judge's Choice award.
Also, Agda and Idris are both dependently-typed languages written in Haskell, both with Haskell-inspired syntax that are available today.
To add a local package to your `extra-deps`, add the path, not the package name, to know where to find it. extra-deps: - path/to/hedge-trimmer - path/to/hedge https://docs.haskellstack.org/en/stable/yaml_configuration/#extra-deps
True, but neither is currently as well suited as Haskell to writing production ready code.
I think it depends on the use case, but for most, yes. I haven't used it in a little bit, but Idris has too many sharp bits poking out of the compiler, so it's really only suitable if you know you can avoid all of them, or you are also willing to hack the compiler. I think the main issue with Agda is performance and ecosystem size; the compiler is much more solid / hardened than Idris. Ecosystem will never grow if people don't contribute.
This is hilarious, I love it.
&gt; But not if you add an unreachable wildcard clause &gt; &gt; type family UnitId (u :: ()) = (u' :: ()) where &gt; UnitId '() = '() &gt; UnitId _ = TypeError ('Text "Unreachable") &gt; {- compiles cleanly -} It's not unreachable! Well, kind of. I can use [impossible types](http://gelisam.blogspot.com/2017/11/computing-with-impossible-types.html) to add a reachable second clause even though `'()` seemingly already covers all the possibilities: type family Succ :: k -&gt; k -- | -- &gt;&gt;&gt; :kind! UnitId (Succ '()) -- UnitId (Succ '()) :: () -- = (TypeError ...) type family UnitId (u :: ()) = (u' :: ()) where UnitId '() = '() UnitId (_ _) = TypeError ('Text "Unreachable") Surprisingly, if I write `_` instead of `(_ _)`, then I _don't_ get `TypeError...`, instead `UnitId (Succ '())` is stuck. Which means that, crazily, `_` is an incomplete wildcard, as there are some kinds which `(_ _)` matches on but not `_` and vice versa! Impossible type are weird.
There are several tricks you can use. One is you can turn to `template-haskell`, where for instance 'Left will give you the `Name` associated with Left, which you can rummage around in to get the package and module name. ghci&gt; case 'Left of Name (OccName i) (NameG _ (PkgName p) (ModName m)) -&gt; (p,m,i) ("base","Data.Either","Left") You should be able to do similar tricks with `Data.Typeable` and `GHC.Generics`. You could rely on a convention that you set it in some define in the cabal file. There is also the fact that you can run `System.Environment.getProgName` which will get the current executable name, as it appears to the operating system. This is good for most situations, but there are some situations involving forking, etc. where it can be a bit off.
Is this currently one of the main development goals of the GHC?
I think it is *one* of the development goal, but I am only a very occasional contributor so I may be wrong.
I have been using Haskell professionally for about 6 years now, in three different jobs. As long as you use Haskell for a purpose for which it is well suited it works well.
Do you program *on* prod? Compiled languages are scriptable though. Our vision for what we'd like with PLs is most languages as DSLs. Perhaps after the core language is extended to support dependent and linear types.
I found the same style to be useful, I sometimes call them "configurable data types", or "data types fast food": https://mazzo.li/posts/customizable-data-types.html .
Not without any external typechecker plugins, perhaps. The \`Symbol\` type, due to historical reasons, is an opaque unit. GHC doesn't natively offer any way to break it down or decompose it in any meaningful way.
My school teacher taught me, that I have to use the \`do\` statement whenever I want to write multiline statements. But if I understand you right, it is not it's main purpose... it's so called syntatic sugar, which allows to write certain statements in an alternative (easier) way?
Not even use `TemplateHaskell` ?
The only thing TemplateHaskell can do is generate Haskell code that is otherwise legal. It cannot do anything that was not already possible by just writing normal source code. However, with TH, it does become easier to work with something like \`\[FakeChar\]\` instead of \`Symbol\`. Lists are not opaque; they can be pattern matched on for \[\] and : at the type level, so you could write a type-level \`Length :: \[k\] -&gt; Nat\`. You would just need to be working with lists the whole time, and not Symbol. TH could make it more tolerable to work with lists of char-like kinds.
Sorry, but I don't get the point of your sentence. If I pass a `Bool` value and an `IO Bool` to a function, the return type is `IO ()`?` What do you mean by `inhabited` and `non-trivil inhabitants`? Thanks for your help.
Thanks.
\` class Listify (sym :: Symbol) (result :: \[Symbol\]) \` in [https://hackage.haskell.org/package/symbols](https://hackage.haskell.org/package/symbols)
well, actually, it does &amp;#x200B; [https://kcsongor.github.io/symbol-parsing-haskell/](https://kcsongor.github.io/symbol-parsing-haskell/)
This package has a brilliant idea! Thanks.
Wow that's actually an amazing idea. I've actually been a bit annoyed by how hard it is to add/change parameters. My only issue is that you have to clearly define stages whereas befire the "stage" came out of the combination of parameters. I think that might be even better though since you clearly get the stage from the type parameter.
I've tried to highlight the important parts of the implementation in this post, [Tic-tac-toe in Haskell](https://dev.to/dwayne/tic-tac-toe-in-haskell-4f9j).
The answer to your question was yes. You can pass a `Bool` and an `IO Bool` to a function. The return type I used was but one example. There are certainly other that will work. An inhabited type is one with values. Uninhabited types are useful under the Curry-Howard isomorphism, where they represent untrue or unprovable statements. A trivial inhabitant of a type is one that is uninteresting. In particular, for function types, function that ignore their arguments are sort of uninteresting. `const $ const (return ()) :: Bool -&gt; IO Bool -&gt; IO ()` is a trivial inhabitant of `Bool -&gt; IO Bool -&gt; IO ()`.
You mostly got it. You could say the purpose of `do` is to write multiline statements, but you can always rewrite your multiline statements to a single expression. It would just be cumbersome to read and write those expressions. `do` does the rewriting for you. The way it works is first it checks if you used implicit or explicit statement separators. If you don't write the curly-braces and semicolons yourself, the parser will insert them for you based on indentation. All of these are the same: foo = do bar baz quux foo = do { bar; baz; quux; } -- Most haskellers like to put separators at the start of the line foo = do { bar ; baz ; quux } The desugaring is actually pretty simple and works like this -- a single expression is just that expression do {foo} ==&gt; foo -- An expression followed by more expressions uses the (&gt;&gt;) operator do {foo ; more...} ==&gt; foo &gt;&gt; do {more...} -- An expression that binds a result uses the (&gt;&gt;=) operator and a lambda do {x &lt;- foo ; more...} ==&gt; foo &gt;&gt;= (\x -&gt; do {more...}) -- The lets in a do block just become regular lets that wrap the rest of the block do {let x = foo ; more...} ==&gt; let x = foo in do {more...} When the desugaring creates another `do` block it just repeats the desugaring process until they are all gone. Here's an example of how desugaring works: ``` main = do putStrLn "Guess a number" guess &lt;- readLine if guess == "23" then putStrLn "You guessed correct" else putStrLn "You guessed wrong" putStrLn "Good game" --Desugared version main = putStrLn "Guess a number" &gt;&gt; getLine &gt;&gt;= (\guess -&gt; if guess == "23" then putStrLn "You guessed correct" else putStrLn "You guessed wrong" &gt;&gt; putStrLn "Good game" ) ``` As you can see it's not that different. It's noisier, and it gets tedious keeping track of parenthesis which expression is nested inside which etc. in more complicated blocks, but it all boils down to `(&gt;&gt;=)` in the end.
&gt; Do you program on prod? If I'm having to fix a production issue, it has happened. It's certainly not preferred or normal, but it have been necessary. &gt; Compiled languages are scriptable though. Not in the way I mean it. But, the main difference I would draw is how those two languages are deployed. I use one language were we deploy the output of a compiler and one language where we deploy the source code directly. Today, mostly the source language is used to fill the gap between what our OS can execute directly, and the binaries we want to run. We also use the source language when the advantages of compilation do not justify the additional deployment burden. &gt; Our vision for what we'd like with PLs is most languages as DSLs. Good luck.
Ah! I had forgotten about the newish `AppendSymbol`. There might be hope after all :)
My article [Compiling Lisp to JavaScript From Scratch in 350 LOC](https://gilmi.me/blog/post/2016/10/14/lisp-to-js) has a section about implementing a minimal parser combinators library and using it. There's also [Intro to Parsing](http://jakewheat.github.io/intro_to_parsing/#getting-started) by Jake Wheat that talks about using parsec.
I would say it's not really about multiline vs single line statements, since you could write code using do notation on single or multiple lines, and you could write it without do notation on single or multiple lines. Rather, it's about avoiding extra parentheses and nested anonymous functions, because it gets hard to keep track of the nesting. But sure, usually code written with do notation is written on multiple lines, so your teacher's rule is fine. I'd also point out that my code didn't treat the `IO Bool` the same as a regular `Bool`. I had to use `&lt;-` to get the value of it, instead of using `=`. That's because `&lt;-` is one of the things that's special in do notation: it's short for using `(&gt;&gt;=)` and an anonymous function.
I think you can likely handle it. You seem quite bright. If you are lacking confidence I taught an 8 year old Lambda Calculus using: http://worrydream.com/AlligatorEggs/
Thanks man. Looks helpful!
You don't think that wide binary sums or products will get clunky to deal with? I would have expected a little more type magic to allow type-level lists of "functors" like in [`generics-sop`](https://hackage.haskell.org/package/generics-sop).
Since you know JS, I will plug my introductory two-part presentation on lambda calc: https://youtu.be/3VQ382QG-y4
Woah! Thanks man. Useful links. Thanks for the talk and github link!
To Dissect a Mockingbird http://dkeenan.com/Lambda/ like Alligator Eggs, a picturesque explanation of lambda calculus.
I think a global counter is a fundamentally bad idea. Couldn't this be implemented the same way you number a heap data anyway? ~~~ uniques n = MkUniqSupply n (uniques (left n)) (uniques (right n))) left n = 2 * n + 1 right n = 2 * n + 2 ~~~ anyway?
It seems like the links to functions and types in the Hackage docs are broken, maybe because of the manual upload?
They could indeed, but note that an IO Bool is nothing like a Bool value -- it represents an action which could be taken to ultimately result in a Bool when executed. The difference between IO String and String is like the difference between /bin/ls and a listing of the files in your home directory (at a particular moment in time). In order to execute an IO action, say x :: IO a, you can write v &lt;- x in the middle of a do-block. The v will have type a and refers to the result of the action. Executing the same action multiple times will generally produce different results each time, and will cause any effects described by the action to occur again. For example, getLine :: IO String will get a line of text from the user, and if we execute it twice: do x &lt;- getLine y &lt;- getLine return (x,y) this will be an action which asks the user for two lines of text, and returns the corresponding pair of (possibly different) strings. This do-expression as a whole has type IO (String, String). So we can use do-notation to glue together IO actions, expressing that the combined action should carry out a bunch of others in a particular order. The type of a do-block is always the same as the type of its last line, as the result of the combined action is the result of the last action in it. return :: a -&gt; IO a can be used to make an action that does nothing except to, well, return the given value.
https://www.youtube.com/watch?v=csaUvkYOkLY
There are two resources I tend to point people at when starting to learn the λ-calculus: 1. Sean Gillespie's tutorial, which begins from first principles and does some actual calculuations, advancing to system F if one desires: https://github.com/sgillespie/lambda-calculus/blob/master/doc/lambda-calculus.md 2. Lennart Augustsson's Lambda Calculus cooked 4 ways, which shows how to implement a λ-calculus in Haskell, including de Bruijn bindings. From there, there are myriad tutorials on how to write different flavors of the calculus. The part I'd suggest you focus on is getting the ideas and exercises into your fingers, then branching to the more complex stuff like System F or Hindley-Milner.
I happily use HIE with vscode and I suffer from the same issues. One thing that improved my quality of life is to keep the HIE output open in the "Output" panel. This way you immediately see when it crashes, so you don't wait for something from it in vain. When that happens I either mind my own business, or if HIE is crucial for my task, I restart vscode (I'd be glad to learn if there's a quicker way to restart HIE!). &amp;#x200B; I've also noticed that the crashes happen the most often when you have A.hs and B.hs open at the same time, where A.hs depends on B.hs. If, in that setting, you edit B.hs, you'll occasionally crash HIE.
Compiling does take a while but for development a lot of people usually either lean heavily on GHCi and interpret it, or they'll compile projects with fast no-optimizations flags and it gets *way* faster doing that. Hell, it's still fast enough to use in production even without optimizations (you should still compile with optimizations for production, to be clear). Rust has a lot of the same problems, but with a distinct lack of a repl (which I miss dearly every time I use it). Their solutions are also much of the same.
A very good related read: https://blog.trailofbits.com/2019/06/17/leaves-of-hash/
yea i'll agree that the repl thing is definitely a life saver vs not in rust Maybe a good summer project I should dig around to see what kind of material there is on trying to help the compiler move along faster
any update on this
You're right! I didn't spend too much time on it --- mostly just mindless find/replace, so I'm not surprised I cocked some things up! Sorry for the confusion! The important parts IMO are: * Don't use giant effects that can do _everything_ you want. Decompose them into smaller effects each of which does one thing and does it well. * Don't be afraid to reinterpret into standard effects. I agree with you and /u/ocharles that I went too far in my PR --- spin up domain specific effects that describe you problem in your own terminology, and then interpret them into standard effects later. * The `Reader` pattern in mtl is an antipattern in `polysemy`. If you just need to pass something constant around. `Input` is a much better fit since it doesn't have `local`.
Why not http 3?
&gt;I don't think any of this is true Which part did I get wrong?
Oh man what the hell was THAT?!
It's playing off the typo in the title.
My mistake, having re-read your first claim, I actually think you are correct. I had misinterpreted it, and will update my answer accordingly.
&gt; in-between steps might not even be good syntax
What makes you think homebrew is unrelated or misleading for installing dev tools on os x?
caskroom io is a spam site. It certainly doesn't belong on haskell.org. Seems somebody was a bit lazy when coding the page, and hid the link instead of bothering to remove it?
Nah not really. The functional nature of haskell more closely matches with the nature of data science. R and more-so Python are popular because the low barrier to entry. The business analysts earning 60k/yr translate some of their excel skills to scikit, watch a few udemy courses, and suddenly they're worth 150k/yr even though they're just making these little 100 line, poorly engineered scripts that glue into ML frameworks.
But it's true that the ecosystem is lacking, correct? There's [http://www.datahaskell.org/](http://www.datahaskell.org/) but that's about it.
http://hackage.haskell.org/package/base-4.12.0.0/docs/System-Environment.html#v:getProgName
I’d tried some impossible types, but hadn’t tried change the pattern to `_ _`. Good find!
We're only up to 2?
Venanzio Capretta gave some great lambda calculus lectures at the Midlands Graduate School back in April. Course notes are here: http://www.duplavis.com/venanzio/mgs_lambda/index.html
It seems the page *used to be* the homepage of Homebrew Cask until the domain expired. Seems if this tool is still relevant it might need to point to the github page instead? https://github.com/Homebrew/homebrew-cask
These lectures are actually a great source. Venanzio was additionally my teacher at uni and he recommended "Introduction to Lambda Calculus" by Henk Barendregt (who was his supervisor) which is available online ([http://www.nyu.edu/projects/barker/Lambda/barendregt.94.pdf](http://www.nyu.edu/projects/barker/Lambda/barendregt.94.pdf)) and is highly approachable. \&gt; Links and resources *for lambada calculus in specific* would be appreciated. The Lambda Calculus, Its Syntax and Semantics by Barendregt is a much more detailed treatment of Lambda Calculus. (Another recommendation by Venanzio, I have not studied this book)
As you note, that whole section is disabled. So it's not on the site in any meaningful way. Indeed it used to be the official site for homebrew cask. That section was disabled when the site went away.
There's no laziness. The link was turned off because the site stopped working. The fact that you can see the turned off thing in the source is neither here nor there. In any case, patches welcome.
I personally dislike FDs on the basis that I think TFs are the way forward, and thus would like to see FDs used less. Besides that they are benign.
I would suggest using TypeFamilies instead of FunctionalDependencies.
I could only see this section because of slow internet. I didn't know that it used to be a legitimate site hence was surprised to see such a link even in normally disabled section.
Well we won't get to 3 with that attitude...
Python and R have become the de facto standards for data science simply because large machine learning libraries for these languages have already been written. Large companies like Google also these libraries as well. In Haskell, one might have to write a lot of the mathy code themselves, while in Python, things may be as simple as calling BlackBox.fit() and geeting decent results.
I know a few people at school (probably myself included) who would enjoy working on things like a neural net library, but only if the kinds of fast linalg tools that python already has (numpy) already existed. I like math but those types of algorithms break my brain. On the other hand maybe i should learn!
I use uMatrix and have scripts off by default and the link appeared for me.
Thanks for all the answers, I did not know much about the history of Haskell. I though Dependent Type has been here for long time. Also, there are something called LiquidHaskell out there, I watched a few Youtube video and I found it is fascinated what you can do with LiquidHaskell which used some sort of Refined Type =&gt; I have no idea what it is. &amp;#x200B; If we can implement Haskell fully **Dependent Type** and **Refined Type**(like **LiquidHaskell**), then Haskell will be awesome.
Haskell has the excellent ad (automatic differentiation) library, which could help make implementing backpropagation (which is basically finding the derivative of a neural network) easier.
The fastest way the ecosystem improves, is if you add to it :) I did the same. We needed to use JSON in our PostgreSQL, so I added [JSON instances and operators to `persistent-postgresql`](https://hackage.haskell.org/package/persistent-postgresql-2.10.0/docs/Database-Persist-Postgresql-JSON.html) and [created the `safe-json` library](https://hackage.haskell.org/package/safe-json) so when message formats between services change, we'd still be able to update the live running services easily without any downtime.
It is 2 until it is 3. ^{[ref](https://www.reddit.com/r/Gunners/comments/9401yf/throwback_to_when_arsenal_twitter_and_the_summer/)}
Wow, very exciting to see QUIC coming to Haskell.
Keep us updated :)
No need to apologize, I don't think it's too likely to run into the issue and it's certainly still very helpful either way. I updated the project to split into multiple examples, including your suggested changes (`example3`). Please let me know if you don't want me to include your changes like that. I also added an `example2` which hopefully you'd consider an improvement over the original, although it still does the giant effect thing.
&gt; What would I have to know to be able to read ghc proposals about it? Have you read [Richard Eisenberg's thesis]( https://repository.brynmawr.edu/cgi/viewcontent.cgi?article=1074&amp;context=compsci_pubs)? If that is too technical, it may still be worth looking up the citations of parts that seem interesting.
I think Haskell is already awesome, but if you want to improve it, please do!
This isn't going to be a response to your question: There should be an answer to this question pinned at the top of the sub. We get this question every two weeks.
the article says "HTTP/2 over QUIC is now called HTTP/3."
Isn't that much more advanced? I'd like to keep my code as simple as possible. The less extensions I use the better.
&gt; Haskell is obviously a good choice for math stuff Except it has a very broken type hierarchy wrt., well, numbers and mathy stuff.
I have to say, this is a new low for Forbes.
For learning purpose I'm working through Okasaki's queues described in [Efficient Amortised and Real-Time Queues in Haskell](https://www.well-typed.com/blog/2016/01/efficient-queues/). Profiler's result for "Strict, Non-Persistent Queues" variant looks much better: 25,957,576 bytes allocated in the heap 13,714,816 bytes copied during GC 2,092,456 bytes maximum residency (6 sample(s)) 1,954,944 bytes maximum slop 1 MB total memory in use (0 MB lost due to fragmentation) Tot time (elapsed) Avg pause Max pause Gen 0 19 colls, 19 par 0.065s 0.019s 0.0010s 0.0095s Gen 1 6 colls, 5 par 0.049s 0.014s 0.0023s 0.0038s Parallel GC work balance: 10.46% (serial 0%, perfect 100%) TASKS: 10 (1 bound, 9 peak workers (9 total), using -N4) SPARKS: 0(0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled) INIT time 0.003s ( 0.005s elapsed) MUT time 0.026s ( 0.024s elapsed) GC time 0.114s ( 0.033s elapsed) RP time 0.000s ( 0.000s elapsed) PROF time 0.000s ( 0.000s elapsed) EXIT time 0.001s ( 0.000s elapsed) Total time 0.144s ( 0.062s elapsed) Alloc rate 1,004,398,173 bytes per MUT second Productivity 18.0% of total user, 38.3% of total elapsed
Ah, thanks. Hadn't seen that convention used before.
I understand what you're saying. That is true with the current paradigm. I imagine a different way of performing the same change: * Change the explicit type or the type declaration to have an external fixpoint * Get fragments at all the recursive references, because they mismatch the type * Quickly navigate through these fragments - use the first fragment completion result to fix the errors. This works because the fragment has the wrong type and the expected type, and can thus offer the right change to make it click. Perhaps a concrete code change example could be used to clarify.
You might want to check this [repo](https://github.com/sweirich/dth) out. It hasn't been updated in a year or so, but the examples are very cool nonetheless.
No TypeFamilies isn’t any more advanced than FunctionalDependencies, I would actually say it’s more intuitive and easier to understand. It also means you don’t need MultiParamTypeClasses at all. For example Ixed from Lens: class Ixed m where type Index m type IxValue m ix :: Index m -&gt; Traversal’ m (IxValue m)
Richard talked briefly about this in his ZuriHac keynote. Make sure to watch that when it's published in a week or two.
Thank you very much for your detailled answer. You helped me a lot.
Yeah I think he said he's optimistic about delivering it in 2020
Thanks for your answer. In my opinion this whole io topic is not as understandable as some teachers expect. But I think some more tries and I get the concept of it. Thank you again.
&gt; attitude altitude
What's wrong with it?
[io-streams](https://hackage.haskell.org/package/io-streams-1.5.1.0/docs/System-IO-Streams-Attoparsec.html#g:1) offers an `parseFromStream` example, which doesn't work in my environment. λ&gt; :set -XOverloadedStrings λ&gt; import Data.ByteString (ByteString) λ&gt; import qualified Data.Attoparsec.ByteString.Char8 as Char8 λ&gt; import qualified System.IO.Streams as Streams λ&gt; import System.IO.Streams.Attoparsec ( parseFromStream ) λ&gt; is &lt;- Streams.fromList ["12345xxx" :: ByteString] λ&gt; let parser = (Char8.takeWhile Char8.isDigit) λ&gt; :t is is :: Streams.InputStream ByteString λ&gt; :t parser parser :: Parser ByteString λ&gt; :t parseFromStream parseFromStream :: attoparsec-0.13.2.2:Data.Attoparsec.ByteString.Internal.Parser r -&gt; Streams.InputStream ByteString -&gt; IO r It's a pity, but I can't solve it by myself. λ&gt; :t parseFromStream parser is &lt;interactive&gt;:1:17: error: • Couldn't match type ‘Data.Attoparsec.Internal.Types.Parser ByteString ByteString’ with ‘attoparsec-0.13.2.2:Data.Attoparsec.Internal.Types.Parser ByteString r’ NB: ‘attoparsec-0.13.2.2:Data.Attoparsec.Internal.Types.Parser’ is defined in ‘Data.Attoparsec.Internal.Types’ in package ‘attoparsec-0.13.2.2’ ‘Data.Attoparsec.Internal.Types.Parser’ is defined in ‘Data.Attoparsec.Internal.Types’ in package ‘attoparsec-0.13.2.2’ Expected type: attoparsec-0.13.2.2:Data.Attoparsec.ByteString.Internal.Parser r Actual type: Parser ByteString • In the first argument of ‘parseFromStream’, namely ‘parser’ In the expression: parseFromStream parser is
Phil always brings great posts, [`Validation`](https://hackage.haskell.org/package/validation-1.1/docs/Data-Validation.html#t:Validation) is not a *newtype* over `Either` so we can't safely [`coerce`](https://hackage.haskell.org/package/base/docs/Data-Coerce.html#v:coerce) it. Using [`Validation` from Roman's blog](https://ro-che.info/articles/2019-03-02-lazy-validation-applicative) the *Functor* and *Applicative* instances can be derived {-# Language DerivingVia #-} import Data.Functor.Compose -- .. newtype Action err a = MkAction (Either err (IO a)) deriving (Functor, Applicative) via (Validation err `Compose` IO)
Sounds good! Keep it up
I thought he was aiming for March \*2021\* to merge it.
Last time I counted we'd got to 3
Ohh alright. Tx for the correction
&gt; One quick note: There is a polysemy Random effect in the polysemy-zoo package. So you could use that as well if you wanted to. Thanks! I actually saw that, but I figured since I was making an example aimed at people who were just starting that it would be more complicated to depend on another package.
They hit a lot of those recently imo.
I'm glad that it doesn't, personally. I'm really interested in DTs and all the work that has been done to make them easier to use but if you added it to the standard you'd have a different language on your hands, one that promotes a significantly different way of thinking. As long as it's a compiler extension people can use them if they have a need but imo haskell itself should stay within its natural "semantic boundaries" because it's unique in that space and complicated enough as it is.
Yup, someone else mentioned that http2 with QUIC is being called 3.
Interesting. I've never used `TypeFamilies` directly so I'm not really familiar with it. How would you define the `class Ctx` using `TypeFamilies` instead of `FunctionalDependencies`?
I assume one or more of the alternate Preludes get it right.
It’s just https://www.quora.com/What-are-some-use-cases-for-which-it-would-be-beneficial-to-use-Haskell-rather-than-R-or-Python-in-data-science. No idea why forbes does the mirroring.
Because they can put ads around it.
Have any other Haskeller's looked at Julia's Flux? Here's a quick introduction: [https://fluxml.ai/Flux.jl/stable/models/basics/](https://fluxml.ai/Flux.jl/stable/models/basics/) (It was hard for me to understand that page, but once I did, it made sense and I was impressed with Flux.) Flux makes me believe Haskell could have an equally good machine learning ecosystem. All Flux has is multi-dimensional arrays (provided by the language itself), automatic differentiation, and then a dozen functions on top of that for convenience, and they all need to play nice together (maybe the hardest part?). I weep when reading Python TensorFlow code, so much noise dedicated to transforming back and forth between Python and NumPy, so many methods to define. I've been studying reinforcement learning with Flux, I'm working on simple problems with small networks, but even so, you want to see the last neural network I defined? function make_network(hidden_layer_size=64) Chain( Dense(state_size, hidden_layer_size, relu), Dense(hidden_layer_size, hidden_layer_size, relu), Dense(hidden_layer_size, length(actions), identity), softmax) end That's the whole thing, and it's fully featured. `Dense(...)` is a function, you can do `Dense(...)(input)` and you get ouput for the layer. `Chain` does function composition. If Julia had Haskell's `(.)` function, you could have written `softmax . Dense(...) . Dense(...) . Dense(...)` for the same result. As best I can tell in TensorFlow you have to define a `forward` method on your neural networks which always seems to be 10+ lines of rather unfriendly looking code. Anyway, I'm talking to much about Flux. I'm unfairly biased towards it because it's the only neural network library I know. My point, again, is Flux shows that multi-dimensional arrays, automatic differentiation, and just a few functions on top of these can produce a nice to use machine learning library, and that an interface that is more functional (Julia doesn't have "classes" only structs and functions) is nicer than what you find in Python.
Ecosystem is lacking, but so are people who are ready to do enterprise-level functional programming
I think deriving `Applicative` for `Validation (Either a b)` would give you an instance which short-circuits on errors instead of accumulating them (the latter is the behavior of the two linked versions).
Actually, on that note, a slightly lateral and broader question: What is the current state of the Haskell labor pool? It seems to me, looking at the economics of it, startups are less likely to choose Haskell for their stack because there are so few people who know the language (?). Thinking like "Why the hell would we choose Haskell and risk employee attrition followed by lack of manpower, when we can just use Node or Python and not take that risk?" This in turn makes people less likely to learn it because there are no job prospects. So it looks to me like a vicious catch22. Believe me, I've looked around, and the pickings are slim. Yeees there are some startups like LeapYear and DFinity and so forth doing it, but these are few and far between, and even if they do exist, who knows what state they're in. It's actually upsetting because this is a tech that I would really like to work with.
They can't get everything right, because some of it is baked into the core of the language definition. However, to be fair, wrt. data science applications that's all not really a hindrance, so I threw you a red herring by clinging to the general expression "good at mathy stuff".
&gt; The fastest way the ecosystem improves, is if you add to it :) thank you for saying that :) have a look at our opinionated but rather up-to-date directory/wishlist of related packages : http://www.datahaskell.org/docs/community/current-environment.html as you said, there's so much to it than neural networks!
You'll probably need to provide some more information about your environment to help people help you. I created a new stack project via `stack new io-streams-example` which was created with the lts-13.26 resolver and got: λ&gt; :set -XOverloadedStrings λ&gt; import Data.ByteString (ByteString) λ&gt; import qualified Data.Attoparsec.ByteString.Char8 as Char8 λ&gt; import qualified System.IO.Streams as Streams λ&gt; import System.IO.Streams.Attoparsec ( parseFromStream ) λ&gt; is &lt;- Streams.fromList ["12345xxx" :: ByteString] λ&gt; let parser = (Char8.takeWhile Char8.isDigit) λ&gt; :t parser parser :: Char8.Parser ByteString λ&gt; :t is is :: Streams.InputStream ByteString λ&gt; :t parseFromStream parser is parseFromStream parser is :: IO ByteString
This is fantastic work, especially going the extra mile to provide a library for other servers - really inspirational stuff.
\&gt; Sorry for such a dumb question There are no dumb questions. :) The most straightforward (and readable) way is to do this via do-notation: doIt cmd args = do o2 &lt;- getO2 o3 &lt;- getO3 let o1 = length args == 1 check o1 o2 o3 Now, if check were pure (returning Bool instead of IO Bool), you could also do the following: doIt cmd args = liftA2 (check o1) getO2 getO3 where o1 = length args == 1 liftA2 is analogous to fmap (which can turn a function f :: a -&gt; b into a function fmap f :: IO a -&gt; IO b), but for multiple parameters. If the type of check were Bool -&gt; Bool -&gt; Bool -&gt; Bool and you wanted to get the 2nd and 3rd argument from IO, you'd lift (check o1) into IO via liftA2 (the "2" indicates the number of arguments): check :: Bool -&gt; Bool -&gt; Bool -&gt; Bool check o1 :: Bool -&gt; Bool -&gt; Bool liftA2 (check o1) :: IO Bool -&gt; IO Bool -&gt; IO Bool liftA2 (check o1) getO2 getO3 :: IO Bool liftA\* also works with other Applicatives (a type between Functor and Monads like IO) like List and Maybe.
`massiv` is nice.
[There future is now!](https://opencollective.com/static-haskell-nix) These are recent advances and the example config work with large, 200+ dep applications :)
 class HasLogger ctx where type LoggerM ctx loggerL :: Lens' ctx (Logger (LoggerM ctx)) instance Monad m =&gt; HasLogger (Ctx m) where type LoggerM (Ctx m) = m loggerL = lens getLogger (\x y -&gt; x { getLogger = y })
You provided a link to your entire blog instead of the summer of code blog post for the "issue wanted" project.
Thank you! Fixed it!
[https://skillsmatter.com/skillscasts/12195-keynote-dependent-types-in-haskell](https://skillsmatter.com/skillscasts/12195-keynote-dependent-types-in-haskell) had some status about 8 months ago at least … (you need to make an account (free) to watch the video)
Thanks for the example, that is an interesting approach,
Yeah most new libraries take the TypeFamilies approach rather than the FD approach. They compose a lot better, so for more complicated situations you almost have to use TFs, but even for simple uses they are a lot more intuitive.
Yes for `Validation` but I derive `Action` using `Compose (Validation err) IO` as a *via type*. Instead of inheriting the behaviour of the representation type `Either err (IO a)` I use that type to get custom behaviour. This uses the [`-XDerivingVia` extension](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#extension-DerivingVia). Am I misunderstanding?
One of the design goals with the DT work going into GHC is that you can avoid them, at least in greefield code. That may or may not be how the ecosystem moves forward; that depends on library writers (and users -- who determine the popularity of libraries). But, IMO, most of the advantages of dependent types can be gained through internal use only, without complicating APIs.
Fixed it. Thanks again for pointing it out :)
`Num` the typeclass is pretty flawed. It's at least a ring (and has no superclass), when there are useful number-like types that are not rings, for example natural numbers. It's also a little more than a ring because of `abs` and `signum`, further ruling out useful number-like types even if they are rings, for example gaussian (complex) integers. It's also directly attached to number literal desugaring, which rules out useful number-like types that you don't want to add or subtract or multiply at all, for example database keys. It also ties addition and multiplication together, when there are useful types that can support one but not the other, or where one of the operations needs a more flexible type, for example numbers with units or lists under `++` and `liftA2 (&lt;&gt;)`. There are other issues (`Fractional` tied to literal desugaring) but `Num` is the most glaring. The `algebra` package seems to do a nice job with creating a significantly better number hierarchy.
I run a startup on Haskell, and I have more high-quality applicants than I know what to do with. I've hired a few people, and they're brilliant. The hiring front of my company couldn't have been a greater success.
Generally from what I've seen a lot of standards follow that pattern. Someone implements a new thing and calls it "old X + new thing" until everyone has decided that's going to be how the standard does it (for example spdy was http + other stuff but never became http2)
/u/_jgt where is your office? or are they all remote?
On the other hand, many numerical algorithms used in practice are not designed to be restricted to specific fields (e.g. the rationals). There have been indeed experiments in mathematically rigorous typeclass hierarchies (such as `subhask` and `numhask`), but then other considerations come into play such as long typechecking times and limitations in how expressive the Haskell type system is in modelling mathematics. My current understanding is: design algorithms that operate over `Floating` or even over concrete types such as Double, and be happy.
&gt; On the other hand, many numerical algorithms used in practice are not designed to be restricted to specific fields (e.g. the rationals). I'm not sure why you say "on the other hand", it seems like you are agreeing me with this statement. &gt; My current understanding is: design algorithms that operate over Floating or even over concrete types such as Double, and be happy. I refuse, even with those considerations in mind, splitting out numeric literal desugaring into a different class would be a significant improvement (would make library solutions nicer for one). Also splitting out `Semiring` and `Ring` superclasses would be a fairly objective improvement.
Haskell would need the same libraries, until then it’s not really usable like R and NumPy are. However, I recently looked at the tensorflow docs and the lack of types in the python api is jarring, desperately needs structure. I’ve heard some analysts say that’s because the concepts aren’t clearly put together and the jumbled API reflects that.
Why yes, I do agree with all the points you made; however I am currently persuaded to not let idealism get in the way of creating working code. I do not have years and the energy to advocate and wait for a refactoring of both `base` and GHC to support a better numerical typeclass hierarchy, so I "make do" with what we have now (which is still evolving ! See all the nice work on LLVM, or the SIMD bindings, or multithreaded arrays such as `massiv` etc. ). Numerics in Haskell is a very complex problem, there have been a number of libraries to tackle it but as far as I know the first objective (performance-focused) comparison between them was started only one month ago : https://github.com/Magalame/fastest-matrices .
Neat. DE10-Nano link not working
[removed]
So far, it has been mostly Stephanie Weirich and Richard Eisenberg working on this. If you check out Weirich's website, she has a [list of publications](https://www.cis.upenn.edu/~sweirich/publications.html), and the '''A Role for Dependent Types in Haskell (Extended Version)''' is listed as "in progress", but the draft is [available on arxiv](https://arxiv.org/abs/1803.06960).
[removed]
It doesn't need an equivalent (in your case). Do-notation is not the same as a Computation Expression. Computation Expressions can emulate Haskell's do-notation, but they can also be used for wildly different things where Haskell's do-notation isn't applicable (at least not without abusing rebindable syntax). The type of `ReturnFrom` in Haskell would be `com a -&gt; com a` which could be (based on the description in F# documentation to "realize the value and then wrap" ```haskell returnFrom :: (Monad m) =&gt; m a -&gt; m a returnFrom x = x &gt;&gt;= return ``` which, assuming that all Monads are following the Monad laws (something F# *can't* assume, but Haskell programmers generally assumme) gives us ```haskell returnFrom :: (Monad m) =&gt; m a -&gt; m a returnFrom x = x ``` This function isn't very useful - it's just the identity function. Which brings us to your example ```haskell example x = case x of A -&gt; ... B c -&gt; do f &lt;- ... let newX = ... example newX ```
It works well, thank you! The solution is simpler than I could have imagined.
&gt; which, assuming that all Monads are following the Monad laws (something F# can't assume, but Haskell programmers generally assumme) gives us Just a quick add to an excellent comment: In F# you can usually assume this if the CE you’re working with satisfies the monad laws. Although it’s certainly possible to even write one that violates all three, generally people can write code with “normal” CEs that are proper monads. I think the bigger difference in assumptions is impurity, which certainly affects this.
Getting started: install Nix That's got to set some kind of record for being the worst advice to have ever been given to a newcomer.
Nothing wrong with `nix-shell -p` for a beginner. And the expression to add packages is a little wonky (wish there was a better interface available), but it's simple and works great. So I'd say the advice in the article isn't gonna hurt anyone and the environment is 100% compatible with using ghc directly and cabal, which most okpine tutorials are gonna use.
[Lol.](https://i.vgy.me/hzW8Gy.jpg)
Tensorflow is fairly low level. Use Kerad in python and you will write similar code as in julia.
Both Cabal and Stack have an ability to load REPL with the specified Haskell packages in the environment without need to install a separate build tool. No need to introduce another level of indirection for beginners.
Hey /r/the\-coot I cannot find an RSS feed for your blog which I'm trying to follow. &amp;#x200B; Also might be cool for beginners like me to have some info about how packages are related. Your free-algebras and free-categories have some overlap with e.g. functor-combinators and recursion-schemes but it's sometimes difficult to figure out how to translate, at least for me. ;-)
Stack is a separate build tool
Stop guys, you've almost convinced me to use Haskell instead of Elixir for my next major products back end API (with a Vue.js SSR front-end). I'm scared to commit to Haskell though. I fear I'll be caught up in the weeds of programming languages and not get anything useful released.
A lot of packages are broken, though. But yeah, otherwise works great for trying out things in a REPL.
Yeah, same question as /u/ocramz . I want to be one of the applicants you don't know what to do with. What's the company? What state are you in? PM me, if you would.
Define foldable.
Informally, something that can be an argument to *reduce* (*fold*).
Well, the issue is that a reduce function is just a different recursion scheme. Thus, all foldable types are inductive, and, without dependent types, recursive. So the answer to your question is yes, but it's kinda informal.
Stack is as much a separate build tool as Nix. Both of them make use of Cabal anyway, instead of reinventing it. Choosing between Stack and Nix is more of a personal preference, and I wish people stopped scaring Haskell newcomers away from using Nix.
Thanks. Can you elaborate a bit more on "a reduce function is just a different recursion scheme"? I understand that any solution using *reduce* can be rewritten recursively. Is that enough to conclude that reduce is a recursion scheme?
If you don't want to specify the compiler version the command can be simplified further into: nix-shell -p "haskellPackages.ghcWithPackages (ps: with ps; [split rosezipper])"
I'm not sure if this answers your question, but there are structures like `Vector`, for allowing random access. `Vector` implements type-class `Foldable`, while not being recursive. However, one could argue such pointer trickery doesn't count.
So, every (inductive) type has a recursion scheme defined by its structure. Take the natural numbers, for example: `data Nat = Z | S Nat`. Its recursion scheme suffices to express any terminating function on the naturals, so it has the type ``` recNat :: a -&gt; (Nat -&gt; a -&gt; a) -&gt; Nat -&gt; a ``` ``` recNat base func Z = base ``` ``` recNat base func (S x) = func x (recNat base func x) ``` , where as the fold has the same power, however, just has the type `a -&gt; (a -&gt; a) -&gt; Nat -&gt; a`. Things with a fold are definitely recursive, at least within a broad definition of this term.
Per se, it is recursive in an inductive way.
If you define "are foldable" as in you can make them a valid instance of the Foldable typeclass I would say no; i.e. take something like data Single a = Single a instance Foldable Single where foldMap f (Single x) = f x is perfectly valid, and not recursive. Admittedly, for such a "boring" type; no recursion is needed....
What might count is the questionable `Foldable` instance for `(x,a)`: &gt; foldr (+) 0 (1,2) 2
Ignore the shiny chasers and you'll be fine.
Do you mean when using Haskell or when choosing which language to use? I have greater risk tolernce for software decisions for my personal projects which this will be.
I am working on a library called implicit-effects \[1\] that uses the same pattern as you described at the base. My library goes a bit further and make use of type families and constraint kinds to shield the implicit parameters from the users. &amp;#x200B; I am near in completing implicit-effects for first release, and it is currently a bit lacking on documentation. Hopefully I can share more about the techniques used in the near future. &amp;#x200B; \[1\] [https://github.com/maybevoid/implicit-effects](https://github.com/maybevoid/implicit-effects)
When using Haskell. Lots of people use Haskell to push the edges of types software development and type theory, but there are also lots of people using relatively simple Haskell techniques to be very productive.
Maybe is Foldable. http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Foldable.html
All of the top level answers I'm seeing are kind of coming at the same right answer from a different angle. &amp;#x200B; You can, of course, make an example of a data structure that is not recursive but implements foldable as /u/Noinia did so simply. But you might respond that that's not an "real" or "interesting" instance of foldable. Which begs the question, what's a real or interesting instance of foldable. The answer may very well be that interesting instances of foldable are basically the recursive ones. Which is, I think, what /u/[megamanisepic](https://www.reddit.com/user/megamanisepic/) was getting at. But maybe not, maybe there's something in between which is Vector as /u/stomir was getting at. But even that is kind of recursive so... *\*shrug\** &amp;#x200B; TL;DR: if you're not assuming that only recursive folds are interesting then no (but the counterexamples may seem boring). If you think only recursive folds are interesting (which you probably should) then yes. In either case, the answer is kind of disappointing. That's probably why you're not gaining traction for your question on stackexchange.
Thank you, I used lts-13.16 and was careless enough to ignore `stack build`'s warning This package indirectly depends on multiple versions of the same package. This is very likely to cause a compile failure. package io-streams (io-streams-1.5.1.0-4fKzQV1ZBEG9tLi3KX3yXI) requires ... Changing resolver to lts-13.26 solves the issue.
how about the rls and other similar plugins for editor integration and stuff running in the container and sshing is nice if you know what youre doing but for me as a beginner its really difficult to every time manually compile things. Without the assistance of my ide
Company is distributed by default. My colleagues are in London, Moscow, and Siberia. I am in Poland, though I spent the past year living out of a suitcase and travelling the world.
Sent!
To work with Haskell you need a build tool. Word Cabal is ambiguous. It means either library from Hackage for passing files with the `.cabal` extension or the `cabal` build tool. Both `cabal` and `stack` build tools are using Cabal library. But `stack` doesn't require to have `cabal` installed. With Nix however you need one more build tool except Nix itself. I'm using Haskell in production for 4 years already and I'm completely fine with either cabal or stack. My point here is that Nix has much worse documentation than cabal or stack. Only small subset of Haskell developers knows Nix. If you have troubles with Nix and Haskell you have less chances to get help rather using plain cabal or stack. Also, because Nix is more general, it's also more complex. Why spend much more time on learning build tools instead of learning the language?
Thanks, that's a very good summary.
With most monads you shouldn't really run into this problem since you can just implement your own type with different names, eg `data Option a = Some a | None` instead of Maybe/Just/Nothing ... but here you're tripped up by list comprehension syntax which requires a list. For the list cases I would just try to break the process into three explicit steps: unwrapping the newtypes, implementing the core member logic, and rewrapping. So in your example case, something like this: instance Monad ListMonad where (&gt;&gt;=) :: forall a b. ListMonad a -&gt; (a -&gt; ListMonad b) -&gt; ListMonad b xs_ &gt;&gt;= f_ = ListMonad [y | x &lt;- xs, y &lt;- f x] where xs = unwrap xs_ f = fmap unwrap f_ unwrap (ListMonad x) = x
beautiful, thanks
There are two mistakes in your code: The second equation of `(&gt;&gt;=)` returns a list instead of a `ListMonad` and then there is the problem of `g x` being a `ListMonad` and not a list. The first one is easily fixed by just adding the constructor, for the second one you need to do a pattern match. For example: ListMonad xs &gt;&gt;= g = ListMonad [y | x &lt;- xs, let ListMonad ys = g x, y &lt;- ys] Also note that the `ListMonad [] &gt;&gt;= _ = ListMonad []` is not necessary, the list-comprehension will take care of that.
`data Five a = Five a a a a a` is an example of a non-recursive inductive type
This. I can agree that "Install NixOS" might be a different story .... (Source: NixOS user)
That's interesting! It is suspicious that compiled `arithmoi` becomes three times slower.
There's not more to it than this. You need to think about the context in which Gary Bernhardt came upon this idea. He spent a lot of time doing TDD in OOP Ruby/Python land, with lots of mocks, stubs, and mutation. He eventually came to realise that programs are easier to test and understand if the bulk of the computation happens in pure functions. This is of course just the normal approach in Haskell world, which is why it seems obvious to us.
I don't have RSS feed, but I usually post on twitter (and sometimes also Reddit) when I post STH. Likely there is some overlap, I haven't explored that yet...
Couldn't you just rewrite the typeclass? As far as I remember, the Typeclassopedia includes the definitions of each typeclass; you could just copy those over, change the name `Monad` to `Monad'` (say), and then write instances for `Monad'` instead of `Monad`.
I have personally become very sceptical of HTTP2. It seems nobody is able to implement it correctly. I have personally found bugs in nginx, Firefox and likely warp that show that their HTTP2 implementations are incorrect, and they are still not fixed: * https://trac.nginx.org/nginx/ticket/1250#comment:4 * https://github.com/yesodweb/wai/issues/673 * https://bugzilla.mozilla.org/show_bug.cgi?id=1540574 node.js suffers from it as well: * https://github.com/nodejs/node/issues/12339#issuecomment-368124078 So let me critically ask: If not even NGINX and Firefox can implement HTTP2 correctly, what is the chance that we can get working QUIC implementations given that QUIC is significantly more complex than HTTP2?
No Windows support is a great way to welcome newcomers.
Update: issue number 2 here (about `stty`) has just been reported at [https://github.com/commercialhaskell/stack/issues/4901](https://github.com/commercialhaskell/stack/issues/4901).
I don't have a problem with teaching new Haskellers nix. I do have problem with bundling it into an introduction to the language. That's a bridge too far, and it's a bad idea for a few reasons. The big, obvious one, is added complexity for no perceivable benefit (given the simplicity of introductory material). Additionally, there's a perception issue: you're being told using a very complex tool is a good idea for getting started with this new language that already has a reputation of incredible complexity. That's going to turn people off. And finally ( and this is my real beef, and it's one that stack has too, to a lesser degree), it's just a bad practice to not begin with fundamentals. You should learn how to use cabal first, before you learn to use something that wraps it's functionality. Because if you don't, you'll be missing context. None of this at all applies to Haskellers that are simply new - if you can add a package to a cabal file and install it locally and globally, and you've done it at least once, cool, good, done, skip to using nix if you want to. I have no problem with that, and depending on what material you're writing, it could be a wonderful idea. But as a day one, install your very first package maneuver? No, that's not a good plan.
Thanks, I've fixed the link now.
Yay Rashad; the font-style choice is \*beautiful\* (it looks like a screenplay font, which makes the post very personal, like a diary/ journal entry). Enjoyed reading about what you're up to; please keep us posted. Also love the footnotes and the highlighted text and am looking forward to reading about the topics posted!
The whole app's dependency footprint is so large :joy:
I sorely, sorely wish the app I work on had that few dependencies 😂
metoo, but it's hard for Haskell ecosystem 😂
This is a weird blog post all around, but I'm especially confused by `Counter`. Why pass around so many effectful objects, each containing their own copies of the same functions? I would think it would be much simpler and probably more efficient to use a class. A direct translation of your system would be something like class Counter m a where incrCount :: a -&gt; m () getCount :: a -&gt; m Int decrCount :: a -&gt; m () instance Counter IO (IORef Int) where incrCount x = modifyIORef' x (+1) getCount x = readIORef x decrCount x = modifyIORef' x (`subtract` 1) or as above, but with a newtype wrapper around `IORef Int`. No storing multiple copies of functions in memory, and no smart constructors needed.
Could you also use `MonadState` to keep track of the count?
I'm not an expert. What I shared are just my lessons learned as per the title, but let me try to answer your questions. &gt; I would think it would be much simpler and probably more efficient to use a class. I'm against defining lawless typeclasses with potentially incoherent instances (it's said that we need two different interpreters, for production and test). A record of functions is a saner alternative IMHO.
AFAIK `MonadState` can't deal with concurrency. `StateT` is inherently sequential.
Damn, I really don't know what to say either :smile:
MonadState is just a class; instances can be as concurrent or non-concurrent as they want, cf. [Control.Monad.State.Concurrent](http://hackage.haskell.org/package/concurrent-state-0.6.0.0/docs/Control-Monad-State-Concurrent-Strict.html). Nothing inherent about it. Classes aren't interfaces, and Haskell isn't an object-oriented language; don't worry, everyone takes time to adjust. Anyway, yes, `Counter` could be scrapped entirely and replaced by `MonadState Int` because `incrCount` is `modify' (+1)`, `decrCount` is `modify' (\`subtract\` 1)`, and `getCount` is `get`. By the way, IORefs aren't intended for concurrency. They're often unsafe and inefficient. It's probably a better idea to use a [TVar](https://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Conc.html#v:newTVarIO).
Small nitpick, but you’re using orphan instances for the JSON codecs [here](https://github.com/gvolpe/exchange-rates/blob/master/src/Http/Client/Forex.hs) which isn’t a great practice given that both data types are under your control. Consider moving those instances into the same module that contains the data types they’re attached to.
`MonadState` is just a class; instances can be as concurrent or non-concurrent as they want, cf. [Control.Monad.State.Concurrent](http://hackage.haskell.org/package/concurrent-state-0.6.0.0/docs/Control-Monad-State-Concurrent-Strict.html). Nothing inherent about it. Classes aren't interfaces, and Haskell isn't an object-oriented language; don't worry, everyone takes time to adjust. Anyway, yes, `Counter` could be scrapped entirely and replaced by `MonadState Int` because `incrCount` is `modify' (+1)`, `decrCount` is `modify' (`\``subtract`\` `1)`, and `getCount` is plain old `get`. By the way, IORefs aren't intended for concurrency. They're often unsafe and inefficient. It's probably a better idea to use a [TVar](https://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Conc.html#v:newTVarIO).
Foldable doesn't imply that the data structure is self-referential. Maybe, (,) e, Identity, Proxy, Either e, Vector, V3, Vec n, are all `Foldable`, in that you can meaningfully enumerate all of the occurrences of their final type argument in a left-to-right fashion. This does not comply with that notion trotted out by Wikipedia about the idea of reducible things.
&gt;I mentioned that we need two different interpreters, one for production and one for test An excellent use for classes! &gt;lawless typeclasses It comes with plenty, such as: "incrCount -&gt; decrCount -&gt; getCount == getCount". Pretty much all mathematical laws, actually, since it's really just a wrapper for an Int and you can do Peano stuff with incrCount and so on. &gt;potentially incoherent instances As long as you don't enable the IncoherentInstances pragma (which you should never do!), GHC will make sure this doesn't happen. It will also warn you against making orphan instances (which you should almost never do!) so you really don't have to worry about this. &gt;A record of functions is a saner alternative IMHO. If it's purity and lawfulness you're concerned about, functions defined once at the top level are certainly much better than tons of (hopefully identical, but not guaranteed) functions being passed around as references. That's why you basically never see records of functions in published Haskell code. (I actually can't recall ever seeing them, although surely there must be some examples somewhere.) Also because Haskell isn't an object-oriented language, which took me a while to adjust to as well. Anyway, you really don't have to worry about more advanced pitfalls mentioned by other writers while you're still adjusting: Haskell is pretty good at preventing problems, so it's better to trust the tools it gives you than to stick OOC or something else on top of it. Just don't use Data.Default and you'll be fine :p
While I applaud all efforts to improve Haskell's reputation and make it more approachable, I can't help but notice that this is another academia-focused presentation of Haskell. There is nothing about how to structure your code (think RIO, effects), how to deploy it, how to avoid common performance pitfalls, or anything like that. It reads more like a "Why Haskell is a cool hobby language" rather than a "Haskell is a general purpose language you can use at work" kind of post.
Thank you for the writeup. I personally find RIO too thick and bloated. May I suggest freer-simple, fused-effects, or polysemy. Polysemy is the latest and arguably the most ergonomic of the algebraic effects library.
Anything that van het converted to a list.
I will defend the author; Using a record is gives you more control. With a class instance based on a monad you say that for each monad there is only one way of doing a `Counter`, which is not true. You might need counters with different behaviour, and some special ones for testing (e.g. a counter that starts at some large `Int`, ...). With class based approach you will be carrying as many copies of `Counter`, but now it will be passed implicitly rather than explicitly, so the memory footprint will not differ that much.
&gt; By the way, IORefs aren't intended for concurrency. They're often unsafe and inefficient. It's probably a better idea to use a TVar. For isolated counters that not need to be "coordinated", `IORefs` would be ok.
&gt;By the way, IORefs aren't intended for concurrency. They're often unsafe and inefficient. It's probably a better idea to use a [TVar](https://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Conc.html#v:newTVarIO). &amp;#x200B; This is not entirely true. IORefs have their atomicModifyIORef sort of functions (which the author uses) and these are designed for concurrent access by using primitive CAS instructions. And for a simple thing like a counter this is enough and much faster than a TVar (have a look at Simon Marlowe's Parallel and Concurrent Programming in Haskell book) as they don't have to manage a transaction log. For more complicated things (especially with transactions), a TVar is of course the more appropriate data type.
Thank you!
&gt; It comes with plenty, such as: "incrCount -&gt; decrCount -&gt; getCount == getCount". This does not hold if the counters are accessed concurrently. The left hand side has an extra observable behavior where the variable could be read by another thread before it's decremented back.
Nitpicky nitpick: `Typeable` doesn't need a `deriving` clause, the compiler derives it for all types already.
The post is so vague that the same person might agree with one interpretation and disagree with another 😐. Quality requires _deliberate_ practice, just practice by itself is not very meaningful.
It would be super-useful to change that code to create deterministic unique ids. These IDs (used to?) get into the final binary making GHC a non-deterministic compiler, unlike for example GCC.
That's also the state for the most popular programming language in the world. I don't know if this needs to be addressed in THE standard.
From the last year, here are the big updates I'm aware of: - Ningning Xie [implemented coercion quantification](https://xnning.github.io/papers/gsoc-report.pdf) in GHC Core. - Ryan Scott implemented a user-facing syntax for [visible dependent quantification](https://ryanglscott.github.io/2019/03/15/visible-dependent-quantification-in-haskell/)
You can follow this particular blog about the issue-wanted GSoC project for more details on topics you're interested ;) https://www.reddit.com/r/haskell/comments/c3tjaw/first_blog_post_on_google_summer_of_code_project/
Interesting, more interesting to me is also how does one end up adding a deriving they don't need. Is it a victim of copy paste town
Obviously no. The key point that makes quantity work is "learn from your mistakes". The Haskell community is all about this- learning from our own mistakes, and learning from the mistakes of others. And also learning from successes, both our own and others. IMHO, it's the other languages that have the attitude of "fifty pounds of butt-ugly misshapen pots coming right up!"
It isn't. An individual programmer can fall into a particular group; a tool can't. If you write lots of code using haskell, you will be in the quantity group. If you sit on your ass and theorize about the perfect haskell program instead of actually trying to write haskell programs, you will be in the quality group.
You're right, will fix soon, thanks Joe! (Glad to see you here too :) )
TIL, thanks! Will update the code :)
Wait whaaaat 🤯? Since when is this true? I've been writing DeriveDataTypeable all the time just out of habit! Also, the Typeable page doesn't mention this at all: https://hackage.haskell.org/package/base-4.12.0.0/docs/Type-Reflection.html
&gt; IMHO, it's the other languages that have the attitude of "fifty pounds of butt-ugly misshapen pots coming right up!" I don’t think this is fair, and I think it reinforces the idea that Haskell programmers are all elitists. Plenty of programmers in other communities, even working with languages like Java, Python, or JavaScript, are also doing their best to learn from their mistakes and engineer the best software with the tools they have available. Besides, not everyone prioritizes the same things Haskell programmers do, and that isn’t enough to declare them wrong. It’s true that most people with the attitude you describe are in language communities other than Haskell, but of course they will be, since the vast majority of programmers don’t use Haskell, and the fact that Haskell’s off the beaten path means it will likely only be found by those actively seeking to learn.
I think sometimes the community gives that perception because there is a large contingent of academic or hobbyist Haskellers who are interested, specifically, in advancing the state of the art. &amp;#x200B; However, these people produce actual code, frequently, and more significantly, they produce (less frequently, but frequently enough to matter) real, quality libraries or other work that demonstrates the techniques they've discovered. &amp;#x200B; The rest of the community will often spend some amount of time testing out these results, sometimes with toy projects, other times at real scale, and pontificate/discuss about which methodologies they found more useful. Again, that results in a lot of chatter that doesn't seem like code, but note that before we all got together to talk about our experiences, again, we wrote code to flex concepts. &amp;#x200B; So, no, we're not in the 'quality' group - We just have a somewhat unusual focus on producing quantity for the express purpose of increasing quality. Which, to my mind, implies quite strongly that the Haskell community has a very healthy understanding of how to use this concept to better it's own collective situation, and not the reverse.
Thanks for your insights! &gt; An excellent use for classes! How do you deal with different instances for the same class without introducing newtypes? &gt; It comes with plenty, such as: "incrCount -&gt; decrCount -&gt; getCount == getCount". Pretty much all mathematical laws Unfortunately this doesn't hold in concurrent scenarios I think. Also that interface is different from the one in the project, there's `resetCount` but not `decrCount`. &gt; That's why you basically never see records of functions in published Haskell code. Interesting. I thought it was more widely adapted! It feels more natural to me coming from Scala but that might just be me and the Scalasisms that I need to get rid of :D &gt; Anyway, you really don't have to worry about more advanced pitfalls mentioned by other writers while you're still adjusting: Haskell is pretty good at preventing problems, so it's better to trust the tools it gives you than to stick OOC or something else on top of it. Just don't use Data.Default and you'll be fine :p Thanks again!
&gt; MonadState is just a class; instances can be as concurrent or non-concurrent as they want, cf. Control.Monad.State.Concurrent Thanks! I know `MonadState` is a class but I thought the only way to instantiate it was using `StateT`. TIL about the concurrent variant!
&gt; An individual programmer can fall into a particular group; a tool can't. I agree with the bulk of this for sure, but tools don’t spontaneously spring forth into existence—people make them. And I think it’s fair to talk about the people making and steering Haskell and its ecosystem as a group, even if it’s not really homogenous. I think a better response to this question is to acknowledge all the ways Haskell *has* learned from what people generally agree were mistakes, repeatedly. The language has evolved: we have ejected features that didn’t work (n+k patterns and impredicative polymorphism, just to name two) and fixed warts in the standard library (the Functor-Applicative-Monad hierarchy, Foldable and Traversable in the Prelude, etc.). But it’s true that we often wait to fix things until we think we know how to do it “properly,” such is the attitude of a research language. We pulled `join` out of the `Monad` class because it didn’t play nice with GND without being unsafe. We should be able to put it back now that we have quantified constraints, but it’s a slow process. The Haskell community *does* learn and iterate, but it does do so in a qualitatively different way from, say, the JavaScript community.
"" could sensibly correspond to `Nothing` or it could correspond to `Just ""`. The first one makes more sense because the semigroup instances line up correctly. I don't know if this is the actual reason though.
Thanks! I found `RIO` and the `ReaderT` pattern quite nice and easy to introduce (for now). I just don't think I'm experienced enough to introduce other effects at work without having a guide. FWIW I've playing around with a lot of these effects and I like `polysemy` the most (which is also mentioned in my blog post): https://github.com/gvolpe/effects-playground
I think ultimately then, your real question is "is there a algebraic datatype that can hold an arbitrary number of discrete elements that is not self-recursive" - I think the answer to that is probably 'no', but it is possible to encode something like a list using two separate types that refer to each other instead of referring to themselves. I don't think you could define a foldable instance for that encoding in Haskell, but it's still certainly capable of supporting folds
If you have such IsString instance for Maybe and you pass string literal to a function that takes Text, this means that after changing the type of an argument from Text to `Maybe Text` your code still will be compiled without any errors. But now it potentially can contain some semantic or behavior error. And this means that refactoring becomes less safe.
Thanks so much for sharing. I’ve never dug through a non-trivial CLaSH project.
You might as well be asking whether a hammer falls into the "use the pointy end" group. Sometimes quality is king. Sometimes quantity will get you where you need to go, in the end. There is no right answer and no best strategy.
Doesn’t hie support cabal.project files? Recent versions of `reflex-platform` include hie in the `prjoect` derivations. (Maybe in shells.ghc.hie or something?). Form there support should just require that your editor try to launch the hie included in the shell.
Unfortunately stack is [ending support](https://github.com/commercialhaskell/stack/issues/4086) for GHCJS, so you're probably out of luck with respect to intero working on a reflex project in the future, if it does not work now. The folks at Obsidian created [obelisk](https://github.com/obsidiansystems/obelisk/) to help bootstrap your reflex projects, providing rapid `ghcid`-style feedback as a part of the program. With HIE, I'm not sure. The project looks great, but I can't help there. It may be worth trying a different workflow and seeing if you like it!
That's kind of the point, to be able to compile without errors. I however agree with the danger of the "wrong" semantic, even though there is only a real problem for "" (as u/theindigamer pointed out)
I'm toward \`""\` =&gt; \`Just ""\` (which corresponds to my implementation above), but I agree it is matter of debate.
That page could definitely use more documentation :) The GHC manual mentions it: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#deriving-typeable-instances &gt; Derived instances of Typeable may be declared if the DeriveDataTypeable extension is enabled, but they are ignored, and they may be reported as an error in a later version of the compiler. I think "deriving" gives the wrong idea that it's automating something that could be done manually. But `Typeable` has a special status (like `Coercible`); it should be considered as a language primitive that isn't possible to express otherwise.
As interpreting `Nothing` and `Just ""` as the same thing by default in `toString` makes me nervous for some vague reason, may I suggest... foldToString :: (Foldable t, IsString s) =&gt; t s -&gt; s -- Not in the library AFAIK foldToString = foldMap toString I'm assuming that avoiding a `Monoid s` constraint by using `foldMap toString` rather than `toString . fold` is reasonable - even replacing `Maybe Text` with a huge `[Text]`, I'd assume it's generally better not to build a single combined `Text`.
While hie does currently not support v2-build, people are actively working on that feature in various projects and gsoc 2019. AFAIK, the plan is to integrate v2-build ASAP, maybe even after this summer already!
You're probably gonna wait till at least fall to hear anything. People dont necessarily think about interns a year out
Yeah it appears that way. I was hoping that maybe some internships happen every year, that I don't know about.
Does joint fusion apply for \`foo\`? Since it is very similar to \`find\`, I would expect that the answer is yes, but the rewrite rules from the paper \*Compiling without continuations\* were too concise for me, to be understandable. \`\`\`haskell find cond = go where go \[\] = Nothing go (x:xs) | cond x = Just x | True = go xs &amp;#x200B; any cond xs = case find cond xs of Nothing -&gt; False Just \_ -&gt; True &amp;#x200B; map fn = go where go \[ \] = \[\] go (x:xs) = fn x : go xs &amp;#x200B; foo xs = case map (+1) xs of \[ \] -&gt; Nothing (x:\_) -&gt; x \`\`\`
Yes, the reason is that it's completely arbitrary and we don't add instances just to make changing the types of some of your functions easier!
Feel free to do it in your code. Yes, you will have an orphan instance, but if it's fine in your domain to conflate `Nothing` and `Just ""`, then go for it.
&gt;How do you deal with different instances for the same class without introducing newtypes? By not writing orphan instances, which the compiler should warn you about (as will other people :p). This means that there are two options for declaring an instance for the class. In the module that defines the class, you can declare as many instances as you want. In any other module, you can only declare instances for newtypes and data that are defined in that module. Even if you do write orphan instances (which is almost never a good idea), the compiler will give you a "Duplicate instance declarations" error if a module ends up with two different declarations for the same type. &gt;Unfortunately this doesn't hold in concurrent scenarios I think. Well, those steps might be called across different threads—and this is why [TVars](https://hackage.haskell.org/package/stm-2.5.0.0/docs/Control-Concurrent-STM-TVar.html#v:TVar) are great! They come with an [atomically](https://hackage.haskell.org/package/stm-2.5.0.0/docs/Control-Monad-STM.html#atomically) monad, which ensures that everything inside it happens as a single transaction. So if you wanted to have a rule "resetCount -&gt; incr -&gt; resetCount == resetCount": class Counter m a where incr :: a -&gt; m () getCount :: a -&gt; m Int resetCount :: a -&gt; m () instance Counter IO (IORef Int) where incr x = modifyIORef' x (+1) getCount = readIORef resetCount x = writeIORef x 0 instance Counter STM (TVar Int) where incr x = modifyTVar' x (+1) getCount = readTVar resetCount x = writeTVar x 0 checkSameValueAfterResetLaw :: (Monad m, Counter m a) =&gt; a -&gt; m Bool checkSameValueAfterResetLaw counter = do resetCount counter valueBefore &lt;- getCount counter incr counter resetCount counter valueAfter &lt;- getCount counter return $ valueBefore == valueAfter In a non-concurrent situation, both `IORef Int` and `TVar Int` always satisfy that law. In a concurrent situation, only `TVar Int` is guaranteed to satisfy it. This is actually pretty cool: by having that law, you have also implicitly stated that TVars, but not IORefs, satisfy the laws of the Counter class with concurrency! On the other hand, if you don't think Counter needs a law like that, then IORefs are just as good. So this is a great example of why laws help define classes. &gt;Interesting. I thought it was more widely adapted! Something I've often found helpful is that any time I'm not totally sure what a function from a library does, I take a look at the source code on Stackage. That way I know exactly what I'm dealing with, and I also see how other people write idiomatic Haskell code. (Except for lenses. That stuff freaks me out.) &gt;It feels more natural to me coming from Scala but that might just be me and the Scalaisms that I need to get rid of :D Ah, that makes sense! I've been meaning to try out Scala—the fusion of OOC and FP seems really interesting. And yeah, it always takes time to adjust. I started out treating it like a math program and filling it with operators like ∈ and ∩ and ∧ and my code was basically gobbledygook. (Don't tell anyone, but I still use ∀ instead of forall for personal projects. It's just so hard to let go...)
Haskell is more expressive than other languages, so you can produce a lot more stuff with less code.
That's kind of the point of strong types, to not compile if there's a risk of wrong semantics :)
Out of curiosity, could one call this a recursion with nothing more than a base case?
My company is going to be doing a lot of data science/ML stuff in 2020. We have haskell interns every summer. If you're interested, send me your resume at [dcartwright@layer3com.com](mailto:dcartwright@layer3com.com). I'll take a look over it and mention it to my boss. Perhaps around Februrary 2020, you could ping me about the internship again via email.
I would say that a function is recursive if (and only if) it calls itself at least once in its definition (possibly after expanding terms in the definition). If you say that functions are also recursive if its definition contains zero calls to itself then I think any function is recursive (and hence being recursive would not mean anything).
Chromium reports that your HTTPS certificate is invalid.
&gt; By not writing orphan instances, which the compiler should warn you about (as will other people :p). I need two or more different instances for testing. I don't think this answers the question :/ Can you expand on the idea? An example would be great! &gt; In the module that defines the class, you can declare as many instances as you want AFAIK you can't write `instance Counter IO where (prod behavior)` and `instance Counter IO where (test1 behavior)`, even in the same module. I think you meant "as many instances for different effects" but this is not what I was looking for that's why I chose to use a record of functions. &gt; this is why TVars are great! They come with an atomically monad, which ensures that everything inside it happens as a single transaction. That's pretty cool stuff, thanks for sharing!
I came to a solution much like this one, back when I was struggling with it. I also didn't like the fact that the type parameter did not really specify what the possible constructors are. I've played around with this idea again, specifically to 'solve' this and finally arrived at this code. It was inspired by the libraries for Free etc. and encodes within a single type argument which constructors are available and in in what 'flavour'. I specified some helper functions for removing or replacing a constructor, which involve unsafeCoerce which gives me an incredibly icky feeling as I'm not sure I understand unsafeCoerce fully, but as it is only used to reorder types around in a type level list, and the order is not **really** 'important'^1, I think it is more or les safe (with a footnote that it actually might not be). The code specifies an expression language with Vars, If expressions and Idents. Code is provided to upgrade Vars such that they use qualified names, remove idents and evaluate formulas containing If and (un)upgraded Vars. Edit the `truthtable` and evaluate `example` and `example1` to see it in action. Comments would be greatly appreciated. {-# LANGUAGE RankNTypes #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE UndecidableInstances #-} -- {-# LANGUAGE GADTs #-} -- {-# LANGUAGE AllowAmbiguousTypes #-} -- {-# LANGUAGE ScopedTypeVariables #-} import Data.Type.List import Unsafe.Coerce newtype QualifiedName = MkQual String deriving Show type family Var (ix :: [ExprFlavours] ) :: * where Var ('VarConstr 'VarString ': xs) = String Var ('VarConstr 'VarQualified ': xs) = QualifiedName Var (y ': xs) = Var xs data Expr (cs :: [ExprFlavours]) = Var (Var cs) | (Find 'IfConstr cs ~ 'True) =&gt; If (Expr cs) (Expr cs) (Expr cs) | (Find 'IdentConstr cs ~ 'True) =&gt; Identifier QualifiedName data VarFlavour = VarString | VarQualified data ExprFlavours = IfConstr | IdentConstr | VarConstr VarFlavour -- Two example formulas form = If (Var "a") (Var "b") (Var "c") :: Expr ('[ 'IfConstr, 'VarConstr 'VarString ]) form2 = If (Var "a") (Var "b") (Identifier $ MkQual "c") :: Expr ('[ 'IfConstr, 'IdentConstr, 'VarConstr 'VarString ]) type family MoveToFront (a :: ExprFlavours) (ls :: [ExprFlavours]) where MoveToFront a '[] = (a ': '[]) MoveToFront a (a ': xs ) = (a ': xs) MoveToFront a (x ': xs ) = Insert x ( MoveToFront a xs ) moveToFront :: (Find x cs ~ 'True, MoveToFront x cs ~ (x ': cs')) =&gt; Expr cs -&gt; Expr (x ': cs') moveToFront = unsafeCoerce removeConstructor :: forall constr ls ls' . (Find constr ls ~ 'True, MoveToFront constr ls ~ (constr : ls')) =&gt; (Expr (constr ': ls') -&gt; Expr ls') -&gt; Expr ls -&gt; Expr ls' removeConstructor f = f . unsafeCoerce replaceConstructor :: forall constr ls ls' constr' . (Find constr ls ~ 'True, MoveToFront constr ls ~ (constr : ls')) =&gt; (Expr (constr ': ls') -&gt; Expr (constr' ': ls')) -&gt; Expr ls -&gt; Expr (constr' ': ls') replaceConstructor f = f . unsafeCoerce qualifyNames :: (String -&gt; QualifiedName) -&gt; Expr ('VarConstr 'VarString ': cs) -&gt; Expr ('VarConstr 'VarQualified ': cs) qualifyNames qualify (Var s) = Var (qualify s) qualifyNames q (Identifier n) = Identifier n qualifyNames q (If c l r) = If (qualifyNames q c) (qualifyNames q l) (qualifyNames q r) removeIdents' :: (Var cs ~ Var '[ 'VarConstr 'VarQualified ]) =&gt; Expr (IdentConstr ': cs) -&gt; Expr cs removeIdents' (Identifier q) = Var q removeIdents' (Var s) = Var s removeIdents' (If c l r) = If (removeIdents' c) (removeIdents' l) (removeIdents' r) eval :: (Find 'IfConstr cs ~ 'True, Find IdentConstr cs ~ 'False) =&gt; (Var cs -&gt; Bool) -&gt; Expr cs -&gt; Var cs eval tt (Var s) = s eval tt (If c l r) = if tt (eval tt c) then eval tt l else eval tt r truthtable :: String -&gt; Bool truthtable "a" = True truthtable _ = False truthtableQ :: QualifiedName -&gt; Bool truthtableQ (MkQual s) = truthtable s example = eval truthtable $ form example1 = eval truthtableQ $ removeConstructor (removeIdents') $ replaceConstructor (qualifyNames (MkQual)) $ form2 ^1: The order might be important if you encode flavours for constructors, such as in the Var constructor... The type it expects is decided by the *first* `'VarConstr` it finds, which is not guaranteed to be the only one! I'm disregarding the issue here because: 1. I don't actually like encoding the flavour, and think you should introduce separate constructors. But I see the potential and wanted to include it because the original post did allow for it. 2. It might be solvable, and reddit might be able to tell me how! I'm imagining a constraint expressing that a `'VarConstr` is found, and that no other `'VarConstr` are found.
&gt;I need two or more different instances for testing. I don't think this answers the question :/ Can you expand on the idea? An example would be great! The code I wrote *is* an example: it's what you would do if you wanted to use TVars for production but IORefs for testing. Then you'd write your functions with the same one I used: `(Monad m, Counter m a) =&gt; a -&gt; m &lt;whatever&gt;`. Once you've done that, you call the same function from both production and testing code; production passes it a TVar and uses STM, and testing passes it an IORef and uses IO. If you wanted to declare an instance in a different file, you'd just use a newtype (e.g. `newtype CounterIO = CounterIO IORef`) and proceed as above, but that's a bit messier. &gt;You can't write `instance Counter IO where (prod behavior)` Ah, I think I see the root cause of your mistakes. If you look closely at the code I wrote above, there are no `instance Counter IO where` declarations. In fact, if you were to try to write that, you'd get a compiler error: "Expecting one more argument to ‘Counter IO’." That's because our class declaration is `class Counter m a`. It has two arguments: `m`, a monad (or monad-like) wrapper, and `a`, the type of the counter. So we have `Counter STM (TVar Int)` (a concurrent variable that runs in the STM atomic monad) and `Counter IO (IORef Int)` (an ordinary variable that runs in the IO monad). Try running the example yourself! You don't have to worry about "AFAIK you can't write"—if it compiles at all (which it does), you can. Here's another example: let's say you want a testing Counter that runs in IO and keeps track of how many times something invokes `getCount` on it. Let's use a record. Because we're defining a new `data` type, this can go in any module—including the same module as the above code, or a different testing module as long as you import the definition of Counter. data TestCounter = TestCounter { testCounterVal :: Int , testCounterGets :: Int } instance Counter IO (IORef TestCounter) where resetCount ref = modifyIORef' ref \x -&gt; x { testCounterVal = 0 } incr ref = modifyIORef' ref \x -&gt; x { testCounterVal = 1 + testCounterVal x } getCount ref = do modifyIORef' ref \x -&gt; x { testCounterGets = 1 + testCounterGets x } testCounterVal &lt;$&gt; readIORef ref If you pass an `IORef` of `TestCounter 1000 0` to the `checkSameValueAfterResetLaw` function, the reference's value afterward will be `TestCounter 0 2`, because the function called `getCount` on it twice. So there you have another Counter instance that runs in IO, and it can measure and test extra stuff on its own! Does that make sense?
&gt;I need two or more different instances for testing. I don't think this answers the question :/ Can you expand on the idea? An example would be great! The code I wrote *is* an example: it's what you would do if you wanted to use TVars for production but IORefs for testing. Then you'd write your functions with the same one I used: `(Monad m, Counter m a) =&gt; a -&gt; m &lt;whatever&gt;`. Once you've done that, you call the same function from both production and testing code; production passes it a TVar and uses STM, and testing passes it an IORef and uses IO. If you wanted to declare an instance in a different file, you'd just use a newtype (e.g. `newtype CounterIO = CounterIO (IORef Int)`) and proceed as above, but that's a bit messier. &gt;You can't write `instance Counter IO where (prod behavior)` Ah, I think I see the root cause of your mistakes. If you look closely at the code I wrote above, there are no `instance Counter IO where` declarations. In fact, if you were to try to write that, you'd get a compiler error: "Expecting one more argument to ‘Counter IO’." That's because our class declaration is `class Counter m a`. It has two arguments: `m`, a monad (or monad-like) wrapper, and `a`, the type of the counter. So we have `Counter STM (TVar Int)` (a concurrent variable that runs in the STM atomic monad) and `Counter IO (IORef Int)` (an ordinary variable that runs in the IO monad). Try running the example yourself! You don't have to worry about "AFAIK you can't write"—if it compiles at all (which it does), you can. Here's another example: let's say you want a testing Counter that runs in IO and keeps track of how many times something invokes `getCount` on it. Let's use a record. Because we're defining a new `data` type, this can go in any module—including the same module as the above code, or a different testing module as long as you import the definition of Counter. data TestCounter = TestCounter { testCounterVal :: Int , testCounterGets :: Int } instance Counter IO (IORef TestCounter) where resetCount ref = modifyIORef' ref \x -&gt; x { testCounterVal = 0 } incr ref = modifyIORef' ref \x -&gt; x { testCounterVal = 1 + testCounterVal x } getCount ref = do modifyIORef' ref \x -&gt; x { testCounterGets = 1 + testCounterGets x } testCounterVal &lt;$&gt; readIORef ref If you pass an `IORef` of `TestCounter 1000 0` to the `checkSameValueAfterResetLaw` function, the reference's value afterward will be `TestCounter 0 2`, because the function called `getCount` on it twice. So there you have another Counter instance that runs in IO, and it can measure and test extra stuff on its own! Does that make sense?
What's a GADT? I haven't been able to find an explanation/tutorial that doesn't go way over my head.
I believe the code comes with the book. If it hasn’t shipped yet, you could cancel the order and place it directly with Manning as they allow access to ebooks immediately when you opt for that medium.
Thanks for that info.
Yeah, but we also have a Foldable instance for `(,) e` which lets `length (1,2)` type check and evaluate to `1`. Types aren't going to be able to remove all semantic errors, or at least, a general-purpose type system won't.
If you can give me a non-recursive representation for the Naturals, I can give you a non-recursive representation for a foldable container.
What's your company's name, out of curiosity? :)
How does this work compare with [Morte](https://hackage.haskell.org/package/morte)? What would be the benefits of modelling the problem with interaction nets instead of CoC?
No, there's no reason not to add it. Haskell typeclasses have an open world assumption, that additional law-abiding instances not already enumerated can exist. `IsString` doesn't have any laws, so adding any sensible instance makes sense. We could propose some reasonable laws for `IsString`: -- fromString converts all strings seq (nf a) x = seq (nf (fromString a)) x -- fromString preserves Monoid fromString (a &lt;&gt; b) = fromString a &lt;&gt; fromString b In which case the instance you propose still makes sense. Don't worry about it conflating `Nothing` with `Just ""` - it precisely doesn't conflate them. One of them can be written as `fromString ""` and the other can't.
hey you might consider posting on news.ycombinator.com there is a jobs thread that gets posted regularly
If your are using emacs, you can use dante. For detail, read this issue. [https://github.com/obsidiansystems/obelisk/issues/184](https://github.com/obsidiansystems/obelisk/issues/184)
Looks interesting, a few questions: How much Haskell experience desired? Is it possible to learn/hone Haskell skills on the job? Why is the company using Haskell, and in what capacity is Haskell used? What is the interview process like?
A GADT is an ADT where you can assign a custom type to every constructor. First, notice that for a regular Algebraic Data Type, the result type (the type to the right of all arrows) of all constructors is the same: data Maybe :: Type -&gt; Type where Nothing :: Maybe a Just :: a -&gt; Maybe a -- Both (Maybe a) data Bool where True :: Bool False :: Bool -- Both Bool In contrast, a GADT can use different return types (as long as they start with the GADT's name): data Perhaps :: Type -&gt; Type where Nope :: Perhaps Void -- not (Perhaps a) !!! This :: a -&gt; Perhaps a data SType :: Type -&gt; Type where SInt :: SType Int SBool :: SType Bool Thanks to that, we can have functions with different types depending on the input: -- f x :: Int or Bool depending on whether x = SInt or SBool f :: SType a -&gt; a f SInt = 0 f SBool = True GADTs can also have constructors with types that don't appear in the result: data Something where Something :: a -&gt; Something or constraints -- Data.Dynamic in base data Dynamic where Dyn :: Typeable a =&gt; a -&gt; Dynamic The common theme is that constructors can carry invariants which are required on construction, and made available by pattern-matching.
Haskell is powering backend services, mostly batch services that create pdf files using the scanned tiff images and the indexing data from sql server. Other haskell services send email notifications, some web applications (intranet). You need to be fluent in haskell. Read the existing code and be able to make changes to it. Most of the code is straightforward, just read some data from sql server, access files on disks, do some operations with them, send emails etc. As long as you know your way around haskell code and haskell toolchain (install and configure your own dev envirnoment, connect to git, fetch the code, push it back) you will be fine. All services are running on linux.
If you haven't already, check out Dante for what is effectively Intero trimmed down and without the tight dependency to stack. Latest versions have built-in obelisk support.
actually does not make sense. \`IsString\` says: this is the way how to treat string as... So, String -&gt; Something and no any Nothing in the side of String world. When you need Nothing you use another "interface", "API", way, etc. I think OP's idea is good.
A prior answer to the gadt question: https://www.reddit.com/r/haskell/comments/bj5s5u/comment/en661kh
&gt; That's because our class declaration is class Counter m a. Sorry, my bad. I completely overlooked your previous response and didn't see that you re-defined the `Counter` to have two parameters `m a`. I see it now and it all makes sense. Thanks for being patient though! :)
This is exactly the recursor for the unit type. So yes.
This appears to be the site for the language: https://www.fstar-lang.org/
You'll need the physical book to download, but you can read online on the publisher's site.
Layer 3 Communications. We're a network security company located in Atlanta, Georgia, United States.
Why do you say that Nothing and Just "" will conflate. It is actually not the case my trivial implementation.
You would've had instant access to the eBook, source code and forums, if you had purchased the book here https://www.manning.com/books/get-programming-with-haskell
It may not surprise you that I am also against the FTP.
Ok, sorry. I read it in the wrong direction.
Could you expand on "are independent of each other"?
When using Gabriel Gonzalez' Turtle library, is there a way to combine 2 [Shell](https://www.stackage.org/haddock/lts-13.26/turtle-1.5.14/Turtle-Shell.html#t:Shell)s into one Shell, which would produce values as soon as they arrive (to achieve a kind of interleaving)? I'd like to combine stdout of 2 separately running processes into one stream and I'm not sure how to do that..
Is it possible to learn this power?
If you are commited to your project, try it. it is a great language. &amp;#x200B; But if you are a hobbyst, probably you will create nothing useful, but you would perhaps find a way to traverse lists using higher level contravariant metafunctors and produce a lot of noise about that.
&gt;5 years of experience oof :(
" The code resembles what one might write in a dynamically typed scripting language. Like such languages, Haskell has a refreshing lack of boilerplate. " This guy don't know the secret code of the mess of horror vacui complications to which every trendy cool haskeller should adhere.
Ohhh, that makes a lot of sense. Thanks for such a thorough explanation :)
Thanks!
Yo; author of SitePipe here; the HTML templates in Sitepipe are just Mustache templates, so if you can work with that it shouldn't be too tough. &amp;#x200B; You can also rig up custom file-readers, so if you use Sass or something like that there's probably a tool for that in Haskell you could wire in, or just shell out to your favourite compilation tool as part of the build. It's built to be easily extensible and just provide the basics for you. There are examples in the repo. Cheers!
&gt;As interpreting &gt; &gt;Nothing &gt; &gt; and &gt; &gt;Just "" &gt; &gt; as the same thing by default in &gt; &gt;toString &gt; &gt; makes me nervous for some vague reason, I agree with the nervous bit, but I don't see how it is the default ? The default implementation actually makes a difference between both. Also, when you have a \`Maybe Text\` you always as a problem dealing with \`Just ""\`
&gt; We could propose some reasonable laws for IsString: I would argue those are very unreasonable laws, for a few reasons. Both laws because they mention classes that are not superclasses of `IsString` (and shouldn't be). The `seq` law because it's a very strange kind of law based around `seq` and `[r]nf`. The `Semigroup` law because there are very useful and sane types that violate that law, such as any regex/parser that treats `&lt;&gt;` as "or".
In its entirety? I have some problems with it, for example I think `Data.List` should probably contain either no functions at all or monomorphic ones rather than `Foldable` ones. But I also think that not having it at all and leaving everything with a bunch of monomorphic functions hardcoded around lists is much worse than the current situation. I love that I can easily interact with a variety of datatypes via the FTP generalized functions, I actually wish Filterable/Witherable were also in base and more overly monomorphic functions were generalized (`filter`). I can understand the frustration with `(,)` and `Either`, although I would argue the real solution to that long term is proper extensible rows/records/variants/sums/products and moving away from `(,)` and `Either`. With `Product :: Array Type -&gt; Type` you wouldn't even be able to make it an instance of `Functor`/`Foldable` etc. you would have to make something like: class PositionalFunctor (f :: Array Type -&gt; Type) where pmap :: (as ! i -&gt; b) -&gt; f as -&gt; f (set i b as)
I mean you *can* define the concept of a `Vector` inductively (just treat it as a list and beg the compiler to flatten it for you), but i'm not sure it's fair to say that it **IS** inductive.
I would say it might be more beneficial to ditch list comprehension syntax and define `data List a = Nil | Cons a (List a)` and implement various typeclasses from there.
Yes, I mean that neither of them uses the output of the other one. `do` blocks are "syntax sugar" to write chains of monadic binds, whereas Applicative lets you express things which can be evaluated in parallel.
&gt;Thanks for being patient though! :) Likewise! :)
Having experience on other FP lang is considered ok? I dabbled with F# and getting my toes wet with ReasonML &amp; Phoenix (Elixir)
New to haskell and I'm dumb. Can't figure out why this isn't filtering out "red" from every sub-list in the list: map (filter (/="red")) [["red","blue","green"],["blue"], ["green", "red"]]
Also the Functional Programming discord has a #jobs channel: https://discord.gg/3MsgP4z
I guess the answer is "it depends on the problem domain". If it's for formal verification, yes, if it's for general-purpose programming, maybe not.
No.
Disclaimer: I've never used Turtle nor have looked really into it, but from the first look I can see that the Shell type implements the `Alternative` and `MonadPlus` type classes, so that you may use `mplus :: m a -&gt; m a -&gt; m a` (and `(&lt;|&gt;)`, respectively) to combine two Shells into one.
Great progress. I'm looking forward to reading more about the project. Are you loading any events into memory at all? If you precache a certain amount outside the current view then performance should improve. The link to the [repo](https://github.com/sighingnow/ghc-events-sqlite) doesn't work.
It uses the original Space Invaders ROM.
Ignore that. I will be evaluating haskell knowledge and experience.
The position is for maintaining existing quite large haskell codebase. Haskell experience is required.
 import System.Console.Haskeline import System.Console.ANSI import Control.Monad.IO.Class main = runInputT defaultSettings (liftIO (setSGR [SetColor Foreground Vivid Red]) *&gt; outputStrLn "Test" *&gt; liftIO (setSGR [Reset]) *&gt; outputStrLn "Test") Works for me in ghci.
This got me thinking about how horrible this would be: instance Read t =&gt; IsString t where fromString = read
Sounds great! I'll keep it in mind.
How would I do the same with getInputLine?
You could probably (I haven't tested it) do it similarly to my second example: getInputLine (setSGRCode [SetColor Foreground Vivid Red] ++ "Please" ++ setSGRCode [Reset] ++ " input: ") The `setSGRCode` function just returns a `String` which can be prepended to any string to change the color of that string (that is how ANSI codes work).
Yep, that worker. Thanks for the help.
Yep, that worker. Thanks for the help.
Thanks for your attention to this project. The current implementation is just a prototype, and many things still need to be accomplished. &amp;#x200B; The broken link in the blog has been fixed now.
Do you accept remote from Europe (UTC + 3)?
This is riveting!
We have teams working from India, so we are fine with candidates from other countries.
It's the current state, so I'm no longer resisting it.
Betteridge strikes again
I thought you could add a git repository to `extra-deps` though. Does that not work?
You can, and the link I gave has some examples.
Ok, for future reference, since I was trying to use two different packages from the same repository, I needed to add that repository to the `extra-deps` twice. Once with the subdirs. I guess that makes sense.
So, my `extra-deps` section now looks like: ``` extra-deps: - git: https://gitlab.com/everythingfunctional/hedge.git commit: be68feb41730e52f4423cb68a8b58c07880d1ebe - git: https://gitlab.com/everythingfunctional/hedge.git commit: be68feb41730e52f4423cb68a8b58c07880d1ebe subdirs: - hedge-trimmer - quickcheck-with-counterexamples-1.1 ```
Since `subdirs` is a list, does this work instead: extra-deps: - git: https://gitlab.com/everythingfunctional/hedge.git commit: be68feb41730e52f4423cb68a8b58c07880d1ebe subdirs: - . - hedge-trimmer - quickcheck-with-counterexamples-1.1
Why yes, yes is does. Thanks a ton.
That seems a bit too subjective. Does holding a PhD related to Type Theory mean you are an expert? A hobby level GHC contributor? Or the fact that you build apps in an industry setting and understand lenses, basic type level programming, and how to pull all of the tooling together?
LambdaConf a while back produced a Standardized ladder of functional programming with descriptions of required knowledge for various Haskell skill set levels. It can serve as a reference unfortunately its not in text format. [https://pbs.twimg.com/media/CydL5EYUsAAI-61.jpg:large](https://pbs.twimg.com/media/CydL5EYUsAAI-61.jpg:large)
This is great. Thank you for the resource :)
No.
Oh, man, are you going to love writing in Idris or Agda, where this is even easier and the proposals are even relevant. Heck, writing things like foldr and zipWith are as simple as writing down the type and then repeatedly case-split and proof search until you are done.
\`Alternative\` and \`MonadPlus\` only do sequential composition (all the values from the first stream, then all from the 2nd stream). After looking through the docs more thoroughly I concluded the library doesn't support this out of the box.
This is wonderful! Going into a type tetris trance and coming out the other end with useful code is my favorite part of writing Haskell, and you've narrated examples of that perfectly.
Like any technical field, there isn't one definition of expert. In my experience, every Haskeller I run into has more and less knowledge in various areas than I do after my years of learning it. And that's the fun part - there's always stuff to learn from others.
Good! Bisection is important.
Thank you! Hopefully I got it fixed.
Looks like it! :)
Neat demonstration! I don't get the sentiment here though: &gt; You know when people say "types are not an alternative to documentation?" I think this is a pretty knock-down argument to that claim. Once you really understand the typesystem, most of the time, types really are the best documentation --- they often tell you exactly what the function does, in a way that English comments never will. 1. Not all functions have the luxury of having a unique implementation based on the type. Sure, in library code you probably have this luxury more often, but less so in application code. 2. Why should a reader have to constantly go through the 1-2 minutes of mental gymnastics of figuring out each type signature, where they could've read the documentation and understood each argument in 10 seconds? This kind of "1 vs 2" strikes me very much like the "types vs tests" or similar "debates"... different tools in your toolbox can be complements, they don't have to be supplements. Nobody is being forced to code on a remote island with only one tool in their toolbox.
I misread the title as “Snake” and was a little confused.
That's a terrible ladder. Not OSHA at all.
I realized this is why I love Haskell so much when I saw a Edwin Brady giving a talk about Idris. We were looking at a half-written function with the types already in place. "I'll just let the compiler finish this code" he said, his laptop chucking away for a few seconds until the correct was inserted in the hole that was left. It wasn't terribly complicated code, anyone reading this could have found the same solution in not much more time than the laptop, but the code was not entirely trivial either. Haskell isn't there yet. Idris has dependent types, of course, and they're able to restrain the program space enough that a search is plausible. But I'm okay with actually writing the characters myself, gives me time to think. The point is that the language itself, with the compiler as its digital representative, is helping me write the code by giving me the ability to put down some solid outlines and then helping me fill them in. Now if only there were even better IDE support for this kind of automated pair-programming...
Terrible ladder - according to it, Bryan O'Sullivan wouldn't be an expert because his libraries don't use fancy type level features. The only point that would fit is "high performance".
This is a really clever article. If you are lurking worth the 5-10 minute read.
Agreed. `data Vector = { length :: !Int, data :: IntArray# }` isn't an inductive definition. `data Vect n a where { Nil :: Vect Z a; Cons :: a -&gt; Vect n a -&gt; Vector (S n) a; }` is, but is an Idris vector, not a Data.Vector vector.
Does the idris vector get flattened by the compiler?
Completely agree with your points, the documentation on many libraries is the main area I'd like more to see improved in Haskell (and more real world, practical, end to end examples). I do get that type holes are great, they've blown my mind coming from Java, and yes, they can be really helpful, but like you say they are no substitute for documentation, they should complement each other.
I'm honestly not sure. I had enough performance issues with the *compiler* that I never got into performance-optimizing my Idris code, so I was only concerned with the denominational semantics. I think it's possible that Idris used `Vect` simply because `List` was already in used, and `LengthIndexedList` was too long and annoying to type. Even though my mind associates both "vector" and "array" with contiguous storage (C++ and Java corrupted me), it simply might not be true for Idris vectors.
This was exactly what I wanted to reply as well. :)
It can really bring some Zen into your programming.
Or, for that example, you could just use Djinn! f :: (a -&gt; b) -&gt; ((a -&gt; c) -&gt; c) -&gt; (b -&gt; c) -&gt; c f a b c = b (\ d -&gt; c (a d)) Yes, I know that doesn't really work for all the other stuff, but still! ¯\\\_(ツ)\_/¯
Nit pick: If I'm not mistaken the term is ["typed holes"](https://wiki.haskell.org/GHC/Typed_holes). "Type holes" are similar but those are holes in types, not in terms.
Not using those things in your libraries doesn't mean you don't understand them and know how to effectively use them.
When you can implement a balanced Red Black tree on a whiteboard and use it to find all primes between Fizz and Fibonacci, all in under 10 minutes.
Not to mention they could have avoided the mental gymnastics by just looking at the code... Also, knowing the implementation of a function doesn't tell you a whole lot about how to use it, or why. Especially with these higher order functions there often are many non-obvious uses.
Are there any experiments into letting software fill in holes like this? (In haskell. Yes Idris is dope.) For library code, where there is 1 way (or few ways) to correctly write a fn, I could see something like a SAT solver really able to help out, especially if the interaction was iterative. In fact, that'd probably work well for a lot of application code, especially when using libraries that harness complex type machinery (I recently wrestled with Servant in a way this could have helped).
Sure, technically speaking, you're right. That said, IIRC, he'd once mentioned in a thread [ can't find it though :( ] that part of the reason he reduced his participation in the Haskell community was the tendency to chase after abstractions. Of course, one is free to interpret this second hand and possibly erroneous memory of mine one way or another.
There's also the justdoit plugin, https://hackage.haskell.org/package/ghc-justdoit f :: (a -&gt; b) -&gt; ((a -&gt; c) -&gt; c) -&gt; (b -&gt; c) -&gt; c f = (…)
This is pretty much what I do as well when I write anything in Haskell.
If you're not regularly using zygohistomorphic prepromorphisms then I'm afraid you can't really be considered an expert.
I think I fit most of the Expert criteria, but miss many of the others...
I think it’s an unreasonable expectation to have regarding a language standard.
To be able to write an arbitrarily complicated program using understandable, well documented, and idiomatic Haskell that could be maintained and extended by less experienced Haskell programmers.
I think typed hole suggestions essentially was that experiment. I know there was an open issue on HIE a while back about leveraging this GHC feature to produce auto-complete / code-lens suggestions, not sure how far along they got with it.
While I appreciate that the commenters here seem to want to express that there’s a lot of nuance behind this question, but in the interest of giving the OP an actual answer that’s scoped a bit to me experience: If you’re writing Haskell in a production setting [0], then a clear understanding of the exception system (sync, async, and impure exceptions [1]) is absolutely a component of Haskell expertise. Lots of Haskellers get very far without confronting the exception system, and it can be a barrier between “extremely knowledgeable and competent” and “expert”, IMO. [0] I’m considering “production” here to mean `IO` heavy systems, not something like a compiler or some pure data transformations. [1] It is especially important to understand how laziness interacts with impure exceptions. Depending on what you’re doing you could have an exception hiding out in a thunk that would only show up upon deep evaluation, which may end up being far away in the code from where it was thrown.
 Newtype approach seems fine by me, especially now that ApplyingVia is being developed. And it makes perfect sense too - I mean, yeah, it's the same data but with different operations on it, that's the point of types.
The shortest answer is unsafePerformIO is unsafe. Use the IO monad to get the correct behaviour. Since the int here is constant, it's very likely that the value is computed only once and then stored. In general if you try to use unsafePerformIO and you don't really definitely surely know what you're doing, it's not going to work how you expect, and how it works is heavily dependent on optimizations.
&gt; You know when people say "types are not an alternative to &gt; documentation?" I think this is a pretty knock-down argument to that claim. &gt; Once you really understand the typesystem, most of the time, types really &gt; are the best documentation --- they often tell you exactly what the function &gt; does, in a way that English comments never will. I mean at the start of the code you literally have an incomprehensible function hoistStateIntoStateT :: Sem (State s ': r) a -&gt; S.StateT s (Sem r) a hoistStateIntoStateT (Sem m) = m $ \u -&gt; case decomp u of Left x -&gt; S.StateT $ \s -&gt; liftSem . fmap swap . weave (s, ()) (\(s', m') -&gt; fmap swap $ S.runStateT m' s') (Just . snd) $ hoist hoistStateIntoStateT x Right (Yo Get z _ y _) -&gt; fmap (y . (&lt;$ z)) $ S.get Right (Yo (Put s) z _ y _) -&gt; fmap (y . (&lt;$ z)) $ S.put s
Lol this ladder. Seems like it completely ignores the actual experience of learning to program. As if everyone follows the path of "undergrad to PhD in PLT research." Imagine a python ladder where in order to be considered even just proficient you had to be mucking around with writing metaclasses and writing C extensions by hand that safely deal with the gil. Ridiculous. I agree that this is an increasing order of complexity of the topic, more or less. But it seems odd to me that you can start a company and ship a product and then be called a beginner because you didn't need to write template Haskell or use lenses.
I've interviewed for this company. Waste of time. They want someone to maintain the Haskell application and eventually move it to .net. I told them I was a 7-8 in the .net world and about 3-4 in Haskell. It would be fine if they would just have said they weren't comfortable with my experience in Haskell but they said they were interested and gave me a take home assignment in C# and Sql. They didn't know what to do about Haskell. So I did the assignment easily and with unit tests and returned it to them. They said they were still considering me and then I did the C# assignment in Haskell and gave them instructions on how to run it with stack. The recruiter told me after that that they closed the position. &amp;#x200B; I'm just a tad upset that I wasted my time on their take home assignments that had nothing to do with the job description and they still couldn't be honest with me and just say "Sorry, we don't think you have the experience we need". Also, if they are hiring for Haskell positions, I hope they know how to give Haskell assignments :/
Sure --- but the type is clear af.
They just \* had \* to throw the profunctor optics in there at the end. Relevant, [https://twitter.com/aisamanra/status/804553283578646528](https://twitter.com/aisamanra/status/804553283578646528)
[https://twitter.com/aisamanra/status/804553283578646528](https://twitter.com/aisamanra/status/804553283578646528)
I think I've seen this approach before in a blog post somewhere.. I can't find it now though; I'll reply if I do see it. If anyone else knows about this blog post, feel free to reply to this.
Maybe: The Haskell expert can solve any reasonable problem thrown at them - be it performance or safety issues, functionality (at least in a certain domain of their choice), or refactoring a large codebase, in reasonable time.
I think you are right. I was approached by them a few days ago to recruit haskell dev. I believe they had no one at the company who knows anything about haskell when you were interviewed because all their haskell devs left after their dotnet faction decided to throw away all haskell code and migrate to csharp. But that was 2 years ago and i think their team failed to make any progress on that front. The haskell code is still powering their entire business and new management is starting to realize that it is not going anywhere anytime soon.
Once classes are explicitly record types, we can give them instances of `Generic`. So instead of deriving for generic types, we can also do deriving for "generic classes". From there, we can start deriving deriving strategies. For example, many classes have instances for product types, which use the instances of each component of the product (products of monoids, monads, traversables...). Now, if you try hard enough, all of these instances could be collapsed into a single definition parameterized by generic types and classes.
&gt; ApplyingVia Cool idea and an interesting way to work within the limitations of current Haskell, but I really really think that embedding ourselves deeper and deeper into newtypes and instance magic is much worse than trying to climb back out and use typeclasses only when they are a net positive. It's similar to how I would like proper row/array backed structural types rather than going deeper into the rabbit hole of Record/Generic/Deriving extensions. I mean for real looking at the examples: sum = fold @_ @(a via Sum a) product = fold @_ @(a via Product a) vs: sum = fold_ addMonoid product = fold_ multMonoid I think we should actively be looking at ways to unify and simplify existing features while keeping them equally if not more expressive. Particularly so that we can then standardize on it in a new Haskell report, rather than patching things with tons of different extensions. &gt; I mean, yeah, it's the same data but with different operations on it, that's the point of types. I'm not against that as a concept, and I absolutely do think `newtype` is a useful concept. I don't want people to get rid of `newtype SqlKey = SqlKey Int` or `newtype ZipList a = ZipList [a]`. I just think that typeclasses that exist for the sole purpose of manipulating one typeclass instance into another is far better expressed as a function, since that is what functions are designed to do very cleanly and effectively.
I think this is the real answer. Language experts aren’t distinguished by being able to rattle off some arbitrary list of concepts, they are distinguished by how comfortable and effective they are working within the language. An expert has enough experience and knowledge to be able to pick up more advanced concepts whenever they become necessary, but they don’t need to necessarily know them all ahead of time. If they did, nobody would be an expert, since Haskell is always evolving.
I don't think any young user could understand that type easily. First there is the type level operator. So it is kind of confusing to have a `':` instead of a `:`. Then, what is `Sem`? Also, you need to know `StateT` so Monad transformer, etc... No seriously, I love Haskell, but, it can be really hard to grasp. And here, you need to have already a bunch of Haskell experience to have an idea about what is going on.
I should have read the original post a few more times and looked back at the `IsString` class because, for some reason, I was seeing `toString` rather than `fromString`. Oops!
I do this a lot too, and it works great, but I wish we could make some progress on type hole verbosity. There's just *so much* clutter there compared to, say, Agda.
Consider this example: - I'm using a typed language to define my data types on both the server and the client. (Maybe the same, maybe not.) - To communicate from one to the other, I serialize the data in one place and deserialize it in the other. - This works great, but at some point I want to change the data. I deploy new changes to the server, but clients are still using the old code, with the old serialization / deserialization. What techniques work well to prevent such a program from blowing up when either the client or the server tries to handle data in a format it wasn't expecting? (And is there a name for this problem so I can read more?)
Not all code (or type signatures!) has to be immediately understandable to a Haskell beginner to be "good." The type is clear if you have the prerequisites. If you don't have the prerequisites, then it isn't surprising for you not to understand the type. The thing about prerequisites though is you only have to learn them once. It's the essence of any learning curve.
The code wouldn't be any more clear if it had `let`s and lambdas everywhere imo.
This looks like a neat solution, even though it's not quite clear to me what problems it solves.
It allows you to manipulate, combine and use typeclass instances as first class values without having to rely on various newtypes and extensions (ApplyingVia, DerivingVia). This should particularly make situations where you have multiple possible instances much nicer, as you can define functions that take in the instance as a regular value, such as `fold_ addMonoid [1, 2, 3] = 6` instead of `getSum (foldMap Sum [1, 2, 3]) = 6`. It also helps when writing out new instances for new types you define, as you can use explicit function calls and combinators rather than having to reach for something like DerivingVia and a bunch of hardcoded top-level newtypes.
http://www.haskellforall.com/2012/05/scrap-your-type-classes.html?m=1 This one?
&gt; Performance-wise from my extremely basic benchmarking it doesn't seem to affect performance, although since GHC assumes you won't be typically taking this approach I wouldn't be surprised if it was sometimes slower (in a hopefully fixable way). It does affect performance: Functions can be specialized for certain instances, but it isn't possible to specialize functions for certain values of their arguments. This is actually very important for many libraries. But I think that this is not a theoretical limitation.
https://arxiv.org/pdf/1807.11267.pdf
Functions defined this way with typeclasses would still be able to be specialized, which would expose the specific `Monoid a` being used, which could then very easily be inlined. So `sum` in the above definition should never be slower than `Prelude.sum` if GHC handles things properly. I realize that some first class usages of this where you never touch typeclasses could be slower (even then you have inlining to hopefully fix that in most cases), but typically then you are doing things that aren't even possible with typeclasses.
That blog post is a good explanation of this technique for sure. It takes a bit of an aggressive stance against typeclasses which I don't personally agree with. I still very much like typeclasses, I just don't like that the underlying structure is impossible to play with and manipulate. It's also worth emphasizing how useful direct access to the underlying dictionary is for avoiding as much need for various `Deriving*` extensions.
This seems like a better link: https://lirias.kuleuven.be/retrieve/519822/
&gt; fmap (y . (&lt;$ z)) $ S.get also code like this, I have a bit of experience with Haskell by now but to understand this I would have to dissect the code.
Is there an option to name your typed holes like in Idris?
In your specific example it probably does not affect performance. However note that for `Functor`, `Traversable` etc, the record will have to contain [`RankNTypes`](https://wiki.haskell.org/Rank-N_types). I think that in those cases the compiler will have problems specialising their uses with concrete types.
I mean there would still be a typeclass involved that can give you the type which can then be inlined, i'm not sure I see why it would be an issue?
TIL I'm not a Haskell expert, only "proficient." I think a lot of those things in the expert tier, like dependent typing and singletons, category theory, and kind polymorphism are broaching the subject of [formal methods]( https://en.wikipedia.org/wiki/Formal_methods ). I would argue that formal methods is it's own field, and not really a requirement for expertise in any programming language.
**Formal methods** In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, discrete event dynamic system and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I'm not an expert on this. Running a program in the lambda calculus is done by beta-reduction. In this case it depends on the definition of the `*` and `+` functions (and also the `1` and `2` values). The lambda calculus doesn't have these function built-in, so they must be defined somewhere. Applying those definitions and performing beta-reduction should allow you rewrite both those functions to the same normal form. The Curry-Howard isomorphism is about types corresponding to theorems and programs to proofs, but you don't need to evaluate the programs, you just need to check the types. (This may be horribly wrong.)
That process can be automated. &amp;#x200B; That remembers me Djinn and other automatic code generators more sophisticated [http://hackage.haskell.org/package/djinn](http://hackage.haskell.org/package/djinn) &amp;#x200B; In the medium term, probably this kind of process will be part of IDE's of strong typed languages and a big reason for haskell adoption.
This would be a reality right now if haskellers would concentrate in doing practical things instead of reinventing getters and setters and loops for traversing structures and the next failed non composable effect system.
&gt;It does affect performance: functions can be specialized for certain instances, but it isn't possible to specialize functions for certain values of their arguments. A specialized implementation of a typeclass member function can be used if the types of the context is known at compile time. However, if desugared, the resulting transformation is equivalent to constant propagation, and this optimization is well known and was implemented in GHC long time ago.
The proposal for explicit typeclsass dictionaries exists in GHC trac for quite some time. Nobody competent enough was interested enough to implement it, unfortunately.
&gt;I really really think that embedding ourselves deeper and deeper into newtypes and instance magic is much worse than trying to climb back out and use typeclasses only when they are a net positive I wouldn't even call that "magic". Behind the scenes, it's quite straightforward safe coercing, isn't it? &gt;It's similar to how I would like proper row/array backed structural types rather than going deeper into the rabbit hole of Record/Generic/Deriving extensions. Can relate to this. Btw, there are instances `(Num a) =&gt; Num (Sum a)`, `(Num a) =&gt; Num (Product a)` so you don't really need much wrapping here, consider: ``` &gt; getSum $ fold [1,2,3] 6 ``` &gt; I just think that typeclasses that exist for the sole purpose of manipulating one typeclass instance into another is far better expressed as a function, since that is what functions are designed to do very cleanly and effectively. Well, we have either several types and one class or one type and several typeclasses (from your examples in the post). I don't see how is this a gain.
&gt; I wouldn't even call that "magic". Behind the scenes, it's quite straightforward safe coercing, isn't it? I mean it's a bunch of codegen with coerces in the middle of it, so it's not super deep magic but it's not at all beginner friendly or obvious how it works. &gt; Can relate to this. For real, at this point it's 100% my biggest complaint about Haskell by miles. &gt; Btw, there are instances (Num a) =&gt; Num (Sum a), (Num a) =&gt; Num (Product a) so you don't really need much wrapping here, consider: [CODE] That code is pretty misleading, because in practice you are not going to be dealing with number literals, those numbers are almost certainly going to come from some external source in non-Sum form. So `getSum $ foldMap Sum myList` is more accurate. &gt; Well, we have either several types and one class or one type and several typeclasses (from your examples in the post). I don't see how is this a gain. That is not true, the number of typeclasses does not increase. The reason the examples above have additional typeclasses is because I actively want to break apart `Num` into multiple classes to make it more useful. If we want the same functionality as before we do not have to add a single extra class. We would just use `Num` rather than `AddMonoid` above.
&gt; Applying those definitions and performing beta-reduction should allow you rewrite both those functions to the same normal form. I'm pretty certain that's not the case. Using Peano naturals (n = Z | S n) and "sensibly defined" atomic arithmetic operations definitely leads to different normal forms for these to expressions even if you do case analysis on x once, and I don't see how Church encoding would help with the fact that you need to do induction on x to prove the equivalence. Beta reduction alone simply doesn't help much with this. But yes, the OP seems to misunderstand the thrust of C-H iso. If you want to prove that forall x, x*x + 2*x + 1 = (x + 1)*(x + 1), then *that* is the type of the "program" you need to write.
It would benefit from some type annotations, though. I would feel pretty lost without an IDE displaying the type of each subexpression.
I will not do a full proof, but here is an example: (a + b) * c mult (plus a b) c mult ((\m n f x -&gt; m f (n f x)) a b) c mult (\f x -&gt; a f (b f x)) c (\m n f x -&gt; m (n f) x) (\f x -&gt; a f (b f x)) c (\n f x -&gt; (\f x -&gt; a f (b f x)) (n f) x) c (\n f x -&gt; a (n f) (b (n f) x)) c (\f x -&gt; a (c f) (b (c f) x)) a * c + b * c plus (mult a c) (mult b c) plus ((\m n f x -&gt; m (n f) x) a c) ((\m n f x -&gt; m (n f) x) b c) plus (\f x -&gt; a (c f) x) (\f x -&gt; b (c f) x) (\m n f x -&gt; m f (n f x)) (\f x -&gt; a (c f) x) (\f x -&gt; b (c f) x) (\n f x -&gt; (\f x -&gt; a (c f) x) f (n f x)) (\f x -&gt; b (c f) x) (\n f x -&gt; a (c f) (n f x)) (\f x -&gt; b (c f) x) (\f x -&gt; a (c f) ((\f x -&gt; b (c f) x) f x)) (\f x -&gt; a (c f) (b (c f) x))
&gt;If you want to prove that forall x, x\*x + 2\*x + 1 = (x + 1)\*(x + 1), then *that* is the type of the "program" you need to write So the identity I wrote above cannot be proved using lambda calculus ? What about reduction using simply typed lambda calculus / calculus of constructions ? Will I be able to reduce it to a normal form in these targets atleast ?
People who are looking for ways to write declarative and composable code are attracted to Haskell. But while Haskell gives you tools to do things, not all the problems were already solved. We’re still figuring out better and more composable ways to do things and as that attracted us to Haskell in the first place you can expect us to keep looking for the best ways to do things.
Thats a life saver! Let me try some more variations. I will try out sum of n natural numbers using the &gt;n \* (n+1) / 2 as well as the for loop version by adding one by one. Thanks a lot! So it is provable in LC. I thought I might require simply type LC for this. Seems STLC is required to avoid paradoxes like y-combinator so that the language does not run into infinite loop. Let me try this for some more expressions. Basically I want to do this for text book algorithms in order to guide the users of my language to always write the most idiomatic code possible eg. Dijkstra, TSP etc.
&gt; a * b = b * a But can you? Honest question, even distributivity is surprising to me, but it seems to be an easier case than commutativity or associativity. In fact, nothing seems to indicate that \f x. m (n f) x is equivalent to \f x. n (m f) x, especially for arbitrary lambda terms m and n, but even with just Church-encoded numerals you need to do some extra work, because there's nothing to beta reduce here.
Just put underscore in front of (undeclared) variable name: `_hole`
The proof of distributivity of addition over multiplication posted above by u/Noughtmare is surprising to me, but I still don't see yet how it would work in the general case.
Please note that this doesn't have anything to do with the Curry-Howard correspondence any more. This uses lambda calculus as a basis for arithmetic in the same way as you could use set theory: https://en.wikipedia.org/wiki/Set-theoretic_definition_of_natural_numbers.
But aren't we trying to prove an identity from set of facts? Is it because this is untyped Lambda calculus and since it lacks types, this does not come under Curry-Howard isomorphism ? Please help me out.
I have some half-baked thoughts for going back-and-forth between classes and dictionaries, but along slightly different lines, keeping the existing classes. Getting dictionaries from class instances is easy enough - for example... dictFromMonoid :: Monoid m =&gt; MonoidDict m dictFromMonoid = MonoidDict mempty (&lt;&gt;) The trouble is using existing functions, but making them use an explicit run-time dictionary. "Creating instances" from explicit dictionaries at run-time sounds wrong, but it's sort-of possible in some cases - at a run-time cost. Wrap the dictionary in a data constructor along with the value. Rather than do run-time checks to ensure dictionaries from different arguments match, steal an unbound type variable trick from ST... data DictMonoid p a = DictMonoid (MonoidDict a) a -- probably keep the data constructor private mkDictMonoid :: MonoidDict a -&gt; a -&gt; DictMonoid p a -- Note p isn't bound mkDictMonoid = DictMonoid instance Semigroup (DictMonoid p) where (DictMonoid d x) &lt;&gt; (DictMonoid _ y) = DictMonoid d ((getOP d) x y) This is broken because, is effect, every single `mkDictMonoid` ends up with a different unbound type variable. The solution is to make one value from another... mk2 :: DictMonoid p a -&gt; a -&gt; DictMonoid p a -- Note p isn't bound mk2 (DictMonoid d x) y = DictMonoid d y The impossible problem is how to instance `Monoid` and implement `mempty`, which has no arguments to extract a dictionary from. A compiler extension that provides a way to `mkDictMonoid` but doesn't wrap the dictionary, instead re-packing it as a compiler-internal dictionary to pass implicitly for constrained functions (so `MonoidDict` can be a `newtype`), might work, but might break higher-rank functions.
&gt;Again, reduction just seems like the wrong strategy for proving universal statements. I am trying to prove the statements not just by reducing them, but by comparing their equivalence of their normal forms. Is it a bad strategy or is not a strategy itself ?
Very good point, it seems that we do require some more information about `m` and `n` to prove that.
It does not seem work when calling with FFI to other language, even in a for loop, C seem to only call the currentTime once
You can prove the two programs *behave* equal, but they are in fact not the same program. If you want to prove mathematically within the *propositions as types* framework that the statement is true, then you have to specify a **type** that captures that **proposition** (as /u/pbl64k did) and then you have to ***provide*** (not *execute*) a **program** that satisfies this type.
Don't you have anything better to do Jon Harrop?
Thanks for the thoughts, definitely some interesting things to think about. Although I will say that it doesn't really hit the requirements I am looking for. Which is: 1) Simplify the language, removing the need for extensions rather than creating new ones 2) Make it quick and easy to define new top level instances based on other instances/dicts without any special extensions. Changing as little as possible of current Haskell, long term I'm visualizing something like: data Monad_ m = Monad { pure :: forall a. m a , (&gt;&gt;=) :: forall a b. m a -&gt; (a -&gt; m b) -&gt; m b } class Applicative m =&gt; Monad m where monad :: Monad_ m data List a = Nil a | Cons a (List a) instance Monad (List a) = Monad { ... } instance Applicative (List a) = monadToApplicative monad instance Functor (List a) = monadToFunctor monad With a generic `cast` class of sorts for situations where there is a canonical conversion from a given `A` to a given `B` you could shrink `monadToApplicative` and `monadToFunctor` to just `cast`.
&gt; But my favorite part is that having a strong type system means I don’t need to use my brain to do programming. I disagree. I’d say it means you use your brain to write types instead of terms 🙂. So, writing terms becomes brain-free, while the effort associated with writing types increases.
I think the main problem you will encounter here is that it is in general not possible to check for function (/ lambda expression) equality, because there are multiple different ways to implement the same function. So even if you decide on a clever normal form scheme (using e. g. laws of distributivity, associativity, idempotence etc. that you have chosen) , it will only be able to decide between: - Yes, definitely equal - Maybe not, but I am not sure. A very simple example of a difficult case would be to find out the equality of a function implementing exponantiation using repeated multiplication vs a function implementing it using exponentiation by squaring.
Perhaps you're right and I was wrong. I tried verifying my claim using a benchmark but I found no evidence in my favor: {-# LANGUAGE DeriveAnyClass, DeriveFunctor, DeriveGeneric, RankNTypes #-} module Main where import Control.DeepSeq (NFData, rnf) import Control.Exception (evaluate) import Criterion (Benchmarkable, whnfIO) import Criterion.Main (bench, defaultMain) import GHC.Generics (Generic) newtype ExplicitFunctor f = ExplicitFunctor { explicitMap :: (forall a b. (a -&gt; b) -&gt; f a -&gt; f b) } data MyPair a = MyPair a a deriving (Functor, Generic, NFData) myPairFunctor :: ExplicitFunctor MyPair myPairFunctor = ExplicitFunctor { explicitMap = \f (MyPair p0 p1) -&gt; MyPair (f p0) (f p1) } examplePairOfPairs :: MyPair (MyPair Int) examplePairOfPairs = MyPair (MyPair 12345 54321) (MyPair 98765 56789) benches :: [(String, Benchmarkable)] benches = [ ("type-class" , mkBench (fmap (fmap (+1)) examplePairOfPairs)) , ("rank-n-record", mkBench (explicitMap myPairFunctor (explicitMap myPairFunctor (+1)) examplePairOfPairs)) ] where mkBench :: MyPair (MyPair Int) -&gt; Benchmarkable mkBench = whnfIO . evaluate . rnf main :: IO () main = defaultMain $ map (uncurry bench) benches Resulted over here with: benchmarking type-class time 4.411 ns (4.378 ns .. 4.461 ns) 0.999 R² (0.998 R² .. 1.000 R²) mean 4.404 ns (4.387 ns .. 4.461 ns) std dev 89.39 ps (44.44 ps .. 165.7 ps) variance introduced by outliers: 33% (moderately inflated) benchmarking rank-n-record time 4.450 ns (4.415 ns .. 4.481 ns) 1.000 R² (0.999 R² .. 1.000 R²) mean 4.413 ns (4.389 ns .. 4.441 ns) std dev 84.54 ps (67.05 ps .. 108.8 ps) variance introduced by outliers: 30% (moderately inflated)
&gt; Haskell isn't there yet. Idris has dependent types, of course, and they're able to restrain the program space enough that a search is plausible. What’s a good example of something that can be expressed in Idris but not in Haskell? I found some examples of using dependent types in Haskell: https://github.com/sweirich/dth so I’m not sure exactly where Idris is required because Haskell is insufficient.
Yes, I'm almost certain it's that one. Thanks for finding it!
&gt; Then, what is `Sem`? Also, you need to know `StateT` so Monad transformer, etc... You definitely need to understand the types involved in a function's type signature in order to understand what that function does. A type signature can't teach you what a `Sem` is any more than it can teach you what an `Int` is. For example, in Java, if you don't know what an `Iterable` or an `int` is, the following function type signature will be incomprehensible: ``` public static int length(Iterable it) ``` but if you know what they are the implementation is trivial.
I would disagree that this is the main problem. It's just that the method proposed, which, as I understand it, boils down to beta reducing then comparing normal forms, cannot prove some perfectly provable statements, e.g. commutativity of multiplication over natural numbers.
&gt; comparing their equivalence of their normal forms. It's one tactic (called, unless I'm very much mistaken, 'simpl' for "simplify" in Coq), but it's not sufficient for proving all that is provable. Both beta reduction and reflexivity of equality are very fundamental and important parts of computer-assisted theorem proving. But they're not exhaustive. Frankly, I would recommend "reading" Pierce's *Software Foundations*. The book is freely available online, and unlike most other textbook, it's perfect for self-studies. Coq, the theorem prover used in it, will tell you whether your proofs make sense or not. It also has an enlightening chapter on C-H iso. https://softwarefoundations.cis.upenn.edu/
Thanks a ton! I will read this. And to be frank I have a bachelor degree in CS from India which does not teach any of these programming language theory other than the stuff related to Turing machines and imperative way. After working in compilers for sometime, I need good resources to learn PLT. I got this from wikipedia [https://github.com/steshaw/plt](https://github.com/steshaw/plt) but this list is huge. So I'll start with the book you have recommended.
A friendly word of warning - it took me four attempts over five or six years to work through SF in its entirety. Despite the name, it's by no means a mild or entry-level textbook. However, it was *totally* worth it, and now tops my personal list off all-time favourite CS textbooks. It's also not necessarily a package deal, even working through the first few chapters will lit wholly new neural pathways in your brain. (Did lit mine.)
Even my neural pathways changed 2 years ago when I read about algebraic data types in Haskell and started exploring functional languages and type theory! Really excited to read the books!
This doesn't address your concern about proofs (which I think GHC is simply not in a position to do at all), but there are others out there sympathetic to the idea of reconciling explicit dictionary manipulation with implicit (coherent) typeclass resolution. Here's a [comment Richard made](https://github.com/ghc-proposals/ghc-proposals/pull/218#issuecomment-479921507) on the `ApplyingVia` proposal: &gt; Despite its being extraordinarily useful, I've never loved deriving-via, essentially for the reasons @DanBurton describes. The key trick in deriving-via is to use newtypes to stand in for named instances. Clever use of coerce makes this work. And work it does. However, I find it indirect. &gt; A case in point is a recent usage of deriving-via: A class I wanted instances of didn't have the right generic default for my purposes. So I wrote a newtype, an instance for that newtype, and then used a lot of deriving-via. But I'd much rather have done this without the newtype (which wasn't otherwise useful): if I could name the instance directly, then this would be simpler. One could argue that the newtype acts as the name of an instance, and that one would be right -- but again, let's not have play-acting: let's just name the instance. &gt; This proposal (which I think holds water and may prove to be just as useful in practice as deriving-via) expands our use of newtypes as proxies for named instances. As an alternative, I'd love to consider [Coherent Explicit Dictionary Application](https://arxiv.org/abs/1807.11267) for Haskell by Thomas Winant and Dominique Devriese. I liked the presentation at last year's Haskell Symposium, but I haven't studied this in detail. Would it lead to a cleaner way to do all this? I've not looked at the paper he references, but it may be a good source of inspiration.
"The result set is guaranteed sorted by the timestamp field, and there is no need sorting the List (or Vector) again in Haskell code, thus reduce the memory usage." You don't really need to sort the List (or Vector), you need to consume in order streams from all the capabilities. Within capability events are in order.
Nice! I would've loved to have had this about a month ago =P &amp;#x200B; Does the eventlog work at all in non-profile builds? (for \[those\]([https://gitlab.haskell.org/ghc/ghc/issues/12620](https://gitlab.haskell.org/ghc/ghc/issues/12620)) \[times\]([https://gitlab.haskell.org/ghc/ghc/issues/9388](https://gitlab.haskell.org/ghc/ghc/issues/9388)) where profile builds remove the memory leak …)
For your own sake, please read comments before replying to them.
&gt; however I am currently persuaded to not let idealism get in the way of creating working code. I think everyone is doing this but it's causing damage all the time. How many \`+.\` -like operators exist now because my type can't implement only \`+\` and \`-\` without all kinds of stuff where I will be forced to just return an error if anyone tries that?
But as a library writer, this won't help because you don't want to impose an alternate Prelude on your users.
Yes? Pure lambda calculi are *evaluated*, never *executed* (there's nothing to execute). So, you could certainly use the vague term "run" to indicate evaluation, which is done by repeated beta reduction (and possible other evaluation steps, depending on what primitives you've mixed into your lambda calculus). Beta reduction itself is "just" an algebraic reduction. There is set of equivalence classes that each contain a single value and all the (closed, well-typed) expressions that evaluate to the value, so there in some sense in which an expression is equivalent to it's value. Impure languages also have a separate part of "run"ing called execution. Running a program that involves execution is not "just" an algebraic reduction.
So function call which yields a value at runtime is still considered algebraic in lambda calculus ? Imperative languages have a clear distinction between compile time and runtime. I just wanted to make sure algebraic reductions of my functions happen during compile time.
So, equality is generally its own type in systems that create proofs based on the lambda calculus (SMT solver's logic here is a bit different), where terms are equal if they are equal up to renamings + reduction. What you're looking to prove is known as extensional equality, where we prove that functions produce equal results for all inputs. In this situation, you would do that by induction (this could be done with Church numerals, although they would need a complicate type). Then we use a few known facts abouth arithmetic, which are also proven by induction.
I'd add a version, for example, \`/api/1.0/foo\`, and update as need be. &amp;#x200B; Will you data types be changing frequently?
What kind of search algorithm is best suited for finding implementation out of type signature? Is it just A\* with smart heuristic?
Some of the heap profiling modes don't require any cost centres and so don't affect optimisation. If you turn off the cost centres you can still use at least \`-hy -hd -hb\`.
AFAIU Haskell's current version of dependent types are just mirrored definitions created by (mostly) template haskell. It also requires a boatload of GHC extensions, of which the interesting ones are rarely used in "real" programming. Using dependent types in Idris is a far more ergonomic experience. Well, as ergonomic as dependent types can be at any rate. One thing I didn't mention is how the *lack* of dependent types (and refraining from some of the more exotic type extensions) makes Haskell's ability to solve *type problems* much better than what you find in dependent languages. While I'm okay with some ambiguity, going all the way to undecidability even in the presence of top-level declarations feels like going a bit far.
&gt; They want someone to maintain the Haskell application and eventually move it to .net. I was going to apply, but if this is true, I‘m gonna pass. /u/vagif can you confirm or deny this?
It is true that they had that intention. But in the last 2 years their dotnet team has failed to migrate any of the haskell codebase. I think the new management is waking up to the reality that haskell code which powers their entire business is here to stay. Still, I cannot in good conscience give you any assurances that they are committed to haskell.
The functions in your example are *extrinsically* the same, but *intrinsically* different. Extrinsic semantics is about the outputs of the function (your examples give the same outputs for all inputs), intrinsic semantics is about the way the function is implemented. Determining whether two functions are intrinsically equal is easy, but extrinsic equality is very hard to determine (probably impossible in the general case).
I know the company and the developer team leaders there and I can confirm that they are very hostile to haskell and want to get rid of it.
Does GHC put a cap on the number of type parameters a type constructor can take?
Thank you. Do you know why they wanted to move away from Haskell? Was it for technical, social, ecological, or other reasons?
Full beta-reduction does not generally occur at compile time. However, things like function inlining and supercompilation can both be seen as performing some beta reductions at compile time. I believe eta expansion/reduction is also associated with certain optimizations. Runtime involves both evaluation (including beta-reduction) and execution. Compile time is not necessary, but never includes execution[1], and generally involves a trade off between speed and number of beta-reductions performed a head of time. It is also a great time for static analysis, including type checking; static analysis should be able to proceed without any beta-reductions being necessary (though it may benefit from doing some of them). Static analysis is, by definition, always done without and before any execution. [1] ... of the target program. It might execute impure macros to generate (parts of) the source program.
I think one of the speaker at ICFP quipped that, while the language assistant would write the terms down correctly in minutes (for a certain non-trivial problem, like balanced binary search trees), it took him/her 15 years to come up with the types that gave such good results. :)
The company that implemented their entire business in haskell was bought out by another bigger company that have their own software systems and their own dotnet team. Clash of cultures. The haskell system was wastly superior so the management decided to migrate their business to it which they did. But their development team leaders are dotnet evangelists so they were hostile to haskell. I left the company 2 years ago and shortly after me, another haskell dev (there were only 2 of us). They had no one at that point. Their dotnet team sold the idea that they can rewrite it. But they failed to do so. A few days ago, some people from there contacted me to help them get a haskell dev.
&gt; and I don't see how Church encoding would help with the fact that you need to do induction on x to prove the equivalence. By the way, what is bad about using induction to prove the equivalence?
Absolutely nothing, but that's exactly (one of) the part(s) missing from the beta reduce then check for structural equality recipe.
Definitely. If I were to explore this code, I'd probably open up `ghci` and throw some hole annotations on parts.
Well, it can handle at least 700, so if it is bounded, it's not by a number I can casually bump against.
&gt;"Creating instances" from explicit dictionaries at run-time sounds wrong This is wrong and comes from wrong assumption &amp;#x200B; ***as for why it is wrong*** When we write a function working with an abstraction we start with a type signature with explicitly named abstract type. Then we say, that we want a set of operations over said type. Usually we do it by applying a constraint on the abstract type in a form of requiring a set of instances of some type classes for that abstract type. The problem here is that we use type-level **constraints** to express **passing data** (a function dictionary) as an **implicit** argument. Naturally, that implicity sometimes becomes an obstacle, and a desire to pass arguments explicitly arises. Currently the idiomatic solution is to use newtype wrappers. However, Haskell actually states that newtypes have zero cost, i.e. they do not exist at runtime. So, basically, it is a way to say GHC explicitly what dictionary to use, but in a needlessly complicated matter. &amp;#x200B; ***As for why it comes from wrong assumption*** As for assumption: instances" are not created at run-time, they are visible and only at compile-time. Consider function with signature listSum :: Monoid m =&gt; [m] -&gt; m When desugared, it becomes something like -- defined elsewhere -- data MonoidDict m = MonoidDict {mempty :: m, mappend :: m -&gt; m -&gt; m} listSum' :: MonoidDict m -&gt; [m] -&gt; m The compiler infers what MonoidDict to use by checking the context at the place of the call for appropriate instances. At runtime instances do not exist, only dictionaries are passed. This can be verified by checking GHC Core dumps. ==== As for what is actually needed to implement explicit instance passing: We simply need a way to bypass mechanism choosing the dictionary when a function with class constraint is called. At the very least it would require exposing the dictionary as a (specialy type of an) argument to a function. This actually was done in Agda.
I think, you should look at *Instance Arguments* from *Agda*.
that table is rubbish. Don't gauge your competence against some arbitrary bullet list of concepts.
&gt; Make it quick and easy to define new top level instances based on other instances/dicts without any special extensions. I'm not sure how you are going to make (Hash)Set / (Hash)Map work without coherence. In particular, how to do `unions` efficiently. (Do you have some magic way to test the Ord/Hashable dictionaries for equality? Would you lift them to the type level and require definitional equality?) I can't think of sometime you need coherence for Semigroup / Monoid, but it seems like they might exist.
This is my favorite answer so far. Understanding the exception system is really important when writing production code. When performance matters even a little bit, other specific things I can think of include: - Knowing how to look at GHC core and see that unboxing is happening where you expect it to. - Knowing how to use GHC's profiling tools well. (I've still never learned this one)
Sounds like [What are Types for, or are they only Against?](https://www.youtube.com/watch?v=3U3lV5VPmOU&amp;feature=youtu.be&amp;t=3691)
I mean I’m not getting rid of normal coherent unique type classes, so you’d use those. I’m just saying the underlying structure should be a value you can pass around and pass into functions with the right signature.
Current unnamed type class instances are only coherent because you can't name or pass them explicitly. As soon as you add even a single named instance, that class loses coherence, because you could pass the named instance to one call, and let the the compiler pass the unnamed instance to another call. If you want to pass type class dictionaries as values without losing coherence (which *can* be useful), check out the [constraints](http://hackage.haskell.org/package/constraints) package. In particular, the [Dict](http://hackage.haskell.org/package/constraints-0.11/docs/Data-Constraint.html#t:Dict) type.
Look closely at what I’m suggesting. I am NOT saying you can pass in an explicit dictionary to a function that requires a typeclass instance. You can only pass it into a function that requires that object as a parameter. What I’m suggesting can be done in current Haskell, it’s just not as ergonomic as it could be. I want things like: foldList :: Monoid a =&gt; [a] -&gt; a foldList_ :: Monoid_ a -&gt; [a] -&gt; a monoidToSemigroup :: Monoid_ a -&gt; Semigroup_ a I’m not suggesting genuinely supporting multiple of the same instance.
Edward Kmett's [Type Classes vs. the World](https://youtu.be/hIZxTQP1ifo) talk is relevant.
Yeah I've seen that before. Great talk! To be clear I am not against typeclasses or their benefits, in fact my suggestion should actually make it easier to make more typeclasses as you can have them share underlying structure easily, and you will also be able to make instances more easily without relying on deriving extensions / a bunch of newtypes.
Reduction is defined by a rewrite rule on some structure (abstract binding trees are used if you read Harper's books). Beta reduction defines such a relation. You can define a kind of equality called beta equality with out of this reduction or directly using equations. This notion of equality is called beta equivalence.
That paper just contains the formal proofs, I think I found the main paper: https://lirias.kuleuven.be/retrieve/519822/
Ouch. Political reasons.
Does the current crop of dependently typed languages actually have circumstances where type-checking because undecidable (as opposed to type inference)? Genuine question (regardless of which you were referring to in your comment).
Fewer than you would think, because most of them are total languages and therefore every function is guaranteed to terminate. Idris isn't, however, so there you go. When you can call arbitrary functions in your types you don't really have a choice but to actually execute them, and then you run into the halting problem.
sorry
Djinn doesn’t use search, it uses a decision procedure for propositional logic.
It's okay, honestly I was really harsh with how I responded.
F* has an embedded DSL that compiles to C code. Used by the Everest project for verifying cryptography. I'd say you can do all kinds of general-purpose things in C.
I am really excited about Meta-F* (the tactics language), since it allows you do everything in one language, unlike with Coq and it's Gallina/Ltac separation. I know Ltac2 is coming up, but I can't imagine how it would be nicer than what's in F*.
But beyond that you also need to parse the arbitrary abbreviations that are scattered all over the place. Imagine reading a book like that: "It w th be of t, it w th wo of t" - Charles Dickens What is Yo? Yoneda? What about Sem? Semantic? These are abbrevations create ambiguity in the code. Same with having to dissect code like fmap (y . (&lt;$ z)) $ S.get Here's a good article about it: https://medium.com/@egonelbre/psychology-of-code-readability-d23b1ff1258a
You might also be interesting in Idris' Elaborator Reflection. It also allows writing tactics / macros in Idris, and have the "full force" of the type checker / inferencer and term inference / proof search to those tactics / macros. I believe "elaborator reflection" techniques may have started with Idris and have been some adopting / adapting in Agda and other DT languages.
Does that mean he is a proponent of extrinsic type systems like in [Cedille](https://cedille.github.io/)?
Also, ghcup used to fail on Ubuntu 19.04 in an ugly way. I was very disappointed that ghcup doesn't reliably work on the most used linux distribution and it cost me a lot.
&gt; While this Gibbon compiler currently handles a very small subset of Haskell, its eventual goal is to integrate with GHC. What do you anticipate the eventual solution to look like? A type checker plugin? Fully integrated support (e.g. region analysis in GHC)? A compiler extension like `ApplicativeDo`? Something else?
After making a complete mess in my previous comment, here's some tested code... ------------------------------------------------------------------------------- -- Using an explicit dictionary to supply an implementation to be used for -- existing code that has class (Semigroup) constraints. -- -- The use of the M type to provide the Semigroup instance means this only -- works where all "methods" have at least one argument, and also implies some -- overhead. -- -- A hypothetical language extension could maybe solve this problem. The M -- type would be simplified to a newtype as it wouldn't carry around the -- explict dictionary. Instead, the implementation of a magic function -- (replacing mkM) by the compiler would effectively turn the explicit -- dictionary into an implicit class-constraint-satisfying dictionary. ------------------------------------------------------------------------------- {-# LANGUAGE RankNTypes #-} ------------------------------------------------------------------------------- module Main (main) where import Data.Semigroup ------------------------------------------------------------------------------- -- Dictionary type (with phantom type argument), holding just one method for -- now. data D p a = D { getOP :: a -&gt; a -&gt; a } ------------------------------------------------------------------------------- -- As with ST, a key trick is to use a Functor/Applicative/Monad for plumbing. -- The data constructor "WSD" should be kept private. newtype WSD p a b = WSD (D p a -&gt; b) instance Functor (WSD p a) where fmap f (WSD g) = WSD (f . g) instance Applicative (WSD p a) where pure f = WSD (const f) (WSD f) &lt;*&gt; (WSD g) = WSD (\d -&gt; (f d) (g d)) instance Monad (WSD p a) where return = pure (WSD f) &gt;&gt;= g = WSD (\d -&gt; let WSD gg = g (f d) in gg d) ------------------------------------------------------------------------------- -- To use existing functions with the Semigroup constraint, need a wrapper -- type that can implement its instance using the dictionary it carries data M p a = M { getDict :: (D p a), getVal :: a } mkM :: a -&gt; WSD p a (M p a) mkM x = WSD (\d -&gt; M d x) instance Semigroup (M p a) where (M d x) &lt;&gt; (M _ y) = M d ((getOP d) x y) instance Show a =&gt; Show (M p a) where show (M d x) = "[" ++ show x ++ "]" ------------------------------------------------------------------------------- -- The key higher-order-function trick, similar to runST withSemigroupDict :: (a -&gt; a -&gt; a) -&gt; (forall p. WSD p a b) -&gt; b withSemigroupDict f (WSD g) = g (D f) -- Common case that unwraps the result from the semigroup wrapper. The -- unwrapping is done with fmap to avoid "because type variable ... would -- escape its scope" errors. withSemigroup :: (a -&gt; a -&gt; a) -&gt; (forall p. WSD p a (M p a)) -&gt; a withSemigroup f g = withSemigroupDict f (fmap getVal g) ------------------------------------------------------------------------------- -- Examples using Monad notation tst1 :: String tst1 = withSemigroup (++) $ do x &lt;- mkM "hello" y &lt;- mkM " there" return $ x &lt;&gt; y tst2 :: String tst2 = withSemigroup (\a b -&gt; a ++ "," ++ b) $ do x &lt;- mkM "hello" y &lt;- mkM " there" return $ x &lt;&gt; y -- Broken example - attempts to use &lt;&gt; for two mismatched monoids (both -- String, but with different operators) --tst3 :: String --tst3 = withSemigroup (++) $ do -- u &lt;- mkM "a" -- v &lt;- mkM $ withSemigroup (\a b -&gt; a ++ "," ++ b) $ do -- x &lt;- mkM "b" -- return $ u &lt;&gt; x -- Error - mismatch -- return $ u &lt;&gt; v ------------------------------------------------------------------------------- -- Example using Applicative notation tst4 :: String tst4 = withSemigroup (++) $ pure (&lt;&gt;) &lt;*&gt; mkM "a" &lt;*&gt; mkM "b" tst5 :: Int tst5 = withSemigroup (+) $ pure (&lt;&gt;) &lt;*&gt; mkM 3 &lt;*&gt; mkM 5 tst6 :: Int tst6 = withSemigroup (*) $ pure (&lt;&gt;) &lt;*&gt; mkM 3 &lt;*&gt; mkM 5 ------------------------------------------------------------------------------- main :: IO () main = do print tst1 -- "hello there" print tst2 -- "hello, there" print tst4 -- "ab" print tst5 -- 8 print tst6 -- 15 ------------------------------------------------------------------------------- The extension idea is still only intended as a thought experiment, though there is a hypothetical point, and even the code above still allows me to call standard functions with `Semigroup` constraints effectively using an explicit dictionary, though I've only used `(&lt;&gt;) :: Semigroup a =&gt; a -&gt; a -&gt; a`. Another comment mentions "Instance Arguments from Agda", so I'll have to take a look later.
Probably best to ask him, but given my understanding of what he says, yes.
He can create programs usimg complex mathematical constructions. For example, semigroup operations inside monads: &amp;#x200B; \`\`\`haskell main = do print $ "hello" ++ " world" \`\`\` And talk for hours about it.
&gt; Edit: &gt; Wait, I think I got it. The induction has to be on the arguments of the function and therefore it will constitute a proof of extensional equality and not intrinsic equality. Well... Yeah, that's exactly it, now that I think about it.
&gt; extrinsic equality is very hard to determine (probably impossible in the general case). I got a bit curious here, isn't extensional equality decidable in total calculi? Um, no... probably not. So I did a little digging. Here's what I found: https://cstheory.stackexchange.com/a/10609 Looks like you're fine as long as you limit yourself to finite atomic types, sums and products. But as soon as you allow universal quantification without rank restriction, or type-level fixed points, you're no longer fine. The details seem fairly mind-boggling.
Awesome results! If GHC/Haskell and Gibbon/Haskell programs will have different memory layout, wouldn't that require serialization? Is it possible to reuse Gibbon in other compilers?
In the case of Purescript, the compiler is written in Haskell, so you could use that to create your own interpreter. I don't know how difficult that would be. Another approach if GHCJS is an option, is to just compile the Purescript to JavaScript and expose the functionality of your build server thought the JS FFI. If you don't want to use GHCJS, then I would start considering something like Python, Lua or Guile which embed via the C FFI (and which I'm sure you will find wrappers for). But not all of these meet the 'functional' requirement.
I tried to find the parsec entry point in the pure script codebase but I'm not too familiar with parsec so I couldn't really find the magical "parse string to AST" function, unfortunately.
&gt; Python, Lua or Guile which embed via the C FFI (and which I'm sure you will find wrappers for). But not all of these meet the 'functional' requirement. Arguably, none of them do.
The effect libraries are making real the old joke of "you need to have a postdoc in mathematics to write 'hello world' in the console" http://hackage.haskell.org/package/polysemy-0.5.0.1/docs/Polysemy-IO.html
If it's an option, [DukTape](http://duktape.org/) is a mildly lovely (.c) ECMA engine that acts like the Lua VM so ... maybe as an interpreter for PureScript's JS?
hmm... OP never said that he needed to execute a build script also inside browsers, so why not just Haskell + some custom DSL based on it (without GHCJS)?
[dhall](https://dhall-lang.org/) is a configuration language (not turing complete for example), but it's typed, purely functional and designed to be embedded (?) (there is a haskell bindings, at least). It may or may not be suitable for your purposes. (disclaimer: I haven't used it, and the website is not very clear at first sight)
The language itself should support conditional compilation. It's an unavoidable part of software development. There a number of papers from the [TypeChef](https://github.com/ckaestne/TypeChef) project which are about solving this problem and I plan to work on a design which will integrate nicely into GHC in the next few months.
Which raises the interesting question: When writing code, do you assume the audience has access to an IDE? If yes, then type annotations are clutter, since the IDE shows the type, if no, code gets confusing without type annotations.
I'd agree that any technique that thwarts automatic refactoring and editing facilities limits our abilities as programmers to work better.
Have you read SICP and how would you rate them relatively in terms of enlightenment? ;)
CPP _is_ harmful, but we don't have an alternative so far, and we cannot survive without conditional compilation (especially as every new standard library release breaks something)
You're consistently toxic in this sub. Please stop it.
It's very difficult to compare, as these two books cover vastly different topics. CS 101 vs. Formal Methods "101". Also, I read SICP over a decade ago, so my recollections are a bit vague. I think I rather liked it overall, and yes, it was quite enlightening. But I also generally agree with the criticisms of SICP coming from the HtDP crowd. Unfortunately, HtDP itself, unlike SICP, is not a great read IMO (even though the content is great). Fortunately, there are some pretty good HtDP-based courses out there, including Gregor Kiczales' MOOC, which compensate for the chewiness of the book.
There are two options not considered here: &amp;#x200B; 1. Put all hackage source code in a single repository and do large-scale refactoring. This is what large corporations do (Google, Facebook, ...). We have the infrastructure to do this as Stackage keeps tabs on the exact location and repo for all (most) packages. We can do this. 2. Create a namespace (package prefix or similar convention) for abstraction libraries. The linux kernel source code is free from CPP except in specific portions of the source code. This is mentioned in the article, but we can lift this notion up and create conventions and package namespaces for this. We can standardize how we do it.
It's an unavoidable part of \*software development\*, but it's completely avoidable in a language.
Depends of how fancy you want your embedded language to be: * you may take lambda calculus interpreter from Hackage * you may embed JS interpreter, and allow any language that compiles to JS (GHCjs, Elm, PureScript) * you may use \`hint\` package from Hackage, and allow any code that GHC accepts - [http://hackage.haskell.org/package/hint](http://hackage.haskell.org/package/hint) If you do not care about application size or safety properties, I would opt for \`hint\`, since it is extremely easy to integrate. If you care about safety properties, then either embedding Elm or using lambda calculus seems to be an option.
Not arguably at all. This is /r/haskell :D We don't need to be too polite to other languages here.
 replyChan &lt;- newChan writeChan commandChan (Just "", replyChan) readChan replyChan I have implemented a multithreaded program using forkOS and channels and am having issues with exceptions: "thread blocked indefinitely in an MVar operation". The exceptions are non-deterministic in the sense that the runtime to get such exceptions varies -- a lot. I am wondering, is there a race inherent in the above code block? It is looking as though the readChan is normally what is causing the exception. Perhaps the runtime thinks there is nothing on the other end of the reply channel until the other thread receives the writeChan?
Would it be practical to introduce a proper syntax to replace CPP, like Rust's attributes?
I agree wholeheartedly with this post. And I think the best practical way forward is to start introducing features to deal with at least some of the cases, in an "in the language" way, so that tooling stands a better chance of being able to process that source. Then the the truly weird cases can still use CPP, but a large subset of code would not need it. &amp;#x200B; And a simple conditional syntax would probably do it, for the first iteration.
In this particular `&lt;&gt;`-example, **Push all configuration to the build system** is an option. ``` if !impl(ghc &gt;= 8.0) build-depends: semigroups &gt;= 0.18.4 &amp;&amp; &lt; 0.20 ``` As i have written recently https://oleg.fi/gists/posts/2019-06-03-compat-packages.html &gt; Decide that I don't care about old GHC (or in general any old version of a package dependency) and move on. can be separated into 1. **Do** care about old GHC (and boot libs), compat packages will help you in that (which are *Abstract away compatibility concerns*) 2. **Don't** care too much about old versions of "upgradable" packages
&gt; the website is not very clear at first sight Do you mean something specific? Do you possibly have a suggestion how to make it better?
Is #2 different from the *-compat packages on hackage?
We should always be able to consider our communication respectful, here in /r/Haskell.
Agree with that too, indeed.
Does anyone have experience loading functions from a dhall file and evaluating them in Haskell or Python? There's a number of times in my job where is would be good to factor the "important" logic into a separate configuration file that other teams can help maintain, while insulating them from the "fiddly" (but, honestly still important) bits. In one case I developed my own XML-based sub-Turing language which worked fine, but I'd like a more general solution (X^(4)X2XT was fit-to-purpose, and still awkward).
I thought Idris would only reduce total functions in types, so no halting problem there exactly, just a refusal to consider two types the same that should be.
Thank you, just a follow up. If I want to get currentMonth as an Int will something like: let min = unsafePerformIO currentMonth Or let ioMin = currentMonth Let min = únafePerformIO ioMin Works or still have the same issue?
The parser entry point isn't terribly interested since you presumably still want to typecheck your code. You can provide the compiler as is with a `--codegen=corefn` flag which will spit out corefn.json files in the output directory. CoreFn is the minimal functional IR the compiler works with https://github.com/purescript/purescript/tree/master/src/Language/PureScript/CoreFn and it is very straightforward to write an AST interpreter for it. The hardest part will probably be providing the stubs for foreign imports.
I'm fine with `Yo` has a value constructor, not has a type name / constructor. I agree that `Sem` should not be abbreviated, since it is a type name / constructor. Abbreviations hurt readability in every language, because they increase the cognitive load by growing the size of the implicit context you have to track. They also contribute to separate maintainability problems whenever you have to deal with the same abbreviation used in different contexts, so you are context-switching while trying to trace data or control flow -- not a fun exercise. Haskell isn't unique here. I honestly wouldn't mind if more libraries on Hackage used "enterprise" or "Java" style type names / constructors -- explicit, even if that means long. field accessors and constructors (or lenses) should have shorter names, since I'm physically typing them more (and the types make the context explicit), but long type names would be an improvement.
&gt; do you assume the audience has access to an IDE? No. I assume they are reading the source code in printed form.
I agree CPP is harmful, and should be avoided where possible - compat packages, carefully ordered import lists, careful abstraction boundaries and type classes can all be used. But some of the other solutions like duplicating whole modules and shoving logic into the packaging system are (in my view) worse than CPP. I tend to have about 1-3 modules in most of my projects with CPP, and avoid it for the rest. Of course I wish Haskell had better ways than CPP, but it doesn't, CPP is sometimes the most appropriate way, so is the right thing to do.
Thank you! Assuming I care about enlightenment more that reading pleasure, would I have the same enlightenment with SICP than with HtDP? I get the drift that SICP is a bit wider in scope when it comes to thinking about computing, while HtDP narrows in on language designs.
Because many Haskellers are not using an IDE or because you think plain-text is a good format? If yes, I would guess that compatability is your main argument for it, no? I ask because I would love to explore whether moving away from plain text could be beneficial. Moving explicit type annotations from code to IDE seems pretty compelling to me.
The major difference is the eponymous design method in HtDP. SICP doesn't really go into that. (But note that the design method in question is by no means a silver bullet, it's a *more* systematic approach to programming, but it doesn't by any means make programming "easy".) Having said that, can't really answer your question as such, too much depends on personal inclinations and preferences imo. Give both a shot maybe, see what works for you?
I don't always have my preferred tools when editing code. It fairly important to me that I can be productive even with minimal tools, preferably only the compiler / interpreter. I'm certainly for good IDEs, but I don't want one to be essential for making changes. I personally prefer add-ons / plug-ins for an existing high-quality text editor, especially some variant of vi since vi is required for all UNIX systems.
/u/Iceland_jack and I fiddled around with using ideas from Ghosts of Departed Proofs + reflection to build instances at runtime. Coherence is guaranteed by an existentially-quantified phantom parameter, like you were suggesting. https://mobile.twitter.com/BanjoTragedy/status/1009102149605838848
Just delete `unsafePerformIO` from your brain for now. It's almost never what you want, even when it's what you need.
Well I looked it, and even though I heard quite a lot about this project before, and had a general idea what it is about, it was very confusing, and I couldn't easily more details to refine (or revise) my previous knowledge I think, first of all, every project web page should start about "what this project is about". And I don't mean big catchy colored sentences, but in no-nonsense language. Like the first thing you see when loading the page is "Human-friendly", then the second thing you see is "Turing completeness is not a feature" - ok but what the hell it is??? It's only the third biggest thing, and even that is well hidden by the b&amp;w colors and all the other noise on the screen, is that it is a "configuration language". So that's one thing. Then I couldn't really figure out what kind of type system it has (at first it's not even clear it has a type system, but I knew that already). I still have no idea if you can even introduce new datatypes, though I know that it has polymorphism (but the only reason I understand what that is because I already know Haskell). Then many of the introduction material is a long series, starting from something long and complicated which is _not_ dhall, doing lots of transformations, ending after many pages in something which _is_ actually dhall. I mean, yeah this is pedagogical, but before doing that, the reader should know what the hell is the goal. So I still have no real idea how the syntax even looks! Then the home page links to the github page, so all of a sudden you are at a completely different page with different organization, different interface elements, different header (which is actually github's header, but just one seconds before it was dhall's header! In fact there are _two_ new headers!). This is all very confusing. Somebody looking it at first does not have the patience to go through all the material, they want to know what the hell this is about, and whether it has any chance of being a good fit to their problems. And so on...
Maybe it's better to ask them using other channels, like discourse or stackoverflow or twitter (these are the ones linked on their homepage). There is a subreddit but it's empty.
In 15-20 years when developers near complete the ecosystem and the broad and deep community becomes self-supportive.
Right now I don't have a replacement I can uniformly apply for CPP in Haskell. Have a conditional export? CPP. Have to carefully manage imports because you are a library and want a wider dependency range? CPP. You can work around this in some places with `template-haskell`. By all means. If course, if there is one library, which always makes me use CPP to work around it... it is `template-haskell`! It changes in non-backwards compatible ways with every release. I do like the idea of, say, using backpack to handle platform-specific functionality rather than CPPing around Win32 vs. Linux etc. concerns. For "big switch" cases like that it can be a huge win. It can be more annoying for "small switch" cases like linux vs. mac where for many applications _almost_ everything is the same POSIX-like backend with a few minor wibbles.
okay, apparently (\`\`\`) doesn’t work for code blocks. lol
I'm very much in favour of this sort of thing. I wrote about something similar "[Scrap all your type classes but one](http://h2.jaguarpaw.co.uk/posts/scrap-all-your-typeclasses-but-one/)". It is also related to Opaleye's use of `Profunctor.Product.Default`. [Compare](https://hackage.haskell.org/package/opaleye-0.6.7004.0/docs/Opaleye-Sql.html), for example, `showSql` and `showSqlExplict`. I think your idea deserves a longer write up. I'd like to see more specifics about what your propsing and discuss the design with you.
Sounds intriguing. Can you give an example?
To add on to what the other poster said, to use it within the IO monad itself, you don't use unsafePerformIO, rather you use binds, or do-notation. There's an official section of the tutorial at [https://www.haskell.org/tutorial/io.html](https://www.haskell.org/tutorial/io.html) that may be able to help you with working with IO. &amp;#x200B; I couldn't figure out which FFI you were working with, but it should treat Haskell functions that return IO correctly, and allow them to be called from elsewhere. So, if you have a function \`foo: Int -&gt; IO Int\` in Haskell, or the like, it should ideally appear in Java as \`int foo(int)\`. If that's not the case, let me know and I can try to help you.
I wish there was a way to serialize the current interpreter state. The datatype could be called `FroYo`.
"schema evolution" or "data migration". I've used e.g. \`safe-copy\` for haskell binary serializations and it worked okay but was awkward (and I don't think I'd use it for client-server communication like you're describing). It's a big subject I wish I was an expert in. &amp;#x200B; You can start here for an overview of some modern-ish serialization formats and how they've approached (or punted) the problem: [http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html](http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html)
For platform dependent code, I don't see how to put it into the build system: I have a program with platform dependent backends, e.g. `Midi.JackMidi` and `Midi.CoreMidi`, but I still wind up with CPP to choose which one to import. I also use CPP for export lists for tests: `module X where (a, b, #ifdef TESTING module X #endif)` It would be nice to have in-language solutions to both of these!
I just made a small patch to `cborg` which uses CPP to support pre- and post-0.7.0.0 `primitive`: https://github.com/well-typed/cborg/pull/203/files I suppose the other option would have been to shrink the support window to `primitive &gt;= 0.7.0.0`. Any thoughts about this usage of CPP?
What does it mean to be a Haskell expert? I don't know. You're what people think you are. So start position yourself as a Haskell expert and with a bit of luck you'll be one.
This cabal issue has discussion, plans, and some existing tools that have been built with regards to the store: https://github.com/haskell/cabal/issues/3333 The source archives can always be cleaned.
I think the TypeChef stuff and backpack will integrate really nicely. For example, I would like to extra backpack signatures from a library, and then combine them into one unified interface with min versions on each definition. Then, if I do this for each dependency of my library, TypeChef will ensure that I do the min versions correctly. Machine-checked multi-library support!
yeah but also non-portable single-platform packages considered harmful.
Haskell is amazing. But it lacks a killer software such as node/spring/laravel/unity/qt etc. It's fun to write haskell but once you realize your compilation to 'insert language' sucks you know you also need to learn 'insert language' to make quality software.
interesting that author is not a math nerd but esoteric oriented..
Do `Midi.JackMidi` and `Midi.CoreMidi` have the same signatures? Perhaps [Backpack](https://github.com/danidiaz/really-small-backpack-example) could help. Either by making you main code "abstract" and instantiating it with different signatures, or more simply by using the [mixins](https://www.haskell.org/cabal/users-guide/developing-packages.html#pkg-field-mixins) clause to equalize the names of module dependencies (which you could then activate or deactivate in the .cabal file). For example, one can use mixins to [substitute](https://www.reddit.com/r/haskelltil/comments/9qa366/easy_way_to_replace_default_prelude_with_the/) the `Prelude` without `-XNoImplicitPrelude`.
what about simple native compilation ?
\&gt; Tracking multiple effects is a pain that does not exist in most languages Oh yes it is...you just can't feel the pain via the type system because there is no type system for it.
Becoming a karate master means, more and more as you learn, knowing how *not* to use karate to solve your problems. Haskell works the same way.
As someone else here once wrote, the ability to specify a program with mathematical precision is Haskell's killer feature but sadly that's a concept lost on most programmers. &gt; your compilation to 'insert language' sucks you know you also need to learn 'insert language' to make quality software With Haskell, I'd rather spend time writing correct software and suffer the compliation times than debug a runtime production bug. As latter is open ended in terms open ended. This is another notion of being able to specify a program with mathematical precision.
&gt; The haskell community is very excited and they have many things to teach. There are many Haskellers out there who are very welcoming, kind, and intelligent. I agree with this! In my experience so far the people who post on forums and the Haskellers I've interviewed with have been welcoming and professional. Everybody knows it's a difficult journey, and in the average Haskell huddle you'll have the whole spectrum of experience levels. The language is notoriously difficult to learn so I think the best counterbalance to that is a welcoming community, which this is.
Iam still fairly new. Whilst I love the language I do not think it is the best *general* purpose programming language. I would see why Rust might be called that or Scala. Even Python. But haskell is a niche language. A nice language in a nice niche though (say that 10 times quick!).
I think Haskell is great but pretty niche. Consider giving Rust a shot. It also has a learning curve, and composition is not exactly as easy. However the type system is great and it runs fast.
`TypeChef` is really only one small piece of the story. Why do we use CPP? We typically use CPP to deal with different versions of upstream dependencies. So to be properly "version aware" you need to have some sort of semantic map between c preprocessor flags that your build system will create and the actual versions of the upstream dependencies with changes that correspond to the given flags. To compile against all configurations it isn't enough to locally be able to investigate how all the flag combinations work in your project, but you need to also map those flag settings to versioned copies of the upstream dependencies. I've yet to see someone supply a story for such a mapping.
`noFactors` is not smart enough. It has to stop demanding elements from `b` before they are generated. Generally this is done my assuming `b` will in increasing order and only operating on the part that is less than (or equal to the square root of) `a`. Alternatively, you could just use the example code from the front page of [Haskell.org](https://www.haskell.org/).
When we played with this same idea inside of `lens` we called it "coindexing". We eventually let it go because our implementation was incompatible with the indexing at least from a type inference perspective. I haven't dug into how your implementation of the idea works, yet, though.
What is Haskell's niche? Writing compilers?
I originally tried to make the lenses general enough to be compatible with all the normal lens combinators but ran into very similar problems. Eventually I compromised on having compatibility with most lenses and traversals, but needing to use custom combinators to actually "run" the operations. It's annoying since a single error lens will "infect" the whole chain, but I've had enough people ask me how to do something like this that I think it's probably handy to have it somewhere; even if it doesn't really fit within the lens ecosystem "proper". Open to any suggestions you may have though!
The process of collecting all in-bounds versions of the dependencies to create the combined interface (which depends on first on collecting all in-bounds versions of *their* dependencies and doing the same for sake of public interface decencies, this is a bottom-up process) creates solution-agnostic interfaces. Type-checking your library against those in TypeChef manner means your library will work in all possible solutions. What's the problem here? The one issue I foresee is this bottom-up process means to check library there must not be any PvP/CPP violations in any of its transitive dependencies. That makes migrating the ecosystem a lot harder so there would need to be some sort of package enclave" where base and other packages are migrated over. Hackage what these cumulative interfaces are, and refuse any release that doesn't meet it's comparability obligations. I imagine the politics of all this will be...fun :).
Is c# niche? At the language level, I don't see why haskell is any more niche than c#/java/any other middling fast non-scripting language. The difference is that a ton of people poured a ton of time into the java and c# ecosystems, and a ton of people have poured a ton of time into learning the languages. Haskell doesn't have anywhere near that investment, so it doesn't seem like as obvious a choice for many tasks.
Hm, by "niche" do you mean obscure/unusual/different? Although I haven't gotten to use rust seriously I think "niche" would more appropriately describe rust. If you already know both languages (and ignoring issues of library availability) I think haskell would be more appropriate in the general case, while rust would shine when e.g. \- you're distributing software (not hosting it) in such a way that e.g. a space leak would be a showstopper or difficult to fix \- you want to target actually resource constrained embedded devices, where GHC might not run (and also where distributing updates might be difficult) \- you're writing a game engine, browser engine, etc and really need to squeeze out as much performance as possible by carefully managing allocations, using SIMD, etc. &amp;#x200B; I've never had a job where I was called on to do any of the above, but I'm sure for others the meaning of "general case" is inverted :) I think the languages compliment each other pretty well (although I think rust's concurrency story is way oversold...)
Sixten looks like a cool project, and it definitely has some different ideas going on in the front-end. We have nothing like the type-representation polymorphism that sixten does, instead we are in the specialization/monomorphization camp. &amp;#x200B; Conversely Gibbon/LoCal support denser (unaligned) data and does a bunch of work to convert idiomatic functional programs (e.g. regular ML/Haskell style code) to operate instead over dense representations. We will give sixten a try for tree processing...
Consider FFI. CPP is used to work around a bunch of platform specifics, not just to check that Haskell code works with Haskell code. Say I write a library that depends via `pkgconfig` on a particular version of the external `harfbuzz` library. I get from the system that tell me the version of the `harfbuzz` library I compiled against, I also get yet other runtime information about the version I was linked against at runtime. The `TypeChef` story doesn't seem to include how to map between these high level defines and the actual copies of the source code they control, just ensures that if you had multiple configurations that were all valid against the bundle of source code you have and the target machine you happen to be linking on that those configurations will be all tested. Latest against latest with all the flag combinations doesn't test against old versions of the downstream dependencies that you might link against. It doesn't deal with finding those system headers in the first place.
This sounds like a perfect use for backpack.
We have considered several options on this front. Think of something like the Accelerate EDSL where Haskell calls "run" on Accelerate code (but Accelerate doesn't call back into Haskell). We're going for that experience but without all the syntactic noise related to embedding EDSL syntax masquerading as Haskell. Rather, it is possible with the right compiler settings to get the transitive closure of the code callable by a given "Gibbon-accelerated" function, as GHC Core, within a compiler plugin. After snatching the GHC Core, our compiler can do its thing while assuming a closed world. From GHC's perspective, the Gibbon-generated code is called through the FFI, like other C code. The data coming across the Gibbon/Haskell boundary is in the packed form, of type `Packed a`, but is basically a bytestring. In this setting, Gibbon basically lifts functions of type `a -&gt; b` to `Packed a -&gt; Packed b`. It's purely an optimization though, not changing the semantics. The idea is to retain the ability to run through plain GHC, where lift becomes a deserialize/serialize round trip. Gibbon has an additional prerequisite, however, that Liquid Haskell's termination checker should bless the code as terminating. Gibbon does not support lazy evaluation, but for terminating, pure programs, returning a Packed value in normal form, there should be no difference.
Thanks for the link! Good to hear I'm not alone about this issue.
sadly it's also something lost on the industry I wish ml/haskell and cousins would have a higher market share..
It could be! I don't know how deeply intertwined backpack is with cabal, but if it is, then it might not be worth trying to integrate it, since I don't use cabal. Also I'm leery of taking on a complicated experimental extension to get rid of a few lines of CPP, but as backpack gains users and acceptance it could shed the experimental part. Is backpack is gaining users and acceptance though?
type level invalid state avoidance
Sounds like a useful feature in any domain... Perhaps the claim is not that Haskell is only good for a small number of domains (which I think is untrue), but rather that Haskell has unusual features (which I think is true)?
&gt; Latest against latest with all the flag combinations It's not just latest against latest. The foreign case is mechanically harder because of the lack of readily available interfaces, but I think no different in principle. Here's a "trace" of how the certified hackage would grow: Release "foo" with version "1.0.0" &gt; data Foo = Foo Int &gt; f :: Foo -&gt; Foo "foo"'s cumulative interface is &gt; #if MIN_VERSION_foo(1, 0, 0) &gt; data Foo = Foo Int &gt; f :: Foo -&gt; Foo &gt; #endif The backpack signature is conditional in the TypeChef manner.(It probably wouldn't be exactly CPP to rule out some nonsense, but I use this syntax to avoid being creative :).) Release "foo" with version "1.1.0" &gt; data Foo = Foo Int &gt; f :: a -&gt; a "foo"'s cumulative interface is now &gt; #if MIN_VERSION_foo(1, 0, 0) &gt; data Foo = Foo Int &gt; f :: &gt; #if MIN_VERSION_foo(1, 1, 0) &gt; a -&gt; a &gt; #else &gt; Foo -&gt; Foo &gt; #endif &gt; #endif Any two interfaces can be squished together in this way. One can machine check subsumption/PVP relation across versions from this. If somebody then tries to write this for "bar" with `build-depends: foo ^&gt;= 1.0.0`: &gt; import Foo (f) &gt; g = ... f (0 :: Int) ... f False ... They should get a TypeChef counterexample where the model induced by the cabal bounds contains a configuration (foo version is 1.0.0) leads to a type error from the use of `f`. If the author of "bar" fixes the cabal file to have `foo ^&gt;= 1.1.0`, now the counterexample is outside the model and there's no more problem.
Sure, but there's all the times you're manually threading effects around because it's actually less effort to handwrite the (&gt;&gt;=) implementation one or two times than set up the transformer stack you need. Not to mention writing monadic code can be decidedly less ergonomic than non-monadic code, as well as less ergonomic than effectful code in other languages.
I have no way to compare ergonomics of controlled effects in other languages since it doesn't exist.
I do welcome a concrete implementation that smashes together all of the versions of all of the upstream dependencies in such a manner. It is, after all taking a concrete step towards that notion of the other layers you'd need on top of `TypeChef` that I mentioned above. The foreign case is the thing that basically blocks this from being practical to me, at least at present. It seems like the kind of thing that some kind of a nix-on-steroids could possibly solve some day. Intra-Haskell dependencies aren't my major concern like I noted. I'd like to see them resolved, but these OS-specific, third-party dep specific concerns are likely the "meat" of the problem. At least they are where I get unavoidable CPP, even when I'm willing to lock myself to just the latest and greatest dependency versions with no backwards compatibility mindset.
I personally use it a lot for scenarios just like this, now. I don't know about broader acceptance, however.
The blog post linked at the end listed people's actual problems with Haskell from the Haskell Weekly survey. I don't understand why that is put under "Haskell hate". The Hacker News post also has some legitimate criticisms.
Yeah, but I think with time--albeit slowly--that will improve.
Have you seen [the `Dhall.Tutorial` module](http://hackage.haskell.org/package/dhall-1.24.0/docs/Dhall-Tutorial.html)? [`spago`](https://github.com/spacchetti/spago) is an application that uses the Haskell bindings. Regarding Python, [`dhall-python`](https://github.com/SupraSummus/dhall-python) seems not to have much progress in the last few months. If Ruby could be an alternative, the [Ruby bindings](https://git.sr.ht/~singpolyma/dhall-ruby/) look pretty usable.
Regardless of the merits of the language itself the tooling is simply too bad to take it seriously... But then again people are also using Python in production despite its awful dependency management.
While I get your point, I feel it necessary to highlight that you've misunderstood what is meant by "general purpose programming language" - see https://en.wikipedia.org/wiki/General-purpose_programming_language
&gt; Give both a shot maybe, see what works for you? Given that I like SICP really much, I think I will stick with it and familiarize myself with the eponymous design method separately. Thank you very much for your insight! :)
I'm not talking about controlled effects, only effects. You kinda have to use Haskell's controlled effects for comparison, because uncontrolled effects in Haskell (i.e. unsafePerformIO) are *extremely* unergonomic. However, often the effects you're interested in only scope over a few lines of code and don't benefit from any extra control from the language's side anyway. And there are ways of controlling effects outside of using the language to constrain them. It's not like when you're programming in Java or C# you have manually verify the correctness of the program every time you're touching a line of code, and it's not like Haskell allows you to edit with reckless abandon. So yes, sometimes it's nice that effects are controlled. Other times it gets in the way. It's not clear to me that controlled effects alone pay for themselves. Though there are other aspects of Haskell that necessitate that effects be controlled in the way they are that for me make it worth it.
Fair points! I also tend into the same direction, but I am not completely sure yet. Sometimes a small Xerox infection occurs and I fantasize about developing code directed by music (idea would be that the music encodes interesting information about the current location/file).
&gt; I do welcome a concrete implementation that smashes together all of the versions of all of the upstream dependencies in such a manner. Wonderful &gt; It is, after all taking a concrete step towards that notion of the other layers you'd need on top of TypeChef that I mentioned above. I'm missing and curious what the other steps after are? &gt; The foreign case is the thing that basically blocks this from being practical to me, at least at present. It seems like the kind of thing that some kind of a nix-on-steroids could possibly solve some day. Yeah I can imagine rigging some libclang-based thing to do this. There is already work in Nixpkgs breaking up packages so e.g. linux vs macOS stuff align 1-1 better (https://github.com/NixOS/nixpkgs/pull/37840 is probably the best example of this, next up on my radar is separating gcc from libgcc and the other usuallly bundled libraries). Slicing and dicing packages like moves some of the configuration to being iter-module rather than intra-module, which probably helps make this easier (in other words, horn clauses where possible). I've been trying to push this same stuff in Rust for a while (https://github.com/rust-lang/rust/issues/41619 is lying in waiting). If we both do it, that could make for very nice FFI. Longer term I hope to completely make an end-run around C, and really poorly factored monster libraries like libc. (https://github.com/retep998/winapi-rs would be an example of really nicely done OS interfacing over there.) TypeChef is important until we get there, and still useful once we are there, but hoping to always keep it the tool of last resort. Besides the horn clause thing, inter-module variation is also perferable in that it steers things exokernel-like designs. Nix/nixpkgs is still useful in that you aren't beholden to distro maintainers who don't share the "ban all C!" goal.
I think the first sentence here categorizing Haskell as niche is the only reason for the downvotes. Rust is great. That said, I'm more productive in Haskell. In fact, I usually use Haskell to make a rough prototype of any project I'm about to do in Rust.
&gt; I'm missing and curious what the other steps after are? Other steps include figuring out how to deal with the cross-platform compilation specifics and external dependencies I mentioned.
Unfortunately, Haskell would be easier that Ruby, and either would be much harder than Python2.
The thing that I like the most about Haskell is how the compiler protects me from a whole host of annoying mistakes that I can easily make in other languages. My code seldom compiles on my first (or even second) try, but when it finally does, 90% of the time it's more or less error-free.
So you mean polyglot version solving? I was going to mention https://github.com/input-output-hk/haskell.nix for in general mapping Cabal into Nix more naturally, but maybe what's also needed is the opposite: teaching Cabal more about *multiple choices* for native packages so it can solve everything all the way down. If Cabal can do all the solving, Nix can do all the build executing, and anything Haskell code sees (Haskell or foreign) has these "cumulative interfaces", I think things will work out quite nicely :).
Its not so much about solving anything so much as it is about doing the exact same hybridization step that you proposed for Haskell where you smash all the different upstream versions together with some magic defines. You'd need to do that to all the external libraries you depend on, and then figure out a corresponding abstraction for the platform to do this in full generality.
I for one think that's totally legit. I get the feeling this is controversial though, which is why I kept this polemic out of the post. \`primitive\` isn't a GHC boot library, so is easy to upgrade. And Stackage has made it a lot more likely that those users of yours who will be using the latest version of your library will also be using the latest \`primitive\`.
Right, yeah the rest is icing on the Cake. More to that point, hnix should make adding abstract interpretation easy enough so we at least know what we need to feed into "libclang-smasher".
Yeah, typeclasses are extremely useful in cases where a type can only have one valid instance of a typeclass for it (take `Functor`, for instance).
&gt;But it lacks a killer software This is my Haskell motivation. I want to keep putting Haskell to work on tasks it isn't best-in-class for and use my time and energy to help push it towards that level of quality. And there's nothing quite like the satisfaction you get from writing a good Haskell library :)
To me controlled effects allows us to have things like `STM` or a similar `DbTransaction` monad. That *all by itself* pays for itself in my book. Having an effect-constrained monad for database transactions makes things like serialization isolation level not only possible, but extremely easy to achieve in Haskell. Trying to safely achieve that with confidence in other languages is a nightmare and rarely done just because it's so hard. For DB-drive apps, I think this feature alone pays Haskell's way. But that's not counting the other numerous places where I've been bitten by lack of controlled effects. In embedded C I often want to viciously restrain mutation of certain variables, and often resort to extremely carefully crafted macros that do half the job of a good monad in Haskell. I actually think controlled effects is Haskell's #1 killer feature. It's why I like Haskell more than OCaml, F#, and Rust. Those languages have features I wish Haskell had, but I can't lose controlled effects.
If you adorn every function with `IO` then Haskell just becomes the same as every other language. It's totally possible to write code that way. It's just plays so little into Haskell's strengths that no one does it.
One thing I've used successfully in the past is [`hint`](http://hackage.haskell.org/package/hint), a library which uses the GHC API to run arbitrary Haskell code at runtime. I've used it to provide scripting functionality for [my cellular automaton simulation program](https://github.com/bradrn/cabasa), where the user can specify an arbitrary cellular automaton rule using Haskell. Keep in mind though that there are limitations to `hint`. Most troublingly, it has difficulty with using dependencies outside `base`: you can do it if you run your *entire program* inside `stack exec` or `cabal exec`, so the environment variables are set up properly, but it's very difficult getting this to work standalone. (For more information, see [this post and replies](https://www.reddit.com/r/haskell/comments/bcoahc/using_dependencies_with_hint/)). Additionally, if purity and/or totality are really important, `hint` may not suit your needs: while you may require the user to write something of type (say) `Int`, which prevents IO from running, they can still use `unsafePerformIO` to do stuff, or they could use `undefined` or `let x=x in x` to crash the program through an infinite loop. If such matters are a concern for you, Dhall is probably a better choice (as recommended in other replies).
It's not the same. You can't use statements inside expressions in Haskell the way you can in other languages. You can't write `putStrLn ("Hello " ++ getLine)` and have it work. It might not seem like a big thing, but it really is if you're doing *everything* inside a monad. In particular, it makes working with mutable references very cumbersome in comparison. Which is fine when looking at Haskell as a whole, but that's not what we're doing in this comment chain.
I managed to make Haskell a fundamental part of my company, but I was in a position where 1) I was the only developer on my team and 2) I was developing our core tech and no one was in a position to tell me I was crazy. Nearly four years later I firmly believe switching to Haskell was the best move we could have possibly made and I still have the full support of the rest of the company. As for suitable projects, I would say anything that requires the safety guarantees that Haskell provides and/or benefit from using a fast compiled language. Without knowing more about your specific use cases I can't say much more. For us our core software had very strict requirements of providing reproducible output, being extremely fast, and being stable in the face of frequent large scale refactoring. That screams Haskell. I suppose if you have some mission critical part of your ecosystem that can reasonably be written in a separate language and needs to be fast and safe that would be a good place to start.
The last time I could find that this question was asked is 10 months ago [https://www.reddit.com/r/haskell/comments/96whuo/what\_is\_the\_current\_state\_of\_dependent\_haskell\_as/](https://www.reddit.com/r/haskell/comments/96whuo/what_is_the_current_state_of_dependent_haskell_as/)
Well...if you take my comment *extremely* literally, then `putStrLn` wouldn't take `String`, it would take `IO String`. And `++` wouldn't be `String -&gt; String -&gt; String`, it would be `IO String -&gt; IO String -&gt; IO String`. This is, of course, silly. But it's part of my point that it's actually more that Haskell's ecosystem has chosen to leverage this distinction and not so much the language. For some set of definitions, `putStrLn ("Hello " ++ getLine)` does actually work. You can even have `IsString (IO String)`! &amp;#x200B; Surely this is no longer recognizably Haskell...but taken to a lesser extreme, you can, to some degree, decide how much of this "purity business" you want to buy into.
So we've strayed a bit from my original point about monads not being very ergonomic, but okay. Clojure has STM with a similar model to what we have in Haskell\*, and from what I've gathered nobody thinks that was a mistake despite allowing arbitrary effects in transactions. In all my years programming with databases, side-effects in transactions have never been an issue. Sure, it's easy to imagine things going wrong with both those things, and from time to time they *do* go wrong I'm sure. But saying the language *must* control effects instead of letting the programmer do it doesn't agree with my own experiences. I'm not even saying controlled effects aren't worth it. I'm saying I don't know if they are, because there are real costs going both ways. I do know that often I prefer to thread state manually instead of using a State or Reader monad because doing it manually is less work and makes the code shorter and less confused. &amp;#x200B; \*As opposed to making all memory transational inside a transaction. PyPy is going that route, which seems... ambitious.
I did not. I have a hard time thinking of more than 10 proper DSLs... and the rest of languages are - by the definition - general purpose. Thus it is such a low bar that we might as well talk about more interesting differences.
Strayed indeed, but you're enjoyable to talk to so I'll keep going! ;) Clojure being the major exception yes...but you can't make your own STM-like things in Clojure AFAIK. Side-effects in transactions aren't nearly as problematic if you're not using serialization isolation which forces you to rerun your transaction on a regular basis (thus effects become duplicated). So people normally compromise with lower isolation because their language doesn't give them good control over managing this danger. Of course, *anyone* can do *anything* in *any* language with sufficient discipline...it's really about level of effort to get it right and keep it right. You're definitely right that the value of monads is hard to measure conclusively. The original comment, however, about the pain not existing in other languages, is simply misguided. We experience the pain, just in very, very different ways. However, I didn't say it was *zero* pain in Haskell. It definitely comes with problems/pain (which is the pain he's referring to). It just can't be compared to some perceived lack of friction in other languages that don't have comparable features. PyPy I think only uses STM internally to replace the GIL. I don't think it's available as a library to users to write their own composable, atomic operations. (Would love to be proved wrong tho)
That's a good way to put my thoughts. Thanks!
It's still not quite the same since in other languages `readIORef` is the same as `id`, so in the hypothetical everything-is-IO Haskell you still can't refactor `putStrLn getLine` to `do x &lt;- getLine; putStrLn x; putStrLn x`. It's getting pretty silly at this point, but still, monadic code feels a bit like assembly with a really powerful macro language on top. You have to be very specific about ordering and can only have one effect per statement, even if that effect is trivial and, in the context, pure. But you do get higher order functions to help you out.
Thanks, I'll give it a try. It does seem like the sort of thing backpack was designed for, so I suppose I should take advantage of ezyang's hard work.
Can't dhall be made Turing-Complete with the dhall equivalent of the Haskell ~~~ data IO a where Pure :: a -&gt; IO a Bind :: IO a -&gt; (a -&gt; IO b) -&gt; IO b Fix :: (a -&gt; IO a) -&gt; IO a GetLine :: IO String PutLine :: String -&gt; IO () ~~~
The technical name is the expression problem. Usually this is solved with tagless final style. I don't know how you'd solve it with serialization though.
What do you mean by your own STM-like things? You only get refs and transactions, but that's what you get in Haskell too. Everything else is built on top of that in libraries. I also feel like pointing out the major reason why STM works in Clojure is how it's immutable by default. This makes it okay to update your values in the middle of a transaction, unlike if you're working with a mutable Vector or HashMap. It's also why they dropped the idea in C#. I should have been more precise when I said side-effects in a transaction weren't a problem. Side-effects in a transaction *are* a problem, but there are no side-effects in my transactions, so there are no problems :) You could call it discipline, but it's really just the simple coding style I use (and taught my team to use) that makes everything more pleasant. You just mark everything final. Ref immutability. I don't really follow PyPy closely, but I think there's both the effort to get rid of the GIL called PyPy-STM, and an effort to expose STM as a library, but that project is just under the PyPy umbrella.
Rust concurrency tools are second to none for resource constrained programs on predictable environments. Haskell concurrency tools are the best around for highly scalable, unpredictable programs that do not need to squeeze 100% of performance from any single CPU. And C is still unparalleled for high performance computing.
ghc and haskell-ide-engine.
Pure flippancy. I meant nothing more by it than there is a generally negative opinion in those posts. I should change the wording if it implying that the criticism is unwarranted.
It's up to you. To me, "hate" is a fairly strong word -- I would personally not put fair criticism/pain points under the label "hate".
It is very interesting to see the push back on this post, which I really never thought almost anyone would see, otherwise I would have been more careful, polished, and thorough. I had a discussion around the lunch table about it, and could really see the argument that the term "general purpose language" is not a very good one, as one can always find problem domains that any language is not a great fit for. But I think the point still stands in that I believe Haskell is far more useful for more programming tasks than it appears from the outside.
The distinction may be stemming from my physics background. There is a friendly rivalry between mathematicians and physicists, and a healthy distrust of their wily, mathy ways. Sometimes, they take it all a bridge too far! On the other hand, I do see the phenomenon where things are continually being moved from my "esoteric math nonsense" list into my "oh yeah that makes sense" list. And yet the esoteric list never grows shorter?
Edited it out. Thanks for the suggestion.
[pandoc](https://pandoc.org/) usually comes up in these discussions.
I wrote Termonad, a terminal emulator configurable in Haskell (similar to how XMonad works): [https://github.com/cdepillabout/termonad](https://github.com/cdepillabout/termonad) I've been using this almost every day for a year or so now. There are a couple more features I'd like to add to it, but it is working pretty well so far. Aside from other obvious development tools (like GHC, stack, etc), Termonad is the only Haskell program I can think of that I really use *everyday*.
We were a Scala shop. I started running an internal Haskell course (got people involved and interested). Two months later I saw an opportunity to introduce Haskell (automation of a manual task). I didn't ask my manager, only shared it with the people most interested in FP. They supported me in doing this and one month and a half later we put it in production. My manager had some mixed feelings because we didn't tell him but he was very happy with the automation working better than expected. #### Summarizing 1. If you work in a team you need to get at least one person interested otherwise it's going to be really hard. 2. Managers don't matter, just tell them you're doing it in Go :) 3. Just do it. Nobody else is going to do it for you.
A `DbTransaction` monad, for example, wouldn't use any Haskell-native types to guarantee atomicity, but rely entirely on the DB backend to do that. You can get the "current" time, read from the DB, write, etc. The actual atomicity is, of course, at the DB layer, but the Haskell monad is ensuring that your code complies semantically with that constraint. You can enforce that only DB operations are allowed, or those derived from DB operations. A more interesting example that I've used recently is interleaving a DB transaction with an RPC caching system. Because we can so easily define what's allowed and what's not, we have a monad that lets us interact with *both* a database transaction and a remote RPC. Obviously these don't share any sort of transaction in real life, so our monad actually supports detecting an RPC call, intercepting it, rolling back the DB transaction, making the RPC call, caching the result, and then restarting the DB transaction. This ensures that every DB transaction sees a consistent state of the cache along with other data. You might accuse us of being "too fancy" but in reality it's not at all hard to pull this off in Haskell! It *would* be fancy if it weren't Haskell...but in Haskell it's not fancy. It's easy. And that's why it's powerful. FWIW I looked more into PyPy. It appears there is an extremely rudimentary way to compose atomic blocks of code, but highly experimental and without the conveniences that Haskell (and likely Clojure) provide: [http://doc.pypy.org/en/latest/stm.html#atomic-sections](http://doc.pypy.org/en/latest/stm.html#atomic-sections)
Benchmarking tools (criterion), property testing tools (QuickCheck), or internal developer, monitoring tools could be good candidates.
That's fine, but changing a definition on-the-fly and only in your head isn't conducive to having the you want.
I love rust for what it is (a low level language), but I have to disagree. Rust's type system is not great. I came to haskell because Rust's type system cannot express basic concepts, due to a lack of HKT and many unimplemented features like impl Trait and GATs, etc ... Rust has a hard job, but I'm not convinced the right choices were made as it pertains to it's type system.
IMO, if you really will only ever need io, then haskell monads aren't a win. However, if you are using anything more complex than a standard io monad, haskell's version of that code is often simpler than other language X's version (unless that other language specifically builds support for that other monad into the core languagae a la async/await in js). And if there is any chance that you might need to swap monads out later on (say, going from one thread per request to async), haskell makes that vastly easier than pretty much non-haskelloid language.
&gt; I honestly wouldn't mind if more libraries on Hackage used "enterprise" or "Java" style type names / constructors -- explicit, even if that means long. field accessors and constructors (or lenses) should have shorter names, since I'm physically typing them more (and the types make the context explicit), but long type names would be an improvement. Certainly. I believe it would be more manageable because Haskell usually has a more flat file structure. In Elm they manage to make it look clean while still retaining very descriptive names. There is a good talk about [it](https://www.youtube.com/watch?v=trgET9YU37M)
&gt; I suppose if you have some mission critical part of your ecosystem that can reasonably be written in a separate language and needs to be fast and safe that would be a good place to start. I see. Makes sense. Will require some convincing of other developers that this is worth to do. But that's how it is.
&gt; Managers don't matter, just tell them you're doing it in Go :) I like the way you think ;) &gt; Just do it. Nobody else is going to do it for you. Yeah, I think at the end is just about that. Great to see it worked out for you, I think the internal course will be a good starting point.
Thank you so much for fixing this leak, it had become more and more problematic recently for me.
My own experience was the following: &amp;#x200B; \- we had an internal reading group on the "Haskell from first principles" book with a few engineers in my team really interested by Haskell \- we had an internal "Haskell guild" with a few passionates as well \- I decided to re-implement a command-line tool previously implemented in Scala to get a better grip on "development experience": build tools, editors and also touch a few libraries which I knew would be required later on for HTTP, JSON, streaming. That tool itself was already helping me daily with my job with much faster startup times than its Scala counterpart \- I discovered at that stage that a Haskell developer in the company had open-sourced a library to access a crucial piece of the company infrastructure, our Kafka cluster! That was essential functionality for free: [https://hackage.haskell.org/package/nakadi-client](https://hackage.haskell.org/package/nakadi-client) \- At the favour of a management change I convinced the new management that we could deliver a new micro-service on the roadmap in Haskell and that the language would actually help us recruit good developers (true, we got plenty of fantastic CVs). That service was a simple stateless one, with nice business logic and reusing a big chunk of the code I had in my command-line tool. They said yes to my great surprise :-). One engineer asked for a department transfer almost the same day! \- Some remaining risks were: how to integrate a Haskell project in the CI / deploy infrastructure (with proper caching for build times and so on), are we going to have any leaks, performance issues, weird compilation issues? With the help of great engineers in my team we solved the CI/CD problem quite fast and didn't get any of the other issues. I also insisted from the beginning for using "records of functions" as a way to structure the service instead of MTL (which gave me lots of headaches with \`nakadi-client\`) or effects (which I am guilty of introducing in a Scala project). This eventually culminated in this library: [https://github.com/etorreborre/registry](https://github.com/etorreborre/registry) to wire components. \- That service was eventually deployed in time, with low resource consumption and totally doing its job. Success! \- The next service was a lot more troublesome. It was a rewrite of a service previously written in Kotlin as a proof of concept (a trial by one engineer to steer away from Scala to see if Kotlin would prove more effective). That service used Postgres so we had to find a suitable Postgres library, but also had an interface with S3 and some image processing [support.](https://support.So) So it was a less straightforward to deliver and we had other fire-fighting in the meantime which did not help \- We implemented other services after that and I left the company a few months later for my current job. I will let others tell you the rest of the story because it is quite interesting as it involves some successful developments with Purescript on the backend! (yes, you read it right). &amp;#x200B; I overall feel lucky to have a technical director who said yes at the time (with some words like "no risk, no reward") but I think it really helped that I prepared myself and my case months in advance. &amp;#x200B; And I seize the opportunity to thank again /u/edwardkmett who asked me at a conference diner: "Hey Eric, what's your reason for not using Haskell?". This is why I would like to give the same push to others now, there's no reason I should have all the joy for myself :-).
&gt; You can see tcRnImports and zonkEvBinds gradually eating up more memory over time: Perhaps I just don't know how to read these graphs, but I wouldn't say tcRnImports is leaking. zonkEvBinds looks like the biggest leak, perhaps followed by zonkEvBndr_zonkTcTy.
Pick something that is essentially doing data transformation between two data-sources. Haskell really shines in that. Anything that fetches data from API-1, does some processing on it, and pushes it to API-2. (API, or CSV files, or whatever). If chunks of said dat can be transformed/processed independently, you can demonstrate the easy of concurrency as well. &amp;#x200B; Try NOT to pick anything that is a basic CRUD interface to an RDBMS. You'll be wrestling with the Haskell records problem and will end-up demonstrating the worst parts of Haskell instead.
We talked on Twitter the other day, and here you are again 🎉 Thanks for taking the time to share your story!
Good point about avoiding records, I'll keep it in mind.
Something small and useful. It's not haskell, but this article on F#, another language I love, is very relevant. https://fsharpforfunandprofit.com/series/low-risk-ways-to-use-fsharp-at-work.html
Good ideas there, thanks for sharing!
I use pandoc to create docx files from my mkd to send of to my supervisor for correction ... then after I incorperated the changes I use pandoc to create tex file which get assembled into the final pdf :-)
You're correct that Rust's type system isn't as full featured as Haskell's but I don't think it is true to say that it isn't great. You have probably heard this before but the lack of HKT makes the language simpler in a lot of ways. There are already lots of people complaining that Rust is too complicated. But I think part of the appeal of Rust is that it brings together lots of ideas from both C++ and Haskell in one language. &amp;#x200B; I don't want to get into a flame war so lets just agree to disagree 😊 I personally like both languages but happen to prefer Rust. &amp;#x200B; And by the way \`impl Trait\` was stabilised in 2018 so its been there for some time.
ShellCheck (if you ever write sh/bash scripts), pandoc for document conversion
I use GHC every day at work, and for the last couple weeks, [CLaSH and Shake every day at home](https://github.com/gergoerdi/clash-spaceinvaders).
Isn't applicative enough for this program? ;)
You don't want to "move" from haskell to CT. Rather, it is better to just start with CT directly, and only later make the connection between the two. A good intro (though it doesn't get to monads) is lawvere and schanuel's "conceptual mathematics." Following that, you may like Riehl's "categories in context."
I wrote about my experiences introducing Haskell at my previous work here [https://tech.channable.com/posts/2017-02-24-how-we-secretly-introduced-haskell-and-got-away-with-it.html](https://tech.channable.com/posts/2017-02-24-how-we-secretly-introduced-haskell-and-got-away-with-it.html) Perhaps it helps. &amp;#x200B; I don't work there anymore, but they actually have a Haskell \_team\_ now next to their Python team, and they're doing large projects in Haskell now (They have more about those projects on recent blog posts).
Read it last week. We need more stories like that ;)
I work on building and improving a deployment pipeline for docker, so I use [Hadolint](https://github.com/hadolint/hadolint) regularly.
Go handles this in a nice way: for platform specific code, you'd have MyPlatformSpecificModule\_windows, MyPlatformSpecificModule\_linux, etc., then the compiler picks which one to use based on which platform you're compiling for. This encourages a clean approach of separating platform-specific code out into separate modules.
I'm not convinced all this tactics stuff is necessary at all. You write &gt;The insight here is that maybe we can just make these combinators a part of the interpret interface directly, rather than have people write them by hand for each interpreter. But in Effect Handlers in Scope, the `weave` method is part of a type class defined on the syntax of an effect, *not* the interpretation. This means it is defined once per *syntax*, not once per *interpreter*. Note that `fused-effects` takes this approach,[the `Effect` class contains `weave` (as `handle`)](https://hackage.haskell.org/package/fused-effects-0.4.0.0/docs/Control-Effect-Carrier.html#t:Effect). Furthermore, in some of my own work, I've managed to derive implementations of `weave` using `DerivingVia` and `kind-generics` (the only generics library I know that works with existentials). This shouldn't be too surprising, because you yourself wrote: &gt;We see two particular patterns emerge in our interpreters: &gt; &gt;distrib $ ma &lt;$ state for the case of an m a argument &gt; &gt;\\fa -&gt; distrib $ fmap mb fa for the case of an a -&gt; m b argument So why aren't we encoding this pattern generically? I think we should! Is there a case where the weaving is different between interpretations?
Well, Clojure being a lisp you can include pretty much the same things you can in Haskell. It's been a very long time since I actually looked closely at it, but I think there was a library that gave you restartable sql transactions? It being a dynamic language you can't exclude bad stuff happening the way you can in Haskell, but that alone being worth the hassle is kind of what we're discussing here. In Haskell, and in general, you can't exclude bad effects without also excluding some good ones. For example, reading and writing to mutable state inside a restarting transaction is no bueno. Usually. It's okay if you're manipulating caches and performance counters though. In fact, manipulating caches has been blessed by the Haskell language so it's no longer considered a side-effect, and GHC does the same for performance counters. Though only for the caches and counters Haskell itself provides (thunks and runtime profiling). It's not possible to define your own "pure" caches and counters in a sane way. So now we circle back to the pain points. Sure, it's more painful and requires more rigor to avoid the pitfalls of malevolent effects in languages that don't control them. But it's also more painful and requires more rigor to encode the benevolent effects in languages that do control them.
GHC's zonking proved problematic
git annex
Very cool approach. I'm assuming this spawns (and persists) a process for every active user since it has to keep and update their VDOM in memory? What happens if users internet connection resets? Are cookie sessions used to connect user back to his VDOM instance?
Thanks for taking the time to write down all that feedback! :) I've relayed it to [discourse.dhall-lang.org](https://discourse.dhall-lang.org/t/feedback-on-dhall-lang-org-from-a-reddit-user/31) where I hope we can use it to improve the site!
I've written a few tools in Haskell for use at work. Here are a few ideas for projects where Haskell shines. Anything where you are modeling probability distributions. Writing up a probability monad lets you calculate exact distributions efficiently without having to result to sampling methods. Anything that requires big integers. Native support for arbitrary precision integers &amp; rationals gives Haskell a boost over other languages at this task (Python is also good here, though). I used this for a tool to calculate powers of 10 for a C++ implementation of [Grisu](https://www.cs.tufts.edu/~nr/cs257/archive/florian-loitsch/printf.pdf). Symbolic calculations. It's really easy to manipulate formulas in Haskell, and to translate code into symbolic versions of that code. I used this to verify that two different algorithms for rotation by quaternions had the same result (well, the optimized one actually *was* different, but the error was 0 when the input quaternion was normalized)
Matterhorn, a matteemost tui client
Yes, it is the PrintHelloWorld effect
Cool projects!
That looks really cool- I’ll have to check that out!
The Turing-incompleteness applies to normalising Dhall itself. While you could add the above data type, any program you write using it will have a normal form. The non-termination comes from a particular interpretation *of* those normal forms - a runtime system, if you will.
Came here to say pandoc too. I use it to make latex a little more manageable, but I don't recommend this to anyone.
Since you already understand Haskell, [Category Theory for Programmers](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/) is a good resource, it teaches CT by using motivation and examples from programming (Haskell in particular).
&gt; I'm assuming this spawns (and persists) a process for every active user since it has to keep and update their VDOM in memory Correct. &gt; What happens if users internet connection resets? Are cookie sessions used to connect user back to his VDOM instance? Not yet, but that's a great idea. Every DOM instance could be kept around in memory and be served back to the client in case it reconnects during a configurable timeout period.
I thought Fortran was the lingua Franca of HPC? Or do you mean more "high performance (general purpose) computing"?
I don't really understand some of your points. Using monads doesn't mean your code is impure. It does make certain interactions more cumbersome than if it were pure, but that's really just a matter of taste/proficiency/etc. I don't really see "being able to reach for mutation in any function" as a feature I actually want. Sure, when you're writing a pure function and then suddenly realize you need to mutate, it can be nice to just "throw it in" but...I don't trust myself or my team to harness this powerful with proper restraint. At this point I'm so accustomed to controlled effects that they don't at all get in my way. I write OCaml and C all the time. I always miss this feature. I never wish Haskell had it...ever. I also really like type systems...so for these two reasons Clojure is the opposite of what I want. This is all anecdotal of course. But if anything I can say that for me, personally, I like the tradeoffs in Haskell.
Cool! I always enjoy seeing the cool things that people do with \`brick\`. :)
I think I'm going to give an opposite answer to /u/sclv. Here's a basic guide to adapting am understanding of Haskell monads to the mathematical definition. 1. Haskell's monads are specifically endofunctors on the category Hask, of Haskell functions and types. More generally, you can talk about monads on any category, and mathematicians do so. 2. In mathematics, monads are defined in terms of natural transformations. These must satisfy certain axioms, expressed as a naturality square. In Haskell, we luck out. The naturality condition turns out to be guaranteed by parametricity, so any polymorphic function (forall a. F a -&gt; G a) can be thought of as a natural transformation from F to G. If you aren't working in Haskell, you have to think about naturality again. 3. Finally, the crux of the matter: mathematical monads are given by an endofunctor and two natural transformations, eta and mu. These correspond to `return` and `join` (along with `fmap` for the functor). So the trick is to show that Haskell's definition using return and &gt;&gt;= is equivalent to the mathematical definition using fmap, return, and join. You can do this by implementing each in terms of the other, and then showing that the implementations are inverses.
Each of the below were of daily use at some point for me: Matterhorn, glirc, shell check, ghc, Haskell ide engine, musedev, cryptol, hlint, gitit, saw, darcs.
I'm all for making this easier. PRs welcome!
That's a really good point... ghc also doesn't care what the file is named, it uses the module name to generate its output. So I could do the same thing as go. Maybe it's just conservatism but I still feel it's clearer when right there in the source file you see "if linux then import A as X else if darwin import B as X else complain", rather than an "import X" where there's no X.hs but the build system will implicitly compile one of X_linux.hs or X_darwin.hs. When I eventually forget I put that trick in the build system I'd be pretty confused how `import X` works when there's no `X.hs`. I can no longer say there's no way to do it without CPP though!
Very nice. And as you say, I can see great potential for internal applications. This design approach doesn't seem viable for any biggish application.
Isn't it both, intermingled? C has a much more useful parallelism than Fortran. While Fortran gets more performance from a node. But, anyway, I have no idea what the latest tooling is. Where does Cuda and OpenCL enter the picture?
Categories in Context is the Riehl deal, best intro to CT I've read :^)
well this is life changing.
It does on new reddit. It doesn't on old reddit or (most?) of the mobile apps. Indenting each line with 4 space characters does (AFAIK) work on all versions of reddit.
I've a follow-up question. In initial `vector` based code the pantry were defined as ``` type Jobs = Vector Entry type Pantry = Map JobName Job ``` The code for "Strict, Non-Persistent Queues" variant looks like: ``` class QueueBranch b where empty :: b a head :: b a -&gt; a tail :: b a -&gt; b a snoc :: b a -&gt; a -&gt; b a null :: b a -&gt; Bool data StrictList a = SNil | SCons a !(StrictList a) deriving (Eq, Show) data StrictQueueBranch a = Q1 !Int !(StrictList a) !Int !(StrictList a) deriving (Eq, Show) instance QueueBranch StrictQueueBranch where ... newtype Jobs a = Jobs {unJobs :: (a Entry)} ... ``` Question: How could `a` in `newtype Jobs a` be constrained to `QueueBranch`?
Read this, with particular attention to the Kleisli Category: [Monads Made Difficult](http://www.stephendiehl.com/posts/monads.html) Though they're equivalent, Kleisli composition is an obvious idea to anyone who studied a semester of undergraduate Modern Algebra (a course I have taught), while the Haskell axioms for monads are frustratingly arcane.
I think that's a fine guide. But in my opinion to make use of that guide, one should understand the mathematical definition on its own -- including some prerequisites including an understanding of naturality, limits, colimits, the relationship to adjoint functors, and arguably even factorizations of monads first. Otherwise it says "here's some stuff you know, and here's how it is a special instance of something you don't know." Knowing about monads in general tells you something about monads in Haskell. Knowing about monads in Haskell is too special a case, imho, to learn about monads in general. It's like asking how to cross the bridge from real numbers to group theory.
This looks really interesting! I'm not sure if it's relevant (I don't know much about this stuff), but I was reminded of the Lockstep Simulation is Child's Play report: [https://arxiv.org/pdf/1705.09704.pdf](https://arxiv.org/pdf/1705.09704.pdf)
darcs? Impressive.
With the use of [GADT](https://en.m.wikibooks.org/wiki/Haskell/GADT)! More: - https://wiki.haskell.org/Data_declaration_with_constraint - https://stackoverflow.com/questions/44754977/haskell-how-to-use-type-class-with-generic-datatype
We can already pass around class dictionaries explicitly with \`-XConstraintKinds\`. &amp;#x200B; [http://hackage.haskell.org/package/constraints-0.11/docs/Data-Constraint.html#t:Dict](http://hackage.haskell.org/package/constraints-0.11/docs/Data-Constraint.html#t:Dict) &amp;#x200B; I'm not sure special syntax for this is desirable; Edward discusses the benefits of coherence at length in this talk: [https://youtu.be/hIZxTQP1ifo](https://youtu.be/hIZxTQP1ifo) &amp;#x200B; What's an example of a definition that's made clearer with this syntax?
That can also be a quick way to get fired. Depends on how the manager handles his mixed feelings.
I've read the 'Conceptual mathematics' book. Partwise, much more then once :-) But 'Categories in context' is a big step. I tried that. It seems like that self education on topics like modern abstract algebra, topology, category theory is really hard. Most textbook miss the little bit extra what you get from a real course from somebody who knows what he/she is talking about.
Thanks everyone! Your help is much appreciated. I've got some food for the summer :-)
I tend to agree the one drawback with Riehl is it presumes some topological knowledge as a baseline. Perhaps a better next step could be Simmons, Awodey, or just reading MacLane very slowly and carefully? Maybe in conjunction with videos from the OPLSS summer school (e.g. awodey's lectures) or from Bartosz (https://www.youtube.com/user/DrBartosz/videos) ?
I think the situation where you would want \`putStrLn ("Hello " ++ getLine)\` to work are the more "scripty" contexts (for the lack of a better word) where speed and flow of writing are more important than stopping and thinking about how you should structure the program. And I'm not even trying to be condescending here -- I think this is pretty much what a lot of the software development world is. But being forced to be disciplined about what entails IO and what not is what is good about the IO monad (IMO). Perhaps the other end of the spectrum, and what you would like, is lisp? It doesn't force to write code in a monadicly typed way but still allows one to have the "programmatic semicolon"/redefine what statement means (which is one of the ways I see monads in haskell). You could probably have pretty neat code if you are a natural at keeping discipline.
The browser will make two requests, one for the HTML and one for the javascript. So, you'll need to handle both requests one way or another. You might find this middleware helpful if you're serving static files: http://hackage.haskell.org/package/wai-middleware-static-0.8.2/docs/Network-Wai-Middleware-Static.html Also, your opening `script` tag is mistyped so that will make it not work.
XMonad is my window manager at home.
I approve this message!
Okay, so I still think about HTTP/1.1; in that the browser is going to send two requests to two different URLs, and your wai application needs to handle them both, sending different contents back for each. HTTP/2 and later might allow the server to push files to the client without a request, but I don't know the details of that.
Do you know how much time generating these files add to the compilation process?
[removed]
The typo is not an issue, My code more complicate than the example that I given, I just try to simplify the question.
Is there a generally accepted "best" logging library? &amp;#x200B; Thank you!
From your comment, If the browser makes two requests, then &amp;#x200B; following line will be called: \_ -&gt; respond $ responseNothing and I can not see any message from **repsonseNothing** which contains a string.
I use glirc every day. It's a complete, console-based IRC client. https://GitHub.com/glguy/irc-core https://hackage.haskell.org/package/glirc
Yes, Bartosz M. seems to be the most approachable source. I've watched a couple of his videos already. Thank you!
This is related to my blog post 'Haskell considered harmful'.
The IO monad is a red herring. The real problem is that there's a difference between regular and monadic/applicative code. For example, say you have a simple expression `foo bar baz`. Simple enough, except in this case foo and baz happen to be partial. In order to deal with this we can use Maybe (or Either someError, same difference). However, now the code looks like `foo &lt;*&gt; pure bar &lt;*&gt; baz`, which is a hell of a lot of extra plumbing. That's a doubling of symbols to express the same general idea. And the plumbing isn't free either; it takes a good number of extra braincycles to wire things appropriately.
I checked its deps, and `vector` and `primes` are the only ones not bundled with GHC. It may not be as bad as you think.
HPFFP does build up to "real" examples near the end of the book, and that's the book I used to bootstrap myself into writing useful Haskell programs. I would still recommend it. Depending on how far along you are, you might be able to self-study the QFPL's [applied FP course](https://github.com/qfpl/applied-fp-course) alongside?
The idea must be in the air. Here is a similar project in Scala: https://github.com/fomkin/korolev.
I don't think I mentioned anything about purity? I don't think that's relevant to either ergonomics of monadic code or controlled effects. I also disagree that monadic code being cumbersome is a matter of taste. If you compare it to handwiring monads through many expressions then monadic combinators are very simple, sure. However, if you compare e.g. a Maybe computation with a normal partial expression using 'error' or non-exhaustive patterns, the non-monadic code will be much simpler, and yet they both express the same idea\*. Or using State: `do i &lt;- get; let result = foo i; put (i+1); return result` versus Java: `foo(i++);`. Four statements to express what Java does in essentially one statement with two symbols. You can use a higher order function to simplify the Haskell code, of course, but it would be a very specific function, unlikely to be found in any common libraries.\*\* Now as for Clojure, the reason I brought it up as an example is because it has the same philosophy as you do about constraining effects. It approaches the problem differently in that it *does* trust the programmers to just "write sensible code", it just makes it easy to write effect-free code by default. Now as I said I'm not very into Clojure myself\*\*\*, but afaict they're still pretty hyped about immutability and referential transparency over there. And that's in a language that allows unrestrained side-effects in arbitrarily restarting computations. Note that I still like monads overall, I just don't like how clumsy they are compared to the rest of the language. As for effects, that's a separate issue from monads (you can constrain effects in other ways). Plenty of programmers balk at the prospect of mandatory constraining effects, including Clojure programmers who also value referential transparency. I'm undecided on the value of the idea by itself, but I also really like lazy evaluation. &amp;#x200B; \*Partial code expresses failure in a way that's hard to deal with in other code, but that is a separate issue. \*\*The fact that the exact list combinator I want is never available is a pet peeve of mine, but one I know can't or shouldn't be resolved. \*\*\*Homoiconicity is pretty cool, but I too like types.
Your perspective is interesting, to say the least. I'm glad to learn from it. Using `i++` as a "win" for anything is highly suspicious IMO. As I tell my students: brevity is not superiority. And that's sort of why I talk about "taste." I actually like the verbosity of spelling out the order of things. The cumbersomeness of the code seems well worth the cost compared to having partial functions, etc. But your perspective is well-considered and worth sharing. Thanks.
Very first sentence from [that book](http://www.math.jhu.edu/~eriehl/context.pdf): &gt;A group extension of an abelian group H by an abelian group G consists of a group E together with an inclusion of G ,→ E as a normal subgroup and a surjective homomorphism E  H that displays H as the quotient group E/G. T Not sure if that qualifies as intro material. :-P
`impl Trait` was stabilized for a tiny subset of the uses that have already been approved in rfcs (see [#34511](https://github.com/rust-lang/rust/issues/34511). More uses of `impl Trait` are planed there is an open RFC right now.
Perhaps monad-log
I can't imagine it would slow down compilation too much considering it's computing everything either way.
Thanks, I'll take a look.
I use darcs everyday.
That first section is really just some historical motivation for those who it makes sense to. I do think it's unfortunate that it drops people in at the deep end -- but I think if you just go straight from the preface to 1.1 nothing pivotal is lost.
While this might seem like a naive approach, I think it's very valuable to have something like this. IMO, a design like this should be our starting point for any system: A single program which owns and controls everything from one place. Then we can use all the normal abstractions and techniques that our language provides us, rather than splitting our logic into multiple programs which communicate, and having to squeeze our rich abstractions through the narrow straw of Javascript and HTTP, or other purpose-specific protocols. If, after writing a single program which handles everything, we find that our system is too slow, then we can rewrite and optimize our system piece by piece, splitting out specific parts to run separate from our main program and communicate over protocols. We shouldn't start with multiple programs and languages and communication over serialized bytes - that sacrifices all the nice abstractions and type-safety that our language gives us! Start abstract and type-safe, and only then optimize when needed.
Reminds me of https://github.com/isovector/suavemente.
What if your package wraps an OS-specific syscall?
Is backpack Good to Go? I haven't seen or heard much about it.
I'm happily using it.
[hledger](http://hledger.org) of course (accounting and time tracking). polimorphic (daily legislation report). At times: pandoc, darcs hub, darcs, ghc, ghcid, stack, shake. Hackage.
Does this have anything to do with [HIE](https://github.com/haskell/haskell-ide-engine)?