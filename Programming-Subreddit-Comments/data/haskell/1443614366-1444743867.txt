Less between you and your parser, more flexibility in terms of structuring your grammar/program/parser, and I'm not sure if BNFC is restricted to some grammar formalism (like LALR), but parsec isn't.
Anyone in Luxembourg or surroundings? I'd be glad to start a meetup if anyone is interested.
I have a feeling it can't be implemented, at least not in a way that has well-behaved instances of `Monad`, `MonadTrans`, &amp;c. But I can't give a precise reason why off the top of my head. So go ahead and try, and let me know how it goes. :]
Your post serves as a nice intro; worth another reddit post IMO. 
In the Netherlands, there is [FP AMS](http://www.meetup.com/it/FP-AMS) in Amsterdam, also organized via meetup; it usually gathers few tens of people (20-50), many of which Haskell users. The host is Michel (@ugly_syntax) and the companies providing the venue have been e.g. Q42 and Marktplaats/eBay
Reading your problem spec, it seems you have four functions that are pure so you should be able to combine them with the list monad: combined :: Int -&gt; [Int] combined input = do ar &lt;- a input br &lt;- ar cr &lt;- br dr &lt;- cr return dr Though I see you do seem to use the Writer effect. Does your problem spec match your solution? Another comment is that the frequent use of (Maybe Int) could be a red flag. Wouldn't pipes that go from Int to Int be simpler? Maybe I'm not fully understanding the purpose. 
Sure, there are lots of things that can be done. But this all points to the perfect being the enemy of the good. There's a policy in place today that can easily be subsumed by tracking this specific bit of metadata, which is really available.
I run the Buenos Aires Haskell Meetup (BAHM) in Argentina, also using meetup.com. Most members come from meetup.com itself and my university CS group, and some from twitter I guess. Last time we were 12 people, so we are not a lot. I usually give the talks/workshops myself since I'm not aware of another person with Haskell experience. The space is provided by Inaka, which are super cool and lend us their office.
`cabal install hlint hindent quickcheck` in the Mac terminal.
&gt; one of the hardest to grok libraries in Haskell Is that a challenge? ;]
Is this what you are trying to do: import Control.Monad import Data.List import Data.Maybe data Val x = A x | B x | C x | D x | E x deriving (Show) answer inputs pipeline = do as &lt;- tails $ (replicate (length pipeline) Nothing) ++ (map Just (reverse inputs)) let bs = zipWith fmap (reverse pipeline) as cs = [ x | Just x &lt;- bs ] return (reverse cs) test = mapM_ print $ answer [5,4,3,2,1] [A, B, C, D] 
Thanks Gershom for the added context. In addition to that, I'll add a couple more points contributing to me making the case as strongly as I did. First, while our location in NYC was undoubtedly a huge contributor to the success we've had in terms of absolute numbers, there have been several Haskell or FP related meetups in NYC before us that did not reach the same size. So being in NYC definitely isn't a guarantee of reaching the number of members we have. This is where I think the split format with a beginner and non-beginner talk at every meeting really helped. But that alone is not enough. Even with that format, we still didn't have that number of members for a long time. So far in 2015 we've grown 66%. Now I can't say precisely what caused this growth. I'm sure part of it was related to overall worldwide Haskell growth. But I can say that this happened around the same time we created a Youtube channel and started making a more concerted effort to regularly post the videos of our talks there. I should also point out that we have not seen this increased growth in membership translate directly into increased attendance at the meetups. So I imagine some of these people don't have more than a passing casual interest in Haskell. But then again, our meetups are space constrained and have an RSVP limit, so that could also be a contributing factor. I should also mention that /u/gbaz1 deserves most of the credit here. He was the person who spearheaded this group and got it off the ground. It definitely wouldn't exist without his hard work and extensive community connections that helped us recruit speakers.
I came to the York meet up because it was advertised on Reddit, never heard of meetup before (and still doesn't have an meetup account), so I guess announcing on Reddit kind of work. Doing so might be seen as a spam from people not near the area .
That's certainly one way of solving it!
I seem to have a lot of trouble with it even though it has very good tutorials and documentation. So maybe for me it has one of the highest [difficult of groking]/[quality documentation]. I seem to be okay with free monads and thanks to [this](https://medium.com/@jaredtobin/practical-recursion-schemes-c10648ec1c29) I think I at least get the point of recursion-schemes.
I agree, it has extensive documentation, but a lot of it (outside of Pipes.Tutorial) almost seems like boilerplate, describing Categories and such, which is fine... but as someone with a problem to solve, it would be nice to see more examples of things being used, especially of bidirectional communication, push, pull, respond, request, etc....
Dang, and recursion-schemes was going to be the first thing I suggested as being more baffling. :[ I can definitely believe pipes has a very high value for that ratio, though.
Possibly related to this icfp paper? http://www.cs.cmu.edu/~joshuad/papers/eo/
The subreddit isn't busy enough for that to be a problem. I mean, let's not start advertising meetups at the level of "come over to my place tonight, I've got pizza". (And even there, I might give it a pass if it's really *good* pizza.) And for e.g. monthly meetups please don't make a post every month. Beyond that? No problem. It was suggested once or twice to add a list of meet-ups to the subreddit sidebar, but updating that would be a hassle and listing every meetup for the next year or so would be too much.
I don't even...
Support for existing formats and editors is definitely something I'd like to implement. The Spine tools look great, it would be good to use a real animation tool.
Please
You cannot. It's a type safe language, aka there are no heterogeneous lists. What exactly are you trying to accomplish with that anyway?
Volunteer package authors are not going to try hard to maintain backward compatibility. Pay an enterprise if you want that kind of thing. We do, however, have Stackage, where everything in the snapshot works now and will work in a few years' time (presuming that you've got a working GHC for your platform then, which is not always easy.)
/u/haski2, what is your native language? I'm assuming it's not english, but really it might be better to find someone who can help you in your native language that continuously posting incomprehensible questions to this subreddit.
Another option to consider is [lens](https://hackage.haskell.org/package/lens-4.13): lst^..traversed.each
i hope to create a list of different type of tuples and CartProd them
I'm not sure what's going on with the `x` and `y` in the example, but creating a Cartesian Product is one way to join to lists of different types into tuples; assuming you want all possible combinations of the original elements. If you just want to pair things sequentially, take a look at `zip`. ``` zip [1..10] ['A'...'Z'] == [(1, 'A'), (2, 'B'), (3, 'C')...] ``` Note that `zip` will return once the first list is exhausted, as you might expect.
It looks like there are many people on this thread interested in collaborating with someone. Until someone organizes something cooler, why doesn't everybody pick someone else on this list to parter with for peer reviews? I'll mentor /u/tojans since he asked, and I'm all set getting mentored myself. Can you guys look through the list and find someone else who wanted to work together?
I love your idea so much. I don't have time to create something like that but I would support it if it existed.
&gt; It's a type safe language, aka there are no heterogeneous lists Strong type systems do not mean there are no heterogeneous lists; in fact, the reason most typed languages don't have HLists is precisely because their type systems are too weak. But the concept is valid and absolutely useful -- it's used in [modelling linear logics in Agda](https://github.com/pepijnkokke/SubstructuralLogicsInAgda/blob/master/SubstructuralLogicsInAgda.pdf).
It's not possible; lists can hold a single type, whereas tuples can collect elements of different types. 
As for the answers about HList and existential types, a more mundane way to create a "heterogeneous" list would be to just create a sum type first.
Is there any reason why you define a new `Aux` class instead of reusing the `Functor` class?
Direct (and somewhat ugly looking) way would be flatten :: [(String, String, String)] -&gt; String flatten [] = [] flatten ((a, b, c):xs) = a++" "++b++" "++c++" "++(flatten xs)
[Chennai Haskell](http://chennaihaskell.org/) attracts decent number of peoples. Although the meetups aren't that regular.
That sounds pretty cool. It also sounds like what pedagogical theorists would recommend: lecturing (even with slides) is supposedly one of the worst ways to teach a group of people something.
There is the [Budapest FP meetup](http://www.meetup.com/fp-bud/) running since 2009, though less frequently in the last few years, and the newer [Budapest Haskell User Group](http://www.meetup.com/Bp-HUG/) running since 2013. There are also Scala and Erlang meetups.
If I was in FP Complete and I had to debug complex code, I would definitely nerd snipe /u/snoyberg!
But how would I use them on Eclipse? I can't even use them through the terminal. If it helps, my initial install was on the Haskell Platform.
There is no library for implementing this as far as I know. I am in fact thinking of writing one (actually something much more general). Higher order matching was proved to be decidable relatively recently; 2007 I believe. This was a long standing unproved conjecture by Huet in fact. No one has implemented the algorithm as far as I am aware; probably someone did but no one as published it. Second order matching is already decidable by Huet's algorithm but Huet's algorithm cannot in general work for 3rd order matching. 3rd and 4th order matching algorithms were found as well (and you can find references to them in the handbook of automated reasoning).
I'd be interested in doing something with Haskell in Omaha, NE if there were more experienced people to get involved. I looked at starting a meetup, but I didn't want to pay the fee if I wasn't sure I could put together something decent. 
Few comments: You are creating `Network.HTTP.Client.Manager` for [every request](https://hackage.haskell.org/package/hastily-0.1.0.4/candidate/docs/src/Text-Hastily-Network.html#getFrom). You may run out of free file descriptors. It is better to reuse http manager. It is a really bad idea to catch `SomeException` without rethrowing it immediately. You are doing that in `getFrom`, and you are retrying 5 times in case of exception. But what if user wants to abort `getFrom` using `C-c` or `timeout`? Catching all exceptions is almost never the way to go. Returning `SomeException` from `IO` makes no sense. Just rethrow it. Better -- make a domain specific data type for errors and return it (or throw). When you [open a file](https://hackage.haskell.org/package/hastily-0.1.0.4/candidate/docs/src/Text-Hastily-Report.html#printReport), make sure it will be closed in case of an exception. E.g. use `bracket` or `withFile`.
Cambridge, UK has the [NonDysFunctional Programmers](http://www.meetup.com/Cambridge-NonDysFunctional-Programmers/) and sometimes Haskell events occur.
*Why* can't you use typeclasses as types? Similar to this in Java: Comparable c = "hello", where you can use an interface as a type. I know that with XRankNTypes and XConstraintKinds, you can do: type Showable = forall a. Show a, but you still can't do: let x = "hello" :: Showable. The heterogeneous collections [wiki](https://wiki.haskell.org/Heterogenous_collections) explains the solution, which requires that "existential values pack up a value with operations on that value". I assume the the compiler could generate the "packing" automatically, so my question is, is there a type theoretical reason why we can't just do: let x = "hello" :: Show, or is this just how they chose to implement Haskell? (EDIT: Abstract data type similarly specify operations you can use, but not the implementation.)
eager-vs-lazy is a strategy difference, but there are other considerations too, like the big-step vs small-step/stack-machine variation, plus others that one could imagine, which are all at least somewhat orthogonal to one another. i don't know a term that captures all of these semantically equivalent variations, so i'm just calling them evaluation styles.
The real answer is I don't know. I know there will be limitations and I know it would be at least a little bit useful but I don't really know where on this spectrum this idea lies. Another thing to consider is how long it will take to find solutions and how many solutions it will give you. It is a very interesting thought to be sure. I'll definitely be thinking about this.
Okay, thanks! Just so you know, what motived me to think about it was an experiment. I did the same thing I explained above, but instead of unification, I scanned the space of all λ-programs by brute-force looking for the right substitution. I was able to find `succ` (000000011100101111011010), `cons` (000000000101110111100101111011010), `S` (00000001011110100111010) and many other combinators this way. I wondered - if, with brute force, I'm able to find simple programs, with unification I could, maybe, find more complex ones? There are `sort` implementations with as few as 150 bits and most algorithms don't take much more than that. Please let me know if you do anything with that.
Yep, I've abandoned it, and I don't think anybody has picked it up after me, there hasn't be any commit since I quit I think.
You can certainly encode evaluation order in the type given that eager languages do so to handle lazy values. Normally it is considered a bad idea for lazy languages to do that because memoization can be hidden and has other values so representing a strict value as an already memoized lazy value gets you the same execution with no type complexity. Heck to be honest Haskell has types that encode strict implicitly as they don't have a thunk as they cannot be delayed.
Thank you. I always assumed genetic programming found correct, but bad solutions (since they are often bloated, am I mistaken?). I don't know much about proof search, thanks.
right, but I would frame it as: sub expression elimination preserves semantics, after the unsafePerformIO already changed them. 
You probably need to add them to your `PATH`. They're located in `$HOME/.cabal/bin`, so add to your `~/.bashrc`: PATH=$HOME/.cabal/bin:$PATH
Existentials allow you to get a homogeneous list of existentially typed entries, which may have different internal representations. You still won't be able to add a plain "5" and "()" to a list.
I hope Leuven's will be starting again soon. We had meetings every two weeks, really interesting topics, accessible for novices but still going deep enough for advanced Haskellers
I help with [Helsinki Haskell User Group](http://www.meetup.com/Helsinki-Haskell-Users-Group/). We try to have meetups on regular frequency, but it doesn't always work out. Meetup.com feels to be quite popular in Helsinki area, and meetup groups are very cross populated. There are e.g. [The Greater Helsinki Area F# User Group](http://www.meetup.com/FSharpHelsinki/) and recent [Scala Club](http://www.meetup.com/Scala-Helsinki/), virtually a group for any PL out there. People are curious about Haskell, just today I ended up discussing our last week's HelHUG meetup at HelsinkiOS-meetup :)
I thought this as well and [implemented a prototype Metropolis sampler](https://github.com/jtobin/observable/blob/master/Observable/Metropolis.hs) based on the cofree comonad; works great for models with a single parameter, but it seems to explode and behave pathologically as soon as you added another one. Glad to hear that someone else had the same intuition though as it makes me think the problem is in my implementation.
##ABOUT YOU **What kind of Haskeller are you? (Select 1 or more)** Hobbyist **What year did you first learn Haskell?** 2014 **How much code do you write at work (or school if you are a student)?** I write some code **When learning a new concept (e.g. Parser Combinators, FRP, etc.) which of these do you prefer (re-order the list)** 1. Examples 1. A tutorial 1. The module haddocks 1. Academic papers 1. The module source code 1. Haskell wiki **In terms of the quality of Haskell resources, which do you find the most useful (re-order the list)** 1. IRC 1. Hoogle 1. Stackoverflow 1. Community blog posts 1. Academic papers 1. Hackage 1. Haskell Wiki 1. GHC documentation ##HOW YOU USE HACKAGE **Use case #1** * When I am looking for a specific structure or algorithm * I will start by going to Google, where I type "data structure hackage" or "algorithm hackage" * That normally takes me to Hackage where I immediately hit the "browse" option and make sure I'm in the latest version of the package * Most of the time that works and I find what I'm looking for * If that fails, I normally try going straight to hackage.haskell.org/packages and using Ctrl+F to see if it is listed there **Use case #2** * When I am looking to understand a specific package * I will start by adding /package/[package-name] to the address bar and trying to work out what the main module is * That normally takes me to one of the module pages where I read the description and look at the types. If I encounter a new type I don't know I will click on it and follow the chain of types until I get to one I understand * Some of the time that helps but I often find I pursue a tree of unfamiliar types * If that fails, I normally go to Google to see if there are any examples or blog posts about how to use it **Use case #3** * When I am looking for a specific function * I will start by going to Hoogle (@ FP Complete) * That normally takes me to module on Stackage * Some of the time that works and I find exactly what I'm looking for * If that fails, I normally to use Google or ask on IRC **Use case #4** * When I am looking to solve a particular problem using Haskell * I will start by Googling "[problem] Haskell" * That normally takes me to a blog post / article / paper that someone has written. From there they normally link to (or name) the module that they use and I look that up on Hackage/ * Some of the time that works but other times I can't find a way to solve that problem * If that fails, I will go to the package list on Hackage and use the index at the top of the page to see if my problem domain is listed there ##COMMENTS (Optional) **What do you think could be done to improve Hackage?** (Add any comments here) **What do you think could be useful to improve the Haskell UX in general?** (Add any comments here)
iirc, bnfc doesn't need a preprocessor like happy, but still needs a template haskell codegen. standard problems with template haskell: haskell syntax is harder to abstract over than haskell expressions, build times are slower, etc 
Why not use Google Forms to create this survey?
Here in the San Francisco Bay Area (California), we've got a couple of groups. There's the [Haskell Hackers at Hacker Dojo](http://www.meetup.com/haskellhackersathackerdojo/) in Mountain View, home to the annual [BayHac](http://bayhac.org/) weekend and monthly meetups. Paging /u/MtnViewMark for comment. There's also the [Bay Area Haskell Users Group](http://www.meetup.com/Bay-Area-Haskell-Users-Group/) in San Francisco, with monthly-ish meetups as well. Nice to meet you in person, /u/wavewave! Related, there's the [Bay Area Categories and Types](http://www.meetup.com/Bay-Area-Categories-And-Types/) meetup at Santa Clara University. I keep telling myself I'll go one of these days, but Santa Clara is kinda far for me. /u/shachaf doesn't appear to be on reddit much these days, but pinging him anyways.
If possible, could we keep all top level replies as survey responses. Any off-topic questions / posts, could you please reply to this comment. As we've had two people ask already, why not a Google Form? EDIT: A google form has been added to the opening post This survey isn't collecting quantitative data, so the advantages of using a webform of some kind become less obvious. The idea of letting people submit responses via Reddit or email is to make it as easy as possible for people to reply - without worrying about how difficult it will be for me to collect responses.
See [my comment here](https://www.reddit.com/r/haskell/comments/3n0vaw/survey_how_do_you_use_hackage/cvjvn5j)
thanks :D
The idiomatic pipes approach is not to use pipes at all and to prefer a simpler list-based solution. The whole ethos of the pipes ecosystem is to use the smallest solution possible, even if that means not using pipes
&gt; What kind of Haskeller are you? (Select 1 or more) * Hobbyist: I use it for personal projects. * Student: I get to use it in my AI class for programming problems, and will be doing a directed study in AI next semester with it. * Professional: Current internship for this year is with Haskell. &gt; What year did you first learn Haskell? 2014 &gt; How much code do you write at work (or school if you are a student)? * Work: 100% Haskell right now. * School: AI class is done in Haskell. Otherwise it is Java or C++. &gt; When learning a new concept (e.g. Parser Combinators, FRP, etc.) which of these do you prefer (re-order the list) * Examples * A tutorial * The module source code * The module haddocks * Academic papers * Haskell wiki &gt; In terms of the quality of Haskell resources, which do you find the most useful (re-order the list) * Hoogle * Community blog posts * Academic papers * Hackage * Haskell Wiki * Stackoverflow * IRC * GHC documentation Use cases: 1. When I am looking for *function documentation* I will start by ~~going to~~ Googling the package name with a function name That normally takes me to *the wrong version of the package* where I *click the Contents link, select the right version, Ctrl+F "index", go there, select "All", Ctrl+F whatever I was looking for*. Most of the time that *gets me where I want to go.* 2. When I am looking for *haddocks for Persistent* I will start by *using my browser search engines to search for Persistent in Hackage*. That normally brings up a huge list of packages with Persistent in the title, where the actual package I want is 24 items down. I try to do Ctrl+F "persistent " but that just highlights all of the instances of persistent in the descriptions. Then I manually scan the page for the package I want and go there. &gt; What do you think could be done to improve Hackage? - Better sorting of search results, perhaps based on how completely it matches the string, or reverse dependencies, or popularity, or # of downloads, or... (etc) - "Go to Version X" link which defaults to the most recent package and offers a dropdown of other packages, which lets you go to an equivalent page for a different version of the package. If I google attoparsec, get to version 0.9.1.2, it shouldn't be hard for me to get to the most recent version in one click. Check out the [rspec](http://www.relishapp.com/rspec/rspec-core/v/3-2/docs/subject/explicit-subject) docs for an example -- "Change Version" is right up at the top.
This keeps coming up. So strongly emphasizing `(.)` and `($)` over their flipped variants -- based pretty much entirely on tradition and mathsiness -- is unduly prioritizing one kind of thinking over another. It is exactly opposed to the spirit of `do` notation, "Haskell is a great imperative language", etc.
Very far, if you separate how you index into data from how it is laid out into memory
yes, it's definitely possible to *encode it* in the type, but it ends up being a vacuous encoding. For instance, Idris has a rule roughly like this: Γ ⊢ M : A --------------------- Lazy-I Γ ⊢ Delay M : Lazy A Γ ⊢ M : Lazy A ---------------- Lazy-E Γ ⊢ Force M : A modulo renaming, these are the rules for the `Identity` type. The only difference between this and the `Identity` type in Idris is that the *evaluator* knows to evaluate these differently. That is to say, `Delay` and `Force` are solely flags to the dynamics, and there's no a priori reason why these statics should entail those dynamics. my interest in the previous comment tho is the possibility of a type operator `F` such that `F A` can have *only* the strict (or lazy) evaluation order. and I doubt that such a thing could exist at all, tbh. I strongly suspect that types cannot even in principle entail an evaluation order, and I think that's partially the *point*. that is, type theory is set up to be a theorem of meanings, not a theory of mean-ing.
Sure, done :)
The response rate to the Google form has already massively outpaced the Reddit replies. So on this one I'm glad people pushed me to do it. Glad to admit I was wrong :)
Though this isn't exactly what I needed, it was enough to help me do what I needed to be done. I needed to take in ints from the user and put them in a list so that I can find the min, max, and sort the list. To get the list I ended up doing this readInt :: IO Int readInt = do int &lt;- getLine return (read int) makething :: [Int] -&gt; IO [Int] makething x = do number &lt;- readInt if number == 0 then return (reverse x) else do y &lt;- makething (number:x) return y
Would this help with #1? https://github.com/noteed/rsync-hackage/blob/master/README.md . But you might be better off just using Stackage.
Note that I _think_ the minimal installer currently has a different bug against El Capitan, which reputedly is fixable by copying over a new `cabal-install` binary. See here for more details: https://github.com/ghcformacosx/ghc-dot-app/issues/41
It would be nice to get a meeting going in the north bay, as the city can be pretty hard to get to.
Would be pretty beautiful eh? Free for 'forward' sampling, cofree for 'backwards' sampling. :)
well, rewriting prelude is easy if time consuming. rewriting enough of the packages that use the prelude (like, all do) to be useful, is the hard part.
Values of a greatest fixpoint have to be lazy.
&gt; In terms of the quality of Haskell resources, which do you find the most useful (re-order the list) turned into &gt; When learning a new concept (e.g. Parser Combinators, FRP, etc.) which of these do you prefer (re-order the list) in the Form.
Looking at what you're doing again, my guess is maybe you're coming from a language like Python, where a "tuple" isn't much different from a list. In Python, a tuple is an immutable sequence of elements list is a mutable sequence of elements. In both cases, the elements may or may not be immutable depending on their types, and of course Python is dynamically typed so you don't know the type of the elements (or the sequences or anything else) until you look at it at run-time. In Haskell, everything is immutable (with `IO` provisos) so there's no need for the Python distinction between tuples and lists. A list in Haskell is a variable-length sequence - the length isn't part of it's static type, and all elements have the same type. A tuple in Haskell is a fixed-length sequence - the length is fixed and part of its static type, and each element can have a different type. And of course Haskell is statically typed (though with type inference). That's why the type of a list is spelled as... [a] -- Only one component type specified, no indication of length But the type of a tuple is spelled as... (a,b,c) -- In this case the tuple contains three elements 
update: now there is some exception control, pointers are hidden in ADTs and managed by `bracket`; I'm currently adding I/O from disk/command line. My aim is to make the interface as stateless as possible; I'd also welcome all suggestions for efficient array-vector-array operations; the spec is: a pointer to a foreign resource is acquired and the contents copied to local memory (`peekArray`), some operations are performed on the copy (think entrywise math operations), then the results are poured back into the foreign ptr (`pokeArray`)
&gt; You pay a performance penalty for programming functionally Is Haskell faster than Scala and if not would this be a critique of functional programming rather than Scala itself?
Also, shout out to my Gothenburg (SE) friends of Got Lambda ( http://www.meetup.com/it/got-lambda ). 10-30 people per meeting, a different language every time (Clojure, Haskell, OCaml, Agda ... ), Jean-Louis (@jellismymind) as the multi-tentacled host, and pizza!
The question about learning new concepts seems odd for a survey about Hackage considering Hackage is primarily about libraries, not concepts.
Unit testing STM was [hard for me too](https://www.reddit.com/r/haskell/comments/3kw0eh/haskell_unexpectedly_making_it_harder_to_unit_test/)
&gt; What kind of Haskeller are you? (Select 1 or more) Professional &gt; What year did you first learn Haskell? Around 2012 (I think) &gt; How much code do you write at work (or school if you are a student)? I'm a developer, coding is what I do &gt; When learning a new concept (e.g. Parser Combinators, FRP, etc.) which of these do you prefer. Everything and the more is better. What I choose first depends on what exactly do I need from it. &gt; In terms of the quality of Haskell resources, which do you find the most useful (re-order the list) &gt; Appreciating that different tools are needed for different jobs - consider how frequently you use them balanced with how good they are - Hoogle - Hackage - Haskell Wiki - Community blog posts - Academic papers - GHC documentation - Stackoverflow - IRC &gt; HOW YOU USE HACKAGE &gt; Please list and describe your browsing behaviour when you use Hackage &gt; Use the template below but feel free to add any extra details beyond this template Use case #1 When I am looking for package docs and sources I will start by going to hackage.haskell.org/package/&lt;pkgname&gt; That normally takes me to docs page where I open module I'm interested at Most of the time that gives me docs I want If that fails, I normally try another module There are often cases when you don't know in which module of a package (or several packages) function or type is living. Would be super-useful to have some responsive search-symbol-in-package button with a sublime-text-like fuzzy matching. &gt; What do you think could be useful to improve the Haskell UX in general? - Add list of packages by author - Make search suggestions based on fuzzy search and make them realtime - Current "categories" mechanism is almost-useless, but there's a big potential in areas such as: - which packages are used with this package? - which packages are considered popular alternatives? This is hard to get "right", but it would be very useful. 
&gt; Another cry for a tip: I am sometimes stuck in an helmbuffer (like I start selecting within the list, change my mind and just want to go back editing stuff) and I still don't know how to go out of it. Does `C-g` do what you want?
Chris Done has written a very good post about heterogeneous lists in [Python and in Haskell](http://chrisdone.com/posts/existentials).
For example when I have a module that communicates with a Webservice: I need to put the types for the responses I get into the Types module to avoid circular dependencies. But the fromJSON instance feels like it belongs into the Webservice module since that is where it is used exclusively and where I would search if I started getting errors from that Webservice.
In this case, it's not so much the practice that is faulty, as it is your logic or understanding of type classes. Not trying to be insulting, but thinking of a type class as relevant at the your single point of use is what is convincing you to use orphan instances.
* Student/Academic * In a university course about functional programming. * I mostly write code, though I'm not a developer. * The Haskell wiki and blogs; papers if they're from Wadler or SPJ (i.e. if they're not arcane mumbo-jumbo). Haddocks I use as a replacement for javadoc. Examples are a last resort, as I find that the need to be "practical" obscures the essence of the concept. * Stack Overflow, Haskell wiki, blog posts, Hackage, papers, the rest * I start with Google; if that fails, it's off to the Haskell wiki. Then I take a crack at the types in the package's documentation. If that fails, I might look around for a paper.
&gt; I am always getting Nothing. Literally. In the example just below that sentence you contradict yourself.
The typical thing is to use a state monad with a counter.
Thanks, edited. It's just typo when I was copying examples. 
Why is that? `HOME` is not set manually. But `XDG_CONFIG_HOME` is set manually in my shell configs, and when I try to lookup it I get `Nothing`. :) 
There is no direct relationship between `lookupEnv` and shell configuration. Basically your shell setups some environment, and your program can then access it. Depending on how you are running your executable you might see different outcomes. So, how are you running it ? Can you display the value of the environment variable just before running it ?
Also run `env` to see what's in the environment of your current shell at a given time.
So there is no `XDG_CONFIG_HOME` here, right ? 
I got the same impression (about the contradiction) when I read the post as well. In your example code, you should show how you set `HOME` or at least say how `HOME` was set.
:) Well, I thought that using `HOME` will not bring to miss understanding. Let me update my original post.
I hope he could join the conversation and help.
Re: your last point, I made [this chrome extension](https://chrome.google.com/webstore/detail/hackage-fu/dnpldbohleinhdgfnhlkofpgkdcfcfmf) which will give you a link to the latest version of a page, but hackage should definitely do this :-)
For people new to linear types: there is a [great paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.5002&amp;rep=rep1&amp;type=pdf) introducing the basic concepts. It's called *Linear types can change the world!* (Wadler, 1990). I found it very well written and it has only basic prerequisites (e.g. familiarity with typing rules).
from the [roadmap](https://github.com/LuxLang/lux/wiki/Roadmap): &gt;v0.3 &gt;- Implement all missing JVM-interop mechanisms. &gt;- Add support for polytypic programming. &gt;- Expand standard library. &gt;- Unify tuples &amp; records. &gt;- Finish type-inference. &gt;- Allow Lux programs to use foreign libraries from .jar files. I'm looking forward seeing how this project matures. hope to see more documentation soon.
ELI5 what are linear types? That is, without needing to be familiar with type theory. Wikipedia talks about linear logic, which is not helpful in the least. What are linear types, what does that term imply, and what can you do with them/what do they give you? 
We have a few relevant meetups in the Bay Area, all of which now use meetup.com. First, there are two Haskell meetups which tend to vary in size over time: * [Haskell Hackers at the Hacker Dojo][dojo] in Mountain View, the same location as BayHac * [Bay Area Haskell Users Group][bahask] in San Francisco which bounces around offices of a few different companies There are also a handful of general programming language and theory meetups: * [SF Types, Theorems and Programming Languages][sfttpl] focuses on learning type theory through a mix of a reading group and talks * [Bay Area Categories and Types][baycat] focuses on category theory; currently organized as a paper reading group but also had general talks in the past It's great to have some variety. I've gone to all of these, and there's definitely something for everyone regardless of interest or level of experience. (There are some I've missed too, no doubt!) Also, we're thinking of starting a regular East Bay Haskell meetup. That didn't really take off last time we tried it and I moved to the South Bay so couldn't help any more, but I'm probably coming back to the East Bay so might try starting it up again. There's a [mailing list][east-bay] if you're interested. [dojo]: http://www.meetup.com/haskellhackersathackerdojo/ [bahask]: http://www.meetup.com/Bay-Area-Haskell-Users-Group/ [sfttpl]: http://www.meetup.com/SF-Types-Theorems-and-Programming-Languages/ [baycat]: http://www.meetup.com/Bay-Area-Categories-And-Types/ [east-bay]: https://groups.google.com/forum/#!overview
 foo x = x + x bar x = 42 baz x = x + 1 `foo` is not linear because it uses its argument twice. `bar` is not linear because it uses its argument zero times. `baz` is linear because it uses its argument exactly once. Linear types impose the constraint that something, usually a variable, is used exactly once.
Linear types, a form of what're known as "substructural types", are a system of types where things we normally take for granted as mere "names" have substance instead. For instance, we're used to the idea of being able to write the function dup :: x -&gt; (x, x) dup x = (x, x) but this takes for granted the substance of `x`, implicitly *assuming* we can merely copy it as often as we like. The other big violator is forget :: x -&gt; () forget _ = () which *assumes* it's okay to just drop `x` on the ground, again taking for granted that it is a substantial object. In Haskell we don't usually write these functions because they're implied, but in a linear type system one must not only explicitly state when one either duplicates a substantial value or destroys it, more than that you might actually be disallowed from doing such things! This is important, as I've hinted, when the language considers variables to be more than mere binding names but relates them to memory or messages or other physical, substantial things. As a type system you might start out with the assumption that all things are substantial and then impart non-substantiality, say, via typeclasses class Effervescent a where duplicate :: a -&gt; (a, a) destroy :: a -&gt; () so that only types which carry around `Effervescent` context can be treated as non-substantial and freely, although explicitly, destroyed and duplicated. [It's of curious note that `Effervescent` is something like `CoMonoid`, too.](http://stackoverflow.com/a/23858109/476408)
&gt; That generally makes type inference undecidable Type _inference_ (and checking) is generally decidable (if sometimes intractable) unless your subsumption relation gets too weird. Showing types to the programmer, on the other hand...
Second French speaking meetup here! I'm the co-organiser of the [Paris Haskell Meetup](http://www.meetup.com/fr/haskell-paris/). Although part of its activity switched to the more recent [FP Paris meetup](http://www.meetup.com/fr/Functional-Progammers-Paris/), which has a broader scope but where people still have a strong interest in Haskell (dedicated OCaml, Scala and Clojure meetups already exist in Paris).
These are all excellent explanations. Thanks. I believe the first time I saw a mention of linear types was in one of the papers for Quipper, the Haskell-based EDSL for quantum computation, where the author mentioned that some or other problem would be obviated if only Haskell had linear types. Form your explanations, I can see the connection better now. In quantum programming you are not allowed to explicitly duplicate or destroy qubits. To fanout you would have to use a trick involving zero'd scratch qubits and the controlled-not gate, and IIRC the only way to "destroy" a qubit is by measuring it, which produces lost energy in the form of heat. I'm also aware that Rust apparently uses linear types (essentially) in its ownership system, though I haven't actually sat down to consider/study that properly. 
Isn't call by push value about implicit effects rather than linearity? There is a connection between linearity and effects of course, but if we start with a total language then call by push value is kind of a moot point since effects would be explicit?
Could you extend this to talk about the "degree" of a function beyond just linear and non-linear? For example, \x -&gt; x is linear; call that *^1 \x -&gt; (x,x) is *^2 \x -&gt; 1 is *^0 Create a sensible algebra for our polynomial type system and perhaps we could then apply techniques from differential calculus to analyze at compile time how the memory footprint changes over time.
I think not :) For what I meant the language has to be total, not just able to talk about totality. Let me try to make it a bit clearer what I meant. In some languages the types have hidden effects built-in. For instance in ML the type `a -&gt; b` is actually like `a -&gt; MLEffect b` where `MLEffect` is the type of ML effects (non-termination + state + exceptions etc.). Haskell has effects in basically all types, e.g. a pair `(a,b)` can have an effect when you access it so it's like `HaskellEffect (a,b)` where `HaskellEffect` is non-termination + exceptions. It seems to me like call by push value and various related things are all about coming up with a set of type constructors that have effects hidden in the right places. With these hidden effects you have evaluation order issues because with effects evaluation order matters. Contrast with a total language (or "language able to talk about totality"). Effects are not hidden in any types. If you want effects you need to explicitly use an `Effect a` type. This also means that you have to be explicit about evaluation order. The difference between strict and lazy evaluation is simply when you run your effects. For instance if you have this in an impure language like ML or Haskell, where f has effects: (f 3, 5) Does that mean (strict evaluation): f 3 &gt;&gt;= \x -&gt; return (x,5) Or does that mean (lazy evaluation): return (f (return 3), return 5) In a strict language if you write `(f 3, 5)` it actually means `f 3 &gt;&gt;= \x -&gt; return (x,5)` and in a lazy language it actually means `return (f (return 3), return 5)`. In a total language you make the choice locally by writing the first or the second. All that stuff about evaluation order and where hidden effects happen is moot. 
Okay cool, knowing that helps a lot then. I'll take a look at the guide and hopefully get myself on track. Thanks!
I have a `Grammar t n` type, with specialized one-liners: mapTerminal :: (t -&gt; t') -&gt; Grammar t n -&gt; Grammar t' n mapTerminal = first mapNonTerminal :: (n -&gt; n') -&gt; Grammar t n -&gt; Grammar t n' mapNonTerminal = second which both conveys meaning and reuses code. but to each their own :-) 
The mock-up you linked to was removed, apparently they remove the ones created anonymously after 90 minutes. Could you put it somewhere else, or describe it?
If you have two terms of degree 2 you have any number of terms of degrees &gt;= 1. A single term of type ^*0 means you're fully back to intuitionism.
Those two types are defined in CBPV... so standard references on CBPV should talk about them. It's essentially the entire point of the calculus.
I'm not saying it's pointless to have Lazy, I'm just saying that the statics / type rules don't entail that "Lazy" means lazily evaluated
Can you drop a link to one these standard references?
this is a great idea. if you want to make that mockup a demo, I've written some (very basic) Objective-C bindings that might help. https://github.com/sboosali/workflow-osx e.g. -- uppercase the contents of the clipboard, and paste the result uppercase_clipboard = do oldContents &lt;- getClipboard let newContents = fmap Data.Char.toUpper oldContents setClipboard newContents sendKeyChord [CommandModifier] VKey if you're on another OS, feel free to contribute bindings for it. the `Workflow` type is a free monad, so with a little work, we should be able to make relatively platform agnostic scripts. 
Here's [Levy's thesis](http://www.cs.bham.ac.uk/~pbl/papers/thesisqmwphd.pdf), here's an [overview/FAQ](http://www.cs.bham.ac.uk/~pbl/cbpv.html), here are [some nice posts by Rob Simmons](http://requestforlogic.blogspot.com/2011/08/embracing-and-extending-levy-language.html) on [his implementation](https://github.com/robsimmons/levy) extending Bauer and Pretnar's PL Zoo implementation of a CBPV-like language [named Levy](http://andrej.com/plzoo/html/levy.html), here's [Levy's book](https://www.springer.com/us/book/9781402017308), and here's a cute paper from Conor McBride implementing a CBPV language he calls [*Frank*](http://homepages.inf.ed.ac.uk/slindley/papers/frankly-draft-march2014.pdf).
I wish all apps embedded a server and exposed an API. the only API most have is keyboard shortcuts and mouse clicks. 
I have been using halcyon and cabal on 10.11 for a bit without major issue.. If you do run into issues sometimes filing a radar can help too! Maybe post your radar # just in case someone can help move it along. 
I wrote a library [error-continuations](http://hackage.haskell.org/package/error-continuations) for this.
Sorry. 
The first chapter of Homotopy Type Theory is godly. It was written by ~20 of the most brilliant mathematicians alive, and it assumes no prior knowledge of type theory. However, don't even look at the rest of the book, including the preface. It's impossibly abstract and mathy. 
&gt; "TYPE THEORY AND FORMAL PROOF" by ROB NEDERPELT How do you think yours and this book would mix? I requested TYPE THEORY from another library so I should've read chapter 1 in homotopy by the time I get it.. will there still be stuff worth reading in TYPE THEORY?
https://leanprover.github.io/ You might have had better luck with "Lean prover".
since I've got your attention, someone else suggested the first chapter of "Homotopy Type Theory," how do you think that would mix with your book?
if you have more questions, the easiest way to get answers is probably [IRC](https://www.haskell.org/irc).
I am happy to hear you like the idea. I use linux and when I have the time I might try making a demo, so far the gui part was holding me back, I am not sure, how to approach it. 
(the link to Software Foundations points to "Simple Easy" ...)
Edwin Brady is currently writing a book https://www.manning.com/books/type-driven-development-with-idris The first chapter is very good thus far.
Oops, so it does. Should be fixed now. Thanks!
Weird, the link works for me. The paper is also available on [Wadler's homepage](http://homepages.inf.ed.ac.uk/wadler/topics/linear-logic.html) as [ps](http://homepages.inf.ed.ac.uk/wadler/papers/linear/linear.ps) and [dvi](http://homepages.inf.ed.ac.uk/wadler/papers/linear/linear.dvi). Thanks for reporting.
Software Foundations is a great introduction to Coq (and dependent types).
Feel free to fire off an e-mail to [Wouter Swierstra](http://www.staff.science.uu.nl/~swier004/), he'll be happy to send you some hints / starter papers. While you're waiting for him to respond, read [The Power of PI](http://cs.ru.nl/~wouters/Publications/ThePowerOfPi.pdf).
I'm currently reading Beginning Haskell, which I find very good, for my level. I prefer it to LYAH. cf. http://www.apress.com/9781430262503
I loved LYAH personally. Gave me enough to start, and then you just have to code as much stuff as you can.
Ryan is a great speaker, he should do more Haskell talks! PS: I had to use `youtube-dl` to watch this as I wasn't able to get it to play in the browser
yes, but greatest fixed points are not a type theoretic notion, they're a denotational notion related to evaluation. additionally, the usual sort of type theories generally don't have a way of expressing that something is a greatest fixed point without compromising on certain core principles. at best they can provide something which can, if you wish, be interpreted as a greatest fixed point, but which can also, always, be interpreted as a least fixed point. the only type theories which allow for true greatest fixed points is stuff like focusing calculi (see Noam Zeilberger's work), which have major distinctions between intros and elims, and the choice between least and greatest fixed point becomes a choice between which rule type (intros or elims) defines the type, and which is derived. in his work, intro-defined types are least fixed points, elim-defined types are greatest fixed points. but the proof systems required to do this are quite exotic and bizarre.
I agree that we should have syntactic sugar to hide effects in the terms so that you can write `(f 3, 5)` instead of the monadic expression. In general the programs with effects should look the same as in an impure language like ML. On the other hand the types should be explicit about the effects. Built-in types like `Int -&gt; Bool` should not have implicit effects, use `Int -&gt; SomeEffect Bool`.
No, beings like human have the power to *reason* and machines do not. If you haven't learned about machine learning and artificial intelligence, I wouldn't blame you for not knowing this but so far today computers don't reason at all by any means. What they can do is *learn*, and *only* that. It's actually a small, fine distinction to make when you're witnessing something like Watson, and in fact I think at some point in the Jepeoardy show the machine inclines as much to exactly what I'm saying. I personally don't know of a foundation that is capable of reasoning aside from organic systems like what is present in humans. As far as *computability* goes, you are right-they don't have any more power than a Turing Machine.
If `+` is not linear (forgets the given `x`) does that mean `baz` is also non-linear transitively?
Normally, yes, but it is also possible to set things up so that a few primitive expressions are allowed to drop things internally. This is similar to monadic computations, in which the pure code is unable to perform any side-effect but a few primitives (or, in the case of IO, a lot of primitives) do have effects and can be combined with the pure code in a principled way.
The value of `x + y` presumably depends on the entire value of both `x` and `y`, so it's not really dropping anything. There might be a difference if you're attaching additional meaning to values with linear types (i.e., as memory locations) but as far as types are concerned `+` should be fine. Of course, the only way to prove that *in* the type system would be to use e.g. recursively defined natural numbers, so for efficiency reasons you'd want a `+` primitive that was simply asserted to be linear.
I never said this was an original idea :)...
And there's the excellent "simpler easier" blog post by Augustus.
Oh, I'm not complaining! It is a source of sadness to me that Plan9 never took over the world. If we could reignite those ideas I'd be all for it.
The main downside is the usual `Codensity` downside, which is that if you inspect the computation twice it has to start over from scratch. When used at most once, it can be a win. This is usually the case with `Codensity`-style constructions.
Who's that a photograph of on the front cover? https://images.manning.com/255/340/resize/book/5/c5c8ebb-d694-41dd-b9f2-364aee5d86d3/Brady-TDDI-MEAP-HI.png
Unfortunately there is no video, but the slides cover each of the examples in detail.
Cool, thanks.
Introduce you to dependent types in what sense? To understand the properties of dependent type systems? Or to understand how to program effectively in dependently typed languages? The two are not the same. If it's the former, then I suggest the book Lectures on the Curry-Howard Isomorphism which builds up from the untyped lambda calculus to Pure Type Systems, covering a lot of (interesting) ground in between. If it's the latter, the I suggest either Chlipala or Pierce's books on programming in Coq, though there are still a lot of tricks not covered by those books that can only be picked up by reading papers and bodies of code like Agda's standard library.
Apologies upfront for the spam! I'm working on a book with my [coauthor Julie](https://superginbaby.wordpress.com/) at http://haskellbook.com/. Our priorities center on making sure a diverse group of people can succeed in learning Haskell and that they'll be equipped to learn how to use Haskell to build things afterward. It's about 2/3s done, current release has ~797 pages and goes up to the Monad chapter. We'll be releasing Foldable, Traversable, Reader, and State on the 15th of this month. plus some add'l content we haven't talked about. Feedback has been very positive so far. Some is noted here: http://haskellbook.com/feedback.html Other comments we've gotten have been: &gt;speaking of the book: I didn't really understand how damn comprehensive it actually is. I'm so glad I went for it; you and your co-author have done a superb job so far. (This was from a member of the IRC community that has been reading it independently.) &gt;I’m only up to page 194, I’ll say more later, but I’ve read a LOT of programming books in my time, and learned a LOT of programming languages, and the biggest thing that’s standing out so far is that there are ZERO technical errors. And that’s for a book that is in beta. (This was from someone that got the book and started sending us feedback of their own accord. They did find some minor technical mistakes later, but that's why this is early access.) &gt;There’s also another subtle thing that I’d like to note that I really appreciate… and that is… when one is “learning” from a resource such as this book, there’s a kind of shape that the resource builds in its sum totality - in impact on its readers. It’s kind of like the topology of the resource, if there is such a thing. Books like LYAH, or Realm of Racket, or _Why’s Poignant Guide to Ruby have a lot of stuff that you *have* to wade through which is not related to the topic in order to get the knowledge out. I love that your book has little of this. https://www.reddit.com/r/haskell/comments/3l6lk3/haskell_programming_just_added_monad_chapter/cv3mvo2 https://www.reddit.com/r/haskell/comments/3l6lk3/haskell_programming_just_added_monad_chapter/cv3uwek https://www.reddit.com/r/haskell/comments/3l6lk3/haskell_programming_just_added_monad_chapter/cv3nwvv https://www.reddit.com/r/haskell/comments/3l6lk3/haskell_programming_just_added_monad_chapter/cv3ytgx https://www.reddit.com/r/haskell/comments/3l6lk3/haskell_programming_just_added_monad_chapter/cv558ml https://www.reddit.com/r/haskell/comments/3laioz/good_book_to_learn_haskell/cv4nbd5 If you're curious as to why I decided to start writing a book, [I wrote about issues with existing resources](http://bitemyapp.com/posts/2014-12-31-functional-education.html).
You mention that you could use your favorite window manager--is this mockup an addition to existing window managers? It's a bar that reports certain wrappers around applications? I'm assuming most applications won't be compatible, so do you have the wrapper for each specific application and have nothing otherwise? Is it going to be a completely different TWM from scratch? Either way, it seems interesting. If I completely understand your idea at some point, and you wish to have the codebase in Haskell, let me know! I'm itching to contribute to an interesting open source project that catches my interest which this might do.
You made a statement about undecidability and then that only reasoning beings can achieve said proofs of equality, when the actual case is that even the smartest prover can only achieve a speedup over the all but the dumbest.
&gt; TYPE THEORY AND FORMAL PROOF Damn, that was a hard pdf to find. But do not despair my friends, its out there, the net giveth.
Currently, there is just this idea. The WM is actually not that important once you can reasonably well interfere with the way it launches applications. I only mentioned TWM because it is the most natural habitat for the thing I have in mind. So what I imagine is that if one wanted to implement this, he/she would probably create a WM "agnostic" bar that could communicate to wrapped applications. Regarding the wrapper, that is a very good question. Many applications integrate with dbus, others might be manipulated to some extent throught X "api", others would probably need a specific wrapper. The idea is, can you launch an app and collect its windows and issue events directly to those windows and switch focus among them? If yes, then clipboard integration into the DE would be possible, even capturing parts (screenshots, screencasts, ...) could be doable. Some applications have their own api that could be wrapped for much more interesting functionality. Also XMonad seemed the obvious choice as it's in haskell and can be configured nicely, to maybe not launch apps directly but rather through the bar so that it can then manipulate them. The bar could indeed let you start new apps too.
I think this is the best Haskell talk for general audience that I've seen which clearly articulates the benefits of purity and also explains the root causes of callback hell. I enjoyed it tremendously. I got also intrigued by the two teams each having 20k lines of code but one of them only having 1/10 of the number of defects than the other team. I wonder how solid is this data? Is it just anecdotal or is the data rigorously collected (and maybe even published?)? It's rare and very costly to obtain data like that in the lab and it would be great to have hard data like that published.
I wouldn't call it "rigorous", but it's based on my observations of the number of tickets we ended up with in our bug tracker.
There [is a language](https://bitbucket.org/roshanjames/theseus) which forces everything to be invertible... and I've wondered about the potential connection to linear types.
Sort of. I would consider it really a ghci session with preloaded stuff so that you could do all the stuff you mentioned but also more :). Sure wayland is a posibility. Do you know anything about interacting with X/Wayland apps? I am not sure how the bar would detect new windows. Basically, apps would need to register with the bar, WM could probably register those windows that you want to manipulate via X events. DBus apps could be wrapped so that they register themselves (but then you have two different concepts: windows vs whole processes/services). **Edit:** Another option is to provide some trivial wrapper for any app, that exposes nothing special, maybe just some kill/signal thing, maybe even some analytics based on pid... and require all apps that you want tu use in this DE to have appropriate wrapper that binds tightly with the app and exposes all that one could desire. Maybe we could move the discussion somewhere more appropriate...
Unfortunately I don't know anything about X or Wayland apps. I would assume that the bar does not need to detect new windows, right? It could just get info from whatever is focused and would be triggered by a change in focus.
Were the two teams roughly equal in terms of programming skills? The critique I often get about such comparisons is that the Haskell team is composed of better programmers and therefore the used language is not causal for the lower bug rate. And there doesn't seem to be much scientific data about the effect of programming language on correctness.
For simple functionality that would probably work. Except the focused window would in this case be the bar :D
Hmm... then in Rust terms does `A -o B` roughly mean that after you provide an `A`, you get back an `&amp;mut B`? How might `A -o B` be represented in memory? A pointer to the inner (missing) `A` and one to the outer `B`? (`&amp;out T`, of course, is just a single pointer.) &gt; Note this only works for strict value types, since there is only one possible hole (itself). For recursive types like list, the whole [sic] could be toplevel (itself) or it could be the tail of the list, so you'd need to specify the total size of the recursive structure, and the size of the hole, to properly allocate memory. Could you elaborate on what this might look like in practice (perhaps with a pseudo-code example)? I don't quite understand the problem that's being addressed.
&gt; `ghc-mod` is a widely used tool, and so is kept up to date with all the changes in the surrounding ecosystem. For example, the recent Cabal changes, and providing support for stack It took some time though to adapt to GHC 7.10 and Cabal 1.22 leaving users out in the cold :-/
I am a beginner and I am trying to solve a problem with dynamic programming. I have tried search for this but the tutorials usually use fibonacci or something like it as an example and it has the property of being a sequence (using zip and so on), meaning it only depends on the last two values. But in my problem, the values I need to lookup are random and I don't know how many lookups I will do, hence why I need some sort of lookup table. I have prepared a snippet of the interesting part and more details here http://lpaste.net/5868279907481550848 I asked on #haskell in freenode, someone suggested constructN in Vector, but I think it is a wrong approach since I do not know the number of iterations I need. Probably good for something like fibonacci though but I could very well be wrong.
But reasoning beings *cannot* arrive at proofs of equality in the general sense. That's what being undecidable *means*.
You haven't told me anything that I didn't already say or know with that, thank you.
Well, then I apologize, but that wasn't clear at all before.
This looks impressive. A mix of refactoring tool that works in cabal context is a killer composition.
Well, according to the repository HaRe itself doesn't even have GHC 7.8 support, nevermind 7.10, or is the README there outdated?
Thanks. And it just needs decent emacs integration. And all the other editors/IDE's out there
A new version will be coming out soon that supports 7.10.2 and up.
I live about an hour outside Philly and use Haskell professionally and this sounds awesome and I would love to go, but I've never been to a conference or anything like this and have no idea how it works and don't really know anyone to go with. Could anyone explain more about how this works and what it would be like? I just can't imagine just showing up some place and talking to people about this? How structured is it?
It is only O(n) amortized to many calls. If you do n calls, to consume the whole list it is O(n²) which is larger than strictly sorting all the list O(n log n). Therefore, no universal rules were broken.
Hard to debug without seeing output. I did notice that in your 'i' function (maybe a better name would be good :), you don't include xres or yres, but that shouldn't change the behavior so much. Couple of style points I can't resist imposing: top level type signatures really improve readability, and it might be better to decouple your model/view a bit more. For example f cs = M.mapWithKey (next cs) cs should probably be its own function. Also, cool! Never seen that automaton Edit: Actually it looks like you want a total of xres*yres cells, so plane should be [0..res-1] to make i correct
It's not very structured at all. If you can possibly make it, I HIGHLY recommend it. Attending hac-phi and rubbing shoulders with other Haskell people has greatly enriched me as a Haskell programmer.
I liked that too. I liked Learn You A Haskell too but Learning Haskell covers real world things. It's the closest to a Real World Haskell Second Edition that we have.
Beat that, Frodo!
If you think it might be fun, then you should absolutely come! It's a room full of people with whom you can dig in to nearly any technical topic that interests you. What I love about it is the rawness of it all. You can just walk in and ask for suggestions about how to approach any sort of programming question. No club membership required!
why then you just add Trifunctor (lol)
Frodo never had to teach anyone about monad transformers. :) 
I really like how organized this effort is. For me personally these reports have been a major reason for trying Haskell Mode.
Thanks for this comment. It is important to get feedback confirming that we are on the right track! The plan is to get more users so that some of them convert to contributors. Everybody benefits!
Saying that `id` actually has two arguments is the Church-style explanation of polymorphism. There's also the Curry-style where we regard `id` as untyped and the types are purely external to the code. 
Here someone who is dumber. I am a bit surprised to find out types converted into function arguments. I thought that types were used only for static analysis, and had no impact on the way the code was compiled
That post/discussion looks really interesting. Thanks for sharing, I'll be sure to check it out :)
Thanks for your work! There is some information on numerical issues in [Polynomial real root finding in Bernstein form](http://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=5245&amp;context=etd) There also was a [recent thread](https://www.reddit.com/r/haskell/comments/3lxz27/80_of_haskell_packages_that_use_floating_point/) about finding numerical issues automatically in haskell. The following paper was linked, and contains useful tips on how to test for numerical issues by hand: http://www.cs.umd.edu/~ntoronto/papers/toronto-2014cise-floating-point.pdf
They are not actual arguments to the functions. The type arguments are erased. The explanation presented in this blog post is very ghc-centric, where types are inserted to be removed again later. 
Are there any other advantages to the new @ syntax in 8.0 other that removing the need for `Proxy`. (Does it do that even?)
They are not "real" arguments, because you can't manipulate them in the function body, because they indeed don't exist at runtime(this will change in future when GHC will get dependent types). They are arguments in the same sense like type arguments for generic methods in C#/Java or template functions in C++ id :: a -&gt; a id x = x is equivalent to C#/Java code: A id&lt;A&gt;(A a) { return a; } or C++: template&lt;typename A&gt; A id(A a) { return a; } Out of all these languages only C# keep type arguments at runtime. To call this function in Haskell you write: id 4 id "Test" in C#/Java/C++: id&lt;Float&gt;(4); id&lt;String&gt;("Test"); though you can sometimes omit type arguments if the compiler is able to infer them: id(4.0f); id("Test"); Haskell having much better type inference, doesn't even let you explicitly specify type agruments at call sites. It is not always convenient, because sometimes even Haskell fails to infer type. In such cases you are forced to use dummy datatypes like data Proxy a = Proxy Next version of Haskell(8.0) will lift this restriction so you can write: id @Float 4 id @String "Test" like in C#/Java/C++
I would rather argue that, because you're going to need an elaboration semantics for type-classes anyway, using one for types as well is not unreasonable from a pedagocical point of view. But I don't think you're "stuck" with the explanation: you can explain type-class elaboration on the type *derivation*, and then give a type-erasure dynamic semantics on the elaborated term. In fact I would prefer this approach. 
I don't think it removes all needs for `Proxy`. It is rather that there were cases before where you had to use `Proxy` in highly contrived ways, and now you can use `@` elegantly instead. You should really have a look at the current draft article, [Visible Type Application (extended draft)](http://www.cis.upenn.edu/~eir/papers/2016/type-app/visible-type-app-extended.pdf), Richard A. Eisenberg, Stephanie Weirich, and Hamidhasan Ahmed, 2016. Research papers start with an introduction that explain the problems solved and the advantages of the solution proposed. This one is well-written and I enjoyed reading it.
I'll read it. Thanks!
You can give a Church-style semantics to typeclass constraint witnesses, which coincides with the semantics of elaborated dictionaries, and I do not object to that part at all. But that does not require having types around (in particular having type abstraction as part of the operational semantics). Note that you remark that "elaborated Haskell is not Haskell", yet the semantics given for type polymorphism above is exactly through an elaboration pass, from the actual source code people (without much types in terms) wrote to an "under-the-hood" more explicit version. Why would this be better? I think modern programming languages mix type and code inference in ways that sometimes necessitate discussing elaboration, or various form of type-directed code inference. As pigworker insists, "inferred" and "erased" are orthogonal concepts nowadays. I'm somehow more satisfied with explaining an elaboration of typeclass witnesses (because they matter in the operational semantics) than an elaboration of type applications (because they don't).
&gt; Note that you remark that "elaborated Haskell is not Haskell", yet the semantics given for type polymorphism above is exactly through an elaboration pass, from the actual source code people (without much types in terms) wrote to an "under-the-hood" more explicit version. Why would this be better? I'm not arguing that it is. &gt; I'm somehow more satisfied with explaining an elaboration of typeclass witnesses (because they matter in the operational semantics) than an elaboration of type applications (because they don't). That sounds like a reasonable position.
This doesn't explain why we would need the @syntax rather than just adding type annotations. Why write `readInt = read @Int` when you can already write `readInt = read :: String -&gt; Int`? Is it just for convenience or do we really gain new abilities from this?
[You might be interested in this.](http://blog.sigfpe.com/2006/12/evaluating-cellular-automata-is.html?m=1)
for one, I think it can replace the proxy/undefined unused argument pattern. old: class Storable a where sizeOf :: a -&gt; Int instance Storable Bool where sizeOf _ = 1 -- (not really) sizeOf (undefined::Bool) sizeOf False new (one day): class Storable a where sizeOf :: forall a. Int instance Storable Bool where sizeOf = 1 sizeOf @Bool 
Consider the case where you have a function with this type: example :: (a -&gt; Char) -&gt; (a -&gt; String) ... and you want to specialize it to: example :: (Int -&gt; Char) -&gt; (Int -&gt; String) It's much easier to write: example @Int ... than it is to write: example :: (Int -&gt; Char) -&gt; (Int -&gt; String) It's also much easier to write: foo @X ... instead of foo (Proxy :: Proxy X) This disparity grows more pronounced the larger the required type annotation becomes The other reason is that it's closer to how things actually work under the hood in GHC's core language. Type annotations are a very circuitous way of saying what we actually meant.
I tried it first with Repa but stopped when I realized I don't know how to compute this particular CA rule with convolution; I decided to test it with more familiar methods first, then rewrite it in Repa if it works. But it doesn't work, and I'm really curious to understand why - there's no point in figuring out Repa if I've misunderstood something basic about the automaton.
&gt; IxUnit is a lifted (), like Generics' U1? Yes, only it's another step up -- it maps any indexed type to a type that has a single inhabitant for any index. &gt; you might be able to reuse some Generics types, btw I'm not sure this was the right decision, but I wanted to make this more or less self-contained, and so I avoided using anything not in the prelude. If it were intended for practical use, re-using as much of the standard lib as possible definitely would have been the right thing to do. &gt; background on IxProj and IxOut and everything else would help (right in the readme) I did more or less just that in the Idris iteration of this thing (which is just a single file liberally sprinkled with comments), I'm not sure I have it in me to do the same with the Haskell version right now. :-) I should probably link that as an introductory reading in the README, though. &gt; start by representing a very simple type (like Bool) Idris version has a definition of booleans as a fixed point of a very silly base functor, but I thought it would be too much here. :-) &gt; that type is glorious That would be the word, wouldn't it? :-)
Thank you, it is an interesting paper.. I had seen it once before. It's a lot to take in but I'll give it another read. Still, I'd like to know what went wrong with the version I wrote above.
Thanks a lot for the response this was very helpful for me. I like the thought of just planning to get some work done and anything else would be a bonus.
&gt; They are not "real" arguments, because you can't manipulate them in the function body, because they indeed don't exist at runtime(this will change in future when GHC will get dependent types). Not really, no. Usually in dependently-typed languages, you cannot inspect the types either (and you could reap the benefits of parametricity all the same). If you do choose to introduce a universe, that's another matter but you'll still only manipulate data.
I'm sad to say that this will be the first year of Hac-φ that I've missed. (The dates coincide with the Google Summer of Code Mentor Summit.) =/ Oh well, there is always next year.
Thanks, I've stumbled upon this notation without reading [the docs](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/other-type-extensions.html#pattern-type-sigs), and I've thought that writing a type signature like that is just a shorthand for writing a declaration signature. 
Is there a good higher-dimensional implementation of that technique? The comonadic approach depends on being able to efficiently "shift" in each dimension.
not language neutral without sum types. do they have them? if so, that would be cool.
Why are exceptions used in Haskell instead of Either? Also, Is there a way I can automatically tell (a la Java) that I'm not handling an exception at compile time?
That would eliminate the need for type families, is that right? 
I am really looking forward to it, thanks.
I wrote [this](http://blog.hoersten.co/post/110096363794/modern-emacs-haskell-mode) a while ago but things have changed since stack. Stack-ide mode is Chris Done's (plus others) latest effort to have a GHC API-based backend for emacs that does everything you're looking for. It's a work in progress. Basically there's ide-backend which is a wrapper around GHC's API. On top of that there's stack-ide which is a JSON IDE server. stack-mode uses stack-ide for things like on-the-fly compiling with flycheck, auto complete with company-mode, and other types of code nav and things with haskell-mode. So far I know of no recent guide since thing's are changing quite a bit recently. There's also a big push in haskell-mode, lead by Gracjan, to strip out a lot of the legacy cruft and things that have built up. Basically a modernization of the code. A lot is going on right now with emacs and haskell. Most is a WIP. Edit: I should say that I prefer GHC API-backed IDE integration so it doesn't fall behind GHC and most accurately represents the language features.
https://github.com/pbl64k/gpif-idris/blob/master/IxFun.idr is exactly what I wanted, thanks!
It conflicts with a prior scheduled trip for me as well, so ditto on this probably being the first I've missed. :-( Anyway, I highly recommend going to anybody that can make it, and also will certainly try twice as hard to go to the next one.
ghc-mod now supports stack
Not really. There's been some work on promoting ordinary functions to the type level, which would eliminate the need to write some type families. But there's nothing on the value level that would promote into \* and open type families don't really correspond well to functions, so it wouldn't subsume them entirely. What (optionally) non-erased types *would* do would be to remove the sort of hoops the `singletons` library helps you jump through. Right now if you want to inspect or pass along the value-level version of a type (say, to return the length `n` of a `Vec n a`) you need a 'singleton type' that mirrors the type you're working with (`SNat :: Nat -&gt; *`) and a type class to map the type (`n`) to its corresponding singleton value (`SNat n`). In a dependent language you'd just be able to use `n` directly, since it doesn't leave the realm of ordinary values just because it appears in the type of your vector.
[I did this](https://www.reddit.com/r/haskell/comments/3kr6m3/update_i_got_spacemacs_working_using_stack_with), but Spacemacs does nuke your previous Emacs setup. It works like a dream, however. 
https://github.com/serras/emacs-haskell-tutorial/blob/master/tutorial.md this is touted as a tutorial but it also serves as an excellent guide to setting up haskell with emacs, unfortunately it doesnt also cover stack, but stack is setup is pretty much orthogonal. so you can set up stack in a standard way alongside your standard haskell setup
I have thought about this as well, but never found any established math concept to steal from (besides the structure at hand). The main issue is probably that vertical and horizontal composition do not commute. If we denote vertical composition with * and horizontal composition with •, then we usually have (a * b) • (c * d) /= (a • c) * (b • d) because the musical scores corresponding to a and c may have different lengths. On the other hand, that is probably a good thing! The [Eckmann-Hilton argument][1] says that any two operations with this property and the same unit element are, in fact, the same! It also follows that they are commutative and associative. [1]: https://en.wikipedia.org/wiki/Eckmann%E2%80%93Hilton_argument
Yeah, I installed it, everything went fine except that it still shows the same error message when I type 'ghci' on the terminal. 
I just tried it. and it works! :) 
FWIW I've also added an accompanying blog post with my findings as a Haskell noob: http://tojans.me/blog/2015/10/04/hangman-in-haskell/
Just typographically, it's a really bad idea to use the same face for emphasis and code. &gt; Make sure you don’t add state you can calculate from other state, use functions for that, otherwise your data structure might get inconsistent. This is [called *normalisation*](https://en.wikipedia.org/wiki/Database_normalization) in the database world! Might be useful to mention that it's not unique to Haskell or anything.
Yes, I agree. Modular implicits are indeed likely not the final answer either. They are still defined by elaboration. Secondly, while modules solve the coherence problem in some cases (like for Set), it's unclear to me whether modular implicits inference still works well when you structure the modules that way, and in more difficult cases it's not even clear how to structure the modules. Modules on their own are one of those "perfect" constructs, so I think whatever solves this problem will be an extension of modules (I mean it will contain modules as a special case, or be orthogonal to modules). I think the same won't be true for type classes. Whatever will solve the problem will be a (partial) *replacement* of type classes, not an extension of type classes or orthogonal to type classes. Or maybe this is all bullshit :)
Should probably consider changing the name to something else, so people don't confuse it with a library for the Pebble watch
There are several routes: 1. `stack-ide` seems to be not on feature-parity with `ghc-mod` but improving rapidly. In the future this will be most welcome as it integrates with `stack`for know the exact environment you try to compile your code in. This approach is "stack-only", but I do not have a problem with that (a `stack.yaml` is easily created, and really saves my ass on a regular basis). 2. `ghc-mod` now supports `stack`. This approach would fit with both "traditional cabal" and with stack-style builds. Though the setup is a little fragile since you need to make sure yourself that `ghc-mod` picking up the right version of GHC. See [my comment on this thread](https://www.reddit.com/r/haskell/comments/3kr6m3/update_i_got_spacemacs_working_using_stack_with/cuzzbz5) for instructions on how I got this to work. 3. Using `ghc-mod` w/o Stack is also an option. I don't see much reason for going this route, as in route nr 2 you manage the GHC version that is being picked by hand. From FPCompletes survey we learned that build tooling and IDE tooling were main points of concern. With Stackage/Stack the build tooling came a long way. Recently I found more activity on the `stack-ide` side of things; as I think IDE tooling that integrates closely with the build tooling is the way to go I really hope this will advance soon. In the meantime I'm on ghc-mod+stack :) 
My approach to this was to think about roughly how inference/unification proceeds. The input expression is `curry . curry` (or equivalently, `(.) curry curry`. Now, we know some types: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) and (up-to α-equivalence) `curry` has both of: curry :: ((d, e) -&gt; f) -&gt; d -&gt; e -&gt; f and curry :: ((g, h) -&gt; i) -&gt; g -&gt; h -&gt; i as its type. Since we want to see where the resulting type comes from, we give the two instances of `curry` visibly different (but equivalent) types. Now, we can simply "line up" the type of `(.)` and its two arguments. We obtain some equations for the type variables of `(.)`'s type, which are: b = (d, e) -&gt; f c = d -&gt; e -&gt; f and also a = (g, h) -&gt; i b = g -&gt; h -&gt; i Notice that we have two *different* equations for `b`. For the original expression to be well-typed, we must consolidate these two equations (i.e. make sure that they are compatible, that is, both instances of some more-general type). The process we use to do this is unification: as input we give two types, and as output we either have an error or a substitution from type-variables to types. For the following to make sense, remember that `(-&gt;)` is right associative, thus `g -&gt; h -&gt; i` is `g -&gt; (h -&gt; i)`: 1) unify ((d, e) -&gt; f) and (g -&gt; h -&gt; i) -- Both sides are function types so recursively unify their domain and -- co-domain types: 2) = unify (d, e) and g -- The second argument to unify is a type variable, so we output the -- substitution: g =&gt; (d, e) Now, we apply the substitution to the -- co-domain types of step 1), but this has no effect (g does not appear -- in either). So we simply unify the original co-domain types: 3) = unify f and (h -&gt; i) -- The first argument is a type variable, so return the substitution: -- f =&gt; h -&gt; i -- We have now successfully unified the domain and co-domain types, so we compose the -- resulting substitutions, to obtain: {g =&gt; (d, e), f =&gt; h -&gt; i}. With this substitution in hand, we can apply it to our original equations, which gives us: b = (d, e) -&gt; h -&gt; i c = d -&gt; e -&gt; h -&gt; i and also a = ((d, e), h) -&gt; i b = (d, e) -&gt; h -&gt; i Notice that both equations for `b` now agree (precisely what we set out to achieve with unification!), and thus we can now use these equations to rename the co-domain type of `(.)`, `(a -&gt; c)` to get: (((d, e), h) -&gt; i) -&gt; d -&gt; e -&gt; h -&gt; i which, after renaming by `{d =&gt; a, e =&gt; b, h =&gt; b1, i =&gt; c}` is exactly the type reported by ghci: Prelude&gt; :t curry . curry curry . curry :: (((a, b), b1) -&gt; c) -&gt; a -&gt; b -&gt; b1 -&gt; c Now, for a similar exercise, work out the type of `(.) . (.)` :-) 
Spacemacs (with its haskell layer enabled) works perfectly out of the box for me. It supports autocompletion, on the fly errors/warnings, jump to definition, ghci repl, hayoo/hoogle integration, etc. **edit:** Make sure you start the repl first by using `&lt;spc&gt; m s b` or some features won't work. 
have you tried http://hackage.haskell.org/package/postgresql-simple-0.5.0.1/docs/Database-PostgreSQL-Simple.html there is an example on that page which matches your code conceptually there are matching libraries for mysql and sqlite. I think persistent also supports raw sql: https://github.com/yesodweb/yesod/wiki/RawSQL#fetching-data
yes, but I'm not sure if I can just switch it out with sqlite without modifying my code, because I'd like to work with sqlite while developing my app and than switch to the "real" database later.
As a Windows GHC user, I couldn't love this more.
Disclaimer: I am a beginner. After a bit of a struggle, I have almost figured it out on how to do this with postgresql-simple library. Basically, if you want the query function to return a list of a particular data type, you have to make that data type an instance of a type class called [FromRow](https://hackage.haskell.org/package/postgresql-simple-0.5.0.1/docs/Database-PostgreSQL-Simple-FromRow.html) that is declared by the library. You can see an example of this in the code of [my side project](https://bitbucket.org/sras/babloos-hs/src/e3bb57a5950274bf1bf1cd71e6b0c45753d8a499/Babloos/Types.hs?at=default&amp;fileviewer=file-view-default#Types.hs-20:33) If you just need a tuple of values, then you can just annotate the expected type, after query function call with the expected types. like [this](https://bitbucket.org/sras/babloos-hs/src/06bffa288865af1fc30f95d7be9afc58ec061aee/Babloos/Domain.hs?at=default&amp;fileviewer=file-view-default#Domain.hs-44:50). Feel free to ping me at skype at sandeepcr529 if you face any more issues regarding this. 
You will find the haskell community is less likely to develop libraries and tools designed to do things incorrectly. The focus on correctness in the language naturally creates a community where people are more likely to care about correctness. The HDBC libs do what you want, but there's a reason everyone recommends against using them.
I think [HDBC](http://hackage.haskell.org/package/HDBC) fits here. It allows you to write sql as strings, and run them using a connection. The connections are database dependent, and there are implementations for [Postgres](http://hackage.haskell.org/package/HDBC-postgresql), [SQLite](http://hackage.haskell.org/package/HDBC-sqlite3), [mysql](http://hackage.haskell.org/package/HDBC-mysql) and an [ODBC connector](http://hackage.haskell.org/package/HDBC-odbc) as well.
Out of interest did you think about implementing the `Num` typeclass for `Expr`?
FYI I've updated the gist, and refactored a bit more: https://gist.github.com/ToJans/e97db3b4ed3902677361
You can also do this by combining the `ad` and `numbers` library: &gt; :m Numeric.AD Data.Number.Symbolic &gt; diff (\x -&gt; x^2 + sqrt x) 1 2.5 &gt; diff (\x -&gt; x^2 + sqrt x) (var "a") a+a+1.0/(2.0*sqrt a) The only difference is that it does not simplify as much.
Funny, I just played around with symbolic derivatives last week. I use AD to actually calculate the derivatives, but my expression language implements Num and supports multiple variables, see https://github.com/JPMoresmau/rnn/blob/master/src/AI/Network/RNN/Expr.hs
It's not GHC-specific. This is the way that all System-F-like systems work. See for example Agda or Idris which provide this exact same feature.
I think Pebble is a fine name. It's a far more fitting name for a calculus library than it is for a smartwatch!
What? The feature described by the OP is basically what every ORM manages to do. Are you saying that SQLAlchemy is doing things incorrectly? I also don't like to use HDBC, but it doesn't necessarily mean there's no way to build up the right abstraction over the SQL layer. It just means a particular implementation in a particular language needs more works.
It would be nice with a more advanced simplifier for `Data.Number.Symbolic`.
Wasn't it supposed to be DBUS?
&gt;The feature described by the OP is basically what every ORM manages to do And it has cost billions of dollars. ORMs are a problem, not a solution to a problem. &gt;Are you saying that SQLAlchemy is doing things incorrectly? Yes, of course.
would type level addition be closed? I.e. is `Vector a (n '+ m)` a `Vector a n` or `Vector a m`?
Quick googling gives me this: [README of OCaml's Cap'n'Proto implementaiton](https://github.com/pelzlpj/capnp-ocaml/tree/98584c5cd3a9fc6370e9cf647876b74e5c4dff50#union-fields).
Do you mean `Vector a (n '+ m)`?
It will still just call out to 'opt' and 'llc', the mechanics there aren't going to change much. Plus actually linking things would likely be a bit more complicated (we need to distribute headers, etc).
Cool. IMO `addHangImage` really belongs right next to `hangmanImages` or even implemented inside it, because it is part of a whole image, and coupled with it. `getNextProgress` probably should be named `nextGameState`? I think: wasLastGuess' = wasLastGuess $ GameState progress' word' newGuesses is not nice as it creates an "invariant-breaking" `GameState`. One easy way to avoid it -- is to avoid putting the `GameStatus` inside `GameState` but make it a function that computes it -- so you don't need to maintain the invariant (that `gameStatus` matches the other fields in `GameState`). Also `wasLastGuess` takes the new state, not the previous one, so `was` is misleading. `exhaustedWrongGuesses` perhaps? Also if it just needs the number of wrong guesses, I think it's nicer to pass it just that instead of constructing a whole `GameState` it won't use. `guessState` everywhere should probably be named `gameState` (I think I forgot to fix that in my gist). The list of words ought to be a vector (though it seems fast enough with terrible `O(N)` indexing :-) ). Why not throw `putStrLn $ unlines $ ` into the `getDisplayState`? 
At first glance this seems amazing, but then I get a niggling doubt. Once we have a fixed version on LLVM, there's a lot less motivation to keep GHC working with tip LLVM. It'll require a maintainer who really cares, as opposed to random people solving local problems--something OSS thrives on.
&gt;&gt; How do you implement a system that is capable of differentiating between "not tested against major version X" and "does not work with major version X" without introducing more information than cabal version bounds currently support? &gt; &gt; Before we even get to that question, It would have been more honest to say "You can't, but..." instead of "Before we even get to that question,... " which I felt was needlessly evasive. &gt; I think we need a much more detailed proposal for precisely how that information can be used in the solver to provide a tangible improvement in the user experience. I have not thus far seen such a proposal. This is also fairly evident. You first try to find known successful versions, then, if necessary, try out untested but permitted versions. (You could print a message if you've gone beyond known success, and you could set a flag to "conservative" if you're building something mission-critical.) You fail if you only have known fail versions left. This makes it less frustrating when there are out of date upper bounds, and automatically includes accurate upper bounds information. You can use tooling to generate accurate information about dependencies, so people can't mess up the policy quite so badly as they currently do. Because you can make it easy to be accurate, you get more compliance. As a fan of using bounds to indicate build success, I'm a little confused as to why you object to recording your favourite data in a way that is unambiguously about actual build success, and therefore less open to use in a way with which you disagree. My understanding was that your primary motivation was accuracy of build success data to ensure that the solver never uses versions that don't actually work, but here it seems you'd sacrifice that goal in favour of the status quo. I suspect I've misunderstood you somehow. (Sorry if so.) 
They got there first though, I'm fairly certain it's trademarked
fixed: https://gist.github.com/ToJans/e97db3b4ed3902677361 Thanks again!
[this](http://www.freedesktop.org/wiki/Software/dbus/)? apps don't need to standardize, for the next app we write to have some basic API. 
AFAIK there are no current plans to require totality of term-level functions before lifting them. Haskell's type system is already woefully unsound as a logic, so adding one more form of unsoundness isn't considered too bad. Compile-time termination can be guaranteed by just limiting the stack/runtime afforded to the type checker; same as we do already for other things in the face of undecidability. However, the bigger issue is about solving for indices in the face of arbitrary functions. There *are* plans to add modular solvers to try and handle the issues that arise here. For example, proving that `x + (y + z) == (x + y) + z` or that `x + 0 == x` (when addition is defined by induction on the first argument). These sorts of issues are already a problem just from typefamilies.
I'd guess that all lifted (non-method) functions will be considered closed, since we can only define closed functions on the term-level. If type-class methods can be lifted *as being type-class polymorphic* (rather than only lifting a particular instance), then presumably they'd be considered open.
Nice! One last thing I'd consider, is perhaps inline `displayState` into `gameLoop`, because then you can throw the 2 lines: c &lt;- getANewChar gameState gameLoop $ gameState { guesses = guesses gameState ++ [c] } into the `Guessing` case alternative, instead of repeating the check of which game state it is.
&gt; Does C-g do what you want? I will try next time. Thanks [updated] well it doesn't 
that makes sense. I have a few questions if you don't mind :-) 1. in what ways is Haskell/GHC unsound besides bottom? 2. what does "solving for indices" mean? 3. by "modular solvers" do you mean type checker plug-ins like [this one for units of measure](http://adam.gundry.co.uk/pub/typechecker-plugins/)? how do dependently typed languages handle such equations? I think you just write a function like `addition_is_associative :: x + (y + z) -&gt; (x + y) + z` and apply it when you need it. 
Neat! As someone still learning the basics, it inspired me to write something similar, so thanks :) Just FYI, the construct &gt; let randomWord = words' !! (randomNumber `mod` wordcount) is bad practice. Obviously it doesn't matter at all here, but it probably doesn't do what you're hoping (choose a word at random with uniform distribution). It will choose the earlier words with ever so slightly higher frequency, because the range of outputs from randomNumber is not divisible by the length of the array. (If you're having trouble visualizing it, imagine that (rand a b) chooses a random number from a to b, and then consider rolling a die by (d = (rand 0 10) `mod` 6)).)
For this to work, what exactly must be on your $PATH?
It's generally a bad idea to change storage engine during development. They behave differently and have different feature sets so you'll save yourself time and headache by picking one that meets your requirements and sticking with it during the life time of your application.
Met be haskforce for intellij will work for you -https://github.com/carymrobbins/intellij-haskforce
[Ahem](https://wiki.haskell.org/File:Haskell-logo-revolution.png)
To fill in the picture a little bit... `ghc-mod` in emacs (and Vim afaik) will immediately highlight errors and warnings, letting you jump between them, and let you do documentation lookup. The part that isn't so great is completion: dot-based intellisense-style completion is *much* nicer than anything I've seen for Haskell. The type checker feedback with `ghc-mod` is really a tremendous benefit -- more so than with Swift -- when you start using typed holes. This ends up playing something distantly related to dot-based completion in that you write the bits you know, and ask for help with the other parts.
Yep.
Please take a look at the documentation for the following functions: * [`groupBy`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:groupBy) * [`isAlpha`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-Char.html#v:isAlpha) * [`unwords`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#v:unwords) Can you figure out how to combine them to create your function?
I always highly recommend haskell-ide for Atom (+ghc-mod, of course). It works beautifully. That is for those who cannot stomach Emacs or VIM.
Thanks - I saw that last night but didn't have time to fix it. For some reason pandoc doesn't like plain apostrophes. I always have to escape them, which I often forget.
Same problem for me on Android Chrome browser. 
I completed the missing parts of the proof in this [gist](https://gist.github.com/argent0/a85b934a36cb1feec51f).
I may be missing something basic, but isn't totality checking impossible in general due to the halting problem?
Also check out [hdevtools](https://hackage.haskell.org/package/hdevtools). It has wrappers in vim and emacs. I used to use ghcmod but type checking took too long, maybe that has changed tho.
I think ghc-mod is equally fast now because it now also has a service called ghc-modi, which works just like hdevtools.
perhaps the slogan of anarchist functional programmers (lol) might be "freedom from ubiquitous (type) coercion". 
Yes, haskell-ide in Atom has become my preferred way to write Haskell these days. Solid offering.
I have cabal, stack, as well as ghc-mod, ghci-ng, hindent, hlint, and stylish-haskell on my $PATH.
The totality checker would be *conservative*: it would disallow programs that *do* halt.
Looks like computing the value in software is the way to go on modern hardware. Intel have a library called SVML that includes vectorized implementations of various trig functions. There are no trig instructions in SSE or the other vector extensions that have basically superceded x87 floats. Going off of my gut, I'd say that the table method hits cache issues and computing sin/cos can be done faster than a full fetch from DRAM. As always though, profiling is the only real way to know which way is faster. 
tell that to Bethesda for trademarking the word "Scrolls"
Nice! Looking really good now. :) You should probably be aware that other posts on your blog do not load, like this one: http://tojans.me/blog/2015/05/07/insanity--start-ups/ @vigilance$ telnet tojans.me 80 Trying 192.30.252.154... Connected to tojans.me. Escape character is '^]'. GET /blog/2015/05/07/insanity--start-ups HTTP/1.0 Host: tojans.me HTTP/1.1 404 Not Found Server: GitHub.com Date: Mon, 05 Oct 2015 06:20:50 GMT Content-Type: text/html; charset=utf-8 Content-Length: 0 Connection: close Access-Control-Allow-Origin: * Connection closed by foreign host.
Hi, thank you for chiming in so quickly :) Re your questions: * `Numerical.PETSc.Algebra` and the other "Algebra" files within `Raw/` are just notes-to-self, I should remove them from the repo for the time being. Once the lowest-level interface is a bit more stable, the higher-level representations of`Vec`s and `Mat`s should be endowed with some intuitive algebraic properties. * Regarding Cabal: haven't had the time to properly investigate it as the build and linking process is a bit involved. For the time being I'm more concerned with getting the binding off the ground * I'm almost positive there aren't any external dependencies, apart from `inline-c`. I'm using GHC 7.8.4 * Exception control: in `Raw/Exception.hs` there is some functionality for checking the integer error codes and throwing a corresponding exception (all raw calls are fed to either `chk0` or `chk1` which performs this function). All allocation/cleanup pairs are wrapped in a `bracket`, which re-throws if needed and cleans up after itself. So no `Either`s, just pure return or an exception, for now. * `Numerical.PETSc.Types` is a C2HS generated stub for compilation-dependent types only (e.g. `PetscScalar` can be either a Real or a Complex according to the PETSc configuration options). However this step is not part of the build yet (it should be performed as a first thing, before the inline-c step, but I can't get the two to play well together) 
Yes, politics open a way for mediocre people that otherwise would never promote. Specially in hard disciplines where mambo-jambo elaborations do not make computer do the job. I think that this symbol is good because it identifies why you are where you are: not for your skills and your contributions to the community, but because, you know... left-leaning gangsterism
I would recommend against that approach. Sqlite will allow you to do all kinds of things that are plain wrong which will make it hard to switch your SQL to e.g. PostgreSQL later. It is better to develop on a strict engine like PostgreSQL and later switch to a more lenient one like Sqlite.
This is awesome! ghc-mod plus HaRe plus a decent editor integration will make Haskell in the large a very pleasant experience!
EclipseFP does as well (and yes, the maintainer quit the project, but it's still fine for the moment and the code is out there for anyone to take over at any time).
That is better by far. And much more ingenious
I think [spacemacs](https://github.com/syl20bnr/spacemacs) is pretty cool! Gives you a lot more than just on the fly typechecking.
This is great /u/alan_zimm , thanks! Questions: 1. **GHC memoization of duplicated expression definitions?** If a definition is duplicated with `dupdef` and given a different variable name, does this make life harder for GHC to memoize the result of reducing the expression to WHNF or NF, if in the original form the definition was used via its variable name in multiple places in its scope? I'm not up to speed on whether GHC memoizes on-demand expressions based on an assigned variable name appearing in multiple places in its scope or the AST of the shared expression itself. 2. **Impacts to memory space by HaRe refactoring?** Would using the `demote`, `liftOneLevel` or `liftToTopLevel` have possible effects on the heap profile of the refactored code? For example, consider the application of `liftTopLevel` on one of two sub-expression's that both use a big data structure passed to their common parent function. Assuming the resulting top level function is not inlined, would the `liftTopLevel` refactoring potentially introduce additional memory costs due to the sharing of this big structure between the one remaining sub-expression and the new top level function? Conversely, is it possible to say anything about *reduced* space consumption when `demote` is used on two top level functions to be inlined so that they can instead share access to arguments from a common parent function, i.e. inling explicitly rather than using the INLINE pragma? 3. **Compatibility with hlint?** IIRC, /u/ndmitchell 's hlint has a suggestion to rewrite into if/then/else expressions the cases where a case expression has two patterns, one for `True` and one for `False`. This is the opposite of HaRe's `iftocase` refactoring. Is the use case for `iftocase` a stylistic choice, or something else? Something to perhaps bear in mind for IDE implementations of HaRe is that if these IDEs already support hlint, they may contradict one another's refactoring suggestions.
&gt; Does it mean that if I am a capitalist, jarcane won't accept my Github pull requests? What am I supposed to do now? fork this and say prayers? It means exactly that. These gangsters want it all for them. No institution without a postmarxist soviet on the top.
I think your document is UTF-8, but your headers or page source tells otherwise. If I manually switch encoding in Chrome to UTF-8 it seems to work. Putting this in the header of your HTML might help: &lt;meta charset="utf-8"&gt;
&gt; Have you tried the workarounds in that thread? Yes the workaround does work but it is really annoying (c-c c-k and then A) and I need to do it again as soon as I escape the edit state (if I forgot the c-o trick) Actually even `&lt;spc&gt;m s S` does not work if the focus is on the repl (it only works when the focus is on the buffer) 
&gt; What problems do you have exactly? The workaround does work but it is still very annoying. You fire up a repl, you need to `c-c c-k and then A`. Every time you enter this mysterious `stuck mode`, you need to repeat the operation. AFAIK the user has no clue why this is happening and why such a cumbersome workflow is needed. Last time I have tried, even `&lt;spc&gt;m s S` was not working when the focus is on the repl. This is a different problem I guess but it just adds up to the annoyance. This is a shame because when it does work, having an integrated repl is so nice indeed. 
same here
Just came here to say I'm excited that something I wrote inspired you! Your library is very cool and this is exactly the type of stuff I love to do with Haskell. Glad you're having a good time with it!
:)
Something fun you can do is actually make a `num` instance from your `Expr` type: data Expr = X | Const Double | (:%) Expr | Expr :+ Expr | Expr :- Expr | Expr :* Expr | Expr :/ Expr | Expr :^ Expr | Apply UnaryFunc Expr deriving Eq instance Num Expr where (+) = (:+) (-) = (:-) fromInteger = Const . fromInteger negate X = (:%) X negate (Const d) = Const (negate d) negate e = -- TODO abs = -- TODO signum = -- TODO Once you have these functions implemented, you can actually do something like this: &gt; sin = Apply ufsin &gt; cot = Apply ufcot &gt; derivative (sin (2+x) cot x) And get a real answer.
Disclaimer: I know nothing Why do we need to prove facts like these? If types can depend on values anyways, why not just wait till you know the concrete x, y and z and then compare the result for equality as we do at the value level?
I have found this to be a worthwhile read for real world programs http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html. You can often make an entirely pure program and then just link the little IO bits at the end with an IO backend. The pure part ends up highly testable and verifiable and the IO is isolated and less likely interact in bad ways with each other. In the real world small programs grow gradually and if you stay in IO the whole time, it will become painful eventually.
FYI this slide deck [1] tells us that effect handlers unwrap the stack, so they are slow; is this still correct? [1] http://users.ugent.be/~skeuchel/talks/efficienthandlers.pdf
I wish the example in the description had top level type annotations. I need the types to understand what's going on! 
effect-handlers author here. Disclaimer: this library happened as a research project during my masters. It's here to explore the design space of algebraic effects. But it has very little to offer since the design converged very close to the extensible effects library. This concrete implementation of effect handler has performance problems. There is a big cost of using handlers since every handler will traverse the computation tree. I'm working on improvements again inspired by extensional effects by Kiselyov et all. There is also the conceptual problem with effect handlers. Neither handlers nor transformers subsume the other. Transformers let you implement more effects (like lazy state) where algebraic effects always commute. This is great where your effects actually commute (you avoid the n^2 instances problem) but it's also bad because it forces your effects to commute even if they shouldn't. There is also the problem of usability. To use my library you need quite some type annotations because GHC cannot infer (it would have to guess) the correct types. 
well, some values that show up in types don't exist until runtime. e.g. read two numbers from standard input, create a vector with the number as its length for each number, append the vectors, then index them with one of the lengths. that's type safe, but only with a law for the naturals that `x ≤ x + y`. 
Still need to know the type to understand what it's doing as a mere human just reading the code. 
Could you give us an example of such an exercise?
Thank you!
In that case it may be time to switch back. It means a less fractured ecosystem and also one less lib requiring my maintenance. The `hdevtools` author has been MIA for a couple years and I've been (poorly) maintaining it in his place.
FYI, I did the benchmark. In my case, using precomputed values results in a 6% performance gain on an Intel Core i7 processor. So it does in fact make a material difference. 
This one : Definea function to convert small letters to capitals which returns unchanged characters which are not small letters. What about characters like 9 or a . Another example : these two looks like the same : Write a function numberNDroots : : Float -&gt; Float -&gt; Float -&gt; Integer that given the coefficientsof the quadratic, a, b and c, will return how many roots the equationhas.You may assume that the equation is non-degenerate. 3.23 Using youranswerto the last question, write a function numberRoots :: Float -&gt; Float -&gt; Float -&gt; Integer that given the coefficients of the quadratic, a, b and c, will return how many roots the equation has. In the casethat the equation has every number a root you should return the result3. And this one : The definitionshould be consistent with what we said in explaining how middleNumber works. You also need to think carefully about the different ways that one numbercanlie between two others. You might find it useful to definea function weakAscendingOrder : : Integer -&gt; Integer -&gt; Integer -&gt; Bool so that weakAscendingOrder m n p is True exactly when m, n and p are in weak ascending order, that is the sequence does not go down at any point. An example of such a sequence is 2 3 3. are 2 3 3 in a weak ascending order. I do not think so. 2 3 4 are good. 
I'm unfamiliar with PETSc, but with bindings, I tend to prefer returning Left over throwing exceptions.
Another example of effects not commuting is `State` and `Error`. This code does different things depending on their order: (put 1 &gt;&gt; throwError "Anything") `catchError` (\_ -&gt; return ()) For one ordering of `State` and `Error` the `put`'s effect is rewound by the `catchError` and for another ordering it's not. Also any effect system needs to have a clear story for handling more complex effects like the `pipes` library. For example, what would be the type signature of the `(&gt;-&gt;)` operator under the effect handler system?
what are some of those things? just curious, I've only used MySQL.
Thank you! 
flycheck is the most lightweight option that "just works" if you're okay with using Emacs. Otherwise, ghc-mod and ${someeditor}, usually atom if you don't know emacs/vim.
From http://hackage.haskell.org/package/extensible-effects : Effects can be added, removed, and interwoven without changes to code not dealing with those effects. Also: the occasional lift etc is confusing for beginners like me. If I understood it correctly you don't need lifting when you are using effects? (src: http://okmij.org/ftp/Haskell/extensible/#MTL-drawbacks )
is there web freamework that doesnt requre TH?
I don't like the USSR-esque flag either. but while I enjoy this subreddit being relatively apolitical, I like the idea of a place for people with who care about (1) functional programming and (2) leftism. although, I think a more general "leftists who program" might be a better fit. when we work for investment banks and create death robots, our actions have "political and social" effects whether we want to or not. even if you think centralized banks promote innovation and death robots protect citizens, that is a political effect, positive or negative.
left-leaning gangsterists are, in fact, the worst leftists
If I understand you well, you are having problems interpreting the exercises. Let us take a look at an instance. &gt; Define a function to convert small letters to capitals which returns unchanged characters which are not small letters. Well, I guess you know the capital version of `a` is `A`. What is the capital version of `9`? It doesn't exist! Hence, just letters of the alphabet (let us assume roman alphabetic, for simplicity) can be capitalized. Basically what the exercise is asking you is to transform `a, b, c, ...` into `A, B, C, ...`. Got it? The others are basically math. If you don't remember the concepts, Wikipedia and MathOverflow may help you. About the weak ascending, it is the about `a ≤ b ≤ c` versus `a &lt; b &lt; c`. Since, `2 ≤ 3 ≤ 3` it is a weak ascending order.
You are suggesting that The Left controls everything?
oke, so in your oponion the middle number of 2 3 3 is 3 because its the number between the 2 and the second 3. it is because it's a weak ascending order that a &lt;= b &lt;= c and ascending order is a &lt; b &lt; c ? 
Nice! For another option, if you can jailbreak your e-reader, check out [KOReader](https://github.com/koreader/koreader). It has great PDF support (alongside ePub and other formats), and can, for example, split columns, reflow text, adjust contrast, and jump back and forth between split and two-column view on the fly. Works great for reading papers on my Kindle PW2. Also available on Android.
yes, but not sqrt
I like about Idris that it forces you to give the signature 
I'm going to keep my eye on this. My bachelor thesis topic is "Implementing an efficient library for handling Algebraic Effect Handlers". Still a lot of reading to do first
Update: Version 0.8.1 is out, which fixes renaming that crosses module boundaries and preserves CPP directives
Maybe you misunderstood the exercise. Let us rephrase it. &gt; Given three numbers, tell me if the sequence is in weak ascending order. &gt; The function that tells you if the sequence is in weak ascending order should have the following signature: &gt; &gt; `weakAscendingOrder : : Integer -&gt; Integer -&gt; Integer -&gt; Bool` So let us see what is expected. - `weakAsceningOrder 1 2 3` should return `True`, because `1 ≤ 2 ≤ 3`; - `weakAsceningOrder 1 1 2` should return `True`, because `1 ≤ 1 ≤ 2`; - `weakAsceningOrder 1 1 1` should return `True`, because `1 ≤ 1 ≤ 1`; - `weakAsceningOrder 1 3 2` should return `False`, because `1 ≤ 3 &gt; 2`; - `weakAsceningOrder 2 1 3` should return `False`, because `2 &gt; 1 ≤ 3`; - `weakAsceningOrder 3 1 2` should return `False`, because `3 &gt; 2 &gt; 1`; - etc... Have you gotten what is expected from the exercise, now? Just to define a function that when I call it, it will behave as the examples above.
Thanks, I did understand it but want to be sure I understand the difference between weakAscendingOrder and what AscendingOrder is. I will work on this exercise so it will behave this way 
It also uses out of date imports. Here is an annotated version {-#LANGUAGE DataKinds #-} import Control.Effects.Eff import Control.Effects.Reader import Control.Effects.Exception program :: Eff '[Reader Integer, Exception String] Integer program = do v &lt;- ask if v &lt; 15 then throw $ show v else return (v+1) run :: Integer -&gt; Eff '[Reader Integer, Exception String] Integer -&gt; Either String Integer run n = runPure . handle exceptionHandler . handle (readerHandler n) res :: Integer -&gt; Either String Integer res n = run n program The signature of `program` means that the computation of the hoped for integer involves both Reader and Exception effects. `run` handles these effects. 
&gt; (I heard he comes if you dance at night in the rain, wearing only a hat) I might test that, if it ever rains again in California
I am also still getting the error.
Nice, now I can see it's a type indexed by a type level list of effects, which goes a long way to helping understand how it works :) 
Correct.
Since those days ghc-mod has also gained quite a lot of environment support that hdevtools is completely lacking (AFAIK). It tries to keep cabal/Stack configuration up to date even when people fiddle with their cabal files while it's running, doesn't explode when people upgrade Cabal/cabal-install, it doesn't just guess the ghc flags based on some heuristics or whatever it actually uses pretty much exactly the flags that `cabal repl` would use for any given component so it works in many more cases now and of course to do that it has to understand Cabal components now and actually figures out which component any given source file is actually used in on it's own. So come join us, we can build one really versatile tool instead of having a billion tools that cover only 10% of use cases each ;)
Hear hear!
Function names must be lower case: f s = zip s (tail s++[last s]) &gt;&gt;= g where g (s, x) | isAlpha s &amp;&amp; isSymbol x || isAlpha x &amp;&amp; isSymbol s = [s,' '] | otherwise = [s] 
&gt; it is because it's a weak ascending order that a &lt;= b &lt;= c and ascending order is a &lt; b &lt; c ? Exactly. I would call the second case *strict ascending order*.
if you have time to do it for this month's meetup, that would be amazing!
It's also really important to clarify in these discussions when one means the `transformers` library or the `mtl` library when one says "monad transformers".
The paper usually cited when explaining why continuations don't fall into the category of "algebraic effects" is this: http://homepages.inf.ed.ac.uk/gdp/publications/comb_cont_journal.pdf A sloppy but intuitive way to describe this is "algebraic effects operate _algebraically_, which is to say, subject to purely local rewrites, or, alternately, in such a way that the interpretation of an expression can be given as an operator on the interpretation of its subexpressions." Continuations are nonlocal, and can cause expressions to need global knowledge for the purposes of interpretation. Another, even more handwavy way to think of this, is that handlers of algebraic effects are _given_ via continuations. So now, analogously to what happens when one tries to combine continuations with exceptions in scheme, we run into the age-old conflict -- two mechanisms that both affect global control flow exist at once, and have no particularly "good" way to not step on one another's toes. (There _are_ it turns out, ways to combine these ideas -- and the above paper describes one approach to this -- but they all end sort of end up picking a particular ad-hoc way to "glue" the continuation monad onto an entire effect stack at once, and no way iirc ends up being satisfactory for all cases). The recent "effect handlers in scope" paper tackles a related place where effect handlers fall down: http://www.cs.ox.ac.uk/people/nicolas.wu/papers/Scope.pdf But by the end, you'll see, they've effectively added enough machinery back to the "effect algebra" that they're working directly with a monad again.
Interesting, thanks! I'll read these.
I'm new to Haskell, but I'm also looking for a SQL Lib. I'm mainly working with Oracle and SQL Server and I thought that HDBC could be the solution. What are the reasons against HDBC? What Libs can I use instead?
just my 2 cents but I find the whole "build a monad transformer stack" hugely confusing, specially due to how the order of the monads matters. For example, I kind of like how in ocaml you don't need monads to do soemething equivalent ot the Writer monad. Just create a mutable queue in scope and you are set.
Please forgive me posting again, but I've completely redesigned the API and simplified my idea. I accidentally clobbered the old post though :/
This made a substantial difference. I think this was forgotten in my implementation. Thanks for your suggestion.
In `mtl`/`transformers`, one ordering just doesn't have a new state at all, and one has the updated state. There's no unwinding of state - so that sounds better.
1. You are comparing apples and oranges. The Reader effect (i.e., the Reader operation syntax) corresponds to the MonadReader type class, not to the ReaderT transformer. The ReaderT transformer corresponds to a particular Reader handler. Obviously the implementation of the hander gives you just as much guarantees as the implementation of ReaderT. In fact, for any Reader handler I can write a monad transformer that instantiates MonadReader. 2. This has got nothing to do with correctness, but possible limitations. Also, for the same reason as above, it is not a valid point. For any effect handler, I can give you a corresponding monad transformer. 3. True. Laziness is not preserved in all compositions of monad transformers. This is not compatible with effect handlers, that are all about compositionality. 
In most cases you should prefer `Either` to exceptions. The problem is that sometimes we can't choose `Either` (e.g. asynchronous exceptions). If you want to handle `Either`-encoded exceptions a la Java, check out my [Ether](https://int-index.github.io/ether/) library.
Same :(
Is that non-commutativity a bad thing? It's perfectly intuitive to me. Complaining that effect composition doesn't commute seems like complaining that matrix multiplication doesn't commute. It just doesn't, and that's how it should be, isn't it? What am I missing?
Glad I could help. Did the substantial difference involve fully eliminating Inf and NaN and is it converging reliably with zeros in increasing order of magnitude? If so, you should be getting as much accuracy in the zeros as can be expected based on whatever their conditioning might be. 
Why can't I view chapter 3-5 of the [Gitbook](https://begriffs.gitbooks.io/postgrest/content/)?
looks good to me but what if two numbers are the same 
What is this supposed to do, in the first place?
I wouldn't worry about exhaustive testing. Unit testing is basically an automated way of catching common mistakes, not the really obscure ones. While unit tests are good my litmus test is basically "Will testing X situation lead to something new?" and "If this test breaks, will some other test break as well?" In your case 2 19 2 is covered by x x+1 x, so it doesn't add anything new. Shuffling the x+1 around as the first and third could potentially reveal new failures, so I would write those.
At second look it's not allright. You are only saying that the function is returning something and not comparing it to something. I thought a good quickCheck sentence looks like this : prop_test a = test1 a == test2 a 
I don't think STM would be possible without using lower-level C bindings to atomic locking primitives.
If "entirely in haskell" is defined the same way as "entirely in go" is (not entirely in go at all), then of course. It is worth noting that this mistake makes go a real pain in the ass, it is horribly unportable because it directly calls syscalls instead of using the standard library like it is supposed to.
&gt;It is worth noting that this mistake makes go a real pain in the ass, it is horribly unportable because it directly calls syscalls instead of using the standard library like it is supposed to. For what it's worth, I've had a much easier time building Go programs and its compiler for Android than I have with GHC on Android, although this may change with the next GHC release bundling LLVM. *Edit: On reflection, I think I found Go code easier to port because of the convention of having platform-specific code in separate files like myModule_x86, myModule_arm with the compiler automatically picking which one to use based on the target arch, rather than because of anything about the runtime. It seems to promote a cleaner approach than using cpp and ifdefs.
I don't actually know. I'm just speculating.
I can't access it as well. I am not the author and i am do not know the Gitbooks platform. Maybe the book is still not finished, or maybe those chapters are available only to premium users
Right, but effect handler libraries treat effects as though they do commute.
Relevant: **Lightweight Concurrency in GHC** [on trac](https://ghc.haskell.org/trac/ghc/wiki/LightweightConcurrency): &gt; This page documents the effort to move GHC's concurrency support from its current location in the C part of the runtime system (RTS) to Haskell. This works builds on Peng Li's earlier work (​http://community.haskell.org/~simonmar/papers/conc-substrate.pdf). Here's an illustration of the idea: https://ghc.haskell.org/trac/ghc/attachment/wiki/LightweightConcurrency/GHC_LWC_Key.jpg The idea being that the LWT scheduler, MVars, and STMs be implemented with Haskell. The bulk of this wiki page was written 2 years ago, and I don't know what the ongoing efforts are in this direction.
With the FFI to call libc functions... maybe? If you allow "and a little assembly" then obviously yes.
Now what happens when you mix writer with continuations in ocaml? Does the queue rewind on the continuation resetting? Of course not, because it is mutable. In the mtl-style you have the _choice_ as to what interaction occurs. Note that this is a general principle of effects -- some commute, some don't. Transformer stacks don't cause this -- they just model it more granularly. Effect systems in Haskell also don't escape this choice -- you just choose the interactions directly when providing the handlers.
Well for what it's worth - and I'm not against doing some work in Haskell, Haskell probably isn't the best suited to doing what needs to be done in a very highly optimized way. The reason it was done in C is because it's a language runtime, and every drop of efficiency counts. I dislike the unsafety and failures of C just as much as anybody though, but there wasn't an alternative like there is today. Systems programming has to have a language that is tailor made to it - so why not do what can be done for the runtime in haskell, but have the smallest and most efficiency critical components done in rust or something safe that allows just as much control? We're talking about a systems engineering job here; each language is suited to a type of task.
Hey, all. I've been completely enamored with Haskell (and Idris) for a while now, and only just recently began trying to take the step of making tangible contributions worthy of Hackage. Even though its been an awesome, thrilling journey, I feel like I've underutilized this great community. I've soaked up a lot of knowledge and wisdom, but never really had the balls to straight up ask for feedback on my code. So now I'm asking. If you have any constructive feedback, I'd love to hear it, either on this, or my first complete Haskell *program* [which is here](https://github.com/BlackBrane/svocab). Also I'd be interested to know that it works on windows or other UNIXy systems besides OSX, since ANSI seems like the kind of thing that may be finicky that way. Hopefully this is just the beginning and I can contribute some more ambitious stuff from here... Thanks.
I cloned the repo and built it using Stack(Windows 8). I tried running your sample code and at first there was an invalid character error from hPutChar. After I set code page to 65001, it worked. But it seems that it still didn't show the correct characters. [Here is a screen shot](http://pasteboard.co/1aXUY6V4.png) of the session. 
/r/haskellquestions, next time. f = unwords . groupBy ((==) `on` isAlpha) &lt;=&lt; words OR f input = spaced where spaced = unwords myTokens myTokens = concatMap tokens origTokens tokens = groupBy sameAlpha sameAlpha c d = isAlpha c == isAlpha d origTokens = words input `words`, `unwords`, `groupBy`, and `concatMap` may need to be imported from `Data.List`. `isAlpha` may need to be imported from `Data.Char`.
I would suggest showing something graphical using [gloss](https://hackage.haskell.org/package/gloss). There are several neat examples in the [gloss-examples](https://hackage.haskell.org/package/gloss-examples) package. I've used it in a 1st year, 2nd semester FP course to develop an Asteroids like game in a couple of demonstration classes. The code is available from my web page: [gloss-asteroids.hs](http://www.dcc.fc.up.pt/~pbv/aulas/pf/folhas/gloss-asteroids.hs). 
&gt; Getting it to perform as well as the one in the GHC runtime, now that's the hard part. Quite. Another attempt to lift things from the RTS is monad-par, which provides a fork/join style API for explicit parallelism and has an underlying load balancing scheduler written entirely in Haskell. This contrasts with the Eval monad, which encapsulates `rpar` for speculative parallelism, for which there is special support in the RTS.
oh, nice!! I'll try this out later :)
well, you HAVE to let them hear the Word from SPJ
Using the FFI is the normal way to get low-level code into a Haskell executable.
This is so cool. As a data scientist I often just want a dead simple graph, have always wanted to do basic stuff in haskell. Will try running it after work today!
Parallel quicksort isn't exactly Haskell's strength. How about symbolic differentiation and simplification, or parser combinators?
I'd love to see the RTS ported to a safer language (rust being the new hotness these days), but Haskell isn't the right choice. Haskell is a fantastic general purpose language. Haskell is somewhere between "meh" and "no thanks" for low level systems programming. An RTS doesn't need to be an instance of low level systems programming, but if you want absolute control over machine representation in order to optimize the RTS, then it suddenly becomes important.
Sweet, I hope you like it and it works well for you. :) Let me know how it turns out, or if you have any suggestions.
I would think that given what you have said C is the only real choice. C++ doesn't provide any safety and doesn't have a stable ABI (yet) so isn't a choice and no other systems language has the mind share that C has. While rust might improve safety it will also tie maintenance to another seemingly arbitrary language.
I'm not sure there is any way to get it to work, see https://ghc.haskell.org/trac/ghc/ticket/4471.
Well C is what it already is written in largely, so really the only place worth discussing is if this is going to change. Obviously we want better safer runtimes, and rust can give you that. C++ provides whatever safety users are willing to bend over backwards for through the template system, but I don't agree with you for not really considering that a solution because it's too damn over-complicated and obstreperous to deal with.
I thought C++14 had a stable ABI?
If you're serious about polynomial curve fitting, check out the existing literature. E.g. https://en.wikipedia.org/wiki/Remez_algorithm https://en.wikipedia.org/wiki/Minimax_approximation_algorithm
Com comentarios em Portugues! I've added the game to my collection of examples. Since have a ton of students working with games, it may be useful in the future.
It's about the association of the binds. (((mx &gt;&gt;= f) &gt;&gt;= g) &gt;&gt;= h) Will tag-compare against Left 3 times (once for each bind) ContEitherT associates the binds correctly. This is essentially the same problem that codensity solves, but on both right and left.
Haskell only has FFI to C so it is a moot point till that huge process is done. Also to get the stable ABI you have to fix quite a few compilation details. It seems that for now C facades are still the easiest way to FFI C++.
From my minor systems programming experience rewrites to new languages take 5 years to provide any tangible benefits. I don't know if it is worth it compared to simply reducing the work that the C portions have to do as GHC is better able to create performant versions of portions of the runtime.
Parsec is a really cool thing to show. 
Simon Peyton Jones. You might have used one of his programs, it's called GHC (although there are many others who maintain it too \^\^). See https://www.youtube.com/watch?v=6COvD8oynmI or https://www.youtube.com/watch?v=jLj1QV11o9g or several other videos.
Haha, I'm too lazy and I'm not willing to put a lot of effort in rehearsal. Also I'm afraid of cabal exploding randomly. 
There are many students working with OpenCV. Sadly haskell is a bit lacking for OpenCV bindings. =/
The paper mentions in **3.2 Primitive transactional memory (PTM)**: &gt; But if `rq` *is a thunk, the evaluation of `(null rq)` might take an arbitrarily long time,* so the lock `lk` might be held for a long time. That does not threaten correctness, but it does mean that all the other HECs might be held up waiting on `lk`! One could declare that the programmer should somehow ensure that this never happens, but it is far from easy for a programmer to be certain that a blob of code evaluates no thunks. Sounds like the [unlifted data types](https://ghc.haskell.org/trac/ghc/wiki/UnliftedDataTypes) proposal solves that, it might lead to a different solution without the “non-trivial performance losses” mentioned by /u/aseipp
Here's a decent intro to the issues: http://garrett.damore.org/2015/09/on-go-portability-and-system-interfaces.html 
Regarding ghc and android, this project might interest you: https://github.com/wavewave/nix-build-ghc-android
I've used some of the CV bindings and agree completely. That's why I switched to Friday.
&gt; For what it's worth, I've had a much easier time building Go programs and its compiler for Android than I have with GHC on Android, although this may change with the next GHC release bundling LLVM. That is mostly due to the fact that the Go compiler has vastly more people working on it and that it is a vastly simpler compiler. 
Told you ;)
Nothing changes. The C runtime is compiled with gcc to native code. Haskell modules are compiled to native code (optionally via llvm). The resulting two object files are linked together.
awesome. will follow!
Whilst I'd use (GHC) Haskell over C for (almost) any project if I'd have the choice, saying "C has crappy debugging tools" is a bit of a stretch, especially compared to (GHC) Haskell's debugging tools, IMHO... I know things are improving with the recent DWARF work, and I certainly applaud these efforts and truly hope they'll be continued, today debugging production-system issues including mysterious 'random' crashes of C code is IMHO easier than debugging some Haskell program exposing the same symptoms (although maybe Haskell limits the chance of encountering random crashes, but that's a different story).
Speaking of laziness, it's probably worth touching on that, if only briefly. I'm a fan of the Fibonacci example for a basic intro: λ. let fibs = 1 : 1 : zipWith (+) fibs (tail fibs) λ. take 6 fibs [1, 1, 2, 3, 5, 8]
Some previous work was combinatorrent https://github.com/jlouis/combinatorrent which jlouis wrote in both Haskell and Erlang https://github.com/jlouis/etorrent, and IIRC pretty much concluded Erlang was overall easier because it had better performance, in terms of fewer memory leaks, I think, and better concurrency primitives. It's rare someone writes the same non-trivial program in two languages like that. Science. I'm interested to see where this one goes. +1
Good point. I mean compared to something like Java, honestly. Not compared to Haskell, and even with all the hate. You have tools like valgrind/ASAN and compiler-rt. Nope, not supported on every platform (Windows is in progress but who knows what actually works), nor every compiler (Clang/GCC only, again fails the Windows test). Valgrind barely even works on OS X. Visual studio? Great tool, not cross platform. Xcode in the Visual Studio bin. GDB? Be prepared to write a shitload of Python to make advanced usage tolerable, and even then it's questionable on Windows. And won't work as well with microsoft compiled binaries, because it doesn't understand the debugging format. You can probably fix some of these things. But there is no fundamental reason it needs to be so, and the reality is I can go download Eclipse with a debugger or VisualVM on any platform, point it at a port number and a 'binary (.jar file) and I'm done. It doesn't even need to be as powerful as any individual tool; I just don't want to learn a dozen different ones for simple things. Hell, by some standard, our _current_ profiling tools are in some way better than C ones because *they actually work on different platforms with a consistent interface*. We have poor debugging tools now, don't get me wrong. But C's debugging situation is awful for many reasons, including but not limited to the fact I have to know like 30 damn individual tools to do basic things like "How do I get a profile graph", which should really not be where my time is going because I want to generate a graph on Windows instead of Linux.
Right, if you take multiple platforms into account. I myself don't consider any non-trivial C code as 'portable' though (at least between Linux/*BSD'ish, Windows, mobile stuff,...), so if the target platform is any of those, your toolset is relatively well-defined, and not insufficient.
This is great. Developed examples of code that avoid the following unholy trinity of tutorial examples and a crazy mind shagging set of types while solving real 'real world' problems are extremely useful to me: 1. Any type of sorting - quicksort is especially despised 2. An implementation of factorial 3. Parsing a 'simple imperative language' Thank you.
Thank you for the feedback! I think you'll like the next post too. I started out with a pragmatic IO approach and then refactored it with just a few abstractions to get the most out of them without overdoing it.
Here's the post to go with it: http://jlouisramblings.blogspot.com/2010/04/haskell-vs-erlang-for-bittorent-clients.html
Use the stack tool which runs much less of a risk of exploding on you. I actually haven't gotten it to fail, yet.
I don't think I can comment as I know nothing about ATS. Using dependent types where performance is critical, it seems like you'd want to have good type erasure or good proof irrelevance. I don't know how the different dependently typed systems compare, but I suspect Idris is actually in the lead in this department.
I think that `lens`-related stuff gets you there: modify (\s -&gt; s { queue = newQ }) is modify (set queue newQ) or, with an operator, modify (queue ~. newQ) Or, given that there are MonadState lenses, | Set.notMember vec vis -&gt; do queue .= newQ visited .= Set.insert vec vis forM_ branches $ \b -&gt; do oldQueue &lt;- use queue queue .= Queue.insert b (getDist b + cnt `div` 2) oldQueue search (( take with grain of salt as I don't know the lens library all that well ))
Which is, I grant you, *every* nontrivial userland application. But it is undeniably nice not to assume such dependencies by default—at the very least, since you don’t necessarily pay for what you don’t use. If you really need or want to code on bare metal, it’s much easier when no runtime is assumed. Even an MMU is more than many real-world control and sensing applications have to work with. Moreover, they currently face a dearth of language choices, none great—the choices are basically C, Forth, and assembly. While it takes a great deal of engineering effort, it’s a noble goal to bring contemporary software development to the full stack, and we begin by ridding ourselves of platform-specific assumptions.
It exploded with me a few times already.
The above is memorized though
Have you seen [automatic differentiation](http://conal.net/blog/posts/beautiful-differentiation)? It's incredible.
I can stomach vim very much, but nevertheless I've switched to Atom for Haskell code.
thanks for the explanation.
Sure, but this is really about showing off how laziness enables self-referential definition and easy handling of infinite lists. Being able to write a definition like this is fantastic for prototyping even if it needs to be optimized into something less elegant later. (Which it may not need, if it's never a bottleneck. But of course, [it can still be done](https://wiki.haskell.org/The_Fibonacci_sequence#Fastest_Fib_in_the_West).)
&gt; till The JVM JIT implementation you might be referring to is MateVM by lewurm. In fact i implemented most of the generational GC for MateVM (the GC is crappy but fun). Your point is totally valid, with C you can squeeze out the maximum of performance. Still, from my experience, performance prediction is, at least at early stages of such a project a much harder problem. In C you need to be awesome in order to get good performance. By contrast, when implementing a RTS in haskell you need to be superhuman in order to get good and predictable performance. [1] http://wien.tomnetworks.com/uni/2013_pppj_implementing_a_java_jit_compiler_in_haskell.pdf
Exactly. One of the best blog posts I've read in in the last weeks. It has the right length, too. Looking forward to the next one in the series.
Hmm, that does appear to be a consequence of this, although it wasn't exactly how I conceived the issue to begin with. So to me that counts as evidence that there's something to investigate here.
Awesome article; This might be a bit extreme, but as a DDD practitioner I'd aim for optimal readability: for your `PWP` definition I'd declare type aliases for `PieceId`, `PieceOffset` and `PieceLength` which alias `Word32`, and I'd do the same for `PieceData` and `PieceBitfield`.
Which effects library would you recommend? Do you know if Brady's library still has the problem that "the composition of other effects with non-determinism is not well-behaved" (quote from your paper)?
thanks for the link. I wonder what changed in the last 5 years since then :)
Yes, I'm afraid so :-) I often resort to a "bilingual" approach when programming, particularly for 1st year student: program entities in English and comments in Portuguese... 
Only a small CLI one for now. Gtk2hs lacks some things I wanted to use so I'm looking into the GObject-Introspection based binding currently.
Please don't. Haskell should be written by Haskell conventions, not how people who don't know the language expect it. What would a C# programmer say if a C programmer asked them to stop using classes so they could understand the code easier?
Thanks. Re: indentation. Atom's support is equally good as Sublime's. Just select a few lines and use &lt;tab&gt; or &lt;shift-tab&gt; to indent/dedent. I am completely addicted to &lt;ctrl-d&gt; for quickly editing a few similar places; &lt;ctrl+up&gt; or &lt;ctrl+down&gt; to move a line or a bunch of lines up/down. Also things like cloning a cursor with &lt;ctrl+click&gt;. Atom's text editing support is fantastic. I am not sure what is the great editing feature missing from VIM. I really liked how Sublime REPL package was set up - it made adding new REPLs easy. For atom, I found * [repl-term](https://atom.io/packages/repl-term) but MacOS only. * [repl](https://atom.io/packages/repl) but no GHCi support - we'd have to contribute it ourselves. But it's a good start.
I don't understand how MMUs got brought into this. Nobody is doubting that it's useful to remove unwanted stuff on embedded devices. But we're not talking about tiny embedded MMU-less devices; we're talking about applications deployed to standard Linux systems that everyone works with every day, written in garbage collected languages like Go and Haskell. You're not going to find this on MMU-less devices. In reality, a `libc` is practically unavoidable in these environments. That's all I was getting at. I do admit for a lower-level language like Go that produces static binaries *if* you stick to pure Go, it's a nice advantage for the cost, and makes pure-Go cross compilation trivial. I can understand it might be worth it for that, especially if you have a lot of your stuff written in pure Go. In practice I think we rely on C code more often. And besides, even *if* I'm dealing with 'bare metal' devices, in some cases a libc implementation is *pretty common* or even desireable (uClibc), and if you expand that to literally just include "no operating system" - a `libc` is exactly what tools like HalVM or Mirage are built on, because it serves as an OK common codebase to assume exists (sure, there's no `pthread_create`, but do you *really* want to reimplement `strstr` or `atoi`?)
Its posts like these that make me question the decision to pursue grad school...
Thanks, I only figured that out after watching the galois videos last night!
or go fullhaskell and newtype them. I've found that the boilerplate of packing and unpacking that comes from newtypijng things pays itself back quite soon upon refactoring.
I think your point is a good one (stick to conventional Haskell) but in this case not applicable (no pun intended), because there isn't a standard convention for this. Actually I see code like that in the wild and by colleagues all the time. Many Haskellers prefer Applicative for parsers over do syntax with intermediate names. I don't, but many `do`. See [this reply](https://www.reddit.com/r/haskell/comments/3nr24c/writing_a_bittorrent_client_in_haskell_1/cvrbgtr) for example.
I'm sad to think of the Haskell Centre retired, but I happily accept ide-backend and stack instead, bringing the it-just-works experience to my desktop instead. 
I can see a possible reason being that the existing signature affords fewer mistaken assumptions and requires less documentation to describe its behavior. In other words, if your action returns a meaningful value, you have to explicitly toss it, instead of implicitly letting when/unless toss it.
Are there any plans to open source the FP Haskell Center?
If you update system libraries, it simply breaks. Right now, when ncurses bumped to version 6, it stopped working on some of my installs.
And with `-XDataKinds` you can ghci&gt; type Hodor = '() ghci&gt; :k (Hodor :: ()) ghci&gt; (Hodor :: ()) :: () 
I'm not sure I follow. Isn't it obvious by the last `f ()` that `Bool -&gt; f a -&gt; f ()` throws away the value?
Current repository and bugs list: https://github.com/haskell/network Would be useful (and extremely helpful) for anyone who wants to help to triage some issues, even if they're not keen or sure about being a maintainer! A few with patches to be made into PRs, a few labels here and there...
so, ignoring the first argument, I can think of two possible implementations of `Applicative f =&gt; f a -&gt; f ()`, whenFalse _ = pure () and, for some `f :: a -&gt; ()` whenTrue' xs = f `fmap` xs but, since `()` is a terminal object, then every `f :: a -&gt; ()` is the same, and so all such maps may as well be whenTrue xs = const () `fmap` xs Now, when this function is applied at `a ~ ()`, we also have that `const () ~ id`, and, by the functor laws, `fmap id xs ~ xs`. We would prefer in that case to replace the unneeded fmap, for performance reasons; the repeated applications of `fmap (const ())` may not be free of cost. This might make sense when multiple uses of `when`/`unless` are composed together: pButNotQ = when p . unless q The polymorphic versions would have to repeatedly `fmap (const ())`, but the monomorphic versions can just pass their argument through.
... &gt;&gt;= flip setInnerHTML (Just "Hello")
[Maybe related to this](https://github.com/faylang/fay/issues/423)?
That -N thing sounds like a big deal to me. Is that a known bug? 
But IDE backend is a set of APIs, and stack is a build tool. The IDE interface and all the wonderful stuff for interaction with git and for doing sharing and collaborative work will go away. Could we know the reasons? Obviously it is a costly service offered for free and this is at odds with sustainability. Isn't an option to pay for it?
Wouldn't `Default` be a more appropriate constraint for that kind of use case? Or even specifying the default value directly in the call?
Default isn't in base like Monoid is.
Really great to hear things are going well guys. As a commercial user of Haskell your contributions to the ecosystem are invaluable.
Vim's editing is built up using a concise grammar. "Just select a few lines" is the trick, you see - in vim, once I know how to indent a line, I know how to indent a paragraph, or from here to the matching brace, or from here to the mark I set ten minutes ago, or from some particular line number to another, or up to a line matching a pattern - in a few keystrokes. I use vim to do some text transformation tasks rather than write a short script quite often, because it's so concise to express your editing intent. repl-term doesn't currently support Haskell, but I'm taking a look at [hydrogen](https://atom.io/packages/hydrogen) now. Not quite a REPL, but interesting nevertheless.
To spell it out for other people, `()` implements the `Monoid` interface, so the following type signature: Applicative f =&gt; Bool -&gt; f () -&gt; f () ... is a special-case of the more general `Monoid` type signature
Some don't like `Default` because it feels a bit, well, icky. What does it mean for a value to be a "default"? It only really makes sense for things like configuration, not more general values. What would the `Default` instance for `Int` be? It depends entirely on what you want to do with it, which is what `Monoid` captures.
&gt; concluded Erlang was overall easier because it had better performance, in terms of fewer memory leaks, I think, and better concurrency primitives I just read the article, and that's not the impression I get. :) The author's conclusion is: &gt; There is no clear winner in the duel. I prefer Haskell, but I am biased and believe in static typing. Yet I like programming in Erlang - both languages are good from different perspectives As for performance, the author does indeed say that laziness "weighs against using Haskell". However, "the performance of the two clients is roughly equal, but more time was spent at optimizing the Haskell code". And that "Haskell gives you faster execution, but Erlang was more than adequate for a bittorrent client in the speed department" As for concurrency primitives: "one very big difference in the implementations is that of STM channels versus Erlangs message passing... I find the Haskell approach considerably easier" 
Kinda wish this wasn't getting downvoted so much. I mean, I think she's trying to be too structural about all this, it really helps to focus on a given community rather than try to cure cancer with a wishlist. But there's an overarching point about diversity in this post that I think we should acknowledge, regardless. There is obviously a significant lack of diversity in software developers and, in particular, the Haskell community. Unless white males have some clear natural knack for programming that no one else does, then I can only really conclude one thing: there's a massive pool of potential talent in the form of critical thinking, creativity, work ethic, social intelligence, differing perspectives, etc. that *we as a community aren't getting an opportunity to access in the first place.* I think that makes us significantly poorer no matter how much one nitpicks on Sarah Sharp's tone or underlying intentions. **As for Haskell-specific stuff**, I think our community could pay more attention to retention than we currently do. We get a ton of people who go out of their way to give Haskell the time of day and I get the feeling (I can't really quantify it, unfortunately) that the casualty rate for learning Haskell is pretty bad. I wouldn't be surprised if a majority of people trying Haskell never get to a stage of mastery where they can work productively on their own real world projects or make meaningful contributions to pre-existing projects that they find interesting, leading them to give up and move on to something else. They write a couple of toy programs, upload them to Github/Hackage, and then disappear. Sure every language proves to be a great filter for beginners who are still learning the basics of programming and might just not be all that driven to follow through on their task at hand, but Haskell is a very interesting language that tends to attract established programmers who can recognize its merits compared to the alternatives. Haskell's not exactly difficult in the long term, but the initial learning curve of getting familiar with a mountain of indispensable libraries and language extensions, as well as applicatives, monads, functors, arrows, etc. is **brutal**. And the result could be anything from yet another person saying "Haskell isn't practical" when it clearly is, to [Paul Phillips](https://youtu.be/4jh94gowim0?t=1h24m50s) saying that the reason he gave up on Haskell before committing to Scala was because of all the pains he suffered just trying to get started. These are lost opportunities. I myself have no master plan (either) for curing this, but I like one of the points Sarah makes on this subject, which is that it should be common-place for there to be a visible list of low-hanging fruit that newbies can tackle just so they can get involved in the community more. This could be as simple as **explicit public requests** for someone to document a library. I guess we could also improve the learning curve a bit by investing some serious time in improving the wiki (it's a pretty public resource and has value in addition to library-specific docs) as well as providing a ton of real world code examples that feature various libraries (examples are pretty powerful when there's more than 1 of them). The post today about making a BitTorrent client in Haskell is basically that: straight forward, practical real world code. It would be nice to see more stuff like that. Just my two cents.
I think for "using haskell" level 1 is ok, but for bringing people into making code contributions to specific things like hackage, etc. there's plenty of work to be done (although GHC itself has done very well here, relatively speaking).
GHC and Darcs both do a good job here. One simple thing any project with an issue tracker can do is to flag issues as "Easy" or "Mentored" to give beginners as sense of entry points. Cabal has taken on several summer of code students, so that's great too. Still, I think we should do better.
[removed]
I'm not sure why NCoC gets to be "beautifully apolitical" - it's just a different political position. The reason CoCs exist is that the behaviour of people in OSS is often awful. When awful behaviour is common and well documented, it's not helpful to argue that "we're all adults" (as NCoC does). Also, "Trigger warning: Monads" is not coming next. That's completely absurd, and a misrepresentation of Sharp's position.
&gt; We technical women appreciate being treated equally by our male peers, with blunt directness and unambiguous criticism. We are not fragile flowers. We can do our jobs without demanding others interact with us on our terms and tiptoe around our fragile egos while being considerate of our ever changing emotional needs. I promise you, it's true. We're not all Sarah Sharps. You know, I imagine that there's more than a few male developers who have steered clear of Linux kernel development because of Linus's well-known communication style. Some people have thick skins, some people don't. Blunt directness and unambiguous abuse drives away people of all sorts. 
&gt;Forcing everyone to play the microaggression game. What's next, "Trigger warning: Monads"? There is often a distinct lack of nuance in discussions of this sort. People sometimes stretch terms like "microaggression" and "privilege" too far. That does not mean those words do not point to actual issues. &gt;codes of conduct are a vehicle for certain people to force their politics into something that could be beautifully apolitical. Is anything ever really apolitical, though? Displacing a quotation, "if you choose not to decide, you still have made a choice."
So very tempted...
[removed]
[removed]
You can avoid the symbolic operators in this case without `do` notation. Do you find this more readable? list = fmap List (P.between (P.char 'l') (P.char 'e') (P.many' value)) I don’t really see the value in naming intermediate results here, other than making the code more vertical/imperative-looking: do _ &lt;- P.char 'l' values &lt;- P.many' value _ &lt;- P.char 'e' return (List values) 
yeah, flip needs to be in front of setInnerHTML
It's quite young yet, but there's ide-backend-client and it's been used with both emacs and Atom. 
[removed]
I wouldn't call this "easy" (or at least not as easy as it should be). Excited for stack integration for GHCJSi. 
I never considered this. Does using effects make the language more accessible? As a beginner I'm currently looking to now implement my own ServerT implementation for a Servant webserver, and TBH it's taking me quite some time, and I barely get what I am doing here... I'm not saying that monad transformers are bad, but maybe there might exist a better way to implement them. I've always found it odd that you can't combine all of them at once, using a list, something like `runAugmented [Reader config,State someState,Except] f` . Another thing that I consider odd, is that transformers are like the singleton of functional programming; you have only one ReaderT and need to compose your data. Looking at OO/IOC containers I'd find it way more intuitive to have named instances of monads that I could access.
Hi friends. I'm a novice haskeller. Trying to get through the tutorial, I stumbled upon this line: program 'sequenceA' ['createVertexShader' vsSrc,'createFragmentShader' fsSrc]&gt;= createProgram_ It says it's straightforward, but I can't make any sense about it. Strings in single quotes hint it's not legal Haskell. GHC insists that I switch on Template Haskell, it doesn't help building though. Please help me understand what kind of language extension (or whatever that is) that is.
This really raises the question of group maintainer-ship; core packages are too important to be maintained by 1 or 2 (or 3) single person. This is a really good opportunity here to change this into a more large and dynamic group. Also the haskell github organisation has a janitor group (which I'm part of, altought no one actually told me), and it doesn't look I can do anything with it (including triaging/labelling issues) ..
Update: My more advanced haskeller friends suggest it's just a typo and I should omit single quotes altogether. However, that leads to another error: Not in scope: ‘program’ Perhaps you meant ‘programID’ (imported from Graphics.Luminance) I can't see definition of "program" anywhere in Luminance source code either. What do?
Cause I'm eaasyyyyy Easy like ~~Sunday mornin'~~ `stack build`
right... i think last night ghcsi was merged with latest ghcjs master branch, so perhaps ghcjs-boot does not need anymore those funny flags with the hashes... so ideally, installing ghcjs is 5 lines ... stg like : git clone url, cabal sandbox init, cabal install, ghcjs-boot --clean --dev, npm -g install socket.io ... tada ! 
what is this?
The elephant in the room is Windows support. Does socket have well tested robust support for Windows? Not that Windows support is so perfect in the network package. But it works, and people have literally years of experience with the network package troubleshooting a myriad of issues that come up with networking on Windows. I would be very hesitant to move to some other package unless I become solidly convinced that the story will be better on all platforms, including Windows.
&gt; We get a ton of people who go out of their way to give Haskell the time of day and I get the feeling (I can't really quantify it, unfortunately) that the casualty rate for learning Haskell is pretty bad. But that is a property of the Haskell language, not of the Haskell community, no? 
Hi! First of all thanks for your both work on ghcjs and reactive-banana and for giving me a very good read yesterday evening with the paper referenced above. &gt; But I agree that I should at least document the difference in behaviour, and probably implement GHC's variant as the default until this question has been resolved. I kind of tried to implement the semantics of the paper [into the ghcjs rts here](https://github.com/ghcjs/shims/pull/24). I might have produced more errors then I fixed anything as all this is pretty new to me, but at least this keeps my reactive-banana based ghcjs app alive :) Would be nice if you could take a look.
First off, let's fix your formatting: parse :: String -&gt; Ast -- eval (parse "+(9,7)") [] [] == (Number 16, [], []) eval :: Ast -&gt; Context -&gt; Memory -&gt; (Ast, Context, Memory) eval ast c m = ( .., c, m) It would help if we knew what Ast looks like, what the result of your parse function is, etc.
Thanks a bunch! From a first look this looks good and I'd like to include this in the upcoming release, but I'll need to review the code to make sure it doesn't break any assumptions in the RTS. If you, or anyone else has any good test cases for verifying weak reference semantics (especially complicated cases, with references kept alive (or not) through waiting threads and through other weak references), I'd be really happy adding them to the test suite. As you might imagine, this part of the code is rather performance sensitive, and I'll probably keep making changes to improve things, hopefully without too much breakage. (also when JS engines start supporting the ES7 proposed finalizers, there's going to be a completely separate code path for browsers that support those)
I ran into a similar issue with `mapM_`: https://github.com/snoyberg/mono-traversable/issues/28
No. There are plenty of first-year computer science students who have never programmed anything at all, and are still able to learn Haskell when taught. If the Haskell community can't bring experienced programmers up to speed (ie, people with *vastly* more experience than 18-year old baby programmers) then that's evidence of a problem, whether in documentation, tutorials, or community support. 
No. It's actually easier to learn Haskell if you have no experience with programming at all, because then you don't have to *unlearn* anything. Also, mathematicians usually have no trouble picking up Haskell, it's mostly programmers that grew up with the imperative paradigm that experience the most difficulty. What I'm saying is that this barrier to entry resides within the language itself and its relation to other languages, *not* with how welcoming the community is as a group of people.
My experienced haskeller friends say Haddock would not link a quoted variable name to its definition if said quoted name is located inside a block of code. We have apparently moved to a next debug stage where it says *Could not deduce (HasProgramError e0)* and *Could not deduce (HasStageError e0)*. I will inquire and get back to you with a patch if I make it or further details otherwise.
Yep, I merged the changes from `master` into the `ghcjsi` branch yesterday so the latest shims/boot libraries work again. Still the REPL is quite experimental, easy to break etc. And the code needs some cleanup before I want to merge it into master. And I'd like the socket.io installation to be automated, perhaps even bundled with the source distribution.
&gt; Unless white males have some clear natural knack for programming that no one else does, then I can only really conclude one thing: there's a massive pool of potential talent in the form of critical thinking, creativity, work ethic, social intelligence, differing perspectives, etc. that we as a community aren't getting an opportunity to access in the first place. Some ideas here: The racial inequality might be caused by economic inequality. Has anyone looked into appealing more to people outside of academia? The gender inequality might be caused by men being socialized to be less risk averse. Haskell is perceived as 'risky' because it is frequently used with more abstract things. Perhaps what /u/Tekmo [is doing](http://www.haskellforall.com/2015/10/basic-haskell-examples.html) by showing how to use Haskell for less esoteric things might appeal more to risk-averse people? It doesn't have to be noob-level stuff, of course, but just "normal" programming. Edit: Can someone tell me why this is sexist? 'cause I don't see the sexism. Help? Edit2: For the lazy: Apparently it's sexist because some cultures dislike risk aversion.
But the front-end, the IDE properly said is not being published as open source. Why? I think that the kind of collaboration that the Web IDE permits can not be possible without the Web IDE. Moreover, the IDE work better than any other IDE with perhaps the exception of Leksah. And I can program Haskell in my phone or tablet with it. Why this part is not being published as open source?
Since eval takes an Ast, eval should have a case for every constructor of Ast. So: eval (Number i) c m = (Number i, c, m) eval (Name s) c m = ... etc.
I think that extensible effects have much better error messages, since the monad is a single one instead of a stack of data structures. http://okmij.org/ftp/Haskell/extensible/exteff.pdf The drawbacks mentioned in the thread are non-problems. I can not think in any major issue caused by them. The use of monad transformers is for me a kind of protection of the learning investment and a barrier for beginners that is artificially maintained by academics and hobbyst haskellers to feel a sense of superiority over the mortals who earn a living by programming in the real world. Avoid success at all costs was a lemma with a completely different meaning when Haskell was not as popular. I was a form of monastical dedication to a noble cause: to dig in the principles of computer science to develop a wonderful programming language based on them. Now that Haskell is popular that lemma means only the reluctance of the academic and amateurs and careerist haskell gurus to abandon the control of his toy to the plebeians, and their contempt for real programmers and real programming problems. I do not say that monad transformers are artificially complicated. I mean that there is a better alternative, and the defense of the former has a lot of the ingredients that I mention above.
[removed]
The `network` package *could* be modified; stability of the current API is important and useful, it doesn't mean that anyone can not create, for example, a `Network.Socket2` hierarchy in the `network` package along with the current API, with an optional soft depreciation notice for the current API, for X years (just a note in the documentation), and a harder one for Y years after the X years.
I'll just leave [this](http://www.dohaskell.com/) here. Given the new [community](https://www.haskell.org/community), [documentation](https://www.haskell.org/documentation), and [news](https://www.haskell.org/news) sections on haskell.org, I think we just need to do some house keeping. As for missing libraries (MSSQL), I wonder if that just speaks to the type of Haskeller which is prominent. Here at work, we only use MSSQL for a legacy system. All of our other systems are using MySQL. In my personal projects, I use postgresql. It could also speak to the size of the community. For instance, [opaleye](https://hackage.haskell.org/package/opaleye) is only now starting to get support for SQLite after having been around for about 3 years. I suspect it'll be even longer before MySQL is a target, and would bank on never for MSSQL.
I appreciate the last few articles you have written, Gabriel. Being told that we need more documentation is one thing, being shown examples (no pun intended) of how to write good documentation, what makes documentation easier to read, and what makes documentation useful is another thing all together.
[removed]
came here to say this.
I am seriously grateful for this post and the idea it represents and which you realise on your blog: the attempt to lower the burden for people to get in and to intensively work on the ecosystem. I say so as a person that actually tries to get a foot into the door but without an education in computer science. Personally, I consider to do something like @beerendlauwers proposes, but for problems which relate to activities in the Digital Humanities domain. However, it's still not time for me to do this. But to come closer to this point these activities you do with this example are tremendously helpful for me. Thx
Why is haskell-vim-now so monolithic and exclusiv and does anyone knows how to get around it? I love vim (use it for Python, my PhD with vim-pandoc and for XML related tasks at my work). Evidently, know that I get a little bit into Haskel I would like to use it for this purpose too. However, I stumble over the fact that: - it replaces my vimrc with a haskell-vim-now vimrc - that is bound to its gitHub repository - an sources out any other configuration into kind of secondary configuration file I see vim more as a framework which opts-in into specific set-ups dependeing on your filetype for example. I have the feeling with haskell-vim-now I destroy my general way to work with vim though I very much appreciate what haskell-vim-now has to offer. Can anyone tell me in how far I grasp the way haskell-vim-now works correctly and if there are any approaches to haskell-vim-now which integrates better into filetype dependent multiple usages of vim?
[removed]
Is there a reason why you call Kleisli arrows "Services"? Most of this stuff seems much more general than just services.
[removed]
 (&gt;&gt;= sequence) :: (Monad m, Traversable t) =&gt; m (t (m a)) -&gt; m (t a)
&gt; the idea it represents and which you realise on your blog Heh, it's actually /u/Tekmo's blog.
MSSQL = Microsoft SQL (Server)
&gt; The real advantage of FP is that it's easier than imperative programming. … once you have taken the effort to absorb a few abstractions. Every now and then, there are well-intentioned posts on /r/haskell that request that `Monoid` or `Monad` be given a different name, so that these concepts are easier to learn. Unfortunately, such a change would have the opposite effect. In a classroom setting, these concepts are easier to learn, simply because students have "no choice": To successfully complete the next class assignment, you have to accept the strange name and simply use it in the assignment, thereby actually *learning* what it does. For self-directed learning, the bar to accepting concepts that you don't immediately understand is much higher, that's why a significant number of people give up. (Certainly not all of them! And that's why mathematicians have an easier time: They're used to not to giving up even though they don't understand anything.) All this is related to the properties of the language itself and to methods of learning, but unrelated to the friendliness of the community. 
You should probably thank /u/Tekmo instead. :P
Specialization of general concepts can make it clearer how to use the library. If you imagine "Kleisli arrow" as a primitive type conceptually, then giving it a more in-context meaningful "Service" name helps with initial understanding.
Yes, it was with everything you said here in mind that I brought it up. I think you did a great thing in redesigning a networking library as a separate package. I'm just wondering if some of the folks being hurt by one of the many open issues on the `network` package could transition to `socket`. If such a transition is successful, then we can tag the relevant `network` issues to indicate this, and prioritize those that aren't easily addressed by swapping networking libraries. If the transition is not successful, then that would be worth knowing about for `socket` development.
Yeah I know. What's a "driver"?
I can answer some of these: * We more commonly use parser combinators over regular expressions * startswith and endswith are named isPrefixOf and isSuffuxOf * There is no standard deinterleave function because usually having your data interleaved like that is a symptom of not using a parser. This is one of those things where there is a more idiomatic Haskell solution * For text interpolation use the "formatting" library By the way, I created the "turtle" library specifically to help onboard Python programmers to Haskell: https://hackage.haskell.org/package/turtle-1.2.2/docs/Turtle-Tutorial.html. It solves a lot of the problems you expressed.
I agree with your gripes. Haskell is not very good at working with strings. In a lot of ways, MissingH feels like Haskell's version of [Ruby's ActiveSupport](https://rubygems.org/gems/activesupport/versions/4.2.4). It's a library that does all the things you might expect the standard library to do. Unfortunately MissingH's dependencies make me (as a library writer) not want to depend on it. But something with no dependencies wouldn't be as useful because it couldn't provide helper functions for time, maps, random numbers, etc. And even if MissingH filled in all the gaps and was available as a single import, it would still suck to start every Haskell program with: import MissingH main = getContents &gt;&gt;= putStrLn . (replace "\t" ",")
Does ODBC not work? https://hackage.haskell.org/package/persistent-odbc
I think that's /u/kyllo's point. From a certain perspective, the answer can't be "We don't have 'drivers'..." because that sounds suspiciously like "No." The answer must be "Yes! We support MS-SQL!"
&gt; No rawstring, no multiline string. Prelude&gt; :{ Prelude| putStrLn "\ Prelude| \I'm a multiline string. \ Prelude| \\ Prelude| \You use a backslash after indenting me, \ Prelude| \and a backslash before newlines.\ Prelude| \" Prelude| :} I'm a multiline string. You use a backslash after indenting me, and a backslash before newlines. That said, you're not the first to miss this particular feature, as it's not often used in the documentation or example code. 
&gt; blunt directness Too many times I've seen directness confused with insults and making uncalled for assumptions about the other party. I've caught myself unknowingly sliding from "being blunt" to outright ranting sometimes, and later regretting this, especially when I've stirred a heated discussion this way. I think that Haskell community is an example for a group where the experienced members don't run people into the ground for disagreeing with them, and I hope it stays this way.
I think that's the point. Magic to entice curiosity.
&gt; "Does it have an MSSQL driver?" You can using MSSQL via ODBC via HDBC. It's old tech, but it works.
&gt; Data.Text.ICU.Regex, but why is this inside IO ? It calls out to an impure C library, most likely.
&gt; I had to find a way to deinterleave a list, eg. [a, b, a, b, a, b] gives ([a, a, a], [b,b,b]). I think you want [`Data.List.partition`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-List.html#v:partition) or a generalization based on `sortOn`+`groupBy`.
&gt; no multiline string [lies](https://www.reddit.com/r/haskelltil/comments/3duhdf/haskell_ignores_all_whitespace_enclosed_in/)
I agree, everything can be done. But that's less convenient than the similar python list[::2] and list[1::2] solution. Thank you.
[removed]
I perfectly understand the reason, but that's an internal implementation issue, in theory a regex engine is pure. Perhaps we can hide it under unsafePerfomIO.
&gt; Perhaps what /u/Tekmo is doing by showing how to use Haskell for less esoteric things might appeal more to women? This is what women are talking about, when they talk about condescension in tech/programming. I would ask that you think about what you said and retract it before I return to this comment when I am (possibly) less angry.
Yes it is. It is especially handy when you handle languages with a lot of \, such as latex. Apparently, this issue can be solved using quasiquote.
I am completely new to both programming and Haskell, and I wouldn't say the problem here is the language per se, no. Learning Haskell as a first language is not really easier than learning it after you're an experienced programmer, due in large part to the fact that the Haskell community assumes people trying to learn Haskell already know one or more other languages and the basic concepts of programming itself. This isn't entirely negative, and I'm sure it's based on their experiences of who they see trying to learn Haskell. That said, none of the beginner material is oriented to people who don't already know those assumed things. The poverty of beginner-friendly learning materials is an aspect of the community, not a difficulty pertaining to the language itself. For a newcomer, that can make the community feel hostile, as if it's trying to push away newcomers, even when no particular individual is hostile. My own experience with the Haskell community has been that most people want to be very helpful but have little to no experience teaching the language to beginners. It contributes to the image of Haskell as being a language for either the academics who are comfortable learning from high-level white papers or the "hairy-chested hacker" types, willing to machete their way through tons of confusing blog posts and get beaten up on IRC. IMO, Haskell doesn't really have to be either of those things, but as something of an outsider to the community, I can see why it's developed that reputation. 
I edited my post. Is it fixed now?
[removed]
[removed]
The `socket` package is very new, which is why I didn't originally just say that everyone should be using it instead of `network`, but that we should encourage trying it out. I don't follow your reasoning for why it would be more productive to work in the space, and the status of being a core package says to me more that we *can't* quickly iterate such a package. The potential of the old `network` API silently benefitting from new development has to be weighed against those new developments subtly changing its behavior. If `network` is in maintenance mode, then I don't see a good reason to rock the boat, and I wouldn't want to tie a new package's development to GHC releases or anything like that.
[removed]
[removed]
[removed]
[removed]
[removed]
you're reaching for "accommodating risk aversion" (unscientific and probably irrelevant even if true) when the community could just start taking the low-hanging of not referring to literally every member as "he", naming binary function composition the "boobs operators", attacking the tone of a post announcing a gender equality workshop, etc 
Personally, I attempt to use 'they' pronouns for people, but yeah, those are probably some other good possibilities. &gt; you're reaching for "accommodating risk aversion" (unscientific and probably irrelevant even if true) Could you clarify why this is unscientific and irrelevant?
That may be the case, but I think that for those who pursue that curiosity, they will end up pretty quickly in some deep theoretical stuff (non-determinism and monads) and not have gained much practical programming ability in the process. I may have misunderstood the point, but I assumed these examples were practical things new Haskell users could try to do. I would assume that new Haskellers are more likely to use `map`, `filter` and `fold` and this non-determinism stuff doesn't intuitively seem to support those operations. Edit: I do actually think it's a cool example. I just wanted to voice the concern that it could be confusing to read.
Thank you for your answer. I'll have a look at parser combinator (Parsec / AttoParsec) which may help. Thank for isPrefixOf and isSuffixOf and the formatting library. Could you detail how you think the deinterleave function is a symptom of not using a parser. For example, if I have a file containing : A: bla B: foo A: bar B: donkey A: something B: weird And I want, as a result, the tuple ([A "bla", A "bar", A "something"], [B "foo", B "donkey", B "weird"]). 
This is great, thank you.
[removed]
I'm sure you can hire haskellers to write these libraries for you. I, for example, am not going to spend my personal time writing libraries I don't need.
[removed]
&gt; trigger warnings Trigger warnings aren't that bad: ["I like trigger warnings. I like them because they’re not censorship, they’re the opposite of censorship. Censorship says “Read what we tell you”. The opposite of censorship is “Read whatever you want”. The philosophy of censorship is “We know what is best for you to read”. The philosophy opposite censorship is “You are an adult and can make your own decisions about what to read”."](http://slatestarcodex.com/2014/05/30/the-wonderful-thing-about-triggers/)
I can't answer that - I've spilled everything I read in the blog. Maybe /u/snoyberg or /u/chrisdoner or some other FP Complete person might like to shed more light. 
It is intended as a joke. Hodor only says Hodor, just like () only has value (). Sorry if this post trolled anyone.
LOL, maybe you can find a way to work in the Game of Thrones "warg" mechanic into monad transformer form. I.e., a monad stack with Bran and Hodor...
[removed]
[removed]
Well, one explaining that particular example could scratch over monad by just stating that &lt;- becomes &gt;&gt;= that is concatMap. It's an explanation with many holes, but it act as bridge for the next thing.
How about: data HodorT m a where Hodor :: HodorT m () 
&gt;postgresql-simple doesn't describe itself as a "driver" but as a "Mid-Level PostgreSQL client library". Is it a driver or not? If you see the dependencies of postgresql-simple, it lists [postgresql-libpq](https://hackage.haskell.org/package/postgresql-libpq) as one. I think this is the real driver that actually talks to the postgresql server. postgresql-simple just provides an even nicer/high level interface to the programmer on top of postgresql-libpq functions.
&gt; We racked our brains in an attempt to find a problem for which this comonad would be a solution — an activity that is not often acknowledged but probably rather common. So true.
Perhaps, but then call them 'content notes' like Scott does.
Most types that are able to be concatenated are instances of the Monoid type class, and thus you can use `mconcat` to concatenate them. Alternatively, `Data.Text` provides its own `concat` implementation which you can use via a qualified import, e.g. `T.concat`.
[removed]
&gt; I finally ended with an haskell program twice the length of the python one, where I had to implement again many features. This is a program to download subtitles that I completed recently, both in [Python](https://bitbucket.org/sras/titley) and [Haskell](https://bitbucket.org/sras/hastily). I haven't really counted LOC, but it appears more or less to be of same size.
In the first example, what is the reason to do: n' \`seq\` delete n' as Instead of just: delete n' as ?
I've had very different experiences than you, evidently. It's almost like our respective backgrounds give us different experiences and opinions that can be valid and worthwhile simultaneously! It would be pretty terrible of me to assume my own experience is valid and true, and yours isn't, right? This is essentially what is meant by "don't tell a minority what it is like to be a minority." &gt; You do not need qualifications to share an opinion. This notion that people are not allowed to disagree with a minority because they are not that minority is absurd. I'd agree that you don't *need* qualifications, but qualifications and context certainly affect how serious the opinion can be taken. "Postgres is a superior database to MySQL." is a meaningless opinion coming from a random person off the street, probably worthwhile from a trusted colleague, biased coming from a Postgres developer, and held in high regard from a database expert. In any discussion *about what it is like to Have Background X*, the people that actually have background X are worth listening to, and frankly, the people that don't are not. If you are not gay, then your opinion on what it is like to be gay is *completely meaningless*. And, again, no one is claiming that marginalized minorities necessarily have more valuable opinions on technical matters.
I have not said that "everything is political"; I have said that community building is inherently political. In some cases it might be political only to a tiny degree, for instance if the community is very small, or perhaps if it operates in complete secrecy. Those caveats do not apply to the Haskell community, though. &gt; Gun control is not relevant to haskell, so leave it at the door. There is no place for arguments over it to be made in relation to haskell. People's views on gun control should have no relevance to their contributions or involvement in the community. I actually agree with this conclusion. However, it is not so because gun control is a political issue and the Haskell community is apolitical, but because nothing that the Haskell community does has any bearing on how the society at large handles gun control, and vice-versa. However, for some social issues, such as the ones being discussed in this thread, there is such a connection.
I would agree that it is reaching a bit. It's likely irrelevant because the things that make haskell "risky" are not the lack of beginner level tutorials that show how to build trivial terminal applications. It's the lack of enterprise support and the fear that businesses won't be able to find developers to replace haskell devs that leave. The unscientific part I agree with as well, and I'll try to explain it. Whenever the topic of wage-inequality comes up, the topic of risk-aversion seems to show up in articles saying that wages are basically equal if you only compare women against men in the same field. The fields dominated by women (being a teacher, nurse, or social worker) are among the lowest paying ones. To explain this, they are deemed risk-averse and less demanding. What's indisputable is that there are less women in (a) jobs where people have a somewhat significant risk of dying and (b) jobs that work 70+ hours a week. Here is the question I'm always left with: Is it really just a matter of risk-aversion or is it that some people are *afforded* more opportunities to take risks than others? In other words, are they keeping themselves out, or are others keeping them out. To be as explicit as possible, correlation does not imply causation. That's why it's unscientific. Statistics can verify that two statements I gave (except that it would give a number instead of saying "somewhat significant), but I haven't seen anything that paints a conclusive picture of why as many women aren't in these fields.
Databases have wire protocols that are usually specific to their product (mysql, oracle, mssql, postgresql all are different). Since they are so different, usually you have a common "database" api that allows a developer to establish a connection to the database (what ever that means for that database is somewhat implementation specific) and execute queries against that database without the developer having to be aware of that underlying wire protocol. If you would like examples, google the terms ODBC and JDBC. Those are two pretty widely used APIs that require drivers.
My interpretation of that point about non-technical issues is very different from yours. You seem to be interpreting it as relating to arbitrary unrelated political issues (like gun control) whereas I'm pretty sure she's talking about the non-technical issues *relevant to the community*. The thing is, OSS communities have lots of non-technical stuff going on, as is unavoidable when people get together to interact. I think she's just saying that this shouldn't be ignored (as is almost always the case!), but rather addressed head-on. For example, a non-technical issue might be, say, someone complaining about being called an idiot on an IRC channel. I interpret Sarah's point as saying that leadership should take a stance on these relevant issues (by, for example, publishing guidelines about conduct on the official project IRC channel). 
The human mind strives very hard to maintain consistency, so when it encounters something that does not fit into its worldview it makes a strong effort to "correct" the inconsistency one way or another. In this case the inconsistency is the expectation that they should be able to understand something but they don't, so the natural motivation is to correct the inconsistency by learning something new. This is also why confidence and self-esteem are very important for learning because they perpetuate this virtuous cycle.
Thanks, your explanation clarifies that issue a lot. Given that interpretation, I completely agree with your assessments.
indeed, but {-# LANGUAGE OverloadedStrings #-} import Data.Text (Text) import GHC.Exts (IsString) -- (inferred) message :: (IsString a, Monoid a) =&gt; [a] message = mconcat [ "first line" , "second line" ... ] messageText :: [Text] messageText = message 
you're welcome. it seems actively maintained too. 
I'd like to correct that and say that haskell is not very good at working with `Strings` (with a capital `S`). The `text` library has a better string type and a lot more utilities on it.
[removed]
[removed]
But here you are assuming that there are no studies that test gender differences in risk aversion independently of job choice, which is not true. &gt; It's likely irrelevant because the things that make haskell "risky" are not the lack of beginner level tutorials that show how to build trivial terminal applications. It's the lack of enterprise support and the fear that businesses won't be able to find developers to replace haskell devs that leave. That doesn't make the risk aversion irrelevant; in fact, it makes it more important because it gives us some alternative methods of handling it than the one I proposed. I would argue, though, that Haskell also seems 'risky' on a smaller scale, i.e. "It's not worth learning Haskell because it's only for academic stuff.", so I still think my methods would work.
[removed]
It was a quick mea culpa. I fixed the original formulation. You're the one projecting a sexist culture onto my comment.
Got it :)
yeah I was well aware you claimed it was nurture-caused, I was just explaining some background about where I'm coming from. to be clear, I think we agree that the people who have more privilege are the ones who can more easily change things. as I said, i see the gender-mediated risk aversion unlikely, and even if true, minor. do you really think it outweighs hearing a respected and accomplished member of the community making even a single sexist joke? for survivorship bias, they're not independent. a community with a large proportion of women will attract more women, by word-of-mouth or a reasonable estimation that it's not as crappy a place as most for them. also, there is an exponential decay in these things: someone has read some article or hears something from a friend, check things out, and leaves if they don't like what they see. 
&gt; the "hairy-chested hacker" types That sounds somewhat related to my point, tbh.
&gt; but even if true, I don't think it's the major blocker. But consider the primary alternative that's been proposed: sexism. The Haskell community seems less sexist than the average programming community, so shouldn't women be *overrepresented* compared to other programming communities, according to the sexism theory? Or is the Haskell community actually more sexist? Or is there some other explanation?
`Monoid` provides a better notion of a default value than `Default`, by way of the interaction between `mempty` and `mappend`. In particular, It should be, for any type class, that the choice of values is "best". For `Default`, since there are no laws that govern how `def` works, You might say `0` is the best default (for some type), but i might think that `1` is better, and we can only agree to disagree. In Monoid, that cannot happen. if we had two possible choices for `mempty`, say `u` and `v`, then we have: u == u `mappend` v -- right identity == v -- left identity and so the choice of `mempty` is always unique, for any `Monoid`. That's why people prefer it to `Default`
&gt; The Haskell community seems less sexist than the average programming community, so shouldn't women be overrepresented compared to other programming communities, according to the sexism theory? &gt; Or is the Haskell community actually more sexist? On what basis are you rejecting the null hypothesis that the Haskell community, as a whole across all locations and not just reddit/IRC/whatever, is not significantly different from the average programming community in this regard? I'm wary of anything that starts with the assumption that we're special and unique compared to mainstream languages in any way that's not a *very* direct consequence of the language itself.
[removed]
[removed]
[removed]
[removed]
What a coincidence very first haskell program was an DNA -&gt; Amino Acid converter that I needed for biology class: https://gist.github.com/arianvp/9f61598b840d00e95faf =)
[removed]
I don't follow R/communism, but if the sub wanted to ban terms like "retarded" or "crippled" would be fine. it really depends on how the number is in those groups feel about the term. I honestly don't see how it's any different than not using any other oppressive language ones I won't name because they become mainstream-unacceptable. also, I don't think talking sexism destroys communities. identity politics can, but there is a huge line between "I'm female, I'm right" and "I'm female, I feel uncomfortable with sexist jokes, I've been sexually harassed at events, you never have, I know what I'm talking about, just listen". if you know any probability and statistics its basic sampling bias / observer effect. if you're gender ratio is ten percent women, you're ignoring almost half your potential members. so your community is already destroying itself by dropping members, or keeping them out in the first place. 
[removed]
yeah, but lots of people who want trigger warnings due to their race or gender have been bullied or abused or whatever. there are always parasitic members of the community who know how to hack the rules, which-hunt-like. the majority of the people advocating for trigger warnings, at least the many people I've talked to, and many more I've been linked to to read online, have totally legitimate reasons. they do get triggered. that's why they call them trigger warning. but who knows, maybe that's just my crowd of crazy leftists. 
[removed]
The way you are moderating based on a _stance_ in this discussion is very frustrating and dangerous to the community.
Oops, that was just a mistake on my apart. I will fix it
well, if it's the case that the majority of trigger warning requests are "illegimate", I think I'd still bite the bullet and protect it. great pain to a few can be worth inconvenience to many. 
[removed]
This is awesome! I really want to learn a GUI framework in Haskell. Right now, I use PyQt extensively at work, and I really love Qt. It helps that Python comes with amazing OpenGL-based plotting libraries that integrate with PyQt. I wish Haskell still had actively maintained Qt bindings (besides /u/komadori's hsqml library). Can anyone compare FLTK with something like Qt for me? What do I lose (or gain) using FLTK that I can't do with Qt? I have no problem doing more work to recreate some abstractions but I want to know if it is *possible* to create a polished GUI in FLTK in a reasonable amount of time compared to Qt. More specifically: * Do apps look as nice? * Can FLTK handle huge data sets? * (Note that I create all my widgets in code, not using the designer, so I won't miss a drag-and-drop tool to make GUIs at all) I'm about to start a couple of GUI projects in Haskell, and FLTK looks like it should be good enough, so I'm glad I stumbled upon this post. My original plan was to create a Python frontend GUI in PyQt and try to use Haskell as a backend through the FFI, but I might try using FLTKHS.
Strong moderation right now would delete every single post in this entire ridiculous digression.
Could you please provide a few usage examples for rendering things with that or what exactly the benefit of it is?
I'm surprised no one else has mentioned this but the `delete` function in the todo list example is actually tricky enough to throw off a non Haskell-native mind. Using recursion to bury into the list with a decreasing counter is really is quite an alien way to do things compared with something like (e.g.) Python where you'd just test for the length of the todos and then use `pop(index)` to remove the item. I had to write out some examples on paper to convince me it worked and I ended up loving its ingenuity but wondering if that was the simplest way to do it. 
I think the complaint is about the exact phrase "trigger warning" as opposed to the more-or-less equivalent alternatives "content warning" or "content note", the former of which co-opts language ("appropriates", if you will) specific to PTSD (although IIRC it was prevalent in communities dedicated to recovery from eating disorders and self-harm before being widely applied).
Did you mean to post this in /r/haskelltil?
In your head, you can use `(&amp;&amp;&amp;)` to split the input into nested pairs (as opposed to e.g. 3-tuples) and then use `first` and `second` for aiming when recombining everything. arrowWithCombinators = (arr (+2) &amp;&amp;&amp; arr (* 2)) &amp;&amp;&amp; arr (^ 2) &gt;&gt;&gt; first (arr (uncurry (+))) &gt;&gt;&gt; arr (uncurry (+)) The actual desugaring by GHC will likely not be as clever. See [the arrows pages in haskell.org](https://www.haskell.org/arrows/syntax.html) and [the relevant section of the GHC user's guide](http://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/arrow-notation.html) 
I think it was worth it, it actually triggered a discussion that revealed things about our community that we probably go day to day completely ignoring. I was really worried last night this comment section was just going to get ignored and drown in downvotes, but there's been quite a bit of back and forth since then. Not what one would have hoped for, but actual talking took place about something uncomfortable to us. That's ultimately pretty valuable.
further reading: http://neilmitchell.blogspot.com/2015/05/handling-control-c-in-haskell.html
[removed]
You've replied to me instead of to u.snoyberg. 
[string-qq](https://github.com/kqr/quickweb-hs-experiment/blob/master/src/Main.hs#L403-L418) (don't look at the rest of the code) lets you do this with TH.
The regex situation is pretty bad. We have awesome regex libraries, but we have *too many* of them, so it's impossible to know which one you want! The existing tutorials for some of them are outdated as well.
If you unpack the tarball and run `stack build`, I would have expected an error message about the missing `stack.yaml` file, along the lines of this: Run from outside a project, using implicit global project config Using resolver: lts-3.7 from implicit global project's config file: /home/vagrant/.stack/global/stack.yaml Error parsing targets: The specified targets matched no packages. Perhaps you need to run 'stack init'? It depends, though, on which version of stack you're running. Anyway, if you just run the command, it will get installed into the "implicit global project", something like a fallback sandbox if you will. You can immediately use the library from, e.g., `stack ghci` or `stack ghc`. If you want to use it in a project, just add `fltkhs` to the list of extra-deps. If the author wants to streamline this even more for users, [adding fltkhs to Stackage](https://github.com/fpco/stackage/#get-your-package-included) would be the next step, and should be an approximate 3-line pull request.
Have you taken a look at the first-stop place, The [Haskell Download page](https://www.haskell.org/downloads)? There you have three options, including the `stack` which was told by /u/k-bx. 
I always wanted to understand statically-typed containers and it looks like the first part of the post will help me, so thanks Bartosz for this and /u/BlackBrane for sharing. 
"simply"
So essentially it's the implementation of a common interface to the idea of "build a connection, map a common set of types, send strings at it, map a common set of types back, consume results", yeah? Opaleye lacks this because it currently is tied to just Postgres, but as it starts to become more general it might grow it's own driver layer. There is HDBC, of course. I think that's what everyone is looking for. There's also [HDBC-odbc](https://hackage.haskell.org/package/HDBC-odbc-1.1.4.4).
That will break if you use the CPP language extension.
I don't, I have a haskell job. The world does not consist exclusively of enterprise bullshit corporations. 99.999999999% of companies that insist on using SQL Server also insist on using 100% microsoft products. They will not consider haskell no matter how many SQL server libraries it has.
Suggestion: You can use Unicode braille characters to get finer detail - [here's an example](http://i.imgur.com/u3Cy6H4.png). The larger gaps in the steep lines actually have space for more dots, but my code is fairly naive.
Operator precedence stuff can mostly be handled as an "third stage" after lexing and parsing. For example, GHC parses all infix operators as left-associative at the same precedence level, then rebalances the tree later once it knows all the fixities. I haven't thought hard about whitespace sensitivity, but that is indeed the piece that seems "least context-free" to me.
The downside of rolling your own lexer is that it's going to be different from the one used in the compiler. This leads to subtle incompatibilities that make it super frustrating for users. The Haskell for Mac IDE (written by /u/chak) manages to use GHC's lexer as-is, but it is incremental in the sense that it only re-lexes part of the code when you change it, so it scales well to large files and provides highlighting as you type.
I was recently introduced to a concept and I unfortunately forgot it's name. I think it was something like "adjustable computation"(or maybe self-adjusting computation, maybe something entirely different). The idea is that if re-computing something with a previous state + a diff can be cheaper than computing it from scratch for the changes, then you have this opportunity to exploit this property for faster updates. I feel like in practice you can always do it manually(e.g. implement your algorithms for such updates), but it's too much effort and it's not clear to me, for example, how to implement a type checker this way. So this led me to think that there may be some research on formulating algorithms in a way that, if possible, this "adjusting" version is generated automatically by the compiler or something like that. (imagine something like FRP) Without such a tool, I feel like propagating changes through the pipeline and updating every intermediate representation and analysis results without running them from scratch is just infeasible. Imagine something like this, you're writing an IDE: - Lexing and highlighting is done this way, this is the easiest part. - Lexer propagates changes in token stream to the parser, but parser has problems like handling incomplete input etc. It has to generate a valid AST in all cases to be able to run rest of the pipeline. - Similarly type checker takes "changes in AST" as input and it has to work on that somehow. - Then you have many more steps, like static analysis steps, from simple things like "unused names" or "name shadowing" checks to more complex, whole program analysis. This is just too much work, unless we have a systematic way to develop such algorithms and exploit this property of updating results from changes. I'm wondering if anyone here knows any research for this kind of things? --- EDIT: I think this is what I'm talking about http://www.umut-acar.org/self-adjusting-computation
As it is mantioned in the post, building a real IDE is not a goal. It is an investigation right now. No users -- no frustation :) I'd like to read more about Mac IDE lexer though. Does it lazily re-lexes till end of the files? My goal is to use the results of the lexer for incremental parsing and typechecking, that is why lazy lexing doesn't work for me.
It's getting a lot better, especially now that FRP is getting more and more pervasive.
Agreed. I started using [ReactiveX](http://reactivex.io/) (specifically RxPY w/ PyQt) at work. It is based on FRP principles, but is implemented in many non-functional languages. It is amazing how much simpler makes GUI and async logic. Since I know Haskell too, it makes me want to use Haskell much more for GUI programming.
I think even identifying the sticking points would be useful, to try and nudge toward a Haskell2030 that is amenable to this approach
`Service` is still more understandable than `Kleisli`, imo.
What's up with the build dependency on `ghc`? Do any programs that link against fltkhs actually have to also link against `ghc` or is there some magic in the cabal file that I'm missing? In my experience depending on `ghc` is a really annoying thing since you're bound to the exact versions of transformers, mtl and the other boot libraries that ghc was compiled with. In this case that dependency is going to impose these restrictions on all users of the library as well so if it can be gotten rid of it probably should be ;)
That is so I can get at [CallStack](https://hackage.haskell.org/package/base-4.8.1.0/docs/GHC-Stack.html#g:3). But you're right, it should only be a dependency if you're compiling with GHC 7.10.2 or greater. I'll change that.
The keeping everything up to date and only recomputing the bits which have changed reminds me of FRP quite a bit. Not sure how you would apply it to incremental lexing though. Maybe you could make a dynamic network from every easily detectable section (file is probably easiest but not very granular) and move on from there. 
I think it shouldn't be a dependency at all, the reasons I explained above also apply to GHC&gt;=7.10. Why do you need to depend on the ghc _libarary_ to get at a datatype defined in `base` thought? One of us is confused I think.
Just realised "easily detectable section" involves lexing, so maybe not the best idea. 
Thanks for trying it out! I just installed `stack` and did `stack init` , added the resolver and did `stack build fltkhs` and I get errors like: Setup.hs: Unknown build target 'exe:fltkhs-arc'. There is no executable component 'fltkhs-arc'. on all my demos. 
The only issue that could arise is a sub-optimal use of memory .. sort of a space leak. So you would build up thunks containing the computation `(n - 1)` instead of just a integer. However as has been pointed out in the thread because the recursion immediately goes back into another call of delete and the first parameter of delete is pattern matched against 0 that means that the thunk will be evaluated and no space leak will occur.
["Likely to land in time" for GHC 8.0.1](https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-8.0.1)
&gt; While I've personally found the FP and Haskell communities to be more friendly to myself as an LGBT person than other SW communities, there's still a fairly vocal minority that is hostile to the idea of being less hostile. Interesting, as another LGBT person I haven't noticed anything of this sort in any online programming community. In my experience sexual orientation in online programming communities is a total non-issue, as it should be. I have seen a handful of ignorant comments about a transgender speaker on a video of a tech conference, but those were promptly downvoted into oblivion.
You may also want to take a look at [this post on ABTs](http://winterkoninkje.dreamwidth.org/103978.html). The "implementing ASTs" part there demonstrates how to do [colored operads](http://ncatlab.org/nlab/show/operad#PedestrianDefColouredOperad) (N.B., both `(:$)` and `(:*)` are the categorical tensor, and `End` is the unit for that tensor). The actual ABT stuff goes beyond what operads do, though operads could surely be generalized to cover the generalized quantifiers as well (presumably similar to how fibrations are used to handle typing environments in categorical treatments of dependent type theory).
What is the error message.
I'm been thinking for a few days and decided to show tejon's example and your explanation with it.
I found this branch on github: * https://github.com/chrisdone/emacs-haskell-config/tree/stack-mode Comparing the README of that branch to the main branch seems to indicate that stack-mode is missing a lot of features: * https://github.com/chrisdone/emacs-haskell-config/compare/stack-mode#diff-04c6e90faac2675aa89e2176d2eec7d8 I haven't actually tried it yet. ---- ^^^ping ^^^/u/chrisdoner
[Here's](https://www.reddit.com/r/haskell/comments/2xp8ls/pdf_gadts_meet_their_match_patternmatching/) the discussion from when this was posted a while ago.
Could you provide some examples of good / bad documentation?
I like this a lot: https://hackage.haskell.org/package/react-flux-1.0.1/docs/React-Flux.html
I've personally been working with Attoparsec lately and wouldn't mind contributing some docs and examples to it over the weekend. I'm also seeing some HaskellWiki pages that could use some love, in particular the page on [Lazy Evaluation](https://wiki.haskell.org/Lazy_evaluation).
I had no trouble connecting to other DBs using HDBC-odbc. I would be surprised if it failed for MSSQL, which I presume has ODBC drivers.
The easiest way to improve the documentation for a large number of libraries is to provide one end-to-end example of using the library directly within the Haddocks
Yep, absolutely the easiest! Of course, it's hardly sufficient to be good documentation, but it goes a long way! Additionally, it should be a *well commented* example
I'm a fan of what /u/Mob_Of_One has done with [bloodhound](https://github.com/bitemyapp/bloodhound).
Bad documentation: https://hackage.haskell.org/package/shakespeare-2.0.6/docs/Text-Hamlet.html Good documentation: https://hackage.haskell.org/package/aeson-0.10.0.0/docs/Data-Aeson.html
What libraries do you think are well-documented?
Ah, I see what you're getting at. Personally, I don't think one-liners count as examples ;) Edit: Additionally, it seems that the documentation has improved a lot lately.
I find that the documentation of the infrastructure itself is sparse as fuck. The IR is well documented, sure, but that's about the only part. The worse example for this is the GC support documentation, I think. That said, the LangRef is pretty spotty on the intrinsics.
So I still have the latter two to look forward to :)
&gt; the Yesod book is not very helpful Why do you think this is? I know quite a bit of work was put into it and I believe it is helpful in many contexts, but what kept it from being helpful for your use case? What keeps the Yesod book from being useful in the most popular use cases?
In particular, it didn't cover how it worked, just how I might use it in some common examples. I didn't see a proper spec of how the language works, what subset of haskell is allowed in antiquotations, or any other precise documentation. It's also completely unclear how to use shakespearean templates from any framework that's not Yesod that doesn't have Yesod's particular implementation of typesafe routing.
Bang patterns are defined in terms of seq...
I had almost exactly the same experience when I starting playing around with Haskell web dev stuff for the first time.
I'm in the same situation here, my 5 items would be (by decreasing order of importance) * What's the library purpose (use case, problem to solve) * at least one end-to-end example to show how pieces fit together * walk though concepts of the libs (if any). Eventually, link to a talk, tutorial or a paper (the more hands on, the better) * If the name of the types are not crystal clear with respect to their purpose, more doc there. that's only 4 items, but these would already be wonderful to have
The Haddock is often spartan, but the Wiki fills most of the gaps pretty well. Documenting something as generic and abstract as Lens is a huge challenge, but I think they're doing an awesome job. [This page](https://github.com/ekmett/lens/wiki/Operators), for example, is a life saver, and something other libraries from *any* language should learn from.
I think part of this is because many Haskellers prefer to develop a thorough understanding of all the moving parts before abstracting them or reaching for a library that does the abstracting for them. An extreme version of this approach is to implement your own toy HTTP server before picking up Warp/WAI; then implementing your own routing, handling, templating, static file serving, database abstractions, form handling, etc., before grabbing something like Yesod. If you follow this approach, then understanding Yesod is no longer a matter of "what is this, what does it do, why does it do that, and how does it work", but rather just "how did they solve these problems that I've solved myself before". It's much easier to understand this way eventually; the learning effort takes longer, but is more thorough and transfers to all the other frameworks, even in languages other than Haskell.
Does hmatrix have any flags that might differ from the defaults if it is installed as subhask's dependency?
If you do it by means of pull requests to improve the hackage docs, then the things you write should be checked by the author of the package who will hopefully correct any serious problems. For blog post type documentation what is a good way to get that checked? Just post it here and ask for feedback? 
More doctests!
1. go to `D:\workspace\subhask` 2. create sandbox there if you don't already have one. 3. run `cabal install D:\workspace\hmatrix`
Have you tried with `cabal sandbox add-source`? See also [Sandboxes: basic usage](https://www.haskell.org/cabal/users-guide/installing-packages.html#sandboxes-basic-usage) in the Cabal manual.
One example of bad documentation is when it tells you nothing that couldn't already be seen from the type signature. Doing so violates DRY and makes it more likely that your documentation will become stale. Stale documentation can become a very significant problem over time, so the potential for staleness should be minimized. For instance, consider this hypothetical doesFileExist documentation. -- | 'doesFileExist' checks whether a file exists. Runs in the IO monad and returns 'Bool' -- -- &gt;&gt;&gt; exists &lt;- doesFileExist "foo.txt" doesFileExist :: FilePath -&gt; IO Bool In this case the English part of the comment tells you precisely nothing that wasn't already evident from the type signature. The example code snippet isn't much better. One could argue that it helps the beginner who isn't familiar with monads learn the syntax for using the function. But I think that is not the right place to be teaching the user this kind of thing. It's bad because it includes no context. The user has no way of knowing that this code will have to exist in a do block. So I could easily imagine them putting it in a where clause. ... where exists &lt;- doesFileExist "foo.txt" To us experienced haskellers that is obviously wrong, but a beginner has no way of knowing that. I think that for the most part this kind of "example" is bad and just gives us the illusion that we are writing good documentation. I agree that we should have code examples because all these little obscure details that I just described are difficult to discover for the new user, but the examples should usually be provided outside the context of a single function. They should provide examples of using several of the API functions together to accomplish something because the hard part of using a library is usually figuring out how to tie the whole API together, not the use of a single function.
http://hackage.haskell.org/package/bloodhound-0.8.0.0/docs/Database-Bloodhound-Client.html you mean the detailed comments with 100 doctests on hackage? 
hmatrix have to use "openblas" flag, so I'm building it with this flag.
Give stack a try. It does a lot better at doing the right thing and suggesting what to do if you do type the wrong command and as it still uses a .cabal file and is basically cabal behind the scenes any tutorial that mentions a .cabal file is still basically applicable. About the only gocha is that stack installs executables to ~/.local/bin so make sure that's on your $PATH (though I think it actually will warn you if it's not). If you do decide to go with cabal-install, make sure you get an up do date version. Cabal itself has made huge, huge improvements in the last few years which you might miss if your distro's cabal package is out of date. Get at least 1.18 as those version support sandboxes. And if you do use cabal-install, use sandboxes.
Well, write a blog post and post here perhaps? If you're wrong, people will correct you.
I was on my phone when I wrote that so I didn't elaborate. I meant if hmatrix might build with a different flag which would account for an error in one case and no error in the other.
The, "how do I use this?" examples a little easier to read on the README, https://github.com/bitemyapp/bloodhound#create-index but I found people maintained the documentation when the CI was executing it and squawking when you invalidated it. It's difficult getting somebody that put up a PR to update the README, but if the doctests complain, they do it before they even put up the PR generally. I don't think they're great on readability, but it's functional. Oh well. Please let me know if there are any gaps in the docs.
There's a heavy reliance on relatively trivial examples in lens. I do think the docs that do exist help a _lot_, but it's easy to get turned around when you actually go to use lens. traverse/plate/\^.. come to mind in particular.
We were thinking of doing something like this as a follow-up to http://haskellbook.com/ One difference is that I'd really like to do a cookbook which explains the idioms and patterns in the library you're learning to use. Understanding that can help a lot and improve your ability to read other peoples' code.
Based on this comment, you may not completely understand the feature set of Stack. You can find out more in [the Stack guide](https://github.com/commercialhaskell/stack/blob/master/doc/GUIDE.md). Please don't repeat the misinformation that some people have been spreading, it really doesn't help new users or the community in general.
What version of stack are you using (`stack --version`)?
Meaning, stack has a solver now?
`0.1.5.0 x86_64`
lol... apparently [option types are abstract nonsense](https://twitter.com/headinthebox/status/621443337049010176) too, so who knows
[**@headinthebox**](https://twitter.com/headinthebox/) &gt; [2015-07-15 22:16 UTC](https://twitter.com/headinthebox/status/621443337049010176) &gt; Nothing : Just a = null : a, using Maybe just moves the null pointer problem one level up, but solves nothing. \#FP101x ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
&gt; It isn't, as it does not leak in the type *how* it is implemented. Uhh.... isn't that what we *want* from types? Constrain exactly which bits of code could have gotten us to this point? Going down this road, Any -&gt; Any is the most concrete function, because you know nothing **at all** about how it is implemented.
We need to somehow make having examples the norm. I don't even agree with well commented example, just an example at all. I'd like a required example field on hackage that you'd have to actively fill with nonsense or a comment saying you don't have an example. There are probably better ways to do it, but making examples expected would be hugely beneficial. 
As tekmo suggested, just write a working example. But yeah, the author should point out any problems before accepting the patch.
Correct. The only thing I need it to install quickCheck and HsUnit. maybe on the rest of the book I need more. 
Another approach is that you can include a `README.md` in your project and if you add it to the `extra-source-files` of your project it will automatically be included inline in your project's landing page at the bottom (with a link to it at the top). One example of this is [the `servant-server` package](https://hackage.haskell.org/package/servant-server) and [here is how it is done](https://github.com/haskell-servant/servant/blob/master/servant-server/servant-server.cabal#L27).
Oh, I'm not arguing against you. On the contrary, I think you're exactly right and this is a big area where we as a community are lacking. I just wanted to point out pitfalls that I've seen in my projects.
I've actually experimented with shrinking the margins before (I have variants of the format headers for this already) We can shrink the vertical margins without any real issue, tho not by much (chapter line and footnotes need room), the issue with horizontal margins is that you really don't want your lines to exceed 40-47 words per line for readability's sake. Before we do the next release, I'll kick it around some more.
All I learned from that is that Twitter is unsuitable for discussing programming language design…
I actually did a quick script to find out exactly how many. The last time I checked 96 of the top 100 packages (by download) or 752 of the top 1000 packages are on Stackage.
Having a `List` means exactly having something that can be filled over. Any other choice is *less* abstract; this is what being an initial algebra means. (The whole reason to embrace the pain of laziness is that an "iterator" for a type is just a `toList` function -- this is a good chunk of the moral of *Why functional programming matters*.)
Can I also make it work that quickCheck and Hunit work with a cabal command
I am very sure I attended the Prolog and FP course you are talking about. The Emacs-plugin was actually very helpful and replaced lot's of text I/O with visual representations of the data structures and problems to be solved (maps, RNA sequences, planning problems ...). It also had a huge test suite for all assignments and was able to catch lots of non-obvious errors with explanations why there is an error. (Besides, the properties of monotonic Prolog code are simply beautiful, and the plugin elevated on that fact to detect the code lines that are responsible for a failing assertion. I mean how cool is that!?) Actually I think a similar system for learning Haskell, or any language for that matter, would be very helpful. (In the web tho, not in Emacs)
&gt; Bad documentation: [shakespeare](https://hackage.haskell.org/package/shakespeare-2.0.6/docs/Text-Hamlet.html) Actually, that's a bad example of the kind of bad documentation we have been discussing in this thread. There is a [whole chapter](http://www.yesodweb.com/book/shakespearean-templates) about shakespearean templates in the [Yesod book](http://www.yesodweb.com/book), linked to in the package description on Hackage. That chapter has just about all of the things that people complain are usually missing: a clear and detailed description of the library's purpose and philosophy, and many fully commented end-to-end examples. The documentation that is missing here is what is usually present in most other libraries on Hackage - Haddock comments with at least a few words of explanation about the use and purpose of each type and function in the library. Unless you always follow exactly the patterns in the Yesod book examples, that can be crippling. But it's a different problem than the ones we have been describing.
Why not just make a separate Prelude that exports specialized versions of functions for teaching purposes? Then when you go over various typeclasses and abstractions, remove the training wheels. It certainly seems like a huge overreaction to switch compilers. Especially with a reason like "Hugs doesn't change".
Folding might be the implementation of `and` but folding is different for each instance so that part is not leaked. Constraining to `List` leaks everything.
Another note: I recently read Oleg and Hiromi's [Freer Monads, More Extensible Effects](http://okmij.org/ftp/Haskell/extensible/more.pdf), and noticed a similarity between their [Union](http://okmij.org/ftp/Haskell/extensible/OpenUnion41.hs) type and my Data.Type.Sum.SumF type. In fact, they use the same machinery to work with unions of effect types as is provided in the Data.Type.Sum module. I have an implementation of their system using types and classes from type-combinators in the github repository, in Data.Type.Eff et al.
But, you won't necessarily run into "cabal hell" when you start having lots of dependencies, so long as you use minimal global libraries and sandboxes. Hackage also figures out how packages play nice with eachother, and does so in a much more general way. But it does so at the cost of the solver sometimes going wonky. But that's different than conflicting global plans. So its a more complicated story. I agree for exercises you're fine either way. But, if you understand the options, as you scale up, you can remain fine either way too.
In fact, `yesod` is one of the few cases where I would unilaterally say that stackage curation as opposed to hackage is a virtual necessity -- at which point (if you're always sticking to stackage) then stack starts to look like it is going to be an easier option at every step (even if cabal is still feasible).
Not sure why `hugs` rather than just saying use `ghc` 7.8.
Remember the "long tail" though. Even if nearly all packages used by nearly all users are on stackage, there will be potentially different odds that any given user will want any particular package not on stackage. (And of course, yes, stack _does_ give ways to handle this, to be clear.)
I think I will turn this one into an example/testcase. Thanks for the suggestion :)
I recently did a ridiculous overhaul of the documentation in the [ed25519 package](http://hackage.haskell.org/package/ed25519-0.0.4.0/docs/Crypto-Sign-Ed25519.html) package, since we'll be using it for Hackage itself. I tried to keep everything commented, always provide hyperlinks to constructors and other types, tell people when an interface appeared (`since` annotations), and a lot of docs on the actual mechanics and some security properties. I've wanted to add doctests but there are only like, 4 functions in the whole API, so the examples could only be very basic. But I should probably still add that - clearly it's a big point for a lot of people.
Yeah, I learned that if I respect someone intellectually - and wish to continue doing so - I should not ever even glance at their Twitter feed.
Don't forget the end-to-end example, even if it's very short in this case. In other words just some quick code showing the generation of the keypair and their example use. Even if each individual function is well documented it helps to have a code synopsis showing how things fit together.
You can read my [lens-tutorial](https://hackage.haskell.org/package/lens-tutorial-1.0.0/docs/Control-Lens-Tutorial.html) library in the meantime.
See [Type-Safe Observable Sharing in Haskell](https://vimeo.com/6679785), especially the graphs around 19:00.
[Presentations](http://chrisdone.com/posts/presentations-update) are a pretty good start for visualizing anything Haskell that's deeply nested or infinite. 
I do agree a bit that Foldable seems to focus too much the fold function. Why is it not just Traversable? Why aren't Foldable and Traversable synonymous? Are there Foldable's that aren't Functor?
tried to install yesod a few times without stack (i haven't done more than a few haskell exercises before) and hit a brick wall. With stack i had to edit one line in a config and it works like a charm
Indeed, it is a good idea. The only possible problem is that your dimensions will limit the parallelism rate you can have.
That is very cool, thanks for it. But I think it is a little different from what OP is asking since it is more of a tool to visualize thunks and could distract you from the actual data. I wonder if there is something that is equivalent to show, but that rendered (the completely evaluated) tree like [those graphy drawings](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Binary_tree.svg/220px-Binary_tree.svg.png) we see on CS classes?
Got it. One other note, the solution you've provided doesn't allow a "streaming" approach, which was part of what I was going for. Given some historical results from partialAverages and a newly created vector, I can find the average of the two using partialAverages.
A specific other choice is less abstract. But a Foldable constraint is hardly so!
Foldable is essentially the Map-Reduce pattern; it takes elements from a data structure, maps them to a monoid, and then reduces the monoids with mappend. Traversable is more like fmap for Applicative types; like fmap it updates values and types while preserving the original structure, but it also leverages an Applicative instance to add the potential for side effects. foldMap f = getConst . traverse (Const . f) fmap f = getIdentity . traverse (Identity . f) ~~As for Foldables that aren't Functors, I think the answer would have to be "all of them", as Foldables are concrete types (kind *) while Functors have kind * -&gt; *. Put another way, [a] is Foldable, but [] is a Functor. Less pedantically, monomorphic containers can't have Functor instances, but they can be Foldable.~~ [Edit: See duplode's response below. ]
https://github.com/mcandre/cspace/blob/master/TreeToGraph.hs Example: https://raw.githubusercontent.com/mcandre/cspace/master/cspace.png
You can get rid of the thunks by evaluating to normal form
How do you distinguish the functions without just testing them?
Erik Meijer is an interesting guy with some strange opinions. 
I agree. Any specific ideas on NT? I wrote a little script to compute Legendre symbols once but that's it. Do you know any resources specifically for computational diff-geo?
[Project Euler](https://projecteuler.net)?
They're in the nature of puzzles/problems, not "projects" to work on. Thanks anyhow.
I did something similar (albeit manually) for a game decision tree: http://derekmcloughlin.github.io/2014/10/04/Haskell-Dice-Of-Doom-Part-2/
have a look here for the categorical bits: https://bitbucket.org/jhinkle/haskell-diffgeom/src 
What about a matrix library that can figure out the optimal order in which to multiply matrices by itself? Lots of type foo to be had there and as by now we have `-XTypeNats` it's high time. And as you mention Geometry and specifically linear algebra: Linear and Convex optimisation? API, (naive) solver, interfaces to other solvers.
Someone starting working a bit on computer algebra recently, but I think that space still has plenty of room for useful contributions.
You know. I had to use those stupid actuarial calculators to take the actuarial exams. This (and the mind-numbing boredom) is why I'm not an actuary. ;) The main issue I have with `hugs` for this usecase is there is no active maintainer and no story at all for how to deal with libraries. I'm actually surprised buy your claim it has better latency than ghci though. What are you doing at the REPL that doesn't start up instantaneously?
If you prefer the commands as I indicated, then it handles all of the issues with proper sandboxing that /u/gelisam mentioned. Using `cabal install QuickCheck` as you indicated is not a recommended practice, as it avoids any kind of sandboxing. If you want to use GHC directly with Stack, you just need to put it on your PATH like you would with cabal usage, and likely set your `GHC_PACKAGE_PATH` environment variable (which is IMO much better than having to remember to put -package-db in all of your command line invocations). You can get the relevant environment variable information from `stack exec env`. There's also an [open issue about making this easier](https://github.com/commercialhaskell/stack/issues/620).
[Here's an example](https://github.com/yesodweb/yesod/issues/1075) where PVP upper bounds used by Persistent caused a user to be unable to install. Reasons: 1. cabal-install defaults to not using reorder-goals (which [I requested be changed](https://github.com/haskell/cabal/issues/1780)) 2. The max-backjumps level is too low 3. Even when passing `--reorder-goals --max-backjumps=-1`, it ["Takes almost 5 minutes for cabal to solve the dependencies properly."](https://github.com/yesodweb/yesod/issues/1075#issuecomment-142017008) On the other hand, Stack by default doesn't do dependency solving, and therefore has none of those issues. (As an aside, these problems are the original reason why Yesod stopped following PVP upper bounds in general, since they ended up causing ~~my~~ *more* problems than they fixed.)
Minor nitpicking: `stack` is able to *call out* to an external solver (which means effectively calling `cabal`), rather than "having a solver" builtin. So when I tell people `stack` doesn't have a solver of its own, that's what I mean by that (and I usually point that out, to make it clear that `stack` still depends on `cabal` for that, and therefore you need to install `cabal` as well if you need that functionality). This isn't misleading, is it?
And to be fair: pointing that out at all is the equivalent of pointing out that Cabal can't compile Haskell code (because it calls out to GHC as an external tool), or that GHC can't link executables (since it calls out to `ld` as an external tool). It's an unimportant distinction for an end user asking "how do I do X?" Now, if someone asked about defaults and you said "Stack defaults to using curation instead of dependency solving," I'd consider that a perfectly valid point. Question: when [issue #116 is implemented](https://github.com/commercialhaskell/stack/issues/116#issuecomment-144211577) and either Nathan's or Tom's dependency solver is written, and Stack adds support to use it, do you still consider it correct to point out to people that "Stack doesn't have a dependency solver?"
Somewhat, with an important difference: Stack still using a snapshot by default for that implicit global, so it will prevent accidentally installing conflicting versions of packages into that database (though if you try hard enough, you still can). Also, forgot to mention the other approach to this that some people like: `stack exec bash`.
a-men
You've got a point there although I don't see `cabal` as *compiling* code, rather as instructing an external compilers and preprocessors (which can be Alex, Happy, GHC or UHC or whatever). Just the same way I wouldn't consider `make` compiling my code, but rather orchestrating compilers and other tools to build my the project. I realise though that many developers don't have this clear distinction in their mental model, and are used to just press a magic button in Eclipse which does all the magic, hiding the details happening behind the curtains... that's not how I learned to program, and I like to be aware of all moving parts in the toolchain as that helps understanding a lot when things start to go wrong (and they will). As to the #116 question, I see where you're going. At some point the lines are blurred to the point where it's hard to tell whether such an implementation detail is even worth pointing out. I'd say the point (for me) is reached when you can't observe the solver being an independent part. E.g. when `stack solver` depends on a component which almost exclusively provides a solver and is always available when `stack` is available. 
also, a little background on matrix chain multiplication complexity is quite interesting : https://en.wikipedia.org/wiki/Matrix_chain_multiplication 
That design pattern is intriguing... I often run up against that exact problem. I'll have to play around with it. 
Yes, that a bug. I opened up an [issue](https://github.com/deech/fltkhs/issues/17) on your behalf. Thanks for reporting!
Haskell 98 was designed expressly to be stable for textbook authors and educators. It doesn't need a maintainer because it isn't implementing any new features and isn't in the least bit broken. People who assume it's bitrotten have forgotten they're doing pure functional programming. Good latency in the sense that it loads up quickly and `:l` and `:r` are super-fast. It's not that ghci is particularly slow, but hugs is definitely faster, particularly on an under-powered notebook. I suspect that ghci would win on throughput, but I certainly wasn't advocating hugs for intense processing. You can install the source code of any libraries (at a compatible version) in a packages directory somewhere and specify that in your search path setting. When the students get to the point where you're teaching them more advanced things and regularly needing interesting libraries, then of course they should graduate onto ghc, but ghc is unnecessarily complex for starters. 
Well, that script is clearly making some assumptions about the format which aren't true anymore.
Just for the record: I've written something similar to a part of your library a while ago as [type-list](https://hackage.haskell.org/package/type-list-0.2.0.0/docs/Data-Type-List.html), and recently updated my [vinyl-utils](https://hackage.haskell.org/package/vinyl-utils) package to include [operations on field lists](https://hackage.haskell.org/package/vinyl-utils-0.2.0.0/docs/Data-Vinyl-Utils-Field.html).
Speaking of which, where *are* we with nested parallelism in 2015?
This isn't exactly what you were expecting, but I recently watched [Evan Czaplicki's Curry On talk](https://www.youtube.com/watch?v=oYk8CKH7OhE) about his efforts to make Elm more usable and approachable than Haskell and I think he ended up touching on some pretty valid points on why people don't use Haskell. This is obviously not a community polling in the form of a talk, but I think it's as relevant to the Haskell community as Garret's talk was to the Erlang community.
Swap: [distributive law](https://en.wikipedia.org/wiki/Distributive_law_between_monads) Some stuff on traversals and distributive laws: [tr](http://arxiv.org/abs/1202.2919)
Fixed! Thanks again! :)
Wow, sounds really interesting, however, I'm wondering if it's not an overkill for what I need. I just need to display list of available option and let the user type the number he wants (a bit like `cabal init`). `brick` seems to be more to graphical interface in text mode, am I wrong ? 
I mistakenly thought it was using the Cabal library rather than cabal itself. Thanks for the correction. Comment edited.
That's right. Should have provided the link.
Yes, it’s for curses-style interfaces and has all sorts of widgets. If you just want a sequential choice thing it might be more than you need (though it is pretty fun :).
great talk
Being able to do things with elliptic curves from ghci would be handy. As I said, SICM is a nice book. It takes the air out of your tires if you're looking to figure out the computational stuff on your own though, that's what the book is about.
I worked with Hiromi Ishii last year during the Google Summer of Code to get an implementation of the F4 and F5 algorithm for computing Gröbner bases efficiently last summer. They are present in the [`computational-algebra`](https://github.com/konn/computational-algebra/blob/master/Algebra/Algorithms/Faugere5.hs) repository, but the work never made it back to hackage. This is primarily because we were looking to merge the package into the `algebra` package, but that work hasn't yet been done.
A lot of projects solve this problem by including small, self-contained demos.
&gt; Using cabal install QuickCheck as you indicated is not a recommended practice, as it avoids any kind of sandboxing. This is not quite right. Since the command, in the sequence of instructions given a few posts above, is executed in a sandboxed directory, the `cabal install QuickCheck` command will only install `QuickCheck` in the sandbox.
You can't. I hope this was a lesson about tractability and problem domain sizes or something. 
you described my documentation generation process, hence the last sentence :)
I actually would love to have a Haskell that's one tenth of the way toward Fortran.
Not sure why you say that. We use the Yesod book as a fundamental tool for getting new developers up to speed with Yesod quickly. The Yesod book is not the end of the story, but, yes, it really is an excellent resource for getting real work done.
Thank you for the explanation. So basically, monad transformers are more than just monad composition, I'm even more confused now :-( (especially about the order of the transformer stack). 
Statically-typed numerics.. it sounds sexy but the road leading to them is winding and full of perils; Apart from [hmatrix-static](https://hackage.haskell.org/package/hmatrix-static), I've just found out about this [article](http://ofb.net/~frederik/vectro/draft-r2.pdf) from 2007, "Statically Typed Linear Algebra in Haskell", repo [here](http://ofb.net/~frederik/vectro/vectro.tar.gz) . (and getting back to the OT, none of this is ... healthy for a beginner) 
There doesn't seem to be a lot of explanation, but this looks like a special case of implicit equations - if you add an `= 0` to the distance function, you get an implicit equation for the bounding surface, though with more useful information encoded at points off the bounding surface than just "not here". The implicit equations I've seen used are mostly polynomial or rational polynomial, so a lot of theory assuming that doesn't apply, but still - yes, very interesting. 
As someone who doesn't really know purescript I felt like I didn't really understand your posts. for example, what are `Aff` and `Eff`? what is `p` in the type of the `ui` function? why `Input a` instead of `Input`? I'll try to get more into PureScript when I have time and I guess your post will be a lot more informative then, but right now just as an intermediate haskeller, I don't really understand it.
`Eff` is a synchronous effect monad, and `Aff` is an asynchronous effect monad (implemented using callbacks on top of `Eff`). Both are refined by a row of effects, so that you can give types like `Eff (console :: CONSOLE, random :: RANDOM) a` ("this is effectful, but it is synchronous, and its only effects are console IO and random number generation"). In that sense, you can think of these as variants on Haskell's IO monad with phantom types to track particular effects.
Probably not `Pointed` don't need to be functor. You can read about it [there](https://wiki.haskell.org/Why_not_Pointed%3F)
It was a brilliant conference this year. And for the first time, when the show of hands for "who currently has a Haskell job" went round it was probably near 50%, perhaps even over!
I'm sad I missed this year. But making two days and twice as expensive meant I couldn't go :(
Lot of packages bring different meaning to the version number. In hackage there packages with version a la semver (A.B.C, like 5.6.2) and others a la pvp (W.X.Y.Z, like 0.2.1.2). As I see it, the problem is that the policy is not enforced. Maybe it would be better to keep the PVP-like schema, with versions W.X.Y.Z, and describe to the user the meaning of each number (W='the real version/epoch, so use can say libblabla version W', X='API breaking changes', Y='API compatible changes', Z='Just fixes, API stays the same'. That way the meaning is clear. Also the compiler/package manager/tool should be able check programmaticaly if API changed in any way (like in ELM), and ask the programmer to increase the correct number or revert the API breaking changes. 
Created a throw-away account. username: haskellit password: the name of this subredditincluding the /r/ prefix
[groom](https://hackage.haskell.org/package/groom) is my go-to text-based visualiser these days. Quick and Dirty. On a related note, I'd really like a way to visualise the type-relationships as a graph.
&gt; Well that's an embedded real-time system, that's a totally different ball game. Aside: I don't think you're right. I mean "Mars Code" is *all* about correctness, so...?
Have a look at [Gudhi](https://project.inria.fr/gudhi/software/), I believe it is the state of the art in this area.
Some tighter control over resources and memory representation is the big thing. One of the big problems with Haskell in performance critical code is understanding and controlling scarce system resources like memory. I think edging slightly in the Fortran/C/Rust direction would gain us a lot and can be done while sacrificing very little.
http://bugmenot.com/view/skillsmatter.com yeah!
http://www.amazon.com/Functional-Differential-Geometry-Gerald-Sussman/dp/0262019345 is something I keep meaning to find a nice way to port to Haskell.
Great talk, thanks for sharing. He makes a lot of great points.
So, when space leaks caused seq and strict stuff to have to be added we all became liars... but at least our programs worked. :)
There are already a few libraries that offer matrices with their length in their type, including *linear*, which gives: V n a as a vector of length `n` containing things of type `a`, so, for example: V 9 Double is a vector of 9 doubles. This is just a newtype wrapper over *vector* from `Data.Vector`. A matrix would then be V m (V n a) But there's no "directly encoded" matrix in *linear*. As far as the types go, adding lengths to your types is one of the *tamest* possible things you can do in Haskell with the type systems...it's just phantom types, which have been in Haskell for forever and is typically introduced pretty early in Haskell education (although in this case, it's a phantom type that's not kind `*`, but the exact same principles apply)...so basically no magic at all. A simple wrapper over a vector would just be: newtype V (n :: Nat) (a :: *) = V (V.Vector a) And defining `Functor`, `Applicative`, `Foldable`, `Traversable` instances for it already gives you almost everything you'd ever want to do with a vector that's polymorphic over its length (get a sum, zip two vectors with a zipping function, turn it into a list, map a function over it, etc.). If you already had a `Matrix a` type that is a matrix of a's, you can do a simple newtype wrapper over that too: data M (m :: Nat) (n :: Nat) a = M (Matrix a) So a 9 x 3 matrix of Doubles would be: M 9 3 Double You'll just need to lift all of your functions to the newtype wrapper. Adding length to a type is just a simple phantom type. Note that I've chosen to make the `a` (the type of the contents) the last parameter, so you can write Functor, Applicative, Traversable, Foldable, etc. instances. Writing things like `product` is pretty straightforward in this way. The part where this gets a little complicated is when you need to do some type-level arithmetic, like a `concat` function that concatenates two matrices side-by-side, for instance, which involves type-level addition. You might also want to write a compile-time safe indexing function. This isn't *impossible*, but it does make it slightly more advanced than the "just a plain ol' phantom type" technique that has more or less nothing going on at the type level. For that, type level numeric literals might not be the best, and some people start referring to peano-based numerical types, etc. etc.; but that's a whole other story :)
Do ```Matrix a m n``` means Matrix of type a values with m rows and n columns ? If that's it you can't do that without using some terrifying black magic. m and n have to be type (you're in signature) and here you want them being data. What you want to add to your spellbook is called dependent types, you can get more info here : https://wiki.haskell.org/Dependent_type . 
[removed]
The flattening of nested data to flat data using vectorisation works fine, but the fusion methods we have so far quite inadequate for the vectorised programs the transformation produces. The DPH team is now focusing on coming up with better fusion mechanisms (in the context of flat data libraries like Repa and Accelerate) and eventually DPH will be revived once they get the fusion mechanisms down.
Correct me if I'm wrong, but this turned out to be something that could easily be solved with phantom types, right? (It took me a while to understand, haha.)
If you're going to use length-indexed matrices, you may find the [reflection](https://hackage.haskell.org/package/reflection-2.1/docs/Data-Reflection.html) package useful, especially this function: reifyNat :: forall r. Integer -&gt; (forall n. KnownNat n =&gt; Proxy n -&gt; r) -&gt; r This lets you bring a regular integer into the type level.
Less a cult, and more a culture: noun: culture; plural noun: cultures 1. the arts and other manifestations of human intellectual achievement regarded collectively. *edit* just read to the bottom and saw the Jonestown reference. That's pretty unpleasant and offensive.
you can't pass in something of type `9`...what values does it have? :P The type `9` has no values. You can use a `KnowNat` constraint from `GHC.TypeLits` eye :: (Num a, KnownNat m, KnownNat n) =&gt; Mat m n a so you can do eye :: Mat 2 1 Double If you know your size at compile-time. Or you can use the *reflection* package to pass in integers as types, like /u/mjmrotek mentioned. Instead of `identityMatrix` returning a `Mat m n a`, it'd return a continuation where the input is a `Mat m n a`: eye :: Integer -&gt; Integer -&gt; (forall m n. Mat m n a -&gt; whatever i want to do with it) -&gt; the result of what i did with it Yeah, admittedly, transitioning from the world of normal value-level numbers to the world of type-level nats is another not-so-simple part :) But once you're in the world of type-level nats, things should be as i mentioned earlier, heh.
Adding my two cents to /u/mstksg 's post - while Haskell doesn't let you pass types to functions (this might change with GHC 8.0 and [visible type application](https://phabricator.haskell.org/D1138)) you can use [Proxy](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-Proxy.html) or [Signletons](https://hackage.haskell.org/package/singletons-2.0.0.2/docs/Data-Singletons.html) if you find yourself struggling with GHC to accept your code when it can't figure out what types do you want. They're both data types that are parametrized by a type without containing a value of that type. The difference between `Proxy` and `Sing` is that `Proxy` has one polymorphic constructor (you can write something like `Proxy :: Proxy 2`) while `Sing` is a data family with one constructor per type. You can write functions like foo :: KnownNat n =&gt; proxy n -&gt; ... foo _ ... = ... to accept either of those.
Cool, didn't know that!
Precisely. We didn't use FlexibleInstances either. I shall leave the solution as an exercise to the reader. :)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/altjs] [Blog post series on the Elm Architecture in PureScript Halogen](https://np.reddit.com/r/altjs/comments/3ofi3h/blog_post_series_on_the_elm_architecture_in/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Excellent! Sounds promising.
Alonzo is our Church. Besides, every cause wants to be a cult: http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/ (My opinion(s) on the matter may or may not differ from those expressed in the linked post.)
Did you follow tutorials for any of these?
I didn't say anything about which one is better. To rephrase, features such as fault-tolerance, concurrency model, distributed programming can be easily qualifiable for the domain/needs. Purity and the ease of reasoning is not that easy to qualify and is usually just a preference and very anecdotal. Yes, the benefit is what I'm referring to. There's nothing that empirically shows that purity, especially *enforced purity*, is better than impurity. Some people love the enforced purity of Haskell and some don't. I personally like purity, but I'm not completely sold on the enforced purity.
Cognitive benefits are hard to showcase, when you're preaching the language to others. Take "easy to refactor"; you have to go through a few examples, show that purity allows extraction or substitution of whole blocks, and code reuse. On the other hand, I can't explain to myself why OOP is "easy to pick up", either. Complexity and complicatedness are not the same thing.. I guess there's a lesson lying in here about the perceived value of here-and-now getting-stuff-done versus maybe-someday ease of refactoring
Speaking seriously, the use of metaphorical language appears when the brain reuse the same circuits for different domains. If you think that your language is your religion, it means that you reason about your favorite language as people reason about religion. That is very bad for the programming language since religious thinking has his domain where it is ok, since it is made for creating the wider community where we live. but it is disastrous when used in other context. Among other things religious thinking tend to establish and obey the religious authority without questioning. tend to elevate the practical concepts of the domain to a sacred level where they become ends and not means. By division of labour, it tend to leave understanding of the doctrine to enlightened people "pastors" and not to oneself. It tend to keep the doctrine in dead languages and do not renew the message. It creates barriers to newcomers in order to increase the division us-others. Tend to differentiate and despise the others. Tend to demand internal sacrifices in the form of spurious, ceremonial things that are not really necessary. tend to create a sacerdotal class of the ones who know. It tend to waste time in minute discussion about details that have no real impact in the real world. Tend to fix the attention to some historical problems and despises others. tend to deny his weaknesses etc All of these elements are typical in a primitive religion and are there in our psychology for evolutionary reasons. And unfortunately they are present in some degree in all programming language communities. This is probably the season for the failure o many of them. Better adhere to a true religion instead of making a religion of everything one enters in.
I've scanned it just now; it feels similar to my approach, except that he doesn't seem to use datatypes for intermediate results. Is my approach a bit over the top in that regard? (I prefer explicit types everywhere TBH)
However kids do need to know that * has "higher precedence" than + . Sure, asking _why_ is a subsequent step very few are willing to make, but I was referring to grown-ups who hopefully can be spared fluffy metaphors. Just because a concept doesn't have a prompt "real world" metaphor (how I hate this term) it doesn't mean we should rule it out altogether from an introductory explanation. When writing tutorials, we can show a few examples, and then this language lets us abstract away their common denominator, and give this object a name.
Very interesting! as a proxy indicator of popular packages there's also a dependency index here: http://packdeps.haskellers.com/
I heard they're on their way
Less than 100 queries per month? That's it? Zero searches from my native South Africa? These numbers are as dodgy as hell. 
The better article IMHO is [Evaporative Cooling of Group Beliefs](http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/). &gt; when a group ejects all its voices of moderation, then all the people encouraging each other, and suppressing dissents, may internally increase in average fanaticism. &gt; This is one reason why it's important to be prejudiced in favor of tolerating dissent. Wait until substantially after it seems to you justified in ejecting a member from the group, before actually ejecting. I think we do OK... on IRC I have noticed a culture of trying to rationally convert even obvious trolls as being a source of pride. &gt; The flip side: Thomas Kuhn believed that a science has to become a "paradigm", with a shared technical language that excludes outsiders, before it can get any real work done. In the formative stages of a science, according to Kuhn, the adherents go to great pains to make their work comprehensible to outside academics. &gt; But (according to Kuhn) a science can only make real progress as a technical discipline once it abandons the requirement of outside accessibility, and scientists working in the paradigm assume familiarity with large cores of technical material in their communications. A possible alternative reason why some see Haskell as a cult?
The number is not an absolute number of queries - it is a relative volume with 100 being the max. E.g. compare https://www.google.com/trends/explore?q=facebook
`Data.Tree` will print an ASCI visualisation nicely, e.g.: import Data.Tree data MyTree = Branch Int MyTree MyTree | Leaf Int deriving (Show, Eq) toDataTree (Leaf i) = Node (show i) [] toDataTree (Branch i l r) = Node (show i) [toDataTree l, toDataTree r] printTree t = putStrLn $ drawTree $ toDataTree t 
The courses were certainly... polarizing, to say the least. I wouldn't call them or the professor in question bad as such - and one certainly learned some interesting stuff -, but the idiosyncratic teaching style wasn't everyone's slice of pie.
Why not just the [Haskell - Programming Language](https://www.google.com/trends/explore#q=%2Fm%2F03j_q&amp;cmpt=q&amp;tz=Etc%2FGMT%2B7) search term?
I think you're right, but I'm still can't figure out an example of something that works with vinyl's implementation but not mine.
You're probably not the right person to ask then, but do you happen to know how good a substitute for the Sequences it is? To whomever it may interest: I've decided to go with the book, supplementing with other's sequences from the website. 
Nice progress! Your code looks good, too. One note about Show instances, there is a convention that Show produce strings of valid Haskell code. I guess this ensures that the entire value can be reconstituted from the String, and that there is an unambiguous way to make a Read instance, if you want that later. For learning exercises, this is a non-issue. Keep it up!
Don't do FP101x, don't install Hugs. I teach a lot of people Haskell, I maintain a free guide that recommends what resources to go through in order to get the basics down here: https://github.com/bitemyapp/learnhaskell The IRC channel #haskell-beginners on Freenode is available if you need help. Install instructions for GHC are located at the guide as well. To answer the question, "Why not FP101x?", I wrote an article describing the issues learners have with various resources, including FP101x: http://bitemyapp.com/posts/2014-12-31-functional-education.html Some people find it difficult to dive straight into cis194 out of the clear blue (that's the first course recommended), I _am_ working on a beginner thru intermediate book for learning Haskell that has a free sample at http://haskellbook.com/. If you're a student or unemployed, contact me at my email address https://github.com/bitemyapp (it's on my github profile) and we can hammer something out (coupon or reviews in exchange for book).
Cults hide information. Progmamming communities disseminate information. Cults brainwash, Programming coomunities educate. Cults cling to dogma. Programming communities evolve and assimilate ideas from other communities. No, your favorite progmamming language is not a cult.
I use it. It's pretty cool. Lots of packages installed. I've read on a HN thread about limitations such as no file/network access is available, but I'm at advanced hello world stage. :) These limitations will be removed in future versions. Sandboxing just makes it a bit harder, you have to make some sort of manifest requesting the access you need.
I'm a true believer but have still ended up in cabal hell.
I just downloaded the Haskell platform (7.10.2a, 64bit) for my OSX El Capitan. The installer went fine, but when I execute ghci in terminal I get the following: -bash: ghci: command not found I've come across some solution proposals, but I've never used symlinks. Is their a straightforward way to make it work? 
A couple things to check. * have you reset your path hash after the install? Type hash -r * what is your PATH set to?
That looks really cool, and reasonably priced as well. I'll definitely check it out. 
http://liyang.hu/church.xhtml
The classic here http://liyang.hu/church.xhtml
If you run ghc 7.8 or earlier you'll be fine. If you run ghc 7.10 you'll also be mostly fine, but there will be a few roadbumps due to changes -- roadbumps people will probably be willing to help you through on e.g. irc. Assuming Meijer's tweet was serious of course, which it may not end up being (precisely due to the lack of ongoing support for installing hugs on a variety of systems that in theory _should_ be). Ghc 7.8 can be run for free, in a terminal, online, with an account at sagemathcloud (https://cloud.sagemath.com/)
I am very sketptical on this, is really a course given by Erik meijer a bad course? I'm subscribing there only to prove you wrong1!11!!
Are there standalone slides of the talks? When code on the slides is discussed, sometimes the camera view is big and the slide view is small. This can be quite annoying, because text in the small view is mostly unreadable. I think it might be better to have the slides permanently in a readable size, instead of switching back and forth. Apart from that, there are a lot of talks I'm excited to watch. Thanks for making the effort to record them!
His course is on Bird and Wadlers Introduction to Functional Programming for beginners, and not on Haskell. I swear I must be missing something about this whole shitstorm on Twitter, because I cannot believe that a group of people can be so downright disrespectful to someone else because they dared not use the 'right' version of Haskell to teach their course. Haskell is not the beginning and end of Functional Programming. 
That hit a little bit too close to home.
This must be one hell of a book.
source?
Here's an email (partial snippet!) we got from a reader recently: &gt;Despite it still being in development with many expansions to come, my experience with the current 800 pages of Haskell Programming has quelled any skepticism I had about its thoroughness or potential use to someone who had already spent two very devoted months studying Haskell. Having read books, blogs, and participated in Haskell's IRC channels during all of that period, thus covering most of the topics that are in the book, I felt like this would end up being more of the same: something that's useful (retreading over similar ground genuinely builds skill and understanding), but perhaps not an optimal use of my time (there are plenty of resources that already exist with which a person can do that, and I felt most suited to 'intermediate' resources). &gt;Yet, Haskell Programming is so damn comprehensive, and so well structured, that it provided me with something that no other resource has been able to: an assurance that once I make it through a chapter, I will have learned the most critical things about its topic, and that anything which demands further elaboration ought to (and will be) addressed later on where it can be more deeply understood. So far the book has been very thorough about addressing each topic in as much depth as is sensible. I was tempted to skip the earlier chapters, but I'm very glad I didn't, and that the book clearly expresses why doing so is a bad idea in its introduction. Though it is targeted towards beginners, it ramps up at every chance it gets; it just doesn't want to make you lift weights before having you stretch. It is neither simple nor inaccessible, which is about perfect for learning. 
Just FYI, if your goal is to learn Haskell, rather than general functional programming concepts, then Meijer's course isn't a great place to start. He's said repeatedly that his goal is not to teach Haskell, but to teach concepts you could use in any language, from PHP to C++. I will leave for others to argue about whether or not that's a good idea (Meijer himself has written a paper about why it's not, but w/e), and I don't know what your specific goal is (FP concepts in PHP, or Haskell per se). If your goal is Haskell, this course isn't what you're looking for, according to Meijer himself. He may be kidding about Hugs (only time will tell) and he may be kidding about switching to Miranda, but what he's been clear about is that he isn't aiming to teach anything language specific. 
&gt; The cases are not so different, although this is a point you did not acknowledge on Twitter either. I apologize for not responding to that point - I thought it was somewhat clear that I agreed with it. Of course, I wouldn't think that you were successful at teaching me the language. If, however, you were giving an introductory course on linguistics, and using that ancient language as a medium for it, then I'd be all over it. 
Erik got me into FP through his educational materials so I certainly respect his teaching abilities.
Hmm... That's Not entirely obvious, especially on mobile. But thanks for the explanation. 😊
[removed]
Sure, my main objection is in the conflict with it being represented as a way to learn Haskell when it isn't. The problem with using Hugs in that case is that people will still sign up for it hoping to learn Haskell, but Hugs isn't that useful for that purpose. I have an interest in pedagogy per se, so slight tangent to follow. Caveat lector: FWIW, I've taught intro to linguistics, and we usually don't do that anymore (it used to be done more often, esp back when people thought Latin was more "pure" than English). We usually introduce concepts of phonology, morphology, and so on, using the students' native language because it's easier for them to grasp the concept from the concrete (that is, the language they know) to the abstract. It takes a while before you get to, for example, a level of syntax so abstract that it no longer matters what language you're talking about, and students often have a very difficult time making the leap to that level of abstraction. So what I'm saying is, based on that experience, I do think it's worthwhile to learn one functional programming language really well *first,* before you try to generalize the concepts. And I say that despite the fact that my interests in Haskell are much less practical than many students' would be. That is, I have little personal interest in making a web app in any language so those practicalities of Haskell aren't that exciting to me. But I understand that most of the people who buy [our book](http://haskellbook.com/) want to do practical things with Haskell per se, so that's what the book must be about primarily. We will ask them to write enough list folds that the idea of generalizable catamorphism appears intuitive; we will ask them to write enough Monoid instances that they start to see monoids everywhere. But they have to write list folds and Monoid instances *in some language* to develop those intuitions (ime), just as my beginning students of syntax drew many trees of English sentences before we started talking about we started talking about using X-bar to signify arbitrary lexical categories. That's my two cents from the trenches of teaching. /tangent
&gt; look at the number of defunct Linux companies. ...which of course is why linux has died?! You have so many conspiracy theories.
I think a lot of stuff on OS X has been broken in the past few years by changes to gcc. I've never had any trouble installing gcc -- I'm sure that will be fine. I'm just going to go with the older version.
:( That feel defeatist to me. Little printf didn't learn what to do, only what not to do. And "become a programmer with a human face" doesn't count as advice. It's both unclear what that actually means and how one does it.
I agree with all of that, and it's sounds like you're expertise in particular will make it a very interesting read for not just beginners but for the intermediate and advanced as well. I respect the efforts of the authors of this book, but I don't have any respect for the two hours spent berating, name calling, and the overwhelmingly unprofessional attitude of some of its authors on twitter. The fact that it was happening to Erik Meijer by people who are, or will be, presumably, marketing a book on the subject made it all the more bewildering.
&gt; The issue we had is that he is needlessly portraying Haskell in a bad light by using Hugs when he could equally well use ghc-7.8 instead. This would also cause his students fewer problems. What problems? Clear error messages? Quick download?
Fair enough. As a teacher, though, I don't have any respect for teachers who, when you raise objections about their pedagogy that you could back up with research and data, say, "well you can't please all the people all the time so I don't care." That's not a professional attitude, and I find it even more disappointing from a luminary such as Meijer. Perhaps on this point we will have to agree to disagree. 
&gt; There are exceptions for even the worst materials, but I am tired of the burnout rates of Haskell learners and I will not stay quiet just because the dude has a PhD. Calling him names and making jokes at him like you were in grade school will surely show him the error of his ways. Fuck him and his PhD, amirite?!
What names did I call him? I was quite sincere when I asked that he consider using Miranda and offered to write the install instructions to ease the migration.
I think that I can unscramble part of the edward kmett comment: if you use builders instead of lists, everithing will be faster. builders compose using monoid. You can see them in Bytestring as well as in libraries like Blaze
&gt; Over the years, your initial enthusiasm has waned - damped by the harsh reality of legacy-code-clusterfucks and AbstractSingletonBrainTumour factories. We’re here to tell you that it doesn’t have to be this way. New programming delights and delicacies await you in the Church of Haskell. Then your enthusiasm wanes as you have to implement the same thing multiple times, a functional version once and a monadic version next, and you don't want to simply write everything monadically at the outset because it's butt-ugly. Records yadda yadda, too, and random variables, and the unsafety of laziness. You see something neat like [HLearn](https://github.com/mikeizbicki/HLearn) being done in Haskell, using common-sense group structure to [speed up cross-validation](https://izbicki.me/public/papers/icml2013-algebraic-classifiers.pdf), and what do you find when you enthusiastically peek under the hood? A [rewrite of the Prelude](https://github.com/mikeizbicki/subhask), that's what. Apparently, Prelude wasn't good enough so a new one was needed. Not the first person to express this sentiment you've seen, either. Brings to mind the bad old days when everyone rolled up their own string implementations for C++. People also tend to use concrete types everywhere instead of expressing things more generally. If it was just that they didn't descend into the abstract nonsense of category theorists it'd be fine, but even in more common languages people use abstract interfaces for data structures more than in Haskell. You start to feel like a fossil when people around you seem to think that abstract data types are "them new-fangled things we don't need", and "good ole' list is good enough for anyone". Ah, verily, your shiny progressive cult seems to gather moss and lose its luster! Oh, and calling C libraries (FFI) is such a royal pain, now that we got on a roll to complain! So yeah. At times a new thing comes along and reignites the spark, but it never lasts, not in this imperfect world. Nothing is gold that glitters, all new wonder is soon lost. Every cultist eventually finds that their idols have feet of clay, in every cult, if they live long enough. (not a guarantee depending on what the Kool-Aid was spiked with though..)
wow! you should definitely advertise that, I don't think there are too many implementations of F4/F5 in *any* language!
Yes. And "human face" is often just a failure mode of those who can't quite internalize the notion that many jobs provide great social utility without being frequent and direct sources of warm and fuzzy feelings for the people doing them. 
Not quite, it is more that strictly more programs can terminate, the asymptotic benefit for many of the ones that do is just gravy. =) The major examples I was aiming for would be taking a snoc-list and folding it. It isn't a terribly good example because not all of the combinators in Foldable will work, but many will. Builders are still trying to spit out all the text you have to pay for all the parts. On the other hand, lets try something different. I have a data type for LZ78 compression. http://hackage.haskell.org/package/compressed-3.10/docs/Data-Compressed-LZ78.html The `Foldable` for that uses a number of calls to mappend proportional to the compressed size of the data, not the plaintext. Similarly, how do you compute x^1024? I hope not by left folding x * x * x * x * x ... 1024 times., but rather by x^2 = x * x, x^4 = x^2 * x^2, etc. You can share intermediate values and use 10 calls to (*).
If you'd like a serious answer to that question, here's a checklist: http://www.csj.org/infoserv_cult101/checklis.htm Further reading, the "BITE model": https://www.freedomofmind.com/Info/BITE/bitemodel.php
Well, there's [me](http://lesswrong.com/lw/l0d/a_proof_of_l%C3%B6bs_theorem_in_haskell/), at least :-)
I think you miss the point a bit: if you want a low stress, easy to set up Haskell env to learn on, this almost certainly fits the bill. If you're happy to tinker around and bang your head on solid objects (which is very much the way I leaned both Linux and Haskell) download form source and get cracking. :-)
Yes, `hash` is a builtin part of bash. I recommend this article for more background: http://unix.stackexchange.com/questions/86012/what-is-the-purpose-of-the-hash-command If you go to the terminal and type: `echo $PATH` It will print out a `:` delimited list of the places your shell (bash in this case) will search when looking for programs. The hash thing I mentioned in like a cache that tells the shell what it saw in a given directory the last time it looked. Unfortunately, if the contents of that directory change, the shell may not notice. Typing `hash -r` tells bash to throw the cache and start over. This is commonly needed when you install new software. It could be that there is some weirdness with El Capitan. I guess it has a new security feature that might be causing a problem, but it's good to rule out the easy/common things first. I hope that helps!
Sorry if I'm treading old ground here, but it always seemed to me that if `Set` as an abstract data type represents an _unordered collection_, then `Foldable` is also incorrect, since `Foldable` implies an ordering. Can someone explain why `Foldable` for `Set` is legit? It seems to me like `Set` only satisfies a variant of `Foldable` where e.g. `foldMap` expects a commutative monoid.
Are you bitemyapp on irc? 
Feedback we didn't solicit we usually receive via the zendesk email address at http://haskellbook.com/support.html Thank you for your thoughts and forthcoming feedback, this stuff helps! :)
But `Set` gives a collection with a _possible ordering_ (you can tell by the `Ord` constraint!). Foldable just happens to pick an ordering, which is legit.
* IDE or text editor support * REPL support * Inability to Google error messages * This thread
[removed]
Yes! :D (I'm actually not very happy about "seq" or "parseq" (or whatever it's called) if we're being pedantic in that they actually *do* mean that semantics change. I just want *evaluation* semantics to be separate from *language* semantics... in a meaningful way.)
Here are some examples of the sort of thing I mean. * tighter control over data representation (e.g. what's a pointer, how are fields packed, what's on the stack vs. the heap, etc.), where lower level features are opt-in when they would otherwise be annoying * some sort of linear (or some other appropriate substructural) typing to use for non-memory resources * opt-in linear (or some other appropriate substructural) typing for memory (still GC by default, seems fine to me)
For purely nostalgic reasons, I finally got a version of Hugs98 to build (on Mac OS). I will be putting it up on GitHub for the curious. However, I do not recommend that anyone use Hugs98. The day GHCi came out way back when, I no longer had a good reason to use Hugs98. I stopped using and maintaining my own Hugs98 builds in 1999 ftp://distrib-coffee.ipsl.jussieu.fr/pub/linux/mandriva-prehistory/6.0/i586/index-html/hugs98-990222-1.i386.html and have rebuilt it now only because it is important to remember history.
FPComplete is [retiring](https://www.fpcomplete.com/blog/2015/10/retiring-fphc) the FP Haskell Center. So thats not an option any more. 
It's fixed now. Thanks for pointing it out/reporting it/providing the correct link.
It's just a type variable here, it could as well be `p` or something else. If you write it this way, both `Proxy n` and `Sing n` would fit; this also makes it clear that the function is not going to evaluate the argument, as it's impossible. But it's all just a side note, I'm pretty sure that in your `identityMatrix` function you wouldn't actually need a proxy argument. `m` and `n` could be inferred from context or guided with a type annotation; proxies are useful for more contrived types.
Thanks for the links. Yeah, I started with the very basics as well, even reading some category theory, which I thought was really helpful. I have been practicing Haskell at [code wars](http://codewars.com), but now I'll give some small projects a shot.
yeah, that would be amazing. thought I read that laziness complicates linear types because it's hard to tell when some data isn't needed. (... or was that regions?). unbox-strict-fields already helps with data representation. and /u/edwardkmett gave a talk about this on library for removing pointer indirection from arrays.
I've been working through the book, and often I want to link someone the post about what I've read and found it to be nearly identical to what was posted on lesswrong.com . Because of the minimal editing, lots of the chapters overlap with each other, and this is much less noticeable when you're working through the posts and they're not right next to each other. The big benefit and drawback is that you get a single canonical sequence. So it's a more approachable starting point but you miss some of the community-provided sequences on other topics. Even then, the book structure has been [replicated on the wiki](http://wiki.lesswrong.com/wiki/Rationality:_From_AI_to_Zombies). HTH.
Your critique is valid, but these questions of meaning, fulfillment, and redemption are tough questions. Most philosophers have gone the same route: it's easier to say what *not* to do than posit some fixed path toward meaning.
Should we just discontinue the Haskell Platform? It seems like it was only sticking around for Windows users for a while, but now `MinGHC` and `stack` solve most of the issues there.
[removed]
You could start every single module with import Prelude () import Haskell2010Prelude But even if you did, you still must use the **Monad** from Prelude, as the compiler will use only it. So once it has **return** stripped, and Pointed and Semigroup inserted... you're stuck with all of that. Further, libraries written to work with 7.10 and beyond have APIs designed on the presumption of FTP and beyond - so you can't avoid the idioms it introduced. So, better to just stick to GHC 7.8, its version of base, and libraries that are compatible with it. It makes a cohesive set to code for.
I feel for Mark; I haven't been very involved in the Haskell community since the FTP change, mostly because I wanted to avoid the miasma that was being promulgated (and I wanted to avoid contributing to it as well). I hesitate to make this post, but I thought I should share a few of my thoughts (which may differ from or mirror your own experiences). But I have felt that the FTP change and the recent "monad of no return" proposal smack of an obsession for atomizing interfaces to their most "perfect" fragmentations. I had two concerns: 1. On the "practical" side, it makes writing simple programs that much more of a rabbit hole. Now, to be fair, this is kind of a nice way to learn stuff, but it can be a bit frustrating if you're new to the language &amp; libraries and just trying to get something done. I kind of liked Gabriel Gonzales' recent post on [basic Haskell examples](http://www.haskellforall.com/2015/10/basic-haskell-examples.html). The fact that changes like these cause so much churn also suggests that having type class signatures be matched nominally rather than structurally may be a poor fit for the problem domain. I also wonder if the community would benefit by taking u/pigworker's suggestions more seriously about language-level utilities for evolving APIs over time. It is also less than ideal if you're trying to teach people how to do something, and the "generalized" type signature requires material that comes much later in the curriculum. 2. On the "theoretical" side, the way that interfaces get factored is pretty arbitrary (that is, for every FTP proposal, there's probably a "bizarro world" version that's equally convincing but totally different). So I would almost prefer an imperfect prelude that let people decide upon the style they want to follow locally in their own projects, rather than imposing a particular set of architectures from the start. Of course, one can always write a new prelude, so it is not a super serious issue, but it's something to think about. **Sure, these changes seem like a good idea now, but in another couple years, someone will come up with a great blog post about how moving `return` out of `Monad` is Considered Harmful, and the wheels will begin to turn again.** We're just gerbils... I often hear that parametricity and the correctness guarantees you get from it automatically justify this kind of change, and I want to acknowledge that parametricity is an incredibly powerful tool. But to be honest, the constant changes to the prelude seem less motivated by correctness concerns (since the Prelude was already correct!), and more motivated by an aesthetic program which is currently "trending". I'm not saying I think the changes are necessarily bad, but I think that the continuous bikeshedding is. The now frequent breaking changes to Haskell's prelude make me thankful that I am no longer a serious maintainer of Haskell packages, and that I do not pretend to teach people FP via Haskell anymore... I miss the kind of Haskell that would show up in a beautiful functional pearl a decade or two ago. Modern Haskell kind of bums me out. Maybe it's too serious, I don't know; it's definitely a lot more dogmatic than it used to be. I think Haskell has grown too old, and no longer provokes the sense of wonder which somehow ML still does for me, against all odds.
&gt; I mainly disagree with the implication that anybody in favor of these breaking changes isn't a "serious maintainer". I consider myself a "serious maintainer" and I'm strongly in favor them. I didn't mean to imply that at all, and I'm really sorry if it sounded like I did. Many serious maintainers are in favor of changes like this; I only meant to say that *I* am no longer a serious maintainer, and changes like this are one (of many) reasons; the other big reason is that I'm not using Haskell for work anymore, so I have no reason to work on Haskell packages.
Nobody has proposed putting `Pointed` anywhere in any basic typeclass hierarchy. And `Semigroup` would be a superclass of `Monoid` not `Monad`.
I think Mark was making a general point that doesn't rely on the particulars that you mention.
Sure. Just clarifying for those that might misunderstand what may or may not happen in the future.
I'm rather sad to hear this Mark. I'm also a bit confused about it, because I still fail to see what the problem is. I have a large chunk of production code at work (10k lines) and we simply decided to all switch from 7.8 to 7.10 at once and I spent the hour or so needed to get it working. I also have at least three GHC versions installed on most of my machines 7.6.3, 7.8.4 and 7.10.2. Most of my Haskell code compiles without a problem (other than a few warnings) with all three compilers. I usually like to get my code warning free, but with this FTP change I've been less pedantic about this. I look at or write some Haskell code nearly every day. I downloaded the release candidates for 7.10 when they first came out. I felt like I was part of the process and hence didn't find the changes scary and in hindsight, they haven't been.
The new minimal platform plans will bring it closer to just a "native way to install ghc and related key tools (i.e. cabal and stack)." Along with that it will ideally be able to retain some sort of role as providing a notion of a "blessed" set of "batteries included" packages, even if they don't get installed prebuilt in a global package namespace. The evolution of that latter part is not quite so obvious to me -- but people really _do_ need a better answer to "how do I regex" than "look at all the hackage packages". But certainly the way that this package-collection element relates to the "platform installer" will need some radical rethinking.
Yes - sorry, I was being quick and a bit loose in my response.
That's one possibility -- I'm not quite sure what will actually shake out in terms of this in the end. Step one is to just go minimal but retain some notion of the official blessed set of packages/versions that is usable by cabal and stack. What might actually get seeded or auto-installed in any particular "less global" way is I think open to a little discussion. Hopefully as we evolve the platform we can start to see what's technically possible and open that discussion up.
I doubt Pointer will reach base, and I (personally) already import Semigroup a lot. but yeah, any Prelude change is a big deal, hence the controversy.
I leverage this profitably with [Bloodhound](https://github.com/bitemyapp/bloodhound/blob/master/.travis.yml). Super easy, super nice.
Well, I don't particularly like Foldable or Traversable - so I don't like having them in all of the Prelude functions. Nor do I particularly like much of the other changes I've seen put forth. My own code projects are now North of 45k lines of Haskell. I do insist on warning free - so this would be a fair bit of ifdef'ing to do - but I'm sure you're right that it would be only some amount of time to make it all compile cleanly both ways.... And at some point I may have to for practical reasons. But for now, I'll just stay at 7.8. But the bigger point is that I'm not on-board with this direction for Haskell's base libraries. I think this is the fork in the road where Haskell lost the option of broad adoption. As such, I'm only making it worse by being the grumpy release manager!
Yeah, so from what /u/sclv says it sounds like it is moving towards being a nice installer for `stack` and `MinGHC`. Is there anything else you'd be interested for a nice user experience on top of that?
&gt; I only meant to say that I am no longer a serious maintainer, and changes like this are one (of many) reasons; the other big reason is that I'm not using Haskell for work anymore I think I'm beginning to see a pattern here but its rather hard to distinguish cause from effect. It seems people who have been more Haskell active in the past than they are currently are the ones that are most resistant to changes like FTP. Thats actually understandable, because the Haskell skills they developed are withering faster when the language changes than it would have if the language wasn't changing. However that should not be an argument against change. 
I have something Travis-CI can't provide, GHC on exotic architectures like PowerPC and Arm :-). However, I also do use Travis-CI for my public projects.
Even if you FlexibleInstances, I don't see how this is possible in general: two functions are equal iff they can be represented by the same set of pairs, which necessitates knowing the value of the function at every point in the domain.
Can you explain the FTP change to someone who isn't in the know?
Sorry, I tried to choose my words carefully. I really did not want to cause offense. However, I *know* my own Python skills withered very quickly (a matter of months) when I stopped using Python regularly. 
https://wiki.haskell.org/Foldable_Traversable_In_Prelude
Looks cool! I'll give it a try. The profunctors there are not compatible with the ones in lens, is that correct? The upside is that you avoid a good number of dependencies.
&gt; Support for compiling and running... Probably because Hugs isn't a compiler, it's an interpreter. &gt; Hugs has a REPL I'm glad you concede this now, but Hugs hardly has anything _except_ a REPL! You're coming across as being unfamiliar with Hugs. &gt; ... ghci treating the command line as an infinite do block 'n stuff ... Those are all things that I recommend that people do in a text file anyway. You don't even need to type :r in Hugs, you just need to save the source code again! It's a mistake in my view to let beginners program in that stream-of-consciousness way, and they just fail to learn Functor if you give them a get-out-of-IO-free card like that. Lest innocent readers take your #HugsFUD further than even you intended I should point out that you can of course add dependencies interactively in Hugs using commands, you know, using the key sequences ghci copied from Hugs! There aren't many places I would recommend Hugs above using stack to install ghc etc, but a first course in Functional Programming is one. 
Isn't Trasvis only for open projects?
Have you used primarily GHC 7.8 or 7.10? If you are using 7.8 or older you haven't been affected by it. :)
Ultimately the problems start piling up: A stack discipline is too restrictive, so you want linear regions. This necessitates linear/uniqueness types in the language. Second, laziness plays hell with object lifetimes. This means its nigh impossible to manage them with regions in the first place. Then the surface language lifetime also gets complicated because the closure has a different life than the thing it gets forwarded to, which widens the disconnect further. JHC was trying to do this with region inference in the RTS rather than in the surface language, but I'm pretty much of the mindset that it is a lost cause.
I'm a newbie to Haskell. What is the FTP and Monad change he refers to? 
&gt; Of course, one can always write a new prelude, so it is not a super serious issue, but it's something to think about. So can that vocal minority that wants to stick to the old ways... afaik all those recent proposals you complain about all had a majority support from the community. Should we really let the minority's desire decide what the majority has to endure? &gt; Sure, these changes seem like a good idea now, but in another couple years, someone will come up with a great blog post about how moving return out of Monad is Considered Harmful, and the wheels will begin to turn again. That's a silly argument. By that we'd be stuck with Haskell 98 forever, as every change could turn out to trigger somebody to write a "X considered harmful" blogpost at some point in the future, and hence would have to be rejected on that grounds. If Haskell gets to the point where we keep discussing changes, but everytime a minority vetos we punt on them, only to bring them up again and again, I'll consider Haskell a dead-end and just move on to a language which hasn't become a victim of its success yet, and does the right things even if it means that code will need to be updated. 
Is there anything actually bad about impredicative types? I've tried to look a little bit into the history, and as far as I could tell the dislike of impredicative types came from the scare of inconsistency in type theory. In the end that turned out to have nothing to do with impredicative types, but rather with Type:Type. Yet even now some schools of type theory researchers do not like impredicative types despite their beauty and power (e.g. Church encodings). Why?
Nice presentation. In all honesty, I think Shake has the potential to be one of the biggest upsets for GHC development in *years*, with a big chance of making development significantly better. I am eagerly looking forward to testing it in due time.
What's (or where is) the canonical learner's Prelude? If we don't have one already, the argument is unfortunately moot (I like the FTP)
Hmm. I wonder if there's something like BrowserStack for architectures? (Genuine question)
The claim that the type-class generalizations are harder for beginners is oft-repeated. Is it empirically tested? I've heard from lots of beginners that it was actually easier on them. Using a language that you can envision how it'd scale up to your future work (especially if you're experienced) is going to make things easier for many beginners. Using 1 language for both beginners and experts instead of a "beginner language" that people assume beginners want (and most of them reject!) eases things - the beginners can read more code and get up to speed more quickly. Everyone speaks the same language.
`Foldable` has laws relating it to `Functor` and `Traversable`, if those instances exist. It should fold over *all* the values of type `a` inside the data-type in some consistent ordering. Isn't this enough of a law?
The [OpenSUSE build service](https://en.opensuse.org/openSUSE:Build_Service_supported_build_targets) is probably as close as you'll get. I've been meaning to try and figure out how to get it working with GHC, but I haven't had time.
7.10. Either way I meant the Foldable change described above by kqr isn't that bad
It works for private repos if you pay them money. The free service for open source is basically an advertisement for that.
Ironically, the code in that image isn't crazy complicated, or doesn't seem like it is. I guess I'm improving? ^^^^tiny ^^^^victory!
So what happened with Algol 68? Judging from the Wikipedia article it was very complicated and disliked by several now famous people. And of course no one uses it today.
Those points are all very valid for students under a teacher. However, the vast majority of people will not encounter Haskell in school. In fact, let's take your arguments and flip them around: let's create a repository with a bunch of proposed Preludes and assorted exercises for both autodidacts and students.
https://wiki.haskell.org/Foldable_Traversable_In_Prelude
Two factors are really important from a language design standpoint. How stable your API is and how well you adopt to change. I am using API primarily for language library (aka Prelude)? C++ has enjoyed relatively decent success at preserving both of these simultaneously and many languages have been fragmented by major API changes. These factors point to major API changes being risky for the language. However the major point is that if the latest Haskell is going to include major breaking changes frequently then big companies won't be interested in an unstable compiler. And big companies investing in the language is a great way for it to grow. Not that I agree or disagree, just listing the logic. I honestly don't know whether stability or polish is best for Haskell right now.
As a newbie I don't see why there's a need for that. Even if somebody isn't familiar with polymorphism, the "here's what `Foldable` means and lists are `Foldable`" explanation will probably take less than 30 minutes for anybody to understand. The Prelude functions still do the same things. I'm probably not familiar enough with Haskell to see what the changes really mean. Help? :)
Oh! Good!
&gt; The claim that the type-class generalizations are harder for beginners is oft-repeated. Is it empirically tested? While it might be true for complete newbies, I don't think it is a problem for people coming from other languages. My not-significant empirical evidence of two (2) subjects is that I tell them that `Foldable` is a bit like `Iterable`, wave my hands, and we can continue with other topics.
Before bringing out the dependently-typed cannons, what about `Maybe`? Or some sort of a `NonSingular` newtype?
&gt; I think this is the fork in the road where Haskell lost the option of broad adoption. I don't think that's how it works. PHP, Javascript and Ruby are terrible languages (IMO) that have seen huge adoption because they provided the right stuff at the right time to the right audience. Other languages are marketed to success (such as the push for Java), or are required on certain platforms (Javascript, C#, Objective C, R). My point is that language features are not terrible correlated with adoption, and certainly much less than killer app, platform lock or marketing.
&gt; I'm not saying I think the changes are necessarily bad, but I think that the continuous bikeshedding is. That is the essence of the point and I couldn't agree more
Great to see this. Past changes to base and core libraries (network, time) have caused quite a bit of confusion and maintenance work. I'd love it if every change had an associated, blessed 'compat' package. That seems to be the easiest way to ensure backwards compatibility without using CPP. This means that adding new stuff to `base` also requires adding it to e.g. `base-compat`. It seems to give us the best of both worlds for some classes of changes: no maintenance burden for package authors, but access to new stuff quickly.
That is *not* an option, or at least you can't do it by changing the `Prelude` only. As soon as someone imports `Data.List`, they would get name clashes. You need not just a simplified `Prelude`, but a simplified `base` package.
I didn't saw that news. Thanks.
To add some anecdotal evidence to u/atc’s point above. I’ve been Haskelling for a couple of months or so and working through Learn You a Haskell. I did not find it a barrier at all – I noted that that some function type signatures did not match what LYaH said they should be, but things still worked so did not worry. And by the end of the book it was pretty clear what was going on..
On the other hand, some of these changes have been wanted for a long, long time, so be unstable rather sooner than later, perhaps?
"If it's not part of the type, it's part of the problem!" ;)
We already discussed this in the past to some extent, but I'd just like to point out that if we had a radically simplified `Prelude` not re-exporting so much from `Data.List` we could have avoided the current mid-flight state of `Data.List` (which was needed to reduce breakage to a minimum) As for the "different" `Data.List` you can currently `import GHC.OldList as Data.List`. With the recent cabal-level module re-export feature, it's easier now to write a shim-package providing a different view of `base`. See also http://hackage.haskell.org/package/base-noprelude for an example for the `.cabal` syntax.
Mark's complaint is not about difficulty in transitioning to a post-FTP GHC. It is about not wanting to do that, because he sees the changes in FTP as a step backward for Haskell, not a step forward.
&gt; Haskell has already enough power and abstraction. More than any ordinary practical programmer could need. Isn't the generalization to `Foldable` *exactly* the kind of abstraction Haskell lacks? Most languages don't need you to micromanage namespaces, but in Haskell, because of the lack of overloading, you do. FTP fixes this to some degree.
It is not and almost never is a zero sum game. Good error messages are *hard* because they stare the user directly in the face at all times. They are especially difficult for a language like Haskell. I'm sure there are lots of ways we could make the errors better. Although I argue it is *not* as easy as just "slap some colors on it", as you need actual *guidelines* and *principles* to work by. Unsurprisingly, it's not an easy task. Even making 'simple' improvements to the error language can be very tricky. Similarly, there was a lot of effort and thought was put into the proposals that have either happened or are in discussion now to the standard libraries. But at no point did this preclude *anyone* from working on error messages, or any other number of things. Indeed, many things have happened concurrently in the same time frame. The reason we have bad error messages isn't because our precious contributor hours ran out, which we only have X amount of among Y people, and they can be allocated to any project ever on a whim. It's far more boring and simple: nobody actually did the work, presumably because it is difficult and likely there are no incentives in place to make it happen, and we have other things to do given that.
There was [a concrete plan](http://projects.haskell.org/pipermail/haskell-platform/2015-July/003129.html) to address the global package db issue: &gt; - The global db has only the GHC packages &gt; - There is a package db for each curated set, Haskell Platform &gt; becomes one such set &gt; - Projects each have their own package db, much like sandboxes. and &gt; - By building and installing the Platform packages into it's own package &gt; db, users get the benefit of building and installing these common packages &gt; only once per system, yet can easily bypass them for any given project if &gt; desired. /u/sclv may know more about the state of that idea
The only way GHC 7.10 affects the `Monad` class is via the [(F)AMP](https://wiki.haskell.org/Functor-Applicative-Monad_Proposal) implementation. So yes, you can't write `Monad` instances anymore without also providing `Functor` and `Applicative` instances. The [MFP](https://wiki.haskell.org/MonadFail_Proposal) will add to this, by splitting off `fail` into its own `MonadFail` class. For [MRP](https://ghc.haskell.org/trac/ghc/wiki/Proposal/MonadOfNoReturn), however, there is a much less impacting transition scheme variant allowing code written without CPP to work across a wide range of GHCs (ranging back to at least GHC 6.12, and continuing to work for at least 5 years from now) unmodified. That would be at least a 10-year-wide compatibility window without any CPP (nor warnings, except optionally at the very end of that window). If a change with such a smooth transition scheme still doesn't meet your stability/compatibility requirements, I really don't know what else will, except for no changes at all, ever.
/u/sclv explains in the linked post that System F avoids contradictions by using the fact that the value level and type level are different. So how is the planned introduction of dependent types in a soon-to-come version of GHC going to work?
Right, that's a good reason, but I was thinking about the dependently typed context where you've already lost type inference anyway.
There seems to be a common misconception that `Foldable` means you **have** to use folds. But Foldable has a `toList` function to convert any `Foldable` to a list. So you can think of `Foldable` as `ToListable` and write: sum :: (Foldable t, Num a) =&gt; t a -&gt; a sum = sumList . toList sumList :: Num a =&gt; [a] -&gt; a sumList [] = 0 sumList (x:xs) = x + sumList xs For [instance Foldable []](https://hackage.haskell.org/package/base-4.8.1.0/docs/src/Data.Foldable.html#line-225) the definition of `toList` is just `id`: instance Foldable [] where ... toList = id So there really are no folds.
Care to elaborate on platform/version etc? It seems to render fine on Windows in Firefox 40.0.2
That's actually why we want to expose the data structure (say, via jSON): so an IDE can take hold of it and show it in a *sane* way. Instead of the recursive "x at pos (0,0) in y at pos (0,5) in z at pos (4,6)", you could visualize and colourize the location, highlight the actual type mismatch (if it's a type mismatch), provide suggestions, etc.
Thought it was a little silly until I got to exercise Mercury. Really great fun moving around the functions in the result composition list. I had to move the `map reverse` clause elsewhere, and then it, literally, clicked! Excellent job.
I'm a fan of FTP, and AMP, and I'm pretty down on the Haskell Platform (in fact, I'd pretty much just like to see it go away). But losing Mark and Mark's perspective is a sad loss. I'm not sure what, if anything, could be done to keep people like Mark and Erik Meijer on board, but if anyone does, we should do that.
Hm, interesting. You solved Mercury with `map reverse`? In this case, it is probably not the optimal solution (involving three functions). Thank you for your feedback!
Ubuntu, 41.0.1, retina display so default scaling is changed. Panel on the left covers part of the text.
It is so hard to have a --noverbose option to strip GHC error messages and make them as concise as in hugs? I know that the extra info is very valuable in some cases, but this can be switched on at any time. I remember to have delayed the use of ghc as much as possible because after having some fluency with haskell-hughs I still did not understand the ghc error messages. They were so intimidating. And then they were nowhere near the size or what they are now.
It's a sign we've entered the "plateau of productivity" phase ;)
https://hackage.haskell.org/package/union-find-0.2
Use free version of BrowserStack for quick tests
Perhaps you should phone home (with permission) the solutions and do some fun statistics:)
Those do follow as a consequence from `length . toList` and IMHO are not surprising at all if you are familiar with the respective `Functor`, `Applicative`, `Monad`, and `Traversable` instances for `(,) a` and `Either a`. Whether those instances could be removed is something /u/edwardkmett can probably tell you... :-)
I am very nervous about proffering an opinion here but I do find it surprising that the length of a pair is 1.
I used it for rust a long time ago, it worked pretty well. 
does this then reinforce the suggestion that there ought to be distinct functions for programming failures and exceptions?
In dependently typed languages, you want to steer clear of it because it is inconsistent (i.e. lets you prove false) in combination with (it feels like) any other interesting type system feature.
&gt; `concat :: Foldable t =&gt; t [a] -&gt; [a]` Randomly: why wasn't this `Foldable t =&gt; t (t a) -&gt; t a`? Which reminds me a lot of `join`, and for `[]` is the same thing.
Haskell is going to have contradictions. Haskell already has contradictions, so adding more isn't really going to break things. In particular, it has `undefined` and `Any`, which are non-starters if you want a consistent logic.
Hi gbaz, I downloaded the the haskell platform from the link that you provided above. Typing ghci to terminal, I get -bash: ghci: command not found I tried hash -r but it didn't help. Could you please help me to install the Haskell platform properly? 
To clarify things a bit: impredicative types/impredicative instantiation in Haskell and impredicative universes are completely different topics. Impredicative instantiation is the ability to instantiate type variables with polymorphic types. This is quite trivially and commonly doable in dependent languages, where polymorphic types aren't really special, although the instantiation itself is often explicit and manual because of inference limitations. Impredicative universes, in contrast, refer to universes where quantification over some type is also in the same type. Some people dislike them because it's hard to get an intuition for why they should make sense. The most convincing thing for them making sense (that I've come across) is the proof in the [HoTT book](http://homotopytypetheory.org/book/) that excluded middle implies that the propositional subsets of types on different universe levels are all equivalent. 
`Foldable` is not enough for the signature you propose. To implement your version of the function, you'd need one of two things: 1. A way to construct a new structure `t a` from thin air and fill it with content, or 2. A way to "smash together" existing `t a`s into a single new one. Neither of the two are something `Foldable` provides. The first one is something like `Monoid` and the second one is something like `Monad` (as you point out) or `Alternative` (a variation of `Applicative` that lets you smash things together.) `Foldable` is for folding a structure down to a value of a known type, not creating a new structure (that'd be an... `Unfoldable`? A `Cofoldable`?) It *could* be Foldable t =&gt; t (t a) -&gt; [a] since `Foldable` does provide `toList :: t a -&gt; [a]`, but! I dug deeper and found out that we already have asum :: (Foldable t, Alternative f) =&gt; t (f a) -&gt; f a which specialises to asum :: [[a]] -&gt; [a] and does what you expect it to. Then `concat` is listed in the haddocks as "a specialisation of `asum` where `f` is `[]`", so that explains why `concat` itself is not generalised: it's meant to be a specialisation. As to why we need a specialisation... your guess is as good as mine.
I had a little conversation with Michael. The FPcomplete IDE was developed in a pre-stack, pre-docker, pre-ghcjs time that makes it hard to maintain and evolve. For this reason among others it is being discontinued. It is a pity because I used to share code with people using the FPcomplete IDE, ready to be executed and played with that is impossible with a desktop IDE. There are other kinds of collaborative coding that are impossible without a web IDE as well. The new School of Haskell however has evolved https://www.fpcomplete.com/blog/2015/05/school-of-haskell-2 And Michael suggested me that it can be taken as starting point for a new IDE. However FPcomplete does not plan to develop it. They can offer some support. It is a question of some brave developers to take the School of Haskell (SoH) code and rebuild the IDE upon it. SoH has much of what is necessary in the server and the client but SoH is mostly stateless, and there are much stateful stuff to do for the IDE, to keep project/user state. https://github.com/fpco/schoolofhaskell I would happily collaborate with my scarce knowledge of the yesod platform, but it is not a trivial task and some yesoder is needed to lead the task... 
This. I can get the libraries I need out of apt instead of a big bundle of ones I might need.
&gt; if you are familiar with the respective Functor, Applicative, Monad, and Traversable instances for (,) a and Either a This is the heart of the problem.
For my students and other newcomers I interact with, it's about error messages. "Not a list" they get "no instance for Foldable" gives two new concepts they need to learn.
It's in the Haskell report ;)
My biggest issue with FTP is that it *isn't* a change to Haskell (yet), but only to GHC. As such, it ought to require a language extension declaration (since it's extremely incompatible with Haskell). Basically, there are many valid Haskell programs that newer GHCs can no longer compile. Luckily we have older versions for now...
That's silly-talk! I'll give you a hint: show f = case f 2 2 of 4 -&gt; "Plus" 0 -&gt; "Minus" See? Easy as pie! :)
&gt;This comment was from one of our most senior engineers with many years of industry experience, after maybe a couple of weeks or so of exposure to Haskell. Did they have any experience in strongly typed functional programming, though? Because if not then it's effectively learning programming again, so their reaction is no surprise.
For those of you not subscribed to the `ghc-devs` mailing list, here is Simon Peyton Jones' beautiful [reply](https://mail.haskell.org/pipermail/ghc-devs/2015-October/010070.html) to Mark which also is pertinent for this discussion: &gt; Mark &gt; &gt; You have given a lot to the Haskell community through your steady leadership of the Haskell Platform, and I want to add my personal thank-you for that. The Haskell community flourishes only because volunteers step up to the (not always comfortable) tasks of developing consensus and then actually getting the job done. You have done this brilliantly – thank you. &gt; &gt; I know that your concerns reflect those of many. In the olden days when Haskell was a university research language, we could change it whenever we wanted and no one minded. But now it is used for lots of things, and people rightly complain about changes. Moreover, thoughtful and intelligent people differ in their judgement about what is and is not a good change. &gt; &gt; These are nice problems to have: they reflect a large, passionate, committed community of people who care about the language, its ecosystem, and its users. &gt; &gt; But of course they are still challenging problems! There is a genuine tension between innovation and change (which make Haskell so dynamic), and dependability and stability (which make it useful). &gt; &gt; I’m sure we will not always get it right. But it is my earnest hope that by respecting genuine differences of judgement, by being willing to see the world through others’ eyes, by being willing to accept a choice that is not our own – in short, by expressing true respect in our dealings with each other – we will be able to work together on a journey in which none of us knows the True Path. &gt; &gt; So I’m very sorry to lose you as the driver of the HP train, but do hope you won’t get off the train altogether! &gt; &gt; With true thanks &gt; &gt; Simon
What's it from? Edit: Oh, Hask. I thought so.
That is essentially the "minimal platform with cabal and stack" discussed elsewhere on the thread -- or rather an elaboration of it with an extra layer of shared "curated set" packages. The first step is just to "go minimal" even without the sharing of curated sets, because going minimal is technically straightforward and there are a few branching paths at the curated set step. Even with the first step though, we get the "clean" global package db as requested. We just don't yet recover all the efficiency of sharing (but in a safer way).