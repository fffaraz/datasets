From the github readme page "Cabal-dev will create a default sandbox named cabal-dev in the current directory[...]". If you ever used rvm, you know that rvm works system wide: you just type "rvm install 1.9.3" and rvm will compile and install the new version in its directory (probably .rvm on Unix). Then you can switch back and forth installations with "rvm 1.9.3" or "rvm 1.8.6". You can even set the default installation "rvm --default 1.9.3". The manager will take care for you of symlinks and other stuffs, so everywhere you can just type "irb" (ruby interpreter) and work with the preferred version. I think would be like if the same would apply for Haskell. Ah, and I'm not a Rubyst or a Ruby developer, though. But I think that rvm is very handy :)
cabal-dev allow you to install isolated packages for a specific project. The main difference is that RVM lets you install and use multiple ruby implementations under the guise of a single 'ruby' command. So you can use and switch between ruby 1.8, 1.9, JRuby, Rubinius etc on the fly and per project. This is actually quite useful in certain circumstances, and makes it easy to test out compatibility between multiple versions of Ruby. Presumably a GHC equivalent would mostly manage and provide the ability to flip between different GHC versions, and their installation/uninstallation. You can basically emulate this with cabal-dev + multiple versions of regularly installed GHC (the *latest installed* GHC on a system overrides the 'ghc' command, but there are also commands relating to the specific versions, so there's ghc-7.0.4, ghc-7.2.1, ghc-6.12.3, etc.) Of course an equivalent would allow you to seamlessly flip between multiple versions which are presumably managed transparently to you, the user - so you'll never need to 'make install' a GHC binary distribution.
Ah ok. I think because haskell is compiled and ruby is interpreted, there is a distinct difference in the required environment at runtime. Haskell you only need a binary that can run (so static/dynamic is the only real twist here), ruby you need all your libraries/interpreters etc at the right versions. Having multiple versions of ghc(i) installed to cross build a single app could be interesting too. Not that any of this should stop anyone from building this "hvm". The utility of the tool should be determined by those who want to use it :P
I don't want any tools, but the package manager messing with my global system. I don't think, that creating an environment with a single command (virthualenv) is too hard for beginners, and I have planned a feature to automatically download and install GHC versions.
You are absolutely right of course, the missing documentation is the reason I didn't announce this package properly yet. My focus for effects 0.2 was to simplify the types, and to make sure GHC could infer them. But sadly I can't do anything about the number of type variables.
thanks :)
There is nothing wrong with writing it yourself; but you should know that this function is included in the Data.List module under the name `permutations` Prelude&gt; Data.List.permutations [1,2,3] [[1,2,3],[2,1,3],[3,2,1],[2,3,1],[3,1,2],[1,3,2]]
This is your p.o.v, of course :) Maybe some people will find handy to have such tool :)
Also lacking [envelopes and barbed wire](http://dl.acm.org/citation.cfm?id=128035).
The recommended use of rvm is to install it locally, and use it with gemsets as a sandbox for each app. Its ability to set the global ruby is not all that important for most of us.
My understanding is that rvm and virthualenv work the same way. rvm (by default at least) installs ruby versions into your home directoy (per-user). With gemsets you then have the capabilities of virthualenv. Personally I wouldn't find it useful to do system wide installs, but I hope there can be collaboration in your efforts.
Have you tried installing multiple GHC's at once? By simply putting the appropriate installation's /usr/bin directory on your PATH, you get this effect. Package binaries, and installed package lists are already managed per compiler version, and using a particular executable will select the correct package database. Cabal also can be told which version of the tool chain to use w/o changing the PATH via the `--with-compiler` flag. From a GHC binary, it will (at least in modern cabal) determine the correct executables for any ancillary commands it needs (like `ghc-pkg`). Mind you, layouts of GHC and Haskell packages varies from OS to OS, and even from user to user. So, YMMV.
 permute [] = [[]] permute xs = [ y:zs | (y,ys) &lt;- select xs, zs &lt;- permute ys ] Just a aesthetic slight tweak: You can pattern match the result of the `select`.
I agree, except that for an application the gems are well handled by Bundler without the needs for gemsets (they could be useful for developing multiple related libraries)
The only use I found for the default ruby switcher was to make 1.9 my default, so when I want to just open a term and pop into IRB, I don't get silly old 1.8. :)
I found a solution: select :: [Int] -&gt; [(Int,[Int])] select [] = [] select (x:xs) = (x,xs) : map (\(y,ys) -&gt; (y,x:ys)) (select xs) no_attack :: Int -&gt; [Int] -&gt; Int -&gt; Bool no_attack x [] i = True no_attack x (y:ys) i = ((abs (x - y)) /= i) &amp;&amp; (no_attack x ys (i + 1)) nqueens :: [Int] -&gt; [Int] -&gt; [[Int]] nqueens cps [] = [cps] nqueens cps fps = [ ps | (p,nfps) &lt;- (select fps), ps &lt;- (nqueens (p:cps) nfps), (no_attack p cps 1)] Apparently the Prolog code didn't use the 'permute' predicate to calculate the solutions, so I didn't have to translate it to a Haskell function :D. But it was no wasted effort because I learned from the answers I got here. Thanks for helping me on my path to understand Haskell :)
Function binding is weaker than operators. :( console.log 40 + 2
Perhaps the biggest downside to Roy is no tail-calls. There are a lot of potential approaches to Javascript, I have been collecting some: http://www.yesodweb.com/wiki/JavascriptOptions Personally I have been shaving enough Haskell Yaks that I have only been using Coffeescript, but it is still just dynamically-types javascript. I have been hoping to hear from others trying out ghcjs, Roy, and other alternatives by now.
Hindley-Milner type checking, haskell-like syntax. I like. The "tracing monad" example is broken, though :( Also, the "types" example needs a little tweaking: it doesn't compile! Change console.log (getName {}) to console.log (getName {firstName: "Dan", lastName: "Burton"}) Interestingly, it checks the type of an object by inspecting its members. Any object with a string for `firstName` and `lastName` is considered a Person; the object can have any additional information attached and it is still a Person.
Structural typing.
Wait, macros too? https://bitbucket.org/puffnfresh/roy/src/ea7e6cb17dac/examples/macro.roy I really like this!
I experimented with it a while ago and I got [this code to work.](http://hpaste.org/50477) I intend on re-implementing it as a proper API including the basic DOM API, and probably a type-safe binding to jQuery. I will say that when you're working with it, if something is wrong, you don't get a helpful error message. The runtime just throws an incomprehensible exception at you. Also, the size of the JS output is rather large. Like 1.8MB for the example I linked above. Fortunately due to its redundancy it compresses to like 60KB or something with gzip. I've been putting off making a server that takes Haskell code and produces JS based on ghcjs for a while. I thought it would be worth doing for others to try, because getting your GHC set-up with working libraries can be a pain. Been leaving it for a weekend for a couple months, might be this weekend, might not.
Yes, this is what we do at Tsuru; we have multiple versions of ghc installed in /usr/local/ghc/ghc-$VERSION, and people use a simple shell script to update their $PATH to switch between versions ("preghc 7.2.1"). GHC already takes care of separating everything it installs in versioned directories (eg. user stuff in ~/.ghc/$ARCH-$VERSION) so everything takes care of itself, including randomly installing multiple versions of stuff from hackage. That shell script that updates $PATH doesn't take care of installing new ghc releases though ;-)
"Non-descriptive variable names"? I've never heard of any code style condoning that. That's just the programmer being lazy :D
Only if it's SPJ.
A [sloth](http://en.wikipedia.org/wiki/Sloth) would be appropriate for a lazy language :)
thanks for the report! On the one hand that confirms many of my bad suspicions, but on the other it is still very tempting to try and make it better.
Just as long as you don't replace key system commands like `cd` :/
Please try out the ghcjs linker I have been working on at https://github.com/hamishmack/ghcjs It removes unreachable functions, splits the remaining ones up into easy to load "pages" (for lack of a better name) and then runs it them through the google closure compiler. The examples come out at 300kb of Java Script (before zipping) and that includes a 200kb file that is only needed for the forkIO example. I will be doing some more work on it this weekend and I'll try to be on #yesod.
 alias rm='rm -i' And always use Make and a makefile to compile and clean your project.
When can div and mod go out of bounds? Something related to sign extension when shifting?
Note: downvoting is not an appropriate way to respond "no" to the poll. In fact, if you feel strongly about *not* having a mascot, you should upvote this so that there will be greater opportunity to discuss why Haskell should not have a mascot.
The speaker seemed impressed when he got to the bit about next-gen Javascript having the manual capacity to construct lazy operations. Quite how they butchered list comprehension as they did I don't know. Are we doomed to have this monkey patched Frankenstein creation with us indefinitely? It seems we have been freed to be chained by the browser.
Personally, I pray for a bytecode language available in the browser, on top of which Haskell can be implemented.
That would be so awesome, I really don't know why this isn't already invented. Or, for example, a binary representation of the AST for Javascript, you could produce so much faster code.
PNACL. That is, llvm.
Maybe something can be done with [native client](http://code.google.com/chrome/nativeclient/).
Downvoting *is* the way you influence signal-to-noise ratio on reddit.
Slightly off topic but I found this video on [Programming Style &amp; Your Brain](http://www.youtube.com/watch?v=taaEzHI9xyY), also from YUI conf, very enjoyable.
Do not want btw, a slightly related Dijkstra quote: &gt; The use of anthropomorphic terminology when dealing with computing systems is a symptom of professional immaturity. source: http://www.cs.virginia.edu/~evans/cs655/readings/ewd498.html
As Brent Yorgey [pointed out](http://www.mail-archive.com/haskell-cafe@haskell.org/msg94781.html): "Sloths are not lazy, they are just slow."
&gt; â€¦so that there will be greater opportunity to discuss why Haskell should not have a mascot. I don't see why we need greater opportunity to discuss this; it's a silly idea and everyone's time would be far better spent actually writing Haskell code rather than wasting cycles on such nonsense.
&gt; Or, for example, a binary representation of the AST for Javascript, you could produce so much faster code. Do you have evidence for this? I don't think the parsing is such a bottleneck.
The trouble is sanely debugging a program running in the browser and every language having to spend a lot of effort building and maintaining their own tool-chains: much duplicated effort. I feel there needs to be a browser revolution, although it may indeed by something more general. There is the deterministic world of static pages where I *should* get the data on my terms, and the dynamic world of apps. Applying the document model to complex UIs seems to break the metaphor; certainly in regards to animation and 3D applications. I don't really know what the alternative would be, an (Interactive|Reactive) Object Model? Do we need to be confined to Objects? A Data Model? Visual Model? Component Model? Cellular Model? I get the strong feeling that we are over leveraging the technology. UIs naturally want to blur state and code, so why fight against it? There has been this desire to extract JS into external files, or script tags, but when in reality it makes sense for the widgets to contain the actual behaviour and content. Having functions bolted on afterwards seems a layer of unnecessary obfuscation. I'm looking at the source code but 'where on earth is that listener added? Oh it is somewhere in one of these 7 external files'. I am reminded of Sussman's recent lecture [We-Really-Dont-Know-How-To-Compute](http://www.infoq.com/presentations/We-Really-Dont-Know-How-To-Compute), where he talks of interacting semi-autonomous cells almost. Maybe JS needs to be the ugly, messy ecosystem to allow such things to evolve, but it always seems so far behind and unnatural.
And what about spending time breaking out of the insular world that is Haskell? Also where is your sense of fun? I'm neither pro nor against, although I have made my recommendation [elsewhere](http://i.imgur.com/a6KNZ.jpg).
Sun tried it in the late nineties. "The industry" has rejected it. (Albeit possibly JVM is not best suited, but nevertheless.)
Java didn't have DOM integration, and if it has now, noone knows about it (at least not me). Requiring a plugin is another factor that makes it unusable for run-off-the-mill webpages. Oh, and Flash uses ECMAscript (different dialect than what's in the browser, but more similar than, say, scheme and closure), too, (web)developers are lazy, and tomcat guys are different people. I guess node.js proves my drift.
Despite popular belief, some of us Haskellers are not writing code 24/7. Also, Haskell having a mascot or not can influence how people think about the language. It sounds dumb but it's the truth.
Considering this discussion "noise" is different than considering a mascot a bad idea.
Hm...let's see here. &gt; laziness implies memory consumption that won't fly on phones and cheap tablets Um...lol? &gt; haskell indeed gives you plenty of rope to be stupid...namely using the type system as an obfuscation mechanism Again...lol? These statements are just bad. Nice try, tr0ll.
Java could talk to Javascript in the page, and vice versa. But it was a crapfest back in the early java age because every browser had a shitty broken non-std javascript impl.
No, but optimizing on pageload is. 
you don't know anything about haskell? lol!
Maybe check out [Elm](http://elm-lang.org/) which compiles a Haskell-y functional language to html/css/js. This is my senior thesis, and I am trying to answer some of your questions :) Keep in mind, it's a work in progress!
Even if you would standardize a binary format today, the engines wouldn't really benefit from this unless they used it as their internal AST representation. Otherwise the binary representation needs to be parsed as well. 
Out of curiosity, what happens if `Z` is the name of a different type already?
It would be great to get the Collection extension used on Wikibooks.org installed on the Haskell wiki to make stuff like this even easier: http://www.mediawiki.org/wiki/Extension:Collection Installing that extension would get PDF-ization and some other nice features for free. Cf. http://upload.wikimedia.org/wikipedia/commons/2/26/Haskell.pdf which is available from http://en.wikibooks.org/wiki/Haskell Edit: I just realized that that PDF above was a one-off export that someone did years ago... Here is the link to get the current content as PDF: http://en.wikibooks.org/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=Haskell&amp;oldid=2149084&amp;writer=rl
There's some sort of mangling scheme described in the paper: http://www.cis.upenn.edu/~byorgey/papers/haskell-promotion.pdf If there's a conflict, the lifted type T is referred to by 'T (prefix the lifted constructor with a single quote).
I wish it had to be referred to like that; the sugar seems confusing.
ok, you're probably right. :) 
How the hell didn't I come across this typeclassopedia earlier? This thing is awesome.
http://www.haskell.org/haskellwiki/Yhc/Javascript/Brief_overview
I'll vote here: No.
I'd rather get rid of that abomination called mediawiki and move to gitit.
Unfortunately, YHC has passed away.
I'm sad to hear. Do you know of some similar projects to core2js? Could maybe be done with GHC core.
I'm not aware of any other effort in this direction. Google returns an [old post concerning UHC](http://utrechthaskellcompiler.wordpress.com/2010/10/18/haskell-to-javascript-backend/), but this does not seem alive either.
&gt; The trouble is sanely debugging a program running in the browser and every language having to spend a lot of effort building and maintaining their own tool-chains: much duplicated effort. Yes and no. Tool-chains for imperative languages generally don't work well for Haskell. Put differently, the duplication of effort is actually necessary, because the languages are fundamentally different. On the other hand, languages are similar surely will be able to share parts of their tool-chain. 
Hmm, perhaps there's an opportunity here then. Let's see if I can get some interest for it in Chalmers.
Fortunately, the YHC project has left a lot of [technical design documents](http://www.haskell.org/haskellwiki/index.php?title=&amp;search=javascript&amp;fulltext=Search) on the Haskellwiki, so some of the efforts can be salvaged. It appears that Dmitry Golubovsk is one of the people that drove the Javascript parts.
By the way, Haskell in the browser might be able to leverage my [reactive-banana library for FRP][1], so there is a compelling use case here. Someone even contacted me, saying that he's working on a server-side GUI library for web browsers and asking for a small feature to reactive-banana that saves bandwidth. [1]: http://www.haskell.org/haskellwiki/Reactive-banana
Thanks! 
Ah, this is interesting. I suppose FRP could fit very well with the reactive nature of the browser. Have you looked at any security related issues (info-flow, isolation of components, etc) in the context of FRP?
&gt; Have you looked at any security related issues (info-flow, isolation of components, etc) in the context of FRP? No, not at all, nor do I plan to. I tend to view security as a special case of static typing and purity. Of course, I'm open to suggestions for reactive-banana should the need arise.
Well, it's done. fastPow :: Integer -&gt; Integer -&gt; Integer -&gt; Integer fastPow base 1 m = mod base m fastPow base pow m | even pow = mod ((fastPow base (div pow 2) m) ^ 2) m | odd pow = mod ((fastPow base (div (pow-1) 2) m) ^ 2 * base) m 
&gt;Haskell having a mascot or not can influence how people think about the language. sure; and I'd rather it have a less silly/superficial audience... EDIT: Isn't Haskell's motto "avoid success at all cost" anyway?
this would make a great addition to real world haskell
This hsenv project does this I believe: https://github.com/Paczesiowa/hsenv Perhaps you can contribute to that effort instead of starting your own. Edit: I see you already had an exchange with Paczesiowa about your needs, so hsenv likely would have been mentioned if you guys were on the same page.
Why do you think it's not a debugging question?
This doesn't trace recursive invocations of f. To do that you would have to parameterize f over all monads and then use Identity for the pure version and IO or Writer for a debugging version.
The original question didn't read that way to me. Anyway, a recommendation of `trace` should definitely come with a big warning that it should only be used for debugging purposes.
For all the Yesod contributors out there: the build scripts in the repos have been updated to call cabal-src-install if it is present. Just "cabal install cabal-src" and hopefully 95% of the dependency issues we've been running into should disappear.
Looks useful. I think this, or some similar functionality to add locally edited packages to the dependency resolution, should be built into cabal.
You're joking, right? The two are completely different algorithms. Try this in python and tell me how it goes: 123456789**123456789 % (10**10) 
I agree, I hope cabal-src becomes obsolete.
Is there a problem with getting code into cabal? It seems like there's an awful lot of overlay work going on that should be working its way back into core cabal. Is it? I mean this neutrally, not critically. I know at least some of this releasing of additional layering on cabal is being done to solve acute crises.
The original Typeclassopedia has been the single most useful Haskell reference for me. I have read it many times, I keep referring to it and read many of the references in the bibliography. It's an amazing piece of technical writing.
Greg and I did discuss this with Duncan before I wrote this. I think that there's an advantage to writing this as a separate tool first: * We can have some rapid iterations to figure out what works properly. * It's a good chance to see if it really solves the problem without burdening the cabal team with maintaining an extra feature. * The code gets out the door much more quickly, to "solve acute crises" as you put it.
In this case, it is simpler and faster only because the functionality is packaged in a library. Perhaps it would be beneficial to make a Prelude.Python module and consider what (if anything) makes sense to move from that into a module inside of base?
Now Simon can ask again about -O.
Avoid "success at all costs". I think the idea is still be successful. Having a mascot would not hurt, and could help.
Btw, what is it in Python?
As google discovered while writing v8 (simplification I know), the source code *is* the most efficient representation of an AST.
And you don't know anything about trolling, my friend.
Could you please explain about how haskell lets you use the type system as an obfuscation mechanism?
Very cool! Is there anything we should be providing on happstack.com to make this easier? Or should we just link to your blog post? 
What `dmalikov` said it is. See http://www.python.org/search/hypermail/python-1994q3/0145.html It is in appropriate places on hackage, see e.g. http://hackage.haskell.org/packages/archive/Crypto/4.2.4/doc/html/src/Codec-Encryption-RSA-NumberTheory.html#expmod
&gt; Do you dispute this? Yes.
Actually, your example Python is calculating [Modular exponentiation](http://en.wikipedia.org/wiki/Modular_exponentiation), which is very different than calculating the "power of large numbers". dmalikov's post is a straightforward implementation. The [cpython implementation of long_pow](http://svn.python.org/view/python/tags/r271/Objects/longobject.c?view=markup) is a bit less straightforward..
Hi habitue, I'm the author of MonadPrompt. Bertram Felgenhauer implemented a nice peg solitaire game using MonadPrompt and gtk2hs which is available at http://int-e.home.tlink.de/haskell/solitaire.tar.gz Here is the call to runPromptC inside that file: -- glue for the abstract UI runGame :: IORef State -&gt; IO () -&gt; IO () runGame st notify = runPromptC (\_ -&gt; return ()) handleUI gameUI where handleUI :: RequestUI a -&gt; (a -&gt; IO ()) -&gt; IO () handleUI (SelectUI b hs ss) f = do modifyIORef st (\st' -&gt; st' { sboard = Just b, hilight = hs, active = ss, select = Just f }) notify handleUI (DoneUI b i) f = do putStrLn ("Game over with " ++ show i ++ " pegs left.") -- game end: set an active tile that restarts the game modifyIORef st (\st' -&gt; st' { sboard = Just b, hilight = [], active = [(1,1)], select = Just $ \_ -&gt; runGame st notify }) notify The key bit is that runPromptC allows the prompt interpreter to return IO () in all cases, while stashing the continuation `f :: a -&gt; IO ()` inside the state to be used later.
RecPrompt doesn't strictly give you anything that Prompt doesn't, except slightly cleaner notation: data PromptPlus m a = PromptZero | PromptPlus (m a) (m a) instance MonadPlus (RecPrompt PromptPlus) where mzero = prompt PromptZero mplus x y = prompt (PromptPlus x y) runList :: RecPrompt PromptPlus a -&gt; [a] runList = runRecPromptM handler where handler PromptZero = [] handler (PromptPlus x y) = runList x ++ runList y `handler` has type `PromptPlus (RecPrompt PromptPlus) a -&gt; [a]`; RecPrompt just takes care of filling in the recursive type in the prompt for you. You could do the same with regular Prompt, it's just a bit more cumbersome: data PromptPlus' a = PromptZero' | PromptPlus' (Prompt PromptPlus' a) (Prompt PromptPlus' a) runList' :: Prompt PromptPlus' a -&gt; [a] runList' = runPromptM handler where handler PromptZero' = [] handler (PromptPlus' x y) = runList' x ++ runList' y Here `handler` has type `PromptPlus' a -&gt; [a]`.
Fair enough, as long as the longer intention is still to merge such fixes and features into cabal-install once they stabilize. I hope people are testing the new cabal-install resolver which I think has now been merged in darcs and should hopefully help avert various dependency issues. I can't help echoing Jerf's sentiments though, perhaps we need some monatorium on the proliferation of cabal tools (cabal-dev, cab, etc, etc)? :)
just suck it up and deal with js on its own terms. yeah we all know it isn't great. who is seriously going to hitch their wagon to "roy"? even coffeescript barely has a critical mass. although i am sure trying to make js be haskell will result in some amusing implosions
That was discussed on haskell-cafe back in June 2011, but it didn't go anywhere: http://groups.google.com/group/haskell-cafe/browse_thread/thread/5b55dd36030d1b54/7e7bb766a7287adb
I wouldn't say it's a bad language to start with, given that it is very different from other languages you may even find yourself thinking slightly more uniquely then others who started with more conventional languages. I say give it a go! :D (Also, once you have the programming mindset and problem solving skills, you can apply that to every language, so you won't be trapped in Haskell and it will be easy to learn another language too (or four, or eight, or sixteen)) :)
yes
Why? What reasoning is there for this, beyond trivial things like "mediawiki is not written in Haskell and gitit is, meaning it's totally cooler and clearly better by every metric I can think of or care about"? I'm not joking here and I don't want superficial bullcrap. As Gwern Branwen said in the thread linked below, gitit does not have very acceptable performance in the darcs backend, and it's not clear if the git backend solves that. Considering the reason people were complaining in that thread to begin with is because it's slow, I don't really see that as off to a very good start. MediaWiki *does* power wikipedia, which makes me think the slowdowns are likely due to other, tangential factors not directly related to the codebase. Furthermore it's not clear gitit supports the needed MediaWiki syntax the haskell wiki already uses, as Gwern also said. So who's going to do the conversion? You? He's also admitted security holes in it as well. MediaWiki has the benefit of being battle worn at this point, at the very least, and gitit just doesn't have the same amount of time or effort put into it, in both development and testing. John has also said he has less time to work on gitit these days, and will likely have less in the future. The project seems to be relatively dormant beyond bug fixes at this stage. Maintenance is a HUGE deal. Are you going to maintain gitit and actively improve it, when the Haskell wiki needs it? If not, who is? Again, I don't want bullcrap about dogfooding or crap about "well, REAL haskellers would want everything Haskell powered, no matter the costs!" ALL of these things need to be seriously taken into consideration before there's even ANY thought of actually replacing what we have now - something that works - with something new, which we don't know will work. I can't stand behind anyone who doesn't take these concerns seriously, because I think they're very serious, especially when they come from people who have actively worked on the project (Gwern and John.) I'm all for world domination, and I do believe Haskell is more than appropriate for a wiki like website. Gitit may or may not be the thing we need on that note. But I'm not for using it if we're going to replace things that *work* with things that *don't work* just because we think they're shiny. That'll just end up as more ammo for ridiculous blog posts about how "unsuitable" haskell really is: because *we* jumped the gun and didn't care.
Well, I found [this](http://learnyouahaskell.com/). Downloaded [x-code 3](http://developer.apple.com/technologies/tools/whats-new.html) and [haskell](http://hackage.haskell.org/platform/mac.html). I started reading the book and I am stuck [here](http://imgur.com/4vWq4). I don't know what txt editor to use to save a file as .hs and I don't know exactly where to save it to once I get it. I figure once I get over this roadblock it will be fairly smooth sailing from there.
Alright. Start off by getting a useful text editor. You don't need anything too fancy (read: expensive) right now, so let's go with the best expense: free. Not having a mac I can't speak for any products, but I head [Textwrangler](http://www.barebones.com/products/textwrangler/) is good. Using the save-as feature in any editor should provide you with the option to either a) select `.hs` or b) custom enter an extension. EDIT: Apparently textwrangler doesn't have syntax highlighting for haskell (which is something very convenient). In light of this I suggest [Leksah](http://leksah.org/) if you a ready for a steep learning curve or [Yi](http://www.haskell.org/haskellwiki/Yi) if you are not. EDIT2: ehird has informed me that Yi is unmaintained and has a steeper learning curve, fortunately I also remembered that my editor of choice, [Sublime Text 2](http://www.sublimetext.com/2) is for mac as well, and it is very easy.
Whoa, Yi has a much steeper learning curve than an IDE like Leksah. Also, it's unmaintained.
Ah, my bad... Once again I don't have a mac. I use GHCI and sublime text 2 (oh wait, sublime text 2 is for mac too, and it has an unlimited free trial, you can try that too)
I downloaded jEdit and Sublime Text 2 and have the function [from this](http://imgur.com/4vWq4) saved as a .hs file on my desktop. I don't understand how to access it from terminal. It says to navigate to where it's saved and run ghci from there in the book. I have to be overlooking a very simple concept here.
Alright. [Here](http://smokingapples.com/software/tutorials/mac-terminal-tips/) is some general information on the terminal. I think what you want to do is run `cd Desktop` in terminal. That will make the directory you are currently looking at `change directories` to your Desktop.
Haskell is a great language, but Haskell might be a bit academic/hardcore math for some people. Also, Haskell is a functional language, probably unlike the language in your calculator. Not many games are being made using functional languages. Since you don't know how to use the Unix terminal (which is very useful to learn!) I'm gonna provide two links that I can recommend a lot, but they aren't for Haskell: * [Eloquent JavaScript](http://eloquentjavascript.net/chapter1.html). This book has code samples with buttons that run the code in your browser. It pretty much can't get any easier. JavaScript is the most popular language that websites use for running client side code (i.e. controlling UI, AJAX, ala Flash). The book has a lot of text though, but you could try and skip that and see if you still understand the code samples. * [Try Ruby](http://tryruby.org/). This website is probably even easier than Eloquent JavaScript to use. It has a built-in tutorial.
how about ... emacs? http://emacsforosx.com/ ?
Well, Yi and Leksah are both multiplatform :-) Personally I use emacs though.
Python is easier for people who know Python.
I'll see your try ruby and raise you one.. [Try Haskell](http://tryhaskell.org/)
On the one hand, starting with Haskell will give you a correct mindset to programming. Focus on referential transparency, totality, and amazingly high level abstractions - these are very powerful ideas that normally take many years of experience to arrive at! On the other hand, switching to most other languages will be a severe disappointment. Not because they're worse, per se - they're just less rigorous and do less work for you. Things you can take for granted in Haskell have to be done manually (and you have to know that you need to do them in the first place) elsewhere. If you want to become a -good- programmer quickly, starting with Haskell (then dipping your toes into Lisp/C/assembly/Ruby once you've got a handle on Haskell) is the best choice I can think of.
for simple haskell programming i like to use either vim or gedit on osx. Vim comes with OSX by default, which is nice. Its also terminal based, so you'll increase your bash-fu a tad. Also, vim proficiency is generally good life skill. Gedit is so/so on the mac, but it has syntax highlighting and nice color themes. I tried Leksah and wasn't super impressed. Haven't tried Yi. Don't use xcode 3. It's deprecated and sucks. Xcode is really meant for c/c++/objetive-c programming. PS: learn you a haskell rocks. When you finish that, consider the book Real World Haskell. 
How far did you go in your Ti-83+ programming? I got jealous of the "real" games like Phoenix or Block Dude, and learned z80 assembly. But it sounds like you didn't do that... so I recommend you try learning it; ticalc.org has [some guides](http://www.ticalc.org/basics/calculators/ti-83plus.html#9).
Um... Pretty sure Yi is maintained or at least has been forked since there are commits *very* recently: https://github.com/yi-editor/yi never used it though so no idea on the learning curve bit 
Although the entire toolchain for that is Windows-centric; if you have a mac... it's probably easier to learn C / *nix instead. But you're missing out! :-)
If you do decide to learn Haskell, I would very highly recommend [this book](http://learnyouahaskell.com/). It is without a doubt the best introduction to a language I have ever found. It's also free to read online. I think people here are hesitant to recommend starting with Haskell because no one does that. I've never met such a person, and I'm not sure they exist. That said, I think Haskell would be a great language to start with because it will force you to learn really fantastic coding practices and strategies from the start. This point is really important, *especially* if you are self-taught. With most languages, the internet is filled with really, really shitty examples and code (JavaScript comes to mind). It will be a while before you have the skills to recognize shitty code, and starting with Haskell will largely solve this problem. Although it is possible in principle to write shitty Haskell, the examples I have seen tend to be very high quality. Also, helpful hint: [Haskell List Documentation](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-List.html). To everyone else: What would Haskell be like if you hadn't learned a bunch of unpleasant languages first? I think my hypothetical reaction would be, "oh, it makes sense that programming works like that" rather than my real reaction which was "holy cow! I've never experienced such cleverness!" Edit: If not Haskell, Python is a nice choice.
I would recommend [Smultron](http://www.peterborgapps.com/smultron/) as a text editor for mac\*. X-code, though personally I never used it, is probably just a huge confusing mess, a plain text editor is a much better start. \* unfortunately, it seems Smultron became a pay-for app sometimes since I last looked, but I guess you can find the old versions somewhere on the internet, which were free.
wow, I wasn't even aware that cloudfoundry was that open. This essentially means that this can be used as standardized auto-deploy mechanism for all our applications including linking up to DBs etc... To be fully supporting the system I guess you additionally need to respect quite a few environment variables...
One of the downsides of starting with Haskell as a first language is that Haskell presents a high level abstraction on top of the machine you are programming. Other, more imperative languages, tend to represent a closer view of what the hardware is actually doing. This goes to explaining why you will often hear Haskell is "more expressive". It allows you to express ides that are complex without reverting to a lower level, more verbose language. This is not to say abstractions are a bad thing by any means, you should just be aware of where you are entering into the fray relative to what the hardware is actually doing.
*What* do you want to learn to program? Haskell is a general-purpose language, but there are certainly areas where it is particularly strong. If you wanted to go back to making games, Haskell isn't a *bad* choice, but it doesn't have a huge community of game developers, and consequently is a bit short on tutorials and the like. On the other hand, if you want to do symbolic manipulation (write a computer algebra system or a compiler, say) or do text manipulation, I would say Haskell is a particularly good choice. These are just two examples -- perhaps the most extreme examples.
Ideally, you'd run in a tight loop, reusing your stack frame with a tail-recursive function, like: fastpow :: Integer -&gt; Integer -&gt; Integer -&gt; Integer fastpow base exp modulo = fastpow' (base `mod` modulo) exp modulo 1 where fastpow' b 0 m r = r fastpow' b e m r = fastpow' (b * b `mod` m) (e `div` 2) m (if even e then r else (r * b `mod` m)) 
Do not learn haskell if you want to actually program something useful early on. Haskell has limited practical applications (basically just servers). After you have learnt a more practical language (java/c#/c++/javascript) you can learn haskell (which will change the way you think about a program).
No, the reasoning is "mediawiki is an inconsistent pile of patches with no structure, overall idea or anything else, a true nightmare to administer and extend". It's worse than sendmail cross-bred with the windows registry. It works for wikipedia because they're the ones responsible for it, in the first place and it has been adopted by so many not because it's the best wiki software out there, but because it's used to host the biggest wiki. I call that adoption bias. When the pirate party needed a wiki, the end result was mediawiki. With the rationale of "it sucks, but other idiots already implemented 99% of what we need. Let's hope we don't need any new features." You can do everything with it, but for everything it can do there's better ways to do it. As to syntax: That's a pandoc thing, and pandoc can already generate mediawiki. gitit does pdf exports out of the box, and could've read the typeclassopedias' LaTeX source with at the most minor fixes, no big conversion hassles. Actually, a wiki should be part of any web framework nowadays, and if noone else, the yesod etc. guys should be somewhat eager to include it. An acid-state based backend would also be well worth considering. If Haskell is actually moving into the area of web frameworks and hosting, and it seems it is, we ought to eat our own dogfood. ...and that's just to give some backing to a *sentiment* I was expressing, which I hope I'm entitled to hold. Last I checked I haven't been declared Haskell dictator for life and people don't actually care much about what I tell them to do, so don't claim I just assigned my minions to a suicide mission. Kill mediawiki and move everything to gitit? That'd be a nightmare. Install a gitit instance and use it alongside? Perfect, we've got a place to dump our .lhs files, at the very least. It might even get to see some more contributors if it's actually used, then. Both mediawiki and gitit support interwiki, so visitors don't even need to really notice.
Newbie here, "cabal update" throw the following error when trying upgrade from 0.1 to 0.2: Downloading the latest package list from cabal-src cabal: Failed to download http://www.haskell.org/00-index.tar.gz : ErrorMisc "Unsucessful HTTP code: 404" 
I honestly don't know what I want to program. My current job is very excel heavy and my next job is going to be very database heavy. That being said, I just want to learn to program so if there is ever a time that I may need it, I have it.
Sorry, that's exactly the bug that 0.2 is trying to solve. Open up your `~/.cabal/config` file and remove the line starting with `remote-repo: cabal-src`. Then you should be able to `cabal update &amp;&amp; cabal install cabal-src`.
Thanks, it works.
&gt; Don't use xcode 3. It's deprecated and sucks. Xcode is really meant for c/c++/objetive-c programming. Aren't the developer tools necessary just to use GHC? I think this is what cvlrymedic was downloading it for.
Doesn't seem to handle IO too gracefully.
LYAH is a fantastic book but it's an introduction to Haskell for people who can already code in other languages. It is not an introduction to programming. Real World Haskell doesn't work for non-programmers either. It's been a long time since I checked out The Haskell School of Expression. It could be a good intro. In any case, I love Haskell but Python seems a better first language.
not sure, its been long enough since i set haskell up that I don't recall. 
That's not the point (see the link): they behave differently.
And besides, the second one is more readable.
A new dependency resolver will not perform the essential tasks that cabal-dev/virthualenv and cabal-src perform. I am glad to see you and jerf concerned about the state of cabal. It would be great to see you apply it towards merging cabal-src and cabal-dev into cabal.
clearly, the second one is better!
I've been thinking about a sytem to provide a similar feature for a while. The idea is to generalise the package index format. Instead of just being used for the remote hackage index, with implicit links to remote tarballs, the index should instead have explicit links, either to remote or to local tarballs or unpacked/local build trees. The cabal program can already install from local tarball and local build tree targets. So you can already do: cabal install ../deps/foo ../deps/bar ./ This is nice but since foo and bar are listed as local targets, cabal will always reinstall them. What cabal-src allows is something like the above, but only rebuilding foo and bar if its necessary to get consistent dependencies. Supporting local tarballs and build trees in packages indexs would cover both the cabal-src use case and half of the cabal-dev use case.
&gt; Java could talk to Javascript in the page, and vice versa. That still isn't remotely like having a bytecode layer integrated with the DOM the way Javascript is.
Do they? By referential transparency they are the same thing, right? :) "fix" just optimizes a bit for memory usage, but idealized machines have arbitrary amounts of memory. fix f = f (fix f) -- lift subexpressions fix f = x where r = fix f x = f r -- inline fix fix f = x where r = x' where r' = fix f x' = f r' x = f r -- by referential transparency, r = fix f still -- So, remove duplicate computation in r' fix f = x where r = x' where r' = r x' = f r' x = f r -- lift expressions (let of let) fix f = x where r = x' r' = r x' = f r' x = f r -- inline r' and discard fix f = x where r = x' x' = f r x = f r -- remove duplicate computation x' fix f = x where r = x x = f r -- inline r and discard fix f = x where x = f x
The description of `fix` in Haskell that was the most intuitive to me was specifying it in terms of defined-ness. -- Consider f :: [Int] -&gt; [Int] f x = 0 : map (+1) x -- a bottom undefined :: forall a. a undefined = error "undefined" -- now see how it works on various levels of defined arguments f undefined = 0 : map (+1) undefined = 0 : undefined f (f undefined) = 0 : map (+1) (f undefined) = 0 : map (+1) (0 : undefined) = 0 : 1 : undefined f (f (f undefined)) = 0 : map (+1) (f (f undefined)) = 0 : map (+1) (0 : 1 : undefined) = 0 : 1 : 2 : undefined fix f = f (f (f (f (f (f (... = 0 : 1 : 2 : 3 : 4 : ... This works because `f x`, for any `x`, always agrees with `f undefined` in any place where the latter is defined. In a way, `f undefined` is a lower bound on the information returned by `f`.
&gt; requires a lot more brainpower to parse It's really mostly a matter of familiarity - "brainpower" to perform a task is a function of how familiar you already are with that task. Besides, in reality most sane/mature C programmers would put some parens in their version, just for readability.
Could someone explain how first one actually working?
just apply the definition over and over again... fix' f f (fix' f) f (f (fix' f)) f (f (f (fix' f))) ... f (f (f (f (f (f (f ... 
I'm still confused about the difference between using `cabal-src` and `cabal-dev -s &lt;common-sandbox&gt; add-source` to make non-released packages available to the dependency resolver...
I think this is a really hard question to answer. I also think that if you want a non-biased answer, asking /r/haskell is not the best place to ask. There are a few things to consider. Are you just learning to program for fun, or are you trying to develop skills for a programming job later in life? Currently, 99% of jobs you'll find will be working with procedural languages. Haskell is a difficult language to learn. Some people think it's inherently difficult and it would be much harder to learn without basic knowledge of normal procedural programming. Other people think the hardest part is un-learning the procedural programming practices and that if you had started with a functional language like Haskell it would be much easier. It's debatable, but it's probably a mixture of both. Personally, I think you should consider: C, if you want to know what's really going on, knowledge of the computer's memory and the bits that make up all of the data you're using, and you don't mind shooting yourself in the foot a few times and spending time fixing bugs that are hard to find. Python, if you just want a nice simple language to learn the basics of normal procedural programming, loops, variables, etc. C#, if you want to make nice Windows GUI apps and learn marketable object oriented programming skills while you learn to program. Java, if you want the same, but with less of a focus on Microsoft and Windows GUI apps. Haskell, if you want to have fun and see how interesting and different that programming languages can be.
`cabal-dev` only makes it available within the sandbox, `cabal-src` makes it available everywhere. They're different use cases.
I teach programming with haskell. Thanks for making a well thought-out comment on the internet -- it's very refreshing.
They exist -- people who start with haskell.
What stack are you referring to? Your code will also suffer from essentially equivalent stack issues in GHC, because it constructs potentially gigantic expressions in the b, m and r parameters whose evaluation will require many nested pattern matches, resulting in a stack overflow. (The stack in GHC essentially represents case expressions which are waiting for their scrutinee to be sufficiently evaluated to match, which in the case of the usual numeric types, means fully evaluated.) Probably more relevant though is the amount of space it will consume before it gets there, at least when I tested it with `fastpow 2 (10^100000) 7`, it quickly gobbled up several hundred megabytes of memory before I killed it to prevent my system from thrashing. ;) However, your version can be fixed by adding a little strictness: {-# LANGUAGE BangPatterns #-} fastpow :: Integer -&gt; Integer -&gt; Integer -&gt; Integer fastpow base exp modulo = fastpow' (base `mod` modulo) exp modulo 1 where fastpow' b 0 m !r = r fastpow' b e m r = fastpow' (b * b `mod` m) (e `div` 2) m (if even e then r else (r * b `mod` m)) (Note that forcing the evaluation of r will also force b and m.)
Nice work! Does it have to be runhaskell? Can we first compile and then run binary?
What was the question and answers again? Something like: What should be the default optimization level in GHC? * -O0 * -O * -O2 I can post that question if people want.
Not really true or helpful. 
But here you have two notions mixed up : fixed point and infinite lists. This complicates things imho. A fixed point is easier to explain on finite things with recursion. Infinite sequences clicked for me really only when I learned the notions of codata and corecursion ( http://blog.sigfpe.com/2007/07/data-and-codata.html )
Typo: ""we **offer** run into "dependency hell" during development"".
have you used cabal-dev with a common sandbox over multiple projects? I didn't realize that was a use case. That would seem to cover most of the same functionality as virthualenv.
Any example of what the stack trace looks like?
By referential transparency, insertion sort and merge sort are the same thing, right? Denotational semantics aren't the only thing that matters.
I was wondering about something else in Prelude: Why `getLine` but `readLn` instead of `readLine`, for consistency?
How does redefining it to use `where` make it tail-recursive? Can you explain how that reuses the stack?
Something like that yes. I don't know if there are other possibilities but if there are you could put them too. At some point someone mentionned "something between -O0 and -O that would be a trade off between compilation and run time speed" . That could be an option too.
As the primary happstack maintainer this sounds potentially quite useful. Before making a release I need to test on all the versions of GHC that we claim to support. Additionally, I would like to automatically try to build happstack from hackage on a nightly basis since sometimes changes on hackage cause a previously working build to fail. Ideally I would start each night with a 'clean' install that just has happstack platform pre-installed. I can do these things now by using separate chroots for each version of GHC. But that is a lot of disk space. I have tried to do this (for just one version of GHC) using cabal-dev.. but I have not been successful in getting cabal-dev to ignore the packages already installed in my global package database..
Since this seems to be bothering you, I'll add in that I agree somewhat and, as my previous comment was ment to suggest, think we should learn from other communities just as I hope they learn from ours. I haven't been paying enough attention to /r/haskell to know, so I ask you: do you see a theme or category in which Python programmers are having a hard time making competitive Haskell implementations? Is there a concrete work you can identify? I worry that tailoring Prelude to particular mathematical optimizations would mean quite a bit of bloat.
OK, poll created: http://www.haskellers.com/poll/2 . Advertise as you see fit ;)
From my adventures in the programming realm I'd tell anyone now that it doesn't matter what language they start with, just one that won't limit you. Every language is more or less the same with different syntax. The differences usually are how expressive you can be in it or read it. Most basic things you learn you can take with you to any other language. I'm still new with Haskell, but it looks like you gain extreme expressiveness to the looks of it with a good amount of flexibility. It may leave you feeling paralyzed a bit in languages like C though.
Haskell: The Math Textbook. Joking aside, there's an inherent tension between "I just want to get to working" and "tell me how it works." If I want to explain to you how to write a script to download a page from a website, you're probably not interested in learning what monads or what iteratees are. But you can't stop it from showing up, because in the end, you're going to have to learn it. What we often try to do is move around the material so you're doing "useful" stuff first, but that necessarily means deferring some material for later.
Here's an example from the commit message: ghc-stage2: panic! (the 'impossible' happened) (GHC version 7.3.20111128 for x86_64-unknown-linux): tcIfaceGlobal (local): not found: base:GHC.Word.W#{d 6w} [(32R, Type constructor `base:GHC.Word.Word{tc 32R}'), (r6O, Identifier `base:GHC.Word.$fNumWord{v r6O}'), (r6P, Identifier `base:GHC.Word.$fEqWord{v r6P}'), (r6Q, Identifier `base:GHC.Word.$fNumWord1{v r6Q}'), (r6R, Identifier `base:GHC.Word.$fNumWord2{v r6R}'), (r6S, Data constructor `base:GHC.Word.W#{d r6S}'), (r6U, Identifier `base:GHC.Word.W#{v r6U}'), (r75, Identifier `base:GHC.Word.$fNumWord_$csignum{v r75}'), (r76, Identifier `base:GHC.Word.$fEqWord_$c/={v r76}'), (r77, Identifier `base:GHC.Word.$fEqWord_$c=={v r77}')] { Main.main GHC.defaultErrorHandler GHC.runGhc GhcMonad.&gt;&gt;= GhcMonad.&gt;&gt;=.\ Main.main' Main.doMake GhcMake.load GhcMake.load2 GhcMake.upsweep GhcMake.upsweep.upsweep' GhcMake.reTypecheckLoop GhcMake.typecheckLoop GhcMake.typecheckLoop.\ TcRnMonad.initIfaceCheck TcRnMonad.initTcRnIf IOEnv.runIOEnv IOEnv.thenM IOEnv.thenM.\ TcIface.typecheckIface TcIface.typecheckIface.\ LoadIface.loadDecls LoadIface.loadDecl TcIface.tcIfaceDecl TcIface.tc_iface_decl TcIface.tcIdInfo MonadUtils.foldlM TcIface.tcIdInfo.tcPrag TcIface.tcUnfolding TcIface.tcPragExpr TcIface.tcIfaceExpr TcIface.tcIfaceAlt TcIface.tcIfaceDataCon }
No I got that, I take it mandatory I will eventually have to learn that stuff. I just want to get the syntax out of the way and start at least making something. I'm still not sure how to get a random number, I can see what I need for it in hoogle, I just don't know the syntax yet to put it together even though I've seen code examples. Perhaps I'm just being impatient though since I already know several languages :P Want to hear about haskell separating itself, cause the language definitely deserves more attention. 
a good one is this [learn haskell in 10 minutes](http://www.haskell.org/haskellwiki/Learn_Haskell_in_10_minutes) for experienced programmers I guess. 
Not all Haskell books teach haskell by comparing to other approaches or languages - some just teach Haskell. 
I think you have a valid point, but it also sounds like you've probably read a lot of blog entries written by excited newbies ("I just figured this out, let me show it to you!"). Have you tried [Real World Haskell](http://book.realworldhaskell.org/read/)?
You have a good point, but honestly, there are a lot of people that approach Haskell coming from an imperative mindset, and they immediately think "oh what a dumb language, I can't even do X", or "X is practically the same in Java, Python, C++...why isn't Haskell the same?" And quite frankly, Haskel *is* sooooo different. It is 1) purely functional, and 2) non-strict. Very few languages (among those in common use) have even one of these features, let alone both. I congratulate you for not needing the "Haskell is very different!" warnings, but be aware that most people are not as accepting of Haskell as you are.
Heh. There are several ways to get random numbers (each of which have their benefits and downsides; the choice is not there just to be perverse!)
True, I have learned a good number of different languages though. Even brushed up a bit on Lisp and wrote some things in it. I eventually stopped using it when I realized a lot of cl is overly complicated than need be like ASDF or the module system which I spent weeks on and still I'm not entirely clear about. In contrast I understood the gist of haskell's modules pretty much within a quick read through. I'm sure I don't get all of it yet but it seemed more than human accessible. 
I understand, it looks very flexible, just need to get the mindset of the process down
Possibly; there may be some "interesting" Ord instances that cause them to differ. That said, my post was tongue in cheek. Of course it matters that they perform differently.
I don't think this has anything to do with infinite lists; here's the gold standard example of fix: `fact` factr :: (Int -&gt; Int) -&gt; (Int -&gt; Int) factr rec 1 = 1 factr rec n = n * rec (n-1) fact = fix factr factr undefined 3 = 3 * undefined 2 = undefined factr (factr undefined) 3 = 3 * factr undefined 2 = 3 * (2 * undefined 1) = undefined factr (factr (factr undefined)) 3 = 3 * factr (factr undefined) 2 = 3 * (2 * factr undefined 1) = 3 * (2 * 1) = 6 Similarily to the infinite list example, we push the 'defined'-ness deeper into the computation. `fix factr n = factr (factr (factr ...)) n` gives you an arbitrary level of defined-ness, just like the previous example gave you an arbitrarily long list.
`^^` the author of one such book. http://www.cs.nott.ac.uk/~gmh/book.html
I don't have a ton of stuff on Hackage but it's easy to lose track of what I've put out there. This is a great tool.
if you really want to prove a point, set up packdeps to email maintainers when their packages go out of date by &gt;1 month or something.. 
That generic stuff is beautiful. Thank you.
This looks great! Thanks for putting in the effort.
So I can type in either a package name or a hackage user id? Does capitalisation have to be correct? There's no confirmation that I've typed a valid package name / hackage user name; no warning when I type something that is neither. This unnerves me: do I get an empty list because none of my stuff's out of date, or because I've typed the wrong thing? This is potentially a great tool, but it feels rough around the edges.
Good point. I've added the list of packages checked to the bottom of the page. It should be more transparent now. Let me know if you have any other ideas to improve the site.
&gt; I just want to get the syntax out of the way and start at least making something. It's not going to be that simple. You'll have to *unlearn* some of your previous programming experience first.
The design imperative of Haskell's module system was "let's not care about this, but not look as silly as C, either". It's really only about namespacing and different files, where something akin to ML Functors would not only sometimes be a nice thing to have. ...well, you *can* look as silly as C, if you want to. `{-# LANGUAGE CPP #-}` and off you go.
I think the main problem isn't technical, but one of community organizing: It's quite unusual to have upstream developers be responsible for bumping dependencies... just imagine getting a mail from a Gentoo maintainer asking you to change a number in your .cabal, you'd think something like "bleeding do it yourself". I see three ways out of this bind: Forget about upper bounds (unlikely), round up a squad of volunteers (even more unlikely), or develop cabal to be an actual package manager and take a bite off off arch's and gentoo's market share, at which point there'd be volunteers. Daring, but possible.
Ok, your method of pushing 'defined'-ness makes more sense here for me as the result is clearly defined and terminating.
I understand why it's using byte strings but I find most annoying when using haskell libraries to have to constantly convert between String, ByteString and Text. It would be great if there was such a thing as a StringLike "standard" typeclass for which that all these libraries would propose an interface (in addition to their most specific interfaces). Maybe that's not the right solution, it's just a random thought, but there's definitely a problem there, for me at least.
Great library otherwise, I love it!
Plea to the author: please consider presenting the slides in a format navigable on devices without a keyboard.
Not my experience from _A Gentle Introduction_, Paul Hudak's book or _Real World Haskell_ but that's neither here nor there. I just thought it important to mention all the beginners who have come to haskell-cafe over the years who haven't taken on board how different Haskell is and try to create OO classes with the `class` keyword and so on.
 #include "c_in_any_language.hs"
This is an interesting comment. I think it shows that Haskell is being approached as a mainstream programming notation (I have a job to do, help me get on with it)... What these folk need is a quick "bootstrap" guide - maybe some of the tutorials out there are like that?
It's not really true that we didn't care about the module design... We wanted essentially the power of the ML module system without its complexity. Apart from named type equivalence, Haskell has the same module-level expressivity as ML. Some features were actually stripped out of the original module design because we realised that type classes could already handle them (with a bit of work). Qualiified names were added in. 
I'll probably change it not to use increasing indentation, as suggested by Ian Lynagh. Any other suggestions?
Hey, thanks! That was one of the more accessible lectures of the quarter.
An introduction to Haskell is here: http://sites.google.com/site/simplehaskell It is a walkthrough of writing a useful program, without an overload of unfamiliar concepts. Instead, it is a sale pitch for *pure-functional programming*, the Silver Bullet. The same program is also written in Python and C#, for comparison of language syntax and to show the applicability of the pure-functional approach to mainstream languages. 
The problem is that there isn't "one way to do it", unless you're planning on pretending that UTF-8 is the only text encoding in existence.
Is that Philip Wadler?
Yup, he teaches the FP course in first semester to first year students every year at the University of Edinburgh. He does this every time he does the lambda calculus lecture and the first years always love him. He's a pretty fantastic lecturer IMO.
Yes exactly, but this is a separate one when he joked about C#. I knew something was up so I pulled my phone out.
Might be best to only be concentrating on one confusing thing at once. Nevertheless, I can recommend emacs as the best eventual editor to use.
Did he do the "Social Sciences" joke this year?
I didn't get your first point: surely anyone can suggest a version fix to a package. Personally I am thinking increasingly about the latter: traditional distro packaging for modern modular languages does not scale well by hand. I want to have a tool that automatically generates and installs rpm packages recursively on the fly.
I find the reverse dependencies function very useful and use http://packdeps.haskellers.com/reverse/&lt;pkg&gt; frequently. It will be good though when hackage2 eventually provides this feature on hackage itself.
Phil Fucking Wadler! I went to his lectures; the man is epic! I now wear the hoodie on the right regularly: http://streetshirts.co.uk/qrcomic
What about line numbers? And is there any chance to attach stack traces to exceptions in the future?
Suggest, yes, but demand? Involving all those people is just ridiculously inefficient, especially when people want to bump some dependency because of some other package the upstream developer couldn't care less about.
I could navigate with my iPhone. 
Thank you for the heads up! I tried tapping on the edges of my iPad but I went nowhere. I'll give it another try.
I'd say O2. Those who don't know any better could miss out on some useful optimizations with O1 (as we've seen happen with O0 as the default). Granted, I generally play around with small projects where compile time isn't an issue, but as your projects grow and/or for just checking if it compiles, you could always set the optimization level yourself.
I'd expect `-O0`, as that's what I'm used to with other notorious compilers such as `gcc`... :-)
Please do share the joke with the rest of the class... :-)
Honest question, how is it for databases?
"Being Lazy With Class" (Hudak, Hughes, Jones, Wadler) says the following about modules: &gt; At the time, the sophisticated ML module system was becoming well established, and one might have anticipated a vigorous debate about whether to adopt it for Haskell. In fact, this debate never really happened. Perhaps no member of the committee was sufï¬ciently familiar with MLâ€™s module system to advocate it, or perhaps there was a tacit agreement that the combination of type classes and ML modules was a bridge too far. In any case, we eventually converged on a very simple design: the module system is a namespace control mechanism, nothing more and nothing less. (http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/) Which shows, at least, I guess, that there are different takes on the modules story. The section goes on to describe how there were initially interfaces and implementations both (c style). I'm quite glad that interfaces ended up being dropped.
See also the [previous discussion](http://www.reddit.com/r/haskell/comments/m5yhx/quick_poll_upvote_if_you_think_ghc_should_enable/).
`O2` might make things slower at runtime as well, or lead to higher memory usage, it's not just slower to compile. *edit*: apparently not true, see below.
I hope it gets into yesod. 
There is a strong direction in newer approaches to teaching Haskell toward the approachability and demystification. See for example: * [Johan's and my Barley project](http://code.google.com/p/barley/) * [A workshop taught with Barley](http://mtnviewmark.wordpress.com/2011/02/13/bayhac-2011-wrap-up/) * [CS240h at Stanford](http://cs240h.stanford.edu/) * [My 'teaser' lecture "Haskell Amuse-Bouche"](http://www.youtube.com/watch?v=b9FagOVqxmI) I agree that for most audiences we should not teach by difference, but teach from presentation of the Haskell approach. To me, this means that concepts like *laziness*, *purity*, *monads*, and *high order* shouldn't lauded in abstract, discussed with hushed tones, or couched in Category Theory, but presented as the powerful programming tools they are. I disagree that they should be in the "history" section - since they are essential to the power of Haskell. But I honestly believe we can teach these in a way that shows how to use them, motivated by real situations. (See my video above for a few ideas of how.)
[This guy says otherwise](http://www.reddit.com/r/haskell/comments/m5yhx/quick_poll_upvote_if_you_think_ghc_should_enable/c2yfybb). But you could be right for all I know.
Ah yes, I neglected the strictness. And just one bang! Such expressive power. So strictness evaluations happen before pattern matching, then. Interesting. I'd've put the bang on both r, or even the second r.
Sure, have a look here for starters: en.wikipedia.org/wiki/Tail_call
&gt; He's a pretty fantastic lecturer IMO. Indeed. I suppose *someone* has to stop SPJ from winning all the best-lecture prizes. ;-)
great slides. at slide 20 though i started feeling like i would RATHER implement this in c than haskell...particularly the "by value" section. at slide 21, even moreso...those ghc pragmas are ugly and tells users we need crutches to get the job done slide 35 speaks to the desperate need to cleanup the package hierarchy, no one will be able to figure out what package is suitable
&gt; great slides. at slide 20 though i started feeling like i would RATHER implement this in c than haskell...particularly the "by value" section. at slide 21, even moreso...those ghc pragmas are ugly and tells users we need crutches to get the job done The pragmas might be verbose, but they are not crutches. Memory layout needs to be under the programmer's control and by-reference and by-value are both useful at times so we cannot have the compiler always pick one. C/C++ offer the same control (but with quieter syntax.) That being said we're looking into changing GHC so it automatically unpacks small fields (e.g. single machine word fields) and offer an `UNPACK` pragma in those cases you don't want that. This should greatly reduce the number of unpack pragmas needed in the common case. It's also educational to compare the control we have over memory layout in Haskell to that available in other GCed languages. For example, in Java you don't even have the option to unpack composite types; by-reference is all you get. &gt; slide 35 speaks to the desperate need to cleanup the package hierarchy, no one will be able to figure out what package is suitable With the exception of the unfortunate presence of two Unicode types (`String` and `Text`), it all seems pretty standard to me. There are ordered and unordered containers, just like in any other language. When to use `Int` specialized versions seems pretty clear, namely when you're storing `Int`s! For clarity: Ordered containers: * `Data.Map` * `Data.Set` * `Data.IntMap` (specialized version of `Map`) * `Data.IntSet` (specialized version of `Set`) Unordered containers: * `Data.HashMap` * `Data.HashSet` 
As you probably noticed, Haskell was a huge influence on Elm's design. I also wrote the compiler and web-server in Haskell which was a really great experience (yay Parsec and HAppStack!). The site itself is written almost exclusively in Elm (excluding the editor and unicode proposal). So far, Haskell backend + Elm frontend is pretty much my ideal environment for creating a website. Elm could use more work of course :P
But that typeclasse's functions could take a (Maybe Encoding) or something like that I suppose.
looks great, is there a download? I didn't see a link.
tl;dr: Not yet. The compiler and language are still pretty alpha (bad error message, limited set of language features, etc.), so I was planning on doing a release when the implementation settles down. I am also doing my senior thesis on Elm's underlying FRP model. The thesis is my number one priority right now, so it will be a while before I can devote enough time to making a legitimate release. This is probably for the best as much work remains to be done on the language itself! In case your curious, this is my tentative idea of how to release an Elm compiler at some point in the future: Given a directory tree of Elm files and media files (images, videos, markdown?, etc.), the compiler produces a server. In dev mode, you get a server on localhost that is actively updated as you make changes. This is pretty much how I develop [elm-lang.org](http://elm-lang.org/), and it is really pleasant. I think this also makes it really easy to install, develop, and deploy websites. Does this sound reasonable? Edit - temporary solution: use the online compiler, compile to new tab, save html (and required js sources). I know this is not ideal, but that will work because the examples are hitting the full compiler. I'll PM you if I make this process more formal (i.e. an export button or something).
It would be nice if you documented more of the language. I'd particularly be interested in the type system (kind polymorphism? typeclasses?)
Will the compiler be open source?
Correct me if I'm wrong but doesn't GHC unpack/unbox strict fields with -funbox-strict-fields?
The most interesting part of any FRP system is how you can describe stateful and dynamically created (and killed) entities. The only relevant combinator I found was foldp, which is not explained in the docs, but by the looks of it it is something like scanlE in Reactive. Be careful there, because you need to be able to specify when a stateful signal is initialised, otherwise you won't be able to describe any kind of dynamic system without leaking all over the place. I recommend the [blog post of Apfelmus](http://apfelmus.nfshost.com/blog/2011/05/15-frp-dynamic-event-switching.html) for a quick summary of the possible approaches to solve this problem. On another note, your top menu doesnâ€™t work in Opera.
Since this project is my senior thesis, I have focused primarily on reactive programming as that's where I hope to make a novel academic contribution. Type theory is not my focus. As a result, the type system is fairly basic right now (only primitive types + lists, limited polymorphism), so there's not much to document. This has been enough for the theoretical work I am doing, but I plan to work towards a Haskell-inspired type system (typeclasses are definitely on the agenda). It will be a while before I am anywhere near the level of expressiveness that Haskell currently has, especially considering my thesis deadlines! That said, I'm sorry about the poor documentation in general! Since Elm is so young, I am planning on providing better documentation once the language design settles down more.
Be sure to post your thesis when you write it, I would love to read it.
Sure thing! That's really encouraging to hear :)
Oh man, that looks pretty nice. What's the manufacturing quality like, decent?
That's the plan.
When would you ever want GHC to *not* pack a single field, single constructor, strict data type? For multiple field data types I understand, as there is sharing vs. copying trade off that only the programmer knows.
Neat. One thing I'd really like to see is a good way to share values/types between Haskell on the server and Elm in the browser.http://elm-lang.org/Elm. I find that I often have some nice datastructure in Haskell which I need to send to the browser where some fancy ajaxy stuff happens, and then I get the data back. But right now that involves converting my nice algebraic datatypes into JSON and then javascript objects and back again. Unfortunately, data structures that work nicely in Haskell do not translate nicely and automatically into something that is friendly to deal with in javascript. Also, it is annoying when the types on the server and client get out of sync. With something like Elm, it would be really nice if I could define my shared types in some file that my Haskell and Elm programs would import. And if the serialization for server-client communication was completely automatic. Obviously, those shared types would have to be a subset of the types permitted by Haskell. But, I think it would still be useful.
The only interface shared by byte strings and Unicode strings are that of a sequence.
It does. It's a bit of a heavy hammer though. You might want to have some fields unpacked and some not, in the same module.
decode vs decode' is an example in the use of lazy evaluation. decoding (often large) RPC messages is a place where not doing it all up front often is a good strategy.
Almost never. However, there are (theoretical) cases: data T = C {-# UNPACK #-} !Int data Box a = Box a f :: T -&gt; Box f (C i) = Box i -- an I# constructor must be allocated here 
I totally agree. Untyped data makes me sad. I'd like to incorporate something like this. Microsoft's OData protocol does this in some form already, although the type models are .NET types. I believe F# can type check an arbitrary OData feed specified by a url, meaning the return type of an ajax call can be inferred by the F# compiler. I like this method because changes to arbitrary data sources (not just ones you own) cause compile errors rather than run-time errors. I think it would (eventually) be feasible to write libraries for both Elm and Haskell that make this possible (potentially with some compiler tricks on Elm's end). Once the data protocol is defined, other functional languages could probably be supported too. I really like this idea :)
Edinburgh university doesn't do computer science, they do "Informatics"; when he does the introductory lecture Phil justifies this by saying "No real science has science in its name: you don't say "physics science", "chemistry science", "biology science" or "social..." ah."
Cool.
You mean semantically?
Thanks for the link; very relevant! One of my goals is to rule out time leaks by restricting the primitives that are available. My FRP model differs from the classical formulation, so I need to give this more thought to be sure I can still have no time leaks *and* provide dynamic stream creation. Ah, the one browser I neglected to check! No JS errors, but it looks like certain events are just not triggered by Opera. Thanks for the pointer! Edit: Just read his conclusion. That's pretty much exactly how I chose to address the problem so far. I have *only* introduced a hybrid Event/Behavior though, no traditional Events or Behaviors.
Maybe the simplest approach would be to lex a collection of Haskell sources and pick the most common tokens?
It helps if you decide upon a few not entirely trivial examples to implement. One of the useful benchmark is to make a tetris or a breakout clone, or both. Also, since this is a web language, it would be interesting to see how to describe multi-page workflows and non-trivial web UIs. Do you have ideas for a higher-level API to make this easy, or are you still concentrating on the foundations?
Yes.
While `02` *might* make things slower at runtime and/or lead to higher memory usage, I think there's generally a much greater likelihood that it will make things faster and reduce memory usage, so I'd say it's worth the "risk".
Ah, I see what you mean: How do I account for the creation of a Tetris block or the destruction of a breakout brick? I don't have a good answer now. My focus so far has been on the basics of constructing GUI's (in the spirit of most of the web) rather than animations as was the initial intent of FRP. I have mixed feelings about FRP for animation. I have not given those two topics too much thought yet. I intend to have a module system that would allow you to reuse elements within a single code base (widgets, headers, footers, etc.). It would also probably be possible to model multi-page workflows as a Stream, switching views upon certain events. That doesn't let you have each page have its own url, but is this sort of what you mean? I haven't thought about any API's to do this though.
The bug fixed by this commit: http://hackage.haskell.org/trac/ghc/ticket/5624 Note that the commit was to a branch, not master. I have no idea whether this will be in 7.4, I'm just subscribed to the list.
do want uk only :(
Not sure how I feel about this.
Well, I did do the first module implementation in 1989, as well as adding several features at various points (though the basics of the module system were already fixed by the start of 1989)... The module system was something that was certainly worried about a lot at the time (and something I'd looked at fairly carefully in my PhD work). We were aware pretty early that type classes added significant power, to the extent that this obsoleted the abstype feature from pre-1.0. We also did a pretty careful and detailed analysis of Haskell modules against SML modules at one point (I think it was around 1993 when were thinking about the changes for Haskell 1.3 - there was even some documentation, I think). We also played around with various kinds of parameterised module - again, multi-parameter classes seemed to add similar power (though parameterised modules might be better for documentation purposes). Yes, we had human-readable/writable interfaces for quite a while (up to about Haskell 1.2, I think), but as the compiler became more complex, they became hard to maintain (and raised the compiler-writing barrier). Unlike C, they were usually automatically generated by the compiler, so not a burden on the programmer. It was necessary to use hand-written interfaces for a few boot-strapping reasons in some of the early compilers though. 
This also fits the Haskell lecture idiom of taking off some outer clothing during a talk. When Simon Peyton-Jones takes off his sweater, you know the talk is about to get exciting!
Wait, so ... Type errors can now happen at runtime? But only if ... um... something is being explicitly coerced? I'm confused.
Awesome slides. I should mention that if you're interested in seeing the layout of data types in memory for your Haskell program and the sharing properties, [vaccum](http://hackage.haskell.org/package/vacuum) is a great package for this. I've taken over maintenance of this package pretty recently and hope to be releasing a new 2.0.0.0 version soon with better docs and more package granularity (graphviz will be split out.) If you'd like to help, [patches are welcome](http://github.com/thoughtpolice/vacuum). :)
Yup, they're quite well made; certainly very comfy and warm. I sometimes sleep in mine.
Ok then I withdraw my remark. But how come "decode" take a ByteSting then? Does it assume an encoding?
Basically what the patch is saying is that for any expression which is ill typed, with this enabled, you will effectively replace the expression itself with the expression `error 'Type error goes here'`. So the error doesn't occur (at runtime) until the ill-typed expression is in fact evaluated. There's not much more to it than that, and the coercion bit is probably more of a reference to (recent) GHC internals. Don't worry about it too much.
I'm happy â€” it means we're one step closer to the [dream GHCi session](http://cdsmith.wordpress.com/2011/01/24/my-dream-ghci-session-take-2/). Of course if anyone starts compiling whole *programs* like this, we're in trouble.
Can I make a feature request? I'd really like vaccum(-cairo) to output these kind of box-and-arrow diagrams where's it's easy to count the number of words used (just count the number of boxes) and see where the indirections (pointers) are.
JSON is specified to use UTF-8 so there's no choice here.
How about function names in prelude, and some other tokens like parens and a few greek variables?
That does sound like a great idea. I've [created an issue on github](https://github.com/thoughtpolice/vacuum/issues/1) for this. I've also begun reviving the gallery on this note - compare your example of the list `[1,2]` on [slide 13](http://johantibell.com/files/stanford-2011/performance.html#%2813%29) with the [vacuum output](https://github.com/thoughtpolice/vacuum/raw/master/gallery/list.png). I made some notes on the differences in the github issue - please let me know what you think. `vacuum-cairo` contains some code that does show the actual values for an indirection, which would also be very helpful for implementing this (because the vacuum output currently doesn't really elucidate the fact that the `S#` nodes take *two* words, one for the constructor, *and* the value.) I plan on taking over maintenance of this package from Don too, and looting that code so it can go back into vacuum proper. It should make `vacuum-cairo` dramatically simpler as a result. Note that I probably won't get all this done by the time I release 2.0.0.0 either. The major focus for it is 1) It will have a new, active maintainer, me, 2) the graphviz output will be factored into a separate package (all thanks to Ivan Lazar Miljenovic, really) and 3) a lot of documentation and code cleanup, as well as utilities making rendering easier. These are all low hanging fruit but I'd like to get them out of the way before moving onto bigger things. Matt Morrow (now MIA, peace be with him) did FANTASTIC work on this package, but unfortunately much of the implementation is very undocumented with little explanation to be found, other than GHC sources. I'm slowly combing through it all and adding relevant documentation among other things. But eventually I'd like it to be a go-to package for diagnosing memory layout/usage of your data structures, and other sorts of low-level bitchwork people don't like doing by hand. I'll be making a posting to `haskell-cafe` sometime in the next week hopefully with my work so far and asking for any complaints/critiques/features people would like to see as I move forward. Be on the lookout.
Already is, just had to put in some CPP magic for the Map to HashMap change and Yesod now works with either aeson 0.3 or 0.4.
This makes me wonder if something similar could be done for ambiguous constraints by inserting `error "..."` where the instance dictionary would go.
Great tool, but the PVP rationale makes absolutely no sense. Having authors update their upper bounds is absolutely nuts. Excuse my rant, but immediately when a new package is uploaded, all packages that could depend on the newer version should be recompiled, tested, and upper bounds should be updated *without a new release*. Specifying upper bounds manually is broken, and only a last resort. The upper bound is decidable by building the package and running the tests. If it is not, then there should be a huge warning on the package saying that it belongs to the tar pit of software - software that will complicate your life and make you miserable. The upper bound is meta-data that should not be part of the package, but an annotation/graph that is kept independently of the software packages. The compiler version, the architecture, and the OS version are just as important as dependencies on other haskell libraries. Does it make sense to release a new version of every package on hackage just because Apple released a new major OS version? Or Microsoft? Or if libc which ghc depends on gets a new release? There is no guarantee that a build will succeed unless _the whole build environment_ is part of the dependency. Even with religious PVP followers, there is no way hackage can answer the question: "If I use the haskell platform on platform X, will package Y build?". It cannot, because it doesn't have proof that the package has been successfully built. The only reasonable solution is to have build bots or to let cabal send information back to a central location about successful build combinations. Wrt bounds, excluding certain buggy releases is reasonable, and lower bounds is a simple way to achieve that. However, explicitly excluding a release is really the only required feature for versioning. Everything else is decidable using a build bot. 
Well, here's one: http://brage.info/~svein/haskell.png
Best news of the day :)
No, he's right, it seems. From the GHC docs: &gt;-O2: &gt;Means: â€œApply every non-dangerous optimisation, even if it means significantly longer compile times.â€ &gt;The avoided â€œdangerousâ€ optimisations are those that can make runtime or space worse if you're unlucky. They are normally turned on or off individually. &gt;At the moment, -O2 is unlikely to produce better code than -O. 
Very clever and original. As a student (I'm working on my final dissertation, too) I can assure you that there are very few original project in our environment. Most of them are shitty webapp or other boring thing. This is a very original idea, both in design and realization! Keep going and keep us posted :)
I don't know what the reasonable default is, but notice that that the sentence you quoted refers to 'the moment', and as one can expect with documentation, the moment in question is no later than `29-May-2003 17:00` since the same sentence is found in [the docs for ghc-6.0](http://haskell.org/ghc/docs/6.0/html/users_guide/options-optimise.html#OPTIMISE-PKGS) Maybe they have contrived to keep it true! It totally goes against my experience, but maybe I write unlikely modules. Thus choosing something at random -- it was in a StackOverflow discussion yesterday -- just now I tried import qualified Data.Vector.Unboxed as V main = print . V.sum $ (V.replicate 5000000000 10 :: V.Vector Int) and wasn't surprised it took `0m14.664s` with `-O` but only `0m4.196s` with `-O2`. But there are so many things to consider, I don't mean to speak to the main question. 
...is it xmas already? There are happening so many cool releases on hackage lately... :-)
Now all we need is to do the same with 'not in scope' errors and we'll have an awesomeific GHCi.
Assuming that -O0 voters prefer -O1 over -O2, and -O2 voters prefer -O1 over -O0, -O0 and -O2 need to have 50% or more to win, otherwise the winner will be -O1.
ohmygod so awesome! You're the boss, bos. EDIT: this is also supremely awesome now that I think about it because prior to this I was using blaze-html to generate my own webpage with an outline of the results for my cryptography package. This is just so much better.
I wonder how analogous this is to a language with dynamic typing... haven't managed to really think it through. (Would the two produce runtime exceptions in the "same places"?)
Just fmap. A lot of fmap.
To be more precise, RFC4627 actually also allows additional UTF encodings beyond UTF-8: &gt; 3 Encoding &gt; &gt; JSON text SHALL be encoded in Unicode. The default encoding is UTF-8. &gt; &gt; Since the first two characters of a JSON text will always be ASCII &gt; characters [RFC0020], it is possible to determine whether an octet &gt; stream is UTF-8, UTF-16 (BE or LE), or UTF-32 (BE or LE) by looking &gt; at the pattern of nulls in the first four octets. &gt; 00 00 00 xx UTF-32BE &gt; 00 xx 00 xx UTF-16BE &gt; xx 00 00 00 UTF-32LE &gt; xx 00 xx 00 UTF-16LE &gt; xx xx xx xx UTF-8 N.b.: the mime-type `application/json` doesn't support any character-set declaration (as its supposed to be autodetected), probably as it's a `application/*`-type, and not a `text/*` But the good news is, that in 98% of all JSON applications, UTF-8 is used... but a conforming implementation might chose to encode the JSON text as UTF-16/UTF-32, so I have some fallback routines in my code which convert UTF other than UTF8 to down to UTF8 before feeeding it to aeson, just to be on the safe side...
but only if you don't consume all of the data contained in the RPC message... otherwise you'll end up spending more time in the GC... since the JSON RPC message must be validated (i.e. the parse-tree skeleton must be created) before any actual conversion to native Haskell data-structure can start... I'm wondering what's actually being deferred when parsing lazy; is it just the actual decoding of numbers and strings into Double/Integer/Text/Bool values?
the interesting thing about haskell is that it's more powerful in some areas than dynamically typed languages - e.g. there's code that will work in haskell and it won't work in any dynamically typed language. in dynamically typed languages, the type of the value/object depends on the previous runtime values, but in haskell value's type can depend statically on 'future' values (their types), e.g.: x &lt;- read &lt;$&gt; getLine print $ x + 2 this will never work in e.g. python.
This is awesome!
fwiw, even `-O1` enables dangerous optimizations as can be seen on http://hackage.haskell.org/trac/ghc/ticket/5671 ;-)
Reminds me a bit of how the historic SICP video lectures start, by saying that neither of the two words in "computer science" are appropriate...
Leave it as is. 
Great stuff! Could we have labels on all the axis pretty please?
Er, no? Firstly, the flot plotting library doesn't support axis labels (I know, right?). But I managed to fudge a way to get units on the x axis; whew! But the y axis has very little meaning on both charts, so it doesn't make sense for it to be labeled. The heights I spikes in the KDE chart are visually useful, but they do *not* correspond to probabilities or any other number you could do anything with.
I'd rather have either of the other options than O1, which seems like a half measure to me.
&gt; My goal is to make web development easier and more pleasant, and I decided to start from scratch. I hope that we will someday think of HTML/CSS/JS the same way we currently think of assembly (i.e. not for people). I love it!
~~Looks like it got merged into HEAD today, which I believe means it was automatically pushed to the 7.4 branch at the time being as well. So we'll be seeing it for 7.4.1~~ Disregard, I misread my own email.
I stand corrected.
whatever you do, don't forget &gt;&gt;= =&lt;&lt; &lt;$&gt; &lt;*&gt; and include everything that might show in a type signature; why bother putting haskell code on the fridge when you can Just put the types on the fridge instead :) It *might* be nice to include fancy lambda, compose, etc symbols.
&gt; But the y axis has very little meaning on both charts, so it doesn't make sense for it to be labeled. If it makes sense to plot it, it makes sense to label it. (ironclad rule!) Looks like the y axis in the circles-and-dots plot is time: &gt; Measurements are displayed on the y axis in the order in which they occurred. I was completely mystified at first, then thought it was simply jitter and the plot itself a jittered plot of the samples, but then noticed that there are patterns and read the text. Something like "Measurement #" would be a fine label. &gt; The heights I spikes in the KDE chart are visually useful, but they do not correspond to probabilities or any other number you could do anything with. This contradicts the explanation after the diagrams: &gt; The chart on the left is a kernel density estimate (also known as a KDE) of time measurements. This graphs the probability of any given time measurement occurring. In any case, if "probability" is a bad word, then simple "KDE density" would be better than nothing. P.S. I wonder why I get some JS junk at the beginning of the page like this: ','","");this.element_.insertAdjacentHTML("BeforeEnd",AU.join(""))};M.stroke=function(AM){var m=10;var [... and so on...] 
Why do you think it a half measure? Why isn't it striking the right balance?
Upvoted before I read it. Was not disappointed. There _is_ a serious discussion to be had, though. Long type variable names have both pros and cons -- as with everything. As mentioned in the article, long names make for some absolutely gorgeous documentation. The flip side, though, is that type errors (and, for those of you who are `-Wall`y, the explicit type signatures that you end up writing down most of the time) can get a bit unwieldy if you're not doing something completely trivial. For example, here's a real life type signature from some real life code: addEdge, subEdge :: (Ord nodeId, Ord time, Num time) =&gt; Life time -&gt; nodeId -&gt; nodeId -&gt; Graph nodeId time -&gt; Graph nodeId time Beautiful, non? But throw in a few function compositions and type inference, and you get a type error with a type that sprawls along several lines, more and more indented as GHC's pretty-printer breaks the type into more and more lines, and you might start believing that you would have been better off with `n` and `t` and fewer over-80-column types.
Thanks for that detailed response! I'm a total geek for language archaeology. It's not surprising that there's lots more hidden behind each brief topical heading in "Lazy With Class", and it's nice to get some of that extra history. 
Here are some [benchmark results](http://basvandijk.github.com/monad-control.html) that compare the original `monad-peel`, the previous `monad-control-0.2.0.3` and the new `monad-control-0.3`. The benchmarks use Bryan O'Sullivan's excellent new `criterion-0.6` package. 
Anybody try this in your python/ghc ? In python: &gt;&gt;&gt; pow(123456789, 123456789, 10**10) 6899632709L In GHC: ghci&gt; Codec.Encryption.RSA.NumberTheory.expmod 123456789 123456789 10^10 3486784401 the `fastpow` implementation by cgibbard got 3486784401 I am not familiar with number theory, but which is wrong?
This seems like a bad habit to get into. The type signature is like the shape of a puzzle piece: it doesn't show you exactly where it was designed to go, but it shows you where it fits. Using short and generic type variable names allows one to see where things fit at a glance. Unusual naming of type variables obscures this. As a programmer, I'll want more information, but not at the cost of quickly seeing the 'shape of things'. A better solution is to have your IDE show the type signature, the parameter names from the definitions, as well as the relevant documentation.
I have similar concerns. Maybe a middle ground would be to use a consistent naming convention in a library, and document that convention in at least one place in the haddocks?
Isn't this problem better solved by using type synonyms? That way, you get a compile-time check as well as the mnemonic. (PhoneNumber and SocialSecurityNumber are both [Int], but you would probably sleep better at night if processPaycheck had the type SocialSecurityNumber -&gt; Amount rather than (Floating t) =&gt; [Int] -&gt; t, no? Maybe I just haven't worked enough with big libraries yet, but it's hard to see the middle ground between functions whose type variables deserve full generality (length :: [a] -&gt; Int) and functions that should pretty much operate on unique user-defined types (launchNuclearMissile :: KeyCode -&gt; KeyCode -&gt; IO World). 
Well nothing could go wrong with that!
I'm not sure type synonyms solve the same problem at all. The problem discussed involves type variables; the type synonyms you suggested in your post renamed type constants.
Er, yes. But doesn't the point still stand? e is supposed to be an error message, yet judging by its type signature, validate is supposed to be just as happy with Just [1,2,3] as with "Help! Something went wrong!", or at least a member of some kind of "Errorable" type class. I guess my point is: if you can anticipate the type well enough to describe it in a type variable, why not describe it with a type (or typeclass)? I'm relatively new to haskell, so the question isn't intended as rhetorical. I'm just curious.
You don't get a compile time check with type synonyms: you can pass one to a function expecting the other. They are just synonyms, after all.
The documentation that is standard fare in haskell is atrocious when there is not really much written about a particular function. That is the only thing I care about.
I think type variables can be useful in the same kind of places where longer variable names can be useful: if the type is specific. If you're writing a mapping function, or a generic algorithm, there is no more useful name for your type variables than `a`, and your variables should probably just be names `x` and `xs`. Anything longer (`element`) would just obscure the meaning. However, if you're in some specific domain function, call the variable holding the user data `usr` or `user` instead of `u`. I think the same logic should hold for type variables, except you should err on the side of short variable names. In my experience, the longer the type variable name, the harder it becomes to understand the type at a glance.
You're right--my mistake. Is there a way to do that without setting up newtypes, which seem to involve a lot of unpacking and repacking? Ideally what I'd like instead of type synonyms are type _hyponyms_. That is, I'd like to write: hypotype KeyCode = [Int] hypotype PhoneNumber = [Int] so that I could use `KeyCode` or `PhoneNumber` wherever a function expects `[Int]`, but not `KeyCode` where `PhoneNumber` is expected or vice-versa.
Well, GUIs also require entities whose life cycles are independent of each other. You might want to open windows (like the chat popup in Gmail) or create formlets and have them communicate among themselves. A simple game is nothing more than a fun way to tease out these problems in a more self-contained setting (say, your only dependency is the canvas).
I don't understand the complaint about long type errors. If you would use short type variables, it wouldn't be any more comprehensible. Also if the layout is the problem, how about improving the output, or use better mechanism (type error visualisation).
Wouldn't it be quit easy to make a IDE mechanism that enables you to see long types as short as possible for overall viewing.
&gt; The type signature is like the shape of a puzzle piece: it doesn't show you exactly where it was designed to go, but it shows you where it fits. This expectation is really surprising to me, who is not a Haskell programmer. Is it widespread among Haskell user?
It's helpful for IDEs, which are often confronted with somewhat incorrect programs during editing, and you still want to run partial semantic analysis on them.
Is it documented anywhere what the actual optimizations are that are turned on by `-O` and `-O2`? The ghc docs aren't very clear on the subject.
BTW [here are some benchmark results](http://basvandijk.github.com/aeson.html) that compare the automatic derivation of `ToJSON` and `FromJSON` using template-haskell, the old generic code using SYB and the new GHC generics. As can be seen, the new generics code is just as fast as template-haskell with one exception: converting from JSON to a big sum type. I'm still thinking about how to make that faster.
newtype avoids unpacking/packing overhead, so newtype KeyCode = KeyCode [Int] is exactly what you want. http://stackoverflow.com/questions/991467/haskell-type-vs-newtype-with-respect-to-type-safety
I like how you subconsciously capitalized "Just".
Even if you made a type class, you would still need to name the type variable that was an element of that class. ...also, if your function works for a type with variables that have no class restrictions, why would you put some arbitrary class restrictions on it?
This is anecdotal evidence from developing *one* program with descriptive type variable names, so take this with a grain of a salt, but: I have found that with the more involved type errors, long names are just noise that your eye has to skip. It's annoying, and it's sometimes just hard to find the few nuggets of useful information among it all. I haven't had the same problem with big types that had short variable names. As for "why don't you just improve the output", I say go for it! Then let me know and I'll shift my threshold between using short names and using descriptive names appropriately. In the meantime I'll be getting things done with short- to medium-length type variable names and strong conventions.
&gt; Is there a way to do that without setting up newtypes [or using data]? No. `newtype` *is* the way to do that.
This already deals with class constraints.
How did you manage to yield a 60x speedup?
A problem with your `hypotype` is with type inference: if I write a function that uses a list of ints without giving a signature, does the inferred type list `[Int]`, `KeyCode` or `PhoneNumber`? Newtypes solve this, since you make it explicit in the constructor or when pattern matching. As for ease of use: wrapping and unwrapping can become inconvenient. Sometimes, typeclasses (`Num`, `IsString` etc.) can help. There is also the [newtype package](http://hackage.haskell.org/package/newtype), which looks interesting, but I've never used it.
Yeah. It's a kind of hard intuition to teach, but the metaphor is that putting Haskell programs together is like putting Lego blocks together: they only fit in specific ways, but in fact those were the ways you wanted anyway.
BTW, in your bonus slide about sizes of common data structures there's an error (or caveat worth mentioning): For ASCII-Strings the cost is 3N, not 5N. GHC's garbage collector deduplicates small Chars and Ints (8 bit, I think). So, a newly allocated String takes up 5N bytes, but as soon as the GC has run, it will only be 3N bytes (which is the list overhead).
Cool. For some reason I assumed it didn't. 
It's because I changed the type of the *Run* function from: t n Î² â†’ n (t o Î²) to: t n Î² â†’ n (StT t Î²) The former returns the computation *t o Î²* that when executed restores the final monadic state of the argument computation. The latter just returns this final monadic state. Here *StT t Î²* is a type function. For example: newtype StT (StateT s) Î± = StState {unStState âˆ· (Î±, s)} Now when you need to lift a control operation through a stack of monad transformers you need to combine the *Run* functions of each of the monad transformers. In the previous design I needed to do some complex and expensive monadic ["rewiring"](http://hackage.haskell.org/packages/archive/monad-control/0.2.0.3/doc/html/Control-Monad-Trans-Control.html#v:liftLiftControlBase) (Look at all the big compostion of *liftM (join . lift)* in there). Because the new *Run* function just returns the monadic state it's much easier and faster to [combine](http://hackage.haskell.org/packages/archive/monad-control/0.3.0.1/doc/html/Control-Monad-Trans-Control.html#v:defaultLiftBaseWith) them. 
Runtime overhead, or writetime overhead? If I understand correctly, when I define: newtype Path = Path String then whenever I want to deal with a Path, I either need to unwrap it by hand or pass it through an unwrapper function like: decantPath :: Path -&gt; String decantPath (Path x) = x Is there a better way?
Well, the idea is that the class constraint would serve that purpose. For instance: (Monad m, Errorable e)=&gt; Form m i e v a -&gt; Validator m e a -&gt; Form m i e v a tells you (concisely) what sort of thing e is without having to write out a descriptive type variable three times. I can appreciate the desire not to restrict the full generality of the function, but restrictions of full generality happen all the time in order to prevent syntactically correct but pragmatically meaningless computations. Or perhaps the folks who wrote `digestive-functors` really did anticipate someone passing Just [1,2,3] as the error, and didn't want to deny them that opportunity. Essentially, there are two views. On the first view, you are free to pass whatever you want as the error argument without specifying what types can serve as error types, and hope that whatever function that actually deals with `e` down the line will catch your mistake if you accidentally pass it `i`, `v` or `a`. On the second view, you define some sort of "Errorable" class and ensure that whatever you pass as `e` is an instance of that class. I'm not saying that the second way is preferable-- I'm saying that I don't see some kind of intrinsic, extra-conventional reason to prefer one to the other. I guess my question turns on what it means to say that "my function _works_ [emph. added] for a type with variables that have no class restrictions". If my function could potentially store a phone number in a list of social security numbers, is it "working" better than its alternative that prevents that automatically with a compile-time check? My general intuition is that if you know enough about what a type is doing in a given function to give it a descriptive name, it seems you might as well try to enforce that through the type system. If you want to say that this is the convention in Haskell, that's a fine and reasonable answer. I'm just saying that it's not obvious to me why we should prefer that point on the safety-generality continuum _ a priori_ rather than some other point.
I think your first question kind of uniquely picks out its own answer: absent an explicit type signature, default to the most general type. It's my understanding that this is bread and butter type inference. The newtype package looks interesting, and thanks for recommending it, but it seems a bit like damage control: perhaps it makes it a bit easier to map over my `KeyCode` type, but it won't let you forget that KeyCodes are really souped-up record containers rather than first class types in their own right, so to speak.
I suddenly started imagining tetris, but with types. :)
That's not what type aliases are for. If you really want to sleep better at night, use newtypes and hide the constructors. That way you can ensure that only valid SocialSecurityNumbers are floating around out there. Why use a naming convention that can mislead since it's never checked, when you can leverage the type system to actually enforce the semantics you wish to imply?
&gt; absent an explicit type signature, default to the most general type. With hypotypes, there may be no most general type. If I write hypotype PhoneNumber = [Int] then `[] :: PhoneNumber`, and `[] :: [Int]`, and neither is more general than the other. Moreover, the only type more general than `PhoneNumber` is `forall a. a`, which is clearly not a good choice.
Nothing doing, but I do think "Just" every time I say "just".
If you'll indulge my naivete here, why is [Int] not more general than PhoneNumber? It seems to me that a rule like "if type t is a hypotype of type s, then s is more general than type t" would give me the partial ordering I want?
You asked the type system to prevent you from using a PhoneNumber when an [Int] was expected and vice versa. Therefore neither is more general than the other (where "more general" means exactly "can be used wherever the other can").
I'm fine with passing a PhoneNumber to a function expecting [Int], just not with passing [Int] (or worse, some other "hypotype" deriving from [Int]) where PhoneNumber is expected. 
Haskell doesn't do subtyping, which seems to be what you're asking for. In principle, it's possible, but type inference in the presence of subtyping is a can of worms that nobody enjoys dealing with.
Stack traces are usually the other way around, no? I.e., the first item is the topmost stack frame. This would also put the most important information first.
it has to be! "...Get rid of the pestilential dependency on the Chart library" -&gt; thanks, thanks, thanks! I don't know how many hours I wasted installing the gtk2hs dependencies on various Mac OS/ghc combinations. 
I think this is a fantastic idea! I'm excited for things like this to increase the incentive to use long names. I think usage of short local variables / alphabetic type variables in regular Haskell style is justified, to a point, but this no longer makes much sense if identifier length no longer effected code layout. If the IDE representation of a local identifier were constant-sized placeholders, then there wouldn't be so much pressure for concise names, and code could be closer to self-documenting. The actual identifiers could be placed alongside the code, perhaps annotating lines connecting identical references. Anyway, I've been working on things along this line. It's not all that easy, really - I've been putting a decent amount of thought and effort into it, and it's still very much a work in progress. Then again, I've only managed a bit of time between schoolwork. Hopefully I'll be able to get a lot more done soon. Here's some old output (lines indicate equality constraints, red boxes indicates sections of the expression that needed to be isolated in order to make it typecheck, class constraints of polymorphic variables are computed but not displayed): http://www.mgsloan.com/sinu3.svg The rendering has since been partially rewritten in the diagrams library, leading to much nicer code, but haven't quite gotten it explaining the relationships between types in the expression yet. I know the visualization above isn't quite "there" yet, but switching to diagrams has made making visual improvements a /lot/ faster. Visualization of the type of Control.Arrow.(***) http://www.mgsloan.com/sinu4.png https://github.com/mgsloan/sinuhask
It leaks memory, but apart from that the first definition works just fine in Haskell.
Sure: Define functions that operate on Paths, and leave the manual unpacking in the rare cases it's necessary as a reminder of the conversion going on.
This is really excellent. I think report could be improved in several ways. * It would be nice to have error bars on summary plots. That will help to understand whether difference between measurements is genuine or just a statistical glitch (if error estimate could be trusted). * IMHO confidence interval would be easier to interpret if they are written using Â± notation. One common way to write asymmetric errors is to place +error on top of -error, or simple write 588.9 +1.3 -1.6ns. It requires less mental effort than subtraction of lower bound from estimate. 
Well, on a related tack, if you use type *families* rather than synonyms, you can often reduce the number of type parameters you need to accept for type classes and for data types.
Not really. Gofer used to have a notion of a 'restricted type synonym' that offered vaguely similar functionality, where you could at least scope access to a type synonym and use that to loosely ape what you describe. http://cvs.haskell.org/Hugs/pages/hugsman/exts.html (section 7.3.5)
In his case I think 'hypotypes' would be a sound extension in that they don't run into the usual subtyping woes, if you don't flip the direction of inference on '-&gt;'. They vaguely resemble the old boxy type machinery, which similarly didn't flip sign. That said, they sound like a pain to implement and I have no idea how they should interoperate with typeclasses, etc. @wvoq, in general you should probably just bite the bullet and use a newtype that way you can implement appropriate classes, etc. for your type.
It would be really be great, if the maintainership status could be clarified for haskell-mode Currently it seems as if haskell-mode was an orphaned project nobody maintains left for bitrotting... :-/ 
I love mine. Its solid. I have to admit my biggest problem is that it doesn't have a front zipper so its a bit awkward to get on/off, but the logo is crisp. I don't wear mine very often, so I'm not aware of how well it holds up to repeated wash cycles, though.
I was able to get mine shipped to the US, it just was a pain navigating the UK-centric web form.
SPJ can be disconcerting. He walks around seeming all prim and respectable and stoically British, but when he starts talking he becomes so incredibly animated. It is truly a joy to behold, though I confess I sometimes feel like I'm watching [Big](http://www.imdb.com/title/tt0094737/). ;)
Um. Roughly, yes. I keep meaning to get to it, but.. All it really needs is someone to vet patches, I think.
Ah you're right! I gave up too easily. Just placed my order, thanks!
Added an example specialization of the Arrow typeclass - hoping to make this thing easily extensible, so that modules can come with definitions for pretty renderings of the types defined. http://www.mgsloan.com/sinu5.png https://github.com/mgsloan/sinuhask/commit/ef22bb47c993ab4cb406b61a15e7e5e0e5572238
http://chrisdone.com/posts/2011-08-21-haskell-emacs.html
Yeah, that looks promising, although the following sounds a bit unsettling: &gt; [..] the project is clearly focused on my personal needs rather than general Haskellers but that doesnâ€™t concern me too much. Iâ€™d expect there to be a substantial overlap, anyway. Nevertheless, as long as `haskell-emacs` isn't ready to officially supersede `haskell-mode`, we still need a maintained `haskell-mode`...
I think the darcs repo is dead. I have a git mirror/fork of the darcs repo on github. I keep it up to date with the darcs repo, but I haven't seen any patches in the darcs repo in a long time. https://github.com/pheaver/haskell-mode I encourage you to submit issues, fork and submit patches. I would be happy to see Chris Done's package become a popular replacement for haskell-mode. I am not very familiar with the code in haskell-mode or Chris's package; I would be interested in discussing the differences between them and if there is any way to get one of them to have the features of both. That discussion could start here: https://github.com/pheaver/haskell-mode/wiki 
One solution is to use ffmpeg to decode your video into raw format and then just pipe it into your haskell program. Then you just read pixel data from stdin(look at the ffmpeg documentation for different pixel formats, pick the one that suits you) and write to stdout which you pipe into ffmpeg again. Use named pipes, it helps a lot.
...does this mean, you'd volunteer to take over officially maintainership for `haskell-mode`? That'd be a great improvement to the current situation... :-)
There is an ffmpeg binding on Hackage, though it's not really polished and not actively maintained. But I managed to get some video on the screen once using it.
What happened to Svein's haskell based indentation tool? No matter which setting I tried I couldn't make haskell-mode stop indenting and let me indent manually. I needed to do that because all 3 indentation modes are erroneous and never seem to realize when a function definition ends. Maybe they cannot and there's a need for a special C-S-m to insert a new function break. Just take HaskellSysInfo.hs posted by Mark here on reddit and try to insert a new line at the end. Both haskell-mode and haskell-emacs will insert 2 levels of indentation instead of 0. It's unfortunate that cc-mode does proper indenting but Haskell editing modes in vim and emacs do not. We should be able to do better.
Try [easyVision](http://perception.inf.um.es/easyVision/). There's a tutorial and some cool examples on their blog.
The purpose of -O is to be the right default set of optimisations for when you want an optimised build. Saying that -O2 should by the default is wrong by *definition*. If necessary what should happen is for optimisations currently in -O2 to be moved to -O.
I'd love to see the opposite: writing a Haskell interpreter in Lisp.
&gt; Currently it seems as if haskell-mode was an orphaned project nobody maintains left for bitrotting. That is a pretty extreme statement. It seems true that Svein hasn't focused on this project for a while, or at least he hasn't recorded any patches in the repo. That is a far cry from saying that the project is "orphaned". Svein is still around and is actively maintaining several important Haskell packages. If you have patches, why not submit them? I assume that Svein would happily accept them, unless you have heard from Svein otherwise.
There's this too: [Write Yourself a Scheme in 48 Hours](http://jonathan.tang.name/files/scheme_in_48/tutorial/overview.html)
&gt; I think the darcs repo is dead. Did you email Svein and ask him about it before making such a statement? If you have patches, did you submit them? 
No, he's roughly right. I'd accept them *eventually*, but "eventually" is, at the moment, an unknown period of time in the future. Might be this weekend, might be next year. I've been trying to get a bit more structure on things, mainly by using beeminder, so there is at least *some* hope that I'll get through the backlog. Still, it'd be far more likely to work in a timely manner if someone else took over. I had such plans for haskell-mode, too. :/ (And yeah, I have two reddit accounts. One of these days I'll fix the password on the other one to something I can remember.)
Came, saw, gave up. Haskell is horrible to parse even with correct programs, and I'm not experienced enough at writing parsers to do it in a timely manner, I don't think. It'd be great, though.
&gt; No matter which setting I tried I couldn't make haskell-mode stop indenting and let me indent manually. Haskell indentation is hard, but the variations available in haskell mode are a start. Ongoing work by Chris and by Baughn is interesting to watch. Are you aware that haskell-mode offers you a choice? Hit tab a few times to see a few different indentation possibilities. Personally, I find that `haskell-indent` is usually helpful for my style. Quite often one of the choices it offers is the one I wanted. In the minority of cases where none of the options is exactly right, one of them is always close enough that I still saved time. In any case, you don't need to enable any of the three indentation modes if you don't want them. Just comment all three of them out in your `.emacs`. 
Am I missing something? Isn't the point of a strongly statically typed language that you know this sort of thing isn't lurking out there waiting to crash at runtime? I guess -Wall would catch the unused variable.
Yeah, that's a good next step: more complicated, but also a lot more comprehensive - if I recall correctly, it's a full implementation of Scheme, whereas what I linked implements just a subset of Lisp.
Time to talk to GHCHQ and ask for comments? We can't let this morph into the famous "parsing C++ is hard and slow". We ought to do better, given that GHCHQ is way quicker at implementing changes than some ISO C++ group.
When inserting a newline in Mark's HaskellSysInfo.hs file posted here recently. What you describe explains what's going on pretty well (not retested yet).
Thanks Yitz, I didn't know that. I will do some experimentation. Did I grok your post correctly that if I omit all add-hooks to select an indenter it won't use any indenter at all?
This is actually exactly what it is like. When working on a project Euler problem, it is very useful to split the problem into discrete subproblems where you know the input will be some form of the input parameters (what the problem asks of you) and the types they take, and result in usually an integral solution. So something like: `solution :: p1 -&gt; p2 -&gt; p3 -&gt; ... -&gt; Integer` Then I figure, well, there are some intermediate representations where I might need to use the parameters to make a matrix of `[[Integer]]`, so I probably need `p1 -&gt; [[Integer]]`, or maybe `p1` is an `[[Integer]]`, and so I might need a function of `[[Integer]] -&gt; Integer` at some point, and so on. More complex solutions involving monads and combinators are arrived at in the same manner design patterns are used in object oriented code. By experience you learn that something to turn a list into a scalar value is a fold or reduction, a catamorphism some will call it. They come in many flavors. Something that generates from a simple seed value many values or a structure is an anamorphism. Most Euler problems take the form, then, of a hylomorphism: a composition of an anamorphism and a catamorphism. You take in initial parameters to construct a structure, then fold the structure into a single resulting value. Recognizing and then constructing the puzzle pieces is an interesting and fun part of programming in Haskell, as generally speaking, given that each piece constructs the right structure, there is generally only a limited number of ways to put the pieces together to construct the right value. So in a way, it is a lot like tetris, or a board puzzle. The more descriptive your immediate functions areâ€”the stronger and more specific the typesâ€”the more likely you are to arrive at only having one correct solution. For example, `[Integer]` is very general and might be used many times, but if you are representing an ordered sorted list, you might want to use a type that makes that explicit, and make sure the function that consumes only ordered, sorted lists takes that type. Or if you are only dealing with unique instances, a `Set Integer` as a result value is more descriptive than a function that returns an `[Integer]` that only contains unique elements.
I see the 'septembre' current version but didn't know this. Thanks a lot for posting this!
&gt; Still, it'd be far more likely to work in a timely manner if someone else took over. ...so, do you *want* someone else to take over haskell-mode?
Well there's my [Alef](http://gergo.erdi.hu/projects/alef/), but unfortunately it has some bugs in its typechecker that I never got around to fix. Maybe one day... one day....
Oh, so it follows [a second-generation, outsideâ€“in, pull-based, multiple-stakeholder, multiple-scale, high-automation, agile methodology](http://en.wikipedia.org/wiki/Behavior_Driven_Development). I'm relieved! (Nice job though!)
Alef looks like something I'd like to program in. You should fix it AND write an explanatory blog post about your implementation of type checking and inference. I would love to read such a thing. 
What does it do ? 
As someone who uses the FIX protocol every day, I am always glad to see more implementations in a variety of languages.
[fixprotocol.org](http://fixprotocol.org/). It's used primarily for communicating financial related things between parties, like trading on an exchange. This library is a communication point for one party, i.e. someone running this should be able to send and receive the appropriate messages to do the financial tasks they wish to do with the other party.
Thnx for the good explanation
There is still room for improvement and we are glad if people want to join and help improving it. So please feel free... any comments, suggestions, improvements are welcomed.
And so you should be relieved, you cynical god you. This methodology describes a cycle of interactions with well-defined outputs, resulting in the delivery of working, tested software that matters. And who doesn't want software that matters?
While I'm sceptical of BDD's relevance to Haskell given the vastly increased reliability extensive use of the type system offers, kudos for the effort, and HSpec looks like a nice tool.
If someone would volunteer, sure, that'd be great.
Well, I don't think it's as bad as C++. :P It's more a matter of me not being very experienced with the whole deal. Haskell is the most complex thing I've ever tried to parse.
Well, there is still the old [Yale Haskell system](http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/lang/lisp/code/syntax/haskell/0.html).
Never been a fan of FAST, but it actually looks reasonable when done in haskell.
&gt; And who doesn't want software that matters? Seriousness of intent is killing all the joy in the profession, I say.
Just in case it was unclear, my second sentence is the second half of the quote linked by godofpumpkins. It do not endorse it...
I like this a lot, but rather strongly feel that the use of geometric shapes plus colour buys you nothing but reduced readability over something like e.g. the Greek alphabet.
How does FIX compare to [OpenTransactions](https://github.com/FellowTraveler/Open-Transactions)? I guess it will be hard to find someone aware of both.
FIX is just a protocol to communicate your "financial" intention to the other node, for example an OpenTransaction node.
This is probably meant to be used as a library, yet LICENSE states GPL. My i ask who you see contributing and using the code?
You can easily set up different layouts for different workspaces: http://xmonad.org/xmonad-docs/xmonad-contrib/XMonad-Layout-PerWorkspace.html
I am aware of this feature but perhaps you misunderstood my request. Upon shutdown, all information of tasks running in each terminal, programs, layouts etc all vanish. I am looking for an implementation (or implement) code that will *preserve* workspaces upon shutdown and allow work to effectively resume from before.
The lincense is meant to be GPLv2; sorry for the confusion I will change this. thnx for pointing this out!
The terms to look for are "session management". I'm not aware of any specific support in XMonad, but I haven't really looked. 99% of the time I hibernate instead of shutting down, so I haven't gone through the effort to find better persistence solutions.
great, ghc-7.4 gets better and better... =) ...and when can we expect a ghc-7.4 release candidate? ;-)
Anyone who needs to communicate to an electronic financial market node providing a FIX interface, like most exchanges/brokers do, in order to fetch some information or to trade; like algorithmic traders.
If you want information *in* the apps to be preserved, the window manager can't help you. The only way to accomplish this is by hibernating instead of shutting down.
In case you didn't know, there is a [r/xmonad](http://www.reddit.com/r/xmonad).
Ah thanks, subscribed.
It will not be in 7.4. Also note that earlier in the day Simon committed a patch to [make `forkProcess` work with `+RTS -N`](http://hackage.haskell.org/trac/ghc/changeset/8b75acd3ca25165536f18976c8d80cb62ad613e4). Previously `forkProcess` only worked with the threaded RTS if you were only using -N1. Either of these features will enable us to do things like make a Cloud Haskell backend that uses multiple local OS processes connected with pipes and/or shared memory. In the longer term we think this would be a good model for really large numbers of cores where having a global GC'd heap starts to scale less well. For example with 64 cores you might want 8 OS process each with an RTS instance using -N8.
Whilst it is (perhaps) true that Haskell's features force programmers down the path to more reliable software, this doesn't mean the software actually operates as intended. What's the point in only knowing you're doing something reliably if you're not sure that outcome of those actions is correct? That said, it's always important to critique testing methodology as we would anything else.
Thanks, i understand the technical purpose. Since this seems to be solving a technical business problem, have you considered using a more business-friendly license like BSD or LGPL?
This is probably not the *answer* you're looking for, but Notion has the [*feature*](https://sourceforge.net/apps/mediawiki/notion/index.php?title=Tour#Specifying_that_a_certain_program_should_always_be_in_a_certain_frame) you're looking for. :) [Edit] That info is for Ion2, and since Notion is based on Ion3 there might be a different, better way. I haven't used (Not)ion in a while.
&gt; Handler b v a jumps right off the page at me, while Handler base view a blends in. This isn't necessarily a 100% valid point because the surrounding context is prose rather than code, but I think it does make the point that the shorter names are easier to scan. And of course, it makes perfect sense. There's less noise for the eye to sort through. This is a splendid example of post-hoc rationalization. "Well, my decision makes perfect sense to me..." &gt; Secondly, if you saw the type Handler base view a, would that really aid a newcomer's understanding more than the single letter variant? Yes! &gt; I contend that in this case it would not. base could refer to a numerical radix This is actually kind of _funny_. I mean, seriously? &gt; To help people more, we'd probably have to go to something like Handler baseSnaplet viewSnaplet a. How about: nestSnaplet :: ByteString -&gt; (Lens view1 (Snaplet view2)) -&gt; SnapletInit base view2 -&gt; Initializer base view1 (Snaplet view2) is that so bad? (On a side note, even if you think I'm the mad one and 'b' and 'v' are fine, please, please just start numbering from the beginning. I think having both 'v' and 'v1' is pretty annoying! I also hate the prime convention, so maybe I'm just cranky.) &gt; it's difficult to tell the long type variables apart from the type constructors Come on, this is what the capitalization requirements and syntax highlighting support are for! &gt; It might not be obvious, but I did put significant thought into the choice of b v a [1], and as a result, any time you see this general pattern of type variables, you instantly have a pretty good idea that you're dealing with a MonadSnaplet. This, I think, it the crux of the issue. If you're writing a library, it is very, very, _very_ easy to fall into the trap of thinking that the abstractions of your library are Really Important Abstractions. So **of course** programmers will go to any lengths to memorize the conventions of your library, because DUH! it's The Most Important Thing. Unfortunately, having extremely well though out conventions is only about 20% of the battle of making something well designed. Another good 20% is making those conventions _discoverable._ It's all well and good that the author of the library can link directly to the spot in documentation where some single-letter type variable convention is explained. But try to find that explanation from the documentation of nestSnaplet: http://hackage.haskell.org/packages/archive/snap/0.7/doc/html/Snap-Snaplet.html#v:nestSnaplet Go on. The best I've got is clicking Initializer, clicking MonadSnaplet, and magically knowing to scroll UP from there to find the explanation in the middle of three paragraphs. There are probably some interesting general documentation problems to solve here with Haskell generally, so it's not like this is the author's fault or anything. How DO you document meaning of type variables? Does Haddock support such a thing? But there's a problem here nonetheless. Multiply that hard to discover type variable convention by the ten or so libraries somebody might want to use. All of which, are of course, Special Snowflakes that the programmer is SO interested in memorizing the conventions of...
It also enables gradual garbage collection of whole processes. The classic Apache/IIS method of fixing memory leaks in code is to kill workers. It works because you can't trust every developer to write bug-free code. That's not true for Haskell, and it's definitely not true for C#, PHP and Perl, the dominant languages of the web.
As someone who has not used Snap, _yes_ `base view a` is more meaningful to me than `something something_else some_other_thing`! (Bearing in mind that `a` is convention for "an arbitrary contained type" and has meaning in Haskell contexts too.) It gives me something to search the documentation for, if nothing else, and a word to say in my head when I read the code. And `Form something something_else some_other_thing thingy some_other_thingy` isn't all that helpful either. You've got a golden opportunity to actually say something powerful in your types, and it's a terrible wasted opportunity when it just contains mumbling about thingies and sommatorothers. You may not want to make a speech, but say _something_.
&gt; This is a splendid example of post-hoc rationalization. "Well, my decision makes perfect &gt; sense to me..." No, it's an observation of the very real visual distinction between a series of single-letter tokens in the midst of others that are multiple letters. &gt; &gt; I contend that in this case it would not. base could refer to a numerical radix &gt; This is actually kind of funny. I mean, seriously? Yes. That example is meant to demonstrate the broad idea, not literally what I would expect the reader to think. &gt; Come on, this is what the capitalization requirements and syntax highlighting support are for! The capitalization requirements aren't enough of a distinction as my example shows. Syntax highlighting doesn't exist everywhere, and I don't recall seeing one that makes this particular distinction. &gt; If you're writing a library, it is very, very, very easy to fall into the trap of thinking that the abstractions of your library are Really Important Abstractions. You can't expect to learn a complex library without becoming familiar with the abstractions it uses. &gt; Unfortunately, having extremely well though out conventions is only about 20% of the battle of making something well designed. Another good 20% is making those conventions *discoverable*. I agree that this is the key issue here. There certainly may be ways we can improve our tools to help the problem. But at the end of the day it's a documentation problem. The state of this particular example will almost certainly appear worse to a random person who comes to the problem via this article than it is to someone who is actually trying to learn how to use the library.
Aside: Azul's C4 collector seems to be able to handle 10s of GB heaps without increased pause times. I wonder if the ideas (which are in a paper) could be ported to GHC.
What is a "capability"? Sorry, programming noob...
http://www.reddit.com/r/technology/comments/n2f0d/i_dont_understand_what_anyone_is_saying_anymore/
&gt; it's an observation of the very real visual distinction Sorry, I'm unclear. I'm referring to _you_ considering extra letters to be _noise._ Someone trying to make sense of that type probably wouldn't call it that! &gt; Syntax highlighting doesn't exist everywhere, and I don't recall seeing one that makes this particular distinction. How about your very own Haddock documentation for the library? :) &gt; You can't expect to learn a complex library without becoming familiar with the abstractions it uses. You can be forgiving, and helpfully remind programmers any chance you get. &gt; There certainly may be ways we can improve our tools to help the problem. I agree this problem _could_ be resolved with tool support, almost fully. &gt; The state of this particular example will almost certainly appear worse to a random person who comes to the problem via this article than it is to someone who is actually trying to learn how to use the library. The think to remember is that programmers coming back to modify even their own code later on pretty much IS a "random person who comes to the problem."
&gt; As someone who has not used Snap, yes base view a is more meaningful to me than something something_else some_other_thing! It's more meaningful, but it's not sufficient for the new user to go on without reading the documentation. Once the user reads the documentation, *b v a* will be just as clear as *base view a*, and it will have all the other benefits of brevity that I mention. Of course you have a golden opportunity to say something, but my point is that saying something isn't the whole story. Less can be more.
&gt; Sorry, I'm unclear. I'm referring to you considering extra letters to be noise. Someone trying to make sense of that type probably wouldn't call it that! This extra information doesn't obviate the need for the new user to consult the documentation. And once that is done and the full meaning grasped, the extra characters *are* noise, by virtue of the ubiquity of this concept. &gt; How about your very own Haddock documentation for the library? :) I'm talking about editors. &gt; You can be forgiving, and helpfully remind programmers any chance you get. This particular example is *everywhere* in this particular library. It doesn't need reminding. Hence my focus on the category of "prevalent" names. &gt; I agree this problem could be resolved with tool support, almost fully. Full tool support would still lose the pattern matching benefits that I mention. &gt; The think to remember is that programmers coming back to modify even their own code later on pretty much IS a "random person who comes to the problem." The operative clause there is "pretty much". And therein lies my point.
Something you can run GHC's green threads on. Basically, an OS thread :)
Thanks!
&gt; A general rule of thumb is that prevalent abstractions should be given shorter names and rare abstractions should be given longer names. I concur. There are a few places where longer names are wise, but in the case of prevalent abstractions, it's easier to spend a small time introducing a single-variable convention and make use of it consistently. Most Haskellers with a decent amount of experience will agree that `m` for Monad makes perfect sense; the same can just as easily be true of web frameworks. Generally people looking at a web framework for the first time should be turning to the prose, not the type signatures. People who become actual *users* of the framework will be grateful for the single-variable convention.
&gt; It gives me something to search the documentation for Why would you search the documentation for the name of a local type variable? One would rather lookup the types involved, where the role of the type variables is â€“ hopefully â€“ explained, no matter their names.
I agree. This seems analogous to using i,j, and k for iterator variables in for loops. Even when you can find a decent name to use, the cost outweighs the benefits. I like being able to easily distinguish between iterator and other variables. Understanding imperative code is also aided by "shape" which is obscured by long iterator names. 
Seems to me that very_long_descriptive_names cater to newcomers, while short, less descriptive, but more compact and therefore easier to get more on a line names cater to more experienced users. To a certain extent this sounds awfully familiar to linux newcomers who complain that cd is not called "change_to_directory".
Translating "b v a" has a higher amount of cognitive overhead then translating "base view a"
Once you're familiar with the patterns you don't even need to translate "b v a". You just see the patterns. This is less cognitive overhead.
Agreed. In this case the catering doesn't actually tell them everything they need to know, it just gives them a misplaced warm fuzzy feeling of understanding. There are definitely times that this warm fuzzy feeling is very helpful. That's why I frequently use names like *nestSnaplet* (instead of *nest*) or *runChildrenWithText* (as opposed to the ruthless *rcwt* that my post might make it seem I would argue for). But in this specific case, since the pattern is used in something like 90% of the API functions, I think the (false?) benefit of catering to newcomers is outweighed by the benefits of brevity.
Blonde, brunette, redhead...
To some extent yes, but "base" and "view" do have more pixels...
Ah! This is pretty awesome.
I really wish this was called something more obvious, like OSThread. "Capability" always makes me think this has something to do with capabilities, which it doesn't.
This is an option... and not intended to be used in normal builds. It's useful for cases where you're playing around with something in GHCi, and want to load and use some parts of a file without commenting out the bits elsewhere that have type errors.
They use x86 hardware virtualization extensions to provide a read barrier for their concurrent collector, IIRC. The idea has existed for quite a while I think, but I couldn't provide you with the early literature supporting that at the moment. Before they started offering x86 solutions (about ~2 yrs ago,) they used their own custom processors (the Vega series) which provided the needed hardware support directly. The algorithm itself is actually pretty simple, IIRC. Overall the design is probably applicable (and usable!) by any programming language, but the killer is that in order to work, their prototype requires 1) Kernel extensions that expose parts of the underlying memory management interface to kernel modules and 2) A kernel module itself that provides a device suitable for allocating memory. So, it's possible, but currently it would require a lot of out-of-band material/setup in order to make it work. Check out the Managed Runtime Initiative, started by azul (it hasn't been updated in a year, making me think it's dead...) which contains a copy of OpenJDK which uses their concurrent collector: http://www.managedruntime.org/ It also contains the necessary linux kernel patches/kernel module to support it.
I think what you're looking for is OS X Lion ;)
&gt; &gt; Handler b v a jumps right off the page at me, while Handler base view a blends in. That's an interesting example. In the Yesod codebase, we have a similar situation, where GGHandler takes a few type parameters (for the subsite, master site, and inner monad). I originally wrote it as GGHandler s m mo But have since been rewriting the code to be: GGHandler sub master monad I think it's much clearer, even for me. I can only assume that it's *much* clearer for the people who didn't write the code.
"""Handler b v a jumps right off the page at me, while Handler base view a blends in.""" Handler base view a is superior for a newcomer. """Secondly, if you saw the type Handler base view a, would that really aid a newcomer's understanding more than the single letter variant? I contend that in this case it would not. base could refer to a numerical radix, and view could have any number of meanings.""" This makes no sense to me. I do not know Snap, but I do know that it has something to do with web frameworks. In what sense could possibly a base radix have anything to do with a web framework? Likewise, view "obviously" has something to do with model-view-controller. That is my initial reaction. You can't assume that the reader doesn't use *any* context in interpreting names. Of course he does. This is a strawman. There is a 1,2,many rule in engineering. Either you have one, two, or if you have more, you have many. Wrt naming things. If there are only one or two things to name, then you can use short names. Basically the whole Prelude is an example of this. If there are more, you need detailed descriptions. Why would there ever be need for more than two general variables? Handler b v a is an example of &gt;2. Nobody can guess anything without detailed documentation. Also, finally, whatever the author finds readable is irrelevant. Readability is defined by whether other developers can read and understand - the authors preference is actually irrelevant in this case. Not that the post indicates otherwise, but I just want to mention this.
I cannot agree more. I think it is so bad that I think there is little hurting Haskell adoption more than the coding convention. Why should readers of Haskell pay such a high price? All engineering languages have gone from being impossible to engineer large systems in, and using short variable names - to large scale engineering where documentation and readability are key. (examples: C, C++, Fortran, Java) Short variable names makes it impossible to make the leap forward. 
This comparison is totally flawed. Unix commands are and should be optimized *for writing*. Source code is and should be optimized *for reading*. You are comparing apples and oranges. 
This seems very dangerous. If you do this and get mysterious deadlocks, don't file a bug, you have probably violated a few tricky rules. To do this safely: * You must ensure that no atexit() hooks are used by any foreign code. * You must ensure that no pthread_atfork() hooks are used by any foreign code. * You cannot use exit() * You cannot hold any locks in the child, or you must ensure that no other capabilities are holding locks when forking. So the choice is between global synchronization or being able to do "nothing" in the child. * Only the thread calling forkProcess can hold any locks, because these locks are the only ones that can be released in the child. * The fact that the child cannot acquire any locks not held by the parent thread quickly excludes access to most of libc, because accessing the environment can deadlock if any other thread was holding that lock during fork(). .. etc. I'd love to see how this is designed to be safe in GHC. 
Wouldn't you be able to delay any conversion of a substructure, including creating the Haskell data, and decoding numbers/strings/subobjects? I agree that in most cases you probably don't save much, but if is an area where there can potentially be unneeded information passed along. This often happens as protocols age and you need backwards compatibility.
Nah. v, v1, v2, ... is merely exploiting an isomorphism between string concatenation and numeric addition, as, duh, both are monoids and 0 resp. "" is the identity. It's really that simple. If you were a real programmer and *cared* about starting indexing with 0, it'd be obvious to you ;)
yes you are absolutely right; i need to rethink this more carefully. thnx for the information
`mo` is really bad, because `m` is, by convention, reserved for Monad. What about GGHandler sub master m ? Especially because there's gotta be a `Monad m =&gt;` there, somewhere. `Monad monad =&gt;` looks silly. Oh, while I'm at it: Let's all use `ap` for applicative functors, calling them `f` is quite imprecise.
I agree that *s m mo* is bad. It's bad because pretty much all the names are massively ambiguous with other more common conventions in Haskell code. In our case, we don't have those ambiguities. That's why I went to the trouble of refactoring *e* to *v*. If, five years down the road, it turns out that 'b' or 'v' becomes overloaded by a much more common convention in the broad Haskell community (like 'm' for monad is already), then I will probably change the name.
&gt; This makes no sense to me. I do not know Snap, but I do know that it has something to do with web frameworks. In what sense could possibly a base radix have anything to do with a web framework? I already commented about this above. &gt; Likewise, view "obviously" has something to do with model-view-controller. That is my initial reaction. And here you make my point crystal clear. In this case it is not about model-view-controller. It is about something completely different. See other comments here where I've alluded this false sense of security that a familiar name might give the newcomer. 
Firstly, there *is* a historical reason for all this: having the monad as a type parameter was added later, *after* "m" was taken by master. This is what convinced me that using s and m wasn't a good idea. I think that having a single "m" versus "sub" and "master" is confusing, but I'm not strongly opinionated about it. As far as "ap" versus "f", I think that's overkill.
If someone looks at my code and sees "GGHandler sub master monad", it's pretty clear what it means. Even if it's someone who's never looked at Yesod before, and has no idea about the breakdown between master and subsite, it gives them enough information to Google for more. If I see "Handler b v a", I have no idea what it means. You might be correct, and that for someone working with Snap it will always be clear and unambiguous, but I wouldn't risk it.
Does Scion provide anything which could help incremental parsing?
&gt; is merely exploiting an isomorphism between string concatenation and numeric addition Hmm, but 1+1 kinda isn't 11.
Having a look at the haddock, you have type GHandler sub master = GGHandler sub master (Iteratee ByteString IO) type GGHandler sub master = ReaderT (HandlerData sub master) I'd say just call the whole thing `HandlerT sub master m a` (non-pointless, I hate having to think about that when looking at types) and `Handler sub master a` What about using "root" instead of "master"? "master" makes me think of control flow, not different sites. I guess I should probably have a closer look at yesod and its type magic, even if all my proposals get rejected I could end up getting sufficient inspiration to fix those issues my half-forgotten and Iteratee library has, before I let it succumb to bitrot in a dark corner of my home directory.
Of course it is. length "111" == 3 == 0 + 1 + 1 + 1 length "11" == 2 == 0 + 1 + 1 length "1" == 1 == 0 + 1 length "" == 0 == 0 Simple base 1 arithmetic. And as you very well know, `mempty` is a polymorphic value. 
They use one tag bit per pointer to indicate whether the pointer has been GC'd or not in this cycle - there's a clash between that and GHC's existing use of tag bits.
At the time of the forkProcess, you have to know what all your threads are doing (both Haskell threads and OS threads). There's no getting away from that - fork() is inherently dangerous in this respect. (it's why I've been trying to convince people they don't really want this, but they refused to believe me :-) So yes, you can't have any other threads in libc or any other foreign code that you don't control when you fork. The RTS ensures that this applies to the threads that it manages, but for your own threads you're on your own. 
It's not exactly an OS thread, so calling it that would be confusing. Its more of an OS thread pool, in which only one can be running at a time. 
I disagree on the master/root thing. We're using the term "master" all over the place for the concept, so it's unlikely to change, but even if we were starting from scratch I like the term for what it's doing. As for renaming GGHandler: that might happen. If you look just a few versions back, GGHandler was actually a fairly complicated newtype wrapper. One of the moves I've been making recently is to simplify all of our monad stacks. One of the goals is to make the type magic less magical, but mainly for simplicity of implementation and performance. __Edit__: Also, the reason we don't rename `GHandler` to `Handler` is that each site gets a type synonym `type Handler = GHandler MySite MySite`, which simplifies the coding. But I do like the idea of HandlerT. I wonder if GHandlerT makes sense...
Ok, agreed. I used the wrong analogy. My question should have been phrased: are you optimizing for reading by a novice, or by someone knowledgeable of the code. Is there a difference? In my view there is.
Thanks! I agree. These were just easily coded place-holders. Using the greek alphabet is a cool idea! This would be a user-customizable thing. One issue is distinguishing related, yet independent types. I'm considering using the same symbol for polymorphic variables that have the same constraints, and differing their color to indicate that they are not actually constrained to be equal. If some sort of alphabet-like symbols are used, then we'd want to map the usual "a b c ..." polymorphic variables to these. But how do we handle the very common case of multiple instances of the same name? In its type errors, GHC adds a number to the name, which can tend to really obfuscate. The equivalent here would be subscripts, or, better yet, modifiers (barred, dotted, caret, apostrophe'd, etc). Something that the old screenshot shows is that all of the like-types are connected. One nice thing about this is it would give convenient horizontal lines on which to annotate the constraints of the polymorphic variable. It would also allow for directly seeing what portions of types are the same polymorphic variable, even without distinct symbols. However, I don't think I'll do this, even though that's where the "sinu" in sinuhask comes from. It just leads to too much clutter, uses up space that could be used for more valuable information. I think it could be very handy when reduced to a particular context, though. In other words, take the identifier under the cursor, and show where all its variables come from.
&gt; non-pointless, I hate having to think about that when looking at types Eta-reduction of type aliases actually changes their meaning, because type aliases must be fully saturated at every use site. So, for example, type F = [] type F' a = [a] type Valid s a = StateT s F a type Invalid s a = StateT s F' a This restriction is admittedly a bit annoying, but fairly important for tractable type inference.
Yes, but people use `v, v1, v2`, not `v, v1, v11`. Also I find it a bit odd that you are considering things only up to isomorphism in a thread about naming -- that is, in a thread where we explicitly care about which one of several isomorphic choices we actually choose.
Yeah, this is not surprising. The move to 64 bits is not as simple as some people assume, and not just for Haskell and GHC. I don't think the answer is to keep using 32 bit applications forever, though. You'll be building subtle unsustainable assumptions into the code you optimize. The answer is to keep reminding the GHC team - who rely on our feedback for direction - that it is a priority to work on gradually re-optimizing GHC for modern platforms. If that little bit of speed makes a difference to you, and if you are absolutely certain that the optimized code you are writing will have a very short lifetime (famous last words), then right, you want the 32-bit GHC.
This is partly why I suggested "unaligning ghc" on the ghc mailing list. The good news is that nehalem and later architectures makes it really cheap to access unaligned memory, so 40 bit pointers should be within reach. This drastically reduces the cache pressure, but makes the 64-bit instruction set and extra registers available.
It's not a warm fuzzy feeling, it's *information*. You want short code because you read it all the time and you can already understand it quickly, and people who have never seen it before want more *information* to help them understand it at all. It may not be complete information, but it's more than just 'b', and that's useful to help jump-start the process to remember what all these things mean. I agree with your main points, especially that pervasive patterns can have shorter names, sounds like a good idea! I think the main disagreement is that you're describing a pervasive paradigm in a very-incredibly-small proportion of the total code out there. It's a little bit like being the most popular person in a room with only 1 person in it. Bigger audience? Shorter variables! The Prelude has lots of short type-variable names, because within Haskell it's very well known. People tend to want transportable skills, so remembering your b's and v's just doesn't help them anywhere except for your small corner of the world. However, people readily swallow up conventions like using i,j,k for indices, because it carries throughout the language (even across languages!).
v2 &lt;-&gt; v'' All that I was trying to point out, HHOS, is that leaving out the number "0" when doing numerical indices has a valid rooting in intuition and maths... even if the actual reason is pretty-printer simplicity. I, for one, don't mind v, v1, v2, ... but would be annoyed and probably momentarily confused by both v, v0, v1, ..., and v, v2, v3, ... . Out of those three possibilities, v, v1, v2, ... is clearly the right choice because of that very isomorphism.
Maybe we need to go back to the bad old days of "near" and "far" pointers?
Btw, how does the JVM w.r.t. to the 32bit-vs-64bit overhead?
&gt; confused by both v, v0, v1, ..., and v, v2, v3, ... I'm pretty sure the andresp was proposing that the sequence go either v0, v1, v2, ... or v1, v2, v3, ... . I think I kind of agree with him that either of those make more sense than v, v1, v2, ... .
Vade retro, satana.
If I see 'v1' somewhere, I expect either v or v0 to be present, too, because I'm predisposed to indexing from 0. That makes v0, v1 .. and v, v1, ... the only logical choices, hence the talk about the isomorphism.
&gt; I don't think the answer is to keep using 32 bit applications forever, though. It would be interesting to see how much performance suffers if only pointers are moved to 64-bit, not (most) ints. Storing all ints in memory using the largest word width supported by the platform is inherently wasteful, and makes little sense in a language where you don't wildly and blindly cast one to the other all the time. BTW, I've been using 64-bit systems only for a long time now (not just servers, but my laptop, too). I didn't realize 32-bit ludditism is still common until reading this post.. My usual reminder of the 32-bit past is Python's zipfile throwing an exception with too large files (yes, it does that on 64-bit systems, too). Amusing that a system would refuse to store something that fits in memory on disk because it is too big..
it's in german :(
In Java, you can use compressed pointers to adress 32GB of memory by shifting the 32-bit pointer left 3 bits. http://wikis.sun.com/display/HotSpotInternals/CompressedOops This is actually the default mode for Java SE 6u23 and later, and Java SE 7 unless you explicitly ask for more than 32GB of memory. I guess they found that it has merits. I am guessing that there are larger savings to be had in Haskell. 
In my view there isn't. My experience is mostly C, C++ and Java. In all those languages there is a difference between readable and unreadable code. That's it. There is also code that requires expert knowledge to understand. That code is almost always less readable than "straight forward code". This is mostly agreed on by my co-workers. Sometimes the person who has to look at the code a couple of years down the line is *you* :-).
It is very easy to load an object file with the GHC API if you know the signatures of the functions in there. I guess you have to fully trust the object file for this though. I will paste an example below. Also, I used hint for a prototype application. I liked it very much; however, there are some tricky performance issues; [here is a blog post](http://pnyf.inf.elte.hu/fp/Economic_en.xml?yes) I found useful. 
Object loading example below. Haskell code: {-# LANGUAGE ForeignFunctionInterface, StandaloneDeriving #-} import BasicTypes import ObjLink import Foreign import Foreign.C deriving instance Show SuccessFlag type Fun = Int -&gt; Int -&gt; IO Int foreign import ccall "dynamic" mkFun :: FunPtr Fun -&gt; Fun main = do initObjLinker load "object.o" load objfile = do loadObj objfile succ &lt;- resolveObjs print succ Just ptr1 &lt;- lookupSymbol "str" s &lt;- peekCString ptr1 print s Just ptr2 &lt;- lookupSymbol "fun" let funptr2 = castPtrToFunPtr ptr2 :: FunPtr Fun let fun = mkFun funptr2 x &lt;- fun 13 7 print x unloadObj objfile C (object) code: #include &lt;math.h&gt; extern char str[]; extern int fun(int a, int b); char str[] = "kortefa"; // "almafa"; int fun(int a, int b) { //return (a*a-b); return (a+b-1); } 
http://en.wikipedia.org/wiki/X32_ABI Though that's not quite the same thing as what you mentioned - both pointers and ints stay 32 bits, but you get to take advantage of the more and wider registers and other advantages(?) of 64-bit mode.
I changed it to LGPL v2.1 - again thnx a lot for your suggestion.
Thanks for the reply. But here your .o is compiled C, not compiled Haskell. Have you tried it? If I got it well, hint would enable you to do the same (since it's written above GHC API), but you prefer not to use it due to performance issues? Finally, concerning: foreign import ccall "dynamic" mkFun :: FunPtr Fun -&gt; Fun As I understand it, you ask GHC to make on-the-go a function that turns a function pointer to a plain haskell function. Is that it? Or is "dynamic" a plain C function (that belongs to GHC API) you have to instanciate the type of on the Haskell side?
My impression was that O1 skips any optimization that could take a lot of (compile) time.
Yes, here we load a compiled C object file. I believe GHCi uses the same technique to load compiled Haskell object files, but I'm not familiar with the GHC ABI (for example you probably need the interface files, too). Maybe there are higher-level functions to achieve this somewhere in the GHC API. Hint is something different, it is an interpreter - it loads Haskell source files. (At least that is my understanding). I used it in a completely different context. Yes, I think you understand dynamic import correctly: it is GHC "magic" which turns a pointer to a C function with the given signature into a Haskell function, runtime.
&gt; you probably need the interface files, too Yes, I meant that, sorry. &gt; Yes, I think you understand dynamic import correctly: it is GHC "magic" which turns a pointer to a C function with the given signature into a Haskell function, runtime. Do you have a resource that explains it?
Is there a difference between foreign import ccall "dynamic" and foreign import ccall "wrapper" (which is used in http://www.haskell.org/haskellwiki/GHC/Using_the_FFI) ??
&gt; Why would you search the documentation for the name of a local type variable? I wouldn't. I would be scanning over the documentation for where it discusses what a "base" or a "view" is. An eyeball scan would do fine. On the other hand, trying scanning over the documentation for a "b". &gt; where the role of the type variables is â€“ hopefully â€“ explained, no matter their names. I will concede the point that if this actually happened with some sort of regularity I would probably not be complaining. But documentation seems to be optimized for Haskell experts; this seems a suboptimal optimization point. (Yes, of course your library shouldn't contain a newbie tutorial, either, but there's some balance points in the middle. We have the usual auto-generated documentation problem where it's so easy to put out the _what_ automatically that the _why_ is lost in the noise.)
I would just use `Test.QuickCheck.Gen.unGen`, which gives you more control than `sample'`. It takes a generator, a random number seed and a size and returns a single generated value. If you want several values you can use something like `vectorOf` in your generator.
Looks like this *is* going into 7.4. Consider it experimental.
It would be an interesting project to try doing something like this in GHC. However, note that we already make use of the low 2 bits of a 32-bit pointer for tagging. Oh, and another big advantage that Java has in this area is that they can choose the memory model at runtime, whereas we have to choose at compile time (and recompile all your libraries for each memory model).
I'm not that home at haskell yet but isn't unGen hidden (not exported)? Edit: Ah, you just needed to import it explicitly.
I didn't know any x86_64 processor had support for 40-bit pointers. Reference?
Makes me think that the sensible choices for GHC are: * X32_ABI like mode, used for most uses * ILP64 for applications with large memory needs I don't think it makes sense to double your memory requirements just so that you can have file byte counters that exceed 4G - especially since GHC has Int64 and Word64 so readily available. Operations on files and streams referring to lengths or offsets should be using those already.
I think the GHC docs should explain it, but it look rather scarce... The FFI addendum does not seem to contain this functionality, so I assume they must be GHC-specific.
wrapper seems to do the opposite: turn a haskell function into a function pointer callable from C.
[plugins](http://hackage.haskell.org/package/plugins-1.5.1.4)? [plugins-auto](http://hackage.haskell.org/package/plugins-auto)? They don't work with 7.2 yet. [edit] I've just tried the plugins-auto sample code in GHC 7.0.3. Damn, that is really impressive. Be sure to compile the Main program with `{-# LANGUAGE TemplateHaskell #-}`.
&gt; if you want an English version convince your prof to invite me for a guest lecture as well. lol @ this :) You could try http://translate.google.com/?tr=f&amp;hl=en on the pdf
It's exported by `Test.QuickCheck.Gen`, but not reexported by `Test.QuickCheck` (whereas the type `Gen` is).
As much as I like Haskell, there are certain problem domains for which Haskell is not a good fit. Dynamically compiling and loading new code at runtime is one of them. Sure there are tools to do it, but they are subject to change or break in the future. Compare this with a language that has `compile` function which is part of the standard. Some languages are just made for this domain. I'm talking about Lisp, of course.
http://www.haskell.org/pipermail/cvs-ghc/2011-August/065174.html My point is that the cache pressure savings, and even the GB*time cost of memory exchanged into CPU is huge. Looking at EC2 costs, you get almost two compute unit hours for the cost of 1GB hour. http://huanliu.wordpress.com/2011/01/24/the-true-cost-of-an-ecu/ What this means is that if your application is using 4GB of memory in 64-bit mode, but 2GB of memory in 32bit mode, then that saving is worth almost 4 EC2 compute units, or the computing power of almost 4 standard EC2 compute instances. Most workload where you hold state would be a clear win I think. In those cases, most of the cost of computing is in the memory. You can see that in the linked analysis. Thus, by "unaligning" or packing data, huge computational savings can be achieved. Actually, disregarding the workload, when you buy computing resources like on EC2, you pay mostly for memory. Technically the X32 ABI linked to above is probably quicker than the 40-bit version I am proposing, but the advantage of a 40-bit one is that it will give you as much memory as you need, you can fit tag bits in there, and since it is already unaligned, you can pack all data structures more tightly - which means that you might end up using as little memory as in the X32 ABI case anyway. Technically, a 40-bit pointer is 5 bytes in memory. You load an unaligned 64-bit pointer and shift it down 24 bits. Since a common technique to deal with tag bits is to shift down and then use the built-in multiplier functionality of the load/store units on x86, this shift can actually be free. Also, it is possible to try to avoid crossing a cache line boundary in the nursery, and pack more tightly in the older generations of the garbage collector.
Also check out [smallcheck](http://hackage.haskell.org/package/smallcheck) which allows you to generate all values of a data type up to a given depth: type Series a = Int -&gt; [a] class Serial a where series :: Series a 
This paper http://www.cs.kent.ac.uk/pubs/2003/1896/content.pdf "Testing and tracing lazy functional programs using QuickCheck and Hat" discusses how to control the random distribution used for data generation. If somebody would please implement Boltzmann samplers for this purpose Haskell could take over the world (again :-). 
There is a downside, however. On my MacBook this takes 11 seconds to link and 0.6 seconds to run. import System.Eval.Haskell main = do g &lt;- eval "f :: Int -&gt; Int ; f x = x * x" [] :: IO (Maybe (Int -&gt; Int)) case g of Nothing -&gt; error "some problem" Just h -&gt; print $ h 7 
I don't think it's a domain. It's a single feature many solutions in many domains need.
I can't wait until the same GHC binary can compile both 32-bit and 64-bit binaries (although I guess it'd need to ship with two versions of each boot lib. A good reason to have fewer of those).
Runtime compilation is coterminous with code generation. There is indeed a domain of problems which are naturally solved using code generation. One could say that Lisp was made for it, whereas Haskell can only simulate it or build implementation-dependent tools for it. For example notice that System.Eval.Haskell.eval takes a string. So if you were generating ASTs you'd have to convert them to strings before evaluating them. Such a step is not necessary in Lisp, where ASTs and code are the same. As another example, suppose you have a function and a list of arguments to that function, both of which are defined at runtime. You only know the output type, so what type are you going to give to eval? You'd probably end up generating code which calls the function with the given arguments and evaluating that. The most Haskell-like approach, I think, is to build your own interpreter for whatever language you're evaluating, which may be Haskell itself (ghci). But some things need to be compiled, and that leaves you with implementation-specific tools which are not part of the standard. (Though nowadays GHC is the de facto standard anyway.) Also think of how difficult it would be to build Haskell ASTs. One might try generating strings instead, but that has problems of its own.
&gt;For example notice that System.Eval.Haskell.eval takes a string. So if you were generating ASTs you'd have to convert them to strings before evaluating them. Such a step is not necessary in Lisp, where ASTs and code are the same. GHCi, version 7.0.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package ffi-1.0 ... linking ... done. Prelude&gt; :set -XTemplateHaskell Prelude&gt; import Language.Haskell.TH Prelude Language.Haskell.TH&gt; :t [| \x -&gt; 2+x |] [| \x -&gt; 2+x |] :: Q Exp All you need is a quotation mechanism. 
Short writeup about this bug: http://labs.scrive.com/2011/12/integers-can-kill-your-haskell-web-app/ 
A related question: Which mechanisms exist for **unloading** or **garbage collection** of (dynamically) loaded code. 
plugins seems indeed more flexible than pdynload, since it enables to load an object file, and not only a module referenced by GHC. (And plugins has also a pdynload function for this ^^ ) I think pdynload is to be replaced by plugins (cf. the upload dates). But in fact there is the problem of Template Haskell growing the size of the executable...
Why is all the code so tiny?
For a really minimalist implementation, check this: https://twitter.com/#!/welltyped/status/23911452343 https://twitter.com/#!/welltyped/status/23911426076
&gt; It would be interesting to see how much performance suffers if only pointers are moved to 64-bit, not (most) ints My guess is that you'd get almost the same outcome. My guess is that pointers take up much of the space in most Haskell programs (except for those heavily relying on unboxed arrays). Consider something simple like `[Int]`. It consists of 5 words of which 4 are pointers, and due to word alignment/padding you'd not save anything if you used `[Int32]`. Look at an `Int` heap object, you have the object header which is a pointer, then you have an `Int#`. If you had two `Int#`s then you could save a word by using two 32bit ints, but otherwise you just loose the extra half-word to padding.
Second day of four day weekend, so I'm just workinâ€™ my way through [the issue list](https://github.com/chrisdone/haskell-emacs/issues?sort=created&amp;direction=desc&amp;state=open). Feel free to add more issues. More general mile stones [here.](https://github.com/chrisdone/haskell-emacs/issues/milestones) 
I had some links in /usr/local/bin made by the GHC 7.2.1 installer which were not cleaned up by the uninstaller, and which broke the HP installation (cabal could not find ghc). After cleaning those up, I now have a Haskell setup that properly works again!
Hmm this is unfortunate as I have to switch operatings all the time. I was hoping there was a way to code some sort of "smart session save" with dbus or something.
What is the status of yesod? Is it still being actively developed? I'm planning to redo some horrible PHP mess I inherited in haskell, and am not sure if I should be going with yesod or snap. Yesod seems nicer from a functionality standpoint, but doesn't seem to have much of a community. The irc channel is dead, there's only 2 developers, and it seems pretty hard to get answers to questions that aren't covered in the relatively limited docs. My biggest concern is persistent, which is basically completely backwards. I know I can turn off migrations, and force all the table names to lower case (please hurry up and fix that so I don't have to), but further support for working with existing databases seems to be non-existent. Is there a way to tell persistent that I have a natural primary key on a table, or does it really force you to have a surrogate serial primary key on every table? Is there a way to generate the model structure from my database instead of the reverse as yesod assumes by default?
Thank you for the report. Could you send me a) the links in /usr/local/bin that were not cleaned up, b) the message when cabal could not find ghc, c) which links you cleaned up (removed? retargeted?). (You can find me as mzero on IRC, or email to mark@ the domain of web site where you got my download.)
I used Gen awhile back to generate some serials in the form: LWQ74236 IZK97057 MTT84566 FBL91704 OUX61740 Here's a gist of the code: https://gist.github.com/1008001 
I'm one of the three developers, and I'd say it's truly not ready yet. Given that loops and assignments are not ready, there's little chance to generate 'real' code without them. So for now we have to resort to natrec and list folds for loops. It is not because we like recursion schemes but because there's no other way to express iteration yet. The syntax is lispy just because it's the simplest syntax to implement. Syntax doesn't matter, don't you know? If we manage to output human-grade code at the back end, any syntax can be added later. So we focus on a proof-of-concept prototype implementation of http://code.google.com/p/inv/wiki/FoldExample and on writing a booklet we can show someone at a conference. The C++ nested structs will go away as we implement defunctionalization. Some improvements can already be seen with -O switch. At some point we should be able to generate plain ISO C, but now we rely on C++ facilities for what we haven't implemented yet. Most of the comments here are not about our idea, but about our implementation, so they are pretty irrelevant. A few more interesting questions are: * Are there any objections to our method of getting human-readable code (namely, defunctionalization of higher-order imperative code with preservation of identifiers)? * Is there any need in languages with C memory model? Aren't languages with "almost" C memory model (e.g. with reference counting or conservative GC) good enough to enable incremental re-engineering of C legacy? * Is there any research in source-to-source translation or preservation of documentary structure we are unaware of? * Is there a need in type inference and high-order control in C? Can C be enhanced at all without losing its attractiveness as the top performance and the top portability tool? BTW we are moving to GitHub: https://github.com/kayuri/HNC/wiki
Hmm, interesting take on yesod. I have not used it at all, but my impression is that it has relatively rich docs (a full "book" plus haddock for all modules), an active blog, frequent releases, and whatever the community/developer size they seem to be finding some innovative approaches to problems. I would be hesitant to use persistent also though.
I backed up my install, installed the 32-bit package, and removed my old versions with the uninstaller. Then I re-built all of my Projects and dependencies. No problems. 
I've only played around with it a bit to test it out, and run into tons of random little issues that are trivial to resolve if you know yesod, but aren't documented anywhere and your best bet seems to be asking in IRC and then waiting a day or two and hoping someone answers. Haddock isn't much for documentation, it is just an API listing. If you don't know what you're looking for it isn't going to be helpful. The book is good for what it covers, but there's a lot of things that aren't included (yet). I mostly like persistent, having a type safe way to access the DB is a pretty important thing. It just doesn't appear to work well with existing databases, only with a new database created by persistent. Since I'm looking at yesod for replacing an existing PHP app, working with the existing database is a requirement.
unGen it is! :)
Omg!, Looks pretty cool. You are making me want to go back and code in Haskell :)
&gt;Storing all ints in memory using the largest word width supported by the platform is inherently wasteful, and makes little sense in a language where you don't wildly and blindly cast one to the other all the time. Exactly. Why on earth are they doing ILP64 instead of LP64?
Am I the only one getting the following error on byte-compile? No setf-method known for hs-process-queue
d(*âŒ’â–½âŒ’*)b
Okay, I tried byte-compiling myself and it was OK. How are you building it? Try pulling again and I added a Makefile in there, try `make`. There are a bunch of warnings but it compiles fine. Also, what's your Emacs version?
Yeah, the IRC channel is sometimes slow, but there's an active [mailing list](https://groups.google.com/forum/#!forum/yesodweb), very active development (with plenty cross-fertilization with Snap), and a non-zero user base. That last point is vague because I have no idea how many of us there actually are, but judging by the number of new names I see on the mailing list, it is growing. If you're so sure upper case table names are a problem, [file a bug report](https://github.com/yesodweb/yesod/issues). IMHO the devs are competent enough that "fix it so I don't have to" isn't a very productive approach. [edit] I feel your pain re persistent, by the way. It is, of course, quite possible to do without, and a lot of thought and new designs are still going into it. Also, have you seen [persistent-hssqlppp](http://hackage.haskell.org/packages/archive/persistent-hssqlppp/0.1/doc/html/Database-Persist-HsSqlPpp.html)?
The snippet compiles with ghc 7.0.3 and 'a' evaluates to 42
Hmmm, I used 7.2.2, will play around with 7.0.3 as soon as I have a chance. What does 7.4 think about this? (edit: the original snippet was flawed, it is ok now and you should see the same error with 7.0.3)
Well, I'm not the best person to answer your question. But I'll give it a shot. Let's write a schematic case expression. I'll assume that the expression has just one clause: case (expr :: t) of (pattern :: t) -&gt; expr' What I mean by this is that a case expression takes an expression of type *t* and a pattern that matches an expression of type *t*. The question then becomes what are the typing rules for pattern matches; i.e., how do you determine whether a pattern can match an expression of a given type. Well, first of all, the expression's constructor must be the same as the pattern's constructor. That holds here; the constructor is Hide in both the expression and the pattern. Second, the types of the variables in the patterns must match with the types of the corresponding subexpressions in the expression to be matched. And as I understand it, this is what's failing here. The variable b can't have the type that's required to match the 42 in Hide 42. Because by using the existential types, you've "hidden" the type, and by some proof-theoretical Curry-Howard stuff I don't really understand, when you eliminate the "exists" from a type you instantiate a fresh type variable that cannot be in use anywhere else in your program. Anyway, the fault with your equational reasoning arguments must be that you're substituting expressions with different types. This sort of explanation would account for your Hide _ case; what must be happening there is that the wildcard is simply not subject to type checking, and the pattern match succeeds simply because the constructors match. Though I'm almost scared to ask what happens if you do this: a = case Hide 42 of Hide b -&gt; 42 **EDIT:** I tried that last example. It works, and evaluates to 42. I should have guessed it from the error message, now that I look at it again, from the mention of "the inferred type of a :: t at equality-test.hs:4:1". My next guess is that the type variable in the existential type just cannot be unified with the type of any top-level expression, periodâ€”even if that type is a type variable like t in the original error message.
I get more or less the same error message as yours on ghc 7.0.3 with the following code: {-# LANGUAGE GADTs, RankNTypes #-} data Hole where Hide :: forall a. a -&gt; Hole a = case Hide 42 of Hide b -&gt; b Has something changed about existential types or GADTs between 7.0.3 and 7.2? Because as far as I can tell, your snippet isn't even using existentials.
Yeah, sorry, I had 2 snippets in my backscroll and I pasted the non-existential one :-( Murphy at work. Corrected the example above to show the existential version. This also explains why camaiocco have seen it being compilable with 7.0.3. I am pretty sure it doesn't now :-)
My thinking is that the usual *skolem constant*-like typechecking of existentials breaks equational reasoning in this case, because it does not consider that the ingredients leading to the case expression are all available in the frame of reference (i.e. semantic scope). Perhaps skolemization should happen only when the frame of reference is left.
That is really disturbing, then
Wow... System.Eval.Haskell doesn't work on my computer: main = do i &lt;- eval "1+6 :: Int" [] :: IO (Maybe Int) print i Always print `Nothing`. Plus, just like argentine_x, it is slow, but it's certainly the fault of Template Haskell (which I activated when compiling). (GHC 7.0.4, Ubuntu 11.04)
&gt; Overall the design is probably applicable (and usable!) by any programming language, but the killer is that in order to work, their prototype requires 1) Kernel extensions that expose parts of the underlying memory management interface to kernel modules and 2) A kernel module itself that provides a device suitable for allocating memory. I heard in recent weeks that Azul's kernel patch requirements are going away. The replacement appears to be some sort of userspace server process that allocates a huge memory pool from the kernel. The VMs on the host are clients of this memory manager process, which suballocates memory for their benefit. Dunno if this this a superior solution or just a second best forced on them because of the perennial problem that kernel writers never want to support GC'ed languages...
You're essentially packing `42 :: Int` into an existential and then trying to unpack it with the case expression, yielding the packed value. Generally with existentials you can't unpack them into an expression whose type mentions the packed type -- you can't unpack `42` into `b` because the type of `b` is the hidden, existentially-bound `a` (from `\forall a. a -&gt; Hole`). (See the elimination form for existential types in System F, or go all the way back to Mitchell-Plotkin's "Abstract types have existential type.") In an untyped dynamic semantics of existentials your suspected equation would hold (or something like it, since the second line is syntactically invalid). But the type inference algorithm is getting in the way because it can't give a type to the branch's expression without using the packed type. In more concrete terms, this violates the typing rules for GADTs given in, e.g., Vytiniotis, Peyton Jones, Schrijvers, and Sulzmann's recent JFP article "Modular type inference with local assumptions" because the existential type of the case branch's constructor appears in the result type. Perhaps at some point the GHC implementation switched to this type system/algorithm?
&gt; My take on this issue is that type checkers mandate information loss ... That is what existential types do. Anyhow, the type of `Hide` is: forall a. a -&gt; Hole The free theorem for this type is: forall T U (x :: T) (y :: U). Hide x = Hide y So, allowing what you want would break parametricity, because it would allow us to distinguish `Hide 42` from `Hide 43` and `Hide 'c'` and so on.
I don't think the latter should work at all. Lets remove the extra assumptions added by the magic of numerical literals: data Hole where Hide :: a -&gt; Hole -- x :: ()? -- x = case Hide () of Hide () -&gt; () --- Couldn't match expected type `a' against inferred type `()' `a' is a rigid type variable bound by the constructor `Hide' at SillyExistential.hs:6:20 In the pattern: () In the pattern: Hide () In a case alternative: Hide () -&gt; () You can't use a case statement to match the constructor (Hide x) against any concrete type for x. I don't think this has anything to do with equational reasoning, either; case evaluation is out of the scope of 'replace equals with equals'. Now, back to your example: you can hide a typeclass, or equivalently, some functions with an existential so that you can get data back out: data IntHole where HideInt :: Integral a =&gt; a -&gt; IntHole y :: Integer y = case HideInt 42 of HideInt b -&gt; toInteger b You can also re-hide the object in another existential: z :: Hole z = case HideInt 42 of HideInt b -&gt; Hide b 
&gt; Hiding should not happen when we are on the construction site :-) Actually, this statement *does* break equational reasoning. Assume that this works: x = case Hide () of Hide a -&gt; a Then this should also work: y = Hide () x = case y of Hide a -&gt; a Now move y to another module: import OtherModule(y) x = case y of Hide a -&gt; a These should be valid code transformations, but it becomes very clear that it's no longer possible for x to typecheck.
Got any references for that? Sounds interesting but I wonder how the performance will be given that they're no longer directly interfacing with the kernel's memory management subsystem. &gt; because of the perennial problem that kernel writers never want to support GC'ed languages *sigh*
&gt; Got any references for that? Somebody I know did a trial of Zing recently. Azul sent one of their employees to help them set up a prerelease version with this new memory management stuff, and explained it to them at some detail. The kernel patching was a big deal for these clients, because their IT department would not allow an extensively patched third-party Linux kernel. Dunno if Azul gave them early access to it on these grounds.
I am using yesod without persistent. Its template engine (hamlet) is awesome. Docs are more or less complete (core functionality). And it has automatic reloading in development mode. 
Interesting. I haven't used the IRC channel much, but a few weeks ago, I went in there and asked a few questions. Got quick/helpful answers. I don't know if my experience was atypical or not.
Awesome info, maybe they'll say something about it later in the year or next year hopefully. The tech is really cool and they've already got many publications about it, so maybe this will also come to light too. &gt; The kernel patching was a big deal for these clients, because their IT department would not allow an extensively patched third-party Linux kernel. Makes sense.
Can you understand the error message? Here is a simpler example; if f:a-&gt;b-&gt;c and g:c-&gt;d then g.f is not well formed, because the target type of f (b-&gt;c) will not match the source type of g (c). If you have a function of "multiple arguments" you have to fill in all the slots before you can push it through to another function.
Oh... I see. That make sense. Thanks.
It helped me to look up the definition of the `.` operator: [http://hackage.haskell.org/packages/archive/base/latest/doc/html/src/GHC-Base.html#.](http://hackage.haskell.org/packages/archive/base/latest/doc/html/src/GHC-Base.html#.) So `jj . ii` is equal to `\x -&gt; jj (ii x)`, which is a bit easier to see the error in. 
Perhaps I am being an idiot, but I do not understand the principle behind this "equational reasoning". I have only just woken up and haven't had my coffee yet. I also agree with ryani. I'm also a little troubled by what type case Hide 42 of Hide b -&gt; b is supposed to have. Haskell has not yet adopted dependent types to the extent that the (wholly undesirable) replacement property fails: you can still replace any subexpression in a type by another of the same type and expect the whole to remain well typed. Correspondingly, by weakening and replacement, if your expression is well typed, then so is \ h -&gt; case h of Hide b -&gt; b but what is that type? In a dependently typed language, I would expect there to be a function holeType :: Hole -&gt; * which extracts the witness of the existential, with holeType (Hide (x :: t)) = t Accordingly, the type of my wee function would be the dependent (h :: Hole) -&gt; holeType h which would indeed do the right thing when h is instantiated with Hide (42 :: Int). Haskell's type system does not have such dependent machinery for talking about existential witnesses. This example is suggestive because it gives a concrete value from which the witness would be extracted, and we can easily see what it is. However, the choice of (Hide 42) as the case scrutinee is totally irrelevant to the Haskell-typability of this expression. Haskell has no way to refer to unpacked existential witnesses. Would that it did.
Really nice to see this picking up again - started using it when it was first blogged, but it added just a tad too much friction against the old mode. Will have to give it a try again soon!
Exactly!
Why is the replacement property undesirable? I see how one could argue that it's worthwhile to give the property up to get dependent types, but "undesirable" per se?
to expand on this, you want \x xx y yy -&gt; jj (ii x xx y yy) but (jj . ii) is \x -&gt; jj (ii x) or adding in the rest of the vars you want to pass \x xx y yy -&gt; jj (ii x) xx y yy other stuff (let p = (x,y) in (fst p, snd p)) == (x,y) (floor $ fromIntegral x / fromIntegral y) == x `div` y 
It's not TH that is growing the executables (AFAIK). It's the fact that plugins is linked against the ghc library I think?
If this works the way I think it does, then here's what it does: 1. You start a transaction, and every memory write is marked. The cache coherence mechanism checks for memory access conflicts with other cores. The old values of all memory addresses written during a transaction are written to a log. 2. If there's no memory access conflict, then the transaction succeeds! Hooray! 3. If there's a conflict, then one of the transactions is rolled back, and the old values sitting in the log get written back to memory. The transaction gets retried with exponential backoff. One key limitation here is that the transaction size is very limited. This can provide great acceleration for small transactions, but to handle larger transactions, a slower software fallback is needed. Haskell transactions tend to be small, thanks to the immutable nature of most of its data structures. It's also a godsend for making things like concurrent lock-free hash tables, or shared priority queues. If they have some hinting instructions for declaring things like "I am going to write address X soon", or "I no longer care about changes to the address that I just read", then this could really speed up implementations of concurrent data structures. For example, if you're traversing a mutable linked list in a transaction, then you want to detect conflicts with the nodes you're looking at, but the part you've already traversed may not be important (depending on what you're doing). So, tl;dr: hardware support for small transactions can speed up some STM systems, and is hugely useful for making shared mutable data structures.
No. sorghum:~/programming% cat test.hs {-# LANGUAGE GADTs #-} data Hole where Hide :: a -&gt; Hole a = case Hide 42 of Hide b -&gt; b sorghum:~/programming% ghci-7.0.3 test.hs GHCi, version 7.0.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package ffi-1.0 ... linking ... done. [1 of 1] Compiling Main ( test.hs, interpreted ) test.hs:6:31: Couldn't match type `t' with `a' `t' is a rigid type variable bound by the inferred type of a :: t at test.hs:6:1 `a' is a rigid type variable bound by a pattern with constructor Hide :: forall a. a -&gt; Hole, in a case alternative at test.hs:6:21 In the expression: b In a case alternative: Hide b -&gt; b In the expression: case Hide 42 of { Hide b -&gt; b } Failed, modules loaded: none. Prelude&gt; Leaving GHCi.
Actually, the article suggests that the implementation is based around the locking of individual cache lines. Multiprocessor systems have to manage the update of the cache line residing global memory or cache by new versions of the cache line in local cache (L1). Consider how a simultaneous atomic compare and swap is synchronized between cores. The implementation I suspect is at work is an extension to the instruction set in order to mark the cache lines associated with particular memory access as being "atomic" for the purpose of the transaction, and then if an update to one of those pages is intercepted during the transaction, all the cache lines so marked are refreshed from global memory and the PC is rewound to the top of the transaction. (Normally, only the affected cache line would be refreshed and execution would continue normally).
I suspect Conor finds it undesirable because it doesn't hold in his favorite languages. :)
For those who prefer email: http://www.haskell.org/pipermail/haskell-cafe/2011-December/097442.html
It seems the age of transactional memory is coming.
I hadn't seen the linked Hinze paper before. It's quite nice!
I don't know why people think that Yesod has only two developers (sometimes people claim it's just me). There are 7 people in the yesodweb organization on Github, and you can see [21 contributors to the yesod repo alone](https://github.com/yesodweb/yesod/contributors). Remember, that Yesod is split into many separate repos, such as hamlet, wai, persistent, and clientsession.
&gt; Would that it did. Yes. Then we could work on making even more complex type systems that add weak sums back in, since we lost them. :)
Also, `where` cannot be used inside an expression; it scopes over an entire declaration, together with all of its guards. Use a nested `let` instead.
I suppose it's OK to ask here if you prefer this format, as long as you don't mind if you don't get very many upvotes. More common venues for these kinds of questions are [stack overflow](http://stackoverflow.com/questions/tagged/haskell), the [#haskell IRC channel](http://haskell.org/haskellwiki/IRC_channel) on [Freenode](http://freenode.net), and the [haskell-beginners mailing list](http://haskell.org/mailman/listinfo/beginners).
The mailing list and stack overflow are definitely more reliable sources of help than IRC. I think the [community page](http://www.yesodweb.com/page/community) points this out. Can you give an idea of what topics aren't covered yet by the book? I know stuff is missing, but it's always best to get input from *readers* as opposed to what I think I should put in. And yes, I'm well aware that the book doesn't cover every aspect of Yesod.
&gt; If this works the way I think it does, then here's what it does: Not quite. The Intel solution is apparently quite similar to the IBM one. It utilizes the fact that the LLC (Last level cache, L3) is strictly inclusive of all other cache contents, and there is a resolve mechanism for handling multiple different versions of the same cache line. So, when you start the transaction, the core starts marking all cache lines you access as private. Whenever you write, the new values only end up in your L1, and are marked as a part of a transaction. If no-one touches the cache lines you marked as yours until the end of transaction, it completes successfully and the values you have in your L1 get written back to the higher levels of cache as usual. If there was a conflict, the L1 lines you have marked as a part of transaction are marked invalid, and the correct value for the memory is found in the L3 and L2. The differences are mainly that there is no active copying of old values, there is no explicit log, and the transactions can be much bigger (in theory up to the size of L1).
How would the limit on the size of a transaction be defined then? I.e. is the question "is this transaction too big" decidable, and if yes, how? What happens if the processor runs out of L1 during a transaction? (And I wonder how this plays with SMT, where you could have two threads running transactions in the same L1 at the same time...) 
I think it would just instantly rollback when it's not possible to continue processing without evicting a cache line that's part of a transaction. That's ok because the system is explicitly "best effort" -- there always needs to be a software fallback, but in practise, the majority of the times the HTM just works. Note that this doesn't only happen when you run out of cache -- the L1 is probably only 8-way, so just 8 horribly aligned writes and a single read that needs to land in the same set would be enough to do that. As for SMT, I don't know. If there still is a L1 shared between threads in Haswell (the cost of duplicating the L1 goes down by half every process node -- they might as well split it soon), it probably works on a first come, first served basis, always leaving at least one way for non-transactional loads.
I have reworked the way I byte-compile haskell-emacs by taking your Makefile and only minimally modifying it. Thanks, it works now. I had to remove paredit.el from lib/ as I already have that. For example EMACS = emacs all: $(EMACS) .....
To call a literate preprocessor from GHC, ghc -pgmL pre ... where pre will be called from GHC as pre -h original input output One should add to output something like the line "{-# LINE 1 \"" ++ original ++ "\" #-}" to fix error messages. This is partially explained in the GHC manual; one also needs to write a dummy preprocessor to determine experimentally how GHC will make the call. This is taken from my own literate preprocessor I've been using for years. In practice the GHC call is most useful for ghci, as I actually use a makefile to organize larger compiled projects. Makefiles can handle preprocessors with any calling convention. 
I usually have up to half a dozen Haskell + packages installations running side by side in directories like /usr/local/ghc-7.0.4-64d, PATH arbitrated. Uninstall by deleting the directory. How could you possibly need to restrict to only on of 32-bit or 64-bit installs at a time? Apple's advice on the "right" places for an installer to put things is poorly considered, if you're stuck like this!
One thing I really miss with the current haskell-mode REPL is the ability to access ghci's auto-completion against loaded modules' symbols... does that happen to work with haskell-emacs's REPL?
Ahh, okay. init.el was only supposed to be a suggestion/setup for running emacs -Q, that's what the lib/ was for, just quickly running to test it out. Glad you got it sorted though. Philip Weaver's currently working on [a proper `Makefile` now](https://github.com/chrisdone/haskell-emacs/pull/33), when he's finished I'll merge that. Btw, paredit modeâ€™s open-round-bracket doesn't play nicely with lambda syntax, as it thinks it's a character escaper. Needs to be modified.
Some time ago, I wrote a GHC literate preprocessor with ReStructuredText support (for Sphinx integration), check http://blog.incubaid.com/2011/10/17/literate-programming-using-sphinx-and-haskell/
The causality is exactly the other way around! I don't think it's good for a type system to be incapable of capturing the purpose of testing. It's often a typesafe transformation to swap the then- and else-branches of a conditional, but it's usually an unsafe thing to do. The replacement property limits the extent to which a type system can manage meaning. Of course, I do want stability under substitution. Otherwise, abstraction stops working.
I'm not entirely sure this is possible, but it may be worthwhile to modify your hibernation scripts to reboot the computer instead of shutting it down. That would get you what you're looking for.
I'd say it is pretty safe to assume that you are not an idiot :-) I understand that below expression is by Haskell's rules the same I have originally given: &gt; \ h -&gt; case h of Hide b -&gt; b &gt; &gt; but what is that type? I'd say, that here we have no choice but say "unknown". Intuitively I'd explain it to myself with *the high fan-in of the lambda bound variable h destroys all the additional constraints (knowledge) that are available in the case of in-situ constructed values* Looks like dependent systems are able to propagate these constraints much better. 
Very incisive analysis, thanks for doing it. I thought your point about the importance of "library evangelism" (a la bytestring) was pretty interesting. Perhaps all the brilliant people writing interesting Haskell libraries are a bit too humble.
Hm...I didn't even know about that feature. I may have to update the summary to avoid confusion. Anansi will probably not work with the GHC preprocessor mode. It is designed to generate multiple files from a single input. For example, your .anansi file might look like: :f Main.hs import OtherModule main = print OtherModule.foo : :f OtherModule.hs module OtherModule where foo = "hello world!" : When you "tangle" this file, Anansi will generate `Main.hs` and `OtherModule.hs` in its output directory.
Not to denigrate your article (which is very interesting), but note that so-called "literate haskell" is not in any way what Knuth meant by "literate programming". In literate programming, the final structure of the code is mostly independent of how it's presented in the documentation. You can do things like write modules out of order, or interspersed with code from other modules (or even other languages). `.lhs` files do not support these features. They are essentially just a modified syntax to `{-` and `-}`. There is no real difference between: some comments here &gt; some code here more comments here and {- some comments here -} some code here {- more comments here -}
You're right indeed. Thanks for the clarification.
This is the future. Mark my words. 
Hm, thanks for tha clarification. I was also confused by this.
Yes, and apparently I made a mistake: 'plugins' does not use Template Haskell.
FYI, work by the student continues on the cabal parallel build patches and for the cabal ghci ones, they are still on my patch queue to merge. So I do expect eventually for both to turn up in your "successful" category.
Right. plugins-auto does a use a bit though.
&gt; NB: This actually works: &gt; &gt; a = case Hide 42 of Hide _ -&gt; 42 Of course it does :) It's easy to give a type to that one, namely `a` has the same type as `42` (the one in the body of the case expression, not the one in the scrutinee). But note that the `42` means different things in these two different occurrences. When we pass `42` to `Hide`, we erase all our knowledge about the type of `42`; at that point is is no longer a number, it is just some arbitrary data. We can pass around arbitrary data, repackage it up, etc., but we cannot do anything that meaningfully depends on the specific datum we have on hand. So when you do the case analysis here you're saying "make sure that `Hide 42` is built from the constructor `Hide` applied to some datum I don't care about". Since `Hide` only has the one constructor, this will succeed. And then you said "if the case match succeeds then return `42`"; and it does, so it does. But it doesn't return `42` in virtue of the fact that the scrutinee has `42`; it returns `42` regardless of the scrutinee's contents. And that's always okay to do, because it's being parametric in the scritinee's contents. You could just as well say: a = case Hide "foo" of Hide _ -&gt; 42
So it doesn't produce a segfault.
It has nothing to do with Apple's advice. It has to do with how various install paths were set up in the installers and in cabal. We can thread $(arch) in there, but we have to make sure we catch all the paths that need fixing and I didn't think I could get that done in this release. My plan is to support it in the next.
I think this is a good opportunity to advertise my [high quality bananas][1], then. [1]: http://haskell.org/haskellwiki/Reactive-banana
Why not just put {-# OPTIONS_GHC -pgmL pre #-} at the top of your file, and have it work with GHCi *and* batch compilation without passing any options?
About yesterday's merge of pheaver's Makefile changes: It's pretty much how I used to byte-compile with my custom Makefile. I didn't submit anything as the README stated this is all alpha and not to be used. So, why do we exlude hs.el and hs-project.el from byte-compile? Why would it hurt to byte-compile them? I would also extend the Makefile to byte-compile lib/*.el and lib/auto-complete-1.3.1/*.el.
Setting ghci and makefile options I do once, then rarely tweak. I don't look at pages of code, I stare, meditate. My own literate preprocessor began life as a way to have comments without junk characters either to delimit comments or delimit code. I extend the indentation rules for code to require some indentation, reserving the flush left position for comments. I then use syntax coloring. So no, I don't want to see compiler directives at the top of Haskell haiku, if I can possibly avoid it!
I know and deeply admire Don Knuth, and beyond frequent scientific use, I just used TeX to help a friend typeset her first novel, more beautifully than any other system could have done. But let's be honest: Aside from pinning science to the wrong side of fixed vs flowable text in an age of e-readers (and Haskell with its profusion of two column papers, no epub alternatives, is a worst offender), TeX is also by far the most ghastly programming language in current common use. There's an opportunity for someone to write a functional math markup language that translates to both TeX and flowable SVG graphics text. So while "The Art of Computer Programming" remains a timeless classic worth studying, I don't give Knuth a pass to lock down the meaning of as basic a term as "literate programming" after one experiment. Literate programming is the enterprise of writing enough about one's code that someone else can read it, and we're discussing tools here that support that activity.
&gt;If you're so sure upper case table names are a problem, file a bug report It is a known issue, that's why I mentioned it. There is no need for me to file another bug report, as gregwebs (one of the devs) did that 6 months ago. I was just adding a "yes, that would really be nice sooner rather than later", so that we don't have to keep adding "sql=" and "reference=" to everything we do when trying to use persistent. &gt;Also, have you seen persistent-hssqlppp? I hadn't, but that is interesting. Looks like a good starting point for what I am after.
&gt;I don't know why people think that Yesod has only two developers I thought that because I've only seen two people claim to be developers, and didn't see any mention of anyone else on the site. The impression given by the site is that it's one guy's pet project. I'd actually be fine with using one guy's pet project, I just wanted to know if it is still a fully supported project or if people are giving up on it. I just don't want to do this project in yesod and then find out "we're giving up on yesod because everyone is just using snap anyways" in 6 months.
Hurray, thank you!!
Technically, [the definition of the `Applicative` type class](http://hackage.haskell.org/packages/archive/base/4.4.1.0/doc/html/Control-Applicative.html#t:Applicative) specifies that, for any type which is also an instance of `Monad`, it should be the case that `(&lt;*&gt;)` and `ap` are equivalent. There is indeed no obvious reason to prefer this order, so I assume the motivation behind that specification was to remove ambiguity by fiat. Consistency here is helpful--it would be a pain to always worry which way the `Applicative` instance is going to order things when writing an expression that uses it together with `Monad`-based combinators.
Does that mean that i do not have to put -rtsopts in cabal file anymore to be able to pass +RTS -N to exe ? 
The book is written by one person (me), and I often times write in the first person. And the blog is written by two people (myself and Greg Weber). That's probably where the confusion came from. The [contributor page](http://www.yesodweb.com/page/contributors) lists 10 people. There are enough companies and individuals basing solutions off of Yesod that there is not real concern of us "giving up on" it in the foreseeable future.
&gt; It is a known issue, that's why I mentioned it. Ah, my apologies. :)
Every time I see a cryptic function name like ($$) or (-&gt;&gt;) or (+&gt;&gt;) I have to look it up and keep reminding myself of their meaning. I think I prefer get / set.. 
Whoops. I forgot that you can't make an operator that binds tighter than function application, so an additional operator is needed to avoid parenthesizing. f |&gt; v = f v infixr 0 |&gt; An example of use: main = start $ do f &lt;- frame [ text := "Playground" ] b &lt;- button f [ text := "Hi" ] (++ " there!") +&gt;&gt; text |&gt; b [ layout := stretch $ widget b ] -&gt;&gt; f
Just to clarify a bit: it's a known issue in the sense that there's a Github issue about it, not in the sense that "we left something broken for six months." There's nothing broken, it's a question of preference. I personally prefer having the table names match the type names precisely, others don't. Also, simply "fixing" this would in fact break almost all existing sites. We're going to have this as a configurable option, but it's pretty low down on my priority list. This is one of those cases where if you want this done quickly, it's time to step up to the plate and write it.
Ah crap. I tested that example and somehow didn't notice the text wasn't getting updated properly. The update definition will actually have to be: f +&gt;&gt; attr = \record -&gt; [ attr :~ f ] -&gt;&gt; record Good thing I'm not a professional programmer. I'd hate to see what I would unleash upon the world.
The |&gt; operator is good for applying compositions to values too. Maybe I'm just missing it but I don't know why such an operator doesn't already exist. Both $ and |&gt; are handy.
Isn't your `|&gt;` exactly the same as `$`, including fixity?
It's not a bad idea for heavily-used functions, either very general-purpose ones or key parts of a library. There's a reason why `(&lt;$&gt;)` ends up getting used more than `fmap` by many people. Setting widget attributes in a GUI library like `wx` seems like a justifiable case to me. You might have to look it up at first, but if you're doing anything with `wx` you'll use them so often you'll memorize them quickly enough.
I'm hilarious. The reason the original definition doesn't work is because I forgot to apply the supplied function to the obtained value. Oh god.
That makes sense and agree with your reasoning. While thinking about it a question popped up in my brain: what if different libraries with totally different purposes define (+&gt;&gt;) to be something totally different from each other.. namespace clash? I imagined a scenario where I'd be using "import .. hiding (this,that)" and constantly shifting my brain from one meaning to another.. But I don't have enough experience to reasonable answer that question.. it's speculative.
Then the library authors should have thought things through more carefully. :] For very generic functions like `(&lt;$&gt;)` the library author needs to take care to avoid clashes, and should be sure that an operator is really useful in context (i.e., tangible benefit from precedence and infix notation), and ideally should work with other library authors to standardize as much as possible. For special-purpose libraries like `wx` that by nature dominate the code that uses them, it's mostly a matter of providing a tidy API and not clashing with libraries likely to be used in the same application. The nice thing is that if the purposes really are *completely* different, the clash isn't bad because you have context to distinguish. It's when the purposes are half-similar that you get problems.
You're right. More embarrassment for myself. I don't remember the situation very well but there was a time I kept having to put parens around compositions because things weren't getting bound right, so that lead me to form the idea that $ could only bind looser than things on its right, which is clearly incorrect. I love strong typing because when the compiler complains I know that it was because my program was incoherent in some way. I hate showing stuff to people because I always make really embarrassing mistakes. But I'm slowly coming to realize that I can't see my own errors and that having my mistakes pointed out by others is a good thing because it helps to clear up the muddle that is my brain. I realize these are very elementary things I'm screwing up on but I believe I will make these mistakes just once or twice, and there's only a finite number of elementary mistakes I can make, so there will be a day when I can move on to more advanced mistakes. That's what I call hope.
Until all your users switch to GHC 7.4+, I think you'll need to keep doing this.
If a library developer chose symbols that conflicted with other libraries he would hear about it, more loudly the more common the conflicts. So library developers tend to have some knowledge of what operators are being used in other libraries and choose their symbols accordingly.
(ugh, reddit is being really annoying about trying to post comments) Well, I have good news and bad news for you. The good news is you'll probably move on to advanced mistakes sooner than you expect! The bad news is you'll continue making elementary mistakes just like the rest of us, haha. I'll let you in on a secret: I make silly mistakes like that all the time. That comment where I asked if the two operators were the same? That wasn't really a question, I actually opened GHCi to check the fixity of `$` before posting it. Almost any time I post more than two lines of code online, I've loaded it in GHCi first to catch any silly errors. Your errors here lie primarily in an excess of haste, not a dearth of coherence. For what it's worth, I actually agree that pseudo-syntax for stuff like `wx` would be nicer than functions with names like `get` and `set`, but there's probably an underlying abstraction that could be distilled here instead of making special-purpose operators for one library.
I salute your persistence.
Part of why I wanted to think of better names for the get and set operations was because I was also thinking about the various proposals for records. I'm still early on in my reading but it seems like the attr and prop technique could be made into something good for records in general. I have social anxiety disorder and it's incredibly challenging for me to try to talk to people about things that matter to me. It's been holding me back and my desire to master computer science more deeply has pushed me to be more confident in social situations. Even a simple post like my root posting leaves me nerve-wracked, so thanks for not attacking me for my dumb mistakes.
Thank you, you're awesome.
You may be interested in the [data-lens](http://hackage.haskell.org/package/data-lens) package.
My copy arrived recently and it does indeed cover natural deduction, however, I noticed that it explicitly mentions that it won't cover dependently typed programming languages (which, as far as I can tell, are the core focus of The Power of Pi). Is there another text that is considered a good resource for beginners that focuses on this area?
Why settle for funny operators? Get: a.x Set a { x = ... }
You will be interested in the following article: [Lightweight linear types in System Fâ°](http://www.cis.upenn.edu/~stevez/papers/abstracts.html#MZZ10), Karl Mazurak, Jianzhou Zhao and Steve Zdancewic, TLDI 2010. &gt; We present Fo, an extension of System F that uses kinds to distinguish between linear and unrestricted types, simplifying the use of linearity for general-purpose programming. We demonstrate through examples how Fo can elegantly express many useful protocols, and we prove that any protocol representable as a DFA can be encoded as an Fo type. We supply mechanized proofs of Fo's soundness and parametricity properties, along with a nonstandard operational semantics that formalizes common intuitions about linearity and aids in reasoning about protocols. &gt; We compare Fo to other linear systems, noting that the simplicity of our kind-based approach leads to a more explicit account of what linearity is meant to capture, allowing otherwise-conflicting interpretations of linearity (in particular, restrictions on aliasing versus restrictions on resource usage) to coexist peacefully. We also discuss extensions to Fo aimed at making the core language more practical, including the additive fragment of linear logic, algebraic datatypes, and recursion. That said, I'm not sure what you meant by your "by extending the type system". I don't want to give the impression that you can express linear types *just* by extending the type system; you still need to do the work of defining your typing judgements in a way that preserves linearity, etc. Kinds are not a trick to get linearity, they're just helping with the bureaucracy of knowing what is linear and what isn't. You still have the heavy lifting to do by yourself. I'm not sure all uses of linearity can be expressed as monads or through parametricity. Some of them certainly can, see [Rolling Your Own Mutable ADT -- A Connection between Linear Types and Monads](http://haskell.cs.yale.edu/?post_type=publication&amp;p=311), Chih-Ping Chen and Paul Hudak, POPL 1997.
| That said, I'm not sure what you meant by your "by extending the type [sic, my original word was intentionally *kind*] system". What I was curious about was whether GHC's *kind* mechanism (as opposed to its type mechanism, since as I understand it kinds are like a type of types) is flexible enough that one could add kinds that were linear types. It looks from the first paper like this is indeed a viable approach, roughly speaking. Great information, thank you very much!
|&gt; operator is used by Data.Seq (along with &lt;| and &gt;&lt; )
&gt; by extending the type [sic, my original word was intentionally _kind_] system Just so you know, for the people you're asking, the term "type system" includes both the typing and the kinding judgments (and often several other auxiliary judgments as well).
"type system" is a kind (sic) of generic word to describe the static semantics of a language. Kinds are part of the type system. You can also more specifically speak of the "kind system" but I don't think that would be appropriate here, as linear types could have effect at the kind level, but most certainly affect the type level (unless you were speaking of linear *kinds* that are yet another thing).
After getting a first round of answers, I addressed them with a tiny benchmark and since then I have had no feedback. I guess that's because SO doesn't notify you. Maybe I'll get some more answers here.
Just use eliminators (like maybe).
Just use eliminators (like maybe).
Can you expand that a bit? [If you mean fold](https://www.google.com/search?q=%22eliminator%22+maybe+haskell) (my best interpretation of those search results), it's not immediately obvious how to replace case statements with fold and maybe in a performant manner.
I've thought it would be nice for case and such to be functions instead of keywords, but... am I getting a rendering error or are you seriously proposing `LAMBDACASE` et al as keywords? My first guess at syntax may not work out but my thought has always simply been case of ConsA a -&gt; ... ConsB a b -&gt; ... and similarly for `if`.
&gt; it's not immediately obvious how to replace case statements with fold and maybe in a performant manner. withcase :: Maybe String -&gt; String withcase x = case x of Just x -&gt; "here: " ++ x Nothing -&gt; "nope" sanscase :: Maybe String -&gt; String sanscase = maybe "nope" ("here: "++) $ ghc -c -ddump-simpl -O2 maybedemo.hs ==================== Tidy Core ==================== Foo.sanscase1 :: [Char] Foo.sanscase1 = unpackCString# "here: " Foo.sanscase2 :: [Char] Foo.sanscase2 = unpackCString# "nope" Foo.sanscase :: Data.Maybe.Maybe String -&gt; String Foo.sanscase = \ (ds1_aeh :: Data.Maybe.Maybe String) -&gt; case ds1_aeh of _ { Data.Maybe.Nothing -&gt; Foo.sanscase2; Data.Maybe.Just x_aem -&gt; ++ @ Char Foo.sanscase1 x_aem } Foo.withcase :: Data.Maybe.Maybe String -&gt; String Foo.withcase = \ (x_adj :: Data.Maybe.Maybe String) -&gt; case x_adj of _ { Data.Maybe.Nothing -&gt; Foo.sanscase2; Data.Maybe.Just x1_adk -&gt; ++ @ Char Foo.sanscase1 x1_adk }
The eliminator for lists is not `foldr`. It would be the following function: listElim :: b -&gt; (a -&gt; [a] -&gt; b) -&gt; [a] -&gt; b listElim a _ [] = a listElim _ f (x : xs) = f x xs
OCaml has the "function" construct: let f = function Some x -&gt; x + 1 | None -&gt; 0 Something similar for Haskell would be great (although preferably with lighter syntax).
Ah, I thought it was meant as a general case replacement, not just for the Maybe type. glguy's post implies other people know "eliminator" as a term. I've never heard of it, and Google isn't proving helpful for conjuring up a definition; is there a link someone could give me for the term?
&gt; Could Haskell be extended to support linear/affine types by extending the kind system to make such types possible? Yes. &gt; Also, even if the answer to that question were yes, does the use of monads and higher-order types (like those used in "runST" function for the ST monad) make linear/affine types completely redundant in Haskell. No. For example, a linear mutable array provides a guarantee that it is never manipulated in such a way that effects are visible. ST guarantees this *externally* but not internally. Linear types also allow for guaranteed freeing of resources without placing them into some sort of stack discipline; ST and similar approaches do not.
White text on red background thwarts reading
I wonder whether it would make sense for lambdas to just introduce a block straight away. For example: \{ConsA a -&gt; ...; ConsB a b -&gt; ...} or: \ConsA a -&gt; ... ConsB a b -&gt; ... The current style, `(\x -&gt; ...)`, would still work as a special case where the block has only one entry with an always-matching pattern.
[Catamorphisms](http://knol.google.com/k/catamorphisms) are [bananas](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.125).
In the lambda case, I think you would need to show how that works with more than one argument, though the obvious thing is to say that it is semantically equivalent to packing the arguments in a tuple and unpacking them in the case clause. \ConsA a, True -&gt; ... ConsA a, False -&gt; ... ConsB a b, True -&gt; ... -- pattern match fail on ConsB _ _, False That's packing an awful lot of power behind one lil' character, though, and the indentation seems very subtle to me there.
How about reusing the parentheses trick from function definitions? So, rewrite my first examples above to: \{(ConsA a) -&gt; ...; (ConsB a b) -&gt; ...} or: \(ConsA a) -&gt; ... (ConsB a b) -&gt; ... Your example becomes: \(ConsA a) True -&gt; ... (ConsA a) False -&gt; ... (ConsB a b) True -&gt; ... I'm not sure I can respond to the "the indentation seems subtle" comment other than to say that Haskell programmers are used to paying attention to indentation anyway. =)
Sorry, it's not a catamorphism. I've also heard the term "deconstructor" for such a function.
While an interesting thought experiment, I hope no one tries to write code this pointlessly point free. I mean, how would one write this in a mathematical formula which is intended for others to read? [; f(x) = \begin{cases} g&amp; \text{if } x &gt; 0,&amp; k&amp; \text{otherwise}. \end{cases} ;] Becomes: [; f = \begin{cases} g&amp; \text{if } fresh &gt; 0,&amp; k&amp; \text{otherwise}. \end{cases} ;] While I generally like stringing point free functions together for elegance (where it's elegant), I don't see how this is an improvement. You could use the lambda case in a mathematical representation and apply it by taking care of the point-free-ness as such: [; \mathfrak{L}(F, B, G)(fresh) = \begin{cases} G&amp; \text{if } B(fresh),&amp; K&amp; \text{otherwise}. \end{cases} ;] [; f = \mathfrak{L}(g, &gt;0, k) ;] But, I'm just having trouble figuring out why someone would do this here as I expect it's intended to make the following seem somehow elegant: [; f = \mathfrak{L}(g, &lt;0, \mathfrak{L}(\mathfrak{L}(k,&gt;3, m), =0, \mathfrak{L}(n,&gt;0, o))) ;]
Well, [Advanced Topics in Types and Programming Languages](https://www.cis.upenn.edu/~bcpierce/attapl/) has a chapter by Aspinall &amp; Hoffmann that makes a very nice introduction. Then if you still want to dig deeper, it depends whether you lean more on the theoretical or (relatively) applied side of things. [Simon Thompson's book](https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/ttfp.pdf) would be my next recommendation for the latter, Zahohui Luo's thesis (extended as a book under ISBN 0198538359, also available in less polished form on the web) the canonical reference for theformer.
Agda has the "simple lambda" (\x -&gt; x) but it also has a "patterned lambda" (\\{ Foo -&gt; foo; Bar -&gt; bar}). It doesn't do the indentation thing for lambdas though.
Either is the most _useful_ error monad in Haskell. Maybe is the most _convenient_. Everything else is type-unsafe fluff.
One way to achieve your goal is to run a VM using software like VM Player or VirtualBox. It's then really easy to save your state of work in the guest OS when you need to shutdown the host OS/machine.
Thank you, I guess that in my head I assumed that there were different mechanisms at work at the two levels. :-)
Wow, I've already learned a whole lot from this discussion about the way that people working on these kinds of problems think. :-) Thanks everyone!
 foo (Just x) = x + 1 foo Nothing = 0 can be done in a single line like so: Prelude&gt; let { foo (Just x) = x + 1 ; foo Nothing = 0 } So maybe we just want to extend this to anonymous functions like: \{ (Just x) -&gt; x + 1; Nothing -&gt; 0 } OCaml does this (more cleanly) with the "function" alternative to "fun" (where "fun" is the equivalent to Haskell's "\" lambda construct) edit: formatting
Oh, okay, so if I am understanding you correctly: the technique of parameterizing over an arbitrary but unknown type that is used by runST to make sure that nothing can escape from its scope can enable one to emulate some of the features of linear types, but the problem is that this still requires discipline on the part of the coder (e.g., the person writing up the ST library) and so in this sense they re not as safe as using linear types which provide statically verifiable *guarantees* that resources are being used in a single-threaded way and are guaranteed to be freed at the end. Thanks!
From [one of the first pieces of haskell code I had ever seen](https://github.com/Eelis/geordi/blob/master/src/Util.h): #define case_of \caseof_detail -&gt; case caseof_detail of
I'm partial to `succ` and `pred`.
Could you elaborate on the type-unsafe fluff?
Someone's never heard of Control.Monad.Error, I take it.
I avoid using those in numeric situations because it turns a generic function like foo :: (Num a) =&gt; a -&gt; a into foo :: (Num a, Enum a) =&gt; a -&gt; a Also, even if the current Float/Double/Rational Enum instances do what's desired here, I'm wary about relying on them to have such a meaning, considering how much resentment there is towards them.
Doesn't that break old code? \Just x -&gt; ... would no longer compile. I suppose we could double the backslash (`\\`) or use a keyword (`fn`?) when we want to introduce an anonymous multi-argument function block, but this still breaks existing code.
if you like point-free code so much and hate variables/bindings, why do you use haskell? maybe you should try a stack language (like factor)?
I agree with you. I also don't see much need to support the proposed syntax.
Thanks. You may also want to try out [Tex The World for Firefox](http://thewe.net/tex/) or [for Chrome](https://chrome.google.com/webstore/detail/mbfninnbhfepghkkcgdnmfmhhbjmhggn). Edit: This was in reply to someone posting a link to a [rendering of the latex I posted](http://snapplr.com/xdmm).
No. `\Just x -&gt; ...` already doesn't compile.
Oh, it needs the brackets already: \(Just x) -&gt; ... In that case, I have no objection to your proposal.
There is a paper from a while back called, "Rolling Your Own Mutable ADT," which describes a way to use monads to force linear use of a certain data structure. Basically, you pass around the thing that is to be used linearly as the state, and every operation of the form: (@s, a) -&gt; (@s, b) (`@` denoting linearity) becomes instead a function: a -&gt; M b Then, by construction, it is impossible to use the resource in a non-linear way, because you just have a tree of linear operations you want to perform on the structure, of which one path will be chosen, and you don't get to refer to the resource explicitly, so you cannot duplicate references to it and whatnot. You can view `ST` in this way, where the linear resource is the heap or whatever. However, not that I've really tried, but I expect this is an inconvenient way to program with such linear resources. At least if you care about the structure of the resource itself. It's fine for `ST`, but if you wanted some single linearly used array, you'd have to write things like: x &lt;- read i y &lt;- read j write k (x + y) and combining two linearly used resources requires a modified monad that works with both, instead of just referring to the two resources directly and having the compiler verify that you don't do anything wrong.
No, that's not what I meant. Sure, the ST library author could screw up, but so could the person implementing linear types. There's actually a lot less to screw up with the former. What I meant is that it's harder for the user because, with ST, he or she still needs to think about mutation and effects. With linear types, everything remains purely functional at all stages. I really wouldn't think about ST as a way of emulating linear types; ST allows you to do a lot of things linear types do not such as having local effects. The reverse is also true. Ideally, you'd have both. That said, I don't see linear types being added to Haskell anytime soon. 
It's page 10, not 20. I can't help you with the problem, but the PDF you linked was very interesting, so thanks for that :)
This is more of a pain than I originally thought. In my implementation the main points are: 1. I made sure to turn off buffering. I don't think it's needed, but just in case. 2. After opening, initially called 'hSeek h SeekFromEnd 0". This gets the end point of the file. 3. Use Control.Exception.catch to catch/retry at EOF exceptions. It would be simple enough to make a "hGetLineM :: IO (Maybe String)", which would match the Python code closer but I'm not sure how useful it would be for other tasks. 4. It seems the call to "hGetLine" doesn't get new lines. As a hack-fix I made it close-reopen the file in the loop and re-seek to the old EOF location (discovered using "hTell"). This quick close/open behavior has some bug (a race?) so I also had to catch DNE exceptions too. So, all-in-all yuck. One yuck point for the exception instead of a maybe type (which a library function could fix, if we care enough to propose such a thing), a second yuck point for hGetLine not seeing new lines. I suspect the second is a bug of some sort (or is this related to file locking behavior that might change?) but am not bothering with it any further.
I've never understood this attitude. "Maybe it's not necessary, but just in case." Why not take thirty seconds more and learn it once and for all? Then again, perhaps those 30 seconds could be better spent. So, in conclusion, nothing.
&gt;One yuck point for the exception instead of a maybe type Wouldn't [Control.Exception.try](http://hackage.haskell.org/packages/archive/base/4.4.1.0/doc/html/Control-Exception.html#g:7) work?
Works as expected on GHC 6.12.3, GHC 7.0.4, GHC 7.2.1, GHC 7.2.2. I think your algorithm's just a bit buggy. import System.IO import Control.Concurrent main = do logfile &lt;- openFile "access-log" ReadMode hSeek logfile SeekFromEnd 0 follow logfile follow logfile = do catch (do line &lt;- hGetLine logfile putStrLn line follow logfile) (const (do threadDelay (1000 * 100) follow logfile)) I'll paste this because it doesn't answer OP's question and cheat them out of figuring their problem out, which is how to make a generator. Care to paste yours? It's probably just a simple oversight, let's fix it.
No problem! &gt; You forgot to seek to the end "hSeek logfile SeekFromEnd 0" before calling follow. Ah, sure, I'll add it so that the example's complete. :-)
This is fantastic! Where can I learn more about the "new and powerful equationnal unification"? Also, why is vec {0} a = VNil vec {n+1} a = VCons a (vec {n} a) translated to: vec 0 a = VNil vec (n+1) a = VCons a (vec n a) rather than: vec a = VNil vec a = VCons a (vec a) which would not need to verify the indexes at run time?
One way is to use the function for making lazily generated IO operations, [unsafeInterleaveIO](http://www.haskell.org/ghc/docs/latest/html/libraries/base/System-IO-Unsafe.html). It's a small change in this case: import System.IO import Control.Concurrent import Control.Monad import System.IO.Unsafe main = do logfile &lt;- openFile "access-log" ReadMode hSeek logfile SeekFromEnd 0 lines &lt;- follow logfile forM_ (take 3 lines) putStrLn follow logfile = unsafeInterleaveIO $ do catch (do line &lt;- hGetLine logfile lines &lt;- follow logfile return $ line : lines) (const (do threadDelay (1000 * 100) follow logfile)) This will print the first 3 lines in the infinite list and then complete. There are some other ways. * Just make a thunk like `getAnotherLine` that you call repeatedly for more values. * I'll have to read more about coroutines as implemented in Python to understand more, but coroutines can be implemented with continuations, and then you could implement the yield function et all. But I'm not sure how `for x in â€¦` syntax works, whether follow(logfile) is a proper list or basically a thunk that you can keep calling.
In the case of vec, you need to know then length at run time! How would your alternative actually work deterministically, after erasure? There is some point to having actual Pi-types, not just a static-only forall.
Here's how one might do it with what I'd call a generating thunk, don't recall if there's a name for it, it's sort of common in functional programming. I'd say this is only a bit like the Python version. There's an explicit loop and you have to say `getAnotherLine`, but it does somewhat abstract the notion of waiting for lines for you. Thought it might be worth a mention. import System.IO import Control.Concurrent import Control.Monad main = do logfile &lt;- openFile "access-log" ReadMode hSeek logfile SeekFromEnd 0 getAnotherLine &lt;- follow logfile forever $ do line &lt;- getAnotherLine putStrLn line follow logfile = return go where go = catch (hGetLine logfile) (const (do threadDelay (1000 * 100) go))
Oh of course, I'm an idiot. I'll try and refrain from further brilliant insights until I am a bit rested.
first, it takes more than 30 seconds to be sure with confidence. when you're hitting you head against a wall over a bug, you start to question your assumptions. For example, one might "know" his mobile device is a linux box and "know" that its network doesn't need to be reset, but he has no idea what the problem is so he might still reboot the fucker just in case. second, when trying to understand something, we look the answer up, but sometimes understanding the answer requires us to understand a handful of other things we haven't seen before. third, all of us aren't as smart as you.
Hey I didn't know the answer either, and there's obviously plenty of people smarter than I am on here. You made good points, sorry if I came off as judgmental.
Is there really no blocking solution to this, like you would block on a socket or pipe that does not have more data available yet?
My algorithm might be buggy but your code didn't work for me either...
Yes! But you'd still need to fix the type of 'e' to get everything compiling. I guess I sometimes like little hack scripts to be able to live in lala land where the concept of failure doesn't even appear in the code. Does that make me a bad person?
 (let f (Just x) = x + 1; f Nothing = 0 in f) You can use the above just like an anonymous function. Not the cleanest, as you have to introduce a new name for it, but it's *almost* as compact as it can possibly get, and this syntax already exists.
&gt; case statements thwart point-free And there's nothing wrong with thwarting point-free, as long as it's in the name of clarity and readability. The suggested lambdacase/functioncase syntax looks hard to read, imho.
Now I worry that I might be over-selling the unification algorithm! The actual answer to your first question should be "in my thesis, when I've written it", but in the meantime there are a couple of papers on simpler versions of the system: "Type Inference in Context" from MSFP 2010 and "Type Inference for Units of Measure" from the TFP 2011 pre-proceedings. Both are available from [my website](http://personal.cis.strath.ac.uk/adam.gundry/).
It seems GNU coreutils /usr/bin/tail [blocks when](http://git.savannah.gnu.org/cgit/coreutils.git/tree/src/tail.c#n1078) following a single pipe/stream by descriptor. Otherwise, it sleeps 1 second.
Thanks! I doubt you're overselling anything given your nice examples :) I've been thinking about trying to start writing a similar haskell extension in which the arithmetic and other obligations are sent to an smt solver for processing (not sure how feasible that is...), and this give a really nice idea about how to proceed... Of course I would need to (at least partially) solve the unification modulo problem.
You could also use one of the coroutine packages if you really want to replicate the python version, but I'm not sure that's what you want.
Okidoke, thank you very much for the clarification! :-)
There is no need for unsafeInterleaveIO. You can use forkIO, a channel and getChanContents. import Control.Concurrent ( threadDelay, forkIO ) import Control.Concurrent.Chan ( newChan, writeChan, getChanContents ) import Control.Exception ( tryJust ) import Control.Monad ( guard, forever ) import System.IO ( hSeek, SeekMode(SeekFromEnd), stdin, hGetLine, Handle ) import System.IO.Error ( isEOFError ) tailFRead :: Handle -&gt; IO [String] tailFRead h = do c &lt;- newChan forkIO $ forever $ do r &lt;- tryJust (guard . isEOFError) $ hGetLine h case r of Left _ -&gt; threadDelay 100000 -- wait 0.1s on EOF Right l -&gt; writeChan c l getChanContents c main = do hSeek stdin SeekFromEnd 0 ls &lt;- tailFRead stdin mapM_ putStrLn ls 
Seems like a round about way to use the `unsafeInterleaveIO` inside `getChanContents`. :-)
That is true, but then you have no proof obligation. Because somebody else did think it through. And generally I prefer to avoid unsafe... functions as often as possible.
&gt;But you'd still need to fix the type of 'e' to get everything compiling. Fix e = `SomeException`; all exceptions will be caught.
Well, there are functions without "unsafe" in front of them that are implemented with, and no safer than, `unsafeInterleaveIO`; `hGetContents` is one. `getChanContents` is more benign because it can't cause IO, just react to it, but it still makes me uneasy.
If you're prepared to move towards type checking rather than type inference, it should be perfectly feasible to do little more than extract constraints and throw them at an SMT solver. Nice inference behaviour depends on how you handle unification modulo, though. It shouldn't be too hard to replace the Presburger arithmetic solver used by inch with a more powerful solver, as erasure means there's no need for evidence. Trying to do this in a more principled way (with a typed intermediate language like System FC) might need a bit more thought...
Sure, whether hGetContents is really safe is quite debatable. But I'd say getChanContents is safe, because all side effects still happen in a "normal" thread.
I'm not so sure â€” I'll agree that [sinks](http://hackage.haskell.org/packages/archive/sink/0.1.0.1/doc/html/Data-Sink.html) are pure on a good day, but `getChanContents` still has the feel of pure code being involved in IO shenanigans to me. You can almost write `hGetContents` with it, after all (the difference being that input is requested whether it's forced or not). Of course, your definition of "safe" might not require purity.
Well, since Chan is an unbounded FIFO queue the forked thread is not influenced by the evaluation of the result of getChanContents. So, I would say this is really pure.
No, but the evaluation of `getChanContents` is influenced by the `writeChan`s made to the Chan.
It's all just logical relations. So is the syntax of types et al., for that matter.
from [the docs](http://www.haskell.org/ghc/docs/latest/html/users_guide/arrow-notation.html) &gt; A simple example of the new notation is the expression &gt; &gt; proc x -&gt; f -&lt; x+1 &gt; &gt; We call this a procedure or arrow abstraction. As with a lambda expression, the variable x is a new variable bound within the proc-expression. It refers to the input to the arrow. In the above example, -&lt; is not an identifier but an new reserved symbol used for building commands from an expression of arrow type and an expression to be fed as input to that arrow. (The weird look will make more sense later.) It may be read as analogue of application for arrows. The above example is equivalent to the Haskell expression &gt; &gt; arr (\ x -&gt; x+1) &gt;&gt;&gt; f It goes on from there to explain the rest of the sugar.
Here's a (fairly old) description of the translation process: http://haskell.org/arrows/sugar.html
&gt; I know the arrow notation must expand into the primitive arrow functions on lambdas, to provide the contexts where variables are bound, just like monadic do works, but I can't figure out exactly how. This is an incorrect assumption. The arrow notation doesn't expand into lambdas quite in the same way as monadic do-notation does. The name bindings in arrow notation are just used by the compiler to figure out how to pipe the arrows together. For example: f = proc x -&gt; do a &lt;- foo -&lt; x bar -&lt; a Since we "pipe" *x* into foo, bind that to *a* and then push that to *bar*, the above arrow notation translates simply to: f = foo &gt;&gt;&gt; bar So the result of the translation doesn't have *a* bound to a lambda parameter anywhere. The only things that translate into lambdas are expressions on the right side of -&lt;. For example: g = proc x -&gt; do a &lt;- foo -&lt; x b &lt;- bar -&lt; x returnA -&lt; (a + b) Now since we use *x* twice, the arrow needs to be split using &amp;&amp;&amp; and the expression (a + b) is translated into a lambda function like this: g = foo &amp;&amp;&amp; bar &gt;&gt;&gt; arr (\(a,b) -&gt; a+b) &gt;&gt;&gt; returnA
Although if it helps, I guess the first example could be thought to go through the intermediate step f = arr (\x -&gt; x) &gt;&gt;&gt; foo &gt;&gt;&gt; arr (\a -&gt; a) &gt;&gt;&gt; bar but the arr elements here are just an identity arrows, so they can be removed altogether.
Implementing things in Scheme is the best way to understand what's going on, I feel. Scheme is so bare bones that you can't smuggle in any of the awesome ideas, you have to roll your own. I once bootstrapped simple types, laziness, and case expressions into Scheme for fun and I felt I came out of it understanding things much better than I had before.
would it be possible to use this: a &lt;- foo instead of this: a &lt;- foo -&lt; () on the rhs you must have an arrow expression, followed by '-&lt;' and followed by arrow's input, but it gets tedious to right "-&lt; ()" over and over.
Interesting. Given that `x` might appear on the RHS of any triple in such a list, is it true that we need to read the entire arrow expression before we can "compile" it into primitives? We can't peel off one set of expressions at a time, as we do when translating monadic-do?
Thank you, this is exactly the sort of document I was looking for.
It seems to me that this is the one area that Lisp's really excel. It might be that Haskell represents some near optimal ideal in functional programming, but I can't understand most of the ideas in Haskell until I build models in Scheme.
What is the significance of this?
Good luck doing that for GADTs, dependent types or type families.
Would you say this complexity is a weakness of Haskell or what? I kind like the possibly naive idea that one can completely understand all the semantics of a language.
That's right. A single line of arrow notation cannot be translated in isolation. There's actually a tool that translates arrow notation to the primitive operations here: http://hackage.haskell.org/package/arrowp I'm not sure if the resulting code is exactly the same as what you get behind the covers when using -farrows in GHC, but it should give you a rough idea about how different things get translated.
Always a good way to spend half an hour, listening to SPJ, and interesting to hear of his involvement in computing in schools. When I was at a UK school (in the 80s) there were two parts of the school which contained computers (well three towards the end when the library got a PC to view CD-ROMs). The maths department had BBCs (later supplemented with ARM-based Archimedes), they were used to program BASIC as part of a subject called computing. There was also a room full of IBM PC compatibles which had replaced typewriters, and typing lessons, with word processing in a subject known as Business Studies. As typing had been earlier, it was seen as training for future clerical workers, not preparation for academic study; so it was another two decades before I finally taught myself to touch-type. It sounds like "computing" died out and "business studies" morphed into "ICT". I'm certainly behind SPJ's efforts to reintroduce it. And the efforts of initiatives such as [Raspberry PI](http://www.raspberrypi.org) to produce the low cost hardware for school students to hack on. 
[See here](http://www.haskell.org/pipermail/haskell/2011-May/022799.html) for the thread with the original proposal to join the SFC, and the linked message in this post for details about joining SPI instead. But the summary is basically: If you don't know why it would be significant, it probably won't impact you directly. Creating a formal organization for the Haskell language and joining a larger group that supports open-source software communities has a lot of benefits for maintaining community resources like Hackage and the Haskell website, participating in Google Summer of Code, &amp;c.
One of the main reason to have a legal entity behind haskell.org is to be able to hold and receive money. It will enable haskell.org to receive (tax-deductible in US) donations, which is something a lot of people want to do.
It's not complex, it's just not easy to implement. You shouldn't have to implement something to understand its semantics.
The annswer to one part of your question can be found in Control.Arrow. -- | Precomposition with a pure function. (^&gt;&gt;) :: Arrow a =&gt; (b -&gt; c) -&gt; a c d -&gt; a b d f ^&gt;&gt; a = arr f &gt;&gt;&gt; a All the arrow notation can be translated into ordinary Haskell using the functions defined there and in Control.Category. The rules are given Patterson's paper cited in a couple other comments.
Not sure if it's just my bad Internet connection, but I can't seem to stream beyond the first 30 seconds. :(
Just love to watch SPJ explaining anything at all.
There are links on the right of the video which allow you to sidestep the awesome silverlight experience.
You don't justify your title.
Shouldn't have to, no, but it doesn't hurt. At least that's what I find. Once I've implemented something, I feel I understand it much better. Some times i have to implement something to understand it at all. Usually not, but sometimes.
What you wrote is good, but I think I fundamentally disagree with this analogy. What you said is more or less true but I think hinting at the idea that you use Monads to drop down into some imperative programming language can be kind of pedagogically bad for newcomers. Every once and a while we see someone on #haskell ask "How you do a while() statement in the IO Monad" because of this kind of thinking. I also think the IO Monad may debatably be the best monad for justifying why you would use monads, but its one of the worst for explaining because of how its implemented in GHC. The list monad on the other hand is really awesome pedagogically because its concise, deterministic, and non-stateful. Just my two cents on the matter.
You may find [this](http://apfelmus.nfshost.com/articles/operational-monad.html) interesting. It takes a view a little like yours - that monads define programming languages in terms of sequences of operations executed by an interpreter.
Well, GADTs kind of *reduce* complexity â€” they remove restrictions and make the language more general, it's just that implementing the type system becomes harder. Dependent types are analogous, with the system becoming even simpler, even more being made possible, and implementation becoming even more difficult. Haskell doesn't have dependent types, though, no matter how many language extensions you use.
monads are like burritos.
Where a burrito is an abstract data type for a computation or arbitrary control flow.
Yes, any instance for `ZipList` is going to violate the monad laws one way or another. There is, however, a valid monad compatible with the `Applicative` instance for `ZipList`, but only if you can restrict the lists in a certain way. The resulting monad is unlikely to be *useful*, but it is valid. I'm being intentionally vague, of course, in case you want to figure out the details on your own. If you'd like more information, [this question on Stack Overflow](http://stackoverflow.com/q/6463058/157360) covers roughly the same topic.
TIL what is that computer on the [Look Around You](http://www.youtube.com/watch?v=BeraIbgXXIE) introduction and why it runs basic.
YAMT
Please, write moar articles, like this, please!
I patched OpenCLWrappers library to use IO exceptions at the start of this month, and it has made its way into a [branch](https://github.com/jkarlson/OpenCLWrappers/commits/exception) of the git repository. I also reported it to the [OpenCL](http://hackage.haskell.org/package/OpenCL) package, and it now uses exceptions too, and has for several Hackage versions. Personally, I think it has a slightly nicer Haskell API and it definitely has better Haddock documentation. I also think the code is a bit cleaner (OpenCLWrappers used `unsafeCoerce` to avoid writing a Bits instance until I patched it out (!)). Just a heads up :) (And no offence to OpenCLWrappers' developer â€” I've just come to prefer OpenCL. It certainly has its advantages; for instance, I don't think OpenCL binds everything OpenCLWrappers does yet.) Also: `Either Either ()`? I hope that's a typo :P
Wow, how had I missed the OpenCL packageâ€¦ was out announced on reddit/planet haskell? I think I'll have to start working on that insteadâ€¦ seems I've just wasted my weekend. Oh well, at least I've learns some things (mainly how the types relate to the underlying functions. clEnqueueNDRangeKernel is quite difficult to use, when just going off the types. I guess I'll start writing wrappers around what's in the OpenCL package, to make life easier. Maybe I can get started on a Vector/Repa based library sooner than expected.
&gt;Wow, how had I missed the OpenCL packageâ€¦ was out announced on reddit/planet haskell? [Seems not](http://www.reddit.com/r/haskell/search?q=opencl&amp;restrict_sr=on), although reddit's search engine is hardly reliable at the best of times. &gt;seems I've just wasted my weekend. Sorry about that :) &gt;I guess I'll start writing wrappers around what's in the OpenCL package, to make life easier. Maybe I can get started on a Vector/Repa based library sooner than expected. That would be cool! I've been idly planning to write a library with typeclasses for the obvious data structures and operations that you can run directly or with OpenCL for a while now, but your project sounds less difficult to implement in the short term, so I'm eager to see what comes of it.
Monad tutorials: now in video form! And I didn't even find this one very clear.
Okay, now we are up on six different OpenCL wrappers that I know of. I'm maintaining the hopencl package (http://hackage.haskell.org/package/hopencl-0.2.0). We have HsOpenCL, OpenCLWrappers, OpenCL, hopencl, OpenCLRaw and the package presented by AMD in early 2011, but haven't released yet (as far as I know). I think we should start working together on this, instead of repeating the same work so many times. You guys might also like to checkout the OpenCL backend for Accelerate I have been working on: https://github.com/HIPERFIT/accelerate-opencl - it does not support everything yet, but the basic infrastructure is there and good share of the operations are working. The most important missing operation is the prefix sum, but I will not have time to look at it again for the next four months, as I am writing my master thesis. Another package you might like to know about is the quasiquotation library language-c-quote, I've added support for OpenCL to it, but that part has yet to be released on hackage. You can find the newest version here: https://github.com/mainland/language-c-quote To see a quasiquotation usage example, look at: https://github.com/HIPERFIT/accelerate-opencl/blob/master/Data/Array/Accelerate/OpenCL/CodeGen/Skeleton.hs
It was announced on Haskell-Cafe, you might like to read this thread: http://haskell.1045720.n5.nabble.com/ANN-OpenCL-1-0-1-3-package-td4864453.html
&gt; Maybe I can get started on a Vector/Repa based library sooner than expected. Please read my comment about Accelerate, you might like to contribute to that. I think it is sort of what you would like to do.
Cool!
So nice to see a Haskell company succeeding.
I actually wrote a bit of Vector-OpenCL code this week. I'll try to write something up next week, so if you don't get there first, keep your eyes on /r/haskell for a link. 
This seems worth quoting: &gt; *â€œWe actually have the reversed problem to most tech companies today. We canâ€™t recruit all the talent that is knocking at our door. We posted a short reply on Reddit about using Haskell and got 10 applications instantly.â€*, says Gracjan Polak, CTO and the second founder of Scrive. Maybe we should start advertising /r/haskell as a secret weapon for startups to recruit employees? Haha.
OpenCLRaw hasn't seen an upload since August 2009, so I don't think it's fair to consider it active. HsOpenCL doesn't seem to be on Hackage, which eliminates it for most use-cases. One advantage I can see of OpenCLWrappers/OpenCL over your hopencl is that they don't depend on the system OpenCL library to build; that means that they get full Hackage documentation and can be installed with a simple `cabal install`. I agree that collaboration would be much better than the current situation; OpenCL or hopencl seems like the most promising basis for me. Does your library have any notable advantages over OpenCL? It would be nice to know if it could be adapted to not depend on the system OpenCL library â€” the OpenCL header files are standard and freely redistributable, so there's no need to link to the library just to use those.
There are some pearls of wisdom in there, but the overall presentation is a bit confusing imo. Not the best introduction to monads.
What if you reinstall HTTP? Although, if things are that borked, I might recommend moving .cabal to a backup and then reinstalling.
Thanks, I didn't know whether it had gotten into a bad state or whether it could be fixed by changing some version constraints somewhere. I'll try reinstalling.
&gt; OpenCLRaw hasn't seen an upload since August 2009, so I don't think it's fair to consider it active. &gt; &gt; HsOpenCL doesn't seem to be on Hackage, which eliminates it for most use-cases. Correct, my point was just that we have wasted enough time by doing the same work repeatedly, and we should probably think about picking out the best of each of these libraries (including HsOpenCL and OpenCLRaw) and concentrate on that, we would get better quality bindings if we join efforts on a single of these libraries. &gt; One advantage I can see of OpenCLWrappers/OpenCL over your hopencl is that they don't depend on the system OpenCL library to build; that means that they get full Hackage documentation and can be installed with a simple cabal install. &gt; &gt; I agree that collaboration would be much better than the current situation; OpenCL or hopencl seems like the most promising basis for me. Does your library have any notable advantages over OpenCL? It would be nice to know if it could be adapted to not depend on the system OpenCL library â€” the OpenCL header files are standard and freely redistributable, so there's no need to link to the library just to use those. Essentially hopencl just needs the header files to be available for compilation. Earlier on I actually just bundled the official header files from Khronos with the package, like the OpenCL package does now (that version was not published on Hackage). For some time ago I discovered some minor differences between the official header files and those installed with the particular version of NVIDIAs OpenCL bindings on my universitys Fermi-server, so I thought it was better to always use the header files on the local machine. I'm not sure whether those header files are part of the standard, and whether we could safely assume that all constants are defined in the same way in all OpenCL implementations. 
Great stuff, guys!
"...and those instructions are defined with monads." You state this as if it's self-evidently true, but it's not, at least to me. (I don't see that function application in C is defined with monads. I don't even see that function application in *Haskell* is defined with monads.)
Cabal has been buggy for a long time, especially on Mac OS X 10.7 Lion. You might have a better experience if you use [Haskell Platform](http://hackage.haskell.org/platform/) to install everything for you.
It does violate the associativity law. For example, if you take xs :: List Int xs = [1,2] f : Int -&gt; List Int f 1 = [11] f 2 = [21, 22] g : Int -&gt; List Int g 11 = [111] g 21 = [] g 22 = [221, 222] Then (xs &gt;&gt;= f) &gt;&gt;= g = [111, 222] while xs &gt;&gt;= (\x -&gt; f x &gt;&gt;= g) = [111]. The analogous instance definition for streams instead of lists works and yields a monad corresponding to the ((-&gt;) Nat) monad under the isomorphism between Stream a and Nat -&gt; a. 
I thought cabal was part of HP, and that was how you installed things in HP. How do I install things otherwise?
This is much, much slower than the usual typing (no pun intended).
I realize this isn't intended to be a production code, but next you might try implementing OAEP, which is critically important to real applications of RSA. Good Math, Bad Math has a nice blurb about [OAEP and why padding is important](http://scienceblogs.com/goodmath/2009/01/cryptographic_padding_in_rsa.php). 
(Uses GHC 7.0.4.)
Chris said in the video comments he doesn't intent it to be point and click (and I assume your comment is about moving the mouse and clicking being slow) but he wanted to show the editing and rendering of the AST.
To add to the other points already mentioned here, it could be a monad if the length was statically restricted.
It might also make editing code on an iPad or other touchscreen tablet more bearable.
Hi, Im the other co-founder of Scrive. Haskell has done magic for us and I think it could be more generally applicable. More startups deserve to find out about Haskell bootstrapping magic. We talk a lot about this when speaking on conferences: http://vimeo.com/22653093 Actually its a known concept. Some time ago I found an article by Paul Graham from 2001 ("A Lisp Startup") where he endorses Lisp in a similar way: http://www.paulgraham.com/avg.html Not only recruitment is better and easier. You also get more dedicated and happy developers.
hopefully when GHC 7.4 comes out there'll be a haskell platform - that's when i'll be bugging friends to try haskell
Ah yes, I thought your username sounded awfully similar to someone mentioned in the article. :] I expect that plenty of people here would be interested to hear more about your experiences doing a Haskell-based startup and how the choice to use Haskell has impacted the company, involving recruiting or anything else. Feel free to submit any links to what you've said on the matter (such as perhaps that video). Oh, and by the way, regarding this article--congratulations!
Why then and not now?
Will do!
Let's pick one then. I'm happy to work on any of them, I just want to make them easier to use/less verbose than writing OpenCL code in C. I'd love to hear what separates the different packages, and people's opinions on why we should chose one above the others. I'll admit that working with OpenCLRaw/Wrappers was not much fun, it seems the author has never heard of do notation, or case statements, choosing to instead use &gt;&gt;= explicitly and the maybe/either functions, leading to huge amounts of very difficult to maintain code imo. Also having used the code, I got the impression none of the authors had actually used it to make anything work, because I found some pretty glaring bugs, that would prohibit things like calls to clCreateContext which didn't have any properties specified (it passes a pointer to an array with a single NULL pointer, rather than just a NULL pointer, causing errors). Anyway, let's get together and make one true OpenCL package for Haskell that's easy and elegant to use. Edit: After trying to compile the OpenCL package on OS X, and having it fail because c2hs doesn't support Apple's blocks, I'm going to recommend against its use. It seems stupid to me to go with a package that can't even compile on the platform that first introduced OpenCL. Edit2: I also get the same problems with HOpenCL.
It might just be me, but last time i did a fresh install of ghc/cabal/cabal-install, right when ghc 7.2.2 came out, cabal and cabal-install couldn't resolve dependencies without me doing some serious tweaking.
Thanks! As I mention in the video: Haskell was the second best strategic choice ever made in our company after the core idea itself. I would love to share more of our experiences with the community. Ill post two links and then if people are interested to hear more I could ask Gracjan to share his thoughts from the CTO perspective. Here is my biz guy beginner perspective on Haskell. Going from terrified to blessed: http://bit.ly/vnXWaG Here is EmilyÂ´s experienced dev and beginner Haskeller perspective when giving a talk at SSWC in Sweden: http://bit.ly/uA4GZX
Cool! Fair warning, though, reddit's vaguely malevolent "spam" filter seems to have it in for you. If you submit something that doesn't seem to show up that's probably why, and I (or one of the other moderators) will rescue it when we notice. Try avoiding URL shorteners, it hates those for some reason, I had to save your comment from the spam filter just now.
Like the idea if it can be made mouse-free. 
Yes, but, why would I want to do that? :)
"right when X came out" - this is why the HP exists. So you don't have to chase bleeding edge things that haven't yet been tested in combination 
I think if you follow this idea through the obvious evolutions (don't prevent me from typing because that's user-hostile but instead highlight problems, etc), you end up with what the better IDEs do, which is continuously partially compile the source and (relatively) immediately highlight syntax problems, and feeding the results to the local Intellisense-equivalent to allow you to autocomplete syntax as well as just APIs. It's a good idea, and I'd say the only reason we don't see it implemented everywhere is merely that it is quite challenging to get all the pieces put together.
Yeah, which is analogous to the `Stream` &lt;-&gt; `Nat -&gt; a` isomorphism pcapriotti described. A list with a static length is isomorphic to `Fin n -&gt; a`, and the monad instance is the same.
Cabal is part of HP. Cabal is equivalent to Perl's CPAN or Ruby's RubyGems; Cabal helps you install, update, and publish Haskell libraries.
ok, so how do I use the haskell platform to install libraries, without using cabal? Do I need to find them and build them manually?
Wait. In order to install Haskell libraries, you should use Cabal. How to do it: 1. Find packages on the big [Hackage package list](http://hackage.haskell.org/packages/pkg-list.html). 1. In a terminal, enter `cabal install YOUR-PACKAGE-NAME-HERE`.
If I had a goatee, I would be stroking it malevolently right now.
Ok rereading this thread, I wasn't thinking straight. edit: But still it would be nice to have the HP be on the same GHC as most people writing haskell are using. Is 7.2 considered bleeding edge right now?
[Erik Naggum's opinion on structure editors](http://www.xach.com/naggum/articles/3064225496493947@naggum.no.html)
You've confused me a little. My question was about the problems I'm having with cabal, and your first response seemed to suggest I should use something else in the platform.
Start from scratch: Uninstall what you have and install [Haskell Platform](http://hackage.haskell.org/platform/), which includes: * GHC (the compiler) * GHCi (the interpreter) * Cabal (the package manager) Then search for particular packages you want to install by browsing [Hackage](http://hackage.haskell.org/packages/pkg-list.html). Then install them one by one with the command `cabal install `SOMEPACKAGE`.
My final thought on the tutorial: How can I make it more _evil_?
I'm using OpenCL on OS X. I have a github fork of it that builds cleanly, but I can't get the test suite to build on both Mac and other OSes. I've filed a cabal-install bug report on its trac, but we will probably have to come up with a workaround. https://github.com/acowley/OpenCL
A real-world benchmark of ghc and g++. The source code is available on [GitHub](https://github.com/mpacula/Collocations-Benchmark).
Monads are like nuclear waste filled burritos which we must handle while wearing space suits.
I'm pretty amazed how much stuff is packed into 37 minutes :)
All of the function prototypes, numerical constants and enums used in OpenCL are indeed defined in the standard. In theory you should be safe to use the Khronos headers.
existentials together with unsafeCoerce can be pretty evil
These things aren't so well-defined, but I would say that its bleeding edge phase just ended with the release of HP, pretty much. I'm writing code using 7.2.2 now. (Installed it about a week ago.) The way it tends to work is that a new GHC will come out, but not all the library authors will have updated their dependencies and ensured that they work with the new version for a bit. By the time HP comes out, you can at least be confident that a reasonable set of libraries has been updated and things should be reasonably usable without too much extra fiddling with packages by hand. If you have a bit of experience using Haskell and know how .cabal files work it's not so much trouble updating GHC the moment it comes out, but you have to be prepared for the potential of a little extra hassle patching things here and there.
Your memoization example isn't memoization at all. You're just defining a new local function which shadows the old one. You can see this by changing the call to, say `fib 29`: fib :: Int -&gt; Int fib 0 = 0 fib 1 = 1 fib n = fib (n - 1) + fib (n - 2) main :: IO () main = let fib 30 = 832040 in do putStrLn (show (fib 29)) If you run this, you get an exception because the `fib` that's in scope is only defined for `30`: *** Exception: Fib.hs:7:11-25: Non-exhaustive patterns in function fib 
Does anyone know if there is any source code released with this video?
[ITA software](http://itasoftware.com) is a Lisp startup that was recently purchased by Google for about $700 million.
Personally I think it's a really good idea. This sort of thing has been tried before without much success but perhaps since haskell has type inference and purity, this might be worthwhile with haskell. I do have some concerns though. First of all, some editing tasks will be hard, for instance, one might want to copy something that is not syntactically correct to then fix it, like if I have this: f x = x + 3 and I want to reuse x + 3 in a new function, but not all of f, so I copy x+3 to a blank line, but that is not syntactically correct. I might also want to copy a AST node to someplace where it doesn't type check and then fix the type error. Maybe it would be nice to be able to turn on and off? Turn it off, do some regular text editing, and then when the syntax is correct and it type checks I can turn it on again. Maybe one could also just turn it off for just one node... Secondly, the only representation that is saved should be the textual source code. Nobody wants to edit source code like its a word document. 
Nobody in the real world uses strings in Haskell for anything that might ever be performance-sensitive. You've got all kinds of stuff going on in Haskell strings that the C strings don't have, it's hardly a fair comparison. A ByteString would be a much closer match to a C string.
I had success with the following patch: http://hpaste.org/55423
Well, you could misuse `do` more.
I third the strong suggestion of `ByteStrings`. Furthermore, I seem to remember that Data.HashTable from `base` is notoriously slow; you could try [Gregory Collins's `hashtables`](http://hackage.haskell.org/package/hashtables).
You link to LYaHfGG. For Great *Good*? Really?
Please use `diff -u` when making patches! This patch has no context lines and so could easily misapply in the presence of minor nearby changes.
&gt; Let's pick one then. I'm happy to work on any of them, I just want to make them easier to use/less verbose than writing OpenCL code in C. &gt; &gt; I'd love to hear what separates the different packages, and people's opinions on why we should chose one above the others. I would also be just fine with working on another package. I think we should develop a single low-level package and let the differentiations be in the higher level libraries building on top of that one. Here is a comparison of hopencl, OpenCLWrappers and OpenCL: * **Naming conventions**: In OpenCL and OpenCLWrappers, the naming conventions is the same as in the C interface, that is they use "clCreateCommandQueue" where I use "createCommandQueue" in hopencl, and "CL_BUILD_PROGRAM_FAILURE" where I use "BuildProgramFailure". There was some critique of my choice at Haskell-Cafe. I also have names such as mallocArray in hopencl, which are completely unrelated to OpenCL, and I agree that this is probably a bad idea. * **Build-dependencies**: OpenCL and hopencl depends on c2hs, which seems to have some problems. For hopencl I need a pre-build script to execute c2hs manually, as the dependency checker in cabal seems to fail. We might be better of with hsc2hs, which was actually what I started with. In addition OpenCL depends on mtl-2.0, I don't know exactly why that is necessary. My package in addition depends on OpenCL being installed on the system, which seems to be a problem as well. * **Coverage**: OpenCL and hopencl seems to be covering much of the same functionality, where as OpenCLWrappers seems to include a bit more, for instance Samplers. It should be fairly easy to extend either OpenCL and hopencl to cover the remaining sections of the standard, I just think we haven't had the use for those sections ourselves and have postponed them for that reason. * **ForeignPtr's**: In hopencl everything but memory objects are managed using ForeignPtr's which means that deallocation is done automatically by Haskell garbage collector. For memory objects you need the extra control. It seems like OpenCLWrappers and OpenCL requires you to call clReleaseContext etc. manually. * **Quality**: I have written a set of unit tests for hopencl, and used a large part of the package for implementing Accelerate, so I'm actually fairly confident that much is working. I have only seen smaller examples using the other packages, but I think OpenCL and hopencl are currently the packages that are most reliable. &gt; Edit2: I also get the same problems with HOpenCL. We actually got it running on Manuel Chakravartys MacBook earlier in december (c2hs, Haskell FFI and Accelerate author). The following section from the .cabal did the trick for him, so it actually should work (I think he was running OS X Lion). if os(darwin) cc-options: "-U__BLOCKS__" frameworks: OpenCL 
Thanks, I will go to including the Khronos headers with hopencl then.
Touchscreen computers are -- for their small size and inexpensiveness -- going to be the first and possibly only computer for a lot of people. Failing to come up with ways to program on a out-of-the-box touchscreen computer without any peripherals will simply mean those people will never be able to get in touch with programming. Beside that, I believe a graphics-driven, touch-driven programming model has a lot of potential in expressing all facets of a programming project in an unified and extensible fashion. Not all code is text: you need interface design, you may deal with pictures, sound and video. Data structures often have complex relationships best represented with a graph.
Up until now Galois has been holding on to the funds from the Google Summer of Code for us and hosting various servers, but a.) this has been rather awkward and b.) prevented us from taking tax deductible donations or really doing much with the community infrastructure, since we don't want to burden them unduly. Joining SPI restricts us slightly more than, say, forming a 501(c)(3) ourselves with regards to providing resources for closed source content, but requires a great deal less management overhead. The restrictions aren't really considered to be all that big of a deal given the existence of the Industrial Haskell Group, which tosses around a fair bit more money and seems likely to remain free of any such restrictions. The IHG among other things funds development by Well Typed, and can provide any resources that might be needed by closed source projects that they feel benefit the community. If nothing else, it provides a clean separation of responsibilities in this regard by limiting our charter.
I think it's probably just demonstrating that *if* one were to memoize the function, its performance could be increased by the amount demonstrated. Either way, it is quite misleading.
I agree that it's unfair in that the haskell implementation is using less efficient data structures. However I would say that it is a fair comparison overall, since the c implementation was using "newbie level" elements of the language so it should be expected of the haskell version too. The reason haskell faired poorly here wasn't that the language is inferior, but that haskell tutorials don't impart enough knowledge on the performance implications of laziness and impurity (in my limited experience). Add to that a standard library implementation of hash map that I keep hearing performs badly and rampant overuse of lists instead of more appropriate arrays in tutorials (rightly so, as the interface for arrays is hideous and the simpler vectors are lacking in awareness among beginners) and you have a situation where a beginner in haskell has to be aware of more than a beginner on another language. That is the real unfair comparison I see.
I'm bugging friends now, and it's been extremely difficult to fight with cabal problems. EclipseFP doesn't really work. install of leksah complains it wants a newer GHC, and various other troubles...
RISK OF IRRELEVANT QUESTION BELOW: I'm quite new to Haskell, and while reading the code, a question entered my mind. In the first functions, you make extensive use of guards. Is this to be preferred to pattern matching? For example, is eea :: (Integral a) =&gt; a -&gt; a -&gt; (a, a, a) eea a b | b == 0 = (a, 1, 0) | otherwise = (d, t, s - q * t) where (q, r) = a `divMod` b (d, s, t) = eea b r preferred over eea :: (Integral a) =&gt; a -&gt; a -&gt; (a, a, a) eea a 0 = (a, 1, 0) eea a b = (d, t, s - q * t) where (q, r) = a `divMod` b (d, s, t) = eea b r
There's nothing fair about using useless data structures. It can demonstrate that absolute newcomer has to learn a little bit more to write effective code.
[This implementation](https://gist.github.com/1493594) which uses *Text* rather than *String* and a pure *HashMap* from the *unordered-containers* package is around 15% faster. There are also some other improvements that can be made: * *sentenceCounts* calculates the words in each sentence and context. If *pairs* would calculate all the words initially we can save some work. * The *CountTable* is currently represented as: type CountTable = H.HashMap T.Text Count And it is created as follows: countTable :: [Pair] -&gt; CountTable countTable = foldl' (\ct (w1, w2) -&gt; H.insertWith (+) (T.concat [w1, " ", w2]) 1 ct) H.empty The *T.concat [w1, " ", w2]* is unfortunate because it costs time and space to concatenate and it takes longer to hash because the key is longer. Maybe the following representation is faster: type CountTable = H.HashMap T.Text (H.HashMap T.Text Count) 
Thank you, this is exactly what I was getting at. Both the C++ and Haskell implementations could be improved. I was interested in how fast GHC could make naive Haskell code, without me having to sacrifice the niceness of Strings and lists or consciously thinking about performance. One of my favorite things about Haskell is that I can use highly abstract data structures and still get decent performance, which I did in my article. I do know that you can optimize Haskell code in a million different ways but then you give up a fair deal of that abstraction. To see what I mean, take a look at the programs at the [Computer Language Benchmarks Game](http://shootout.alioth.debian.org/u32/haskell.php): the fastest Haskell programs are rather ugly and look suspiciously like C. Their source is even longer than Java in many cases! That's why I wasn't interested in the "best case" performance but in the "default case" performance.
Sidechannel attacks for 100
That's just false. You're extrapolating that all fast Haskell programs have to look like ones on Language Shootout. There's a lot that can be gained from optimization at Haskell level without losing any abstraction.
The problem is that the default data structure (the one that's in the base package, in the Prelude, is called `String`, etc.) in Haskell is "useless". We need to get to the point where the correct data types are the standard ones. Python recently fixed a similar issue in the migration to version 3 (distinguishing between text and byte strings), and the Haskell community needs to do the same.
Right, but it is pretty safe to assume that the Haskell programs on Language Shootout are almost as fast as they get, i.e. ~2.5x slower than C. I know this is not completely an apples-to-apples comparison, but that's roughly within a factor of 2 of what I measured with my lousy Haskell code in that blog post. I think that's a testament to how good GHC is.
&gt; Maybe the following representation is faster: &gt; type CountTable = H.HashMap T.Text (H.HashMap T.Text Count) [This](https://gist.github.com/1493594/a477aaf758513c7dde4a2b2d242a4ebdeb9e8349) improves performance by 50%!
You should ask SPJ for the implementation of his `launchMissiles` function that causes "serious international side-eï¬€ects." It might make a nice supplement to your laser system.
What about H.Hashmap (T.Text, T.Text) Count? There's Hashable instance for that case. Anyway your code makes my point valid: C++ version takes 5.55 s to process whole input.txt Haskell double hashmap version takes 13.49 s x86_64 Linux GCC 4.6.1 GHC 7.2.1 AMD Phenom X4 945 Haskell version is at least as abstract as initial Haskell code (IMO more abstract and cleaner).
As an innocent bystander in this, I prefer pattern matching over guards, but the latter are more flexible if you need to actually perform some computation first. Clearly the OP prefers guards throughout.
That's great, thanks bas_van_dijk! :-) My only reservation is that Text, HashSet and HashMap are custom packages...
Haskell has truly proved to be a secret weapon in finding good people and I'd like to encourage other startups to use it to their advantage. Well, until everybody uses Haskell, that is :) Haskell serves as high-pass filter for motivation, math skill and inclination to learn programming craft. We are having great fun here while discovering endless amusement in functional programming in general and Haskell in particular.
&gt;These things aren't so well-defined, but I would say that its bleeding edge phase just ended with the release of HP, pretty much. Except... the HP is using 7.0.3. So if we say that the moment a GHC gets into the Platform is when "most people" can be expected to have it, then 7.2 isn't there yet.
Yes my program uses: [base](http://hackage.haskell.org/package/base), [text](http://hackage.haskell.org/package/text), [split](http://hackage.haskell.org/package/split), [hashable](http://hackage.haskell.org/package/hashable) and [unordered-containers](http://hackage.haskell.org/package/unordered-containers). But what's your reservation with that? All these packages are a *cabal install* away and are available as packages in a number of Linux distros. Also note that your C++ program uses the big [boost](http://www.boost.org) library. 
I tried to stick to what comes standard with ghc. Anyway, I love your code :-) I'm changing the C++ version to follow the same pattern, and will report back. Thanks!
Great! I would love to see the plots of the [new code](https://gist.github.com/1493594/82a4bc3ccd2bed941200a1c0838f3447c5c5cd80)...
What problems did you find with EclipseFP?
Hey, I'm having difficulties applying the patch. So I have in my folder hs-ffmpeg-patch (which contains the code in that hpaste), and old-hs-ffmpeg, the directory which contains all the code. I then use the command patch &lt; hs-ffmpeg-patch, but it gives me the error "patch: **** Only garbage was found in the patch input." Do you know how to fix that. Thanks for the help, btw!
A nice idea for a tutorial, perhaps, but unfortunately this one has a fairly bad error; it gives a misleading perception of how `let` works. `let 2 + 2 = 5 in 2 + 2` is not simply changing what `2 + 2` means, as one would expect from a term-rewriting language; it is redefining the function `(+)` locally. This misconception is repeated in the "memoisation" example mentioned in another comment, which it really just declares a local `fib` function that only works on one specific input. Even in a term-rewriting language, this would only memoise `fib` if the language was dynamically-scoped! That seems to have been removed from the tutorial, though.
Imho it definitely can be made mouse-free.
At Sensor Sense we recently hired a new Haskell developer to replace me because I'm moving to another Haskell startup. The recruitment process was exactly like described in the article: we posted our job ad to reddit, haskellers.com, cufp and the haskell mailinglists and got over 10 responses. We were impressed by the number and quality of the applications.
&gt; Secondly, the only representation that is saved should be the textual source code. Nobody wants to edit source code like its a word document. Actually, saving as AST could be very useful for teams and large companies. Currently when Alice renames function `foob` to `somethingMoreDescriptive` she has problems commiting her change to source control without breaking stuff because Bob already added another call to `foob`. With source saved as AST with GUIDs for stuff etc, this problem could be solved easily. 
Text is almost as unavoidable as ByteString; it's as standard as it can be, and it's in the [Haskell Platform](http://hackage.haskell.org/platform/), which is closer to a standard library than the `base` package; "what comes with GHC" is something the Haskell community is moving away from as a set of base libraries. `unordered-containers` is also a pretty popular package, with 29 [direct reverse dependencies](http://packdeps.haskellers.com/reverse/unordered-containers); among them, the popular aeson JSON library ([and thus](http://packdeps.haskellers.com/reverse/aeson) the criterion benchmark tool), Snap, and Yesod. It's true that GHC comes with a useful set of base libraries, but most of them are included because *[GHC uses them](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Libraries)*.
Hey, you seem like you understand diffs, so I'll ask you as well. I'm having difficulties applying the patch. So I have in my folder hs-ffmpeg-patch (which contains the code in that hpaste), and old-hs-ffmpeg, the directory which contains all the code. I then use the command patch &lt; hs-ffmpeg-patch, but it gives me the error "patch: **** Only garbage was found in the patch input." Do you know how to fix that. Thanks for the help, btw!
Or why not create a darcs patch directly: darcs get http://patch-tag.com/r/VasylPasternak/hs-ffmpeg cd hs-ffmpeg &lt;hack&gt; darcs record darcs send -o patch.dpatch And [create a ticket](http://code.google.com/p/hs-ffmpeg/issues/list) with the patch attached. 
Is there a summary someplace of the changes for this version? 
Hmm, well I've never actually tried applying a patch not generated as a unified diff before. Doesn't look like `patch` actually understands this format at all, at least I can't see a relevant switch. Suspect your easiest solution (absent tiph15 posting a more successful patch) is to apply the patch by hand, it's not too long.
Oh! Hah, somehow I missed that, apparently they've gone with 7.0.4 rather than 7.2.2. Well, I haven't run into any version-specific problems with 7.2.2 yet, but perhaps they figured it's still too new.
lol
`do` is unnecessary in each of the examples, but I figured newbies would want to add their own, custom print statements, and without `do`, they wouldn't know how.
Perhaps I should rewrite, or even remove those examples. I can get by in Haskell, but I'd consider myself a newbie (only use the IO monad, don't understand arrows). I wrote this in part to help me understand Haskell better.
Agreed. I should rewrite the example to use [true memoization](http://www.haskell.org/haskellwiki/Memoization).
`unsafePerformIO` mwuhahahaha Or [DDC](http://disciple.ouroborus.net/), an eager-evaluation Haskell clone. I'll never understand why one would want to omit lazy evaluation, it's an incredible feature.
As far as I know, DDC does not omit lazy evaluation, it is simply not the default. In practice, I would say that in 50% percent of time, I want lazy evaluation by default, the other 50%, strict by default. So basically what I want is to be able to switch the default on a module-level basis.
Is `seq` not enough?
No, it's very inconvenient. Also if you just forget to place a single seq somewhere, or some data structure is not fully strict, it will still blow up in your face.
Wait - your C++ version uses STL extensively. Why is it fair to use STL in the C++ version, but not the commonly used Text and collection types in Haskell?
Lazy evaluation is 1) useful 2) tricky. I wrote a little [genetic algorithms](https://github.com/mcandre/genetics) library and Haskell presented me with my first stack overflow. Monadic recursive calls are strangely lazily evaluated unless you use `seq` in just the right place.
Use lens: http://hackage.haskell.org/package/data-lens-2.0.2 http://hackage.haskell.org/package/data-lens-template It allows extracting part of record, changing it and combining it with another operation. It can be combined within StateT. data-lens-template Template Haskell code generates lens ("extractors") for every property that has name starting from "_" like _typeMap. That's just 1 way to do it, there might be others. For instance as far as I understand GHC doesn't generate new unique ids to things that have something unique about them by taking new value from the counter. It just reuses that value and ensures that different types of "uniquables" have separate ranges.
&gt;No, it's very inconvenient. Bang patterns are less so.
Ouch, why are you depending on `haskell98`? Your code isn't even valid Haskell 98, since it uses hierarchical modules. You should depend on `base` instead.
Thanks. What is the difference between [Data.Lens](http://hackage.haskell.org/package/data-lens-2.0.2) and [Data.Lenses](http://hackage.haskell.org/package/lenses-0.1.4)? I am reminded again how sorely hackage packages need to be extended with a description of what they do and examples. Just a bunch of operator definitions are not cutting it.
I recommend you to look at the haddock documents of fclabels package. http://hackage.haskell.org/package/fclabels It elaborates more on practical uses of lenses. 
This. Please.
It comes with its own scion version, which is for some reason not available from hackage (I had to explain it, as he simply thought "scion doesn't work with the GHC versionin the platform"). When it fails to install, there's no obvious recourse. Then, none of the obvious options (build, execute) are available, and it isn't clear why. Completions seem to work, and then they fail, again with no obvious reason. My friend said that if you try to install it on Windows, it's even worse, I'll ask him for details tomorrow.
Thanks. I imported haskell98 for something really basic like Maybe or fromJust, can't remember what. Replaced haskell98 with base and committed to GitHub.
So there are three different packages on Haskell that are all dealing with the same thing? I don't know with which one to go anymore.
yes, in this case, there are several packages which essentially does the same thing. This is now a general problem of haskell hackage since we have over 3000 packages and some definitely overlap with others. New hackage will have a feature to get rating/feedback from users so that one can determine which to use more easily. 
I used STL because it comes with pretty much any C++ compiler. I do understand your concern though: a lot of people consider Text etc. pretty much a part of the language. I am working on a revised version :-)
&gt; Surprise, language evolves! Thanks for the healthy dose of anti-prescriptionism. I was curious, so I checked the [*OED*](http://oed.com/view/Entry/13474) online. Here's a bit more evidence in defense of 'automatons'. + The *OED* lists (without comment or disctinction) both 'automata' and 'automatons' as plural inflections. + I was able to find four citations using 'automatons' in the entry's examples, the earliest from 1796, and one each from G. K. Chesterton and Nabokov. I doubt English has ever been very consistent in how it handles plurals of Latin or Greek nouns in -um/-on.
It does exist, but it's very uncommon when used as a computer science term: 22,600 hits site:ieeexplore.ieee.org "automaton" 676 hits site:ieeexplore.ieee.org "automatons" 68,700 hits site:ieeexplore.ieee.org "automata" 8,090 hits site:dl.acm.org "automatons" 739,000 hits site:dl.acm.org "automata" Interesting point on both sides, really.
Cool, looking forward to that. I think I will stick to Data.Lens as it is the most convenient to use together with StateT.
And this trend will continue: GHC will come with just what it needs because it uses them. Haskell Platform will be more equivalent to what a user gets when they install a C++ system. So, if you wanted a more apples-to-apples comparison, sticking to what is in Haskell Platform would be more appropriate. That said, it is hard to make it apples-to-apples: Even with Haskell Platform, the ease with which users can use cabal to get and use any of the packages on Hackage means that HP will tend to include less than a C++ development system which has no such facility. Commercial C++ systems include tons of libraries, and the C++ environment most of us use on Linux is augmented by tons of things considered standard in a Linux distro.
This is really good, do you know of any other lectures by him, or similar haskell tutorials? I really like it because he approaches things from top-down, going from the general principles into details. As opposed to the usual "lets start hacking code" approach taken by so many today.
Looked good to me as a great supplement to "Learn you haskell". Do you have any references to a better ones out there?
Use as-patterns and record wildcards, and use the record syntax on the existing record rather than the constructor: {-# LANGUAGE RecordWildCards #-} incrMemOffset = do st@EvalState{..} &lt;- get put st { memOffset = memOffset + 1 } Or without as-patterns or record wildcards: incrMemOffset = do st &lt;- get put st { memOffset = memOffset st + 1 }
[Overview of the different lens packages by the author of data-lens](http://stackoverflow.com/a/5769285) and some examples on [how to use the same](http://www.maztravel.com/haskell/lensExamples.html).
Apart from grammar discussions, this is a nice example. It would be cool to show a practical example. I know the article says "the efficient approach in most practical applications is to translate the NFA to an equivalent deterministic finite automaton," but nonetheless it would be cool to see it in action. Isn't this basically how Prolog works? What's the smallest Haskell implementation of Prolog?
Nah, English is pretty consistent in how it handles pluralizing loanwords. There's only one rule: Eventually, everything ends up treated like English plurals. The only thing that varies is how long it takes for the source language's plural form to vanish from use. :]
Thanks. I already stumbled upon both links during my research, though :)
Yes I think this is pretty much regarded as one of the best introduction to Monads: [You could have invented Monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) Then to place Monads in context of other important Typeclasses take a look at [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia#Monad)
Take a look at the reverse dependencies on packdeps if you want to pick the one that is most popular as a dependency. I can recommend fclabels.
Thanks. I decided to go with Data.Lens because of its convenient integration with the State monad. I now have code like this, for example: evalAST (AVarDecl varType varName) = do typeMap %= Map.insert varName varType which is pretty sweet. 
[Whenever a word from a foreign language enters English it becomes an English word, and gets an English plural.](http://www.youtube.com/watch?v=wFyY2mK8pxk#t=78s)
Ultimately for something like this I use `ghc-mod` which basically couples Emacs flymake + a helper program. It's quite convenient for a lot of Haskell code, and does increase my productivity IMO, but it doesn't work for every use case.
 f :: IO () Ok.
Saw 11 comments, clicked, saw 8 comments about the correct singular form of the very interesting thing I wanted to see 11 comments about. :-)
Fixed, thanks! My excuse is that English isn't my native language and all my CS education was in Finnish. :)
That's an endomorphism on universes, not the universe itself.
Oh, true, but the implementation details are all already in the RTS. *Handwave*
Yes, it is useful. And I love it. It's just not *always* the best default. Did try to work with files of size of tens of gigabytes with Haskell? That is one situation when strict by default is the correct choice.
Bang patterns are better, still, optional strictness is not modular. You have to forget only a single strictness annotation in a big program to make it blow up.
Note: Text comes with the language ;)
Or even just a `Map`.
Interesting post!
Learn record update syntax: compile ast = do e &lt;- get put e{label=newValue}
Did you see the suggestions by donri and Flarelocke. They are much simpler. 
The [fclabels package](http://hackage.haskell.org/package/fclabels) has a function to conveniently update parts of a record in the state monad: [modify](http://hackage.haskell.org/packages/archive/fclabels/1.0.4/doc/html/Data-Label-PureM.html#v:modify). To use it, you first need to derive the labels for your record. You can do this using template haskell, with the function [mkLabels](http://hackage.haskell.org/packages/archive/fclabels/1.0.4/doc/html/Data-Label.html#v:mkLabels): $(mkLabels [''YourRecordType]) This will create a label named after each record field. If the field starts with an underscore, the label name will be the same except without the underscore (_field -&gt; field). If the field doesn't start with an underscore, the label name will be the field name, capitalized and prefixed with the letter 'l' (field -&gt; lField).
Thank you very much. How could I miss the the State monad support of fclabels? I will look into it.
Why simpler? They are indeed simple, but a lot more verbose than the example I showed above.
The OED emphatically does make a distinction in this case; the CS use is put under a separate heading and it is noted in advance that they use -a. This is not really a case where there need be a war between prescriptivism and non-prescriptivism, the question is semantic. it is plain that greek neuter nouns ending in -on are sometimes pluralized with -ons and sometimes with -a, and every English speaker is aware of this, mostly unconsciously, we don't notice it because it usually has semantic content. Where is does, pretending to indifference between them destroys the expressiveness of the language. I don't know if one can say anything too general about pluralization of -on words, but the examples in the OED (which admittedly I wouldn't trust as far as I could throw it, which isn't far) suggest that the cases for 'automaton' are following a familiar law of the English language where it receives foreign words, which it is its peculiar genius to do with amazing uh, sluttishness, since it has been a hermaphrodite Germanic-Romantic language for 1000 years. The rule is something like this, where there is an opposition between abstract and concrete, one prefers the germanic-tuetonic for the concrete things, and the foreign, especially romance-latin for abstract things; where there is some contrast of low and high, rustic and sophisticated, we use germanic-teutonic for rustic things, romance/Latin for more exalted things. Thus worth is Germanic, value is Latin; craft Germanic, art Latin; teenager Germanic, adolescent Latin; work Germanic, labor Latin; wage Germanic, salary Latin; strength Germanic, power Latin; etc etc. It's a bit complicated but it is all-pervasive and is unconsciously present in all native speakers; it was not invented by dictionary writers but is what we do and how we communicate. In the present case we have a choice between a native English plural and a foreign Greek plural, both being familiar to all speakers. The consequent rule would be something like this, that we use the -ons plural for actual concrete individual things -- e.g. 'automatons' for robots, or (as is done with many of the old uses of automatons listed in the OED), animals or human beings when viewed *a la Descartes* as mere machines; the plural in -a is clearly required where 'automaton' pertains to something abstract as in NFA. Similarly things like lepton are pluralized leptons because they are concrete individual existing things, not abstractions. A nice case of a double reception of the same word with different endings is 'criticize' which came over the centuries to mean 'run down' -- in face of this people began, against the stupid complaints of prescriptivists, to use 'critique' as a verb -- meaning, to subject to judgment, evaluation etc, irrespective of the outcome as good or bad, which is what criticism had formerly been, cp 'literary criticism', 'music critic' etc. which retain the older meaning 'criticize' lost. So this was just a popular effort to re-acquire the semantics that formerly belonged to 'criticize'. The result is that we have two verbs based on Greek kritein (judge), and the language is more expressive for this reason. Similarly we have two plurals for the Greek import 'automaton'. Using automatons for NFA's would be destructive of expressiveness. Thus, to get them both in one sentence, suppose the automata you define in your Haskell module can be implemented in some sort of actual machine; we could say, faced with several such devices, 'these automatons implement subtly different automata' -- which has an intuitive rightness that I think `TTamm` was feeling, there's no reason to think he was really on a prescriptivist bandwagon, though he may have thought he was.
Are you sure this prevents side-channel attacks? I'm not saying it doesn't, but in Haskell you've got lazy evaluation, as well as not having guarantees on order of evaluation, both of which could make seemingly side-channel attack resistant code not so.
Could you please elaborate what NFA -&gt; DFA conversion has to do with Prolog? (One is a method for making deterministic recognizers for regular languages, the other a logic programming language.)
***:O*** Very excited to start using this; I'm going to try this on some projects now. Thanks big time to Neil for releasing it, I understand it took some effort to get it open sourced! Thanks for posting, I wouldn't have been aware of it otherwise.
Or HashMap (http://hackage.haskell.org/package/unordered-containers)
Thanks, I indeed forgot about that. Now I have fclabels working, works well so far.
Just that Prolog is a non-deterministic language. The article is about executing non-deterministic machines.
What you're getting at here is not so much a matter of absolute correctness or prescriptivism, but the subtle and implicit connotations of word use that indicate *fluency*, particularly fluency within the dialect of a socially defined (rather than geographically defined) group. Using "automatons" in a CS context may not be outright *wrong*, but it does invoke undesired connotations and marks the speaker as possibly being an outsider. Someone very sensitive to such minor missteps could probably have used that, combined with the fact that shangas clearly knows the material otherwise, and made an educated guess that he was not a native speaker. :]
https://github.com/chrisdone/structural
&gt; However, its choice of internal representation makes me throw up in my mouth a little bit. &gt; &gt; The type T it uses to represent a lens is internally defined as &gt; &gt; newtype T r a = Cons { decons: a -&gt; r -&gt; (a, r) } &gt; &gt; Consequently, in order to get the value of a lens, you must submit an undefined value for the 'a' argument! This strikes me as an incredibly ugly and ad hoc implementation. Bah. Anything that stores separate get and set functions internally requires deconstruction twice for modify. This sort of "ugly" internal representation avoids that, and it's *internal*, so you never have to see it.
 evalAST (AVarDecl varType varName) = do (mapLens varName . typeMap) ~= Just varType is sweeter. Edit: or evalAST (AVarDecl varType varName) = do (typeMap &gt;&gt;&gt; mapLens varName) ~= Just varType P. S. edwardk's `data-lens` is the best package IMHO. 
edwardk's `Data.Lens` representation does not have separate `get` and `set` functions. It uses a coalgebra for the store comonad instead. i.e. `r -&gt; (a, a -&gt; r)` suffices.
First, the article describes a finite-state recognizer (which, as the name says only recognizes strings, it does not have output). Any non-deterministic finite state recognizer can be transformed in a deterministic finite-state recognizer that recognizes the same language. So, I am assuming that you are talking about finite state transducers, which have output and cannot always be made deterministic. A finite-state transducer is a restricted turing machine, while pure Prolog is turing complete. Ergo, it is not possible to transform a pure Prolog program to a finite state transducer. An efficient machine for executing Prolog is Warren's abstract machine: http://wambook.sourceforge.net/wambook.pdf
Great! I am wondering about an example use function (when I get time I'll try writing it): * Automatic dependency scanning of C files, with correct handling of auto-generated code * Including unbounded rescans of auto-generated code that uses #includes itself
It's unclear if what they suggest is simpler or not. But certainly once you get to sophisticated nested record structures for the state, lenses are the way to go for accessing fields.
Sorry I didn't notice darcs produces this patch format by default. So I converted it here: http://hpaste.org/55462 You can apply the patch with a command like patch -p1 &lt;file.patch
Where is the definition of &gt;&gt;&gt;? I am not completely happy with this, though. You have to use Just even if inserting into a map does not require it. This stems from the type of the lens, which requires that getting and setting must be the same, but Map.insert takes a value that is of a different type than what Map.lookup returns. The author sidestepped this by simply defining that setting through mapLens either inserts the value (Just key) or deletes the key if Nothing is passed. Elegant on some level, but a compromise on another.
Simpler for the simple cases, perhaps. Lenses are composable, which is very useful with deeply nested structures. This fclabels code: set (city . place) "Amsterdam" would with record syntax become something like \r -&gt; r { place = place r { city = "Amsterdam" } } Lenses are "first class" rather than syntax.
[(&gt;&gt;&gt;)](http://hackage.haskell.org/packages/archive/base/4.4.1.0/doc/html/Control-Category.html#v:-62--62--62-): it is just `(flip (.))`. The idea isn't to think of inserting into a map, rather consider a map as a function from `Key` to `Maybe Value` (with a side condition that all but finitely many outputs must be `Nothing`). `(flip lookup)` provides this interpretation. The `mapLens` provides access to each `Key` allowing you to get and set its associated value. As usual in Haskell, we are nudged into to think denotationally (ie thinking of a map as representing a function) rather than operationally (ie inserts and deletes). But if you prefer your `Map.insert` code, I won't argue too strongly against it.
What advantage is there to doing it the first way?
Perhaps it is spelled "automatons" but pronouced as **/É”ËËˆtÉ’mÉ™tÉ™/**. 
&gt; The idea isn't to think of inserting into a map, rather consider a map as a function from Key to Maybe Value (with a side condition that all but finitely many outputs must be Nothing). (flip lookup) provides this interpretation. I agree and realize that. &gt; The mapLens provides access to each Key allowing you to get and set its associated value. For getting the value, *Maybe Value* is a suitable type, which is why this is the type of (lookup key map). But for the other way, it does not make sense to make the type of the value you want to insert be *Maybe Value* (which is why the type of the value in (Map.insert value key) is not *Maybe Value* but just *Value*). With mapLens however, even in setting the value, the type is *Maybe Value*, which is what I am not too happy about, for the reasons I outlined in my previous post.
&gt;around 1982, I attended a seminar on syntex-sensitive editing in Pascal at the U of Oslo. editing the structure of the program was the Hot Thing back then Does anyone know where it all went? It seriously feels like we're reinventing wheel at times. Supposedly there's a good reason why this trend fell into oblivion, but I would be interested to learn more details about its plight.
Getting to just write ``memOffset`` instead of ``memOffset st`` and being able to pattern match against the value constructor, in case there's more than one.
I didn't get it open sourced, instead I rewrote it from scratch in my spare time... Consider this a preview release, I expect to develop it further in the coming weeks. In particular this version hasn't been used in anger in any big projects.
See Examples.Self, which does roughly the same trick, but for Haskell with imports rather than C with includes. The secret is to have Foo.c.dep/Foo.h.dep, which is the 1-step dependencies of the file, and Foo.c.deps which is the N-step dependencies of the file. You get the unbounded rescans and regeneration of everything for free - if you #include a generated file, it just pops into being at the right point. If you change a file, the minimal but correct set of things rebuild.
This version of Shake is not the library in use at Standard Chartered (closed-shake), and is also different from the one Max worked on (open-shake). It is a from scratch implementation, which has not been tested to any significant extent - it can build itself but that's all the testing I've done. It's based on new and simpler theory, which eliminates the entire idea of an Oracle, and generalises rules, to give more simplicity and expressive power. I'd describe it as a nice improvement over the other two shake's. There are two demos at http://community.haskell.org/~ndm/darcs/shake/Examples/Self/Main.hs and http://community.haskell.org/~ndm/darcs/shake/Examples/Tar/Main.hs , which should be a good starting point. I'm still working on the implementation, so things may change (although I suspect nothing too fundamental), and I'm also working on a paper which will serve as a good source of documentation. I'll be writing a blog post when I think it's ready for people to actually try, but I'm not going to stop anyone giving it a go :)
Man, I can't wait to try to convert parts of our (horrible) build system to shake :)
Thanks for the link.