Thanks for the details, I'm able to reproduce this. I've opened up an issue against the http-client tracker about it: https://github.com/snoyberg/http-client/issues/9.
Good to hear, thanks!
Scotty looks pretty nice and it will probably be what I use to do my next project; but what I really want in a web framework is a more rigid system that forces developers to define their requests and responses up front. The idea is to force users to define their business models up front so that http considerations don't leak into the actual business logic of the application. I haven't really thought through the full API I'd like to see but it would end up being something along the lines of: class RequestModel a where parseRequest :: Request -&gt; a class ResponseModel a where sendResponse :: a -&gt; Response get :: (RequestModel a, ResponseModel b) =&gt; Text -&gt; (m a -&gt; m b) -&gt; Route get route = ...impl... post :: ...impl... data RegisterReq = RegisterReq { email :: Text } data RegisterResp = RegisterResp { error :: Maybe Text } instance RequestModel RegisterReq where parseRequest req = RegisterReq $ params "email" req instance ResponseModel RegisterResp where sendResponse (RegisterResp (Just err)) = do json $ object [ "error" .= errorMessage ] status internalServerError500 sendResponse _ = json $ object [ "ok" .= ("ok" :: String) ] myRoute = post "/register" (\req -&gt; return $ RegisterResp Nothing) This was just a quick writeup and it could all be prettied up with some applicatives and monads but I think it would really help the quality of a lot of the web projects I see out there to have this separation (not necessarily in haskell land). I don't really think the above example is a good demonstration for why it's a good idea as it really only becomes useful once the complexity of the app grows.
You can definitely do that, but ultimately they still need to be in a top level splice in your HeistConfig, so you still need to account for possible duplicates there (i.e. even if you manage to put 99% of nodes into a compiled splice used by other splices, you can't escape eventually having to add one or more nodes to HeistConfig in the same "namespace"). Faucelme's suggestion is one way you can manage the problem. Another is to use #! when creating splices instead of ##, as it will throw an error on duplicates. I just added a footnote about that combinator per mightybyte's suggestion (but note that it's still a runtime error).
Elaborating on my comment about type systems catching errors, I use type systems to design my data structures. When I develop functions that consume my designed datastructure, the type system forces me (or in the case of Haskell, glares at me) to consider all corners of my input. It is this consideration of all inputs which helps prevent me from making incorrect assumptions about my program behaviour which in turns prevents errors. I certainly will concede that the term "catching errors" evokes an image of me writing a program and then some sort of typey net sieving the program to determine if it is okay or not, which is manifestly *not* how I use the type system. So I apologize for using that phrasing. I use types as a design tool to help prevent the generation of improperly behaving programs in the first place. Indeed the strongest benefit I receive from type systems is the ability to make sure that all patterns in a match cover all inputs, and this benefit isn't even strictly enforced by Haskell's type checker.
A pain? It's pretty much the same as any other compiled language? 1) install nginx/apache 2) upload app, run as daemon, proxy requests to app 3) ??? 4) profit 
Also, for the game of life example, you can define other coactions for other topologies, and take their coproduct to make them changeable at runtime.
If anyone's looking for a more complete guide, I had written [this tutorial](http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html) a while back.
I should have said: a pain to deploy to free hosting environments for someone relatively inexperienced at server setup... which describes me and probably many others who are nevertheless interested in coding a website.
If I'm not mistaken (and I may be because I am on a train and do not have paper to write down the dualization and verify it atm), fancy people would call this a [right comodule over a comonad](http://ncatlab.org/nlab/show/module+over+a+monad).
If you're referring to `authenticate-oauth` then I can list out a few reasons why I didn't use it. First, I'd like oauthenticated to take on as few dependencies as possible—it's not successful there yet mostly due to the `Network.OAuth.Simple` module. This is why I was excited to implement it atop the `http-client` architecture as it's now a bit less opinionated in it's dependencies. I'm also a little hesitant to include the entire RSA framework as, at least in my experience, HMACSHA1 is the most important signature method to implement. Second, I needed to take advantage of some ambiguity in the OAuth spec which allows the OAuth parameters to be passed in the authorization header, the query string, or the entity body. This is configurable in `oauthenticated` with the `ParameterMethod` type. Third, I didn't want to force an exception handling/bracketing methodology onto users. I'm personally a bigger fan of `exceptions` than `monad-control`, but I feel like your OAuth client lib shouldn't be making that choice for you. I really appreciate how `http-client` leaves this up in the air and `oauthenticated` does not generate any of its own exceptions. Finally, for my own purposes I spend a lot of time generating new tokens and credentials and so I wanted a slightly more typesafe method of handling them. `oauthenticated` uses a phantom type scheme to differentiate between `Client`, `Temporary`, and `Permanent` `Token`s and `Cred`entials. This has already given me a lot of pushback in my own code preventing some errors. `oauthenticated` is almost certainly a little bit of NIH—always a hard tendency to avoid—and it may have been possible to get `authenticate-oauth` to do what I needed, but I feel like OAuth will be a major component (read: pain in the ass) for the current project I'm undertaking so I wanted to get the points above smoothed out. Given the serendipitous timing of `http-client` filling the role of bare-bones HTTP client that I was looking for, I felt like it was worth the few days it took to build `oauthenticated`.
Neat!
whats new in this version?
great tutorial
CUDA 5.5 and OS X Maverick
&gt; this release includes OpenSSL support both via the pure-Haskell `tls` package and the OpenSSL library fyi, the pure `tls` implementation should benefit significantly from GHC 7.8's newly exposed GMP crypto-related primitives thanks to [this](https://github.com/vincenthz/hs-crypto-numbers/commit/58317833e1b3a35eb203513d2799390cb1c459f0) and [that](https://github.com/vincenthz/hs-crypto-numbers/commit/1a1da584c026fc261f59be53f985520fd0fcb57e)
btw, I see only CUDA mentioned... what does this mean in terms of using `Accelerate` "out of the box" on Linux for those of us who prefer to avoid buying NVidia GPUs in favor of the more openly documented Intel and AMD GPUs?
You're stuck with the interpreter. I'd really like to see more 1st class supported backends for Accelerate. A high performance CPU backend would certainly be welcome for a lot of us - along with OpenCL for non-nvidia graphics.
oh, i agree with your criticism. i did not like the big dependency footprint as well and thought about porting it to http-client myself. the not so tight types did bite me in my (to be released) discogs library at first as well, so i agree with your design choice here. thank you for releasing it. a question about you not wanting to handle pvp before 1.0, do you expect many breaking changes?
Or just use Haskell and not bother with an insane language. What’s next? Haskell in PHP? Ok, at least PHP got a garbage collector. All C++ got are “smart pointers”… more than a dozen incompatible implementations… which are actually *slower* than modern GCs, but at least they are not “evil” “slow” GCs, right? C++ … I’m not touching that nightmare fuel. **I’m more of a: “Have low level functionality in Haskell, and be done with it.” type.** I want my kernel to be Haskell. Hell, I want my firmware to be Haskell!
You may be interested in [MFlow](http://mflowdemo.herokuapp.com/) for web programming without caring about low level details. The framework is in charge of back button management, routing, Ajax etc.
How would the compile times of programs written in such a language compare to GHC?
Why would C++ want a garbage collector? You misunderstand the purpose of C++. 
I'd say that the entire Network.OAuth.Simple module is experimental and subject to change—I feel it's almost out of scope for the library. I also have some amount of concern around the Network.OAuth.Types.Credentials module. I could probably release PVP, and probably should, really, but wanted to leave that warning since I didn't yet have any outside validation on my design choices. That all said, I'll move it to PVP very soon.
I created a [github repo](https://github.com/pfcuttle/twentyfour-2013) with all examples from this year's series. The scotty article was a little bit difficult to get running because I thought there were no source files (no link at the bottom), and because I ran through weird compilation errors.
Meta questions: Does "24 Days of Hackage" only cover libraries, or applications as well? BTW, great work giving us samples from hackage. I like it a lot.
Most modern uses of C++ are real time (games) and most GCs suck at predictability. Yes, it's a horrible language, but there are few alternatives and Haskell isn't one of them. Rust might be.
The original plan was to blog about things you can `cabal install` from Hackage. I don't suppose there's much reason it can't be applications, but I was more interested in blogging about libraries, as to the Haskell developer, they tend to be the more interesting parts of Hackage. Glad to hear you're enjoying it :)
Yeah, no. modern standard C++ has only 2 pointer types that when used together can be, in some cases, compared to garbage collection: the shared_ptr and the weak_ptr. Other than that, there are the normal *, and unique_ptr. A c++ programmer would only use shared_ptr in a select few situations, if you find yourself using it a lot, you're doing c++ wrong. If you can, then by all means avoid c++ as the plague, it has warts the size of Mount Everest. But if you want predictable performance characteristics, you can't (yet) get around using c++ (and that will also involve avoiding shared_ptrs if you do). 
Looks good, consider running a quick ``cabal init`` on each of the subdirectories so we get a snapshot of the libraries that you got each of them working with.
Thanks for the nice write-up! I couldn't agree with your last paragraph more... my goal for Scotty is to connect pure Haskell to URIs in the most simple and unsurprising way possible. To keep up with WAI 2.0, I released a new Scotty this morning... I tried the code in your post with it, and things worked for me, but someone let me know if they have an app that blows up. Issues can be reported [here](https://github.com/scotty-web/scotty/issues). Thanks again!
Lovely stuff, I've added a link. Thanks!
Sweet! I'll link to this from the calendar page.
This one might be a little on the weak side as I'm still very new to these libraries! Let me know if I'm not being idiomatic or have made typos, etc. Glad to hear everyone is diggin' the posts.
On a related note, what's the story with DB migrations for persistent these days? Last time I looked, the persistent auto migration stuff didn't seem suitable for production environments where you might need to roll back or make less trivial changes than adding new columns and tables.
Another would be able to give a more complete answer, but there are all kind of knobs in the quasiquoter that you can use to fit your schema. Documentation in the relevant haddocks would be helpful though ...
I thought they you could like: https://github.com/yesodweb/yesod/wiki/RawSQL Is this the type of access you wanted?
The persistent auto migration is intended to handle safe migrations, where there's no possibility of data loss. This is actually quite a nice property for a production environment - as it can really save you when things do go wrong. I believe that the recommended approach to this, which has worked quite well at FP Complete, is to have a table in your database storing which migrations have been performed. Then, on startup, you check this table before doing a given migration. Some of these migrations can be performed before the automatic migration, and use raw sql execution - this way you can delete columns and tables. Another approach that works quite well for large migrations is to temporarily leave in the old tables / fields. Then, after the persistent migration you use normal persistent functions to migrate the data. At some later date (after all your databases have migrated), you can remove these extra tables / fields. All of this migration stuff, including the automatic persistent migration, can be run in a single transaction, so that if anything goes wrong, your database goes back to a consistent state. There has been some discussion of improving the builtin support for migrations: https://groups.google.com/forum/#!searchin/yesodweb/migration$20plus/yesodweb/_m6AIbZeaFg/ZAOTxueocBwJ But any such solution here is going to be complicated, perhaps more complicated than just writing a migration - there's no way for persistent to automatically divine what you intend to happen.
Hey look, a monad tutorial! Just what we needed.
Yep! I'm aware of the stereotype :) In all seriousness, I've never seen monads explained this way, if someone can find me a tutorial that explains them in a similar way, I'd love to read it!
Well your post doesn't explain monads, so there's that.
I understand monads, but apparently I don't understand middle school math. It's been a long time, but I don't think any of the monad tutorials really helped me. It eventually clicked after I'd been writing programs that used them for a while. You don't really have to understand *monads-as-a-concept* to use them in Haskell programs, although it obviously helps if you're doing advanced/production stuff. You just have to understand the semantics of the particular monad you're using.
I think the "original sin" of a lot of monad tutorials is the assumption that describing instances of monads can lead to insight about the general monad class, which in my opinion isn't the case. The monad definition itself is pretty trivial (only two functions: ``(&gt;&gt;=)`` and ``return``) but the real insight comes from understanding the monad laws.
I think this is a great point. I don't code Haskell in my day to day life, but I do study Haskell to try to find ways to write better code elsewhere. Monad tutorials could probably be put into two camps: tutorials for people learning Haskell, and tutorials for people learning the concepts behind Haskell that could be applied elsewhere. This post was definitely an attempt at the latter, however superficial it is.
Fair enough, I don't really have the background to speak properly to the underlying category theory. I guess a more accurate title would be something akin to "I will attempt to introduce some motivating scenarios for Monads in a way that doesn't require that you are already adept at Haskell." Which really isn't a fit for this subreddit now that I think about it. At any rate, appreciate the response.
I don't really have anything useful to say in reply to your response, but I do want to say this: I don't think you needed to delete your post. If no one else has said so: thanks for putting in the time! It's obvious you spent quite a bit of effort on it, and your motivations are good. Oh, and: You said you didn't quite get monads. Do you "get" types like Maybe? If so, that could be a path to full understanding. If you think of Maybe as a context, the two main monad operations are: 1. Put something into the context. This is **return** — `return :: a -&gt; ctx a` 2. Take something out of the context and apply a function to it. That function returns something in the same context, but not necessarily of the same type. This is **bind** or **&gt;&gt;=** — `bind :: (cxt a) -&gt; (a -&gt; ctx b) -&gt; ctx b` Specific to Maybe, the types would be 1. `return :: a -&gt; Maybe a` 2. `bind :: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b` It should be really simple to imagine how you'd write **return**, and you can probably write your own Maybe monad **bind** with the normal semantics of a Nothing at any step resulting in a Nothing for the whole computation. Don't know if this helps you at all, but if it raises any questions please feel free to ask.
I think [Dan Piponi's monad tutorial](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) is required reading for anybody considering writing a monad tutorial.
I have a query in a web app that uses a hilariously complex query using several subqueries and postgres-specific features, saved in a .sql file, just using persistent. It is definitely possible to do raw SQL. It's not even remotely difficult, and it marshals the parameters and data in and out as usual.
The variadic function example is nice -- I'd never thought of using dependent types like that.
I'm posting this because while begriffs just recently posted the source to his new, improved take on the Heroku buildpack, the goals here mirrored what I've been able to achieve using it. I've been selling Haskell at my company for a while now, completing a fair set of smaller projects with it, but I wanted to make the move to having more of our web application built using it. Being able to launch a web app on Heroku in just a few minutes time is a *really compelling demo* that opens the door to talking more deeply about what Haskell can do. It's not even a real selling point of Haskell at all. Lots of other languages can run a web app on Heroku and nothing that Haskell does changes that fact too much. It's just a great conversation starter. I'd love to see other collections of quick wins with Haskell. Threepenny GUI comes to mind as another one I think could be very successful, as does Diagrams.
awesome :) 
I recently had the experience of Haskell exceding the quickness of Rails/Node when I tried FPcomplete's Haskell Center. No more installing of packages, or commandline-fu to get the beast going. Just Haskell with everything I needed to get a basic app up in the cloud, stright from my browser! It's commercial, yes, but so are Heroku+Cloud9+Github. You can always reuse your trusted Emacs/CLI/UN*X skill to break out of the SaaS-cage; or learn them gradually when you need them while stepping off the SaaS-stepping-stone. 
Bleh, I'd forgotten Accelerate doesn't use OpenCL, what a pain. I'd hoped that things would've moved that way by now with the wide adoption of OpenCL.
In some cases you can nicely wrap your monster query in a view and browse it as a regular persistent model. This, with 9.3 json support saved me a lot of boilerplate code.
It could theoretically support OpenCL. Patches are most welcome!
Yep, it lets the conversation move _beyond_ that point and into what's actually interesting and different about the language. It's exciting.
Shouldn't it be `Maybe Text` instead of `Text Maybe` in the status update type definition?
can you please stop being so arrogant. or at least try not to express it in every post.
Congratulations Simon! This is a well deserved honor (even if unnecessary, as actions speak for themselves). We only talked a few sentences so far, but it was a great pleasure to meet you in person last year!
I've written game-like soft-realtime stuff in Haskell, and while the GC gaps are somewhat noticeable, it's not really a big issue in practice. Maybe you don't allow that in AAA++ Hollywood-budget top-10 games, but for anything else GHC is more-or-less ok, and if there were a Haskell implementation which actually focused on predictable GC, it would be A+ OK. C++ game programmers are not gods either, they lose much more with sloppy programming than they would lose with the GC.
Maybe this is an angry rant, but I actually agree.
I agree with the message, but not the details. * Writing elementary code is not only for beginners. I prefer writing very simple code (when possible) over using the full power of Haskell (and by power, I already mean Haskell98, not only fancy GHC extension which make it even much more powerful). * Start with pure code, not IO. *Especially* in case of doubt. * While it is true that you can always parametrize later, and in fact I usually do that, almost always it turns out that the parametrized code is better (fortunately, Haskell makes refactoring a bliss, compared to other languages). * qualified imports - I'm undecided on this.
I thought Peter Capaldi was going to be the next Doctor.
Zippers definitely seem related in many ways. It would be interesting to see in what other ways zippers can be related. &gt; In fact, for the version in the post, (fmap extract . localize) will replace every element in the tree with the top element. Whoops, seems I made a mistake.
Sat next to him at lunch at a workshop once. Super nice guy. Also one of the best presenters in the field (not that the bar is set terribly high for that distinction, but he is quite good).
A PhD is "Doctor of Philosophy". This is "Doctor of Science". Don't know if he had a PhD or not though.
or [type-safe c-like printf](https://groups.google.com/forum/#!msg/idris-lang/z0j1NPDo3Gc/7W0RCdveP5cJ)
&gt; Threepenny GUI comes to mind I've been wanting to put live demos of the Threepenny examples on the internet, but haven't gotten around to it yet. Heroku has free accounts, so I might try that one. Of course, if somebody sends a patch, that would be awesome! 
No, spj did not have a doctorate. 
Congratulations! It is heartwarming to see this recognition of Simon's amazing work.
Just goes to show how efficient functional programming is.
You know there's a notion of sentences in English. They end with a dot. You should try it sometime :)) 
Scala isn't that much worse than Haskell, by the standards of languages that are actually used in businesses. (J and K don't count, they are most likely unmaintainable.)
While there may be a need for OpenCL in the future personally I don't see a major need right now. Given that CUDA is better supported by NVidia and AMD doesn't seem to be making a serious push into the server side world I would rather see all the work go into the CUDA backend. If AMD launched a competitor to Tesla and AWS started rolling it out I might change my mind, but for many problems OpenCL is just a standard with one real implementation.
Yeah, it pretty much sucks today. The loader will be fixed 7.8 so that c++ libraries can be linked alongside of TH. I know in my own work I have not really adopted lens since there are so many dependencies on c code.
Weird, slides weren't working for me tonight (they were a few days ago). Anyone else had this issue ?
Did anyone attend? How was his talk? I planned to go, but didn't make it.
Nice work; glad to see it. So, this is a side issue, but... Sorry, no programming language can prevent poorly structured code bases and jumbled unstructured messes. Don't get me wrong - I'm very excited about the rapid expansion of Haskell use at our company as more and more of our dev team comes on board, and the many advantages that brings. But that doesn't exempt us even the tiniest bit from the need for good software engineering practices.
I don't think so. As said, you're not writing a Haskell type, but some other made-up syntax. The `Maybe` keyword there is a bit like `null` in an sql table definition: a modification to the previous declaration. If you ask me, using `Maybe` here is confusing, something like `Nullable` or any other non-existent identifier would be clearer.
Ok, I've heard that 7.8 just uses the system linker / loader and should be compatible with all dynamic libraries. It seems that ghci can load other dynamic libraries like the C runtime or libpng etc. just fine, but I guess there's something about the C++ runtime it doesn't like. I had a project with TH (repa stencils) and C++ code, and the only way to compile it is to use a Makefile and omit the -libstc++ for the compilation. It seems that's not possible with cabal. GHC 7.8 fixes everything! ;-)
Congratulations Dr Simon!
I like threepenny-gui for making quick UIs. The last time I used it I ran into some bugs, but I suspect they've all been resolved by now--I was using a relatively early version of the library. Apart from those bugs, the experience was very good. It was much easier to set up than wxHaskell or GTK--I had no build problems at all! Some screenshots in the post would have been nice, coincidentally.
Well, now I know how to respond when people say that you need a PhD to program in Haskell...
I'm curious, what's the difference?
one important point to understand about REPA and Accelerate is that they (fundamentally) are not general purpose array libraries. This is not a bad thing, because what they do well, they do REALLY well (which is a testament to the quality of work on them). Areas where REPA / Accelerate **shine**: * pointwise operations. They both have pretty sophisticated generalized variants of stream fusion * local "convolution" steps. A good example of this is local blur filters in images Areas where repa / accelerate are less than awesome: * they don't support writing numerical algorithms in the "direct" style that many folks are accustomed * they don't have a good story for memory locality. memory locality quality is perhaps the biggest factor in performance of algoritms that are built on top of matrix multiply. 
What a glorious abomination. Good work.
I see, the problem was than that I was only reading the code.
Not everyone who tries or wants to play with Accelerate is looking to use it on some crazy server with Tesla cards. I certainly don't have a need for that. A lot of us just have mid-range (or integrated) GPUs and would like to also play along without having very sub-par performance on our machines. Even a CPU backend would be welcome (there used to be an experimental LLVM backend I believe.) Programming with it is certainly a lot easier than doing GPU programming "manually" and makes it possible to experiment with offloading workloads more easily (maybe I want to write a game and try it for something, since it's just so easy, etc.) Also, there are at least 4 OpenCL implementations to my understanding: AMD, Intel and Nvidia (as part of CUDA) all have their own toolkits. It should even be possible to target Clang for a 100% BSD3-free CPU-based OpenCL implementation. It might even work on GPUs (LLVM supports PTX and the R600 AMD GPU IL.) Or am I misunderstanding you?
there are libs that use C++ AND TH and are cbalized. look at llvm-general
AMD just released an incredibly powerful GPU with 12GB RAM specifically for scientific computing, this may be the start of their push. My problem is that OpenCL is supported by everyone; Nvidia, AMD, Intel etc. CUDA is only supported by Nvidia and as far as I can tell receiving just as much attention as OpenCL.
Goodness, I might stop using Vim for Haskell programming.
I doubt they would have said "Unusually, he does not have a PhD" if he actually had a DSc. That would just be silly.
Awesome!
Time to give up on vim. Maybe take evil out for a spin.
I really like to see such development environment improvements for Language Haskell. Awesome catch chrisdone, thanks.
Very interesting! I tried to compile a few different versions of this library, but they all fail to build for me with different errors, so I can't actually play with it myself. What do they do different? I see they link 'extra-libraries: stdc++', so that must make any TH / ghci fail, right? I don't see anything in the custom Setup.hs to prevent that either. What's the trick here?
Source link?
I'm getting the following error when I run `cabal install`: src/Main.hs:45:25: Expecting one more argument to `ast' In the type signature for `fix': fix :: AppFixity ast =&gt; ast -&gt; ast 
&gt; The last time I used it I ran into some bugs, but I suspect they've all been resolved by now Please don't hesitate to drop me a line or peruse the [issue tracker][1]! :-) [1]: https://github.com/HeinrichApfelmus/threepenny-gui/issues?state=open
why am I using vim again?
The source is not available online. It is very primitive though (and a bit messy IIRC), so any generic advice on how to make it more e-reader friendly should be applicable. If desirable, send me an email or pm or whatever, and I can email the source code to anyone interested.
Cute. But - how are the error messages?
I thought you were talking about esqueleto, which I don't know much about. persistent has always supported raw SQL.
 diff --git a/src/Main.hs b/src/Main.hs index ad5fd24..476d5eb 100644 --- a/src/Main.hs +++ b/src/Main.hs @@ -42,7 +42,7 @@ outputWith action typ code = code _ -&gt; error "Unknown parser type." -fix :: AppFixity ast =&gt; ast -&gt; ast +fix :: AppFixity ast =&gt; ast SrcSpanInfo -&gt; ast SrcSpanInfo fix = fromMaybe (error "fix") . applyFixities baseFixities instance Alternative ParseResult where
Will a -&gt; b be the type of a pure function in the source language?
Yes!
Welcome to emacs! I hope you enjoy your stay. As a former vim addict myself, I believe you will.
How about we use upper bounds when it's feasible for the maintainer to implement them, but don't use them (or use looser upper bounds) when it's not.
This problem of upper bounds should be a lot less bad now that Hackage supports changing the versions bounds of dependencies on previously uploaded packages. We just need to start using this feature.
Why this would be impossible to implement in Vim?
I don't think it's impossible, just a matter of time and dedication.
I followed the instructions in your README, but I get this error on Emacs startup: Warning (initialization): An error occurred while loading `/home/pete/.emacs': File error: Cannot open load file, structured-haskell-mode To ensure normal operation, you should investigate and remove the cause of the error in your initialization file. Start Emacs with the `--debug-init' option to view a complete error backtrace. With the `--debug-init` option: Debugger entered--Lisp error: (file-error "Cannot open load file" "structured-haskell-mode") require(structured-haskell-mode) eval-buffer(#&lt;buffer *load*&gt; nil "/home/pete/.emacs" nil t) ; Reading at buffer position 2063 load-with-code-conversion("/home/pete/.emacs" "/home/pete/.emacs" t t) load("~/.emacs" t t) #[0 "\205\262 The problem is with the line (require 'structured-haskell-mode) I have no idea why it isn't working. My [dotfiles are on Github](https://github.com/pharpend/dotfiles). I will push the broken emacs configuration to a branch called "shm". any help would be greatly appreciated.
The README is wrong. It's called shm, not structured-haskell-mode.
I don't even want to imagine.
As Mikhail says in a comment, he is working on an `--allow-newer` feature to selectively ignore upper bounds, for use in situations where you're trying out packages with later versions of their dependencies. We're just going through the review process at the moment: https://github.com/haskell/cabal/pull/1604 **Update:** review done. Bryan has tried it and it works for his use case. It'll be in the next cabal-install release.
That worry is remediated by using evil-mode! (Extensible Vi layer for Emacs.)
This error is fixed by https://github.com/chrisdone/structured-haskell-mode/commit/c17f28bc4707592573a0b2cc8af97053a58dab67
I'm not sure this notion of "coeffects" really is a coeffect. I'm not absolutely sure what a coeffect is (though I find Petricek, Orchard and Mycroft's choice to name "coeffects" semantic phenomenons that arise from comonads rather convincing), and I understand there is a bit of room for discussion here, but I would bet that the coeffects discussed in this blog post are not the real thing. The argument is that private r with {Alloc r; Read r} in f r () should not run with `f : [region r] . Unit -&gt; S {Write r} Unit` because `r` was only declared with markers `Alloc` and `Read`, and the `Write` (co)effect is not allowed here. Why not, but isn't that just the usual monadic effect, with a `with ...` notation having the semantics of "you're not allowed to infer *more* effects?". I don't see what would be fundamentally *dual* about this. OCaml polymorphic variants are sum types with row variables. ``[&lt; `A | `B ]`` is a polymorphic type that can be instantiated with any sum type having at most the constructors `` `A`` and `` `B``, while ``[&gt; `A | `B ]`` can be instantiated with any sum type with constructors at least `` `A`` and `` `B``. Then there are effect-inference systems using row types, such as Daan Leijen [Koka](http://research.microsoft.com/en-us/projects/koka/)'s language. I would expect a combination of those to give something like the "coeffects" described in this post, while not really being different from other effect systems out there.
That's what I did — 5-6 years or so of pure vim usage, and I really like what Emacs has become (with Evil.) What drove me away was vimscript. In the end it was impossible to create what I wanted in Vim without major headaches. I keep going back to vim for certain things, such as git merges and incremental commits with fugitive. I also like that vim starts up much faster, so I use it for one-off edits, and reserve Emacs for serious work.
Does something like this exist for sublime text?
As a long time vim user who switched to Emacs, IMHO vim doesn't feel as alive as Emacs, especially when you're working in a language with an interactive mode.
That still requires bos to look at all the dependencies of his test suite and manually bump their bounds on hackage. It almost certainly has to be bos, because he hasn't actually released his new version yet.
build-depends: base ==4.6 !important
I haven't been following the debate too closely, but I can't help but feel like the effort is being put towards finding a social solution to a technical problem. I mean, when I read this: &gt; the test suite depends on a chain of third party libraries that somewhere has an upper bound on an older version of text ...I think "well let those libraries use an older version of text". Obviously there are technical reasons why we can't just do that, but I think ultimately that's going to be the only solution that actually solves the problem. I think it's clear from the debates that there isn't a PVP that can save us. It might be time we re-evaluate how we package and manage Haskell libraries. It seems like packages don't have enough independence, and that dependencies permeate through every package that transitively depend upon it. This seems like the problem that needs to be solved.
They're simple gif animations. I made a wee F9 keybinding for Emacs to run this: (defun screenshot-frame () "Take a screenshot of 400x200 pixels of the Emacs frame." (interactive) (shell-command-to-string "sleep 1;import -window 0x3c000a3 -crop 400x200+13+0 +repage /tmp/frames/`date +%s`.png")) (Replace the window id with the window id of your target window, which you can get with `xwininfo -display :0`.) Which I would execute after running a command or pressing a key (it sounds painful but it's not, and it allows you to make mistakes). The sleep call is to ensure that the buffer has finished updating. I also disabled `blink-cursor-mode`. Then to preview the animation so far I would use animate -delay 35 /tmp/frames/*.png If some frames were redundant I'd remove them. And then finally to write out a .gif I'd use convert -delay 35 /tmp/frames/*.png out.gif I found the whole thing quite convenient!
Perhaps this is a crazy idea: but wouldn't it possible to automate the process of upper bound adjustment at least for packages that have a test suite? You have a known good range, that you build against, and then you incrementally increase the bounds on dependency by attempting rebuilds and testing them. You could do this locally (which in bos' use case would be just a single build and test that you would have to do anyways) but also have a bot on Hackage which continuously re-adjusts bounds. The same technique could also go the other way: tightening known bad bounds instead of loosening known good bounds.
The --allow-newer flag would be really nice in combination with some automation. If package B depends on A, and a new version of A gets uploaded, a bot could build B with --allow-newer and report (or automatically bump) B's upper bounds if the build is successful. Maybe doing it completely automatically would be bad in the case that a semantic change is made that doesn't actually break the build, but it'd be nice to at least get emails saying "B broke because a new A was uploaded" or "B would break if you used the new A" or "B builds fine with the new A" would be handy.
Yes it'd be useful to know both ways round, but it's not a simple balance of numbers. You have to remember who runs into the problems. With it not building at all, then ordinary users are left confused. With a package that works but doesn't use the latest version of something, ordinary users are still ok, and the people we inconvinece are hackers such as ourselves who know a bit better what to do. The `--allow-newer` flag will help people like us in the latter situations. It's not something you'd want most people using by default, but it reduces the effort in trying the latest versions of things, like Bryan was doing with testing `text` with 7.8.
Well its certainly using coeffect in a very different way than the Petricek/Orchard/Mycroft definition! Here it seems to just be "an effect in the negative position". Hopefully the language will sort itself out. I do think the connection with work on boxes and is interesting, since those systems are really about _staging_ -- so we can look at "effects" in DDC as just regions that need to execute in different staged contexts, and the enforced ordering being that which arises from staging contexts. Neat!
Yes, the solution to that is what we sometimes call "private dependencies". Indeed it would help in some situations, though there are still plenty of situations where dependencies are exposed.
We need this automateld using stackage and a sat solver.
good question! And one I don't know the answer to. I have some ideas, but nothing really figured out. Regardless, the typing rule for `callcc` would probably give the answer. 
Personally I like the idea of soft and hard bounds combined with the idea of adding hard bounds after a package is released when build failures are discovered. That way the first person building a package with a dependency (or the first few) can add that hard bound once it actually applies but people who want stability over bleeding edge can still use the "best guess when it might break in the future" style of upper bound common now.
&gt; Regardless of terminology, the post has a cool idea. I think I didn't understand why the idea was cool. To me the code snippet with {eff1; eff2} in expr means "type-check `expr` and check that its monadic type is a subset of `{eff1; eff2}`". That seems like an interesting surface syntax to have, but still something (pretty simple) about effects. What have I missed? (I think I remember there was something about co-effects mentioned in Ross Tate's productoid work, but it was only hinted at, not detailed, and maybe it was closer to the present notion. I'd be interested to know more about that as well.)
Oh gawd, `!important` in CSS is such a hack, but maybe we need a hack to solve this problem.
Remap Capslock to Ctrl. 
Live coding hasn't been happening for decades with supercollider, it only became possible in supercollider 3.0. The guy saying "we had to invent it" is Nick Collins, who is a core supercollider developer, and did have a big part in exploring live coding in supercollider from 2003 onwards, alongside people like Fabrice Mogini and Julian Rohrhuber. That said we've been careful to recognise earlier work, including putting out a CD of live coding pre-history which went back to 1985.
I'd move back to Emacs, but currently running just one sandboxed GHC in haskell-mode sends my computer to a crawl. I'm happily bilingual, so I just popped over to Vim, but shm is *compelling*.
I can't remember exactly what we said, but it probably wasn't exactly this, I'm happy with the overall impression though, apart from the crappy headline.. It's a fun article.
There's no shortage of good ideas (and yours is a good one). They get proposed by various people every time this issue is brought up. It's just the matter of someone putting effort into getting it done. (And I'm not implying it's gonna be easy.)
I don't think there's much difference between using and not using stackage for an end user. Very few, if any, packages are somehow patched on stackage. So if you're lucky enough to require only packages that are in stackage, you won't run into problems even if you're installing them in a usual way using cabal (assuming you use sandboxes). And if you're not that lucky, well, it won't help you much to know that the package you need is not in stackage if you still need it.
I am highly in favor of this solution.
Right, that's what I've been imagining (with automation but a human still in the loop).
Why do you think this requires a SAT solver? To enumerate options or something else?
They work fine for me. (FF 25.0.1)
Actually, it's the [least useful kind of constraint](http://www.reddit.com/r/haskell/comments/1ns193/why_pvp_doesnt_work/cclhcmb).
I prefer [uglymemo](http://hackage.haskell.org/package/uglymemo) for memoization. It's not based on any clever ideas, but is simple and fast. Its only drawback is the name :)
Have you seen emacsclient? By running an emacs server in the background, you can cut the starting time way down for one-off tasks. 
Yeah, I have, and I find it kinda cumbersome. Don't know why, but my working process just hasn't adapted to it. I guess the pain of just calling `vim` for that one file I want to edit is next-to-nonexistent, so I don't have a reason to get used to emacs-server.
Implementations don't get much simpler than that!
Still not working for me under chromium 31. Gonna download FF to see. Thanks. update: works fine under FF nightly 28.0a1 (2013-12-08). time to investigate why chromium fails to get the slides/sync.
As someone who uses Sublime Text, and really never used vi, nor emacs, how can I best get started with emacs? Besides the tutorial that it comes with. I also use mac, so I'm not sure what to do when it wants me to do meta+v for example.
Here's perhaps the orginal classical StackOverflow answer about memoization http://stackoverflow.com/questions/3208258/memoization-in-haskell/3209189#3209189
of which library? all i said is look at llvm-general to see how they do it. One key thing is you have to use extern "C" to generate C ABI style stubs for functions Also don't call the ffi'd c++ functions from TH. If you don't link example code, people can't help you look at llvm-general http://hackage.haskell.org/package/llvm-general-3.2.7.2 
&gt; I keep going back to vim for certain things, such as git merges and incremental commits with fugitive. Have you tried magit? I really love doing incremental commits and merges with magit, so I'd recommend it if you haven't tried it. &gt; I also like that vim starts up much faster, so I use it for one-off edits, and reserve Emacs for serious work. If you have some interest to improve that speed, you can get instant start-up with something like John Wiegley's [use-package](https://github.com/jwiegley/use-package) aimed at exactly this result. 
How about org-mode? I have an emacs configuration the way I like it for plain editing (but nothing else) but emacs-style bindings wear the living hell out of my hands in minutes' time. However, org-mode is really sweet and I would like to use it. Either way, I just started working on switching again. I have been planning it for a couple of months, but I have all my vim stuff set up to autocompile and whatnot with just a couple of keystrokes, and with emacs I end up just running it in tmux and compiling in an adjacent pane since I don't have the fluency down yet.
Well, you told me to have a look at llvm-general, so that's what I did. What other library could I possibly be talking about? ;-) Like I said, I can't make that library compile, so I can't see / try it out and understand how / why it works. It has 'extra-libraries: stdc++', meaning that ghci / TH will try to load it when firing up the bytecode interpreter, which does not work with the current release of GHC. So I'm asking if you can tell how they avoid this well-known problem inside of the llvm-general library. If I just stick a dependency on libstc++ anywhere in my cabal file, all TH / ghci will fail because it can't load that library, completely independent of whether it is actually used or not from TH etc.
Probably a way to get running off the ground is to try someone else's config. You can try [mine.](https://github.com/chrisdone/chrisdone-emacs) You needs Emacs and Git installed, everything else is either in my repo or pulled from git submodules. Here's [an example of following the steps in the README](https://gist.github.com/chrisdone/5231bbe3c9c1521632d8/raw/c16d6970c08f3c8885bc51a10aec6d30fbe94ee2/gistfile1.txt) that I just ran on my other laptop. You probably want to comment out [this line](https://github.com/chrisdone/chrisdone-emacs/blob/master/init.el#L62) about god-mode, because that enables a strange input style like Vim to avoid having to hit Ctrl. If you like arrow keys for code navigation, remove [these lines](https://github.com/chrisdone/chrisdone-emacs/blob/master/config/global.el#L145). But this is my own personal Emacs setup, so it will have haskell-mode and structured-haskell-mode, magit (Git repo integration) already setup ready to go. If you don't like the dark theme, comment out `(zenburn)`. Keybindings that are different or additional to normal Emacs are [here](https://github.com/chrisdone/chrisdone-emacs/blob/master/config/global.el#L133). My haskell-specific keybindings are [here](https://github.com/chrisdone/chrisdone-emacs/blob/master/config/haskell.el#L84). Open a .hs file in a project and run C-` to start GHCi. Things should be mostly self-explanatory from there. Look at the keybindings and play around. On the mac when it says Meta it means the alt/option key. I used Emacs on the mac for a while, it sucks a little bit because there's only one Ctrl key on the mac whereas I type chords with two hands. 
I think since it uses unsafePerformIO anyway, it might as well use a fast hash table rather than a persistent data structure like Data.Map. As the persistence isn't needed.
This looks amazing, but it uses emacs which is INFURIATING (imho). Hopefully someone can take the haskell-structured-mode binary and create a wrapper for sublime/vim/leksah that can take advantage of it.
i need to know more about what OS and version of everything you have installed. For all i know you could be using minix on a sparc machine! I can guess what problems you're having, so as a freeby i'll tell you what your machine is and such, but next time you have to share information or no one can help you ever. I"m going to guess you're on a mac, and you have xcode 5. I'm going to guess you did "brew install llvm" at some point. 1. Install a real gcc (brew tap versions ; brew install apple-gcc42) should do the trick 2. make sure you your ghc settings file points to a real gcc rather than clang 3. "brew install llvm --use-gcc". 4. cabal install llvm-general should work tada 
yep
Perhaps using Data.Map is threadsafe, and using an IO hashmap wouldn't be?
Here is an idea that might help: With Petricek et al's coeffect system, define: type Z s b = C s () -&gt; b Where 's' is a coeffect type. Then with the Lucassen/Gifford effect system, define: type M e b = () -&gt; T e b where 'e' is an effect type. Now use 'S e b' for both 'Z s b' and 'M e b' and save yourself some trouble.
I just aliased `emacs` to run `emacsclient` and have the server start up when I log in. Then it's no different to just calling `vim` except my session is preserved between the different frames. That said, it probably isn't enough of an improvement to matter too much.
It could keep on using an MVar to synchronize access to the hash table. I am guessing the performance benefits of the hash table would outweight the extra serialization this might cause. With more care, the hash table could simply be thread-safe, if the edge cases are handled properly (two concurrent lookups failing, and then two concurrent inserts of the same key/value pair).
Unfortunately that feature hasn't actually been enabled yet (due to some bug or another)...
I was hoping to use a C hash table, and not a Haskell one, though a Haskell one could be good enough and thread-safe more easily. I don't think STM is super-fast in general. It gives decent speed for nice semantics, but every benchmark I've seen comparing STM to lower-level alternatives had put them on top.
A good concurrent hash table will be way faster. You can't expect a very general tool like STM to perform like a specially engineered solution to a very special case.
Thanks for posting! Going through it.
Done!
I fly in that afternoon! Is there any cost involved?
Yeah, I would prefer a two vendor ecosystem over a single vendor and that would make an opencl backend attractive. We are probably at least a year away from knowing if AMD is going to be a competitor in this space. Only time will tell.
Different strokes for different folks. I use accelerate exclusively on GPUs. I test on my laptop and can deploy into AWS using a CC instance with Teslas. It is certainly possible to target many different platforms. However if implementing multiple backends would take away resources from the CUDA backend then I don't see many advantages in adding support. CUDA is likely to continue to be the platform of choice on NVIDIA cards for the foreseeable future given that their OpenCL driver has historically been slower (although I have not looked into this recently). Given the choice today between improvements to the existing backend and new backends I would choose adding features to the CUDA backend. If AMD becomes a significant player in this space then I could see serious demand for OpenCL. But those are just my thoughts from where I sit. I appreciate the not everyone uses accelerate at work.
Apologies for being imprecise or describing my problem poorly. I didn't mention any platform or any specific code because I don't think it matters for my question. The libstdc++ issue I'm running into is this one: https://ghc.haskell.org/trac/ghc/ticket/5289 Like sseveran said, that seems to be fixed in GHC 7.8 since there's new linking code. I have a real gcc installed and don't have xcode 5 etc. I don't think there's really any good known fix for this till we have GHC 7.8. What I'm asking is, how does llvm-general avoid this issue? Simply sticking 'extra-libraries: stdc++' into a cabal file will trigger the bug above on ghci / TH invocation. You can fix this issue with a Makefile by simply not specifying libstdc++ on the TH compilation steps, and just giving it on the final linking step, but I haven't figured out how to do it with Cabal.
I once asked on StackOverflow how data-memocombinators works (almost 3 years ago, has it really been that long?) and Luke Palmer himself gave a pretty great answer. http://stackoverflow.com/a/4980838/208257
That's a fitting acknowledgement of his huge contribution to computer science and education. Simon's energy and enthusiasm is an example to us all. 
Just switching from Data.Map to Data.HashMap gives a nice speedup (if you can hash). But I've not updated the package for that.
Nice to see progress on `--allow-newer`. What I really would like though is allowing `--constraints` to loosen individual constraints; it can currently only tighten them. So `cabal install --constraints 'text &lt;= 1.0'` or whatever.
This reminds me strongly of the type system for handlers in Kammar, Lindley and Oury's [Handlers in Action](http://homepages.inf.ed.ac.uk/slindley/papers/handlers.pdf), which is based on Paul Levy's Call-by-Push-Value. The type `S e a` looks very similar to their `U_e a` type, which represents a suspended computation that requires a context that can handle the effects `e`. `box` seems similar to their `{ M }` construct, which turns an effectful computation into a thunk. `run` seems similar to their !V, which takes a thunk and runs it. `extend` seems similar to their `handle M with H`, except that in Disciple you don't get to choose `H`. As far as I can tell, in Disciple, reading is always interpreted as reading the store and writing is always interpreted as writing to the store. I think that, in terms of the Kammar et al.'s system, "coeffects" are simply operations that are not handled by a handler (i.e., an `extend`), but simply forwarded to the next enclosing handler. The article says: "a boxed computation with a coeffect can only be executed when the bearer has the capabilities listed on the box". I read this as saying that the bearer must be able to handle the operations named by a value of type `U_e a`.
If you are taking in advices for next 24 days of Hackage, I suggest [MFlow](http://hackage.haskell.org/package/MFlow). Features awesome ideas.
No, no cost involved.
You're a bit behind the times. The Community Edition of the IDE is free. I'm also curious about the license issues. The source on github is already available with a "do whatever you want" 3-clause BSD license. We require you to provide all code content posted to our users with that license. We have a slightly less restrictive (not that it could be a lot less restrictive) license to the company. Would using the 3-clause BSD license to the company change your mind?
&gt; I can't help but feel like the effort is being put towards finding a social solution to a technical problem. Taking this thought a bit further, one might argue that the PVP shouldn't have a social component at all. What if hackage/Cabal decided your version numbers for you? Theoretically it could do this for us and eliminate the chance for human error. Sometimes I forget that I added or changed the API and incorrectly do a minor version bump. I think a hackage/Cabal that determines version numbers for you is an idea worth exploring. The immediate objection is that users want to be able to communicate things in version numbers that cannot possibly be known by hackage...say that a package is making a 1.0 release that signals to the public that it should be considered much more stable and the API more reliable. That could still be done with a system like this. With a version number of the scheme a.b.c.d, we could give the user control over a, and give hackage complete control over b, c, and d. I think this idea has a lot of promise. The point of version numbers is to communicate things about how a piece of software changes over time. The point of the PVP is to standardize that communication, so the meaning of version numbers is not ambiguous. The point of this idea would be to more strongly enforce the standard.
This is also my opinion regarding current problem. We must distinguish package-local deps from shared deps. To achieve this, we may turn the current cabal deps listing to a private set, and introduce a new field to list packages that must be shared among a binary. Additionally, GHC must at least handle this feature (can it use multiple versions of a package at the moment ?), and if possible check that exported data is consistent with shared dependencies. I feel that a good share of deps nightmares would go away with this feature. And we could then move on and focus more accurately on how to solve the remaining issues.
Lovely! Cross-referring to the other thread, I also like the `base &lt; 10` mandatory upper version bound :)
It does look like uglymemo written with the hashtables package http://hackage.haskell.org/package/hashtables does better for this particular problem (the 'cuckoo' method was best). Timing `fib 40000` real 0m0.435s -- uglymemo/hashtables real 0m0.529s -- Data.MemoCombinators real 0m0.754s -- uglymemo real 0m0.802s -- uglymemo/unordered-containers-hashmap http://lpaste.net/96787 
&gt; It could keep on using an MVar to synchronize access to the hash table. It doesn't even need an MVar. An IORef should be enough, since the operation inside the `modifyMVar` is pure. So you can use `atomicModifyIORef`. Or even just `modifyIORef`, since at worst a race condition will result in some duplicated pure work.
A hash-table would not be pure, though? One way to make it thread-safe is a lock (the MVar) or you could try to make it lock-free, or use fine-grained locking. 
Cool! Please be sure to share the survey results :-)
Ah, you are right, I was thinking of a pure hash-map like data structure such as unordered-containers. But even for a normal hashmap you don't strictly need locking if you can tolerate some recomputing, you just need read and write barriers or atomic updates.
Done!
I'm now not so sure that this is all that useful in comparison to the work involved in maintaining the information. I don't think we want end users being put in a situation where their tools automatically speculate that some version of something builds with a later version of its dependency. So if it's explicit, using `--allow-newer=text,blah` then it doesn't matter if it was a hard or soft upper bound. The only time it would help is that the solver could make better suggestions if solving fails. That's then a smaller niche, and the effort to maintain the extra info of hard vs soft is non-trivial. I'm not sure it's worth it.
You mean allowing older too? You can use `--allow-newer=foo` in combination with `--constraint=foo ...`, but it's true that that only loosens the upper bound. Adding a flag to allowing older &amp; newer is of course possible, though less obviously useful. Got an example use case? Want to send us a patch?
It was a targeted search, the code has been there from at least 2009.
Wow - thank you ... hope to install this soon with cabal install :P
Instead of suggestions, it could just try different configurations automatically.
What you're suggesting sounds like locklessness, which is attainable but not trivial. It might be the way to go, though.
If this function was inlined (or the `BufferOp` `buf_append` function to be statically known) and all the stream lengths were known, would this run in O(n)? Does `Data.ByteString`, etc., perform this type of fusion?
Nice work! From your readme: &gt; Should I use this? &gt; No. Dear god. No. If you build it, they will come.
How does one get visibility on these sorts of performance issues? 
How did you find this? Was it just a matter of compiling up a profiled version and going 'well, *that* shouldn't be taking 99% of realtime...'?
Well, I didn't find it, `nominolo` found it, and I think it was mostly profiling and staring at the code. The fact that `cabal` was so CPU bound has been suspicious for a long time. We thought it was the untarring because `cabal` prints "tarball downloaded" or whatever very quickly... however it's just that the thunk was still sitting unevaluated at that point.
Which function are you proposing be inlined? ...for that matter, what is "this type of fusion"? It's not obvious to me that there's any fusion involved here.
Not sure if this is a good place to put this. This was actually supposed to only be about List and using it to solve logic problems, but I have a lot of personal friends who don't use Haskell and I thought I would write a couple of lead-up entries instead. It's probably not the most useful to most people here but I thought I would just leave it here in case anybody was interested :)
I was thinking if it could be known that `Data.ByteString.append`s were occurring, would something like what is done in `vector` (http://hackage.haskell.org/package/vector-0.10.9.1/docs/Data-Vector-Fusion-Stream-Monadic.html#v:-43--43-) happen? Would it end up as a single allocation of the appropriate size and a copy of each `ByteString`? I'm not saying it would be very likely here - quite a few things would have to line up just right, but was just wondering if there was an ideal situation where a fold doing `ByteString` `append` and `concat` would have comparable efficiency.
I am a big big fan of `MonadPlus` - it took me a long to really get the feel of it, but now I find it indispensable. Thanks for sharing the love on this sometimes-overlooked type class! 
And once you've built your app, you can publish it and submit it as an entry into the FP Complete competition and maybe earn a little extra cash for your effort.
If this was in HTTP, what else was affected?
This is the Haskell version of: for(int i = 0; i &lt; strlen(s); ++i) Of course it's not exactly the same but the complexity O(n^2 ) vs. O(n) is.
Ganesh released the fixed version of `HTTP`. To get the fix use: $ cabal update # one last time $ cabal install cabal-install --constraint='HTTP&gt;=4000.2.10'
&gt; What if hackage/Cabal decided your version numbers for you? Theoretically it could do this for us and eliminate the chance for human error. Sometimes I forget that I added or changed the API and incorrectly do a minor version bump. I think a hackage/Cabal that determines version numbers for you is an idea worth exploring. You can easily write a tool that helps you detect when you need a version bump according to the PVP (Some time ago, I hacked up some proof-of-concept script for to create an [API delta report](https://gist.github.com/hvr/6648575) which could be easily extended to suggest the minimum required version bumps), but it can do so only for changes visible at the type-level. Otoh, such a tool would have a (NP-)hard time deciding whether any semantic change in the implementation requires a major version bump, so unless it suggests a major version, you'll still have to check whether a major version bump is needed.
It may actually be very non-deterministic. It depends on the number of newline characters in the gzipped tarball (yeah, that function is really weird). But the variance should be a lot lower than before, now.
Yikes—http://packdeps.haskellers.com/reverse/HTTP
Yeah, semantic change detection is obviously hard, but I think a lot of mileage can be gained from API deltas. Of course a tool to automate this would be handy, but my point is that it might be a much bigger improvement to the community as a whole if we absolutely enforced the whole thing by making hackage do it automatically.
&gt; The reverse was taking 79s, now it takes 0.01s. Note that `cabal update` still takes some time, it is just the "reverse" that takes 0.01s. $ time cabal update Downloading the latest package list from hackage.haskell.org real 0m21.907s user 0m3.280s sys 0m0.573s $ cabal --version cabal-install version 1.18.0.2 using version 1.18.0 of the Cabal library (This was after taking nominolo's suggestion and reinstalling cabal-install with the new HTTP release.)
Not with gcc, it optimizes it and don't do the redundant strlen on each iteration. Gcc has lots of special case optimizations for functions it knows about.
I heard a rumour that type checking (and I guess type inference) could be done this way.
ghci provides several of the features utop provides compared to the plain OCaml REPL out of the box...
Try with emacs 24. That sounds like an old Emacs version.
&gt; Yes, we should really make cabal update incremental. Yes, this. I always assumed we were waiting for Hackage 2 to arrive. Took about 30 months more than I expected, but now it's here, maybe someone can get a patch through the defenses.
So this is akin to the termination checker that Coq provides for it's `Fixpoint`s (Simple structural recursion)?
It's actually [more complicated than that](http://prog21.dadgum.com/185.html).
You can use these sorts of solvers for many NP-complete combinatorial problems where you would be tempted to write a brute-force backtracking-based algorithm. [For example, I once used a SAT solver to find solutions for a flash puzzle game](https://github.com/hugomg/hexiom). Using an SMT solver would be similar and might have made modelling the problem a bit simpler. The most interesting result I got was that I could declaratively add a restriction in my model to rule out symmetric solutions. This reduced the runtime in one of the highly symmetric inputs from 30 minutes to a couple of seconds. https://github.com/hugomg/hexiom
*Why on earth* is it reversing the order of "lines" in a gzipped tarball?
Not quite, since we will require information from the refinements. One canonical example of where structural recursion is insufficient is: qsort [] = [] qsort (x:xs) = ls ++ [x] ++ rs where ls = qsort $ filter (x &gt;) xs rs = qsort $ filter (x &lt;) xs Here, you need to know that filter returns smaller (or rather, not larger) lists... 
That's why explicitly mentioned gcc. But if course, you shouldn't rely on that kind of implementation details.
One approach is we can represent every package in the "package universe" as a proposition with dependencies as logical implication: DependsOn(p, q) ≡ p | ~q And conflicts as: Conflict(p, q) ≡ ~p | ~q A package P specification is a list of rules giving the set of packages the package depends on (`d0, d1, ...`) and the packages it conflicts with (`c0, c1, ...`) so the constraints for the solutions are just a large SAT problem. Spec(P, D, C) ≡ Depend(P, d0) &amp; ... &amp; Depend(P, d1) &amp; Conflict (P, c0) &amp; ... &amp; Conflict(P, cn) The [Opium paper](http://cseweb.ucsd.edu/~lerner/papers/opium.pdf) for apt-get that uses this approach.
There aren't any defenses! Send us the code!
Ahhh, thanks! That helped me understand it a lot better.
I was thinking that normally you only want to loosen a few fixed packages, not just ignore any and all upper bounds. But apparently `allow-newer` can take a package name?
somewhat related: [haskell/cabal#799](https://github.com/haskell/cabal/issues/799)
I'll be naive and ask: why not use an existing tool like `git`? I feel it'll be easier to implement than the current system and will be reusing a lot of existing infrastructure (from `git`). [Homebrew](http://brew.sh) seems to work very well using `git`.
Well, even with gcc it's not always the case. &gt; The GCC C compiler, as it turns out, will move the strlen call out of the loop *in some cases*.
For HM-like type systems, a specially-tailored algorithm is probably going to be significantly faster though?
I suppose this is what LiquidHaskell is doing?
Check out [Liquid Haskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/01/01/refinement-types-101.lhs/) which adds refinement types to Haskell using an SMT solver. In general, smt solvers are great for modeling the semantics of programming languages. This can be used for everything from verification to program synthesis. This is where the theory of bitvectors and maybe arrays becomes very useful. Unfortunately, I didn't have room to expound on these more interesting uses (although I should probably have alluded to them). Each topic needs a whole post to properly explain!
Yes, new Disciple, Call-by-Push-Value (CBPV), and effect handler systems are all strongly related. One key difference between Disciple and CBPV is that the former doesn't distinguish between pure computations and values at the type level. I haven't thought of a practical reason to do this when the compiler does partial evaluation / optimisations that might reduce pure computations into values at compile time. 
Perhaps because the `Ord` version is either very complicated or has very complicated dependencies.
FWIW, this is also the route 'opam', a fairly recent but well-received package manager for OCaml, took.
Definitely agree with this sentiment. What I want to see is how this was found.
Here's an implementation of Cabal's version constraint solver written using Z3: https://gist.github.com/NathanHowell/5688647. Compared to the version in cabal-install, Z3 can handle _all_ of Hackage (at the same time) and solve constraints quickly. Many orders of magnitude faster than Cabal.
Same with Go (`go get` clones Git/SVN/Mercurial repos and builds them.)
Sounds far-fetched.
Prove it. Implement it. For matters of practicality, the burden of proof falls on the shoulders the one making the claim of simplicity. 
 import qualified Data.Set as Set ordNub :: (Ord a) =&gt; [a] -&gt; [a] ordNub = go Set.empty where go _ [] = [] go s (x:xs) = if x `Set.member` s then go s xs else x : go (Set.insert x s) xs
And already you've added a complicated (and cyclic!) dependency to base. You can't use the containers package.
I said it's complicated. I meant it's too complicated for base. The question was why there isn't one by default. This is a factor.
"A is suboptimal in certain situations, therefore A is useless." Not sure what to call this fallacy, but it clearly is one. Even when restricting to just those suboptimal situations, that doesn't make it useless.
- `Either` is never introduced, but mentioned in the paragraphs starting with "However, if the failure type is a monoid, Either can be made in general a MonadPlus". - The paragraph ending in "and specify" should lead to a code example, but it's missing. (The entire part of the text seems to be somewhat jumbled up). - All "basically" could be removed, it makes it sound like the claims are somewhat-but-not-really like you describe them. (For example, mzero is not *basically* an alias for `Nothing` when you're in `Maybe` -- it *is* an alias.) - If you're aiming for people that aren't very familiar with Haskell, I think parentheses are often clearer than `$` application, for example `guard (even n)` instead of `guard $ even n`. On the bright side I think the text may be helpful to beginners and is well written, but since I understand Monad I have lost the ability to judge that.
I don't know why you're getting shit for this. It makes sense to me. `nub` removes elements from a list requiring only a way to know if two values are equal. Semantic correctness aside, a significant amount of production code depends on this function with these exact constraints, and imposing an `Ord` constraint will break that code. So, such a breaking change is obviously not an option (in the short term at least).
Because you add to a list by cons'ing onto the front. I'm assuming the algorithm reads "lines" and stores them in a list. Each new line gets added to the front of the list. So after everything has been read, the list has all the file chunks in reverse order. The original code got around this by using `flip`, which reversed the arguments to buf_append. The new version simply reverses the list before calling buf_concat. This is a *very* common issue. It's the whole reason Hughes lists (a.k.a. difference lists, e.g. http://hackage.haskell.org/package/dlist) exist.
How about: import Data.List (sort) ordNub :: (Ord a) =&gt; [a] -&gt; [a] ordNub = uniq . sort where uniq (x:xs@(y:_)) | x == y = uniq xs | otherwise = x : uniq xs uniq xs = xs Needs only base, is very simple and exploits Ord to get O(n log n). I also always wondered how come it's not included.
Color in ghci would be awesome. When I was playing around with Idris, the colorful REPL excited me far more than it probably should have :)
I wouldn't mind reading some follow-up posts ;)
What is the relationship between `MonadPlus` and `Alternative`?
I've used both enough to say that I don't really prefer one over the other. Utop is a lot prettier and has a much better completion UI but ghci has plenty of features that utop does not since it's basically a pretty wrapper around the standard OCaml top level.
It doesn't preserve the ordering the way `nub` does. Sometimes it matters, sometimes not.
That fallacy is quite common in politics and social policy I find. "We can't use more renewable energy because there will still be a deficit we need to make up with fossil fuels." "We can't make our towns more bike friendly because the Netherlands took years to get to its current state." I guess this is the perfect driving out the good.
I am not familiar with utop can you give an example of an API hint? Some would consider getting the type information an API hint and others would expect something like a python doc string.
Thank you quchen for pointing out that I accidentally moved a deleted part of the footnote to the middle of the article. How embarassing! Glad to see it now rather than a week later.
I use utop quite often as well (writing OCaml at $DAYJOB), but like the completion UI of ghci more than the utop one. Des gouts et des couleurs...
I had no idea such a service existed! Sounds really interesting. Does it work just like a text message from the user's end?
"Type systems don't catch all bugs so I'm not losing anything by using an untyped language" is another one we hear a lot.
How is something determined to be to complicated for base?
That's somewhat different. 'go get' requires you to tell where some sources are to be found. opam and cabal/hackage provide repositories which contain meta-information about packages &amp; where their sources are. 
Pretty much, but since it's mediated by an app on the phone it's more like a whatsapp message than an SMS message. There are also some other options like high priority messages which are resent until acknowledged by the user.
The question is not how fast, but how much faster.
&gt; Each topic needs a whole post to properly explain! Looking forward to these posts ;) I kid, you did a great job with this article, and these comments are already pointing me in directions of further reading. It's very interesting work!
When you type a letter, it will give you some hint about what is in the API. And it is colorful.
Neither of your suggestions are lazy like `nub` is. As with ordering, sometimes it matters, sometimes it doesn't.
Might I interest you in this old Reddit post of mine, then? http://www.reddit.com/r/haskell/comments/144biy/pretty_output_in_ghci_howto_in_comments/
Done too. That actually makes me think about my ideal matrix/array library. I may write a post elaborate on this, I'll paste the link here when it's online :-)
I love how you say it's simple and then point out how every known solution isn't. 
I'm a bit puzzled on why this is getting downvoted, and how come Reddit is not showing it in the "new" section of the subreddit.
Great question! By incrementing the 'lowestValue' by 1, every element in the stream will have incremented a counter. This allows for us to keep track of exactly how many elements have been seen so far in our stream. It is useful data to have for streaming data sets of unknown length.
Lazy I/O bites again!
How about rsync to download the new index? Maybe it would need to be uncompressed, but sounds simpler.
I think the question was more about the lines part than about the reversing part. 
Contact the moderators, it's probably caught in the spam filter. 
Or how about "lazy IO is unsuitable for some programs, therefore one should never use lazy IO"?
Actually keeping the packages in source control and checking out the repository is the same principle, though. Where the packages come from is almost completely irrelevant to the discussion.
Fine, but this give a fairly pathological result on long-enough streams of mostly-unique values: I actually get `fromList [("9999", 101)]` with yogsototh's example, which is nonsense. Wouldn't it be better to use an explicit counter, i.e., `(Map a Integer, Integer)`?
like tab completion?
That's entirely possible :) 
`cabal update` is not downloading packages. It is downloading a list of the packages available.
`go get` is the equivalent of `cabal install`, not `cabal update`. There is no `cabal update` with Go since, like nicolast said, you just specify a repository URL--there is no central index. But the existence or lack of a central index does not impact the discussion about whether using git/other version control for the packaging tool (for the metadata and for the packages themselves), i.e. one repo for the metadata, and one repo for each package, is a good idea.
&gt; You're a bit behind the times. The Community Edition of the IDE is free. Ah, sorry, I keep saying "IDE" because the term became synonymous with the whole product in my head, but I mean the app server part for hosting Haskell apps. &gt; I'm also curious about the license issues. The source on github is already available with a "do whatever you want" 3-clause BSD license. We require you to provide all code content posted to our users with that license. We have a slightly less restrictive (not that it could be a lot less restrictive) license to the company. Would using the 3-clause BSD license to the company change your mind? It's mainly about the text contents. I'm happy to write tutorials under CC-BY-SA for everyone like I did for the Wikibook, but "everyone" would include both the community and FP Complete. On the other hand, code snippets embedded in SoH tutorials are fine, they can be covered by a simple permissive license like the Haskell wiki. After all, the point of tutorial code is that people can copy, try and use it in their own projects.
The hackage page has some great examples: http://hackage.haskell.org/package/sbv There are examples of solving puzzles (sudoku, n-queens, etc) My favourite is this: http://hackage.haskell.org/package/sbv-2.10/docs/Data-SBV-Examples-BitPrecise-Legato.html This proves that a complex 8 bit multiplication algorithm works for a certain processor. 
I'll be going into detail about this more later, but `Alternative` also has the concept of success/failure, except instead of answering how to chain succesive successes/failures in series (like `MonadPlus`), it is instead concerned about how to *choose* a success/failure over another, in parallel. Instead of using `&gt;&gt;=` to sequence in series, we use `&lt;|&gt;` to sequence multiple parallel an independent choices. The property we want to see is that the **first successful option is the one taken**. That is, if we have `empty &lt;|&gt; pure 5 &lt;|&gt; pure 10 &lt;|&gt; empty`, the result would be `pure 5` --- the *first* successful option. (Note, `empty` is applicative-speak for `mempty` and `pure` is applicative-speak for `return`, they are identical concepts, but it is again a historical accident) The most significant thing to recognize is that these choices are in *parallel*, and do not depend on each other. They do not chain from one another, they are all independent choices that have no dependency between them. This is why `Alternative` is **not** related to Monad-ness --- to say something is a monad is to say that we found a way to chain operations. However, even if you could not chain operations, you could still have `Alternative`, because it has nothing to do with chaining. `MonadPlus` was originally conceived to solve both the problems of **chaining** successes as well as **choosing** them, before people realized that it is useful to be able to choose things that aren't monads yet have the idea of success/failure. That's why `Alternative` was created --- to give this ability to non-monads as well.
Nice, I did not know about this! Btw are there the legal/distribution reasons why this isn't integrated with cabal (as opposed to lack of cycles...) ? If so, note that for this application, one doesn't need full SMT but just propositional SAT solvers, of which there are many... 
As you said, the Monoid instance for `a -&gt; b` is the source of the confusion here. `PQ.singleton` is never actually run, it's just passed to `mempty` and discarded. To see why, look at the type of `mempty` in your usage there: You know that `mempty PQ.singleton [start] :: PQ.MinPQueue b [Node]` (`::` meaning "has type", and I removed the backticks) because it's being passed to `dijkstraInternal` and it typechecked. You also know `[start] :: [Node]`, so that means `mempty PQ.singleton :: [Node] -&gt; PQ.MinPQueue b [Node]`, and that `mempty` in this case is `mempty :: (k -&gt; a -&gt; MinPQueue k a) -&gt; [Node] -&gt; PQ.MinPQueue b [Node]`. Because this specific `mempty` is from the `Monoid (a -&gt; b)` instance (specifically, it's `Monoid ((k -&gt; a -&gt; MinPQueue k a) -&gt; [Node] -&gt; PQ.MinPQueue b [Node])`), we can look at the `Monoid (a -&gt; b)` instance definition to see what the value of mempty is: `mempty _ = mempty`. So `PQ.singleton` is discarded (by the `_`) and we're left with `(mempty :: [Node] -&gt; PQ.MinPQueue b [Node]) [start]`. Substituting the definition of `mempty` again, we get `mempty :: PQ.MinPQueue b [Node]`, an empty priority queue. In short, `PQ.singleton` isn't returning an empty list, it's simply never evaluated because it's discarded by the `mempty` definition of `Monoid (a -&gt; b)`.
Sounds pretty cool. The video construction is pretty annoying. It just shows the speaker talking and flashes the slides for half a second. Anybody got slides? **EDIT:** Here they are! https://www.cs.indiana.edu/~lkuper/talks/monotonic-determinism/ricon-2013.pdf
The key feature of the algorithm is that it is honest about the results. Actually for each key in the map it should keep track of max over-estimate, see here: https://github.com/Yuras/space-saving/commit/9df61e4c49711ba7bf27eab061db057f4981b022 So you always knows that the actual value is somewhere in the range [entryValue - entryError, entryValue]. Your change fixes the particular patalogical case (most likely introdusing a number of other) loosing the key property -- you can't anymore know that your results are correct. The original paper (I don't have a link on hands, but google should know) contains a number of lemmas about properties of the algorithm, I'd recomend it. And yes, there are a lot of patalogical cases. I tried it few weeks ago, and found it not suitable for my data. Not a silver bullet. But the idea is smart.
It is easy to constuct patalogical event sequence. I found it completely not suitable when keys appears and disappears offen. Unfortunately the original paper ( http://www.cs.ucsb.edu/research/tech_reports/reports/2005-23.pdf ) doesn't discuss that...
Awesome, thanks!
Another approach would be to explicitly parameterize TestTree by the resources it requires to run. For example, TestTree () -- The current version, no parameters TestTree (IORef a) -- I can haz an IORef? Then, we would have withResource :: IO a -- acquire resource -&gt; (a -&gt; IO ()) -- release resource -&gt; TestTree a -&gt; TestTree () with some suitable function for obtaining the ambient `R` when inside `TestTree R`. I don't know whether this trick would actually work for your package, but I've used the trick to handle similar problems elsewhere. Basically the problem is that you need to set up a staged language which can be "run" in different ways (e.g., actually executing the tests, having access to the test AST for printing stuff out, etc). But to do so, you need to explicitly capture the staging, which `(a -&gt; TestTree)` cannot do but which `(TestTree a)` can.
You can check out [her research blog](http://composition.al/). I'm sure the slides are there somewhere, but too lazy to search for you
Um, why on earth do your minimizing/maximizing functions need to return a new map? minimum' :: (Ord a) =&gt; M.Map k a -&gt; Maybe (k,a) minimum' = M.foldWithKey step Nothing where step k v Nothing = Just (k,v) step k v x@(Just (k',v')) | v &lt; v' = Just (k,v) | otherwise = x Or, if you prefer: -- This is a partial function, since M.findMin is partial! -- The choice of M.findMin vs M.findMax is arbitrary. minimum' :: (Ord a) =&gt; M.Map k a -&gt; (k,a) minimum' = M.foldWithKey step =&lt;&lt; M.findMin where step k v x@(k',v') | v &lt; v' = (k,v) | otherwise = x For predictability reasons you probably want to make sure the comparison is actually stable, which the above are not. Here's a cutesy way to do that: minimum' = swap . (M.foldWithKey step =&lt;&lt; (swap . M.findMin)) where step k v x = x `min` (v,k) swap (x,y) = (y,x) And you can remove the partiality by keeping track of any arbitrary key–value pair in the map, which also speeds things up by avoiding the need for `M.findMin`.
Lindsey used [these slides](http://www.cs.indiana.edu/~lkuper/talks/monotonic-determinism/fhpc-2013.pdf) at FHPC 2013 to give a talk with the same title.
`Alternative` has a reasonable name and decent consensus on how it should be inmplemented.
I understand all of that. But, now that we have Alternative, is there a reason to use MonadPlus? All MonadPlus instances are Alternative instances too with some minor tweaks
Probably by using the profiler, I'd guess. The tricky part is that in order to use the profiler, you need to have installed all the dependencies with profiling support, and the offending library should probably be compiled with "-fprof-auto" so that ghc treats non-exported functions as cost centers. Setting "ghc-prof-options: -prof -auto-all" in the library section of a cabal file seems to be one way to do that. http://stackoverflow.com/questions/11747493/automatically-allocate-cost-centres-in-ghc
It's for the same reason you would use a Monad over an Applicative - - bind. MonadPlus allows you to sequence operations based on the results of previous operations due to bind, whereas Alternative, which is not necessarily a Monad, cannot provide this. Try doing something like chaining `halve`s without bind, only using Alternative. The key is that MonadPlus provides a description of the *behavior* under bind. without it, we don't know how to treat mempty under bind for that success/fail behavior. Alternative doesn't tell us anything about a success/fail under bind. Well, that's a lie, the rules of Alternative and Applicative do hint at what behavior under bind must satisfy in specific situations (in the same way that Applicative hints at some behaviors of bind in general) , but it is not a full enough of a description to be useful on all uses of bind without MonadPlus's complete description. 
I think I might have been a tad ambitious with today's post and we don't really explore a lot of `gloss`... hopefully people will still find it fun and somewhat useful though!
 stepGame :: Float -&gt; (Board, Play) -&gt; IO (Board, Play) stepGame _ = id Ought to be `return` :)
FYI I think ranjitjhala replied to you at the top level.
Thanks! I always get these last minutes abbreviations wrong.
Top (out of two) comment on this video &gt; Please, next time show the slides 90% of the time instead of the person. Hand gestures are sometimes important... but only in very specific cases which were not present here. I tried to find the slides on speakerdeck, but they haven't made it there yet. I also tried the ricon website, the archive redirects to 2013/east.html. So I tried west.html. Got a horrid looking page: http://ricon.io/archive/2013/west.html This is unfortunate. [Her blog](http://composition.al/blog/categories/lvars/) on lvars category has many posts that want to learn more. The [link](http://www.cs.indiana.edu/~lkuper/talks/monotonic-determinism/fhpc-2013.pdf) /u/torstengrust mentioned seems to have some slides that were indeed featured by the video.
gloss is awesome! My only gripe with it is that it's pretty inefficient in the way it renders. I wrote a Sokoban clone with gloss that works okay for the most part using only primitives (filled rectangles, etc), but I've also made a Julia set renderer and noticed that the lines actually take quite a bit of time to be drawn to the screen. It's an excellent library, it's easy to use, and it's amazing for getting things on screen quickly, but it should be noted that there are libraries out there (like the direct bindings to OpenGL) that perform a lot better for more intensive applications. Once again, though, wonderful post about a wonderful library! :)
That's very interesting. I think I can really use this. About the freeze function she mentions at the end. As I understand it, freeze is non-blocking. Now, it seems to me that there are two kinds of lattices: Set-like lattices which can never be full (in theory) and Pair-like lattices which can get full at some point. I think that Pair-like lattices can have a special freeze, which can block until the pair is full, and won't throw errors after valid monotonic writes (i.e., a write that would either ways do nothing because we're already right beneath the top). That would allow you to write streaming fork/join style programs. Would that be useful or is it better abstracted with the regular Par monad?
With cabal install cabal-install --constraint='HTTP &gt;= 4000.2.10' I get an error about constraint. cabal: expected a package name followed by a constraint, which is either a version range, 'installed', 'source' or flags This gives me no error cabal install cabal-install HTTP&gt;=4000.2.10 Windows has a different behaviour? Bug? 
Any way to detect the reverse dependency of the specific function used? 
[explained here](http://www.reddit.com/r/haskell/comments/1sh67u/the_reason_why_cabal_update_takes_so_long/cdxnr5l)
You win the gold prize!
I feel like this subreddit needs a css trick to make inlined code more readable - a la SO.
Please do so!
Not that I know of.
It was my 'learn Z3' project, so it's a little clunky... and yes, it only uses SAT, so any incremental SAT solver should work. sbv doesn't (at least when I checked) offer any incremental support so I went with the z3-haskell bindings and hacked it up a bit. I did email MSR's licensing contact a couple times about the feasibility of using Z3 in something like Hackage v3 but never heard back. It's current license definitely isn't a good fit for cabal-install. Having a fast version constraint solver in Hackage would allow for a lot of interesting features though...
Yeah it probably would be better to use an explicit counter. There has already been work done towards developing a data structure specifically for this algorithm. You can check out a Java implementation [here](https://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/stream/StreamSummary.java).
Those are some great examples of alternative min/max functions. For the article, I chose easy-to-read functions over a more optimal solution. If someone is serious about making a space-saving library for haskell, then they will definitely need to write better optimized functions and data structures.
Thanks for the clarifications, I suspected I would be off the mark. Do you have any thoughts about the code example I presented? Is there a well defined way to express that we don't want `length` to be called on potentially infinite lists, or maybe I'm being unfair in dissatisfaction with what Haskell currently considers valid programs?
i've not yet filled the survey, partly because i'm rolling my own tools (as you know). If you want i could still fill it out
Very cool.
To be fair and honest what I wrote was SFML-control, whereas I'm just co-maintaining the low level bindings per se, which were written by Marc :)
CVC4 might be a more interesting candiate perhaps? It's fully BSD licensed. I'd really hate making core tools dependent on 3rd party libraries though unless they can be easily shipped with Cabal. Some SAT solvers are insanely small on the other hand, so maybe that would be better anyway (you could ship it with Cabal, for example.)
The possibility of nontermination is a side effect, in a sense, and maybe should be explicit in the same way that `IO` is. You could have an expressive set of non–Turing-complete combinators and opt in to potential nontermination. The problem is that there are many “sweet spots” (primitive recursion, recursion schemes, structural recursion) but no obvious place to draw the line. We will always find more ways to prove termination for *certain* classes of programs; we just can’t prove it for *arbitrary* programs. The conjecture (as with types!) is that people don’t write arbitrary programs. ;) 
maybe try taking the spaces out around the ' &gt;= '
&gt; I think that Pair-like lattices can have a special freeze, which can block until the pair is full, and won't throw errors after valid monotonic writes That is already accounted for in the second LVars paper, [Freeze After Writing](https://www.cs.indiana.edu/~lkuper/papers/lvish-popl14.pdf). While the "consume" operator of the first paper would blow up if you tried to put the same value again, the new formulation of freezing remembers the state at which the variable has been frozen, and allows to write to a frozen state if you don't actually update the value (you are writing a smaller or equal value). This edge condition can happen even in what you call "set-like lattices". 
Have you looked into gloss-raster? I haven't used it personally, but I imagine it would be more efficient for rendering Julia sets.
Can someone please explain the differences between the goals of this library and the goals of the [diagrams](http://hackage.haskell.org/package/diagrams) library?
I'd say you can get pretty pictures quickly on your screen with both, simple animations too, but gloss would be more suited when you need to support user interaction (that's the 'play/playIO' bit of gloss). 
Your `Resource a` is isomorphic to `Maybe a`, and I described this approach in the article (search for «hypocritical»). As for `singleTest`, it's a low-level function which is used by test providers to implement higher-level interfaces. As an example, for HUnit the high-level interface is testCase :: TestName -&gt; Assertion -&gt; TestTree
Huh. I totally missed that paragraph. That's embarrasing. Consider this a vote for that possibility then. However, I do think that two points of my post are still useful: * By keeping the `Resource` type abstract, even though, as you point out, it is isomorphic to `Maybe`, you are not only reminded that you shouldn't use it before hand but also prevented from using it before an actual test. By having the only interface be in `TestM`, you don't have to worry about partiality or using the value high up in the test tree. * Though you do need to teach the providers about `TestM` on a basic level, there isn't much work to be done. The new function for, for example, HUnit would be testCase :: TestName -&gt; TestM Assertion -&gt; TestTree testCase name = singleTest name . fmap TestCase The only change is the `fmap` and the type, assuming the type of `singleTest` is changed appropriately. EDIT: formatting
Neat! I tried to bind SFML to Haskell 2 years ago, but I paused it because I wasn't sure of what it would bring in the end when compared to the use of separate libraries (easy to use window/events libs already existed in Haskell, same for image loading). The unsure point was sound playing (Sönke Hahn bound sfml-audio, and only it, to Haskell in the past, so maybe he had the same thoughts). This was for the functionalities. SFML's API imposes also its architectural point of view, and for this specific part it even seemed counterproductive to use SFML in Haskell: Static Images/mutable Sprites division doesn't map well to a functional setting, where you'd rather not carry Sprites around and modify them. This makes great sense in C++, but in Haskell, whether you want persistent sprites or just record associating display info with textures is a choice that should be left to the game developer, as both may be valid depending on your game's architecture. Mutable Sprites get in the way when you're using FRP, for instance, so in that case you'll have either to workaround them, or to re-develop rectangle drawing. So for these reasons, it seemed that using existing libs with a bit of glue to recode rectangle drawing (pretty straightforward) and a functional API to handle sprites was preferable, but I would be very interested to know how SFML/SFML-control compares to other solutions like GLFW+JuicyPixels+OpenAL/sfml-audio. I know I wasn't the only one to wish for a SFML binding for Haskell, do other people have had the same feelings as me regarding the API?
I have not! Maybe I'll try to dig up that code and use gloss-raster instead to see if it makes a difference, thanks!
I think this was pretty much the same frustration Marc went through when binding SFML. I think that in general this kind of feeling is pretty much the same every time we have to interact with a foreign libraries which uses mutation as side effects willy-nilly. As regards audio, I think that again almost a 1:1 mapping is exposed, but higher level solutions would be appreciated and indeed considered for inclusion. There was a guy, Sam, which contacted me via email and that has such a audio library, so probably it will be incorporated into the SFML organization :)
Yes, I spoke about Sprites but same applies to sounds. It's just it doesn't have the same impact because you much more seldom have to mutate sounds.
First, changing `testCase` probably isn't a good idea, because suddenly you have to insert `return` in all your tests which don't care about resources. So that means adding an extra function, like `testCaseWithResource`, to every provider. The `IORef` approach didn't require any modifications to providers. Second, `TestM Assertion` is still awkward, because you have to deal with two monads (`TestM` and `IO`) instead of one. So your code will look like testCaseWithResource "myTest" $ do x &lt;- getResource "x" y &lt;- getResource "y" return $ do -- actual test here Ideally, you should only work in one monad, like testCaseWithResource "myTest" $ do x &lt;- getResource "x" y &lt;- getResource "y" -- actual test here And that's actually possible with the `IORef` approach, but not with the `TestM` approach. I agree that each way has its own merits. But until there's a clear winner, I prefer to go with the simplest approach possible.
Yeah, Carter and I talked about the possibility of good CVC4 bindings... just gotta find someone willing to do the work :-) the Z3 bindings are quite good. The idea with the Z3 based solver wasn't to integrate it into Cabal, but into something more like Hackage... to have the server do the version solving when possible and fall back (partially) to the client for local packages. The constraint set takes up a couple hundred megs of ram and would always be up to date (no cabal update required). And since it is so fast, you could build Hackage UI features that use the constraint solver.
Her name is Lindsey, not Linsey. Any way to fix the title?
Thank you. I've been trying find a solution on how to out how to push notification to my phone from Haskell for the last week. I'll definitely try this out. I was trying to find an XMPP solution, so I'll ask: Why not an XMPP solution?
Isn't one of the demo's for gloss-raster a julia set calculated with repa? Edit: no, it's [quasicristals](https://www.youtube.com/watch?v=v_0Yyl19fiI&amp;list=PL0EE84C90A8D65CD3). Same use case, though. And it's pretty fast.
Is it possible for containers to provide ordNub, as well as a rule that says "If you are running nub on something that is ordered, swap in ordNub"? It seems like something expressible, given everything that is done with stream fusion, but I don't know enough about rules to be confident...
Yes, you can use `--allow-newer=foo,bar,baz`.
https://twitter.com/lindsey
AFAIK you can't edit reddit post titles.
Rock on!
Ah. I misread. Well, thanks Marc. And SFML-control looks interesting, too! 
When you write `instance TstTypClss (TstTyp a) where` you are giving an instance for `TstTyp a` for *all* `a` at once. However in the instance you write `Tpl (0,0)`. The `0`s there have to be of type `a`. But `0` is not of type `a` for all `a`. It is only `a` if `Num a`. Restrict your instance to `instance Num a =&gt; TstTypClss (TstTyp a) where` and it will work.
Perfect. Thank you. instance Num a =&gt; TstTypClss (TstTyp a) where ... It works. I thought I've already tried. But, I have done this: instance TstTypClss Num a =&gt; (TstTyp a) where .... and finally got an error. Beginners fault, maybe. :-)
Since, we see that there is a lot of redundancy for Int, Integer, Float and Double. I also tried: instance Num a =&gt; TstTypClss a where tstFnc n 0 = 0 tstFnc n 1 = n tstFnc n x = n + (tstFnc n (x - 1)) But, this does'nt seem work. Illegal instance declaration for `TstTypClss a' (All instance types must be of the form (T a1 ... an) where a1 ... an are *distinct type variables*, and each type variable appears at most once in the instance head. Use -XFlexibleInstances if you want to disable this.) In the instance declaration for `TstTypClss a' 
Have been trying to figure out one of the templating libraries, so many thanks! Presumably, if I get the tag names wrong in the haskell code (e.g. write "kidd" instead of "kiddo") I get a runtime error? Does it make sense to try to have more static checking there? Or is it not worth the bother?
I can certainly see the desire for static checking. However, in order to do static checking you'd have to have a complete list of all tags that are valid in your markup language. We could make a list like this for HTML5, but it seems to me that non-standard tags are common enough that this would be more trouble than it's worth. I could be wrong though. I'm definitely open to the idea.
I was wondering about this. Even if you couldn't validate that a template runs properly—and that seems fair because it's an untyped language that's primarily runtime evaluated—you could perhaps provide two guarantees more easily (1) ensuring that your splice names don't get overwritten much like `(#!)` does and (2) providing early rejection of bad templates at runtime. The other idea might be to completely compile in templates using template haskell and ensure that they're valid at that stage. That completely destroys the runtime fluidity, though, and I feel like that's a large part of typical web design template workflows.
&gt; by trying to reimplement as much of it as possible This was my strategy for learning libraries too, then I tried lens..
Add the following to the top of your source file: {-# LANGUAGE FlexibleInstances #-} Or type the following in GHCi: :set -XFlexibleInstances Or add `-XFlexibleInstances` to your `ghc`/`runghc`/`runhaskell` call.
Wish I could have made it. Look forward to seeing the talk put up if it ever will be :)
Have you read [this post](http://twanvl.nl/blog/haskell/cps-functional-references) that was basically the precursor to `lens`? It helped me a lot to understand the library in a simpler and less general context so that I didn't have as big of a leap to make when Edward started generalizing things further.
Oh this was a few years ago, I've become a bit more comfortable with lens since then :)
Why not move Data.Set to base?
They all work, but you really should use the first one in 99% of cases
Oh yeah use case: if instead of stripping upper bounds we could set a higher upper bound that is still accounted for (but overrides any upper bounds specified in any `build-deps`), that would be useful for finding out which upper bounds does build? Don't expect any patches though. Sorry!
One thing you have to know is that "records of functions" are exactly what implements typeclasses internally in GHC. So typeclasses are really useful to provide syntactic sugar above this, but it's useful to know that a record of functions would gives, when compared to the use of typeclasses: - same expressivity (no power added by typeclasses, which may be the reason why Elm developers don't rush to implement them) - more boilerplate (you have to manually pass around your records instead of just letting the compiler stick the right method call at the right place...) - more flexibility (...which implies you can compute/pass around record of funcs at runtime. You can't do that with class instances, which are akin to static records) So when things get hairy with typeclasses, it might be a good idea to try to translate that to "records of functions", and see what it gives. It may result in code and error messages easier to understand (that's my opinion) by beginners. So without knowing it, you're doing with records of functions exactly what you'd be doing with typeclasses and existential quantification: transforming instances into manipulable records that can be stored in lists, passed around, etc. Have a look at the ConstraintKind extension when you have time: it can be used to simplify the use of existential quantification.
The "[Shakespearean templates](http://www.yesodweb.com/book/shakespearean-templates)", including hamlet templates for HTML/XML, have static checking. That works because interpolation is done at compile time using TH, and the interpolation variable are actual haskell variables. This allows checking for much more than just typos - interpolation is type-safe.
If your matrix is an list of list (like [[Int]]), then you can just traverse the outer list building up a list of (count :: Int, sum :: Int), then map over that to divide sum by count. That said, I don't think such a question qualifies for "Daily news and info about all things Haskell related: practical stuff, theory, types, libraries, jobs, patches, releases, events and conferences and more..." :)
Right... Sorry. Where do you think I can post such (functional programming theory related) questions?
 columnAverages = map average . transpose where average col = sum col / fromIntegral (length col)
Ok, that's the most straightforward solution, but it only works for "small-data". I'm looking for an "on-line" algorithm (though we can assume that the length of the prices list is known, or that we only need the sums). A solution would be this: columnSums = foldl add start where start = [0,0,0,0] add l1 l2 = zipWith (+) l1 l2 I guess I'm trying to come up with an idiom to "lift" the (+) into add automatically...
What? These kind of discussions happen here all the time.
What do you mean, lift (+) into `add` automatically? It doesn't get much simpler than columnSums = foldl1 (zipWith (+)) 
Ok thanks, that's exactly what I was looking for!
I've been wondering if someone developped for Haskell a CSS-like selectors based templating library (similar to enlive in Clojure, or to some extent Nokogiri in Ruby): it goes a long way to let the HTML developer stay within his domain, as template remains in **pure** HTML. You just fill-in the template like you'd alter a web page in javascript, by accessing tag through id, class, path, etc. and adding content/children tags.
No, it means "compiled during startup" at runtime, not compiled during compilation. It just attempts to find as many parts of your templates that are fully static as possible and pre-convert them to bytestrings prior to any actual render requests.
What I love about heist is it's simplicity. I can design all my HTML and CSS using standard design tools. Heist is simply functions that take in HTML nodes (including children) and output new nodes. No control flow or anything. Easy and powerful. Do you ever feel like there's too much going on in hamlet?
I'm ok and happy with you posting it here - but if you (or anyone) really think that this is an issue you could try haskells IRC channel or stack overflow
Indeed it happened often lately, but still. This does not match the topic of this subreddit, as written on the right of this page. Maybe it should be updated to follow the trend. As far as I am concerned, I come to /r/haskell to read news about haskell, and I go to stack overflow to ask technical questions or answer to them. In my opinion, topics matter a lot to preserve good visibility. Also, here on reddit, /r/haskellquestions was created specifically for "providing assistance for programming and solving problems in the Haskell functional programming language. Here you can also ask for feedback on your code and ideas.". Sound appropriate, doesn't it ?
How does this feature actually work? Is the version of the package left unchanged but its cabal file silently changed - if so how does the cabal-install client know to grab a newer version?
 import qualified Control.Foldl as L import Control.Applicative prices :: [[Double]] prices = [[10,4,13,20], [12,3,11,25], [9,1,16,24]] result = L.fold ((/) &lt;$&gt; L.sum &lt;*&gt; L.genericLength) prices instance Num a =&gt; Num [a] where fromInteger = repeat . fromInteger (+) = zipWith (+); (*) = zipWith (*); (-) = zipWith (-) abs = map abs; signum = map signum instance Fractional a =&gt; Fractional [a] where fromRational = repeat . fromRational (/) = zipWith (/); recip = map recip It would probably be better to make such a `Num` instance for `ZipList` (using the usual 'applicative numbers' approach, where `(+) = liftA2 (+)` etc.). If we add a similar `Num` instance for `L.Foldl` , we can just [write](http://www.reddit.com/r/haskell/comments/1jo9gm/haskell_for_all_composable_streaming_folds/cbgo6fn) result = L.fold (L.sum / L.genericLength) prices or if we are summing `ZipList`s result = L.fold (L.sum / L.genericLength) (map ZipList prices)
We already do both of your points. When using compiled heist, (#!) does exactly what you're talking about because all splices must be bound at load time. We've been doing #2 since day 1 at the level of syntax checking. You don't have to actually request the template. It gets loaded and parsed when Heist is initialized (usually when your application starts up) and if there are any errors, you app won't even run. To me this gives us most of the advantages of compiled templates while still preserving the benefits of runtime templates. &gt; The other idea might be to completely compile in templates using template haskell I have definitely thought about doing this. While not the primary purpose of Heist, it would allow people to opt for more static checks if they were ok with losing the runtime reloading speed and flexibility. &gt; That completely destroys the runtime fluidity, though, and I feel like that's a large part of typical web design template workflows. Yep, we have found the fast change-reload-test loop to be a pretty big win as applications get large because you avoid GHC's performance penalty. See this [screencast](http://devblog.soostone.com/posts/2013-06-17-snap-template-reloading.html) for a real-world example with one of our large projects.
It's not compile-time static checking, but compiled heist can translate to load-time static checking, which IMO is just as good in practice. The perfect case in point is the (#!) function mentioned above. (#!) exits with an error if you previously defined a splice with the same name. This would probably be bad behavior if you were using interpreted heist because it might not happen until you're serving a page request. But in compiled heist, it's guaranteed to happen when your app loads which gives you very high confidence that you've eliminated that class of bugs.
Lens is kind of fun to reimplement once you get your head around some of the weird class overloading parts. You can also try implementing pure profunctor lens for a bit of a different challenge.
I feel like it'd be fairly reasonable to implement this in the same way that Heist is implemented—it just has a built-in element *name* predicate on your Splices map, you could replace that with something more general as long as you handled recursion right. I'd doubt that the Heist team would want to implement that though.
Type safety is arguably not as big of a win in this case because at the end of the day you're interpolating bytes. Now I'll grant that this same argument could be applied to argue against Haskell for programming in general, but I think that it's not quite the same thing due to the fact that templates lie at the end of the computational pipeline. With Heist you can still end up getting all the benefits of type safety in the Haskell code that implements your Heist splices, which in my experience is good enough.
Yeah, if only this site had some sort of built in mechanism for the readership to indicate whether they valued each submission or not...
Here's an augmentation of [random_crank's answer](http://www.reddit.com/r/haskell/comments/1spmhc/most_elegant_way_to_express_matrix_column_average/ce01g9h) using the `ProductProfunctor` typeclass. I haven't put a package for product profunctors on Hackage yet, but I will at some point as it seems remarkably useful. The first thing to note is that for type safety we should be summing a list of tuples rather than a list of lists. Now some explanation about product profunctors: `p4` allows you to take a 4-tuple of product profunctors and turn them into a product profunctor on 4-tuples. It's strongly analoguous to `liftA4 (,,,)`, but a bit more general. Using `p4` we can combine four "aggregators" (values of `Foldl`) into an an aggregator on a 4-tuple. `average4` and `averageAndSum` show how we can neatly compose the aggregators to get new ones. import qualified Control.Foldl as L import Control.Applicative import Data.Profunctor prices :: [(Double, Double, Double, Double)] prices = [(10,4,13,20), (12,3,11,25), (9,1,16,24)] average :: Fractional a =&gt; L.Fold a a average = (/) &lt;$&gt; L.sum &lt;*&gt; L.genericLength average4 :: (Fractional a, Fractional b, Fractional c, Fractional d) =&gt; L.Fold (a, b, c, d) (a, b, c, d) average4 = p4 (average, average, average, average) averageAndMax :: (Fractional a, Fractional b, Ord c, Ord d) =&gt; L.Fold (a, b, c, d) (a, b, Maybe c, Maybe d) averageAndMax = p4 (average, average, L.maximum, L.maximum) result1 = L.fold average4 prices -- *Main&gt; result1 -- (10.333, 2.666,13.333, 23.0) result2 = L.fold averageAndMax prices -- *Main&gt; result2 -- (10.333, 2.666, Just 16.0, Just 25.0) -- Library code class Profunctor p =&gt; ProductProfunctor p where empty :: p () () (***!) :: p a b -&gt; p a' b' -&gt; p (a, a') (b, b') -- Ideally this should have been given by the foldl package already instance Profunctor L.Fold where dimap f g (L.Fold u s r) = L.Fold (\x a -&gt; u x (f a)) s (g . r) instance ProductProfunctor L.Fold where empty = pure () l ***! r = (,) &lt;$&gt; lmap fst l &lt;*&gt; lmap snd r p4 :: ProductProfunctor p =&gt; (p a1 b1, p a2 b2, p a3 b3, p a4 b4) -&gt; p (a1, a2, a3, a4) (b1, b2, b3, b4) p4 t = let (a, (b, (c, d))) = nest t in dimap nest unnest (a ***! (b ***! (c ***! d))) where nest (a, b, c, d) = (a, (b, (c, d))) unnest (a, (b, (c, d))) = (a, b, c, d) 
I was talking with a friend about this problem and he recommended this service as he uses it from a php based system. I decided to reimplement it in Haskell so it would integrate better into another project which is also in Haskell. I only found out about XMPP when I was nearly finished.
I'm aware of how they work, though a little rusty still on how compiled heist is working behind the scenes. Tangentially related, is it the case that Heist isn't code? They're a reduction scheme, so I feel like you could definitely encode lambda calculus in your Heist templates.
And if you want averages you can do the fantastic things people suggest, or just cheat and do it the easy way: columnAverages = (\ (n:xs) -&gt; map (/n) xs) . foldl1 (zipWith (+)) . map (1:)
What's the easy way to get the averages and the sums and the maxima together?
I'm not sure there is an easy way for that. It just happens that the average problem had an easy, non-general solution.
I can see eschewing the final version if you're not used to thinking in point-free style, but I can't abide not going at least as far as the first version I offered. Sure, your code's inefficient, but that's not my point. My main complaint is that your code is misleading— you're throwing away type information and then you're forced to get it back in an arbitrary way. This is an anti-pattern in any language, but especially in Haskell! In your code you do the following: minimum' = toSingleton . minimumBy comp . M.toList (minK, minV) = head . M.toList . minimum' $ m Even performing the *exact same thing* performance-wise, consider this code instead: minimum' = minimumBy comp . M.toList (minK, minV) = head . M.toList . toSingleton . minimum' $ m The only difference here is moving the call to `toSingleton` from the definition of `minimum'` to the call site. But now that we've made this change, the ugliness becomes blatantly obvious. Because it turns out that: head . M.toList . toSingleton == id You already have the minimum key–value pair in hand, but you throw this type information away, pretending you only have a whole set of key–value pairs. But now that you have a set of key–value pairs and you can only handle one of them, you're forced to resolve the ambiguity by picking one arbitrarily. By removing `(head . M.toList . toSingleton)` you will improve the legibility of the code significantly. And along the way you're make things faster and remove the call to a partial function, both of which are good things but neither of which was my main problem.
Don't be sorry. Your question was fine judging by all the community codegen. And it shows how "helpful" is hardwired into so many Haskellers.
He means forkIO is amazing...
I realized it too now. :) Maybe I was thrown off by [the Dude](http://blog.estately.com/assets/TheDude.jpg)’s possession of “Amazing”. I wonder what that stuff costs a kilo… ;)
yes sir, sorry for typo ;O
No problem. I noticed you’re not a native speaker. I’m neither. I’m loving your characters/jokes in there, by the way. Keep up the joy and motivation! :)
Nitpicks: `Control.Concurrent` is a module in the `base` package. Also, all the redundant `do`s are making me dizzy :) fanOfClassicMusic = do forever $ do putStrLn "Dude Garbage is garbage" instead of: fanOfClassicMusic = forever $ putStrLn "Dude Garbage is garbage"
Loved it, so easy to read!
The only real control flow I use in hamlet are the "$forall" for loops and the "$if .. $else". The rest just behaves as I would expect from a simple markup template. There is the "@{}" syntax if you want to render a url and want to take advantage of static checking (e.g. `&lt;a href=@{HomePagePath}&gt;`) but you could as easily just type in the url yourself.
Sure was created for that purpose. But it has 285 readers, while this one has 13 thousand. So while that's the ideal solution, it is not the practical solution for someone asking advice. 
i like "do" it lets me think that "i get into monad naow". Ben doing Erlang so i'm kinda still in partial sequential mindset.
I hate to say it: but it looks horrific, is redundant and will cause most who read it to think that you don't really know what do notion is about (whether true or not) and I would recommend avoiding it if you want to collaborate with other people.
Maybe he invented it quite some time ago? This algorithm seems to have been around for around a decade or so now.
Woah... Thank you for that.
It makes it easier to drop in other statements before/after.
Does this surprise you? Prelude&gt; do 3 3 I wonder, what monad do you get into thaen? =)
Pandoc is freaking awesome. I used it to quickly add a bunch of responsive snippet verification into a project I was writing up by creating a tool I called "Pandoctor" in about 50 lines of code to execute arbitrary filter scripts on code blocks. REALLY useful. https://github.com/sordina/pandoctor/blob/master/pandoctor.hs
This reddit gets about three new topics a day. It seems premature to split it into more granular subreddits.
Do I do the wrong assumption if I think to it as something more similar to opengl, aka much lower level and essentially 3d and transformation based?
Pandoc is an excellent tool. I have mostly used it in combination with another haskell project called gitit. Both of which make doing documentation and conversion so easy.
i'll skip over the exec unless i see the whole script. you could use `{ A &amp;&amp; B; } || C` if you want to group to do "Run A, then Run B; if either A or B fails, Run C" which is how i read you comment. this might pass shellcheck cleanly but i have not checked yet. i will have to withhold comment on the expansion in single quotes; it may be necessary and intentional, or there may be a different or other idiomatic approach. redirection occurs with a strict precedence, and pipelines can be a single command; the bash manual says this. ` A pipeline is a sequence of **one or more commands** separated by one of the control operators | or |&amp;.` emphasis mine. so it is possible that you could unintentionally truncate a file before reading it on stdin. the stdout handle is opened first, truncating the file to be blank (unless you use `&gt;&gt;` operator) and then it is read later by the `&lt;` operator. if the redirection operators had `|` or `|&amp;` between them then it is likely safe and this bug should be reported on github. when throwing away variables from a read operation taking advantage of wordsplitting performed by `read` on IFS, _ is often used because it is later overwritten automatically by bash. i think shellcheck acknowledges this. word-splitting has a very few limited uses, but in most cases it is preferred that you use an array of many elements instead of a string scalar variable of character separated values (especially when that character is just a space). i would recommend trying to rework this using arrays and running it through shellcheck again.
Isn't this a religious war ? It's like having curly braces or not around a single statement after an if clause in C. if (condition) { do_something; } vs. if (condition) do_something; sure the braces are redundant but some people love them. 
AFAIK GLFW (like GLUT) is a windowing system for OpenGL which captures input as well, but does nothing other than these. SFML embodies these features, and also has audio and image handling capabilities, and runs on all three major platforms as well. So I don't see why would one want to use GLFW instead of SFML. *(On a second thought, I see one possible cause: if one can't generate a build environment with cmake for CSFML, so he can't build HSSFML, like me... I could use a little help. Tried to follow the instructions [here](https://github.com/SFML-haskell/SFML/wiki/Installation-on-Windows), but it seems that cmake doesn't find SFML. I can't paste the exact error message, as I'm writing from work now, but after I get home, I can.)*
It wasn't until I started browsing the source code that I realized that I could click on the image and get it to start.
It appears to be compiled with `haste`.
And the font styling specifies Comic Sans MS.
The difference would be that something like if (condition); do_something; is less noticeable wrong than if (condition); { do_something; } In addition adding statements is less likely to be wrong if you always have the curly braces. On the other hand in haskell you can't add an extra expression and get it to still compile, so there is no need to add a redundant do.
Comic Sans has a long and healthy tradition in the Haskell community: https://twitter.com/bodil/status/405770118783053824/photo/1
I had troubles installing csfml myself because documentation is insufficient. i'm from the cellphone, but I'll send you the fix as I get home. Btw is as simple as putting findSFML.cmake inside /usr/local/cmake/share/Modules (not sure about the path, but you get the gist)
Well [/r/haskellquestions](http://www.reddit.com/r/haskellquestions) is pretty dead, with barely a post a month and about 2 orders of magnitude fewer subscribers.
I'm using Pandoc to generate my website.
One excellent feature the post doesn't mention, but which I have made use of several times, is how easy it is to add your own transformations on the internal Pandoc format, using the generic traversals in [Text.Pandoc.Generic](http://hackage.haskell.org/package/pandoc-types-1.12.3/docs/Text-Pandoc-Generic.html). In just a few lines of code you can express things like "run this String -&gt; String function on all the code blocks".
And this one: Prelude&gt; do return 3 3 
Pandoc is the bomb-diggity. It seems like it's actually one of the few libraries that's used widely across a lot of different people, not just in the Haskell community, which is cool. I actually discovered it the other day because I'm moving my blog to hakyll and it uses pandoc for html conversion. I love it. It's amazing. Edit: This is off-topic, but something I've been thinking about a lot lately. &gt; Pandoc has separated the conversion of documents into readers and writers[...] Does it seem to anyone else that I and O are not necessarily connected? I thought of this idea a couple of months ago and ever since then I've been seeing the genearl idea scattered in different areas. For example, Pandoc having Readers and Writers that do different things, and Pipes having Producers and Consumers that sort of have the same function. From a program standpoint, reading data from a user at arbitrary times can break a program a lot more easily than writing data to the console can. Why is it that we necessarily connect I &amp; O everywhere they're mentioned? Further, why don't we have an I monad and an O monad? Why are they necessarily connected? Just curious.
&gt;Endosemitrailer lol
It is part of the doge meme AFAICT
Side note: the joke reminded me of one Edward's own jokes, which is the type signature for the `_Pure` prism in the `free` library: _Pure :: forall f m a p. (Choice p, Applicative m) =&gt; p a (m a) -&gt; p (Free f a) (m (Free f a))
Could also be bias from another language. In Ruby, code blocks can be surrounded by either {} or more commonly do/end. There, it doesn't matter if your block is 1 line or many, you start it with the same syntax. Could be that haskell's do notation just looks like a whitespace-significant variation of that.
Ahh, yes. I agree that namespaces provide a decent solution to this problem. Hit me up on IRC and we can discuss the possibilities in more detail.
Likewise.
GHCi executes `Monad m =&gt; m a` as `IO a` and prints the result. GHCi simply prints the result of a pure expression. Therefore `do 3` and `do return 3` produce the same output in GHCi, even though the first expression has type `Num a =&gt; a` and the second has type `(Monad m, Num a) =&gt; m a`.
For those who, like me, stared dumbly at the picture, looking for a detail they missed and wondering desperately why they weren't getting the joke: click on the picture.
I don't think I understand the joke here. Is it the fmap in the type variables?
I did that (I'm on Windows, btw), but that didn't help. I even defined SFML_ROOT in cmake (and set it to SFML's dir), and copied every lib file to SFML/lib. Cmake finds libGlew and things, but not SFML. Here's the error message: Could NOT find SFML (missing: SFML_SYSTEM_LIBRARY SFML_WINDOW_LIBRARY SFML_NETWORK_LIBRARY SFML_GRAPHICS_LIBRARY SFML_AUDIO_LIBRARY)
Yes. I thought it was clever because lenses are typically universally quantified to work over all functors (although this one wasn't because it was a prism), so `forall f m a p` is sort of like a joke saying it works for all `fmap`s.
 Prelude&gt; do 3 :: Int 3 Okay, there is some weird GHCi magic going on here. And it's obviously cheating. Prelude&gt; do x &lt;- 3 :: Int; x ERROR I think it is just desugaring `do m` to `m` without actually checking Monad stuff.
This one made more sense to me than the other one. GHCI defaulting will specialize `return 3` to `IO Int`; it will run the IO action (which does nothing) and print the result: 3.
&gt; i will have to withhold comment on the expansion in single quotes; it may be necessary and intentional, or there may be a different or other idiomatic approach. What? There is no better approach. Quoting characters like $ is the entire purpose of having single quotes in the language. The alternative is \\-escaping. The whole point of '' is that slash-escaping is an error-prone burden. &gt; emphasis mine. so it is possible that you could unintentionally truncate a file before reading it on stdin. the stdout handle is opened first, truncating the file to be blank (unless you use &gt;&gt; operator) and then it is read later by the &lt; operator. if the redirection operators had | or |&amp; between them then it is likely safe and this bug should be reported on github. Like I said, I was redirecting stdin and stdout to the same tty. Nothing is being truncated because the file is a character device that cannot be truncated. Also, it sounds like you are confused about the pipe. That isn't "safe" in the sense of allowing the first process in the pipeline to read the file before another process truncates it. The processes in the pipeline are all started at once (before any terminates), and only after the shell opens the files to redirect. &gt; word-splitting has a very few limited uses, but in most cases it is preferred that you use an array of many elements instead of a string scalar variable of character separated values (especially when that character is just a space). i would recommend trying to rework this using arrays and running it through shellcheck again. Arrays are specific to bash, zsh, etc. There are no arrays in POSIX shell, except for $@.
This is a very approachable and newcomer-friendly post. Great job!
Changelog 1.0.0.0 * Added support for Unicode 6.3.0 to case conversion functions * New function toTitle converts words in a string to title case * New functions peekCStringLen and withCStringLen simplify interoperability with C functionns * Added support for decoding UTF-8 in stream-friendly fashion * Fixed a bug in mapAccumL * Added trusted Haskell support * Removed support for GHC 6.10 (released in 2008) and older 
I think the Haskell community is too intelligent for tradition. As tradition is basically when people forgot the actual reasons, and play a game of monkey see, monkey do. And that’s not exactly what I think of when I think ”Haskell community”.
You know you’ve learned too much Haskell, when “a monoid in the category of endofunctors” makes perfect sense to you, and seems simple enough to use it to explain monads to others. Yes, that actually happened to me. And yes, I know it was originally a joke. That makes it even worse. I love my Haskell though. :)
If I might, the particular bug in `mapAccumL` was found using LiquidHaskell... https://github.com/bos/text/pull/50
&gt; `a a r d v a r k`
Just to state the obvious (sorry for that): is SFML installed at all? On unix systems once solved the cmake error it worked like a charm for me. My suspect is that findSFML.cmake might be misplaced.
Actually, there's no cheating! You're right when you say it's desugaring `do m` to `m` without actually checking `Monad` stuff, because that's what [the spec says to do](http://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-470003.14). The desugaring rules are: do {e} = e do {e;stmts} = e &gt;&gt; do {stmts} do {p &lt;- e; stmts} = let ok p = do {stmts} ok _ = fail "..." in e &gt;&gt;= ok do {let decls; stmts} = let decls in do {stmts} In fact, just that first rule is enough: it says that the term `do 3` is identical to the term `3`. There's nothing in the term `3` that screams `Monad`, so there's no specialization; it just has type `Num a =&gt; a`! In comparison, `do x &lt;- 3 :: Int; x` (although presumably you meant `do x &lt;- 3 :: Int; return x`?) desugars to `3 :: Int &gt;&gt;= \x -&gt; x`; since `(&gt;&gt;=)` has a type that mentions `Monad` then suddenly you have a problem.
What is worse is I don't remember consciously choosing to write the explicit `forall`. =)
`b a c k s t a b` got a chuckle. =)
No need for sorry for stating the obvious, better to rule out stupid mistakes early :) If by "being installed" you mean that I compiled them and copied them to the appropriate places (as per the instructions) then my answer is yes. I've copied the findSFML.cmake file to the same directory where the other findXXX files are in cmake-2.8 directory in Program Files (and instructions on the SFML forum say exactly the same).
`mapConcurrently` is such an easy way to parallelize an existing program. We're using it a lot.
You'll probably want the signature of tstFun to be: tstFun :: (Integral b) =&gt; a -&gt; b -&gt; a Eq seems unecessary and Integral will make tstFun "asdf" 3.14 a compile error instead of an infinite loop
Well it was originally in *Categories for the Working Mathematician* because it does make sense to explain them that way to someone when they're reading a Category Theory textbook. It was a joke that made it a meme today though.
Finally a titlecase function. I must have ad hoc defined it 10 times already. Thanks!
&gt; * Added support for decoding UTF-8 in stream-friendly fashion Which should pave the way for pipes-text!
You should thank Ben Gamari, who specifically requested this to make `pipes-text` efficient
&gt;(&amp;lt;%$=$) :: (Semidetachedoid (Endofunctoroid p (Preendocobicycle u b w'' z g') (-&amp;gt;) c'') (-&amp;gt;) s) =&amp;gt; Biendofunctor (Cobimonocycle a p (Proendoprism s' (Representableoid (Semirepresentableoid b b c r a d i o) (-&amp;gt;) (Biendosemimonoid (Preendomonoid (-&amp;gt;) (Semidetachedoid (Endosemidetachedoid (-&amp;gt;) (Semibisemiprobono g' (Birepresentableoid (Comonoid (Birepresentableoid v a j) w' f m)) l e') n m'' b) a' (Endoprobonooid (Semiarrow b') (Biendoprestrong o (Endosemidetachedoid x' i (Semiprobonooid (-&amp;gt;) (Biendostar q)))) l e')) (-&amp;gt;) o (Strongoid (Semimonocycleoid d i d a t) u''))) (Endostaroid a a r d v a r k) z) (-&amp;gt;)) d (-&amp;gt;)) z (Bipreendocategory s (-&amp;gt;) k) Are these generated via markov chain? Best parts: * `Proendoprism ` * `Semirepresentableoid b b c r a d i o` * `Endostaroid a a r d v a r k`
I had fun reading all the versions that are in the hackage list to my roommate. &gt; Good going guys! We've reached an arbitrary boundary! 1.0.0.0! 
I really like async. When I first saw it, it looked a lot like the Async library for OCaml, which I am very used to using, but in fact it is quite different. In particular, the exception handling in the async package is far superior. Also the fact that real GHC threads are involved instead of just atomic jobs in some scheduler all in the same, non-preemptive thread is a huge plus.
:)
It's super nice for organizing scoped definitions without hitting layout rules or lifting to a common `where`. Intentionally silly example: foo x = case x of Nothing -&gt; 50 Just x' -&gt; do let y = x'**(1.5) let z = y - x' x' * y * z
well, the better approach may be that you don't need to evaluate an expansion there at all. often this problem comes from sprinkling an eval somewhere in your script and subsequently realizing you have to suppress evaluations here or there to make the script work correctly again. again, there may be other approaches. it just depends on how you put the script together and what this line is doing exactly. idiomatic shell doesn't suppress expansion of vars or exprs except when 100% unavoidable so we have come up with a lot of solutions that often turn out easier to handle in the long term. since you're dealing with a character device, sure, that's grand. you probably can't screw that up since it shouldn't even be seekable or anything. however, shellcheck currently makes no effort to recognize special files and treat them as such. in this case, just ignore the error. sorry that it annoys you, but patches will be welcome. arrays are not posix (i wouldn't call them specific to every shell...bash, zsh, ksh, mksh...yeah nearly every shell as arrays and identical access syntax) and that can be a problem. in some cases, we actually do use `"$@"` as a safe alternative. the point to note here though is that you probably shouldn't be writing POSIX scripts. they are very rarely necessary, and you should evaluate your needs to verify that you HAVE to write a painful POSIX-adherent script instead of being able to use modern, good features line `[[` and arrays. if i saw your script there are probably many changes i would make to it. might even clear up most of your problems here. adhering to POSIX very strictly is great when you are guaranteed to only have `/bin/sh` available and nothing else. otherwise, people often take advantage of bash3/bash4/perl/python/ruby whenever they can because POSIX is a pain in the dick.
Could you elaborate why Haskell's Async exception handling is better than OCaml's async exception handling? AFAIK OCaml's Async uses monitor trees reminiscent of erlang with its supervision trees. I don't see a big problem with the approach and I see at as a big improvement over Lwt that "inlines" the error in the monad. Most of my experience is using Lwt however so I could definitely not be aware. As for the comment about GHC threads, it's a trade-off really since Async in OCaml guarantees code in between binds will not be interrupted. But it's true the cost is steep since it doesn't play well with the outside world at all.
Wow, you sure are annoying. &gt; well, the better approach may be that you don't need to evaluate an expansion there at all. often this problem comes from sprinkling an eval somewhere in your script and subsequently realizing you have to suppress evaluations here or there to make the script work correctly again. As I said originally: "I'm using single quotes exactly because I want to prevent $ from expanding. I'm passing a shell script via chroot dir sh -c 'script'. Is this going to create a warning every time I do that??" &gt; the point to note here though is that you probably shouldn't be writing POSIX scripts. they are very rarely necessary, and you should evaluate your needs to verify that you HAVE to write a painful POSIX-adherent script instead of being able to use modern, good features line [[ and arrays. As I said, "I'm working on initramfs shell script." Do you think I should put bash on the initramfs? Please. &gt; if i saw your script there are probably many changes i would make to it. might even clear up most of your problems here. Haha, gee, thanks for the offer.
Oh, weird. I've always use let-in for that sort of thing, but I never really liked the "in" part. foo x = case x of Nothing -&gt; 50 Just x' -&gt; let y = x' **1.5 z = y - x' in x' * y * z Syntax highlighting makes it slightly less annoying, but the "in" just feels out of place and superfluous.
(To distinguish between them, I will refer to them as hasync and asyncml.) I am not familiar enough with LWT to intelligently remark on its exception handling. I have experienced at least the following problems with exception handling using asyncml: * Exceptions can go uncaught very easily. This results in either crashing the program or errors being silently hidden, depending on context. * Exceptions that are caught are totally out of band from the control path that set up the job in the first place. It's harder to tell how to handle it, and it's extremely hard to tell where it came from. Forget stack traces. You have nothing to go on but printf debugging, assuming you can duplicate the conditions of the error at all (often difficult in concurrent systems). * You have to handle exceptions differently depending on if you think some scope might throw at most one or possibly more exceptions. The default is at most one, after which exceptions will not be caught by the same monitor. If you need to catch them all, you have to use a stream. * If there is an exception raised in some job, any deferreds that were waiting on that job in order to become determined will just block forever. If you weren't loud enough about the exception to make sure the program dies proptly or notifies you of what happened, parts of your program just hang mysteriously. * Monitors can be expensive to create over and over. This can be especially bad if they are nested within each by some long running recursive function, leading to a memory leak. hasync handles exceptions in the way I think you said you disliked about LWT, by storing the exception as the result of the async job. I don't know why you don't like it, but it's so much easier to work with to me. Unlike how I imagine LWT works, you don't have to do anything special to catch the exceptions; it just works out of the box with no extra knowledge required. As for the preemptive threads of hasync, you can get an even better guarantee than asyncml's of useful atomicity just using STM. The preemptive threads are important also because CPU can block just as badly as syscalls; the fact that a CPU-heavy job stalls the whole system can be disastrous, and the fact that the OCaml runtime doesn't schedule threads fairly means you can't even rely on In_thread to help.
Such generosity! 
On the question of the I/O monads (I haven't thought about it much so I may be completely off-base) - most of the time, a program will be doing both input and output. There are two ways (that I can see) to get data from the I monad to the O monad. First, a `runI :: I a -&gt; a` function, but that breaks referential transparency. The other way is to have a monad transformer `IT (O a)`. This seems like a better way, but at this point, you aren't really gaining much over `IO` as it stands now.
Note (I just realized this the other day) that recent versions also include optimized, more specialized operations for these purposes in [Text.Pandoc.Walk](http://hackage.haskell.org/package/pandoc-types-1.12.3/docs/Text-Pandoc-Walk.html).
Proendoprisms sound super-useful!
Could you tell us who "we" are? :)
I'd love to, but it's very new to me and I'm not sure I'll find enough time to tackle it :(
Ok, I was asking because CMake must take care of copying them for you, so you don't need to copy anything anywhere. Unfortunately I'm not on windows, I'll see if I can spawn a virtual machine later this weekend so that I can reproduce the error :)
...but what justifies the super-major version bump from `0.11.3.1` to `1.0.0.0` which [requires a lot of packages to update their upper bounds](http://packdeps.haskellers.com/reverse/text)? The changes seem to justify only a minor bump according to the PVP afaics.
That would be greatly appreciated, many thanks.
API is stable but you have to jump 1.0 some day
...why do you *have* to (except for aesthetic reasons)? After all, major bumps have quite a cost associated with it, especially the more popular a package is, and should be only performed if there's an actual reason for in terms of compat-breaking API changes.
PVP reserves both of the first two parts of the version number for the major version. It is useful for stable packages to be able to say "these versions belong to the same series (1.x), even if PVP dictates that the A.B must increase with this change". It has no meaning to PVP but it has a social meaning. Happstack changed to 6.0 after 0.5 which made it possible to continue with 6.1 rather than going from 0.6 to 0.7. PVP might say you have to bump the major version but you might still want to communicate that the essence of the API is the same and the changes aren't major. 0.x is for fast-evolving early development.
I used to use it to get emacs to indent nicely for me. Not the greatest reason, I know...
Oh sorry :) "We" in this case is [Silk](http://silk.co).
Side note, I often point to pandoc when people ask how practical compiler classes are. Here's a tool that many people use, find extremely helpful, and is exactly a compiler: go from one document language format to another. They have parsers and intermediate representations and pretty printers and all that stuff you learn in your compiler design class at university.
This is why I use bounds of the form `text &lt; 0.11.3.1 &amp;&amp; &lt; 2` when the current version is zero. I expect that the current API will be stabilized into 1.0.
What you propose is a catch-22: a package cannot upgrade its major version number until its stable, but once its stable it will break all downstream packages to upgrade its major version number.
They look like randomly generated trees. However, the way they generate the trees doesn't seem to decay with depth, so whether or not it terminates is up in the air.
&gt; This is why I use bounds of the form `text &lt; 0.11.3.1 &amp;&amp; &lt; 2` when the current version is zero. I'm confused... this kind of (upper-only) bound does not seem to make any sense to me... isn't that effectively equivalent to depending merely on `text &lt; 0.11.3.1`? :-/
Could be a typo. They may have meant `&gt;` in the first term.
Aww you took my name! https://github.com/mgsloan/themplates , that's ok, I wasn't really using it anyway. That code, which could really use a cleanup, got inlined into http://github.com/mgsloan/haskell-quoter anyway Anyway, this is a better use of the name - cool stuff!
Great, I also made a project templating program. It's good study for me to see how similar program designed and implemented in two ways. http://hackage.haskell.org/package/hi https://github.com/fujimura/hi
I have a major crush with your series. Thanks for writing them.
I meant to type `text &gt;= 0.11.3.1 &amp;&amp; &lt; 2`.
Seriously, one of the best things I read all day. When I get a chance I gonna go back and read all the ones from last year.
Great series! Make sure you do [ad](http://hackage.haskell.org/package/ad) one of these days. It's an impressive package.
When doing the acid-state happstack crashcourse nobody ever explained me why I need all these typeclasses while `Typeable` seemed sufficient to me and I still don't have the answer: http://happstack.com/docs/crashcourse/index.html#ixset-a-set-with-multiple-indexed-keys newtype Title = Title Text deriving (Eq, Ord, Data, Typeable, SafeCopy) newtype Author = Author Text deriving (Eq, Ord, Data, Typeable, SafeCopy) newtype Tag = Tag Text deriving (Eq, Ord, Data, Typeable, SafeCopy) newtype WordCount = WordCount Int deriving (Eq, Ord, Data, Typeable, SafeCopy) edit: yes I get why it is `Ord` and `Eq` . was pointing at `Data` and `SafeCopy`
I would love to learn about `ad`, but I think that's an area of programming that's pretty heavily outside my area of expertise :( I'll have a think though.
`SafeCopy` is needed for anything that acid-state needs to store, which includes the state itself and the arguments to any updates and queries. I'm not sure you actually need `Data` though. It's possible Jeremy added those derivations on reflex. Have you tried compiling without them?
so why do we use `deriveSafeCopy` in some places and `deriving (SafeCopy)` in others?
Thanks, I enjoyed it. Talk is given by the auther of the Sodium framework. I only have experience with reactive-banana but I'm delighted to see that sodium is similarly designed so I might give that a try.
I think Data is needed for IxSet, but not for acid-state.
What does Safe mean in this context, btw?
It will terminate if every node has strictly less than one child on average, assuming an independent identically distributed distribution of children for each node. http://en.wikipedia.org/wiki/Galton%E2%80%93Watson_process#Extinction_criterion_for_Galton.E2.80.93Watson_process
I'm not proposing at all to upgrade the major version just to communicate that the API has been frozen. Quite the contrary I'm suggesting to remain at major version `0.11.*` if the API is stable. With the PVP in place, there's no need to artificially perform major version bumps for the sole purpose of communicating something redundantly that's already been expressed by the PVP's versioning scheme and at the cost of forcing every package depending on `text` to update their upper bounds for no good reason. Btw, as a somewhat unrelated example, see TeX whose "API" is considered frozen and has remained at version `3.1415926` for some years now, instead of going for an arbitrary discontinuous jump to version `4.0`.
Why are you deliberately ignoring the PVP? What makes you believe that your code is going to still compile against the API of e.g. `text-1.1.*`, `text-1.2.*`, ..., `text-1.12.*`, ...? After all, if `text-1.0.0.0` has to make a major version bump to `text-1.1.0.0` it'll be because it contains potentially API breaking changes.
When it someday causes me a problem I'll look back on this comment and rue my arrogance.
My univerity does an introductory course in haskell and also offers the Languages and Compilers course in haskell. so that's a good thing I guess :) http://www.cs.uu.nl/wiki/TC/WebHome
I don't think the average is less than 1 identically distributed, as it branches pretty wildly. Usually with `Arbitrary` trees in Haskell we instead control for explosive growth by passing in a size parameter and decreasing the branching factor gradually or controlling for the overall size directly rather than using a terminating Galton-Watson process.
well the latter works only if you use newtype deriving and you're newtyping over something that already has it. i'd imagine you need the former in other circumstances.
We also maintain two haskell compilers. Namely, UHC (http://www.cs.uu.nl/wiki/UHC) !!!which compiles to javascript as well!!! and Helium , a dialect of haskell which focuses on cleaner error messages . Helium is used in our haskell introductory course to get students used to the paradigm http://www.cs.uu.nl/wiki/Helium/WebHome Here is the material of our introductory course. though the material is in dutch, sadly enough: http://www.cs.uu.nl/wiki/FP/WebHome
Thanks for the general tips for using Async. You probably didn't mean as such but I think I will definitely find it helpful. &gt; hasync handles exceptions in the way I think you said you disliked about LWT, by storing the exception as the result of the async job. I don't know why you don't like it, but it's so much easier to work with to me. Unlike how I imagine LWT works, you don't have to do anything special to catch the exceptions; it just works out of the box with no extra knowledge required. Lwt has a type `thread` that is analgous to `Deferred.t` with one major difference. It has another state to denote failure. So basically something like `type 'a thread = Determined of 'a | Undetermined | Failed of exn`. The problem with this approach in Lwt is that it quickly becomes more difficult to deal with exceptions properly since you must now remember to handle normal OCaml exceptions and Lwt Failures. (There is a syntax extension to help with this but it's ugly). For example here's a [gotcha](https://plus.google.com/+AlessandroStrada/posts/5Vk3dcP6T4k) (Sorry for the G+). Overall my main gripe with monadic error handling in OCaml is that it quickly becomes cumbersome when the number of monads grows. I end up being lost in a sea of local module opens and massive, incomprehensible type errors. OCaml doesn't have the monadic machinery such do notation, typeclasses, monad transformers to make it convenient like Haskell. &gt; As for the preemptive threads of hasync, you can get an even better guarantee than asyncml's of useful atomicity just using STM. Yes this is another advantage of Haskell, the various concurrency tools end complementing each other. Overall, Haskell's concurrency story is a lot more coherent. &gt; the fact that a CPU-heavy job stalls the whole system can be disastrous, and the fact that the OCaml runtime doesn't schedule threads fairly means you can't even rely on In_thread to help This is true but you really can't blame OCaml for this since it lets the OS schedule threads.
OK, I have succumbed to the "monad tutorial fallacy". As part of learning Haskell, I guess everyone must write one as part of the process of understanding monads. Looking forward to feedback to help my understanding grow.
While I appreciate another tutorial, I have no idea why this would be a fallacy.
[And a Google search on the term didn't help?](http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/)
Just as a pointer, when you name a function `maybeExampleNonMonadic` or `f1` you're doing it wrong. Preferably, all the examples you show should be real-world applications of something. If you can't come up with a real-world example of what you're trying to show, perhaps you are trying to show the wrong thing. Show us *why* we would want to do something, don't just show us that it's possible to do something. An added benefit to this real-world approach is that it makes it a lot easier for the reader to understand what the example does when they can derive most of the properties from intuition instead of having to build a mental model completely from scratch. It makes reading more enjoyable. ---- Another smaller criticism: Whenever you see `m &gt;&gt;= return . f` what you really want is an applicative functor, and not a monad. I saw the "`&gt;&gt;= return .` pattern" somewhere in your post. You might want to rethink that example as well, to really use the full potential of the monad. (If you can't come up with an example, perhaps you should really teach applicatives before you start with monads!)
See Brent Yorgey on the subject of fallacy: http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/
Oh, *that's* why. I had never come across this article. Thanks for linking me `^_^`
I think that _Real World Haskell_ covers the "real" part very well: http://book.realworldhaskell.org/read/ But, when reading that book (and other monad tutorials) I still got confused. What I tried to do here was to isolate the mechanics of just the monad operations. For me, they were getting buried in the "real" usages.
I have another article that describes Functor to Applicative to Monad. I'll post that when it is ready. I do not think introducing applicative functor here will help with understanding the basic mechanics of Monad without introducing yet more terminology to come to grips with. Thanks for the feedback.
It's a fantastic piece, and probably the article I refer to the most when talking to others about Haskell. Hah.
Oh, see, I think that's where you commit the fallacy! Now that you understand monads, you see clearly how the pure mechanics of the monad operations work, but if you were presented to those before you understood them, they would be little more than a soup of characters. At least that's what I think. I could be wrong. The "meaningful examples" advice is just something I've picked up along the way of writing a bunch of quick tutorials and guides for other people. It generally takes less effort for people to follow along with a tutorial if the examples look like code they would write themselves. Nobody would write code like p :: Thing -&gt; Bool foo :: [Thing] -&gt; OtherThing foo xs = let (bar, baz) = split p xs in g $ k * (f bar / f baz) and trying to figure out what that does takes some mental power that could instead have been spent at understanding the more important things. It would be so easy to transform that example into something like isMember :: Customer -&gt; Bool memberProportion :: [Customer] -&gt; Int memberProportion customers = let (members, nonMembers) = split isMember customers in round $ 100 * (length members / length nonMembers) which, while longer by character, makes sense and is something the reader can appreciate very quickly. ---- Although I have no real science to this, so it might very well be that names like "foo" make learning a lot easier. That's just not the way I think it is, which is why my gut reaction to seeing something akin to "foo" in any tutorial example is to try to rewrite it as something meaningful.
Oh yeah, I absolutely agree. I just think the examples should preferably be such that the reader will later say, "Of course! I see now why monads are needed and useful!" instead of "Why didn't they just use an applicative there and skip all this monad crap?" (And as a side point, I think the concept behind functors, applicatives and monads should be taught in that order. Trying to understand monads as a concept requires understanding a lot of separate things, some of which would not be needed if the learner is already familar with at least functors (and preferably applicatives as well, for good measure...))
Anyone examining the `time` library who also has familiarity with `vector-space` and `lens` should check out [`thyme`](http://hackage.haskell.org/package/thyme) as well. `time` being as ubiquitous as it is, it's hard to use any other time library in practice, but `thyme` does a good job clarifying the relationships and behaviors of the various time types. For instance, it has built in through UTCTime's AffineSpace instance that `Diff UTCTime = NominalDiffTime`, which is kind of nice.
&gt; "... if you were presented to those before you understood them ..." Agree - each person's path to understanding could be another person's path to more confusion. Believe me, I read a lot of monad tutorials and did not really understand until I built the examples I show in the article (which clearly took me some time since I did not know what I was doing when I started). And, of course, I have more understanding to do, as evidenced by some other comments on my post "This is just false." ! 
Thanks for the feedback. If you don't mind, I will ask for further clarification point-by-point. "That's imprecise. A value of type `m a` is (usually) not a function." --- Not sure what this means. I don't think that the article ever claims that `m a` is a function.
Instead of let fiveHours = 5 * 60 * 60 later = fiveHours `addUTCTime` myTime one could write (using the [time-lens][] package): let later = modL hours (+5) myTime [time-lens]: http://hackage.haskell.org/package/time-lens-0.3/docs/Data-Time-Lens.html 
Piping the output of one function into the input of another function is composition. Bind is not like composition (.). Bind is more like application ($). The analog to composition for monads is called "Kleisli composition" (&gt;=&gt;).
The same sort of reporting could be shown on OS X I believe using the new APIs in Mavericks: https://developer.apple.com/library/Mac/releasenotes/Foundation/RN-Foundation/index.html#//apple_ref/doc/uid/TP30000742-CH2-SW15 This could also be used to say display the progress of the compilation of a file in the finder, possibly based on the progress of its dependant files. There's a good chance that doing this would be overkill... but also very cool =) Edit: Also, whoever wrote the example code in those docs has a nice sense of humour: You never have to tell the user they hit the Cancel button! AppKit's error presentation methods in NSResponder and its subclasses take care of not doing that for you, but we're not using one of those here. 
The thyme library is also a lot faster and memory efficient. It has to be, that's what it was written for. In the case I used it, it was 6x faster at parsing.
It's also not as lazy: `ordNub [1..]` won't terminate. (Good example for an O(n log(n)) algorithm performing poorly: n is infinite.)
Also, deriveSafeCopy allows you to specify an initial version number, this is necessary if you will need to perform migrations (since you'll need to increment the version number)
Just FYI: git clone whatever cd whatever rm -rf .git git init History has been erased :). But this is a great idea, too!
I have to agree. I'm pretty new to Haskell (though I _have_ read "Real World Haskell") and am interested in FRP. The talk seemed to go over the real FRP stuff too quickly and too "Haskelly" (he talked a bit too much about "general" stuff before that) for me.
It causes us huge problems, constantly. In our commercial shop, we always need to compile against library versions that are not the very latest, so a lot of things have changed on hackage since the first time we used those versions. When people use meaningless upper bounds (or worse yet, no upper bounds at all), that throws us into deep cabal hell trying to re-create manually a complex dependency graph that can easily have over 100 nodes.
I agree that a bump to 1.0 will cause a lot of inconvenience for a central library like `text`. It's going to cost us dearly. But it's not contrary to the PVP. The PVP allows developers to use the first component however they'd like. Moving it from 0 to 1 for `text` to indicate stability is indeed something that needed to be done at some point.
&gt; Added support for Unicode 6.3.0 to case conversion functions Shouldn't this be done in `Data.Char`? There may be efficiency reasons why some of `Data.Char` needs to be re-implemented in `Data.Text`, and in fact, we may get to the point some day where `Data.Char` is just a re-export of `Data.Text` functionality. But in any case, it's important for the two libraries to be compatible.
But still, I think the title "Stop Regression Testing" is a huge exaggeration. It would be a big mistake to stop regression testing.
I think it refers to the versioning, ie. using binary or cereal directly would be unsafe in the sense that you might find yourself unable to load a state if you change your data types. What safecopy adds is a version tag for each type that is serialized, and a migration system that lets you upgrade an old version to the new one.
I think it's only used for `ixGen` in ixset, which is not used in the crash course.
But there may not be a function, that's my point. getLine &gt;&gt;= putStrLn Only one function is involved, not two. (Excluding &gt;&gt;= itself)
I found that `time` by itself can't really cope with future or past times, as its only conception of a timezone is a single offset. The `timezone-olson` package plugs this gap.
How is the transactional update implemented? Do you use a priority queue to order the updates, or invalidate all nodes in a forward pass and then compute the changes in a backward pass, or something else? What is the time complexity?
Does AffineSpace represent [torsors](http://math.ucr.edu/home/baez/torsors.html), or is there additional structure?
It would be nice if *vector-space* depended on *time* and provided instances for it. Being a core library, *time* cannot depend on non-core libraries.
IIUC, a torsor can be called an affine space if the underlying group is the additive group of some vector space. So affine space is a special case of torsor.
`Date.Time.LocalTime` is not just for working with time zones and conversions. It provides the `TimeOfDay` type and functions for working with it, so it's one of the most commonly used modules in the library.
This is just a paraphrase of the Klop fixed point combinator joke: K = (L L L L L L L L L L L L L L L L L L L L L L L L L L) where L = λabcdefghijklmnopqstuvwxyzr. (r (t h i s i s a f i x e d p o i n t c o m b i n a t o r)) Yes, K is actually claimed to be a fixed point combinator in the untyped lambda calculus. No, I didn't try to verify that claim myself.
As you say - a template language without static type checking can be good enough, just like a programming language. In practice, we have found that static type checking has pretty much the same benefits for the template language as it does for the programming language, while still allowing us to delegate major amounts of product support and feature implementations to people who are not Haskell programmers.
What is your problem you might start with?
I cant even start the program, it goes to "I'll guess " and then it doesnt show its best guess. And i cant seem to find the solution to make that work.
"can't seem to make it work" made it sound like you couldn't compile the program, and "can't even start the program" sounds like you couldn't run the program, but "it goes to \"I'll guess\"" clearly indicates that the program runs. The code works for me, and it correctly guessed my code. Which numbers did you give it?
Nevertheless - there is a lot in the thyme library that ought to be integrated into the time library.
Ah, cool, I hadn't seen that --- thanks for the link!
I agree! Lens too perhaps, though they're "false" lenses, so there's a bit of a conflict, despite the convenience.
If I compile it without optimisations it takes about 20s to make a guess. With `-O2` it takes about 10s. In GHCi I haven't managed to wait long enough for it to make a guess yet! I imagine it's just rather slow.
Ooh wait I found it! Thanks for the help.
time can be hard to get to like, initially it can seem to be a maze of tricky type conversions, all different. Also, I often seem to skip over useful instances when reading library documentation. I recently needed a haskell version of cron + at for scheduling events, and built the core calculation of the next time to run the event in under 100 lines of code, using time. http://source.git-annex.branchable.com/?p=source.git;a=blob;f=Utility/Scheduled.hs That's more sophisticated than cron, which sadly wakes up every second to check if anything matches. My code can instead calculate how long to sleep until the next scheduled event, which needs quite a lot more calendar math. I shudder to think of writing that in the less strongly typed time libraries I've experienced in other languages. Especially nice to be able to do stuff like this: endOfMonth :: Day -&gt; Day endOfMonth day = let (y,m,_d) = toGregorian day in fromGregorian y m (gregorianMonthLength y m) mnum :: Day -&gt; Int mnum = snd3 . toGregorian yday :: Day -&gt; Int yday = snd . toOrdinalDate 
I really hope they have regression tests for the generator. 
On top of that there's the Olsen timeseries Library which gives you semantically correct time zones as well
Pandoc is indeed a great tool, and a great model for a powerful way of getting things done. But as someone who works in the document industry, statements like "there really is no need to look further for a document converter" in this post would be funny if they weren't sad. Document conversion is a hard problem - really hard. There are semantic impedance mismatches and technical impedance mismatches that are genuinely difficult to overcome. Pandoc is not, and cannot be, a "magic pill" that solves all document conversions. However, it's a versatile tool for many kinds of very simplistic conversions, if that's what you need, and a good base on which you can build some kinds of less simplistic conversions.
Another issue with the time library is that it assumes that offsets are always a whole number of minutes, which isn't true when dealing with certain very old timestamps (depending on locale, usually more than 65 years in the past) Of course, not very many people care about timestamps that old *and* need accuracy down to the second.
Hi, how should we understand the fact that 'time' depends on 'old-locale' ? How old is 'old-locale' ?
The NICTA course will be given in Feb 2014 in Australia: https://groups.google.com/forum/#!msg/haskell-cafe/t2gfDQiFDDA/wNPK4Z8JfZwJ
OK, I see your point. It seems that saying "pipes the *output* of one function into another" is imprecise because it implies two functions whereas what is precisely happening as a monadic value is being "piped" into a single function. As part of my understanding (and so as not to lead others astray) I have changed the section heading from "definition" to "initial intuition". I have seen a number of tutorials say "bind" is similar to composition but with "extra" type adaption. Most notably: http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html
|| A Monad causes the first function in a pipe to be evaluated before the second function (important in a lazy language). |This is just false. Monad instances which happen to be strict about their left &gt;&gt;= argument will evaluate it first. Other monads won't. Thanks. What is a useful way to state how a Monad implies ordering?
I'd like to see a blog series that treats Eilenberg-Moore categories and how you can build them from Kleisli categories.
The tricky part is dealing with the commutative squares necessary to talk about morphisms in that category in Haskell. It is reasonably straightforward from a category theory perspective, though.
Well, that might work if you inline the function somewhere where you have the `Ord` constraint already, but it wouldn't help if you passed it to something else first, say `map nub xs`, because it wouldn't pass the `Ord` dictionary into map, correct?
It implies ordering in the same way the subtraction operator - implies ordering. By using non commutative operator for chaining programs. Additionally, the right side has a data dependency on the result of the left hand side. This doesn't guarantee but strongly implies an ordering, at least on the data flow.
Old. But that dependency is only used for C-style human-readable time string formatting and parsing in `Date.Time.Format`. It would be nice if someone would do a real, modern locale library in native Haskell. But i18n is arcane, complex, and constantly changing. And getting it into the GHC core libraries would be an even greater challenge.
BTW it'd be great to have this parser code refactored into a separate library (or say move it to `time`). I'd very much like to make sqlite-simple to reuse this code and I'm sure there'd other users too.
And I really like this simple and clear introduction to async. Thanks to ocharles for a great series! Looking forward to the conclusion of this series, and to another round next year.
You're welcome. Good luck with your program.
Yes, if I read the article without the title, I would have guessed the title to be something along the lines of, "Turning Regression Tests Into Compiler Tests." The positive point is that testing that a defect has been corrected in your program or library is more powerfully realized by augmenting the compiler to catch the problem. But clearly the compiler, or type checker, should be tested.
I learned what a monad is long before I learned Haskell, and then EM categories seemed like the whole point of them. I was surprised that in Haskell the Kleisli category is so useful.
This has to do with timezones. You can't use ZonedTime to represent local time in say, Kolkata before 1941, or in most of the United States before 1883. The timezone "offset" is always a whole number of minutes.
Yep. I presented on one of the workshops, and it was recorded, but nothing has been released yet. I haven't heard anything about it. 
I am trying to sell this to industry, so that's why I spent so long on answering "What's the point of all this?". My actual audience so far has mostly been people who already think FRP is a good idea. **I am hoping the FP people who have now seen this talk might find this a useful resource for informing their industry friends.** I'll take your advice and expand on the 'how it works' part. It's a bit tricky because, to get the idea across fully would probably take a whole hour. Perhaps it's better just to go over it relatively quickly and at least let people understand that it's strange, but not complex. I did try not to be Haskelly, but it seems I failed. :( e.g. talking about "lift" (a useful concept) instead of "applicative" (an unnecessarily esoteric concept for the purpose). Perhaps I'll re-do the example in C++ next time. Haskell is just easier. :)
Student taught classes make a lot of sense. That the demand is so high, despite the lack of for-credit, is even more encouraging. I wouldn't say this was a revolution "caused" by Haskell. It's more a brought on by the passion that these students had for learning and sharing things that were exciting to them. Still, good to know that Haskell is one of those things that can incite such passion. :)
More content to bookmark! Thanks! I'm pleased that this particular set of lectures has materials for all levels. I'm especially interested in the streaming I/O and advanced type theory lectures.
I remember being so hyped when the slides were released each week back when this course ran. It was when I started learning Haskell too, very good motivation due to all exercises and the stuff I did not get made me read LYAH and RWH. Here are the slides if anyone is interested: http://shuklan.com/haskell/
It's gotta be rough coming up with your own course for the first time. I wonder if the guy wrote anywhere a blog or a set of posts reflecting on his experiences?
That's not the case here. We have: class AdditiveGroup (Diff p) =&gt; AffineSpace p But AdditiveGroup is any group, it doesn't have a scalar multiplication operation.
Nah clearly you need a generator generator.
Glad you're learning haskell and glad you got help, but please take these questions to stackoverflow in the future.
Very nice!
I removed the parts about evaluation, fixed it to say "monadic context" etc., removed the inserts/extracts stuff. I did not change the part that tries to distinguish between "effectfulness" and "readl" IO. For beginners, like me, this caused me a LOT of confusion, not knowing that when monad discussions were talking about "effects"/"effectfulness" they were talking about functional effects - versus the IO monad that enables "real" side-effects. Plus, one more level of separation, showing that monads are independent of their use for "effects". Hopefully my understanding and the document of that understanding has improved thanks to your help.
How would it be useful for sqlite-simple? I wrote the parsers to parse the particular timestamp syntax that postgres emits, and printers to emit the same. It's not clear to me that it would be useful elsewhere. 
Twist: This reply was generated randomly.
In previous years, much (if not all) of the video recording was done by Malcolm Wallace. The turnaround time from filming to processing the videos and posting them online was close to super human. Unfortunately this year he was not able to go to ICFP. We've been spoiled in previous years, but this is more 'normal' from my experiences. If they aren't up in the next month or so I would contact the General Chair and see what's up.
That definition doesn't have any ground in Haskell, though. All Hask functors are endo, and we don't have (or rather, use) the notion of monoidal types. You'd need a whole bunch of category theory unrelated to Haskell to find that simple enough. 
http://homeschooltables.co.uk/ Going to be submitted to the Apple App Store soon.
I'm confused about the way `mod` is defined: &gt; {-@ mod :: a:Nat -&gt; b:{v:Nat| (0 &lt; v &amp;&amp; v &lt; a)} -&gt; {v:Nat | v &lt; b} @-} I can see from the implementation why `b` has to be less than `a`, but can't both the type and the implementation be simplified to {-@ mod :: a:Nat -&gt; b:{v:Nat| 0 &lt; v} -&gt; {v:Nat | v &lt; b} @-} mod a b | a &lt; b = a | otherwise = mod (a - b) b
These are great! Keep up the good work.
Why of course, you're right! Fixed. Thanks! 
I think you can get the app without money. Google Play fixes the problem by setting off payment. Umm...
Repa and JuicyPixels are both fantastic libraries! Note that there's also a direct JP&lt;-&gt;repa link through http://hackage.haskell.org/package/JuicyPixels-repa. You didn't touch on what is probably to me the most interesting feature about repa, its stencil support (http://en.wikipedia.org/wiki/Stencil_codes). I was originally introduced to Repa because I wanted to implement the standard/naive Game of Life grid algorithm with it, and I saw this beautiful implementation using repa stencils: http://www.tapdancinggoats.com/haskell-life-repa.htm Clever! The main issue I had with repa stencils were the limited support for boundary conditions, but turns out this is fixable by writing your own stencil application code. Partitioned arrays make repa less rigid than it seems at first. Here's my own Game of Life program with four different parallel repa implementations, including a comparison to manual parallelism with async + vector and a serial/parallel C++ version over FFI: https://github.com/blitzcode/haskell-gol/tree/master/parallel-glfwb Apart from the boundary condition hiccup, I found repa performance can be a bit fragile. It's easy to make some innocent change that has a huge performance impact, so it's probably good to have a rough idea of how fast things should run. Overall, it feels like a really nice library though. Hope everybody has a look! ;-)
The bug itself is an acceptable mistake, the investigation and fix wasn't too hard, but the sad part of the story was 4 years of the bug making a very popular tool insanely slow and people blaming the network or Haskell or whatever before someone said "wait, this can't be right".
&gt; &gt; A Monad is used to pipe the output of one function into another. &gt; &gt; That's imprecise. A value of type `m a` is (usually) not a function. It's imprecise, but I don't think it's as wrong as you're claiming. A value of `m a` is not typically called a monad, as you know, but rather a (monadic) action. It is indeed the monad datatype (or rather its Klesli category) that allows piping the output of one Kleisli arrow into the input of another. Using the word "function" is definitely a bit of a stretch though. 
&gt; Whenever you see `m &gt;&gt;= return . f` what you really want is an applicative functor Why Applicative and not just Functor?
bos amd friends taught a class at Stanford (?), his lecture notes go into a bunch of interesting intermediate topics.
JuicyPixels-repa has the potential of being faster since it can directly blit the image data into repa without scanning it entirely.
It appears that [landing.html](https://github.com/ocharles/blog/tree/master/code) may eventually be that index, though it's empty at the moment.
See the [first two links](http://ocharles.org.uk/blog/).
Thanks for this great comment! I did originally try with `JuicyPixel-repa` but had difficulty getting it to actually save correctly. I kept getting an image with almost nothing but a red channel, and after battling with it for a few hours I had to ditch it to just get something done. Stencils (and that Game of Life stuff) is really interesting, but was a bit out of the scope for this post. I'll add your comments in as part of the conclusion. I'll also mention the rest of the repa family, like `repa-algorithms` and other bits and pieces. Thanks for the comment!
If your monad does not involve IO then the computation is pure and you can test it as usual.
That's it, thank you.
Bind isn't similar to composition, really. Bind (flipped) is similar to application. `a =&lt;&lt; b` is like `a $ b`. `a &lt;=&lt; b` is like `a . b` 
That's not really the `mod` from Haskell, which is defined for Int (in this special case) rather than Nat.
Thanks for this! I've been meaning to try out `repa` for control/optimisation. How does it compare to `hmatrix` for regular mathsy stuff like you might do in MATLAB? I'm interested in the code style as much as the performance!
I see. That is a very good point. A `TimeZoneSeries` has the same problem, since it is just a list of `TimeZone`s. Perhaps I should rectify that. 
I use emacs, but I don't use particularly many of its most powerful features, just `haskell-mode` and `hi2`. Can I switch to Yi and have powerful Haskell editing at my fingertips from the start? Maybe I should just do that :)
I'm in the same boat. Tried to switch to Sublime Text a few times, but always went back. It would be really cool to write customizations in Haskell, though.
Very nice! Could anyone comment on the technology behind this? Does yi use `haskell-src-exts` and GHC to figure this out or something? Very cool.
&gt; It would be really cool to write customizations in Haskell, though. It would be incredibly cool. I'd love to customize my emacs more, but I really don't want to have to go through the machinations of writing emacs lisp.
What are a haskell aware capabilities of Yi? Here's the list that's important to me: - Syntax highlighting - Code navigation (jumping to the function under the cursor on some keyboard shortcut) - Type info (showing type of functions, variables under the cursor) - Autocompletion. I'll take even dumb autocompletion - cabal and ghci integration. There are many other nice things i'm accustomed in emacs, but i can live without them. 
I wish people wouldn't use haskell-src-exts for these sorts of things, simply because it doesn't support a lot of the GHC extensions that I like to use..
plus indentation for me
The shapes are a regular type-level list of types. For example, the fold type: foldP :: (Shape sh, Source r a, Elt a, Unbox a, Monad m) =&gt; (a -&gt; a -&gt; a) -&gt; a -&gt; Array r (sh :. Int) a -&gt; m (Array U sh a) so `foldP f z a` returns an array of dimensionality `dim(a) - 1`. You can't express this without knowing something about the structure of shapes.
Yeah, I've used it before for some in-house tools at my work. It's not too hard to get working.
I really don't know enough about how rules are applied to say.
It has a nicer interface without the gigantic toolbox of algorithms. For most of those routines, MATLAB uses Fortran that has been optimized over several decades, so you probably won't beat its matmul or fft with repa. If you can't build what you need from existing MATLAB routines, though, repa should be (?) faster. Last time I used MATLAB, it was pretty slow for code that used non-vectorized loops or recursion. It also has a recursion limit that was a real PITA when doing some ARMA stochastic modelling.
`wai` is still doing alright, but `jRuby` seems to be beating it handsomely. I remember seeing a post from Michael Snoyman a while back where he had found an easy performance fix, did that get merged in? I don't use Haskell for performance, but I think that is one of the nice side effects for me.
&gt;Syntax highlighting Yes but a bit limited. We need to add the ability for highlighting of arbitrary syntax elements (just a look-up or something) and modify the lexer slightly to recognise these constructs. See https://github.com/yi-editor/yi/issues/465 for some issues for the devs. At the moment, syntax highlighting is just fine out of the box. Not the best, but usable. &gt;Code navigation (jumping to the function under the cursor on some keyboard shortcut) If you mean jumping to definitions using TAGS, that's already implemented. Just generate your TAGS with something like hasktags. &gt;Autocompletion. I'll take even dumb autocompletion Not that I'm aware of yet. ghc-mod emacs library provides ability to complete with things in scope so we can almost certainly port that. &gt;cabal and ghci integration. You can run the GHCi repl (ghciLoadBuffer) and load your stuff in. No Cabal integration that I know of (yet!) Yi is very much work in progress!
The basic mappings are there. AFAIK there are no tests for these keybindings though so that's certainly something that should be looked into, hint hint~
&gt; Can I switch to Yi and have powerful Haskell editing at my fingertips from the start? Maybe I should just do that :) Sorry, I can't recommend Yi as something you can just jump into for a few reasons: 1. There's no default config. You can run --as=emacs but that's not very good. You can peek into yi-contrib directory for some user configs or check out mine at https://github.com/Fuuzetsu/yi-config for a fairly simple set-up which uses external libraries (theme, case split &amp;c). 2. The documentation sucks. It's either outdated or non-existent. Unfortunately things are also fairly hard to find and often have cryptic names. I hope to improve the documentation over the next few weeks (document everything, update Wiki, provide examples) but I am still at the point where I'm just getting the feel for things. 3. There aren't many modes available and no minor modes: for example, if you're using some cool minor mode you can't live without in emacs, it's almost certainly not in Yi yet, sorry. The upside is that you can write it in Haskell.
Here's the [source code](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/wai/bench/wai.hs) for the Wai benchmark. I can't spot any obvious performance bottlenecks. I'm curious to see how Wai performance changes after the new IO manager gets merged in GHC 7.8.1.
I am the creator of the case-split in the OP and I can say with honesty that if you want to write extensions and already know Haskell, you can start now. Scour the docs for functions on point, buffer etc. etc. and everything else should be just your regular Haskell. In fact, I'm writing a paredit library for without any actual Yi code. Once it's fairly ready, I'll be able to just import it in some module and connect the on-buffer operations with the AST manipulations. Again, if writing extensions in Haskell is what you're after, please jump right in! We really need more people. 
With how naive the current implementation is, it'd be very easy to copy. In fact, probably easier considering that emacs already has some native ELisp interface to ghc-mod. I do not recommend doing so, at least not until the split is sane (no variable repetition, work on records, ‘type’s, only allow to case in actual case expressions). All in all I think that exploiting what TypeHoles does would be the best but that's GHC 7.8 only…
So according to [here](http://www.techempower.com/benchmarks/#section=environment&amp;hw=i7&amp;test=json) and [here](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/toolset/setup/linux/installer.py#L158) they use the `ghc` that comes with Ubuntu 12.04, so improving performance on the compiler front won't happen until `ghc-7.8` ships with an Ubuntu LTS release.
Please file bugs for such things! In fact, please check https://github.com/yi-editor/yi/search?q=performance&amp;source=cc&amp;type=Issues first. Next thing to try would be to try fundamental mode (you can switch modes at runtime with ‘setAnyMode’ function which again is not documented…) and if that works, try different Haskell modes: there's fast, clever and precise. You'll have to edit your config a bit for those. Please see https://github.com/Fuuzetsu/yi-config/blob/master/yi.hs#L12 on how I do this.
Good point! Rather than forcing the inputs to be `Nat` we can just push that requirement into the output type: http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1387309622.hs That is, assuming that we still want `gcd` over `Nat` we just require the property that `mod` produces a smaller output *if* the inputs are non-negative. (Of course one should separately prove that spec holds for the prelude's `mod` ...) 
This has been asked about in the yesod mailing list and the response was basically that until the new IO subsystem planned for the next version of GHC is released they don't see a point in performance tuning.
This is the answer. The biggest bottleneck currently with the Haskell builds is the IO manager. Since 7.8 is (and continues to be...) just over the horizon, there's not much point in tuning until it's released.
Here's the [list of extensions not supported yet by haskell-src-exts](https://github.com/haskell-suite/haskell-src-exts/issues/19). Maybe those are exactly the ones you like to use, but mostly they are either quite new or rarely used. Some of them are added by pull requests and hopefully will be merged soon.
Yes, the build Michael Snoyman made got merged in. It's the reason why Yesod is currently much higher than Snap. It avoids some of the problems with the current IO manager by manually forking processes ([seen here](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/yesod/bench/src/yesod.hs#L169)). Hopefully once we can use the new IO manager it can go back to something more idiomatic.
I don't see Haskell in this benchmark. It mentions Yesod, Snap and Wai though.
Cool, I was wondering what the forking was about there. Thanks for the explanation.
Not so good...
So not before Ubuntu 16.04 then most likely?
Does it start with something other than black on black text for certain parts of the syntax highlighting in the default theme by now? It did that the last few times I tried it, suggesting it was a dead project or only used by people who don't use a white on black terminal.
any place documenting what the new IO subsystem's going to be like?
There is a paper written about it that was presented at The 2013 Haskell Symposium. "Mio: A High-Performance Multicore IO Manager for GHC"
As snooty as the parent comment is, I must agree a bit. I am *very* wary of web frameworks that don't focus on staying light.
Do you work professionally as a web developer? What's your framework of choice? Do you mean "light" in terms of features (Rails vs Sinatra)?
I've been playing with this lately: https://github.com/jkozlowski/kontiki/blob/issue-2/bin/test.hs see the 'simulation' function (I cheat there a bit because for now my base monad is IO and I use unsafePerformIO to evaluate that, but I'll get rid of it in the final version). You need to provide a function that runs your monad to 'monadic', like e.g. 'runState' for the State monad (just remember to partially apply it to your initial state). That's how I've been able to do it anyway, but it took me a while to figure out being new to monads and all... Hope that helps
Really? No, LambdaCase? That's my new favorite.
If, like me, you were tempted to give `yi` another spin after seeing this post, and if you are using OS X Mavericks + Homebrew, you will probably find the following links useful. https://github.com/gtk2hs/gtk2hs/issues/1 https://github.com/Homebrew/homebrew/issues/14123 
[It's in the queue.](https://github.com/haskell-suite/haskell-src-exts/pull/33)
i've been tempted to try hacking on yi several times, due to wanting a vim clone with better scripting. every time, i've been put off by the fact that it's simply not fast/responsive enough to be a viable editor (and i'm nowhere near good enough a haskell programmer to fix that deep an issue). i continue to be excited by the underlying promise of yi, but i really think the performance issues need to be improved if it's going to get any sort of uptake.
Thanks! And I guess `hmatrix` inherits that speed, being a binding to similarly low-level C libraries?
I wish I had this write up a few weeks ago. It would have saved me a few hours of digging into the various hashtable implementations, many of which are no longer maintained. Next year, do 24 days of hackage in January, so I can have it all year long. ;-)
Is debian stable a requirement? edit: Yeah it looks like a requirement is to use build scripts to setup your environment. I suspect allowing docker containers would make this competition more flexible. edit2: No, it actually looks like many of the other frameworks have very custom install routines. No reason why one shouldn't be able to add an additional ppm as [here](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/toolset/setup/linux/installer.py#L37)
Emacs and Behind-The-Firewall are two very exciting features to see come to Haskell Center. I'm sure the API needed for Emacs integration means FP Complete or the community can add support for more editors with relatively little effort too.
Probably if I were hacking on Yi, my first priority would be dogfooding. The earlier I start using some thing to hack on that thing, the better. Motivation for further features comes for free after that.
Working from his code, how would one edit it so that it rejects empty usernames and redisplays the login, perhaps with a warning. I'm just getting started with threepenny, and I'm pretty lost.
I found Yi incredibly difficult to hack on. 
Sounds about right to me. PHP seems to have a lot of bloatware libraries.
From your link: "An affine space is a torsor for a vector space." So the additional structure is that of the underlying group. However, very many interesting torsors are affine spaces.
Right, but the AffineSpace class actually defines any torsor, not specifically affine spaces. It should have been named Torsor or AdditiveTorsor instead.
GHC 7.8, just 2 months away. always has been 2 months away, always will be 2 months away.
I'm building a json heavy python3 website. Should I just stop now or what?
What was difficult about it?
Can you say a few words on how the behind-the-firewall version works? IIRC GitHub just sells a VM image you can easily deploy if you want a local version of the website. That sounds kinda attractive, just dropping a disk image on any OS and having a perfectly configured Haskell environment with hundreds of packages, a local hoogle + haddock, all set up etc.
Next LTS is, too, just over the horizon - it's the upcoming 14.04. I so hope the new GHC will be in.
I believe AdditiveGroup is precisely groups, which makes "AffineSpace" precisely torsors. I don't think there's any more structure. For example, [thyme:Data.Thyme.Calendar.Day](http://hackage.haskell.org/package/thyme-0.3.1.0/docs/Data-Thyme-Calendar.html#t:Day) is an instance of "AffineSpace", with "Diff Day = Int". Int is a group (considering addition), but it is certainly not a vector space.
I doubt it. That is just 4 months away, they are probably already in some kind of feature freeze phase or will be very soon. 
Everything the parent says is basically claiming that something is the "opposite of what Haskell stands for". Such people are poison. 
I always think it's a bit of a shame that many languages emphasize the ordered associative containers over the unordered ones. Many have also gotten them into the standard much later (C++ since TR1 / C++11, Haskell build-in since the current HP 2013.2.0.0). Most of the time it doesn't matter if the map is ordered, but the performance penalty can be severe. One surprising thing (to me, at least) I've found in my benchmarks is that container's IntMap has faster insert / delete. Johan Tibell already touches on this in the performance analysis you linked, but maybe it's worth showing with some charts as well: http://htmlpreview.github.io/?https://raw.github.com/blitzcode/lru-bounded-map/master/report.html (Just ignore my custom containers and look at Map/HashMap/IntMap) Using Data.Hashable, it's of course trivial to use an IntMap everywhere a HashMap can be used, but watch out for hash collisions. As you already said, the more limited API of HashMap vs Map can be a bummer. It's especially annoying if you could use operations like updateLookupWithKey and have to emulate them with two HashMap operations. Not only do you then traverse the data structure twice instead of once, you also might pay the cost of computing the hash twice. This can be quite costly when you use long strings like file names or URLs as keys. Another gotcha worth mentioning: size for Map is O(1) while it is O(n) for HashMap! Finally, I'd really encourage anyone to have a look at the source of unordered-containers. It's very well-written and I've actually picked a few tricks from it. It's also a really good introduction to HAMTs!
According to Trusty Release Schedule Debian Import Freeze is 6 Feb and Feature Freeze is 20 Feb.
Yes, you're right. I keep thinking `p` is the group and `Diff p` is the torsor, when it's the other way around. One of these days I'll get it straight.
I disagree! You probably have some great knowledge about al the different possible libraries now. Maybe you want to blog about that ;)
&gt; With release 2.3 of FP Haskell Center, Emacs users will be able to leverage the libraries, build environment, and IDE features of FP Haskell Center. OK Chris Done, I know this was you :)
&gt; but that's GHC 7.8 only… No worries, `haskell-mode` has some 7.8-only features (such as `:complete`) already — and 7.8 isn't even out yet.
That's right. I'm writing a technical blog post about it now, but the short of it is that when planning Emacs support we decided we'd make [a program](https://hackage.haskell.org/package/fpco-api) similar to hdevtools/ghc-mod that would be editor-agnostic and have a simple communication protocol. Sublime, Vim or Eclipse integration would be tip top and very welcome. **Heads up:** you need the personal license for this. I know some redditors will want to know that ahead of time.
And John Wiegley!
[See here](http://www.reddit.com/r/haskell/comments/1t4zsl/fp_haskell_center_23_released_emacs/ce4ltk3). None of us on the team are Vimscript gurus but you're welcome to make a binding.
Fair enough. Though it's hard to motivate myself to do much vimscript hacking. Would rather spend such free coding time as I have writing haskell. I guess that means I should be hacking support into [Yi](http://www.reddit.com/r/haskell/comments/1t3a0f).
I tried using Yi on Arch as well about six months ago. It worked well and I liked it, no real trouble with the dependencies and the speed was fine. It didn't become my full time editor because, as TFA points out, I'd miss out on a lot of modules that are available for vim. 
The current implementation of unordered-containers does trade off lookup time (which is much, much better) for somewhat slower inserts. The good news is that we can improve the insert time for unordered-containers (with some GHC work). containers on the other hand is pretty-much at a dead end due to cache effects (binary trees are too deep compared 16-ary HAMTs).
Makes sense, I'd expect the traversal code to be memory bound! In my benchmarks / use case I was actually rather surprised how well the binary containers performed on lookup compared to the 16-way HAMTs. It's of course just a single use case and hence not useful to draw any general conclusions, but if you look at the 'lookup (w/o LRU upd)' benchmark group in http://htmlpreview.github.io/?https://raw.github.com/blitzcode/lru-bounded-map/master/report.html the only slow outlier is really Data.Map. The HAMT based containers (HashMap and LBM_CustomHAMT) perform best of course, but the binary ones (Data.IntMap and the binary Trie based LBM_CustomHashedTrie) are not far behind.
I tried it a long time ago but at the time the way yi used dynamic configure scripts kind of put me off it. It seems like it uses dyre now (xmonad like configuration). So maybe its better now?
And nodejs??
Forgive my ignorance, but why are events associated with values? Instead of an `Event MousePosition`, wouldn't it be conceptually simpler to have an `(Event (), Behaviour MousePosition)`? If there was only `Event ()`, maybe it could simplify the issues about whether events are simultaneous and how to merge them.
Well, I meant it in a manner that I can't just implement it all in terms of TypeHoles because only a few people would be able to use it.
Hopefully we can remove the prelude thing. Seems it's an artifact from years ago.
What protocol did you choose to talk to editors?
A simple single-line-and-close-connection with JSON. Super easy: chris@retina:~$ echo '{"tag":"MsgAutoComplete","contents":[247,"src/Main.hs","getC"]}' | nc localhost 1990 {"tag":"ReplyCompletions","contents":["getChanContents","getChar","getContents","getCurrentRoute"]} chris@retina:~$ echo '{"tag":"MsgTypeInfo","contents":[247,"src/Main.hs",11,11,11,22]}' | nc localhost 1990 {"tag":"ReplyTypeInfo","contents":[[11,11,11,22,"getContents","IO String"],[11,11,11,30,"getContents \u003e\u003e= run","IO ()"]]} 
You have a typo; in `olliesWishList` your `fromList` seems to have wandered off course a bit. :p
That would require it to be in Debian Testing by that time though as far as I remember.
How good are the vim keybindings? 
this post is obnoxious and, markedly, lacking in humor. i don't know if you've actually looked around this community, but it's an incredibly warm and helpful place. feel free to leave.
Absolutely not an issue – I would hold it's the other way around. You have come to the right place! Haskell is an excellent tool when you are a mere mortal and want to write great programs!
On the contrary, I think you will have more issues when learning Haskell (or anything, really) if you think you are better than everyone else.
It's a troll. Don't feed them.
Just giving back to the community what drew me into it in the first place: the incurable niceness of everyone.
We won't impose such a restriction for enterprise customers.
We based this release directly on user feedback received from June through November. Thanks to everyone who goes to the effort to try our betas and send meaningful feedback -- and to those of you who received and answered survey emails, which helped a lot in guiding our priorities!
As a reminder, starting last month, you can use most of the features of FP Haskell Center for free (Community Edition license). We sincerely hope that you'll find it useful enough to upgrade to a paid license so we can pay these guys who are working on it full time -- or ever better, ask your company to get you a Pro license if you could use it at work. Many thanks to those who've found a way to subscribe. Full academic licenses, Community Edition license, and of course School of Haskell are all free-as-in-beer, consistent with our philosophy of giving back extensively to the community. We also continue to financially support our folks spending time on Yesod, Fay, GHC, and other open-source projects that we use.
So is paying going to let you run the service locally?
If you can concretely state the problem I'd be happy to look at it. -jp-repa maintainer
This is awesome. Is it possible to combine this with GHC's built in type nats so that I could type `5` instead of `(SSucc (SSucc (SSucc (SSucc (SSucc SZero)))))` ?
&gt; (which can always be interpreted as an attack, even though you did not say anything blessing). Totally random reply to a month-old comment, but are you by any chance French? The English sense of "bless" doesn't really fit here (although it could be a cute turn of phrase). On the other hand I don't see "Frenglish" every day.
## TODO Link to Haddock I think you wanted to add a link to haddock. :)
As of now, the answer is still unclear (to me, at least). The main advantages of singletons, as I see them: + More expressive -- one could (probably) encode more sophisticated properties with them, without being restricted to the logics that SMT solvers can handle. + Part of GHC's type system, which allows neat type-directed code synthesis as in the `vReplicateI "hi" :: Vec String (Succ (Succ (Succ Zero)))` example from the article above. The main advantages of LH: + More automated checking &amp; inference, since one is not limited to the unification-based decision procedures that lie at the heart of GHC. + The invariants are /not/ baked into the type but live as separate refinements. This lets one program with regular lists and attach invariants -- e.g. about length, elements, ordering etc. -- with predicates, without having to create specially indexed list types (and associated operations) for each property. However I'd say its still early days and I suspect that the approaches have complementary strengths that may be combined. 
&gt;I don't buy this. As I said, I haven't profiled it yet, this is just a guess based on me switching to fundamental mode and having a smooth experience. It might be something totally different. &gt;Emacs does all of this in friggin elisp and instead of lexing it uses regex for syntax highlighting which should be much slower, all done in the main ui thread no less. I imagine what's happening is lexing whole file vs regexing parts of logical blocks around you but again, I don't know and it's something that still has to be checked. &gt;And emacs isn't slow. Pfft, if I open something like Haddock spec tests in emacs and try to do anything as exotic like pressing ‘return’ or god forbid, ‘tab’, I have to wait like 5-10 seconds. When hacking on Haddock over summer, I frequently just turned off haskell-mode to move things around. I think a better way to word that would be ‘emacs isn't that slow on smaller files’. It seems much better today, only about 1-3 seconds wait. -- In any case, this isn't an editor war: I don't know why Yi is slow. I'd love to give you a better answer (because then I could go and try to fix it) but at this moment in time, I don't know. I just wanted to make it clear that no one is making any excuses.
PokerPirate: It already works with type nats; that is, there are singletons defined for Nat and Symbol. It's just not so clear how to make good use of type nats in this scenario -- there's no built-in induction principle we can use to write something like vReplicate. As a partial solution, in GHC 7.8, you can write this: type family U a where -- "U" for "Unary" U 0 = Zero U n = Succ (U (n-1)) Now, you can write the singleton for 5 as (sing :: Sing (U 5)) -- not too terrible. ranjitjhala: Agreed completely. I was blown away by the Liquid Haskell demo at ICFP and am looking forward to seeing these techniques combine somehow. -- Richard
&gt; I think it's quite well known that unrestricted recursion is incompatible with the programs-as-proofs interpretation I hope not. Unrestricted recursion is entirely compatable with proofs as programs. NuPRL supports partial functions, it is just a type diference to assert functions are known total. More generally though, I don't think logic has to equal total. The total function space coresponds to proofs of implication. The question is what is the logical interpretation of a partial function space?
It may be that they're lexing in different ways. Syntax highlighting in Emacs happens on intervals and specific changes incrementally. I.e. if some region of text changes, it just re-renders that region. It also only renders what's visible on the screen, that's why having a big block of text that fills the screen is uber slow but a file of 2k lines is fine. 
I think a manual pre-singletons encoding of this is like [this](http://lpaste.net/92328) by putting the size in both the type and the value and then in the `second` function it's impossible (see the commented out lines) to write this function wrongly (at least structurally, you can always pass another `a` value). Singletons seem to merge these two together so that there's an implicit `(a,n)` when you write `a`, thus making it _actually_ impossible to write `second` wrongly. Properly dependent.
These are the design issues that each FRP system treats slightly differently. Many FRP systems are based on one type "signal" which is more or less what you describe, instead of splitting them into events and behaviours. Personally I think the abstraction where an event has an associated value is semantically cleaner, because it prevents values "hanging around" when they no longer have meaning.
that is (logically) equivalent to `A -&gt; B` so I think it isn't really an answer. `_|_` means something rather different in denotational semantics than it does in logic.
Cool and all, but I can't help but think it would be easier to just add pi types and make Haskell properly dependently typed. Alternatively GHC-based support for singletons seems desirable. 
Pretty lacking; kind of like every other vim emulation out there (although yi isn't leading the pack there either, I think). A lot depends on how much of vim you actually use, though.
Seems legit.
&gt; Is any of this supposed to be new, or are you summarizing the existing literature in preparation for a future post? It's just a summary of some existing stuff, but not preparatory for anything. Rather, it's just a collection of these different things in one place, so that the collocation might provoke interesting discussion. &gt; In "Γ, x = M ∷ a ⊢ ctx", what does "ctx" mean? Are you trying to say that "Γ, x = M ∷ a" is a valid context only if "Γ, x ∷ a ⊢ M ∷ a"? It a judgment that means that the context to the left of the turnstile is well formed. In typical settings, we wouldn't want to have, say, a context that declares two different types for `x`, so we should not be able to prove x :: Int, x :: Bool ⊢ ctx Typically, the proofs for `ctx` include the following: ------ empty-ctx ⊢ ctx Γ ⊢ ctx Γ ⊢ T type ---------------------- fresh-var-ctx [x is not in Γ] Γ, x :: T ⊢ ctx For the `def` rule, it's addition to these two amounts to saying exactly what you said, yes. &gt; I think it's quite well known that unrestricted recursion is incompatible with the programs-as-proofs interpretation, which is why proof languages use termination checkers to ensure that programs use structural recursion or some other provably-terminating scheme. Or is that exactly what you are trying to say? It depends strongly on what you mean by programs-as-proofs, I guess. Certainly, the programs are proofs in the type systems under discussion here. But not proofs in the normal logics of interest, precisely because the normal logics of interest do not have contexts like the ones discusses here. But that's ok, because logics are a dime a dozen.
Useless thing to prove? `A -&gt; B \/ T` is the same as `A -&gt; Maybe B`! That's pretty useful to prove!
I would avoid conflating recursion and partiality. You can be partial without having any tricky recursion principles. You can also have partiality and non-termination as distinct things. The typical semantics for both is to say that they denote the bottom value of the definedness lattice for each type, but there's no reason you can't have a double lattice with two bottom-like values, one for undefined and the other for non-termination. This is(sort of) what Haskell does, and it's the only sane thing to do in a real programming language. You're right that logic doesn't have to equal totality, however.
&gt; General Recursion via Sketchy Primitive Fixed-point Combinator &gt; ... needs a lazy evaluation strategy Is this actually true? Wouldn't fix f x = let x = \() -&gt; f x in x work? It wouldn't be of type `(a -&gt; a) -&gt; a`, rather `((() -&gt; a) -&gt; a) -&gt; a`, but it does the same job.
In real life you'd probably prefer something like this if you can't use built-in type-level nats: data Nat = Nil | Zero Nat | One Nat plus :: Nat -&gt; Nat -&gt; Nat plus (Zero x) (Zero y) = Zero $ plus x y plus (Zero x) (One y) = One $ plus x y plus (One x) (Zero y) = One $ plus x y plus (One x) (One y) = Zero . plus (One Nil) $ plus x y plus Nil y = y plus x _ = x 
You're right of course. And I think that's the whole point of these kinds of experiments - to try different approaches and get some practical experience so we can get it right when we add it in for real. Idris takes the approach of jumping in feet first right now, more like what you are suggesting, and that's very interesting, too.
I'm voting this post up. The OP is making a bona fide effort to understand monads and to share both the process and the result with us. That is clearly seen by the comments here. I applaud these efforts, and I hope haroldcarr will continue to feel welcome as a contributing member of our community. And thanks to Peaker for all the great feedback he provided.
&gt; not well understood from a logic/proof theory perspective nor from a category theory perspective Is this true, though? The theory doesn't have to change, the existence of `fix` says that every endofunctor `F` has an initial algebra `μF`.
Well, the problem with a new GHC is that it isn't just that one package, every other important Haskell package has to be updated to work with it first before it can enter any sort of stable distribution. It could be done if it came out now but I doubt that it will be possible to make those Februrary deadlines if it is released even a month from now.
Ah, that points out the flaw in my reasoning. It's not quite like Maybe because you can't match on the proof to find out whether it is B or not.
Yeah. It's like every type is wrapped in Maybe, and you have to work inside the Maybe monad. That's a more accurate analysis of the categorical semantics of it all.
&gt; In any case: Who said it is absolutely and completely irrelevant to the validity of an argument. Get your terms straight. [Ethos](http://en.wikipedia.org/wiki/Modes_of_persuasion#Ethos) is a classic element of rhetoric. (Argumentum) ad hominem is trying to reject a speaker's argument by attacking their ethos. Neither of them are invalid tools. By using ethos properly, a speaker can show their connection with the topic, which is more important than you think. If the topic is baldness, it matters a lot whether the speaker has always had hair, has become bald, or wears a wig to cover up baldness. It shows [which side of the elephant you've touched](http://en.wikipedia.org/wiki/Blind_men_and_an_elephant). On the other hand, hypocrisy is definitely a threat to an argument (implying that the argument is unrealistic), and uncovering hypocrisy is technically argumentum ad hominem, and not necessarily a fallacy. --- By the way, this &gt; And really there are better things to do than to change a mind that doesn’t want to live in reality. is ad hominem, because your subject is the speaker, and you attack their ethos, just to say you can ignore (soft rejection) their argument.
I doubt it's a bad thing. I'm not too well-versed in category theory, but I think that identifying core building blocks of programming allows for better optimization. All those \*morphisms have much better known theorems for composition than the building blocks of imperative languages.
Thanks! I will now try to find one of those existing signal-based FRP systems.
It's definitely Haste. I recognize the horrid regex I use for the Unicode bits. :)
I agree. To the downvoters: I find it perfectly plausible that one day somebody will decide to publish a paper in Comic Sans exactly because SPJ's legitimate usage of Comic Sans (as a good font for presentations) has been turned into a "tradition". I also find that the probability of more than three papers, articles or blog posts in Comic Sans being published in near future is high enough to be worth a dozen of downvotes.
What's the advantage of the extra `Zero`?
At the very least, [CUFP has been up for a month](https://www.youtube.com/channel/UCfSUv7I_aHgzcnXMcd8obsw?feature=watch).
Actually, type-level binary sucks because the recursion depth is too high. You want [type-level decimal numbers](http://hackage.haskell.org/package/data-fin) in order to be able to compute things effectively. Or, y'know, actually implement type-level numbers using machine arithmetic, GMP, etc.
I'm pretty sure it doesn't. How would you construct said functor in terms of fix? Unless, of course, you are including a type-of-types, which is... something.
Shake let's you have a binary shake-progress on the $PATH, so writing a Mac OS X specific version and hooking it up should be relatively easy. I don't have a Mac, but would welcome a patch. The Windows once is in C#, so I'd be happy to get anything implementing the Mac, as I imagine Objective-C might be easiest.
I have some time off, I'll see if I can decipher the horrible mess that is Objective-C. Not sure if a Foundation only app will work or if it'll need to be a Cocoa app to get a dock icon. 
"Optimized for decades" doesn't mean that much when CPU/memory architectures radically changed in the last decades. Any decades old optimizations are likely to be deptimizations...
I read through the slides in one evening. They are surprisingly good; written in good humor, not unlike Learn You A Haskell. Highly recommended.
Huge thanks to /u/tekmo for his help on this one. Also big shout out to /u/ocharles for giving me the idea to move my blog over to Hakyll after seeing his. I'm a little fuzzy on some of this stuff so I'm open to criticism and totally understand if anything is just flat wrong!
For the record, 24 Days of Hackage is persistently wonderful. I feel like you get a lot of niggling critiques from Reddit during this process—but I don't think a single person wants anything besides the most incredible product possible. I love this project.
JuicyPixels-repa 0.7 should solve your problem and give you a fancy helper function to boot. The helper `imgToImage` converts from Repa based `Img` to JP base `Image` type. Then you can just use the original JP save functions such as `imageToPng`. Right i &lt;- readImageRGBA f let i' = imgToImage i -- ^^^ probably a bad function name, I admit (it's based on the types) L.writeFile "/tmp/someImage.png" (imageToPng i')
The most standard "toolset" that you're going to find is the haskell [OpenGL bindings](http://hackage.haskell.org/package/OpenGL) that have been purported to be brought up to speed with OpenGL 3, however there are still plenty of inconsistencies. [The authors](https://github.com/haskell-opengl/OpenGL) do tend to accept patches pretty quickly though, so it's easy to contribute. The OpenGL tutorials located [here](http://www.opengl-tutorial.org/) focus on "modern" OpenGL (3+), and translate pretty well to Haskell. In terms of toolsets, the one that has seen the most work put into it is [LambdaCube](http://lambdacube3d.wordpress.com/), although I haven't seen any serious projects that use it yet. I'm currently [working on my own](http://www.github.com/Mokosha/Lambency) to get a feel for some graphics algorithms using Netwire (5) and the latest OpenGL bindings (2.9.2.0).
Most of all, that documentation is completely non-existing\* (as somehow always for Haskell libraries), and the original OpenGL documentation is of no use if things are different because Haskell is different. __ \* No. Documents generated from undocumented code with Haddock do *not* count.
This is slightly true for the OpenGL package. However OpenGLRaw mirrors the C library pretty much exactly, and all of the original OpenGL documentation can be used. In fact I have ported several OpenGL applications this way in a fairly straightforward manner. Not only that, but you have access to everything in modern OpenGL at that point.
This is crying out for a type synonym. type Catching e ma = ma -&gt; (e -&gt; ma) -&gt; ma Control.Monad.Trans.State.liftCatch :: Catching e (m (a, s)) -&gt; Catching e (StateT s m a) Control.Monad.Trans.State.liftCatch catchError :: (Monad m, Error e) =&gt; Catching e (StateT s (ErrorT e m) a) etc. (Sorry, the above isn't meant to sound snarky.) Thank you for spotting the pattern.
For a loose translation of the first tutos of opengl-tutorial.org to Haskell, see here: https://github.com/YPares/Haskell-OpenGL3.1-Tutos (I use OpenGL-Raw, which allow more direct translations from C code. If you (OP) want to use the OpenGL package, I'd suggest to also try vinyl-gl, to really benefit from the added abstraction)
Oooo, I'm also working on some `netwire`/`OpenGL` stuff (as of yesterday). You're a lot further down the line than me! Looking good :)
Here are the docs about which command line parameters shake-progress needs to take: http://hackage.haskell.org/package/shake-0.10.10/docs/Development-Shake.html#v:progressProgram If the Mac one can take more settings, feel free to expose them, and I can modify the Windows one to accept and ignore them so they can be used from Shake.
It depends on the stuff you want to do and how good you know opengl. Maybe you start with the OpenGL package (as others have suggested). If you miss things, you can probably find them in the OpenGLRaw package. Ywen suggested vinyl-gl. Here is a nice blog post: [Postmodern Haskell and OpenGL: Introducing vinyl-gl](http://www.arcadianvisions.com/blog/?p=388) If you need cool things like bindless textures, which are currently missing, you could help the community by adding it to OpenGLRaw or create a new Issue at https://github.com/haskell-opengl/OpenGLRaw/issues.
We also support transient like inserts e.g. `fromList` uses it and is about 2x faster than doing repeated inserts. We could think about exposing it in `ST` if we wanted. Another options would be to add a few more high-level construction functions (e.g. `unfold`) that cover the majority of construction cases.
Haskell is not for all! Because many refuse to learn, to think and to understand. They want things to be mashed and spoon-fed to them, are the loudest screamers when they aren’t, and see asking them to turn on their brains as an “insult”. (Insult… what a childish primitive concept!) If they want to turn on their brains to use it, of course they can. But it’s not our job to please them and spoo. They can’t be pleased until it is ~~MS Clippy~~ Siri on a shiny bling tablet with clickwheel UI with auto-correct for a keyboard… and the some. If you listen to their whining, you’ll end up becoming just another pathetic crippled tedious dumbed-down shit, together with all the other iOSes and Windows7s and FirefoxOSes and ChromeOSes and Ubuntus and TypoScripts. We already have that. For *once* I would like to keep something for *us*: People want power and efficiency and elegance and emergence, over simpletonness. And I’m not asking! I’m telling! P.S.: This is Reddit. It is *made* of those brain-dead iTard NPCs. Hence this is obviously going to get downvoted. Because those idiots actually think Reddit votes mean anything whatsoever in the real world. \^\^
I disagree with the premise that increasing adoption requires dumbing Haskell down.
Maybe darcs is better than git now.
Haskell is cool. We get it. Now could you please stop acting like an asshole? People like you are the reason people think Haskell is for people who want to brag about their intelligence, not anyone else. Believe it or not, situations where languages that are not haskell are useful *exist*: you don't have to be mentally impaired to use them.
It took me a while to understand that `state.maybe` was just function composition! I've never seen this style, without spaces. Do other people use this?
OpenGL seems to be largely supported at the lowest levels, the various higher level abstractions on top of that are very much hit or miss. As for GUI in general, I've found that to be in a very sorry state. Gtk+, wxWidgets, and SDL seem to be the most stable of the GUI libraries, and you can usually get them to work with relatively little fuss in Linux, and I'd assume (although I have not tested) on Mac as well. On Windows however that's a very different story. The last time I attempted to get any graphics code working on Windows I ran into a major bug in the Windows port of GHC that prevented it from properly calling libraries (I.E. anything not compiled by GHC itself). The time before that I got SDL to work, but only after recompiling it from source, copying various DLLs to assorted arcane locations, and replacing various utilities with versions provided with different versions of MSYS from what was bundled with GHC. I also attempted to get Qt working, but that seems to be completely broken currently and at the time I attempted it the last update to the QtHaskell project was over a year old. TL;DR; If all you care about is Linux (and maybe Mac) and you stick to one of the big three GUI libraries (and I'm counting SDL in this list) it largely works. If you want support for Windows, good luck.
Sometimes people use that no-space-style with lenses, to more closely mimic standard OO syntax usage. I prefer not to treat the lenses specially; it encourages thinking of them as some sort of "magic" instead of Haskell functions and data. (In particular, if you get stuck as thinking of lenses as "magic OO accessor syntax" you will be more likely to miss the case where it would be useful to pass possibly-composed lenses around directly, which would really be missing out on the power.) (Of course this can be overcome, but I prefer just to not tempt fate. This attitude is informed by some experiences I've had at work working in Perl, where I did a little something slightly fancy with what are in fact bog-standard functions, and have spent years trying to explain to people _they're just functions, damn it, rather simple ones, actually, stop freaking out about them_. YMMV, and I'm not worried about the experienced Haskell community, but I think it can be a problem for beginners.) Amusingly, the OO community will sometimes go the opposite direction; while tradition there is to write `A.B().C()`, most language syntaxes I know are perfectly fine with `A .B ( ) . C ()`, sometimes even with newlines permitted in various ways, and sometimes this will abused to create a "DSL". See Ruby. And you get the corresponding "magic" problem there too, where users don't realize "it's just fancy method calls, you can still use all the language abstraction features freely..." and start reverting to bizarre BASIC-esque programming habits, blatent code duplication, etc., because they don't realize they haven't left the original language and are mentally working in some impoverished subset.
I the OpenGL raw bindings to write a 3d rendering system from the ground up during a course I took last semester -- it worked well with tutorials out there for normal OpenGL but I would have liked (/would like) a nicer interface than the raw bindings, for sure!
Seems weird that you don't recommend GLFW-b binding, which seem to be in pretty good shape. Did you try and have problems with it? And I'd like to point out that SFML was recently ported to Haskell ([SFML on hackage](http://hackage.haskell.org/package/SFML)). Maybe Windows support will be better, I don't know if it has been tried.
I love these descriptions of websockets, that make it sound as if full duplex over a single TCP connection were some sort of new thing.
Please go away.
I only did it that way to mimic lens syntax where people also usually omit whitespace around function composition of lenses to simulate accessor notation.
Not to play devil's advocate, but I don't think he implied that. He never talked about increasing Haskell adoption, quite the opposite actually.
I sort of lumped GLFW-b in with all the other OpenGL stuff which more or less works. Some of the OpenGL stuff works better than others, but aside from issues doing translations to/from equivalent C code I've never really had much problem with it. The rest of my comment was aimed at the traditional GUI libraries (Gtk+, Qt, wxWidgets, SDL, SFML, etc.) not any of the OpenGL libraries. I haven't actually used SFML, but if it's like SDL it should work just fine in Linux, and probably fine on Mac as well (assuming XCode isn't causing issues with GCC anyway). Due to the previously mentioned bug in GHC I'm not sure if it's practical to do anything in Windows that uses FFI unless you install a version of GHC that's newer than the one bundled with Haskell Platform and deal with all the headaches involved with that.
Yeah. Let's treat functions and operators as first class and not just punctuation. By that I mean leave spaces around them. h = f.g might be easier to write but h = f . g is easier to read. 
For something extolling intelligence, that is the dumbest trolling attempt I have seen in quite a while.
Your blog is wrong!
I've played around with the higher level OpenGL bindings a bit, but I haven't experienced enough 3rd party Haskell libs to really know what makes a good Haskell interface. Could you elaborate on what you feel the questionable design choices were?
Well my point being that transients offset the seriousnesss of any insert penalties, for my purposes I'd rather have 32-ary HAMTs that play nice with cache. Exposing via ST would be really cool.
I'm interested in hearing a haskellers view of what he calls "different definitions/meanings of purity"
Sorry, I was mocking TheGoodMachine. Now that I recognize the name - this guy has been obsessing over how intelligent Haskellers supposedly are in this sub for a while. It's a bit sad, I think he's got self esteem issues. 
Damn. Hit submit, and *then* realized I had totally missed the "mostly dead" joke opportunity. Ah well.
there is an 'edit' button and a 'delete' button
I think my fingers would fall off!
In the context of this discussion, isn't it mostly about "different definitions of equality"?
52 weeks of Hackage then?
Whoever runs that website should rethink their decision to host Javascript on that &lt;long string of random letters&gt;.cloudfront.net domain. It will make it much harder to get users of NoScript and similar plugins to repeatedly enable it when the random string changes. A CNAME would seem sensible here.
Site idea seems cool. 
One big one is the difference between Haskell's purity and e.g. C++'s `constexpr` (or CTFE in D, which I'm less familiar with): whether you (or any of the functions you transitively call) are allowed to waive enforcement with `unsafePerformIO` or such. That makes sense for functions called normally at runtime, but not so much for ones evaluated at compile time, or if you want to do other 'interesting' things with them (compiling for the GPU is another example I heard, though I don't know how that works).
I really like this series. It gives me something to look forward to at the end of the day all month long
That random string doesn't change usually. It's just a unique ID for their cloudfront-hosted assets.
Haskell's 'pure' functions mutate stuff all the time. fromMaybe ' ' maybeChar -- If this gets evaluated, there may occur a mutation, invisible to the language but still physically present, to maybeChar, which may end up evaluated to weak head normal form if it currently isn't.
&gt; Haskell is the reason I am still a programmer today. I always come at that from the other side—Haskell is the reason I was willing to become a programmer. Prior to that I'd been heavily indoctrinated by engineering and mathematics to think of programming as a menial, artless task. Proficiency in it was something to be desired since it let you be a more effective engineer, but anything beyond Matlab sufficiency was almost looked down upon. I'd spend my time learning about biochemistry instead of lambda calculus. But Haskell was too beautiful to ignore. It so clearly was doing things more right that what I had seen before that it wormed its way into my mind and 5 years later blossomed into what it is to me today and became the vehicle for me to find wonder in CS.
Also: some contents (code snippets, mainly) are not rendered correctly by my Chrome browser. 
That's a breath of fresh air after listening to Gilad's talk.
You can't get that time back.
Link?
Funny to see how I've had a completely opposite start with CS and programming: for me, it's always been the most interesting thing. Then I went to a university in California where CS is at the top of the implicit pecking order, which helped :P. On the other hand, Haskell did get me to consider abstract mathematics much more seriously. I always thought it was completely impossible for mere mortals, which Haskell really helped me overcome. Learning about the deep connections between CS and logic made it even more interesting and accessible. That said, I now think math is just a branch of CS, so my worldview is still consistent :). The fact that it's often viewed the other way around is just a funny historical accident.
It was on the front page of r/programming: http://www.infoq.com/presentations/functional-pros-cons?utm_source=infoq&amp;utm_medium=QCon_EarlyAccessVideos&amp;utm_campaign=QConSanFrancisco2013 
/r/programming ***** ^This ^is ^an [^automated ^bot](http://github.com/WinneonSword/LFB)^. ^For ^reporting ^**problems**, ^contact ^/u/WinneonSword.
If the bug is what I encountered - an unknown section in the generated obj file I discovered it is rather easy to use the gnu 'objcopy' program from MinGW to remove the pesky .drective. Once that was done everything compiled and runs fine.
Very cool to see this here. I'm the co-founder of Gibbon and also the creator of this learning flow. We launched Gibbon a week ago with the premise that it should be way easier for people to learn from each other. We are always open for feedback. You can reach me here or on my email petar [ at ] gibbon.co
That's me :) We are using Amazon's cloudfront domain name and not an alias because we want to use 'https'. Creating an alias on Cloudfront and adding your own SSL certificate is very expensive (600$ per month).
Yes, I agree there is a lot of room for improvement here. Haskell is a difficult subject and I'm still searching for some short introductionary chapters. If you happen to know any, I would be happy to add them.
Hi Johan, I built this during the ZuriHac this year; see [ExampleGeneric.hs](https://github.com/MedeaMelana/Piso/blob/master/ExampleGeneric.hs). This code derives `Iso` values, but you could probably trivially write `fromIso :: Iso a b -&gt; Boomerang e tok a b`. The code is reasonably tricky because the number of `Iso`/`Boomerang` values you want for a datatype is equal to the number of constructors in that datatype, so this `Generic` instance uses a derived datatype for that and then has you pattern match on a list structure to extract the values. The trickiness is increased by [GHC Trac 7268](https://ghc.haskell.org/trac/ghc/ticket/7268) which requires you to pattern match via an extra indirection. Should be fixed in GHC 7.8 though; really looking forward to that release! Also, I'm still looking for better names for the datatype and library before I release it on Hackage. Something more descriptive than Piso/Iso. If you have any suggestions I'd love to hear them. :-)
Which is also what already is happening in Python.
&gt; Prior to that I'd been heavily indoctrinated by engineering and mathematics to think of programming as a menial, artless task. I argue, and I have no idea if I'm alone in that, but I argue that Haskell has a pretty high "skill ceiling." Java code written by an expert with ten years of experience can be read by a Java beginner with only one year of experience. The beginner can learn from reading this and start writing similar code themselves (although they might not know about all the architectural decisions that went into that code.) With Haskell code, it's different. A Haskell expert with ten years of experience will write vastly different code than a beginner with one year of experience. The beginner might not even be able to understand the expert code yet. There are simply so many more opportunities for abstraction that it takes time to learn them all. This, of course, has both benefits and drawbacks, but it's one of those things that make me want to learn more Haskell. I know the thing I write today in 20 complicated lines can be written in 7 better lines by someone slightly more experienced than me.
Maybe it's true for Java, although I think you're underestimating the impact of the absurd number of heavyweight frameworks and OO design patterns. They have a ton of their own abstractions, except they're entirely ad-hoc and full of XML. Because XML is *fun*. I've only seen Java in industry at a company with very high coding standards, but even there it looked nothing like what people would have learned in college. (I was lucky and didn't have to write Java myself, but I did have to modify a few Java files inside a language's type system.) On the other hand, I think it's significantly more pronounced for certain other popular languages, especially C++. That hasn't really held C++ back at all. Also, I've poked around in the Linux kernel a little bit, and the C there is not very close to normal C. Pretty hard to follow, actually! So even having a "simple" language^1 doesn't help overcome this effect. ^1: I'm not actually convinced C is actually simple *per se*, but that's an argument for another time.
Actually you can, since the new Generics in GHC generates a pattern functor which allows you to create a derived datatype. That means you can write a type function that, for example, works like this for `Maybe`: TopLevelBoomerangTerms (Maybe a) ~ (NothingBoomerang, JustBoomerang) which means you can write: rNothing :: NothingBoomerang rJust :: JustBoomerang (rNothing, rJust) = deriveBoomerangs See my other comment for working code.
&gt; I'd been toying with writing my own programming language for 15 years [...] and when I found Haskell I realized that here was a language that not only did all the things I was trying to do, but it did them better. ah ah, the exact same, except for maybe the 15 years was only 8-10 years (but i still want to write my own version of haskell, with a bit different design decisions :)
Is there any way to read this without selling all my personal data to quora? It's ridiculous.
I was hoping for some little payoff from Gilad's talk. I was disappoint. 
Eward, why are you using Quora to write such quality material? You are helping them sell what people write, and I don't understand what your benefice is in doing that. You are already a high-profile member of the community, and could be just as visible by posting here on reddit, or anywhere else you so desire. So far I've only seen one thing valuable on Quora, a post by pigworker. This is the second one. I find that annoying -- I hope that at least you're getting paid for this.
&gt; With Haskell code, it's different. The biggest problem is that you can't google `*&gt;?=~`.
I had to jump through a few hoops to finally get to read this piece - it's too bad that Quora is such a closed system, and honestly, by the time I'd gone through all the screens to join, the review was not worth all that trouble. Admittedly, I was pissed off at not being able to simply read it, and would have had a different reaction if it had been more easily available.
I don't understand why they don't just put a real sockets API in, if they really want that sort of functionality from frontend JavaScript.
Hoogle/Hayoo are usually the first choice when looking for a name. When you know the package it comes from, you can search for it in the Haddock overview as well. [Here's the full list of Lens' identifiers, for example.](http://hackage.haskell.org/package/lens-3.10.0.1/docs/doc-index-All.html)
&gt; (but i still want to write my own version of haskell, with a bit different design decisions :) I have considered this, but each time I decide against it because Haskell is the language with the libraries I like, and my pet language wouldn't have any of that support.
My theoretical solution for that is to keep the language close enough that you can compile both Haskell to it and it to Haskell.
&gt; (but i still want to write my own version of haskell, with a bit different design decisions :) so does edward. and he does right now. see ermine.
&gt; or anywhere else you so desire. I suggest, in order, the Haskell wiki or github, via their html repo thingie.
yes but Edward is much more productive while needing much less sleep than me :) And he does that as a paid job.
Wouldn't r/programming be more visible to "outside people" than Quora? There may be other places for discussion that don't involve giving legitimacy to a budding walled garden. I'm familiar with the "we're not choosing where people are" argument in general, but it would only be a powerful motivation here if Quora was unavoidable. StackOverflow is, I think, but I'm quite sure Quora isn't right now -- and I don't see how the world would be better if it was.
Click the `Close` link on the signup prompt
That's never been a problem for me. If nothing (Hoogle, Hackage etc) comes up with anything, it usually doesn't take long to ask in `#haskell` either. There's always someone who knows and can provide an explanation for how it works.
Note that these "types" (really tags) are not actually types in the type-theoretic sense, and therefore this is not actually a type-checker but a run-time type-tag assertion framework.
I agree with your concerns about helping to cultivate a walled garden. If Edward had, for instance, submitted a tutorial or similar educational materials, I would wonder why he was donating his time and expertise to a for-profit entity. In this case I would guess that most people who read this review will already be inside the walled garden, and potentially many of those people have never heard of Haskell. So it's a question of whether it is worth lending legitimacy to Quora in order to promote Haskell to this particular readership. That said, I'm glad to see you and many others in this thread voicing concern over the issue. And I might be underestimating it myself.
Well SO has a strong deletionist policy and even under a more generous one, answers such as that are ill-suited for it.
https://www.fpcomplete.com/hoogle?q=%23%25%3D
This is exactly why I wrote it there. I've been rather actively trying to reach out to users of Haskell elsewhere outside of our bubble here on reddit and on #haskell. To that end I've been helping grow the Haskell Programming group on facebook, which has more than tripled in size since I joined and started actively answering peoples questions. Quora is another community where there are a lot of people with real questions about Haskell, but very few of us there to answer them.
So, you might want to start by implementing the rotateLeft and rotateRight functions.
Thanks for your reply. Would you consider having a blog somewhere (as suggested by Tekmo below), where you could centralize content you produce on these multiple websites with fragmented communities? I'm not suggesting you stop posting on Quora, Facebook, Google+ or wherever, but instead that you re-release whatever you feel valuable (eg., worthy of re-linking over reddit) to a more neutral space. That would provide a lot of advantages; besides giving each of them visibility, it would enable the superior technology of syndication feeds to help people keep up-to-date with what you write, instead of waiting for it to eventually be reported in one of the communities they actively follow.
You'll need to record the heights (or balance factors) of your trees as well as the value in their nodes. Otherwise you can't know when to rotate. Perhaps take a look at Okasaki's red black trees, to give you an idea of how a self balancing tree is implemented in Haskell. Note red black trees are not the same as AVL trees, so you won't be able to just copy the implementation.
Yeah, that does bug me, too. This is why I like reddit because it's very well suited for open-ended contributions and discussion.
Thank you so much!
I think updated support for [external core](https://ghc.haskell.org/trac/ghc/wiki/ExternalCore) in GHC would be a huge boon to would-be functional language designers. 
I liked his further attempts at the end to discover monads through repeated examples. If you scraped off all the FUD it was actually kind of nice. I also enjoyed the Guy Steele and William Cook ("On understanding data abstraction, revisited") references he left. Almost everything else though... ugh.
This was a paper distantly referenced in Gilad's talk that I found very interesting. It describes a way of thinking of Objects from first principles that is very compatible with Haskell I think.
Almost everything I post to google+ or facebook is just a link to third party content. * My content is mostly on comonad.com * A fair bit of the more recent material is on fpcomplete.com/user/edwardk The main reason I have been using them is because the presentation they offer is rather slick with the dynamically executable Haskell. And there'd be more there, but I'm having a hard time dealing with the general lack of feedback I get due to the lack of comments, and the difficulty of ensuring users see new content when it is posted there. e.g. Having no RSS feed I can send people to kind of hurts, posts redate to the latest edit, so making corrections makes them look like a new post, etc. All of these things are fixable, though! I've been meaning to link up to my fpcomplete articles from comonad.com though. Thanks for reminding me to do so.
Why are you wanting to use the llvm backend for ghc? Also what llvm version? What architecture? Have you considered just enabling swap disk? It's important to note that ghc 7.6 can't be used with clang without some Hacks. And that the llvm backend isn't available for all architectures (and the converse is true too, eg arm is llvm only). Have you seen Joachim 's Travis ci build script for ghc? It manages to build a very un optimized ghc and run some basic tests in under 50 minutes on the Travis vms 
That quote rang true with me, too. I'll be graduating from my undergrad in CS next year, and I can't help but think that I'd be bored to tears if I weren't constantly thinking about Haskell and how my coursework translates to it. I'm actually a little bit worried about entering the job market because I don't love programming in other languages. I've tried plenty but I really, really hate hitting bugs that just would never come up in Haskell due to the type system and the fact that we can reason about code easily. I also feel like writing Haskell code is almost the way of the future, but it hasn't caught on at large. I don't really want to be stuck working on massive OO systems, riddled with bugs that people may have forgotten about popping up in my day-to-day life. It sounds like hell to me. I've been thinking about what I want to do once I graduate for the past three years, because you can't just say "I want to program!" and find your way into a job you'll enjoy. But what I really want to do is pretty simple: *program in Haskell*. I don't care what I'm doing, I just want to use Haskell. Maybe this suits some jobs better than others, and I'd really like to find out what those are, because that's where I need to flock to. Ahh, just ranting. But I wanted to say something here because you guys are awesome, Haskell is awesome, and career choices are stressful!
Bug reports: - after "The laws that the instance is required to satisfy are dual to the", the markdown has a typo (single quote should be a back quote) - the shouty printer behaviour example is missing exclamation marks in the output 
I Love the fp complete stuff 
These are, of course, me messing up formatting Tom's work. Whoops! All fixed now - thanks for the report dave4420.
Indeed. I saw it the other day when going through some random slides on speakerdeck, I thought I might share.
If I defined an object to be a co-recursive (record) type, how wrong would I be?
And one I've just spotted: in "Who’s output is now a bit louder:", "Who's" should be "Whose" or "Its" or "The".
An even more interesting instance of Contravariant is a probability density function (and its discrete analogue, the probability mass function). 
I've been trying to write it so that it shows something useful to a wider audience, but still demonstrates the benefits of understanding the theory.
You have `Behaviour` in some places, and `Behavior` in others.
I was really excited by contravariant functors when I first saw them because they were abstracting a common pattern that I had been encountering (the `Equivalence` and `Comparison` instances). Then I found the `on` combinator in Data.Function which was all I really needed and is much easier to use. I haven't encountered anything else that really lent itself to this abstraction.
I think I'd like to know just that as well. I've been trying to figure out for a while now how badly inheritance looks in that model.
except his presentation of monads makes very little sense, since `&gt;&gt;=` is not a message to an object. I like objects. I'm working on codata. Many things are objects (like functions), and seeing objects everywhere is wrong but I don't think it is a sin. But, the interface here is not the monad interface. `&gt;&gt;=` could be a data constructor (the exact opposite of an object method). Of course, the reason "monads" are usefull is the ability to write code that abstracts over all monads. I think that Gilad misses this percisely because he wants monad to be an object interface, which is overly limiting.
the linux kernel has its own style of C for sure, but I don't think it is overly weird. It is just that you are dealing with a giant piece of software without that many comments. And it is very low level code. So it takes people a while to figure things out. The linux kernel is one of the only code bases which you are expected to read entire books about before you can really contribute.
I suppose I am always giving leeway that these presentations are trying to aim for comprehension by an OO-oriented crowd more than absolute accuracy. It's smart to talk about destructors and interfaces when talking to an OO crowd, but Gilad's ultimate talking point that you're foolish if it's not objects all the way down keeps that divide from being examined.
When you hit this kind of dilemma, take it as a sign to start dreaming of your own startup. Being unsatisfied with all other options and a yearning to make something better is the reason new companies should be formed. Think of software you hate for not being in Haskell and start working on it or make something entirely new. You have a good sense of programming language aesthetics and how a program should ideally be written; don't let this philosophy go to waste by being a 9 to 5 enterprise Java drone for the rest of your life. 
Sorry, inheritance was the wrong topic: I'm curious to learn more about statements about why Haskell does not have subtyping. I feel like you get structural subtyping for free on codata.
What's a computer language with a true product? What is the implementation cost of such a thing?
Take a look to my repo, i've implemented them yet, with comments that explain each step. https://github.com/massyl/haskell-functional-data-structures/tree/master/Data/Trees
I don't know if this is widely known, but another way to read Quora content without logging in is to simply append "?query=1" to the URL. Edit: I think either I imagined this or it no longer works. Doesn't seem to work much now. :(
True products can be implemented in a haskell like way as tuples. You just can't do certain things with them. Languages like Coq and Agda have true products because they don't have bottom. In Haskell without `seq`, you can produce what is essentially a true product using modules module Products (Pair,fst,snd,pair) where data Pair a b = MkPair { fst :: a, snd :: b } pair :: a -&gt; b -&gt; Pair a b pair = MkPair you need the module hiding to prevent pattern matching. `MkPair` can't be exported. In haskell with `seq` and `unamb` and modules a version of real products is perhaps possible. I discovered this about a year ago, and other than an email exchange with Bob Harper haven't done anything with it because it is so ugly pair :: a -&gt; b -&gt; Pair a b pair a b = ((a `seq` ()) `unamb` (b `seq` ())) `seq` (MkPair a b) or at least I think that works. It is pretty terrible to have to use `unamb` (which doesn't even have a coherent space semantics since it is not stable) to get products in a supposedly lazy language. Some object oriented languages (smalltalk varients in particular) might also have true products. Most probably don't, but get called "object oriented" anyways. To understand any of this, see this http://james-iry.blogspot.com/2011/05/why-eager-languages-dont-have-products.html specifically Dan Doel's comment on why Haskell does not have products.
IIRC, he said in his talk he didn't see the point of writing code that abstracts over monads.
Tell me more please.
That's cool and I didn't know that!
I tried to write this name in American English to mimic the simple-actors library but I am too used to writing it the British way ... 
Hi Ollie, thanks for letting me write a post in your great series! I'm happy to be part of it. If anyone has any questions I will be happy to answer.
I have a friend working on something related to PMF, more detail would be instructive and helpful, please!
Very, very cool. Thanks for sharing.
Hrmm. I'm pretty sure that doesn't work unless you only ever apply isomorphisms, making it not a `Contravariant`. Consider: newtype PMF a = PMF { runPMF :: a -&gt; Double } with the double for the probability mass. unit :: PMF () unit = PMF $ \ () -&gt; 1 We can now contramap (const ()) unit :: PMF a assigns everything probability 1, even if there is more than 1 thing. Similar issues arise without surjectivity. You can almost get something proportional to a PDF/PMF, though, which sounds more like the kind of thing you'd do. =) I say almost, because off the top of my head that works unless you map all of your elements to something with 0 probability mass/density, in which case even that assumption looks violated. =( That is unless I completely misunderstood what you were getting at!
That's interesting. So what constraints can we put on the f we contramap with in order to guarantee that the PDF integral remains 1? By the way, jared tobin wrote [an interesting library](https://github.com/jtobin/measurable) where probability measures are monads. 
So far the only valid constraint I can think of is "is an isomorphism".
And don't forget that github pages supports [custom domains](https://help.github.com/articles/setting-up-a-custom-domain-with-pages). You can use [jekyll](https://help.github.com/articles/using-jekyll-with-pages) to get a fairly custom blog and [disqus](http://disqus.com/) for comments if you want something more hands on than wordpress.
&gt;With Haskell code, it's different. A Haskell expert with ten years of experience will write vastly different code than a beginner with one year of experience. The beginner might not even be able to understand the expert code yet. I personally don't feel like this is true at all. I've used Haskell as my main language for about a year now, with about a year of dabbling/tutorial work before that. I regularly look through other people's source code on hackage/github and find it all quite readable. I've started looking into GHC, and I'm finding that a bit tough, but I don't think that has anything to do with language abstractions. It's just such a big project.
I pointed this out when Haskell introduced the non-typeclass seq, but nobody cared. 
It is even simpler, you don't even need fixpoints: http://lambda-the-ultimate.org/node/4468#comment-69927 Functional programming + records + a tiny bit of syntactic sugar gives you object oriented programming.
https://twitter.com/ID_AA_Carmack/status/368402499419394049 They're everywhere, it seems!
f has to be measure preserving, which in the discrete case means it must be bijective, as edwardkmett points out. 
We would love your contributions over at /r/programmerhumor!
It doesn't do the necessary, alas. Object interfaces are mixed variance -- consider a `Set` interface with an`equal` or `union` method. Mixed variance means that the type operator you are taking a fixed point of won't be functorial in any category where least and greatest fixed don't coincide. 
After whining about Quora, I went back and read the review, which is good. I also enjoyed the piece you linked to [How Developers Stop Learning](http://www.daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner). Haskell is a beautiful, elegant, well-designed language, but after years of struggling to get it, I just don't think I am smart enough to program in that language. Maybe it's because I do real-world programming, where "side effects" are entirely the point, and maybe because I don't have the mathematical/academic background to grok monads, monoids, functors, and all the other things that Haskellers joke about. I've taken a crack at Haskell several times over the years, and each time I stick with it a bit longer and learn more, but in the end I get discouraged and quit. Now, what I'm trying to do is to learn ML, and once I'm comfortable there, I'll take another crack at Haskell. I've got more than 25 years of experience coding in various languages, but Haskell is too distant from everything I've worked with.
Someone on r/programming suggested "?share=1".
&gt; but after years of struggling to get it, I just don't think I am smart enough to program in that language. In my experience, sometimes it is just a matter of some mentoring. It took me way longer to learn Haskell than it took some of my friends who had me to mentor them, after I knew Haskell well. &gt; Maybe it's because I do real-world programming, where "side effects" are entirely the point, All the more reason to use Haskell! &gt; I've taken a crack at Haskell several times over the years, and each time I stick with it a bit longer and learn more, but in the end I get discouraged and quit. Besides mentoring, persistence is key. 
He's also a big fan of language semantics which aren't affected by type inference, though. I can see how `return` is a frightening pattern to abstract over if you feel that way. I think having type-optional languages and layered type systems is very interesting, if not terrifically Haskell-like.
add ?share=1 to the end of the url
Link avoiding quora crap — https://www.quora.com/Reviews-of-Haskell/review/Edward-Kmett?share=1
All you have to do here is to depend on a specific version of the package (text in this case) in your application. If the version bounds are too restrictive you have to unpack it, modify the package cabal file and add it to the sandbox for your project. I don't really see how that is easier.
That is something that has always bothered me about the whole idea of having versions 0.99 or similar for betas and release candidates and similar versions. It would make much more sense for 1.0.0 to be the first release candidate of version 1.0 without the implication that it is stable already since it is much more similar to the most stable 1.0.x version than to the most stable 0.9.x version.
Ugh. It would seem then I don't understand what such "non-positive" interfaces denote. It seems I ought to be able to do something "funny" by implementing `ISet`'s `union` method in a peculiar way, but maybe the only "funny" thing I can do is make a non-terminating function without using a recursive let.
You get compile-time type safety between shaders and the rest of the pipeline, e.g. attributes in VBO vs. in vertex shader. Additionally, there's type classes you can implement for your own types to enable them as attributes, varyings, uniforms, etc.
Given the kind of code I write I almost find that position unfathomable. I really think his position comes down to living in a world where types are just annotations a la Curry, not part of the definition a la Church and therefore can't change semantics. What a crippling way to work. I _love_ the fact that Haskell plumbs my typeclass dictionaries everywhere for me. It can DWIM with principles.
* SPJ = Simon Peyton Jones * BOS = Bryan O'Sullivan * ST = Simon Thompson * BH = Bastiaan Heeren * GS = Guy Steele * AM = Anil Madhavapeddy 
That's really cool actually. Is it pretty straightforward to translate any shaders you might find into this DSL?
&gt; Doaitse Swiestra: I’ve been teaching Haskell. The slide says some math things are hard, but these are not the problem. But inside Haskell, there’s another language; a halfway Prolog interpreter with lots of extensions which say few people know how to get amazing things done; I’m convinced in the end, this is not the way we want to do them. So I think this is going the wrong way; take things away from the current state of depenetly typed languages; this is not teachable or transferable. Libraries depend on these extensions; it’s not clear if it’s coherent or not. That’s the real threat. I'm not sure I understand—he's suggesting increased idea flow from DT languages to eliminate typeclass prolog?
Yes
He doesn't want to eliminate it - he wants the term language to be the same as the type language, which is something the DT languages are better about.
* C = ?? 
So the alternative is to require each type implement an instance of a 'Seq' typeclass? That doesn't seem too bad, or am I misunderstanding?
Maybe I shouldn't have said that there is anything fundamentally wrong with it, I just found it _much_ easier to work with the OpenGL-Raw bindings and add some convenience functions like getUniformLocation :: GLuint -&gt; ByteString -&gt; IO GLint getUniformLocation prog str = withGLString str $ glGetUniformLocation prog that correspond to the underlying C-Api but with some of the marshalling of the Raw bindings hidden, than to use the admittedly more typesafe high level bindings. From what I can tell a lot code in the high level bindings is there to make the object model of opengl, where every object is just represented by an GLuint more typesafe by defining things like newtype Program = Program { programID :: GLuint } deriving ( Eq, Ord, Show ) uniformLocation :: Program -&gt; String -&gt; GettableStateVar UniformLocation uniformLocation (Program program) name = makeGettableStateVar $ fmap UniformLocation $ withGLstring name $ glGetUniformLocation program From a Haskell perspective this might be superior, we are tagging and untagging GLuint's with their role as Programs, Buffers and so on. But it deviates from the C-Api enough to necessitate constantly browsing the Documentation, whereas using OpenGL with the Raw bindings is effortless. It comes down to the fact, that the C-Api is documented on some 500+ pages, whereas the documentation of the higher level OpenGL bindings consists of references to the C-Documentation and one has to work out for oneself, what the correspondence is. Also because the C-Api is constantly evolving the higher level library was outdated for a long time, whereas the low level raw bindings can just be auto generated. If the high level bindings would be just some convenience functions on top, they could be updated effortlessly. 
Sure it is. It's just a functor from C^op -&gt; D or C -&gt; D^op (equivalently)
Bug report: In the second line of the equational reasoning for dimap there should be a \g instead of a \h.
If you look at it from this (categorical) angle, then yes. Still it sounds wrong (or at least confusing) from the Haskell programmer's perspective, where I assume that the phrases functor and contravariant functor refer to the corresponding type classes.
Thanks, I'm really pleased it was helpful!
i didn't know ermine is so old!
I don't think that's bad, but other people (John Hughes being very vocal) didn't like it. The objection being that if you seq something polymorphic you need to change the type signature. 
Doesn't seem to bad to me either. Perhaps someone with more knowledge can enlighten us.
It should be called CovariantEndoFunctorOnHask
Any chance a video of that panel will be available at some point?
Ermine was started when I was a consultant. I napkin-sketched what the compiler should look like and Apocalisp ran off and implemented it when I wasn't looking. Now we have a language! ;)
I really recommend looking more into how profunctors are used in lens. The entire lens team has pushed them really far. Here's one quite fun use of Profunctors and lenses, something Shachaf Ben-Kiki showed me once. type ProTrans p s t a b = p s t -&gt; p a b type Iso s t a b = forall p . Profunctor p =&gt; ProTrans p s t a b How can we create something of this type? If it must work for all Profunctors then we can only be using the methods in the Profunctor class, so without loss of generality if `f :: Iso s t a b` then f ≡ dimap (as :: a -&gt; s) (tb :: t -&gt; b) If we had that `as . tb = id` and `tb . as = id` then this would clearly live up to its name and represent an `Iso`morphism between `a ~ b` and `s ~ t`. Using the Functor laws on the covariant functorial parameter and the Contravariant laws on the contravariant functorial parameter we can do this id ≡ dimap id id ≡ dimap (as . tb) (tb . as) ≡ dimap as tb . dimap tb as ≡ from anIso . anIso which lets us define a way to invert our `Iso` (this is the lens `from` combinator) from (dimap a b) = dimap b a So profunctors lead to a nice representation of Isos. Their implementation is just a bit trickier. We need to create a profunctor that simply stores the functions passed to it by dimap. data Exchange x y a b = Exchange (a -&gt; x) (y -&gt; b) instance Profunctor (Exchange x y) where dimap f g (Exchange ax yb) = Exchange (ax . f) (g . yb) from :: Iso a b -&gt; Iso b a from iso = let (Exchange ba ab) = iso (Exchange id id) in dimap ab ba (^.) :: Iso a b -&gt; a -&gt; b (^.) iso = let (Exchange _ ab) = iso (Exchange id id) in ab This continues further by adding more constraints to the `p` in `ProTrans`. type Lens s t a b = forall p . Strong p =&gt; ProTrans p s t a b type Prism s t a b = forall p . Choice p =&gt; ProTrans p s t a b There's also ekmett's [PHOAS For Free](http://www.reddit.com/r/haskell/comments/1mo59h/phoas_for_free_by_edward_kmett/) post which talks about how to view HOAS as a profunctor and get some nicer structure for the effort.
Gibbons' *Unfolding Abstract Datatypes* gives something like `recursion-schemes`' `Data.Functor.Foldable.Nu` data Obj f = forall self. Obj (self -&gt; f self) self for a strictly covariant functor `f` embodying the restriction that you can only examine the `self` object's internal representation, but would need to be able to `equals` or `union` using only the public interface `f` of the other object type Set a = Obj (SetF a) data SetF a self = SetF { isEmpty :: Bool , member :: a -&gt; Bool , union :: Set a -&gt; self ... } So, that's one way to get rid of contravariance—enforce really strict representation encapsulation. 
Don't dependently typed languages like Coq go even further than Haskell in this regard? I was under the impression that Coq has a full backtracking logic programming thing for inferring missing pieces of programs (which is basically what type classes have to do, they infer the type class dictionaries).
Coq has backtracking type class instance search. On the other hand, agda has basically no instance search, an 'instance' is only used if a single one with a matching type is in scope.
The use of profunctors in lens is very cool. That's where I first got the hang of them. 
Yes! You can use Hakyll too, if you want to generate your blog using Haskell. I did exactly this last week; you can see my blog over at http://5outh.github.io !
&gt; Maybe it's because I do real-world programming, where "side effects" are entirely the point This is precisely why I *prefer* Haskell over impure languages. When side effects are the whole point, the ability to track them in the type system is crucial. In order to reason about effects you need to know both what can happen and what cannot. In impure languages you can sometimes talk about what effects are possible, but you can never talk about those which are impossible; which in turn makes it profoundly difficult to keep all your ducks in a row. If my program is pure, then I can get away with using an impure language— because I will, in point of fact, not need to reason about effects. However, if my program has effects, I'd much rather use a pure language— because that way the computer can check my work, instead of me needing to simulate everything in my head.
Thank you for the detailed response. &gt; I don't know if this is related. Changing the release schedule is certainly something that is open to discussion, but it's never really been dependent on the amount of work/people available. There are people who want a shorter release schedule though, so it's completely reasonable to talk about on its own (I just don't think a dockerfile in and of itself will change anything here, and adding developers isn't necessarily related to scheduling.) It's not a matter of schedule of just GHC, it's a matter of eyes on it and all of the small delays that add up for everyone downstream. The library writers, the frameworks that depend on those libraries, the application layer libraries that depend on those. It all adds up to a lot of waiting for an end user, which is why even if GHC 7.8 dropped tomorrow, it will take months for the Haskell Platform and everything else to catch up. That sort of thing just makes GHC less desirable when important features like the new IO manager are perpetually "two months away" - and this is speaking optimistically. &gt; Don't bother - bootstrapping the compiler with LLVM bootstrap is rarely needed, occasionally buggy, compiles slower, and doesn't really bring a measurable performance increase (in fact, the native code generator beat it at some important things in the runtime at one point, so it might not be faster!) These are all good reasons not to build GHC with LLVM. I only chose `perf-llvm` settings for the stage 2 compiler because I was had the impression that LLVM codegen is The Way Of The Future(TM) and the other paths will be gradually deprecated. I strongly believe in the principle of least surprise - if LLVM is going to be the default eventually, then the library writers and end users of GHC should start testing that now and make sure it works. &gt; Not reliably AFAIK. The way the build system specifies the 'all' target is a bit tricky, so there are a lot of dependencies (find . -type f -iname '*.mk' | xargs grep all-target and read around.) &gt; &gt; To be honest, I don't really see a point in doing it this way. I don't really see dockerfiles as a solution to a development problem but a deployment problem. They are somewhat related, of course. Dockerfiles are just a neat way to get a reliably constructed environment on a wide variety of hosts. The reason I want to split up the build is because my laptop has trouble running the `.\make all` without hitting an OOM. If I hit an OOM in the middle of the monolithic `make`, the entire `make` must start over because all intermediate computations are discarded. &gt; This leads into another point, which is that I don't really see why a Dockerfile should be used to build GHC, but one containing a version of HEAD as a system compiler would certainly be useful. That is, let people develop on their own systems how they want - but allow isolated containers for people who want to test their source code. If a person is going to work on GHC, I highly doubt a docker file will be the most comfortable solution for very long. Agreed! This first Dockerfile is largely for users of GHC, not GHC devs. &gt; Accomplishing this is a lot easier in fact, too - just build a version of GHC HEAD on, say, an Ubuntu 12.04 machine (spin up an AWS instance, your VMWare/Virtualbox, whatever.) Create a binary dist, put it somewhere online (like community.haskell.org) and have your Dockerfile just wget and install it into a 12.04 base image. Your dockerfile is simpler, easier to maintain as GHC goes forward, and independent of the build environment. People can also test it with an LTS Linux release, which helps ensure, well, stability. The problem with this is that if I accept your solution, now I have many problems: 1. Creating the binary dist 2. Having a place to put it and establishing a process for updating *that*. 3. Now I have to maintain both a Dockerfile stored in GHC and a third party hosting service and maintain them. Or upload big tarballs to GitHub, I suppose. 4. Now I need a process for updating those tarballs. Those are a lot more things to implement and they will consume a lot more of my time than I like! Unless someone is volunteering, my first goal is to have a single point of failure, single script in a Docker container that reliably builds GHC-HEAD. Something other people can immediately (if not very quickly) start playing with and which doesn't place a substantial cost in terms of time and effort on my part. 
You're a programmer, do something about it. For example, run this in the JS console: document.body.style.fontFamily = 'Arial'
LLVM 3.4, x86-64 on Ubuntu 13.10. I work around compatibility by building stage 1 using `-fasm`, which builds GHC-HEAD without the LLVM backend, and then stage 1 builds stage 2 with `-fllvm`. I haven't seen Joachim's build script but am looking into it now. Thank you!
The problem isn't the backtracking search, per se. The problem is that we're mixing two very different paradigms. If I want to write a function on the term level, then I write it in functional style; if I want to write a function on the type level, then I write it in prolog-esque style. Languages with full dependent types get rid of this discrepancy by re-using the term-level language at the type level. This greatly simplifies the conceptual model of the language, since you only have one language/paradigm instead of two.
Though, tbh, there are a number of nice things about playing type class prolog. In particular, you get the benefits of logic programming where you can define both a function and its inverse(s) at the same time. It'd be cool to have this sort of functionality on the term level as well. But then, this gets into that whole functional vs logic programming debate.
That's a fair point. I personally think it is a bad thing that `Functor` means 'endofunctor on *'. It is rather limiting.
Here's a couple of notes for you! 1. `prettyPrint` is `unlines` from Data.List 2. In `main`, you can get rid of the lambda expression and just use `mapM_ makeWordFile` 3. `getWords` need not use `$`. You can remove the `book` at the end and in the function definition, and replace `$` with `.` 4. Since you're only using `discardCount` once, I'd just inline it. It's actually a lot easier to read your code that way. :) 5. `getFrequency` looks a little iffy to me, I'll discuss this in detail: You have the following: getFrequency :: [String] -&gt; [(String, Int)] getFrequency wordList = [ (word, length $ filter (== word) wordList) | word &amp;lt;- nub wordList ] You're using a lot of expensive computations here when you don't have to. You can write this function like so: getFrequency :: [String] -&gt; [(String, Int)] getFrequency list = map (\x -&gt; (head x, length x) ) grouped where grouped = group . sort $ list This still isn't super ideal, but it should at least make sense to you and will perform better because you only have to traverse the list once. You're traversing it `n` times when you use `filter (== word)`. (Also, as an exercise, you should try to make this function pointfree!). Overall this looks great for a first use of Haskell, great work, and I hope this helped a little! :) Edit: One more thing! I noticed a rogue `reverse` call. Typically you should try to avoid those at all costs. You can do just that by swapping `compare` in `sortWords` to `flip compare`. Where `compare x y` compares `x` to `y`, `flip compare` will compare `y` to `x`, so you get the opposite ordering without the `reverse` call! Edit 2: As /u/tWoolie mentioned, `getFrequency` can be rewritten in an idiomatic way like so: import qualified Data.Map as M getFrequency = foldl' (\word -&gt; M.insertWith (+) word 1) empty Here we're using a strict left fold; if you haven't seen them yet, you will, and working through this example might be a good way to learn!
You may notice that the performance is not all that it could be. Have you considered using [`Text`](http://hackage.haskell.org/package/text-1.0.0.0/docs/Data-Text.html) instead of `String`? Also, have you tried writing some tests for your functions, to verify that they work as intended, and do not reject valid words? i.e. what is the purpose of rejecting words that have punctuation in or around them? Edit: I should have said this before, but this is a really great first program, and it's great that is simple enough to keep on a single page, yet is deep enough to introduce you to the subtleties of the language and is cost model. Keep up the great work, and ask plenty of questions. 
Possibly, although I should point out that there are many languages with both sums and products. The calculi mu mu tilde has a very elegant version of both for example. In a true object oriented language you could define object{ def fst = a def snd = b } and this satisfies the laws you want except for eta, even under strict evaluation, because you don't evaluate under binders. Objects don't look like binders, but secretly the object keyword is a co-case statement. Getting eta requires some effort, but it can be done. This is important because Palmer/ lazier functional programming has problems with both implementation and semantics. You have to give up *stability* of functions, and this is a very important property to getting a lot of nice things to work out. I'm not saying lazier functional programming is not a cool idea (it is!) just that you a reaching for a more powerfull tool than is necessary. I think the conflation of "strict" with "has co-products" and "non-strict" with "has products" is a consequence of the limits of the natural deduction/STLC as a foundational calculus, in the future I hope we can fully seperate these dimensions.
I will definitely look into the use of Text. What is the difference in performace and usage between Text and ByteString? The reason I reject words with punctuation is because I couldn't figure out a way to strip the punctuation in a clean and idiomatic way. But also, I assume that a word will appear both with punctuation and without so I should catch it nonetheless.
You could use `filter` to remove anything that's not a ~~character~~ letter. `Data.Text` has a `filter` function of its own also.
oh and I think you should use `isAlpha` instead of a possibly incomplete list of nonletter symbols.
as /u/fc31gi2N said below, an idiomatic way to strip punctuation is `filter isAlpha`. You can use this with `Text` also, using [Data.Text.Lazy](http://hackage.haskell.org/package/text-0.11.2.3/docs/Data-Text-Lazy.html). `Text` is better performant for processing pure unicode text than `ByteString` is.
The difference between bytestring and text is that bytestrings literally work on arrays of bytes (which loses clear deliniation between non ascii characters), whereas Text works on utf8 encoded textual data, but in a space/time efficient way. The Text library also contains functions specialised for dealing with textual content. Performance is similar between the two.
The idiomatic way to get frequency lists is Data.Map.insertWith
Why would you build the frequency list with a right fold? There is no shortcutting available, and no cons work to speed up. All you're doing is building a big chain of thunks that all have to be evaluated as soon as you get the first value out. This is actually the canonical case for a strict left fold.
Very intriguing. Hope you'll update us on how it went when everything is built!
We would like to emphasize (incorporating feedback) that the task is optional. It's there to give a chance to younger Haskellers, or those who have predominantly worked in corporate environments without a chance to open source their most interesting code (I'm thinking of SCB in particular). If you have substantial contributions already, you should of course skip the task and just link us to GitHub. 
A bit of additional information would be helpful. Like what continent the offices are on.
Asia
Sure will. A blog of the Haskell experiment is long overdue. The problem is that success begets more work. So we never have time to start that famous blog we've all been talking about for months... Edit - oh well, didn't quite go as planned. I left last month to do my own thing. I do wish the team all the best and they might still manage. So much more I would like to say, but you'll understand if I stay quiet for a while. Edit 2 - after a change of leadership, the project was scraped. The Haskell team writes a few things modularly on top of the PHP heap. Some people followed me. Some of the departure letters were hard to read. I now encourage people not to attempt rewrites, unless there is a seriously good incentive to complete them. 
I like it. Maybe a useful approach for certain problems. But, as is now, your business logic is too coupled to your framework (perhaps to write a short example), what's happen if you run some instances of the same workflow (processing the same data)? do you need distribute STM?. I think, a better solution is write a "statical" business logic, delegating transactions to the backend (yes, the traditional way); you can run many instances from many channels (workflows, command line programs, web services, ...) at the same time, all use the same static business logic and transactions (e.g. a book is just sold) are granted from the same backend. I think the traditional approach is much simpler, flexible and robust.
Thanks again, ezyang, for your fabulous note-taking! This is a really interesting session that I missed, and I'm glad to be able to re-discover it here. I won't be attending POPL so I hope you are, and will be on the keyboard again.
Everyone who works at SCB has a good repertoire of open-source software publicly available already. If they didn't, we wouldn't have hired them in the first place. I am very glad you removed the project requirement - it certainly puts me off applying for jobs, as it can value spare-time over ability.
It's perfectly feasible. You just have an ADT that can be compiled to Javascript, say, and also interpreted within Haskell itself. You don't need a typeclass.
Looks a lot like the Lazada ad earlier. My guess it's the same company. 
They copied an earlier ad.
This. Every last word of it.
what's wrong with filesystem acl?
I suppose that's still extensible in the runtime environment, since all you have to do is find a way to compile or interpret the data type. The thing that put me onto the idea of a typeclass was the idea that you could have the language primitives as part of the typeclass definition, e.g.: data Javascript = JSFunc [String] Javascript | ... instance Lang Javascript where compile (JSFunc args code) = "function(" ++ commaDelimit args ++ ") {" ++ compile code ++ "} ... -- Language Primitives func args code = JSFunc args code ... and so anyone hoping to add their language would only have to implement a select few language primitives, like how to set variables, call functions, etc. The end result wouldn't be as expressive as most existing hs2js solutions, but it would enable hs2* code.
I don't think your typeclass buys you anything here, over just a couple of functions.
Fucking Starcraft against someone who's probably an Asian guy, yeah right.
That's fair and understandable, especially for people at your level. I clarified the task requirement (which was never compulsory, although I should have made that clearer initially by adding "mutually exclusive") after hearing this from you via someone from your team. To explain further why we do it: - the first time we put out an ad, we had 200 applicants. 198 of them had no Haskell (or FP) experience. Maybe 50 bothered writing something other than 'Greetings' or 'Dear Sir/Madam' in the cover letter. Bypassing HR takes time and effort, and I feel very strongly that I should review every CV. So we put up a (much simpler, ~2h worth) filter the next round. - on the second round, the filter worked wonderfully as both a signal that we were serious about Haskell and to stop the swarm. Within a week we had 30 applicants, around 20 of which were at least qualified enough to be considered. We gave detailed feedback to every one of them. The top guy (who by now many of you have identified) was hired without completing the task - he just sent us his numerous contributions on GitHub and Youtube and said "sorry, I don't have time unless I'm sure I'll get the job". We hired candidates 2 and 3 as well (even though we officially didn't have the budget for it) because of the strength of their task solution and what it said about their experience and style. Based on this we decided that clearly (most) people considered that the value of doing the task (a chance to work on a Haskell team in a job market that has relatively few opportunities) outweighed the value of the time lost solving it. In three cases people actually mailed us a solution despite not wanting the job, just for fun. Expecting a strong response (which we have definitely had so far), we made the task harder this time. One last reason that we put up a task and that we will continue to do so, is to test motivation. If you are fortunate enough to be in a situation where you can be picky about applicants (such as when you have 20 serious candidates per spot), something like this shows who is determined enough to bother using their spare time to show motivation. This was especially important in the early days here when Haskell was built in stealth mode, and we were actually forbidden to use the language anywhere (well, a solution with a Haskell backend and JS front is JS, right?). Edit: the biggest thing that puts me off applying for jobs (and others in the team) is unnecessary, time-consuming admin. I include in there dealing with HR's various hoops, having to compile an up to date CV and provide references, wearing a suit for any number of meetings, answering unnecessary questions like "why do you want to work here" and so on. Pretty sure all in all the task is considerably less time and mental effort wasted, with much higher chances of success and at the very least some interesting lessons if failed.
He's pretty good.
I thought so too. Sister company, different team. Same investors.
Different investors, e.g. last funding round from Tesco for Lazada only; the companies have separate management and do not communicate, and Lazada HQ is in Bangkok (where a Singapore salary would go much further). The key difference for candidates is that there are no Haskellers in Lazada doing 6 months of prep work for you (and believe me that prep work was not easy nor fun), and they are sticking to the PHP stack, which causes a lot of upstream problems. We killed an attempt to rewrite in Go, but it was close. On the upside, Lazada offers a chance to build a team from scratch, and I'm pretty sure that in 6 months they too will be writing their own Rebuild ad. 
Epic.
Nothing. Why?
That's correct, I used C when I did not know who the person was.
You're absolutely right. Sorry, I wrote this really late at night and wasn't thinking, thanks for pointing that out -- I fixed it.
&gt; The key difference for candidates is that there are no Haskellers in Lazada doing 6 months of prep work for you (and believe me that prep work was not easy nor fun) I feel kinda stupid as I don't understand what's meant here... could you elaborate more on what "prep work" actually is? 
We tried something like that some time ago. The whole thing kind of worked, but wasn't more than a very rough prototype. We played with Haskell, JavaScript, FRP JavaScript. What we did is make some language parametrized expression language, build an overloaded prelude of functions for this and interpreted this as Haskell, or compiled it to JavaScript and also a functional reactive library for JavaScript. Code can be found here: https://github.com/sebastiaanvisser/AwesomePrelude/blob/master/ Language parametrized expression type: [Lang.Value](https://github.com/sebastiaanvisser/AwesomePrelude/blob/master/src/Lang/Value.hs) Specialized to JavaScript: [Lang.JavaScript](https://github.com/sebastiaanvisser/AwesomePrelude/blob/master/src/Lang/JavaScript.hs) Specialized to Haskell: [Lang.Haskell](https://github.com/sebastiaanvisser/AwesomePrelude/blob/master/src/Lang/Haskell.hs) Compiler: [Compiler.Pipeline](https://github.com/sebastiaanvisser/AwesomePrelude/blob/master/src/Compiler/Pipeline.hs)
Wow, where did you advertise this job? I had the polar opposite when I was looking for Haskellers, I only got one rubbish applicant (Dear Sir/Madam) and a hell of a lot of awesome CVs. I put my job ad here: http://hackerjobs.co.uk/ (which as you'll note from the domain is UK specific) but I got a lot of applicants from around the world too. Best of luck with your search, it's great to see the Haskell pool of jobs becoming a bit deeper! 
Yes that was my experience too - I think all of the decent people would have found me without the job ad. Getting retweeted by dons is just like MasterCard: priceless :-)
More type signatures, please! When I read lines like "What it returns is a STM variable with False as its content," I think, "If only there were a concise standard format for describing such things."
My fiduciary duty to the shareholders forbids me to expand on this, sorry.
`cat &lt;file&gt; | tr " " "\n" | sort | uniq -c` Not to spoil the fun of Haskell. :)