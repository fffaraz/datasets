yesod web framework has a command line tool that also provides scaffolding generator `yesod init` starts a wizard that will ask you for some preferences and output a fully fledged site with persistence(several dbs available) and some example code. all coded by community-accepted best standards and following a reasonable architecture 
Using yesod, one can bootstrap a complete application using the yesod-bin package. `yesod init --help` for more info.
&gt; what Roman calls idiomatic Haskell is a matter of taste Isn't that basically the point of idiom though? You can write Fortran in any language but it leaves a bad taste in the mouth. Idioms can be opaque but they're *our* type of opaque and anyone else's way of doing things seems wrong.
“Big” frameworks have utilities that will do it for you. `yesod init`, `snap init` etc. 
https://github.com/yesodweb/yesod-scaffold/ -- checkout its various branches.
&gt;&gt;&gt; The point is that there's already (.) with defined semantcs. Lens redefines it in unnatural way &gt;&gt; It doesn't redefine it. It uses the usual (.) operator. &gt; I know. This is not what I'm talking about So what are you talking about, then?
Some people have the opposite experience. That diagram is my primary resource when I need to figure out how to write some lensy bit.
&gt; This is the conflation of two meanings of the word "depend". lens "depends" on a JSON parsing library and a gzip compression library in the "cabal" sense that it compiles against them to provide features for them. It does not "depend" on their functionality to provide its own functionality. The expression problem all over again.
Why is that bad? I looked at the documentation of `^?` and it says that "Perform a safe head of a &lt;..&gt; Traversal"
I'd prefer to have another operator that only compiles when you know you are targetting at most one element.
&gt; Haskell has a rich history of adopting powerful abstractions that are impervious to those new to them. But... monads were boiled down to small core before they received widespread use. In fact, only recently has the community decided to make the small core a bit larger (adding **join** and the Applicative super class constraint). Lens simply doesn't have a small core.
As someone who does not use the lens library everyday. I did not have a problem with the example in (1). below :: Traversable f =&gt; APrism' s a -&gt; Prism' (f s) (f a) I knew it would take a prism like _Left. Left 5 ^? _Left == Just 5 `(^? below _Left)` should then take [Left 5] and give back Just [5]. I also had to know to ignore the A in APrism'. Which I just had as a vague memory and confirmed after I tried the above in ghci. I however can not read all of the type signature in prism module and understand what is going on. Mostly though I think this is due to me not yet understanding or internalizing the meaning of the various type synonyms more then anything else.
&gt; It's disappointing that I have to give errorProne the same parameter twice, and that no, I can't use join to get round that. It feels like Haskell's type system is getting in the way for once. The issue is really that lens families shouldn't have four type parameters, they ought to have a single *type family* parameter, e.g. `Lens x = Functor f =&gt; (x s -&gt; f (x t)) -&gt; s -&gt; f t`. But Haskell doesn't allow type families as arguments to types.
Ah, so an operator that works only with `Getter`s, like `view`? EDIT: Nah, `view` also works with traversals, I see your point clearer now.
Ideally one that works with only "affine folds" (folds of zero or one element) but encoding affine folds seems to be somewhat tricky ...
The lens dependency that bothers me is TH. Take a look at the sad state of getting lens to build on Debian's multiple architectures: https://buildd.debian.org/status/package.php?p=haskell-lens&amp;suite=sid I don't know what's up with the test suite failures (bad assumptions about floating point?), but the missing Reflection is lack of TH. Debian already has a patch extending lens's DISABLE_TEMPLATE_HASKELL ifdefs, but it seems to need ongoing maintenance. My worry now is that I'll have Haskell code that is happily deployed and working on architectures where TH is not supported (ie, arm), and a library it uses will start using lens, and then I'll be stuck.
Yes, http://hackage.haskell.org/package/data-lens -- which is what I use.
I should probably add this is one of the reasons I like Haskell so much - there's always something else to learn. I'm fairly proficient at Python and I think it's a great, well thought out language but I don't enjoy it in the same way as I did when I was learning. I still like writing in python and I still enjoy it as much as I did, but there's also an enjoyment to learning new things. Haskell seems to have an endless supply of those new things and it's a lot easier to see yourself getting better at something during the beginner stage than the intermediate/advanced/superhuman/god stage. Then again, I may just be talking nonsense :P
Arguably it can have a small core ( i.e. [lens-family](http://hackage.haskell.org/package/lens-family-core) ) the debate is mostly about where the lens instances and operations of all the common types should live since they can't become hard dependencies of the data structures packages ( vector, container ). 
Could we include more categories for `GPLens`? For example, `Arrow`s without `arr`.
While `lens` is wonderful, the dependencies are ridiculous and the type synonyms hurt as much as they help. I hope for more emphasis on using `lens-family-core` going forwards so that not every use of records depends upon `aeson`. I've had to stop using `lens` (and thereby `linear`) in some projects to reclaim sane sandbox reconstruction times. The type synonyms, meanwhile, hide the arrows we usually rely upon to figure out how things fit together. Without them, though, we would have to invent our own jargon to simplify casual reasoning about recurring patterns, so I don't know what can be done here.
Nearly all operators follow simple rules in lens, see for example [here](https://github.com/quchen/articles/blob/master/lens-infix-operators.md). So `&lt;&lt;&amp;&amp;~` is composed of `&lt;&lt;`, `&amp;&amp;` and `~`. `&lt;&lt;` means "return old value", `&amp;&amp;` just stands for the boolean and operation and `~` means modify. So that operator takes a lens and modifies it's value by ANDing it with the argument. It returns the old value of the lens. 
I wrote `data-lens`, and then promptly started smacking my head into the ceiling of what it could do. Every time I would write something with that style of lenses and needed to do something just out of reach of it, I'd have to stop, scrap _everything I had just written_ and start over. `lens` is an exercise in avoiding that.
So not even `&lt;&gt;` for semigroups?
Notice that the type is much better if you use `_1` from `lens-family-core`: (_1 %=) :: Monad m =&gt; (a -&gt; a) -&gt; StateT (a, b) m () ... and if you use `lens-family` instead you get the `MonadState` version of that.
I've really got to try this out sometime. I wanted to use `lens` in a library recently but it was a non-starter because of its massive number of dependencies. `lens` would probably be fine for a big system that has plenty of dependencies anyway, but I wouldn't want to make a library depend on it.
I switched from this exact setup to haskell-mode and emacs, with flycheck, ecb, and many other plugins including evil-mode to keep vim key combinations. I'm much happier with emacs, the plugins are much more robust and things "just work" for me much more easily. In my experience, Emacs makes for a decent IDE, whereas vim really shines only in text editing.
I'm generally not a fan of semigroups (and semigroupoids) because I don't like proliferation of concepts and abstractions.
I'm actually open to accepting patches that would make lens build cleanly for stage1 platforms. I've done most of the work to ease the way, including hiding all the ANN pragmas for HLint, behind `#ifdef HLINT` guards. There are very few uses of {-# LANGUAGE TemplateHaskell #-} within the package itself. Control.Lens.At, Control.Lens.TupleIxedTH, Control.Lens.TH, System.IO.Error.Lens src/Control/Lens/At.hs + src/Control/Lens/Internal/TupleIxedTH.hs use it for convenience to make Ixed instances for tuples. That could switch to CPP. src/Control/Lens/TH.hs provdes the template-haskell API, but you should be able to build it at least on stage1. src/System/IO/Error/Lens.hs is the tricky one to get right because the generated code varies a fair bit across GHC versions. If someone wanted to go through and get all the CPP written to protect a non-TH version of that across GHC versions back to 7.4 then you could shed the dependency there as well. That should give you `stage1` builds of lens for embedded systems. If you wanted to pitch in on that front, I'd be happy to help.
Anyone who hasn't used overloaded `_1` hasn't lived.
If you put all the things for other libraries into an `-extras` package, you will probably have the same complaints in that when someone needs `-extras` for one thing, they pull in all of hackage. If you instead have a bunch of `lens-` packages, then the logistics of versions and package uploads get complicated. As a user, I'd prefer a proliferation of support packages that can be absorbed into the packages they support over time, but I appreciate, and sympathize with the developers' desire for, a single umbrella package.
Could you be more specific about when you felt `lens-family-core` was insufficient? Also couldn't most of the library be written with horrible type signatures and no `lens` dependency or do you use `lens`es in the implementation as well as the API?
I put the UML diagram up there to speak to a different audience -- the audience who understands UML diagrams, but not mathematics. It let's me show it to a crowd that only thinks in those terms and say 'you can even see how to access our accessors' with UML. I get a number of complaints and kudos in almost equal measure for most of the design elements in lens, and try to fix things when the balance tilts too far in one direction. With a project this size, I can't please everyone, so I choose to try to be true to a few goals: * Be non-prescriptive about how people should use the library * Put the laws front and center in everything we do. There are no "monadic lenses" in lens, because nobody can give me a consistent set of laws for them. * Let almost everything work with everything for fundamental reasons. We bend over backwards in the implementation to ensure you can use all the combinators in any sensible way with the lens variants that should make sense with them, but without making up something to get ad hoc overloading. Any such overloading should exist because it is the only sensible definition. * Fit into the cracks in the namespace. It uses gerundive nominals, past tense, leading _'s, borrows the `_1` convention from scala, starts prisms with _Foo, etc to get out of your way and try to maximize the chance that its names won't conflict with yours, so you won't be forced to import it qualified. * Let people define lenses without having to depend on lens. This lets us break the terrible library dependency order problem up, and makes it possible for other libraries to choose to supply a lens or two if a user asks for it, without having to haul in the library. * `lens` doesn't try to be the smallest thing that can work. `lens-family` already existed when `lens` was written! Consequently, we decided to ship "batteries included" for the Haskell platform. I hope that I don't try to defend lens from a 'people shouldn't be scared of math' standpoint. I try to be very clear that lens puts together lots of very fundamental things, but I freely admit it does so in very strange ways. It is necessary for lens to work that it leak some information into the types. Otherwise every time I added a point in the lattice of possible optics, I'd need a new set of combinators. If you think lens is bad now, make it take 12 times as many names. ;) If I only wanted to appeal to the mathematics crowd, a lens would look more like type Iso s t a b = Profunctor p =&gt; p a b -&gt; p s t type Lens s t a b = Strong p =&gt; p a b -&gt; p s t type Prism s t a b = Choice p =&gt; p a b -&gt; p s t ... `Strong` and `Choice` and `Prism` are made up words, though strong here alludes to monadic strength, choice to `ArrowChoice` and prism was needed for something that sounded like it split. `Strong` and `Choice` are close to saying these are `Representable` and `Corepresentable` profunctors respectively, but make slightly weaker claims. This wouldn't serve the goal of adoption, however, as it'd mean you couldn't define a `lens` quickly just using stuff from the `Prelude`. &gt; Certainly the proliferation of arcane syntax like ^., .~, %~ isn't helping. The choice of those 3 operators is something that the lens community spent an awful lot of time trying to get right. I'm sorry that you do not see the utility in them. However, descendants of them go right back to the genesis of lens as a 20 line tiny library so I could talk about field access nicely inside of a physics engine. I personally find that working with `x^.foo.bar.baz` is very convenient when thinking of lenses as field accessors. It appeals to the vulgar sensibilities I acquired writing in imperative languages like Pascal, C/C++, Eiffel, etc. before coming to Haskell, and it nicely encapsulates all of the power of Erik Meijer's ["The power is in the dot"](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.146.5411) behind a sightly more principled back end that can now sensibly be fully bidirectional. If you don't like operators. you can use them as `view`, `set` and `over`. In lens literature `view` is commonly called `get` and `set` is commonly called `put`. If you never want to work with anything more complex than a lens then that is enough to get by.
This is what drew many experienced Haskellers to the language: the community believes strongly in finding better ways of doing things. While we do have some conservative voices that keep us from always running headlong into the flames of change, there aren't many PL communities of significant size that are so ready to adapt based on convincing evidence or argument. Lens is a striking example of this.
Sure, there are things that are better supported on Windows (I guess gaming is still one of the things) and other things that are better supported on Linux (like GHC), I didn't say that you'd suffer on Windows generally, but rather in the case of GHC, as GHC seems to be better optimized for the Linux platform.
That's a nice summary of the core, but the role type classes play distributes the burden of understanding quite broadly. Understanding how various type classes give rise to lens semantics requires thinking about those type classes more carefully than most have done in the past.
FWIW, I bet it was GLUT and not OpenGL/OpenGLRaw that gave you grief :)
Building packages with external c/c++ deps is hard on windows. Another way to ask my question above is: Does the HP have many (any?) packages like that? It sounds like network can be painful and possibly OpenGL (although I think that one is no longer tricky, I've cabal installed the latest OpenGL with no problems several times on windows).
Flipped `^.` is just `view`. `view` is even more powerful, as it works in any MonadReader, not just `(-&gt;) s`
The fact that that is undocumented, I am sorry to say, is entirely, utterly, and completely my fault. I added it, and many like it into the library but left out documentation, which I really ought to write.
If you ever composed `map.fmap`, or `fmap.map` did the Prelude redefine `(.)` on you? Or were you rather misattributing its semantics? Semantic editor combinators were using this order long before `lens`.
I am very open to improved documentation. Documentation fixes go through very quickly as pull requests go.
https://github.com/ekmett/lens/wiki/FAQ#lens-core
After I exploded category-extras, development on it effectively stopped for 2 years, and I never found a good place to put all the pieces. Worse almost nobody else could find all the pieces, either, as everything had been collapsed down to Haskell 98 cores, extension packages, etc. It had to happen, but it lost almost _all_ inertia at that point. `lens` has grown pretty stable, subject to rather minor tweaks on the periphery, but if you run through the dependencies there are only ~3-4 that could be removed without undue pain and doing so would then complicate explaining the batteries included policy.
I also hate the magic with records. Define the fields one way and just know that you'll access them with a different but similar symbol.
That's pretty presumptuous. Sometimes the "old value" can be behind a potentially-failing lookup that you've already guaranteed succeeds. Why do another lookup?
It's nice that there is a kinda-defined vocabulary for lens operators, but we already have an expansive and fairly well defined vocabulary: the English language! (now I suppose, one could argue operators are language agnostic, that's a plus, but the documentation is all English so there's no real point there)
My counter-argument arises from the fact that I don't want to walk twice through a state monadic stack when once will do. class HasFresh t where fresh :: Lens' t Int next :: (HasFresh s, MonadState s m) =&gt; m Int next = fresh &lt;&lt;+~ 1 next = do x &lt;- use fresh fresh .= x + 1 return x both bump and returns the previous result of a counter. The former works executes in half the time, is more succinct and isn't needlessly naming a temporary. You're welcome to use the other approach or not, but I'd be remiss if I denied someone who wanted the opportunity to write the former. Some of these lens chains can be arbitrarily complex. Folks have written them so that they can go down and edit appropriate leaves of a trie for instance. Going down there twice when you can go down once do something and bring an answer up seems pretty straightforward to me.
Without some massive cleanup, this is utterly out of the question.
I'm not sure what you're getting at, can't you just use pattern matching? case lookupIsh value of Just v -&gt; foo value v _ -&gt; bar value This is again presumptuous, that it's simply `Maybe a`, but you understand what I mean I hope. 
Is the first one equivalent to next = do x &lt;- use fresh x &lt;$ (fresh .= x + 1) instead?
In that case I'll often break things across lines. It's hard to miss a &gt;&gt;&gt; prefix.
You took my phrasing of "running headlong into the flames" as unequivocal advocacy?
Of course, either or both value could be a function, but the semantics still differ.
No due to the bit where you said: &gt; convincing evidence or argument Sorry if I gave the wrong impression. I've tried to phrase my comments as clearly and politely as possible.
no binary minus? No division?
It's not a hard and fast rule, but the point is to make people think really hard about defining new operators because they require a significant amount of conceptual buy-in from the rest of the community and we all have to learn how to read those operators if we want to meaningfully expect to share code. In the case of binary minus and division, everybody in the community understands at a glance what they mean so they are totally acceptable. The problem is when lens introduces hundreds of new operators, which imposes a huge burden on the entire community to learn to read each and every one of those operators. I understand that there is a consistent naming scheme for those operators, but even learning that naming scheme is just a bit too much to expect for every single member of our community. The reason composition operators get a pass compared to other operators is that it is easier to infer their meaning since they are always performing some sort of monoid-like or category-like combination operation.
Just to be totally clear, I did not mean to complain about the documentation, but rather to point out that I do not think there is any fundamental problem with `lens` that makes its types unintelligible.
I'm talking more about the API for gzip and aeson, but as you said elsewhere these are either in base or *nearly* in base!
I don't think there is a single answer to the question you are bringing forward. Which is better: * &lt;&lt;&amp;&amp;~ * \`oldAndModify\`
How would you write the following combinator without accessing the map twice? alterWithResult :: Ord k =&gt; (Maybe v -&gt; (v, a)) -&gt; k -&gt; Map k v -&gt; (Map k v, a) 
&gt; Running headlong into the flames of change is not always a good thing and conservative voices are extremely valuable. Change is not always good Agreed &gt; and I think the amount of discussion this article is gaining proves that there is not a convincing argument for lens The article isn't 'are lenses good or not', but 'are lenses haskell', and IMHO you got your "discussion fact" backwards. Discussion exists because there are good arguments for lenses (look at lenses user base), but also acceptable concerns againts. If nobody used or had concern against lenses, there wouldn't be a discussion around them. &gt; and it is therefore not just conservatives holding us back Are you implying that lenses are holding us back? Experimenting is the first step to move forward. 
I think this is an interesting and important point that should be grappled with by the community clearly. 1. The idea of `lens` itself is quite "functional". At least, the original post doesn't seem to deny that. 2. The things that are being complained about are the result of working the ideas out to their logical conclusion, with a lot of work put into this, and a number of false starts along the way. (Isn't that better than arbitrarily sticking to one of the false starts, and insisting that it must be right because it must be right? That's a common pattern in programming....) 3. The logical conclusion of the functional idea of "lenses" is "not idiomatic Haskell". I don't know that I fully agree with 3, but I don't fully disagree with it either. So... what exactly does that mean? A lot of the discussion here sort of acts like lens came forth fully formed from the head of Zeus, but it didn't, so if the end result isn't "idiomatic Haskell", where exactly did it go wrong... or, alternatively, is our idea of idiomatic Haskell impoverished and in need of expansion? I'm at least sympathetic to the latter.
That is a strange complaint. It is like if a C# programmer, in the days before automatic properties, would say "I hate properties. Define the fields one way and just know that you'll access them with a different but similar name".
I'm not sure, because I'm too lazy to look at the definition for `Map`. But I see your point, assuming the lens combinators are 'optimised' like the latter would be. Sorry for the cop-out answer. :)
I think what he is trying to point out is that this is what he fears will happen.
I like the cut of your jib, Mr. Bowers.
&gt; Yep! Hopefully I've cleared things up a bit. :) You did! :) 
We also have an existing vocabulary for modifying variables we can borrow from C and C++ `+=`, `-=`, `*=`, `||=`, `&amp;&amp;=` are all things that programmers have been using for a long time. `lens` just lets you work with them over a larger set of types in a principled purely functional manner. You also can now edit multiple targets using the vocabulary you grew up on. `%=` comes from 'mod-equals' drawing on the intuition from that programming heritage. `.=` is the closest I can legally get to `:=`, the old Pascal assignment operator, since = is taken. Why `~` in the functional analogues `%~` and `.~` ? I wanted the ability to work with both the functional style and the state style in scope as this was painful in `data-lens`. It is important from a pragmatic speed perspective that lens can do its operations in one pass through the monad stack and/or target data structure. It is also important from a semantics perspective that the laws say that this is always okay to do and is equivalent to a more naive implementation for any legal optic. Using the split up 'use' and 'set' pattern doesn't scale up to working effectively with traversals. The vocabulary breaks down. This forces us to bring in more vocabulary from `Data.Foldable` and `Data.Traversable`, but for many container types and lenses the simple operator vocabulary fits precisely at the level of detail needed to express your problem.
I do wonder how many Haskellers understand UML diagrams. I certainly had no idea what the heck that huge GIF meant until I was already 30% of the way through understanding lenses by actually using them. Prior to that, I couldn't even have told you which end of the diagram to start at to find the fundamental building blocks!
&gt; Case in point: the recent wreq library. lens-family-core doesn't have the JSON lenses used by wreq. 
Take the one object category whose morphisms are represented by strings that are not operators. Now all operators can indeed be composition in this category. This in fact has a very elegant monoidal structure, and given your criteria I don't see how you could possibly find this a bad idea.
As a magic box or as something you understand?
no array multiplication! no turnstiles in logic! no partial ordering operators, no modulus operator. wow math just got way simpler, thanks tekmo!
Disclaimer: I wrote the scotty-starter thing. I now use snap for a large project I'm working on (nothing against Scotty I still use it). In the beginning, when the project was small it made sense to isolate pieces of functionality by routes, db operations, types, logic on those types, and static content. As your project grows I recommend apportioning areas of functionality for your app. "Registration", "Login", "Payments", "Newsletter", etc. And keep all routes, db calls, types etc. in each corresponding folder. It's an art not a science. I just found a horizontal structure more conducive to large project architecture as opposed to vertical. With that being said, check out acid-state w/ it's remote module for persistence if you want to stick with haskell.
&gt; the amount of discussion this article is gaining proves that there is not a convincing argument for lens and it is therefore not just conservatives holding us back Nay, that follows not.
&gt; If you come from an OOP background and read foo\^.bar.baz then chances are you won't expect the data-lens ordering. ;) But I do. Because I don't a crap about someone's OOP (mine included). PS: I see now why you asked about `map.fmap`. Nope, doesn't click anything. For me it's immediately `(map.fmap) x = map (fmap x)` :-P 
&gt; I'm only talking about ordering. Prelude&gt; :t map . fmap map . fmap :: Functor f =&gt; (a -&gt; b) -&gt; [f a] -&gt; [f b] If you notice, that's [f b] and not f [b]. Do map and fmap redefine (.) in an unnatural way?
Yes, but map.fmap if you look at the nesting's yields something that composes in lens order. (a -&gt; b) -&gt; [f a] -&gt; [f b] &gt; For me it's immediately (map.fmap) x = map (fmap x) :-P Lenses do the same 'snap' for me. (traverse.traverse) f is just traverse (traverse f) (_1._2) f becomes _1 (_2 f) etc.
Actually, I love mathematical purity. We've build a half dozen lens libraries in the profunctor style on #haskell-lens, and variants on the theme. It just doesn't fit the Haskell ecosystem. Perhaps we'll tackle lenses in ermine in that form. There is no 'unidiomatic ermine' yet. =) There are also some annoyances that arise in categorizing traversals as having a constraint on the profunctor that require it to be representable by an applicative functor, vs. manipulating it in a more arrow-like form that doesn't handle well certain forms of infinite coproducts that arise in practice when you talk about things like Dynamic. The price we pay for lenses in Haskell that work with Prelude traversals is actually quite small. working with p a (f b) -&gt; p s (f t) for an Isomorphism costs nothing, as f = Identity can be selected, and you get back where you started. You can view this as working with the profunctor composition of the the starting profunctor with UpStar Identity ~ (-&gt;), an (~&gt;) is the unit of composition. Nothing is lost that you could express before and nothing is gained. Traversals then express directly as Prelude traversals, and composition with them collapses the choice of profunctor to (-&gt;). The only thing of value that is lost is that when we talk about a prism, we can't turn it around to get an "Unprism" with the same `from` combinator that inverts an `Iso` and we can't turn it all the way back around into a `Prism` by turning that around again. That's it. We had to add 're' rather than use 'from' for prisms, and you can't double twist a prism back into a slower version of itself. In exchange the entire Haskell ecosystem gets lenses. I'll take the trade for now. ;)
gzip went in because we nominally cover the platform and it was easier to include than explain the one-off exclusion. aeson went in because it provided a cleavage point from the API for lens-aeson to respond to 0.7, and at the time aeson looked to be going directly into the platform as well. If I had it to do over again? I'd probably have kept `aeson` in `lens-aeson`. The `zlib` dependency should be a free part of the platform that just gets some basic support from lens as a result of the batteries included policy.
My kudos for your work and acumen in choosing to publicly document your work.
&gt; Furthermore, I find lens makes certain tasks more idiomatic in Haskell. That's interesting. I haven't gotten to use `lens` much yet, but I find myself thinking I'm not writing "idiomatic haskell" any time I use a `case` statement, so I'd probably agree.
I don't understand what you are describing. Please implement your idea within Haskell.
Yes, but those two previous lens libraries Edward released were not Van Laarhoven lenses, which enabled two new key features: * They permit polymorphic update * They permit subtyping relationships implemented through type class relationships I believe Edward's two previous attempts failed not because they were simple, but rather because they were not Van Laarhoven lenses. The more appropriate experiment is to compare against `lens-family` and `lens-family-core` before concluding that a simple core is a failure.
I cannot believe that video has 714,177 views
Sorry about deleting the last post. Some issues resolving www.cryptol.net - so I replaced it with cryptol.net which is supposedly better.
I've re-posted with cryptol.net instead of www.cryptol.net. Even people inside the company are having issues with nslookup on the www prefixed url. Odd.
I'll say it again, fclabels forever, if you want something closer to an axiomatic core.
Have pure profunctor traversals been figured out yet?
There are a lot of generality arguments between lens and lens-family as well. Not the least is the polymorphism difference expressed in lens-family v. lens-family-core but also many other points in the lens subtyping hierarchy.
I totally agree with this, and would probably also add the controversial opinion that I don't particularly care what "idiomatic" haskell is. One of the explicit (albeit tongue-in-cheek) goals of the Haskell language committee is to "avoid success at all costs". As soon as a few giant companies have a vested interest in Haskell, they will start pushing for disallowing backwards-incompatible changes, and the language will lose its agility. Trying to nail down what is or is not "idiomatic" strikes me as being similarly stifling.
well, array multiplication is the composition operator in the category of vector spaces and linear maps... but as to the others...yep.
Well, if you want to go by hackage stats puts 651 downloads vs. 42051, but that is a poor judge of quality regardless. 
I think people like the idea of "diving into" a value instead of "building a pipe" out of it, starting from the targets. With obvious respect to CPS/categorical duality these notions aren't separable, but it might be possible to massage the documentation to be more properly illustrative.
aeson should be part of the platform. 
And that logic takes us down the path of Java and Objective-C, if not COBOL. Sure, we could use English, but there's a reason mathematics has its own syntax: it's easier to scan and *far* less verbose. With lots of text, you have to read the whole thing to figure out what it does. With symbols, you can just quickly look over the structure of the code without having to actually read it. If code is read more often than it's written, it's skimmed even more often than it's read. And just using English text doesn't help with that. `&lt;&lt;&amp;&amp;~` turns into a whole sentence in English. Using a few operators like this turns into *a whole paragraph*. I figure it's like syntax for calculus or the like. Sure, we could replace the integral symbol with "the integral of", `dx` with "the derivative of x" and so on, but would that be an improvement?
Similar to many of the other categories in this fantastic summary, I think AcidState should have been described together with Persistent. They are dual to each other. In AcidState you describe the data with Haskell types and then data schema are generated automatically. In Persistent, you describe the data with schema and then the Haskell types are generated automatically.
I think there's a lot of empathy there, but also exactly one new concept. Lens is not expecting someone who has never heard of Traversable to be able to grasp every combinator immediately. It also introduces a really new idea of a Prism and sticks to the heart of that idea religiously. `below` is a natural extension of those two ideas and with sufficient familiarity to the lens concept (not even `lens` itself) `below` is natural and its type is easy to read.
I've thought in similar patterns myself. lens reminds me of Python's twisted library. twisted too requires everyone to buy into its programming, because if your library doesn't it will block the event loop*. I think things that require tight coupling across the library ecosystem is bad. * Which is by the way why Haskell's green thread approach is so nice. It doesn't make every library buy in to the I/O manager. Some libraries can (e.g. network) and will then perform better, but if your library uses blocking syscalls you're still fine. 
Yes, it is a poor judge of quality. We should appeal to the specific merits of both libraries since popularity is easy to skew due to: * network effects * marketing of libraries * perceived stability or activity of maintainers
 booly :: Uni -&gt; Bool booly (Bool b) = b booly (Number x) = (x /= 0) booly (String s) = not (null s) booly (Dict d) = not (Map.null d) booly (Tagged _ x) = booly x -- probably wrong booly (Function _) = True booly Nil = False unot :: Uni -&gt; Uni unot = Bool . not . booly -- cue javascript !!x shenanigans if_ :: Uni -&gt; Uni -&gt; Uni -&gt; Uni if_ c t f = if (booly c) then t else f 
I seem to remember something about `Generic` being faster too. It's certainly the preferred way. 
If you want to be really evil, you use booly as the implementation of inu for IsUni Bool.
&gt; far less verbose I think that sometimes verboseness is more clear. Having really dense code makes it less maintainable later
&gt; If I were relatively new to C++ and came across a single page that discussed Clang and LLVM and the STL and Boost and GTK and Qt and template metaprogramming and auto_ptr and debuggers and testing frameworks and virtual inheritance and on and on and on, I'd find that overwhelming also. I think you'll find that [here](http://www.parashift.com/c++-faq/)
Note that polymorphic updates are unrelated to vl lens.
Right, but they were a feature missing from Edward's two previous lens libraries. Van Laarhoven lenses were just one way to make that work well.
I'd rather write (and read) something like next = useThenModify fresh (+1) 
How about this: &gt; let f x = mappingL _1 _2 reversed x &gt; view (f . from f) ("abc","def") ("cba","fed")
mflow's backtracking monad seems to break the monad laws. In particular: BackT (return (BackPoint a)) &gt;&gt;= return = BackT $ do v &lt;- return (BackPoint a) case v of ... BackPoint y -&gt; do z &lt;- runBackT (return y) case z of ... other -&gt; return other = BackT $ do z &lt;- runBackT (return a) case z of ... other -&gt; return other = BackT $ do z &lt;- return (NoBack a) case z of ... other -&gt; return other = BackT (return (NoBack a)) /= BackT (return (BackPoint a)) 
I disagree. I believe that the switch to Van Laarhoven lenses was the reason for the package's success, not the batteries included.
 &gt;The whole reason people are upset is because lens has been successfully packaged. That's a wildly unfair characterization of the criticism seen throughout this thread.
But lens-core family etc came out first. 
No one cares about packages that aren't popular, idiomatic or not. Seems tautological.
They're not field accessor functions, they're applicative field updaters. The composition is "backwards" in the same sense as that (&gt;&gt;) would be backwards. 
Yes, some people have argued that typeclasses were a mistake for the same reason. But over the years I think we found that, when used judiciously, typeclasses can be very convenient and expressive without their polymorphism causing too much loss of semantic clarity. The main problem is, well, when they're *not* used judiciously. I guess we'll soon see with ORF. And as you say, it's not nearly as big a deal as typeclasses.
Yes, but nobody knew it existed, including myself.
Yes!
I think they're a bit like view patterns. Say I have a guard that checks if (f myArg) is Just with an even number in it. It's nice I can use ordinary pattern matching for this. This is actually consistent with list comprehension guards that can also pattern match with similar syntax. The same syntax/arrow direction is also found in do notation, when pattern matching with &lt;-. 
Have you ever used Haskell?
They can't have a `Category` instance because there are four type variables.
If you want to add the same 2 type params to the other representations, you can just the same. I agree that it was the first lens library to solve this rather than throw away usefulness/generality for implementation/api simplicity. I have always found the previous lens libraries useless because of this and have even used ad hoc th code to generate polymorphic SECs and used that instead (until lens came along)
As a bonus, if you use a LRU cache instead of a Map, you can put an upper bound on memory usage.
I'm sure /u/twanvl knows a little bit about that ;-)
That's exactly the point. The usage in do notation and list comprehensions *seem* similar, but are actually totally different semantically. That's what makes it all the more confusing and dangerous, and part of why I would never let those into my code.
`hardCoerce`
But look: test v= runBackT $ do x &lt;- BackT (return (BackPoint v)) &gt;&gt;= return y &lt;- BackT (return (BackPoint v)) return $ x == y Will return True for all v's 
This particular diagram is just an outline of the "is a" relationships, and additionally lists the operations that belong under each abstraction. There are no "fundamental building blocks" in the diagram, because it's not showing composition relations, it's showing subtype relations.
No one may criticize unpopular packages for lack of interest, but that does not imply that criticism of a popular package is wholly due to its success. 
Because it comes with a whole bunch of common packages you would otherwise have to install with cabal or something.
How does everyone format their source code properly? This is usually the main weakness of any text editor with haskell.
Maybe the JSON library could add those lenses itself if lens were not a giant dependency. I don't think lens should define lenses for every other library in the name of convenience because then it just becomes a vicious and self-perpetuating cycle.
Not without either going through the existing "`Representable` by an `Applicative`" machinery or losing traversals for things like `Dynamic`.
I think the author was mostly worried that `.` is used 'backwards' in a lens context. For example, if you use standard records you might write `data &amp; _supProp . _prop` whereas lenses would be composed in the opposite order: `data ^. prop . subProp`. Also, is `bind` really 'backwards'? It doesn't represent composition in the way that `.` does, does it? (Don't hurt me, I know nothing about maths :P)
By this logic, `putStrLn "foo"` and `putStrLn "bar"` (or similar in your favourite non-magical monad) are equivalent ...
The problem I have with the "axiomatic core" of fclabels is that it is inconsistent. There are no laws provided or providable for half of the constructions offered by the package. That was actually a large part of what drove me to write `lens`. Nobody could tell me what the heck a "monadic lens" meant or what its laws were. As far as I can tell that is still the case. BTW- I'd love to be wrong here.
The notion of a `PLens` has a lot in common with the `zippers` that were split out of lens, with the difference being that one hand can't get the level you are at in the type, and on the other hand don't have to. PLens has the fold-like structure for reading, but only provides Setter-like access for putting things back in. That said, nothing should actually prevent it from being extended to a more traversal like version data PLens f a b = PLens { plensGet :: a -&gt; f b , plensModify :: forall g. Applicative g =&gt; (b -&gt; g b) -&gt; (a -&gt; g a) } the only thing lost is `PLens ((-&gt;) e) a b` and things composed with it. Anything that can have have a valid `PLens (Free [])` could be used as a valid `Zipper` or `Traversal`.
I've never been a fan of `return`. IMO, it shouldn't even be taught to newbies, since they invariably confuse it with return in imperative languages. `pure` is a better choice because there's no `pure` keyword in any imperative language that I know of.
Do all these operators get a pass? Do they? Or is it just that _you_ give them a pass :-P (&gt;+&gt;), (&gt;~&gt;), (\&gt;\), (/&gt;/), (//&gt;), (&gt;\\), (&gt;&gt;~), (+&gt;&gt;), (\&lt;\), (/&lt;/), (&lt;~&lt;), (~&lt;&lt;), (&lt;+&lt;), (&lt;\\), (//&lt;), (&lt;&lt;+)
I learned UML in school. I'm not sure how usefull the lens diagram is, but I don't think it is weird to know how to understand UML.
try asking on the yesod group: https://groups.google.com/forum/#!forum/yesodweb
any progress on getting this in vim or neovim?
Thanks, missed that one and I will read it too.
First, as I said in my original comment: &gt; I am saying this as somebody who has violated my own rule and regretted it. Second, half of those operators **are** composition operators of a category. Read [here](http://hackage.haskell.org/package/pipes-4.1.1/docs/Pipes-Core.html#g:2) to learn more.
 x,y :: Monad m =&gt; BackT m () x = BackT (return (BackPoint ())) &gt;&gt;= return y = BackT (return (BackPoint ())) ignore x = x &gt;&gt; return () ex :: BackT IO () -&gt; IO () ex act = ignore $ runBackT $ do () &lt;- act lift getLine BackT (return GoBack) one_getline :: IO () one_getline = ex x infinite_getline :: IO () infinite_getline = ex y Quite observably different!
I would add Iso to the list * Iso' s a -- s ~ a * Prism' s a -- ∃c0. s ~ c0 + a * Lens' s a -- ∃c1. s ~ c1 * a * AffineTraversal' s a -- ∃c0 c1. s ~ c0 + c1 * a (not in the lens library due to unfortunate technical reasons) * Traversal' s a -- ∃c0 c1 c2 .... s ~ c0 + c1 * a + c2 * a^2 + ... * Setter' s a -- ∃ C : Functor. s ~ C(a) (not 100% sure this is correct) Actually I don't know how to fit Fold', Getter', and Review' into this scheme. The feel more incidental than fundamental. * Getter' s a -- s -&gt; a * Fold' s a -- s -&gt; 1 + a + a^2 + ... * Review' s a -- s &lt;- a Edit: Moved Setter' around. I always thought Setter was incidental, but it appears to be fundamental.
The result of `:print` depends on how much you have already evaluated of those expressions. If I evaluate nothing, they look just the same to me except for names of variables. In any case `:print` reveals implementation structure for debugging that isn't part of Haskell's defined semantics.
One more thing that comes to me is [pointfree](http://hackage.haskell.org/package/pointfree).
&gt; It doesn't represent composition in the way that . does, does it? Strictly speaking it doesn't; however, when we write m &gt;&gt;= f &gt;&gt;= g &gt;&gt;= h we are composing `f`, `g` and `h` ^((in the Kleisli category)^) . The above is equivalent to m &gt;&gt;= (f &gt;=&gt; g &gt;=&gt; h) where `(&gt;=&gt;)` is the composition operator proper: (&gt;=&gt;) :: Monad m =&gt; (a -&gt; m b) -&gt; (b -&gt; m c) -&gt; a -&gt; m c It can be trivially defined in terms of `(&gt;&gt;=)`, and vice versa. g &gt;=&gt; h = \x -&gt; g x &gt;&gt;= h m &gt;&gt;= h = const m &gt;=&gt; h $ () So when discussing the direction of composition it does make sense to mention `(.)` and `(&gt;&gt;=)` in the same breath.
Like I already said: &gt; The reason composition operators get a pass compared to other operators is that it is easier to infer their meaning since they are always performing some sort of monoid-like or category-like combination operation. I know that you tried to rebut this answer, but I never understood your argument and you haven't clarified it.
I opted for the latter, I always opt for that, I don't like the idea of installing more than I need. I don't like batteries included, I like a system which allows me to easily get whatever batteries I need. Downloading the Haskell platform in its entirety to me is as unelegant as simply including every single module you can think of regardless of if you use them. Is the compiled code the same? Yeah, but the source is bigger. Swap out source with diskspace usage and you have your analogy.
You should always teach people with positive phrasings. So instead of "Lenses **don't** compose backwards" you should say "Lenses **do** compose normally". Additionally, you want to send a clear succinct message, so tighten it up as much as possible. We should shorten this to: **Lenses compose.** And really, that's the end of the story :) ----------- Okay. All of that was meant tongue-in-cheek, of course. I really like your explanation and it gives me a new way to think about lenses. Thanks!
&gt; You should always teach people with positive phrasings. Is there a reason for this?
No, really, I can't do that. Everyone agrees they compose. People just mistakenly think they compose *backwards*, which is what I'm trying address. If I used some other naming, it wouldn't be as much of a denial of that claim. The point was to say, "no, they *don't* compose backwards, you're wrong". :P
And, for what it's worth, I'm not a big fan of the Scheme path. Having `compose` instead of `.` makes it much uglier to use, which isn't helped by the fact that functions aren't curried by default. (It's ameliorated a bit because you can use a single `compose` to compose any number of functions, but that's a different discussion.) Ditto for looking things up with `(vector-ref ...)` instead of `!`. While I might find Racket code easier to read *initially*, before I'm familiar with the codebase, I've found the Haskell approach with operators preferable for extended projects. I think I have a good base of comparison, because I worked on two very similar projects in Racket and Haskell almost simultaneously :).
Yeah but that's nowhere near as uniform in naming conventions, which I feel is important to make the point as obviously as possible. Half the problem, I feel, is that people read "lens" and don't think "map-y thing". So by drawing very tight parallels between map functions, including in the naming convention, it seems more "obviously right" to me.
What's the relationship between these packages? And what's with the `Family2` namespace?
Yes, several. First, positive reinforcement is more useful than excluding all the negatives. Saying "don't do this" doesn't actually tell somebody what they should be doing. In better cases, this can be iterated several times, "don't do 'a', don't do 'b', don't do 'c'..." until the person finally figures out what they should be doing instead. This is frustrating, a waste of time, and generally unhelpful. Secondly, negative reinforcement can be counter-productive. In order to understand "don't do x", you need to have a conception of what 'x' is. Simply forming this conception often reinforces the behavior, which is the opposite of what's intended! You can think of this as, people tend to forget the "don't" part of the phrase and just remember the behavior. Finally, people are often perverse. For many people, being told not to do something simply makes them want to do it more. Forbidden fruit syndrome, if you will. It's not that there's no place for telling people not to do something, but generally positive phrasings lead to better learning.
So imagine a set of strings where each element corresponds to a Haskell program, or a fragment thereof. This is more restrictive than we need to consider, but I think the restriction is useful to guide our thinking. Furthermore, you can split this set into strings that correspond to operator names and those that do not. The second set (non operator strings) is the set we use for arrows. As stated, this category will have one object so we ignore it. Now we just need to understand composition. To get our composition operation, we will map all the strings of operator names to composition in this category. As an example, imagine these three strings: * "&lt;&gt;" * "test1" * "test2" The last two will be arrows and the first one is a valid operator name so we map it to composition. Thus, (using o as the composition operator for the category): "test1" "&lt;&gt;" "test2" ==&gt; {identifying "&lt;&gt;" with composition} "test1" o "test2" We still need to prove the category laws but I'll stop there. In summary, this is a way to take Haskell source and treat it a monoid (of strings), with a special case for mapping operators to mappend. In other words, all Haskell operators are composition in this category. And it's a monoidal category to boot. -- Note: Your comment makes it sound like you think that category theory must be implementable in Haskell to be valid. I suspect that really you just didn't understand what sclv was proposing and hoping that Haskell could be a common language to make things precise. Hence my elaboration above. Additionally, I'm currently just learning category theory and working through the proposed monoid was good homework for me.
Idk it seems a bit silly to reject things like `max`/`min` semigroups on the grounds of not fitting into the currently widely spread abstractions.
While not trivial something like this would work pretty well with a JIT based haskell platform.
This is the other side of the great Haskell "your language is my library" coin. This will be perfectly acceptable after a couple more iterations. Just give some time for Haskell to digest this "new" paradigm - it *will* adapt and it will be great.
I do not think I can call `+` that operates on Integers the same operator `+` in your example. So to my naive understanding of category theory it seems like you allowed the meaning of the operators to vary and found a trivial category. Tekmo's argument on the other hand is something like. Given an operator with a fixed meaning is there a category that it acts as composition in. If so then it is easier to reason about and in Tekmo's view has potential to be a valuable infix operator.
ghc-mod and ghcmod-vim has always worked out of the box for me, even using cabal sandboxes(it knows about the local .cabal file as well). As for the type checking utility -- I never really use it myself. It's never been a part of my workflow. But if you use syntastic, you could perhaps install both ghc-mod(and ghcmod-vim) as well as hdevtools and use hdevtools for checking types of expressions and ghcmod for the rest. I had both installed and switched over to just using one of them the other day. I chose hdevtools because it supposedly does some things better. However, for some reason the quickfix buffer's output from hdevtools(which wasn't really the quickfix buffer, but some other buffer I don't remember the name of) was very badly formatted compared to ghc-mod to the point where I switched back. tl;dr: You could use parts of both hdevtools and ghc-mod. 
This argument seem to allow the meaning of the operators to vary though, which seems to miss Tekmo's point/argument. The `+` operator that acts on `Integers` is not isomorphic to `+` in the above monoidal category presented by sclv and detailed by you, dagit.
I am writing the Hakyll code for a GitHub Pages blog at this very moment. &gt; What I'm really wondering is, how much hand-HTML does Hakyll require you to do? As little (or as much) as you want. A default HTML template is needed to load the stylesheets and arrange the site-wide layout. Beyond that, you can default to Markdown (or any of the other input formats supported by Pandoc) and only resort to HTML when you feel the need. Tiny templates for things like entries in post lists are most easily done in HTML, though in such cases you often have the option of avoiding templates and using Blaze to generate the HTML snippets from Haskell. &gt; Does Hakyll have any support for visual "themes"? If not, are there libraries/examples of visual layouts I could choose from? It doesn't. You have full control over the CSS though, so you can deploy any suitable CSS package (for expediency, I will use the minimalistic [Pure CSS](http://purecss.io) in the blog). If you need inspiration, many examples of Hakyll sites can be reached from [the project site](http://jaspervdj.be/hakyll/examples.html). &gt; Does Hakyll integrate well with Github pages? The main annoyance is that GitHub Pages forces you to keep the build output under source control. The only way to have a sanely organized repository is to keep the sources and the deployed version of the site in two orphan branches and maintain each of them in a separate clone. You can see my setup, and instructions on how to arrange it, in [my blog's repository](https://github.com/duplode/duplode.github.io/tree/sources). It is rather convoulted, but once ready it works very well. &gt; How hard is it to get things like a Facebook like button, twitter sidebar, disqus comments, etc. working in Hakyll? Is there anything equivalent to Octopress's plugins? I am not aware of any plugins. Such things are probably not difficult to arrange with Hakyll templates though. Here is a [random example](https://github.com/blaenk/blaenk.github.io) of a Hakyll blog which uses Disqus for you to explore. &gt; Octopress advertises itself as dealing well with code snippets. Does Hakyll deal well with code within pages? Really well. Hakyll uses Pandoc for page rendering, so we get fully-featured and configurable [syntax highlighting in Markdown](http://johnmacfarlane.net/pandoc/README.html#verbatim-code-blocks) as well as out-of-the-box support for [literate Haskell](http://scr.stunts.hu/hakyll.html) (the page behind the link was generated directly from the source of the Hakyll program for the site). 
I am somewhat in the same boat. I wanted to create a blog / personal website, and I have a deep loathing for menial adjustment of margins, paddings and such and very little love for HTML. Though CSS is better now than it has been, it's still not very fun. I decided to go with Hakyll. I'll try to give some input, but it's been a while since I worked on my site, but I remember a few of the good points. - You have the full power of pandoc when using Hakyll. Pandoc is pretty friggin' awesome. - I don't know Octopress, but Hakyll is not a 'complete solution with batteries included'. You have to write some HTML, CSS and then use those as templates/themes where appropriate. As such, it doesn't 'deal with code snippets'. You do that. - For CSS, I went for [Clay](http://fvisser.nl/clay/). This is a pretty damn sweet way of writing CSS. This gives you: - Automatic generation of vendor specific prefixes for all those not-completely-but-almost-cross-platform stuff. - Programmatic generation of styles using combinators. - Your CSS is now typechecked. Say good bye to obscure errors because of typos. Well, I'm kind of out of time now, but I definitely think Hakyll is a good choice for static websites. I was able to add MathJax support to my site and some fonts from google easily, and there is no reason you wouldn't be able to add something like stylesheets for code and such without any problems. 
Thanks, I guess I'll try hdevtools next. I've never really used any of these tools for my day to day works; too much trouble to set up and keep working. But I thought I'd give it a try again, and so far everything seems to have improved a lot!
The analogy to maps really isn't mine, it's been mentioned before by others. But I've never seen the analogy property explained. It's always in some other context, about how higher order things (self) compose to produce self-similar types, which isn't really the point.
That said, I remember reading a theoretical paper where it was shown that a learning algorithm can never approach a model without negative reinforcement. In other words, if you've never told what not to do, then you have no idea what the space of things you should do actually is. Purely positive reinforcement simply tells me a finite amount of things that I should do, but then I am confined to only those things. With negative _and_ positive reinforcement, I can draw some inference between the two to be able to make judgements myself as to what I should and should not do.
d*x* usually represent the *differential* of *x*, where e.g. d*x*/d*t* (or simply *x*' in less ambiguous situations) is the derivative of *x* with respect to *t*.
&gt; we all have to learn how to read those operators if we want to meaningfully expect to share code. &gt; &gt; In the case of binary minus and division, everybody in the community understands at a glance what they mean so they are totally acceptable. While I think I agree with your general sentiment (be careful with defining new operators) this comes off as odd. *Any* new name means we all have to learn how to read it. Just because it uses letters instead of symbols doesn't mean it's immediately obvious how it works. A name of less than 20 characters can impossibly reveal the intricate details of how the corresponding function works. "foldMap" doesn't mean very much until you know its type signature and semantics. You have to learn those. You can't just read "foldMap" or "traverse" once and immediately understand what it does (sure, it traverses, but traverses what? and what does it do to it?) It's the same thing with operators, only that they use symbols instead of words. As far as I've been able to gather, there are simply "word people" and "symbol people" and one camp can't understand how the other has problems with their own mode of communication, even though they probably are equal in the big picture.
&gt; This is impossible with lens, which takes overloading to the level mainstream Haskell probably hasn’t seen before. This reminded me of C++ expression templates. 
I've just deployed a [tiny Scotty app][app] using a MongoDB backend on Heroku. I wouldn't consider it a _fantastic_ example to follow, but it shows the basics and has database integration, which was the tricky part for me. It also has no frontend - it's a pure JSON API that interacts with a frontend hosted elsewhere. But figuring out Blaze or whatever other template engine you want to go with isn't too difficult. [app]: https://github.com/knowledge-map/simple-storage/blob/master/src/Server.hs
If I've understood your question: Yesod has the `PathPiece` type class, which it uses to convert the pieces of the requested path (i.e. each bit between slashes) from Text to whatever. If you look at [1] you'll see it's rather easy to implement yourself. The routing section in the Yesod book [2] goes into more detail. [1] http://haddocks.fpcomplete.com/fp/7.4.2/20130829-168/yesod-core/Yesod-Core-Dispatch.html [2] http://www.yesodweb.com/book/routing-and-handlers
Should that be Lens x = Functor f =&gt; (a -&gt; f b) -&gt; x a -&gt; f (x b) or am I missing something?
&gt; Octopress advertises itself as dealing well with code snippets. Does Hakyll deal well with code within pages? Almost. You can syntax-highlight to your heart's content but dynamic inclusion of code snippets is still a bit up-in-the-air. I [recently had cause](http://dougalstanton.net/posts/2014-01-28-write-code-to-write-posts-to-write-code.html) to hack some in, feel free to suggest improvements. I [hacked together support for compile-time-inclusion](https://github.com/dougalstanton/dougalstanton.net/blob/master/src/blog.hs#L52) of source but it's not very flexible and doesn't let you choose subranges of files, only whole files. You can [examine the syntax here](https://raw.githubusercontent.com/dougalstanton/dougalstanton.net/master/posts/2014-01-25-quickcheck-tests-for-c-cpp-code.markdown) &amp;mdash; as I say, not perfect but if you're developing a tutorial-style blog post it's really important that your code compiles!
When I helped a friend start a GitHub Pages website, I set it up as follows: * The source repository is private (local to his computer). * The site is built into a separate repository, which has a GitHub origin. When deploying, the build repository is first collapsed so that it only contains the current state, not a history of all the content changes.
Oops - I guess I should have thought a bit more about that, and read OPs post properly too. **EDIT** - Actually, despite the oops... Lenses use the same abstraction whether you're reading an element, writing it, or doing something else with it. They have to choose an order to compose in and stick with it. Why choose the order for extracting an element as opposed to the one for inserting an element? OK, what do I mean? fst.snd :: (a, (c, b)) -&gt; c The `fst` is selecting from the inner tuple. The focus is specified innermost-first left-to-right. let outer x = (1, x) let inner y = (y, 3) (outer . inner) 2 -- (1,(2,3)) The `outer` is of course building the outer tuple - and thereby selecting which element is the focus for `inner` to deal with. The focus is specified outermost-first left-to-right. This is, of course, back to the values-with-holes claim I deleted. How you focus on a particular element depends on why and how you're focusing on it. Composing for inserting an element into a structure focuses outermost-first left-to-right, just like a lens. 
I am the package author. There's a company looking for websockets transport, too, and we are working on it together :)
I thought wreq used aeson-lens. But now I see it's Data.Aeson.Lens that is in lens package. What it has to do there????????
works for geany too.
For certain learning algorithms that's true. For others it's not, afaik.
That was even easier for me: downloaded the zip, opened it, double-click on a font, click "install" on the preview screen. Ubuntu 13.04. Done. 
I switched from Octopress to Hakyll. Octopress was very quick and easy to get up and running but once I'd done that I felt like I still didn't understand how any of it worked, so any time I wanted to add a feature I was stuck. At the same time, I was just starting out learning Haskell. Hakyll took a bit more effort for the initial setup but it felt "mine", and I have since added some reasonably involved utilities (hooking into Pandoc's markdown -&gt; html renderer to make modifications to my document based on information in the header), which I couldn't imagine doing with Octopress.
This is great! Thanks Galois for another great piece of open source software! I wish I had this when I implemented pbkdf2 in pwd-store :P
awesome. just after heartbleed i was (again) looking into cryptol. i hope it gets some attention in the mainstream-ish programmer community so it gets the necessary cryptographic review. does anyone know how cryptol ensures that no obvious timing side channel slips in? **edit**: answering myself. it does not have any backends yet, so there can be no side channels per definition. in the hacker news discussion adam folzer (working on cryptol) mentions that these are coming back though.
&gt; orphan Wouldn't it be more modular to reverse the dependencies, so that there can be a small core lens library that everything else depends on? This would still avoid orphan instances but satisfy the need for a small lens library.
Having consistently named operators is definitely a great thing. However it does require you to memorise the language of these operators and if you don't spend a large amount of your time in lens you may have difficulty with it. As you say, you could puzzle it out or use a cheatsheet but I would argue that is more work than having the operators have descriptive names that tell you what they do. Your code becomes longer but that may also mean you end up splitting it up into logical parts which to me would be easier to me. Obviously this is subjective!
Perhaps not the amount, but the kind of discussion.
BackPoints are supposed to be used in a few lines in the code. I didn't find a way to have the effect required without breaking the law. In fact to allow backtracking to a backpoint inside a procedure, it is necessary to end it with breturn: breturn x = BackT . return $ BackPoint x example: in the code below, the backtrack will ignore the backpoints of procWithBackPoints unless it finish with breturn() instead of return() So yes there is a very big difference _when the code is backtracking_ between the behaviour of both expressions, but there is no difference when going forward in the "normal" flow direction. main= runBackT $ do procWithBackpoints blablah fail "" procWithBackpoints= do backpoint1 blah blah backpoint2 blah blah breturn ()
They'll be bringing all that back over time, right now most of the heavy hitting verification/synthesis is all in Cryptol 1 still.
Great font ! A small issue though: The rendering is weird with &gt;&gt;&gt;, we get a double &gt;&gt; followed by a single &gt;
Can you explain what is a "COM syle" object system? Or refer me somewhere?
TomsFastMath defines fp_int with a fixed size FP_SIZE, &gt; /* Max size of any number in bits. Basically the largest size you will be multiplying &gt; * should be half [or smaller] of FP_MAX_SIZE-four_digit which is obviously not what you want to do. A library for multiprecision has to be able to dynamically adjust the size of the operands, which can only be done with heap allocation. Besides it solves platform independence in the worst possible way by a huge numbers of ifdefs in the header file. A sane implementation would split the library in a portable and non-portable part and use the build system to include the right files. See for example the implementation in plan9: https://github.com/brho/plan9/tree/master/sys/src/libmp Any serious software that uses gmp will realize what you said and will use a custom allocator, especially since it is explicitely pointed out in the manual.
I think you mean "Lenses as implemented by `lens` and `lens-family`", since most other lens implementations do compose the other way.
I'm in the same boat as you. I think the lens library is kind of obnoxious (tho I do think the core idea of lenses is beautiful). But despite that, this backwards bs isn't what's wrong with lenses.
Sounds great, will have to give it a go. Thanks :)
I disagree. Compare: foo x | guard1 , guard2 , guard3 = ... Each guard can "terminate" this branch. [ ... | ... guard1, guard2, guard3 ] Each guard can do the same. Now add pattern guards: foo x | pat1 &lt;- guard1 , pat2 &lt;- guard2 , pat3 &lt;- guard3 = ... [ ... | pat1 &lt;- guard1, pat2 &lt;- guard2, pat3 &lt;- guard3 ] Each pattern mismatch in each guard terminates the branch in both the list monad and the pattern guards. 
That is perfectly reasonable. I've been known to send people there as well and to even help support `lens-family`/`lens-family-core` users on #haskell-lens. The existence of that package in many ways allows `lens` to have its various excesses. It is compatible enough that anything providing support for `lens` will automatically provide support for `lens-family` and aside from missing a lot of the indexed machinery, everything you build there will work when/if you get forced into `lens`. ;) You may want to go with `lens-family`, despite it being one more dependency, as the type synonyms in `lens-family-core` are very strange in an effort to keep them Haskell 98. Ironically, I started `lens` because having to juggle 3 different packages to write "hello world" with `lens` struck me as awkward packaging. I found that using the same names for type synonyms with different semantics between `lens-family-core` and `lens-family` made them very hard to use, so I went off and built my own little `lens` module inside of another codebase and it er.. grew. 
The person you're talking to is twan van laarhoven, as in, the guy that wrote about _van laarhoven lenses_, the exact style of lens used in `lens`.
 I agree that `lens` is more in the haskell spirit than `yesod`, but I must also confess that I share most of the frustrations that the original article lists.
What's the point of working with arrows if it needs to be `ArrowApply`? As shown `ArrowApply arr =&gt; arr a b` is then isomorphic to `Monad m =&gt; a -&gt; m b` which is a lot easier to work with.
The problem is when you work out the things that would need to live in that small core, they need extensions that not everyone would be comfortable having their package depend upon. It requires universal buy-in to work. The current approach can be implemented unilaterally.
&gt; This is a problem that doesn't exist to the same extent with other PLs. A Java novice can usually get the gist of a program written by a Java expert. I would argue that that says an awful lot more about the ability of java to cripple your expression of thoughts than it does about Haskell.
Wit ghc 7.10 we'll be getting new support in the compiler that will enable you to work more explicitly with field accessors to build a lens sans template-haskell if that is your preference. lens will be able to supply a combinator that can called on the field accessor itself to generate a type changing lens.
Einstein died denying the consequences of his theories. He'll just have to face the truth: the trusted way of doing things with field accessors is clumsy, lenses work better,[ they're different and there's nothing wrong with that.](http://www.reddit.com/r/haskell/comments/23x3f3/lenses_dont_compose_backwards/) 
Actually, the json lenses weren't the problem. The problem was the lack of indexed lenses. I kept offering `bos` 15 line workarounds that worked with `lens-family` and 1 liners that worked with `lens` and eventually he decided it wasn't worth it and switched. The json stuff was added after he was already there because it was so easy at hand.
I traded in my Sun Certified Enterprise Architect hat a long time ago. I figure if I "speak truth to power" on this point rather than self-censor I can at least save some of them. ;)
Update --- it just occurred to me that assigning the result of skipSpace might work so I just tried inserting foo &lt;- skipSpace and that in fact works but leaves me with a different question. I didn't have an assignment (bind?) there because I "knew" they were all spaces. So how come I have to have that assignment and what's in foo?
But nevertheless, thanks for the writeup. It did bring this aspect out and clarified a lot for me as well. I always felt that lenses do indeed compose in the right and natural way for me but I couldn't explain it clearly.
I'd say lens is the "new" idiomatic Haskell. The point about "idiomatic" in general, is that you have to learn a specific way. For example, a preference of using `$` to reduce the amount of brackets in code. Lenses do offer a specific way of focusing into datastructures, it's just that it is "new" not "standard" yet.
That sounds interesting. As much as people are talking about lenses, they're obviously a big deal and I expect them to get some kind of syntax support at some point. I'll be keeping an eye out for how this goes. And I wouldn't have a big problem using TH, so long as it follows the Smalltalk-ish style of "you can only cheat if you don't get caught". I.e. if you're deriving symbols then either I better not be able to see them (if they're something the underlying framework needs) or I need a way to directly and explicitly specify them. I really hate "convention" for things like this.
Correct if wrong but is laziness more hindrance than help here? Curious, how many Haskellers would trade lazy by default (assuming a magic switch could make Haskell strict) in exchange for line number stack traces and an Eclipse/IntelliJ-esque IDE? As a Scala user, where the language is sometimes vilified as a poor man's PHP, I wonder where a strict Haskell and a vastly improved Scala intersect. Not that either are likely to happen anytime soon, if ever, but it is interesting to think where both languages will be in 5 years, both in terms of core language features and available tooling. Presumably not the same as now ;-)
My vote is on Hakyll. There is really no concept of themes in Hakyll because you have direct control over the html,css,js. You could try a template from http://bootstrapzero.com/ and work from there. I think this would be a beneficial approach to someone that is new at html,css. You could also try using something like Clay to generate you CSS from Haskell (http://jaspervdj.be/hakyll/tutorials/using-clay-with-hakyll.html). You will have to write at least some html to get your base templates setup. You could experiment with using Blaze templates in Hakyll. I'm not sure how this would work, though. There are tons of example sites and source on the Hakyll site: http://jaspervdj.be/hakyll/examples.html. I've found it very simple to integrate features like Disqus (it is just an html snippet) and social media icons. Check out Hakyll partials if you want fine grained control over their placement. Hakyll deals pretty well with code snippets for my usage. It uses Pandoc to generate pages from Markdown, LaTeX, and rst which generally picks up syntax and has support for highlighting. I really enjoy using Hakyll and I've deployed several sites using it. Good luck. 
FWIW- if you look at the generated haddocks you can find the lenses from the template-haskell constructs.
You know very well fclabels isn't about monadic lenses, it's about lenses that can work with some effect using a custom category. For example totality, partiality, failure, etc. If you instantiate this category with `(-&gt;)`, or `Kleisli Maybe` you get lenses which inhibit the exact same laws as total and partial lenses in `lens`. Providing laws for the abstract base type `Point` doesn't seem very useful to me, as long as you can provide laws for the specific instantiations.
It's also a useful strategy to start with a misconception and work your way backwards toward understanding. The "don't teach don'ts" is a rule of thumb to guide you in the right direction, not prohibit you from doing your job.
Personal preference I guess; only reason I used arrows is that they are used in fclabels. Kind of orthogonal to the main point of the blog post though.
In "foo" there's a **()**. **skipSpace** is invoked only for the effect (recognizing spaces and consuming them). The following is working ok for me: parseIP :: Parser IP parseIP = do d1 &lt;- decimal skipSpace char '.' skipSpace d2 &lt;- decimal char '.' skipSpace d3 &lt;- decimal skipSpace char '.' skipSpace d4 &lt;- decimal return $ IP d1 d2 d3 d4 By the way, since the "shape" of your computation does not depend on the values obtained during de computation itself (the d1, d2...) it could be expressed very easily in applicative style: parseIP' :: Parser IP parseIP' = IP &lt;$&gt; decndot &lt;*&gt; decndot &lt;*&gt; decndot &lt;*&gt; decimal where decndot = decimal &lt;* skipSpace &lt;* char '.' &lt;* skipSpace 
Oh, okay, he's talking about the syntactic representation of the operator and not the operator itself. That was the part that was confusing me. Thanks!
Actually my point is that even with `Kleisli Maybe` they very much _do not_ exhibit the same laws! The issue is subtle but by adding a 'modifier that can itself actually fail' the suite of laws available to you becomes quite different and the `lens`-style laws borrowed from `Traversable` do not hold. I don't deny that it is useful. I merely deny that it offers or even can offer the same set of laws, and note that nobody has been able to tell me what the actual laws for it are. My objection here is very much to `Lens (Kleisli Maybe)` not to the existence of a general construction that works in other categories. The closest I've seen is Russell's attempt at viewing it as (costate :+: identity) comonad coalgebra, but then none of the ones that use the update seem to fit in that box. Another solid attempt for putting it on firmer footing is by Michael Johnson and Bob Rosebrugh who construct form of deta-lenses/c-lenses in Span that they call partial lenses. Their construction differs a great deal from the Kleisli Maybe construction. They do provide a path to generalize over other categories though, including ord and cat rather than just set and provide motivating examples. I do not deny that some generalization involving a category exists. I merely point out that Kleisli Maybe doesn't appear to be a legal inhabitant of that extension. I'm serious that I'd like to be wrong here, because monadic lenses would be really really useful, I just can't reason about them! I've been working off and on on how to adapt Johnson and Rosebrugh's work on delta-lenses and c-lenses to lens as they provide a lot of compelling examples with Ord and Cat that bear no real resemblance to the usecases we get in Hask and they can use a great deal of intensional information from the category it works over. They have great applicability to databases and the view-update problem. Their formalism also shows that the 'constant complement' property of lenses isn't fundamental, but is something that emerges because of the properties of Set (or Hask). Their more exotic lenses don't wind up with that property, while still retaining the same laws.
Categories (in `fclabels` at least) are convenient, because you can specialize them to normal function space by just using `(-&gt;)`, when using monads you actually need the `Identity` newtype. Also composition tends to be very naturally expressible using categories instead of monads.
If I interpret your question correctly, then it's simply to return the handle with the contents, i.e: getContentsOfFile :: String -&gt; IO (String, Handle) getContentsOfFile f = do ... return (contents, handle) main = do (contents, handle) &lt;- getContentsOfFile "PriceData/Daily/Yhoo.csv" hClose handle EDIT: formating
Cryptol is both a compiler and a theorem proving toolkit. Normally what you do is you'd take an arbitrary piece of C/VHDL/Java code, write a Cryptol specification, and then show they're equivalent using the theorem proving tools. Constant-timeness in some ways intimately depends on the algorithm, it's not something I think should (or can) be handled in general way. You can for example write an S-boxed based AES implementation, but this is not side-channel resistant. On the other hand, you could write a bitsliced AES implementation that is. Cryptol would let you do either of these. And then you could use it to prove the S-boxed version is equivalent to the Bitsliced version automatically! But it won't turn one into the other, you have to be closely aware of what you're actually trying to specify, because the spec *itself* has security ramifications like this. As another example, GHASH (the MAC component of AES-GCM) is pretty crazy difficult to write in constant time, and the NIST specification is actually not constant time either. On Intel machines at least you're totally bound by the speed of PCLMUL for the field multiplication. A very fast constant-time AES-GCM in software is actually worth a paper publication. Cryptol really isn't going to help you with any of this I don't think (I haven't tried GHASH in Cryptol yet), but it can at least verify your implementations (which may be low level, difficult to analyze, and speed/safety oriented) are totally sound, thanks to the high level spec which is simple enough. And AES-GCM is great in hardware where it is safe and can be parallelized - so you can prove that VHDL is the same code too. *That's* the powerful part, and that still defends against a lot of room for implementation error. On the other hand, it's pretty much impossible to *not* write a constant-time implementation of, say, ChaCha/20. And poly1305 pipelines nicer on general hardware and is open to a wider bus to speed it up in software implementations - and is naturally constant time. So Cryptol isn't really in the question here, these properties of speed and safety just naturally fall out of the design. You can also use Cryptol e.g. to generate code like VHDL or C from these specs. But again, it still won't magically turn S-boxed AES into something it isn't. At the end of the day, Cryptol doesn't magically insulate you from those kinds of specification issues. Because they're specification issues with security ramifications - so you have to specify it right. What it *does* give you are the tools to make a very strong statement about the correctness of given implementations across a range of languages *and* different specifications, even ones with tricky semantics, and walk away pretty darn confident you're OK. The high level nature of the specifications also make hand-verification and examination far easier, so it really does reduce the time you might spend otherwise ensuring your components are correct.
You are running into the classical problem of lazy IO, and there are like tons of articles about it and libraries (`pipes`, etc) designed to solve this problem. For your case, it should be sufficient to solve it with `withFile`: main = withFile "PriceData/Daily/Yhoo.csv" ReadMode $ hGetContents &gt;=&gt; putStr
You might also consider [clckwrks](http://www.clckwrks.com) -- which is designed to be a direct competitor to things like wordpress. It is used to host [clckwrks.com](http://clckwrks.com), [happstack.com](http://www.happstack.com), and several other sites. clckwrks is a full server -- not a static generator. But it does attempt to support the things you wanted: 1. the page contents are just simple markdown (but can have embedded html) 2. it has support for themes -- and you can switch themes at any time 3. it has support for plugins such as the blogging plugin itself, media gallery, bug tracker, and more 4. it can do syntax highlight of Haskell source code That said -- clckwrks is still missing a lot of features. Some stuff should be easy -- for example, it should be modified to allow you to use pandoc or other markdown processors. Other stuff is (potentially) more work -- for example there is currently no 'comments' plugin. Also, it would be nice if the page/blog post contents could be stored in git/darcs/hg so you could edit via the browser or your favorite text editor. The [filestore](http://hackage.haskell.org/package/filestore) library does the really annoying low-level stuff already. But someone needs to integrate it into the clckwrks page plugin.
Actually, you're already closing the file, even without the `hClose`. According to [the documentation of `hGetContents`](http://hackage.haskell.org/package/base-4.7.0.0/docs/System-IO.html#v:hGetContents), the file becomes semi-closed after `hGetContents`. That means that the file handle will be closed once `contents` is completely evaluated. In your program, this is done by the call to `putStr`.
`readFile` :-P 
I don't think your arguments are that strong. &gt; Except all of that is wrong. Lenses are not accessors. The right way to think about lenses is as focusers. This is quite an assumption. It's ok to pivot your view on things, but don't tell others with a different view they are wrong. Viewing lenses as accessors isn't wrong, it's even very intuitive, because people are used to this way of thinking. Expression order in Haskell is an interesting topic in general and I don't think there are wrongs in rights in this. Sometimes one view is intuitive, sometimes another is. The fact that the composition order in `lens` confuses people isn't because it's wrong, it's because it's not very close to their intuition. Intuition based on using *similar* concepts. Like accessor functions.
This is the "Saddam Hussein wasn't responsible for 9/11" problem. A positive reinforcing title might be **The only direction lenses compose is forward**.
This is my first time trying emacs in a long time also. You need haskell mode sudo apt-get install haskell-mode ~/.emacs.d/init.el (let ((font "Ubuntu Mono")) (set-default-font font nil t) (set-fontset-font t '(8500 . 8800) font)) (setq haskell-font-lock-symbols t)
If you want to download several URLs in parallel by just sparking more threads, would the magical Haskell I/O manager and green threads make it as efficient as the event based methods (poll/epoll) in, for example, multi_curl? If I understand it correctly that is how the I/O manager will do it anyway?
Yup, +1 to readFile. Here is the link to the docs for [readFile](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:readFile). You can click on the source link to see how it is implemented.
This seems tricky, because modifier composition requires ArrowApply, which has Arrow as super-class. Intuitively this feels like it should be possible though. 
Neat -- I'd always just thrown fmaps together (fmap . fmap . ...), and never thought to examine the types. Thanks for provoking some reflection.
You did and your answer is exactly what I wanted. I forgot I could return a tuple
&gt; Viewing lenses as accessors isn't wrong Yes, it is wrong. Lenses are 100% not accessors. The only way to use them as accessors is to specify `f = Const` and feed in the `Const` constructor. *Then* you have an accessor. But you no longer have a *lens*. &gt; it's even very intuitive, because people are used to this way of thinking Intuitive, incorrect ways of thinking are the problem.
I don't (yet) understand &lt;$&gt;, &lt;*&gt; and &lt;* More importantly, while your version is much more succinct, I think it does so at the expense of readability/debugability/extensibility. For example, what if you have 30 (or 100!) of fields. What if one field changes? You'd be sitting around for hours trying to count and match up the parse elements in that application style with the data type to find the right one to change.
I clearly also don't understand how to put an asterisk into a reddit comment (sigh)
I suggest the excellent talk by Edsko De Vries called "alternatives to lazyIO, which gives excellent insights. Also google for "pipes haskell London", for the equally excellent talk by Oliver Charles. Despite being a talk on Pipes, the first 15 mins are a refresher/explanation of lazyIO and its pitfalls :)
Well, to grok multiple-arg compositions, you should really just fiddle with the type theory. If you really wanna blow your mind, tho, consider what happens when you chain together `(.)`, as in `(.).(.).(.)`.
Look at it the other way: `$` is just `&lt;*&gt;` for the identity functor. Similarly , `()` is `unit` for the identity functor, and `(,)` is `(*)` for the identity functor. I like to view more general notions like `&lt;*&gt;` as the fundamental operations, we just happen to use the more specific `$` a lot more often :)
It seems a little odd to build a tool like that with support for dozens of languages but only one OS. Isn't the hard part about it usually the support for multiple operating systems using different paths,... for the language runtime installations, global library paths, config files,...?
For what's it's worth, I have a hakyll-powered personal website here: [alpmestan.com](http://alpmestan.com/) with the code [on github](http://github.com/alpmestan/alpmestan.com). It demonstrates a bunch of standard hakyll stuffs + some extras. Feel free to ask any question you'd have about it.
Yes, indeed. But what is this generalized thing? This general pattern of having functor-lifted systems? That's what I'm curious about. `f (a,b)` is not a monoid, but it's kinda a monoid, at least if `f` is `Monoidal`. What is this general thing? I think categorically, probably you could form a special category -- given a monoidal category C, and a function F : C -&gt; D, the category Mon(F,C) is the image of F, such that for every X and Y in C, there's a tupling operation tuple[X,Y] : F X ⊗' F Y -&gt; F (X ⊗ Y) or something like that? maybe more generally, for an endofunctor G? But then that looks like it's related to distributivity of a functor, at least for these simple cases. I'm not sure if there's a similar way of doing monads and the derived-type dual (if there is such a thing). So what the heck is this general pattern?
The first program I had was this https://github.com/kototama/haskell-photos-gallery but it wasn't all my code, it was more like playing. I have experience with Clojure so it helps me but I wouldn't say that Haskell is easy. I had help from IRC and stackoverflow. 
http://comp.mq.edu.au/~mike/pub2000.html (starting from the end) is also a good resource for them.
I don't think so. The idea is that the type family `x` "picks out" the type of the subpart from the type of the whole. I think the type you suggest would be less expressive, because it would require being able to build a concrete type for an entire structure given just the type of some subpart. Unrelatedly, however, I suppose `s` and `t` still ought to be arguments to `Lens` along with `x`.
readFile doesn't read UTF on Windows by default (like in *nix). So sometimes you need to reimplement the wheel.
It's monoids all the way down :-) Applicative functors are monoid objects in a category whose objects are endofunctors and whose monoidal product is Day convolution.
`f &lt;$&gt; m` is just `fmap f m`. For Monads this is what that means: `f &lt;$&gt; m` is `m &gt;&gt;= return . f` or `do { x &lt;- m; return (f x) }` `a &lt;* b` is `do { x &lt;- a; b; return x }` `a *&gt; b` is `a &gt;&gt; b` or `do { a; b }` 
You have to be aware, that hdevtools doesn't support cabal and sandboxes out of the box. There're several forks of hdevtools (e.g. https://github.com/maximkulkin/hdevtools) that support cabal and sandboxes or you can use cabal-cargs (https://github.com/dan-t/cabal-cargs) to make the default hdevtools aware of cabal and sandboxes. If you want to use hdevtools with the vim plugin syntastic, then ensure that you're using a quite recent version of syntastic. 
You know what else can check at compile time that there is no 'infinite' case? Just using the original semigroup structure, and not trying to cram things into abstractions that don't fit for no reason. This is a laughably over-engineered 'solution' to a needlessly self-imposed problem.
I know most tutorial just mention makeLenses, but lens also provides TH functions that require an explicit name mapping like you want. For makeLenses it's the very next definition in Control.Lens.TH, makeLensesFor.
Indeed, and the same with `+++`.
this is in node.js and mongodb so webscale. they aren ot.
He wants to close the handle before the putStr. Which means he wants strict IO. So using functions from Data.ByteString, for example, would do.
That works fine for data-lens style lenses, but when you want to start dealing with type changing assignment you get stuck and need to use a new operator because you can't just abuse a category with argumnts of kind (*,*), and when you want to deal with composing lenses and traversals, etc. together to just work you also get stuck, and everyone has to buy in, and you lose traverse, etc. By the time you're done this design has a half dozen strikes against it. =/
You can disagree without being rude
Not sure if there are further distinctions but pronk looks unmaintained and never got a proper release.
&gt; Yes, it is wrong. Lenses are 100% not accessors. The only way to use them as accessors is to specify f = Const and feed in the Const constructor. Lens is one layer of abstraction out from accessor; it seems not quite right to say that they're '100% not accessors'. When a user intends to use a lens as an accessor -- a perfectly copacetic thing to do! -- she may find that the composition order violates her mathematical intuition. She's not *wrong*, she's surprised, because in the context of her intuition the abstraction has become a bit leaky.
If you want to actually run the code, I'd recommend following though using the Haskell version, rather than implement a new dialect of ML so you can run the custom ML code. ;) The code should be equivalent throughout. If you have any questions those of us who love the book on #haskell would be more than happy to help you along.
&gt; right-to-left &gt; print =&lt;&lt; (liftM $ foldr (+) 0 . filter odd) (return [1..10] :: IO [Int]) doesn't seem quite right. I prefer this instead: print . foldr (+) 0 . filter odd =&lt;&lt; (return [1..10] :: IO [Int]) which is even shorter and arguably simpler than the left to right example
I have been studying Idris, which is strict by default but supports non-strict expressions via a Lazy type annotation. Otherwise it is similar to Haskell, but with dependent types. Now I am suddenly imagining that implementing Okasaki's data structures with dependent types might be fun.
Brian O'Sullivan released wreq (https://hackage.haskell.org/package/wreq-0.1.0.1), which bases its API around Lens. The lens-ification of APIs is a controversial topic around here and was due for a blowout. Here's the blowout.
For one, I wanted something self-contained with as little moving parts as possible as I was interested in identifying potential bottlenecks in the GHC runtime. Also, with `pronk`, I saw sub-optimal results (noticeable worse than `weighttp`) but wasn't sure what the cause was, so writing a minimal HTTP client implementation (which is a trivial task if you don't aim for full RFC-compliance) was the quickest way for me to narrow it down. Also, having a minimal custom HTTP library allows to do things (like low-level time-measurements of individual protocol phases or caching the HTTP request message construction) that are out-of-scope for a general purpose HTTP library. Finally, keeping the library stack `pronk` builds upon building with GHC HEAD was a bit of accidental complexity, and right now it doesn't even build anymore with GHC 7.6.3 as its build-dependencies have diverged away API-wise ^((with no upper bounds to tie them ^down) However, one of the things I stil have on my TODO list is to rebase `pronk` on top of the `uhttpc` library and compare the numbers. Ultimately, I hope that `uhttpc` can serve as a base-line for identifying bottlenecks in the high-level HTTP libraries, and help optimize those to become as optimized as reasonably possible.
This issue troubles me too! A version with `&gt;&gt;&gt;` is coming soon. Edit: Ok, `+++` too Edit: Download it here! https://github.com/i-tu/Hasklig/releases/download/v0.3/Hasklig_0.3.zip
Awesome, Cryptol seemed very interesting to look at from a dependently-typed programming perspective. Might give this a spin.
You should consider writing your code using `Prompt` or `operational`; both of those libraries allow generation of many quite complex monads that are guaranteed to meet the monad laws, and give you a simple way of thinking about how you want the operations to work. The main problem with your design that makes it impossible to meet the monad laws right now is that 'goback' always goes back to the "closest" point, which means it's impossible to write a sub-computation that can internally backtrack but is part of a larger computation that may need to backtrack further. But there are lots of ways to solve that problem and come up with something safe that works just as well. The problem with violating the monad laws is that they correspond to logic that most programmers take for granted, so if you break them, you'll cause common code transformations that look totally safe to break. For example, the law you are breaking means that these two pieces of code are not equivalent: foo1 = do x &lt;- something y &lt;- somethingelse x return (y + 1) -- lets say i use that pattern a lot and so i want to factor it out... helper = do x &lt;- something somethingelse x foo2 = do y &lt;- helper return (y + 1) `foo1` and `foo2` are no longer equivalent, and if changing one of those to the other broke my program, I'd be *very* confused.
Alice ML has a 'lazy' keyword.
Reading the commit I’m wondering why `(&lt;**&gt;) ≠ flip (&lt;*&gt;)` and is [this](https://github.com/ghc/ghc/commit/88c9403264950326e39a05f262bbbb069cf12977#diff-4255ad26ea25b80cc5d2a001a2e23e1eR344) instead. Is there a reason for that?
...and what does `cabal-install` do differently on Windows/OSX that you'd advise against using it?
I've got a few of these lying around. I'd suggest starting with his implementation of lazy queues. Encoding the length of each end of the queue is pretty simple and coinductive lists are remarkably easy to prove things about. There are also a million and one dependently typed binary trees floating about, particularly red black ones since they're so easy to encode at the type level. [CPDT](http://adam.chlipala.net/cpdt/html/toc.html) discusses them in depth. If you decide to go all the way and translate everything, I'd love to see a blog post on it!
So does SML/NJ
So does OCaml, which also have the advantage of actually having a community ...
Anki is a flashcard program which helps you memorize large quantities of facts based on spaced repetition. As you see a fact more often, Anki will intelligently space out how often you are given the fact to review. I've found it's great for memorizing many small things, such as foreign words... and perhaps operators exported from Control.Lens :) This deck is fairly small at the moment and I *highly* encourage contributions to it :D. It does cover most of the basics, though!
And the disatvantage of not having Okasiki's book written in it :) The biggest deviation of the book from SML 97 is that it heavily uses laziness and introduces a corresponding lazy keyword. Both SML/NJ and Alice ML support the laziness used in the book making them straightforward to use for playing with the book. Though admittedly SML is on the out it seems.. shame.
&gt; Others don't like the fact that because the types are so general it is very hard to build intuition for what they do by reading the code without a lot of math background in strange areas, and even then it puts them together a bit sideways. Is there material on the structures that are used to *implement* lens? There's a whole lot of secret tech buried in there.
[So does OCaml](http://caml.inria.fr/pub/docs/manual-ocaml/extn.html#sec216). ;) I did it once, the main difficulty was to translate "fn" to "fun", "$" to "lazy" and to use "let" to declare bindings. It should be doable for anyone interested in this book ! :D
Clicking on that link, I see it's implemented like this: openFile name ReadMode &gt;&gt;= hGetContents That is, it's the same as `getContentsOfFile`!
Thank you for an explanation! That cleared few things for me.
You're welcome!
If you want to start fast you can use the source code of my website: http://yannesposito.com/Scratch/en/blog/Hakyll-setup/ The source is here: https://github.com/yogsototh/yblog You'll get 3 themes, code syntax color + a way to highlight part of code. Disqus for comments, a script to deploy on github pages. And many other small features. On the other hand, one blogging platform I would find interesting is ghost. It is written in an evil dynamic unityped language. But you'll find a lot of themes better than mine (I am not a designer). The blogging experience feel also better, but you might not be able to use github pages as ghost is not a static website. https://ghost.org Note, if you really want you could use the HTML templates of ghost for your website and then be able to use ghost themes. But that'll need some work in HTML/CSS. 
Not much; mostly chatter in the #haskell-lens channel. You can dig up some material on profunctors and representability out there from third party sources of course.
&gt; I wrote my first Haskell program, it is command-line utility to parse JSON and apply Lens expression to the input. Welp, I quit.
I've fixed the floating point rounding assumption on HEAD in the test suite. The `reflection` errors across half of those platform appear to be something in their build scripts changing the dependencies of lens to remove `reflection`. The dependency is listed in the cabal file. It isn't in the packages they build with, so it isn't surprising they can't find the `Data.Reflection` module.
That could be a fun way to work through the book. Especially with the explicit Lazy annotations in Idris feeling a bit like the ones in the book.
Clay is pretty great. I'm a bit bewitched by [GSS](http://gridstylesheets.org/) at the moment, though.
We have a [Haskell meetup for the Washington, D.C. area](http://www.meetup.com/Haskell-DC/) which meets in Arlington, VA! I know that there are some attendees who come from Baltimore.
Could have sworn I'd have noticed that dependency on `lens-family-core` - guess not. I did notice they were by the same author, so I wondered if one had superseded the other. Thanks!
I tried with Leksah, the special symbols just didn't show up, I got garbage instead. Win8 x64, Leksah beta
I'd add that OCaml provides pattern matching on lazy values. (lazy foo) is a pattern if foo is a pattern. That makes laziness pretty tractable.
Oh, I had no idea. Do you know if anyone's bothered to port the book's code? If so it'd be optimal for studying it.
Wow! That is a very neat API. And lenses indeed give it a uniform feel. 
For the lenses part of the lens package there is stuff around (sorry, I don't have the links on me), but I'm not sure about all the fold/traversal stuff that's also in the package.
Any chance of a video?
Likewise.
If you ask for help over on #haskell-lens, I'm sure you can get ideas from folks for more advanced deck entries.
I tried this, but I'm clearly not getting the right results. Any idea what might be causing this, or do you have a sample .emacs which produces correct output? http://imgur.com/a/FiDYk
I'd be down for splitting the difference between the two as a Marylander :).
Where are you located? I'm down by Silver Spring -- we might be able to arrange a car pool.
Arlington is a bit of a haul for me, though I suppose I could swing it. What's the intended experience level? I'm very new to the language, but it was love at first sight. 
[This](http://twanvl.nl/blog/haskell/cps-functional-references) is the basic idea behind how they work.
That's a fantastic idea. We can meet at the Baltimore NODE hackerspace at 403 E. Oliver St., quite close to Penn Station (I'm a member and we'd love to have more people using the space). I put up a Doodle poll for how often and when people would like to meet; if you're interested, please do fill it out. http://doodle.com/uyqyermmap5sv5va
Maybe [monoidal functor](http://en.wikipedia.org/wiki/Monoidal_functor) is what you are looking for.
No way! I wish I had known!
I've REALLY been looking forward to this one. There's potential for this to become a really big deal.
Thanks
Also near Silver Spring.
Any chance you could get in touch with whoever invited you down and point them at this thread?
You wouldn't be the only newbie, that's for sure. I'd describe my experience level about the same way.
Fascinating. Having worked with MVC frameworks like Rails and AngularJS I'm very excited to see how it looks in Haskell. I have only a very muddy sense of what MVC really represents from my prior experience, but I suspect that the ideology behind this library will make the concept much clearer!
Applicatives are precisely closed monoidal functors. In general, there are other sorts of "X functors" in category theory that are functors that also happen to respect some other additional structure. Monads actually _don't_ fall in this category. But you'll have a hard time finding exact analogues for most of those in the same way as with applicative because "monoidal, closed" captures a big chunk of the category of hask. e.g. you have "linear functors" but you can't write those directly as haskell functors because hask isn't a linear category, etc.
What an interesting library, bravo Tekmo.
I cannot claim to be the sole person who invited him down, merely one of the agitators, but I have let the PL group at JHU know about this emergent effort. :)
Nice. How hard would it be to extend to web apps?
This library is potentially interesting as a set of abstractions for building sources and sinks and managing flow between them. However, it has _nothing_ to do with the actual mvc pattern, which is actually _not_ a bad pattern when done right. I had a terrible time trying to figure out how it worked until I mentally renamed "View" to "Sink", "Controller" to "Source" and "Model" to "Pipe". 
Yeah, I don't see any way to connect your Model to a persistence layer, unless you used a Controller for input from your db and a View for output to it.
I would say your suspicion is spot on! And thanks for this 'composing maps' analogy, this hasn't crossed my mind. When I started with lenses ti hasn't crossed my mind that it could be in order different than record selectors as I came to lens for the solution to record selectors problem. It took a day or two to decipher error messages that resulted from this simple mistake. Although lens documentation says 'Composing lenses for reading (or writing) goes in the order an imperative programmer would expect' it could be put in more prominent place. 
I like this a lot.
Thanks for the hint. I´ll look but I´m afraid that it is not so simple. Anyway the backtracking effect in monadic computations solve a number of general computation problems, like tracing, undoing transactions, synchronization etc I think that is in the interest of the haskell community to come up to a solution. See for example here how the bactracking monad is used to undo long running transactions: https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/how-haskell-can-solve-the-integration-problem Yes it is possible to create a subcomputation that may internally backtrack but is part of a larger computation that need to backtrack further. At any moment is possible to fail, even when backtracking, and this propagates back and up in the execution tree. If that would not be the case, there would not be possible to do routing in MFlow beyond a single level. What you mean as a problem, is precisely the intended effect desired, since the way the individual computation ends, determines what effect has to be applied. I hope that the collective intelligence of the Haskell community find a better solution. I do not understand how the haskell community has been living without a backtracking monad until now. Oleg found a solution using the continuation monad, but continuations are not serializable and a bit too intrusive for many purposes. That precludes the use for my problem. http://osdir.com/ml/haskell-cafe@haskell.org/2012-03/msg00645.html By the way, I remember that I tried some alternatives, one of them with exceptions. But this was more simple/elegant for the intended usage, internal to the ask primitive. I have to admit that the design decision may not be the best if the backtracking effect has to be used outside of a few cases internally in a library like MFlow. I´m looking at my old code to figure out what happened....
right. monads don't fall into it. that's fine. i'm just seeing that there are ways to define fairly clean little functors on (codes for) types, which can then be applied to the signature for some mathematical structure, resulting in other interesting structures. like, given some structure `s` with it's signature, you can lift `s` to be `lift f s` (for functor `f`), producing some new kind of structure. for the appropriate choice of `s` and `lift`, the result is applicative functors, for a different choice, it's monads, etc. this seems like an interesting thing.
no chance. it was not recorded.
&gt; However, it has nothing to do with the actual mvc pattern I disagree. The definition of [MVC on Wikipedia](http://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller) seems to sort of agree with how Tekmo has defined it. The controller is the entry point where data comes in, the view is the exit point where data goes out. And the model has the rules that govern how inputs get turned into outputs. You do have to squint pretty hard to see it, but there *is* a vague resemblance.
Edit: forgot to say the most important thing - great work! This is a useful endeavour indeed. I started exactly this, but decided I didn't want to be the bottleneck for having the deck grow. Unfortunately, it seems very hard to have collaborative Anki decks. They don't have a good plain text representation, so Git is out, and the online collaboration stuff still doesn't seem to be good for multiple-writer scenarios. At this point I got fed up and wanted to write my own flashcard program based on convergent data types so I could solve this once and for all... but you can imagine how far I got with that :)
I find `handles` rather confusing. It only handles the first match (if any) in the traversal. Why not all? If it really is desirable to only handle the first match this is another example of the need for a type of affine traversals (traversals with zero or one target). &gt; However, right now we're not interested in transformations from functions to functions. Instead, we're interested in transformations from monoids to monoids, so we're going to invoke a different set of functor laws for our Controllers: Presumably you are actually still interested in them as transformations from functions to functions, and you want the original functor laws to still hold, right? The monoid-preserving laws are just *additional* laws.
I think this matches up with the IMonad/IFunctor classes in [index-core](https://hackage.haskell.org/package/index-core), is that correct?
:(
Anyway the BackPoint constructor can not be used outside of the library, so it is unlikely that confusion. Maybe it is better to hide even breturn (which uses BackPoint) and create more explicit constructions like "compensate", defined in the first link above. The last version of MFlow as well as the supervisor package have constructions like "onBacktrack" or "compensate" that avoid the confusion you mention 
Is this related to functional reactive programming in some way?
Maybe /u/bos could comment on future plans for `pronk`?
So I struggled a bit longer than necessary with your last tip about `pointfree`, it just would not work out. After some googling, etc. it turns out that the `formatprg` is expected to wait for input from `stdin` (Try out `sort` or `fmt` without options on the commandline`). tl; dr; Just use `set formatprg=xargs\ -0\ pointfree`
Sounds like Standard Chartered.
It seems to be a very restricted form of functional reactive programming. "Restricted" doesn't mean "bad" though. It just means "specialised to a particular common use case".
Je l'avais déja en favoris mais je n'ai jamais eu le temps de le lire, en tout cas j'adore votre blog
I'm also very interrested!
I'd be interested in coming so long as it gets stood up quickly. I'm moving to Boston quite soon!
I think it's be simpler if Views were just [contravariant functors](http://hackage.haskell.org/package/contravariant-0.1.2/docs/Data-Functor-Contravariant.html).
Tekmo may talk about this more, but we had some discussion of "slow" FRP a while back which `mvc` is reminiscent of. The idea being driven by how to describe a long-running, partly-scheduled, partly-reactive interaction with a human over a fixed channel. You might send them a reminder like "walk the dog" tomorrow but you might also respond immediately to questions like "are you alive?" (a: "Daisy, Daisy, give me your answer do. I'm half crazy all for the love of you.") and even use those reactions to change the structure of future reactive or scheduled communication. Which, once you get that concept in your head, is kind of "slow" FRP.
Ok!
Slightly confused because was expecting a discussion of Free (Control.Monad.Free http://hackage.haskell.org/package/free-4.7.1), but instead recieved a discussion of category theory and HMonads? The intro is all well and good, and then ends that section with the claim "We're going to want monads in the category of endomorphisms of Hask which I'll call Endo." This is the point where he looses me. It's not the discussion of natural transformations, categories, and arrows which confounds me, it's why one resorts to this machinery when Control.Monad.Free seems to be fine without it. In other words, what is HMonad doing that Control.Monad.Free is not? (Admittedly, I may just have to spend some more time reading the article in depth - perhaps I can answer some of my own questions)
The entry on profunctors in "24 days of Hackage" is a good start: http://ocharles.org.uk/blog/guest-posts/2013-12-22-24-days-of-hackage-profunctors.html Profunctors seem to be related to Arrows, however they are not implemented in terms of them and there's no 100% overlap. For example, there's no requirement for a Profunctor to be an instace of Category.
do you not find this problematic? that something so important as Lens had absolutely abysmal educational material for it?
et voila https://www.fpcomplete.com/user/psygnisfive/from-zipper-to-lens hopefully that helps
They are contravariant functors. `View` implements `Contravariant`. However, (I think) you can't implement something like `handles` using `contramap`.
Huh, it never occurred to me that would work. I will make this change soon. Thanks for the neat suggestion! :) Edit: Also, to clarify, I *do* intend both functor laws to hold. That sentence from my post was only meant to clarify that the functor design pattern reference was referring to only one of the two sets of laws for that specific example.
Is there a reason that some functions use under_scores and others use camelCase?
One of the original test cases for this app was a project by Tony Day to create a web application that would track several metrics and display continuously updating graphs to a browser using websockets. You can find his code [here](https://github.com/tonyday567/pipes-emitter). That used a very old incarnation of the original idea behind `mvc` and a lot of the issues he ran into motivated the `Managed` type.
That is exactly how you connect your `Model` to a persistence layer.
I think "nothing to do with the actual mvc pattern" is too strong of a statement. When I named the library I wanted to choose an existing domain that I felt it mapped the most closely onto. Functional reactive programming was out of the question, so I picked MVC because out of all existing design patterns because it was the closest match in terms of architecture and suggested use cases.
Yes: the functions which use underscores are examples. If this was a library, those identifiers would not be part of the public API.
&gt; mapHead.mapTop.mapJust :: (a -&gt; b) -&gt; [Tree (Maybe a)] -&gt; [Tree (Maybe a)] Should be (a-&gt;a), no?
Ah, ok. I think it's just an issue of "burying the lead" or sequencing in the article. Maybe I do better when it's explicit why an abstraction is being introduced early on to get the overall picture, and the explanation is provided afterwards.
There are some differences between the `mvc` library and functional reactive programming. Functional reactive programming has separate notions for events and behaviors. Events you can think of as discrete occurrences and behaviors you can think of as continuous values over time. `mvc` is event-only. Also, functional reactive programming tends to frame most things as sources transformations on sources, or folds over sources. `mvc` also emphasizes sinks and gives them equal weight abstraction-wise. The third difference is more of a soft idiomatic difference rather than a hard technical difference: functional reactive programming tends to emphasize a (somewhat) uniform type for connecting components, whereas I prefer to define components in several categories and then use functors to unify them to agree on a common category. You can get a hint of this by reading [this section](http://hackage.haskell.org/package/mvc-1.0.0/docs/MVC.html#g:6) of the `mvc` documentation, but it really goes much deeper than that and it will be subject of upcoming blog posts.
Your implementation is missing 2 critical design components of `lens`. 1. With `lens` whenever you compose a `Traversal` and a `Getter` you get a `Fold`, etc. It takes the LUB of the features required automatically. 2. All the combinators can work with any of those data types. What you have is crippled in both of those cases. The fact that you have to reimplement every accessor for every supertype demonstrates this limitation better than anything else I can say. You can of course write manual conversions and litter your code with them.
The first paragraph explains exactly what the goal is. &gt; As Dan Doel points out [here](https://www.fpcomplete.com/user/dolio/many-roads-to-free-monads), the gadget Free that turns a functor into a monad is itself a kind of monad, though not the usual kind of monad we find in Haskell. I'll call it a higher order monad and you can find a type class corresponding to this in various places including an old version of Ed Kmett's [category-extras](http://comonad.com/haskell/category-extras/dist/doc/html/category-extras/Control-Monad-HigherOrder.html). I'll borrow some code from there. I hunted around and couldn't find an implementation of Free as an instance of this class so I thought I'd plug the gap.
Anki can definitely import csvs. If they can't have assets embedded into them, I'd check out the other formats that are available out of the package. If I recall correctly, Anki at least mostly python so writing your own importer as either a core contribution or a plugin may be doable.
Yes.
Note that `MMonad` from [my `mmorph` library](http://hackage.haskell.org/package/mmorph-1.0.2/docs/Control-Monad-Morph.html) is the same as the `HMonad` from the post. I call it a monad in the category of monads.
Already done. =)
Couldn't you just typeclass the combinators to solve those issues? 
I would rip out the IO from the processing pipeline, and make all the functions return type Either. Once that is done, your processing with error handling is simplified thanks to the Either monad. input &lt;- fmap concat . mapM readFile $ files let result = do input' &lt;- processInput opts input return $ processJSON opts (JsonInput input' v) case result of Left ...
I've just gotten hdevtools working, albeit with a hack to find my (cabal-dev) sandbox. It's super fast! I think this could become a standard part of my workflow, even despite the fact that it will probably require some fiddling to get it to work with varying sandboxes.
The COM or "Component Object Model" is a style/library for object oriented C. In this case "COM style" refers to a system where you have an implementation of existentially quantified type, together with some "interfaces" that you know it implements. In particular, if one of these interfaces allows you to check what *other* interfaces the type implements dynamically, than you can do all the normal OO things (and then some). Although, the user coding style ends up being a bit odd.
You'll find that inference for the typeclass solution is less robust than the trick encoding provided by `lens`. I tried it and found it wanting. Information about the types required flows both forward and back through the constraints in lens. This enables the combinators to 'change their demands' as you use them on types that give back different answers. Even if you remove that functionality, it is also a lot more maintenance. We _found_ most of the various optics just by looking at what constraints were actually demanded by different applications. That exploratory benefit is also lost as you have to pay n^2 in the number of combinations just for composition, everything has to know about everything else, yielding expression problems, and it is easy to screw up and get it wrong. The current set of optics isn't 'complete' it is just the points in a much larger space of refinements that are possible that we find useful. The list of 6-7 is about a 5^th of the list actually offered by lens. http://hackage.haskell.org/package/lens-4.1.2/docs/Control-Lens-Type.html So you're signing up for ~150 instances that are easy to screw up and are an approximation of the right behavior as opposed to the current approach which just works out of the box with Prelude types. In the end you're replacing something that works for reasons governed by laws with a bit of ad hoc polymorphism ripped from the culture of perl for less utility. I'm rather completely uninterested in going down that road.
We use the full 4 arguments when you want to typecheck something like over _Left length (Left "hello") That utility is important.
Have you tried simply to map **(parseOnly parseIP)** over the list of ip strings? Also, Stack Overflow is another good place to ask Haskell questions.
It's not my team AFAIK. Could be another group in the bank though.
Is this like the client-side of acme-http?
I know! That's why I said using type synonyms was clever :)
Don't you also need a control channel so the Controller knows what to push to the Model?
So the intended use case was to keep as much statefulness in the `State` layer of the `Model` and then periodically persist to disk if necessary (using something like `acid-state`). Anything more complicated than that is outside of the scope of the `mvc` library and requires a more general abstraction (such as a free monad-based interface to the database). Edit: [This is what I mean](http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html) by a free monad based interface.
Oh right. Turns out logic and math is actually more clever than me.
I didn't realize that. Thanks for pointing that out! :)
I feel like I've strayed into /r/lenses/. 
ap
This remark is really inexplicably mean and thoughtless.
"ap" if I absolutely have to pronounce it. But when being forced to read a combination such as f &lt;$&gt; x &lt;*&gt; y I prefer to read it as &gt; liftA2 f x y or simply &gt; F lifted into applicative and applied to x and y. In general, I'm not entirely comfortable pronouncing most of the operators.
&gt; The definition of MVC on Wikipedia[1] seems to sort of agree with how Tekmo has defined it. only if you oversimplify it 
I mean mvc is a nice pattern. There are nice patterns that are not mvc. If you call the not-mvc thing mvc then it is confusing. I tried to understand this using the idea of "mvc" and was confused. I then renamed mvc as I described above and was no longer confused. Is that mean? Is that thoughtless? I'm not mean about the library, which I say is potentially interesting (I'd have to use it to say for sure). I _do_ disagree about the naming. Honestly, even getting people to agree about what "mvc" means at all is confusing enough, usually. Adding another notion to the mix that you have to at best "squint to see" really doesn't help us at all.
Perhaps a more generic name like "pipes-interactive" would be appropriate?
… and `Free PostgreSql` is left as an exercise for the reader. 
There is 100% overlap in one direction. An `Arrow` is _any_ `Strong` `Profunctor` that is also a `Category`. arr f = lmap f id = rmap f id `first` and `second` are `first'` and `second'` respectively.
Brackety-splat, or “ap” if I’m boring; as in “f fmap x ap y ap z”. Edit: `&lt;$&gt;` is brackety-cash, before you ask.
I like the core of `fclabels`, but I find I can't reason about the generalized categorical construction it offers in terms of laws. `lens` actually came about from trying to generalize the basic notion of a `Lens` in a different direction than fclabels, scalaz, etc. were going, that let us retain the laws.
I'm trying to downplay the significance of `pipes` for this interface. The really heavy lifting is actually in the `Input` type from `pipes-concurrency` (which is actually unrelated to `pipes` and I plan to factor it out into a separate `pipes`-independent library so `conduit` can reuse it). The other reason I want to downplay `pipes` a little bit is that I want to discourage people from always programming at the full feature set of `pipes` and try to use narrower abstractions to build up the model when possible, such as using `ListT` or `State` kleisli arrows or even pure functions, and then only promoting them to `Pipe`s as late as possible. See [this section of the `mvc` documentation](https://hackage.haskell.org/package/mvc-1.0.0/docs/MVC.html#g:6) to get an idea of what I'm aiming for. Also, there is a practical concern that I would prefer not to change the name after releasing the library because it would be a really large inconvenience. However, if I end up writing a paper on this I will consider a more appropriate term (possibly a new one) that is more precise.
None taken. I spend a lot more time building things and somewhat less on presentation than perhaps I should.
I tend to just read `f &lt;$&gt; x &lt;*&gt; y` as `f x y`.
Rarely do I need to say it, but when I was explaining applicatives to a co worker, I kept saying "apply f to x and y". Even though "apply" usually means $ or &lt;$&gt;. If it is just f &lt;*&gt; x, then I might say "apply lifted f to x". 
That's exactly what abstractions (like MVC) are: simplifications and patterns.
I think "interactive" is the key bit for me since it feels like an expansion on the ideas behind our beloved "interact" function. I can see not wanting to push the "pipes" connection, but at the moment the only way I saw to create a model was out of a pipe?
I am curious what your definition of mvc is.
If &lt;=&gt; is the spaceship operator, doesn't that make &lt;$&gt; starbucks?
By “oversimplify it” I meant “apply to some specific simpler case”. MVC already has a meaning and it's different. Like, 1) it's completely ok for Model to store to db, 2) it has nothing to do with View. And I'm not saying it's good to do it, but the name is taken. 
This was a pretty good explanation that helped me understand how lenses are build. Especially the idea that that they are "lifting" functorial modifiers of the target to functorial modifiers of the whole was really helpful. The type of lens almost makes sense now! Why does the version in the library have the `forall` though?
I agree: the `interact` metaphor works really well because it highlights how the model works well and (equally nice) it highlights the flaws of the `mvc` model (the disconnect between inputs and outputs). However, "interactive programming" is too broad of a term, so I'd like to qualify it as perhaps "concurrent interactive programming" since that's one of the new twists that `mvc` adds. Edit: Or maybe just "interactive concurrency"
In this case the `State` layer of the `Model` is playing a role analogous to the database, but I agree that if you want to interact with a real database then it's too simplistic and you should replace the `State` layer with a free monad that abstracts over database interactions.
The forall shows up in places like type Lens s t a b = forall f . (a -&gt; f b) -&gt; (s -&gt; f t) and demonstrates that the `Lens` type itself is invariant to the choice of `Functor`. If you're producing a lens it looks like this mkLens :: In -&gt; Lens s t a b mkLens :: In -&gt; (forall f . (a -&gt; f b) -&gt; (s -&gt; f t)) mkLens :: forall f . In -&gt; (a -&gt; f b) -&gt; (s -&gt; f t) where the last step is quantifier lifting and done implicitly by Haskell. The tricky part occurs when consuming a lens: eatLens :: Lens s t a b -&gt; Out eatLens :: (forall f . (a -&gt; f b) -&gt; (s -&gt; f t)) -&gt; Out now the quantifier cannot be lifted and we need to provide a truly polymorphic `Lens` type as an argument. This is where you need to enable RankNTypes.
The name has been taken and re-taken repeatedly since the 70s. I don't see a problem with it being re-taken yet again for a related set of concepts.
Pronounced "s'worcestershire"
The first is short for the second whenever it gets let generalized. I.e. they are both equivalent when they're annotating a name somewhere. The explicit forall also plays an interesting role in `type` definitions since it lets you refer to the `f` variable which wasn't bound on the lhs of the equals.
Reminds me of http://conal.net/blog/posts/semantic-editor-combinators
I don't understand why there's so much hate on the choice of calling it "MVC". It *is* MVC. The key idea —the only idea— behind MVC is the tripartite separation of a program in order to isolate concerns, in particular: separating the program along the lines of maintaining an internal "model" which can thence be manipulated or viewed from outside, and then separating manipulations from views. This idea is profoundly simple, which is part of why it has held on for so long. The bit about side-effects isn't especially relevant to MVC as such, though it's not a terrible idea either. If our "models" are pure, then they actually resemble what are called models in other areas of mathematics; which I count as a good thing. The fact that "models" are wretchedly impure in most programs with an MVC structure stems more from the fact that people are using impure languages and ideologies; it has nothing to do with MVC itself.
How, exactly, do you see this as not being MVC? For the sake of argument, let's accept the Wikipedia definition. In what specific ways does Tekmo's library violate the definition on Wikipedia?
I think you would really like Conal Elliot's post on [semantic editor combinators](http://conal.net/blog/posts/semantic-editor-combinators), which was the intellectual ancestor of Van Laarhoven lenses.
Brackety-splat! That's great! I'll just leave these here: http://poetry.about.com/od/poetryplay/l/blwakawaka.htm http://www.dict.org/bin/Dict?Form=Dict1&amp;Query=ASCII&amp;Strategy=*&amp;Database=jargon&amp;submit=Submit+query
To be fair, the request/response stuff that predated monadic IO was the sort of thing that was so easy to get wrong that it'd be hard to defend against the beauty of a monadic or continuation-based approach.
Yeah, you could say that.
Via HN. Discussion: https://news.ycombinator.com/item?id=7637278
There's also the generalizations that you have in `semigroupoids`
I'm working on another way to reduce the boilerplate for them.
An excellent paper. (But there's more to the story, of course.)
Strangle?
Yes, the notion that events are different to behaviours in FRP seems unnecessary. Looking at the MVC pattern convinced me that there's no such thing as "continuous values" - A behaviour is just a request to a Controller to issue an event. MVC is a good fit for FRP.
Alright, the consensus for Baltimore HUG meeting times appears to be monthly meetings on Tuesday evenings. May I propose that we kick it off on May 6th at 7 PM at the Baltimore NODE? I've also created https://groups.google.com/forum/#!forum/bmore-hug for the lot of us; somebody more social-media adept may wish to pass it around. :)
It flares up every few months.
We actually avoid taking lenses as Rank-n types in argument position. We just restrict ourselves to the concrete choice of `f` used in the function. This is how we permit you to work with any other optic, we ask you to ask yourself for the instances needed by the choice of optic. You get stuck with Rank-N types with the lens synonyms because GHC isn't smart enough to figure out that something returning a Lens on the right hand side of an -&gt; is actually still Rank-1.
Via HN. Discussion: https://news.ycombinator.com/item?id=7653013
I guess `&lt;*&gt;` could be starship? 
I think the classic '88 paper does a good job of explaining this. http://www.ics.uci.edu/~redmiles/ics227-SQ04/papers/KrasnerPope88.pdf
My objection here is that what's labeled as a "model" if anything is too _impure_, conceptually. It is a broker between events from the "controller" to updates to the "view". A model should _just_ be that "State" bit, so to speak (or the moral equivalent, be it backed by a database, or whatever). It doesn't act or mediate actions or the like. It is just acted on by the controller, and it just transmits updates to its state or responses to queries regarding its state. You can build mvc on top of what this package provides, because it is a general purpose package, but it actually doesn't have enough to encourage or even really facilitate an mvc-like pattern in my mind. In my mind you'd want a model to effectively be a data structure endowed with an algebra of actions on it, a controller to be some process that generated a stream of such actions, and a view something that via push and pull kept itself in correspondence with the state of the model. Now that's sort of what you describe, and its something that one could perhaps write with this library (but one could also write it without this library!) but its not really the core toolkit provided here, where almost all the work seems to go into the box we've labeled "model".
Yeah, I got that working eventually. It's very frustrating as a noob struggling to do in haskell what I could do in C++, Pascal or Python in about 2 minutes. As for stackoverflow, it has been my experience that reddit is just as good and often better for both getting and giving help. I find the rules/restrictions imposed by stackoverflow to be tiring.
Huh, I know the (concrete choice of `f`) pattern but I never examined it closely enough.
This escalated quickly. I'm in. Looking forward to seeing you all there. 
Oh hi there :P I would scrap heroku and put your remote state on ec2, digital ocean, etc. Heroku doesn't give you root access I believe. Just make the ports available and point the client at the server ip and voila you're up.
&gt; We actually avoid taking lenses as Rank-n types in argument position. That explains why I could not find any actual problematic case :) So I had to do with a contrived test. Take fooL :: Lens s t a b -&gt; String fooL = const "Foo" `mapped` is a `Setter`, and so it has an explicitly quantified type which is more restrictive than `Lens`. `fooL mapped` doesn't typecheck: Prelude Control.Lens&gt; fooL mapped &lt;interactive&gt;:41:6: Could not deduce (Settable f) arising from a use of `mapped' from the context (Functor f) bound by a type expected by the context: Functor f =&gt; (a_tM -&gt; f b_tN) -&gt; f0 a_tM -&gt; f (f0 b_tN) at &lt;interactive&gt;:41:1-11 Possible fix: add (Settable f) to the context of a type expected by the context: Functor f =&gt; (a_tM -&gt; f b_tN) -&gt; f0 a_tM -&gt; f (f0 b_tN) Alternatively, I can implement `mapped` in the style of my toy lenses, without using explicit quantification: fooL' :: Functor f =&gt; ((a -&gt; f b) -&gt; (s -&gt; f t)) -&gt; String fooL' = const "Foo" sets' :: ((a -&gt; b) -&gt; s -&gt; t) -&gt; (a -&gt; Identity b) -&gt; s -&gt; Identity t sets' m k = Identity . m (runIdentity . k) mapped' :: Functor f =&gt; (a -&gt; Identity b) -&gt; (f a -&gt; Identity (f b)) mapped' = sets' fmap And then, `fooL' mapped'` works fine. *Main&gt; fooL' (mapped' :: (a -&gt; Identity b) -&gt; [a] -&gt; Identity [b]) "Foo" That being so, my questions boil down to: - Does my example properly capture the situation we are discussing? - Does the explicit quantification bring any benefits, or is it no more than an effect of the GHC infelicity you mention? 
Thanks for pointing out the `forall` issue; the post probably shouldn't be completely silent about it. I will add a note about it once I figure out how to explain the situation (as per the parallel subthread) in a way that is not too distracting.
We want the lower rank type so that `sets` can work with all of the variants. Explicit quantification has the benefit that you can pass the same lens to two combinators that need different choices of `f`, without using something like `cloneLens` on it. However, as you use lens more and more you'll find you can basically always avoid those cases, by dipping deeper into the `lens` bag of tricks. You know, the harder to motivate combinators that I get so much crap about. ;) The downside of explicit quantification is that you need an explosion of additional combinators.
don't use HaskLig. You can see that "Ubuntu Mono" worked for me.
That was my point. As the expression isn't evaluated, it doesn't seem correct to say that the *value* of `"Hello" ++ " World"` is `"Hello World"`
In a History of Haskell, it mentions that &gt; In working out the detail s of these approaches, we realised that in fact they were functionally equivalent—that is, it was possible to completely model stream I/O with continuations, and vice versa. Thus in the Haskell 1.0 Report, we first defined I/O in terms of streams, but also included a completely equivalent design based on continuations ... even though continuations were considered easier to use for most purposes. So Haskell 1.0 already had a continuation based API, but monadic IO was quickly realized to be better than stream-based IO, or continuation based IO written on top of streams: &gt;The monadic approach rapidly dominated earlier models. The types are more compact, and more informative. For example, in the continuation model we had readFile :: Name -&gt; FailCont -&gt; StrCont -&gt; Behaviour &gt; The type is cluttered with success and failure continuations (which must be passed by the programmer) and fails to show that the result is a String . Furthermore, the types of IO computations could be polymorphic: readIORef :: IORef a -&gt; IO a writeIORef :: IORef a -&gt; a -&gt; IO () &gt;These types cannot be written with a fixed Request and Response type. However, the big advantage is conceptual. It is much easier to think abstractly in terms of computations than concretely in terms of the details of failure and success continuations. The monad abstracts away from these details, and makes it easy to change them in future.
But [ and ] are brackets. Shouldn't it be called chevronity-splat?
Well when we say that, we mean value in the sense of Haskell denotational semantics, which does not include the implementation-peeking `:print`.
That depends why he wants to close the handle early. If the point is to make sure the file can be re-opened elsewhere, you're probably right. If he's just making sure he doesn't forget to close the handle later, he's better off with `withFile`. In other cases, maybe neither of these is adequate.
Lang ast rang.
My understanding of category theory is so bad that I'm not sure whether you are talking gibberish / joking or this is a correct definition.
Eye of mordor.
Yes, I think in Cabal you have to specify 'default-language: Haskell2010' which needs 'cabal-version: &gt;= 1.10'.
It does get frustrating at the beginning, until one gets comfortable with monads, maps, folds... But some things (like defining recursive descent parsers and anything tree-related) are actually easier in Haskell. I recommend the Typeclassopedia if you haven't read it already: http://www.haskell.org/haskellwiki/Typeclassopedia
&lt; and &gt; are angle brackets, so they have rights too.
Langastrang sounds like a Skrillex production.
Obviously `&lt;*&gt;` is a TIE fighter. Specifically, a TIE Advanced like Vader flew.
because it _is_ `ap` (for `Applicative` and not `Monad`).
maybe laser surgery is an option for you. i doubt pills work.
As we all know, `Free F` is a monad in Hask. The point of the article is that, moreover, `Free` (N.B., not passing the functor argument) is a monad in Endo.
Extensions are extensions because they deviate from the standard. Probably most of that page should be updated to say Haskell 2010 instead, being the latest standard, although the differences between the two aren't huge. When that documentation is saying "in Haskell 98" it does so to contrast how the extension deviates from the standard, typically by lifting or adding a restriction or by adding a (language-level) feature. It should be noted though that GHC even with no extensions enabled isn't completely standards compliant, at least so I hear.
Oh, continuous values are good for some things. Things like the current time are better understood as continuous values than as signals. But ultimately they don't add any actual power, unlike things like switching.
Because he doesn't want to write in Common Lisp? I don't get your point. Lisp is not the best language for everything, specially if you limit yourself to a single dialect. Calling Lua from Haskell would be useful to add scripting to a Haskell application, and calling Haskell from Lua would be useful to transport Haskell libraries to Lua, for example.
Of course. Many times. Why? Are you telling me that at least one of these points is not valid? So maybe you will answer me a couple of questions. Ad1. IDE with autocomplete and remote REPL (hot code reload) Ad2. SDL2 or any decent library not on xxxGPL, GUI designer preferred, cross-platform Ad3. Above + documentation, I won't accept C++-like : f :: Int -&gt; Int -&gt; Int code. Ad4. this cannot be done unfortunately, because Haskell is not extensible(programmable) language Additional: ARM compiler All of these consistent A to Z without a need to switch between platforms or juggling compilers. Why I'm asking for this? Because C++ seems much better than Haskell for any job including concurrency. I really like those bureaucracy talks about monads, type category, HM and all, but that doesn't solve real problems.
Nice write-up, but I think the criticism of Linear/Uniqueness types is undeserved. It's not that heavy machinery, and it's pretty easy for programmers to understand.
You do know that you can place C++ objects into `malloc()`ed or other managed memory, I assume? Use the placement `new` syntax, and `sizeof` to figure out the allocation size. Then you can have a call to the destructor in a finalization function, but that will not free the memory, leaving it to the GC to actually do so.
Noticed which subreddit you're in?
Also, if you are interested in MVC applied to FRP, check [this](http://elm-lang.org/blog/Pong.elm). It's in el, which if you don't know, is like Haskell, but: * has no typeclasses * has records * has no `IO` but only `Signal`, for which `&lt;~` and `~` are like `&lt;$&gt;` and `&lt;*&gt;` * Compiles to webpages(not only the JS, but also the HTML and CSS). With that you should be able to understand the example.
Has unique types (from the Clean language) been considered as an option at the time?
Weird! When I saw Erik Meijer speaking about these kinds of issues, he always seemed like he's moved on to the imperative dark side. I remember he said something along the lines of "OO" being more powerful than higher-order functions because you don't pass functions, but a whole vtable of functions as arguments. Arguing that purity is not a good idea in some avenues, etc. Did Erik just play devil's advocate, or did he change his mind again?
&gt; Python has no good IDE Pycharm is actually excellent. The start times and memory usage are a pain but autocompletion, refactoring etc work well. You're point still stands though because pycharm has only really become popular recently. &gt; Documentation: Language-wise, Haskell is probably one of the best documented languages. Library-wise: It is about average. No better and no worse than other more-popular languages. It seems pretty bad for libraries. Apart from a few notable exceptions there are often no examples 
In particular, there's a lot of interplay between higher-rank polymorphism and concrete representations. Both approaches have their benefits, but usually we try to avoid switching between them too much, whereas `lens` revels in doing so.
&gt; It seems pretty bad for libraries. Apart from a few notable exceptions there are often no examples There are rich static types, instead. For a beginner, they are much worse documentation. For an expert, they are much better documentation. I already conceded that Haskell has a steeper learning curve, and this is part of it (and it should be addressed!)
&gt; For an expert, they are much better documentation. I don't think they are better for an expert. They are probably adequate but an example and an explanation never hurt anyone! 
Uniqueness types had not been invented at the time (according to §7.1 of the paper /u/pjdelport linked to).
Speaking about 'power' of features of programming language is generally highly confusing. You can (at least) mean two things: 1. power ~ how many degrees of freedom there are 2. power ~ how easy it is to reason about something Objects are powerful-1) because subtyping, encapsulated state, but not powerful-2) because you never know what might happen. I'm not saying that powerful-1) and powerful-2) are *opposites* of each other, but I'm saying we shouldn't use the word "powerful" for PL features.
Based on your comment, I went to look for his criticism; unfortunately he just briefly mentions the topic dismissively. Anyway, the reason I believe that he is wrong (for some application domains) is that there are applications out there that need timing guarantees and therefore can't use GCs. If you want to have pure languages that can collect memory without GCing, you need that kind of type system. As I'm a firm believer that pure languages are the way to go and there is an industry forced to use C -- what can I say, looks like we need linear types or something similar, doesn't it? Edit: &gt; and it's pretty easy for programmers to understand. 100%. I sometimes wonder how you would build pure PLs that you could teach to kids, inspired by those Smalltalk/Xerox PARC experiments (google the video!). Monadic IO seems to completely fail in that regard. Sure, Monads are amazingly useful and powerful far beyond IO -- but are they "a kid could do it"-easy? Doesn't seem so. Unique references, however: "you have an apple. you eat the apple. you don't have the apple any more". Kids ARE GREAT at reasoning about ownership. That's the first thing you learn, isn't it? Seems like you'd call something owned a 'thing' and something immutable 'knowledge' or 'fact' or 'idea'.
The pattern is that we're using the functor `f` to zoom in on some subcategory of Hask (namely the image of `f` in Hask). Let's call this the Im(age) category of `f`. * The image of the unit object in Hask is a unit object in Im. N.B., the unit object of Hask is also a terminal object of Hask. * The tensor of images in Hask (spelled: `(f a, f b)`) are the image of tensors in Im (spelled: `f(a,b)`). N.B., the tensors in Hask are also products in Hask. * And the image of evaluation morphisms in Hask are evaluation morphisms in Im. This, of course, follows immediately from the fact that the image of tensors is a tensor: `fmap eval . (*) :: (f (a-&gt;b), f a) -&gt; b` where `eval = uncurry ($)` The key thing here is that we can codify when these three properties are guaranteed to hold, namely when `f` is a strong lax monoidal functor, aka `Applicative`. The fact that `f` is a monoidal functor means that it preserves tensors and the tensor unit. The strength and laxity of `f` give us some extra nice laws we want in practice in order to make good use of the fact that it's a monoidal functor. *Edit:* by "strong lax monoidal functor" I mean a "lax monoidal functor with tensorial strength" (cf., [the original paper](http://www.soi.city.ac.uk/~ross/papers/Applicative.pdf)). I do not, of course, mean that it is both a "strong monoidal functor" and a "lax monoidal functor" since that's contradictory (cf., [Wikipedia](http://en.wikipedia.org/wiki/Monoidal_functor))
&gt; They are probably adequate but an example and an explanation never hurt anyone If I had to choose between a Haskell rich type, and an example, I'd choose the former. Haskell libraries are *comprehensively* covered by types (unlike other languages' where sometimes you only have a name to work with!) and semi-comprehensively covered by explanations (similarly to other languages). There are less examples, but to an expert, the types are that much more meaningful.
Because lens-style libraries tend to use lens idioms for their public interface, thus requiring any users of the library to program that way.
'Power' is clearly powerful-1) and certainly not powerful-2)!
I think the [empty](http://hackage.haskell.org/package/empty) package is terrible. Not only because of the plagiarism of Magritte, but because the metaphor isn't appropriate. Magritte's whole point was that it was actually a painting, not a pipe; but this really is a package. Furthermore, the word "package", even if it were truly a French word, would have been masculine, not feminine. This package does not live up to John's usual very high standard of quality. Oops, not popular enough. Never mind.
My understanding is that he's very good about not believing in a language for the language's sake. It's a good thought experiment to wonder if OO is more powerful than HOFs. It's a good position of mind to be willing to adopt that stance quickly. It's an especially good position of mind if your job is often to bring HOF-style features to a community of OO programmers in a way that feels natural to them. I was reading about Comega recently and loving the fine line he draws between pushing people forward into a new, weird brave world and making sure nothing seems so unfamiliar to someone excited about C# 2.0.
Probably the most important feature of `Monoid` is `mappend`. Most of the time, when you can define `mappend` for your type, you can also define `mempty` in a natural way, so you have a `Monoid`. (And then `mempty` can come in handy, too.) But sometimes, there is no natural way to define `mempty`; then you have a `Semigroup`. The operation `&lt;&gt;` for a `Semigroup` is the same as `mappend` for a `Monoid`; in fact, `&lt;&gt;` is defined as an alias for `mappend` for `Monoid`. Here are two examples of a `Semigroup` that is not naturally a `Monoid`: * Non-empty lists. In fact, these are defined in the semigroup package itself. * Rectangular regions on a canvas anchored at their upper-left corner. There is no natural `mempty` because the empty rectangle has no upper-left corner. You can make any `Semigroup` into a `Monoid` simply by adjoining an artificial element to serve as `mempty`, or equivalently, by wrapping your type in `Maybe`. But that's usually not worth the trouble. I once wrote an application that needed the rectangular regions in the second example above. At first, I made it a `Monoid` by adding an extra constructor that represented an artificial "empty rectangle". But my code soon became bloated and complicated by constantly having to deal with the special case of the fake "empty rectangle", which never actually comes up in real usage. Then I switched to `Semigroup` - it was a joy and a pleasure. Unfortunately, `Semigroup` is not a superclass of `Monoid` as it should be. So if you need to use both a `Semigroup` and a `Monoid` in the same module, you can only use `&lt;&gt;` for one of them due to the name conflict. You have to use a different name for the operation on the other.
What do you think about these terms: * Featureful: How many degrees of freedom there are * Simplicity: How easy it is to reason about something
It's worth thinking about what the word "leverage" means here. It's the mantra I've used for what I seek in my programming over time. I got to a point where simplicity/safety/types enabled more leverage.
&gt; I remember he said something along the lines of "OO" being more powerful than higher-order functions because you don't pass functions, but a whole vtable of functions as arguments. He knows this isn't the case because a vtable dispatch is equivalent to a pattern matching on a sum. It's more powerful in one way, and less powerful in other ways. FP and OO are duals, so neither is more powerful than the other. They each provide extensibility in orthogonal ways.
This seems worthy of a fine little cabal package!
If I'm reading this right, it lets you get around the fact that constructors aren't exported if you have a `Generic` instance; isn't that bad in certain ways because it allows you to break invariants on the data (e.g. `Set`)?
They were not quite around, and they are also a replacement for monads. They can be used for similar purposes as the state monad, though. 
Why would you want to do this except to break abstraction? Generics are designed to write generic algorithms, not just to rebuild data constructors :/
The connection between OO and HOF is actually pretty cool. I don't think there's a big difference between them if you know some tricks in either setting :) There is an isomorphism between objects and self-recursive records. Dobi does a great job describing this from [a C++ to Haskell/Elm perspective](https://github.com/Dobiasd/articles/blob/master/from_oop_to_fp_-_inheritance_and_the_expression_problem.md). The rough idea is that you can think of an object like this: data SpaceShip = SpaceShip { render : SpaceShip -&gt; Element , update : Step -&gt; SpaceShip -&gt; SpaceShip , toString : SpaceShip -&gt; String } In Elm, you have [structural typing on records](http://elm-lang.org/learn/Records.elm) so any SpaceShip can provide different implementations of any of these functions and possibly hold *additional* data and functions as well. The same kind of tricks can be done in OO languages to emulate HOF. So I don't think there is an extremely solid case regarding expressiveness here.
Thank you!
Thank you!
Feed, troll, etc.
Yeah, that certainly works for getting at the pattern of this particular way of deriving new types. But the question is more broad than just that one way. Both applicatives and monads retain function application, but in different ways, for instance.
Sure. But as in the other comment, it's not just about monoidal structures.
Waiting for haskell in duolingo! On the serious note, the need to use flashcards to memorize library operators suggests the presence of something deeply wrong 
Laziness is an effect! Put it in the type system!
Linearity is an effect! Put it in the type system!
I understand this relationship, though I found the comment about one being more powerful than the other very peculiar (when at least in this aspect, they are equally powerful). 
I thought he could have been serious, but then I went into his history and found that he used a function from a C-imported function to prove that haskell was actually just like C. That's like 6/10 trolling for creativity.
It's part of [TMR23](http://themonadreader.wordpress.com/2014/04/23/issue-23/)
Sometimes you can have particular cases that are sufficient to give you every general case. But linear types and monads aren't an example.
What kinds of situations would you want to do this? I can see the value of calling lua from C++/C++ from lua, since C++ is a very low-level language, and doesn't lend itself as well to DSLs or scripting. In Haskell though, there are so many really cool ways to write DSLs that I don't understand why you would want to use lua. This is neat, but I guess I don't understand the motivation.
I see. Then again that appears to be optional. Look at aeson and lens-aeson, you can split them out if you want to. 
I found it a good explanation of the concept and plumbing of Generics, even if I wouldn't necessarily use it in this way.
&gt; It should be noted though that GHC even with no extensions enabled isn't completely standards compliant, at least so I hear. Indeed, see http://www.haskell.org/ghc/docs/latest/html/users_guide/bugs-and-infelicities.html#vs-Haskell-defn .
Why would it be a partial mapping? Can you give an example?
Yes please.
Are we *sure* that we cannot fit OOP inside FP? Maybe I'm deluded by the "mostly functional" side of life, but I don't see how the two are mutually exclusive.
Absolutely - the moment you add a `Generic` instance, theoretically all bets are off about making assumptions that your invariants are preserved.
Depends on what you mean by OOP, it's not a clear cut term. But we can certainly model simple class like structures in Haskell much the same way you model them in C which doesn't have them either, by passing around structures of functions. It's just that usually there are more natural/better ways of programming in Haskell
I am not sure about "nicer", but VCLua and IUP ([see the lua-users wiki](http://lua-users.org/wiki/GraphicalUserInterfaceToolkits)) might be interesting because they are small and cross-platform. (In the case of IUP, though, I suppose it would be easier to writing bindings to C rather than to Lua.) 
Or the ASCII equivalent of 〈angle brackets〉, anyway.
Ah, I'm getting better at reading and understanding this stuff! He's missing a `return` at the end of his definition of `sequence`. It should end with `return (a:as)))`.
Do you have a link/explanation about the partially applied functions thing?
Let me invent terms here: * There are 'button features', features that allow you to do something; button features grow the set of expressible programs * and there 'hand rail features', features that disallow you to do something -- they shrink the set of expressible programs, but more so the set of expressible 'bad' programs. Subtype-polymorphism is a button feature (it allows you to have a different implementation act 'as-if'), while linear types are a handrail feature (they constrain what you do with them, but they allow you enforce protocols of mutable state, compile-time-GC, ..) I'd call Haskell hand rail-ey and I'd call Javascript button-ey.
The wiki is perhaps not as clear as it could be. Monads are *exactly* about ordering computations where the one computation might depend on the result of previous computations. I think the wiki is trying to say that a monad does not *have to* take advantage for this possibility of dependency.
Yes, he is indeed.
I agree with the second half of your comment. Laziness should be in the types, but I don't think it should exist as an effect. Rather, I would like the distinction between laziness and strictness to be encoded as a distinction between greatest fixed points and least fixed points. In other words, I do want to encode it in the types, but not as a monad or an effect system.
Actually, I'm the author and I use it with cabal sandboxes. The tool store common tags file in your ~/.cabal/packags folder, and use Cabal as a library to resolve dependencies. So in fact he does not interact with the sandbox in any ways, as the tool work with sources only... that's perfectly fine :-) If you want to install the tool thru a sandbox (because you don't want to pollutate your global cabal). You can install it in a sandbox and then copy the executable to ~/.cabal/bin manually.
The signature of (&gt;&gt;=) which you interpret as sequencing is also used for substitution: specific occurrences of constructors (often referred to as "variables") are substituted with some other fragments. The result is then a new data structure (say AST) with a possibly changed type parameter.
You can compile load code from a quasiquote with libffi/the unix package. There is an example at the bottom of http://lpaste.net/50837
Minor nit. The canonical Haskell solution that we should be comparing to is some variant of fizzbuzz :: Int -&gt; String fizzbuzz n = show n `fromMaybe` ((guard (n `mod` 3 == 0) &gt;&gt; return "Fizz") &lt;&gt; (guard (n `mod` 5 == 0) &gt;&gt; return "Buzz")) The relevant point being is that we don't want to treat the empty string as a sentinel value because sentinel values are evil. I'm trying to determine if Maciej's solution is fundamentally better or if it is just a hand-transformed CPS version of the above (i.e. the algorithm is the same as above, but compiled in a different way.)
But you need a `Generic` instance around, which is normally not the case in polymorphic code. So you are mostly safe.
Even after 2 years living in London I'm still doing it wrong! Thanks for pointing out this, I'm trying to get better at it but it's damn hard when having spent the rest of my life using 'he' in my native language. Let me know if you have an issue with *it* ;-) I got some positive feedback so hopefully it should install and run smoothly.
Commutative monads are not about sequencing because do x &lt;- foo a y &lt;- bar a baz x y and do y &lt;- bar a x &lt;- foo a baz x y yield the same result. For commutative monads, the order of `bar` and `foo` is (denotationally) irrelevant, just like with a set of let bound variables. But commutative monads, such as `Maybe`, are still an important useful class monads.
How about this do y &lt;- bar a z &lt;- baz x y x &lt;- foo a return z Does it yield the same result? I would say not.
The important line to read in this paragraph is: &gt; So while it is correct to say that monads can be used to order operations, it would be wrong to say that monads are a mechanism for ordering operations. Effectively there are commutative and non-commutative monads, non-commutative monads model computations which are order dependent and commutative are order independent. IO is non-commutative and (Maybe, Reader) are commutative.
That's more true, yes. You need a non-linear modality in linear logic. I was just being silly. :p I wonder tho if there are type theories where linearity is the indeed a modality. Hmm.
laziness is an evaluation strategy, strictness is a semantic property. you can be both lazy and strict. lazy contrasts with eager lax contrasts with strict
Exactly! You just stated the essential merit of double blind reviewing :)
I don't know. does do rec y &lt;- bar a z &lt;- baz x y x &lt;- foo a return z work with `-XRecursiveDo` ? Prelude&gt; do { rec { z &lt;- return (x+1); x &lt;- Just 3}; return z} Just 4 Prelude&gt; do { rec { z &lt;- Nothing; x &lt;- Just 3}; return z} Nothing Prelude&gt; do { rec { z &lt;- return (x+1); x &lt;- Nothing}; return z} Nothing 
Not handy. I'd work up an example, but I don't have Clean installed any more, and Dana Harrington's dissertation isn't online anywhere I can find it any more either.
Precisely, it does not work. So the Maybe monad is not order independent after all. And do foo bar would be different from do bar foo Why do all these people say that there are "commutative" or "order independent" monads when they are all order-dependent? It's confusing. I think that defining a monad as *a semantics for sequencing computations* would greatly simplify describing Haskell to newcomers. Sure, a Maybe monad sequences computations differently from a Cont monad, but the key thing in all of them is sequencing. Monads arose in Haskell as a way to impose order on an otherwise call-by-need language - it's as simple as that.
In a commutative monad, the transformation from the serial `x &lt;- m; y &lt;- n` to the concurrent `(x, y) &lt;- liftA2 (,) m n` is always valid *when `x` does not appear free in `n`*, that is, when the ability of monads to express *context dependence* is not taken advantage of. Context dependence implies sequencing only because you can’t get very far evaluating the `a -&gt; m b` without first evaluating the `m a`.
~~Indeed. I considered mentioning `MonadFix`, but decided that was kinda beside the point.~~
&gt; when the ability of monads to express context dependence is not taken advantage of But then you don't really need a monad, can get by with an applicative. So thanks to all the commenters, I can conclude that my initial intuition was right and monads are about sequencing - in the same way, as Tekmo said, as function composition is. Thanks to everyone.
They are, but the sequencing is about data dependencies, not evaluation order. It's a very important distinction that a lot of people are not good at making.
[But](http://www.haskell.org/ghc/docs/6.12.3/html/users_guide/syntax-extns.html) &gt;Then each resulting rec is desugared, using a call to Control.Monad.Fix.mfix &gt;The mfix function is defined in the MonadFix class, in Control.Monad.Fix And MonadFix is a different beast than Monad.
Applicatives also enable ordering/sequencing of computations; e.g. `getLine *&gt; putStrLn "hello"`. They don't allow the second action to depend on the first one.
I found a copy of the dissertation in the Internet Archive Wayback Machine: http://web.archive.org/web/20060614184053/http://pages.cpsc.ucalgary.ca/~danaha/uniqueness-types.ps
Yes. Thats why I'm asking why 24 y.o. language does not have its own IDE?
Your example (with `a` and `(a,r)`) is actually a perfect example of why your approach does not work. How exactly would `x` be defined? `x a = (a,r)`? You don't have to know anything about type families to see what is wrong here: where did `r` come from? It is not in scope. You could have a type function `x a = (a,Int)`, say, or some other specific choice for `r`, but you cannot have it magically guess the right `r` to use in general. (Note that `forall r. (a,r)` is *not* what we want; it describes tuples whose second component can have any type you choose). This is why it has to go the other way, because when going from the type of the whole to the type of a part, information is (in general) lost. You cannot have a type function going in the other direction because it would have to make up information. As for partiality, indeed, `x (a,r) = a` is partial, but this is not as big of a deal on the type level; indeed, it's commonplace. A type function given some type argument on which it is not defined simply gets "stuck", i.e. does not reduce, which can never cause things to go wrong at runtime.
Awesome! I've cloned this and started playing around with it. Thanks so much!
Data dependency is different from sequencing. If one computation depends explicitly on the result of another, the computations can't be rearranged even outside a monad. consider: foo :: String -&gt; String foo a = a ++ "foo" bar :: String -&gt; String bar a = a ++ "bar" then (foo . bar) "" == "barfoo" (bar . foo) "" == "foobar" so `foo . bar` isn't the same as `bar . foo` and they don't commute. However, consider the following in the Maybe monad: foo :: a -&gt; Maybe a foo a = if even a then a else Nothing bar :: a -&gt; Maybe a bar a = if a &lt; 0 then a else Nothing do r1 &lt;- foo a r2 &lt;- bar a return (r1, r2) 'foo' and 'bar' don't depend on each other's results so the two computations can be executed in an arbitrary order without changing the result of the overall computation. This is because the Maybe monad commutes, and non-dependent computations (assuming they converge) may be rearranged freely by the compiler. now suppose you're working in the IO Monad instead: foo :: a -&gt; IO () foo a = putStrLn "foo" bar :: a -&gt; IO () bar a = putStrLn "bar" then, if you try: do foo a bar a the compiler can't rearrange the two computations *even though there aren't any explicit dependencies between them* because it would output a different message. Functions in the IO Monad don't commute because the IO Monad is specifically intended to sequence real world actions. In an arbitrary monad however (e.g. Reader, Maybe, Either), this need not be true.
If there are files missing in a package, you most likely deleted the installed package files but didn't remove the package from the database (default locations for package db is in `~/.ghc` , packages are installed in `~/.cabal`, check the package db locations with `ghc-pkg list` since the location depends on how you installed GHC, Haskell Platform uses ~/Library/Haskell` ). Check that all your packages are in working order by running `ghc-pkg check` before starting. Also check that the `cabal` and `ghc` commands on your PATH are really the correct ones. Perhaps you need to run `hash -r` after installing. As for the GHCJS program itself, you might want to delete the `~/.ghcjs/` dir if something has gone wrong wrong there, then just run `ghcjs-boot --init`. Come to `#ghcjs` on freenode if you still get stuck. Due to travelling I'm not online as often as normal, but I try to check the channel a few times a day. Also I'm currently in the process of fixing the last major bugs before releasing GHCJS on hackage, and changing how the `ghcjs-boot` program works with release versions (so that normal release installs don't depend on tools like git and autoconf anymore) 
I deleted all those... I think something really great would be a full list of commands that did everything. There is probably something minor I'm getting wrong on the process that is spoiling everything. I'll try again tonight, thank you!
Sorry; I misread and though you were replying to me rather than to DreamyDreugh. Please disregard my other reply.
Erik doesn't change his mind. He just tried to shock his audience. :)
A fairly accurate description. 
Which us why Haskell is a non-strict language (rather than lazy). Arvind insisted on the wording. 
Well, as g__ has mentioned above, with just an applicative, your effects cannot depend on previous results at all. With a commutative monad, it's possible to have these dependencies and still switch independent actions around safely (when doing so, one should probably stick to do-notation and enable -f-warn-name-shadowing :) EDIT: grammar
More specifically, when you define `SomeDataType`, include this: instance PathPiece SomeDataType where ... Or if your type is a `newtype` you can often turn on `GeneralizedNewtypeDeriving` and just do this: newtype SomeDataType ... ... deriving PathPiece That works for many common kinds of types, and automatically allows you to represent values of your type in the route the way you would expect. Or, if your type is represented by multiple piece of the route, such as for a file path, use `PathMultiPiece` instead of `PathPiece`.
Just curious here. When I did my "let's try to write fizzbuzz" thing in Haskell, I came up with this: fizzbuzz :: (Integral a, Show a) =&gt; a -&gt; IO () fizzbuzz x | x `mod` 15 == 0 = putStrLn "FizzBuzz" | x `mod` 5 == 0 = putStrLn "Buzz" | x `mod` 3 == 0 = putStrLn "Fizz" | otherwise = putStrLn $ show x Would you be able to tell me why you went with your version instead of something like mine?
If you are writing an interpreter, and you use the [bound](http://hackage.haskell.org/package/bound) library, for instance, then you will end up with a type of expressions: data Expr v = ... where (if you do everything right) `Expr` is a monad. The purpose of `(&gt;&gt;=)` in this monad is to substitute in for variables in the expression tree, replacing them with `Expr`s. This has nothing to do with 'sequencing' or 'ordering' actions. It is just about munging variables in an expression tree. Of course, when you use this monad, I think it is rare to use `do` notation, because the result of just listing expression trees in it isn't typically anything you'd actually want. So I'd say that by only thinking about monads in terms of `do` notation, you set yourself up for failing to understand how they might not be about sequencing. There is a sense in which you can see all monads this way, including the ones commonly used in Haskell. And certainly this is closer to their use in actual category theory.
Also, the [Try Purescript](http://try.purescript.org/) website has been updated to the latest version of `psc`.
Man. Haskell is 24 years old. Judging by the way things go in its backyard you will wait another 10 to have something useful. Pick a language that is good enough here and now and don't look back. Java + LibGDX should work with all these platforms. If you want something haskelly I would take a look at Frege (it can be compiled to Java 6 which is compatible with Android).
I'd expect that more satisfying answers would come out of a crisper question. From similar conversations, I gather that a lot of confusion stems from saying "monads are" instead of "`Monad` is". Ditto for asking questions about "monads" vs "`Monad`". In each case, I suggest considering: are you talking/asking about the abstraction (`Monad`) or the instances (monads). If the latter, do you mean all, some, many, most, etc? Secondly, what do you mean by being *about* ordering/sequencing. Also, keep in mind that `m a -&gt; (a -&gt; m b) -&gt; m b` (along with `a -&gt; m a`) is only one view of `Monad`. There are also `(a -&gt; m b) -&gt; (m a -&gt; m b)` and `m (m a) -&gt; m a`, both of which I prefer (especially the latter). Personally, I see the notion that `Monad` is "about sequencing" as an unfortunate by-product of packaging them in a way to appeal to an imperative mindset. 
Surely "Variant" would be a better name? * Covariant - varies with its argument * Contravariant - varies against its argument * Variant - varies somehow * Invariant/Nonvariant - doesn't vary at all
It's interesting because people used to say that you had to have a PhD to do functional programming not that long ago (last time I heard it was from a grad student around the year 2000). 
Because those few who want one haven't ponied up the huge amount of time it would take to make one. Also, it is not typical for a successful language to be dominated by a native IDE. This is a weird suggestion you're making, really. It's also really weird to bring this up right after complaining about reinventing the wheel.
`(&lt;&gt;)` is a synonym for `mappend`. https://hackage.haskell.org/package/base-4.5.1.0/docs/Data-Monoid.html
does the Haskell spec also specify a particular evaluation strategy?
It is a synonym for `mappend` declared in [Data.Monoid](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-Monoid.html#v:-60--62-).
Have you tried [Hoogle](http://www.haskell.org/hoogle/)? It does not provide the correct result for `(&lt;&gt;)`; but for many operators it is a good way to find documentation.
FizzBuzz without ifs: fizzbuzz n = take n $ zipWith (\i fb -&gt; head $ filter (not . null) [fb, show i]) [1..] $ zipWith (++) (cycle ["","","Fiz"]) (cycle ["","","","","Buzz"]) If you do not like one long line: fizzbuzz n = take n $ zipWith (\i fb -&gt; head $ filter (not . null) [fb, show i]) [1..] fizzBuzzStream where fizzes = cycle ["","","Fiz"] buzzes = cycle ["","","","","Buzz"] fizzBuzzStream = zipWith (++) fizzes buzzes 
For a code illustration of point 3, consider: fizzbuzz :: Int -&gt; String fizzbuzz n = show n `fromMaybe` ((guard (n `mod` 3 == 0) &gt;&gt; return "Fizz") &lt;&gt; (guard (n `mod` 5 == 0) &gt;&gt; return "Buzz") &lt;&gt; (guard (n `mod` 7 == 0) &gt;&gt; return "Wuzz")) versus: fizzbuzz' :: (Integral a, Show a) =&gt; a -&gt; IO () fizzbuzz' x | x `mod` 105 == 0 = putStrLn "FizzBuzzWuzz" | x `mod` 35 == 0 = putStrLn "BuzzWuzz" | x `mod` 21 == 0 = putStrLn "FizzWuzz" | x `mod` 15 == 0 = putStrLn "FizzBuzz" | x `mod` 7 == 0 = putStrLn "Wuzz" | x `mod` 5 == 0 = putStrLn "Buzz" | x `mod` 3 == 0 = putStrLn "Fizz" | otherwise = putStrLn $ show x 
I wonder why hoogle doesn't find it?
Ok yeah. I'm so locked up in the idea that developers do all the development that I didn't even think about user-supplied scripts. Neat-o.
Huh. How are you imagining encoding it in the types that doesn't automatically give rise to a monad?
Why does laziness make it harder to make an IDE?
Not linearity, but uniqueness. Uniqueness is a 'forgettable' property, and can be modeled as a modality. When you work out the lattice of how the different substructural logics fit together linearity sits at one extreme of the lattice, and uniqueness at the other. (well, until you start including environment ordering) 
I'm inclined to forgive Erik for this because he spends a great deal of his (very) valuable time trying to educate others and improve the ecosystems that he works with. When you make such a deep and personal commitment to helping others it's easy to lose sight of things and get too passionate about it and accidentally become inflammatory.
I am brand new to haskell and wanted to play with your solution here and this is what I got when I loaded it into ghci... any thoughts on what I did wrong? λ&gt; :l fizzbuzz.hs [1 of 1] Compiling Main ( fizzbuzz.hs, interpreted ) fizzbuzz.hs:2:21: Not in scope: `fromMaybe' fizzbuzz.hs:3:16: Not in scope: `guard' fizzbuzz.hs:3:55: Not in scope: `&lt;&gt;' Perhaps you meant one of these: `&gt;&gt;' (imported from Prelude), `&lt;' (imported from Prelude), `&lt;=' (imported from Prelude) fizzbuzz.hs:4:16: Not in scope: `guard' Failed, modules loaded: none.
Full disclosure: `--whatever-other-options` is usually `--with-gmp-libraries=/usr/local/lib` and `--with-gmp-includes=/usr/local/include` to make sure I get my brewed `libgmp`, but alter as necessary.
Would you please post that "10 line example"?
This seems to directly contradict your previous request that the content be judged by it's intrinsic merit rather than the credibility of the author.
Hayoo finds it.
That's not a statement about his content, but rather his tone. His tone does not impact how I judge his content.
 -- Least fixed points (informally known as "data") data LFix f = LFix { unLFix :: forall x . (f x -&gt; x) -&gt; x } -- Greatest fixed points (informally known as "codata") data GFix f = forall x . GFix { seed :: x, unfold :: x -&gt; f x } -- The base functor of a list data ListF a x = Cons a x | Nil -- A finite list type List a = LFix (ListF a) -- A potentially infinite list type List' a = GFix (ListF a) In other words, if you want to define an inductive type you use least fixed points. If you want to define a coinductive type you use greatest fixed points. These are two separate types (i.e. you can't pass one where the other is expected). Also, I may or may not be correct about this next point, but I believe that because the least and greatest fixed point representations are not themselves recursive, you can safely strongly normalize them, which has other nice properties. This is stolen from Wadler's [Recursive types for free](http://homepages.inf.ed.ac.uk/wadler/papers/free-rectypes/free-rectypes.txt).
I'm really unhappy with the whole FP/OO supposed duality. I think there's initial/final duality, but that's a whole hell of a lot more specific.
1. To avoid copying large structures, you need to use clever datastructures which reuse large parts of the old structure. For example, to change the first element of a list `x:xs`, you return a new list `x':xs` consisting of the new head `x'` and a pointer to the old tail `xs`, which doesn't need to be copied. There are other immutable datastructures optimized for other kinds of single-element modifications. The standard text on the subject is Okasaki's "Purely Functional Data Structures". 2. I don't know, but it's probably much more than a C struct with four doubles would take. If you need to optimize space usage, I have seen people recommend unboxed primitives and packed arrays. And if you need more control on the exact memory layout, you can use the [binary](http://hackage.haskell.org/package/binary) package to serialize your data to raw bytes.
Generally, if you "modify" an immutable data structure, what's really happening is that you are building a new one that shares as much of the original as possible. For example, if I have an infinite list of values: ones :: [Int] ones = 1:ones ... and I write: x :: [Int] x = case ones of _:ones' -&gt; 2:ones' [] -&gt; [] That will reuse the tail of `ones` rather than making a new copy. This is called "sharing", and it's safe because the data structures are immutable. For point #2, unless you explicitly unpack fields of a constructor they will be represented by pointers, so there will be one pointer of overhead per field. To unpack the fields you would write: data Bar = Bar {-# UNPACK #-} Double {-# UNPACK #-} Double {-# UNPACK #-} Double {-# UNPACK #-} Double That will then be represented by four contiguous `Double`s in memory without any indirection. I think you would love Johan Tibell's slides on high-performance Haskell, which cover these topics and more. You can find them [here](http://blog.johantibell.com/2010/09/slides-from-my-high-performance-haskell.html).
[You could ask him]( https://twitter.com/headinthebox). Also, &gt; Arguing that purity is not a good idea in some avenues, etc. I'm curious how you came to this conclusion about the article. The central point is that "partial-purity" is really impurity. I didn't read it as a value judgement on purity vs impurity. I might make the analogy with a food label that says "fat free!" but on close inspection, it says "(99%) fat free (by ingredient)". Package contains a stick of butter, colored with 99 different food dies. Is it mostly nearly fat free?
Thanks.
Ahem. PDF.
I know but sometimes it's extremely useful to get a different perspective and I've found that reddit users tend to provide better answers than SO users.
I think so. The reason I'm not sure is that I don't know whether or not "total" includes coinductive types. If it does, then yes, that's a total language.
I know you didn't write this solution yourself, but guard (n `mod` 3 == 0) &gt;&gt; return "Fizz" is a bit silly when you can just do "Fizz" &lt;$ guard (n `mod` 3 == 0) It's like using x &gt;&gt;= return . f instead of fmap f x ...it's a bit of an abuse/hack on the Monad instance because you aren't even using Maybe as a monad at all.
Thank you --- I suspect as much for immutable structures but I was concerned about data structure representation. One extra field (a pointer) isn't too bad.
&gt; Type class instances must now be named Does this means that you can select which type class instance you will use for a given type?
That language is pretty much what Harper calls M in PFPL (sec 14.2)—so long as the signature functors are strictly positive. There's a theorem of totality at thm 14.3 (apparently proven in a later chapter, but I haven't gotten so deep yet). If you allow non-polynomial functors then you lose termination as you can embed a Y.
I did write the solution myself. You are entirely correct in your assessment.
I'm having a lot of trouble getting your code to compile, and I don't see how staying in the IO monad would avoid space leaks. Instead, my first instinct is to enable [bang patterns](http://www.haskell.org/ghc/docs/latest/html/users_guide/bang-patterns.html) with the GHC extension (i.e., put the line `{-# LANGUAGE BangPatterns #-}` at the top of your file). This lets you make certain things use eager evaluation instead of lazy evaluation, which should cut down on space leaks.
If you want to connect to a remote state on amazon there shouldn't be a problem, just call openRemoteStateTLS with the port, ip and correct secret key. I have used the tls module and I would recommend it.
There, it should compile now if compareIO is defined. I guess I just don't understand how the IO monad interacts with laziness. If I say a &lt;- someIOFunc and I don't actually use a, the function someIOFunc still has to run and have its side effects before the program can continue, right? I'm aware of things like BangPatterns, but I want a better understanding of when I need it and when I don't.
Why do class instances have to be named?
&gt; If I say `a &lt;- someIOFunc` and I don't actually use a, the function someIOFunc still has to run and have its side effects before the program can continue, right? Nope! You can define whatever stuff you want in `IO`, and the only pieces that will get run are the ones that need to be run to get the `main` value of your program. Take a look at [my attempt to explain monads and `IO`](http://www.cs.hmc.edu/~adavidso/monads.pdf) from when I first learned Haskell (see section 4 in particular), and tell me if it makes any sense (it might not; I had only just wrapped my head around the idea).
&gt; For point #2, unless you explicitly unpack fields of a constructor they will be represented by pointers, so there will be one pointer of overhead per field. It's really two pointers of overhead per field, because each `Double` that is pointed to by a `Bar` has a header word pointing to the info table for `Double`'s constructor `D#` (in addition to the actual double value).
Interesting, thanks for the link. I just skimmed the paper, but it seems that they only encode affine types, as they allow weakening of any variable. Enforcing relevance might be difficult, as you can just return things from the state monad without using any actions. But I need to have a closer look. (In any case, it looks like a nice approach).
Unpacking can only be used on strict fields, so those need to include the `!` for that: {-# UNPACK #-} !Double 
Right now the names are just there for JS interop, but actually since overlaps are no longer allowed, there's little to be gained from specifying the instance by name.
They are named to allow them to be referenced from the JS side without having to know the implementation detail of how names used to be assigned. Also, type class instances can be imported by name using the FFI, which comes in handy when writing out type definition files for separate compilation.
What do you mean by overlaps no longer allowed? In the mailing list of bitC, the main creator of the language raised a concern with type classes: they don't let you to define an instance differently, for using in a small part of the code (without affecting the rest). Sometimes there are more than one valid instance. The Haskell way to deal with this is define a newtype if you want to write your own instances - but if you named the instance perhaps you could avoid this. Also: why not make the name optional in the grammar? if it's not provided, the compiler will mangle its own name.
Overlaps are not allowed in the sense that _at the call site_ to a constrained function, exactly one unifying instance must exist for the type class. Note this is different to Haskell, in which uniqueness must be guaranteed at the time the instances are defined. We used to allow overlaps, and relied on ordered instances to resolve the case of multiple matching instances, but it lead to confusion when the instance graph wasn't trivial. With the small amount of experience I (and others) have had naming instances by hand, I would say the extra effort has paid for itself already with the ease of debugging it provides. It's a nice form of additional documentation.
&gt; Overlaps are not allowed in the sense that at the call site to a constrained function, exactly one unifying instance must exist for the type class. Note this is different to Haskell, in which uniqueness must be guaranteed at the time the instances are defined. I don't really understand this. Is the current state of Purescript more general than Haskell (accepts code that Haskell would reject otherwise)? Is there some doc explaining those issues? &gt; I would say the extra effort has paid for itself already with the ease of debugging it provides How is it a form of debugging? You mean when debugging the JS code? From the example that was given, I would guess that instance names itself are trivial and uninformative (eg. showNumber for.. the Number instance of Show. Perhaps the compiler could come up with this name on his own) and they could be generated automatically without much loss. The issue would be some mangling, but the editor could inform the user what's the mangled name for a given instance when it's highlighted. Actually: do Purescript generate [source maps](http://www.html5rocks.com/en/tutorials/developertools/sourcemaps)? It should make debugging easier.
&gt; I don't really understand this. Is the current state of Purescript more general than Haskell (accepts code that Haskell would reject otherwise)? Is there some doc explaining those issues? There isn't a document comparing the two yet, but the short answer is that neither is a subset of the other. The [compiler docs](http://docs.purescript.org) try to explain the type system features of PS independently. &gt; How is it a form of debugging? You mean when debugging the JS code? Yes, exactly. It makes it easy to see at a glance which type class is being used. &gt; From the example that was given, I would guess that instance names itself are trivial and uninformative (eg. showNumber for.. True for these simple examples. When dealing with instances with super-instances, the names can be more informative. Best example I can think of off the top of my head: instance bivariateDistribution :: (Arbitrary a, Arbitrary b) =&gt; Arbitrary (Tuple a b) &gt; Actually: do Purescript generate source maps? It should make debugging easier. Not yet, but it's [on the roadmap](https://github.com/purescript/purescript/issues/121).
Strong updates would be "possible" w/o linear types, but they'd be ~~stupidly~~ dangerous. C has strong updates, Assembler has strong updates. Edit: strike out a word
Looks like you gave me 'half a link'. Thanks :)
Thanks!
Thanks. I see, instance names can be useful for complex types. For simpler ones (which should be the majority for most code) it may be a nuisance. Also you use =&gt; but in the docs there is &lt;= for superclass. Are they the same, just with the arguments reversed? (that is, is instance x :: A =&gt; B the same as instance x :: B &lt;= A?)
`&lt;=` was chosen by the developers to emphasize the fact that superclass implication flows right-to-left. It is a little unfortunate that the syntax differs from Haskell. I would say that writing instance names is a very minor nuisance, but would probably mostly be a problem for library implementors, not usually for library users, correct?
There is a way to get the first few elements early using `pipes`: import Pipes import qualified Pipes.Prelude as Pipes -- Taken from: http://en.literateprograms.org/Merge_sort_%28Haskell%29 mergeSort :: Monad m =&gt; (a -&gt; a -&gt; m Ordering) -&gt; [a] -&gt; Producer a m () mergeSort _ [] = return () mergeSort _ [x] = yield x mergeSort compareM xs = merge compareM (mergeSort compareM xsL) (mergeSort compareM xsR) where (xsR, xsL) = split xs split :: [a] -&gt; ([a],[a]) split xs = go xs xs where go (x:xs) (_:_:zs) = (x:us,vs) where (us,vs)=go xs zs go xs _ = ([],xs) merge :: Monad m =&gt; (a -&gt; a -&gt; m Ordering) -&gt; Producer a m () -&gt; Producer a m () -&gt; Producer a m () merge compareM pL pR = do x &lt;- lift $ next pL case x of Left () -&gt; pR Right (aL, pL') -&gt; do y &lt;- lift $ next pR case y of Left () -&gt; do yield aL pL' Right (aR, pR') -&gt; do comp &lt;- lift $ compareM aL aR if (comp == GT) then do yield aR merge compareM (yield aL &gt;&gt; pL') pR' else do yield aL merge compareM pL' (yield aR &gt;&gt; pR') compareIO :: Int -&gt; Int -&gt; IO Ordering compareIO x y = do print (x, y) readLn main = runEffect $ mergeSort compareIO [1, 0, 5, 2] &gt;-&gt; Pipes.take 2 &gt;-&gt; Pipes.print Here's an example run: &gt;&gt;&gt; main (2,5) LT&lt;Enter&gt; (0,1) LT&lt;Enter&gt; (2,0) GT&lt;Enter&gt; 0 (2,1) GT&lt;Enter&gt; 1 That prints out the first two sorted elements and stops sorting once it reaches that point.
Tried that (i.e. fizzbuzz :: Int -&gt; String fizzbuzz n = show n `fromMaybe` fz &lt;&gt; bz where fz :: String fz = ("Fizz" &lt;$ guard (n `mod` 3 == 0) ) bz :: String bz = ("Buzz" &lt;$ guard (n `mod` 5 == 0) ) but I get Couldn't match type `[Char]' with `Maybe String' Expected type: Maybe String Actual type: String In the second argument of `fromMaybe', namely `fz' In the first argument of `(&lt;&gt;)', namely `show n `fromMaybe` fz' In the expression: show n `fromMaybe` fz &lt;&gt; bz Couldn't match expected type `Char' with actual type `[Char]' In the first argument of `(&lt;$)', namely `"Fizz"' In the expression: ("Fizz" &lt;$ guard (n `mod` 3 == 0)) In an equation for `fz': fz = ("Fizz" &lt;$ guard (n `mod` 3 == 0)) Couldn't match expected type `Char' with actual type `[Char]' In the first argument of `(&lt;$)', namely `"Buzz"' In the expression: ("Buzz" &lt;$ guard (n `mod` 5 == 0)) In an equation for `bz': bz = ("Buzz" &lt;$ guard (n `mod` 5 == 0)) 
I disagree. What is computational order if not the temporal order of the computations? As I've already said (in greater length) what is sequenced in a monad is part of its abstraction. There's the potential to have data dependencies only in one way, but that's just a restriction on the set of possible abstractions a monad can represent. Also, obviously there's sequencing in terms of how pieces are assembled, but that's the implementation of the abstraction - how bind turns a syntactic sequence into an expression that expresses whatever your monad expresses. 
Why should one use this instead of ghcjs?
D'oh. Affineness not linearity. Right. I'm not sure I'd want to say that it's opposite linearity tho.
From http://purescript.readthedocs.org/en/latest/intro.html#related-projects: &gt; Projects such as Haste, Fay and GHCJS aim to use some combination of the GHC compiler itself and/or its intermediate representation, Core, to perform some of the tasks involved in compilation such as parsing and type checking. This usually gives the advantage that tools and libraries can be shared with Haskell, but often at the cost of the size of the generated Javascript. This is the main practical difference between PureScript and these projects.
It's the structural order of computations. Consider this simple example: print $ (1 + 2) * (3 + 4) The computational order of the problems is that the additions are both ordered before the multiplication, because the outputs of addition are the inputs to multiplication. But the computation can occur with either addition first, because the computational order is partially ordered not totally ordered. But consider now an even more illustrative example owing to laziness. let xs = [1+2, 3-4, 5*6] in print $ xs !! 1 The computational order here isn't merely partial, but it's incomplete: only one of the elements of the list is ever computed at all. The other's aren't even computed, so can't come first. But let's make it even clearer, since both of these still have connections to temporal ordering. let ys = [1+2, 3-4, 5*6] in print $ map (/7) ys Here, the map occurs first, transforming the list initially into the list `[(1+2)/7, (3-4)/7, (5*6)/7]`, and only then is the math performed. Yet the computational order of `ys` here is the same as in `xs` before, because the inputs to the list constructors are still the outputs of the mathematical operations. The dependency between computations and subcomputations is always the same regardless of the actual temporal ordering. Monads give you a way to specify certain kinds of sequential *dependencies*, which might be computed in any *temporal* order.
&gt; It's the structural order of computations. I still disagree. I don't doubt that you understand monads, but I still think you're expressing it in a misleading way by talking about computations. Monads are a way of specifying expressions, which only indirectly relate to computations. "Structural" is correct, but it's a structural order that expresses how a part of the expression is assembled. As that structure is expressed using syntax, it's a syntactic ordering. Syntax is superficial. For example, the expressions `a &gt;&gt;= b` and `b =&lt;&lt; a` are equivalent - ordering is significant (neither `&gt;&gt;=` nor `=&lt;&lt;` is commutative), and the syntactic ordering is opposite, but that's just syntax. Two different expressions with two different syntactic orderings giving rise to the exact same computation. `(1 + 2) * (3 + 4)` isn't a set of computations, it's syntax - an expression that happens to describe a set of computations and a set of possible orderings of those computations. The set of possible orderings is further constrained by the semantics of the language (lazy or eager, which are names of both exact expression orders and of semantics that arise from those orders), the design of the optimizer (e.g. strictness analysis), and the context in which the expression is used. In the case of a monad, the ordering of computations is constrained by the fact that the bind operator only passes results in one direction. This constraint arises from the existing constraints on evaluation order, as a consequence of the way the expression is assembled. IOW it's a level of indirection between the syntax and the computations that's implemented by the way the expression is assembled. 
Not quite - it only kicks in for 'small' types because you don't necessarily want to `UNPACK` everything (and add pressure to the compiler). Specifically, 7.8 will automatically `UNPACK` strict fields whose types are less-than-or-equal-to the word size (but also special cased for `Double` and `Float` as well as they always have hardware registers, too).
Wow. I find it very nice that PureScript -&gt; JS code looks as minimal as possible. Definitely will look forward to play with PureScript in future.
That makes sense. I thought I'd remembered that `Double`s count as "small" to GHC in this case, it's good to know for certain. &gt; (but also special cased for Double and Float as well as they always have hardware registers, too) I assume from this that they're special-cased even on 32-bit architectures? Your phrasing seems to imply that all (or at least, most) architectures support `Double`s at the hardware level, including 32-bit ones. Unfortunately I don't know a lot about the specifics of modern architectures.
Now I'm paranoid about what happens when I need to store more data than fits in RAM. But I'm going to pursue this avenue, thanks!
That's because fz and bz have type Maybe String, not String :) Try removing the type signatures.
It sounds to me like you have fundamental misunderstandings about monads and computation, and I don't think that there is anything we can do at this point to overcome this. Suffice it to say: you should take into consideration that multiple people who you agree are knowledgable are saying you're wrong, and thus you might want to actually examine that possibility.
Sorry about that ; I usually remember to add a tag to the title. :/
Pretty much everything supports floats and doubles, even the terrible x87 FPU stack. Realistically just about everyone has *some* kind of support for it these days, so unpacking them (so they can hopefully be stored in floating point registers) is worth it, even though they aren't strictly word-sized.
I support this direction. Making the Application type more complex again, though, especially to require Rank2 seems bad to me.
Awesome, thanks for clearing that up for me :D
It's their internal draft, which they've put online to edit together. Wait till HCAR is out.
I hear that Rank2Types should be avoided quite a lot on here - what is the reasoning? So we don't to use an extra type system extension or is there some deeper problem to do with say loss of consistency? 
Well it must also be noted that with ghcjs you may use hackage libraries without doing any additional work. Is that the same with PureScript? Also that mentions the current state, it's not impossible to improve the situation regarding the size of the generated code. And ghcjs is currently focusing on that.
Have you tried the built-in :print and :sprint ?
Higher-order types interfere with type inference.
... are you sure you understood the concept here? IMO you are talking about the ghci-debuger - Chris is giving us a really impressive emacs front-end (turns out I was the stupid one here - sorry)
wow - very nice ... I think I'll have to learn emacs in the end ... oh my
I'd like to think that I do understand the concept, yes. Just wondering if :print and :sprint could be used to a similar effect. 
FP and OO are certainly too broad as categories to make that claim meaningful in every single instance, but more than just initial/final duality apply. For instance, dependent pattern matching vs. polymorphic method dispatch, multimethods vs. first-class cases, multiparameter type classes vs. multimethods, etc.
See [this SO thread](http://stackoverflow.com/questions/2139463/tracking-down-errors-in-haskell). Things may very well have changed since then but I recall seeing on the 7.10 roadmap built-in stack trace support with line numbers, so apparently not there yet. Tooling is a major weak spot in the Haskell ecosystem, adoption of the language will suffer until the tooling problem is solved.
oh - sorry - you want this feature for the debugger as well! Well +1 for this (I really thought you was asking to use :print instead of what he is doing)
"Uniqueness" is a modality that can be forgotten leaving you with a normal unrestricted type that can be contracted or weakened. The ability to contract can be forgotten leaving you an affine type. Then the ability to weaken can be forgotten leaving you with linear. You can of course do the latter two in the opposite order to detour through relevance on the way down to linearity, but this 'lattice of forgetfulness' about what you can do with a variable is what I was referring to. Each one of those edges corresponds to a valid transformation you can apply. Going in the other direction on the other hand isn't possible in general. 
there are quite a few. subscribe to the rss feed and you get them.
That's what Acme.\* is for?
I'm not sure I see the duality in all of those. Are there more resources to go into depth there?
Ah, makes sense - a reason I try to avoid GADTs is because GHC sometimes can't infer my types, which is very annoying.
but not the one it needs?
Looking at the ApplicativeDo proposal, why would it desugar do x &lt;- A y &lt;- B -- B does not refer to x f x y into join ((\x y -&gt; f x y) &lt;$&gt; A &lt;*&gt; B) and not just (\x y -&gt; f x y) &lt;*&gt; A &lt;*&gt; B ?
A small suggestion: Wouldn't it be nicer to instead of giving the response body the type `(Maybe Builder -&gt; IO ()) -&gt; IO ()`, make it `(Builder -&gt; IO ()) -&gt; IO () -&gt; IO ()`? Or perhaps even make it opaque, data ResponseStream = ResponseStream { responseStreamPart :: Builder -&gt; IO () , responseStreamFlush :: IO () } 
sounds good. open a pull request. i'll accept.
Hmm? Your `(\x y -&gt; f x y) &lt;*&gt; A &lt;*&gt; B` doesn't even have the same type. Actually, I don't think there is a function that inhabits Applicative f =&gt; (a -&gt; b -&gt; f c) -&gt; f a -&gt; f b -&gt; f c
It's possible to write such function using fixed-vector-hetero. It's experimental package for working with product types. {-# LANGUAGE DeriveGeneric #-} import qualified Data.Vector.HFixed as H import qualified Data.Vector.HFixed.Class as H import GHC.Generics (Generic) data Coffee = Coffee Int String Char deriving (Show,Generic) instance H.HVector Coffee pp :: Show a =&gt; a -&gt; IO a pp x = print x &gt;&gt; return x cleverConstructor :: IO Int -&gt; IO String -&gt; IO Char -&gt; IO Coffee cleverConstructor a b c = H.sequence $ H.toContVecF $ H.mk3 a b c Note that code isn't fully generic. One need to choose constructor function with correct number of arguments. It's indeed difficult to write down type of constructor function. Your idea with Returns type family is quite interesting. I'll need to explore it more.
:print binds the inner thunks to new names, easily forceable. I don't know about the interface issue, that's a good point. 
Well, it occurred to me later that you might be using the word "computation" specifically to refer to the arguments to the bind operator, even though "computation" is a general word that even you used for an expression that had no binds in it. Personally, I just call those things actions, though that word can be misleading too - e.g. the "actions" in the `Maybe` monad evaluate to Maybe values (with holes for RHS bind arguments, of course) so not very action-like. Nevertheless, a term is needed, "arguments to bind" is a bit cumbersome, and "computation" is used for computations generally - and it's hardly surprising if talking about ordering computations gets confusing when that term is commonly used for temporal ordering of primitive operations when discussing evaluation order to explain the lazy semantics of Haskell. Of course I can't easily argue against "multiple people" unless you specify who they are and reference the specific cases where they contradict me. I tried clicking "context" on my comment - the only person to reply to me here is you. Maybe you should examine the possibility that appealing to undefined authorities may be even less valid than appealing to authorities in general, and that you haven't justified your claim that I'm wrong. 
This is great, thanks for sharing. I had a similar route to yours, learning ML at university and working with Python professionally before becoming a full-time Haskell developer. Hopefully your story will inspire others on their path to Haskell. We are not inundated with jobs but there are some to be found if you sharpen your skills and keep your eyes open.
It's being used for a variety of small projects right now. I use it for a small DSL inside a larger Javascript application, among other things. Others use it for writing pure libraries, and I've also seen the effects library used to type things like websocket interactions and asynchronous client-server sessions. There are a number of bindings to Javascript libraries in [the purescript-contrib repos](https://github.com/purescript-contrib) including a work-in-progress library for writing declarative UIs using a free monad approach: [scrap-your-markup](https://github.com/purescript-contrib/scrap-your-markup). There are plenty of libraries which could be wrapped using the FFI to the same effect though, and contributors are always welcome :)
very cool, thanks for the info
&gt; you can be both lazy and strict. I'm fairly certain lazy generally refers to an implementation of call by need, which is semantically distinct from strict (call by value).
HCAR should be out next week or the one after that. I'm still looking for contributions :D
Interesting, but the problem is that you have to write `a b c` and manually plumb those arguments around. My solution is all about not knowing how many arguments you need, and letting the generic representation drive that.
**The good news:** Nice, I didn't know about :print! The interface here actually isn't that bad. **The bad news:** [It's broken in 7.8.2](https://ghc.haskell.org/trac/ghc/ticket/9046) as I just found out trying to use it.
&gt; list : (nil : a) -&gt; (cons : a -&gt; List a -&gt; a) -&gt; (xs : List a) -&gt; a &gt; list nil cons [] = nil &gt; list nil cons (x::xs) = cons x xs Took me 5 minutes to find something inappropriately strict in the Idris standard library. Isn't explicit laziness great?
Depending what you mean by Haskell you can already get far; if Haskell is GHC, iOS should work now (https://github.com/ghc-ios/ghc-ios-scripts) rather easily. For Android it's possible to get it running too probably, but it's easier if you use another Haskell compiler like http://ajhc.metasepi.org/. It's all not very optimal yet but possible if you don't mind spending time on trying / hacking a lot. Edit: HTML5 wise; http://www.haskell.org/haskellwiki/The_JavaScript_Problem
I think this is a great move. I haven't digested the actual changes, but I think it is the right separation of concerns. 
Very old index perhaps
Huh? Hayoo and hoogle at your service.
You might like to start with the [First Steps](http://www.purescript.org/posts/First-Steps/) post. An alternative is to start with the [grunt-init template](https://github.com/purescript-contrib/grunt-init-purescript/). Generally, the steps are - Check out the repo. - Make sure you have `npm install`-ed `grunt-cli` and `bower`. - Use `npm install` to pull build dependencies like `grunt-purescript`. - `bower update` to pull code dependencies. - `grunt` to build. Also, the #purescript Freenode channel usually has people around who can help.
You *are* kinda using filter as a in there. It could be `(\i fb -&gt; fb |&gt; show i)` where `|&gt;` is the triangle from the paper (Exhibit C), and it'd look cleaner. fizzbuzz n = take n $ zipWith (\i fb -&gt; fb |&gt; show i) [1..] fizzBuzzStream where fizzes = cycle ["","","Fizz"] buzzes = cycle ["","","","","Buzz"] fizzBuzzStream = zipWith (++) fizzes buzzes "" |&gt; s = s xs |&gt; _ = xs 
Awesome work! It's really great to see improvements in logging, debugging, visualization, editor integration, and so on. Keep scratching those itches!
Would you recommend PFPL over TaPL for an intro book? i.e. if you had to choose only one, which would it be? PFPL is considerably newer and they run at about the same price, so I was thinking it might be more beneficial to pick that one up instead. What do you think?
&gt; Rather, I would like the distinction between laziness and strictness to be encoded as a distinction between greatest fixed points and least fixed points. This is often suggested (for example, Bob Harper's a fan of this). But it's not really sufficient. Just because something is inductive doesn't mean you want it evaluated eagerly. For instance, it is possible to productively generate an inductive list (power-list, for instance), and then consume it, and it would be good for such an intermediate list to be generated on-demand, despite being provably finite.
I like that. I was a bit nervous to get rid of the `Maybe` wrapper, but your approach is both more efficient, and if we went with `ResponseStream`, clearer as well. I don't have a strong bias on the two formulations you've given, I'd be interested in hearing other input.
The problem is that we still need to provide some way to acquire scarce resources in an application which are used in a streaming response. Do you have any ideas for what that API change would look like?
Great story, you're the kind of programmer any company would be glad to have. A little off topic: but these kind of points are a great counter example to the common talking point that it's impossible to hire any haskell developers. I think the opposite. A haskell job posting will be overflowing with qualified candidates.
The important difference to me is that the second view generalizes nicely to other arrows: `(a ~&gt; m b) -&gt; (m a ~&gt; m b)`. And it more clearly reveals a pattern with `Functor`, `Applicative`, `Monad`, `Comonad`.
Absolutely, it's an embarrassment of riches for people hiring haskellers!
It looks like that date was going to conflict with a DC HUG meetup; does the week thereafter (the 13th) work?
Thanks for the nice compliment!
Can someone explain the idea behind the SMT solver in the type-checker? Would that only be relevant when UndecidableInstances is on? Or would GHC be extending the types of constructions it knows to be decidable...?
A binary tree that must be balanced: data BBTree a = Leaf a | Branch (BBTree (a,a))
What I really like about this is that it visually demonstrates laziness. I see this as a really good teaching tool for showing how only the parts of the structure that are forced will be evaluated. Normally printing is considered to be "force all the things", but this printer turns printing into "click to force", which is very cool for demonstrative purposes.
An integer-like type which is only ever prime numbers: primes :: [Integer] primes = 2 : 3 : 5 : 7 : ... -- use your favorite prime generation method data Prime = P { getIndex :: Int } fromPrime :: Prime -&gt; Integer fromPrime (P i) = primes !! i toPrime :: Integer -&gt; Maybe Prime toPrime n | n == n' = Just (length ps) | otherwise = Nothing where (ps, n':_) = break (&gt;=n) primes 
Come work for us!
The first and third are reasonable in Haskell. Number two is possible, but not nice. It would be better in a language like Idris. 
While it's true that Haskell isn't dependently typed, you can get a surprising amount of mileage from having a somewhat interesting kind system like Haskells. We can build proof terms and stick them in separate kinds to give some mockery of dependence. Of course, bottom still tramples about causing inconsistencies but if you ignore it as most do we're not so bad off.
Awesome post, thanks for taking the time to write out your story!
 type Application = Request -&gt; (forall b. (Response -&gt; IO b) -&gt; IO b) Rank 2 types can be avoided by using an unexported "token". data Responded -- empty data decl or whatever. Just don't export any constructors. type Application = Request -&gt; ((Response -&gt; IO Responded) -&gt; IO AppToken) And why not just call a spade a spade? type Managed = ContT Responded IO type Application = Request -&gt; Managed Response Is there an assumption that (Response -&gt; IO b) will only be used once? It would be more Haskellish to enforce that in the types somehow. Because what you've got looks like it has accidentally granted the user too much inversion of control. This is also a criticism of /u/tekmo 's `Managed`. If you expose the `Managed` constructor, you are allowing the user to do things that you probably did not want to allow them to do.
&gt; Why "f Int-&gt;Int-&gt;Int" instead of "f width:Int-&gt;height::Int-&gt;Area::Int" ? Is anyone with at least minimum practical knowledge piloting this Haskell aircraft? If you really want it in the type system, yes, Haskell can totally do that. type Width = Int type Height = Int type Area = Int f :: Width -&gt; Height -&gt; Area It makes way more sense than your way to do it: a width won't be one time an `Int` and another time a `Char`. And Lisp doesn't even have static types, so I don't know what the point of comparing with lisp is. But that's not what static types are for. If you want to annotate arguments, you can also do: f :: Int -- width -&gt; Int -- height -&gt; Int -- area Which is easier to read than having it all on a line, and it only uses comments. In good libraries, even if they are C bindings, normally what is done is using Haddock documentation, and it does fit the bill of annotating functions like you want. I looked it up and it turns out that the library you found didn't: that's just a bad haskell library. [Example of a better one](http://hackage.haskell.org/package/SDL-0.6.5/docs/Graphics-UI-SDL-Video.html). Note how, on functions like that one, the arguments are annotated in the type, just like you want. This is not done with macros, but with comments, because there's no need for a macro. It is an annotation for the programmer. And while you can do it as a type, as I showed you, it doesn't make sense, as you'll do the same kind of operations with widths, heights and areas: adding, subtracting, multiplying, etc. Types in Haskell are supposed to express what kind of operations you are going to do with the value. Also, saying that this makes Haskell "like C" shows that you don't really know Haskell enough. This is a triviality compared to the actual differences between Haskell and other languages.
Going by the ["Total Functional Programming"](http://www.jucs.org/jucs_10_7/total_functional_programming) paper, it does.
No problem!
Looking at the code, what if: ResponseStream H.Status H.ResponseHeaders ((Maybe Builder -&gt; IO ()) -&gt; IO ()) Were: ResponseStream H.Status H.ResponseHeaders (IO ()) ((Maybe Builder -&gt; IO ()) -&gt; IO ()) Where that extra action is whatever finalizers are needed and always gets run, even when there are exceptions in the streaming action. Although it seems like that could also be solved by making authors write exception-safe streaming actions for this case and do finalization at the end of the streaming action.
This looks fantastic although the use of exceptions for error reporting seems a bit out of place. I can only wonder why this was preferrable to, e.g., `Either`.
Great post, definitely mirrors a lot of my experience as well. Writing Haskell definitely feels like we're playing the "long game" betting on a future that's a few years out. I'm convinced the arc of history bends toward functional programming, it just bends very very slowly.
Well, for `mvc` the `Managed` type makes sense because it's not just for resource acquisition and release but also setting callbacks for signals and running actions in forked threads. However, I agree that it might not be the best fit for `wai`. It was only an off-hand suggestion I made and I don't feel strongly about it.
In general I think there are 2 ways you can go about creating restricted types like you mention. Other commenters have given specific examples of both methods. The first is to create an algebraic data type (or set of types) whose structure encodes the invariant you're trying to capture. For example creating a red-black tree out of the following data types. data RedNode a = Red a BlackNode BlackNode data BlackNode a = BlackRR a RedNode RedNode | BlackRB a RedNode BlackNode | BlackBB a BlackNode BlackNode | Leaf The second method is to create an abstract type whose implementation could represent values which break your invariant, but only provide public safe functions to convert to or from it. While, this method doesn't use the type system to *encode* the invariant it does use it to ensure that any values of your type that are constructed must conform to it. Furthermore, you might look into [LiquidHaskell](http://goto.ucsd.edu:8090/index.html). Using it would step outside the bounds of what is strictly Haskell, but allows you to enforce the sorts of restrictions you mention through more natural annotation since it uses a solver to do the heavy lifting
I think it's mostly targetted at checking relations when DataKinds are used.
quoi?
This is a function that does case analysis on a list, but the default case is not marked lazy, so it is not properly delaying its branches. I also noticed that the `(&amp;&amp;)` and `(||)` operators will not short circuit properly. I'm sure there are plenty more oversights, because that's what happens when you have explicit laziness. You have to be diligent about what arguments should be non-strict, but people aren't. I almost used Scala as an example, but I figured that's an easy target, and Idris would be more compelling.
Who knows, Lennart! "♬ The future is not us to see ♬" :P
A function such as insert :: (Ord a) =&gt; a -&gt; BinTree a -&gt; BinTree a returns a new tree while preserving the original one. So in a real program you would not write just `insert 5 tree`, as if `insert` was mutating the tree. Rather, you'd bind the result to a separate variable. Note that immutability doesn't mean the whole structure will be copied at every operation. Here is [a relevant SO question](http://stackoverflow.com/questions/3233473/immutable-data-structures-performance?rq=1) about that.
&gt; but I'm having a hard time imagining a reason we need multiple mid level APIs that are practically identical. Who says they are _practically identical_?
Does this actually show the real thunks though, or rather the individual traversals of the `Data` instance? Both would be nice to have, and we have the former with `:[s]print` although without this nice interface. The downside is if you try and print a strict structure you'll still flood the screen, so I think there's a use case for both variants.
So this requires `Data`, right? I wonder if a nasty hack with `OverlappingInstances` could be abused to fall back on `Show` without requiring any changes to downstream code being presented. Might be an acceptable use of instance overlap. Or just forcibly add derivations for the `Data` instances if missing, since I presume this runs in a GHC interpreter. It would need to be recursive and smart enough to follow instance contexts, though. Anyway looks awesomely promising, great stuff! :-)
For those who aren't following the link, and making it to the end of the article, the nub of the article is this, which I think is very elegant: fizzbuzz n = (test 3 "Fizz" . test 5 "Buzz") id (show n) where test d s x | n `mod` d == 0 = const (s ++ x "") | otherwise = x No imports, no monads, monoids, functors or applicatives, just functional! (Still haven't fully got my head around it though.)
You could do this with C, it's not really a feature of Haskell's type system.
In my opinion this is exactly why it was absolutely the right decision for us (Snap) to not use wai. If we had, then this would be completely breaking us (and not on our terms). The wai package is not a standard. The fact that it is changing this radically after version 2.0 proves this. It does not have broad adoption. It is a yesod/warp-specific interface. The name "wai" is misleading because it leads uninformed people to think that it actually is a standard. It is perfectly fine for it to exist as its own package, but I think it would be better if it was named yesod-wai, warp-wai, warp-core, or something along those lines. In the Snap ecosystem, our package snap-core functions as the rough analog of what wai is to yesod/warp. They differ a bit in that snap-core is a bit higher level than wai, but they serve roughly the same purpose. snap-core defines an API for web servers and snap-server is a web server that implements that API.
I'm not sure what you mean by saying it's not properly delaying its branches. Can you elaborate?
While having a common API may sound compelling, there's also a significant cost involved in maintaining a common API shared by multiple parties: You lose flexibility, as multiple parties are involved if you want to enhance/change the common API, because, for instance, you'd want to try out a new idea. Just try imagining how an incompatible API change had to be handled to ensure that you'd be able to continue mixing WAI components from different vendors.
Thanks for finding this! I made an [issue report](https://github.com/idris-lang/Idris-dev/issues/1121). In general, you shouldn't expect every little detail of the Idris library to be right at this moment - but we're working towards it, and each bit of feedback helps!
I imagine not in the case of Happstack, seeing as the plans for Happstack 8 are at the level of a research project with very specific goals that are not simply "serve up web applications in Haskell". It might make it easier to implement a WAI adapter with less overhead (by not involving conduit at all in the translation to pipes operations) but I'm skeptical it would make a big difference and that GHC wouldn't optimize away much of that overhead anyway. Even so, one of my own hopes for hyperdrive (the warp/wai in Happstack 8) is for a completely pure interface and none of the others provide this: WAI runs in IO regardless of conduit and Snap uses io-streams which sort of makes it a point to run in IO. This would mean that adapters could only unidirectional: you can easily translate pure computations to IO but you can't do the inverse without `unsafePerformIO`, so while you could run Happstack on WAI you wouldn't be able to embed WAI in Happstack, at least not without an additional impure interface. Dropping conduit doesn't help that situation, and in fact conduit could perhaps be useful in making WAI pure by providing the abstractions needed for continuation based iterators. *Disclaimer: I'm not on any sort of official Happstack team, I only have a few contributions and regular chats with Jeremy, the current maintainer. His vision with hyperdrive may differ from mine.*
While I am aware of haskell compilers supporting each of those targets, I don't think any compiler supports all of them. Furthermore, even if there was, it would be hard to get the same program to run on all those platforms because the UI technologies used by each platform are so different. Unity's cross-platform abilities are very impressive. The only other system I know of with similar cross-platform abilities is Qt Mobile, and to a lesser extend Java. If you find anything else, please let me know!
A big part of the issue is that the community hasn't settled on any solution to the streaming problem, either. In fact you pretty much see the same contenders reflected in each, here: WAI = conduit; Snap = io-streams; future Happstack = pipes. Of course then the question is why we can't agree on a streaming solution either!
Does being able to switch from snap to warp or running a snap site on /foo and a yesod site on /bar really add that much value? These separate communities exist for the same reason there are cliques in high school...because different people have different values and prefer different ways of doing things. Chances are that if you want to run a snap site on /foo, you will also want /bar to be a snap site. Furthermore, Haskell web development is too young to have standards. We're still exploring and discovering new and better abstractions. Standards are things that emerge over time as best practices are developed. We're still figuring out what those best practices are. The existence of multiple separate communities isn't a bug, it's a feature. It means that we're exploring more of the problem space.
"type tetris" Love this phrase. Some times I'm astonished with how well you can stumble through a problem once you know what types you need to get to/from.
What would the data type for the first example be? ("a string with no more than 3 'a' characters")? Can you actually encode that in the type system, or is it a similar trick to the prime example elsewhere in this thread?
While this (and other things) are possible in other languages I am under the impression that they are not common. IMHO this is due to a very different mindset towards these kind of mechanics. Eg you can have typesafe newtypes in C++ ... But nobody uses them. 
Also, try working through http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours. It is easy enough to get it working as a desktop calculator but moving on to stored values, etc. gets into the work you are asking about. It is in a real world setting too so you would be solving a realistic set of problems.
Yes, that's essentially right. Of course, there are differences as to how those interfaces are designed and implemented. However, there is no technical reason why a snap-core -&gt; wai adapter, or vice versa, wouldn't be possible --- ultimately you're just sending bytes down a socket --- and the new snap-server can actually be used without the Snap monad at all (although you would still need the Request and Response types).
Sorry, I read that sloppily. I was thinking of a maximum of 3 chars not 3 'a'. Again, that one is very awkward to encode in Haskell. 
The proper type looks like: list : (nil : Lazy a) -&gt; (cons : a -&gt; List a -&gt; a) -&gt; (xs : List a) -&gt; a Just like `boolElim` has type: boolElim : (b : Bool) -&gt; (t : Lazy a) -&gt; (f : Lazy a) -&gt; a
The design space is not just about the HTTP spec. It's about creating a socket networking interface to that spec in Haskell types. There are many different ways that can be done. There are at least six different approaches for the network streaming IO problem alone: lazy IO, iteratee, enumerator, conduit, io-streams, pipes. These are currently open research problems as demonstrated by all the work going on recently in the conduit, pipes, and io-streams ecosystems.
Good examples here of a more typical use of the type system to annotate the code for the programmers who have to read it. http://www.linux-magazin.de/Online-Artikel/Shell-scripting-with-type-safety-using-Haskell/ A lot of this can be done in C or other languages, but Haskell's syntax makes this much nicer. Having to pack and unpack structs in C can be a real chore. Plus it is not idiomatic either so other coders will look at you oddly.
Haskell is quite good at facing the issue of "lack of anticipation of particular values", because its precise type system allows you to [make illegal states unrepresentable](http://www.reddit.com/r/haskell/comments/21mja6/make_lllegal_state_transitions_unrepresentable/). This allows you to write functions which do anticipate every possible input, because there are fewer possibilities to anticipate and because those possibilities are defined in a way which makes it easy to list them all. That being said, Haskell's precise types do not look like "a value of this type, except it also satisfies this predicate", like the examples you gave. If this is what you want, a more appropriate choice might be a dependently-typed language such as Idris or Agda. Those languages do allow you to write predicates, and then they require callers to prove that the values they pass satisfy the predicates. Both of those languages are based on Haskell, and as such it would be wise to learn Haskell first. Haskell's precise types are of a different flavor: instead of arbitrary predicates, you create rules which describe how valid values are formed, and the only valid values are those which can be formed in this way. The advantage of this approach over the predicates approach is that callers don't have to prove that their values are valid arguments, since anything they can construct using the rules is going to be a valid input. For example, let's build a type representing lists of booleans containing at most 3 `False` values. data AtMost3False = Nil3 | True3 AtMost3False | False3 AtMost2False data AtMost2False = Nil2 | True2 AtMost2False | False2 AtMost1False data AtMost1False = Nil1 | True1 AtMost1False | False1 Valid values of type `AtMost3False` include: Nil3 True3 Nil3 True3 (True3 (True3 (True3 (True3 Nil3)))) False3 (False2 False1) False3 (True2 (True2 (True2 (False2 (True1 (True1 (True1 False1))))))) but not: False3 (False3 (False3 (False3 Nil3))) Of course, this representation is not really a list of Booleans. It's easy, however, to convert them to an ordinary list of booleans: toList3 :: AtMost3False -&gt; [Bool] toList3 Nil3 = [] toList3 (True3 xs) = True : toList3 xs toList3 (False3 xs) = False : toList2 xs toList2 :: AtMost2False -&gt; [Bool] toList2 Nil2 = [] toList2 (True2 xs) = True : toList2 xs toList2 (False2 xs) = False : toList1 xs toList1 :: AtMost1False -&gt; [Bool] toList1 Nil1 = [] toList1 (True1 xs) = True : toList1 xs toList1 False1 = [False] And that's the essense of Haskell's precise types: instead of a type plus a predicate, we define a new type which is isomorphic to the set of values which satisfy the predicate, and then we implement conversion functions if necessary. Of course, not all predicates can be converted in this fashion; a list of at most 99 `False` values would be quite annoying, for example. But there are surprisingly many commonly-encountered predicates which can. I thought that the predicates you have listed were good examples of predicates which are too complicated for Haskell, but as the other comments prove, with enough clever tricks it might be possible after all!
True, but you also don't really need to, right? If you export a type called 'ProperString' and you can only construct it using a mkProperString function you are on your way. Access is a little annoying but this would be perfectly acceptable Haskell. If you did the same in something like C with a "struct ProperString" you could not keep coders from abusing the abstraction and people would be bothered by the performance overhead incurred over just using a raw char*.
You are looking for *refinement types*, which you can find in, e.g., Liquid Haskell, as another comment mentions; see http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/01/01/refinement-types-101.lhs/ , or possibly dependent types.
&gt; if c == Empty &amp;&amp; b == Empty &gt; then True &gt; else False Redundant `if` makes me sad. :(
I think a better way to grasp whether the type system catches interesting bugs or not is to work backwards: Take a few actual bugs you've encountered. Preferably expensive ones. Ask whether and how Haskell's type system could prevent them. IME, the answer is that it almost always could, with moderate effort.
What you're asking for could be implemented with [dependant types](http://en.wikipedia.org/wiki/Dependent_type), something which Haskell doesn't support (not sure if it will in the feature, there were suggestions and ideas). Languages like Idris and Coq support dependant types, making it possible for you to define types that depend on other types or even values. You can, for example, define a type that represents only even numbers, and your program will fail to compile if the compiler can't prove that to be the case. I have only a surface understanding of this however, so I'm not sure how your questions would be implemented in either language. 
Thank you!
And as an added bonus, that even rids you of the `Eq a` constraint. --check if node is leaf isLeaf :: BinTree a -&gt; Bool isLeaf (Node _ Empty Empty) = True isLeaf _ = False
Which is *only now* being abstracted from the API... And that's the whole point. It's taken us this long to converge on this abstraction. We're better off as a community not locking all the frameworks down by committing to this API that has just now been announced and hasn't even been released yet. There's value in parallel exploration of different approaches.
Chris Done is really awesome, structured haskell mode and this!
You may be right that having a common package would be a great thing, but I don't see how the current situation is the result of "pettiness", rather than the quite reasonable explanation Doug gives. Maybe now is the right time to try to reconcile the APIs, and maybe you're the right person to do it; it sounds like you have a strong vision and interest in this happening. I think if I were a snap developer I would see very little upside to trying to do something like this, but maybe you can offer a convincing vision (articulating concrete benefits to users, like your blog example) and show some code to show the way.
&gt; Try to contribute to a “famous” Haskell OSS: I was able to land this job also because I had experience with web dev in Haskell. But I had experience mostly because I contributed to Snap. There is a substantial difference to say “I have used Snap”, as opposed as “I used Snap and I have implemented feature X”. If people are interested in following this advice, [there was a thread ~10 months ago about accessible open source projects.](https://pay.reddit.com/r/haskell/comments/1gvc4w/with_beginner_to_intermediate_haskell_knowledge/)
Having some trouble with this - keep getting "Failed to connect: CurlRecvError" is that in connecting to hackage? I can see it fine in the browser.
&gt; There is a reason every other language has a common HTTP API. None of the other languages have as powerful and expressive a type system for expressing abstractions and controlling side effects. &gt; A very appropriate analogy. I just think it demonstrates why the problem shouldn't exist, rather than offering a good explanation for its existence. Well then we'll just have to disagree that a phenomenon we see in every high school of a sufficient size (and also remains relatively unchanged years later at class reunions) as well as all over other human interactions doesn't indicate something fundamental about the way humans organize. &gt; So lets be concrete here. What is the problem? When it was discussed awhile back it was a fundamental disagreement. 
&gt; how do I manipulate said structure while still preserving a consistent reference to it? You are still burdened by impure, imperative thoughts. Leave state behind! Embrace the pure functional paradigm! Seriously now. In an object-oriented language, you might be creating new objects and calling methods on them in order to modify them, but that's not how things are done in Haskell. Values never change. Instead, you create new values which you pass on to the rest of the computation, either by returning them, by returning a new value which contains them, or by passing them (or a new value which contains them) as arguments. In fact, that's exactly what you did in your implementation of `insert`: there was an old `c1`, you made a new value `newc1`, and instead of replacing the existing `c1` like you would do in an imperative language, you constructed a bigger value `Node v newc1 c2` containing it and you returned this bigger value. Larger programs are no different; you're still deconstructing and reconstructing values, it's just a much bigger value containing every single piece of data used by your program. But it doesn't feel that big, because just like you did in `insert`, you only examine the top-level constructor, and you delegate the rest of the work to a subcall.
For a compile-time balanced AVL tree, you can look at the [avl-static](http://hackage.haskell.org/package/avl-static) package I wrote a few months ago. The compiler can always prove that instances are balanced, and the API functions all must maintain this invariant.
I immediately started learning Emacs today when I saw this. It's just the last straw. I'm highly proficient with Vim, and maybe I'll give up on Emacs (it's kinda hard), but there's so much good Haskell stuff with Emacs that seems better than what Vim has available.
I think this the first step in actually achieving a standard interface.
Interesting variation maybe: What if the divisors and substitute words were only supplied at runtime?
&gt; Haskell having a powerful type system prevents us from having a common API for a trivial thing? Even the most ardent haskell haters don't go that far when spreading FUD about haskell. It's not FUD. It's a truly much bigger problem space. And it's not trivial, as demonstrated by the existence of the OP. &gt; Refusing to attempt to work together with others on something that should be totally uncontroversial and to the benefit of everyone wanting to use haskell for web programming does not seem so reasonable. Now you're the one spreading FUD. There was a discussion about approaches. And that discussion ended with us agreeing to disagree. &gt; That is not concrete. What are the problems with the API WAI provides? For some reason I don't feel obligated to dredge through mailing list logs to satisfy your demands. &gt; If snap will actively refuse to use a common API under any circumstances that is up to you, but please just be clear and upfront about it so the rest of us know that there is no technical problem to deal with. We didn't actively refuse. Snap was actually the first web server on the scene (other than happstack's server which they plan to replace). Attempts to agree on a common API in the intervening years have not been successful. We are open to revisiting the issue in the future if it seems appropriate. But I think the fact that nobody has bothered to develop compatibility layers thus far supports the argument that this isn't really as big of an issue as you seem to be making it.
&gt; It is hardly surprising that it takes a long time for an API to change to something that nobody was working towards or even discussing. Maybe you should consider the possibility that this is simply evidence that it's not as important as you think it is. &gt; But that hasn't happened. Uh, yes it has. It just apparently didn't happen in the same way you would have preferred. People choose to work on things based on what provides value to them. So if it's so valuable to you, why don't you go write a wai-snap or snap-on-warp package instead of demanding that people do google searches and recite history for you?
Here are some spelling/grammar mistakes I noticed as I was skimming: one of the false myths\*; I fell\* in love with Python; typically\* a Project Euler challenge; it was running on\* the JVM; for something totally new and scary\*; Constantly sharpen\* your saw
Then go do it and come to the stakeholders with a proposal. If you took the time to get to know us you'd discover that we're all reasonable people.
That particular case should be easily purified by abstracting `IO` to `Monad m =&gt; m`. The bigger issue is that request bodies require `IO` for iterative processing. This in turn implies things like that you can't serialize or cache or `deepseq` requests and not QuickCheck properties of requests purely. You could have a pure `Request` constructor and convert to that by consuming the request body in IO first, but now your pure functions will have to be bottoms for the impure constructors. You could have separate `Request` types and now your API becomes complicated. I'd really like a web server interface that is pure in its core, but it's not as easy as you might think. You could make the request body a pipes `Producer` for example and say that a live web server will `yield` chunks in IO and a testing interface can supply dummy values purely, but now how do let the request body be consumed twice without leaking memory or alternatively how do you prevent it from being conusmed more than once while staying pure? Maybe you need to make the whole request cycle run in a `Consumer` and then that raises further questions, etc etc. In the end I'm not sure we solved the issues of caching and serializing requests anyway, since `Proxy` is abstract. Incidentally these are all problems lazy IO *solves*, by faking purity. It doesn't really solve the memory leakage issue though (which under my constraints can't be solved, except maybe with some crazy special-purpose fake-pure disk swapping) but what's worse it makes it easier for otherwise avoidable leaks to slip through, and generally makes for unpredictable performance. But perhaps we just need to parametrize `Request` and `Response` by a base monad and say that `IO` is the base monad for web servers but leave it abstract in the interface to let pure monads stand in for `IO`, which would statically prevent trying to process a `Request IO` from pure code but still allow you to pre-process it in IO to a pure variant. This may bring other downsides and new trade-offs, though. Sorry for the just-woke-up, pre-morning-coffee stream-of-conciousness brain dump. Hope you find anything here useful, otherwise this comment may serve as a note to self.
This escape from Java is also familiar. It just keeps hunting us. 
Okay, it is clear now - at least clear enough for me to mention the `forall` in passing in the post. Thank you.
Yea, I was expecting them to be! There was a fair chunk of text to write, and my emacs ispell plugin probably missed them. Thanks! I'll edit the post :)
&gt; It is a yesod/warp-specific interface. To be fair to them they have one or two backends snap doesn't have, and IIRC scotty uses wai also. 
&gt;A blog is a blog. There's tons of blog software you can download. Wouldn't it be cool if there was one in haskell, and people could use it in their existing site regardless of what framework they were using and what framework the blog app was written in? [Hakyll](http://jaspervdj.be/hakyll/) provides this exactly :) It generates a static site, and therefore can be used in any framework. Hakyll is quite popular too.
It started with you asking about standardization and myself and several other people suggesting several possible reasons for why we don't have it and/or it might not be as valuable as you seem to think. But our arguments didn't seem to convince you and somewhere along the line the discussion escalated. So no, I don't think that was our reaction at all. Since you seem to still feel strongly about the issue and seem interested in working on it, here's my answer to your request for more details: I don't remember the details and was not involved in all of the conversations, so I can't really answer your question right now in a reasonable amount of time. I would suggest that you start by looking through the archives of the Haskell web-devel mailing list. For the Snap side of the story look for posts by Greg Collins. Reddit is also probably not a good place for this discussion, so I would suggest talking to some of the people in the appropriate IRC channels after you've informed yourself of what was discussed in the past on web-devel.
Great for employers, but it doesn't sound so great for us regular programmers who really like Haskell. Haskell is probably great for me to use as a hobby thing - and there is a lot of valuable stuff to learn - but to actually work with Haskell seems to be for the creme of the crop of the FP hobbyists, and/or PhD graduates. And at the same time, going back to other languages that aren't as... opinionated when it comes to typing and such sometimes feels like a downgrade. Do I really want to spoil myself with desserts in my free time, only to have to go back to eating oatmeal during work hours?
You and me both :P I'd recommend trying out Lemmih's compact map. It's old and needs to be ported (I've done it I just haven't put it on github). I tried a similar project where I serialize and gzip compress the values inside of a Strict HashMap (Keep the keys uncompressed for fast lookup). It was the difference between storing 400k key value pairs vs. 6 million, inside a HashMap String Person. https://github.com/dmjio/TinyMap Also, using ekg to check your memory consumption is a good idea as well!
Also, there's no reason why you can't use multiple remote acid-states.... If you truly are paranoid make one state per HashMap/IntMap, run all executables on one server (different ports obvi). As you grow move each HashMap to their own server.
If you give up, there's always http://hackage.haskell.org/package/groundhog-postgresql :P 
Note that on OSX ~/.ghc does not exist, the default location is ~/Library/Haskell/ take a look at that directory to ensure you didn't forget to nuke anything there.
Wonderfull! And now I'm inspired to relate some similar stories for the local Haskell meetup.
Divide and conquer style algorithms or one with non trivial data dependencies are not necessarily a great fit for GPU programming. In Haskell you would want something like the Par monad, with some granularity cutoff to achieve the best speedup and this on a multicore CPU rather than a GPU. If you can re write your problem to look like a matrix computation ( I don't think you can) then you could use the accelerate package to run it on a GPU. I don't think it's the right shape.
Certainly a great read, but it only makes it more obvious for me that you have to be absolutely outstanding to even get a chance at a good job...
Making things possible by using abusive hacks does not make it ready even for hobby projects.
Are you sure this would run slower on the GPU? Each clock on the GPU could perform as much as ~512 operations, while the CPU is limited to 4~16 depending on the number of cores... Note I am talking about the reduction of huge trees with millions of nodes... I'll take a look on the par monad, sounds neat.
What if another functions Width requires floating point? Your way requires separate scopes for every imported function. Using comments is much better way, but 100% of FFI imports are plain C imports without meaningful names. I don't know if this is on purpose or just dirty throwaway code, but code like that shoudn't be allowed to repository. Your first point is no different than "#define Width int". My 'C' point is still valid. Type system should allow me to create localized per-function aliases or better: keyword arguments that do not interfere with FFI. 
GPUs are not magic: they are terrible for anything requiring lots of control flow and pointer chasing. This scenario is pretty much the worst use-case imaginable.
&gt; Each clock on the GPU could perform as much as ~512 operations As long as you do exactly the same operation on each of your bits of data. But tree algorithms branch all the time, so you almost never do.
I am not sure what you mean, there is no branching nor pointer chasing involved on this kind of tree reduction at all...
How is this the worst case?
&gt; There's no need for three different incompatible ways to say "give me a request and let me pull fields out of it". That's an overly simplistic view of the HTTP protocol. See e.g. [WebSockets](http://en.wikipedia.org/wiki/WebSocket) or other forms of (ab)use where you deliberately want to break up that simple one-shot `RequestMsg -&gt; IO ResponseMsg` communication pattern into something more flexible/interactive. Also, how does WAI expose the `100-continue` feature? 
I think the more "Haskellish" approach here is to use the [Acquire data type](http://hackage.haskell.org/package/resourcet-1.1.2/docs/Data-Acquire.html) which would allow bracket semantics without allowing the user to run the continuation multiple times.
The issue with this kind of approach is that the handler code (e.g., Warp) becomes somewhat spaghetti-ified by having to make sure that all async exceptions are masked appropriately. This also doesn't allow for `ResponseBuilder` to acquire a scarce resource, which would make sense if someone wants to do lazy I/O on top of WAI. __EDIT__: Actually, I take back the spaghetti comment. That might work out just fine as Warp doing something like: mask $ \restore -&gt; do res &lt;- restore $ app req restore handleResponse `finally` runResFinalizer res The question then becomes whether the finalizer should be included in `ResponseBuilder` as well. It *also* begs the question of whether this is easier or harder for application writers than the CPS approach. I know in Yesod it would be easier for me to get things working with the CPS version, but that may not apply universally.
Having never tried to do *exactly* what you are doing, no I am not sure. I have some limited experience, but obviously that is not substitute for actually having a go! I am sure it would make for an interesting afternoon. Simon Marlow's book (no link, I'm on my phone) deals with this kind of thing pretty extensively. 
"standard" library implies there is a standard. We're still working on it, and it's still a research language, so there will be mistakes like this (There's quite a few actually. Sorry folks.) There's lots of advantages to laziness, but strictness has advantages too. There's no definitive right answer, however convinced some people might be... 
I don't know how to write it in Haskell but this kind of computation is pretty easy to do on the GPU. Thinking about it in terms of textures, here's how I'd lay out the problem (as a game programmer): * Expand out the tree so that it has even depths everywhere. Create an extra `NoOp T` constructor to fill in the extra nodes. * Create the tree structure itself into a texture. Store the operator and indices into the data. * Fill in the values at the leaves of the tree into a 1d float texture. * For each depth of the tree, draw a line which calculates the results of the operation at that depth. * Your result is a 1x1 float texture with the answer. Example: Add (Add 1 2) (Mul 3 (Sub 5 1)) =&gt; expand to maximum depth Add (Nop (Add 1 2)) (Mul (Nop 3) (Sub 5 1)) =&gt; tree sizes array 3 2 1 =&gt; operator texture (uint8x4, last entry unused) -- nop = 0, add = 1, mul = 2, sub = 3, div = 4 [ depth 2, 3 elements ] [ depth 1, 2 elts ] [depth 0] (1,0,1,0) (0,2,0,0) (3,3,4,0) (0,0,0,0) (2,1,2,0) (1,0,1,0) =&gt; data texture 1.0 2.0 3.0 5.0 1.0 Pixel shader something like this: int4 op = tex1d(operator_tex, index * 1.0/index_texture_width); float left = tex1d(data_tex, op.y * 1.0/data_tex_width); float right = tex1d(data_tex, op.z * 1.0/data_tex_width); float op0 = left; float op1 = left+right; float op2 = left*right; float op3 = left-right; float op4 = left/right; float res = (op.x &lt; 2) ? ((op.x &lt; 1) ? op0 : op1) : ((op.x &lt; 3) ? op2 : (op.x &lt; 4 ? op3 : op4))); return res; Implementation: one draw call per tree depth to evaluate all the nodes at that depth, then rebind textures and go again. There's probably an easier way to organize it using GPGPU techniques, but I haven't done any work with those yet.
&gt; every other language has a common HTTP API I don't believe *every* other language to have such an API. Which common API is out there for e.g. C and C++? I don't see Apache, lighttpd, or Nginx offering the same C API to hook web-apps into the web-server...
PS Actually I'm not sure if you'd want to use DPH (or a GPU for that matter - GPU's are traditionally data parallel processors). I might be pointing you in the wrong direction. In general, if you can write your computation using maps and folds it might be a good candidate for employing a data parallel strategy. In this case Add and Mul are two different operations so it's not really in a convenient form for SIMD (single-instruction-multiple-data). Also, the tree-like data structure would have to be flattened somehow which is what DPH tries to do. This could be better suited to http://en.wikipedia.org/wiki/Task_parallelism where par and its kin come in handy.
See [this](http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter34.html), [this](http://stackoverflow.com/questions/8524487/kernels-that-run-fast-on-multicores-but-relatively-slow-on-gpu), and [this](http://www.hwsw.hu/kepek/hirek/2010/06/p451-lee.pdf). Basically it's because GPUs are SIMD processors, while CPUs are MIMD: the cores of a GPU are largely forced to do the same work on different data, while the cores of a CPU aren't.
consider this program: foo = do x &lt;- [1..10] y &lt;- if even x then [1,2] else [3,4] return (x + y) main = print foo The *order* the statements in `foo` get executed has only a passing resemblance to the order of the statements in the code--the list monad backtracks like crazy. While it's true that the `return (x+y)` line is only executed with a bound `x` and `y`, I don't think it makes sense to say that the list monad is about ordering computations. The Haskell wiki is trying to dispel the beginner notion that because Haskell uses monads for IO sequencing, that similar sequencing applies to every situation in which a monad is used.
It depends on how well balanced the tree is and how many operators you need to employ. If the tree is very deep and irregular then things will go awry... It's also quite complicated to manage and re-compact the texture at every level.
There is a lot of both branching and pointer-chasing in your example. Pattern-matching on the node type counts as branching, and the subtrees are all represented using pointer indirection. Here is some roughly equivalent C code to your example—notice the switch (branching) and pointer dereferences: struct T { enum { ADD, MUL, DIV, SUB, NUM } tag; union { struct { struct T *left, *right; } node; int value; }; }; int eval(struct T t) { switch (t.tag) { case ADD: return eval(*t.node.left) + eval(*t.node.right); break; /* other cases elided */ case NUM: return t.value; break; } } 
&gt; I'd really like a web server interface that is pure in its core, but it's not as easy as you might think. You're especially going to encounter difficulties once you need to deal with asynchronous exceptions.
I know how GPUs operate. The example doesn't need any kind of pointer chasing! There is absolutely nothing forcing you to use Haskell's representation of structs. There is no branching. You can perfectly represent your trees as strings on the memory.
Another path to getting Haskell jobs is to actually pursue higher CS education. There's plenty of research projects that would be fine with using Haskell for whatever they're doing, even if it's not PL related. Academia is much less concerned with language choice. Also, if you do PLs research you can actually work _on haskell itself_ which is also cool.
I **am** a regular programmer. I don't hold a PhD, and I'm not even so strong in Math. I wrote this to tell people that is not impossible to get a job if you really want it. As I said "even if you are not Ed Kmett, you should try anyway"! At least employers will you know exist!
Can async exceptions arise in pure code unless generated by the system? I guess `unsafeInterleaveIO`? I imagine that a pure interface would either instantiate to IO as the base monad, or otherwise not be able to cause any problems that would also require IO to deal with. Of course you can't handle even synchronous exceptions without IO, so you couldn't express a catch-all 500 Internal Server Error response, but you *can* map over exceptions, and a live web server will of course eventually require IO anyway. The point is to have a pure core, for example for testing purposes, for sending simulated requests inside a request handler, and generally to get the benefits of purity in the core. It feels silly that I have to set up an `MVar` to to create a `Request` in Happstack, and I imagine the situation is similar in the other web stacks. But even without going full-on pure, it would be useful to have pure request and response objects that can be serialized and cached, something dcoutts has said he wanted for hackage-server for example.
When talking about monads, we like to think of a `x :: m a` as an `m`-style computation that produces an `a`. This is a fairly standard way of talking about monads. The type of bind -- `(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b` -- tells us precisely that bind takes an `m`-y computation of an `a`, and a way to get from `a` to an `m`-y computation of a `b`, and feed the result of the first into the second so that the `m`-y components of the `m a` computation are dependency-wise "before" the `m`-y computations of `m b`. If you do this with `IO`, for instance, it means that this: `putStr "hello" &gt;&gt;= (\_ -&gt; getStr &gt;&gt;= putStr)` will always print "hello" before getting a string before printing that string (because `IO` ordering is temporal). This is not true if you just had `let`'s binding the computations, or if you had some kind of naive sequence construct like in Lisp, thanks to laziness. The monadic interface enforces that the computational dependency, as expressed by *bind* (i.e. first argument's computations precede second arguments), are maintained.
Cheers, I'll check that out. Do you know of any way for an app to monitor its own RAM usage? I've had a brief look around but found nothing so far. It seems like a good idea to be able to automatically swap data out if the RAM usage is nearing the limit, or at least send a maintainer a warning.
I recommend you read [LYAH chapter 9 on IO][io] to get a feel for Haskell programs that talk to the 'real world'. Note that one of the examples does actually read and write data from a text file, but this is only for persistence across multiple program executions. Also, consider the 'state' that you see in a simple program like this: main = loop [] where loop state = do putStrLn "Enter a word!" word &lt;- getLine case word of "" -&gt; putStrLn $ unwords $ "You entered:" : state __ -&gt; loop $ state ++ word Is `state` modified? Does it need to be? [io]: http://learnyouahaskell.com/input-and-output
Nah, both of them are for regular, flat data parallelism (although nested data parallelism is on the way for Accelerate). 
The `Reader` monad is an excellent example. There's absolutely nothing interesting about the order of statements in the `Reader` monad. The only thing the `Reader` monad means to do is to say "hey, I have this 'global' variable here; feel free to read it". The only thing `Reader` does is make implicit that we're passing that variable around; we get no other benefits over using non-monadic expressions. But yes, monads are about sequencing (in a subtle way). However, that's also one of the problems with them. The `Reader` monad only *wants* to give us access to some implicit variable, but in order to do so it forces us to phrase our program as a sequence of statements— when the sequencing of those statements doesn't matter! For the case of the `Reader` monad, things should be transparent enough that the compiler can recognize that the sequencing is irrelevant; but for many other monads we end up over-serializing our programs, introducing new data dependencies that we didn't mean to. And it's those extraneous dependencies which are part of why we call monads and imperative languages "impure". Monads give us cleanliness of separating different classes of side effects, but they do not free us from the problem of over-serializing our programs; if anything, they reintroduce the problem to the otherwise unserialized lambda-calculus.
This is great. Just today I was wondering if I am ever going to get that dream job that is programming in the language I love. Reading these lines gave more energy to keep trying! Thank you!
Look at that first post carefully, because I don't think it supports the conclusion you're drawing. Firstly—Haskell does indeed represent these values with pointer indirection, more or less like my C code. The solution in that first post requires that you first pad out the tree, which *necessarily* amounts to some amount of pointer chasing. So yes, you can ship your data off to the GPU—so long as you've already done your pointer-dereferencing first. It doesn't alleviate the need for pointer traversal entirely. Secondly—the GPU code in that post still has branching! Right here, using the ternary `x ? y : z` operator: float res = (op.x &lt; 2) ? ((op.x &lt; 1) ? op0 : op1) : ((op.x &lt; 3) ? op2 : (op.x &lt; 4 ? op3 : op4))); All you've managed to do is obscure the pointer traversal and ignore the branching. Both of them are very much *inherent in the problem that you're describing*. EDIT: I should point out that, so long as the GPU in question is a SIMD machine, the branching is going to be less and less efficient the more node types you have. SIMD processors can't branch in a traditional sense; they have to take every possible code path and then selectively ignore values afterwards. So given a tree with five node types, the SIMD code will for every value in the vector evaluate all five code paths and then throw away four of them.
You may want to take a look at various papers by Akimasa Morihata, for example "Macro Tree Transformations of Linear Size Increase Achieve Cost-Optimal Parallelism".
yitz nailed it: semigroups are for whenever you have a "monoid" that doesn't have a natural definition for `mempty`. Mathematically, a semigroup is just an associative binary operator; of which there are too many to worry about. But as a type class, `Semigroup` is for when you have a "natural" or "obvious" such operator for your type (e.g., appending for lists, addition for natural numbers in Presberger arithmetic, the meet or join for semilattices, etc). There are two sorts of problems you can run into. The first is when you have no good candidates for `mempty` (e.g., appending non-empty lists). The second is when you have too many candidates, all equivalent. For the latter, consider discrete finite probability distributions (i.e., a finite set of values each of which has a weight, such that the weights all sum to 1). The natural semigroup here is adding the distributions together (i.e., the weight of `x` in `A+B` is equal to the weight of `x` in `A` plus the weight of `x` in `B`, divided by two). The simplest such distributions have a single element with weight 1. But, we can't use that for `mempty` unless there's some sort of "natural" or canonical value to pick as the element; which isn't the case for most types. This really is a case of having too many candidates for `mempty` because in practice when dealing with discrete finite distributions we often really wish we had some distribution with a single element of weight 1, and we only get one when the type of values we're taking distributions over has a natural empty value.
You're right that other people's use of lens doesn't force my code to use it (as witnessed by my code not using lens and still seeing plenty of use), but the code of people who use lens is very inaccessible to me. It's almost like having to learn a new language.
I don't agree: strong update is dangerous in these languages precisely because it's not mediated by the type system. With linear types, you can get safe strong update and encode some interesting patterns with it -- such as incremental initialization of data structures, or typestate.
You need to branch on type of operation at least. 
Multiply-add is actually a pretty common SIMD operation. 
&gt; Can async exceptions arise in pure code unless generated by the system? You can get a ThreadKilled exception at any time, yes. I actually don't know of any other way to sanely implement timeouts.
You're welcome! Never give up!
Hi there! I have no degree in anything, I code in Haskell, and I regularly have sex with multiple women at the same time. It is ridiculous to think that only "ivory tower" academics use Haskell. It's equally ridiculous to think that category theory is some arcane moonspeak that is intentionally obtuse. Now fuck off, troll; some of us are actually interested in learning and coding in Haskell.
1) I feel like we should optomise for application writers and not backend writers, since there will be more applications than backends. 2) I also feel that people using lazy I/O know what they're getting themselves into wrt finalization. The pure-values responsebuilder shouldn't need a finalizer. Anyone who wants that could use responsestream with a single item if they really want to. 3) I don't do a lot of streaming in my apps, but the CPS version could force me to enable Rank2 just to write an app. Also, it removes the nice simplicity that is request-in-response-out.
As someone who routinely mounts sinatra apps as part of Rails apps and uses generic Rack middleware with Rails apps -- yes. The benefit to a common low-level is *huge*.
:ROFL: - you think you will get any reasonable answer to this kind of rant/trolling? Ok I'll try: Haskellers are talking CT (from time to time) because if you got an duck why shouldn't you call it a DUCK - there was a time when CS was seen as a part of mathematic so go figure. (BTW: yes the concepts are difficult to get, and yes you have to really work a bit if you want to understand it all - BUT you can be productive in Haskell without having a clue about any of this stuff and just use it ... just the same as with any other lang. out there)
so the idea is that this is a huge tree of nested operations? you could certainly do this on a gpu. assuming it is perfectly balanced (so has 2^n leaf nodes, and a depth of n) you'd put each leaf node in a separate "thread" (work item). that item would then do each of the four operations in turn (is this an add? then add. is this multiply? then multiply. etc). so you'd "waste" 3/4 of the time in the work item (only 1 of 4 ops is useful) but you're doing n in parallel, so it's still a big win. next you repeat but only use even work items, and you pull in one value from the neighbouring odd work item. next you repeat but only use multiples of 4 and pull in the neighbouring even. etc this is just a standard tree reduction that you program all the time on a gpu. so you'll process a tree with 2^n nodes in n passes, with 4 tests and operations per pass. in practice it's not that easy because you need to worry about communicating data between work items, and shuttling data back and forth to the gpu. but that's the general idea. [edit: deleted complete brain-dead time estimate. sorry about that.]
Indeed. I have no love for Yesod as a whole, but I love WAI and use it for all of my apps.
I have to admit: I seem to be unable to work neither in VIM nor in Emacs (using SublimeText for my Haskell editing) - but maybe there IS hope ... Can you point me (or someone else) to some emacs tutorial(s) for IDE-disabled/spoiled dummies?
I agree that HTTP can be more complex than that, and 100 is a good example. Websockets are not, though, since Websockets are not HTTP :)
Read an abstract algebra book, (they are really short), and the typeclassopedia. Category theory is later. 
I'm thinking things like timeouts are on the web server (implementation) side of things; does it need to be part of a pure core interface?
I would have split the operands into separate textures anyway though. Not that I think this is a great problem for vectorization, but it is doable. And yes, this doesn't solve all problems. I was only pointing out that one thing. 
I think you have these developers totally mischaracterised. They are not bitter rivals at all. They in fact ensure that their libraries are not locked in and hard to use with each other. I think it's great to have several options for the various components of a web app. I think "standardization" is a really difficult ideal to reach, especially when everybody has different ideas about how things should work. I don't think the situation is anywhere near as bad as you're making it out to be. 
&gt; Do I really want to spoil myself with desserts in my free time, only to have to go back to eating oatmeal during work hours? Yes, you do want to spoil yourself. Work for the paycheque, and write Haskell for love. In my own experience, Haskell has made me a better oatmeal programmer. :)
Whether it's standard or not, I still think that word is confusing. Moving on from that, though... Thanks for pushing me to think this through again - I had forgotten some things. I think we're telling each other what we already know in different words. However, if we're still in disagreement, I agree with your earlier sentiment that I'll interpret now as "it's time to agree to disagree". I thought it better to pull this to the front - the rest is little more than me working through details of how I view things again - but still, here we go... &gt; If you do this with IO, for instance Well, IO (and ST etc) are special cases. Explicitly black boxes, with the effect ordering defined by the Haskell standard. The primitive actions don't necessarily *have* data dependencies that would enforce the correct ordering of their effects, so the language simply declares that compilers must act as if they did. You can think in terms of threading the world as an argument, and IIRC GHC even literally has a token to represent the world, but the standard mandates no such thing. Even then, there's a slight failure of equational reasoning. For correctness, the IO monad is dependent on the single redex rule. If an expression were duplicated, then each copy reduced separately, that would result in the effects being duplicated and occurring at two different times - a semantic change. Although the single redex rule (part of laziness) is applied anyway, normally it's purely a performance issue - in the case of expressions with IO (or ST etc) effects, it's important for correctness. Simon Peyton Jones comments on this, of course, in Tackling the Awkward Squad. Furthermore, the effect isn't (necessarily) the whole action. For composed actions, there's typically plenty of pure expression mixed in with the primitive IO-actions to form that larger action. It's the effects that are ordered. For example, there are times when people talk about moving all the `return`s to the end and throwing away all but the last. Perfectly valid as they're really talking about algebraic re-arranging, not just swapping a arbitrary things around. The point is that there's always an alternative way to rewrite an expression in which effectless actions are moved and combined - see e.g. explanations for free monads. For pure monads, "effects" aren't IO effects, they're whatever you happen to call an effect in the abstraction your type provides. The implementation of the effect ordering is by the mathematical meaning of the expressions and how they're assembled. Data dependencies are significant and the definition of bind restricts what can depend on what, and certainly I agree that's important too - it's how you implement the abstraction for your monad. So, although I hadn't got everything as clear as I should have before, the basic point is... 1. The arguments to the bind operator - the actions (or, I guess, computations) - have a syntactic order expressed using bind operators and/or do notation. 2. What the ordering of actions means depends on the type of monad - it's part of the abstraction the monad provides. 3. Effects are sometimes really effects (IO, ST, ...) but sometimes just part of the abstraction - something that will be described in the purely-evaluated result - the thing you're choosing to order in your monad. 4. The effects aren't always the same as the actions. The effects are consequences of "executing" the actions. 5. For IO-like monads, the ordering of effects is ultimately by decree. The effects are ordered so because the Haskell report says so. Data dependencies are significant, but not the whole story unless you invent some fake ones. 6. For other monads, the implementation of the ordering of effects is just functional programming - writing the expressions for bind etc to get the desired behavior. The data flow that's possible is a significant constraint on the kinds of abstractions that can be implemented as monads - that's the common pattern for the overall monad abstraction. 
Jekor, in addition to doing great Haskell videos, makes great Emacs videos as well! [Have a look.](https://www.youtube.com/user/jekor)
From your question, I am uncertain if you have understood that you could do : let tree1 = insert 5; tree2 = insert 6 tree1; tree3 = insert 1 tree2 or if you did understand but think that cumbersome ? You must see though that we probably won't use tree1 and tree2 in the rest of our code so giving them a name is useless, we could directly do : let tree = insert 1 (insert 6 (insert 5 empty))) Or even : let tree = foldr insert empty [1,6,5] Which we could even make into a function : let tree = insertList empty [1..100]; insertList t xs = foldr insert t xs Also the same variable name can be used for different values of a tree... as long as it is local to a function, which is most likely, you could build a global tree available from everywhere in your program, but you seldom have this need. For instance : ys `allIn` xs = let treeSet = foldr insert empty xs in all (lookup treeSet) ys I assume you create a `lookup :: Bintree a -&gt; a -&gt; Bool`. See how treeSet will have different values in different invocation of the function but still have the same name ?
https://en.wikipedia.org/wiki/Semigroup (Just do append “in Haskell” every now and then) 
I'm just reporting from some friends' experience trying to find Haskell programmers. Lots of people with excellent qualifications wanting high salaries.
These old-school power editors weren't really made to be "intuitive," so you have to practice a bit before you get comfortable with them. The built-in tutorial of Emacs is actually fairly useful. It's kind of slow-paced, starting with like "how to move the cursor," but learning it from the fundamentals helps. Once you know the basic cockpit controls, the next step is to configure the editor just enough to let you work on your project. That might simply mean installing haskell-mode. Then comes the rough period of trying to work without knowing all the cool commands and useful plugins. Browsing around on EmacsWiki can teach you a lot, plus looking at other peoples' configuration files and just stealing whatever looks useful. I used to have an enormous Emacs configuration, but now I'm mostly back to the basics. Here's some stuff I find indispensable for coding: * ``magit``, a powerful Git client, but easy for the basic workflows; * basic no-tabs settings; * ``column-number-mode``; * ``projectile``, for finding files in the current repo; * ``autocomplete``, for (very simple) completion of symbols; * ``ag`` and Projectile for ridiculously fast codebase grepping; * ``show-paren-mode`` for balancing those parens; * settings that disable leaving backup files in the repo; * ``auto-revert-mode`` which automatically updates buffers when disk files change; * the command ``align-regexp`` for doing syntax aligning; * an understanding of how to use keyboard macros; * plus a nice theme and font.
monads? why and how do monads come into play with that?
Hi legoodguy. Functional programming a la ML can seem very strange to someone coming from an imperative background. Haskell is a descendant of ML, and an even stranger one, because it is lazy by default and pure. Feel free to hang out, read to introductory articles, play with Haskell and ask questions here, on StackOverflow or on the #haskell IRC channel. Hope you enjoy it.
 =&gt; expand to maximum depth Add (Nop (Add 1 2)) (Mul (Nop 3) (Sub 5 1)) If this really is the exact operations being performed (add, subtract, multiply, divide), as opposed to being representative operations used to ask a quick question as a stand-in for non-trivial operations in the real code, it is likely that in the amount of time your code spent "expanding to maximum depth", which requires a full traversal of the tree and almost certainly some moving around of the elements and will be dominated by memory access time in any non-trivial problem size, you could have simply done the calculation on the CPU.
I think you misread the parent or I'm misreading you, because it sounds like you agree - *without* linear types, strong updates are dangerous; *with* linear types they are not.
What does this mean, exactly? If we have a type: data Expr = Num Int | Var String Can I then use `type Foo = Maybe Num`? What uses are there of this? What are the inhabitants of the promoted data constructor `Num`? Or are there no inhabitants so it can only be used as a phantom type? Same questions about types promoted to kinds. 
Hmm I see the body of the post was deleted. Perhaps the trolling was in there.
Can we also put partiality in the type system? I understand that leaves the pure parts not-Turing-complete but I think I'm comfortable with that.
To be clear, when I was talking about Yesod here, I meant Yesod as an *application*, implying that it might be easier for application writers to use the CPS version. But that's just a guess, I'm not certain. And I *am* fairly certain that the CPS version will make it harder for middleware writers. Your arguments about ResponseBuilder make a lot of sense, I'm beginning to lean in favor of your proposal of just including a finalizer in the `ResponseStream` constructor. I think after the Reddit discussion dies down, I'll take this idea, Twan's idea for the better flushing interface, and any other thoughts from this thread, and put them to an email in the web-devel list for a "final decision" period.
I wonder how much this would hold true if you had to add markings indicating lazy/strict in both cases (... and might there be useful ways to leave it polymorphic?).
The main idea you need is to recognize that GPUs really want to chew on matrices, and so you want to turn a datatype into a matrix. The way you can do that is to note that 1. most datatypes are sums-of-products, 2. in vector spaces, the sum and product are the direct sum and the direct product, and 3. finite-dimensional vector spaces can be represented as matrices Essentially, this turns a tree structure into a matrix where the entries tell you whether one node is a subterm of another one. Using this representation directly will be pretty inefficient, since this matrix will generally be extremely sparse. So next, you want to use GPU-friendly sparse matrix representations for the AST, and then you can basically translate your functions into GPU code. See the POPL 2011 paper, [*EigenCFA: Accelerating Flow Analysis with GPUs*](http://matt.might.net/papers/prabhu2011eigencfa.pdf), by Tarun Prabhu, Shreyas Ramalingam, Matthew Might, and Mary Hall for a good example of this, plus some discussion of what you'll need to do to engineer something that actually runs significantly faster than sequential code.
You can have typesafe newtypes in C. I use them...
It's a reference to a Saturday Night Live skit: "Needs more cowbell" is the full phrase
Do you think it's worth having another one? I don't know how much the situation has changed in the past 10 months
Of course you'll understand :) imagine a time before you learned anything about programming and are looking at a generic java/C website. how much would you understand? how much do you understand now? all it is is familiarity with a brand new thing. 
great paper, thanks!
I know, and as I wrote in the post I was probably even a bit lucky. True, not everyone has time to write "greenfield" libraries, but what about starting to contribute to some of them? If you think about that, do you think an employer would appreciate more a 1 source file, unmantained library or contribution to a "famous" one? I would put my odds on the latter. At least it will make you appear as someone which is able to work in a team and is able to manage large code bases, a certainly appreciated set of skills imho :)
same for me.
I don't know. It might help some people, but I suspect the sheer amount of noise that would introduce would cause a lot of people to just start ignoring it. For instance, people sometimes make mistakes in _Haskell_, where they just start making things strict by rote, instead of considering each annotation.
Geez... you're out of the country for five years, and suddenly you don't get pop culture references... to things that happened 8 years before you left the country. Thanks :)
Sure, I actually contributed some stuff last night. But that was odd in that I didn't feel too tired/had the time to do this. I can only do so much programming after programming the whole day at work. The ideal thing would be a period of 'funemploymemt' but that requires money. 
Of course I should have checked knowyourmeme first, I guess I forgot I was on Reddit ;).
The idea is that you encode the rules of the type by how you can legally construct it. Essentially, you can just apply simple filter rules when you build the type, and not export any other ways to build that type. Thats cheating, yes. It isn't encoding it in the type system... it encodes the rules in code, and uses the type system to enforce those rules. Here is the hard one: toPrime :: Integral a =&gt; a -&gt; Maybe Prime toPrime x = if isPrime x then Just x else Nothing So how does that meaningfully define the type? Well, if I put "Prime" in a type signature, then whatever is passed in HAD to come from some valid constructor, and I know whatever is inside there is a prime. I eliminated a class of errors (where someone passes in the wrong type of integer), so long as the invariant are maintained. And accidentally ignoring those invariants is hard, since it means having to explicitly modify the code that generates the primes, checks the primes, or use unsafeToPrime or something similar to generate nonprime "Prime" types. Really easy to avoid that. The tree one is rather hard if you want it to be transparent (not sure how that would work exactly), but if you just want it as a collection, with binary tree complexity... pretty sure Map is already that.
there is a big overlap.
&gt; Websockets are not, though, since Websockets are not HTTP :) There's no reason your webserver shouldn't be able to support them either.
Such reasonable very sense wow
You're welcome! :)
The code that kvensiq gave is very close approximation of how pattern matching on constructors works inside of GHC. It is inherently a branching operation that matches on the tag of the outer constructor of the value being scrutinized.
There's still pointer indirection in my solution; the left/right samples in my shader are 'pointers' to the node data for that level of the tree. The tree structure is described by the way that the depth 0 nodes refer to the output from the depth 1 nodes which refer to the output from the depth 2 nodes. You can kind of get away without pointers if your tree is complete and regularly shaped, but that involves wasting more operations on no-ops at the lower levels of the tree.
&gt; this would be completely breaking us (and not on our terms) The whole idea is that /u/snoyberg is putting out a suggestion and asking for input. Why not contribute to the discussion? Obviously snap devs would carry a huge amount of weight. It would be really, really cool if there were more interoperability. This is a level where that's reasonably simple - why not jump on this golden opportunity?
Need a "Chuck Norris Facts" site but instead for you. To put the cherry on top, half the facts will be true.
I don't understand any of the comments about letting the free theorems do the work. :/
This link isn't exclusively for your benefit as I suspect you may have read it already: http://ttic.uchicago.edu/~dreyer/course/papers/wadler.pdf
indeed, it definitely doesn't get at the question :p but it's important background info
See http://www.reddit.com/r/haskell/comments/246e39/disemboweling_wai_aka_gutting_out_conduit/ch55df7
&gt; Which is reasonable when your languages are 95% similar across the board. Bit of an aside: In general I think people forget what it's like to try something truly new, and fail to realize how rarely we're called upon to do that as adults. The clearest example I've seen: for galas or pops concerts, orchestras will often have a "guest conductor" (say the mayor, or a local celebrity) for one piece. It's almost uncanny the way giving someone a baton and telling them to move their arm in time to the music (hence the scare quotes around "conductor") can make even the most accomplished and respected professional whatever look like a child. Everything about what they're doing becomes suddenly useless and comical. I think it's good to trust that feeling if we want to learn and do truly new things.
I agree 100% with the sentiment. I'd say I'm on average _worse_ at handling complexity than most programmers. This mainly comes from having to handle implicit assumptions (ex. i'm slightly embarrassed to admit that I get terribly confused when methods don't say their return type somewhere in the header, and have to write haskellish signatures in the comments to stay sane). I'd argue the main two reasons for the "big-brained" stereotype are that thinking in a haskellish way multiplies "effective intelligence". "By relieving the brain of all unnecessary work, a good notation sets it free to concentrate on more advanced problems, and in effect increases the mental power of the race." -- Alfred Whitehead The other is that haskell is just _different_ compared to what people are used to, like the integration trick feynman used to appear superhumanly brilliant in some cases. I'll contrast by saying that there probably IS some big-brained effect surrounding haskell, but not for the reason commonly held. It does not take more intelligence to master the language, rather, it takes a certain kind of intelligence to recognize the merit of such an approach in the face of "social stigma". As well as to self-educate without a well defined path (as far as I know, very few people learn haskell "on-the-job" out of necessity) 
&gt; Also, using ekg to check your memory consumption is a good idea as well! Yes, ekg. Even if your app crashes w/ acid-state you won't lose your data, every transaction is logged to disk as an event. 
If you let the type be sufficiently polymorphic it greatly shrinks the design space. If you hand me a function `id :: a -&gt; a` in Haskell I can pretty much tell you it either spins for ever or hands you back the argument. It might `seq` the argument, but if we use fast and loose reasoning ([it's morally correct!](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html)), I'll just assume it hands back its argument and can be justified in thinking that way. On the other hand if you give me the "simpler" monotype `Int -&gt; Int` I'll stare at that code seeking bugs, because the design space is so much larger. When I write a function, if it doesn't need a particular choice of instance, I don't make it. If it doesn't need a constraint, I don't use it. Why? Because it constraints the space of possible implementations. Moreover, the free theorems I get for those new function become stronger. I get to say more about how I can move that function around in my code for free, without any extra effort.
yes, that's all good and well for toy cases, but what effect does this have on *actual* programming? that's what I'm asking also that isn't describing free theorems but polymorphism
Big yes for that, thank you! Edit: oh... link for full paper? ):
The simplest "how" is to make your code as (parametrically) polymorphic as possible.
I've known the need for self-education without a well-defined path to chase people away from things like Django in the earlier days when all you had was documentation to read. I suspect you may be right that that quality specifically attracts a particular set of brains. Some people are great at solving those sorts of learning problems, and some aren't, but that doesn't necessarily mean more or less intelligence-- there are just so many styles of learning and Haskell or the Haskell community isn't at a point yet where it can cater to all.
Please do feel free to say more if you get the chance. These concise histories are always enlightening.
that's using polymorphism, not free theorems
I sincerely hope there's not a poor soul out there trying to learn Coq or Agda thinking it's simpler than Haskell..
For example, if you're writing functions that can be expressed with functions from Data.Traversable and not, say, specific list functions, just write `foo :: Traversable f =&gt; f a -&gt; Blah` instead of `foo :: [a] -&gt; Blah`. And the fact that you only use things from Traversable, and that it appears in the type signature, well that tells something to anyone reading this function about what it does. And this works with large projects, it just requires a lot of familiarity with all these useful little classes. But that lets you kind of design things "horizontally". It's almost like these typeclasses were "component", and then every function you write kind of declares which component it needs by putting typeclass constraints. Except that it's finer-grained.
It's an excuse I've gotten from Clojure users for fobbing off learning Haskell. It's irritating because they're trying to seem sophisticated or like they "know" typed languages but it betrays them.
That's really not at all a fact about free theorems, that's just a fact about types. I think you're misunderstanding the question.
You might be interested in `djinn`, which Lennart built. You can find [his announcement here](http://permalink.gmane.org/gmane.comp.lang.haskell.general/12747). It basically takes a type and creates an implementation of that type.
&gt; I get terribly confused when methods don't say their return type somewhere in the header You and me both. This bugs me so much.
All types you mention are already promoted in 7.6. The next evolution will be to also promote GADTs to the type level. I doubt, though, that IO or other built-in types will receive a promotion, as well as type classes.
oh ffs, `djinn` has nothing to do with free theorems. why is everyone in this conversation not paying any attention to the actual question I asked?
Perhaps you could clarify your question then, because all I saw you write about this was "I don't understand any of the comments about letting the free theorems do the work. :/"
It doesn't directly. I'm not trying to write a tutorial. I'm trying to suggest how opportunities for free theorems arise. When you have parametric data types and you start to constrain them by laws then free theorems can arise.
I still don't get how you would use these promoted data constructors/types... 
Well, here's an explicit example. Suppose I have a function mergeFruits :: (Person, Int, Int) -&gt; (Person, Int) mergeFruits (person, apples, pears) = (person, apples + pears) which takes a `Person` and adds together the number of apples and pears that they have. From the type signature alone an API consumer cannot tell that the operation that derives the output `Int` from the input `Int`s does not depend on the `Person`. Nor can he/she tell whether the `Person` returned is the same as the one that was input. If I rewrite the signature to be mergeFruits :: (person, Int, Int) -&gt; (person, Int) then he/she *can* deduce these properties (both of which arise from free theorems).
`djinn` *is* to do with free theorems. `djinn` works by using parametricity to determine the only possible implementations of a signature. It knows that only certain implementations are possible because all implementations must satisfy certain conditions (free theorems).
***this is not what free theorems are*** clarification in the clarification comment. \*sigh\*
There is perhaps a bit of a non-sequitur in what I said. Increased parametricity does improve the strength of the free theorems for the functions I write. It also constraints the design space of functions to force what I write impementation-wise to be one of a vanishingly small set of options. So perhaps, it would have been better for me to say I abuse parametricity because it does both of these things, not that I abuse free theorems. The causal link is in the other direction.
aha ok. that's far less exciting. i was hoping for some insight into a neat way of deploying theory for practical purposes. :(
I'm sorry I really don't get it. Parametricity is useful because parametricity is what gives rise to free theorems. For example if I have a parametric function f :: (a, Int) -&gt; (a, Bool) then I know it doesn't do anything to the `a`, and the `Bool` only depends on the `Int`. That's an example of using a free theorem. Is there anything which needs clarification here?
When Idris showed up on HN there were a few "interested in the power of dependent types" trying to make that jump. I think Edwin had to drop in and mention casually, explicitly that Idris has Haskell users as its target audience.
Relational parametricity and free theorems are two sides of the same coin. You can't have one without the other.
the fact that they're two sides of the same coin does not mean they are the same thing.
Properties derived from relational parametricity are exactly those that are a consquence of free theorems, so I fail to see why you want to distinguish the two in this case.
what's not to get? an equation is not a parametric type. they're not the same. i asked about the former, not the latter. that they're tightly related is not relevant to the question, per se. for example, free theorems can be used for program transformation and for proving correctness. this is *because* they're equations. the constraints that parametricity places on inhabitants, and thus the source of free theorems, is not the same as the theorems that are generated. so i wanted to know what the free theorems were being used for. edwardkmett has since clarified that they were actually something of a non sequitur, and really he was just talking about how parametricity is good.
Since nobody else mentioned it yet, I have to plug evil-mode. It's a very good Vim emulation for Emacs. It lets you keep most of the muscle memory, though you will have to learn some Emacs lisp to bend to your will since there's no vimscript emulation. (Though I doubt you'll miss vimscript as a Haskeller.) Using evil still let's you do all the things that Emacs allows as well.
Man that logic is insane. "Y type system is much more powerful than X type system but is a bit less powerful than Z type system, therefore I'll learn neither and stick with X." Great. You managed to avoid having to learn something. So impressive.
Right, that is the free theorem and from this free theorem I deduce that fst . mapPair f g . swapPolymorphic = fst . swapPolymorphic . mapPair g f so that f . fst . swapPolymorphic . ((), ) = fst . swapPolymorphic . mapPair g f . ((), ) so f . fst . swapPolymorphic . ((), ) = fst . swapPolymorphic . ((), ) . f so fst . swapPolymorphic . ((), ) = id A few derivations hence I conclude that anything with the type of `swapPolymorphic` must have the implementation `\(a, b) -&gt; (b, a)`. I fail to see how this is not a practical use of free theorems.
&gt; the constraints that parametricity places on inhabitants, and thus the source of free theorems, is not the same as the theorems that are generated I maintain that these two things *are* the same. The properties of the inhabitants of types are *exactly* the free theorems for those types. Can you name me a type and a property that all inhabitants of the that type satisfy that is not a consequence of the free theorem for that type?
&gt; the free theorem isn't telling you there's only one implementation I think it is. Here is why: (N.B.: I'm not particularly good with those things, so please point out any nonsense!) -- Free theorem: for any f and g mapPair f g . swapP = swapP . mapPair g f -- mapPair definition (\(x, y) -&gt; (f x, g y)) . swapP = swapP . (\(x,y) -&gt; (g x, f y)) -- For arbitrary values a and b (\(x, y) -&gt; (f x, g y)) . swapP $ (a, b) = swapP . (\(x, y) -&gt; (g x, f y)) $ (a, b) (\(x, y) -&gt; (f x, g y)) $ swapP (a, b) = swapP (g a, f b) -- Make f = const a; g = const b (\(x, y) -&gt; (a, b)) $ swapP (a,b) = swapP (b,a) (a, b) = swapP (b, a) -- Q.E.D. 
I know at least one person who got hired by SC without a popular library on Hackage, so don't take that literally. I'm sure, however, that it helps a lot. 
eh... that might do it.
&gt; Can you name me a type and a property that all inhabitants of the that type satisfy that is not a consequence of the free theorem for that type? The free theorem for `Nat` is boring: for all `x :: Nat`, `x = x`. Here is a property of all `Nat`s: for all `x :: Nat`, `x + 0 = 0 + x`. The property is not a fact derived from the free theorem, nor could it be: that free theorem holds for any type!
There are two things that I think need to apply for something to be a standard (the way I have been using the word in this thread). The package/API needs to be *stable* and *broadly adopted*. Neither of those characterize wai, and my position has very little to do with that. Packages like bytestring and text are standards. Bytestring had one backwards incompatible change in more than six years. Text has had breaking changes in the past few months, but before that it was stable for three years.
Yes my question was rather silly. I should have phrased it much better! I mean "Can you name me a type, and a property that all inhabitants of the that type satisfy as a consequence of it being parametrically polymorphic, that is not a consequence of the free theorem for that type?" 
As I said, the uniqueness of various definitions certainly is one. If you try to provide an inhabitant for the type `a -&gt; a` you quickly find, through blind poking, that there's only one solution. Standard proof search techniques such as trying to find verifications yield that rather quickly. Same for polymorphic swaps, etc. Properties like *`g :: [a] -&gt; [a]` can only rearrange things* aren't directly given but become obvious when trying to implement it -- more obvious, I would say, than from the free theorem `map f . g = g . map f`, tho YMMV.
not that I can think of. can you prove to me that there are no such properties? :)
&gt; Really its just that when I wite haskell I write code I can actually for once in my career actually reuse. Not plan to reuse. This is such a unique feature of Haskell. I believe the reason for this is that it Haskell allows the programmer to express very general things. You write a function to do something, you realise it can be expressed in a more general way with typeclasses, and all of a sudden you can use it for all kinds of things you never even thought about in the first place. It's fantastic.
The free theorem for `f :: a -&gt; a` is `g . f = f . g` from which you immediately conclude that `f = id` and thus there is only one inhabitant. I don't know of any other way to prove this. Actual proofs of properties for other types can also only come from free theorems, as far as I know.
yes, that seems to be a tricky aspect of it.
I'm actually not sure how you prove that formally using the free theorem. I guess maybe with some reasoning about how `(.)` has only one identity? But how you show it using proofs is simple: there's only one proof you can construct: ---------------------- var a type, x : a !- x : a ----------------------- abs a type !- \x.x : a -&gt; a ------------------------------ forall !- /\a.\x.x : forall a. a -&gt; a Working bottom up, there's only one rule you can apply at each point to construct a verification, so there is only one proof term.
At each step, there's only one applicable inference rule. Starting from `forall a. a -&gt; a`, the main connective is `forall`, and there is nothing in the context, so the only rule we can apply is an introduction rule. That puts `a type` into the context. Now our main connective is `-&gt;` so we can apply `-&gt;` introduction, or try to eliminate on something in the context, but there is no type elimination rule, so we must apply `-&gt;` introduction. Now we have `a` as our goal, and there is no `a` introduction, and we have `x : a` in our context but there's no `a` elimination, so the only option is the hypothesis rule. so because at each step there's only one choice, there's only one choice over all!
Let me rephrase that: with this change, could `io-streams` be plugged into this interface just as `conduit` is?
Interesting, but couldn't you introduce another type? How do you know that won't lead to another proof of `a -&gt; a`?
Introduce another type how?
You'd have to define better, but testing for the "Maybe" in your formulation (or a null string in the example) is replaced by testing for the "Halt" in the paper. I would guess that the null string version would be the most efficient in terms of CPU cycles.
I'm not familiar with GPU programming, but is it generally not possible to do something like float[] ops = {op0, op1, op2, op3, op4}; float res = ops[(int)op.x]; as a way to avoid branching?
(Note: I am no expert, this is just what I have gathered over the past few months.) What does OOP do that Haskell does not? If you create a "Widget" datatype, and compile it. You also write a GUI library that uses the Widget, also compiled. Now, someone else wants to extend the functionality of the widget. They want to to change some details about how it is rendered, add more state to it, and so on, but in such a way that it can be used everywhere that the original Widget could be. Now, Haskell can *almost* do that by separating Widget into two parts: a typeclass interface, which is used by the GUI library and allows swapping out of different widgets, and the datatype itself which can be "wrapped" in order to reuse some of its data and functionality, but it is not quite the same. It MIGHT be possible to extend some syntactic conveniences behind the scene to make this a bit easier, but as is, it seems a bit harder to do this in Haskell then in your run-of-the-mill OOP language. On the other hand, temporarily adding additional interfaces to existing and compiled classes is something that I don't generally see OOP languages not doing. Once a class exists, the only way to add an interface is create a new child class that implements that interface. Admittedly, that new class can be used anywhere the old one was, but I can only do that once... I can't have a different way of ordering people in two different modules, for example. 
"People willing to trade their freedom for temporary type security deserve neither and will lose both." - B. Franklin, first programmer. In my opinion Haskell's type system is similar to electric fence. Tiger will jump over it, snake will crawl under, but at least cattle won't spread. 
Last time I tested this, which was implementing SimCity (HLSL on DirectX 9), the pixel shader compiled array lookup code into branching (or, more accurately, conditional moves). You could do array lookups in the vertex shader, but I think that they only worked for constant inputs (like a table of matrices you would pass in to the VS), but not on computed values. That made it work great for animating skinned characters, for example. It's likely the state of the art has improved since DX9 though.
I assure you Haskell will have nothing to do with it. There are better newer languages out there and there are already worse languages with much better tools.
In fact what I described in my initial response was not enough to resolve correctly dependencies according to what is installed (sandbox or global). This was spotted by ekmett and it's now fixed in 0.0.1.7 (which will match exactly with your sandboxes).
That's not how the var rule works. The var rule is given as --------------- var G, x : a !- x : a with `G`, `x`, and `a` being schematic variables, for contexts, variables, and types, respectively. Notice that it has no premises, and it is applicable only when the conclusion is a particular shape, namely, that of a variable at a type, which is in scope at that type. If you tried to use it like you're doing, you're giving it a premise for the second usage, which is an invalid use of the rule. Also, I'm not sure what the stuff on the right of the `!-` would even mean -- judgments in the system I'm imagining (System F, to be specific) are either of the form `G !- S type` or `G !- M : S`, whereas yours is of the form `G !- M : S, N : T`. Not impossible, but not standard by any means.
Yes, System F has products so you never actually need two typing judgements on the RHS. OK, so how about deriving both G, x : a !- x : a and G, y : a !- y : a from two empty judgements. That's a possible route isn't it?
Of course. Any of the streaming apis are in principle convertible to the others. You could even use `Handle` there if you wanted to. The question is why on earth I would be interested in doing that, when I already spent eight months thinking about, experimenting with (I tried a **lot** of different formulations), and implementing what I decided was the simplest, cleanest, and best low-level streaming interface I knew how to make, written specifically with this application in mind.
OK I take your word for it. This is beyond the limits of my knowledege!
Why does id `seq`ing the argument change anything? Forcing the result to WHNF forces the argument to WHNF whether or not you seq the arg?
You mean the cattle are the bugs? And the fence is fencing off the the part of a programmer's sub-conscience where the bugs emerge from?
Good point. Here it doesn't matter.
I was originally just talking on channel to someone else who was already familiar with these terms. Had I known it was going to be broadcast to the world, I would have chosen to use different vocabulary.
I'm more than happy to make it not beyond the limits of your knowledge, if you want! :) Hit me up on IRC whenever; I'm augur.
Edward Kmett is the tiger.
What lost freedom are you worried about, in particular?
It's particularly annoying since monads have worked brilliantly well for us even if some people abuse them to make overly contrived APIs. Something you don't need monads to accomplish.
It depends on the power-to-weight ratio you need. Haskell is at a place where it's a practical choice with a productivity that is better than the presently mainstream languages. It provides that productivity with a massive boost in safety, reliability, maintenance, refactoring etc. However, with things as they are right now with dependently typed languages - you start losing productivity and refactorability ("agility" if you like) once you get into Agda/Idris. There might be a reason not to use Haskell. Needing extremely tight control of memory lifetimes is one, where you'd use C++ or Rust instead. You might have ecosystem lock-in too. Python/Ruby/Node.js/Golang users are good candidates for induction into Haskell just because their ecosystem lock-in is much weaker. If the ecosystem lock-in isn't extreme and a GC'd language is appropriate for your problem, Haskell's probably the nicest choice right now. That calculus might change later if DT languages become less painful to write proofs in or if proof reusability/composition improves. (Avoiding churn/thrashing when types change) When Haskellers talk about "false economy", it's partly because people are using different time modalities / area-under-the-curve calculations for Haskell vs. their present language of choice. They're heavily penalizing Haskell just because it's unfamiliar, yet they're eating egregious productivity and reliability costs just so they can avoid tearing the bandaid off *once*. So they're comparing a one-off cost to a psychologically-very-well-hidden ongoing cost. Obviously if you need top-to-bottom validated software, things like Coq/Idris/Agda start to make a lot more sense. All the same, Haskell would still be a good glue language here (which it is, for Agda). Idris was itself written "for" Haskellers and the tutorials for it are written with Haskellers in mind. So even if for some incredibly unlikely reason you *are* going to leap directly to Idris, you'd still benefit from knowing Haskell. A language like Idris is very rewarding to learn and one of the most promising "practical" uses for it is JS web apps! Worth learning for its own sake, but yeah, for practical day-to-day - Haskell.
If you check their history, they seem to have a penchant for trolling.
As well as Rob's [rdf4h](http://hackage.haskell.org/package/rdf4h) there's also [Swish](http://hackage.haskell.org/package/swish), but neither provide direct support for OWL (well, it depends on what format your ontology is in).
If you're doing accounting in Excel, you don't need a typesystem. If you're writing code for a spaceship, you should theoretically get the richest typesystem that is practical for that application. I do research on linear dependent typing, and I'll be the first one to tell you that it's not going to replace PHP.
As someone with a C++ background, I've been fortunate to not have this problem in most programming, but when I've had this problem in dynamic languages, it's been 'fun'. Especially functions that return results of functions, or even languages that don't have a type system and can return multiple types of things depending on how it feels. An int today, an object tomorrow, etc. So much fun I miss out on in Haskell.
I feel like Haskell's "black sheep" quality is a bit overstated. I can get a lot of the same benefits in F# or Ocaml. Of course, the latter's lack of monad syntax and the former's lack of typeclasses or modules means that it's significantly more awkward, but we're still really far from Python or Java.
I though [this post](http://jeltsch.wordpress.com/2013/02/14/the-constraint-kind/) by Wolfgang Jeltsch was a great overview. It's worth reading even if you won't end up using it. For me, it made the whole "Constraint Kinds" extension much more understandable—it's a lot simpler and more elegant than I had assumed.
Oh, I thought EKG would just present a dashboard for a human to monitor the application - didn't know it could be used in-app as well! Surely if a crash were caused by running out of RAM, acid-state would attempt to load up the database and immediately crash again.
I have not run into this issue; that package compiles fine for me. Are you using [Cabal sandboxes](http://coldwa.st/e/blog/2013-08-20-Cabal-sandbox.html)? If not, then building your project within a sandbox would significantly increase your chance of building dependencies without error.
&gt; Type system should be flexible enough to release me from writing comments. Uh
Haskell has [a stable core](http://git.haskell.org/ghc.git/tree/HEAD:/docs/core-spec). It's actually called "Core."
Most of the uses of monads in my code have nothing to do with anything that could remotely be considered an "effect".