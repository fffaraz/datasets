To my surprise I had no trouble with the Haskell Platform and homebrew sudo brew install gmp sudo brew install boehmgc and then, before installing Epic and Idris, updating my `~/.profile` with the additional path bits mentioned in the epic repository https://github.com/edwinb/EpiVM/blob/master/README.Mac -- making the substitution of homebrew's `/usr/local` for macports' `/opt/local` throughout. In the past, if I remember, it was the readline dependency that defeated me, but that has been booted in favor of haskeline. 
I appreciate the example, but I'll admit I'm still not "getting it". Is there a more detail explanation somewhere? Perhaps in a paper or runnable examples I can play with to see the difference?
This headline sounds like a piece of Star Trek technobabble :p
That's not accidental :).
I developed web applications for several years until I stated my PhD, and now I'm starting a on such a project for my current employer, a deeply Haskell loving CS department. The older version, that needs to be rewritten, is even written in ocaml. After evaluating things, it looks to me that Snap is the most sane way of writing a web app in Haskell -- but it is still **lightyears away** from anything I would call simple, rapid web app development. :( Even with dbpatterson's improvement above or below, such a common simple thing should not be that convoluted. I remain unconvinced that Haskell, however much I love using it, is the right tool for this job.
Having written several small, though non-trivial (a couple thousand lines of haskell, plus an equal amount of templates, etc) web applications with Snap, I actually think it is reasonably pleasant to use. I think the real strength that haskell has is that it is a lot easier to write code that is correct, and even more importantly, add new features without breaking existing ones. Most of the "rapid" development frameworks are based on very dynamic languages (like Ruby, for example), and the only way that you can be at all sure that making a change won't break things is to have really extensive test coverage (or a really extensive and time consuming manual testing process). And once you are writing tests for everything, you have eliminated the speed benefits over just writing the code in a (relatively) primitive framework like Snap, because you have to explicitly write all the low-level details in the form of tests, even if the code is written at a very high level. Where haskell (and snap) have an advantage is that once all the boilerplate code / common conventions have been wrapped up in libraries, you will be able to write haskell web code very quickly, while still having reasonable guarantees about correctness (granted, haskell code should have tests too, but if you structure your types right, you can get a lot of coverage without too many tests). At least, that's my hope for it.
I like that! But is it needed to convert to the `FreeStore`? I think the `CartesianStore` is a zipper as well. And it generalizes to families of types: data CartesianFamStore fam a where Unit :: a -&gt; CartesianFamStore fam a Battery :: CartesianFamStore fam (b -&gt; a) -&gt; fam b -&gt; b -&gt; CartesianFamStore fam a I'm also very curious what you think of the alternative definition of Multiplate.
This exact question was [asked on SO](http://stackoverflow.com/questions/8369114/haskell-function-to-determine-the-arity-of-functions) recently. The short answer is kind of, using some GHC extensions.
Thank you! Very much :) Stack Overflow seems to be quite helpfull! :D
I did a quick implementation of CartesianFamStore as zipper, and it seems to zip backwards. (Or I did something wrong.) https://gist.github.com/1642796
In this case, something that supports the API enter, next, modify and leave.
foralls are not required in GADT syntax. There is an implicit forall b. in the Battery constructor. And in my code I use a datatype (another GADT) to define what types are part of a family. It is called Fam in the example. Its constructors are the proofs (type witnesses). So to say that a type `b` is part of a family `fam`, all we need is a value `fam b` as proof.
Why do you want to do this?
I've been thinking about a very similar concept for PostgreSQL, but I'm skeptical of your approach. What bothers me is that, in your code, the order of IO operations appears to be driven by lazy evaluation. This has a lot of pitfalls. My approach would be to use a Haskell thread to read replies eagerly, and then place them into a channel to be turned into an `IO [Reply]` lazily. That way the part that is `unsafeInterleaveIO`'d doesn't matter. I've made an attempt to implement my idea, but I can't use libpq, the official client library, because it doesn't support pipelining. I used Chris Done's pgsql-simple library, but unfortunately his code wasn't written with pipelining in mind. While I was able to fix some obvious order dependencies that were superfluous in his code, I wasn't successful in making it pipeline either.
I thought Haskell called those patterns irrefutable, or am I confusing two different things?
Interesting. I worked out the result of `zapper [1,2]` and you get Battery (Battery (Unit (($(:[])).(.).(:))) 1) 2 aka Battery (Battery (Unit (\x y -&gt; [x,y])) 1) 2 What is notable is that the last parameter of the list in on the outside. So my `freeCartesianStore` as defined is unfortunately flipping the order of the applicative functor (and hence it isn't an idomatic transformation). I guess a correct implementation of `freeCartesianStore` is: -- There is probably a nicer way of writing this function. freeCartesianStore :: CartesianStore b a -&gt; Free (Store b) a freeCartesianStore (Unit a) = Pure a freeCartesianStore (Battery v b) = freeCartesianStore v &gt;&gt;= liftF . (flip store b) 
Tho it's certainly the main thing I think of doing whenever I see one of these Half Life posts.
[A more likely candidate.](http://www.reddit.com/r/haskell/comments/ol7vm/purely_functional_declarative_game_logic_using/) [Links here](https://github.com/leonidas/codeblog/blob/master/2012/2012-01-17-declarative-game-logic-afrp.md)
I just read several sections of the last paper as I'm really interested in this continuous/discrete difference. It seems that continuous time is referring to the ability to define "real-valued functions" over time-valued input. That is, mathematically you have the ability to define functions from R to R (other types are possible too, and of course we really mean approximations of R, the important thing is that the domain is R). I'm not an expert in animation programming, but here is my understanding of it. In traditional animation programming, you also define "real-valued functions" and you compute with some particular point, t0, that is within the range of the animation. In both cases we have a function with real domain and we sample that function. Extrapolating this to more interesting animations than linear interpolation, I would expect traditional animation programming to potentially make use of state from the previous "frame". Fran seems to allow for that as well. In the context of continuous time I don't see what FRP is bringing to the table. Haven't I always been able to use continuous time in my animations?
Once in a while, something offbeat, is alright. All work and no play etc., He is new to this subreddit and posted it on somebody else recommendation. You could've been a little easy on him. 
No problem, that is pretty neat.
The list comprehension and do notation (for list monad) have the same meaning, that doesn't mean they get translated or compiled the same way, they do not. The do notation is desuraged into monad bind etc which turns into `concatMap` etc. GHC desugars list comprehensions into uses of `foldr` and `build` which are then rewritten via list fusion.
Using stream fusion should fix that, IIRC it fuses concatMap. Iff GHC desugars things to whatever concatMap is in scope, that is.
&gt; Iff GHC desugars things to whatever concatMap is in scope, that is. I would be very surprised. The do-notation desugars to the underlying monad functions (bind, etc.) which are defined in their own module, so they probably can't access the concatMap that is "currently in scope".
Quite efficient indeed. On my machine, it stays at 4.2MB of RAM and uses little CPU (around 1%).
With a list comprehension you know it's a list. With a monad comprehension (or do notation) do you know when it's a list? Type inference will sometimes tell you that it's specifically a list, but other times it might be polymorphic `Monad m =&gt; m a` and get instantiated to list later. Do you really want to make desugaring be type dependent? Do you want it to depend on whether you know locally it's a list or not? I'm not saying those are bad things necessarily, but that's not how its done now. Right now it's simple: list comprehensions are desugared specially while monad comprehensions are desugared in the general way.
I'm well aware. And my complaint generalizes: very nearly all functions with an even somewhat polymorphic return type can be coerced into taking more arguments than a naive reading of their type might conclude.
Perhaps "product patterns" (instead of unfailable patterns), "sum patterns" and "assertive sum patterns" (instead of irrefutable) would be better names?
Nice idea, but I think we don't need to go as low-level as that. The explanation by dcoutts seems to be the main reason: the do-version is not fused.
author confuses axioms with assumptions. math is still not a science.
I'm way out of my depth, and a few coffees and graduate courses short of fully understanding your thesis, but, could OP's problem by solved (theoretically) by providing some List-specific implementations of concatMap or (&gt;&gt;=) that take advantage of List's well-behavedness (which may require redfining Monad type class in a semantically equivalent but more exploitable way)?
The first thing I'd try is to look at whether we can get as good list fusion (still with the foldr/build system) with the standard monadic desugaring. So you'd look at the definitions for the instance Monad [a] and see what is going on with the rewrite rules. instance Monad [] where m &gt;&gt;= k = foldr ((++) . k) [] m Both `foldr` and `++` are fusible. It'd take a bit of fiddling to compare what you expect to happen (unfolding and rewriting manually) with what currently actually happens.
Sigh. I either need to start making oodles of money to pay travel expenses, or move closer to one of these Haskell hotspots. Or teach tons of people Haskell near my location, and create my own Haskell hotspot.
Where are you based ?
use ghci, it lets you look at things you are working on, which is easier than introducing and eliminating IO from code
Laziness is irrelevant; it wouldn't work even if cabalamat did ``x `seq` y `seq` sum (fst (unzip gameResults))``.
Expanding on this, here's how you'd write `playStrat` with `trace`: playStrat s = let gameResults = [scoredGame s opponent gameLength | opponent &lt;- strats] in trace ("gameResults = " ++ show gameResults) $ sum (fst (unzip gameResults)) 
This works thanks. Although when I run it in ghci, this happens: *Main&gt; let x = allStrat *Main&gt; x [("allC",gameResults = [(60,60),(0,100),(60,60),(60,60)] 180),("allD",gameResults = [(100,0),(20,20),(24,19),(24,19)] 168),("titForTat",gameResults = [(60,60),(19,24),(60,60),(60,60)] 199),("cud",gameResults = [(60,60),(19,24),(60,60),(60,60)] 199)] *Main&gt; x [("allC",180),("allD",168),("titForTat",199),("cud",199)] obviously what's happening here is it's lazy so the value of x isn't calculated until I ask for it; still, a little disconcerting the first time it happens. 
&gt; Because of the way that Haskell handles size effects and therefore IO, you'll get sucked into the IO Monad trying to print things. Yeah. Though I expect i'm going to have to be sucked into it anyway at some point, since I'll need to know how monads work to properly use Haskell.
If you do `import Control.Deepseq`, you can do `rnf x` after that to get all the debug output in one go without having it interleaved with `x` itself. (You might need to `cabal install deepseq` first.)
I think hotoatmeal meant that your function doesn't need to return an IO result, but if you use print inside it, it will have to return an IO result... and so will its caller, etc. It's not good for a function to be tainted with IO just for debugging purposes.
I'll ask stupid question: what is this whole FRP for if anything of moderate size written in this style (see Frag q3 "port") gets really obfuscated? Is there any program bigger than toy size written in it? If not then why? Control flow in Source engine for instance is based on few assumptions (imperative, sure but they simplify thing a lot): - sequential execution of Think() per entity - mutation of data structures like BSP tree - access to other entities during specific Think for instance Missile::Think(), Missile::Explode() can mutate Player::m_Health. - kind of continuous time (gpGlobals-&gt;curtime dependency in Think, prediction and interpolation) - time can go can "reverse" - it snaps to last time where server and client agreed on world state How would one write networked game where there are N players, every one of them can fire rockets, rockets can be homing/guided/dumb and all without any mutation/IO or in FRP?
Laziness is irrelevant in this case, so in bringing it up you confused matters.
Building on this response, if you manipulate "big" data structures, dumping them using [Groom](http://hackage.haskell.org/package/groom) is really helpful for a quick read. There is a rationale behind it in the [author post](http://blog.ezyang.com/2010/07/groom-human-readable-show-for-haskell/)
The reason they are not executed (evaluated) is due to Haskell's laziness. You never use x and y, thus print "gameResults = " and print gameResults are not executed (evaluated).
Please explain to me why you think laziness does not matter in this example. There is a clear distinction between never having created the IO actions, and never having invoked them. In the OP's example he does neither, and you discredit me for pointing both problems out instead of just spouting off the solution.
This exact issue would still exist in a language identical to Haskell which was strictly evaluated. It is not that the `IO a` has not yet been evaluated yet; indeed, an `IO a` can be executed multiple times with no relation to evaluation (consider `replicateM_ 10 action` — `action` is not "re-evaluated", a new action is just built up from sequencing it multiple times). It is that an `IO a` is not an `a`; it is a description of a side-effectful computation with result type `a`. It is no more related to laziness than any other aspect of Haskell is related to laziness; it's about Haskell's model of IO. BTW, you seem to be taking things personally; nobody is discrediting anybody, and nobody is objecting to pointing out root causes rather than giving direct solutions. The problem is that, in this particular case, the root causes you pointed out aren't correct.
Regardless, any evaluation order leaves x and y as dead code in the OPs example meaning the IO is never performed.
Since you can explicitly make `x` and `y` *not* be dead code — indeed, be explicitly forced, as I showed — and still have the same problem, that really doesn't make much sense at all. Yes, it's true that they're not forced. That's also *not the problem*; forcing them doesn't fix anything, so calling laziness the "root cause" is misleading. Even if every `let` binding was made strict, the problem would still be exactly the same.
This looks very interesting, but (as unfortunately tends to happen) it hurts my brain. If I'm seeing correctly, the differences with regards to boilerplate are: - you don't have to write a Plate record - you don't have to write mkPlate - you do have write a type witness GADT - you do have to write an EqT instance for it - you now pass type witnesses around where previously you had projection functions And as a consequence it becomes possible to write zipperPlate, which otherwise wouldn't have been? (On the other hand, I'm assuming you can no longer write projector-per-constructor (rather than per-type) plates, which used to be possible?) How would you write 'expr2' using "standard Multiplate"?
I too tend to debug other languages using "print" statements. But here is what I find most successful for Haskell. Make a file. Fill the file with your Haskell program, adding one function at a time. Keep each function small and performing one precise function. As you add each function, reload the file in GHCI (using the ":l" command). Test your function in GHCI by calling it and seeing what it returns. Make additional functions that are test cases that return boolean True when they succeed. Boolean "AND" together all the tests together to see whether they all pass. I find that if I feel I need a "print" statement in the middle of a Haskell program then I have structured the program wrong--I need to rework it to be more "functional", built up by the nesting of simple functions.
neat. Also try my package for inserting print statements that gives line and file information: http://hackage.haskell.org/package/file-location
Quite possibly, the time it will take doesn't depend on whether you're around.
and by this I mean have everything work via both ghc and ghci
Well, people seem to have covered the proximate problem, so let me step up one level and mention that GHC has a debugger. The reference that really finally got me going was [Issue 10 of the Monad Reader](http://www.haskell.org/wikiupload/0/0a/TMR-Issue10.pdf). I think it's out of date in some ways now, but probably nothing that renders it useless, and the rest you can clean up by reading the [GHC debugger docs](http://www.haskell.org/ghc/docs/latest/html/users_guide/ghci-debugger.html). This also has the interesting effect of letting you witness the lazy evaluation order, which I found helpful in partially internalizing it. (I can't claim I deeply and intuitively understand it, but it's less mysterious.)
Welcome to Haskell :) See the other comments about debugging methods. The only way to use print statements for debugging is to put your code in the IO monad: playStrat s = let gameResults = [scoredGame s opponent gameLength | opponent &lt;- strats] in do print "gameResults = " print gameResults return (sum (fst (unzip gameResults))) 
Utah. Go to http://haskellers.com and double-click the map a few times, zooming in on Utah. That's me. :)
thanks! gtk2hs isn't the hard part, and per se if you built gtk et al without doing all the setup of the c libraries, everything will still build, but what will happen is: either that certain programs will fail to run in ghci (for one class of mistakes that are specific to homebrew issues), and with another scenario (for certain programs when compiled with ghc and run as binaries) being that either a program will silently emit an empty file for ps/pdf output, or crash when drawing to an x11 gui window. granted, some of these problems are specific to how homebrew approaches the inclusion of certain libraries, but it does seem to be that brew is the only really actively updated package manager thats tailored to the mac. (i might be wrong, but thats my perception) [edit: it looks like macports has been more active lately, but i'm out of date on these things] [edit2: whats the process using nix look like?] 
I second that. Recently I started to develop using this method, and it saved me tons of problem later on.
It does add to the difficulty of debugging with printed traces though since it's difficult to discover when, if ever, your statements are evaluated. Of course it's the purity which actually kills the method.
&gt; Besides, the result type of fromIntegral can often be inferred from context, so it doesn't have to be specified. And in fact, that is the case here. The function `mkStdGen` has type `Int -&gt; StdGen`. So you can leave out the `" :: Int"` part, and the compiler will correctly infer that the type should be `Int`.
Oh, cool. A lot of software innovation has happened in the Provo/Orem area. It would be great if you could get some of the talented people around there interested in Haskell. :)
Not sure if we can call difference lists a form of the codensity transformation. Note that [] is not a free monad. (Not denying the similarity, though.)
Is `fromIntegral` implemented as a library function or as part of the compiler? If the former, what file is it implemented in (so I can have a look at the implementation)? In general, how do I find where an arbitrary function `foo` is implemented? 
Nice. The only thing that bugs me about all these resource management libraries is that they only address the problem of `IO` resources and are tightly integrated with the `IO` monad, when I think the problem they are solving is (slightly) more general than the `IO` monad. All resource management libraries are motivated by some sort of base monad that can be interrupted at any time (otherwise we wouldn't need to worry about finalizing resources). `IO` is the obvious example, but so is `ContT`, which can exit at any time. In fact, sometimes I wonder if there is a deeper link between `ContT` and `IO` in terms of scheduling and if anybody has any insight into this I'd appreciate it. So imagine that you have a monad like `ContT State` where you would like to bracket stateful commands: action :: ContT State a action = do modify (+1) finally doSomething (modify (-1)) ... where `finally` would be some command that signaled you wanted to perform one last action (`modify` in this case) even if `ContT` is used to exit prematurely. So I guess what I'm trying to say is that maybe a pure and more elegant solution to resource finalization exists in terms of `ContT` and if we could find the parallel to `IO` we could apply it to `IO`, too.
Thanks for this. I've got further along this plan than I've ever managed before on OS X. But I've hit a snag. I'm using GHC 7.4 (which may be a mistake), and I get a funny error during cabal install glib. System/Glib/hsgclosure.c: In function 'gtk2hs_closure_marshal': System/Glib/hsgclosure.c:110:0: warning: passing argument 1 of 'rts_evalIO' from incompatible pointer type System/Glib/hsgclosure.c:110:0: error: void value not ignored as it ought to be I wonder if this is 7.4 being more picky, or some new incompatibility.
Obviously you can't print out an intermediate result in a function, but I would honestly recommend that you factor `gameResults` into a separate function and test that function using ordinary IO statements rather than resort to `trace`: gameResults s = [scoredGame s opponent gameLength | opponent &lt;- strats] playStrat = sum . fst . unzip . gameResults As you can see, it takes the exact same amount of code to just split off `gameResults` into its own function, but now you can test it individually in GHCi: *Main&gt; let s = &lt;whatever you want to test&gt; *Main&gt; gameResults s &lt;output&gt; There's nothing wrong with debugging print statements, but there is something wrong with trace, namely that it uses `unsafePerformIO`. Long story short, the more you use `unsafePerformIO`, the more you will start to tear your hair out when things don't behave the way you expect (including debugging output, as you just demonstrated). For one-off scripts I use debugging print statements all the time, but I actually use the `IO` monad and actual `print` statements rather than `trace`.
Waddayathink? https://github.com/mcandre/genetics
The documentation on Hackage has "Source" links to the right of type signatures, and you can find the documentation with Hoogle as Animhumer suggested. My mistake for not realising that you can, in fact, omit the type annotation in this case, by the way; yitz [pointed out my error](http://www.reddit.com/r/haskell/comments/oqt7o/converting_from_int32_to_int/c3jepc2).
[Is Haskell suitable as a first language? - StackOverflow](http://stackoverflow.com/questions/3362915/is-haskell-suitable-as-a-first-language)
The issues with ContT m behaving differently to IO largely arise from ContT being a codensity monad/cps'd, which gives you a continuation that can be run multiple times since it lives out on the heap, rather than one that can only be fired off once, because it lives on the stack. This leads to the headaches that plague MonadControlIO, where Cont, etc. are concerned, since you need to deal with the fact that you can't really _do_ the resource cleanup until you throw out the entire monad or parameterize it away so that you can't use any of the values that remain, requiring effectively the universal quantification trick that drives ST s to know when its safe to call your 'finally' code. Otherwise doSomething could capture the current continuation, and return it, so that someone else could invoke it. Your example as written could time travel to a no longer valid state when you use it with ContT IO. And when you generalize the quantification trick used in ST s (and which would be needed for any monad with non-determinism/multiple invocations) this to allow these parameters to be nested according to a stack discipline (because we don't have the linear types needed to make a nicer one), you get more or less the regions that Oleg likes to use.
...but isn't that the whole point of `regions` to allow to statically check at compile time you don't perform I/O actions on a handle you "explicitly" closed (even though "static", but in a way the current compiler can't help us detect)? On the other hand, maybe it's possible to provide an `unsafeHClose` nevertheless, which delegates the proof burden to the programmer to make sure he properly sequences the I/O ops in a region scope...
Why does it break without the monomorphism restriction?
Right, which is why- even though `ResourceT` and regions are very similar- they have different purposes and both deserve to exist separately. I believe that `ResourceT` is the right approach for handling scarce resources, specifically because of the point Faucelme is making. It might very well be possible to expose `unsafeHClose` or the like, and thereby have the best of both worlds... perhaps. But then you're exposing an unsafe operation to users and expecting them to use it. IMO, you've now defeated all the extra safeguards regions gives you, and you may as well stick with the simpler `ResourceT`. In other words: * If you want static guarantees that you aren't using a closed resource, use regions. * If you want immediate release of resources, use `ResourceT`. * If all you care about is that your resources are deallocated eventually, use either one.
I actually wouldn't want average users using `performUnsafeIO`, but it's far less of a problem in reality than `unsafeHClose` is for regions. `performUnsafeIO` is the equivalent of `unsafeIOToST`. In other words: if you want to do low-level stuff in `ST`, you're going to have to touch something unsafe at some point, whether it be `performUnsafeIO` or something else. The other reason I avoided regions is that I think it would introduce a lot of complexity throughout the codebase. At the very least, every function and data type would now need to have two extra type variables (`s` and `pr`). And that's inherent to regions, not something we can work around.
Please be helpful and factual when being trying to help and provide facts. Thanks!
Or disable javascript to get a gracefully degraded sectioned linear page. :)
Try disabling javascript
So how far into which books do I have to read before this becomes comprehensible?
&gt; For the monoid m to be valid, the identity element must be isomorphic to 0 on the addition of natural numbers, What does that even mean? Why not simply say "*id* must be the identity element in respect to operator ⊗"?
That sounds like an argument for learning Ocaml before Haskell, rather than an argument for starting with a proper imperative language.
I can't. I didn't learn it from books. But here are some relevant wikipedia articles: http://en.wikipedia.org/wiki/Category_(mathematics) http://en.wikipedia.org/wiki/Monoid http://en.wikipedia.org/wiki/Monoid_(category_theory) http://en.wikipedia.org/wiki/Functor http://en.wikipedia.org/wiki/Monad_(category_theory)
I was hoping to help people out who hadn't encountered monoids in real life. Suppose it makes it more confusing?
I'm teaching a subset of Haskell now as a first programming language to 11-13 year olds, and it's going great. But before there's any hope of giving you a complete answer, you'd have to say a bit more about who is learning and what their goals are.
I think they have, though I can't remember what it was called...
There already was one (I can't remember the name, but it was developed in Canada, IIRC).
Haskell was my first language and I started solving interesting problems with it soon. I don't know if other programming languages are better to get started, because I only can have one first-learned-language in my experience. But I plenty satisfied with my (accidental) choice of Haskell.
Thanks! 
Honestly it makes more confusing for those of us who do know monoids (and want to know more about the connection with monads). I assume you're making an informal comparison, but it took me a while to figure it out ("wait, (real numbers, +) form a monoid but they aren't isomorphic to the naturals... hmm, can a single element be isomorphic to other? etc"). I'm not saying informality is bad, it's just that if you throw "isomorphic" there it throws us readers off the track. For instance, "id must be the identity element, like 0 on the addition of natural numbers, etc" might be better. If you want to teach someone who doesn't know monoids, then I think you'd have a much bigger task (need to start with a few concrete examples like string, naturals, etc, and a few properties they all have). But I don't blame you for not providing all that.
Ah great, thanks. I'll whip up a quick patch to add a single page function when I'm home.
You make excellent points; thanks for weighing in.
&gt; (Naturals, +) is isomorphic to (Strings, ++) Are you sure?
Look up Baez's "Tale of n-categories".
(Naturals, +) is isomorphic to ([()], ++) where [()] is the list of units, assuming only finite, total elements. It is not isomorphic to ([Char],++). Interestingly, with Void as the empty type, ([Void], ++) and the same totality setting is isomorphic to the trivial monoid.
This has proven excellent advice, thanks.
Not loading for me currently, either. It did right after it was posted though.
Or put another way, here is the thought process for avoiding trace. [scoredGame s opponent gameLength | opponent &lt;- strats] So I have that calculation embedded in playStrat. I wonder what is really coming out of that? Well, I could try wrapping trace around that value. But, since I thought about tracing it, I'm now obligated to extract a new function. gameResults :: Whatever -&gt; Whatever -&gt; [(Integer, Integer)] -- Just guessing gameResults s gamelength = [scoredGame s opponent gameLength | opponent &lt;- strats] Sometimes ghc finds my problem now. Just the addition of the new type makes GHC point out that my function doesn't match the type I was thinking of. Sometimes that isn't the case. Now I can just hand test the function in ghci. ghci&gt; map (\x -&gt; gameResults "bob" x) ["a", "b", "c"]
OK, try now?
Thank *you* for writing the post.
Just a comment, though, the code in that first link is implementation stuff and deals with things like unboxed types that are NOT a standard part of Haskell. Learn Haskell first, and don't let yourself get confused by that stuff that's not-quite-standard-Haskell
easyVision--once you get it installed, has all the low level stuff worked out. The biggest pain is getting the Intel's IPP libraries. The rest of the dependencies are straightforward. Check out the tour folder for some really basic examples. You can build some very sophisticated processing pipelines as well. https://github.com/AlbertoRuiz/easyVision.git
Your surprise seems to indicate that you think dcoutts made his studies in the field of "philosophy", while doctor in philosophy is the name used for non-medical "doctors" and doesn't mean the person studied "philosophy" as in r/philosophy.
A patch would be great! I added the ones I did because I already had them around and needed them for a tutorial I was writing. I was hoping to add the rest in the next release (not too far in the future). A patch would make sure that happens !
I started with 6502 assembly back in the mid 80's when I was 13. I still think this is a good starting point. There is very little abstraction involved so you're really programming the computer itself. After this it's useful to learn abstract techniques and at high level I consider functional to be very good for this. C is to low-level, Prolog is too domain specific, OO is usually just some high level abstractions added to low-level core. My first functional language was Clean and I found the unique typing shortened the learning curve need to get something really working. I think needing to understand monad in Haskell makes it less 'friendly' as a first language.
I think I've given a clear explanation of why expectation and reality would be at odds here: there's no consistent way to keep the side effects from the transformers. So currently, `monad-control` follows a very simple rule: ignore all transformer side-effects. Your blog post is pointing out that this isn't what you want. Now, I can't actually argue against any implementation that would fit your requirements, because the only implementation that seems to do that is MonadCatchIO and you're distancing yourself from that. All I can say is that it's impossible to have an implementation that will preserve all transformer side-effects. To make your example work, therefore, you'd need to have a package with the following rules: * Keep transformer side-effects... * except where we don't. There's no doubt in my mind that the simple, universal rule that side effects are ignored is better than an inconsistent rule. Perhaps you have an implementation in mind that allows you to retain the side effects in all cases. If so, that would be something worth discussing, and I could be persuaded to look at alternatives to `monad-control`.
&gt; Let me rephrase: it ignores the side-effects of the non-main execution branch. That's not true either. &gt; All I can say is that the prior art in this field (MonadCatchIO) is inherently broken. Nobody is disagreeing with that. What we are talking about is fixing up the basic approach of putting the control operators into one or more type classes, and then being a lot more careful and conservative about getting the instances for standard transformers "correct". But part of the problem is that first, we need to agree on what is correct; unfortunately that's a problem I don't feel like I have very deep insight into at the moment. I would very strongly suggest that this is the right approach, for one it gives implementors the freedom to implement the control operators as they see best, including using monad-control, instead of forcing everybody to use the definitions in Control.Exception.Lifted and having to provide a monad-control. This approach subsumes monad-control, and allows for much saner semantics in a number of cases. You point out things semantics are clearly unimplementable in pure Haskell, and then use that to justify the semantics of monad-control here. That's fallacious. 
&gt;&gt; Let me rephrase: it ignores the side-effects of the non-main execution branch. &gt; That's not true either. Citation required. These auxiliary functions can all access the context and cannot modify it. If you want to say I'm wrong, please do so with an actual code sample. &gt; Nobody is disagreeing with that. What we are talking about is fixing up the basic approach of putting the control operators into one or more type classes, and then being a lot more careful and conservative about getting the instances for standard transformers "correct". But part of the problem is that first, we need to agree on what is correct; unfortunately that's a problem I don't feel like I have very deep insight into at the moment. From an implementation standpoint, I think that' a bad idea. You'll end up with exponential complexity: every single function will need to be implemented for every single possible operations. And there are lots of different operations (just look at lifted-base). I see three problems: 1. It's difficult to write these in the first place and guarantee they're all correct. 2. Implementing new transformers will be a huge pain. 3. Instead of looking to a single definition to understand semantics of a specific transformers, or a single definition to understand the semantics of a specific operation, you'll need to look at each and every instance to be certain you understand what's going on. You might think point (3) isn't so important. I would say the fact that people are getting confused about the current monad-control/lifted-base setup already proves it's important. &gt; I would very strongly suggest that this is the right approach, for one it gives implementors the freedom to implement the control operators as they see best, including using monad-control, instead of forcing everybody to use the definitions in Control.Exception.Lifted and having to provide a monad-control. This approach subsumes monad-control, and allows for much saner semantics in a number of cases. So far, the only case I've seen that would provide "saner" semantics is the implementation of a `finally` clause. For the record, I'm fairly certain we could implement the semantics you're looking for with the existing `monad-control` approach (I hope Bas sees this and can either confirm or deny this belief), it's just that the current one *makes more sense* to the people who have been implementing things so far (Bas, Anders, and myself). &gt; You point out things semantics are clearly unimplementable in pure Haskell, and then use that to justify the semantics of monad-control here. That's fallacious. No, misstating my argument is fallacious. I've stated that having full side effect tracking is unimplementable, so I'd rather have a consistent lack of it than a hybrid approach.
The test program is here: https://github.com/erikd/wai/blob/master/warp/test/readInt.hs 
But if you are doctor of philosophy, it suggests that you studied a part of the field of philosophy extensively, does it not? Like if I am have a PhD in math/physics/compsci it means I studied it for a long time.
&gt; Citation required. Firstly, the statement you quoted from the `monad-control` documentation clearly is specific to particular operators, not an overarching principle behind the module Secondly, I've sent you this code before: import qualified Prelude import Prelude hiding (putStrLn) import Control.Monad.State.Strict import Control.Monad.Trans import Control.Exception.Lifted type M = StateT Int IO putStrLn = liftIO . Prelude.putStrLn test :: IO ((),Int) test = runStateT (do x &lt;- get putStrLn ("Prologue with state: " ++ show x) bracket (do x &lt;- get putStrLn ("Initializing with State: " ++ show x) put 2) (\ () -&gt; do x &lt;- get putStrLn ("Finalizing with state: " ++ show x) put 4) (\ () -&gt; do x &lt;- get putStrLn ("Doing with state: " ++ show x) put 3) x' &lt;- get putStrLn ("Epilogue with state: " ++ show x') ) 1 
&gt; But if you are doctor of philosophy, it suggests that you studied a part of the field of philosophy extensively, does it not? It does not. [PhD](http://en.wikipedia.org/wiki/Ph.D.) == "Doctor of Philosophy" That is every PhD (be it math, physics, compsci, etc). It's just a naming issue and doesn't carry much meaning. The common name for this is "Doctor of Philosophy". Suppose I created an improved Segway and called it "Unicorn on Fire" and everyone buy it and I become a millionaire and people start saying: Today I came to work on a Unicorn of Fire. You could come and say "how can that be, unicorns don't really exist.." but that's just a name people use for this vehicle. Same thing people use the name "Doctor of Philosophy" and it has nothing to do with the common meaning for philosophy as in r/philosophy. 
That's What She Said
Anyway, I agree that the original title was a bit hysterical and I've toned down the rhetoric a bit.
&gt; You'll end up with exponential complexity: every single function will need to be implemented for every single possible operations Mikhail has argued that you in fact only need to implement type classes for six or so primitive functions; they correspond fairly closely with primops. Of course, you still have to make sure they interoperate and define generalized versions of all of the composed functions, but that's a hard problem anyway, when you're putting lots of orthogonal features together. &gt; Instead of looking to a single definition to understand semantics of a specific transformers, or a single definition to understand the semantics of a specific operation, you'll need to look at each and every instance to be certain you understand what's going on. Arguably that means you shouldn't be writing a more concrete interface; judicious application of "You ain't gonna need it" helps reduce complexity. &gt; I've stated that having full side effect tracking is unimplementable Anyway, you can do this if you embed the pure effects in IO. But it's kind of grody.
Do you know the differences between CV and [HopenCV](http://hackage.haskell.org/package/HOpenCV)? I guess they are competing implementations. CV *seems* more mature at a quick glance though...
Seems like the best bet. I am yet to dare stepping into fiddling with the actual framework rather that just making a client using it yet though. :/ CV seems the way to go.
Later feed back then what you were probably looking for but it is working now.
Merging binary with cereal would also be nice.
&gt; Functor composition (Λ f g α. f (g α)) is encoded as follows: data FC f g α = FC { runFC :: f (g x) } Am I missing something or should that `x` be replaced with `α`?
I get that readInt64 fails the property on 2147483648 (and possibly others). *Edit:* 64-bit Intel OSX, 32-bit GHC 6.12.1.
&gt; `fromIntegral` is a generic function: `fromIntegral :: (Integral a, Num b) =&gt; a -&gt; b` AIUI, `Num` is an abstract superclass for numbers (I'm using OO terminology here -- what's the correct haskell term), and Haskell automatically converts it to whatever numeric type is required, "sniffing out" what the callee wants. If my understanding is correct, would it be possible to write a function `coerce` that converts any numeric type to the right numeric type? Also, since functions somehow "know" what the return type should be, how would you write a function that prints out what return type is expected? (Or is this not possible?) 
Yes, confirmed on 32 bit Freebsd with ghc 7.2.2 Fortunately, that implementation is there only for speed testing an actually isn't the one being used in Warp. 
Please have a look at the latest version of the test program which is here: https://github.com/erikd/wai/blob/master/warp/test/readInt.hs The version that Warp is currently actually using is readIntWarp. 
HopenCV is a low level wrapper, which is designed to be used through [cv-combinators](http://hackage.haskell.org/package/cv-combinators) package. Both this and the CV are partial wrappers that aim to for higher level interface for OpenCV. I think that cv-combinators has a nicer interface, but it is much more limited in scope. The story for having two of these is that I started mine sometimes around 2005 and was not allowed to publish it until around 2010, which was bit after that Noam Lewis presented his cv-combinators. The hackage version of CV is about to get an overhaul soon, so using the dev-branch on github is recommended.
Cool!
the postive prompts text made me lol. NSFW https://github.com/egonSchiele/twss.hs/blob/master/data/PositivePrompts.hs
Released on hackage: http://hackage.haskell.org/package/htrace and http://github.com/jkff/htrace Sorry for the self-PR, but I have good intentions - I hope that this tiny thing will be useful to people :)
Can Android decode H.264?
Are the slides (not the presentation view video) online?
Yes, I am sure. The problem seemed to be that the video got stuck on Android.
The explanation of the "vtable"/dictionary implementation of type classes was very helpful! Previously I thought, because of the use of the name "dictionary", that it would be something like inefficient method lookup in javascript or multi-method dispatch in Common Lisp.
Yackage: [correct](http://hackage.haskell.org/package/yackage). It's a tool originally written at my company (Suite Solutions) for managing our packages, and that my employers let me open source. There are quite a number of those btw, I have awesome employers :) submodules: Doh, fixed, thanks.
-r also works in busybox. Solaris non-GNU sed doesn't seem to have an option for that. The most portable solution might be an awk or perl one-liner.
I'll assume Haskell is used in some of the solutions.
yes - I was glad he gave that explanation as well! This has always been a bit confusing to me.
The answer is indeed no, unless you consider parametric polymorphism to be equivalent. However, parametric polymorphism doesn't let you do things like query the type of the item and then branch on that. Parametrically polymorphic functions have to be genuinely independent of the actual type of the argument.
If anything about that pops up, I'd like it as well.
No. I'm not too familiar with Racket, but in CL, a _type_ is a set of lisp objects, and the types then form a lattice, with `T` naming the top and `NIL` the bottom. These types are like the sets of a material set theory, such as ZFC. In type theory, there is a structural perspective. You cannot ask whether terms of two types are equal. Consequently, types do not contain one another. These are like the sets of a structural set theory, such as SEAR.
Typed Racket is a statically typed dialect of Racket with a type system similar to that of standard ML or Haskell, but consistent with the idea that it will interact with untyped Racket. So it isn't much like CL's types.
In this way, it is much closer to CL's than to Haskell's. The types of Racket are still sets of objects which form a lattice. I think Any and Nothing are the extrema, what CL calls AND is called U, etc. correct me if I am wrong. This is fundamentally different from normal type theory, where to ask if x=y requires that we know x:t and y:t.
Maybe you can use GHC's Show instance of ThreadID to make it work with concurrent programs - although this is really entering the unsafe unsafe Haskell regions ;) Anyway, looks like an interesting twist to the normal trace!
are you on ghc 7.2 then? That is just a testing library: you can remove it and the testing from the install script. Might be easier to try yackage.
It's called subtyping, and plain old type theory can handle it (though the particular type system of Haskell doesn't have subtyping).
_shrug_ Sure, and if you have subtyping, you can have a top type. My point is you should think of Haskell like SEAR and not like ZFC.
I don't think I can name clients publicly, but some of our biggest clients are running Haskell code on their production servers. A number of our clients have Yesod-powered services, and one of our clients is shipping the Warp webserver onto all of their customer's systems to create an embedded webhelp.
&gt; you don't need it, and type variables are better. Debatable. Try to avoid letting Haskell become your 'Blub' language.
I really know nothing about advanced shell scripting, I certainly don't know the history of the sed options, and I would have no idea of what the best way to solve this issue. All I know is that insulting Patrick, the guy who just fixed the bug, is not useful. Thank you Patrick for the quick turnaround.
what are you going to do with it that you can't do with a type variable?
The only thing I can come up with is allowing `_` as a type variable. I've seen cases where it was useful. *There is even [a feature request](http://hackage.haskell.org/trac/ghc/ticket/3699) for something similar.
haXe is actually functional, with first-class functions, algebraic types and a compiler written in OCaml (so we can suppose there has been some functional influence). Although not really related to functional programming, it also features static, strong and at least partially inferred and structural typing.
ghc-7.4-rc1. I'll try that.
I second this - I haven't seen a feel-good approach to GUIs just yet. But I've still got a soft place in my heart for Haskell - once you get over the initial hump it's cheap and cheerful with quite a bit of expressiveness, the way things should be.
Don Stewart seems to produce an endless quantity of feel-good presentations which are pretty good for making people enthusiastic about good engineering. It always works for me anyway!
I think this refers to the paper ["A functional programming technique for forms in graphical user interfaces"][1]. Note that my [reactive-banana][2] library now enables you to use functional reactive programming for GUI programming as well. [1]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.66.4498 [2]: http://www.haskell.org/haskellwiki/Reactive-banana
no need at all to apologize. it helps everyone when people advertise and advocate for their own code.
&gt; Haskell simply forgoes subtyping, so it's hard to explain in Haskell terms why Any might be useful, because quite frankly it has no meaning in a system that doesn't have subtyping. so if we make haskell a very different language, and complicate the inference process, then it'll be useful. check. 
What is the type of vectorOf, btw?
You need to pick an element type for your matrix. For example, sample (arbitrary :: Gen (Matrix Int))
Yeah, that works. :) But I would expect the above to work too since &gt; sample (arbitrary::Arbitrary a=&gt;Gen a) works fine. It produces units. '()' that is. **Edit:** &gt; sample (arbitrary::Arbitrary a=&gt;Gen [a]) Works too. And: &gt; sample (arbitrary::Gen (Matrix ())) It may be sufficient to use a specified type. It would be interesting to know why these one's works and not the one in the previous post though.
[vectorOf :: Int -&gt; Gen a -&gt; Gen [a]](http://hackage.haskell.org/packages/archive/QuickCheck/latest/doc/html/Test-QuickCheck-Gen.html#v:vectorOf)
&gt;People : Don't over engineer &gt;●Typed FP folks can be overly tempted by &gt;category-theoretic patterns: &gt;fmap (fmap (fmap (fmap … &gt;●With type inference, hides subtleties. &gt;●Can bite when refactoring – code will continue to &gt;type check, but is silently picking an unexpected &gt;instance &gt;●Take a lesson from startup land : build the &gt;minimally viable thing, before trying to build a &gt;“fully general” solution So true, it's nice to get "permission" to do this from an expert. I feel guilty writing code that is more monomoprhic that it could be, but then I get myself in performance/logic trouble jumping to a polymorphic/pointfree version before I have working simple version or two. Last week I wrote a function: fmap2D :: (a-&gt; b) -&gt; Seq (Seq a) -&gt; Seq (Seq b)) fmap2D = (fmap . fmap) :-) 
That's using extended GHCi defaulting rules; it wouldn't work in actual code, and I personally don't tend to like it in GHCi either.
haXe is an amazing idea, with the noisiest content-free home page I have seen recently. http://haxe.org/doc/intro should be the home page.
Good post; just a small question: &gt; Subtyping is, very roughly, a relationship between types such that if b is a subtype of a, then any expression of type b can be used in any context that requires an expression of type a. Does that make polymorphic types subtypes of their specializations? E.g., is `(forall a. Foo a)` a subtype of `(Foo Int)`? (The substitution isn't allowed when the polymorphic type would cause typeclass ambiguities, but maybe that doesn't count.)
the simon is on it http://www.reddit.com/r/haskell/comments/muq40/commit_base_master_add_tracestack_string_a_a/ https://plus.google.com/u/0/107890464054636586545/posts/XE4T6hHm3tK (there's a number of G+ posts about 7.4 profiling and stacktraces at above
Yes, polymorphism induces a subtyping relation. forall a. T(a) &lt; T(S) &lt; exists a. T(a) for any particular `S`. And various types should preserve the relation in the usual ways: S1 &lt; S2, T1 &lt; T2 =&gt; (S1, T1) &lt; (S2, T2) S2 &lt; S1, T1 &lt; T2 =&gt; (S1 -&gt; T1) &lt; (S2 -&gt; T2) But many type checkers (GHC's included) won't really implement this in full, so you'll have to write appropriate identity functions to make the instantiations work out. I expect it's not sufficiently restricted to be possible to fully implement good inference/checking for this.
New programmers should definitely be subjected to both at the same time, if this is at all possible. Or at lease some simpler functional language. Not only is it simply advantageous to learn both paradigms, for a greater understanding of how the machine works, but going down the road of C-style OO languages and trying to learn Haskell afterwards has been a major exercise in WTFery / how to unlearn everything I know about programming.
No, sorry.
Yup, I've been seeing your posts about reactive-banana on the subreddit for a while now. Seems interesting, but I'm as yet still a Haskell outsider (haven't read through lyah and rwh yet) so I'm afraid it's a little beyond me at the moment :) Thanks for the link to the paper!
I have a pre-release version of the new bytestring-lexing online at: http://code.haskell.org/~wren/bytestring-lexing/ With the modified benchmark at: http://code.haskell.org/~wren/bytestring-lexing/test/bench/readDecimal.hs Let me know how the benchmarks come out on your systems, especially if it's wildly different from the numbers I quoted here. Just as a usage question, how is the function used in Warp such that you can ignore the remaining bytes after the number? Oh, and feel free to email me rather than posting here. You have my email from the Cafe list, right?
http://alessandrovermeulen.me/2012/01/26/getting-rid-of-javascript-with-haskell/
Your current approach for working with existing libraries seems to be relatively reasonable. Working with existing libraries is still not nearly as comfortable as with GHC. This is due to the fact that cabal support for UHC with an alternative backend is somewhat limited. It would be great to get someone investigating how this can be improved. The HTML generation is very basic at the moment. It will generate HTML that includes all required JavaScript files and that's it. If you want to, say, import jQuery, you will have to add that to the &lt;head&gt; section yourself. It is recommended to create a new HTML file for this (copying the generated imports), since the generated HTML file will be overridden by on subsequent compilations.
Just in case, I'd like to mention that several lens packages already exist, like [fclabels][1]. [1]: http://hackage.haskell.org/package/fclabels
Time to join the cabal, then. ;-)
Yes. Adding 'Show a =&gt;' makes it work. Interesting. *Edit:* Another, not so surprising remark is that there will be no 'undefined error' since the matrix doesn't need to be evaluated, unless I switch 'show _' with 'show (M _)'. where I've declared Matrix as: data Matrix a = M (Array (Int,Int) a) Pattern matching the constructor will cause the matrix to be evaluated. Pattern matching is (almost?) the only thing that causes evaluation - a recently acquired understanding. :D
Simpler arbitrary = aM =&lt;&lt; arbitrary
Just to check, are you compiling with optimizations? Also, have you tried a specialize pragma.
Nice, thanks!
(With apologies for not properly reading this paper,) at [altjs.org](http://altjs.org/) there are 4 HS-to-JS projects, and the CoffeeScript project maintains a [page that lists yet others](https://github.com/jashkenas/coffee-script/wiki/List-of-languages-that-compile-to-JS). These are: * **code-generation backends** for Haskell compilers: [UHC](http://www.cs.uu.nl/wiki/bin/view/Ehc/UhcUserDocumentation#5_7_3_jscript_Core_based_JavaScr), [YHC](http://www.haskell.org/haskellwiki/Yhc/Javascript) and [GHC](https://github.com/pedromartins/ghcjs) * **New HS compilers** to target JS (IMHO gargantuan undertakings), including: [Lambdascript](https://github.com/aculich/lambdascript), a compiler of a subset of HS to JS, written in HS, and [JSHC](https://github.com/evilcandybag/JSHC), a Haskell compiler written in JS Someone in the know, is the UHC back-end considered the most promising project? Why? And is a GHC back-end (which would be more convenient for 99% of Haskellers) harder for some reason?
`example@example.com`/`example`
UHC — most feature-complete/usable JS compilation at present, GHC — works but not feature-complete, YHC — dunno, couldn't even build the compiler. I have [a fairly out of date comment](http://www.reddit.com/r/haskell/comments/j9nef/that_seems_like_a_challenge_whatever_happened_to/c2aas9k), on a previous discussion that might interest you. Maybe at this point it's worth making a HaskellWiki page dedicated to The JavaScript Problem.
I created [a wiki page](http://www.haskell.org/haskellwiki/The_JavaScript_Problem) for this problem and the existing solutions, we ought to add to it and refer to it when this question crops up. I just pasted in my existing list for now to kickstart.
I can think of one small exception to "Haskell doesn't have subtyping" - [Bottom (⊥)](http://www.haskell.org/haskellwiki/Bottom): &gt; Bottom is a member of any type, even the trivial type () So it's a very shallow hierarchy, in which all types are supertypes of {⊥}.
Wow! Nice!
More videos from the YOW! conference will be published over the next two weeks. The other SPJ video is scheduled to come out on Feb 9th.
Also the comment there by 1nine8four mentions CompCert, a verified C compiler for which no incorrectly compiled code could be found. I'm under the impression that haskell does not have formal semantics (unlike ML for example) but how is the ghc output verified for correctness? I've heard that haskell is used a lot in the financial sector, I imagine correct code is very important there, so is this an issue that they have to be aware of?
Ouch.
Probably not what you were thinking of, but have you seen [Yesod](http://www.yesodweb.com/)?
Formal semantics of a language and a 'verified toolchain' are different things. Formal semantics a la SML allow us to always unambiguously determine the meaning of an expression or statement, no matter the context. We may always follow the rules in order to determine what an expression denotes. No language implementation can deviate from it - otherwise it's not the same language. A compiler can still mis-compile that, however - that's just a bug. OTOH, a verified compiler has proofs that tell us that *no transformation it makes on the input program will ever change the semantics, no matter what they are*. It will never mis-compile a program. C does not have a rigorous formal semantics in the same way SML does. Neither does Haskell. It does have a specification and standard, but it is not 'formal' in the same sense SMLs' is - i.e. standardized denotational semantics. CompCert does have such a specification however AFAIK - but it's worth noting it can't handle *all* of C just yet (no duff's device or variations, var-args, or `double double`.) In general, "verification" - note the finger-quotes - is done realistically like any other compiler: lots and lots and lots of tests. Like all other compilers in existence (modulo compcert,) GHC has exhibited bugs that make working programs miscompile, crash and misbehave at runtime, etc.. (Side note: I've found GHC remarkably robust, personally, but I've had my share of miscompilations and compiler errors creep in from just about every single one out there.) In the case of GHC or Haskell, this isn't even *just* a problem of the compiler: the runtime system is its own beast. It is very possible for subtle bugs to creep in here, so even if you proved that every transformation in GHC is totally semantics preserving, there could still be, say, a garbage collector bug that causes it to follow invalid references, crashing your program[1]. That's a whole verification topic in and of itself. This is basically a fact of life for any toolchain that hasn't been formally specified and implemented in the way CompCert has. Verified toolchains are IMO very important. However, GHC in particular strikes a very delicate balance between research and industrial platform. Full verification is still a tedious process, even if it has made tremendous leaps in the past few years. I think that a verified Haskell compiler would be a very worthy project and laudable goal - but bringing it up to par with GHC for example, is a *totally* different ballgame, and GHC is basically the de-facto implementation for serious work in just about any sector of the Haskell community (for better or worse.) It's an accepted trade-off, given what we currently have. [1] You may enjoy this paper: http://web.cecs.pdx.edu/~apt/icfp10.pdf - A Certified Framework for Compiling and Executing Garbage-Collected Languages. It's actually built on CompCert!
(Sorry for the late response, just got back from POPL!) The ErrorT instance would be fixed so that (1) doesn't apply. This is pretty simple: you are only allowed to have MonadException on an ErrorT m if the underlying monad m had MonadException, and you implement the functions by composing the underlying catch functions and ErrorT functionality. I think IO'ifying some of the arguments is a pretty good way to go, but there some issues I haven't worked through yet. For one, it's less user friendly (I just don't know how much less). I think you swapped the monad for the release and the middle functions? If an exception is thrown in the body, the pure monadic side effects shouldn't be retained (you'd have a hard time implementing that anyway.) Apparently, it's well known that the IO monad should not be modeled as a state monad, but as a concurrent calculus (so, for example, putIORef sends a message to an IORef actor.) This is news to me!
Very nice talk, Don! Interesting to hear that you are working in Risk / Finance as well! I'm still unsuccessfully trying to sell at least F# to the folks at my bank (.NET shop), Haskell is still totally out of the question... So, are you hiring :-)?
Yeah, compiled with optimizations. What kind of specialize pragma do you mean? Where would I put it?
Interesting read, but I'm not sure about the statement &gt; The cost with Haskell is that pure, lazy, monadic is hard and slow and long-slogging (although it's proved remarkably solid.) ...compared to what? OCaml?
Accumulation is better done with the writer monad. Though in this specific case we don't need a monad at all. gTest (name, "Hello") = (Sum 1, [name]) gTest (name, "Bye") = (Sum (-1), [name]) gTest _ = mempty summed = first getSum . foldMap gTest But in less simple examples we may want to use the writer monad. sTest (name, "Hello") = tell (Sum 1, [name]) sTest (name, "Bye") = tell (Sum (-1), [name]) sTest _ = return () sAcc = mapM sTest list sSummed = first getSum $ execWriter sAcc
Probably Scala. 
I wonder where he found a Haskell cross-compiler for ARM... I believed the only current way to get ARM native bytecode was to run GHC **on** an ARM platform...
&gt; C does not have a rigorous formal semantics in the same way SML does. Oh, but it does! http://code.google.com/p/c-semantics/
That project has not been updated since June 2011 and only supports an old version of GHC. In general GHC on iPhone business is shrouded in mystery for mere mortals.
&lt;rant&gt; Sadly for comprehensions largely miss the point of monadic programming with do: Forcing you to put _ &lt;- before every line, not letting you build defs or vars, requiring a map (via yield) at the end rather than letting you end in an action, not allowing you to put a val at the front, so you wind up with a whole other { } block surrounding it to set up half of them. Then of course there is the problem that you need to trampoline every monad you define or it'll just blow the stack for any non-trivial problem, because flatMaps are by construction not self-tail-recursive, and scala can't do any other tail call optimization, there is no way for them not to. So you need to choose between crashing when sequencing a monad over a list longer than a thousand entries or the loss of an order of magnitude in performance. Every time I use the for sugar in scala I trip over ONE of these. I get a lot of work done in scala, but the language is very noisy. &lt;/rant&gt;
We all appreciate the great work you're doing, Michael. Keep it up!
He did say "now", and I agree that I'd be curious to know what the difference is. That post was written before conduits moved away from mutable state.
Thank you. I have fixed this. :)
i could be wrong but I believe the reason there haven't been updates is that the direction of ghc on the iphone is to take advantage of llvm 3.0 and ghc 7.4.1
Making an Int annotation cuts down the list comprehension method from 4s to 0.8s, and the do-notation version from 7.8s to 5.3s. So much more speedup for the list comprehension method, and still a large difference. 
Add some code examples, prose-ify the bullet points, and throw it on the blog, then. :)
Nice approach! Thanks for sharing.
I think I wanted to put too much in a single blog post. Sorry about that. It is still possible to write projector-per-constructor plates. The family GADT then needs a value per constuctor: data Fam a where ConW :: Fam Expr AddW :: Fam Expr MulW :: Fam Expr EVarW :: Fam Expr LetW :: Fam Expr AsgnW :: Fam Decl SeqW :: Fam Decl Another thing that changes is that you can't use the record update syntax to change one projector and keep the rest the same. That's what the `famIf` function is for. A big advantage is that working with plates generically becomes a lot easier; all the functions from the Multiplate library become one-liners. I don't think you can write expr2 with standard Multiplate.
thats awesome! congratulations!
amazon says March 22 http://www.amazon.com/Developing-Web-Applications-Haskell-Yesod/dp/1449316972/ ed: can't wait, i love writing in books in hiliter
Great! Would there be kindle edition?
amazon page says it is 100 pages?? sounds like a bound edition of the yesod docs
zomg! congrats!
It's a bound edition of the [Yesod book](http://www.yesodweb.com/book). It does not include the Haddocks or the wiki content.
Wow, there's way more content there than when I last checked. Thanks. I love Yesod and am looking forward to reading the book.
Consider fs :: Free (Store Bool) Char fs = Free (store f True) where f :: Bool -&gt; Free (Store Bool) Char f True = Pure 'a' f False = Free (store (\b -&gt; Pure (if b then 'x' else 'y')) False) Then we have that pos (f2s fs) = [True] but peek [False] (f2s fs) = (error "Pattern match failure in getPeek") 
Looks more like a moth to me.
The first tutorial they read was bad; the second one was good; now haskell makes sense? Looks like someone just found their first burrito. Up ahead: four more rounds of alternating between quitting in frustration and returning to the light. 
A simple approach is to hide the data constructors and only export "smart constructors": module Pronunciation (Consonant, makeConsonant} data Consonant = Consonant { voiced :: Boolean, aspirated :: Boolean } makeConsonant v a | v &amp;&amp; a = error "cannot be both voiced and aspirated" | otherwise = Consonant v a This way, no one can create an invalid value. Otherwise, I think the tri-bool solution is ok. You can make it more palatable by providing record-like "accessor" functions like this: data Consonant = Consonant VA data VA = Voiced | Aspirated | Neither deriving (Eq) voiced (Consonant va) = va == Voiced aspirated (Consonant va) = va == Aspirated This means that checking voiced/aspirated works the same as it does for the previous example: if voiced c then .... Of course, adding more options is more cumbersome this way, since you potentially double the number of constructors with each new option.
Your three-way split value is probably the best way to do it. Since you're not trying to develop a full phonological theory that is totally general, you can specialize to the "phonological theory of Chinese dialects" wherein there is a three-way split in distinctive features. This isn't as bad as you may think, tho, since non-voiced aspiration is a matter of voice onset timing: VoicedUnaspirated is just -VOT, UnvoicedUnaspirated is 0 VOT, and UnvoicedAspirated is +VOT. Voiced "aspiration" of the sort found in, say, Hindi, is not aspiration of the normal sort, at least phonetically. But like I said, you're doing a *phonological* model, of a restricted set of languages, so you should feel free to invent whatever feature system is most adequate for that set of languages, and in this case it's a tripartite distinction, exactly like you have.
Let's increase 1st derivative on S curve hoogle, GHCi and typeclassopedia are your friends, and Amazon has some good books that shd get more attention: G Hutton's, S.Thompson's, Bird Functional Pearls ------------ http://www.scs.stanford.edu/11au-cs240h/labs/resources.html http://acm.wustl.edu/functional/haskell.php http://haskell.org/haskellwiki/Category:Tutorials http://ezyang.com/haskell.html http://www.amazon.com/s/ref=nb_sb_ss_i_0_11/185-4841017-3908113?url=search-alias%3Dstripbooks&amp;field-keywords=haskell+programming&amp;x=0&amp;y=0&amp;sprefix=haskell+pro%2Cstripbooks%2C257 --------------- lastly: get whiteboard, make your own cheat sheet, put it up on flickr/github. http://news.ycombinator.com/item?id=3506463 
The principle problem with this approach is it violates the maxim of "Make Illegal States Unrepresentable" and you're giving up compiler checked correctness in favor of logically checked correctness. Its definitely necessary in a lot of situations but I'm not sure its necessary here and he/she loses a lot of the haskell advantage with this approach.
Glad to see there's some work being done on an OpenCL backend, I'd find that a lot nicer to work with not having an nvidia GPU anymore.
Phantom Types / GADTs are great to check these kind of properties at compile time. Here's an example from 'http://www.haskell.org/haskellwiki/GADT'. The List type has a "marker" if it's empty and the safeHead function can only be called with a nonempty list. Calling safeHead with an empty list gives you a type error. data Empty data NonEmpty data List a b where Nil :: List a Empty Cons:: a -&gt; List a b -&gt; List a NonEmpty safeHead:: List a NonEmpty -&gt; a safeHead (Cons a b) = a Didn't test the following: data Voiced data Aspirated data Consonant a = Consonant where VoicedConsonant :: Consonant Voiced AspiratedConsonant :: Consonant Aspirated doSomething :: Consonant Voiced -&gt; ... doSomething c = ... doSomething2 :: Consonant Aspirated -&gt; ... doSomething2 c = ... 
I always forget about phantom types. I've spent too long not writing Haskell...
Well, GHC 7.0.3 has "just" 142 extensions ;).
While not a solution to your concrete problem, as your domain is linguistics, did you have a look at [Grammatical Framework](http://www.grammaticalframework.org/)? You get dependent types with it, and as we all know all problems vanish if you throw enough dependent types at it.
This seems like a good lead to a question I've been pondering in the back of my mind as a beginner. How can I "mark" that an array is already sorted? I want to define a function that only operates on sorted arrays, but at the same time I don't want my marker to prevent the sorted array from being used by standard array functions. Edit: I meant lists but the solution is probably applicable to both. I want to avoid redefining things like head, tail. I'd like to be able to call the same sort on a typical, unsorted list, as well as an list that's already sorted and is marked as such. Am I asking too much? Thanks all.
Perhaps like this: sort :: Array a Unsorted -&gt; Array a Sorted sort array = ... getElem :: Int -&gt; Array a b -&gt; a getElem idx array = ... 
Actually, you don't need to enable any language extensions - phantom types work out of the box with GHC these days.
*Am I asking too much?* I think so. To be able to mark a list you need a new type and this type isn't compatible with the list type. That would only be possible if head/tail would use some kind of type class instead of the list type, which defines the used interface. Then you would be able to make an instance of this type class for your list type. 
 $ ghc --supported-languages | grep -v ^No | wc -l 72 
That's what gentooers call a "meta package". The code is in the packages it pulls in...
Yes. As I understood it, it is the most mature openCV binding. The github version is quite ahead of the cabal release too, so that one is probably the better choice. &gt; git submodule add 'https://github.com/aleator/CV.git' lib/cv :)
I think you'll find wrapper types with smart constructors to be useful. Here I'm creating naturals as a subset of the integers. module Nat (Nat (), nat, fromNat) where newtype Nat = Nat Integer Deriving Eq, Ord, Show nat :: Integer -&gt; Nat nat n | n &gt;= 0 = Nat n | otherwise = error "Tried to make nat out of negative number" fromNat (Nat a) = a instance Num Nat where (Nat a) + (Nat b) = nat (a + b) ... The module definition exports the type Nat, but not the constructor Nat. So the only way other modules can make a Nat is to use the nat constructor, which checks that the integer is greater than zero. Anyplace you come across a Nat you can be sure it wraps a non-negative number.
... which of course is rather pointless with Cabal unless you're re-exporting their modules. I guess if you're just playing with cabal install though ...
You're version would allow the sorting of already sorted arrays.
Half expected this to end by replacing State with the reverse state monad and disappearing the problem in a puff of magic.
Did you meant something like this: data MarkedList a = MarkedList [a] deriving (Show) liftML :: ([a] -&gt; [b]) -&gt; MarkedList a -&gt; MarkedList b liftML f (MarkedList ls) = MarkedList (f ls) *Main&gt; liftML (map (*2)) $ MarkedList [1,2,3] MarkedList [2,4,6] *Main&gt; liftML tail $ MarkedList [1,2,3] MarkedList [2,3] Yes, that would make the handling of MarkedList easier :). 
This is more what I had in mind: ([a] -&gt; [b]) -&gt; MarkedList a -&gt; b There is no guarantee that (f ls) is sorted. I just wanted to use the compiler to enforce my assumption that the input list is sorted. TYVM.
Well, portage 2.2 has sets, which fixes things. Though considering the development history of portage 2.2, I sometimes fear we've got another HURD at hand. Hmm... does cabal support deprecating a whole package? That could be a way to stop people from depending on meta packages.
I thought I saw it here before with a title having something to do with lenses. Anyways, this is all great stuff. Acid-state, in particular, is particularly amazing. I took a look at IxSet when I was starting my current project and didn't quite grok it, so ended up rolling my own stuff using a combination of mostly Data.IntMap and Data.Trie. I shortly plan on figuring out how lenses work as my update code isn't half as pretty as my query code. I feel like my NIH syndrome is reaching terminal status when I find myself reimplementing basic parts of RDBMS systems but it's great to be able to quickly build a persistence layer with exactly the data structures you want.
I welcome any competitor to IxSet. IxSet gets the job done.. but there is certainly room for improvement. Of course, one nice thing about acid-state is that you are not locked into using any particular data-types. So that makes it a lot easier to experiment or to use specialized types. Another interesting alternative is HiggsSet, http://hackage.haskell.org/package/HiggsSet
Would it be possible to post some reasonable benchmarks as well? It would be helpful to understand where performance lies vs. (semi-) alternatives like Redis.
This is a new position which is open to students of any nationality, i.e. worldwide. As noted in the advert however, non-UK students will require additional funding to cover tuition fees, whichh is awarded by the University on a competitive basis. There is no need for applicants to apply for such funding at this stage in the process. If you would like to apply please follow the instructions in the advert, rather than clicking on "apply online".
Without having confirmed it, my guess would be that acid-state performs better as it usually runs in the same process as the application and there's no need for marshalling values (at least for queries).
If my acidic state has a lot of (unobservable) sharing in it, it is reasonable to never checkpoint to ensure that when the state is reloaded that sharing is restored?
Sadly `ixLens` isn't a lens since it doesn't satisfy the lens laws (which for some reason are not documented edwardk!). As `addUserId1'` illustrates at least two lens laws fail: getL l (setL l nv r) = nv setL l nv2 (setL l nv1 r) = setL nv2 r 
I've been meaning to take a closer look at partial-lens as lenses into Maybe (like ixLens and mapLens) are sort of cumbersome with plain data-lens. Any plans for a MonadState API for partial-lens?
What you're looking for is "monad transforms" I haven't been doing Haskell long enough to have much experience with these, but at least you know what to google now. The library is mtl ("monad transforms library"). In your case, I think you need lift. So if msg is of type Maybe String then main :: IO () main = do msg &lt;- boxIt "herpin' and a derpin'" unboxed_msg &lt;- lift msg putStrLn unboxed_msg should work. I think. Edit: Formatting of the code.
It depends on how big your transaction log gets. If you never checkpoint, it will take longer and longer to start your application because the whole transaction log has to be replayed from the last checkpoint forward.
I think that `maybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b` is underutilized. maybePutStrLn :: Maybe String -&gt; IO () maybePutStrLn = maybe (return ()) putStrLn main = do let msg = boxIt "blah blah" maybePutStrLn msg
They need to work for all `Ix`s, not just `empty` and `fromList [Person fname lname]` (and also need to work forall `ixLens field`).
&gt; Any plans for a MonadState API for partial-lens? I have very vague plans; but if you really want it done anytime soon you could copy `data-lens-fd` and adapt it to `partial-lens-fd` in the (hopefully) obvious way. After all, `partial-lens` essentially a modified version of `data-lens`.
Essentially, the failures I had before [this commit](https://github.com/dag/data-lens-ixset/commit/56027bd30d541722382fefb632b44232340873a5) were right? I was thinking that if you *only* ever use ixLens, the situation can not arise. But perhaps that is insufficient to be a legal lens?
Even if you only use `ixLens`, you can still build `Ix`'s that will illustrate the failure of the lens laws. In particular, `fromList` should be definable using `ixLens`.
Are the Lens Laws you're referring to outlined [here](http://www.scs.stanford.edu/11au-cs240h/notes/zipper.html#how-should-lenses-behave)? Those make sense, but is there a theoretical basis for those laws? I've been thinking about this stuff this past week, and wondering how to express properties of lenses that can fail or are polymorphic in some monad, etc. Interestingly I just posted a really rough lens "roundup" [on my blog](http://brandon.si/code/haskell-state-of-the-lens/) last night, and would love if people want to weigh in with whatever is on their mind.
 main :: IO () main = case boxIt "herpin' and a derpin'" of Just msg -&gt; putStrLn msg Nothing -&gt; putStrLn "Oh no, Mr. Bill!" 
There is this guide to using VirtualBox to get Arch Linux/Haskell up http://www.purelazystatic.net/2011/12/virtualbox-arch-linux-xmonad-emacs-pt-1.html
For this simple example, the `MaybeT` can work, but it's probably bad practice to completely short circuit `main` every time you hit `Nothing`; usually you will want to do something nicer like print an error message, let the user try inputting again, or something like that.
Thanks!
I use GHC on Windows and it works just fine. The only package that's slightly tricky to install is 'network' (but you just need an MSys shell for that; I use the one included with Git for Windows). I also don't run cabal with administrator rights, so there's no way it could corrupt the installation (i.e. I install all packages into the user directory). The problem with upgrades breaking dependencies is annoying, but I think it's not specific to Windows.
So MinGW/MSys is better than Cygwin for the Haskell dev environment? I hadn't used or been very aware of MinGW before, but I noticed that Haskell Platform ships with some MinGW content. MSys shell is the solution for the "undefined reference to `acceptNewSock' " error?
If you can compile network using Cygwin, then you probably don't need MSys. Which version of network did you try? I just tried compiling network-2.3.0.9 and it fails, but 2.3.0.8 compiles fine.
Note that we do expect the next major release of the Haskell Platform to be based on GHC 7.4.x (yes, for various reasons we skipped the 7.2.x series). Edit: oh, and credit to my collegue Ian Lynagh for getting the release out!
And here's the [GHC 7.4.1 Announcement](http://www.haskell.org/pipermail/glasgow-haskell-users/2012-February/021764.html)
I would love to see benchmarks as well. Someone needs to take the initiative though and do it :) Would be nice to have something to brag about... For me, the big win in using acid-state has been the time savings as a developer .. which is harder to benchmark. 
I'll have to check when I get home. On my machine where I am now, if I add Haskell Platform's mingw gcc to my PATH (a step I just realized and I haven't tried at home yet), yes, I can * fail to `cabal install install network` (2.3.0.9, today's default) * `cabal install network-2.3.0.8` (and a ton of dependencies it slurped in, I stopped it before it finished) * `cabal install --reinstall -p network-2.3.0.8` * `cabal install --reinstall -p aeson attoparsec blaze-builder hashable primitive unordered-containers utf8-string scion-browser` (that took a bit some iterated dependency chasing) * except that at the very end of all that, after `network` installed, but before it was actually used, after building all the dependencies except scion-browser itself, Building scion-browser-0.2.6... [ 1 of 20] Compiling Scion.PersistentBrowser.FromMissingH ( src\Scion\PersistentBrowser\FromMissingH.hs, dist\build\Scion\PersistentBrowser\FromMissingH.o ) [ 2 of 20] Compiling Scion.PersistentBrowser.TempFile ( src\Scion\PersistentBrowser\TempFile.hs, dist\build\Scion\PersistentBrowser\TempFile.o ) [ 3 of 20] Compiling Scion.Packages ( src\Scion\Packages.hs, dist\build\Scion\Packages.o ) [ 4 of 20] Compiling Scion.PersistentHoogle.Util ( src\Scion\PersistentHoogle\Util.hs, dist\build\Scion\PersistentHoogle\Util.o ) [ 5 of 20] Compiling Scion.PersistentBrowser.DbTypes ( src\Scion\PersistentBrowser\DbTypes.hs, dist\build\Scion\PersistentBrowser\DbTypes.o ) Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. [...lots more packages...] Loading package network-2.3.0.8 ... linking ... ghc.exe: C:\Users\cultic_raider\AppData\Roaming\cabal\network-2.3.0.8\ghc-7.0.4\HSnetwork-2.3.0.8.o: unknown symbol \`_acceptNewSock' ghc.exe: unable to load package `network-2.3.0.8'` So that's the sort of problem I have on Windows. 
&gt; - The Num class no longer has Eq or Show superclasses. &gt; - There is a new feature kind polymorphism &gt; - There is a new feature constraint kinds &gt; - DataKinds I feel like: 1. Every GHC release is Christmas. 2. [This](http://image.shutterstock.com/display_pic_with_logo/237823/237823,1298557816,3/stock-vector-mad-scientist-laughing-insanely-by-his-laboratory-desk-71865166.jpg). &gt; You can now build GHC with Alex 3.0. Does this mean that the horrible Unicode hacks in the parser are gone? Or is that yet to be done?
At least under Chrome, the `title` attributes on the `div`s show up as tooltips whenever you mouseover them. This is quite annoying :) Not sure who has control over the page HTML...
I tried to reproduce your problem, but for me scion-browser-0.2.6 compiles fine. I used: GHC 7.0.3 (and the included MingW gcc) and network-2.3.0.8 and the MSys shell from Git. So unfortunately I have no idea why it doesn't work for you. 
Well, I thank you for trying. I wasn't really looking for specific debugging of my troubles, but instead asking for guidance for what exactly other folks do, that I can copy. I appreciate the advice about MSys, I'll give that a whirl when I have a chance. Edit: MSys didn't help. That gives me the same error as running under bash shell in Cygwin: configure: error: C compiler cannot create executables See `config.log' for more details. Edit: Hallelujah! I got gcc to work under cabal by: * Uninstalling everything gcc-related from Cygwin (except for the required libgcc1), * deleting the bogus cygwin/bin/gcc.exe that Cygwin left as litter * and adding Haskell Platform's mingw/bin dir to my path. Now I can get to recompiling all my packages with "-p" for profiling. Thanks for your advice. The part about config.log appears to be a joke, because cabal doesn't tell me where its working dir is that contains config.log. It's not in under the cabal\logs directory. 
I think what GP's question comes down to is how does the submission's approach compare to QuickCheck's use of the Coarbitrary class to generate functions. I.e., this bit of QuickCheck: -- | Used for random generation of functions. class CoArbitrary a where -- | Used to generate a function of type @a -&gt; c@. The implementation -- should use the first argument to perturb the random generator -- given as the second argument. the returned generator -- is then used to generate the function result. -- You can often use 'variant' and '&gt;&lt;' to implement -- 'coarbitrary'. coarbitrary :: a -&gt; Gen c -&gt; Gen c instance (Coarbitrary a, Arbitrary b) =&gt; Arbitrary (a -&gt; b) where arbitrary = ...
&gt; The Num class no longer has Eq or Show superclasses. So this is less painful and more important than making functor a superclass of monad? Here's [the proposal btw](http://www.haskell.org/pipermail/haskell/2000-October/006147.html)
Because there's possible Num instances with undecidable equality, which currently has to be fudged, and Show is a pointless requirement in any case.
Yes.
So that if you create your own data type, and you want it to be an instance of Num, you are not forced to make Eq and Show instances for it as well.
Ahem, I think you mean point*free* requirement.
Yes it's less painful because for code that used to be valid with just a Num constraint but fails to compile now, you need only add Eq or Show constraints to a type signature. Afterall, you must already have those instances since they were required to write the Num instance previously. In the case of adding a Functor constraint to the Monad class, you will need to add instances. The code for the instances is hopefully trivial given the monad instance, but the size of the change is both a) more code than just adding Eq/Show constraints, b) more common. I would say adding the Functor constraint is far more disruptive. Hopefully we'll get there eventually. If someone made a tool to "upgrade" the code by providing the missing functor instances perhaps the proposal would get more traction?
Hurrah, Hurrah, Hurrah!!
Played nice with [MoboPlayer](https://market.android.com/details?id=com.clov4r.android.nil&amp;hl=en).
I'm being picky here... A claim that this chooses fairly from the set of computable functions is certainly false, as is any approximation to such a claim. In fact, this particular example chooses *unfairly* from a rather ugly and *finite* set of functions with no clean definition: those that arise from random number generator seeds and that particular arrangement of split/random function applications. Perhaps you say we ought to ignore the imperfections of the random number generator and assume that each use of random is choosing fairly and independently between True and False... fine, but then this would choose between ALL functions, computable or not! Yes, that is impossible, but it's not correct to throw the word "computable" at the answer and pretend it's all okay. The contradiction is resolved by precisely the imperfections of the random number generator. Once you drop the false assumption of a perfect generator, it's clear that there are in fact only finitely many choices -- StdGen uses a fixed size state -- and it would be a real miracle if the map from random number states to functions were *really* one-to-one, so in fact some of this finite set of possibilities are probably much more likely than others. In practice, it probably looks random and pretty arbitrary though. I just think it's probably better to make the stronger claim that it chooses from all such functions -- and then qualify with a modulo the quality of the PRNG -- than to make a claim that's true *neither* with an ideal *nor* real PRNG.
There is also the [Test.QuickCheck.Function](http://hackage.haskell.org/packages/archive/QuickCheck/2.4.2/doc/html/Test-QuickCheck-Function.html) moule which creates functions that are showable.
instance Functor M where fmap = liftM instance Applicative M where pure = return; (&lt;*&gt;) = ap Why is adding trivial code at the top level "far more disruptive" than adding trivial code to type signatures? It might be more code and more common, but it doesn't look like magnitudes of difference to me.
 ghci&gt; let nan = 0/0 ghci&gt; nan == nan False Also: ghci&gt; let minus_zero = -1 / (1/0) ghci&gt; 0 == minus_zero True ghci&gt; 1 / 0 == 1 / minus_zero False so floating point equality doesn't satisfy Leibniz's law either. Removing the Eq instance for Double (and offering a non-overloaded rawEq :: Double -&gt; Double -&gt; Bool) wouldn't actually be such a bad idea. It would remind people to be careful without being that cumbersome.
I have to agree. I it's rare for a given project to define more than one or two monads (and giving them Functor and Applicative instances is already good style), so the adaptations should be not only completely mechanical, but also extremely local.
Drop those parentheses around xor =).
A process is underway for improving records on the ghc-users mail list and ghc wiki.
&gt; GHC will now reject a declaration if it infers a type for it that is impossible to use unambiguously. &gt; [...] &gt; foo :: forall a b. Wob a b =&gt; b -&gt; [b] Why? It should be perfectly possible to say bar :: forall b. Wob Int =&gt; b -&gt; [b] bar = foo shouldn't it?
This is insane but cool.
* I think it's a good practice to use explicit import from modules. Like: import Data.Bits( xor ) import Data.Char( toUpper ) * You can see if it's possible to use `foldl'` instead of `foldl` http://www.haskell.org/haskellwiki/Foldr_Foldl_Foldl' * Why you define sum, instead of use standard sum? * Add signatures to better reading of code. * You can use `inputString:_` instead of `inputString:args`, because args name is not needed. Compile with -Wall to see warnings and use hlint, you'll get lots of advices.
You could even inline the whole thing, if you feel so inclined main = xor 0x444C . sum . map (ord . toUpper) . head &lt;$&gt; getArgs &gt;&gt;= print Though I'd probably stick with at least abstracting `getKey` into its own function, as you have done. main = getKey . head &lt;$&gt; getArgs &gt;&gt;= print or alternatively main = getArgs &gt;&gt;= print . getKey . head `&lt;$&gt;` is in Control.Applicative, and is a synonym for `fmap`.
But for 99% of the use cases, doubles will never be NaN. NaN is usually an error condition, the most sensible thing is to consider it as such.
Actually, I commented on that post, but I'll repost my remarks here. You can actually implement that behavior without even rewriting the Pipe type. You instead define a type synonym (or newtype): type PipeG a b m r = Pipe (Maybe a) b m r Then you define a "guarded composition" in terms of lazy composition: -- haven't settled on the composition symbol yet f &lt;|&lt; g = f &lt;+&lt; (yieldMap Just g &gt;&gt; yield Nothing) ... where yieldMap is just a helper function that applies the "Just" to every yielded value of "g". And the identity pipe in the category of PipeG is just: id :: PipeG a a m () id = do x &lt;- await case x of Just a -&gt; yield a &gt;&gt; id Nothing -&gt; return () The reason I like this approach is that it is much easier to reason about and prove that it satisfies the category laws. I was talking to Paolo about it and the final approach will probably still bake this behavior in for performance, but you can still reason about it as above. Using this guarded pipe composition, you can trivially implement lazy behavior with resource management. The only disadvantage of guarded composition is that the return type is constrained to `()`.
Even ignoring NaN, I'm sceptical of the utility of a Map with Double keys, considering it's all about direct equality comparison of floating-point values.
&gt; So this is less painful and more important than making functor a superclass of monad? The issue here is that we don't want to have an upheaval every time someone discovers/defines a new type class that sits somewhere between Functor and Monad. We need the technical tools to add such intermediate levels without disrupting user code. 
The fact that `(/)` doesn't act as you expect it to on Doubles does not mean that the Eq instance for Doubles is broken; at most, it means the Fractional instance is. (The Eq instance *is* broken, but because of NaN, not division.)
While I agree that the changes are local, they can still be annoying. I had to fix half of Hackage before the 7.4 release so at least the most commonly used packages would build.
The Docbook XSLT has control over the page HTML. This is an annoying fact of many a documentation.
thanks, will retry soon.
I've been working on ImplicitCAD for a few months now. I'm quite pleased with where it has gone, but I don't really interact with many other haskellers in Real Life. So I'd be really interested in hearing thoughts about ways to improve code quality, etc. In particular, I think it is important that ImplicitCAD be easily extendible for a new comer -- I've taken particular pains for files like Graphics.Implicit.ExtOpenScad.Primitives to be friendly. Also, is my use of unicode a big issue? I think it makes the code a lot nicer... I'd also be interested in thoughts about improving performance. It's tolerable, but faster is better! EDIT: And of course, thoughts about the (ext)openscad interface or Haskell API are extremely welcome! 
I find the simplicity of Pipes to be very attractive, and this is a great extension to the power of Pipes without complicating them too much.
Why egregious? Heck, it's the most compatible with the standard out of all of the changes.
The identity pipe should be: id = do x &lt;- await case x of Just a -&gt; yield a &gt;&gt; id Nothing -&gt; return () Your id pipe doesn't work because it doesn't terminate when it receives a Nothing. Guarded composition works precisely because the only way a pipe can send a Nothing upstream is by terminating. The identity GPipe must terminate in order to forward the termination upstream. If you used my identity pipe in your example, both would give the same behavior, namely printing "B". Your second example didn't work because your identity pipe swallowed the Nothing without terminating so it awaited another value and consequently brought down both itself and p. Had it properly terminated upon the first Nothing, then it would have given p a chance to output the "B". And you are right about yieldMap. It's easier to just replace it with `pipe Just &lt;+&lt;`. As far as swallowing return values, you are right that you can't generate folds if the return value is constrained to `()`. Let me think about it.
Yeah, I see your point now. In fact, this is remarkably similar to the dilemma I noted in my documentation tutorial for `toList`. Any attempt to detect upstream termination fails associativity when you insert a non-terminating Pipe in-between, violating the Category laws. I need to build up a solid intuition for why termination-interception seems to conflict with the Category instance. I'll reply with another comment after I've thought about it a little bit.
This looks excellent. Everytime I use OpenSCAD I dream of having a CSG language embedded in Haskell because from a language design standpoint OpenSCAD is pretty dire. You should give more prominent mention to [your blog post](http://christopherolah.wordpress.com/2011/11/06/manipulation-of-implicit-functions-with-an-eye-on-cad/) that explains the theory behind your implementation of the CSG models. I found it very interesting.
D'aww, thanks! And you should play with it. More specifically, you should get excited and make things!
Ok, here's another idea I haven't yet fully vetted. So, I basically agree with your overall premise that `await` has to be modified in order to be able to gracefully intercept shutdowns. What do you think of changing the `Await` constructor to: data Pipe a b m r = ... Await (a -&gt; Pipe a b m r, Pipe a b m r) The first value is the normal continuation and the second value is the continuation if the upstream pipe terminates before yielding a value. Note that it need not be a finalization routine. It's just a fall-back set of behaviors. Then you can define a few primitives: tryAwait alternative = Await (return, alternative) done = forever $ return () -- utility function await = tryAwait done -- give up if there is no value From those two primitives you can then implement any kind of finalization behavior you like and you preserve return types. So, for folds you would just do: toList :: (Monad m) =&gt; Consumer a m [a] toList = do x &lt;- tryAwait (return []) (x :) &lt;$&gt; toList For resource management you would do: read1 = do x &lt;- tryAwait (finalize h) finalize h Of course, I still need to think of a better way to automatically tag the finalizer to every await statement within a block so you can simulate "finally" behavior. I'm still not sure if I can modify the Await constructor as above and still have composition satisfy the Category laws, but what do you think so far?
how fast is it compared to cgal? (to clarify: how fast is it to perform a series of csg operations on primitives and then produce the boundary representation?)
I was hoping this would be an alternative typesetter.
I haven't done any serious benchmarking... Just a bit of comparison with rendering the same objects in OpenSCAD (which uses OpenCSG and CGAL). The results vary depending on the object: sometimes I'm faster, sometimes we're about the same, and, most often, they're faster, often by a lot. Performance is definitely something I want to work on. Right now I have very strange performance characteristics because I'm using two very different algorithms for rendering, one of which is much faster but can't handle a lot of cases (so I fall back to the slower one). Any objects with CSG at the top use the slower one if you want to compare them... For now, if you need speed, my library is probably not the right thing for you :(
When are we gonna get a new cabal-install?
&gt; For now, if you need speed, my library is probably not the right thing for you :( dunno, do you guarantee termination? pretty sure cgal doesn't. certainly acts like it doesn't.
[What every programmer needs to know about floating point](http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html) I have seen lots of dubious code because floats do not do what people think they should do. For your scenario try [Decimal](http://hackage.haskell.org/package/Decimal-0.2.2). I cringe when I see programs handling things like money using floats. Floats are approximate. I don't want my money handled approxmiately.
That `Await` constructor is exactly equivalent to Await (Maybe a -&gt; Pipe a b m r) so I think that works. Note that your version of `tryAwait` (which I'm going to call `tryAwait'`) can be implemented on top of mine, and vice versa. tryAwait' alt = tryAwait &gt;&gt;= maybe alt return tryAwait = Just &lt;$&gt; tryAwait' (return Nothing) I don't like your `await`, though, since it just returns bottom instead of terminating. If you define `await` like this: await = tryAwait' discard then, using the composition defined in my post, you can have something like: finally :: Pipe a b m r -&gt; Pipe () b m r -&gt; Pipe a b m r finally main finalizer = main &gt;+&gt; finalizedId where finalizedId = do x &lt;- tryAwait case x of Nothing -&gt; pipe (const ()) &gt;+&gt; finalizer Just x -&gt; yield x &gt;&gt; finalizedId
You need some basic knowledge about floating point numbers to be able to use Double properly
That right there is a damn useful thing to know !
I often [ask HLint](http://ompldr.org/vY200cg) for some advice.
1) Arithmetic in Haskell is simple: operators don't coerce values of one type to another type, ever. So `Int` stays `Int` and `Double` stays `Double`, one might say `Num` types are closed under their operations :). You can read this in the type signature of the operators: (+) :: Num a =&gt; a -&gt; a -&gt; a This can be read as "Given a type `a` that is an instance of `Num`, `(+)` is a function that takes two `a`s and returns an `a`." 2) The hierarchy is a little annoying (and doesn't make sense, to be honest) but Fractional, Real, RealFrac and other things are type classes, whose instances include the types you are often concerned with. The type classes define the operations one may perform on instances of the type class. For example, (+) is something implemented in the instance of `Num Integer`, i.e.: the Integer instance of the `Num` type class. The thing to remember is that if you want to write generic code, you can write their types like this: square :: Num a =&gt; a -&gt; a square x = x * x And this will work for any instance of `Num`, which is nice, because `Double` and `Int` and `Integer` and many other types are instances of `Num`. But at the same time, if you put in a `Double`, you will always get a `Double` back. The type clearly says that no matter what `a` is, you get `a` back. But you need to be careful, because `Num` only defines a few properties, the arithmetical ones you are likely concerned with are: `(+)`, `(-)`, and `(*)`. Note the lack of division. Integers don't divide cleanly and there are many "ways" to divide integers (round up, round down, banker's rounding, &amp;c.) and given this ambiguity, the `Num` type class was not given a division operator. There is a function "div" which has type `Integral a =&gt; a -&gt; a -&gt; a`, that's what you want. There is a type class that defines `(/)`, but it is for instances of the type `Fractional`, i.e.: `Fractional Double`. There is no `Fractional` instance for `Int`, and hence, you cannot use the `(/)` operator. Just like in other languages, one ought to avoid conversions as much as possible, and explicit types should be used when attempting to describe the functions involved. icoc should be declared as a type that meets your precision requirements. Double? Rational? Decimal? And a wrapper should exist around the output of `coc` and `doc` converting them to the appropriate types. So, if you are unconcerned with the division throwing away the fractional part of your result, replace `/` with "div" in infix notation, i.e.: rationalDefector h = let e = 0.01 icoc = ((coc h)+e) `div` ((coc h)+(cod h)+2*e) idoc = ((doc h)+e) `div` ((doc h)+(dod h)+2*e) The backticks around the function make it infix, so that it operates like any other infix operator does (such as `(+)`).
Conversion between types in Haskell is designed so that if you want to shoot yourself in the foot you have to manually load the gun first. Meaning, Integers, Doubles, Ints, Floats, and other numeric types have different properties, overflow differently, and act differently under some operations (like rounding). So unlike languages that will coerce for you, Haskell checks your types and yells at you where you could possibly have a problem, even if you don't. However, converting types is easily done, you just need to [explicitly do it.](http://www.haskell.org/haskellwiki/Converting_numbers) edit: You probably also want to use the % operator instead of /; [see here](http://www.haskell.org/haskellwiki/Generic_number_type)
Sure, `/`, given two Doubles, returns a Double. If you're looking for `Int -&gt; Int -&gt; Double`, then try `intDivDouble a b = fromIntegral a / fromIntegral b`
It's called (/); Double is an instance of Fractional too. Class Hierarchies in Haskell aren't the same as in other languages; a class is more like an instance specification (which can optionally require other interface specifications), and each concrete type can choose to implement or not any classes it wants to. [Here](http://academic.udayton.edu/saverioperugini/courses/cps343/lecture_notes/images/typeclass.png) is a graph of the numeric class hierarchy in Haskell; you'll note the absence of any concrete types! You can use :info in GHCi to find what classes a concrete type implements: Prelude&gt; :info Double data Double = GHC.Types.D# GHC.Prim.Double# -- Defined in GHC.Types instance Enum Double -- Defined in GHC.Float instance Eq Double -- Defined in GHC.Float instance Floating Double -- Defined in GHC.Float instance Fractional Double -- Defined in GHC.Float instance Num Double -- Defined in GHC.Float instance Ord Double -- Defined in GHC.Float instance Read Double -- Defined in GHC.Read instance Real Double -- Defined in GHC.Float instance RealFloat Double -- Defined in GHC.Float instance RealFrac Double -- Defined in GHC.Float instance Show Double -- Defined in GHC.Float
Nope. There's no way to tell the inferencer that you meant to use the Wob Int b instance you got passed in when bar calls foo.
As long as the GHC programming language continues to be awesome, I'm happy to continue programming in it.
This is a bit disingenuous; c++11 added type inference, anonymous functions including precise specification of variable capture (which matters a ton in a language with mutable variables), and move semantics (parallels to linear logic), among many other smaller changes. Now, given that c++11 was supposed to be c++0x... GHC releases every year and yet manages to continually blow me away with new and awesome stuff. THAT's the impressive thing.
&gt; The answer to 'why can't it just work' is tricky. Most languages with numeric coercion don't have type inference; that is, you say "int x = ..., double y = ...; return (x+y)". But given `x :: Integral a =&gt; a y :: Fractional a =&gt; a` &gt; &gt; what type should (x+y) have? I would say that in a language that does fractions, an int plus a fraction should be a fraction (trying to keep exactness as long as you can). IIRC that's how Smalltalk does it. 
&gt; So the answer here is, if you want the entire calculation to be done using Double, why aren't all of your inputs Double? They are now, but the `coc` etc functions do counting so it's natural that they would return an integer. Having them return a double obfuscates the code.
&gt; OK, what's the function that'll convert any numeric type to a double? `realToFrac` (`fromIntegral` will work too for `Int -&gt; Double`, but `realToFrac` is more general given that the destination type is `Double`).
I recommend `fi = fromIntegral`...
You miss that his function is still a function on numbers, it's just also a function on variables.
Ok, it turns out that I was sort of on the right track and I actually implemented and fixed my idea. Long story short, the finalization routine must be in the base monad and not a Pipe and you can't use the return value. The Pipe type I ended up using is: data Pipe a b m r = Pure r | M (m (Pipe a b m r)) | Await ( a -&gt; Pipe a b m r , m ()) | Yield ((b, Pipe a b m r), m ()) In other words, I append a fallback monad to both await and yield, so you can intercept both an await failure AND a yield failure by cleaning up some resource. Composition becomes: instance (Monad m) =&gt; Category (Lazy m r) where id = Lazy $ pipe id Lazy p1' . Lazy p2' = Lazy $ case (p1', p2') of (Yield (y1, m) , p2 ) -&gt; Yield (fmap (&lt;+&lt; p2) y1, m) (M m1 , p2 ) -&gt; M $ liftM (&lt;+&lt; p2) m1 (Pure r1 , Yield (_, m2) ) -&gt; lift m2 &gt;&gt; Pure r1 (Pure r1 , _ ) -&gt; Pure r1 (Await (f1, _) , Yield ((x2, p2), _)) -&gt; f1 x2 &lt;+&lt; p2 (p1 , Await (f2, p2') ) -&gt; Await (fmap (p1 &lt;+&lt;) f2, p2') (p1 , M m2 ) -&gt; M $ liftM (p1 &lt;+&lt;) m2 (Await (_, m1) , Pure r ) -&gt; lift m1 &gt;&gt; Pure r I haven't vetted the case statement ordering yet, but you get the idea. The `yield` and `await` primitives just use `return ()` as the fallback and you can implement `tryYield` and `tryAwait` primitives this way. In fact, this makes obvious something we both hadn't considered in your initial approach, namely that yields can fail and we were never intercepting them. That would have been a problem for a Producer that had to finalize a resource when its downstream Pipes terminated. The nice thing about this approach is that you still keep the free monad, which is nice. You can check out this implementation from the "try" branch of my github repository. I like to use the following pipe for testing finalization: pass = forever $ tryAwait (putStrLn "Await failed") &gt;&gt;= tryYield (putStrLn "Yield failed") A couple of examples to show it in action: &gt; runPipe $ printer &lt;+&lt; take 3 &lt;+&lt; pass &lt;+&lt; fromList [1..] 1 2 3 Yield failed &gt; runPipe $ printer &lt;+&lt; pass &lt;+&lt; take 3 &lt;+&lt; fromList [1..] 1 2 3 Await failed I'll need to go over it and check the Category laws. `id` is still `forever $ await &gt;&gt;= yield` and `discard` is still `forever await`.
This is more or less what [Functional Java](http://functionaljava.org/) is trying to do, I think.
You asked for feedback, so I skimmed the code and wrote what I thought could be improved. The code looks good and nice to read :-) But here are my suggestions anyway: First, you can run hlint on your codebase, it has an awful lot of suggestions. I suggest not using open unqualified imports in production code: Open unqualified imports make it easier to write code, but harder to read it ("Where does some name come from?"). Code is read many more times than it is written, one should optimize for readability. Closed-unqualified is fine, but I personally find qualified imports preferable most of the time. Open-unqualified makes code unnecessarily brittle. An example breakage is the current "bitmap" package: http://hackage.haskell.org/packages/archive/bitmap/0.0.1/doc/html/src/Data-Bitmap-Pure.html `import Foreign` and `import System.IO.Unsafe` bring two different implementations of "unsafePerformIO" (the Foreign one is a deprectation wrapper around the System.IO.Unsafe one). So bitmap fails to compile on newer ghc's and now requires the attention of a busy author. Unnecessary brittleness! ------- In your code: https://github.com/colah/ImplicitCAD/blob/master/extopenscad.hs Where does "writeSVG" come from? Or "runOpenscad"? Consider SVG.write is virtually as concise and makes the origin of a name clear. It becomes possible if modules are designed for qualified imports. Also "S" is a funny name for "Data.Map" (it's usually "M"). ------ https://github.com/colah/ImplicitCAD/blob/master/Graphics/Implicit.hs appears to be using unicode-names. At least I can't really type these in, and the benefit over ascii names is rather small. ------ I really suggest compiling with -Wall. Without -Wall, even basic errors such as forgetting to cover some patterns are runtime errors rather than compile-time warnings/errors. ------ In https://github.com/colah/ImplicitCAD/blob/master/Graphics/Implicit/ExtOpenScad/Expressions.hs you write: variable :: GenParser Char st (VariableLookup -&gt; OpenscadObj) variable = liftM (\varstr -&gt; \varlookup -&gt; case lookup varstr varlookup of Nothing -&gt; OUndefined Just a -&gt; a ) variableSymb No need to use liftM on a concrete type -- fmap needs no import and is more recognizable. Also, `\varstr -&gt; \varlookup -&gt; ` can use the sugar: `\varstr varlookup -&gt; ...`. Also, your lambda is really inlining the implementation of fromMaybe: variable = fmap (\varstr varlookup -&gt; fromMaybe OUndefined $ lookup varstr varlookup) variableSymb After you read http://conal.net/blog/posts/semantic-editor-combinators you can also use this nice little pattern: variable = (fmap . result . result) (fromMaybe OUndefined) lookup variableSymb ------- https://github.com/colah/ImplicitCAD/blob/master/Graphics/Implicit/ExtOpenScad/Statements.hs uses the pattern: try a &lt;|&gt; try b &lt;|&gt; try c ... Why not: tryMany = choice . map try to eliminate boilerplate? Great work!
*After you read http://conal.net/blog/posts/semantic-editor-combinators you can also use this nice little pattern: variable = (fmap . result . result) (fromMaybe OUndefined) lookup variableSymb* That sounds like lenses, which I highly suggest. Especially the implementation of fclabels on hackage. You define the setter and getter of e.g. a record field, and can then combine them to access deeply nested fields. data DeepRecord = DeepRecord { deepField :: Int } data TopRecord = TopRecord { topField :: DeepRecord } deepFieldL = lens deepField (\newValue deepRecord -&gt; deepRecord {deepField = newValue}) topFieldL = lens topField (\newValue topRecord -&gt; topRecord {topField = newValue}) Now you can combine the lenses to get/set/modify fields. get (deepFieldL . topFieldL) topRecord set (deepFieldL . topFieldL) newValue topRecord modify (deepFieldL . topFieldL) (\oldValue -&gt; oldValue + 1) topRecord fclabels can automatically derive the lenses for records by using TemplateHaskell. Which I don't like that much, because for this you have to prefix your field names with '_'. I'm just using cpp with this macro: #define LENS(field) field##L = lens field (\value record -&gt; record {field=value}) To get the lenses defined above you then write: LENS(deepField) LENS(topField) 
Yes. `quot` rounds towards zero, while `div` rounds towards negative infinity, and `rem`/`mod` are defined to be consistent with that, so that ```(x `div` y) * y + (x `mod` y) == x``` and ```(x `quot` y) * y + (x `rem` y) == x```. &gt; (-5) `divMod` 3 (-2,1) &gt; (-5) `quotRem` 3 (-1,-2) For positive values, they are the same (except that `quot` and `rem` are slightly more efficient, since that's usually the native form of division supported by the CPU).
&gt; I first attempted installing GHC 7.4.1 from the prebuilt x86_64 binaries provided from haskell.org/ghc but they are linked against an old version of libgmp, and rather than try any ugly symlink hacks ...what's so ugly about a simple `apt-get install libgmp3c2`? :-)
What is the practical difference for mere mortal programmers between 'fclabels' and 'lenses'? I chose lenses for my current project just because the name sounded better. 
The lenses package has several issues, as outlined in [this Stack Overflow answer](http://stackoverflow.com/questions/5767129/lenses-fclabels-data-accessor-which-library-for-structure-access-and-mutatio), and I don't think it's maintained; the two most popular lens packages by far these days are fclabels and [data-lens](http://hackage.haskell.org/package/data-lens). I prefer data-lens, since it's simpler (I don't like arrows much, and fclabels is full of them) and, IIRC, faster; it's used by [Snap](http://snapframework.com/) among others. But in practice they're probably pretty much the same; both have Template Haskell support for deriving lenses from record definitions (built in to fclabels, available in [data-lens-template](http://hackage.haskell.org/package/data-lens-template) for data-lens). (If you go with data-lens, you'll probably want to use the [data-lens-fd](http://hackage.haskell.org/package/data-lens-fd) package, allowing you to use lenses in any `MonadState`; it's split out from data-lens for portability reasons.)
Depends entirely on what you're trying to do. For a course I'm taking at Uni right now I need to use C to get the close relationship to pure asssembly and machine instructions. For a course in object oriented programming Java was a requirement. I've studied some of gcc:s behaviour with C too, in my free time, as part of an argument with a friend. Otherwise, for my own hacking, I've used Haskell exclusively since, well, a while back. I guess I could do with Python, but the lack of a static type system would not be pleasant. Writing Java frankly makes me feel bad; it goes against so many of my perceptions on what programming should be like.
1) Libraries a) general libraries written in other languages are difficult to port b) stuff close to the operating system is written in c because that is a more performant and is far easier to mesh with the raw hardware than FP languages 2) Programmer base 3) Existing code bases -- are largely written in non-FP languages and are difficult and costly to port. 4) Convenience -- some times its just easier to knock out a quick python script. 5) What FP language do you choose? Arguably some languages are more pure than others (Haskell's IO monad comes to mind as impure). How do you choose between typed and untyped lambda calculus? Lisp has been around since the 70s is that the best option? 
&gt; ... I don't like arrows much ... For the basic usage, the only arrowness you really see is the '.' operator, which behaves like the one for functions, which most Haskell users certainly already know. I think that fclabels feels a lot more functional than data-lens and it's syntax just fits a lot better into Haskell. 
&gt; Haskell's IO monad comes to mind as impure Does it? What approaches to IO are more pure than the `IO` monad?
Because [Haskell is useless](http://www.youtube.com/watch?v=iSmkqocn0oQ).
Because it comes with difficult to diagnose performance problems, and the toolchain to help fix them is still immature. At least for the case of lazy and pure. 
Who is we? I've been programming in Haskell for 7+ years and I have never ever looked back.
I write Haskell code to communicate with myself. I write Python code to communicate with people who understand Python best. I write other languages for people who understand those languages best. But for me, I write Haskell. What do you write for you?
"Haskell's IO monad is impure" is a rather hand-wavy statement. Can you give an example of a Haskell expression that is impure?
 &gt; And personally, I'm not too fond of data-lens' operators either, but it offers prefix forms as well — getL, setL, and modL. Ok, I haven't been aware of them. So my examples above are just in the same way writable in data-lens? &gt; Of course, fclabels doesn't define state monad functions at all. Also fclabels has them: http://hackage.haskell.org/packages/archive/fclabels/1.1.0.2/doc/html/Data-Label-PureM.html 
"Performant" [is not a word](http://boulter.com/blog/2004/08/19/performant-is-not-a-word/).
&gt;So my examples above are just in the same way writable in data-lens? Yep. &gt;Also fclabels has them: http://hackage.haskell.org/packages/archive/fclabels/1.1.0.2/doc/html/Data-Label-PureM.html Oh, sorry, I couldn't find them; I've removed that sentence from my post. Annoyingly, not only does that module (and Data.Label itself, thanks to `modify`) clash with Control.Monad.State's names, it clashes with the pure Data.Label module too; that was one of the things that made me stop using fclabels (back before 1.0, thus why I couldn't find the state monad functions).
Inertia. People look to Haskell to find a better imperative language -- not many people are willing to make the paradigm shift. Modern programming languages like Python and Ruby are "good enough" and get a lot of work done. People will, in general, prefer to stay in their comfort zone and work at 80% efficiency than move out to get that extra 7%.
I've been writing pretty much nothing but Haskell (and a wee bit of Agda) for the last 10 years.. don't know what everyone else's excuse is..
The reason this is getting downvoted is because SPJ himself was pretty annoyed by the way this quote has been repeated out of context again and again.
But hasn't it been done in an obvious tongue-in-cheek manner? I thought the programming community would be much more mature than that.
Nope. IO continuations aren't reifiable. That's a purely denotational thing that only *coincidentally* completely agrees with the fact that physicists so far didn't manage to build CPUs that travel in time. You could, with some work and lots of plumbing, make them reifiable. Go ahead.
I have tried over the past 10+ years to get people to stop using that, but I think it's a losing battle against an evolution (new word being coined) in the language.
This doesn't explain downvotes, given that I *have provided* a link to the actual context.
Could you expand on the "large domain model" thought? I'd like to understand more.
&gt; 1) Libraries The lack of certain libraries can indeed be an obstacle. I once thought of writing a Tribler client in Haskell, but that requires elliptic curve cryptography (curve sect233k1) and right now I don't feel like on working on an ECC libary.
fclabels have a serious impediment that made me abandon them altogether in favor of a [bit of TH](https://github.com/Peaker/bottle/blob/master/src/Data/AtFieldTH.hs) written by a friend that generates the SECs that we need. Consider this record: data Record a = Record { _x :: String, _y :: IO a } If you generate an fclabel for x, all is well. You get the label that can usefully lift modifiers as in: `(String -&gt; String) -&gt; Record a -&gt; Record a`. The problem is with polymorphic fields such as `y`. The fclabel for `y` gives you only this modifier: `(IO a -&gt; IO a) -&gt; Record a -&gt; Record a` and not the more general type: (IO a -&gt; IO b) -&gt; Record a -&gt; Record b So given that I already get a useful "getter" built-in to Haskell, the only thing fclabels give me is an artificially-restrictive modifier function (The setter is simply composing a modifier with `const`, so I ignore it here). I'd much rather just use TH to generate a non-restricted modifier function, and use the built-in getter. The only thing I lose out on is the grouping of the getter and modifier together, but in practice this has not been a problem for me.
I must admit that I still use Perl for the great utility of calling `perl -ple '...'` in the midst of a shell pipeline. This is the exact context for which Perl was designed, and it's still by far the best language for that task (with a high power-to-weight ratio, extreme domain-specific brevity, etc). However, for as much as I love Perl, I no longer write programs in it; by the time something doesn't fit on a commandline anymore the power-to-weight ratio has shifted in favor of Haskell, to say nothing of debugging and maintenance issues (if it's longer than a commandline, you *will* be maintaining it, no matter how throwaway a script it is). Never could stand Python. There's just something demented about that language that I can't quite put my finger on. Can't stand Java either, though it has been forced on me in dayjob coding from time to time. Never did play around with Ruby much; it has some promise, but it's hard to give up the power of purity, the power of a decent type system, and the lightweight syntax that I get from Haskell.
Because functional languages require a lot of mental overhead and unnatural way of thinking compared to "mainstream" procedural/object languages. Unless you are born to be a mathematician, perhaps.
Please explain?
&gt; The example is not symbolic differentiation Is it not? I thought it was doing that, rather than calculating f'(x) = (f(x+deltax)-f(x))/deltax (which is of course a perfectly valid thing to do, for many purposes).
&gt; Imagine, if you will, a moment when you want to promote your code from using scalars to using matrices So that each of my doubles is now a matrix? I suppose that could happen for some code, but not for the application I'm writing. &gt; In this case, fixing toDouble would be easy, and just leave a misnomer If you mean keep calling it "toDouble" but making it do something else, I'd regard that as a bad practise that is bound to bite you at some point in the future.
&gt; The fact of the matter is that FP can be hard. Thank you for pointing that out. I'm a lurker here ; I love Haskell and FP as a whole but I've tried and tried but I can't write anything non-trivial in a functional way. I mean Project Euler is fun and all but whenever I try my hand at doing something more involved than that, I'm stumped. It's like you know the words of a language but have no idea on how to form a proper sentence. I hope I make sense.
You can't undo all functions: http://en.wikipedia.org/wiki/Surjective_function
In Haskell, you can write instances of "Num" that aren't strictly "numbers" as you might call them. There are the instances you are familiar with, Int (integers in a finite range), Integer (arbitrary length integers), Double and Float (using machine floating points), Ratio and Rational (rational numbers of Integer numerator and denominator, or arbitrary instances of Integral respectively). There are more I've failed to mention, and since you can define your own, there's almost no limit. For example, if I define: square :: Num a =&gt; a -&gt; a square x = x * x That's "defined" automatically for *all* instances of Num, even ones that haven't been written yet. So if you later define a data type "Complex" that is an instance of "Num", huzzah, you've defined "square" for complex numbers! Well, what if you define a data type that doesn't represent a number at all, but say, a real valued variable? So you can define a "Differentiable" and make it an instance of Num. What does that mean? It means that "square" is now defined for differentiable variables. You don't have to change the definition of square! All "square" has to know is that it's operating on "things that define `(*)`", i.e.: multiplication. Our "Differentiable variable" type would define `(*)` and there are rather simple algorithms for dealing with multiplying two derivatives, so it's not too hard to figure out. So you can write something like: square' :: Num a =&gt; a -&gt; a square' = derivative square That is, you can have a "derivative" function that takes a function and returns a new function that is the derivative. Just because something operates on instances of "Num" doesn't mean the things it's operating on are *specific* numbers. They could be generalizations of numbers, or groups of numbers. One could make vectors and matrices instances of Num* and then you get the ability to add, subtract and multiply them in a sane manner. Which, again, goes back to why the division operator isn't defined by the Num type class: division of matrices and vectors is ill defined. (*) - Can you tell why vectors with type-defined length, or matrices with type-defined width and height would not be instances of Num?
I had some fun posting this answer to SO a couple weeks ago; I think it gives an interesting perspective on how Type classes work, so I'm throwing it on /r/haskell. Also, /r/haskell are the kind of people that might enjoy playing around with lambda calculus.
In german [it is](http://dict.leo.org/ende?lp=ende&amp;lang=en&amp;searchLoc=0&amp;cmpType=relaxed&amp;sectHdr=on&amp;spellToler=&amp;search=performant).
Plenty of sense. It gets easier with time but I recommend finding some small project to write in haskell that's at least 200 loc. 
For those who missed the punchline: the blog post content contradicts the title. Also, "blog" is not a word. (And "punchline" once wasn't a word.) 
You linked to a video, which has much lower conversion rate than text. 
There is actually a very detailed answer to this question on StackOverflow: [Why is Haskell used so little in the industry?](http://stackoverflow.com/questions/2284875/why-is-haskell-used-so-little-in-the-industry)
That's not what he or she meant by undoing a function, I think it was this: for pure f, you can let y = f x and you still have x to use again as if you had *never computed* y; but if f say, writes something to a file besides computing a value, then if you do y &lt;- f x, when you later stop using y and just use x again the writing to the file doesn't magically undo itself.
Some universities are starting to offer classes using Haskell! I'm excited to see what comes of that.
70s? I thought it was more like the 50s.
See Campers' comment : at first sight, it seems like a very good place to start.
Right - in fact I think if anything it's helped nudge people to check out Haskell.
&gt;Yes, but you can reason about it the same way as the State monad No, you can't; as I said, that model does not admit concurrency. I don't object to the use of IO to model imperative programs at all; indeed, it's one of the most expressive models of imperative programs in practical languages today. I'm taking issue only with the IO-as-state description. Also, I don't believe IO has a semantics or behaviour grounded in mathematics at all; it's called the sin-bin for a reason. Yes, it's a monad, but that doesn't mean we know what its semantics are; just that they obey the monad laws. In fact, IO probably doesn't have any semantics at all, because it can do things like observe sharing and the evaluation order of pure code (thanks to exceptions), which are all supposed to be impossible; i.e. to give IO a mathematical semantics, we would have to describe it in terms of a Haskell that is not referentially transparent. I feel compelled to point out that Haskell *is* my favourite programming language, and I find monadic IO incredibly convenient, but we shouldn't oversell it — declarative it's not — and we shouldn't promote incorrect models like `State RealWorld`.
And if you want to be pedantic, assembly is still heavily employed. There are a bunch of things you can't do in a GC'd language, like **write a GC for the language itself,** and there are things you can't do in C, like write application entry points, system calls (the userland part), synchronization primitives, etc.
For those following along at home, the new [bytestring-lexing](http://hackage.haskell.org/package/bytestring-lexing) is now publicly available.
 export EXTRA_CONFIGURE_OPTS="-p" ... dang. TIL that this makes bootstrap.sh install profiling libraries.
Yes, I just realized this too and updated the blog post accordingly. Was that not what you wanted?
I would probably do something like this: hasBars :: (a -&gt; Bool) -&gt; Int -&gt; [a] -&gt; Bool hasBars f n = go 0 where go i [] = i &gt;= n go i (x:xs) | i &gt;= n = True | otherwise = if f x then go (i + 1) xs else False Example of use with Maybe: *Main Data.Maybe&gt; hasBars (isJust) 2 [Just 1, Just 2, Nothing] True *Main Data.Maybe&gt; hasBars (isJust) 2 [Just 1, Nothing, Just 2, Nothing] False It will fail if there are more entries in the list than the maximum value of Int, but you could change the type to Integer or something else.
I don't think it's being pedantic at all. Assembly isn't employed on nearly as wide a scale as C is. I mean, it isn't even close... By some standards, C is the most popular programming language. I just don't see how that can be true while the "malloc vs. GC" thing being solved. It isn't solved. I don't even think it's appropriate to say that one is better than the other. There's enough demand for manual memory management that there are legitimate and *popular* reasons to go both routes.
Pardon me you are correct.
I don't think that the effort of converting a Foo constructor into a Foo observer predicate is worth it. The more generic solution—which you do get close to—is simply to apply a ziplist of predicates to a ziplist of arguments.
I've become comfortable with Haskell about 18 months ago, and gradually switched to Haskell from Python for all my general coding needs. I only touch Python because I have to work with other's code.
For those who don't see what he's talking about, just change hasTypes to: zipPreds :: [Foo] -&gt; [Foo -&gt; Bool] -&gt; Bool zipPreds fs ps = and (zipWith ($) ps fs) Then to use it as OP desired, you could do something like: zipPreds foos $ map sameConstructor [bar, baz]
Installing on a fresh copy of Ubuntu 11.10, trying to make ghc, it complained about not having curses. This fixed it for me: sudo apt-get install zlib1g-dev libncurses5-dev Not sure if zlib1g-dev was necessary as well, but that's what was suggested somewhere when I googled the problem :) Other than that, the whole install went quite nicely. thanks!
The word seems to have taken off in the 70s: http://books.google.com/ngrams/graph?content=performant&amp;year_start=1900&amp;year_end=2008&amp;corpus=0&amp;smoothing=3 
TIL: FP is exactly 7% more efficient than imperative programming.
They are usually called streams. There are libraries for them, for example [this one](http://hackage.haskell.org/package/Stream).
'Cause the boss pays me to. ._.
As most of those functions make perfect sense for finite lists, too, I think it'd be prudent to typeclass them so you don't have to qualify everything by module all the time. Something along the lines of [edison](http://hackage.haskell.org/package/EdisonAPI-1.2.1), but actually used in practice. We've got associated types, by now, I think a complete rewrite and reorganisation is in order.
A few packages [depend on Stream](http://packdeps.haskellers.com/reverse/Stream); nothing depends on the other two. streams seems to be [more comprehensive](http://hackage.haskell.org/packages/archive/streams/latest/doc/html/Data-Stream-Infinite.html) than Stream, though; if I was choosing a package to use myself, I would probably go with streams.
Ooh, [packdeps](http://packdeps.haskellers.com/) is exactly what I was looking for; great.
On the fly compilation is, by far, the best feature IMO in ghc-mod, so I basically know if my program will typecheck or not even before leaving emacs. It saves me a ton of time. With hsenv and using the GHCi REPL, it's even better. There are other fancy features it offers like completion and auto-fill functionalities, but the on-the-fly compilation is, IMO, the best part.
We also don't build GCs atop systems that are GC-aware, making the job of getting GC to perform well even harder. As an example, no mainstream desktop or server OS has a mechanism for the kernel to indicate to applications that they should shrink their memory use if they can, and that CPU usage to shrink memory use is acceptable right now; nor is there a mainstream desktop or server OS that indicates to applications whether the system is currently CPU or memory constrained. This is relevant beyond pure GC applications, too - for example, your browser may maintain a memory cache of recently visited pages ready to display if you hit the back button. If you're low on memory, it may be better to discard the oldest part of that cache than to swap. Similarly, if a program is short lived, it may be optimal to allocate memory and never free it - if it turns out to be longer lived, deallocating memory becomes necessary. The exact transition point depends on what else needs memory in the system - and that needs kernel-level support, to tell you either "eat memory, it's fine", or "actually, I can make good use of any memory you can free up".
Sure, if it were only mine, and not many people's and servers. The point is that the technology is not yet as modern as the idea, and that compatibility is well served by having a stable simple works-everywhere mode, with an optional decoration layer on top for non-critical improvements that aren't universally supported. Coincidentally, a bug came up at my office on Friday about a UTF-8 encoded character getting mangled somewhere in the pipeline between our provider's data feed and our receiving code. 
Do you mean that Edison is not used, or this API idea is not used. Edison seems like a big deal (lots of important data structures, and the largest "interface decoupling from implementation" example I have seen in Haskell), but in my limited travels I have never heard mention of it outside of "it is code for Okasaki's book/thesis". Data. Sequence, as an example, seems to get more attention in the "advice for practical programmers" department. (DList is another one I discovered recently that seems more useful than you would guess from how rarely it is mentioned in efficiency advice) 
That's a cool little utility, thanks!
I'm afraid I don't understand what it is you're suggesting or what you think Edison does in regard to name reuse. I read [in the docs](http://www.haskell.org/ghc/docs/edison/users004.html) that &gt; I have attempted to choose names that are as standard as possible. This means that operations for different abstractions frequently share the same name (empty, null, size, etc.). It also means that in many cases I have reused names from the Prelude. However, these name clashes should not be a problem because I expect Edison modules to be imported qualified (see Section 3.1). If, for some reason, you choose to import Edison modules unqualified, you will usually need to import the Prelude hiding the relevant names. which is exactly the same as what I have done. In particular, I don't see how typeclasses solve the name-reuse problem.
Edison provides alternative implementations of equivalent data structures. Containers and the "standard libraries" (ByteString, etc.) provide different data structures - sometimes with differently typed operations (ByteString is not a functor so *map* cannot change types); often where a data structure doesn't support "common" operations efficiently - e.g. DList cannot do any introspective operations efficiently, *viewr* is efficient on Sequence but bad on List. So you end up either with classes for one operation, or rag bag classes like ListLike - neither seem appealing for the standard libraries which should be "gold standard" code. 
You might want to read [this](http://stackoverflow.com/questions/3362915/is-haskell-suitable-as-a-first-language) and provide some support for your post in the future. Thank you, though.
I maintain streams. The repository is on github, and I'm more than happy to accept patch requests. ;) 
Should `findIndices` and `elemIndices` return possibly finite lists? Either way, they'll hang on going past the last value...
&gt; Why does an infinite list structure, as terribly trivial as it is, seem to not exist in the libraries? Part of it is the fact that, due to laziness, plain lists suffice for most uses. The only time you really want proper streams rather than lazy lists is when you (a) know that the list is indeed infinite, rather than only possibly infinite, and (b) you want to convey that information in the types. While the former is frequently important in mathematical theory, it doesn't seem to come up very often in programming. And the desire for the latter has to be sufficient to overwhelm the utility of having access to the numerous list-based operations. If we could come up with a decent family of type classes for expressing list-like operations then we wouldn't have to pay so much for abandoning lists. Unfortunately, it's harder than one may think to come up with those classes since there are many cross-cutting concerns (head access a-la list, tail access a-la DList, random access a-la Sequence, finite vs semi-infinite vs infinite,...)
Wow, that guy has a twisted notion of what it means for a language to be functional... It feels like he is arguing that every "language" is functional, independent of whether the semantics of the language are in the traditional sense.
Thanks. That was it.
No, because InfList has infinite length, you never know if you've found the "final" index. But you can always use `take` or `takeWhile` to drop down to finite lists again.
another attempt. * cloned hamlet, wai, and persistent repos * worked through one or two issues while building hamlet, wai, and persistent * finally yesod built I was probably confused by the blog-post that yesod.git and scripts/install therein is all one needs. thanks for confirming it works and encouraging me to retry :).
This is how I would deal with the + problem for a random RPN language I just made up: http://hpaste.org/57497 Check out the 'arithmetic' function and 'mismatch' / 'test_mismatch' example.
Great work! Be sure to let us know when the new release is available on hackage.
 data MySubSet = Type1 | Type2 data OuterType :: MySubSet -&gt; * where OuterType :: t -&gt; OuterType t Here *t* has kind \* not *MySubSet* and thus it's invalid. A more correct one would be: data OuterType :: MySubSet -&gt; * where OuterType :: Type1 -&gt; OuterType Type1 But if *OuterType* expects a first argument value of type *Type1*, what possible value can assume this type? Actually no value can have the type *Type1* since *Type1* is a promoted type. Promoted types are used for type-level programming.
Can you expand on why the fallback code can't have non-() return types?
Oh, don't get me wrong. I love ghc-mod. In fact, I contributed cabal-dev support I like it so much. :)
May I be so bold as to point out [hayoo](http://holumbus.fh-wedel.de/hayoo/hayoo.html). Major discoveries there. Ok, yes, it would be nice, you're right.
Thank you!~
I'm pretty sure Starling Software no longer exists, but their code lives on at Tsuru Capital. And afaik, it is not open source at all (when you've got a good thing going that's making you money like that, you don't tell other people). I say this is a previous intern at Tsuru.
Ah, gotcha. I'd never heard of that technique before. However, my point still stands: this is doable in C++ and Smalltalk. It is almost doable in Python (it doesn't work for sin cos etc functions, because they are implemented as functions and not methods and are therefore not overridable). Now, all these languages have automatic numeric coercion. Therefore ryani's comment that automatic coercion makes this technique impossible is simply false.
No problem! I've added those two to the post. I had trouble remembering what prereqs I needed as well, and two more can't hurt. :)
You might be thinking of [this](http://www.reddit.com/r/haskell/comments/n1rhk/fixhs_a_financial_information_exchange_fix/) Fix protocol implementation. It's not a trading platform, though. It's just a protocol that exchanges and brokers and funds use to send orders, etc. 
It's jargon, or started as such. Most jargon is not found in an ordinary dictionary.
Both aren't used, but I mean the API.
 instance Container ByteString where type Elem = Word8 Yes, ListLike could be nicer than it is, but not having nice things isn't, by itself, an argument why we can't have nice things. 
&gt; 1) Libraries If a language has few users, few libraries will be written fo it. And if there are few libraries, it will be of limited usefulness, so few will use it. One strategy to break this is to build the language from the start on a platform that has lots of libraries -- such as the JVM. Which is a key to Clojure's success. Would it be possible to run Haskell on the JVM and more importantly do it so one can easily call all Java's libraries?
Let's look at this the other way: why are those languages that are successful, successful? Some possible reasons: **They have a big company behind them.** Examples: Java, C#. **They build on another popular language, initially retaining compatibility with it.** Eg C++, Objective C. Objective C is also backed by Apple. **They have a killer app, a problem domain that they are an/the obvious solution for.** Eg Javascript for client-side web programming. PHP or Ruby+Rails for server-side. Objective C for iPhone apps. Python incidentally managed to grow without any of these. How? By growing organically. It helped that it is both easy to learn but also expressive for the expert user; so it's regularly touted as a beginner's language but is also used as an infrastructure language by clueful organisations (e.g. Google). Haskell is -- in my experience at least -- harder to learn than Python. One reason is it violates the principle of least surprise, e.g. This: Prelude&gt; sqrt 3 1.7320508075688772 But: Prelude&gt; let x = 3 Prelude&gt; sqrt x &lt;interactive&gt;:1:0: No instance for (Floating Integer) arising from a use of `sqrt' at &lt;interactive&gt;:1:0-5 
&gt; However, porting haskell to the JVM would be extraordinarily difficult Really? Why? &gt; to really use the libraries correctly they would need to be wrapped in an FP fashion which would require much more work than simply calling them That's an important point. It's important that a Haskell programmer not just be able to call the libraries, but call them *without a lot of faffing around*. Modern programming often consists of using existing libraries to do much of the heavy lifting in the code. This is a good thing: if I'm writing a game I don't want to have to write my own graphics primitives. If I'm writing a word processor I don't want to have to write my own windowing system or font handling code. Nor should a web programmer have to write code to handle cookies, or reading POST requests or producing JSON. So if a new language doesn't talk easily to existing libraries, there's a whole host of application areas it will be unsuitable for.
The porting would be difficult because you have to write a completely new compiler with completely new optimizations. You are also targeting a platform that isn't too friendly to Haskell, the JVM is only very good at running languages that are like java. So, you may be able to get a compiler working quickly enough but it will not be close to production ready for months or years. You can look at the other examples of language ports to the JVM for evidence of this. I do not know enough about the internals of haskell compilation or the JVM to explain any more technically.
Haskell is nice :) simply there was not many good resources to learn before "Real World Haskell" (resources that did not require PhD :D ) to see on examples new things. You need to shift a bit your mindset from imperative to functional. The other problem is that cabal is not really cool, you can use off cabal-dev like BoS but in general it should be more like ruby gems, node npm or python pip :) . But this is probably just matter of time ? (i'm right yeah ?) 
What ithika is getting at is that the imperative paradigm is just an another abstraction, there is nothing fundamental or easy about it. As he mentions, programming computers at the hardware level, for instance with [VHDL][1], is very different. As I like to say: programming computers is not about shoehorning your ideas into existing languages. Programming computers is about inventing new languages that can naturally express your ideas. [1]: https://en.wikipedia.org/wiki/VHDL
I think this is unnecessarily complex. What is needed, IMO are "views" into hackage. If there is a way for anyone to add attributes / annotations to hackage packages, then people can attach build results etc. to packages independently of the author. Then by filtering on attributes / annotations, one can ignore the cruft. 
The Num infer comes from the use of an Integer Literal (5). GHC 'rewrites' f to: f :: a -&gt; a f 5 = fromInteger 5 And, because fromInteger has type `Num a =&gt; Integer -&gt; a`, f needs to be declared as: f :: Num a =&gt; a -&gt; a 
Are there any specific factors by which you would state that UNIX has a downward course?
I think the current Haskell definition is correct. Normally Curried functions will be supplied with all their arguments, in which case the application wil be first order, as will the function definition. Haskell allows higher-order uses to be constructed for first order functions. Under the Haskell definition, a partially-applied function application will be higher-order (which is correct), as will a function expression considered as a series of applications up to the point that it's final argument is supplied (also correct). A function definition that takes only some arguments and therefore constructs a function as its result will also be higher order. Almost all uses in Haskell are actually second order rather than higher-order, but that's another story. I don't see why a higher-order function has to use any of its arguments? drop f = id seems pretty higher-order to me! 
Don't quote me on this but I heard that it's different for haskell because it's compiled and that rvm like goodness for haskell isn't a copy and paste process. Does anyone have any verification of this?
&gt; As far as I can tell, the main thing that prevented Smalltalk from being directly used in an actual project was "hermetic environment syndrome" Yes, which among other issues makes using outside source control systems problematic. &gt; all Smalltalk implementations I know of run in their own virtual machine, tightly integrated with their own IDE, using their own old-fashioned GUI libraries Two that are not like this are GNU Smalltalk and Little Smalltalk. &gt; UNIX does seem to be on a downward course and not about to start climbing back. Most smartphones run a Unix, so I expect there will be around 5 billion Unix installations by the end of the decade!
If Haskell makes it big, it will probably be as a side-effect of a framework written in Haskell making it big. My guess is that this framework will involve FRP and depend on the Haskell type system.
&gt; If Haskell makes it big, it will probably be as a side-effect of a framework written in Haskell making it big. Entirely possible. Look at the boost Rails gave Ruby.
Oxford (where Richard Bird, and probably others, were contributing to Haskell's development). I should add that I was majoring in maths; I don't know for sure if the CS students first course was FP or imperative, though it certainly appears that it's [now](http://www.cs.ox.ac.uk/admissions/ugrad/Computer_Science_core_1) Haskell. Practicals were mostly in [Orwell](http://en.wikipedia.org/wiki/Orwell_%28programming_language%29), which I don't remember supporting Monadic IO. Thus the kind of program you could write was somewhat limited (mostly solving puzzles), and I didn't take Haskell any further until SPJ's 2007 OSCON [talk](http://blip.tv/file/324976) demonstrated Haskell as a practical language and reawakened my interest.
I think a better solution would be to package an AMI with each release of GHC so that anyone could very easily set up an instance to test whatever they need.
Here's my preferred definition of order as a function on types: ord (a -&gt; b) = max (ord a + 1) (ord b) ord t = 0 if t is not of the form a -&gt; b for some types a and b A function type t is said to be higher order if ord t &gt; 1. This means that, for instance, whenever a is a non-function type, ord (a -&gt; a) = max (ord a + 1) (ord a) = max (0 + 1) 0 = 1 ord (a -&gt; (a -&gt; a)) = max (ord a + 1) (ord (a -&gt; a)) = max (0 + 1) 1 = 1 ord ((a -&gt; a) -&gt; (a -&gt; a)) = max (ord (a -&gt; a) + 1) (ord (a -&gt; a)) = max (1 + 1) 1 = 2 The order being a property of the type means that what the function itself actually does with its arguments doesn't (and can't) matter. So even your specialised const will be considered higher order by this definition. In other words, I basically agree with you, plus some details. :)
I have been polluted by imperative programming entirely my entire programming experience, so I cannot answer this hypothetical. I have often wondered what would happen when somebody learns Forth first.
I was introduced to FP pretty early on. Most of my friends, however, are either indoctrinated entirely by imperative and OOP techniques, and some of them find it difficult enough to maintain *those* skills.
OIC, you mean like data List a = a :&gt; List a | Nil data InfList a = a ::: InfList a class Filter c where filter :: (a -&gt; Bool) -&gt; c a -&gt; c a instance Filter List where filter p Nil = Nil filter p (x :&gt;xs) = if p x then rest else x :&gt;rest where rest = filter p xs instance Filter InfList where filter p (x:::xs) = if p x then rest else x:::rest where rest = filter p xs with a new typeclass for every operator? Interesting …
There are many functions, though, where one can't write *ad hoc* polymorphic type signatures. For instance, even though folds/catamorphisms are well-defined, how would we write: class Fold a where fold :: (???)
I don't think cabal is worse than the similar tools in other languages. I think it's trying to solve a harder problem, since Haskell code is split up into so many more smaller libraries.
Any particular reason you wrote "tree" in the title if you're talking about graphs? There are several packages that implement different kinds of trees (e.g. Data.Tree), but you shouldn't expect to find (general) graph algorithms there. There are several algorithms in the fgl package under [Data.Graph.Inductive.Query]( http://hackage.haskell.org/packages/archive/fgl/5.4.2.4/doc/html/Data-Graph-Inductive-Query.html). If you want to contribute, IMO providing better documentation and profiling/benchmarking (and possibly patches) for that library would be much more beneficial than creating a competing library. 
I think that one could profitably argue that is the most likely outcome, but I think there is one huge possibility that could prevent that outcome, which is basically "Haskell is right", and true, pervasive purity and laziness really does make the difference between a usable FP language and one that still inevitably gets sucked into the imperative morass. We've seen significant features sucked from Haskell to other languages, but in general, those features may be cool and awesome, but they don't actually turn the language into Haskell in any of the good ways. LINQ doesn't change the fact you're still in C#, and it's still just a good imperative language. On the flip side, witness how STM is fairly easy in Haskell and appears to be effectively impossible to retrofit on to any other language. (As I understand it Clojure built it on day one.) If Haskell builds up a stable of features like STM that simply can not be pulled out of Haskell and put into other languages without essentially turning them into Haskell, Haskell may yet win. Or something that is clearly Haskell' (rather than C#++). Stay tuned.
&gt; and depend on the Haskell type system Yeah, I don't see an *untyped* Haskell framework gaining much popularity.
Like [this](http://www.haskell.org/ghc/docs/6.12.2/html/libraries/base-4.2.0.1/Data-Foldable.html): class Foldable t where fold :: Monoid m =&gt; t m -&gt; m foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; t a -&gt; m foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b foldl :: (a -&gt; b -&gt; a) -&gt; a -&gt; t b -&gt; a foldr1 :: (a -&gt; a -&gt; a) -&gt; t a -&gt; a foldl1 :: (a -&gt; a -&gt; a) -&gt; t a -&gt; a &gt; with a new typeclass for every operator? Interesting … Well, not *every*, though coming up with decent subsets is not trivial. OTOH, now that we have [constraint synonyms](http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/constraint-kind.html) it's possible to write something like type InfList a = (Foldrable a, Consable a) which reduces context clutter a lot. Given foldr, cons and decons you can implement filterBy, btw., no need to implement it twice. Fitting e.g. Set or Map into Filterable is yet another task, as those two structures aren't order-preserving.
Are you employed?
It seems to be up but VERY slow. Maybe everybody is hammering the server while trying to learn Haskell after that awesome Haskell Superbowl ad last night?
Thanks. The slides are pretty awesome by themselves in any case. 
And servers. Which is where more and more stuff of significance is. Seems to me like a rather upwards course. Well, for POSIX-like systems - not UNIX specifically, but I don't think that surrealize was making such a distinction.
The definition comes from logic. A zeroth order logic has no variables, a first order logic has variables ranging over individuals, etc.
As a Haskell newbie, I have to say I think the barrier to entry is a bit higher than any other language I've tried in terms of setup time and difficulty. I don't know much about the tradeoffs involved, I just think it should be taken into consideration. Cabal is just not easy to figure out and use effectively when you're a beginner on your own just trying to get a program to work.
If I was being mean spirited I could say that Lisp's *functional-ness* stopped when it had map, filter and fold (whatever it called them). Most of the functional goodies we like nowadays (pattern matching, type inference, ...) come from the ML and David Turner (SASL, Miranda) language families. 
So what did you use haskell for in 2002?
The features of Haskell that can be copied wholesale were mostly around beforehand in ML and the Turner languages (SASL, Miranda). The big exception is Type Classes which have been picked up by Mercury, Clean and other languages (that I've forgotten). Excepting F-sharp, I don't see any likelihood of another industry backed FP language coming along. Similarly I don't see anyone making a prettier Haskell (if we paraphrase Matz's goal of Ruby to make a prettier Perl). So I don't see Haskell having a successor at least for the next decade. If there is going to be a mainstream FP language, I think it would have some form of subtyping close to what people expect of OO, it wouldn't have a HM based type system. I'm not sure anyone is about to design this. 
[This](http://learnyouahaskell.com/).
&gt; functors are my current problem, and I don't want to move onto monads until I get them down I think you may have the cart before the horse here. You can certainly do useful things with monads without understanding functors in general. Also, functors will most likely be easier to understand once you've worked with monads.
wasn't smalltalk's initial problem that implementations were proprietary and very expensive? haskell does not have this issue.
Yeah, 1:56 EST and it WFM.
I agree cabal is not good enough for the Haskell community as it is. We need to replace version dependencies with signature dependencies. However, there's really nothing that's done by these package managers in other languages that cabal doesn't do -- it's just that code on hackage is divided into much smaller units with more inter-dependencies. So given so many more packages and relations, the version number dependency solution does not scale.
Yes, we need to replace Cabal with something better. The Haskell ecosystem is built from many more tiny components than other languages' eco systems, so the package management problem is harder in Haskell. The fact that new compiler releases aren't shy about breaking backwards compatibility makes it even harder.
The one with Clint Eastwood talking about Hindley–Milner type inference? I thought I just dozed off and dreamed it.
Back then I was working on the OS team for Lindows (later renamed Linspire). We Haskell and (a little) OCaml for a variety of tools needed for building an OS distribution. Things like the hardware detector, tools for build an .ISO from a debian package list, and autobuilder for automatically building debian packages and tracking dependencies, tools for automating backups and mirror debian repositories, tools for assisting with i18n, automated tools for updating the bugzilla database, etc. So, basically, we needed a bunch of tools, and Haskell was our language of choice. At the time a lot of other people would have chosen a combination of bash, awk, and perl for similar tasks. But we found that it was easier to write it in Haskell. Plus, that little 'throw away 10 line script' had a tendency to become a 1000 system critical script. The scripts that were in perl/bash became nightmares, but the ones written in Haskell grew much more gracefully and reliably.
Rumor has it that the [Dead Tree Version](http://shop.oreilly.com/product/9780596514983.do) never goes offline.
Thanks for that. I was going to try to explain the error, but I've been out of the Haskell world for a while and forgot about the monomorphism restriction.
Haskell is unusual in that functions are curried by default. Almost all other languages are not. When you write const x y = x you probably don't *think* of it as a higher-order function. You *think* of it as a function that takes two inputs. But The Truth is, it *is* a higher-order function. const 3 :: Num a =&gt; b -&gt; a I think its fair to say that in Haskell literature, when someone refers to a "higher-order function" they are *probably* referring to a function that takes a function as input (possibly not even the first input). forM_ :: Monad m =&gt; [a] -&gt; (a -&gt; m b) -&gt; m () You would probably call this a "higher-order function" in the sense that it takes a function as input, even though technically it does not; it only takes a `[a]` and *produces* such a higher-order function. [a] -&gt; ((a -&gt; m b) -&gt; m ()) So it's good to be aware of how the literature might use the term, but I see no reason to concretely declare that "higher-order function" means something different in Haskell parlance than it does anywhere else.
Well, I ended up with a generic divide-and-conquer function, where you can specify the maximum depth. I tested it out with a merge sort; it's interesting to be able to have a "somewhat" sorted list. Anyways, not sure how helpful it would be, but you could do something similar for Eq-like and Ord-like typeclasses, and simply give up with a result of EQ if the maximum depth is reached.
Well, there's the cross-platform bytecode, which isn't a maven feature, but it's a language feature that makes maven's job much easier. One could imagine adding a ports-style system to Cabal where your cabal client would fetch precompiled binary code from a public repository, falling back to source if a platform-specific binary is not available. Also, (and I don't understand how this works in Haskell/caba/ghc-pkg, so I misunderstand it), Haskell has ghc-pkg underlying cabal. ghc-pkg registers and hide/expose packages and versions. I think the benefit is that Project X doesn't have to explicitly name versions for all its dependencies (but that becomes a misfeature in complex environments), it just gets the "currently exposed version" from ghc-pkg. It gets weird when I want to upgrade a version of a package, because of something about how sharing works. In maven, your Java installation system doesn't care a whit about which packages are your favorites, or that you have different favorites for different projects or libraries. Instead, each project has its own POM file that declares its dependency packages and versions. Thus, as long as I have downloaded all the dependencies I need, there is no way to mangle project X by fiddling with packages and versions on my system. The worst I can do is delete a needed version, and the fix for that is to just reinstall it. (And that reinstallation, including download from an approved remote repository, is a built-in feature in maven. When I ask maven to *compile* my project, maven will just download any missing packages, with no fuss. With Haskell, my build will fail, and I have to flip over to "cabal install" to get the packages I need.)
That's a great talk. This talk was the first time I realized that a language with no null pointer exceptions was possible. Amazing realization.
&gt; GHCi is not Haskell Then maybe it should be renamed G-definitely-nothing-to-do-with-haskell-Ci
&gt; who tried to use his begriffsschrift to figure out how to deport Jews Everybody who knows what the "Begriffsschrift" is, knows that what you claim here must be plain nonsense by necessity. 
Those guys got snapped up by Twitter. Who knows, maybe part of the Twitter ecosystem runs on Smalltalk?
Perhaps you should read http://en.wikipedia.org/wiki/Gottlob_Frege#1924_diary and Michael Dummett's comments on Frege because you go making such silly remarks.
I just like enlightening people to hilarious incongruencies in otherwise incredibly smart people. Perhaps voxfrege was interpreting what I said as some sort of critique of the language in question, or even the man's logical works. It was nothing of the sort, any more than Phil Wadler's comment about Gentzen being a stormtrooper during his Google Talk was meant to do anything other than observe a really bizarre incongruency.
&gt;&gt; Can complex numbers in Haskell do something more than they would in say Python or C++? &gt; &gt; Yes. &gt; &gt; let x = 3 + 4*i &gt; &gt;works without needing to overload a special version of + for Complex that works on integers. You are mistaken, because it is possible to do it in C++ without needing to write an `operator+(int,Complex)`. The way you'd do it is, roughly: class Complex { Complex(float real, float imag) //... create a Complex from two floats Complex(int) //...create a complex number from an int } Complex i = Complex(0, 1); Complex operator+(Complex a, Complex b); Complex operator*(Complex a, Complex b); and then when the compiler sees: x = 3 + 4*i; it writes code something like this: x = complex+(Complex(3), complex*(Complex(4), i));
Please provide evidence that he "used the Begriffsschrift to figure out how to deport Jews". That the diaries express antisemitic sentiments is not the question.
The one in Debian testing, 7.0.4 ATM.
Looks like a bug to me too.
The one in my package manager. The Ubuntu 11.04 system I run still has 6.12.3.
I never really had a problem with Cabal, even as a beginner. What exactly is not easy about it? I'm genuinely curious
You must be quicker on the uptake than me! Honestly, I can't remember exactly. I just remember having trouble finding instructions, figuring out what to do with a cabal downloaded file (perhaps it was one that couldn't be installed from the command line?), running into what seemed like inexplicable dependency issues. I don't recall exactly, and I barely ever use it, but I do remember it being one more thing a beginner can trip on.
How would you prove that the following two functions are equal to each other? f :: Integer -&gt; Int f n = if (n &lt; 3) then 0 else length [(a,b,c) | a &lt;- [1..n], b &lt;- [1..n], c &lt;- [1..n], a^n + b^n == c ^ n ] g :: Integer -&gt; Int g n = 0
I happened to have this recommended to me in another thread where I mentioned how I felt I just didn't "get" functional programming. I read a part of "Learn you a Haskell..." previously. I read Paul Graham's "ANSI Common Lisp". I read... a lot of stuff. Whatever the exact language, I understand the language syntax and constructs but whenever I work on something non-trivial, I'm stuck. Like I said, it's like knowing all the words of a language but being unable to make sentences beyong "Me like apples." So I started reading this book... and it's got me shit-confused. The first thing to realize is this is NOT a self-contained book (i.e. no need to learn stuff from somewhere else). It barely explains anything. For instance, one of the exercises states to take 2 command line arguments, convert them to numbers, add them and display the result. I started by simply trying to convert a number using what had been show up to this point. main = do args &lt;- getArgs number &lt;- read (args !! 0) putStrLn ("Hello, " ++ show number) To an imperative/OO programmer like me, this made perfect sense. Obviously, GHC didn't agree - and with good reason. I spent an hour figuring out what was wrong and much face-palming ensued. I'm just glad I had read "Learn you..." before and finally tried using a **let** statement. So basically, after skimming through the rest of the material, this looks like a very solid **second** Haskell read. Learn the words first, and this might help you figuring out how to build sentences.
This is a bug. It's [Trac 5719](http://hackage.haskell.org/trac/ghc/ticket/5719) specifically. It's already fixed in HEAD: $ ~HSENV ghc --version The Glorious Glasgow Haskell Compilation System, version 7.5.20120112 $ ~HSENV cat &gt; constraint-kinds.hs {-# LANGUAGE TypeFamilies, RankNTypes, ConstraintKinds #-} import GHC.Prim(Constraint) class F fa where type Cxt fa :: Constraint type Cxt fa = () feed :: (Cxt fa) =&gt; fa -&gt; (forall a. a -&gt; a) -&gt; fa instance F [a] where feed x f = f x foo = feed [1,2,3] $ ~HSENV ghci constraint-kinds.hs GHCi, version 7.5.20120112: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. [1 of 1] Compiling Main ( constraint-kinds.hs, interpreted ) Ok, modules loaded: Main. *Main&gt; ^C This was found by Bas van Dijk and reported [here](http://www.haskell.org/pipermail/glasgow-haskell-users/2011-December/021318.html). Simon later said this was a bug and fixed it, but the changes never got merged into the 7.4 branch (which had split at that time.) I reported this to Ian when RC2 went live to see if it could be merged, but unfortunately it was too late and the patches didn't apply cleanly anymore [so it was left out](http://www.haskell.org/pipermail/glasgow-haskell-users/2012-February/021757.html). Bas outlines a workaround in his original email. You'll have to use it in the meantime I suppose. :(
Yea, it really shouldn't be called an intro book. It's a great way to learn how to compose all the pieces you've picked up into a full program though.
But wasn't this particular problem more of a Haskell peculiarity (as to what the do notation actually means) than something wrong with your thinking? I mean, the `let` statement means pretty much the same thing as a bind from an algorithm point of view.
Meanwhile, it looks like you do not uphold that claim anymore, as you deleted your comment. Got skeptical yourself?
 hasBars n xs = all isBar $ take n xs
It relies on the totality of the predicate you pass in. Here's a simpler version: data Nat = Zero | Succ Nat deriving (Show, Eq) searchNat :: (Nat -&gt; Bool) -&gt; Nat searchNat predicate = if predicate Zero then Zero else Succ (searchNat (\x -&gt; predicate (Succ x)) forsome :: (Nat -&gt; Bool) -&gt; Bool forsome predicate = predicate (searchNat predicate) Since `predicate` is required to be total, in particular it has to terminate on `infinity = Succ infinity`. That is, it can only look at a limited number of `Succ` constructors before it returns. plus Zero y = y plus (Succ x) = Succ (plus x y) times Zero _ = Zero times (Succ x) y = plus y (times x y) fromInt :: Int -&gt; Nat fromInt 0 = Zero fromInt n = Succ (fromInt (n-1)) test :: Bool test = forsome (\x -&gt; times x x == fromInt 15) Here test tries to find whether any number squared == 15. (Hint: it returns False). How does this work? Well, searchNat never finds an answer, so it returns `(Succ (Succ (Succ (.... forever))))`; we'll call that `infinity`. Then `(== fromInt 15)` peels off fifteen Succs and gets to the case `Succ _ == Zero` and returns False. Note that this fails for predicates that don't terminate on infinity, for example, `forsome (\x -&gt; x == Succ x)` diverges.
I hate being the negative guy, but I really doubt that will happen. Yesod is just more of the same old faux-mvc junk that rails has spawned. It is a fundamentally bad design for web development, limits code reuse and testability, and yesod offers very little benefit vs rails and the million other rails clones. Happstack at least tried to do something different, which is what needs to happen for a new framework to take off. Happstack was clearly not the right something different, but at least it was something different. Notice how much success lift got (scala framework) despite being buggy, undocumented, and only partially working. You can't offer people a better way to do things by copying the old way.
You're missing the point. What your professor meant is that in the general case, you cannot prove whether two arbitrary functions are equivalent. He didn't mean that there are no functions for which you can prove their equivalence.
&gt; char: bad arg Hmmm. Unicode? Did you try changing your locale from POSIX to an utf8-enabled one?
As others have pointed out, your prof. should have said you can't *in general* prove two functions are equivalent. If there were a way to do this in general, one could do all kinds of magical things, like proving Goldbach's conjecture by defining a function *f* that returns an infinite list of all Int's &gt; 2 which are the sum of two primes, and another function *g* = [4,6..]. Then just apply your algorithm to prove or disprove f = g.
The roadmap for hackage inevitably includes support for user-submitted build reports coming from Cabal, so instead a package page will show what GHC versions/platforms/constraints it built under, as opposed to just having 'fail/pass' like it does now (many packages fail to build for example because they lack FFI bindings on the machine.) Of course, the people working on Hackage 2 are short in number. It's not the most glamorous of tasks, but sorely needed.
Mozilla is using it as part of one of their research projects to develop a parallel browser engine (*it will not be used for Firefox*.) If it doesn't succeed outside Mozilla and instead is only used for this, I think they consider it a success anyway (their words, IIRC.) The niche it's meant to fill is similar to that of something like Go or D: safe and robust systems level programming. It attempts to go by good research rather than innovate itself (the combination of features is kind of novel.) By default there are no NULL pointers (instead they use option types,) no double-free/use-after-free, any pointer manipulation is relegated to an unsafe sub language for C interfacing, etc. It is immutable by default. They want it to have predictable, good performance (but it's unoptimized right now.) There are different storage semantics for different requirements on memory management, which is important at low levels. It has concurrent tasks built in, much like D2 or Go. It also has actual generics, unlike Go - turns out that's kind of a big deal. It also has other nice features like batch compilation, ADTs/pattern matching (which is a huge thing by itself, I don't know why any modern, statically typed language does not have them these days,) lambdas (which can be stack-allocated, GC'd, etc), macros, type inference, resources (reminiscent of RAII,) and they're working on the encoding/unicode story now. It also now has Haskell-style typeclasses, which they call interfaces. Most of these are features I miss a lot in many languages (lambdas and ADTs especially.) There's also typestate which is a type-level predicate language the compiler enforces, but I don't know if it'll stay the way it is now (there's been quite a bit of talk of removing/improving it, it's somewhat unsatisfactory at the moment.) On the whole I think it's a pretty nice language with great ideas for its domain. There's still a lot of semantic churn and fluxuation going on everywhere (bind syntax, a FFI for the Rust runtime, operator overloading, regions, etc.) It's somewhat large as a language, but I don't think that's too much of a problem as long as the features are cohesive (bare minimal languages just shift the burden onto library authors and users like Scheme, and complex languages with feature-collision give you headaches, like C++.)
How about this: http://stackoverflow.com/questions/3581381/how-does-the-haskell-compiler-handle-the-where-statement Might have to add a case for the empty list. 
Yes, that's probably how I would write it, with a base case for the empty list returning `True` since `head` is partial: same :: Eq a =&gt; [a] -&gt; Bool same [] = True same (x:xs) = all (==x) xs EDIT: for fun, it would also be nice if we had a safe `head`, then it could just be: same xs = maybe True (\x -&gt; all (==x) xs) (headMay xs)
Even if we don’t have a safe `head`, it is still possible: same s = and $ zipWith (==) s (tail s)
Thanks everybody, I kicked myself!
I think ListLike attempts have simply been too monolithic and not designed well. I think a good design for list-like is probably possible, though, and the design space was not really explored, even without constraint kinds.
7.2.2 - mainly because it's in the OpenIndiana pkg repos. Kinda weird that they are more up to date than Arch or Debian.
I found their prebuilt osx packages (looks like "GHC-7.4.1-x86_64.pkg") install and run without problem. Some of the packages on Hackage need tweaking though, but mostly it's just getting environment variables and configuration paths right. edit: Oh you said 10.5, nm
Yeah. I wish there was someway to tell GHC that I want string literals to be the type Text or Lazy Text instead of String. The OverloadedStrings extensions makes the literals (IsString a), which just causes a bunch of overlapping instances problems.
But I want to match against : and [] :(
Why not use a polymorphic fold instead?
Causes ambiguous types, mostly. Overlapping instances are caused by instance definitions.
&gt; If I need the performance of manual memory management, then I'll need to be precise enough about it to just use C You can continuously apply this reasoning. If I need the performance of C, I'll need to be precise enough to use assembly anyway, right? Some features of C++ are very welcome over C IMO (notably generics and namespaces; the language is a bit more typesafe,) but both languages are riddled with sharp edges. &gt; For example, there doesn't appear to be an exception system. So how would failures be reported? Exceptions don't exist because it's hard to reason about program state at the time of resumption. Typestate predicates are an example: should an exception occur and be recovered from later, what's the state of the programs typestate predicates at that point? This is probably do-able but complex to achieve in a way that isn't magical or brittle I'd imagine. There have been several mailing list discussions. Failure is designed around tasks, which are also the basic notion of concurrency; failure of a task is reported to its supervisor and the task dies and is unwound, freeing all resources (which provide RAII-like destruction once they go out of scope) in the call-chain. This is similar to Erlang's philosophy. &gt; But then, so do lots of things (D and go are both aiming at that arena). I'm not seeing anything in Rust that would appeal to a C++ programer. Rust is significantly safer than Go, D is probably more on par, but neither for example eliminate NULL pointers which is fantastic by itself. Second, there are plenty of features I think many C++ programmers would find valuable like I outlined above: type inference and lambdas, which are in C++11 anyway, pattern matching and ADTs, macros, and a non-insane compilation model. Some of these exist in D too. &gt; the get operation on a map just says it fails. Hrm, that's not really clear what that means yet. It means the executing task dies and is unwound. Personally I would say `get` should return an `option` type indicating failure by default, with an unsafe alternative. Failing a task is a bit overkill. They'd probably accept a patch for this I'd think.
&gt; Ok, but pretty much everything you've listed there is something Haskell has. Except predictable performance. There are a lot of domains that Haskell is not very well suited towards because the combination of laziness and gc makes worst-case latencies essentially unknowable. Rust has been designed from the outset to constrain worst-case latency. This matters a lot in, for example, computer games.
Using lazy patterns: same xs@(~(x:_)) = all (==x) xs
&gt; Well, yes, of course. But in practice, it's faster to write the C, check the profiler, and hand optimise the specific trouble spots. And in practice it's faster to use the STL, Templates, RAII, namespace and exceptions to make my program more robust, rather than a metric clusterfuck of return-value checking and corner cases, and painful repetition at different types (CPP needs to die anyway, so don't point that out,) and then hand optimize the specific trouble spots after the fact. &gt; On the points about Rust and exceptions: The main take away I'm getting is just reinforce the feeling that it's not yet mature enough to say much about; that failure handling model is not documented in the tutorial; nor is it clear from the reference that this is the dominant paradigm. It's passed over in the Task section near the end, but it could be more complete and pointed out as the default mode of operation, I agree.
&gt; What is the other stuff that Rust has that would make one use it instead? This is a complex question; you'll get 20 answers if you ask 10 people. If it was vs Haskell, I'd probably go with Haskell because it's more mature and I'm more intimately familiar with it. But if you were giving me the choice between this and Go, I'd probably rather have rust: it has a better feature set by almost every measure and is safer to boot. You pose this as someone who writes Haskell, so the feature set looks like what you always deal with. Of course it doesn't look like anything special - it's certainly not new to me. What about someone who *doesn't* know Haskell, but knows, say Java? Or even Go? Why would *they* choose Rust over XYZ? I'd probably say because it has a more familiar syntax, provides them with more safety than other pointer-riffic languages, and it has a more familiar performance/evaluation model. It also has lots of other groovy features, like anonymous functions, and concurrency built-in. Languages don't exist in a vacuum on their technical merits alone, so maybe there's a reason people actually use languages like Go for example (which I personally think is a joke, almost - no generics is braindead,) or why Haskell hasn't taken over the world, despite amazing technical merits. &gt; Doesn't appear to have that to me, short of linking to C - in which case, Haskell can still do that. No. The main detail is that all values may have different kinds of storage semantics, which is the main point - you don't have to drop to C in order to have knowledge about whether or not a value is stack-allocated, or GC'd, for example. Both are useful in their own right. These semantics also apply to functions, so you're aware when say, a lambda is actually stack allocated. There are other benefits; unique pointers and move semantics give you a very cheap means of sharing data between tasks safely (since only one reference to a unique pointer can exist at any one time, you can move uniques between threads safely at only the cost of copying a pointer, etc.) GHC actually allows a bit more fine grained control of memory layout too if you're aware of the heap layout and say, UNPACK pragmas and unboxed tuples. You don't get total control, but it's quite helpful for speed/cache. They'll let you unpack composite data types completely, removing the overhood of an indirection through the constructor. That's better than a lot of GC'd languages will let you do (like Java.) &gt; Closer to the machine, for easier compilation? Maybe, but that'll take the optimiser to be implemented before we can judge if it's helping. I think the optimizer story is interesting, because if we're to look at Haskell for example, people are way dependent on GHC's optimizer behavior for some stuff, and changes in its optimizer have proven to be brittle or necessary for reasonable space/time usage in some scenarios (and even this is hazy; ref. transparency says we can do unlimited inlining/CSE, but in practice we don't, because it will kill sharing, which can turn a program that terminates correctly into one that doesn't.) It's much more predictable than in the past, and on the whole it's really good, I'm still floored at how fast/efficient GHC is on average, but it's still something of a brittle point sometimes. &gt; It looks like it's aiming for 'a bit of everything' approach, to being broadly applicable. In which case, it's way too early to say anything about it - at a minimum, it'll need some performance behind it, so shootout benchmarks or similar. Yes, it is very young. Haven written some small one-off programs in it (stupid stuff; a thread-ring implementation, some C bindings, etc) I still think the feature set is quite nice, and for the space and competitors it's up against, like Go or D, I think it's a very nice contender.
In general I think it's more that the performance model of say, Rust, is *more familiar*. It's very possible to reason about performance and memory use in Haskell and GHC, and more abstractly, the cost of an expression, it's just different - laziness can result in non-local reasoning of space/time use, because expensive operations may not actually be expensive if you don't look at them. As a result, reasoning about the cost of an expression happens at the *use site*, not the definition. I agree overall though, that GC/laziness can possibly have very unpredictable performance/latency properties, which is crucial for some applications.
&gt; the combination of laziness and gc makes worst-case latencies essentially unknowable I hate it when people make this claim. It's usually based on little more than hearsay and perhaps some negative experiences brought about by trying to pretend that GHC is magic. Laziness is not unpredictable. I've actually been bitten by strictness in OCaml a lot since I've started using it. I'm not trying to say something negative about strictness; it's just that I had gotten used to laziness. Conversely, those who are used to strictness tend to struggle with laziness more than those who are not.
Sounds too advanced for me :(
This tutorial went a long way towards convincing me that most non-trivial Haskell programs are going to be indistinguishable from a big ball of mud.
An algorithm point of view that doesn't make explicit note of when side effects may occur, as Haskell does.
&gt; non-trivial programs are going to be indistinguishable from a big ball of mud. ftfy. imho large Haskell code tends to be a much prettier ball of mud than large SomeOtherLanguage code.
You mean a safe `head` *in standard libraries*, I assume? http://hackage.haskell.org/package/safe
Yes, thanks (safe is actually where I pulled `headMay` from.)
Well, pattern-matching against (:) and [] is just what foldr does for you. You give it arguments specifying how to handle (:) and [] cases. It will also recurse for you (and you may not want this, so we can write another fold function which doesn't recurse). Then, you can use this foldr function with non-lists too (e.g: Text.Lazy). The main problem with this approach is that if you want to use more complicated pattern matchers (multiple/nested patterns), then this doesn't really work well. But for the base case/simple matcher it works well.
This answer needs more love! Basically: 1. For simple algebraic formula, they can be reduced to normal form and compared piecemeal. 2. For provably halting programs (e.g. Total Functional Programming, most probably by using Agda or similar), they can be either reduced to normal form and compared piecemeal, or be proven by induction, or something. 3. For a provably halting program and a provably non-halting program, you know they are non-equivalent. 4. For all programs in general, you can't prove equivalance or non-equivalence.
lol
oops, that's what I meant. I knew something sounded wrong. Regardless of the name of the error, it is still annoying :)
&gt; It means the executing task dies and is unwound. Personally I would say get should return an option type indicating failure by default, with an unsafe alternative. Failing a task is a bit overkill. They'd probably accept a patch for this I'd think. There is another method, called lookup() I believe, which returns an option type. get() is for convenience when you know the value is in the map.
Sure, but then the program to test them no longer fits in memory :)
Apart from the verbose syntax, what are the problems you see with view patterns?
or just `same xs = all (== head xs) xs`
The general case is not just "very hard", it's "provably impossible".
No, that's it... just the verbosity versus directly pattern matching on list constructors.
Isn't that more of an issue with the GHC RTS not being optimized for a hard real-time environment?
Actually, if `length xs &lt; n` then mine will give the wrong answer.
Sorry not to answer your question directly, however, this should not be neccessary. Unless you have some very special needs, I would suggest you use one of the MongoDB [Snaplets](http://snapframework.com/snaplets) and modify them (or roll your own) if needed. You then simply add it into our App state data type and initialize it in the main application initializer. For the [snaplet-mongodb-minimalistic](https://github.com/Palmik/snaplet-mongodb-minimalistic) you can optionally (in case you plan to have only one MognoDB snaplet in your application) make your App state an instance of HasMongoDB class to make the usage less verbose.
~~I think that doesn't work for the empty list, since *group []* is *[]*.~~
It's really just a point free conversion, and should be perfectly readable to anyone who is comfortable with monadic bind (which if you are a Haskell programmer this is probably the case).
That depends what you mean by "equal": if you mean "always return the same value for the same inputs", you're right. If you mean "have identical source code" then it's quite easy to tell whether they are equal :-)
I thought immutable was transitive in Rust. Immutable data can be shared between tasks while mutable cannot. 
GHC's GC is generational. [Here's a paper from '04 on incremental GC in GHC.](http://research.microsoft.com/en-us/um/people/simonpj/Papers/non-stop/index.htm)
Although that argument is not specific to functions. x, y :: Int x = 1 y = if x == y then x + 1 else x All this really shows is that your `b` and my `y` are absurd, they're not good arguments for `==` on functions or `Int` being absurd.
After getting through the first 2 chapters, I have to agree with you. Although the book says you can learn Haskell, I was having to guess the constructs used by about page 4, and felt like I was just copy-pasting code. Also, he does the 'let's use a smart library to do the heavy lifting' trick, which is a smart way for production code but shitty for a tutorial.
Yes, I have. Also, this has basically nothing to do with laziness.
A: I believe they plan to use multi-instantiation. B: The Rust developers have been changing this story recently -- I think the current story resembles type classes. See http://doc.rust-lang.org/doc/tutorial.html#interfaces
This is a fairly involved issue that has been under quite a bit of discussion lately. The short answer is that Snap currently uses MonadCatchIO instead of MonadBaseControl for exception handling. These are two different approaches to the same underlying issue. However, different people have different concerns about both formulations. As of yet, the community has not reached consensus on how to solve this. See the following links for more information. - http://blog.ezyang.com/2012/01/monadbasecontrol-is-unsound/ - http://blog.ezyang.com/2012/01/modelling-io/ - http://www.haskell.org/pipermail/haskell-cafe/2012-January/098389.html That said, I'll echo Palmik's suggestion that you check out the existing snaplets providing MongoDB support for Snap.
I ever only see this example.
&gt; the way it forces you into a specific OO coding style What are you talking about? Python is multi-paradigm, and using too much OOP is even frowned upon.
Hey, I ran into that problem as well. My solution was not to concatenate the number as in main = do args &lt;- getArgs putStrLn ("Hello, " ++ show (read (args !! 0) :: Int)) How do you solve it with a let statement?
Am I the only one who thinks persistent's SQL support is bizarrely out of character for the haskell world? Normally haskell land is all about doing things the most correct way possible, and persistent seems to be actively encouraging people to do things wrong. Even on the basic level there are weird assumptions about how SQL databases work that make no sense, like expecting every table to have a single int column as the primary key. Where did that come from? &gt;SQL does not explicitly support complex data structures Since when? Relational databases are all about storing very complex data models in a way that retains consistency and allows ad-hoc querying. &gt;However it is a common strategy to serialize data structures to something like a JSON string and then save them to a SQL column This is horrible and the completely wrong approach. This is the kind of thing you see in php/mysql land, but it seems very out of place in the world of haskell and postgresql. Relational databases are to store data in a relational manner. Lists and sets in particular are trivially represented as tables, and this is in fact the entire purpose of relational databases. Serializing these things and dumping them into a single row in a single column not only kills your ability to query them, but also kills your consistency guarantees. Is this just a matter of not having any RDBMS people working on yesod? Is there a real need to try to provide the same interface for very different storage engines? Would there be any interest in making persistent into a nosql-specific library and doing something similar to scala-query or macaque for SQL databases?
Oh I see. Serves me right for commenting without access to ghci.
I had not seen the snaplets, I'll check those out, thanks.
&gt;Strings are just lists of characters, but still SQL databases support them natively. It also supports operations on them. &gt;That's right, but there's nothing stopping SQL databases from natively supporting JSON. There's nothing stopping them from storing your data in /dev/null either, but encouraging bad choices doesn't make them good choices. JSON is an exchange format. Storing it in a database with full support for everything the database offers requires fully re-implementing the entire database engine to work on JSON instead of working on relations. This is pretty silly considering you can just use relations. If you don't want relations, why are you using a relational database?
&gt;Yesod is very used to dealing with Ivory Tower arguments by now :) That is, simply asserting that something is the theoretically "correct" does not make it work for all use cases in real life. Seeing someone in the haskell community dismiss correctness as "ivory tower" would be funny if it weren't so sad. What part of the relational model does not work for all use cases in real life exactly? And which part of it makes horrors like throwing your data consistency away the appropriate solution rather than using a network hash table like mongo? &gt;This SQL pattern makes a trade-off in favor of easy scalability at the expense of making querying difficult and making consistent partial updates of an embedded data structure impossible This SQL anti-pattern exists only in the world of PHP/mysql "learning is hard so lets not bother" mentalities. And it hurts scalability, it doesn't help it. I always picture haskell land as a place where people go and pick up a copy of An Introduction to Database Systems and learn rather than copying PHP worst practices. &gt;and we believe users will be much happier with this as an option than with nothing. Certainly some set of users want this. My question was, is there a lot of overlap between that set and the set of people who like haskell? It would certainly not seem out of place in PHP land, but in haskell land it seems bizarre, as haskell's ecosystem is often heavily focused on correctness. It seems odd for a community that gets excited about providing compile time correctness checks to want support for a pattern that can be summarized as "throw away support for correctness checks because I don't know how to write a join". &gt;Bottom line is that if this is not appropriate for your use case then please don't use it for it. Obviously I am not using it. I didn't ask "what should I do?", I asked "am I crazy, or does this seem very anti-haskell in nature?". &gt;It sounds like you are passionate about supporting other use cases. Please get involved in development and send us a patch. I don't want to sound to committal, as I work in scala and only get to haskell in my spare time for fun, but this is why I asked the questions in the last paragraph. Persistent is very much anti-relational in it's current form, and whether or not that will or should change is largely dependent on why that is the case. As I said, I can't just "send a patch", as persistent makes too many fundamental assumptions that don't make sense for relational databases. We'd need something designed specifically for RDBMS support like scala-query or macaque. So again the question becomes, is there interest from the powers that be in having such a library supported in yesod "out of the box"?
My impression was that the new XCode is not required at all, they just switched because Apple stopped shipping old XCode/SDKs. You can compile it yourself if you are in the mood (I never tried, but other people had success). I actually think the whole issue could be solved simply by offering a flag deciding which version of the system SDK should GHC link against (for example on my machine there are two SDKs, one for 10.4 and one for 10.5. On new machines, by default, there is only the newest, I believe). You may need minor changes in the runtime, but I believe they are really minor. See also this [somewhat related ticket](http://hackage.haskell.org/trac/ghc/ticket/3354)
Yeah. I've been wanting 'StringLiteralsAs' lately, too. Seems like it's time to dump String for Text altogether; having both is just painful.
&gt;It would make it easier to have a conversation if you calm down and drop all the rhetoric Your perception of my calmness (or lack thereof) is not accurate. And I have not used rhetoric. &gt;I didn't realize that you weren't aware of the motivating use case for embedding JSON into SQL column values I am aware of why people do it. Because they don't understand the relational model. Scalability problems are not solved by putting encoded data in an SQL database, they are worked around by using a distributed hash table and writing a specialized database on top of it yourself. Facebook is a bad example as they do tons of things horribly wrong and cost themselves millions of dollars as a result, but even they don't encode multiple pieces of data into one column in an SQL DB, they use cassandra for those things. &gt;many data sets do not get to a large scale Have you actually tried using relational databases for large data sets? I think where you perceive the line of "too big for sql" to be is far lower than where it really is. We have 143 tables, the smallest of which (excluding enum style tables) is 140 million rows. The largest broke 12 billion rows last month. The average query joins over 30 tables. None of this is a problem at all. The vast majority of web apps fall well within the "easily handled in a normalized relational database" spectrum. Just because the "mongodb is web scale" fad is big right now, doesn't mean the last 40 years of accumulated knowledge was wrong.
Is the difference between your two `b`s that the first is effectively `\_ -&gt; ⊥` while the second is effectively just `⊥`? Then they could only be distinguished using `seq`, and that doesn't seem a very meaningful difference.
&gt;Greg isn't dismissing correctness as "ivory tower," he's dismissing assertions of correctness as ivory tower. Which is just as absurd as dismissing static type checking as being ivory tower. Let me ask you: is it invalid to have an "any" type in haskell that throws all static type checks out the window? Of course it isn't invalid, but it certainly offers a lot less in the way of correctness guarantees. Suggesting that people "need" to store structured data in an unstructured way inside of a structured data store makes just as little sense as suggesting that people "need" to have untyped variables in a statically typed language. People "want" that, because they don't want to learn how to do things the "correct" way, where "correct" is the way the system is designed to work. I am not suggesting SQL DBs are objectively correct any more than I am suggesting haskell is objectively correct. I am saying that my perception of haskell is that the community is largely people who value doing things "correctly" where correct is "if the case is already handled by a set of mathematical rules, then do that instead of using an ugly cludge". &gt;However, what makes you think that Persistent is non-relational? The fact that adding support for side-stepping the relational model was done, but support for using the relational model was not? Persistent can serialize a list into JSON and stick it in a column, but can't do the standard "put that list into a related table". Joins weren't initially supported, and were added as an after thought, but they are the fundamental operator of relational databases. The support for joins is still limited to single columns I think? And as I mentioned, there is this bizarre requirement to create single column integer primary keys on all tables. I realize that is a very common anti-pattern in the PHP/mysql world, but it is a fundamental barrier to supporting actual relational operations. I understand that I come off sounding like I am bitching about yesod, but that really isn't my intention. I am just genuinely curious if others share my perception of persistent as being un-haskelly (un-haskellish?). I'm not saying it is bad or that you shouldn't do it, just wondering if I am alone in thinking it an odd approach for the language it lives in.
 Scalability problems are not solved by putting encoded data in an SQL database, they are worked around by using a distributed hash table and writing a specialized database on top of it yourself. On the one hand we are in agreement that this is a great approach. On the other hand this is a funny troll comment the way it is stated as the absolute best way. Often times people need to get things done right now and have limited time/money/approval to write specialized databases or deal with the additional infrastructure complexity of bringing on a new database. I hope you understand the use case now, or at least stop pretending as if you don't. Perhaps if you wrote some nice libraries for people they might be able to use the solutions you advocate on more projects.
It seems to be a little old. Do you think the situation is better now?
&gt;On the other hand this is a funny troll comment the way it is stated as the absolute best way. Often times people need to get things done right now and have limited time/money/approval to write specialized databases or deal with the additional infrastructure complexity of bringing on a new database. You seem to have misinterpreted my statement. It is not a value judgement, it is a statement of fact. Serializing a list as a JSON string and dumping it into a varchar column does not solve any scalability problems. &gt;I hope you understand the use case now, or at least stop pretending as if you don't I explicitly stated that I do understand the use case ("I am aware of why people do it. Because they don't understand the relational model"). I simply questioned whether catering to this use case is a "haskelly" thing to do.
It wasn't intended to be "trollish". You'll notice I didn't claim to be smart at all, nor suggest anyone else was dumb. It was a genuine question that I honestly wondered about. It could very well be that I am in the minority and most people don't find it an odd fit at all. I have no way to guess other people's subjective opinions on style. &gt;Do not like it ? Do not use it. Why is this the standard response when it is an obvious non-sequitur? I didn't ask "should I use this?", I am stuck in scala land anyways. I just was curious if it seemed out of place to others, and that is the question I asked. It just struck me as odd that yesod appears much more popular than happstack, while happstack would seem to be more stereotypically "haskellish" to me. &gt;And i am in that group. Are we defining "enterprise" as "places that only use expensive products" or a more general "big stuff places"? I don't know that the former will use haskell either, so the lack of oracle support may not be such an issue. As to the latter group, we're plenty happy with postgres.
Perhaps your question looks like trolling because you are assuming that just because some random dudes on the internet implemented an arbitrary library doing arbitrary things in haskell, then it must be because they have a blessing of entire haskell community or at least some secret "steering committee" and that's how "things get done in haskell" and that's whom you are pleading here. Lets try to entertain a possibility that it's just random dudes doing random things that has nothing to do with the rest of haskell community and how "things are done" or supposed to be done :) In the end haskell has libraries like HaskellDB
But the hard (impossible) problem is checking for equivalence in the general case. Anyone can write a function that gives up if something unexpected happens.
In dependently typed languages, there's this semi-convention for representing type defs as data. So in Agda, for instance data Desc : Set1 where done : Desc arg : (A : Set) -&gt; (A -&gt; Desc) -&gt; Desc rec : Desc -&gt; Desc And you give an interpretation of this into an Agda function as [[_]] : Desc -&gt; Set -&gt; Set [[ done ]] X = Unit [[ arg A D ]] X = Sigma A \ a -&gt; [[ D a ]] X [[ rec D ]] X = X * [[ D ]] X So for instance the type of natural numbers might have the description NatDesc : Desc NatDesc = arg Bool \ b -&gt; if b then rec done else done When you interpret this using `[[_]]` you get [[ NatDesc ]] X = [[ arg Bool \ b -&gt; if b then rec done else done ]] X = Sigma Bool \ b -&gt; [[ if b then rec done else done ]] X = Sigma Bool \ b -&gt; if b then [[ rec done ]] X else [[ done ]] X = Sigma Bool \ b -&gt; if b then X * [[ done ]] X else [[ done ]] X = Sigma Bool \ b -&gt; if b then X * Unit else Unit So now if you take a fixed point of `[[ NatDesc ]]` you get the naturals. Nat : Set Nat = mu [[ NatDesc ]] zero : Nat zero = (false , tt) suc : Nat -&gt; Nat suc n = (true , n , tt) Conor's [OAAO](https://personal.cis.strath.ac.uk/~conor/pub/OAAO/Ornament.pdf) goes more in depth on the topic. The way you want to think about this, tho, is that a value is a list (built out of existing types like products and Unit), and constructor choices are the first element of a list, and the nature of the rest of the list is determined by the choice of the first constructor (e.g. if you chose false, here, the list is just the type Unit, but if you choose true, the list is the recursive type argument followed by Unit, i.e. zero and suc).
I have mixed feelings about Persistent. Separating Key and Value and joining them with Entity is a good design decision. IMHO it makes good use of the type system, at the expense of making a few trivial operations a difficult to figure out for us newbies. What I think persistent is obviously good for is common code. The interfaces associated with PersistentUnique provide a nice interface which all the auth backends can share, which has already saved me a ton of work. The question is, do I want to use Persistent for all my app code? I don't know. I'm starting off that way. It remains to be seen if I will continue that or switch to raw MongoDB, or maybe that new rawSql thing if it turns out to make sense for mongoDB. 
Yeah who is this we we're speaking of? I didn't let the fact that I'm in a C and Perl shop stop me. And I'm only an egg.
Wait, it's not possible to cross-compile with GHC? Why not? I mean, if you have code for generating ARM assembly from Core, why can't you compile that code on x86 and get a cross-compiler?
What I got from this is that Delphi and R are difficult to learn.
Please detail the steps to generate ARM assembly from Core on an x86 host. The only cross-compiling method I have ever seen mentioned is the old C code generator backend, but it was already deprecated at the time and missing functionality. The NGC backend can only create code for the architecture it is compiled on unfortunately. The situation may finally be changing with the new LLVM backend though, sounds like a few issues still need resolving: http://hackage.haskell.org/trac/ghc/wiki/CrossCompilation Edit: [This diagram](http://3.bp.blogspot.com/_Br4acuar1QM/S-OwnYREOZI/AAAAAAAAAPw/2ZELH-agCFU/s400/pipeline.png) seems to suggest Core is pretty early in the pipeline. I'm not really sure what hand waving is necessary to get from there all the way to assembly code.
Unfortunately we're not large enough for that kind of investment to make sense for us.
Ah, very nice. Thanks!
Embedded systems are a hard place for almost all languages on that graph.
&gt;There is absolutely no sense in which they are a reasonable way to frame your argument. I am not making an argument. I have no interest in convincing the authors to do something different, I was interested in whether other people in the haskell community thought the existing design felt out of place.
Oh, I don't think Delphi is hard. I don't know how it is in US and other countries but Delphi used to be fairly big in Brazil, almost as much as VB. So it was very much a "blub" language, much more so than Java. And it's still widely used in legacy systems. As you can imagine, the average Delphi developer is much closer to "average" than to the kind of programmer that uses github. (I'm not saying that github users are geniuses, just statistically above average)
R is a pain. Used and developed apparently by non computer science people and it shows, as various choices of the language just ... *differ* ... from mainstream languages for no discernible reason. Like, arrays being 1 based, and methods like substring being *inclusive* rather than *exclusive*. It's just annoying.
I think things like "no one has heard of it" and "library quantity, not quality" have improved, but not significantly enough to reverse the statements. "can't hire programmers" is sort of true and untrue; there certainly *are* people that would love to ditch their Java/PHP/C++ job for a Haskell job. The trouble is these people generally aren't cheap; most people don't learn Haskell during their CS or ISYS or IT undergraduate (or graduate) degree, and since Haskell *is* different than the contemporary mainstream languages, managers are immediately repelled. You can't just throw a handful of Java programmers at a Haskell project and expect good things to happen. The Haskell community, toolchain, and libraries are growing and improving constantly. Look back in another 5 years, and I think you'll find Haskell still gradually gaining more and more traction. The only reason Haskell would grow stagnant is if academia got bored with it, and that seems unlikely to happen anytime soon.
For me, section 6 of [A History of Haskell](http://www.haskell.org/haskellwiki/History_of_Haskell) was quite enlightening. Granted, I read it back in 2007 when is was fresh. At the time it was the best whirlwind tour of type system features/extension I could find. What is missing is of course recent developments like Type Families and the GHC 7.4 goodies.
Basically, [Delphi](https://en.wikipedia.org/wiki/Embarcadero_Delphi) was the Windows offshoot of the famous Turbo Pascal. Its recent history is rather bumpy, though.
To clarify the point: the map is incrementally evaluated just in time as results are consumed, so the first element is off the map-evaluating stack before the recursive call happens. 
I know it's widely used. I use it every day, and hate it every day. It's nice in a few special circumstances, but mathematicians use it for everything, unfortunately.
Sure. I don't mean to criticize you. I don't even dream of introducing Haskell in my company. But I think it's funny to say that X is not practical enough, while not really having a practical need for X -- and in the business sense, "practical" simply means it helps you to make money ("essential" is too strong), and if so then a few grands doesn't sound a lot of money compared to, say, your yearly salary. 
&gt; If you don't want relations, why are you using a relational database? I'm not interested in debating the rest of Persistent's design with you (I don't know it, for one thing), but I think that this specific rhetorical question is slightly overstated. If you happen to need a dumb key-value store then using a relational database as if it were a dumb key-value store is a perfectly sensible thing to do because there are really, really high quality relational databases available off-the-shelf that work very well when you use them as if they were dumb key-value stores.
The book is now up-to-date with Yesod 0.10. I also have some reviewer feedback on the book sitting in my inbox, and plan on making a number of edits in the next week or so.
&gt; My impression was that the new XCode is not required at all I could be mistaken, or it could be just for the official binaries or something. I do recall trying my hand at it and running into the XCode problem when I did. Back when the decision was made to stop supporting the old XCode I was one of the folks opposing it precisely because of this issue, though I'm sure it's a lot easier now that they only have to support one XCode/SDK. Honestly, I'm not sure what XCode really has anything to do with in the GHC build chain (other than for doing Framework bundles as a distribution method).
&gt; Besides, isn't compiling any program with a large dependency chain hard with 6.12 nowadays? The only difficulties I've run into have had to do with things like the MTL-2 transition. But that has more to do with the cruft of a long-running system, rather than being an issue with it being old. Other than that I haven't had any issues. Most packages still use classic Haskell (H98+MPTCs+...) and haven't yet switched over to things which are GHC7-only. The day will come, now that type families seem to have settled down a bit; but it's still a ways off yet.
&gt; For example, dynamic linking is a relatively recent addition in GHC, and the lack of cross-compilation is still a serious show stopper. Maybe the GHC people are missing a trick here. GHC is written in Haskell, and Haskell is often touted as being good for writing compilers and other lanugage tools. So, why don't the GHC people write some really good language tools (possibly changing Haskell's semantics if that would help)? Then they could use them to write the next version of GHC a lot more quickly. By "really good language tools" the sort of thing I have in mind is [RPython](http://tratt.net/laurie/tech_articles/articles/fast_enough_vms_in_fast_enough_time), where if you write an interpreter in it, you get a JIT compiler for free.
There's also really high quality dumb key-value stores that work very well when you use them as dumb key-value stores. That's what I mean, "use the right tool for the job" seems more haskelly than "use the same tool for every job no matter what".
A bit tricky figuring out what the monad instance being used is.
I don't think there are much differences between the low-level APIs in different SDK versions. I once managed to link against 10.4 on a 10.5 system by recompiling the base library and the runtime, even though I know nothing about GHC internals and the methodology was randomly changing lines in the middle thousands-line configure scripts...
Link as proof please? I may have missed it four times in a row, but if I did, I don't feel like I'm likely to find it if I look a fifth time.
Thanks for pointing that out, I hadn't noticed it. After a brief review of the code, it looks like unix is just used for debug output. Perhaps we can get some Windows-equivalent code in there. I'll ping Leon.
Lua has 1-based arrays too. (Well, actually, you can make them start at 0, but the standard library functions won’t know about it.)
easily the most interesting thing happening in haskell right now
Horrible.
... And Leon already released a new version without a unix dependency: http://hackage.haskell.org/package/postgresql-libpq-0.7
&gt; But my current fun project has use "fun" technology as a requirement. I think that should go on a t-shirt somehow.
If you're only concerned with using Arrays, then: class Sortable t where -- having the parameters this way around means -- Array might be a Functor sort :: Array t e -&gt; Array Sorted e instance Sortable Sorted where sort = id instance Sortable Unsorted where sort = -- ... Alternatively, use a MPTC: class Sortable a t where sort :: a t e -&gt; a Sorted e instance Sortable Array Sorted where sort = id etc.
I also did a write up of this talk when he gave it at Galois a while back: http://blog.ezyang.com/2010/06/well-founded-recursion-in-agda/
2008 paper: [Parallel Generational-Copying Garbage Collection with a Block-Structured Heap](http://community.haskell.org/~simonmar/bib/parallel-gc-08_abstract.html)
&gt; Not without significant wrangling, making the code brittle and unmaintainable. So...like C++ code then? /troll
Not beyond simplistic testing.
I like Conor's response to the issue on ezyang's blog -- if you need well-founded recursion, you're using the wrong data structures. I'm not entirely sure I agree, but it's thought provoking and that's always fun!
Looks like an ideal problem for Prolog... Not a language I'd care to learn though. DFS and BFS both require a program to go through instructions and adds a branch to a stack and follows a different, until it's done with one particular 'thread', at which point it goes to the next branch in the stack. There is no 'algorithm', as far as I can tell, for getting through all the branches in the shortest amount of time, because you're essentially doing the same thing no matter what. If you're trying to the get to the goal in the least number of branches, then what algorithm (DFS, BFS, A*) does of course matter
Thank you very much for this talk Eric!
So, is there a reason for the left half of the audience to be exclusively composed of bald men ?
Not sure it is the most interesting thing happening in the Haskell world, but for sure it is the most publicized one, here and elsewhere ...
Interesting.
Snoyberg's a strong speaker. Good talk, covers the good points and snappy.
Something that would be nice is timing information. I'd love to see some criterion-esque kernel density estimates of the times each of my routes is taking =D. 
I started reading Mac Lane's *Algebra* in December and suddenly lattices are everywhere. I had thought lattices had dwindled as a topic of interest (at least in mathematics) since the the book was written. Is there a lattice resurgence in CS? It seems an obvious fit, but I hadn't heard mention of that pattern in my school days. 
Happstack uses web-routes, none have been implemented for Snap.
&gt; Inertia. And the learning curve. Haskell has the steepest learning curve I have encountered. (I program in other functional languages. So that's not a problem.)
I was able to download it with rtmpdump. Command line: rtmpdump -r 'rtmpe://video.infoq.com/cfx/st/presentations/11-nov-yesodwebframework.mp4' -o 11-nov-yesodwebframework.mp4 
Egh, sorry. Non-mobile version is http://trojansax.blogspot.com/2012/02/haskell-adventure-project-euler-191.html
Mobile one was actually kind of nicer to read for me. Awesome post, thanks for writing this!
Good read, though I think your final solution overcomplicates things by trying to stay in the style of your initial naive solution. Imagine it a bit like a (mangled) DFA with 6 states representing the valid combinations of suffixes/lates (0As 0Ls, ..., 2As 1Ls). It's easy to work out the transitions (once we've seen an L we can never escape, etc.), then we want to count the paths of length n: -- 0As 0Ls, 1As 0Ls, 2As 0Ls -- 0As 1Ls, 1As 1Ls, 2As 1Ls transition (a, b, c, d, e, f) = (a+b+c, a, b, a+b+c+d+e+f, d, e) countTotal (a,b,c,d,e,f) = a+b+c+d+e+f compose n f = foldr (.) id (replicate n f) answer n = countTotal $ compose (n-1) transition $ (1,1,0,1,0,0)
I refuse to read comments that start with "i refuse to read posts that start with".
I find it interesting that he thinks case expressions and if/then/elses are vastly different control structures. I've always though of them as one and the same. (Although I suppose familiarity with ADTs is to blame there.)
Cool, I'll check it out!
`(**)` works on floating point numbers, while `even` only works on integers. The trick is to convert between the two with `fromIntegral`. sum (map ((**(-1)) . fromIntegral) [x | x &lt;- [1..n], even x]) you can simplify this a bit by not using `map`, and instead putting the calculation inside the list comprehension. And there is a function `recip` that gives the reciprocal of a number: sum [recip (fromIntegral x) | x &lt;- [1..n], even x] 
Depends what you mean by "functionally equivalent". They're certainly not operationally equivalent. Case statements can be optimized quite a bit in order to minimize redundant checks and reorder the bodies of each case when this would help, and case statements never have to call out to functions which means the code is more efficient with regards to overhead. Whereas because we cannot decide the behaviors of functions in general, guard statements must be compiled exactly as written.
I'm torn on this. Darcs really doesn't have much to do with Haskell other than that's its implementation language, so it's really not topical on /r/haskell and deserves a wider audience than /r/haskell is going to give it. On the other hand, I can't imagine there is enough discussion to keep an active subreddit on it. I think, if anything it belongs on /r/compsci or /r/programming.
&gt;Is there really that much to talk about with darcs? There is /r/git and /r/mercurial so why not /r/darcs? Patch theory can also be discussed in /r/darcs so there may be more to discuss with darcs than with the latter two. &gt;I'd almost prefer just funneling those submissions to /r/haskell While darcs is close to the haskell community people use darcs/patch theory with non-haskell projects.
You could decrease on n, enumming down, and then reverse the list. But I get your point.
You can definitely do that with Factual API (check out the Geo type in the ReadQuery module). There could be other filters you could apply to narrow it down further too. The examples in the github repo should help you get started and the API documentation is really easy to follow
I think so. Git/hg are more or less the same in terms of the abstract representation of things, but darcs is quite different. There was a passionate discussion on r/git or r/programming or something one time. I look forward to more such discussions.
I don't think it is fair to call aeson "underdocumented". It uses familiar and ubiquitous monad/applicative mechanics and documenting it more would involve re-teaching common Haskell patterns. There is of course absolutely nothing wrong with helpful posts like this. Let's just not be too aggressive with our language. 
Also, [2,4..n].
Is Darcs still worth anything? The original "theory of patches" document isn't even on the web any more, which seriously gets in the way of, say, telling new people what's cool about it (I had this problem fairly recently). I think I recall some of the generalizations of the theory being abandoned. Last time I had a need to use Darcs, I found that people were still hemming and hawing bout whether an equivalent to `git rebase -i` would be useful. As far as I can tell, they've never used git and have no idea what's possible in the version control space. Most folks I know who use Haskell version their Haskell using git.
Would it be right to say (based on the example in the blogpost) that build rules in Shake form a monad, whereas in Make it's only an arrow? If yes, are there any advantages (e.g. kinds of queries or transformations on the build rules) in sticking to an arrow?
I can't really comment on the categorical relationships, but what you say seems plausible. In Make, given a set of total build rules (they all terminate, none raise an error) you can determine whether building a set of targets is total without running any of the rules. In Shake, there are no easy guarantees.
It's mostly right to say that. See also: http://jaspervdj.be/posts/2012-01-14-monads-arrows-build-systems.html
Very interesting!
Wow, I'm impressed. Starting here: &gt;Now, the idea is to construct a set of 4 analytic functions whose Taylor Series have coefficients that obey these recurrence relations and initial conditions. I got lost... can you provide links to any background information to help me understand what you're doing here?
Just you.
This really showcases the power of Haskell's type system. I thoroughly anticipate the day when Haskell is the de-facto standard for scientific computing.
Thanks! Have an upvote! I've been banging my head on monad transformers, trying to use the StateT transformer working with ST Arrays. So this is good to help me generalize what I just learned.
I cannot find any tests in XMonad that covers usage of the X monad. How do you test your code? I do see extensive property testing of the pure datastructure, but for example all code in Main.hs - how do you test it?
Interesting. I think these are running a version of Forth. http://www.greenarraychips.com/ 
It forgot the most important practical reasons: your printer runs Postscript and that's a concatenative language. PDF is also concatenative.
Adam Megacz's [generalized arrows](http://www.cs.berkeley.edu/~megacz/garrows/) omit 'arr', for reasons related to those Ryan Ingram refers to in his comment on the article.
Most (all?) concatenative languages are stack languages where the symbols you type are each an operation on the stack. Even "values" like `1` are merely operations that push the denoted value to the stack. Consequently, you reason about concatenative languages as sequences of stack operations, and so the natural way of writing it is whatever direction most naturally comes to you for the order of actions. Since most of the world inputs characters left to right, and therefore left = earlier in the inputting process, it's almost certainly more natural for them to also think of left as earlier in the execution.
Wow, I didn't expect this to be posted in r/haskell. I'm the first author on the paper, so if anyone has any questions or comments feel free to ask!
It is true that we read from left to right and the unix pipeline behaves this way, too, but in programming languages we typically program from right to left: f (g (h (x))) i.e. start with x, apply h, apply, g, and apply f. There should at least be an option for people who prefer this ordering to program in prefix notation.
That is why I like to use arrows the turn this around.
Here is where I get stuck: What are the primitive functions of concatenative programming? For example, let's say I want to write the equivalent of the following haskell function: twice f = f . f I don't know what's the primitive function to take a variable off the stack? Is this like Haskell arrows or kappa calculus? Would I use something like fanout and "ArrowApply?"
There is an option: write your own language. As a general rule, concatenative languages are really easy to implement.
That was really interesting. Especially the slides [here.](http://www.cs.berkeley.edu/~megacz/garrows/megacz-pop-talk.pdf)
On a related note, the time would be nice if revdeps on http://packdeps.haskellers.com/reverse or hackage were sortable. Something like [a bookmarklet to sort the table](http://jan.moesen.nu/code/browser/bookmarks/20050329-bookmarks.html) would work in the mean time, I guess. (EDIT: switched broken user script to working bookmarklet host) (EDIT2: switched broken bookmarklet to a proper-sorting bookmarklet host) 
Arrows with `arr` add a few laws on top of Category+Applicative. It would be better to just use Category+Applicative and explicitly specify those laws (or use empty classes) for them, so we can re-use all the Applicative code instead of re-implementing it for Arrow. If we remove `arr` from Arrows, they become useful. However, after removing `arr` you have to add various less powerful methods to replace it. I'm not sure why you'd want to remove `&amp;&amp;&amp;` or an equivalent-in-power `split` primitive that turns a unit into a tuple. pkturner's link specifies even more useful Arrows, because the pair type is also parametrized, which means you can use sum types rather than product types (useful for arrows that do event aggregation).
Someone on IRC set up an instance for demo purposes I guess, yesterday: http://hackage2.uptoisomorphism.net:8080/
Not that happstack people aren't helpful and productive. The maintainer of happstack keeps hackage 2 up to date in for example porting it to acid-state.
Perhaps creating a commited working group to take care of Hackage 2.0 would solve the issue? It's not clear to me who should actually be responsible for handling this matter. Clearly whoever it is doesn't really have time for this.
&gt; It seems like it would be more broadly appealing if they used the exact opposite syntactic order for concatenation. It may seem that way at first but it's not what you want. If you actually do some concatenative programming you'll realize that you think in a very procedural manner most of the time (which is part of my problem with it) while keeping a stack in your head. The left-to-right notation reflects this sort of "do this, then this, then this" manner of thinking. The author of [Cat](http://www.cat-language.com/) originally tried right-to-left and scrapped it very quickly if my memory is correct.
Cute ;) If someone could setup an active hackage2 read-only mirror of Hackage that would be pretty useful. :)
Good idea - I had thought of that too but haven't had time to look into it.
Many thank yous. It sounds very helpful.
Yes, that would be me. I've also placed [a copy of the code on my GitHub](https://github.com/isomorphism/hackage2) since that's more convenient for many people than having it collect dust in a darcs repo with no shiny web front-end. If anyone wants to hack on it a bit, I'll update the demo instance with useful patches. This is all entirely unofficial and unendorsed by anyone actually responsible for this stuff, of course, in part because I'm not sure who *is* responsible for it. As I remarked on IRC, I tend to operate on the basis of "I do what I must because I can", and I thought the community might appreciate having a running instance to play with.
What, the carefully hand-selected packages I added to the demo instance aren't enough for you? Man. Those packages are pretty much everything you'll ever need if you ask me.
:-) I guess that explains why they don't have any reverse dependencies!
To me this is a problem of economics. Everyone is willing to complain that it isn't ready, but nobody wants to be the one to do the unpaid work. How about a Hackage 2.0 campaign that everyone contributes $20 to?
Nice, I find this paper and your approach very interesting. I am investigating proving Haskell properties using ATPs for my Master's thesis. I wonder if you have continued working on this project, and one very interesting part of the future work is support for data types. How far have you gotten there? Anyway, thumbs up!
I'd chip in.
Kickstarter? The problem is, of course, that all the caveats about software projects apply: late, missing functionality, developer quits, etc.
I disagree in the sense that: It seems the bulk of the work is already there and done (according to those links above and camcann's 2.0 instance being largely functional). Rather at *this stage* it is more a problem of momentum/activation energy. It is unclear what needs to be done, how to get access to sparky, who one needs to talk to etc... Once those things are addressed then money may work if the right person(s) can be found. 
Thanks, I've just read this paper before reading your responses. Because my instance is still quite useful, I've decided to use the undecidable instances extensions.
I'm not sure I entirely agree. Basically, I doubt that any likely amount of extrinsic motivation is going to help here. It's not *just* a matter of getting something done and moving on, though an initial push would help; maintaining Hackage and handling any tedious logistics that requires is an ongoing commitment to some degree. It's hard to get someone to agree to that unless it's something they want to do anyway, and offering some token amount of money is liable to turn a hobby into a chore and actually discourage anyone from stepping up. Note that "funds to get something built, but with no long-term maintainer" is apparently how the *current* situation arose. Quoth #haskell: &lt;edwardk&gt; it needs a maintainer &lt;edwardk&gt; that was the problem after gracenotes put it together &lt;lispy&gt; I see &lt;edwardk&gt; he finished it and was all set to put it up somewhere, and then iirc, ross said 'great, now you can maintain it!' and wanted out of the dealing with hackage entirely and gracenotes paniced since, a 'google summer of code' was going to turn into a 'google lifetime of bondage' Unless you plan to raise enough money to pay someone for working on Hackage (and perhaps other Haskell infrastructure) full-time, I'm skeptical that funding helps anything.
As the maintainer of Happstack, I am willing to do whatever is needed on the Happstack side of things to support Hackage 2.0. I do occasionally check that Hackage 2.0 still builds. And I did do a bunch of work to upgrade it from Happstack 5 to Happstack 6, and from happstack-state to acid-state. But, I think the problem now is not technical. It's just that no one wants to be take responsibility for running the site. It's a lot of responsibility with no perceived benefits. 
 compile $ reverse program ? :P I mean, I get what you're saying, to some extent, but I think the programming thought process will be sort of difficult that way.
Unfamiliar to whom? The look and feel and hackage 2.0 is almost identical to 1.0. The primary difference is that the default font size is a bit smaller. What makes the migration hard? I am pretty sure all the migration code is already written and working. You seem to be implying there is something wrong with the way Hackage 2 was designed and built. Though, in reality, it seems to have been built much in the way you suggest -- a verbatim replacement for Hackage 1. The issues is not that it is bogged down because the developers wanted to add too many features -- the problem is that no one wants to volunteer to run the server for free, because once you volunteer you may never be able to unvolunteer. And then you will forever be obligated to maintain Hackage 2 for free. What we need to do now is figure out how to share the responsibility of maintaining the server. A way for people to volunteer their time, but also a way for people to politely escape if they don't have time or desire any more. For example, perhaps we have 4-6 admins. And admins volunteer for 6 months at a time? At the end of each term, and admin can step down or stay on. If they step down we put out a call to the community to find a new admin? 
I think the problem is not who *should* be in charge, but who is *willing* to be in charge. 
Actual printing may matter less, but surely that means there are even more PDF and .ps files flinging about the internet. Or are we moving to non typeset things like html/mobi/etc.
Right. Hopefully someone will volunteer to create a plan for getting Hackage 2 live, and for putting together the administration team and policies. The nice thing about being that volunteer is that your job can be done when Hackage 2 is live. You don't have to get is live *and* then be on the hook to maintain it forever and ever. You just need a transition plan and people to take over once it is live.
As I recalled it, the current maintainer looks at it from time to time as new compilers come out, but otherwise doesn't deal with it very often and was unwilling to spend the time to learn how to do all the things he already knows how to do on the current code base on the newer much cleaner, but different code base.
It really was done. It was just a matter of finding a maintainer. Then the summer of code ended and Gracenotes and everyone else got sucked into other things. I'm as guilty as anyone as I was at least nominally the mentor for this project.
Any word on getting Trac back up? I've asked in #haskell, but nobody seems to know if/when life will return there.
Nothing to see here. This is not the bug you're looking for. Move along. (Hammer Principle does not currently report well when its rankings are based on small amounts of data. As a temporary fix I have pruned the cases where the amount of data is *really* small, as is the case for Agda and this statement from the aggregation)
Can I ask why? that Are they afraid there will be much more work involved - because the tooling will be different or because the code is newer or that they are afraid they will end up maintaining both at once?
There's a clue in the text of the right hand bar, but I don't hold not reading it against you - I probably wouldn't have either if I hadn't written it. &gt; For items where we've got a lot of responses the numbers should be pretty good. For the rest you can probably just expect them to hang around the middle of the rankings wondering what they're doing here. The site probably needs a better way to indicate when this is what's going on.
Here's a longer explanation of the general problem: http://www.drmaciver.com/2012/02/agda-is-now-mainstream/
I thought this was cool... https://github.com/colah/ImplicitCAD Not sure it matches your description.
Kickstarter sounds fine to me. There are a lot of alternatives now and I don't know if any are more appropriate.
http://hackage.factisresearch.com/ Give it half a day or so for the mirroring to finish. See also the [discussion on haskell-cafe](http://www.haskell.org/pipermail/haskell-cafe/2012-February/thread.html#99422) 
&gt; cronjob an rsync (Is this advice still correct?) Not if you're using the new server code. See the [setup and mirroring instructions on the wiki](http://hackage.haskell.org/trac/hackage/wiki/HackageDB/2.0)
&gt; Perhaps creating a commited working group to take care of Hackage 2.0 would solve the issue? [I had a go at that](http://www.haskell.org/pipermail/cabal-devel/2011-October/007803.html), with a little (but not enough) success. May yet work with someone who can keep the momentup up, keep people motivated and get them to talk to each other and review each others patches.
Are the small sample-size problems aggravated by confusion with Ada?
Yes, for some work I was doing at NICTA. I migrated to using the GHC API though, because it turned out I needed a full Haskell AST (also, website seems down for me. Bad press for Yesod).
Thanks Chris! That's the one I was looking for.
packdeps took down the whole server. I was taking shortcuts in my serialization code by using the `Read`/`Show` instances, which used too much memory (I'm hosting about 5 sites on an EC2 micro instance). I just switched to aeson for the serialization, and memory usage went from 300MB for 50MB. Shouldn't have any trouble now :).
I call it eitch-journal. This naming is a little awkward. :-P Simply I honored xournal which was my favorite application and named after Windows Journal which had been my favorite application when I used to use Windows. Of course, now my favorite application *is* hxournal!
I couldn't install this on windows 7 / HP 2011.4 cairo,glib,iteratee-compress failed. Something fishy with my haskell instalation?
That was it, gtk needs to be added to path before a cabal install gtk. Edit: not completly solved yet,some package fails: iteratee-compress-0.2.1.0 poppler-0.12.2
I posted this to the proggit thread on this article: I agree about the lack of a really pleasant straightforward tool that mirrors the HP-48 experience -- which I remember as just tossing some values on the stack, then figuring out how to combine them later. Since Haskell is my language of choice, what I tend to envision is basically a stack-based Haskell REPL of some sort. I'm not quite sure how it would handle partial application, or pushing a function vs. applying it, etc. But the general notion would be to turn ghci into a better interactive calculator... I'd love it if somebody picked up this idea and ran with it. A competitor interactive client to the ghc api besides the ghci repl would probably also help in keeping the api clean and usable, and maybe inspire other clients as well...
It's not obvious how. The issue is not uncertainty in comparisons so much as difficulty turning that uncertainty in comparisons into uncertainties in rankings. Also what to do in cases where there are no comparisons at all.
*flatten* is dfs and *levels* is bfs. You might want to write a function prune :: Int -&gt; Tree a -&gt; Tree a which limits the depth of the tree before applying flatten.
Ok, fair enough. Thanks!
Too bad they didn't retain the default view by means of a typeclass proposal.
Dang, I remember the example, but I don't remember the speaker or where to find the slides. It may have been Jason Dagit, but I'm probably mistaken.
I've been wanting something like this for a long time. Something like Mathematica meets GHCi. Add an inline editor, so you can edit the code after typing it in (and the embedded object update to match), and it'd be the perfect Haskell IDE. If you keep touch interfaces in mind while developing it, it would also work remotely on phones and tablets.
This sounds similar to luite's wolfgang lambda. I wonder if luite's a student and could continue working on this for his project :)
Thanks for suggestions. xournal development team is now rewriting xournal in C++ and call it xournalpp (I am not sure whether this will be their final name), so anyway xournal2 is not a good idea for them, I guess. I have been thinking of changing name, but it was my first priority. dimi sounds good but unfamiliar to me. From my language, it can be "grimpan" which meens doodling pad.. I will think about it more! Thank you. 
I saw that one. It's called DrScheme.
I don't think 1 is possible because of [this](http://www.haskell.org/haskellwiki/GHC:FAQ#Do_I_have_to_recompile_all_my_code_if_I_upgrade_GHC.3F). And if cabal is ever replaced, I'm almost certain its successor won't be maven. Haskellers like to make and eat their own dog food.
Ah, "the math is probably involved" is my code for "It's actually very difficult". ;-) Basically, you would have to derive the crazy formula mentioned here http://www.evanmiller.org/how-not-to-sort-by-average-rating.html yourself, with the added difficulty that you don't have numerical data, but something else. However, knowing the principles behind the derivation, I don't see much of a problem with lists of preferences instead of numerical data, though. It reminds me of Condorcet voting.
"With Maven I merely reference my dependencies, and our IDE's just deal with classpath and downloading issues in the background." Isn't this what cabal does? You list dependent libraries in the .cabal, type "cabal configure ; cabal build" and it fetches the required ones for you. You might get an error if you have incompatible or multiple versions of a library, but that would happen no matter what build system was used. "...no particularly easy way to upgrade all my packages" Cabal used to have this, and I believe you still can if you force it, but it's a bad idea and can break things. If the new library X needs the new library Y but your app is written to use some older version of Y, you'll get problems; you'll have to change to the new Y. 
Hehe, yes, I've seen that, and that was the only point in my life when I've questioned whether learning Haskell was a good idea.
Is video or audio of this talk anywhere?
I think these are largely issues of expectations and background. Having used systems like apt and CPAN I found Cabal to be quite intuitive, but I can see how it could be confusing for someone coming from an IDE-driven environment. On the flip side, I found Maven to be rather opaque and frustrating at first, and there's plenty I still don't like about it. I don't think the issue is individual vs. team, but command line vs. IDE (and there are plenty of large projects using command line-based build tools). You're right that this will keep many commercial shops from considering Haskell, but frankly it wouldn't be a good fit at most of those places anyway. 
I think Haskell is a good fit for medium sized shops. We've been using Common Lisp for years. Large shops will always avoid due to hiring issues, but there's no particular reason Haskell can't dominate the small company area.
cabal-dev installs dependencies for the project into a sandbox, and you can use it to get repeatable builds. (cabal-dev is new, and I think intended to get folded into cabal at some point.)
To maximize perversity, I unregistered everything locally registered, and did `cabal install yesod`. Again I did not need to intervene and it took real 8m52.447s to install all 88 libraries, viz attoparsec-0.10.1.1 base-unicode-symbols-0.2.2.3 base64-bytestring-0.1.1.0 binary-0.5.1.0 SHA-1.5.0.0 blaze-builder-0.3.1.0 blaze-html-0.4.3.1 byteorder-1.0.3 cereal-0.3.5.1 css-text-0.1.1 dlist-0.5 data-default-0.3.0 cookie-0.4.0 entropy-0.2.1 enumerator-0.4.18 attoparsec-enumerator-0.3 asn1-data-0.6.1.3 failure-0.2.0 fast-logger-0.0.2 hashable-1.1.2.2 case-insensitive-0.4.0.1 http-types-0.6.9 largeword-1.0.1 mime-mail-0.4.1.1 path-pieces-0.1.0 primitive-0.4.1 ranges-0.2.4 email-validate-0.2.8 safe-0.3.3 semigroups-0.8 shakespeare-0.10.3.1 hamlet-0.10.8 shakespeare-css-0.10.7 shakespeare-i18n-0.0.2 shakespeare-js-0.11.0.1 shakespeare-text-0.10.5 simple-sendfile-0.2.0 socks-0.4.1 stm-2.2.0.1 system-filepath-0.4.6 tagged-0.2.3.1 crypto-api-0.9 crypto-pubkey-types-0.1.0 certificate-1.1.0 cryptohash-0.7.4 pureMD5-2.1.0.3 pwstore-fast-2.2 skein-0.1.0.5 tagsoup-0.12.6 transformers-base-0.4.1 monad-control-0.3.1 lifted-base-0.1.0.3 conduit-0.2.1 attoparsec-conduit-0.2.0 blaze-builder-conduit-0.2.0 unix-compat-0.3.0.1 unordered-containers-0.1.4.6 utf8-string-0.3.7 vault-0.1.0.0 vector-0.9.1 aeson-0.6.0.0 cryptocipher-0.3.0 cprng-aes-0.2.3 clientsession-0.7.4 resource-pool-0.2.1.0 pool-conduit-0.0.0.1 persistent-0.8.0 persistent-template-0.8.1.1 tls-0.9.0 tls-extra-0.4.3 wai-1.1.0 wai-logger-0.1.4 warp-1.1.0 xml-types-0.3.1 xml-conduit-0.5.2 xss-sanitize-0.3.1 yesod-routes-0.0.1 zlib-bindings-0.0.3.2 zlib-conduit-0.2.0 http-conduit-1.2.6 authenticate-1.0.0 wai-extra-1.1.0 yesod-core-0.10.1 yesod-json-0.3.1 yesod-persistent-0.3.1 yesod-form-0.4.1 yesod-auth-0.8.1.1 yesod-0.10.1
[this](http://stackoverflow.com/questions/1830005)?
Use cabal-dev instead of cabal.
Btw, long time lisper myself (Lispworks, sbcl, AutoLisp) i moved on to clojure. So if you are a java programmer you may find clojure fitting your development environment nicely. As for dealing with cabal. To avoid clashes for different projects use cabal-dev or [hsenv](https://github.com/Paczesiowa/hsenv) If you need to manage a team of developers, install and maintain your own local hackage repository. This way you will be insulated from the breaking changes If your entire team uses the same OS, you actually can compile one time and then just copy .cabal and .ghc folders to other machines. As long as everyone else has the same OS and the same ghc. You would have to though write a small script to replace absolute paths in many files inside .cabal and .ghc folder. But that's not hard really. 
There's actually an [interesting paper about maximum entropy based voting systems](http://www.votingmatters.org.uk/ISSUE26/I26P3.pdf). It's not really tractable given the number of entries to compare though. 50! is a bit of a large number. (and also it gives you a probability distribution over orderings, not an ordering at the end. This is ok with voting because vote lotteries are the right thing to do anyway, but it's not ok with aggregation)
An infinite list and a list produced after infinite computation are not the same thing. Sorry to get your hopes up on the halting problem.
&gt; Example: If I have Project A using Foo 1.0.0, and project B using Foo 2.0.0, how do I build each with Cabal? cabal install A B Nothing about GHC packaging (or the use of `cabal-install`) restricts you to having one version of a package installed at a time.
I can't wait. The slides are very tempting, but there were a few details I couldn't quite make out and it'd be nice to get the full story.
That looks like a really cool project! I'd love to participate (as a student) in GSOC for Haskell, so I'll definitely be keeping my eye on this.
&gt; it took real 8m52.4447s to install all 88 libraries [required by Yesod] Nice computer...on mine it took somewhere between 30 mins to an hour. And that was the install that worked cleanly. I, like many others, have tried installing during those unfortunate upgrade times where some things have bumped versions, while others have old constraints. When you run into problems, that adds another several hours onto the experience (possibly re-installing ghc) which is *not* fun.
slight irritant in the slides: "idris syntax is influenced by Haskell" - the "fl" in "influenced" is just a square. This sort of thing shows up in various places.
now idea is more crystalized into Cream Pan (package name : creampan ) it evolved from grimpan (already exist) -&gt; grimpad (grim has bad meaning) -&gt; Kreampad (why K?) -&gt; Creampad (some coffee product exist) -&gt; Creampan . Sounds like creamy pancakes, not so bad to me.
Yes.
Having tried out some Prelude replacement ideas at various points, I recommend an all-or-nothing approach when it comes to replacing standard type classes. You invite a lot of compatibility headaches that way, and in my experience by the time you mess with `Num` and `Monad` you might as well just throw your hands up and replace everything. You can still add compatibility support for standard types, but you might as well use all new stuff internally. The more modest approach is to preserve compatibility as much as possible at the per-module level--replace some standard functions, consolidate definitions, tidy things up in general--but leave type classes and very basic stuff alone. In the latter vein, I've been slowly building a combination of my favorite utility functions and frequent imports along with fixing some pet peeves about the standard libraries. The code is [on my GitHub](https://github.com/isomorphism/Overture) if anyone wants to look at it for inspiration. It's a terrible mess at the moment, though, because the bulk of it consists of chunks from scattered utility libraries I had sitting around. Suggestions are welcome. :]
Sounds like it might be the 'fl' ligature (diagraph, strictly) in Unicode, and your client doesn't have ligatures supported in that font. Yep, jut checked, that's a single code point encoding those letters. Which, by the way, is deprecated behaviour - ligatures and diagraphs are supported only for compatibility with old stuff, and shouldn't be used with new content.
It would be neat for HLint or similar to scan your code and emit "signature is not most general type possible" notations to detect bugs and opportunities for generalizations. I enjoy writing functions without signatures and letting EclipseFP+HLint show me what it finds, often giving me a free generalized function I hadn't even noticed. Those get moved out from the app to a utility library. 
SPJ seems like he's constantly excited about everything he talks about. 
You're confusing display with representation. That's exactly the source of the problem above - that the semantic content is a ligature, instead of the semantically important pair of characters, and the use of a ligature left to the rendering infrastructure. 
I have imagined how cool this would be, but it seems like the huge stick in the mud would be the size of GHC compiled binaries. Waiting for a 5+ MB haskell binary to download every visit seems like a big downer. Perhaps they allow liberal caching of these nacl binaries though? Has anyone seriously evaluated how much work this would be? I'd imagine it'd all be in the assembly generator deep in GHC?
I think you don't need to download it at every visit if you use HTML5 local storage.
It took me about 10 minutes to build yesod on my brand new macbook pro, although I didn't time it precisely.
That makes sense for an individual hacker, but as a commercial developer this makes me want to bang my head against the desk. Our support staff, like many support staffs, is stretched thin at times, and compiling locally every time would be an unacceptable burden. Regardless about how you feel about Java, it is much easier to deliver a collection of .JARs and know it'll work on JVM &gt;= 1.5 (which is oh, every machine worth deploying to commercially).
Yesod is frequently a bit of a mess with dependencies due to its broad range of packages and rapid dev cycle. Happstack used to be like this but got much better. I wouldn't chalk up experiences with Yesod as at all emblematic of the usual hackage/cabal experience.
Perhaps [this](http://www.reddit.com/r/haskell/comments/m23h2/ghc_feature_request_warn_about_unused_constraints/c2xivs4) comment is what you were remembering?
This is a good proposal. aserrano's proposal last year[1] (linked to in comments) was excellent and should also be a good source of inspiration. [1] https://docs.google.com/document/d/1OXN19SEvVuYIXI7nq9z8af1bsRd4ZDCR_O1RicJDc0c/edit 
I've thought about it recently. The main backend work would be making sure GHC outputs assembly that the NaCl assembler is kosher with (it has a modified assembler that uses special psuedo-ops like `nacljmp` for the ABI, etc.) There was an OCaml patch that allowed the compiler to produce NaCl compatible binaries, but that googlecode project page is gone now. I was actually quite surprised at how minimal and non-invasive the patches were on that note. There are also a whole other set of things to keep in mind. For one, it would be incredibly difficult to work GHCi into this; the stage2 compiler needs to be able to run on the system its built for IIRC, so you'd somehow to have to make the build system go through `sel_ldr` or something which I'm not even sure is possible. That means you have to axe the stage2 compiler and all of its features (notably DPH, and template haskell, which both need GHCi and its dynamic linker.) Also, GHC isn't terrifically good at cross compiling at the moment anyway, which is one of the real benefits of the NaCl toolchain: you always get amd64 and x86 binaries spit out, and Chrome picks out the correct version. So you may need to make two totally separate binary distributions and somehow mash them together or something. You're at least going to need two copies of the same library for amd64/x86 to make builds work. There are lots of other details. NaCl just started to move to dynamic linking and a glibc implementation, but manifests require a bit of manual work now. NaCl also has some pecularities in the data model (IIRC, it's LP32-ish, always.) On the whole I think a registered stage1 compiler is quite plausable, however (if only for amd64 or x86, one of the two.) You could probably even get the threaded RTS to work - the NaCl API does support pthreads.
&gt; Cabal sucks. I've tried three times to get Yesod to install, to no avail. When was that? Any time in the last month or so was problematic because Yesod (and the stack of libraries it relies on) was undergoing large internal changes. The bleeding edge is the bleeding edge for a reason. Yesod went from 0.9 to 0.10 because the authors still don't think it warrants a 1.0 release number. As for the library issue, if you want pre-compiled binaries, get them from your Linux distribution (eg Debian, Ubuntu, Fedora) or from whatever packaging tool is current best. 
Try typing `circle 50 50 50` in [Try Haskell!](http://tryhaskell.org/).
In case you missed it above hsenv is waaay better. It is particularly great for yesod.
Thank you! 
I'd encourage you to play with sage notebook if you haven't. The thing about following it as a model is that its widely used, appreciated, and successful. So there may be improvements to its model, but through a fair amount of iteration they've already found a sweet-ish spot in the design space. A worksheet is sort of like a "live" displayed module. Sharing can be done a number of ways (and completely ignored for the SoC but added later if the project takes off) but conceptually just means giving someone else access to read/run your module. I don't think it means running off to invent some fancy notion of cloud-pluggable code or the like.
You can hack more dependencies with Secondary Expansion, but it's secondary expansion, not tertiary expansion - there's no limit to how many of these phases you might require.
I wouldn't hold my breath for PNaCl. LLVM isn't particularly fast (and load times matter for the NaCl people). LLVM bitcode also isn't actually portable at this point -- there are various platform assumptions baked in even at the higher level. For example, what should the portable bitcode look like if the input program contained the expression `sizeof(void*)`? They have to solve some pretty difficult problems. 
Right. I too wouldn't hold my breath for PNaCl. There was a recent discussion about this kind of stuff in the past few months on `cfe-dev`, and the fact that LLVM has many built-in assumptions about a platform is a major show stopper. The IR and optimizers also have many built in assumptions about the *target*, which is another major problem. The `sizeof(void*)` thing isn't so much of an issue as the ABI dictates all pointers on all systems are 32 bit (even amd64, IIRC.) But there are other bigger problems like this: if you want LLVM to generate correct code for some constructs, on some platforms, you have to construct the IR *specifically* so the backend can lower it properly. As an example, on amd64, Clang has to emit very careful IR when it compiles code that passes structures as a parameter to a function, by value. It emits IR that the amd64 backend lowers into code that correctly abides by the AMD64 SYSV ABI. If you don't do IR construction correctly, it'll generate invalid assembly code. But on ARM, the story is similar: Clang has to emit specific IR for constructs to lower properly in the ARM backend, and abide by the ARM ABI (by-val structs are just one of the more obvious examples, and they're especially complicated on amd64.) So now, you no longer have 1 piece of bitcode that can work properly on say, amd64 and ARM. The lowering passes for the two targets require different input bitcode to be crafted, even if the input source code was identical. Depending on the target you're looking at, you'll need different bitcode for each one. Floating point is another example that's also horrific. Different processors are going to have vastly different FPUs, and you can't possibly take the IR for one target and lower it properly to another with this in mind. It might not even be expressible given the instruction set (maybe FP registers are much wider/smaller, for example.) There was a talk at the european LLVMdev conference recently that addressed these concerns and gave an example of a 'higher level' IR that really *was* platform independent, built on top of LLVM. It's work done by the South Korean Govt. IIRC, so there's a strong possibility that it'll never see the light of day, unfortunately. More generally, LLVM isn't a virtual machine - it's a compiler IR. It's not suitable or meant to be used as a platform for target-independent optimizations or bitcode distribution: by design you have to bake some decisions about the target platform into the code you give it.
Nice! Seems like it's a bit scrappy, though, so the code probably won't be very useful. But their experience is very valuable.
Oh hi Ranjit! Didn't know you were a redditor! Someday you'l have to explain to us how that liquid types stuff works...
I totally agree that rendering ligatures is very nice and makes reading much more pleasant. However, either rendering engine has to perform ligature, or the encoding system has to provide codepoints for them. With everyone converging to Unicode, their position on this fact matters, and is expressed here : http://unicode.org/faq/ligature_digraph.html I mostly agree, although the transition will certainly be annoying.
The solution is to not register all libraries globally but use something like cabal-dev which builds libraries per project. It's currently missing a few features to make it really convenient, but that's the way to go. (You could even avoid unnecessary rebuilds by adding a smart hashing system.) Regarding deployment, the best solution is to ship a statically linked executable. This might not work in all situations, but it should work for most.
Overture is such a great name :-)
I think .appcache would also let you cache arbitrary binaries. Not positive though.
Maybe we're just in another long period of embarrassment. Only instead of side effects, this time it's namespacing and typeclass relationships. 
If you're concerned about commercial adoption for Haskell, you're barking up the wrong tree. Having to recompile everything because libraries ship as source is a minor annoyance. Having to modify your code because the libraries make breaking changes is a Big Deal.
I don't think anyone dismiss the usefulness of ligatures. The question isn't whether or not to make ligatures, but rather which component should make them. Although they have been encoded as separated codepoints in the past, the proposal advocated by the unicode commitee is to let the font logic do the job. PDF can embed fonts, so that should not be a problem. Although slightly rhetorical, your point about hyphenation is good :)
Hm, maybe I have a fancier computer than I thought? -- crypto-api took 29 sec with -p, and 14 sec without. Or maybe its some dependency thats the real problem... haskell-src-exts is the package that always makes me think the computer is going to melt.
Addressing the point on why users of Ruby or Python frameworks may be more productive, I do think a major reason is just that Ruby and Python are dynamic languages. Using Django (the only framework that I know well), it is easy to provide information to all the views in your system: write a middleware (which is simply making a class with a process_view method), and have that method add a field to the request object that it gets. As an example, if I have a favorite number (7) that I want to expose to all my views, I just have to do this: class MyMiddleware(object): def process_view(self, request): request.favorite_number = 7 Now, if I add MyMiddleware to django's list-o-middleware, all the requests given to all my views will have a ".favorite_number" field on them. I do think that there is a great potential for Haskell in web programming, but I haven't seen anything nearly as simple as this. I think Snap's snaplets have at least as much power, but there's still more cognitive overhead to using it. That may be a problem that is tough to overcome.
As it turns out, the amount of time I dedicate to a project has a significant correlation with how many cute names I can give things. So yes, I am quite happy with the name "Overture", and settling on it was a major factor in motivating me to finally collect all my utility libraries in one place. I am also aware of just how silly this is, but there you go...
Non-total 'head' has been in functional languages *forever* ('hd' in SML etc.), why change the semantics of something with years of history? Either petition for its removal or accept it as is.
Hm, I understand that definition and why it works, but not why it's necessary. I tried removing the tuple part, leaving just the vector but then it would not compile. Can someone who groks Idris explain?
Because it's a *bad idea* is why? Removing it entirely would be okay I guess, but why not replace it with something more useful instead? Lacking a standard `MonadFail` I'd probably just use `Maybe`, though.
If you want a constant just put it in your Import.hs file in Yesod or somewhere else where it will be in scope for your Hamlet view. favorite_number = 7 If you want something that changes per request that is set in middleware you can do roughly the same in Yesod (I don't think you want to use snaplets or Yesod subsites). The last WAI release added a vault parameter for inserting arbitrary data. Getting your data back out is an IO operation, which requires one extra line of code in your Handler. Setting up the vault the first time does mean learning a simple (but not well documented right now) API rather than just assigning to a mutable variable. On the other this is not a use case our users seem concerned about and personally I don't think I have ever wanted do directly propogate request information into a view. There are more guarantees with the Haskell version that lead to easier to maintain code. In general dynamic languages are much more seductive for a quick start, but their performance is poor and they require huge amounts of tests for any kind of safety and you still end up spending more time debugging and dealing with the consequences of what Haskell could catch at compile time.
So with the tuple the type is analogous to this? (a -&gt; Bool) -&gt; Vect a n -&gt; (forall p. Vect a p)
The 'vault' api really confused me - since I would have to communicate the vault key created in the middle-ware to the application through some side channel, it almost looks easier to just communicate the information that would have been stored in the vault instead. But I'm guessing I'm missing something about how it is to be used.
Is anyone here familiar with [Nix](http://nixos.org/nix/)? This approach would be a great fit with the Haskell philosophy. Indeed it's a bit embarrassing that Haskell does not already follow such an approach to packaging, given its emphasis on purity. There's a project [Hack-Nix](http://www.haskell.org/haskellwiki/Hack-Nix) which lets you use Nix for Haskell package management. But I'm not really sure how it works.
 filter f xs = (filterLength f xs, sizedFilter f xs) . o O ( Not `filter f xs = sizedFilter f xs ** filterLength f xs`? Nevermind, unimportant detail I suppose.) So I'm still not getting it. Bear with me. I have no idea why the length must be calculated in the tuple rather than directly in the vector's length type variable. I thought to help me grok it I'd implement these two functions that you suggested and then maybe I would see why it's so obvious that the length needs to be a separate value returned, filterLength : (a -&gt; Bool) -&gt; Vect a n -&gt; Nat filterLength p [] = 0 filterLength p (x :: xs) = if p x then 1 + filterLength p xs else filterLength p xs But I got stuck on what `filterLength f xs` is supposed to mean inside a type. It's a type-level function, I grok that much and it seems sound. But my code doesn't unify. sizedFilter : (f : a -&gt; Bool) -&gt; (xs : Vect a n) -&gt; Vect a (filterLength f xs) sizedFilter p [] = [] sizedFilter p (x :: xs) = if p x then x :: sizedFilter p xs else sizedFilter p xs But it cannot unify (`Can't unify Vect a (filterLength f xs) with Vect a (boolElim (p x) (S (filterLength p xs)) (filterLength p xs))`). What did I do wrong? Regardless, I think that the above code is orthogonal to the reason for the tuple, the main point was **why** do I need to do this `_ ** x :: xs'` business when `(::)` *already knows* that `(x : Vec a n) :: (y : Vec a m)` is `(_ : Vec a (m + n))`? What am I missing that's so obvious about the leap from `… -&gt; Vec a p` being clearly universally quantified whereas `… -&gt; p ** Vec a p` is obviously existentially quantified? Or maybe I should just go learn Agda and say goodbye to all my free time. ;-)
I just am soooooo tired of unit testing. At work (Java, big complex shrink wrap app) I often spend more time thinking "how can I unit test this?" than I do actually solving the problem at hand. At home, for fun and profit, I'm working in Yesod and not writing any unit tests. I'm finding that refactoring my app is no more or less difficult than when I use dynamic languages and, say, Rails. 
&gt; But I got stuck on what filterLength f xs is supposed to mean inside a type. Idris is dependently typed. Values can appear in types. `filterLength f xs` is a natural number, so you can use it as the length index of a vector. For `sizedFilter`, you're going to have to be careful, because the value of `p x` is going to influence the type you have to return, so you have to make sure that when you inspect it, it propagates the information to the right places. I wouldn't be surprised if `if` isn't good enough for this purpose, but I haven't tried writing this in Idris. Anyhow, it was simply immediately obvious _to me_ that `p ** Vect a p` is the syntax being used for a dependent sum, which is analogous to an existential quantifier, because I know how `filter` has to work. `p ** Vect a p` binds the variable `p`. You wouldn't necessarily know if you weren't familiar with this kind of thing before. The other type is perfectly analogous to Haskell, though. (a -&gt; Bool) -&gt; Vect a n -&gt; Vect a p really means the equivalent of: forall a n p. (a -&gt; Bool) -&gt; Vect a n -&gt; Vect a p just as it does in Haskell. And the reason it doesn't work is the same as the reason that: foo :: Int -&gt; b foo 0 = 'c' foo _ = 65 doesn't work in Haskell. But in Idris, you can write something like: foo : Nat -&gt; b ** b foo 0 = Char ** 'c' foo _ = Nat ** 65
Wild guess, the two branches of your if-then-else have different types. The ** syntax used in the library implementation is dependent-pair syntax. see 3.7.3 of http://www.cs.st-andrews.ac.uk/~eb/writings/idris-tutorial.pdf
It is short for a fresh existentially qualified type variable. And I think Idris should be able to figure out what to fill in for it. Note that in the current implementation of `filter` there's `_` everywhere on the left half of the tuple.
There is `MonadPlus` in `Control.Monad` (from the `base` package): head :: MonadPlus m =&gt; [a] -&gt; m a head [] = mzero head (x:xs) = return x
I'm not really so concerned with how well Yesod can handle the common things, but with how well it can be adapted to strange things. If I don't like your sessions, I'll drop in my own. Don't like your users? Great, I'll write my own using your code as an example. If everything (especially common functionality such as per-Handler users, connection pools, and messaging) is implemented as middleware, then aspiring developers have examples to build from. If it's all in the core, then where do I start when I want to learn how to extend Yesod to fit my needs? I "use" Django, but really just for its routing and templating. For the rest, I've modified their middleware to meet my needs. It's always been a breeze, and I've never had to go into the Django core to make changes. I think all frameworks should be this way, with a minimal core functionality that supports easy extensions, and with a great set of default extensions that a new user can learn from. Maybe Yesod has a great extension framework. I know it's built on great ideas by really smart people, but I haven't seen much discussion on how you write your own functionality. The book doesn't appear to have a chapter entitled "Extending Yesod" or something similar, but maybe it's in there. I do know that Snap's 0.6 release with snaplets allows you to do everything that Django allows, but it's a bit more up-front work (for probably a ton of in-the-end gain). I don't know much about Yesod at this point, but I am definitely interested in it. 
After more thought, I came up with 'hoodle.' It's haskellization of doodle. Although the discussion about naming is being progressed in google group, I would like to show it here since this reddit is the original discussion starter. 
Wait. What is -p?
I understand people have frustrating results sometimes with the output when typing Cabal commands... but it's really not productive to just say Cabal sucks and Maven (or whatever else) is better. Maven is *also* not up to solving the difficult problems that Cabal solves, mostly successfully, every day. Pretty much the only package system I'm aware of that *does* solve these hard problems is Nix, and even Nix is not perfect. What you really have problems with is the rate of change of Haskell packages, which tends to break things a lot more often than other languages. I'm not saying that as an excuse. As a community, we've largely made the choice to work on the technology for packaging rather than tweaking the social factors. Unlike the Java community, we don't maintain strict social norms against breaking changes. And unlike Ruby, we don't strongly advocate per-package isolation tools. So while our packaging technology is better than Java, we do still bear greater responsibility for its remaining flaws because we've deliberately chosen to lean on them more. But when it comes to productive advocacy, it's still useful to keep in mind that Cabal doesn't fail because "Cabal sucks". Rather, it fails because we rely on it to solve harder problems.
Java 1.5 was released in 2004. That's about concurrent with GHC 6.2. I really don't want to imagine what Haskell would look like today if GHC HQ were unwilling to make any breaking changes in binary formats since 6.2.
snaplets are about the same as subsites in Yesod. They are a good, but heavyweight mechanisms for sharing code in comparison to using middleware or just adding some configuration (like a database connection pool) into your foundation data type. There is a branch to be merged any day now that will let you write your own session backend. This is done through the Yesod typeclass. In general, most things that Yesod does are extensible or at least modifiable through that. I am very open to pushing things down into middleware (and it is possible now that we have a vault), but Yesod can provide a lot more help than WAI if you are getting into complex things. We have considered the idea of a Yesod middleware for that where you have access to the Yesod API. One of (at least my) goals after 1.0 is greater extensibility. If you have any ideas on how we can improve things (after trying out Yesod), please let us know. Here is the [chapter on configuring Yesod](http://www.yesodweb.com/book/typeclass). It is now behind the code base for sessions and the ability to write your own js loader.
I've used "Abstract" and "Preamble" in the past for my toy replacements, as they are things that tend to float in the same general area as the Prelude in most document styles. ;)
True, Indigo promoted a major improvement in Maven integration. No one at my office volunteered to try it out though. I'd definitely take a look next time starting a maven project from scratch. Anyway, my main point was that Maven's difference from Cabal has not much to do with the IDE integration aspect. Cabal (configuration, not much of package installation) has an [Eclipse plugin](http://eclipsefp.github.com/) now too, and it has also improved a lot in the last several months :-)
Thank you for the offer, but my issues are all theoretical at this point! Which makes them a bit less than issues, in a way.
Multiple versions are not the problem, GHC and Cabal handle those fine. The problem is when you need to have multiple *instances* of the *same* version.
IIRC you'd be better off doing: cabal install A cabal install B since when multiple packages are specified at once Cabal will try to make sure there's a coherent way to install all of them (i.e., with the same versions of dependencies).
Very good analysis. I still maintain this is because the Haskell community has a (subconscious) leaning towards individual hackers, and therefor less consideration is given towards repeatability for CI and team work.
That point was mentioned above. Honestly a cultural difference between the two I hadn't noticed (I don't do much serious Haskell work, so I've yet to be bitten by changing libraries).
Thank god. Git is much better supported, all around.
That's fair, I got it installed recently. That being said, it's disconcerting the way the Haskell community seems to handle releases and major changes, which I would describe as cavalierly. 
&gt; 1) Libraries are shipped as source. I think re-compiling in order to upgrade libraries is silly, and frustrating. GHC has some support for LLVM, I believe that Haskell should probably produce some standard package for LLVM compiled files similar to a .jar, to facilitate easy library shipping. The idea behind Cabal packages is that distros (or OS X/Windows) convert the source packages into binary packages which ships using the distro's package manager (e.g. apt-get.) Source packages (using cabal-install) is for the bleeding edge. Admittedly, since there's such a furious development pace in the Haskell universe at the moment almost everyone is following the bleeding edge. Long term the intention is that most people will get their packages in binary form from their distro.
Isn't it curious that recursive functions are not inlined by ghc, but the most trivial wrapper to a recursive local function can be inlined ? The "map = go ..." example looks like something that should be done by the optimizer, not by hand.
Writing the entire app in haskell and compiling the client side code to js is extremely enticing but is it more complicated then just using ghc for half the app and uhc for the other? I thought I heard somewhere that Haskell doesn't have a common calling convention which, if true, would complicate matters when calling yesod/ghc functions/values in client/uhc code and visa versa. Not to mention all uhc values have to compile to js values. Also part of what's good about js is jQuery, which would need to be callable from UHC code. I remember a project by someone that did exactly that but the code was manually written, and any updates to jQuery, let alone some other js library like ace editor, would have to be manually imported to haskell. A tedious task. Or is it actually simpler then that, and one could write a simple script to just compile the UHC code to js and manually add the output to the yesod assets?
Yes, that's the *static argument transformation* and it's currently not performed automatically because it's not always beneficial. Note that the `go` function has a free variable `f`, so the call to `go` actually allocates a closure. It pays off to do this if the wrapper gets inlined at the call site *and then specialised* to the particular argument (at the expense of code size).
What abouut, for example, making each notebook live in its own cabal-dev environment, so you can play safely with different packages and test them? Maybe later it can be integrated in the social features of Hackage 2.0.
If I found a monospace font with ligatures, wrote Haskell code in a utf8 encoded text file using them, and then fed that to GHC, would it compile? And would an angel get it's wings? 
Regarding the friendliness of the community here: In my experience, lack of or at least a practice of abstaining from use of mutable state seems to be the key. Common use of mutable state implies a grouchy community. Relegating mutable state to niche uses implies a happy community. Witness the Haskell and Clojure communities. I'd be curious of someone could produce a counterexample. 
&gt; functional programming has a property that is often lacking in other languages... at dinner last night we were discussing it, and I dubbed it: "The Princess Bride" property --- which is, that a piece of code means what you think it means That's comedy gold right there.
Thanks for your thorough reply (and the link to the paper). I will look into how (if) this could be roughly translated into Haskell (perhaps with some limitations). I'm also sorry it took me so long to react.
Ryan Newton posted some code for using a simple CAS instead of an atomicModifyIORef to the parallel mailing list which paid off pretty well.
The immutable communities have grouchy copies of themselves somewhere, but they can't replace the originals.
not trying to be a jerk about it, but ... In my experience, lack of or at least a practice of abstaining from using turnips as swords seems to be the key. Common use of turnip swords implies a grouchy community. Relegating turnip sword fighting to niche uses implies a happy community. Witness the Claymore and Katana wielding communities. I'd be curious of someone could produce a counterexample.
3d printers?
The paywall is indeed not much welcome. But still, if someone wants to implement cutting edge stuff thanks to new features in the glorious compiler, doing so under the GSoC, and possibly document it properly is a great idea.
If you use `cabal-install` then it defaults to doing user installs. It's only if you use the Cabal library directly (e.g., by running `Setup.hs` directly) that things default to doing global installs (though maybe that has changed to prefer user installs too). The docs are almost surely insufficient on this issue. Since it's well-known in the community, there hasn't been much impetus for writing it down; but it definitely should be added to the docs so that newcomers can learn the folklore more easily. Of course it'd be better still if GHC's "global" cache were kept distinct from the private cache of things GHC uses directly. You shouldn't have to hack around with paths just to do a standard global install, IMO. (Even if that hacking is pretty easy.)
If you're a university student your university library probably gives you access to papers from every journal under the sun. If you're not then google often links directly to a pdf if you search by title in google scholar. But that doesn't always work so I can certainly sympathise.
Further reading: [today's irc logs around 13:00](http://tunes.org/~nef/logs/haskell/12.02.16)
[Nix](http://nixos.org/nix/). This is what the replacement for cabal should look like.
&gt; You can use -funbox-strict-fields on a per file basis if UNPACK is too verbose. TIL this ^ =D Side note: I find mashing letters in front of other words to be distracting. All I see here is "FUN box", just like for Yesod I keep seeing "WHAM let"
But you can't write a wrapper library for every useful javascript library, and you can't automate the wrapping because js has too little information and too much possibility for mutation. How could one go about using an arbitrary js library from UHC haskell without manuallly writing a wrapper for every function in it?
You *could*, but that would only buy you numbers - not all of factor's other constants (like string, arrays, etc), so I feel like that would be more of a false friend than a help.
Okasaki has a great treatment of related stuff: http://www.eecs.usma.edu/webs/people/okasaki/pubs.html#hw02
Does *and then specialised* usually need an explicit SPECIALIZE pragma at the call site? And I guess this optimization opportunity would be lost if the "unknown functions" get passed around by two or three intermediate HOFs between the specialized caller and the specialized callee. 
The traditional advice as mentioned here is to profile before trying these optimizations, but it seems that sprinkling bangs on function arguments and data types is a lot less work than profiling (and relatively automatable in a pre-compile build step, which keeps the source looking pretty), and good advice in so many cases. Why not go with unboxing strictnesss annotations from the start (for every concrete data type and non-HO function) and only pull back when an infinite datatype is needed or profiling shows wasted computations? I guess that goes against the design of Haskell, but it seems like so many times the answer to a head-desk-banging performance problem is to add strictnesss, and it is trivial to stumble into a case where this happens in even simple code. Maybe it would save a lot of one-off profiling to do something like this: Add a "pragma" for "strict-by-default", and run tagged files or file sections through an automatic strictness-annotator pass. I put 'pragma' in quotes, because it doesn't have to be an actual GHC feature; a programmer can write a simple script do it (best if using the GHC/Language APIs). In general I understand there is in inclination to trust GHC to do the right thing because GHC devs consider a lot of use cases and corner cases and interactions, but GHC has to win *on average*; it doesn't know whether *my* file or project needs to be optimized for the infinite HOF function-slinging small-data high-flexibility case or the finite cache-friendly data-heavy specialized case. (Perhaps could be an interesting research project to develop flag packets that optimize for one or the other of these cases or others, and investigate if libraries/projects in the wild can be classified into a few small categories, each with its own appropriate packet of "highly opinionated" optimization flags.) 
Good question. I tend to use unpack and unbox interchangeably.
I definitely optimize my *data types* up front. I think this is good design in any language. Modern CPUs are so cache dependent that thinking about memory layout up front is a must for decent performance. The upside is that once you've thought about the strictness and memory layout, the latter being controllable via `UNPACK`, you don't have to do much else to make your Haskell program run really well.
Huh?
You beat me to it. :) I've been trying to wrap up a blog post about this very subject (using almost identical syntax, no less) for a couple of days now.
Ok, I wonder about something still, if you write code that makes heavy use of this feature, it seems to me that you'll eventually want to write a term whose type is a _big_ tuple. But GHC seems to only support tuples up to about a hundred of elements. So I'm worried about the scalability of this. It seems that for helper/library functions, you don't really care since you'll be polymorphic on the rest of the stack and just operate with the parts you need (which, hopefully, won't be that many), but when you sit and write the actual computation you want to perform, won't you overflow the tuple type and have the type checker simply fail and tell you it can't type your program?
Upon further reflection.... ByteString was originally called "fast packed string", where "packed" string meant "array of unboxed" char. Even today, the one-line descrition of [ByteString](http://www.haskell.org/ghc/docs/latest/html/libraries/bytestring/Data-ByteString.html) begins "Fast, packed", and the `pack` function converts a List of boxed Char to an unboxed array entity. It seems that ByteString uses "pack" to mean something very similar to how GHC uses `UNPACK`. Confusing. 
Here's the clever bit: &gt; The stack needs to be heterogeneous, so the easiest way to model it in Haskell is to use a tuple. However, there are no generic functions to operate on n-arity tuples, so we will store the stack as nested pairs and use unit `()` to signify an empty stack. I have to say, I would not have thought of that. Very nice.
The googles did nothing? http://www.cs.helsinki.fi/group/nodes/kurssit/rio/papers/shavit_2011.pdf
This is a pretty nice and simple way to use heterogeneous lists in Haskell. I've used it before, using a simple alias makes it even nicer: (&amp;) = (,) Give it a nice fixity, and you can use it to leave out the parentheses: 1 &amp; "hello" &amp; True :: (Int, (String, Bool))
Spend a little time working with Simon to make shelltestrunnder better. He is a responsive maintainer.
Nix has a nice solution for that.
Yes, I might do that indeed. It's not that far from being what I want, really.
The conventions used to make that stack calculator work automagically with currying are fascinating; sort of like church encoding: a stack is a function that takes a function that operates on the stack.
That's actually [exactly the encoding](http://higherlogics.blogspot.com/2008/11/embedded-stack-language-for-net.html) I used a few years ago to build a typed code generator for .NET (the .NET IL is a stack language).
I use nix on my mac on my ubuntu box, but have not run across that nugget, mind sharing?
Heterogenous lists are a lot harder to work with in a statically typed language, because you have to specify the types (in order) of everything in the list. At that point, it is more like a record, because the format is fixed. To use lists the way they are typically used in Lisp requires some amount of dynamic typing, which is doable in Haskell, but it requires throwing away the benefits of the type system. It's usually not worth the trade-off.
cabal-install is a build tool (like make), it was not meant to be a package manager. However, I agree that some of these features would be nice.
I don't want to ruin everyone's parade, but it sounds like your requirements were all satisfied 15 years ago with the introduction of package managers in the unix-like OS's. Is there something special you want to do that can't be handled by apt/rpm/pacman/portage/etc etc?
damn, thats awesome! great work!
I'm just pointing out that nix binaries are a pure function whose inputs are the previous binaries and the new source code. A malicious upload of a binary can be identified by the fact that it either the build of the binary won't be reproducable or the source will contain the malicious change. 
It should be emphasized that the relationship is a triangle of correspondences between Logics, Lambda Calculi, and Categories. This is why the applications are so rich.
Something is wrong with it + IE9, one core goes full 100% use and the page doesn't respond. Firefox worked fine.
I would love to see something like ruby bundler for haskell and a page with lots of real life simple examples :) 
If you want an even better speedup, you can write a closed form solution (although it's admittedly less pretty). https://gist.github.com/1858601 Solves for n = 1,000,000 in 70ms My haskell is pretty basic, so I imaging there's more performance to squeeze out of it, but it's reasonably fast :) I used your isValid function to check my implementation for n &lt;- [4,1000].
It does already more than the common `Makefile` would do, i.e. resolve and install dependencies (which imho is one of primary jobs of a package manager)...
Is there a reason that HSX uses a pre-processor rather than quasiquotation?
It never rendered on my android either :( I'll check it out when I can. 
I think it predates support for quasi-quotes in Haskell.
&gt; so the only really problematic cases are recursive functions that are not tail-recursive (like the foldr example in the article). Even this is no problem for typing because the stack only goes by 1 in the case the type checker is concerned about. Polymorphic recursion handles the rest.
OIC.
Funny thing is, these days this is me with s/Python/Haskell/ and s/Haskell/Agda/...
In chrome on a mac I get the your name field and a blank page. Ah. For others: type your name then hit tab or click in the bottom area and start typing.
&gt;by using the [hsx| |] it is sometimes easier to get syntax highlighting and other editor features to work correctly. Really ? i am using QQ heavily in my Yesod web app and i'd love to have a proper syntax highlighting in emacs. Could you please tell me what editor supports QQ syntax hilighting ?
haha ;p Yes that's true, but i thought nowadays this was easier. Yet I'm amused, web browsers are OS on their own.
There's [this](https://github.com/pbrisbin/html-template-syntax) for Yesod+Vim.
&gt; I'm just pointing out that nix binaries are a pure function whose inputs are the previous binaries and the new source code. I am under the impression, perhaps naively, that small changes in source code or build options can result in drastically different binaries. As a result the best option is to always work from source ignoring previous binaries. &gt;A malicious upload of a binary can be identified by the fact that it either the build of the binary won't be reproducable or the source will contain the malicious change. So instead of a blessed build farm a user distributed build farm that uses inference to decided when and which binary to release to the masses? It makes sense that a functional build system would help in this regard, but I am ignorant of how easy or hard this is to implement in more mainstream build systems. In other words I am ignorant of specific examples or comparison where nix gives a significant advantage. Are there any that you can share?
http://www.reddit.com/r/programming/comments/pvrxu/simple_chat_app_using_ji_a_haskell_library_for/ here you go.
Some laws are not respected by the standard library. For example, x &gt;&gt; mzero is not really mzero in IO (but the stdlib still sports a MonadPlus instance). EDIT: Correct description of broken law.
wouldn't inlining definitions of (.) and id lead to ghc optimizing it away without any knowledge of this law?
Yeah, but in that case I'm not sure that the laws for MonadZero are rigorously founded in the first place, so I don't really object to that.
The function you want is [(\\\\)](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-List.html#v:-92--92-). [Hoogle](http://www.haskell.org/hoogle/) is helpful for answering questions like this. You can enter the type you are looking for (`Eq a =&gt; [a] -&gt; [a] -&gt; [a]`), and it will find anything that matches. You don't need to get the exact complete type, either.
&gt; is an empty list its own type? Nope. The `[a]` type has two constructors, `[] :: [a]` and `(:) :: a -&gt; [a] -&gt; [a]` &gt; how would I write an assertion for a recursive function that ends when one of the input lists is empty? An assertion? Not sure exactly what you mean, but suppose you're writing a function on lists. The simplest way to do it is to pattern match on the list data type's two constructors: someFunc :: [Int] -&gt; Something someFunc [] = ... someFunc (x:xs) = ... This is the simplest way to write any function on an "algebraic data type": just handle the case for each of its constructors via pattern matching.
I've been seeing blogs refer to function constructors as type assertions. Also it is the syntax of the pattern matching that I'm having trouble with. Thank you for clarifying the empty list for me. 
&gt; It should not be too hard to do, though if we want it really automatic then we need an automatic way to inform clients of available mirrors. Well, `cabal update` is already a part of my `cabal install` habit. Can the information downloaded in this stage just include a mirror list?
Aside from Hoogle, when you're about to do something basic onto a list, it's always a good idea to look through [the documentation for the Data.List module](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-List.html).
Wadler's work didn't use any primitives (I understand that a primitive is a function), his algorithm works on functions definitions and their shape, so probably it could handle some of these laws, but not because they are general laws for some type class, but because a specific instance and its definition raises this opportunity.
I'd chalk it all up to growing pains. Haskell has had a very long, slow gestation period. In due time this will all be worked out.
Sounds a bit like my dream of a hackage stable repo. Some people have also suggested that tagging in hackage2 could help too though I am not sure that would go far enough.
&gt; that small changes in source code or build options can result in drastically different binaries I shouldn't have said "previous binaries." I should have said relevant parts of the build environment. You are right. The question is, can you possibly capture _all_ the inputs which produce the result so that a build is truly reproducible by anyone with the neccesary hardware? Nix allows you not only to download the source that produced the result, it also captures the full build environment that produced the result. In addition, Nix provides a way to reproduce that build environment. It follows then that you could rerun the build and see if you also get the exact same result. If you could not reproduce a build then you might suspect either a bug or a malicious actor. I think the more us pure folks learn about it, the more we will like it. I was delighted to see dcoutts is aware of it and is stealing from it. The comparison you made is apropos also. Nix is verifiable reproducable builds. The leading brand, so to speak, is "blessed build systems." 
Also a rough breakout for the cost of distributing the tarballs would be interesting as that is possible to offload to a mirrorpool / cdn.
Have you seen the Julia language? http://julialang.org/images/web_repl.png
I think what I really want is something better than the browser and XML. It would be great if we could pass S-expressions around: table (4, 4) [0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3] ...and style rules better than css (with vars and functions) and no cascading craziness.
You can use a QR decomposition. [Numeric.LinearAlgebra.Algorithms.qr](http://hackage.haskell.org/packages/archive/hmatrix/0.13.1.0/doc/html/Numeric-LinearAlgebra-Algorithms.html#v:qr)
The columns of Q are an orthonormal basis, the same as the result of a call to orth.
&gt; produce some standard package for LLVM compiled files similar to a .jar, to facilitate easy library shipping. Unlike Java bytecode, LLVM bitcode is target-dependent, so you can't just ship .bc files to clients.
If there are any glaring or dangerous mistakes, then please let me know and I will correct them ASAP.
This implementation is a distillation of my ideas on reactive programming over the last few years. I hope it proves to be a simple, practical and powerful system for general use.
&gt; $ cabal unpack IOSPec Should probably be IOSpec.
how about some example code?
Type 'cabal unpack sodium'. The examples directory has a test suite and a simple whack-a-mole game called Poodle Invasion. I'll expand on the examples slowly. Take a quick look at poodle.hs here: http://www.hip-to-be-square.com/~blackh/sodium/poodle.hs.html
Did you send patches to all these libraries' maintainers ?
as an author of a functional reactive library would you like to comment on where this library fits next to other libraries? for a topic like frp the more the merrier, but does this library bring anything new to the table? you mention that it's industrial strength for example, how proven is that? 
Maybe we need a FRP library shootout to compare ease of use, code readability, performance, integration with UI frameworks etc...
http://www.cs.uu.nl/wiki/bin/view/Center/WebHome Doaitse Swierstra at Utrecht University (i did my masters there, just awesome)
But which is more reactive?
Thanks for taking the time to respond. I see myself downloading this for a spin on the weekend. &gt; yay Heinrich bring it on and let's raise the bar. Take it outside gentlemen, there'll be no bar fights in here. Although, as JPMoresmau suggested, I'd watch a shootout.
This looks great! May I suggest separating "primitives" from "derived" combinators in the Haddock with section headers? For instance, `count` and `countE` can be (and are) implemented trivially in terms of `accumE` (which is itself a derived combinator) and `hold`, but they're listed alongside primitives like `execute`, which makes it more difficult to see the core of the model. Admittedly, what constitutes a primitive is debatable, since optimised primitive implementations can be given for things implementable as derived combinators; I'd list those as derived combinators in the documentation, since the optimisation is an implementation detail. I'd also suggest putting the documentation of `Event`/`Behaviour`/`Behavior` above all the combinators, to make the list easier to scan. A public source repository would be nice too.
&gt; I believe reactive-banana doesn't have a switch primitive as yet. Indeed, reactive-banana does not have a switch primitive *yet*. There is actually a huge design choice involved, more details [here][1]. The key point is that you are forced to put the return type of `accumE` into the `Reactive` monad, while I made the decision to return a pure value. I wanted to avoid the additional noise of having to use a monad, but I don't know which style is more pleasant. (Friendly `accumE` vs cheap dynamic event switching.) I've been looking around your source code a bit and I have a question: do you observe sharing? For instance, consider the following situation e1 = expensive &lt;$&gt; e0 e2 = something &lt;$&gt; e1 e3 = somethinelse &lt;$&gt; e1 e4 = merge e2 e3 where calculating an occurrence of the event `e1` is very expensive, so you want to share the result between `e2` and `e3`. At the moment, I have the impression that the expensive event is actually duplicated in the definition of `e4`. [1]: http://apfelmus.nfshost.com/blog/2011/05/15-frp-dynamic-event-switching.html
Another contestant could be "peakachu". Also be sure to "cabal install DefendTheKing" which is a cute (though somewhat unfinished) game. "peakachu" works somewhat differently, with no equivalent for "Behavior"s, only something similar to Events. 
awesome, please put this comment in the haddock contents.
I don't quite understand what you mean, then. The space leaks, etc. are basically caused by the choice of model; I can't think of a combinator that would be implementable as derived, but have a space leak unless made primitive. Most of the derived combinators in sodium seem to be fairly simple, so there shouldn't be any problems organising them in the documentation.
One particular problem is alluded to above by apfelmus: you can get suboptimal sharing. With feedback and so on this can easily develop into space leaks.
Sure, but I don't think any of the combinators in FRP.Sodium have any such problems.
I'm just finishing up my Master's at Indiana University, where we have a couple of notable Haskellers in the Computer Science faculty, [Amr Sabry](http://www.soic.indiana.edu/people/profiles/sabry-amr.shtml) and [Ryan Newton](http://www.cs.indiana.edu/~rrnewton/homepage.html). Quite a few of the graduate students are Haskellers, and [Wren Thornton](http://code.haskell.org/~wren/) is across campus in cognitive science and computational linguistics (though he just gave a PL talk... One Of Us!). And, while he's not much of a Haskeller, [Dan Friedman](https://www.cs.indiana.edu/~dfried/) knows a bit of FP ;)
You also forgot the recent Netwire, by Ertugrul Söylemez. One of its design specificities is that it uses ArrowChoice and doesn't need switching primitives. It's then AFRP, yet way simpler than Yampa/Anima. A recent thread on Haskell-Café ([Haskell-cafe] "Best" FRP package for newbie) has advocated it.
Please add Sodium, and this great comparison, to the [FRP wiki page](http://www.haskell.org/haskellwiki/Functional_Reactive_Programming).
I don't see peakachu on the [FRP wiki page](http://www.haskell.org/haskellwiki/Functional_Reactive_Programming) yet.
I'm curious about what's unsupported. I guess we'll know more when he posts the user guide.
There is plenty of stuff that is not supported right now, but I am working on it. Eventually though, I will probably not support anything that doesnt relate to turtle graphics in some way. If you want to know the builtins that are currently supported, you can look at : https://github.com/deepakjois/hs-logo/blob/master/Logo/Builtins.hs
I always see fun about it. My next will be LoVeX. :)
Chris Okasaki has done some [work](http://www.eecs.usma.edu/webs/people/okasaki/pubs.html#hw02) in this area.
Yeah, I read the paper after it was linked from the part 1 discussion. It's impressive how far he got with just plain functions.
I'm a newbie Haskell coder, and I always like seeing what other people have done. I'm doing to go try to understand your source. Thanks for a cool project!
I did message the maintainers for IOSpec, numbers, and lambdabot. Originally README.md only contained the notes at the top, and was intended only for them. Then I figured that people might appreciate knowing the general approach to take to resolve cabal install problems, so I wrote up the rest. The maintainer of IOSpec already posted a new version, and as for numbers, if someone else hasn't done it already, I think I'll go ahead and post the new version (maintainer said it was ok).
There is a reason: Each invocation of synchronously creates a new transaction, in which state updates of holds are atomic. I'll add this, along with all the other suggestions people have made, to the docs.
&gt; I haven't been ambitious enough to try to avoid the use of the monad. It solves a whole lot of implementation problems, at the cost of some syntactic noise. That's cool. As said, I'm not really sure whether my approach is syntactically more pleasant as it will make switching more noisy. It's good to have an alternative approach! &gt; You're quite right. Expensive is duplicated. Performance is not going to be spectacular, but it shouldn't be awful. Ah, I see. Implementing sharing is a lot of trouble because it competes with `merge`. Took me a while to get it right (reactive-banana 0.2 and onwards).
Parallelism is a big subject and what I've done is a small step into a way that I think has potential.
I have the question regarding the type of listen: listen :: Event p a -&gt; (a -&gt; Reactive p ()) -&gt; Reactive p (IO ()) I might have misunderstood, but IMHO logic would like its type to be: listen :: Event p a -&gt; (a -&gt; Reactive p ()) -&gt; Reactive p (Reactive p ()) So you can unregister you listener within Reactive (as the listener itself works in Reactive, it could then unregister itself). But it's maybe impossible...
good one for a blog article.. please also include elm-lang.org; it is not haskell, but does has a haskell-like syntax, implements FRP in the language instead of as a library. i wonder how much easier it is then using an FRP lib on haskell.
nice to see this.. i wrote kturtle some time back, c++, basically to learn language design. digging in to this code for some good bedtime reading :)
While I'm not fond of concatenative programming, the TH code is explained very well with compelling examples. I like it =).
I only care about unique numbers and was also looking to use Integers. I originally had just simply defaulted to a list. Set differences are going to be the most arduous task, is there a reason I shouldn't just always use sets instead? For reference I'm making a prime number sieve to use in project Euler problem solutions.
Is the code for this up anywhere (github for example)?
Sounds good, but the time is probably not quite ripe enough yet, most FRP libraries are still in flux. I just got a marvelous idea for a logo, though.
Why not just do this? tens :: Int -&gt; String tens 1 = "ten" tens 2 = "twenty" tens 3 = "thirty" tens 4 = "forty" tens 5 = "fifty" tens 6 = "sixty" tens 7 = "seventy" tens 8 = "eighty" tens 9 = "ninety" 
Not yet. Will do.
What is the benefit?
easier to read
Also, less intermediate data structures
I would use a set for that.
Leftmost outermost evaluation and lazy evaluation are computationally equivalent. Haskell uses pointers and sharing to compute values because it's faster than simple text replacement, but the results are equivalent. Haskell will share objects that have the same name. If I do `(a+b,a+b)` then `a+b` will get evaluated twice, but `a` and `b` will get evaluated once. Haskell knows that `a` and `a` are the same because they have the same name, whereas `a+b` doesn't even have a name. If you give it one by saying `let p=a+b in (p,p)`, then it has the name `p`, and so will only be evaluated once. You can test this yourself with `Debug.Trace` in ghci: import Debug.Trace let f a b = trace "you called f" (a+b) (f 1 2,f 1 2) let p=f 1 2 in (p,p)
To be clear, it's not defined in the spec that this sort of sharing takes place. I believe all single-threaded implementations of Haskell preserve this sort of sharing. If you use GHC with threading turn on, though, things become a bit more complicated. If you want to preserve sharing in a concurrent setting you'd have to make sure that each thunk is evaluated only by one thread. Using a lock for each thread is way too expensive (it has been tried). Instead of avoiding this race condition, current versions of GHC allow two threads to evaluate the same thunk, but put a limit on the amount of duplicated work. There also is a check when updating that ensures that if a thunk has already been updated it is not updated again. AFAIK, this check is not atomic, so it could still happen (although very rarely) that sharing is lost due to two threads evaluating a thunk concurrently. TL;DR: The Haskell standard doesn't prescribe sharing. Concurrent GHC only preserves sharing with high probability, but doesn't guarantee it.
I haven't read the spec; I'm just going off of what I remember from SPJ's talks and white papers, so thanks for the clarification. Although [the wikipedia article on lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation) says that lazy evaluation requires sharing. And I recall a paper about `par` saying that when GHC evaluates a think it puts a "blackhole" pointer there so that other threads know that thunk is being evaluated, which doesn't limit duplicate work, it just makes it highly unlikely that two threads will evaluate that thunk since the second thread will see the blackhole and wait for the first thread to write something before continuing.
Doesn't matter much with memoization. Personally I'd probably write: tens = (!!) $ words "_ ten twenty thirty fourty fifty sixty seventy eighty ninety"
This strategy is called "eager blackholing", GHC no longer does this. The details are in Simon Marlow et al.'s ICFP'09 paper ["Runtime Support for Multicore Haskell"](http://research.microsoft.com/apps/pubs/default.aspx?id=79856). In short, GHC only adds blackholes when a thread gets descheduled. This leaves a window (random guess: 20ms) for thunks to get evaluated by two threads. If the thunk's evaluation code creates new thunks there will be duplication of work but not necessarily loss of sharing. That is, due to the mentioned check when updating the thunk most likely one of the duplicated structures will be discarded. It's important to keep in mind that these are details of one implementation which may change in the future. Haskell the language does not guarantee that sharing is preserved. Another example where it could be useful to intentionally violate sharing is simonmar's later work (ISMM'11) on per-CPU local garbage collection. In that setting copying data can sometimes increase GC performance.
Haskell uses lazy evaluation, call-by-need, which means results are memoized to avoid computing them again. In this particular case: Prelude&gt; :m +Debug.Trace Prelude Debug.Trace&gt; let tens = (!!) $ (trace "doing it" $ words "_ ten twenty thirty fourty fifty sixty seventy eighty ninety") Prelude Debug.Trace&gt; tens 5 "doing it fifty" Prelude Debug.Trace&gt; tens 8 "eighty" Note that the words operation and construction of the list happens only once.
It shouldn't be that hard, right? Just recursively walk over the tree and merge nodes. I will try my hand at this later this week, if I can find the time. And if I can convince ghc to run the benchmarks. So far it refuses because criterion depends on aeson, which depends on unordered-containers. At least, I think that is the reason. The actual error message is about a circular dependency between criterion and aeson.
Not when, readNumber 0 = "zero" exists too.
You're right. In that case, most people would prefer the following form to yours, though : readNumber a = attachPowers . readTriplets . toTriplets . triplify . toDigits $ a
Hopefully it's not that hard. I've mucked around with the benchmarks (on the hamt branch) and they now build with Cabal. Try: cd benchmarks cabal clean cabal install --only-dependencies cabal configure cabal build dist/build/benchmarks/benchmarks 
The question is, if I have something like let square x = x * x square (1+2) (as in the Wikibooks link above), how does GHC or whatever implementation know to share the result of evaluating (1+2) across both arguments to (*) in the square function? That is, how does it go from an unreduced graph (i.e., a tree) to a reduced graph, where common nodes have been merged?
The point is that the name "`x`" is the same. Each name in the source code corresponds to a single node in the graph. Drawing a graph would probably help here.
One way of thinking about sharing (without the graph-reduction part) is if we make the heap explicit in our terms. That is, in call-by-name evaluation we have the following beta reduction rule: (\x. e1) e2 ==&gt; [x/e2]e1 where the `[_/_]_` denotes the substitution function in our metalanguage. That is, in call-by-name we perform the substitution at the meta-level which can lead to duplication of `e2` in the object-level term. Whereas, in call-by-need (aka lazy) evaluation we have the following beta reduction rule: (\x. e1) e2 ==&gt; let x = e2 in e1 where the `let_=_in_` denotes allocation on the heap, stack, etc. Or if you prefer, we could use some other notation like: h |- (\x. e1) e2 ==&gt; h, x:=e2 |- e1 That is, when we perform the beta reduction we create some location/name/cell into which we store e2. Because the binding is being done in the object level term (which now involves both a heap and a term proper), we can avoid duplication of `e2` in the object-level term. The important thing here is that evaluation continues on `e1` and we just leave the binding around on the heap. If ever we need to evaluate the expression `x`, then we find that location in the heap and continue evaluation there; once we get to (weak) head-normal form, we ensure that the (w)hnf result overrides the previous binding for `x` (thus we get sharing), and then evaluation returns to whatever the calling context was that wanted `x` evaluated. 
Of course, you can just wrap everything in maybe and return nothing in any exceptional case, and you'll be just as bad off... so lacking a null type isn't really a guarantee of safeness (used in the broad sense), but it does encourage better habits.
It used to be that GHC performed CSE never, because of the potential for space leaks and other issues. Now that strictness analysis and the like have improved significantly, GHC is willing to perform CSE on the (rare) occasions where it is convinced that it won't alter the observable semantics. But, as before, if you really want CSE then you should do it manually.
Yeah, shouldn't be that hard to implement. The only tricky/interesting bit would be the merging of the bitmaps. I'd do it later this week if I had time. 
You'll also need to convert `BitmapIndexed` nodes to `Full` nodes as they fill up. Shouldn't be hard: check if the bitmap is 16 ones and if so wrap the resulting array in the `Full` constructor, otherwise use the `BitmapIndexed` constructor.
Thank you guys for the responses! Tonight I'll be doing a bit more research online and I'll compile a list so that people can use for the future in the event someone has the same curiosity as I do :)
thanks, that works.
My bad, I need to sleep more, I guess. "require".
Actually, it is a guarantee of safety so long as you write total functions and the compiler can warn you if you write a partial function. Moreover, the Maybe in the type signature documents the possibility that it can fail, distinguishing it from functions that cannot fail, in contrast to a language with exceptional values where every function can potentially fail. The difference between Haskell's Maybe and, say, Java's null pointer, is that we only have to check Haskell's Maybes (and the type system reminds us to), whereas in Java we have to check (almost) everything and the Java compiler will not warn us if we forget to, leading to a run time NullPointer exception.
Well, as far as that goes, you could more easily write it: tens :: Int -&gt; String tens n = case n of 1 -&gt; "ten" 2 -&gt; "twenty" 3 -&gt; "thirty" ...
I think that it starts out as a graph that is not a tree. I'll try to show this graphically. Based on your wording, I think you imagined a tree like : * / \ x x A graph is more general and you can just start with: * ( ) x * ascii art is hard
Seems an awful lot like Data.Conduit but without the resource usage guarantees. For the Conduit version of a telnet client see: http://www.mega-nerd.com/erikd/Blog/CodeHacking/Haskell/telnet-conduit.html
Well, in the meantime, why not at least just mention it there, and perhaps include a simple link?
This is great. If only GHC Core had such wonderful documentation!
Nice work. You might want to compare with Roel and Bas' [numerals](http://hackage.haskell.org/package/numerals) package, on hackage.
I think GHC does tend to sometimes memoize function results... See also: http://stackoverflow.com/questions/6208006/any-way-to-create-the-unmemo-monad
Oleg comments [here](http://www.haskell.org/pipermail/haskell-cafe/2012-February/099689.html).
Ah ok. I was underestimating the savings in not having to construct the cons cells. And indeed, "(repeat 1) !! &lt;billion&gt;" is fast even on the first run, since repeat just creates a cons cell having itself as its tail. Thank you!
To Oleg: I appreciate your correction very much.
Yeah, but even then, after looking up the definition, it's not quite the meaning I wanted. The standard does encourage lazy evaluation as beneficial, it's just not required.
Good point.
dynamic auto complete for typeclass
Oleg's a lurker
No worries :)
Done.
Thanks! I will review it today.
I've merged the commits. Will try to run a benchmark of the whole package again and see what things look like. I will look over the whole exported API, so the "hamt" branch is not lagging the current release in terms of functionality, and then make a release.
Agreed, my lack of mathematical knowledge is holding me back. &gt; The reason the research papers do not provide pseudo code is because that would require them to choose a particular optimization algorithm, which would be irrelevant to the content of the paper. This is not an obstacle to code, pseudo- or otherwise. The authors could have chosen an optimization algorithm just for the sake of having one, or they could have taken an optimization function as an argument to the overall l1 function.
The implementation turned out to be too lazy. Here are the results after making it appropriately strict: benchmarking union/union mean: 287.1938 us, lb 285.4331 us, ub 289.0761 us, ci 0.950 std dev: 9.323764 us, lb 8.212534 us, ub 10.75624 us, ci 0.950 variance introduced by outliers: 27.753% variance is moderately inflated by outliers benchmarking union/fold-based mean: 639.1612 us, lb 635.7320 us, ub 642.9521 us, ci 0.950 std dev: 18.51856 us, lb 16.01451 us, ub 21.68863 us, ci 0.950 found 4 outliers among 100 samples (4.0%) 4 (4.0%) high mild variance introduced by outliers: 23.837% variance is moderately inflated by outliers 
I think you mean mean a problem like min |x| s.t. Ax=b where |.| is the l1 norm. These can be solved with linear programming. hmatrix-glpk (also by Alberto Ruiz), provides access to the glpk package. But it sounds like you should read a little bit. Perhaps start with the Wikipedia and do a google Scholar search. Candace and Tao have a nice intro paper. For the tl;dr crowd, "compressed sensing" is a family of algos using l1 regularization as above. This is used as a proxy for l0 regularization, which nobody does because of the complexity. l1 often produces sparse solutions (l2 never does). So if you somehow know the solution is supposed to be sparse, it might work unreasonably well for your problem.
Oops, I'll have to work out a good way to achieve that on the post itself! The post mostly describes how to write a type checker and inferrer for the simply typed lambda calculus. The presentation's unusual and includes a (nascent) idea I think can go places but didn't want to give the flashing lights treatment at this point - if you've got past the modified typing rules and gone meh, that part probably hasn't stuck out for you.