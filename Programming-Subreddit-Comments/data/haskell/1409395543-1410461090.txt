Well for one thing Fay is not yet Haskell, a big one being that you can't really do monads in it yet (mainly because it doesn't support typeclasses yet) I don't really know about the state of Haste or UHC yet though.
Love the comments on the Perl6 comment that in &gt; "in current [Perl6] implementations, they're checked at run-time, but they're nothing preventing them to be checked at compile-time" tee hee.
Thank you! All the simplifications in 2.0 are really encouraging. I'm also looking forward to the reduced compile times in 2.1. I wonder why ghc struggles so much with those generated source in the first place. It's slightly alarming to see your build times increase so dramatically with the number of models (actually, even modules that simply have a large number of plain old records seems to suffer).
UHC is pretty simple IIRC
Nice tutorial, thank you for sharing :) I think there are two typos in the definition of the composition operator in the *How do lenses compose?* section: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c (x . y) z -&gt; z (y z) This should probably be: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c (x . y) z = x (y z) Or with arguably more intuitive names: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c (f . g) x = f (g x) In the section *Where do I go from here?* you write: &gt; This has no use in records as the types can't change, but if we take something like a tuple, there is no reason why an update cant result in a tuple with different types inside, [...] The distinction between records and types like tuples seems a bit weird to me. I think you could define a 2-tuple with record syntax as follows, and still be able to change type instances of `a` or `b` to different instances of `a` or `b`: data Pair a b = Pair { fst :: a, snd :: b } or even update types to completely unrelated types. But I did not use lenses in practice yet, so I might be wrong on this.
Did you miss that part? &gt; just change symbols' places to their proper unicode codes. 
I don't understand what you are trying to say actually, sorry. I'm using your exact solution but my Unicode symbols only take 1 width
You need to change the font so e.g. new double-width ‘::’ is in place of normal ‘∷' etc lelf.lu/files/SourceCodeProLU-Light.ttf (it's incomplete) 
&gt; lelf.lu/files/SourceCodeProLU-Light.ttf Ooooh thanks. I'll look into this.
Probably the most effort I ever spent on making a joke. It was a nice excuse for getting more familiar with how haddock documentation works. :)
So you don't consider Arrow's `(&amp;&amp;&amp;)` to be "well-behaved"?
Not exactly what you've asked for, but will likely save you a bunch of time writing specialised programs where a generalized program will do. http://harelba.github.io/q/ https://hackage.haskell.org/package/txt-sushi
Even if a joke, it can probably help model stuff like what happens in bash. However some would probably prefer Text or ByteString
You are joking but I have seen at least one university-level FP lecture where the problem's domain was modeled using strings rather than more appropriate representations. Meaning that the students would not be assisted by the compiler when making typos, there would be no coverage checking, etc. My eyes are still bleeding.
Filed: https://ghc.haskell.org/trac/ghc/ticket/9526. If you know who can help with that, please add them to CC.
Ah, that makes sense. I have relied on the D1-C1-S1 sequence in the past, and agree that after D1 only C1 makes sense. However, would it not make sense to stop at the outer most part that a primitive type still has? "Faking" a constructor (C1) seems misleading to me. Also, is there some documentation that lists these invariants? That would be very useful in general - the haddocks of GHC.Generics only give examples, not a specification.
I think hClose blocking indefinitely is a bug, I've never heard of that behavior anywhere before. Can you reproduce it reliably?
Sorry, I probably should have put a DTSD trigger warning there. (Note: µ(DTSD) = Dynamically Typed Stress Disorder.) On a happier note, I actually used Strings as the type of a date at one time, then recalled that Data.Time exists and left a comment telling me to switch to a more precise type, which later on I did. Hopefully that anecdote helped with the bleeding. :) Also that is likely a good time to be the smartass who asks tricky questions during a lecture.
Any code size reduction numbers from the tail call optimizations?
Definitely `ByteString` :)
&gt; Maybe it'd be worth starting up a mailing list for it? Or, dunno, a subreddit? /r/haskellgamedev is now live!
When I was learning Haskell for a semester project, I had a huge argument with a team member about this, nearly a screaming match, because he refused to change his mind about a design that was exactly this. I tried so hard to convince him, even with sane arguments like "What about typos?!", but he just stopped accepting everything I said. Ugh, years later and I'm still irked about this.
Fay can "do monads" :) just not support generic options on all types with monastic interfaces 
 get n tree = fold (.) (:) id tree [] !! n I'm not sure if that satisfies your criteria, since `(!!)` is a recursive function, but you could probably fuse its definition into the fold pretty easily.
Most likely not. It's essentially just good old trampolining for now, with some tricks to remove most of the overhead.
...and 0.4.1 is out the door.
Firstly, I would ask yourself if there is a better way to do what you want to do. When I was starting out I tried to use arrays all the time because that's what I was used to. As time goes on, I'm starting to think of them as a very niche data structure. But, assuming you do have a use case for a random access updatable O(1) structure, (maybe something like a grid of pixels in a video game) then the fact that you're updating something implies it has to be IO*. The operations can be found here: https://hackage.haskell.org/package/array-0.5.0.0/docs/Data-Array-MArray.html You want writeArray, called like writeArray theArray theIndex theNewValue -- From inside the IO monad *Technically, this sort of thing can also happen in the ST Monad ("state transformer monad"), something that encapsulates a computation that uses mutation in such a way that the mutation is unobservable outside the monad. 
~~```"yes yes yes"```~~ ```concat $ repeat "yes "```
`fix ("yes " ++)`
I'll elaborate a bit by explaining the type signature because I know it would have confused me as a beginner. (MArray a e m, Ix i) =&gt; a i e -&gt; i -&gt; e -&gt; m () The things on the left hand side of the "=&gt;" tell you about what type classes the types must belong too. Ix just means something that can be used as an index so we can simplify by pretending we're always going to use Int (MArray a e m) =&gt; a Int e -&gt; Int -&gt; e -&gt; m () What does the a Int e mean? If a were a function, it would be being applied to two arguments, Int and e. Actually it's sort of like a function that works on types. It's a type constructor. Remember how you always apply Maybe to another type to get a concrete type? Maybe Int, Maybe String. Same idea, but we don't know a. Let's just give it a descriptive name for now. writeArray :: (MArray arrayType e m) =&gt; arrayType Int e -&gt; Int -&gt; e -&gt; m () Another tip, when you see a type variable called m it probably means m is some kind of monad. Let's make it concrete too. writeArray :: (MArray arrayType e IO) =&gt; arrayType Int e -&gt; Int -&gt; e -&gt; IO () Can arrayType be anything? No, the constraint says that whatever the types are (arrayType, e, IO) together must be an instance of MArray. MArray is defined at the top of the file: class Monad m =&gt; MArray a e m class Monad m =&gt; MArray arrayType e m -- With our more explicit naming Here we can see that our guess that m was some Monad was accurate. Now we look at the instances MArray to see what sort of arrays are defined out of the box. The most interesting one is: instance MArray IOArray e IO It tells use that if m is IO and e is absolutely anything, arrayType must be an IOArray. The signature of writeArray collapses down to: writeArray :: IOArray Int e -&gt; Int -&gt; e -&gt; IO () Hope that helps. If nothing else, writing this has made me realise how much implicit technique there is in reading type signatures.
You may want a traversal. You can actually do this over any Traversable Functor if you like. This'll be a bit like a lens. -- values which know "where" they are in a sequence newtype At a = At (Int -&gt; (a, Int)) deriving Functor at :: At a -&gt; Int -&gt; a at (At f) = fst . f instance Applicative At where pure a = At (\i -&gt; (a, i)) At f &lt;*&gt; At x = At $ \n0 -&gt; let (gf, n1) = f n0 (gx, n2) = x n1 in (gf gx, n2) As an example run, this function annotates every element in a Traversable structure with their index ind :: a -&gt; At (a, Int) ind a = At (\n -&gt; ((a, n), n+1)) label :: Traversable t =&gt; t a -&gt; t (a, Int) label = flip at 0 . traverse ind And here's one which applies a function whenever the index matches a predicate chng :: (Int -&gt; Bool) -&gt; (a -&gt; a) -&gt; a -&gt; At a chng p f a = At $ \n -&gt; if p n then (f a, n+1) else (a, n+1) applyWhen :: Traversable t =&gt; (Int -&gt; Bool) -&gt; (a -&gt; a) -&gt; t a -&gt; t a applyWhen p f = flip at 0 . traverse (chng p f) applyAt :: Traversable t =&gt; Int -&gt; (a -&gt; a) -&gt; (t a -&gt; t a) applyAt n = applyWhen (== n)
What distinguishes a strict map from a regular map?
The impression I got from Learn You A Haskell was that the ST monad workered as a wrapper for a State -&gt; (Value, State) function. Could you expand on that last point you made?
What's the advantage in this method compared with a simple array? Might the applyAt function you describe be inefficient since it's essentially a filter - thereby requiring that all elements in your data structure be mapped over to find the element with the right index?
State -&gt; (Value, State) is the state monad. The ST monad is a little different. Let's say I wanted to add some numbers in a list up. I could do it two ways. With recursion, with all values being immutable: int add(list) { if(is_empty(list)) { return 0; } return list[0] + add(tail(list)); } Or with a loop and mutable state int add(list) { int acc = 0; foreach(element in List) { acc += element; } return acc; } But notice how the caller doesn't care which implementation add actually uses. Add is a pure function. That's the basic, 10000 ft idea of the ST Monad: we should be within our rights to use mutation so long as we don't let anyone know what we did. The clever bit is the types involved work together ensure that mutable state was used is never observable from the caller (the technique uses things called phantom types—I don't quite know how this works, I'm afraid, I just trust that it does). Here's an example I copied from the HaskellWiki import Control.Monad.ST import Data.STRef import Control.Monad sumST :: Num a =&gt; [a] -&gt; a sumST xs = runST $ do -- runST takes out stateful code and makes it pure again. n &lt;- newSTRef 0 -- Create an STRef (place in memory to store values) forM_ xs $ \x -&gt; do -- For each element of xs .. modifySTRef n (+x) -- add it to what we have in n. readSTRef n -- read the value of n, and return it. Why use ST? I can think of two reasons. 1) You have an algorithm that is written in terms of mutable state and you want to use it. 2) You can't think of a way to make an operation efficient without using mutation but you don't want to make it IO. The ST monad is important for doing bulk array updates because otherwise you would need to copy the entire array for each index update, which would not be very efficient at all. But please, don't be too trigger happy with ST. There may be other solutions like the (//) operator, amap, or using a different data structure.
I mostly jumped on your ambiguous "a data structure" wording. This works uniformly over any Traversable.
 cycle "yes "
This may have been one of the representations with terrible performance, but I came up with an encoding scheme for conduit using stream fusion a couple of years ago, described [here](http://gereeter.blogspot.com/2012/10/steam-fusion-for-conduits.html). I never did much rigorous performance testing, so I'm not sure how effective this method is, especially compared to your work, but it fully encodes a conduit in a stream. If I remember correctly, GHC was having a hard time looking through the calls to `liftM`, so it might be worth replacing `PipeM (m s)` with `forall x . PipeM (m x) (x -&gt; s)`.
Honestly, I'd prefer if you just posted all the game related haskell stuff here.
The really good stuff can always be x-posted
ST and State are two different types. State is indeed a wrapper for (State -&gt; (Value, State)) functions. ST is for observably pure calculations that mutate state. That is to say, things that have impure mutating implementations, but expose a pure interface. For example, you can take a regular immutable array, thaw it into a mutable array in the ST monad, sort it mutatively, then freeze it back into an immutable array. We've only copied the array twice and the sort happens in-place, but your sort function exposes a pure interface.
Just use an if / then / else and putStrLn to display an answer.
&gt; What's the point of a splintered off sub if all the content gets xposted? I was clearly not suggesting that.
The motivation for teaching lenses seems to be using them for immutable data. That's nice and all, but I'd like to see someone pimp the ancillary uses such as parsing json or xml. I got to use xhtml-html-conduit-lens to parse xml in several near one liners, leading to code that was like: doc ^.. xml . node "items" . node "item" . node "title" &amp; over each toTitle where toTitle :: Text -&gt; Title and that was a really pleasant use of lenses. 
Not sure if this is what you're looking for or if you're looking for more theoretical foundations as oppose to implementations, but there is a good entry on [GHC in the Architecture of Open Source Applications](http://www.aosabook.org/en/ghc.html). This might help with understanding how GHC currently works and nailing down some terminology.
I think it is assumed to block indefinitely (if it flushes buffers, for example, it depends on the other side of the pipe pulling the data). The real question is whether it is interruptible or not. If it is interruptible, then all invariants are lost when it's cleaned up. The fix is to have `bracket`, `finally`, etc do an [uninterruptibleMask](http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Exception.html#v:uninterruptibleMask), rather than a normal `mask` for the cleanup parts. Example: https://github.com/ElastiLotem/buildsome/blob/master/src/Lib/Exception.hs This means that most existing code that uses the existing `Control.Exception` combinators is unusable and needs to be rewritten on top of the more correct combinators. I really wish this goes into the offical base: Deadlock prevention by being interruptible is important. But cleanup invariants are even more important.
You can use `let` statements in a `do` block: let average = (read first + read second + read third) / 3
Make a function avg3 :: Int -&gt; Int -&gt; Int -&gt; Int then make an if (avg3 f s t &gt;= 75) then putStrLn ... else putStrLn ...
The term "sufficiently smart compiler" is often used facetiously since a provably optimal compiler is impossible due to the Halting Problem. Hence, compiler theory is approximately concerned with finding "principled" optimizations which work reliably for large classes of programs. You might hope that eventually we will have found very general classes of optimizations producing fast code for all human-writable programs. For general compiler books with a somewhat functional slant, you could look at Appel's texts. You could look at Ur/Web, a higher-order functional language somewhat like Haskell. It has the amusing property that compilation either fails or produces a C binary that never needs to perform garbage collection. Of course, this relies on a certain static analysis, and one possibility is to write languages whose semantics facilitate analyses to improve performance. Languages such as ATS, Rust, ML Kit, Mezzo, and Disciple allow various other region typing systems, and, e.g., Rust programs having an appropriate type don't need GC either. Haskell's effect typing of `IO` can be viewed as a simple but profound step in this direction. By taking advantage of dynamic as well as static information, we get JIT compilation which has produced very fast interpreters such as for Java, Lua, and Javascript. You could also look at tools like GHC's rewrite rules and HOOPL which allow programmers some control over the translation of their programs. Humans can sometimes produce very fast low-level code for some problems, so you might consider some AI/ML-type approaches to creating good C/assembly.
&gt; What's the point of a splintered off sub if all the content gets xposted? The people who are interested in gamedev in particular don't have to sort through everything.
I recently wrote this: import Control.Monad.ST import Control.Arrow import Data.List import Data.Function import Data.Word import qualified Data.Foldable as F import qualified Data.Vector.Unboxed as V import qualified Data.Vector.Unboxed.Mutable as M type SymbolFreq = (Word8, Word) freq :: F.Foldable t =&gt; t Word8 -&gt; [SymbolFreq] freq = F.toList &gt;&gt;&gt; f &gt;&gt;&gt; V.toList &gt;&gt;&gt; zip [0..] &gt;&gt;&gt; filter ((0 /= ) . snd) where f :: [Word8] -&gt; V.Vector Word f xs = runST $ do v &lt;- M.replicate 256 (0::Word) mapM_ (go v) xs V.freeze v go v s = do n &lt;- M.read v (fromIntegral s) M.write v (fromIntegral s) (n+1) It counts number of occurrences of given Word8 in eg. [Word8]. Local function 'f' creates mutable vector and runs function 'go' over elements of eg. [Word8] - 'go' function takes mutable vector, index and increments element at that index. Finally 'f' function will copy mutable vector into immutable vector.
&gt;I don't know how to get the value of the average to create an if-else statement. If average &gt;= 75 then putStrStrLn "You Pass" else putStrStrLn "You Fail" 
With better knowledge about the problem, human can write more optimal code in lower-level language like C, than compiler of higher-level language would be able to produce. Partial work-around for non optimal high-level code would be code profiling which would pin point problematic parts suitable for rewriting or implementation in lower-level language. Abstraction in higher-level languages isn't for free. It costs some overhead - just compare size of compiled program which prints "Hello world" written in various languages. For bigger programs however this overhead is acceptable, since it makes other things easier - including high-level optimizations. Some links: [http://donsbot.wordpress.com/2008/06/04/haskell-as-fast-as-c-working-at-a-high-altitude-for-low-level-performance/](http://donsbot.wordpress.com/2008/06/04/haskell-as-fast-as-c-working-at-a-high-altitude-for-low-level-performance/) [http://community.haskell.org/~ndm/supero/](http://community.haskell.org/~ndm/supero/)
&gt; I'm sure there is some seminal text on compilers or even Haskell compilers that I'm just not aware of. It's a little dated but Simon Peyton-Jone's book [The Implementation of Functional Programming Languages](http://research.microsoft.com/en-us/um/people/simonpj/papers/slpj-book-1987/) still covers a lot of ideas and techniques present in GHC today. &gt; Does the author mean to imply that something better than GHC is going to come along out-of-the-blue? Or that the GHC will be improved someday to give us equivalent or near equivalent C code? Is he being facetious? Not facetious at all, if you look assembly (``-ddump-asm``) for the code that GHC generates for the inner loops for certain classes of problems you'll find it's often identical to the code generated by a C compiler. There are even certain cases where GHC knowing certain high-level properties can generate vectorized code that's more efficient than C. It's an open problem of how to do this for a larger class of programs, but GHC is pretty much the state of the art in functional compiler design. I recommend reading about the [Core-To-Core passes](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=5642C28F5B1B23795970F49699CBAE21?doi=10.1.1.48.7721&amp;rep=rep1&amp;type=pdf) in GHC to read about the wild ways GHC will transform your program to get optimal performance. There are a few other compilers like MLton which do interesting things like full program monomorphization, which specialize all occurrences of functions to specific types instead of using a uniform representation like Haskell does.
Prediction: this sub dies within a month.
That's certainly relevant. I remember reading the blog post when you wrote it, and also remember putting it on the list of things I should really look into, and then simply forgot. Bad move on my part. My first guess about `PipeM` is that it might make more sense to get rid of it entirely, and instead wrap the `Step` result in `Stream` with `m`. I don't think there's ever a need for `Leftover`, since `Leftover` only applies when there's monadic binding, and (unless your encoding works quite differently) you can't monadically bind together two streams. That essentially leaves two differences between your encoding and mine: * You've added the finalizer field to `Emit`/`HaveOutput`. * You've added the `NeedInput` constructor. Those two changes are pretty modest, and might completely fix the problem of having to write all of the conduit functions twice (once in stream format, once in conduit format). This is definitely worth looking into further, thank you!
GHC doesn't do vectorised loops apart from what's blindly done by llvm, which doesn't use high level information.
&gt; The term "sufficiently smart compiler" is often used facetiously See also blog post on [the myth of the Sufficiently Smart Compiler](http://prog21.dadgum.com/40.html).
Right, but /u/sccrstud92 was countering that everything _shouldn't_ be x-posted, only the cream of the crop, or stuff that is more relevant to Haskell particularly than to game development (i.e. announcement of a new Haskell library, rather than devblog of a game that happens to be using Haskell).
Thanks! This is very, very useful. I had a feeling he was being facetious.
Hello there, welcome to r/haskell Your post looks like it got downvoted a bit and that's because people here don't tend to encourage posts which might sound like "please solve my homework". If you instead say something like "I'm learning haskell and I'm stuck on a homework assignment. I don't understand how to do if/then statements and do-notation" you will probably get a ton of help. You can also ask on the #haskell irc channel on freenode
I see [Zalora](https://www.zalora.com) is behind this project; I remember they were hiring Haskellers some time back. Great to see they're taking "functional" down to their infrastructure, and make their work available to the community as open source. 
Automated (Virtual) Machine / service / application deployment (genre Chef, Puppet, Ansible, Salt,...).
Don't forget that by the same logic, [C is a purely functional language too](http://conal.net/blog/posts/the-c-language-is-purely-functional). The only difference is that it has an IO monoid instead of an IO monad. 
Event sourcing makes it possible to answer questions such as "what was the state of the database at time n" and "when was this field set to the current value". I feel pretty certain that the databases/applications of the future will be built on something like event sourcing. It makes a lot more sense to me to build a database by storing all the changes and deriving the current state from those, than to "manually" perform a bunch of mutations on an object without any way of knowing what has happened before. No one is saying that event sourcing is some magic bullet that allows you to unsend an already sent email, but a strong type system like the one Haskell has makes it possible to encode these kinds of invariants so that only truly reversible effects can be reversed. 
A similar problem illustrating the technique is to implement the tail function using foldr. 
Agreed. I don't think this sub is busy enough to justify splitting anything away from it, anyway.
With synergy paradigm enhancement.
I don't think they'll kill us off that fast. ;)
This would make sense... if the trouble with /r/haskell were that there's just too much content to follow it all.
He means that there should be no separate subreddit *at all,* and all the game stuff will be posted here anyway (not cross-posted since the other place doesn't exist). It's a reasonable opinion, even if I (obviously) disagree. The argument for a fresh subreddit is that all the relevant info can be found in one place. The counterargument is that there's not enough subject matter and/or not enough interested parties to keep it going. But right now it looks primed to break 200 subscribers in 24 hours, which I think is a strong enough core group. Also, in the worst case, I *don't* mind things being cross-posted to both places; isolating a particular interest makes it easier to find information, even if that information is available elsewhere.
In C function application causes (may cause) side effects. In Haskell never.
That's not even true. noSideEffects :: Int -&gt; Int noSideEffects x = unsafePerformIO $ do print "Look ma, no side effects!" return (x * 2)
When people say this sort of thing they generally mean ‘outside of using `unsafePerformIO`’, because once you use that all bets are off.
Utilizing disruptive agile crowdsourced neural nets and predictive analytics to gamify the internet of things.
I know, but the words "In Haskell never" kind of alarmed me. There seems to be an awful lot of people who believe the words "purely functional" are literally and unequivocally true in relation to Haskell, while reality is more complex than that. You know, some time ago, here on this reddit, someone posted the 30 most depended-upon packages on Hackage. I've gone through their sources with a simple text search and found that 11 of them use `unsafePerformIO` and/or `Debug.Trace` (which is the same thing), 2 use `unsafeInterleaveIO`, and more than 10 use the C FFI. And this in some of the most heavily used Haskell code! So when people proudly talk about Haskell being oh so pure and how you can't do IO in pure functions, etc - it just looks like ignorance or self-deception to me.
People who disagree with your assessment of `unsafe*` functions are probably neither ignorant nor self-deceiving, but rather see things differently from you. For my part I would say: 1) `unsafePerformIO` has nothing really to do with haskell per se; it's a GHC compiler hook exposed as a (perhaps too) convenient library function. Same with `Debug.Trace` 2) The fact of whether effectful code can or cannot by any means be run from pure code compiled by GHC is not nearly as important as the expecations the community has w/r/t purity; if you expose a function that's not referentially-transparent it's a bug, and it's wrong. 
[This blog series](http://tel.github.io/2014/07/12/mutable_algorithms_in_immutable_languges_part_1/) (3 parts) is an excellent introduction to the use of ST to implement algorithms that rely on mutable state, and how it achieves some of the safety it does. It's tough going at times, but very well-written.
It's exactly what caused the ebola outbreak in Africa.
Well there's also the fact that many languages treat strings as sequences of bytes with Unicode knowledge tacked on later. Only seems fitting for us to use ByteStrings :)
That's pretty nifty!
It's more precise to say that side effects are not tied to evaluation order in Haskell, whereas they are in C. Therefore Haskell permits equational reasoning and C does not, because equational reasoning induces changes to evaluation order.
Ada is a language that has offered these features for several decades, and made them even more powerful with Ada 2012 with Type_Invariant and Dynamic_Predicate clauses. [Have a look here](http://www.embedded.com/electronics-blogs/industry-comment/4405929/Using-Ada-2012-to-say-what-you-mean).
I was shocked to try the following and have it work: Data.Map.fromList [('a',2),('c',3)] &amp; over (ix 'c') (+1) Does anyone know if this is efficient or is it running over the entire data structure on each update?
Can Upcast be used for provisioning of ordinary non-Nix cloud instances?
&gt; I disagree on laziness: GHC does a lot of work to remove unnecessary laziness where possible. But cannot achieve zero cost. Laziness with zero-cost would require a whole-program transformation, eliminating any need for thunks by restructuring the program so that the required evaluation order is expressed directly. Even then, claiming that all thunks would be eliminated is a bit of a cheat - results would still sometimes need to be saved for later re-use. The transformation needed is simple enough in principle, but impossible to use in practice for non-trivial programs. The time and space complexity is insane. That's at best - I haven't worked out all the details, but I suspect correct programs might cause the transform to loop. The existing approach to optimizing away laziness (where strict and lazy semantics are equivalent, use strict evaluation) is very effective. Strict evaluation can be forced explicitly when needed. Existing libraries tend to handle a lot of important cases for this. There are tools available to analyze performance issues in order to determine if and where optimizations like explicit strictness and unboxing should be applied (though TBH I've not used them). Also, in strict languages, sometimes we need to fake laziness too - recreating the same (or worse) overheads, but needing to code the implementation ourselves rather than leave it to the compiler. Basically, GHC does a lot of work to make laziness practical, but that's not the same as making it zero-cost. 
Yesod is getting better by the day.
This looks like sequent calculus elimination rules to me: Rank : rank, Suit : suit &gt;&gt; P; _____________________ [Rank Suit] : card &gt;&gt; P;
Currently you can easily make it spin up arbitrary AMIs, but NixOS is required to be inside of them to configure any of it.
I've started writing a html-parsing based lens tutorial, it's not finished but slowly coming to life :-)
Solving problems while you're not working on them is a quite usual thing, AFAICT. This happens to a lot of people I know (including myself).
I don't just 'have' this, I *do* this. I consciously think about my current projects(most often in Haskell) and some hurdles I have run into or may run into to calm my mind, so that I can fall asleep. Lots of great ideas and solutions have come to me this way. In a way, I guess you can say Haskell puts me to sleep. hue^hue^hue^hue^hue
I'm interested what the move of config-files to env-vars will look like for me. Somehow I'm afraid I will get a lot of env-vars (db creds, elasticsearch creds, upload-path, secrets, tokens, etc.). I'll probably move them to a shell script -- not sure if that's good of bad.
Yeah you can do some neat experiments. The following is taken from [Dependent types and vectors (again)](https://groups.google.com/forum/#!msg/qilang/3lAyZhxQ4sw/HtSJs9JXtEsJ) : (datatype subtype (subtype B A); X : B; _____________________ X : A;) (datatype positive if (and (number? X) (&gt; X 0)) ____________ X: positive; ________________________ (subtype positive number); _______________________________________ +: (positive --&gt; positive --&gt; positive); ) (datatype integer if (integer? X) ___________ X: integer; ________________________ (subtype integer number); ) (datatype natural X: positive; X: integer; ============ X: natural; ) (define positive? {number --&gt; boolean} X -&gt; (&gt; X 0)) (define natural? {number --&gt; boolean} X -&gt; (and (integer? X) (positive? X))) (datatype index X: natural; ============= X: (index X); if (and (natural? X) (natural? Y) (&lt; X Y)) _____________ X: (index Y); X: natural &gt;&gt; P; __________________ X: (index Y) &gt;&gt; P; ) (datatype safevec V: (vector K); ________________ V: (safevec A K); K: (index K); ========================= (vector K): (safevec A K); \* K: (index K); *\ ______________________________________________ &lt;-vector: ((safevec A K) --&gt; ((index K) --&gt; A)); ______________________________________________ vector-&gt;: ((safevec A K) --&gt; (index K) --&gt; A --&gt; (safevec A K)); ) \* -------------------------------------------------------------------------------- *\ (define safevec-init {(index N) --&gt; (safevec A N)} N -&gt; (vector N)) \* -------------------------------------------------------------------------------- *\ (define safevec-ref {(safevec A K) --&gt; (index K) --&gt; A} V L -&gt; (&lt;-vector V L)) \* -------------------------------------------------------------------------------- *\ (define safevec-set {(safevec A K) --&gt; (index K) --&gt; A --&gt; (safevec A K)} Vec I Val -&gt; (vector-&gt; Vec I Val)) \* Test with this: *\ \* (safevec-ref (safevec-set (safevec-init 10) 3 3) 3) - OK*\ \* (safevec-ref (safevec-set (safevec-init 10) 3 3) 12) - type error*\
I don't know how to edit the title, to make it more informative. Some Icfp 2014 videos are up already.
That definition isn't about how List is defined, it is how the List monad is used. Using a monad typically involves repeated calls to bind, which creates a sequence of steps to be executed.
Exactly. Even when the operational behavior seems completely different, monadic programs (also for Maybe and List) are very much sequential. That's the whole points of monads, sequential composition.
How does this tool compare to existing tools like Puppet, Chef, Ansible, Salt, and especially NixOps?
Indeed. I missed that before.
I would have protested this view before the [zseq paper](http://homepages.cwi.nl/~ploeg/papers/zseq.pdf), I think. But it opened my eyes to the power of treating monadic composition as an explicit sequence of binds. As an operational (pun half-intended) viewpoint, it has a lot of value. I still don't think it's a very good way to explain things, though. Honestly, I think the best way to explain things is via Kleisli categories, and showing how monadic composition is just slightly generalized function composition.
Having used Chef before (stupidly complicated), Ansible (rather nice, very straightforward) and NixOps (quite awesome), there are a lot of advantages to the 'Nix model of deployment': In general: - Declarative configurations. - You use Nix to write your machine configurations. - That means you don't need to learn a new language (**if** you know Nix). - Because Nix is a programming language, you can actually write powerful abstractions for your configurations. - All configurations are reproducible: with a specific version of `nixpkgs`, your environment can always be replicated quite easily. - Transactional rollback comes for free on your machines, so you can undo things as easily as you did them. - Like other tools, you can also still allocate non-NixOS resources (like EBS volumes). - Like other tools, these are multi-cloud, so you can use them with other providers in one network. - Easy testing. NixOS has fully automated testing functionality (even for full multi-machine networks) based on QEMU/KVM. This means it's possible to do *reproducible* continuous integration for your machine networks, which is something way beyond what I was able to do with other tools. Having used Ansible the most extensively before, there are some real issues I had with it in the past, but also some nice bonuses: - Ansible actions are supposed to be idempotent (to ensure reproducibility), but because these just default to running imperative commands on the machine, it is actually **impossible** to ensure reproducibility and purity of an image built from an Ansible playbook. - OTOH, this is kind of an advantage in certain ways, because you can use it to remove other redundancy. You can have a rule to say "Do all security upgrades on all machines" as a one-shot rule, and not every machine has the same set of rituals to do so. In practice this is super useful for admistrative purposes. - Ansible actions are typically described in YAML files, separated along logical points like the 'roles' a server can fulfill, encouraging you to reuse descriptions. But... - YAML is awful for reuse IMO. It doesn't actually give you any abstractions over the modules ansible offers, it only allows you to 'segregate' certain actions, roles, or files from others, which may be instantiated with different variables. This means abstraction is basically a form of template substitution over other YAML files. - Because of YAML's lexical structure, I often found myself confused by the quoting rules. For example, you may have an action like `action: allocate_ebs_volume size=1G name=holy crap name`, but this is really confusing when you get to `shell` actions I found. This one is maybe just a personal thing or I'm dumb. The quoting rules for variables is a bit more straightforward in Nix IMO, but maybe I'm just acclimated. - I've found ansible in general extensible, but if a module doesn't offer you something, you often just have to write it yourself in weirdo YAMLese, checking all kinds of weird conditions to remain idempotent, which is a major pain. Or you have to drop into Python. - Ansible is, IME, much faster than e.g. NixOps, but there are numerous reasons for this, mostly because NixOps has to import closures over the network, where Ansible just runs commands. - Ansible works on arbitrary machines. You can use Ansible for a NixOS machine (if you wanted I guess), but Nix-esque tools don't work on non-Nix systems. - Ansible is easy to use incrementally - you don't have to switch completely. NixOS is often a bigger buy-in. Now, there are some specific differences that Upcast has from NixOps: - NixOps separates 'physical' descriptions (e.g. 'an m2.large EC2 host with 2 1TB EBS volumes attached') from 'logical' descriptions (e.g. 'a NixOS machine running Nginx.') You don't specify the 'physical' aspect of a machine in Upcast. Instead, a machine includes resources describing the environment (e.g. 'an ec2 environment') and sets them up. IOWs, it seems as if Upcast collapses these two ideas. - Upcast does not use SQLite to contain network data - it uses shared text files to store deployment data instead. This means multiple users of an Upcast network can each deploy (since you can keep the deployment information around), whereas with NixOps, you really can't 'share' a deployment right now, it's single-user only. This is a big limitation right now. - It supports more EC2 features. - It doesn't support other features (like other providers, or virtualbox support). - Upcast is written in Haskell! I think this is a reasonable overview based on my experience.
Thanks for this. It's going in my list of "posts to point people to."
It does seem like having to put the type name in the function name eliminates one of the advantages of Haskell: not having to type types all the time because of type inference.
I feel guilty for getting the Internet points. I'm just glad Malcolm is back at icfp. 
I haven't used applicatives much, but my intuition is that the main difference from monads is that they don't order the computations. Like if you do `fn1 &lt;*&gt; fn2` it doesn't matter which is evaluated first, but if you do `fn1 &gt;&gt; fn2` then depending on the monad it might.
Oh weird. What's the benefit of using them over monads then? It almost seems like applicatives should be the default and monads a special case for when you need to pass variables, and people just use monads because they were added first?
That's true, and that's why I agree it's a useful distinction. But a very IO-ish program in Haskell is not going to have lots of pure code to reason about with equational reasoning. You'll have tons of binds, instead, which are not equations, and thus much less useful for that reasoning.
I understand the desire to use the functions you know and love coming from an algebraic background, but personally, I seriously *hate* when the text I see is not what is actually in the file. Not just in Haskell but in all the contexts where using `conceal` could potentially make sense, like latex. Why continue using a fixed width font here anyway?
Repost of http://www.reddit.com/r/haskell/comments/2epdwp/ivory_language/ ?
EDIT: I just noticed that I misread your question, so the below may not actually be helpful There's definitely a historical thing going on, with monad versions of many functions that work perfectly well with applicatives, because applicative was not a superclass of monad as it should be. (e.g. liftM vs fmap/&lt;$&gt;, liftM2 and liftA2, etc). But there's more to it than that, I think. do notation works with monads but not applicatives, although there's a new ApplicativeDo extension. And, allowing for one result to determine the rest of the action is a useful capability to have. As far as I understand, with parsing, if you can't do that (applicative parsing), you're restricted to context free grammers. However, with monadic parsing, a parse result can determine how the rest of the text is parsed, allowing for using context.
Are they though? Doesn't Haxl model concurrable queries with Applicatives? Isn't the point that order can be nondeterministic? For a State type (like IO), you would clearly implement Applicative with order. But I don't think it is true for Applicative in general.
There *are* benefits, but I think someone better versed in the matter will provide better examples than I can.
And^now^I'm^replying to^myselffff
Collapsing happens in a lot of monads, but isn't essential—the free monad is your counterexample.
Rust programmer and Haskell fan here! I just saw Ivory today, and I wondered the same thing. One large difference is that Rust is attempting to be a general-purpose programming language, and Ivory is "not a general purpose programming language. It aims to be a good language for writing a restricted subset of programs." Rust has safe heap allocation, and Ivory doesn't have it at all. I don't know a whole ton about embedded work, but it seems strange to me that much of this thread says you need heap allocation: I've always though stack allocation is much, much, much preferred. This may be my inexperience though. Rust uses LLVM as a backend, rather than producing C. That's just first impressions. I'm glad others are working on this problem!
There's a different kind of sequencing going on in applicatives versus monads. Examining their free versions can be illustrative: data FreeMonad f a = Wrap (f (FreeMonad f a)) | Return a data FreeApplicative f a where Pure :: a -&gt; FreeApplicative f a Ap :: f a -&gt; FreeApplicative f (a -&gt; b) -&gt; FreeApplicative f b You can see here that `FreeMonad` looks a bit like `Fix` newtype Fix f = Fix (f (Fix f)) while `FreeApplicative` looks a bit like a (non-empty, type-changing) list data List a where One :: a -&gt; List a Cons :: a -&gt; List a -&gt; List a For this reason, I like to think of applicatives as sequencing "in a line" and monads as sequencing "in a stack".
Is `Free` really a counterexample? It introduces extra structuring to information using an arbitrary functor, but you can eliminate that extra structuring and work with it as is it was just a flat value with the use of `&gt;&gt;=`; the right hand side of `&gt;&gt;=` lets you ignore the functor's structure entirely. It's also collapsible at a type level given that `Free f (Free f t)` can be forced to be of type `Free f t` using `(&gt;&gt;=id)`. Consider the case of `Free Identity t`. It's roughly analogous to adding a natural number to `t`, something like `(Nat,t)`. `(&gt;&gt;=)` hides away the natural number chosen and you can decrement it simply by using `(&gt;&gt;=id)`. `Free` has introduced extra structure but you don't need to care about it to work on values of type `t`. You can also decrement the natural number so long as it is greater than `0` by using `(&gt;&gt;=id)`, so there is a special base version of that extra structure (a single one, given that we're using `Identity` rather than a functor that can encode more than just `()`).
Indeed it appears to be. I hadn't seen the original post.
Collapsible at the type level, but all the layers of fs remain. Perhaps I misunderstood your interpretation of collapsible.
I agree with what tel said and would like to add a different angle: I'd emphasize the (a -&gt; m b) you see with bind as opposed to the (a -&gt; b) passed to Functor and Applicative and then put it into English thusly: Nested production and then *reduction*, of (monadic) structures. You can pass a -&gt; m b to fmap or &lt;*&gt;, you'll just keep stacking up the m (m (m (m ...). Have you ever tried nesting a bunch of IO? When do the side effects get popped? So, I see monads, given that join :: m (m a) -&gt; m a is what's unique to the interface, not fmap or return, as being about reduction.
Even in a program that is heavy on IO you still have the monad laws for IO. It may not seem like much, but it enables libraries like `pipes` to prove more sophisticated properties that don't break in the face of `IO` because these properties only assume that the base monad is a monad. The same is true for all monad transformer libraries.
What about more than one yesod app running on the same server? Wouldn't env variables clash? 
Perhaps better would be to make config reading pluggable? This way we could have several config modules we could chose from. By default yaml file would be used. But dev could chose to use Enviroment variables module instead, or database module. 
note that my talk actually talks 8 minutes into the associated video :) 
you dont have to make env-vars global. for instance: &gt; YESOD_DEVEL=true /path/to/app/bin/yesod Making that global means doing: &gt; export YESOD_DEVEL &gt; /path/to/app/bin/yesod Even in this last case you could still override in the afore mentioned way. 
Should just be a question mark next to a picture of shoulder shrugs.
So a good way to put it would be, monads give you dependent sequencing. The next action can depend on the result of the previous. In applicative that isn't the case.
Check out io-streams which is in the IO monad by default and makes the types a lot simpler. https://hackage.haskell.org/package/io-streams
Kudos to Malcolm Wallace (and everyone else responsible - I guess this isn't a solo effort). The speed with which these videos are uploaded is just incredible.
Or even multiple servers running in the same process (apparently the "best" solution to the redirect-to-TLS-server-when-connecting-to-http-only problem (this seems needlessly awkward and annoying to me). Being able to provide each server with a different config makes life a lot easier than trying to set the same setting two different ways for the two servers.This use of env vars immediately made me feel uneasy when reading it, because it now means configuration is no longer centralised and can be set from any number of places almost silently. I'm pretty new to Yesod but this change seems like a really bad idea to me.
you so don't sound like how i expected you to sound, lol
You can quickly realize why this is not the case by thinking about any parsing applicative. Something like `foo &lt;$&gt; a &lt;*&gt; b` will certainly parse with `a` and then with `b`. 
I've sent him an email some time ago without any response. I then started a thread to re-build parsec's community somehow, but this didn't gain any support. I hope this (homepage disappearal) will help.
http://i.imgur.com/GwleDFj.jpg
I'm a bit troubled by these three commands in this order in the "Trying It" section: echo 'GhcStage1HcOpts = -O0 -fasm' &gt;&gt; mk/build.mk echo 'GhcStage2HcOpts = -O0 -fasm' &gt;&gt; mk/build.mk cp mk/build.mk.sample mk/build.mk If the two optimisation lines are so important it seems a pity to immedately erase them.
I was puzzled by that too.
there wasn't a microphone, so voices are pretty distorted I think.
It's sometimes called the "Full Employment Theorem for Compiler Writers": - No algorithm exists to determine whether or not an arbitrary program halts. - Therefore, it's there must always exist some program that doesn't halt, and which the compiler doesn't recognize. - Thus we can add a special case to optimize this program to "while(true);". - Thus yielding a compiler which optimizes strictly more programs!
Accurate! I had just been watching King Of The Hill :P
That would be the [AMP](http://www.haskell.org/haskellwiki/Functor-Applicative-Monad_Proposal), for readers who don't follow GHC too closely.
&gt; Nix &gt; Goals: extensibility u wot m8
Types in general. Sum types. Types to document libraries. Lack of exceptions/random, rampant non-localism. Lack of state. Strong tendency to centralize impure code. Speed. Non-leaky abstractions. Types with laws associated to them. Nice communities. A good Text type. Monads. Simple language semantics.
Immutability, sum types and composability of functions, mainly. Edit: Those are the ones *most* languages are missing or get wrong. There are plenty of languages lacking static types at all, which is incredibly painful.
Pattern matching is so incredibly useful.
The type system. Haskell's is so ahead of the pack that programming in other languages feels dreary once you've been spoiled by generalized algebraic datatypes.
- separating the type of something from its definition - immutability - currying - not having to hide functions within (static-) classes. - writing code that abstracts over higher-kinded types. - (G)ADTs
Just because I'm crazy and will nitpick -- Haskell doesn't have GADTs either. Though it's a decently common extension.
Parametricity.
You know, I think I retract my dissent here. I understood "collapsible" differently than I think you meant it. I'm not sure I love it as a concept, but `join` surely can be described in some notion of "collapsibility".
You even leveraged Haddock to add to the effect: "Safe Haskell: Safe-Inferred"
Explicit mutability, explicit effects, strict non-optional type system, casual and mostly-transparent laziness, a module system that's *almost* powerful enough, a clear and unambiguous distinction between unicode strings and bytestrings, endless abstraction opportunities, strong compile-time guarantees, absence of utterly shitty libraries, and most of all, a super helpful community full of people way smarter than myself.
So what does this give us? A Coq-checked proof of correctness for arbitrary Haskell code?
It depends on the quality of the code I am working with. On poor quality spaghetti code codebases, I am missing explicit effects the most. Nothing else really matters, at this stage. On better quality, proper object-oriented code, I am still missing explicit effects, but I also start missing other things - like, type classes, type families, GADTs, pattern matching, currying, etc.
Catamorphism encodings of sum types. C doesn't have sum types, but it has typed function pointers. A struct of function pointers lets me relatively nicely/safely encode sum types in C (much nicer and safer than unions). Another one: I want to bind a variable in a C macro. But C macros do not have "let". Instead, I encode "let" in the preprocessor as a redex. Split part of the macro to a new macro and bind the expression to the parameter. This isn't strictly Haskell, but I've learned this from the FP world. "newtypes", I encode newtypes in C via struct-of-one-element. Learned from Haskell that types are cheap, bugs are expensive. I define custom types everywhere. I return structures by value (this is actually reasonably efficient in C!). I use "const" on all my variables by default. 
I don't know if you'll find anything but I recommend reading [this](https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf). It's a famous paper describing a number of efficient, purely functional data structures.
I think you missed the question, which was what Haskell features you miss when you're not using Haskell, not what features from other languages you wish Haskell had. That being said, I actually much prefer Haskell's multi-line strings to python's, because Haskell lets you set the starting point of each line, so you can keep your indentation clean. An example is like this: let str = "hello there\n\ \everybody!" in str Whereas, if you did something like that in python, you get to either make ugly indentation: def fun(): str = """hello there everybody!""" return str or you will have a malformatted string (random whitespace in it): def fn(): str = """hello there everybody!""" return str So, I like Haskell here :) EDITS: formatting errors...
Sum types, function composition and currying. Type classes. Laziness, but only sometimes. In JS, types. There are also some things that I do not miss, notably GC and module/package system. Oh, and forcing the character type to be Unicode.
In python you have: def fn(): str = ("hello there " "everybody!") return str
Do you mind explicating the macro trick ?
Thanks for the detailed reply! Rust does do bounds checking, but yeah, it's always at runtime. More static guarantees would be really neat. There is a very limited form of static assert in Rust: http://doc.rust-lang.org/rust.html#miscellaneous-attributes &gt; static_assert - on statics whose type is bool, terminates compilation with an error if it is not initialized to true. But yeah, that's very, very limited.
In other words, Haskell is a language which allows you to safely remove the safety it gives you. Therefore it is both unsafe and safe at the same time which - according to the principle of explosion - means that every time someone uses a part of the lens library which has a type signature containing `s t a b` a kitten gets stabbed.
Apparently, judging by the downvotes, we're too sophisticated for your low-brow meme humor here on /r/haskell. Take that crap back to the exact same community on #haskell!
I knew I liked #haskell for a reason.
This is obviously not what the OP is looking for, but if I really wanted this in Haskell for some *practical* reason I would just use an opaque datatype and some smart constructors. -- An integer value in the range 1..10 module Small (X, toX, fromX) where data X = X Integer toX :: Integer -&gt; Maybe X toX i = if i &gt;= 1 &amp;&amp; i &lt;= 10 then Just (X i) else Nothing fromX :: X -&gt; Integer fromX (X i) = i Giving you excellent type safety in you entire application. You only need to prove the smart constructor correct.
The bitboard chess-engine in Haskell interests me as well, I have written an engine old-style (without bitboards) years ago in C and have followed the development Crafty on and off. I'd love to collaborate. The bitboards in chess inspired me to use bitboards in word games (http://boardword.com/static/bitboards.html) - in Haskell. They are more than fast enough there, at least. But it is more a question of using lookuptables than updating them.
No, it doesn't prove anything about arbitrary Haskell code you feed the compiler. Instead it gives you a Coq-checked proof that the optmizations and passes that the compiler performs on your program cannot result in an invalid program being created. IOWs, the compiler can never perform an invalid transformation on the code you give it, and the transformation is proven to preserve semantics of the input program Note that in the case of this project, only one pass, the *flattening transformation* for heterogeneous arrows, is written in Coq. I imagine part of the reason for this was to formalize such a transformation while developing it - heterogeneous arrows are really the focus of the work, not so much the Coq transformations (IMO). A very similar project for LLVM is known as [Vellvm](http://www.cis.upenn.edu/~stevez/vellvm/).
I'm pretty sure you don't actually need mutable bitboards. I do a lot of fast low level computation with `Int` and `Double`, and I've never needed to use mutable versions. The compiler is pretty smart (especially with llvm) about keeping these things in registers and automatically making them mutable when it helps. In my experience, mutable structures are only really needed on "big" types like vectors that can't fit into a few registers. If you were having performance issues, it's almost certainly caused by unwanted lazineess in your code and not the lack of mutability.
MVector Bool is probably a bad idea for a bitboard representation. Word64 is a better idea. Bitboards commonly use bit-twiddling to calculate results on the whole board, simultaneously. For example, you can take your knight bitboard, do some shifting to generate the squares your knights attack, and bitwise-and it against the bitboard containing all of the opponents pieces in it. You can then easily see whether or not your knights can attack *anything at all* by just comparing the result and comparing it to zero. Really, /u/FrankAbagnaleSr doesn't necessarily need an array. He might be able to get away with newtype BitBoard = BB Word64 deriving (Ord, Eq, Bits, Show, Num, ...) data Board = Board { whiteKnights :: !BitBoard blackKnights :: !BitBoard whitePawns :: !BitBoard blackPawns :: !BitBoard ...} or he might need type Board = MVector BitBoard
Unfortunately it sounds like including all this machinery in GHC would be a significant amount of work to maintain and keep up to date (especially as any changes to the Core language would possibly require proof rewrites, and Core can often evolve in very significant, non-trivial ways!) Given the sparse resources of GHC developers, and the increased burden of developing compiler passes in something like Coq, it seems like something people would rarely take advantage of during development, if at all. That said, formalizing Core (AKA System FC^Pro as we know it today) and writing Coq proofs about it seems like something that could mostly be done *outside* of GHC anyway - no need for us to be in the 'critical path' so to speak. You could then perform the tomfoolery needed to link it back to GHC, using a compiler plugin or the compiler API. [Vellvm](http://www.cis.upenn.edu/~stevez/vellvm/) does this for example, by formalizing the LLVM IR (separately), developing passes with Coq, then plugging them back into the compiler later - minimal LLVM hacking needed.
A lot of people are converging on this as an approach to app deployment and configuration: http://12factor.net/ I don't use Heroku, but if you're going to have stringly stuff that exists outside of the build, I'd rather it be environment variables than a config file. Bonus points if your binary dumps the env vars it expects with documentation thereof upon request.
This remains the definitive talk I send folks to when they come to me with questions like: "Well. How practical is Haskell, anyways?" Excellent material. 
&gt; (actually Haskell doesn't have ADT, we should call them sum types). ?
Thanks for the tutorial. Looking at it, it looks a lot like Conduit/Pipes. Is Machine different from them or it is also trying to solve the same problem ?
Feel free to x-post this to /r/IntelliJIDEA if you like!
You can use something like #define let(a,b) (typeof (b) a = (b))
Machines is in the same rough design space. The major trick is that it splits construction and operation into phases in order to use the CPS transform trick more effectively and enables input from multiple sources.
The central type in `machines` is very similar to a unidirectional pipe from `pipes`. The main differences are: * `machines` has better asymptotic complexity for some use cases (`sequence`-like functions) However, I maintain that these use cases are obsoleted by all streaming libraries, including `machines` itself * `machines` aggressively uses type synonyms to simplify types, even more than `pipes` and `conduit` (which says a lot) * `machines` has fewer type parameters in its central type than `pipes` (but not `conduit`), which improves the user experience for type errors and type inference * `machines` has better support for branched topologies. You can emulate this using `pipes` or `conduit` but it is more verbose
Was that a boxed array of integers?
Well not really. `foo &lt;$&gt; a` returns an `f (b -&gt; c)`. So it won't parse the thing until you apply it to `b` with `&lt;*&gt;`. Or at least that's what the types say, idk if some strict-optimization is happening in `foo` or `f`.
It might be a lot to ask, but can you give examples of the points you've raised? It'd be great to see some side by side code to compare thetwo or three libraries.
hah, OK: - being able to extend the language so as to use GADTs ;)
Whoa! TIL Hasklig. I wonder if it works in terminal. 
Thanks for the response. Please see my edit to the post for the source. I am using Data.Vector.Unboxed.Mutable and Data.Vector.Unboxed and Data.IORef extensively, with unsafe writes and reads. If I were to give it another shot though, I would rewrite it from the ground up.
Thanks for the suggestion. When I was asking the question, I was thinking responses would just be whether it was possible in general (given a sufficiently smart Haskell programmer). But I am pleased that there is so many people willing to give more involved help.
Thanks for the response. The bitboard representation is probably going to be essentially necessary as fast move generation for sliding pieces (magic or rotated bitboards) relies on them. In addition pawns have a nice en-masse move generation with bit shifts, and the other pieces use a direct look-up table. Perhaps your scheme could be bit twiddled to get a bitboard quickly enough.
You are right that I don't need an array at all. I was using an array in the old-C way: use an array if it gets tiresome to write out the names. The biggest reason I would want an array is that it would be nice to have a zero-cost abstraction similar to move&lt;WHITE&gt;(args) and move&lt;BLACK&gt;(args), like is done with templating in C++. That is, it would be nice to be able to write one function and have to work for both sides, so the bishops would be an array of two bitboards.
Thanks. I want to get as completely away from mutability as possible, as I feel like I am simply writing C in do statements when I am using it extensively. If the whole thing is mutable, then I've done nothing interesting or new really.
¯\_(ツ)_/¯
Thanks for the response. I don't think it is directly relevant, but I haven't seen it and it looks like good reading nonetheless.
I was using an unboxed mutable vector of Word64 types I believe for most of it. That and IORefs. See my edit for source.
I believe I am using the IORefs for part of it. I am going to look into it more. (My post has a repo now)
Here's an example of where `machines` is asymptotically faster than `pipes`: -- Both `pipes` and `machines` have `await`, so this is -- valid code for both of them replicateM largeNumber await The time complexity of that for `machines` is O(N) and the time complexity for `pipes` is O(N^2). However, this is sort of a pathological case because it's not streaming at all (you're collecting a bunch of results into a large list, which defeats the point of using a streaming library). In fact, that's true for most of the examples where `machines` has better asymptotic complexity. Also, for the cases where you really need to do this in `pipes` there is a workaround that recovers O(N) time complexity and it's documented [here](http://hackage.haskell.org/package/pipes-4.1.2/docs/Pipes-Tutorial.html#g:10) in an appendix of the `pipes` tutorial. As an example of type synonyms in `machines`, consider this synonym: type Source b = forall k. Machine k bSource That's the synonym for a pure source of `b`s. `pipes` and `conduit` have no similarly concise synonym in their libraries because almost everything is at least minimally parametrized on the base monad `m`. There are advantages and disadvantages to having polymorphic type and the `pipes` tutorial also documents some of the related tradeoffs in [this appendix of the tutorial](http://hackage.haskell.org/package/pipes-4.1.2/docs/Pipes-Tutorial.html#g:9). Regarding type parameters, the most complicated `machines` type has only four type parameters: PlanT k m o a Contrast this with `pipes`, where the core type has 6 parameters: Proxy a' a b' b m r I actually was wrong about `conduit`, it now only has four type parameters (it used to have a fifth parameter corresponding to a leftover type). I'm correcting my original comment. The main reason for the two additional parameters in `pipes` is to permit the upstream flow of information. You can see an example use of this feature in a post that was just submitted to this subreddit today: http://www.corecursion.net/article/2014-09-02-Rakhana:_A_Stream_based_PDF_library However, it does increase the complexity of the inferred types. I wish `ghc` had an extension that allowed using type synonyms to simplify inferred types and error messages, especially since it would also benefit the `lens` library. Finally, for branched topologies, `machines` provides the `Wye` and `Tee` types. I'll use the `Wye` type as an example, which lets you feed two sources to a machine that has two inputs. The syntax and types look like this: input1 :: Source a input2 :: Source b wye :: Wye a b c capX input1 wye :: Process b c input2 ~&gt; capX input1 wye :: Source c -- or equivalently: input1 ~&gt; capY input2 wye :: Source c In `pipes` and `conduit`, the way you simulate two inputs is to nest their monad transformers within themselves. For example: pseudoWye :: Consumer a c (Consumer b m) () input1 :: Producer a m () input2 :: Producer b m () Then the way you feed the two inputs is to connect and run twice: runEffect (input1 &gt;-&gt; pseudoWye) :: Consumer b m () runEffect (input2 &gt;-&gt; runEffect (input1 &gt;-&gt; pseudoWye)) :: Effect m () However, it gets even hairier once `pseudoWye` is a `Pipe`, because you can't even supply `input2` until you've completed the outer pipeline. You also sometimes have to reorganize the monad transformer stack using `hoist` and `lift`, too. You also can't commute the two inputs, like this: commuteInputs :: Consumer a (Consumer b m) () -&gt; Consumer b (Consumer a m) () Well, technically you can if you use the internal `pipes` API in `Pipes.Internal`, but `pipes` does not expose this because doing so would allow violation of the monad transformer laws.
Or equivalently filter ((==0) . (`mod` 5)) [0,3,5,13,25]
&gt; The bitboard representation is probably going to be essentially necessary ... Perhaps your scheme could be bit twiddled to get a bitboard quickly enough. If you look closely, he's suggesting storing each of the 4 bits of the representation in 4 separate Word64s, instead of storing them contiguously. So generating a proper bitboard is a matter of doing a tiny bit of bit twiddling. white (Board a _ _ _) = a pawn (Board _ b c d) = b .&amp;. complement c .&amp;. d whitePawn :: Board -&gt; Word64 whitePawn b = white b .&amp;. pawn b for example, gives you a bitboard of the white pawns for the cost of 3 bitwise ands and a complement. It's a fairly simple representation, so you can try profiling it.
The key thing I'd try to look for is if you need all the information in your rotated boards. e.g. do I only need to track that in my 45 degree boards that a thing is white vs. black and/or a queen/bishop of that color. That may let you bit twiddle it down to just a couple of boards you can mask and combine. I was going to mention the bishop/queen case as a separate concern, as well as the issue of representing castleability, etc. One real benefit of not using a mutable bit board representation is that it becomes much easier to talk about transposition tables, game history, etc. if you aren't paranoid about destroying the only copy of the board you have. The trick is not getting killed by whatever evaluation you are doing that isn't fully in a worker-wrapper transformed optimized tight loop. That is where you'll probably run into issues. Now, there may be a more invasive option. One trick I tend to use when I want to write a thing but Haskell isn't fast enough to make it go is to write it in Haskell in a different way: Make an EDSL to express the problem space, turn my domain specific problem into a language problem, then write a compiler for my domain that does every domain specific optimization I can think of and have it spit out efficient code to something like LLVM/CUDA, etc. The benefit of that model is you can know it can eventually hit whatever speed limit that is achievable in _any_ language and you still get to think and code in Haskell. The cost of that model is of course that it is a very very deep embedding, and you then need to write a compiler to write a program where if you're willing to accept whatever you get a more direct shallow encoding of the domain as a library / program in haskell would do just fine.
Oh yeah, I actually even wrote a [library](http://hackage.haskell.org/package/modular-arithmetic) for something like this. (Or, at least, the special case of integers modulo some number.)
You can use the free or operational monad to do a more precise encoding (what you're proposing looks a lot like the operational monad). In the case of the free monad if you wanted an API like: command1 :: Req1 -&gt; Command Resp1 command2 :: Req2 -&gt; Command Resp2 command3 :: Req3 -&gt; Command Resp3 Then you could ensure that each request was matched up with its particular response using: {-# LANGUAGE DerivingFunctor #-} import Control.Monad.Free data CommandF x = Command1 Req1 (Resp2 -&gt; x) | Command2 Req2 (Resp2 -&gt; x) | Command3 Req3 (Resp3 -&gt; x) deriving (Functor) type Command = Free CommandF command1 :: Req1 -&gt; Command Resp1 command1 req1 = liftF (Command1 req1 id) command2 :: Req2 -&gt; Command Resp2 command2 req2 = liftF (Command2 req2 id) command3 :: Req3 -&gt; Command Resp3 command3 req3 = liftF (Command3 req3 id) You can even skip the last three boilerplate definitions using Template Haskell provided by the `free` library. Then I think it would look like this: {-# LANGUAGE DerivingFunctor, TemplateHaskell #-} import Control.Monad.Free import Control.Monad.Free.TH data CommandF x = Command1 Req1 (Resp2 -&gt; x) | Command2 Req2 (Resp2 -&gt; x) | Command3 Req3 (Resp3 -&gt; x) deriving (Functor) type Command = Free CommandF makeFree ''CommandF Then that will generate all the commands for you.
Here just for you: https://www.youtube.com/watch?v=nFuwS9M7jkQ
Maybe the author meant abstract data types?
It has at least been [captured by the Wayback Machine](http://web.archive.org/web/20140528151730/http://legacy.cs.uu.nl/daan/parsec.html).
Notice that one might have hoped to use the pre-cut `PB.count` for this problem but its type is like that of `fold`. There seems not to be a convenient better type though; a return type like `m (n, r)` would be of use in some cases, but the natural hope here was simply to `maps` or `transFreeT` over the lines. In the end though, one wants to collapse to a producer, so the only available target functor is itself `Producer` -- which in this case is always a singleton, so to speak, not something that appears in its type. There's probably some simpler arrangement available in here somewhere, I can't tell. 
is this for fizzbuzz? :)) 
As I understand it, returning the `f (b -&gt; c)` requires `a` to have parsed something successfully first.
no, just for great good.
One could just as easily say that talking about 'x' is evil. There is no sense in naming something when that name means nothing.
Unn, you are correct. It is even in the description: http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Applicative.html#v:-60--42--62-
https://hackage.haskell.org/package/pointfree [julian@valyria ~]$ pointfree 'filter (\x -&gt; mod x 5 == 0) [0,3,5,13,25]' filter ((0 ==) . flip mod 5) [0, 3, 5, 13, 25]
Then you might like Scala better :) it'd be "_ % 5 == 0" - short, no arbitrary names, and no point-free weirdness.
Others have already pointed out practical solutions for the different actual problems you might be having, but technically, _cannot_ exist a purely functional implementation of arrays with O(1) read and write operations (though I don't remember what is the proof). But there are "workarounds", e.g. if you only ask for O(1) access on the "latest" version of the data structure, then diff arrays or the ST monad would work. If you don't need O(1), then an IntMap would work. Etc.
Of course there is sense in naming something even if the name is vacuous. It makes it much more readable. With tacit/point-free style, you often have to use combinators to rejigger things, such as `(.)` and `flip`, and as a result you have to do some intermediate computations in your head to figure out the intended meaning of the program, where the pointed style would make it obvious what's going on. For instance, if I write map (join (*)) xs it's not obvious what's going on, but if I write map (\x -&gt; x * x) xs it's much easier. Sure, `x` doesn't mean anything, really, but it does give you something to hold on to mentally.
I've written quite a few board game engines in my day (see http://www.ioccc.org/2001/dgbeards.c, http://dinsights.com/POTM/LOAPS/finals.php, and http://tron.aichallenge.org/profile.php?user_id=1507 for some examples). Most of my engines were written in C, and one or two in Java. After awhile both of those languages became deeply unsatisfying and I searched for other better languages. I considered a fairly wide range of languages from D to Go to OCaml to Scala to LISP but never found one that I was satisfied with. I suppose one might argue that the C++14 standard might fix enough of my old frustrations, but now that I've been doing full-time Haskell development for four and a half years that thought is revolting. I started writing a chess engine in Haskell awhile back, but then moved on to more productive endeavors. If I ever find time to get back to game programming, I will definitely do it in Haskell. When I last played around with the problem, I wasn't worrying too much about the board representation. I would probably recommend starting with a simpler board representation like 0x88. 0x88 can be surprisingly fast for simple engines. You can build up generalized game playing infrastructure around that and then swap in a more sophisticated board representation later. This approach gets you thinking about higher level things which Haskell is better suited for. It also still requires you to deal with some mutation / impurity to implement things like transposition tables, iterative deepening, time-limited searches, pondering, etc. But it stays away from the really low level bit twiddling stuff that could end up being a big time suck. If you get that working fairly well, then you can go back and revisit bitboards for your board representation. If you can't get bitboards to be efficient enough in Haskell, then you might consider dropping down to C for some of the low level coding there. A lot of C engines drop down to assembly anyway, so you would be in good company. If you're really hard-core, I like /u/edwardkmett's suggestion of building an EDSL to generate C/LLVM/CUDA code. The extra pain of building a compiler can be a really nice tradeoff if the low-level optimization thing isn't your cup of tea. If you're not experienced/interested in the really low-level asm/CPU architecture levels of optimization, you may not even be able to get to within a factor of 5-10 of the speed of the top engines anyway even if you were using C.
Yes, I realize it's terrible. It's my first real project in Haskell, so any help would be appreciated. Also, giant thanks to somebody who posted a BF in Haskell tutorial a loong time ago, or else I couldn't have done this. On a side note, I'd really like some help making this Turing-complete.
Readability is highly subjective. If you find a certain form hard to read there is no reason to assume other people experience the same. For example, I'd be perfectly happy reading the version of @bheklilr posted below: filter ((== 0) . (`mod` 5)) [0, 3, 5, 13, 25]
Oh, I want that. Pls to have GHC extension. I'm also a big fan of the TupleSections extension, which gives you something like the above, for building tuples. ('a',,,True) :: a -&gt; b -&gt; (Char, a, b, Bool)
Avoiding lambdas for the sake of it is probably misguided. Any particular instance of avoiding lambdas might be beneficial though. This is one of those cases where it doesn't really hurt and may bring a slight bit of extra readabliity. 
Imagine you want to do: #define FOO(x) ((x) * NUMBER_OF_ELEMENTS + a, \ (x) * NUMBER_OF_ELEMENTS + b, \ (x) * NUMBER_OF_ELEMENTS + c, \ (x) * NUMBER_OF_ELEMENTS + d) Repeating the sub-expression ((x) * NUMBER_OF_ELEMENTS is annoying, and macros do not have `let` or `where` clauses. But how can we encode `let`? In Haskell, we can do: (\name -&gt; (name + a, name + b, name + c, name + d)) (x * NUMBER_OF_ELEMENTS) (i.e: use an applied lambda (also called a `redex`) as a form of `let`). In the C preprocessor: #define FOO(x) FOO_HELPER((x) * NUMBER_OF_ELEMENTS) #define FOO_HELPER(x) ((x) + a, (x) + b, (x) + c, (x) + d) 
Examination of core shows that it seems to be equivalent to performing a `lookup` followed by an `update`. So not as efficient as `updateLookupWithKey`, but not unreasonable. Huh. I just looked at the `Ixed` instance for Data.Map, and `lookup` followed by `update` is exactly what it does.
Clojure has something similar, where it is `#(= (mod % 5) 0)`, or, if you want multiple arguments, `#(= (mod %1 %2) 0)`.
This is the argument in favor of golf code : - he compiler can read it, so it's readable. - it's shorter, therefore there is less to read making it more readable. Then you can easily deduce easily that, the shortest version is the most readable ;-) Readable code shouldn't only be easy to read but ideally show the author intent. If the author is thinking "I want to filter element that are dividable by five" the \x -&gt; x mod 5 == 0 is the way to go. It says what's on the tin it's easy to write and every body can read it. Point free version are only readable by people used to point-free version. When you see (==0).(`mod` 5) If you have never seen it before it , it takes some effort to figure out what's going out and especially the use of `backtick` : (==0) (mod 5) type check, has the same type signature but does something totally different (strange, I though In Haskell, when something compiles , it was bug free) And even if your are used to point-free style, chances are that in your mind you see it as `\x -&gt; x mod 5 == 0`. So why not writting it as it is ? Point free style, might be readable, but is usually hard to write or at least harder than the lambda version. However, I don't like lambda either, I would probably prefer filter dividableBy5 [0,3,5,13,15] where dividableBy5 x = x mod 5 == 0. It works and show my intent, but that a matter of opinion. 
Yes, I'm not saying its like completely awful. But for beginners, I feel its best to promote avoidance. There's a weird tendency for beginners to think "ah! Haskell! I should write things point-free!"
I don't see how the MS site can help us here, I cannot find any docs on parsec there. Except [this](https://research.microsoft.com/pubs/65201/parsec-paper-letter.pdf), which is not docs... Enyway, as pointed out in another comment, the internet archive has a copy of it.
Antoine Latter replyed with the following links: https://web.archive.org/web/20140529211116/http://legacy.cs.uu.nl/daan/download/parsec/parsec.html https://web.archive.org/web/20140528151730/http://legacy.cs.uu.nl/daan/parsec.html
The problem with it in scala is it is almost never clear what delimits the scope where the _ is found without being in a context where you are checking a type. In a language where inference is already basically dead its a viable convention. You'd need a sibling mark to indicate where the scope starts to retain nice properties.
The same module you wrote should work *mutatis mutandis*: import Control.Monad (unless,forever) import Control.Monad.IO.Class (MonadIO, liftIO) import Pipes import qualified Pipes.Prelude as P import Data.Char import Data.ByteString (ByteString) import qualified Data.ByteString as BS import System.IO (isEOF) ioSource :: MonadIO m =&gt; IO Bool -&gt; IO a -&gt; Producer a m () ioSource k f = go where go = do cond &lt;- liftIO k unless cond $ do liftIO f &gt;&gt;= yield &gt;&gt; go lineSource :: MonadIO m =&gt; Producer ByteString m () lineSource = ioSource isEOF BS.getLine skipHeader :: Monad m =&gt; Pipe a a m r skipHeader = P.drop 1 countCommas :: Monad m =&gt; Pipe ByteString Int m r countCommas = forever $ do line &lt;- await yield (BS.count comma line) where comma = fromIntegral (ord ',') filterBad :: Monad m =&gt; Pipe Int Int m r filterBad = P.filter (/=2) main = P.length pipeline &gt;&gt;= print where pipeline = lineSource &gt;-&gt; skipHeader &gt;-&gt; countCommas &gt;-&gt; filterBad -- $ pbpaste | ./machines2 -- 2
holy fuck i love you ps it's spacekitteh
I don't know what you mean by strong, but no one has ever written a **competitive** chess engine in anything other than C/C++. Even Java, which is quite quick compared to most things seems to be too slow/high level. And it is not for lack of trying, mind you. Various enthusiastic people regularly embarks on chess in their favourite language, many thinking it is "as fast as C". The only engines in other languages who have been even near the top are written in Delphi. See: [Chess programming languages](https://chessprogramming.wikispaces.com/Languages)
Cool! My only comment is that you should prefer data Direction = Left | Right over using a `String` for that. Obviously there are plenty of ways you could make this run more efficiently if you were really serious about this. For example, you'd want to start by using something more efficient than `[[Char]]` for your representation of a program, since each lookup is `O(n)`. You could use `Data.Map` instead, or `Data.Vector`. But your code as it stands is simple and clear. Great job! As for Turing completeness: Seems clear to me. If you had used `Integer` instead of `Int` (thus making your language even less efficient), you could even easily prove Turing completeness directly just by showing that arbitrary `if-then-else` and `while` can be implemented in a subset of your language. But even without that, I'm sure that there is some similar language that has already been proven Turing complete that you could implement in your language. E.g., perhaps the original BF. EDIT: Oops, `O(n)`, not `O(n^2)`. Not *that* bad. Still not great though.
That is my favourite Scala feature
&gt; My fix was to rewrite the reporting logic so that it no longer made this distinction. Does it mean that a non-published package is hidden in the web interface but visible in 00-index.tar.gz ? Or I guess it means that `cabal install` is perfectly capable of installing a package by name as long as the corresonding URL works even if not present in the 00-index.tar.gz ?
&gt; I suppose one might argue that the C++14 standard might fix enough of my old frustrations, but now that I've been doing full-time Haskell development for four and a half years that thought is revolting. If I ever find time to get back to game programming, I will definitely do it in Haskell. You might also be (eventually) interested in Rust.
That is awesome, thanks!
So `Control.Foldl.ByteString` has folds which are designed to generalize the folds found in `pipes-bytestring`, but it's missing the `count` fold. I will add the following fold to it, then: count :: Num n =&gt; Word8 -&gt; Fold ByteString n And then you'd be able to write: comma_count = purely Pipes.Group.folds (count 44) . drops 1 . view PB.lines I also forgot to mention that even without this fold you can simplify your code a little bit using: comma_count = Pipes.Group.folds (\n b -&gt; n + B.count 44 b) 0 id . drops 1 . view PB.lines `Pipes.Group.folds` is a convenience utility for folding each subset of the stream individually, resetting the accumulator at the beginning of each subset.
Here's a post I wrote up demonstrating how `pipes` streams lines and other stream subsets in constant space: http://www.haskellforall.com/2013/09/perfect-streaming-using-pipes-bytestring.html
I'm a beginner, and I distinctly remember thinking that lambdas were somehow less preferable to something like point-free style (whatever that is). I don't think this was ever explicitly said, but was a wrong notion I picked up from lurking.
&gt; That's the synonym for a pure source of [bs](http://www.urbandictionary.com/define.php?term=BS) I nominate this completely taken out of context quote for the Haskell Weekly News Quotes of the Week. 
One of the beginner resources (I think RWH?) mentions that lambdas aren't as heavily optimized as named functions by the compiler. I don't think that's true anymore, but it's probably the origin of this behavior.
Not there, unfortunately. Photos?
The [conduit code for this](http://hackage.haskell.org/package/conduit-combinators-0.2.8.3/docs/src/Data-Conduit-Combinators.html#line) isn't too intimidating. Note that it uses mono-traversable to abstract over different containers (e.g., `String`, `Text`, `Vector Char`). The approach is to allow the user to provide a `Sink` to be called for the line, which allows `line` to force the draining of the rest of the line.
Not that it's a good idea, but there's a unix utility called `pointfree` (you can get it by `cabal install pointfree`) which answers this for you: λ. pointfree "filter (\x -&gt; mod x 5 == 0)" filter ((0 ==) . flip mod 5)
Awesome! My first haskell project was a brainfuck interpreter and it was a great learning experience. Also yours looks much nicer than mine did, so kudos. :)
I think /u/yitz 's point is that by using an Int the length of a while loop is limited to the maximum size of an Int. This is not enough to show Turing completeness but it would be if the boundless Integer was used instead 
I agrees that lambda seems somehow wrong and heavy map (+5) [1..5] Is *cleaner* than map (\x -&gt; x+5) [1..5] Maybe it's only because the lambda syntax is a bit awkward and the scala idea map (_ + 5) [1..5] would probably be better and unify both ways (kind of). Point free style gets messy very quickly when you add `flip` and `.` and maybe because `.` is the *wrong* way round ... 
Looking forward to the following posts on this. Still, I have to get this off my chest. Your CSV example: 1. Is a homegrown CSV parser. Eeeek! Rule 14 of software engineering is "Never write your own CSV parser." 2. Will break on quoted delimited files where a newline or delimiter occurs inside a quoted field.
The `pipes` policy is to enforce constant-space usage for production code. That's why the `pipes` prelude does not enforce constant space for demonstration `String` utilities like `Pipes.Prelude.stdinLn`, but once you switch to using `ByteString` or `Text` you have to begin to get streaming right. Note that you can still write programs that don't run in constant space, but `pipes` libraries try to avoid this as much as possible. You can actually do it exactly like the Pythonic approach if you use the `pipes-group` library, which basically encapsulates all the idioms written in that post. You can read [the `pipes-group` tutorial](http://hackage.haskell.org/package/pipes-group-1.0.1/docs/Pipes-Group-Tutorial.html) to learn how to write `pipes` programs that greatly resemble the Pythonic approach. I definitely don't consider line-by-line parsing to be the idiomatic Haskell approach.
&gt; Like if you do `fn1 &lt;*&gt; fn2` it doesn't matter which is evaluated first, but if you do `fn1 &gt;&gt; fn2` then depending on the monad it might. Problems: 1. You're mixing up *order of evaluation* and *order of effects*. In `fn1 &gt;&gt; fn2`, `fn2` might be *evaluated* before `fn1`. But unless the monad in question is commutative, the *effects* of `fn1` must be ordered before those of `fn2`. 2. Applicatives are no different in this regard. In `fn1 &lt;*&gt; fn2`, generally, the effects of `fn1` come before those of `fn2`.
:) 
I also wrote a befunge interpreter as one of my first haskell projects (where I first used monad transformers!). Just dug it up and uploaded the repo here: https://github.com/jberryman/Befunge93
&gt; What's the benefit of using [applicatives] over monads then? Applicatives give you lots of constructions that are hard or impossible with monads: 1. Applicatives compose much more straightforwardly than monads do. The product of two applicatives is an applicative, as is the composition of two applicatives. With monads, you need complex solutions like monad transformers. 2. A free applicative ([see tel's comment](http://www.reddit.com/r/haskell/comments/2f6501/wikipedia_in_functional_programming_a_monad_is_a/ck6g3ly)) is basically a list of actions plus a pure function to combine their results. You can inspect what actions a free applicative will perform without executing it. With free monads, you often have to do it because the actions chosen will depend on the results of other actions. These take a bit of study to understand, but here's how I'd put it in friendlier terms: `Applicative` is of most benefit when you're writing a library. A library that exposes an `Applicative` interface instead of `Monad` is able to more powerfully analyze the code that you compose with the library, and has more and easier choices of how to interpret those actions by reusing stock applicative or monadic types. From the point of view of the clients of such libraries, the type is less powerful than a monad, but has some other advantages. E.g., the Haxl library transparently parallelizes and caches your web service requests, but only works for cases where you know ahead of time what services you will need to call before calling any of them.
oh tere you are, no need to keep looking
Good point :p I probably shouldn't have used a "CSV" file as an example, but I wanted to keep things simple. In my defense, I often deal with streams of tab separated records that have no notion of quoting (i.e., a quote is a quote is a quote). That's why I (unfortunately) can't use `pipes-csv`: it has no option for disabling quoting.
Cross-posted this to /r/haskellgamedev.
This is truly excellent. Is not the first time I watched it and probably won't be the last one.
Part of the reason to make things pointfree is so that you can factor out small subexpressions and name those behaviours for easier reasoning: isZero :: Integer -&gt; Bool isZero = (== 0) modulo :: Integral a =&gt; a -&gt; a -&gt; a modulo x = (`mod` x) (f .: g) x y = f (g x y) -- * divisibleBy :: Integer -&gt; Integer -&gt; Bool divisibleBy = isZero .: modulo filter (divisibleBy 5) [0, 3, 5, 13, 25] The partial application gymnastics (`*`) required to compose functions of different arguments are necessary because in Haskell’s type system, all function types consume and produce exactly one value, and composition is defined in terms of application. Now, I wouldn’t be keeping up appearances if I didn’t insert the usual plug about [concatenative programming](http://concatenative.org/), in which composition *is* the default. In [Kitten](https://github.com/evincarofautumn/kitten), for example, you might start with this code: [0, 3, 5, 13, 25] { -&gt; x; x % 5 = 0 } filter Then make it pointfree (and consequently postfix): [0, 3, 5, 13, 25] { 5 (%) 0 (=) } filter And refactor it into this: def isZero (int -&gt; bool): 0 (=) def divisibleBy (int int -&gt; bool): (%) isZero [0, 3, 5, 13, 25] { 5 divisibleBy } filter So avoiding lambdas is worthwhile *when the language makes it natural*. In Haskell, that’s not all of the time, but I find this solution eminently readable: filter ((== 0) . (`mod` 5)) […] It says what it means: “keep the elements that are equal to zero modulo 5”. 
Okay! It's pretty much playable now!
Maybe some parts will click for you reading through that code since it's a problem domain you're familiar with now. I do think the state monad is nice here, but you might find you like your straightforward passing of program state better. There's always something to learn with haskell!
Esolangs were how I discovered Haskell too. So many interpreters and compilers were written in it, and I ended up following that trail.
Wikipedia has some general info and typing rules: http://en.wikipedia.org/wiki/Simply_typed_lambda_calculus You can use Hindley-Milner type inference instead: http://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system http://www.grabmueller.de/martin/www/pub/AlgorithmW.pdf But I find constraint-based type inference cleaner and easier to extend with new features (this paper has a good description of algorithm W as well): http://dspace.library.uu.nl/handle/1874/23951 STG could be useful as an intermediate step between your language and LLVM IR: http://research.microsoft.com/pubs/67083/spineless-tagless-gmachine.ps.gz So.. you still need a parser, GC, optimizations if you want...
How does the performance compare to using `GHC.Generics` with typeclasses?
often, pointfree is easier to understand. IMO, there should be a short (one character) `(&gt;&gt;&gt;)`, so you can do: filter ((`mod` 5) &gt;&gt;&gt; (== 0)) [0, 3, 5, 13, 25] which is more readable to me
Yea, I guess the state monad just hasn't really "clicked" for me yet. I just don't get the process of using it, and all of the examples that I've seen are really contrived or complicated.
I feel like I've been lied to, but I don't know by whom.
err..this is a nitpick, but the standard term is divisible not dividable.
Says [here](https://github.com/i-tu/Hasklig#no-support) that neither the OSX Terminal nor iTerm2 is supported.
Perhaps I was a bit too concise in the post. `00-index.tar.gz`, and by extension `cabal` itself, doesn't know about non-published (candidate) packages at all. That is handled by the build bot, which downloads a separate list from Hackage, pulls a bunch of URLs from that, then feeds those URLs into `cabal`. My fix didn't change that; I only worked around it by having `cabal` generate reports for URL-packages as well. I've since made a few edits to the post which should make this more clear. Admittedly I'm pretty new to this blogging thing, so this kind of feedback is good.
Thanks.
There's also the other SPJ book: [Implementing Functional Languages: a tutorial](http://research.microsoft.com/en-us/um/people/simonpj/Papers/pj-lester-book/) &gt; Relationship to The Implementation of functional programming languages &gt; &gt; An earlier book by one of us, [Peyton Jones 1987], covers similar material to this one, but in a less practically oriented style. Our intention is that a student shoud be able to follow a course on functional-language implementation using the present book alone, without reference to the other.
I didn't measure it yet, but it should be about the same. The function that is passed to `createA` or `gtraverse` is the same code as you'd otherwise have as the instance for `K`, while the instances for the other cases are always the same boilerplate code. One thing I'm not sure about yet is if I should put `{-# INLINE #-}` everywhere. I came across several uses of GHC.Generics that did this.
I was just wondering about that the other day - thanks for inadvetently answering the question I hadn't asked yet!
What's about speed of compiled code? I'm afraid GHC might be confused with imperative patterns.
Hmm... `set!` pretty much gives you shared mutable state, and I don't think it possible to do that without real mutable references. From my little experience I can say an interpreter for any practical language would end up in `IO` anyway, especially when you chuck features like string interning and threading into the mix. So usually trying to avoid `IORef` isn't worth the effort.
&gt; `Put` is a `Monoid` (with `mappend = return ()` and `(&lt;&gt;) = (&gt;&gt;)`), so we can use `gfoldMap`. He meant that `mappend` is off there and should be `mempty`.
I have occasionally defined (|&gt;) :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; c (f |&gt; g) x = g (f x) which (I believe) is the same as the forward-composition operator in F#, and also looks a bit like a Unix pipe.
Ah! Thanks!
I've heard these drivers in real life!
I think you want something like [gearman](http://gearman.org/), to distribute work. It can be backed by different stores and can be redundant - I don't think there is a working client yet for Haskell - I did find this [work in progress](https://github.com/anchor/gearman-haskell). Building the server in Haskell is also possible of course but is a lot of work. I am working on such a [project](https://github.com/jeremyjh/free-agent), to make a more general type of framework for exactly this sort of use-cases but its a long way from ready - at least a couple of months out from an initial release. It uses distributed-process to communicate between nodes and acid-state to store the job definitions and schedules, and the nodes will be able to distribute work between each other in an ad-hoc fashion as long as they share the correct capabilities and geographic zones. Unfortunately I have only a few hours I can work on it each week so it is taking ages and won't be able to help you at this time I think. I would be interested in hearing though how you end up solving this and what kind of difficulties you run into, or features you would need.
Eh.. oh you're right. Haha. whoops. Fixed to something that makes sense :) This is what I get for not really having good examples on hand.
This looks really cool. I wish I could either be at the talk or understand how context-local heaps work from just the slides. But alas, neither of those two are going to happen. Any help?
At work, something that is working well for us is to use RabbitMQ as a way of distributing work between different workers. There is a [package](https://hackage.haskell.org/package/amqp) on Hackage for RabbitMQ and the amqp protocol which is stable and usable for production, and the author is nice and responsive.
If you know a way of doing it without the unsafe*, let me know!
I don't understand that fully, but why not make `IO` explicit there ?
Just look at the only place where startTime is used, 8 lines down. The start time stores the result of an IO action, not an IO action. The idea is that I'd like the tick to be zero at the program start, not midnight etc. So I store the start time in a global variable, hence the unsafePerformIO 'trick'. If I want to avoid using this unsafe primitive, I'd have to explicitly store and pass around the start time everywhere or be in a special time monad etc. You can find the same usage of unsafePerformIO in the Trace module to create a top level MVar. The idea is the same, I find functions like getTick and traceS to be basic enough that I want to be able to call them anywhere, without having to lug around a Time/Trace record or be in a special monad that does it for me. This is of course a debatable decision, and the program will work perfectly fine if you remove all of this and just return the time without any delta from getTick.
I don't know, English is not my first language and both terms are in the dictionary, so I had to chose one. Maybe that's the point of point-free style, you don't have to worry about that type of things ;-)
Sure, but in the post you replied to I already suggested that solution and explained that the very goal of this entire construct is to avoid doing just that. Here: &gt; If I want to avoid using this unsafe primitive, I'd have to explicitly store and pass around the start time everywhere" &gt; I find functions like getTick and traceS to be basic enough that I want to be able to call them anywhere, without having to lug around a Time/Trace record or be in a special monad that does it for me But I can avoid all that at the cost of the mental anguish over using one unsafePerformIO. Others might opt to rather have the burden of threading a start time around manually, of course ;-)
So an extensive conduit user I am hoping machines matures to the point where we can move onto it. A key differentiator (as I understand it) is the ability to represent more complex graph execution plans. We have achieved a number of equivalent things with conduit but did all the heavy lifting ourselves. Also the code is quite large now and it can be hard to figure out exactly what is going on. From where I sit pipes/conduit/enumerator/iteratee/other all achieve basically the same result with varying amounts of pain. If machines is able to become a special purpose runtime (like LINQ in .net) it would be of great benefit to us. Also support for controllable parallelism would be great.
Monad Transformer stacks are so hard to grok! Especially when trying to decide which better matches the problem at hand.
&gt; String replaced with Text Switching to Text is not too difficult, if you commit to it wholesale: `-XOverloadedStrings`, and `import qualified Data.Text as T`. Then, sprinkle with `T.unpack`s to satisfy the remaining libraries that require `String`. I never looked back! :-) 
I think a concrete example would be very helpful.
Warp is a good choice, assuming HTTP is a requirement. It also sounds like you only have a one url for the server, so you can turn off the url parsing, which improves the speed: https://hackage.haskell.org/package/warp-3.0.0.4/docs/Network-Wai-Handler-Warp.html#v:setNoParsePath I think this is the most stringent requirement. &gt; Should be robust and reliable. A power failure should not corrupt the data. I think that is another way of saying it must be durable. This means you need to immediately serialize the request before returning success. This will negatively effect your latency. You also have this requirement &gt; If the "POST" does not work the first time, it should retry a specified number of times before failing So basically this is another way of saying your service can tolerate failure. If that is true, then I'm not sure I buy the durable requirement. I'm just guessing here, but I think you want to make sure that you can only loss some small of data. Clarifying these requirements will inform your design. Also how long are you waiting before sending the urls? If it is short amount of time? Is it possible the timeout will have already occurred by the time the request arrives. How much timing precision do you need? 
To compute anything you need to force something. If you know what you're going to force (i.e. "need") you can just force them in parallel.
It took me *forever* to grok monads! Just keep reading about 'em, and working through examples. You'll get it, and love it.
If it was me, I wouldn't worry about the scaling part for now. It's easy enough to slap a proxy in front of your servers that spreads the load once you reach that point, but just adding requests, sleeping, and sending a post is something that's going to use a very small amount of resources. Unless you're trying to handle load on the level of an extremely popular service such as Twitter or whatever, you'll probably never even need more than one. The server is probably going to spend most of its time idle, waiting for timers to expire. Since you weren't more specific about the "get JSON as a payload" part, I'll assume you mean a connection will just result in sending a JSON chunk to you. If you get to control the protocol, I'd suggest JSON encapsulated in a [netstring](https://en.wikipedia.org/wiki/Netstring). It's convenient to work with, because you get the length of the message up front. You can use Aeson to decode the JSON request. I'd use Attoparsec to parse the netstring (very easy with the `parseWith` function) and just write the server code myself since it's so simple. You can just forkIO each new connection after you accept it, and if you compile with threaded RTS support and the `-N` RTS option, it'll automatically use all your CPUs. Probably the best way to handle the dispatching of the posts is to have a scheduler thread that calculates the time until the next event and sleeps until then. There are some priority queue implementations on Hackage that may be useful here. The lazy way to deal with it would be to forkIO a thread for each timer and just have it sleep the defined amount of time. You can probably get away with the lazy way until you're going to have tens (maybe hundreds) of thousands of timers waiting at the same time. Your route to dealing with stuff such as power failures or network errors is going to be essentially the same as in other languages. Probably the simplest way to handle it would be to use a database server like PostgreSQL and just add the timers there when they are submitted, and remove them when they complete. PostgreSQL also can handle JSON quite gracefully.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Netstring**](https://en.wikipedia.org/wiki/Netstring): [](#sfw) --- &gt; &gt;In [computer programming](https://en.wikipedia.org/wiki/Computer_programming), a __netstring__ is a formatting method for [byte strings](https://en.wikipedia.org/wiki/String_(computer_science\)) that uses a declarative notation to indicate the size of the string. &gt;Netstrings store the byte length of the data that follows, making it easier to unambiguously pass text and byte data between programs that could be sensitive to values that could be interpreted as [delimiters](https://en.wikipedia.org/wiki/Delimiter) or terminators (such as a [null character](https://en.wikipedia.org/wiki/Null_character)). &gt;The format consists of the string's length written using ASCII digits, followed by a colon, the byte data, and a comma. "Length" in this context means "number of 8-bit units", so if the string is, for example, encoded using [UTF-8](https://en.wikipedia.org/wiki/UTF-8), this may or may not be identical to the number of textual characters that are present in the string. &gt; --- ^Interesting: [^Bencode](https://en.wikipedia.org/wiki/Bencode) ^| [^Comparison ^of ^data ^serialization ^formats](https://en.wikipedia.org/wiki/Comparison_of_data_serialization_formats) ^| [^Simple ^Common ^Gateway ^Interface](https://en.wikipedia.org/wiki/Simple_Common_Gateway_Interface) ^| [^JSON-RPC](https://en.wikipedia.org/wiki/JSON-RPC) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ck94z48) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ck94z48)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
It contains derived work. Compare [this](https://github.com/YoEight/rakhana/blob/master/Data/Rakhana/Internal/Parsers.hs) with [this](https://github.com/Yuras/pdf-toolbox/blob/master/core/lib/Pdf/Toolbox/Core/Parsers/XRef.hs) and [this](https://github.com/Yuras/pdf-toolbox/blob/master/core/lib/Pdf/Toolbox/Core/Parsers/Object.hs) If the author is reading this: it is ok to reuse code, but you probably should retain copyright notice... 
I did mention the code was based en pdf-toolbox in Parsers.hs module header. Please tell me what I can do to make this right. 
You're totally right, I somehow skipped right over that when I was reading your reply (if I'd noticed it I wouldn't have bothered replying). My apologies!
The fact that in Haskell 'bottom' is everywhere make some ADT derivations impossible to express. Personnaly I'm not hit by that fact. Likewise, Scala keeps saying it has ADT which is not true neither. Haskell ones are clearly closer to the concept though. On the same line, what about Haskell Functor ? Category theorists will not agree calling it Functor based on their own definition.
And good luck with your project. Don't underestimate the amount of work, PDF is pretty complex. And I hope you'll be able to come with better API design then mine.
Thanks, I will apply your remarks. I'm aware of the amount of work. I'm currently implementing PNG prediction which is really annoying and still being a tiny part of the PDF mess. I hope I didn't imply that I started Rakhana because I didn't like how you designed your API. Because, it's not. I really wanted to see what happens when 'opinionated' libraries like pipes and lens meet a mess like PDF specs
Data constructors aren't exposed to the user. It's all about implementation details I exposed to share my thoughts on the post. The user have to use an API that makes the dirty pattern-matching internally. Still, you made a valid point and I'm not happy with the current encoding. I tried this one with no success: data TReq a where Seek :: Integer -&gt; TReq () [..] Get :: Int -&gt; TReq Strict.ByteString newtype TResp a = TResp a with Tape and Drive defined like this type Tape m a = forall r. Server' (TReq r) (TResp r) m a type Drive m a = forall r. Client' (TReq r) (TResp r) m a I'm open to suggestions *updated Tape and Drive type definitions
People have also proposed versioning individual functions
The patch is [here](https://github.com/YoEight/rakhana/commit/039a5c336cca523ba480325c8c9f5720c9359297)
Is the json payload coming from a web client? If not it would seem simpler and faster to have that process just write URL, TIME, CONTENT, to a database (later it can be a fancy distributed one, maybe; probably lots of good options here), then have your haskell daemons poll the database for `time &lt; now AND status == unclaimed`, or something and attempt those http requests. If you have very strict timing requirements, then that is going to be the difficult part of the problem I think.
No harm done ;-)
Any hint on how you're going to deal with the scheduling? I'd think about just getting an RDBMS like Mysql in, and having alot of workers do: SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; SELECT * FROM urls ORDER BY time ASC LIMIT 1; DELETE FROM urls WHERE id = {url_id}; COMMIT / ROLLBACK; My assumptions are that the DB could handle alot of these queries, and that READ UNCOMMITED isolation level would allow workers to avoid selecting rows that are already being worked on in a non blocking manner. Though I've never used it before so this may be wrong.
&gt; This will unquestionably run at &gt;60FPS/1080p on any 200 bucks desktop GPU from the last year or two. You could do it in WebGL then, I think it supports shaders. Then people will be able to see your work with one click :-) Don't know if Haskell and WebGL work well together, but if you can tolerate JavaScript, Three.js is an amazing library and should make things pretty easy. Or you could try things out on Shadertoy, it already has some raymarching stuff.
If you're interested in getting paralelism then you'll need to overlap the evaluation of sub-expressions; as Ikcelaks said, the Eval monad and Control.Parallel.Strategies allows you specify potential SMP paralelism in this way. I suggest you study these before attempting distributed paralelism.
I can see benefits in situations where modules are mutually recursive, making it unfeasible to distribute them separately.
Isn't the problem here simply that `F Bool` doesn't exist in the signature definition of A? module A where type instance F Int f :: F Bool module A where type instance F Int = Int type instance F Bool = Int f = 42 If we had written an arbitrary type `T` instead of `F Bool`, this would be clearly nonsensical: module A where type instance F Int f :: T module A where type instance F Int = Int type T = Int f = 42 It's nonsensical because the `T` from the implementation is simply not in scope in the signature definition. We would either have to write: module A where type instance F Int type T f :: T module A where type instance F Int = Int type T = Int f = 42 In which case the type T would be hidden, so we can't do `f 2`. Or we would have to write: module A where type instance F Int type T = Int f :: T module A where type instance F Int = Int f = 42 In which case T is exposed, but it's known to be an int, so `f 2` is also disallowed. Similarly, `F Bool` from inside the implementation module is simply not in scope in the signature module in the original version, so the signature module A where type instance F Int f :: F Bool Can be rejected as incorrect for the implementation module, because F Bool has a different meaning here. We'd either have to concretely define `type instance F Bool = Int` in the signature, or keep it abstract and define `type instance F Bool` in the signature and `type instance F Bool = Int` in the implementation. If you look at it like this, it's perfectly fine for conflicting type instances to exist, as long as they aren't exported. For example if we have the signatures: module Foo where x :: Int module Bar where y :: Int and implementations: module Foo where type instance F Bool = String x = ... module Bar where type instance F Bool = Int y = ... It would be fine to have: module Baz where import Foo import Bar because the type instance is hidden in the signature so the conflict can never be observed. With this interpretation, an orphan instance just becomes an instance that shadows an earlier instance (which may or may not deserve a warning). Or am I completely missing the point here?
No, I think it is just really slow right now. It took a few minutes to call up one of the documentation pages from Google.
Hackage has been flaky for me throughout the day also.
Slow &amp; sometimes unusable for me too. I got 504 at one point, when using Hackage. If it's too slow for you, try https://www.fpcomplete.com/hoogle which hosts documentation for a lot of packages (but not all).
&gt; While splitting packages into smaller ones comes with a price, splitting modules is cheap On the whole I think I agree with you, but if we did what you suggest, then splitting modules would become much more expensive. It would be a backwards-incompatible change and it would require major module version bumps. Part of the reason it's cheap to split modules right now is precisely because they don't have versions associated with them.
Most types that admit an instance of any given typeclass admit an “obvious” instance, so the Haskell community generally gets away with the nonmodularity of typeclasses because there usually aren’t competing instances in the scope of a single application. Where we don’t get away with it, we have the `newtype` hack to paper over our lies. But that doesn’t change the fact that using typeclasses for things like equality and algebraic structures is fundamentally a mistake. Equality is *not* a property of a type; it’s a relationship between a type and an equivalence function. A type *cannot* be a semigroup; a type and an operation together constitute a semigroup. And so on. Typeclasses are awesome and useful for their original purpose of bounded polymorphism and principled overloading. We should keep using them for that, but perhaps explore other means of expressing anything more clever. 
trying to update a system using haskell-platform-2014 ... real fun - every cabal install takes like forever and is most likely to fail
This unreliability of Hackage is a real problem. There are businesses that are based on Haskell development and the flakiness of Hackage basically grinds their productivity to a halt every time it fails.
Just so that I understand correctly, `ComparisonSet` would actually be a functor that requires `t` to have some comparison operation, yes?
This is why private Hackage servers are a real boon :)
&gt; Isn't the problem here simply that F Bool doesn't exist in the signature definition of A? Didn't he address this point in the section "The basic proposal: require all instances in the signature"? The issue is that you can't have local, non-exported instances. All instances must be exported or you cannot guarantee that they will be unique.
Thanks for a great talk! I had the slot after yours, and following up on your performance was really tough. 講演、大成功！
Nah, I can't really tolerate JavaScript or WebGL, I'll stick with good old Haskell, C++ and regular OpenGL for the time being ;-) IQ already has a nice Julia Bulb on Shadertoy though, had the link in the project description on my website: https://www.shadertoy.com/view/MdfGRr
The json payload will be coming from a web client or another server. The timing requirements are not strict. 30 second precision is good. I don't need millisecond precision. If a POST is supposed to go out on December 12 2014 at 9:45 am.. it doesn't matter if it goes out between 30 seconds before or after 9:45 am.
we tried RabbitMQ with python. It didn't work for our load.
http://hackage.scs.stanford.edu is a mirror you can use as a backup
What I meant when I said "get JSON as a payload" is that, a client connecting to this server will only send JSON payload. And the server I need to write will only be capable of recieving JSON payload. I've never heard of netstring. Looks interesting. Any other benefits other than convenience to work with? is it faster? Im not sure that having the length of the payload upfront will have any advantage for my situation. And by "write the server code myself" you mean, no need for warp? You mentioned using postgres. I was hoping that there was someother lighter way of dealing with the backend. But I kind of realized that I might have to use postgres and persistent anyways.
My impression is that the very least we can do is endorse the demands formulated by the moderators of /r/BlackLadies who are trying to get the admins to make things better. [Here is the thread](http://np.reddit.com/r/blackladies/comments/2ejg1b/we_have_a_racist_user_problem_and_reddit_wont/) in question.
In ML module systems, binary operations still work, because the act of picking a dictionary for a set is not something done when you create a set, but rather whan you instantiate a set module. The act of instantiating a module generates a new type (the type of sets of nats) which then multiple sets can be typed as. And of course, union now works because the types are, in fact, the same, and the sets aren't actually carrying around a dictionary.
Luckily, /r/haskell seems to be a pretty decent place. I normally call shit out whenever I see it and I haven't seen anything in this sub, so yay \o/
The infrastructure seems very slow from here, too...
http://hdiff.luite.com/ too -- see instructions there.
What we are seeing here is that moderators are actually powerless for the tools at their disposal are not efficient enough, the admins refuse to take actions and even shadowban said mods for complaining about the racist outpourings they have to fend off.
I'm making a control theory library and want to prevent a block from saying it is both linear and nonlinear, for example, or a source block saying it is also a sink block. 
You've posted the same post on a number of programming subreddits - at this point your comment can't be directly for /r/haskell, which is a decent place (for now) so seems a bit spammy. What are you trying to achieve?
Sounds like you should be using a data type, not typeclasses.
Probably using IORefs and unsafe read/writes everywhere you might as well just use C++. I wonder what a chess DSL could look like - as edwardkmett talks about. CUDA could be a really interesting - is it possible to turn the whole thing into a breadth first search to get enough data-parallelism to make it worthwhile? Or is all the move-pruning stuff too intricate to achieve that? I wrote a chess engine many years ago, it used bitboards but was not very strong - and I'm sure the state of the art has changed quite a bit since then.
I don't think the creation of private subreddits is relevant to /r/haskell or the other programming subreddits you've posted on. We're never going to go private, as that's uninclusive and people will simply accuse us of being in an ivory tower (an unfortunate image problem we already have). I suppose what I'm trying to say is that while this is an important issue for many subreddits it seems off topic for /r/haskell.
I'd like to think that is not the case *by accident* but rather because the community as a whole is inclusive and supportive. Hence the call for support: when I'm inviting someone to come and ask questions on /r/haskell, I am not expecting them to stay in this specific community but also to join other ones which is a big deterrent at the moment given that these communities may not be able to protect themselves from abuse.
It's not about going private, it's about getting reddit to acknowledge the problems some subreddits are facing and provide them with pragmatical answers rather than shutting them off. E.g. It does not make sense for /r/rape (TW, obviously) to go private because they are here to provide support for anyone and everyone.
"Shut up and suffer in silence". Here we are talking about getting admins to take the matter in their hands and do something. Contacting them privately has failed already and the only solution left for the mods of /r/blackladies and other communities under constant attacks is to get this out there.
Can you elaborate? I'm trying to do something very similar to the generalised arrow classes, basically.
But /r/haskell is just fine. So why bring up such discussions to a unrelated discussion room? For me, its like spam to see this posted here. There are more related subreddits to talk about this. Its a true advice not to talk about trolls, when there are none. Especially, when mods can't do anything. 
&gt; But /r/haskell is just fine. Hence why I'm calling for stating our solidarity rather than asking for measures to be taken here. &gt; Especially, when mods can't do anything. The point is to get that to change. And you cannot get things to change by not doing anything.
The point of the arrow and generalised arrow typeclasses is to give standard interfaces to data types that have similar structure. Why don't you just define the data type you want first? Then you can ask what type class instances are useful for it. 
You'll have to show me a source that mods are being shadow banned by admins. Besides, if the mods complain, they are not the right person for the job. Better and/or more mods can step in. Why exactly are you posting this to /r/haskell? Have you experienced people being abused in this subreddit? If not, all of this is premature.
&gt;The point of the arrow and generalised arrow typeclasses is to give standard interfaces to data types that have similar structure. Exactly, which is what I'm trying to do. Several sets of the typeclasses have conflicting logical structures (e.g. a causal block cannot be allowed to run backwards through time when connected to a a block which runs forward through time).
Sometimes i like to troll around. But all the Haskell communities are so awesome, i wouldn't even try it and it wouldn't work at all. Heard about a troll who's visiting the #Haskell IRC, flamed around all the time. He expected to get kicked. No one flamed back. He only got good advices and he even considered to take a look at Haskell. Oh and there are some nice papers about troll research on the web on how to handle troll situations. 
Yes.
Right, but the idea is that it's *fine* to have locally non unique instances, as long as they aren't exported, in the same way that you can have non unique types. module Foo can define a type T = Int, and Bar can define a type T = String, and as long as they aren't exported there is no conflict. Same thing when you have F Bool instead of T.
&gt; First, it's perfectly valid to have F SomeType without having an instance for it in scope; the expression simply doesn't reduce. So the analogy with referringto a type that is not in scope doesn't apply directly. The idea is to disallow this. You need to have a declared (but not necessarily defined) instance. Or is having the ability to use an instance that is neither declared nor defined actually useful sometimes? Doesn't a non defined instance always behave like the empty type, and is therefore useless? &gt; I'll give you an example shortly. Thanks!
Is a lack of funds why Hackage is unreliable? Would donating to Haskell.org fix the problems with Hackage? 
Indeed. If you think about it, in mathematics the term "nonlinear" actually means "not guaranteed to be linear". So linear is a subset of nonlinear.
Type families are not equivalent to simply defining several type synonyms. If you have an open type family `F` which takes a single parameter (of kind `*`), then so long as that type family is in scope, then `F Bool` is in scope. If there is no instance `F Bool`, it simply does not normalise to another type, and is therefore morally equivalent to the empty type. It's not a type error to mention an type family instance that does not exist, just as it's possible to write a function with a typeclass constraint `(Show (Int -&gt; Int))` despite there being no such instance.
If you split a module in a way that changes the API, you will have to bump the major version number no matter what. I don't see how my suggestion makes it more expensive.
Because I'm not creating an implementation but an interface.
&gt; What exactly should the admins do in your opinion? It is not my position to formulate demands in place of the people who are deeply affected by the repeated attacks. However, a couple of things come to mind: the possibility to turn a filter applying a "moderation pending" status to all comments but the ones for users who have positively contributed before or the possibility to share a list of banned users across various subreddits to only name two. &gt; instead of sacrificing free speech No one sacrifices free speech: white supremacists can still tout their own horns on their own subreddits; they just cannot harass other communities on the subreddits they have created to exchange peacefully.
I'm a student (self taught Haskell), but I imagine that a pure novice would need prior experience in a functional language to be productive during a three month span
This is one well-known instance people have been circlejerking over for ages. I do want to believe that this is representative of the community.
It is sensitive topic which you wouldn't discus on public - but internet is public. Why people expect that it would be different on the internet?
What you are saying is that we should offer no protection to people being targeted because it won't change the minds of the racists? Easy to say when you don't really have to suffer the consequences yourself...
I think it is rather issue of trolling. It is easy to provoke someone on sensitive topic.
Note that I used the term "morally" here for a reason. It is of course, not exactly equivalent to Void, because the term `F Bool` could, in a post-hoc fashion, be declared equal to an inhabited type. Until then, it must be treated as an uninhabited type - you can't match on it, for example. It's not exactly equivalent to void because it is of course possible to write a function for Void-elimination (`forall a. Void -&gt; a`) but a similar function for `F Bool` is not necessarily total - it _may or may not_ be inhabited, whereas `Void` is _definitionally uninhabited_. Really, it's more equivalent to an abstract/polymorphic type than a definitionally empty one.
The types _are_ your interface, is what I'm saying. Make the Block type parametric on the concrete implementation, and make _that_ belong to a type class (If you still feel you must) data Block a = Block Linearity Direction a doStuffWithBlock :: BlockClass a =&gt; Block a -&gt; ResultType doStuffWithBlock Block _ Sink a = (...) doStuffWithBlock Block _ Source a = (...) 
&gt; the term F Bool could, in a post-hoc fashion, be declared equal to an inhabited type. With what I proposed, this cannot happen. The F Bool in the signature cannot possibly get an instance defined from anywhere, since the signature does not import any module that exports an instance for F Bool, and it does not declare an instance for F Bool itself. I probably didn't explain it well, but I'm not sure how to explain it better.
I know you mean well but I'm not suffering from the XY problem; I really *do* want to know how to prevent types from implementing both typeclasses from a logically exclusive pair.
IIRC, asciidoc's specifcation more closely resembles a specification than the markdown "specifcation". The markdown "specification" is actually just a bunch of ad-hoc examples and doesn't really cover how they compose/interact or the fine details, which is why there are at least 6 "dialects" of markdown (some with multiple implementations) and few features work across all of them. I feel a rant coming on, so I'll just state that this is excellent news and I look forward to improvements in markdown. For now, I also prefer asciidoc, even with it's smaller deployment surface.
I cannot answer about the specifics of hiring or using Haskell in a team but I can tell you a few things that relate to that. I am using Haskell for about 8 years (or is it 10?), since the middle of my university years. I quickly grasped it. I started with the Gentle introduction, read it it two hours (maybe I read only the first half of it which was simpler, in those two hours). Since then I'm using Haskell almost every day, although not for my day job (which uses Python). I feel like I didn't made much progress in Haskell in all those years... Right now, since about one year and a half, I'm writing [Reesd](https://reesd.com) after my day job. While Haskell is touted to have a steep learning curve, I seem to have stopped in the middle of it. All my code is pretty dumb. It is in no way more clever than the code I write in Python. It is actually simpler. I almost don't use things that are quite normal Haskell these days, such as GADT. I'm not proud of it but that's the way it is. The conclusion I have is that if you can write, say, Python, you can write Haskell. From there, if you improve your level in Haskell, it means you're improving your ability to write code beyond what you could do in Python. Typical job advertising in the Haskell community set the bar quite high, much higher than what is assumed to be a very good level of Python. I feel that using Haskell helps me a lot. I write dumb code then try to spot the similarities. Because Haskell is quite light weight, you can spot them easily. Then I refactor. Once it is refactored a first time, the code seems decent, short and to the point. But not clever. While refactoring I make tons of mistakes. Almost all of them are caught by the compiler. Those mistakes woul be very costly to fix in Python as they would be discovered mostly at runtime (which means you would have to write tests to exercise the code, only to eliminate type errors). I believe that a website with side-by-side comparison of Python and Haskell (which would be written in a non-clever way) would be astonishing: the Haskell code would be simpler. Update: I was taking my shower and kept thinking about the comparison Python/Haskell: Being an average or a good Python programmer or being an average or a good Haskell programmer don't mean the same things; the words are not on the same scale. A common scale would be the ability to write an average or a good program. If you can do it in Python, you can do it in Haskell. Your ability is the same in both languages (but one is better at helping you). Now when you write your program in Haskell, you're lured into learning at being a good Haskell programmer. If you succeed and apply it at writing your program, you have written an excellent program, not just a good one.
If you depend on Hackage in such a way that you feel it being down is an important problem to you, you can solve it pretty easily: run a mirror yourself. There is mostly two options: you can use the exact same Hackage server that is run at haskell.org, or you can simply have, say Nginx serves a static web site. There are also existing public mirrors out there. I have myself started to setup one: [hackage.reesd.com](http://hackage.reesd.com). If you feel like providing a public mirror yourself, you're welcome to download the whole ~3.7GB content from it using rsync, and announce it somewhere. Is there a wiki page listing known mirrors ?
Could you give an example of what you will actually gain from preventing types from implementing a logically exclusive pair?
Ok, an example would be RULES optimisations which would produce incorrect code if something claims to logically be both classes from the pair.
You should probably include all RULES optimisations as laws for your typeclasses.
I'm not sure this is exactly what you want, but I think you can do something like what you're describing with type families. {-# LANGUAGE TypeFamilies, EmptyDataDecls #-} data L data R type family F a class (F a ~ L) =&gt; CL a where ... class (F a ~ R) =&gt; CR a where ... Since for somy type T you can only either have type instance F T = L or type instance F T = R T can only implement one of CL or CR
Hmm, that's a fairly good approximation. Thanks!
This is what I was suspecting :\ Alas
When I come to a niche subreddit like /r/haskell I want to read and maybe post about the topic. If I want to concern myself with matters of social justice I'll go to a subreddit who's topic is social justice. The matters you're talking about aren't unimportant but neither are they what this particular subreddit was made for. I don't believe it's necessary to "strive for diversity" everywhere. All I expect and want from /r/haskell is Haskell. When people are attacked that's a different matter and should be addressed. However it's not /r/haskell's business unless it's a problem within /r/haskell.
Ahh yes, good point.
So one of the things Backpack is working towards is parameterized modules, right? So what if, as a convention, when you need an orphan `instance F X` in module `A`, you parameterize `A` over a module providing `instance F X`. The actual orphan is placed in a separate module `A.Orphan_F_X`. Then, if anyone wants to use `A` you can use `A(A.Orphan_F_X)` or `A(Data.X)`, `A(Data.F)` depending on whether `F X` is still unclaimed by its parents.
When I direct people to reddit's /r/haskell... Oh wait! I don't because I don't want them to be abused if they have the temerity not to stay inside the walled garden and go to another subreddit which happens to be raided periodically by angry hateful asshats. Making sure that the places where we have interesting technical discussions are safe **is** very much a problem that concerns us. Unless you're pretty okay with a bunch of great programmers being left out because of their being different in a way that makes asshats harass them.
Is it not enough to enforce it by contract, as you are apparently already doing?
&gt; And by "write the server code myself" you mean, no need for warp? Yes, that's what I meant. From your description, I don't think you could use Warp anyway. It's an HTTP server, so if you wanted to submit JSON to it you'd have to do so in the form of an HTTP request. If you want to deal with just a pure JSON request, you'd need to use something else. &gt; Im not sure that having the length of the payload upfront will have any advantage for my situation. Okay, then how are you going to know when you've read enough input? You could line-delimit it, but then you'd have to forbid newlines in your JSON submissions. Depending on what character encoding you use (UTF8, etc) you might have to worry about some character encodings (or possibly malicious/invalid encodings) that embed a newline or other special character in multi-byte character sequences. You don't have to worry about that stuff when using netstrings, since you just read digits until the '`:`' character, then you read in a chunk of data without caring about what the bytes represent and finally the '`,`' character. It's human readable, and it provides some weak verification of proper format. &gt; is it faster? To answer that question, you need something to compare it to. But in the vast majority of cases, the overhead associated with processing a couple of characters for your message format (netstring or otherwise) will be dwarfed by stuff like network latency. &gt; I was hoping that there was someother lighter way of dealing with the backend. But I kind of realized that I might have to use postgres and persistent anyways. Well, you don't *have* to use it, and there are other potential solutions. However, it does seem like the most convenient choice here. I know there are some packages on hackage for representing state durably (think one was called acid-state). There are also other backends in the "nosql" vein you could try if you wanted to. I think Postgres is nice since modern versions have good JSON support. You don't even need to define a schema, you can use *it* as "nosql".
Sure but that should be enforced by your types, not by typeclasses.
Oh good. It sounds like having daemons that just poll for scheduled tasks from the db would be fine then. Let the database handle durability, transactions and scheduling logic I say.
Thanks for your input. From the Scala side of things, in my experience smart co-op students picked-up the language quite fast, even the functional stuff. However, there is a i-can-do-this-like-java element to that in Scala that isn't present in Haskell.
Your fifth point is pretty interesting. I haven't considered that approach. It's not useful for all things, but is definitely a good way to teach someone.
That still only works because your functions are globally unique to that type, which is something you don't have with implicits.
Insofar as it concerns not alienating potential Haskell programmers, it is our business here. Addressing the matter when someone is attacked is all well and good, but if it happens often enough, people who quite reasonably don't want to be attacked will just as reasonably stay away, regardless of how an incident is, or would be, addressed. This is a very real concern and has a lot to do with why some communities stay pathologically homogenous despite being ostensibly welcoming. If there are concerns about the moderation of this subreddit, I'm happy to see those raised, but we can't really do anything about what happens in the rest of reddit, which is what the linked article is about. 
This seems like a pretty workable solution! I suppose the user will be able to manually choose the instance through the package manager, if needed. I think this will be required for serialization/deserialization typeclasses. You don't want any change in FromJSON if you already have a bunch of serialized values.
&gt; But that doesn’t change the fact that using typeclasses for things like equality and algebraic structures is fundamentally a mistake. Equality is not a property of a type; it’s a relationship between a type and an equivalence function. The main issue is that once you can have different instances for the same type, you generally have to care about *which* instance you've used. This generally means that you need to package up instances with data - for example, a Set needs to hold onto an Ord instance so it doesn't use a different instance when inserting an item and taking it back out. On the other hand, where would the Ord instance live in a List? Consider the problem of getting the largest (Int, Int) out of a [[(Int, Int)]]. Obviously, it would be confusing if some of those comparisons used different Ord instance - say, if some comparisons used Manhattan Distance, others used Chebyachev, and others were a regular Euclidean distance. If you want different instances of the same type, you open up a large can of headaches for seemingly minimal gain.
I saw the link. My problem is that from what I can see, it's a lost cause. The reddit admins don't and likely never will care and 90% of the problem is fundamental to how reddit works. Frankly, the baseline culture on reddit consists primarily of people who have a large emotional investment in not acknowledging problems like this and the admins are part of that. Did you read about the kind of harassment going on that sparked this? Anyone with a spine and an ounce of humanity would permaban the users responsible for that harassment; the reddit admins clearly have neither. As far as I'm concerned, the most likely result of co-signing that would be making /r/haskell *also* a target for the festering wastes of biomass doing the harassment. We don't need that.
I know CUDA is used for really fast perft computations (using the new dynamic parallelism in newer NVIDIA devices). I think the problem with using it for the actual engine is that, as you say, the move pruning requires communication between threads.
&gt; It takes too long to learn enough Haskell to be really productive. IMHO. &gt; Third point is very true in my experience as well. The time it takes to go from complete novice to being able to understand a modern library like like ``wreq`` is far too long. Hopefully with more writing on the subject we can short circuit this process. 
To answer 3, there is an old study in which students with no knowledge of programming performed better and faster using haskell: http://www.cse.iitk.ac.in/users/karkare/courses/2010/cs653/Papers/hudak_haskell_sw_prototype.pdf
&gt; the type instance is hidden in the signature so the conflict can never be observed I'm pretty sure that breaks down once you consider existentials or higher-rank polymorphism. Bad things happen if Foo and Bar respectively expose functions of types (Typeable a) =&gt; a -&gt; F a -&gt; F a (forall a . (Typeable a) =&gt; a -&gt; F a -&gt; F a) -&gt; Int
Rather than storing dictionaries everywhere, perhaps types (implicitly) indexed by instances would work. Imagine insert :: (o :: Ord a) =&gt; a -&gt; Set {o} a -&gt; Set {o} a union :: (o :: Ord a) =&gt; Set {o} a -&gt; Set {o} a -&gt; Set {o} a
Minor pedantry: it only works because you have a proof that the two functions are extensionally equal, by virtue of being intensionally equal, by virtue of being globally unique. A sufficiently powerful type system can (I believe) get around this (I'm thinking dependent types and a proof of equality), as can a sufficiently powerful static analysis (but I repeat myself), even with implicits.
Anecdotally, when we advertised a Haskell co-op position at the local university for my company we got a higher average calibre of applicant than we normally get. We didn't end up hiring any of them, but that's because circumstances changed for us
Maybe distributed network of hobbyist-run mirrors would help? Or maybe it would be feasible to add optional BitTorrent support to cabal?
Ah, but that is the point of *applicative* functors, which is that if a module is instantiated identically (by the implicit) at different points of the program, the resulting types are still the same.
Here too the same reasoning applies. If you have a signature: module Foo where f :: Typeable a =&gt; a -&gt; F a -&gt; F a and implementation: module Foo where type instance F Bool = Int f :: Typeable a =&gt; a -&gt; F a -&gt; F a f = ... some definition that makes use of F Bool = Int ... Then this will be rejected, because while f has that type inside the scope of `type instance F Bool = Int`, it does not have that type in the signature because in the signature we're talking about a different `F` than inside the implementation where we've redefined F.
That is a wonderfully lucid explication! Thank you so very much!
Yeah, I apologize about this. The TL;DR is that the server hosting hackage has had abnormally high load, which means the server was alive, just really slow. I've still been investigating the causes. We've had several reliability issues with our host virtualization system the past few months, and it's becoming a real issue. One thing I have done in the mean time is stop the builder process, which means hackage *should* actually respond fairly OK now. I'll enable the builder when I think I've got more of a grip of what's going on. OK, so we really need to expediate our move off of this server, because it's really underpowered and we need to separate the build machine from the web server. This will also alleviate the load on *other* servers, since they're hosted in the same space as Hackage. We can actually separate them off for once and keep them separate. Right now we have (in essence) 4-5 major services running on one host system. That means one rogue bug causing high load hurts all the machines. The major hold up is that it requires syncing a relatively large amount of data (80GB, the hackage DB) across some oceans, so that we can bring down the old instance, start the new one, and switch things over successfully (and guess how many times the network will shit out going from Germany to North Virginia?) It will naturally come with some downtime too since DNS entries will need to change. I've been working towards this, but our admin team is a bit swamped right now, so I'm somewhat alone at the moment. Finally, I've been considering using a CDN/proxying service to help make the site at least *helpful* while things are wonky. I've thought about putting [Cloudflare](https://cloudflare.com) in front of Haskell.org, but haven't made any hard decisions yet about that. Given all the issues we're having now, perhaps I should just buckle down and do it this weekend if I have the time.
I can't speak for the Committee, they have all the donation money under lock; I'm just an admin. Donating is always helpful. If you can, it would be great! Right now, it goes to servers, AFAIK. Thanks to some friends at Rackspace, we have a chance of *not* paying for that stuff, and getting increased reliability as a bonus. So soon, we will hopefully begin paying less and less for our hosting needs, and things will work better too! So to answer your question directly: no, I don't think this is a problem that will be immediately solved by throwing money at it. Right now the actual problem is human bandwidth, because there are a limited amount of people to maintain these things right now, and we all have jobs and lives and school and families, in different time zones, and doing different things. We had *no* automation to fix and deploy these things before, so unfortunately we fix cruft by hand a lot. And after working the last thing anyone wants of course is their servers to break, requiring investigation and more work. It is a relatively thankless and on-call kind of thing, to be completely honest (but it *is* fun too. Just not when things break unexpectedly.) But much of this can be alleviated with smart working (automating things and making them more redundant.) It's becoming clear however that the costs are getting higher as a result of us putting this off. So we really need to take the plunge and move critical services to more reliable infrastructure, IMO.
You can't do this straightforwardly in Haskell, but if you're interested in reading about design for a language that does support this feature: "Instance Chains: Type Class Programming Without Overlapping Instances," J. Garrett Morris and Mark P. Jones. In ICFP 2010. http://homepages.inf.ed.ac.uk/jmorri14/pubs/morris-icfp2010-instances.pdf
In other news, standard markdown is apparently going through it's first renaming to [common markdown](http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/).
So here is a question: if we can do without orphan instances at all, can we also do without ragamuffin instances at all? In my experience, the latter only make sense when you have tight enough control over things that you can get away with pulling some very delicate tricks, and are typically used for clever type-level hacks. Now that we have closed type families, equality constraints, etc., are there remaining legitimate usecases for such instances? It might be a breaking, wrenching change to toss them, but I don't know if it would be all to the bad, if properly done over a span of time. Regarding orphans, I used to have only one legit use case, and I think I see a way around it. I have written a library "frob-utils" where "frob" is something like databases or parsers or webservers. It provides functions polymorphic over some structures. Now, if I want to have multiple backends, I create frob-utils-a, frob-utils-b, etc. where a b and c are all other people's packages. Each package just contains the "glue orphans" so that users of frob-utils-b need not depend on package a, and vice-versa. I used to like that pattern. Then, I realized that one could instead write frob-utils such that each function is generalized to take an explicit dictionary of the functions it needed, rather than just require the typeclass constraint. Now, in frob-utils-a, I specialize such functions with what they need from package 'a', etc. This is basically a "by hand" modular module system, and in fact a real module system could subsume, one hopes, this use case entirely. That leaves us with a _genuine_ omitted instance. I claim that while it may be ugly, we can always work around the lack of this. Say Foo needs instance Bar, and is already an instance of Baz and Quux. Now, I just say newtype FooWithBar = FooWithBar {getFoo :: Foo } deriving (Baz, Quux) instance Bar FooWithBar where ... and use FooWithBar everywhere I intended to use Foo previously, wrapping and unwrapping as necessary. Now, my code is backwards compatible with Foo getting equipped with a Bar instance, and avoids orphans. (It also is a bit uglier, but these things often are).
&gt; I seem to have stopped in the middle of it. All my code is pretty dumb. It is in no way more clever than the code I write in Python. Man, I feel you. Maybe because I'm not using Haskell daily, or taking on particularly challenging projects, but I feel exactly the same way.
What's the point of theoretically allowing non-exported type family instances, if it's only allowed when you don't write any code at all that actually depends on the instance?
Wreq is mostly http-client + Aeson + lens so you can avoid boilerplate of reaching into a bag of JSON received via HTTP. A lá what dynlangs do. I don't think it's that big of a deal and there's no reason to use it if it makes you uncomfortable. It's a nice convenience for people that are comfortable using those three libraries together.
IMVU was able to get people totally new to Haskell, but already a programmer, bootstrapped in ~8 work days partly because a codebase already existed. I don't think you need 3 months to get somebody that's already a programmer started with Haskell at all. I would estimate (based on past experience giving in-person tutorials) that at *most* 2-4 workweeks spent pairing on a codebase would be needed and that I could realistically cut them loose after 1 or 2. I've given coworkers 1 or 2 tutorials approx. 1-2 hours in length in the past who were off writing their own Haskell code after a week or two of self-learning on their own time.
Having taught Haskell in person and over the internet a fair bit, I think that's excessively pessimistic. http://www.reddit.com/r/haskell/comments/2fj0v5/anecdotes_and_advice_on_hiring_junior_or/ckabfc2 My time estimates and anecdotes don't involve prior functional experience.
Does this fact change if we don't allow orphan instances?
In addition to the type system, QuickCheck works really well. If the programmer is really junior, someone seasoned can write the tests and define the types. For what it's worth, I got hired for my first job to hack on OCaml.
Haskell helps you keep the boring bits boring. You don't want `interesting' code.
Interesting idea! While not the same, this reminded me of [eternal compatibility in theory](http://www.haskell.org/haskellwiki/The_Monad.Reader/Issue2/EternalCompatibilityInTheory). Which, as far as I know, no one does, as it makes your module names ugly, adds maintainer overhead, and makes it tedious for users to upgrade. Also, it doesn't work for typeclasses unless something like the proposed [instance templates](https://ghc.haskell.org/trac/ghc/wiki/InstanceTemplates) extension gets implemented. I'm not really proposing this idea, as I'm not sure if the infrastructure change is worth it, but here's an another idea: name: a-package version: 1.1 library exposed-modules: Foo, Bar -- This is similar to "hs-source-dirs", but instead of defaulting to ".", -- it defaults to there being no "compatibility modules". compatibility-dirs: older Then, the file "older/1.0/Foo.hs" could exist, and re-export Foo (via package qualified import, but augmented). Users with a "&lt; 1.1" constraint, but only a dependence on Foo could then use 1.1, even if Foo's API had some trivial changes (as described in ECT). The absence of "older/1.0/Bar.hs" would indicate that clients with a "&lt; 1.1" constraint cannot use 1.1. It seems rather inconvenient to need to recopy the entire module hierarchy for these old versions, so maybe instead a "Bar.hs" would exist, but with some pragma. Yuck! I've thought of a reason this is rather intractable: it means that cabal's dependency resolution would depend on your set of imports. However, imports can vary due to CPP, and cabal sets CPP flags after coming up with a build plan. So, you can't really know one without knowing the other. The proposal here avoids this by having an extension to the "build-depends" syntax, but I imagine that users wouldn't use this until specific breakage occurred.
Is this possible with the tools we have today? Does GHC export the necessary information from compiled modules for cabal to deduce all of this?
I inherited a co-op student that I put to work on existing Haskell code for two of her three terms. After an initial week or two of getting familiar, she was turning around projects as fast as I could gather requirements. She was exceptionally smart and a hard worker, so I'm sure that helped. 
The 3 months in this is the length of the total co-op. When you say bootstrapped in ~8 days, do you mean it took about 8 days for someone to become productive in the codebase?
Does anyone have a decent syntax.css setup for coloring Haskell with pygments? This current one is obviously not very good, but attempts to search for one mostly turn up blog posts about migrating to Hakyll.
This is just my second haskell project. Would love to get some feedback.
Hi, I write Haskell at Google. I just graduated from Imperial College (where Haskell is taught as the first programming language) and am doing this as an internship until I know what to do next. So I cannot tell you much about how it is to hire programmers into your team, but rather from the other side. My first internship at Google was three years ago, on a Java project. Despite excellent mentoring and me knowing Java-the-language encompassingly, it took quite long to get started and write some non-trivial code (more than two weeks). By contrast, I got into the Haskell code of my current project much quicker (for some examples consider [this](http://git.ganeti.org/?p=ganeti.git;a=commitdiff;h=6cd1f9ef355aa6966ee5e23adcbc67ff5c0bfdfa;hp=9805e173ae2578cdf10b6fd7883ce07cc045fc5f) and [its tests](http://git.ganeti.org/?p=ganeti.git;a=commitdiff;h=0fd81c8ed7fd6542510fb0f195bb88e599557eea;hp=8cc1ddce80cd0d0f7acbd7b81d309ec7b9099d1a), which I wrote short after joining the team). That is also because this project is open-source code and thus somewhat closer to other code in the wild than the Java project, but what I believe has more of an impact is that Haskell manages well to abstract over the very common things we do, more than it happens in Java, which makes it very easy to get started once you know the concepts. Let's be more precise here: Every project has some context in which you mess around, and the context forces upon you how you have to work. In Java, it is important to know the details of the context in order to get things done, which means learning a lot of very specific things. Java's abstraction mechanisms and object oriented design patterns make things flexible and pluggable, but they don't really generalise things in the way that *you don't have to care about them*. In Haskell, the context is often easily captured in a Monad or Applicative, and you can approach it like "I don't have to care what your Monad is in order to write sensible code in it". In fact, I haven't even bothered to look up what the `ResultT` monad exactly does in which I work most of the time (maybe I should do that on Monday, just out of interest; I suspect that it's somthing with ErrorT semantics and maybe some extras). It is the combination of pure code (that protects you from all the weird contexts that teams and companies can come up with) and things *you don't need to care about* (while still using them correctly, automatically) that makes it very easy to get into Haskell projects, no matter the size. It has worked for me at both Google and Tsuru Capital. So I think that is what allows people to get productive fast without knowing muny specifics. Yet, this works only with people who have climbed up the steep learning curve and are now sliding down the other side of it. You have to know the generalisations I mentioned above in order to be able to not have to care about the specifics. If you don't, industrial Haskell will feel more complex than it is, similar to what it feels when you just start learning Haskell. So if you hire somebody who has no experience at all with the generalisations that are useful to know for dealing with your code, you will likely make them unhappy. If you have enough time to push them up the curve (learning something very general often means using a lot of different specific instances of it, and that takes time), then that's fine. If you don't, I believe it might be helpful that you use your interviews to find those people that understand the generalisations we use in Haskell all the time, at their essence. Then they will be able to ignore your specifics and get productive quickly. (Of course you should also check if your applicant can write real-world programs, but "show me some Haskell project you made", where the applicant can choose their own context, should easily tell you that.) Because you're talking about university/college towns, here's another thing: I believe it helps if you advertise yourself to people as a Haskell employer long before they become employable for you. There are some people who learn Haskell just for the sake of it, but I found it very motivating to know that there were some high-profile individuals and companies with whom I could work after spending some more years on learning Haskell for real. Hope this helps :)
Thanks for the link, haven't seen it before. One thing I don't like about such schemes is that they don't encourage use of up to date versions of libraries. Let's say you depend on a parsing library 1.0, and that library has an arbitrary code execution vulnerability. Then 1.1 with an API change is released, and you happily keep using it through "older/1.0". Then the bug is discovered and fixed in 1.1.1. You don't get the fix, because branch 1.0 is unsupported, and its developer doesn't feel like going through the old versions, checking which ones are vulnerable, and fixing them or deleting from "older".
It occurs to me the logic of this argument can be used almost verbatim when replacing the words Haskell with Javascript/Scala, and replacing Agda/Idris with Haskell.
Yeah, it's some weird bug with github pages not really 100% supporting github flavored markdown, should be fixed now.
Addressing some of Roman's points: - Type inference The claim that dependently typed languages lack type inference is a point that is often bandied around, but I think it's quite misguided. Of course dependently typed languages have the potential to perform Hindley-Milner-style type inference when the types that you use are all Hindley-Milner style types! So dependently typed languages can be *at least* as good at type inference as a language like Haskell; in fact, they can perform type inference for many types that cannot even be stated in Haskell. Implicit arguments (solved by inference) are often used *heavily* in Idris/Agda, and so it sounds strange to me to see type inference in Idris/Agda construed as something negative. Yes, it is true that type inference cannot be performed for some of the more complicated types. This is a fact of life for dependently typed languages, but, at least in my opinion, not a reason to avoid them. In particular, I disagree with the idea that you "pay the price" for "the whole codebase." Also, note that type inference is undecidable in general with GADTs. - Libraries Creating libraries for dependently typed languages is an interesting problem. I worked for a while on making a tree map (like Haskell's Data.Map) that took advantage of dependent types; for example, supplying proofs that a key either exists or does not in tree. There are oodles of things to compute and prove, it's not clear what a nice API would look like. For example, a proof that a key either exists or does not is a much larger data structure than a simple boolean true/false. I am really excited to see what these sorts of libraries will look like.
This was super valuable. Thanks so much for the detailed response! The anecdote about the Java-comparison summarizes well what I believe are the advantages of Haskell, especially w.r.t. juniour programmers. One quick question: do you know what the average on-boarding time is for a student co-op / intern at Google on the Java side of things? You said it took about 2-weeks to get productive. What are their expectations for ramping up summer students? (I know that Google takes their time, in a good way, with employee onboarding). PS - I was actually wondering the other day if Ganeti is still an active project at Google!
Yeah, productivity is very relative, especially w.r.t. junior programmers. However, I think you captured the notion I'm interested in (e.g. don't need to pair with them as much).
Thanks for the detailed and thoughtful responses!
I don't agree with all the points made here. But there is a kernel of truth here that there basically isn't yet an incentive structure in place either academia or industry for someone to go out and build the GHC equivalent of a dependently typed language. And by GHC equivalent, I mean a language that builds a powerful type system but also the gritty engineering problems ( FFI, concurrency, optimizations, efficient runtime, Windows support, package management ) that at the end of the day really decide adoption. Frankly, I'm always surprised that a open source compiler at the level of GHC even exists at all.
Are you aware of the various HTML parsing libraries we already have? Most of them are admittedly not really designed for writing a browser engine, just making sure that you've checked them out and have found something missing in each of them :) (disclaimer: I'm the author of one of them)
&gt; Of course dependently typed languages have the potential to perform Hindley-Milner-style type inference when the types that you use are all Hindley-Milner style types Obviously they have the potential. The question is, is it available today (or at least on the developers' roadmap)? Can I type ``` f x y = x ``` in Agda and get the inferred type?
&gt; checked them out and have found something missing in each of them There are over 50 hits for "html parser" on hackage, for this small subset of html writing the parser by hand is probably faster than figuring out which package to use (or it would have been, if I didn't decide to write two parsers for giggles). Regardless, the point of the exercise isn't really to do everything with external libraries, and I'm trying to stick to libs in the Haskell Platform as much as possible.
Idris?
&gt; As for "erasure", you have no idea how hilarious it is to hear people talk about "Haskell's erasure story" and how its nice properties derive from a phase distinction. I think I heard Agda's erasure problem pointed out during either Stephanie's or Conor's talk, and no-one contradicted it. Then, during their tutorials, both Ulf and Edwin IIRC admitted to have that problem. Do you have any examples that would demonstrate that Agda or Idris are at least as good at this as Haskell?
Same as any subject really - read books and do the exercises in them and lurk in places like #haskell, /r/haskell, planet.haskell.org, etc
Yes, this is essentially the argument made by the gradual typing folks. The difference is that transitioning from dynamic to full static types seems to have lower cost and higher benefits than transitioning from static to full dependent types.
&gt; Efficient maps can be annoying to write in Haskell, so we'll import a HashMap from containers. HashMaps come from unordered-containers.
Whoops, fixed.
The programmer in me says yes; the aerospace engineer in me says no :P
Listen to the programmer; the aerospace engineer doesn't know anything about programming.
Funny timing. I just created a library with a similar goal (but a different take on the API) the other day. Here it is: https://github.com/Soostone/stoplight It basically functions like a semaphore, which I prefer because it leaves it completely to the user to decide what kind of concurrency (or lack thereof) they would like.
Learn You a Haskell is definitely a nice read, allthough it describes itself as: &gt; This tutorial is aimed at people who have experience in imperative programming languages (C, C++, Java, Python …) but haven't programmed in a functional language before (Haskell, ML, OCaml …). But from skimming the first few chapters again, it seems to me that the entry barrier is indeed relatively low. If you are willing to investigate the one or other term, which might not be introduced in the tutorial, by yourself, this should be worth a try. As resources for this investigation I would add stackoverflow to the communities mentioned by /u/bobdudley Have a great time learning and don't hesitate to ask questions. I've also experienced the Haskell community as pretty helpful and friendly. 
That's not true of all dependently typed languages, though, just those that favour explicit universe polymorphism. Once again, this is a design decision, not a technical limitation. Even then, you're hardly "caring about type abstraction/application" when you write a type signature like: f : {a b : Set} → a → b → a And the explicit quantification is not strictly needed, either. Idris allows you to write the type signature exactly as in Haskell. f : a -&gt; b -&gt; a f a b = a 
I take issue with this notion: &gt; you can satisfy the majority of your dependently typed needs in Haskell right now, no hidden charges It's actually got lots of hidden charges. From annoying restrictions in the type system (absence of a proper type-level lambda calculus), to the performance penalty you are forced to pay because all proofs are considered runtime relevant. If you actually have some dependently-typed needs, Haskell is a pretty awful choice of language. Anything more complicated than Stephanie's example at ICFP would warrant switching from Haskell, in my opinion. Haskell's kind-of-dependent-types extensions are great for elegantly expressing things that once upon a time would rely on sophisticated phantom types and crazy type class magic, but actually dependently-typed programming is not really the same thing. For example, right now there is no direct way in Haskell to use the type system to tell you that `xs ++ [] = xs`, even though this is a pretty straightforward exercise in any DT language. The way to do it is to write ++ on the type level, write a value-level version that operates on singletons, using the type-level ++ in its type, and then showing that the type-level ++ satisfies the property with an equality proof. This is awful, chiefly because it restricts you to only proving properties about functions that can be expressed in Haskell's limited type families language.
Just to make a quick example, I made a tree data type indexed over its length, and an insertion function: data Tree : Nat -&gt; Type -&gt; Type where Leaf : Tree Z a Node : Tree ln a -&gt; a -&gt; Tree rn a -&gt; Tree (ln + (S rn)) a insert : Ord a =&gt; a -&gt; Tree n a -&gt; Tree (S n) a insert val Leaf = Node Leaf val Leaf insert val (Node l x r) with (val &lt;= x) insert val (Node l x r) | True = Node (insert val l) x r insert val (Node l x r) | False ?= {tree_right} Node l x (insert val r) tree_right a constrarg val ln rn l x r value = rewrite plusSuccRightSucc ln (S rn) in value (That tree_right is a bit annoying, but needed to fix up the length index). Naively compiling this, you'd find that all the insertions would also need to do some arithmetic to keep the indices up to date. There's no particularly easy way to get Idris to print this in the typechecked form, but one of those RHSs above would elaborate to: Node (S ln) a rn (insert a ln {constrarg} val l) x r That is, making all the implicit arguments explicit and caching the tree size in the node. The compiled version of insert's case tree internally looks a bit like this (as output by an Idris debugging feature than slightly tidied up to remove explicit namespaces and fix indenting): insert(e0, e1, e2, e3, e4) = case e4 of Leaf() =&gt; Node(Leaf(), e3, Leaf()) | Node(in0, in1, in2) =&gt; case (&lt;=(____, e2), e3), in1) of False() =&gt; Node(in0, in1, insert(____, ____, e2, e3, in2)) | True() =&gt; Node(insert(____, ____, e2, e3, in0), in1, in2) So the tree has had all the irrelevant stuff removed, and the arguments insert never looks at have been replaced by ____ (we could remove them entirely now of course). The basic idea is that Idris will aim to remove everything that you don't write explicitly in your data type, and will issue a warning if that turns out not to be possible. Currently this warning is switched off by default, but that will change soon. It might even erase things you didn't realise you didn't need (perhaps this is not such a good idea :)). I glossed over all this in the tutorial - it's certainly a problem, but we're not very far off a solution.
Slightly off topic but a lack of experience may work in your favor. When I learned Haskell, I had to fight a lot of my intuition; you won't have to do that so much.
Of note, the Oxford Uni CS department actually recommend their undergrads don't go out of their way to learn any programming languages before they start, specifically so when they're taught Haskell in the first term they don't need to "un-learn" previous assumptions. They'd rather you studied maths than programming at school.
What I don't like about all those Haskell type system extensions is that while they move towards to a dependently typed language, even the simplest dependently typed program requires us to use a dozen of extensions and weird syntax. Also, interaction between all those extensions are not clear. Do I have an unsound system if I use those two in combination? No ideas. And it's not just two extensions we need just to be able to have some subset of dependent types, it's sometimes 5+ extensions. I've been writing Coq for several months. Although I'm not an expert in any sense, I wrote a lot of "certified"(I think sometimes called "verified") programs which completely rely on dependent types. While in Haskell I'm confused by dozens of extensions, inconsistent syntax, unknown interactions, even the most complex types are so easy to express in Coq, I think that this effort in Haskell is just not worth it. I also don't understand all those discussions about "erasure". As far as I can see from Coq, it's, at least to some extent, a solved problem. See Prop vs Type universes. None of my certified programs in Coq extracted to OCaml or Haskell with runtime redundancy. Just my two cents.
Awesome, thanks for the reply. I really enjoy reading your blog btw, keep up the great work! :)
What is your background?
Why, thank you!
Neat! With closed type families, this approach allows closed type classes: {-# DataKinds #-} {-# TypeFamilies #-} type family ImplementsX a :: Bool where ImplementsX Int = True ImplementsX Char = True ImplementsX a = False class (ImplementsX a ~ True) =&gt; X a where -- ... 
Let’s wait for `{-# LANGUAGE TotalityChecks #-}` first. Then we maybe can revisit the ‘Haskell is enough’ bits.
http://www.codeworld.info/ is a site where you can try Haskell online, look at existing examples, and draw fun pictures, animations and games. It's main goal is to teach math, but I also think it's a good starting point for learning Haskell. 
This looks like fun, thanks! 
None whatsoever. Up to that point, she had only used Java and TCL.
I can hardly tell how long it takes in general, I've done only one Java project myself there so far. It certainly takes longer than 2 weeks to be really productive; after 2 weeks I could start to work on non-trivial things, but I felt really fast at doing so after two months only. That's expected though, and you can start working on your intern projects much before that. Yes, Ganeti is still active - check our Git log, mailing list or IRC channel :).
i also suggest the hutton book, also erik meijer is doing an edX mooc on functional programming and he's using haskell as the primary language he also suggests the hutton book https://www.edx.org/course/delftx/delftx-fp101x-introduction-functional-2126#.VAr-mvmwK9o
I would agree with that
The video at YouTube https://www.youtube.com/watch?v=E30ZvEVExI0
Earlier this year I needed a scalable lazy token bucket implementation and as there didn't seem to be anything like that on Hackage I hacked up something that seems to work reasonably well. However, this posting reminded me to finally document and publish it in the hope that it might be useful to others. https://hackage.haskell.org/package/token-bucket 
&gt; do I have an unsound system if I use those two in combination? No ideas. While I agree with much of what you're saying, it's worth noting that the new GHC typechecker uses a very nice algorithm that is _parameterised_ over the system used. This makes it much clearer that at least the more "principled" extensions (e.g DataKinds, GADTs, TypeFamilies) have sound basis and interact in a sound way, because the algorithm is known to be sound with respect to the typing rules regardless of the exact typing rules used, provided that those rules meet some fairly modest conditions. Although less thoroughly-expounded extensions like GeneralisedNewtypeDeriving have caused problems in the past. Also, Prop vs. Type works but I think there are better ways to do this (see Edwin's thesis).
I looked at the various HTML parsers in Haskell a while ago, and as far as I can tell none implemented the HTML spec. I found one that implemented the tokeniser, but without the parser that's not that useful. 
Hope you can stick with it and parallel the Rust impl all the way. It will be interesting to compare and contrast the final results.
This is because people change the code, since it works like a wiki... That will be inproved... The todo example has been changed by someone..
I am in the same boat as you, somewhat. My background is physics, with nearly zero programming experience so my advice might not apply. (Well I took introductory Java course In college and it was enough to stop me from enjoying programming for a decade) 1. Graham Hutton. Programming in Haskell. Best starting point. A lot of concepts are explained in a very precise way and working through exercises at the back of the book are very good at retaining what you learn. 2. Simon Thompson. Craft of functional programming. This is the book I am working through now. It introduces a quick check and proving correctness very early. Which is nice because it seems like a good habit to develop. 
I'm sure it's a very thankless job you do, and it's probably not said enough but Thank You! It's probably hard to overestimate how much we all depend on the hard work you do.
I started knowing a very little bit of imperative programming (in perl), and started learning from Paul Hudak's "Haskell School of Expression". I think any of the book suggestions here would be great. I think in terms of "getting a head start" I would suggest practicing solving algorithmic puzzles like project euler; just learning how to solve little problems declaratively with recursion really stretched my brain at the time.
julesjacob: Are you proposing that any (transitive) mention of a type family in a signature is illegal? What if I have `z :: F Int` in my signature?
Your API is way more interesting!
Very nice, thanks for sharing. I like the timestamp approach to avoiding bucket leaking overhead; would be very useful when you have lots of buckets and not all of them are used all the time. In my design, I wanted FIFO guarantees for multiple blocked waiters on the bucket, hence the use of semaphores. Your design will be indeterministic due to threadDelay. I'll do some thinking sometime to see if there's a way to marry the two. 
[99 problems](http://www.haskell.org/haskellwiki/H-99:_Ninety-Nine_Haskell_Problems) in Haskell
Why not have one typeclass for both and have a value or type that is which one?
&gt; Agda does not have any *explicit* erasure It doesn't? I had assumed that the point of [irrelevancy annotations](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=ReferenceManual.Irrelevance) was to explicitly mark which parts could be erased. At the end of the documentation there is a note saying that "[irrelevant things are no longer erased internally](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=ReferenceManual.Irrelevance#Nomoreerasing)", but I think this is about printing better error messages, not about generating worse code.
There are two ways to go: you could just find a simple tutorial and start doing very simple stuff in Haskell, or you could come at it from the other direction (theory, to be precise category theory). If I were you, I'd brush up on my math, watch some category theory lectures, maybe buy and start reading a seminal textbook like Lawvere's _Conceptual Mathematics._ With Haskell, it's very rewarding to have a background where you already know things like monads, functors, monoids, et al. But equally if you're not mathematically inclined, you could still have a lot of fun just discovering Haskell's clean and spare style of functional programming. You could treat the category theory-inspired things like monads and such just as objects with properties and behaviours, and get quite far. So there are different ways to approach it.
If you have `z :: F Int` in the signature, then `F Int` must mean the same thing in the signature as in the module implementation that you give that signature to. For example this is legal: module Foo where f :: String -&gt; F Int z :: F Int module Foo where f :: String -&gt; F Int z = f "Hello" This is fine since the `F Int` here is coming from whatever module will later be mixed in with this module, and that module will have to define `f` too, to give us a way to actually produce an `F Int`. This is fine too: module Foo where type instance F Int z :: F Int module Foo where type instance F Int = String z = "World" Since here the F Int is declared in the signature, so it will be exported, so the F Int inside the signature means the same thing as inside the implementation. However, this is not legal: module Foo where z :: F Int module Foo where type instance F Int = String z = "World" This is illegal because in the signature `F Int` means "whatever F Int is defined by a module that will eventually be mixed in to this one", whereas inside the implementation `F Int` is `String`. Since the type String is not compatible with "whatever F Int is define to be by a module that will eventually be mixed in to this one", the signature is not a valid signature for that implementation. On the other hand, this is fine: module Foo where z :: String module Foo where type instance F Int = String z = "World" :: F Int Since here the type instance is local to the module (not exported), it will be invisible to the outside world so it cannot create a conflict with anything. And since the type of z is compatible with String as declared in the signature, this type checks. In particular, if you have this common pattern: module Foo where z :: String module Foo where import Bar import Quux ... tons more imports here ... z = some computation using functions from the imported modules That's fine even though Bar, Quux and the rest may define their own type classes or type instances, because those are all local to the implementation module Foo, and not visible in the signature and hence not visible to modules that will later import Foo via its signature. In summary: everything is allowed, except defining a type instance in the implementation module that is not declared in the signature module, if that type instance is mentioned in the type signatures in the signature module. In other words: if an implementation module defines a type instance, and its signature module mentions types that use that type instance, then that type instance must be declared in the signature. Does that make sense?
The plan is to play catch up to the Rust implementation and then go back and expand the html and css a bit. So far what I've found is that if you just naively follow along and port straight to Haskell, it looks [pretty similar](https://github.com/Hrothen/Hubert/blob/master/src/Html/Parser.hs) to the Rust version, but if you write it in a more typically Haskellish way, it's [quite nice](https://github.com/Hrothen/Hubert/blob/master/src/Html/Parsec.hs).
I don't think very many people would agree that category theory is helpful for learning the basics of the language. It may be helpful for understanding intermediate and high level abstractions, and especially for understanding and deriving relations between them. But for an intro haskell class, I don't think it'll help anything. I could be misguided.
Idris represents a major effort to take these practical matters quite seriously. In the last two years, we've gone from not even having tab completion in the REPL to having editor plugins that do code inference for at least some types. There have also been major improvements to Javascript code generation, [erasure of compile-time-only data](http://www.reddit.com/r/haskell/comments/2flour/dependent_haskell/ckansey), and many other things. The fairly small group of people working on the language and its implementation do not have unlimited time, it is true, but as a mix of academics and hobbiest hackers it seems that your pessimism about incentives is unwarranted. We're certainly not there yet, but things are going in the right direction.
In both Agda and Idris, you can include holes in the type signature. It works better in Agda, but only due to lack of time so far in Idris. You can then use the ordinary code inference features (e.g., Agsy or Idris's proof search) to fill in these holes. It's less different from your Haskell workflow than you seem to think.
Can someone explain why TreeBuild -&gt; TreeView was fast/O(1)? 
Yeah yeah, fine. ^^^narc
Your work on this should be applauded, please don't read my comment as a criticism of Idris itself. If anything it's a comment of my pessimism about our fields's priorities. I've been around FP long enough to see plenty of fledgling languages get to similar levels of development as Idris and then fizzle out for various reasons ( PhD students depart, research groups change, tenure, etc ) and the incentives to do long-term engineering work that doesn't necessarily generate papers just isn't there. It's too early to say anything about Idris's future 10+ years out. I think we're all watching it with great interest though.
I suppose I'm a bit less pessimistic than you, but I don't otherwise have any disagreement with what you've written. After all, a number of industrial-strength languages and implementations have resulted from research and research-industry collaboration, including OCaml, Scala, GHC, Coq, F#, and so on. Not every language needs to go on, and in some sense I'll be disappointed if we don't end up with something much _better_ than Idris in the end, but let's see what happens!
interesting...
I'm in the exact same place. No previous programming experience. I totally recommend the [Haskell Wikibook](http://en.wikibooks.org/wiki/Haskell)! I read Learn You a Haskell and looked at most of the other things people here recommend and nothing else comes close to the Wikibook in actually making sense to a new programmer. Plus, if you ever find something confusing, you can work to make sense of it and fix it for the next guy! I went through and thoroughly revised it while I read, so I can vouch for the current quality!
If I'm not mistaken, that's because you only view the root node, so you build just that and its left and right children are yet-to-be-built (i.e are of type TreeBuild). More details in [the paper](http://homepages.cwi.nl/~ploeg/papers/zseq.pdf), section 4.1.
Most of them, including the one I wrote with a coworker ([taggy](http://hackage.haskell.org/package/taggy)), aim at extracting data, not being the basis for a browser engine, and that's one of the reasons why I can imagine the OP wanted to build his own.
It's not O(1), it costs as much as one of your operations on the first operand, but there isn't really any way around that. The point is that the additional overhead created by the build/view construct is O(1).
If that's the case then its no better than CPS, but according to the speaker it is better, so I don't think that is the case. But I haven't read the paper yet.
What's wrong with Qt Quick?
I like the Haste IDE you did, would you be interested in making bewleksah? There is nothing set in stone yet ([almost no code written](https://github.com/leksah/leksah/blob/master/bew/Main.hs)), but the very rough plan is to use ghcjs-dom to create an alternative UI for Leksah that might eventually replace the current one. bewleksah would talk to the existing leksah process via http to offload the heavy lifting (and to take advantage of all the existing code). If it works well enough, we might be able to drop the Gtk UI code altogether and use WebKitGTK, atom-shell or just connect with a browser.
I've started out with YLAH, I'll check out the wiki next. I'll know where to come if I have any issues. Thanks!
curious: what got you started on this? Doesn't seem common for people to get the idea of starting on Haskell because it comes up less often when people start looking at programming for the first time…
Wow, that is a *very* slick interface indeed!
To be fair, this is one of the use-cases for unsafePerformIO which SPJ mentioned in the original IO paper: "Nevertheless, when the proof obligations are satisfied, unsafePerformIO can be extremely useful. In practice, I have encountered three very common patterns of usage: * Performing once-per-run input/output, as for configFileContents. (...)" http://research.microsoft.com/en-us/um/people/simonpj/papers/marktoberdorf/mark.pdf, p.12, bottom So rest assured that you have SPJ's personal approval. :)
Thanks, I've been meaning to implement almost exactly this for a while now, but it hasn't yet been too pressing as I found that simply rate-limiting at my average wait time worked well enough. Having something for implementing a capped exponential back-off for retries would also be nice... but again, I've found that a fixed retry rate has (thus far) worked ok.
The main issue is that type classes and their instances are not first-class values that we can manipulate functionally within the Haskell language. As a result we have to introduce many new language extensions to do things with type classes that would be trivial to accomplish with ordinary values.
&gt; I seriously hate when the text I see is not what is actually in the file Let's split the problem into two: reading and editing. When I want to *read* something scientific, I prefer proper notation, i.e. things that we see in books. As an example, I can compare this: \frac{\partial^2\Psi}{\partial x^2} + \frac{\partial^2 \Psi}{\partial y^2} +( \frac{8 \pi^2 m E}{h^2}) \Psi(x,y)=0 with this: http://goo.gl/LuwEpj Now, with editing... I, probably as you, don't like *editing* in WYSIWYG-mode, because: a) WYSIWYG-editors often alter the context/structure of what is typed, not just the representation; and b) those editors change input methods, despite that it is easier to type \frac{}{} than fighting with the editor's UI. Concealing only changes the representation (not context/structure), and does not alter the input method. Therefore it does not have a), nor b). It is still not ideal (alignment/indentation), but it is better than nothing. (To make it ideal somebody needs to combine concealing with structured mode, I guess.) &gt; Why continue using a fixed width font here anyway? Oh, I am all for it, if it makes things better. But the only gain would be proper kerning, with the downside is that nearly everything else will be broken, since so many things assume monospaced fonts.
Up to syntax. You can write f : ? f x y = x filling in the function body first and then get as much help filling in the type as with any other hole.
i say we just scrap 'em
I think for efficiency it's faster to learn haskell and agda then category theory (you have access to constructions of category theory in agda).
But if too exported an instance and a function polymorphic in F, like for example a method of a class with an associated type, and Bar used that method, wouldn't Bar need to also expose those instances? I think I'm starting to understand what you are proposing, but the notion of "use" is still a bit fuzzy, and I can't think of useful examples where it improves on just requiring you to reexport everything. If you can only write concrete applications like F Int in an interface without having to expose every instance you use, why not go farther and reduce the application to whatever F Int was defined to be? Then you get the simple rule that you have to reexport instances if you mention the family in your signature at all.
A good example of type classes as first class values is agda's instance arguments (almost: instance arguments don't have a uniqueness check (yet!)), where type classes are modeled as records with some special syntax to turn record projections into ones that use instance search and to tell the compiler that it should consider a function as an instance during search.
It should be possible. Haskell's tooling is rather poor in places like this, though.
The implementation of typeclasses is not perfect, but don't forget that it's still one of Haskell's greatest inventions, one which made monadic IO possible a few years later. E: I guess you could have monadic IO without monads. Or monads without typeclasses. But to get the delicious Haskell drink, you have to mix all the ingredients :)
I agree. My personal stance is that type classes for mathematical abstractions are okay, but I wanted to point out the most common objection to them and why they should be used with restraint.
I don't like how they are always the sticking point preventing a superior module system. Or when such a system is finally figured it will perforce be burdened by all sorts of corner case rules, inscrutable errors and syntax piccadillos to accomodate them. I also get annoyed by the stratification of appropriateness. I can use them, but you should not. e.g. It is ok to use them for abstract or type theorectic domains but somehow not ok for any other domain. Well major problems domains are relative and that domains core abstractions are as valid captured as typeclasses as any other domains. If typeclasses really are only appropriate in one tiny subspace in the grand universe of software problem domains then they are inappropriate for the language in general. 
LYAH isn't great for non-programmers IMO. It's a shame there aren't more resources for absolute beginners to start with Haskell. Programming 101 courses at some universities use Haskell and it seems to work really well. Maybe OP should look up the lecture notes to one of those courses. These textbooks are aimed at beginners and used in intro courses, but they're not free * http://www.cs.kent.ac.uk/people/staff/sjt/craft2e/ * http://www.cs.nott.ac.uk/~gmh/book.html These courses are aimed at beginners and have lecture notes and assignments available * http://www.seas.upenn.edu/~cis194/ * http://cs.anu.edu.au/student/comp1100/ (This one even has lecture videos) Personally, I'd also consider going with a Lisp version of SICP if I wanted to start out with a functional language.
yeah, but I my application is actually a web scrapper that I want to limit the rps, but it's ok if a request takes a long time... So I want to *start* new processes at the rate, not really caring when they finish.
Australian National, in Canberra (Australia, obviously). What kind of project? 
That last link is literally the course I'm preparing for. I can't think of a much better starting point. Nice find. Thanks! 
Haha, no way. It's the course I did in first year as well. It's a really good one, too. Hit me up if you ever want any ANU-specific advice from a grad.
You don't need a uniqueness check in Agda, because instances may not be unique, and that's fine. Something like Data.Map in Agda would be a module that is parameterised by the ordering used.
They're still not perfect. IO, for example, is not a monad. (IO, join, return) is a monad. This is more of a problem where something like Monoid where the type is not sufficient to decide what monoid you mean.
I totally will. Thanks heaps!
Nah, Idris will demand a type signature. Of course, In Agda at least (probably also in Idris) you can do: f : ? f = λ x y → x 
&gt; E.g. when programming with effects, I first write the term, then let the effects (in the form of class constraints) be inferred by the compiler. If you want to leave something for the compiler to figure out, put a hole in there and automatically fill it in later once you've written your code.
&gt; would be very useful when you have lots of buckets and not all of them are used all the time. yeah, that happens quite easy with network (e.g. HTTP) servers, where you want a per-tcp-session rate-limiting for instance, where you quickly get to handle well over 1000 idling buckets. Moreover, I needed a low memory footprint, so each `TokenBucket` currently needs one (unpackable) `IORef` pointing to a 3-Word heap object (on 64bit archs). I also have another version in the works using the new CAS update primops using a packed word array, for when you want to operate on an fixed-size array of `TokenBucket`s
I find the first-class modules claim against type-classes weird. If there's a conflict between first class modules and type-classes, who in their right mind would prefer modules over type-classes?
Thanks!
Do you know any implementation of FizzBuzz using filter?
That's why it was a mistake. It's too late to remove them now.
Here's mine: fizzbuzz n = take n $ zipWith (\i fb -&gt; head $ filter (not . null) [fb, show i]) [1..] fizzBuzzStream where fizzes = cycle ["","","Fiz"] buzzes = cycle ["","","","","Buzz"] fizzBuzzStream = zipWith (++) fizzes buzzes 
I have been pretty sure the coming GHC AST annotation had to support a Functor, now I am convinced.
&gt; But if too exported an instance and a function polymorphic in F, like for example a method of a class with an associated type, and Bar used that method, wouldn't Bar need to also expose those instances? Do you mean something like this? module Bar where f :: F a -&gt; something module Bar where f :: F a -&gt; something f = ... implementation ... module Foo where import Bar type instance F Int = String ... use f ... This is fine, since in both the signature and the implementation of Bar, no instance is added to F. So the signature matches the implementation, because F has the same meaning in the signature and in the implementation (namely "F means whatever it means in the module(s) that will eventually import this one"). Note that this is fine even if the use in Bar is not polymorphic. For example: module Bar where f :: String -&gt; F Int z :: F Int module Bar where f :: String -&gt; F Int -- this is kept abstract to be filled by mixin composition later (I'm not entirely sure if backpack even allows this?) z = f "Hello" module Foo where include Bar type instance F Int = Bool f :: String -&gt; F Int f "Hello" = True f _ = False The only thing that's illegal is defining a signature Bar that simultaneously mentions an instance F T in one of its type signatures (or mentions F polymorphically), and does not declare or define F T, while at the same time having a implementation Bar that does define an instance F T. That's a problem since now we can have two conflicting instances (one inside and one outside the module), and that conflict is observable since values of type F T can cross the module boundary (since the type F T is mentioned in the signature). If any of those 3 conditions is not satisfied, then there is no possibility of observable conflict. In particular the case where F T is not mentioned in the signature, but F T is defined in the implementation, or in one of the modules that the implementation imports, then the use of F T is purely internal to the implementation. That is a case where this doesn't require you to reexport everything. Even if inside the module F T = String and outside F T = Int, since F T is not mentioned in the signature, values of type F T cannot cross the module boundary so it doesn't matter.
I only quickly skimmed the comments so far, so excuse me if I'm repeating something here. There are a lot of good ideas on how to get started with Haskell here, so I'll just skip that. However, *what* I'd like to recommend to you is to learn about programming languages in general. There's a good book called [Programming Language Pragmatics (not an amazon link)](https://www.cs.rochester.edu/~scott/pragmatics/). [Here's a pdf](http://infoman.teikav.edu.gr/~stpapad/ScottCompilers.pdf). I'm not sure if this is legal, but it's the third result for a google search. Depending on you're background I'd suggest that you skip straight ahead to chapter 3, where you'll get a good idea how programming languages constructs translate to a machine model. This is not very Haskell specific, but it will help you to understand any programming language from an abstract and high-level viewpoint. Apart from that, enjoy your journey. Getting to my Masters in CS where the six (education-wise) best years of my life. :)
That's an interesting take on it. And I think you're right. A general overview will help when I eventually have to move between languages. Thank you. 
For reference, here is the [asciidoc home page](http://asciidoc.org/). It is focused on a specific software application called "asciidoc". There actually doesn't seem to be an official "spec" for it; there is only documentation for the software. But the software documentation does seem to define the language quite clearly along the way. The home page seems a little confused about this point. The introduction starts out with "AsciiDoc is a text document format for writing notes, documentation,... [etc.]" and ends up with "AsciiDoc is free software..." The external resources links don't mention any alternative implementations at all. Note that pandoc can render any of its input formats as asciidoc. It currently cannot read asciidoc as input, but it looks like adding an asciidoc reader to pandoc would be fairly simple. 
It's not quite what you asked for, but I think you may find [this SO post](http://stackoverflow.com/questions/3431225/tools-for-generating-haskell-function-dependency-control-flow-graph) somewhat relevant. Edit: to be clear, this is for control flow/functional/module dependence. 
[Robert Harper.](http://existentialtype.wordpress.com/2011/04/16/modules-matter-most/)
People who experienced some ML languages and realize that Haskell's module language is really terrible. :) Also, you don't need *first class* modules to have issues with type-classes. The simple fact that type classes are global and not delimited by normal scope rules already causes issues.
Agda doesn't allow this at least. It wants at least the function type `? -&gt; ? -&gt; ?`
I believe that /u/aseipp and "thoughtpolice" on #haskell are one and the same person.
"if that type instance is mentioned in the type signatures". This is the crux of the issue. I have (belatedly) posted an analysis as a postscript to the blog post, but here is the prime example module A where type instance F Int = Int f :: Typeable a =&gt; a -&gt; F a f x = case eqT of Just Refl -&gt; x :: Int Nothing -&gt; undefined module ASig where f :: Typeable a =&gt; a -&gt; F a module B where import ASig type instance F Int = Bool -&gt; Bool g :: Bool g = f 0 True -- oops http://blog.ezyang.com/2014/09/open-type-families-are-not-modular/
Yeah, no one links to it much because it's old, broken, and no one uses it :) I think these "compatibility modules" would be used to describe API changes. In other words, performance and behavioral correctness improvements in 1.1.1 would still affect the exported 1.0 API. It is true, though, that if the API is improved to encourage correctness in user code, then these benefits might not be passed down. Here's an old, 3/4 baked blog post I wrote about this stuff: http://www.mgsloan.com/wordpress/?p=219 . The brittleness of typeclass hierarchies it describes would be fixed by an extension like this one: https://ghc.haskell.org/trac/ghc/wiki/InstanceTemplates . I think this is the right way to fix that particular problem, and as a result, Haskell could express all trivial API changes!
Yup, it's a good idea! HSE also has a functor for the AST annotation.
I understand that ML style modules are much nicer than Haskell's. OTOH, Haskell's type-classes is much nicer than ML's lack of proper solution for overloading. It's hard for me to imagine going back to lack of overloading, whereas not having a good module system does not seem as horrible.
He claims type-classes are a "minor special case" of modules. But type-classes do instance resolution/choice. Modules have nothing of this sort. If you include the "any non-trivial Haskell program is full of unsafePerformIO", Robert Harper seems to say a lot of silly things about Haskell.
To add some anecdotal evidence of my own, it has taken most people I've known who've learned Haskell about a month before they were able to be productive at a beginner level with it. I also know it can take quite a long time to get up to speed on a new code base. How well these two can parallelize I don't know. I do know that a summer coop is generally 10-12 weeks in length, and now there are only 6-8 weeks to play with. The last week will probably be spent orchestrating a handoff of whatever code they owned, and at most places making a presentation about what they did with their time. That leaves you with 5-7 weeks of dev time. Again this assumes the process of learning the code base is perfectly overlapped with learning Haskell. That's why my estimation was a little pessimistic. Obviously I wouldn't want to discourage hiring Haskell coops, for obvious reasons. 
Right, that's certainly not legal because the signature mentions `F a`, which is a different thing in A than in ASig, so the types don't match up. It's still not clear to me why the orphan restriction is the right thing to do here. To me it seems an ad-hoc solution that may in practice approximate what you really want, but is not clearly The Right Thing. It couples where a type is defined with type instances. Where a type is defined has in principle nothing to do with what type instances you should be allowed to define. You say "While it is true that these two type instances are overlapping and rightly rejected, they are not equally at fault: in particular, the instance in module B is an orphan." I would say that they *are* equally at fault. Sure, with Haskell's old module system orphan instances are a thing, because with the old module system there is no way to not export a type instance from a module. But that's the root cause of the issue, and when designing a new module system you have the opportunity to fix that flaw! On the other hand, the rule that "if an implementation module defines a type instance, and its signature module mentions types that may use that type instance, then that type instance must be declared in the signature" seems to me exactly what you want here. Sure, you may need to approximate "types that may use that type instance", but that doesn't seem very hard. For example: if you define the type instance F T in the implementation module, then it needs to be declared in the signature if any of the F T' in the signature unify with F T. But then again, I feel like I may be completely missing the point? :)
Very cool! I also like the idea of having relative source spans, as mentioned in the ghc-devs thread. This would be particularly useful if these could be wrapped in **Maybe**, where **Nothing** indicates "I don't have user information for this, choose a good location". Then, rules such as those in Chris's [hindent](https://github.com/chrisdone/hindent) could kick in and choose a position. This would allow you to move sub-expressions while preserving layout. I'm sure you're already thinking about such things for the purposes of refactoring, but figured I'd mention it anyway!
That's exactly what ghc-exactprint will do.
Fantastic!!!
Have you seen my retry package for fully customizable exponential backoffs? http://hackage.haskell.org/package/retry https://github.com/Soostone/retry
Well, there is always Scala with both first-class type-classes and a pretty ML-like module system. :-)
Who in their right mind can live without a module system? I hope that whatever conflict there is between type-classes and a module system in Haskell can be resolved with some manageable compromises. I doesnt need to be the perfect module system but there must be one. Even Java has a better story here. Large scale development needs module/package interfaces and pluggable implementations.
I have to admit that this is quite elegant.
Hopefully, a proper module system will help here and take over one bad type-class usage: creating a typeclass as a poor-man signatures/interface so that different implementations can be plugged in.
Or wxWidgets?
I didn't want to reference our conversation in the blog post because I didn't know how under-wraps you wanted to keep not-yet-done stuff. Sometimes it's nice to avoid having lots of people bugging you about deadlines until things are ready :-)
It's also worth pointing out that in a dependently typed language you can actually encode and enforce proofs of such laws within the language.
As I understand it, OCaml will soon have a system for overloading based on implicit modules. OCamlLabs are implementing it, and there's a short paper describing the proposal [here](http://www.lpw25.net/ml2014.pdf).
In fact, there are no nontrivial (i.e., not initial) "negatable" objects (in the sense of having an inverse with respect to the coproduct) in any category, as pointed out for example by Schanuel, "Negative sets have Euler characteristic and dimension".
How is Java's story better here?
Nice! I was working on a Mandrill package a couple months ago but never got around to finishing it. It's great to see someone else got around to it!
One example: all our services/executables are developed against the SLF4J Logging API/interface provided by another package developed by someone else. We dont care what logging impl will be used we simply choose one when packaging the WAR (we dont need to recompile when switching an impl) or leave it open to our OEM partners to choose. One partner deploys this WAR to the WebLogic web container (this is similar to choosing among Snap, Yesod, etc) which uses the JVM builtin logging framework. Another partner may deploy it to Tomcat which uses a different logging framework. We dont have control over these decisions and we dont need to ship one version of the product for each logging framework out there. Take a look at what the Mirage project is doing with the module system provided by OCaml: is the same principle. More specifically, watch Anil's ICFP2014 presentation on this (no link - on my phone). There is no separation between interface and implementation in Haskell. Using type classes for this is a cumbersome hack. EDIT: Here is the link to Anil's keynote: https://www.youtube.com/watch?v=UEIHfXLMtwA
Indeed you can. An example of this, http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.155
There are some [attempts to combine the syntactic flexibility of type classes with a better module system](https://www.mpi-sws.org/~dreyer/papers/mtc/main-long.pdf), which I think is worthwhile for Haskell in the long run. The current problem is that the current incarnation of type classes in Haskell are inherently nonmodular because of the [global uniqueness of instances](http://blog.ezyang.com/2014/07/type-classes-confluence-coherence-global-uniqueness/). So Haskell will end up getting a more awkward module system unless type classes get revamped somehow.
1. Typeclass resolution fundamentally relies upon being able tell that two types are *not* equal, in order to resolve overloading. 2. To control resolution, programmers therefore require a mechanism to distinguish between different uses of structurally identical types. 3. The mechanism Haskell supplies for this purpose is generative type definition --- namely, datatype declarations and newtype. 4. These work via a *side-effect* of creating a fresh type value, which is known to be different from all other types. (You can see the side effect in the very word `newtype`!) 5. As a result, the full substitution rule would become unsound if type declarations were expressions that could occur in arbitrary subterms. 6. So type, class and instance definitions are restricted to the top-level of the program. 7. Distinguishing top-level from expression context basically screws you if you want a clean equational theory or a decent module system. Note that in this story, no one has been stupid -- all the deviations from the lambda calculus in the design of Haskell were done for deeply considered reasons (except for recursive let by default, that's just sad) -- but deviating from the lambda calculus is fundamentally a course beset with peril when you're designing a language. 
Great to see this work! Thank you Charles I look forward to using it.
You don't need laws to reason about code. In larger code bases with lots of pragmatic not-so-beautiful code it often perfectly fine to loosely reason about your code by providing some informal intuition of the meaning of the type class. Maybe in the form of some documentation. Laws sound cool for toy examples and maybe core libraries, but I don't think they scale well to large projects where technical perfection is not the main priority. Still being able to reason informally about your code base can be invaluable.
Looks great! If Haskell had OCaml's module system and implicits rather than typeclasses, I'd be a completely happy chap. But my issues with OCaml (most importantly untracked effects) make it not an option for me.
And all the other stuff I don't want to have to care about, like null and effects :)
/u/bitemyapp's list has been mentioned and includes this, but it's worth pointint out [NICTA's Functional Programming course](https://github.com/NICTA/course) by Tony Morris and Mark Hibbard. I haven't used it myself but it's pretty highly recommended by those who have used it. Tony also has some filmed lectures on some important topics that are quite clear.
Sort of, but probably not in the way you're hoping. You can clearly represent a formal subtraction type A - B as a pair of types [A,B], but the values of [A,B] are NOT pairs (a,b). For example, Either a b - b ~= a, but pairs (Either a b,b) are clearly not equivalent to values a. In order to construct an [A,B], you have to normalize it by voiding any B options in A. If A doesn't have any B constructors, you've got a "strictly virtual" type, which isn't much use on it's own (though you can still compose it with other types and maybe get something useful back out). Also sometimes interesting things happen if you replace product with some other monoidal tensor ex. coproduct This comes from Joyal's work on combinatorial species. A good haskell-centric paper being http://www.cis.upenn.edu/~byorgey/pub/species-pearl.pdf The "construct formal inverses as pairs" as with church numerals is a special case of the very general notion of grothendiek group completion, or even more generally, the Int construction / Geometry of Interaction construction. I expect you're familiar with much of this, since your proxies are exactly the GoI construction on pipes :P (example for public consumption http://semantic-domain.blogspot.com/2012/11/in-this-post-ill-show-how-to-turn.html)
I'm definitely with you on this one. I tried LYAH as a first programming text and it was a waste of my time. 
ghc has had implicits for a while. They don't get much love, though.
Pick your poison. :-)
The important parts of modularity, implementation hiding and encapsulation are proven to work relatively well with even the most basic module system, as in Haskell. Even c sports highly modular code with some discipline. You don't need first class modules parameterized by modules to have modularity. You do need type classes to support overloading and nice things like seamless QuickCheck.
That has nothing to do with either module system. In Haskell you can code to an interface rather than an implementation too. Just use a logging type class, let the user implement the specific monad. Or pass around an explicit interface dictionary.
I'm a little late to the party but from memory didn't you change your stance somewhat on typeclasses? Since then has anyone come up with an idea for how first class typeclasses could be introduced to a language (haskell or otherwise) without the drawbacks of possibly passing incorrect dictionaries around (as I recall that was the main disadvantage of scrapping typeclasses)?
Looks great! Don't forget to repost this in /r/haskellgamedev :)
Congrats on the release! Sorry I couldn't help out on the code :(
Haha it's fine Ollie! I didn't want to wait too much, so I decided to release out the library despite it is not as complete as I wanted, to encourage other people to contribute :)
With the strong type systems that are often associated with FP, maintenance is also cheaper, because the type system helps you understand existing boundaries and avoid breaking things.
I would love to see a changelog from netwire 4 to netwire 5.
This is great, keep it up! I tried to play around with netwire after reading /u/ocharles's [Getting Started with Netwire and SDL](https://ocharles.org.uk/blog/posts/2013-08-01-getting-started-with-netwire-and-sdl.html) tutorial but it's netwire 4 based.
Can you elaborate on "recursive let by default?"
Both of these solutions are inadequate compared to ML's module system, although no worse than Java's solution. Java's solution has one improvement over Haskell here: To go from a concrete implementation to an abstract one, the code that uses the concrete implementation does not necessarily have to be changed to compile against the abstract one. Using typeclasses or record dictionaries, the type of user code is always changed.
&gt; You do need type classes to support overloading and nice things like seamless QuickCheck. Not true, instance arguments work just fine for such a purpose. Implicit modules (proposed and being implemented for OCaml) also do just fine for this.
A module really has nothing to do with a specific type, so they shouldn't be type directed. Combining a module with a type directed overloading means you can't have multiple instances for any given type which makes things like Monoid rely on crappy newtype hacks. A nice compromise is to have module parameters be declared "instance arguments" which allows them to be resolved in a type-directed way, but only Agda has an implementation of this AFAIK, and OCaml is working on it.
Functors (in the ML sense) solve this problem. A functor can be (statically) _applied_ to a module matching a signature. For example, your `Data.Map` module would actually be a _functor_ that takes an `Ord` module for the key type as a static parameter, and give back a useful module as a result. So you would actually say something like structure IntMap = Data.Map(IntOrd) And then use your `IntMap` functions as normal. If you defined another IntMap with a different ordering: structure IntMap' = Data.Map(IntOrd') If you tried to union a IntMap.Map with a IntMap'.Map, it would be a type error. To solve the overloading issue, OCaml is now working on implicit modules which should make overloading possible as well by automatically figuring out which module to use as an argument based on context.
Can you explain how seamless QuickCheck would work with "instance arguments"? I agree that implicit modules would work, but they're more like type-classes and less like SML modules.
If Java imported the names from a module and used them directly from there -- how would you avoid changing the Java code when you want to switch the implementation to be abstract?
You'd switch the class you're writing out for an interface. The calling code need not be wiser.
Sure, in Agda you'd take an instance argument ⦃ Gen T ⦄ where T is the type you're looking for and it will find one in scope to use. Anyway, I find with QuickCheck I end up using forAll for most of my properties anyway (because there is no default generator or the default one is not adequate), so I don't need the typeclasses for most of my variables. Also, using Arbitrary often leads to orphans, as most packages don't provide Arbitrary instances.
I can't see how this is optimal. I'm trying to put matrix chain multiplication in this framework but it seems to not work.
And Scala.
Scala's modules are run-time values though (although I suppose this is true of Agda as well if you use the Agda record system, but the module system is compile-time only now that I re-examine it.).
Again, all this functionality has been available in Scala for some time now, and it works great. Note that in Scala implicit arguments are not only used for emulating type classes, but also for passing implicit contexts etc.
In Scala, this is done at runtime. ML's are statically resolved. Runtime instances have a number of drawbacks that ML modules do not. Most specifically, you can rely on canonicity of instances in the static case, but not in the dynamic case.
I suppose I begin to understand, due exactly to global uniqueness, each type as their data and relevant structure/laws which are available. This is a nice default and `newtype` is a clunky way to override it or become more refined in the choice of structure/law taken allowing data reuse.
And I would have considerably more peace of mind having a new hire set off on a large strongly typed code base, compared to a large runtime typed code base. Another one would be the ease of refactoring that many experience when using a strongly typed FP language.
It makes sense on no planet to say that Int's only natural monoid is that of addition or multiplication. You can't favour one over the other.
The author should have tried our (NICTA's) [AutoCorres](http://ssrg.nicta.com.au/projects/TS/autocorres/) tool. Proving directly on top of the SIMPL spec would be a trifle difficult even for simple stuff, but AutoCorres gets rid of a lot of the noise and allows you to prove a much simpler state-monadic HOL program correct, and it handles the refinement down to SIMPL.
Good chance to get some advertising...
I guess I find it confusing to add an annotation indicating something which the compiler can obviously detect. 
Not sure what you mean by runtime. In Scala the implicit arguments are resolved at compile time using scoping rules. Do you mean that the construction of implicit instances are done at runtime? That is true for instances created by implicit functions (def), but you can also have implicit values (val) which are stable identifiers in Scala (i.e. their value doesn't change during runtime).
And thus neither one has an instance. I see `Int` as being only the data and having the structure only when you're dealing with `Sum` or `Product`.
&gt; …the new GHC typechecker uses a very nice algorithm that is parameterised over the system used. This makes it much clearer that at least the more "principled" extensions […] interact in a sound way… I’m intrigued — is there somewhere I can read up on this in more detail?
Wow - what a horror story. The original link of this thread no longer works, as announced by Atwood in your link. And it has been renamed again - for the last time, hopefully - to [CommonMark](http://commonmark.org).
&gt; Right, that's certainly not legal because the signature mentions F a, which is a different thing in A than in ASig, so the types don't match up. OK, in that case, you seem to be advocating to disallow all transitive mentions of F, unless you could have directly converted it to the reduced type. &gt; Sure, with Haskell's old module system orphan instances are a thing, because with the old module system there is no way to not export a type instance from a module. But that's the root cause of the issue I suppose this is a case where type classes make the issue more clear than type families, namely *what users expect*. (I touch on this in http://blog.ezyang.com/2014/07/type-classes-confluence-coherence-global-uniqueness/ ). Users expect type classes to behave the same across module boundaries, even when the type class is not mentioned at all (e.g. to enforce abstraction on the set data type). The inability to not export a type instance is a fundamental problem, but it's not one we can change.
We had some really really great talks this year (to the detriment of other workshops that ran at the same time, except FARM, I heard FARM was magic ) Every talk was good / interesting! And the associated Q&amp;A parts were pretty fun too! 
one of the earlier slides does mention shorter -&gt; reduced costs &amp; less errors -&gt; reduced costs ... :P
In the past, I've spent time looking for my bugs in which I introduced recursive calls by mistake. It would have been nice if I could have asked the compiler/editor to highlight my recursive calls instead of my having to debug my code.
This is nothing to do with implicits, and everything to do with modules/ML functors. A parameterised module in Scala is a runtime object parameterised by another runtime object. See [this post](http://stackoverflow.com/questions/23006951/encoding-standard-ml-modules-in-oo) for more differences between the two. In particular, note that Scala settles for path-dependent types whereas ML gives a much richer dependent theory. Edit: I'm pretty sure if you have a module Map, parameterised (implicit or not) by a module Ord, which defines a Map type in terms of the element type of the Ord, Scala will either not compile it the way MLers expect, or it will identify two Map types instantiated with different Ord instances. I'm pretty sure Scala does this, because if it is doing proper generative or applicative modules in the ML sense this sounds like it'd be impossible to typecheck in Scala's general version, and it would also be difficult to encode in the restricted type system of the JVM.
I see, thanks. I think I should get around to playing with Ocaml to experience the module/functor based solution kamatsu described to see which I prefer. I think I'm starting to understand some of the complaints against type classes as they exist in haskell.
Sum and Product aren't types though. Sum i is not a "good" usage of newtype, like a unit of measure or a special type of quantity would be. It would be perfectly natural, for example, to `mappend` using the Sum instance to a `mappend` using the Product instance. They're just newtypes designed to get around the annoying specificity of the typeclass mechanism. An even worse example is something like the `Kleisli` instance of `Arrow`, as it _does_ have a default (and often wrong) instance without the newtype.
Still, in this example there's nothing stopping you from passing in two different ord dictionaries and then trying to union them, which would be unsafe.
Maybe if you want to shadow a value? f x = let x = firstStep x in let x = secondStep x in x But in general i see it as a non-issue, although some people like `let` to desugar into straight lambda calculus without some recursion operator.
[All workshops](https://www.youtube.com/playlist?list=PL4UWOFngo5DXuUiMCNumrFhaDfMx54QgV)
&gt; So dependently typed languages can be at least as good at type inference as a language like Haskell [...] Alas, this isn't true. Modern type inference algorithms lean heavily on the assumption that type constructors are injective. That is, from knowing `F a` equals `F b`, you want to conclude that `a` equals `b`. In ML and Haskell 98, this is true. This assumption is *incredibly* powerful, because it lets you infer the value of type variables from facts about composite types. But as soon as you have nontrivial type-level computation, then this assumption becomes false. (Indeed, when type families were first added to Haskell, GHC's typechecker was unsound!) The reason is that you can write a type function `F x = Int`, and now knowing that `F a = F b` tells you nothing about the values of `a` and `b`. Because GHC wants to support good inference, it demands that any use of a type family is fully applied, so that it can eagerly unfold definitions and reach injective type constructors. But this approach is hostile to type-level lambdas, which are essential to a dependently-typed programming style. Type inference for rich type systems is a very open problem. This is good for my job security, but not so good for hackers who just want dependently-typed programs to work the way they should. :/
It's not an intrinsic problem but rather a lack of libraries. [Coq's dev have started fixing the problem](http://coq.inria.fr/distrib/current/stdlib/Coq.Numbers.BinNums.html).
&gt; Modules have nothing of this sort. [Harper, along with some well known Haskell academics, proposed a solution to this problem eight years ago](https://www.mpi-sws.org/~dreyer/papers/mtc/main-long.pdf) 
Without knowing anything more about them, what would make them probably unsafe? In any event, statically enforcing invariants is a good thing. There are known methods that statically enforce the map ordering canonicity invariants without requiring global instances (namely, proper modules). So, why propose an unsafe solution when safe solutions exist?
I think everyone else here has done an adequate job of presenting the principle objections to typeclasses. Since the majority here seems to be on the "typeclasses bad, ML modules good" train, as primed by the original question, I think it is important to point out that typeclasses give us a number of things that we have access to in no other way in Haskell. I *do* have issues with typeclasses that haven't been addressed here. e.g. I find that the design of Haskell's typeclass machinery currently requires every user to pay for every direction in the lattice worth of typeclasses above them, which makes it very hard to get Haskell users to agree to split up a class into simpler parts that sum to the same thing, as they split into something that now requires more implementation work for every supplier of instances. If we say an equational theory lets you supply operations and laws about those operations, Haskell can let you refine equational theories by adding operations, but the laws remain implicit and do not supply definitional power in supplying the definitions that belong in your superclasses. A finer grained set of theories in Haskell currently will always require more work to implement than a coarser grained set, so you wind up in cases where folks favor a smaller set of theories over a more accurate set of theories that closer approximate the actual invariants available to you. However, since nobody else is, I'll play devil's advocate to Turner's and others' objections to typeclasses, and take the side of typeclasses with few equivocations below. I apologize for the rather "stream of consciousness" writing below. I've got to run momentarily, so I have little time to organize my thoughts. ML modules are a very powerful code organizational tool. However, my experience with them suggest that when I want to work with several modules with similar functionality the constant disambiguation they need gets old fast and when I go to refactor code that uses a lot of ML-style modules, far less of the code makes it through the refactoring without change. I want ML-style modules for Haskell. They are very very good organizational tool. However, I do not want them at the expense of typeclasses. The cases where they are unequivocably "better" than typeclasses strike me as exceptions rather than the rule. e.g. with typeclasses if you wanted to take a module worth of code and allow a global decision about which random number generator to use, you'd have to parameterize every data type, method and class in the module with some type argument to pick the generator. This has a global effect upon the API of the module, so many users would choose not to clutter up the API with the option, choosing simplicity over power. Typeclasses occasionally are messy when the typeclass involved has members that appear solely in positive position in one member function and solely in negative position in another member function. e.g. inference for which `Monoid m` you want is usually fine, as while `m` occurs in positive position alone in `mempty`, in `mappend` it occurs in both. This means you can chain mempty and mappends to get an output `m` that lets you push information about the selection up the tree. Similarly inference for `Eq a` works nicely as you have `a` occurring solely in negative position in `(==)` and `(/=)`. But `mempty == mempty` has to lean upon defaulting to pick an instance, as nobody has propagated out the type information. These cases are handled very well by ML modules. e.g. you can parameterize the module on its random number generator. Implicits attempt to address this in some sense by offering a third point of view to address the idea that the downside is that of course, you can't just "make up an instance on the spot" (without `reflection`) in Haskell. They can be used without tying them to a type, which lets them offer an occasionally more convenient version of the former module story, however, with fewer guarantees of consistency. But, there is a huge price to be paid for that power. All diagrams in the category of constraints commute. (Ignoring incoherent instances). e.g. If you go from `Ord a :- Eq a` by superclass constraint then from `Eq a :- Eq [a]` by instance or from `Ord a :- Ord [a]` by instance and then to `Ord [a] :- Eq [a]` by superclass constraint you get the same answer. This matters a great deal when contrasted with something like implicits from Scala or what Coq, Idris and Agda call typeclasses. Why? We commonly refactor code in Haskell to move constraints out to the use sites. The very correctness of this global code transformation requires that we always be able to get the same instance in both places. A naïve answer is to try to index the object by the instance involved in some way. But we often have very complex "dependent" constraints involved. e.g. `Compose f g` can be `Functor`, `Foldable`, `Traversable`, `Applicative`, each in turn depending on the properties of `f` and `g`, which is being captured by the index you'd add? In category theory there is a notion of a [property-like structure](http://ncatlab.org/nlab/show/stuff,+structure,+property), which is described over on n-lab as &gt; Intuitively, property-like structure can be described as consisting of “properties which need not automatically be preserved by morphisms” or “structure which, if it exists, is uniquely determined.” e.g. Given a `Monoid m`, there is at most one unique, correct way to build a `Group m`, why? It merely equips you with a way to find the inverses in `m` that already exist, and the choice of inverses is unique. These property-like structures are the things where implicits and typeclasses are equivalent in power. You can of course freely choose to "implicitly" supply a property-like structure, as no matter which one you supply it'll always be the same! However, the majority of the instances we work with in Haskell are not mere property-like structures. In short, if Haskell had ML-like modules in addition to typeclasses, I would definitely use them, but if it had to give up typeclasses to get them, well, I'd go off and write another language to ensure that at least someone was taking the typeclass side of the bet! The benefits they offer in terms of global coherence, easy refactoring and the ability to _safely_ "dumb" down the data types without risking the users violating the abstractions of your more abstract data types is too great.
How does your reader solution indicate to the union that these two sets are somehow consistently ordered to `union`? You have no type information to supply that fact. You're now forced to rely on conventions and hope that the user doesn't violate your expectations. It gets worse when you take the more Haskelly API of moving the constraint to the use site, as there `insert` and `lookup` in the same `Set` can get different orderings and yield inconsistent answers and violate global invariants of the Set structure. I'm rarely just working with a couple of sets in my program.
Yes, in passing, but listen to the talk, it is all about time to market and specifically discounts FP in mature markets.
To be fair, on that front it is given by Joe Armstrong, who has a great deal of experience from Erlang, which is very much focused on doing just that first half, letting you write code you know will fail, figure out how to handle the cases you missed, hotswap it in, and at all costs, keep running, and not so much about type-based reasoning, which really does pay out most of its dividends in the area he discounts in the talk. Alas, we have fewer (and much smaller) big money Haskell success stories than he has big money Erlang stories, so perhaps his message about "how to make money" is er.. right on the money.
Ah, by "instance arguments" you mean implicit arguments? Those subsume the power of type-classes indeed, but are unrelated to first-class modules. Of course having the power of type-classes *and* first-class modules is better than having only one. But if you have to choose one, I don't see why you'd choose FC modules. As for Arbitrary -- I don't really get the orphan hate, especially in something for testing like Arbitrary instances. I've never encountered any problem with orphans in tests or applications, and for libraries it's not a big deal (can provide the orphans in a separate package). I don't usually use forAll. Most of my types have a usable Arbitrary instance.
Unless they `extend` it with a subclass rather than `implement` it as an interface. In Haskell you can also swap the implementation of say, `Data.ByteString`, with `Data.ByteString.Lazy`, and it would have a similar affect on the program as in Java, assuming the methods used are properly polymorphic. I'm not sure there's a very deep difference with Java here. Perhaps you can write a tiny artificial example of a class you'd swap with an interface that would work with Java and could not work with a compatible module export signature in Haskell?
&gt; Unless they extend it with a subclass rather than implement it as an interface. I don't understand how that relates to what I was saying, so I mustn't have been clear. Java code A refers to some concrete Java type B with some methods x,y,z. To make the code A general and abstract and work for a variety of implementations, it suffices to rename B to B', and make an interface B with methods x y and z, which B' implements. In Haskell, to do the same thing, you'd have to typeclassify or explicit-dictionary your whole interface which would require A to change their code to match. 
Right, in ML modules, for example, there is a static assurance that the same instance is used.
What does "introspect into a function" mean, and what would it allow you to do?
you're right. I forgot to mention an important point, two 'subtractive types' A-B and C-D are *defined* equal if A+D = B+C, this is the sense in which they are "formal inverses". In haskell this gives us a family of isomorphisms rather than actual compiler-verifiable equality. The fact is, we *can't* construct an arbitrary value of type A-B, we can only construct types A-0. Consider the GADT data Sub a b where Positive :: a -&gt; Sub a Void then we also get an isomorphism inl :: Sub a b -&gt; Sub (Either a x) (Either b x) inl (Positive a) = Positive (Left a) In order to represent your equation Either (Sub a b) b &lt;-&gt; a, we need some sort of structural subtyping that I don't know how to do (uniquely) in haskell. Basically something like (x :&lt; a) means a ~= ... + x + ... this is exactly the class "there exists a Prism x a" from the original post. unit :: (b :&lt; a, x :&lt; a,Sub a b ~ Sub x Void) =&gt; a -&gt; Either (Sub a b) b unit a = case a of BConstructor b -&gt; Right b x -&gt; Left $ Positive $ aToX x counit :: (b :&lt; a, x :&lt; a, (Sub a b) ~ (Sub x Void)) =&gt; Either (Sub a b) b -&gt; x counit (Right b) = BConstructor b counit (Left (Positive x)) = xToA x aToX is a partial function simply handling the constructor renaming, but lacking a case for BConstructor. I don't think we can actually do it this way leaning on the typesystem, since the isomorphism is not unique. This should illustrate that the only way to make a value of type Sub a b is by injecting normal values, but we can then play with the type signature to get types with a non-zero negative part. Hopefully that made it clearer rather than more confusing
This runs into the same problem, it just lets us curry the argument up to the module level. Keep in mind, we can swap out f and g fairly readily for ones with different properties in something even as simple as `Compose`. In a world where you have to store some kind of index, that doesn't work at all. When you get to more complex constructions like monad transformers where you can hoist in and swap out the transformer you are parameterized on the 'index the module' story falls apart faster and faster. Going back to the 'equational theories' comment I made, we you can view it this way. ML modules let you talk very neatly about the pushout of two theories. You can smash things together and talk about a thing being both a Monad for ((,) e) and a Comonad for ((,) e) for instance if you want. This sounds great! But really, the problem is this is the wrong notion of 'pushout of theories' / refinement. I don't want to write a module that implements the machinery to supply both. I want to write a module that pushes out the constraints conditional on their superclass requirements. e.g. `instance Monoid e =&gt; Monad ((,) e)` says that only if e is a Monoid do we have `(,) e` is a `Monad` and `instance Comonad ((,) e) says that `(,) e` is a comonad all the time. The module story is good at dealing with the intersection of these cases, but the typeclass story lets you nicely articulate the "possible intersection". e.g. `liftM = liftW = fmap` should be required when *both* of the instance's constraint bodies are satisfied with the mindset that you can cleanly move into and out of these cases. The ML story requires me to build something specific for every point in the lattice of theories I want to instantiate, and makes it _vastly_ harder to pick up and drop properties as I change things around into configurations where more or fewer of those properties hold. On the other hand, the Haskell story currently requires me to pay for every refinement in the lattice of theories above me in the lattice of theories when I go to write instances. I do think we can at least fix the latter, but I find the former highly non-composable. It means I can't just use a Functor to swap out an argument, and have other properties magically light up, I have to write some one-off combinators to switch the structure between types chosen by different modules to get the new structure.
I'm trying to understand the implementation of Cloud Haskell. In the Original paper http://research.microsoft.com/en-us/um/people/simonpj/papers/parallel/remote.pdf It is stated that : " serializability of a function is not a structural property of the function, because Haskell’s view of a function is purely extensional. In other words, all we can do with a function is apply it; we can’t introspect on its internal structure" I'm not sure what introspection mean in this context.
Ooh, thanks! I’m reasonably well-versed in the PL theory background, but yes, that is a darn big paper :-)
Deleted
For the paper you linked to, it just means that a function is opaque. You apply a value and get a value. And no more. introspection is often referred to as reflection. I think difference is rather vague... and often depends on a specific language community. For java/c# it refers to accessing runtime representation of values. Which can include, the type signature of a method, all methods in a class, etc. The closest you have in Haskell is template Haskell. Works at compile time. But my knowledge is weak on that.
Thanks, the concrete implementations made things much clearer. &gt; we can then play with the type signature to get types with a non-zero negative part. Hmm, how? I can see that by adding an equality constraint, you made it possible to discuss `Sub a b` instead of `Sub x Void`, but we couldn't do that with `Sub a (Either b1 b2)`, because that would never be equal to `Sub x Void`. Or was `Sub a b ~ Sub x Void` supposed to represent an isomorphism between `Either a Void` and `Either b x`, since that's the way you defined equality between subtractive types?
Consider a lambda like `\x -&gt; x+2`. From the context of Haskell code, the *only* thing we can do with that value is apply it to another value (of the correct type). We don't know anything about it, e.g. which functions it may call internally, or if it's undefined on any inputs, or anything else. Because we can't see the internal structure (e.g. the AST that ghc would generate for this), we can't serialize it easily. It's this ability to see the structural details that's meant by introspection in the paper.
(The table left out in the OP can be found [here](http://www.cse.chalmers.se/edu/course/TDA555/lab1.html).) Here's something to help you on your way: to pad a string *s* with spaces so that it is *n* columns wide, you need to add *n - length s* spaces. Thus: pad n s = s ++ replicate (n - length s) ' ' Now one row in your table is, say, `pad 10 (show x) ++ pad 10 (show y) ++ pad 10 (show y)`.
Yes, I agree. But it is important to note that his kind of success is extremely rare. Almost all "let us be first to market" startups fail. There is more money, as a sum sum over all companies started, in incremental lower risk development in mature markets. The risk and rewards are both lower, but the sum definitely higher. This should influence advice given on what technology to use for what purpose.
We need a more concise articulation of this argument, but I think your pushout characterization is getting towards the pain some (including me) have felt when trying to make heavy use of modules for abstraction in SML or Coq (I've done far, far less Ocaml, but I'd guess its similar pain). As you say, you end up with a big wodge of code that, it turns out, satisfies a variety of interfaces. I much prefer slicing things more finely, which is something that type classes somewhat succeed at. 
I think you can do that in Scala as well using type projections (or extra type parameters): def union[T, S1 &lt;: Set[T], S2 &lt;: Set[T]](s1: S1, s2: S2)(implicit sameOrdering: S1#OrdType =:= S2#OrdType)
... which leads right back to all the objections I have above about uses of implicits for non-property-like-structures.
Here's a silly idea, but there might be a kernel of a good idea in it. union :: Set s a -&gt; Set s a -&gt; OrdConstraintMonad s a (Set s a) runOrdConstraint :: OrdDict a -&gt; (forall s. OrdConstraintMonad s a r) -&gt; r This way we can be sure that all the `Set`s inside the `OrdConstraintMonad` computation use the same `OrdDict`. No one in their right mind wants to do all the plumbing for this by hand but perhaps there's an easier way of achieving the same sort of thing.
Except FARM. :(
Thanks to Malcolm for uploading all these!
The guy who videoed FARM did so because he was upset that last year's videos didn't go up, so I have faith that he won't make the same mistake. *crosses fingers*
Yes it's a fantastic job.
Introspection is something like what happens in JavaScript &gt; function foo(x) { return x; }; undefined &gt; foo function foo(x) { return x; } In other words, the runtime keeps the *source* of `foo` around for later. Mostly this is useful for debugging in Javascript, I think. I've never personally used it. But, even in Javascript there exist functions which we cannot understand in this way &gt; var x = []; undefined &gt; x.concat function concat() { [native code] } That `[native code]` bit tells us that the internals of the function are *opaque*: all we can do is *call* this function, not inspect it. This gives rise to trouble if we try to send this function to another node to operate. In FakeScript &gt; otherNode.perform(x, function (val) { return val.concat(val); }) * screeching noises as the other node fails catastrophically * The only way around this is if the two nodes could someone come to a consensus about what exact native code `concat` refers to. This may or may not be possible or easy. Now one of the major problems with introspection like this is that it's easy to determine which functions are built-ins and which aren't. Fundamentally, there will always be that distinction between your code and the built-in code. For instance, the moment we take advantage of this introspection on `foo` we can no longer replace it with an optimized native call (despite its simplicity). And so it comes down to whether you think of functions as something that arises from an AST or the more abstract "callable thing" notion. Haskell universally selects the latter... it's a much simpler semantics which allows for more interesting work behind the scenes. Unfortunately, it means that userland reflection of functions—again, useful for sending computations off to other nodes—is massively more difficult.
&gt; In this case you could not "have directly converted it [F Bool] to the reduced type", because we don't know what F Bool is. But there is no way you could have produced a value of type F Bool or inspected it without knowing what F Bool is. &gt; In an ideal world I would argue that users should use the type system itself to enforce uniqueness, rather than relying on instances being globally unique Alas, not the world we live in. &gt; (proposal) Complicated, right? And it's still approximate. If you want to convince people like me and SPJ that this complexity is worth it, we better have a really good use-case for this functionality. (I guess it's worth remarking, while trying to understand the rules you had been proposing, I essentially came up with the same private closed type family encoding that you described here. So I think it would work. But it's still approximate.)
With apologies to anyone who read the post when it first went up, before I fixed HTML.Parsec. I am definitely gonna set up tests before going any further. Incidentally, does anyone know: * how `manyTill anyChar (try $ char foo)` is different from `many $ noneOf [foo]`? * why pygments is coloring everything after `'\"'` even though it's escaped? ~~Also, I'm gonna stop posting every update to reddit, unless you guys would like me to continue. It feels kind of spammy.~~ Edit: Ok, I'll keep posting updates.
Well, they promised to upload them
If you keep posting it, it will get picked in haskellnews.org and I'll be able to read it via RSS :)
Don't stop posting, this is good content. 
Keep posting, it's interesting :)
Thanks for this! I've always referenced the table on the lens github page but this is much more helpful as a quick reference because it has example code.
`&amp;~` is the correct one thank u (`$~` is opengl's state accessor operator)
A quick note on the site usability: I have Javascript disabled, and the title is covering the center of the page and can't be scrolled away. I see that with js, it fades away nicely, but a gracefull fallback should exist.
it was the least-bad ghost theme i could find quickly but i've found a bunch of shit things with it so im going to try and get a better one
Disclaimer: This package may not actually use "the html5 parsing algorithm", which I will look up. But it probably has a quite close behavior, except that it will accept non-compliant documents too, for """reasonable""" html errors. It's aimed at extracting data as much as it can, trying to fix html errors on its way. Just because it's nost listed, I'll mention [taggy](http://hackage.haskell.org/package/taggy) and especially [taggy-lens](http://hackage.haskell.org/package/taggy-lens). It's quite equivalent in features to most of the libraries you mention, but it's the most efficient of all the other libraries I benchmarked it against, along with hexpat, wrt memory usage and speed, and it has lenses/traversals/prisms that make extracting information a piece of cake. Doing CSS selector-ish things amounts to writing a traversal that'll only pick elements with some attribute -- this is trivial to do with taggy-lens or any html parsing package that also provides lenses/etc for ease of use, as in one-line-of-code trivial. If you prefer approaches from other libraries, you should of course keep learning them :-)
Thanks, what about HTML5 parsing? (it's how the modern browsers parse HTML nowadays, eg. [Firefox](https://developer.mozilla.org/en-US/docs/Web/Guide/HTML/HTML5/HTML5_Parser))
This is a great for a novice such as myself, and I'd love to see more Euler problems written up like this! 
I don't understand why this is a problem. &gt; This matters a great deal when contrasted with something like implicits from Scala or what Coq, Idris and Agda call typeclasses. Why? We commonly refactor code in Haskell to move constraints out to the use sites. The very correctness of this global code transformation requires that we always be able to get the same instance in both places. Can you elaborate as to what this refactoring is and why it's desirable?
For example: module Foo where z :: F Bool -- to be mixed in f :: F Bool -&gt; String -- idem dito y = f z Sure, you can't do anything *interesting* with an F Bool when you don't know what F Bool is, that much is obvious: if you don't know what type something is, then you can only pass it around. But that's always the case, and it's not a feature particular to my proposal? &gt; That part is not complicated. The complicated part is understanding when you are allowed to write an internal instance and then have it type check against a signature which doesn't implement the internal instance. I see. I don't find it very complicated. If you had F T = Bool in the implementation, just imagine that you had F T = Int in the signature. Would your implementation still match the signature? In that case you're ok, otherwise you're not. &gt; Is it permissible to put this under the signature: No, it isn't, and it shouldn't be, even in an ideal world. If you have this module: type Foo = Bool f :: Foo -&gt; Foo f x = x Is it permissible to put this under the signature: f :: String -&gt; String Just because a function happens to be the identity doesn't mean that you can use it as an identity function on a different type. Whether a module matches a signature should depend solely on the types of the definitions in that module, and not on the values of the definitions. That is exactly why I duplicated identifiers like x_internal_123 :: T x_internal_123 = ... x :: Q x = x_internal_123 instead of simply replacing the type annotation `x :: T` in the implementation with the `x :: Q` from the signature, which would allow your example to type check.
Why write out StateT with base monad of Identity istead of using State (which is just a type synonym for that anyway)
`assert` could also have a few variations such as: assertf :: String -&gt; (a -&gt; Bool) -&gt; a -&gt; ParserS () assertf s f a = if f a then return () else throwError s -- Not sure about this one assertOn :: String -&gt; (a -&gt; Bool) -&gt; ParserS a -&gt; ParserS a assertOn s f p = p &gt;&gt;= \a -&gt; assertf s f a &gt;&gt; return a then you might end up with assertOn "missing &lt; in open tag" (=='&lt;') consumeChar etc. Other parser libraries (including Parsed if I remember rightly) have a way of labling sub computations to give you better error messages: (foo &lt;|&gt; bar) ? "foobar" which can give you nicer output.
If one is on a mobile device with chrome, asking for the desktop site seems to make the blocking logo go away.
I expanded the monad stack when I was debugging some type error and I never bothered to shrink it back.
What is FARM?
You actually don't need to do it this way at lal (and you simply shouldn't, because downloading all of hackage is very expensive in server and bandwidth resources). All of the dependency information is already cached locally on your machine, and the packdeps package gives you direct access to it. Also, https://github.com/bos/packrank shows how you can use this information to calculate some more interesting stats.
&gt; Sure, you can't do anything interesting with an F Bool when you don't know what F Bool is, that much is obvious: if you don't know what type something is, then you can only pass it around. But that's always the case, and it's not a feature particular to my proposal? My claim is not that you can't do anything interesting when you don't know what F Bool is, but in fact, that you can never do anything interesting. In particular, with your example, how are z and f being "mixed in"? Surely whatever mixes them in must define an appropriate instance; and furthermore (and most importantly), that instance *will be visible here*. &gt; If you had F T = Bool in the implementation, just imagine that you had F T = Int in the signature. Would your implementation still match the signature? In that case you're ok, otherwise you're not. Uh, so what are you saying for this example: type instance F Bool = Bool data Ex = forall a. Ex (a :~: Bool) (F a) x :: Ex x = Ex Refl True Are you advocating just looking at the type of the implementation, or the innards of the implementation? If I'm required to look at the actual implementation to tell if it's kosher or not, that counts as complicated in my book, because it means we need to keep track of type family reduction as an effect. I retract the "approximate" comment for now, it seems I still don't understand the proposal.
I agree and I don't. You basically describe a problem that comes from "pattern matching out of the universe". I've been thinking about this and suspect we could _reconceive_ what type classes "mean" in a way that doesn't fall prey to this, while maintaining the same behaviour. What if instead of thinking of type classes as functions on types, we explicitly admit that these are function on "type names" or "codes" in some sense. This then lets us think about that generative semantics in some sort of more direct way -- i.e. making them act more like the story for modules. Sorry if that's a bit handwavy, but it should give the gist.
SPLOOSH Any chance of getting the slides?
I don't see how this contradicts my point. When you have Hindley-Milner style types, all the type functions are injective. And unification in Idris checks for the injectivity of type functions for the reason that you mention. Now, of course, it certainly can't confirm that any random type function is injective! But in the case that the type function is a type constructor, the compiler will know that it is injective and be able to perform the reasoning that you mention. I agree with you that there are many challenges with dependently typed languages for type inference of some of the more complicated types. But in my practical usage of Idris, and my (albeit limited) knowledge of how Idris works, it seems to me like there is little issue for inference for the Hindley-Milner style types.
lets not be so cynical. This is technical forum where people talk from experience and that includes what they might be working on.
You might like the posts that I wrote a few years ago: http://zenzike.com/tags/Euler
I think you misinterpret my point -- this is a good chance for teams with FM tools to advertise. So they should.
Thanks for this! A few typos found so far: * "a functor that’s in a functor" should be "a function that’s in a functor". * `(1, “hello”, 5) &amp; _1 +~ 6 = (7, “hello”, 10)` Should be: `(1, “hello”, 5) &amp; _1 +~ 6 = (7, “hello”, 5)`
Some of my solutions to the first 16 or so. https://github.com/kvanberendonck/hseuler
Is it Haskell, or understanding how to break down the top level problem of generating the table that is causing you problems?
&gt; Stuck, I moved on from SPARK and tried Frama-C and its Jessie plugin. Even if SPARK was working for me, using C (even a subset) has advantages: it's convenient to use in other projects, there exist verified compilers for it and there's an extension to CompCert that allows for static verification of constant-time behaviour (although I've lost the paper!) Is the author referring to [CerCo](http://cerco.cs.unibo.it/)? The description sort-of fits (static verification of concrete complexity, use of Frama-C/Jessie), but is partially inaccurate. CerCo wasn't an extension of CompCert, it was a clean slate implementation of a verified C compiler that we wrote in a different proof assistant. Whereas CompCert deliberately avoided many uses of dependent types, CerCo was written in a style that made heavy use of dependent types to stress the proof assistant we were working in (Matita) and explore the design space of verified compilers. We also went further than CompCert in producing a verified optimising assembler, out of necessity as the assembler also had to pass timing/space information backwards, and producing machine code rather than stopping at assembly code. There were other differences too, like our greater use of verified checkers of unverified components, as we learnt from CompCert and Leroy that verifying register allocation algorithms is a massive pain and a waste of time. As for the project, concrete timing and space information was lifted back through the compilation/assembly chain and used to augment the original C source with annotations that could be used by external tools (e.g. Frama-C) to derive cycle-accurate timing bounds on a C program. The technique applied to all C programs accepted by our compiler (i.e. loops, function calls, I/O etc). Loop complexity was expressed in a parametric fashion, so the annotations would state something akin to "this loop takes 128 cycle to set up, and each body executes in 52 cycles, and we need 34 cycles to finish book-keeping after we exit the loop". It was up to the programmer to then place an upper bound on the number of times loops would execute in our Frama-C plugin to get the final bound for the entire program (usual practice in embedded software anyway --- perhaps where the reference to "constant-time" behaviour comes from).
fixed, ty
Conduits is a cool name. That's all I like about it, though (but the article implied I'm allowed to hate!)
&gt; how manyTill anyChar (try $ char foo) is different from many $ noneOf [foo] There are two differences I can immediately see. 1. The first definition requires a `foo` to be in the Stream to successfully parse. 2. The first definition will also consume `foo` when the second does not. Prelude Text.Parsec&gt; let p = manyTill anyChar (try $ char 'a') Prelude Text.Parsec&gt; let q = many $ noneOf "a" Prelude Text.Parsec&gt; parse p "" "b" Left (line 1, column 2): unexpected end of input expecting "a" Prelude Text.Parsec&gt; parse q "" "b" Right "b" Prelude Text.Parsec&gt; parse (p &gt;&gt; getInput) "" "abc" Right "bc" Prelude Text.Parsec&gt; parse (q &gt;&gt; getInput) "" "abc" Right "abc" On another note - you don't need to use `try` when you have a parsing function which could only consume one character. Using `lookAhead` instead of try will also fix problem 2. 
While it led to a nice illustration of some Haskell techniques, the author's approach to solving difficulties encountered with #5 was not the easiest approach. In PE, the rule is: if you encounter a difficulty, it is almost always best to solve it using math, not using more advanced programming techniques. In this case, you really don't need to factor anything. You just need to know, for each prime number `p &lt;= 20`, the largest power of `p` that is still `&lt;= 20`. The answer is the product of those.
I am [depending](https://github.com/benjumanji/methuselah-generator) on it :D 
Yes, tuples are first class data types. 
I think the submission is referring to the comment referring to /u/chak and a segmentation fault. 
I was pretty happy with PE as a way to learn haskell — until I realised that I was basically doing the exact same thing over and over again: -- solve the problem solveProblem :: String -&gt; String solveProblem = ... main = interact solveProblem After a handful of problems, I found that I was (re-)learning plenty of maths, but my haskell had for the most part stagnated.
 ghci &gt; :t (,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,) &lt;interactive&gt;:1:1: A 64-tuple is too large for GHC (max size is 62) Workaround: use nested tuples or define a data type lol "just use nested tuples"
One more typo: the example for `&lt;&lt;*~` actually uses `&lt;*~`.
I'm on it! Planning to publish problems 6 through 10 before the week has ended.
Actually, only using 2-tuples and nesting them can be pretty natural for some scenarios. It's a poor man's heterogeneous list: (&amp;) = (,) infixr 5 &amp; test = 1 &amp; "foo" &amp; True &amp; ()
fixed that as well (the "extra rules" bit was quite rushed)
&gt; Eventually something will be mixed in but you don't know what that will be. It's worse than that. Let's take a more fully fleshed out version of your proposal: module Fam where type family F a module FSig where import Fam z :: F Bool f :: F Bool -&gt; String module Foo where import FSig y = f z Foo has successfully put together some computation involving `F Bool`. However, there is still the problem of instantiating `FSig`. Under the rules you have described, this is invalid: module FImpl where type instance F Bool = Int f _ = "foo" z = 2 What if we move the type instance to FSig? Well, in that case, Foo does see the instance too. I suppose you could explicitly declare `F Bool` as non-reducing in FSig (`type instance F Bool`), but I'm not really sure what that means... &gt; If you are declaring `data Ex = forall a. Ex (a :~: Bool) (F a)`, then it's illegal. Well, then take it one step further: `Ex` was defined at the original definition of the type family, and not in this module. (And its constructor is exposed). Or how about if it's abstract in the signature, but there is another module I can import which lets me see the constructor? (This is what I have meant by "transitive mention")
Yeah, you did put in the effort to tie things in nicely, but plenty of people were posting answers without much of any feedback at all though, which is why I felt the need to comment on the matter.
TreeView -&gt; TreeBuild is O(1), as it just constructs a singleton sequence. TreeBuild -&gt; TreeView is O(1) as it just peels a single layer off the first thing in the sequence builds the bounded number of concatenated sequences below. Each concatenation is O(1) due to the choice of a good catenable output-restricted deque from Okasaki.
Matrix chain multiplication requires you to figure out the optimal order to do the multiplication to minimize the size of intermediate results, so peeling off matrices from the left one at a time, and concatenating chains of matrices doesn't help you achieve optimal multiplication order.
Yeah. And even a brute force solution can look elegant: foldl1' lcm [1..20]
&gt; However, there is still the problem of instantiating FSig. Well, z and f can also come from the outside: module Bar where import Foo type instance F Bool = Int f _ = "foo" z = 2 Now FSig is not explicitly instantiated with any module FooImp, but rather it's all being mixed in from the outside. &gt; I suppose you could explicitly declare F Bool as non-reducing in FSig (type instance F Bool), but I'm not really sure what that means... Right, if you want to give the signature FSig to FImpl, then you need to declare the `type instance F Bool` in FSig. It just means that FImpl defines the type instance, but FSig isn't letting you know what type it is. It's analogous to defining a concrete `type T = Int` in FImpl and declaring `type T` in FSig. &gt; Well, then take it one step further: Ex was defined at the original definition of the type family, and not in this module. (And its constructor is exposed). Or how about if it's abstract in the signature, but there is another module I can import which lets me see the constructor? (This is what I have meant by "transitive mention") You mean like this: module Fam where type family F a data Ex = forall a. Ex (a :~: Bool) (F a) module Foo where import Fam type instance F Bool = Int x = Ex Refl True module FooSig where import Fam x :: Ex This is illegal because FooSig is now mentioning F a, while not declaring type instance F Bool. Whether you put the definition of Ex in a module that you import or directly in FooSig doesn't matter. So I think by transitive mention you mean that if A imports B and B mentions F a then A also mentions F a. That is correct. The main point is that if a type instance is purely internal to a module, then you can hide it with a signature, just like you can hide internal value and type definitions with a signature.
In fact, the only downside I've found to nested 2-tuples in the pattern matching is uglier.
They certainly obey the isomorphism, but other functions might not respect it implicitly. It's morally inarguable that (a, (b, c)) ~ ((a, b), c) ~ (a, b, c) but it's much more arguable as to who and when and why you can tell the difference between those three. (Edit: sorry, forgot to say *morally* which just adds to the complexity instead, really)
am I the only one who finds it pretty outrageous that there isn't a set of tools available in the core of the language to juggle betwen tuples of various arities? Having a function of type (a1,..,an) -&gt; an+1 -&gt; (a1,...an+1) does seem kinda hard to manage in the relative simplicity of Haskell
the isomorphisms fail in the presence of bottom don't they? 
Tuples are unpleasant since they are not inductively defined.
The `assert` is quite ugly.
I would really like to have strongly typed generic tuples in a dependent language. I mean: data Tuple : List Type -&gt; Type or data Tuple : {n : Nat} -&gt; Vec n Type -&gt; Type such that runtime tuples are still implemented as contiguous pointer arrays. This would also make our generic product representations a lot faster and space efficient than the current practice of nesting pairs. 
Not with that attitude.
http://functional-art.org/2014/cfp.html -- Functional ARt, Music, etc.
My second example is still a heterogeneous tuple, it's just indexed by length too, which could possibly allow us to do some optimizations and fancy tricks I haven't thought of yet (in comparison, it seems to me that your link deals only with homogeneous vectors). 
Ooh, sorry, you're right. I don't think lenght really adds a lot, because if the type list is known at compile time, we can already know its length at compile time anyway.
best of all, you get source locations without any hackery! 
/u/ocharles will be around in a bit to promote type level lists. (The pun was initially unintentional, but I'm sticking by it.)
It would be nice to have this automatically for 'error' on "debug" builds. GHC 7.10 has some support for DWARF, right? What will I'm be able to do with the DWARF support? How useful will gdb be?
But I've just found a heterogeneous vector [library](http://hackage.haskell.org/package/vector-heterogenous) from the same author. Not as pretty as a natively supported implementation but good work nonetheless.
This blog is amazing. I hope posts will not get deleted after the book gets published. 
gdb / lldb are supposed to be able to work with 7.10 dwarf (once it lands and is cleaned up), though of course there may need to be some sort upstream patching to/plugins for gdb/lldb to allow fancy things perhaps. that is, i think the default in 7.10 for ALL build ways will be to equip ALL exceptions with the stack trace metadata (computed via the dwarf stuff i guess?). NB: i'm not involved with the work for merging that stack trace stuff into 7.10, so I cant speak authoritatively on this matter, just as a kinda sorta informed observer
As Tom said - I really think prominent place tuples have in `base` in a mistake, and it's often significantly better to work with heterogeneous lists. We have enough support for these in GHC 7.6, and the inductive structure present in them makes them *significantly* easier to work with than tuples. They do come with a bit of an overhead, but I'm not sure significant that is - e.g., is the linear rather than constant lookup time on relatively small collections problematic or insignificant?
Is this a cross-post from /r/haskell_art ?
Which heterogeneous lists are you recommending? The ones I've tried feel a lot more awkward than tuples. What am I missing?
I think there is some trouble when it comes to bottoms.
what a hidden gem
Also, you can use `GHC.Stack` with GHCJS too.
Yeah, if you're going down the tuples-as-nested-pairs route, you should probably use something strict in its second parameter... infixr :*: data a :*: b = a :*: !b that way, `a :*: b :*: c :*: ()` is strictly (heh) isomorphic to `(a, b, c)`. I think.
For profiling builds, you can have that automatically using `+RTS -xc`. See http://stackoverflow.com/questions/25359461/finding-where-loop-happened/25359633#25359633 as an example. Correct me if I'm wrong but I don't think DWARF support will be for emulated, in-Haskell stack traces, like what `GHC.Stack` provides.
i didn't say that they're the same :) yes, the 7.10 dwarf stack traces are different from these GHC.Stack ones oo, goodpoint about -xc, didn't know about that :) 
Thanks, this will be helpful.
Newtypes are not as good as dependently typed programs. Sure. Next.
Just like Bible. 
For debugging purposes you could use [Debug.Trace](http://hackage.haskell.org/package/base-4.7.0.1/docs/Debug-Trace.html) which is in the base package and so already included in your Haskell setup. There is a section on using it on the [Haskell wiki book](http://en.wikibooks.org/wiki/Haskell/Debugging).
&gt; With smart constructors, our data model is fundamentally underconstrained, so we patch that up by restricting who can create the data. That’s putting a band-aid on the real problem. &gt; Why not just solve the root issue — viz., that our data model is underconstrained? “Restricting who can create the data” is not a band-aid solution. A major point of using types is to ensure that values are valid by construction, and a major point of modules is to provide encapsulation. It’s not that the data model is underconstrained, it’s that the constraints don’t live in the type system, and that’s fine! This code is easy to write and easy to understand: newtype Email = Email Text parseEmail string = if isValidEmail string then Just (Email string) else Nothing The equivalent, translating the grammar of email addresses into data types, would be tedious and mostly pointless for the kinds of tasks that are usually done with email addresses (validate and stringify). It is—or should be—totally irrelevant to the user of my module whether I represent parsed emails as strings or trees. It seems that a larger issue is calling `newtype` and `data` tags “constructors” when they don’t actually perform initialisation and can’t report failures. 
Have to beware of space leaks with the writer monad though. http://stackoverflow.com/questions/7720929/space-leaks-and-writers-and-sums-oh-my 
This is neat! I was wondering whether it would be possible to get complete "call trees" given a specific thunk? Basically what I'm looking for is a function that forces the thunk, gets the SCC trace of every evaluation that takes place and combines the whole thing to a tree that shows what the thunk is composed of. This could be used (for instance) to implement a custom coverage reporting library.
You should probably say in the examples for `??` that it can also be used with the function functor (might not immediately be obvious): `map ?? [1,2,3] $ \x -&gt; ...`. In this usage, `??` makes a "hole" that can be filled though function application later on. 
in part
&gt; The original paper mentions this, but basically Haskell doesn't actually have any way to do ML-style sealing, so it would require a new language feature in the language (e.g. you can't implement seals just using elaboration.) Right :( Couldn't you implement `A seals B` by renaming all types in B to some internal name, and then only exposing a newtype wrapper of it? I guess you'd have to insert the right projections everywhere so it would basically amount to implementing sealing yourself, though. That way the Ex type of the sealed Foo would end up a different type than the Ex type you'd import from Fam, so you couldn't mix them up. &gt; In your example, you can't say type T = Int; you're now ambiguous whether or not you're referring to T from Fam or the new type synonym. Sorry I meant data T = Int, but I guess that's still illegal?
The [actual patchset for your browsing pleasure](https://github.com/ghc/ghc/commit/d94de87252d0fe2ae97341d186b03a2fbe136b04).
In all major Haskell web frameworks at least, logging is well supported, featureful, mature, and widely used. Of course, by nature a lot of what happens in web apps is in the IO monad. In addition, there are quite a few libraries on Hackage that support logging more generally. A few examples: * [hslogger](http://hackage.haskell.org/package/hslogger), a rich multi-level multi-channel logging framework patterned after Python's logging. * [fast-logger](http://hackage.haskell.org/package/fast-logger), a very simple and flat but very fast logging system that uses multiple cores. * [monad-logger](http://hackage.haskell.org/package/monad-logger), a monad transformer that adds multi-level logging capability to a monad transformer stack. Source code location is supported via Template Haskell. And many others. All of those do require the IO monad. But for production code, that makes sense. Logging is definitely a side effect; it would be unsafe for that to be happening outside the IO monad. For *ad hoc* logging during debugging, using `Debug.Trace` from pure code is fine.
&gt; Same reasoning applies, even if you made that syntax check (data T = T Int) Yes I'm always mixing up syntax from haskell, ocaml and f# :( Thanks for the discussion and good luck with backpack! 
Is the talk in English? I'd like to and be able to show up this time but I'd hate to impose by forcing everybody to switch to English.
the problem with heterogeneous lists is (as you mention) that they incur a linear performance overhead. The solution to this is pretty simple, define a heterogeneous array instead (which you can using an array of `Any` internally). I've done this for my own stuff, and think [u/PokerPirate](http://www.reddit.com/user/PokerPirate) might even have put an implementation on hackage somewhere. I don't have performance numbers, but if this isn't as fast as tuples I would consider it a bug in GHC.
Do you have a minimal Pipe example of this lying around ? Thanks.
Does this mean that `guard`'s type will be changed to `Alternative f =&gt; Bool -&gt; f ()`? Also `when`, `unless` and several others?
I do not believe the AMP proposal contained such changes, it focused on breaking the least amount of code possible. However it is possible that such changes will come in the future if there is enough support. More information: http://www.haskell.org/haskellwiki/Functor-Applicative-Monad_Proposal
Can someone suggest an easily understandable reference on data vs codata? The stream example in the post looked like a typical stream implementation in any functional language.
[The start of the main changes](https://github.com/ghc/ghc/commit/d94de87252d0fe2ae97341d186b03a2fbe136b04#diff-66) The only thing before that is compiler/parser/Lexer.x getting a `Functor` and `Applicative` instance compiler/prelude/PrelNames.lhs having the various movement of things and lots of dealing with `empty` and `&lt;*&gt;` being in the Prelude and not in Control.Applicative.
Thanks, this helps. I'm interested in this comment from the book: &gt; In fact, trace uses a dirty trick of sorts to circumvent the separation between IO and pure Haskell. That is reflected in the following disclaimer, found in the documentation for trace: &gt;&gt; The trace function should only be used for debugging, or for monitoring execution. The function is not referentially transparent: its type indicates that it is a pure function but it has the side effect of outputting the trace message. This sounds like we're cheating. Isn't the whole purpose of IO Monads to avoid this sort of shenanigans? What's so special about debugging that it's allowed to get away with generating output, when ordinary code is not?
It's special because (a) it doesn't affect execution and (b) it can't be relied upon. Due to laziness etc trace may not run as many times as you think, or even at all, *and that's all the guarantee you get*. It is a hack but it's no more a hack than attaching gdb: it shouldn't ever change execution.
Wouldn't these infinite streams give you infinite loops? You'd have to add some laziness to make it work in Idris, right?
What does it mean to say that writer is safer than IO?
So we should think of a trace as a helpful annotation, of sorts, to the program, rather than being part of the program itself?
Is there a description of this change?
https://gist.github.com/tel/7d60be60f85dc45319ba Edit: Fixed up the example a bit
It seems like pattern matching allows you to deconstruct sums case by case, while copattern matching lets you deconstruct products piece by piece. As a result, we shouldn't be able to construct sum-of-product types with copattern matching, nor should we be able to decide which tag to use this way during construction. Because every datatype normalizes to a sum of products, but not a product of sums, this notation is not quite as useful. There may be a way to hack it but I'm not sure it's theoretically sound. The idea is that copattern matching on codata wants to do incremental refinement, like a bounded form of object inheritance/subtyping, rather than directly determining the tag. for example: codata Person : Type where name : Person -&gt; String age : Person -&gt; Int castle : {King,Queen,Joker} -&gt; String wife : King -&gt; String husband : Queen -&gt; String queen : Person name queen = "queenly" age queen = 42 husband queen = king in data notation this is data Person = Joker {name :: String, age :: Int, castle :: String} | King {name :: String, Age :: Int, castle :: String, wife :: Person} | Queen {name :: String, Age :: Int, castle :: String, husband :: Person} queen = Queen {name = "queenly", age = 42, castle = "itshercastle", husband = king} But that's not quite right, because the codata should leave open the possibility of a generic Person, with only the name/age - there is no direct translation to data. In any case, you can see the copattern matching approach is much more concise for this case. 
As mentioned toward the very bottom of the gist, Idris already has laziness to support codata.
Wow, having `Applicative` in scope by default would be amazing.
Here's a quick overview. First, you need to ignore Haskell. In Haskell data and codata are unified and so the standard way of producing infinite data structures in Haskell will throw off your mental model. Instead, let's consider data as being defined by an arbitrary-but-finite number of constructor applications—thus we no longer have infinite lists. We can package those constructors together into a signature functor data ListF a x = Nil | Cons a x deriving Functor where the functorial parameter `x` corresponds to a previous application of one of the `ListF` constructors. We can see each application of a constructor as taking `ListF a [a]` to `[a]` by smashing that constructor down: applyListF :: ListF a [a] -&gt; List [a] applyListF x = case x of Nil -&gt; [] Cons a l -&gt; a : l But again note that we're only allowed to make finitely many applications of the Functor—if you're familiar with `Fix`... that's the wrong idea. One way to make this work is to use a type like newtype Mu f = Mu { forall r . (f r -&gt; r) -&gt; r } If you look at what it takes to fill in the blank aList :: Mu (ListF a) aList = Mu $ \smash -&gt; _ you'll see that you end up repeatedly picking some branch of `ListF a` and smashing it down with `smash`. Finitely many such operations forms a "data" type. Mu (\c -&gt; c (Cons 1 (c Nil))) :: Mu (ListF Int) Further, "data" types enjoy a fold principle cata :: (f a -&gt; a) -&gt; (Mu f -&gt; a) cata smash (Mu f) = f smash --- On the flip side, we might look for functions over a signature functor like `x -&gt; f x` for some `x`. These are types which are finitely destructed or observed. They're "codata". data StreamF a x = Stream a x deriving Functor data Nu f where Nu :: (x -&gt; f x) -&gt; x -&gt; Nu f We can build them by merely providing some inner "state" and an "observation" ana :: (s -&gt; f s) -&gt; s -&gt; Nu f ana = Nu and then once you're given a `Nu f` you can make finitely many observations of it view :: Functor f =&gt; Nu f -&gt; f (Nu f) view (Nu f x) = fmap (Nu f) (f x) The idea is that `data` and `codata` cannot be mixed. If you could apply `cata` to codata you'd get infinite loops again.
We're still working through the full proposal. The goal is to get to a state where base doesn't export one crippled version of a combinator from Prelude and then elsewhere an uncrippled version, forcing most users to have to hide the former to do anything, and lowering the adoption of more general tools. This was actually the original issue that the core libraries committee was formed to address, but we put it on hold until the AMP played out. The original proposal was discussed in "Burning Bridges", a bit over a year ago. There was rather overwhelming support for doing something to change the status quo, but at the time, no entity empowered to actually make a decision. Simon formed the core libraries committee so that we could make headway on these logjam-like issues. The intention would be that folks who really want something like the existing behavior for teaching can turn instead to the haskell98 and haskell2010 packages to get a simpler teaching environment without the bells and whistles, but that most people would take those training wheels off fairly quickly in order to work with the rest of the Haskell ecosystem. Now that the AMP is ready to go we can flesh out the original proposal. Why did we have to wait? `Traversable` gets affected by AMP. As a result of it, `mapM`/`sequence` should move out of the class to top level synonyms for `traverse/sequenceA`. This lets like Simon Marlow use it and the `ApplicativeDo` extension and to still use the `mapM` vocabulary for `Applicative` actions , and just get parallelism for free. This also blocked the move of the core of `bifunctors` into `base`, which was proposed and passed a few months back for similar reasons. Right now we're mostly taking small concrete steps towards this end and continuing to build / validate GHC as we go.
You should think of it as something you do to debug runtime problems, and then remove. Its not for production run-time logs.
You don't get a stack, but you can use [file-location](http://hackage.haskell.org/package/file-location) (uses template-haskell) to get a source location.
Sometimes jamming a printf into the middle of your code is the easiest way to do things. No one thinks it's sound or well designed, it's merely a debugging tool; you wouldn't ship code using Debug.Trace, but using it during development can make life much saner at times.
It doesn't actually print anything; it just accumulates a list of logging messages which you get as part of the result of running the writer monad, which you can then print out after the fact. This can be a problem when your problem is something like an exception halting execution of the writer because you'll never see the written log messages.
The problem with -xc is that it produces a TON of extra stack-traces that are unrelated to the error call. It looks like `errorWithStackTrace` will be much more log-friendly :)
He packs the IO monad together with some extra baggage into a single `Evaluator` type. So it's not pure code, IO is still there.
I have been working on this for a while now, after taking over bos' original project a couple of years ago. Fedora is using this tool for its Haskell packaging, and OpenSuSE too I believe. It should work and be available for current Fedora and EPEL releases. The [readme](https://github.com/juhp/cabal-rpm/blob/master/README.md) file contains more usage examples: eg $ cblrpm spec conduit creates an RPM spec file of the conduit library, and $ cblrpm install yi will try to package, build and yum install yi and its dependencies. $ cblrpm srpm idris creates an source RPM package for idris. $ cblrpm missingdeps lens lists the dependencies for lens that are not packaged yet (yes, we're working on it...;). It can still be made smarter: currently cabal-rpm doesn't really take dependency versions into account for example, leading to failures sometimes. I'd like to support self-contained packaging using cabal sandbox too.
There's a lot of `CPP` for versioning like: +#if __GLASGOW_HASKELL__ &lt; 709 Are these annotations kept around forever or is there a point in the future where the code will be cleaned because these old versions are no longer supported?
&gt;Because every datatype normalizes to a sum of products, but not a product of sums, this notation is not quite as useful. Why can't you just use the distributivity of product types over sum types to turn a sum of products into a product of sums?
Good point! IO seems to seep into many things the Evaluator touches, unfortunately. I had to concede the fact that every built-in function was nonpure because it might have to deref an `IORef`. Tis a shame.
There comes a point where X version of GHC can't build the HEAD source anymore (I believe we're upto 7.4 being incompatible..), and presumably that's when they will get removed. Of course, a lot of the time people just forget or are busy on other stuff, so they tend to stay around longer than they need to.
I think /u/pigworker wants an example of the syntax to define and use such types with copatterns; that's what I want at any rate.
Which are the crippled combinators in Prelude?
Even data List :: [*] -&gt; * where Nil :: List '[] (:*:) :: a -&gt; List as -&gt; List (a ': as) Would be a good start. That lets you construct lists with only a little extra syntax, and likewise for pattern matching. If the aim is to simply replace tuples with something with an inductive structure, I think that would fit well.
By that I meant the ones that Data.Traversable / Data.Foldable exports with more general signatures from the very same package. Virtually every combinator in these has a Prelude version the more general form collides with, so to use the more general version requires manually hiding the Prelude version. http://hackage.haskell.org/package/base-4.7.0.0/candidate/docs/Data-Traversable.html http://hackage.haskell.org/package/base-4.7.0.0/candidate/docs/Data-Foldable.html
Wouldn't it be more helpful if it showed `main.hs:(8,10)-(8,whatever)` as the source location of the `errorWithStackTrace`?
How would you turn, e.g., `T1 + (T2 * T3)` (i.e. `data T = C1 T1 | C2 T2 T3`) into a product?
I'm not convinced this is possible in intuitionistic type theory without implementing the sum by ways of a `data` type. In classical type theory it is easy. The problem is that `par` does not exist intuitionisticly and if it did it would allow for typing the excluded middle. 
There was talk of also making `join` a method of `Monad` with default definition in terms of `(&gt;&gt;=)`, and vice versa. I see this is not part of the patch. Was it eventually decided against? (I hadn't kept up with what the final AMP proposal ended up being.)
Three cheers for /u/aseipp &amp; friends: Huzzah! Huzzah! Huzzah!
This was dropped for the time being due to an unexpected conflict with the work on roles: in the current state, having `join` in `Monad` would break `GeneralizedNewtypeDeriving` for `Monad` in many cases. Obviously this is unsatisfactory, so hopefully a more general `GeneralizedNewtypeDeriving` can be devised! Or we could just introduce `UnsafeGeneralizedNewtypeDeriving`...
Right. My point is indeed that nu X. A + B is a very common and very useful type construction. I would like to see how copattern notation delivers programming for these things. All the examples I see are about streams, which are product-like things lending themselves to a convenient projection interface that might not be so nice for sums. I just want copattern advocates to give sum-like codata their best shot and show us how it turns out. (Oh, and don't forget the landmine that is dependent matching on codata.)
Is the overhead at compile time rather than run time, or is there code which is still polymorphic at run time which takes a performance hit?
Makes sense, thanks for explaining!
Yes, saying that we don't end up having UndcidableInstances was a blunder on my side. What I was thinking at that moment was that we do not support promotion of UndecidableInstances as this could lead to unsoundness of type checking. 
For throwaway "what the hell is going on" situations, Debug.Trace is the pragmatist's solution. Ignore the writer monad stuff. Writers has its uses but you don't want to restructure a bunch of pure code just so you can print some values. Be warned, lazy evaluation means things might not happen in the order you expect—which could be instructive. However, I hardly ever use Debug.Trace any more because... GHCI. If you open up the file in ghci you can play about with the function interactively. Often this is as good as throwaway trace statements in other languages. Because Haskell is pure you don't have to set up piles of implicit state to run a function. Everything is in the type signature. But sometimes it's a bit tedious to construct a complicated argument (like a record type) in ghci and you would lose it if you quit so you could... Write a unit test with hunit or quickcheck. Yourself in sixth months will thank you. This is one of Haskell's big wins. All the info you need to test a pure function is there in its type signature. I have occasionally spent days in java tracking down a all the implicit state needed to make some code testable (it involved JMS messages and database rows needing to be present) but with a pure function you don't need to worry. But maybe you still want to do some... Principled logging. I'm not actually sure about this one. Refer to other answers. However, working with a big legacy java code base has made me wonder whether only logging during IO (at least in production) isn't such a bad thing. I have found several cases where the logs have suggested that the app has done X but when I look at the code careless copying and refactoring has caused the code that does that writes the log to migrate away from the code that does X. It looks like: if(doWeWantToDoX()) { log.info("doing X"); doAdditionalThingSomeManagerWanted(); log.info("Done thing management wanted"); prepareToDoX(); actuallyDoX(); }
Great answer! Thanks! Also, I feel your careless refactoring pain.
There is a difference between *logging* and *tracing*. - Logging is part of the application specs (example, on a web server you'll need to record all the HTTP request), and should use an IO monad. Using `Debug.Trace` for this would be cheating indeed and is not recommended. - Tracing, like debug trace, is not part of application. It's only there to help you to fix a bug or something. It's not a end-user requirement. Using `Debug.Trace` can be see as cheating but is better than rewritting everything using IO Monad, fix a bug and then rewrite everything to remove the IO Monad (and then realized that there is another bug ). 
I've heard and read about this before, but I'm not sure what it means. Can someone explain on an intermediate level without category theory or other advanced math, what it actually means that Applicative is now a superclass of Monad? What does it mean for a Haskell programmer and how can they benefit from it? Is every Applicative now also a Monad?
Agda and MiniAgda are the only languages which currently supports copatterns, as far as I know. In Agda it is possible to obtain neelk's T - aka the Delay monad - by using a mutual definition: http://www2.tcs.ifi.lmu.de/~abel/msfp14.lagda What do you exactly mean with dependent matching on codata?
&gt; what it actually means that Applicative is now a superclass of Monad That means that when you have a monadic value then you can use it with the `&lt;*&gt;` operator to sometimes formulate side-effecting operations in a more concise way. &gt; every Applicative now also a Monad No. 
Why was `Alternative` moved into `Control.Monad`? Wouldn't it make more sense in `Control.Applicative`?
Thanks! &gt; &gt; every Applicative now also a Monad &gt; &gt; No. And the other way round? Is every Monad an Applicative?
Is there any reason not to use this in place of `error` everywhere?
That was easy. Thanks again! :)
This page explains the problem with roles: [Roles2](https://ghc.haskell.org/trac/ghc/wiki/Roles2) 
It's the other way round: every Monad is also an Applicative. Thing is, if you have an existing type that implements `Monad`, you can always make it `Applicative` as well: instance Applicative MyType where pure = return (&lt;*&gt;) = ap Unfortunately, for historical reasons this hierarchy was never enforced. A type could implement `Monad` without supporting `Applicative`, even though by all means it could and should. This lack of enforcement led to many problems, including: * Code duplication: since Applicative and Monad were separate, library authors had to write duplicate combinators for both. `fmap`/`liftM`, `&lt;*&gt;`/`ap`, `traverse`/`mapM` are all examples of this problem. * Slower code: Applicative code is often faster (Simon Marlow's Haxl is a great example), but because many functions only used the Monad interface, they missed out on these optimizations. This patch fixes Monad so it's a subclass of Applicative, as it should have been. It doesn't fix the two issues I mentioned &amp;ndash; that'll come in a later proposal &amp;ndash; but it's an important first step. To the average Haskell programmer, not much will change. You'll be able to use a few useful functions without having to import them, but that's about it. It's essentially a cleanup, making explicit a convention that pretty much everyone follows anyway. The [other thread](https://www.reddit.com/r/haskell/comments/2fxqts/amp_has_landed_in_ghc_main/ckdt434?context=1) has some details on what'll happen next, if you're interested.
I hadn't actually seen wl-pprint-extras before, so this is based on my quick perusal of the Haddock. Both libraries give `Doc` a type parameter, but it doesn't look like wl-pprint-extras actually lets you use it to carry around a payload. In any case, I couldn't find anything like `annotate` in annotated-wl-pprint. So I'm not entirely sure what the type param is for. The `Doc` in wl-pprint-extras implements more type classes - it's an `Applicative` and a `Monad`, while annotated-wl-pprint just makes it a `Functor`, because I couldn't think of a reasonable meaning for `pure`. It seems that wl-pprint-extras has a bit of fancy rendering stuff that's not in annotated-wl-pprint as well. Perhaps /u/edwardkmett can shed more light on it.
Thanks for your response. I agree that it's unrealistic to hope for everybody to agree on every details, and on the fact that there are already overlapping libraries. Perhaps I shouldn't have focussed so much on how the *library* landscape will become divided. I mentioned libraries because they are concrete artifacts which are easier to talk about, but really, it's the divide between people with which I am concerned. Nobody writes articles about how monads suck because applicatives are stricter and then get downvoted into oblivion. Whereas clearly, people do care about whether `fromJust` is a small convenience or an abomination, and I think this reflects a deeper philosophical disagreement on where the language should go. Hence the suggestion to use different languages. Even dependent types are a red herring, in that making them easier to use would allow more low-cost high-correctness libraries to come out, and both camps will prefer those to the high-cost high-correctness solutions. So what's left of my argument is: those who think `fromJust` is an abomination should switch to Agda or Idris, because those languages fit their programming philosophy better than Haskell does.
I see. The `List` type that /u/ocharles defined has linear overhead to access elements. But you could have a heterogeneous array like you mentioned whose type was still given by a type level list, right?
You're right, I get: $ ghc-pkg list transformers c:/Program Files (x86)/Haskell Platform/2014.2.0.0\lib\package.conf.d: transformers-0.3.0.0 C:\Users\BAK\AppData\Roaming\ghc\i386-mingw32-7.8.3\package.conf.d: transformers-0.4.1.0 but if I try to `ghc-pkg unregister` it: ghc-pkg.exe: unregistering transformers-0.3.0.0 would break the following packages: regex-posix-0.95.2 regex-compat-0.95.1 regex-base-0.93.2 parsec-3.1.5 network-2.4.2.3 mtl-2.1.3.1 HTTP-4000.2.10 haskeline-0.7.1.2 ghc-7.8.3 fgl-5.5.0.1 either-4.3.1 free-4.9 profunctors-4.2.0.1 bifunctors-4.1.1.1 semigroupoids-4.2 comonad-4.2.2 contravariant-1.2 distributive-0.4.4 monad-control-0.3.3.0 transformers-compat-0.3.3.4 transformers-base-0.4.3 exceptions-0.6.1 MonadRandom-0.3 (use --force to override) Obviously a lot of essential packages from the Haskell Platform depend on this old version, whereas `either` likely wants `0.4.1.0`. How can I resolve this now?
Someone might be doing that at the moment.. for the meantime, you can convert to Docbook and then use the Docbook reader.
I'd unregister `transformers-0.4.1.0` by running `ghc-pkg unregister transformers-0.4.1.0 --force`, and then reinstall necessary packages with a constraint, e.g. `cabal install either --constraint 'transformers == 0.3.0.0'`. The reason to stick to the old version is that the `ghc` package- which can't be upgraded- depends on it, and therefore you're stuck with it. I actually [wrote a blog post on this](https://www.fpcomplete.com/blog/2014/05/lenient-lower-bounds). Using Stackage ensures that you can only ever install transformers-0.3.0.0, and thereby bypasses the whole issue.
Thanks a lot, that solved it! I'll check out Stackage as well.
Yes, I expected to see such mutual definitions. Even if you introduce sum-structure by use of dependent records, something of the sort seems inevitable. It strikes me as expressive enough but not as ergonomic as one might hope. The "musical notation", whilst far from perfect, did at least make this sort of code more readable. As far as dependent matching on codata is concerned, the question is whether one can prove case analysis principles like caseStream : (X : Set)(xs : Stream X)(P : Stream X -&gt; Set) -&gt; ((x : X)(xs' : Stream X) -&gt; P (x :: xs')) -&gt; P xs or caseDelay : (X : Set)(xd : Delay X)(P : Delay X -&gt; Set) -&gt; ((x : X) -&gt; P (now x)) -&gt; ((xd' : Delay X) -&gt; P (later xd')) -&gt; P xd Coq's treatment of coinduction *does* allow these to be defined, but then loses subject reduction, because you can give a proof of equations like unfold f a == (let b = f a in (fst b :: unfold f (snd b))) which reduces to `refl`, even though the equation does not (and should not) hold definitionally. Agda, whether with musical notation or copatterns dodges this bullet by making such dependent case analyses impossible to define. The annoying thing is that, up to observation (ie bisimilarity) these dependent case analysis principles make perfect sense and are rather helpful for reasoning. The good fix is to have a propositional equality which is extensional enough to admit bisimilarity as equality for codata. You can add codata to Observational Type Theory and make bisimulation the equality for them. Meanwhile, we should expect that such codata already exist in Homotopy Type Theory, by a "depth-n-approximant" construction.
It's great, isn't it? I really like Dominic's stuff. I just wish he had time to write more!
The idea is that all people have name/age, but King,Queen,Joker have some extra fields, so they "inherit" name/age from Person while adding some of their own. It's supposed to be a set rather than a tuple - if the type was just Person -&gt; String, it would only be partial if there were more constructors. As it's written, you're right that it's the same because it includes all the constructors anyway, but the natural framework uses *open* sums with cocase expressions on destructors (like duck typing), rather than case expressions on constructor tags. The weird subtype/naming convention I used above is just a hack to get it to match up with some data declaration, when really codata and data are not the same, and codata should either be duck-tagged or, most naturally, just not allow top-level sums. Again, I'm not sure this fits in soundly to a system like idris
Perhaps this paper by Hoffman et al. http://www.cs.yale.edu/homes/hoffmann/papers/veristack2014.pdf 
Although I note that `(&gt;&gt;=)` has still been given a default definition in terms of `join` and `fmap`. I'm not sure that's actually useful, what with `join` not being in the class.
I don't really like that you have just directed the OP to the writer monad with no caveats. The writer monad is almost never appropriate for long-running programs that do logging, unless you want to accumulate a large list of the log messages in memory. (Yes, occasionally, one can structure the writer computation such that log messages are computed incrementally along with the result, but you get zero help from the typechecker in doing this, and if you screw up, you get space leaks.)
I would imagine runtime as it expands to nested case analysis which means forcing more thunks and traversing what is essentially a linked list.
The difference between `Inf` and `Lazy` in Idris is that `Inf` uses the productivity checker, while `Lazy` uses the size-change checker.
Well applicative style (e.g. foo &lt;$&gt; x &lt;*&gt; y &lt;*&gt; z) requires &lt;$&gt; to apply first, and &lt;*&gt; is also infixl 4. Sometimes I think their should be something like operator families, so you can only use things where the binding is obvious. Not sure what that would like in practice though.
It's infixr 0 $ 
I wrote up [this post](http://www.haskellforall.com/2014/02/streaming-logging.html) explaining how to do this.
I wouldn't call sigma types products, more the other way around (product types are just a special case of sigma types). There's a reason they are denoted with sigma and not pi, after all...
Good stuff
I'd be all for adding `UnsafeGeneralizedNewtypeDeriving` as a flag.
&lt;$&gt; needs to match the fixity of &lt;*&gt;, &lt;* and *&gt; or it is basically useless.
It was necessary to shuffle some things around internally to make the inheritance hierarchy work. It is still re-exported from its more traditional home.
So operator precedence should be a partial, not a total order? You would be able to say: * `&lt;$&gt;` has the same precedence as `&lt;*&gt;` * `&lt;*&gt;` binds tighter that `&lt;|&gt;` * `&lt;|&gt;` binds tighter that `$` * etc. which would allow you to work with applicatives comfortably, but would require you to be explicit by adding parens if you wanted to mix applicative operators with, say, `*` or `++`. Sounds like a cool idea. Does any language implement something like this?
Since coinductive types can be viewed as a product of sums (and dually, inductive types as a sum of products), I wouldn't expect to be able to very elegantly define codata with a top-level sum type using copatterns. Of course we can describe codata with a sum structure with one trivial projection: codata X : Type where out : X -&gt; Either A X a : A x : X out x = Left a But that doesn't get us very far in terms of usefulness. However, the dual problem occurs for products as inductive types, which are not very elegantly handled by standard pattern matching: data Prod : Type where in : A -&gt; B -&gt; Prod A B fst = \p: Prod . case p of in a b =&gt; a In this case we have to make a trivial case analysis on the product value, even though there is only one case. As suggested by [/u/davidchristiansen](http://www.reddit.com/user/davidchristiansen), something like sigma types could be used to encode the choice introduced by sum types: codata Y : Type where tag : Y -&gt; Bool out : (y : Y) -&gt; if (tag y) then A else X a : A y : Y tag y = True out y = a Whether such a solution is desirable in practice is unclear, though. Perhaps the underlying problem is that copatterns are not very well suited for codata with sum structure, just as case analysis is not very well suited for inductive data with product structure?
I think this is a great example of a change with a good benefit to cost ratio: fewer mysterious "head: empty list" errors at runtime, in exchange for one extra import for those times when you think you know what you are doing. I would also give `head` and `tail` slightly longer names, to reflect the fact that they are not intended to be used very often.
I encountered a bug along these lines in https://ghc.haskell.org/trac/ghc/ticket/9268. We don't have specific tests for this kind of issue; hopefully it would get caught by the general test suite. In this case the symptom was that any program that ran long enough to GC would crash, so it was quite obvious that something was wrong, the problem was just figuring out what...
I'd love to see experimentation here. I worry a little about explosion of relationships, though.
Yes, sigma types are a generalization of cartesian (non-dependent) products. I.e. all cartesian products are sigma types, but not all sigma types are cartesian products. But, if I remember my HoTT correctly, sigma types are just special cases of pi types. I.e. we can take `T1 + (T2 * T3)` and represent it with pi types--dependent product types--alone. Perhaps I'm misunderstanding things, but I believe HoTT lets us represent anything with suitably complex dependent products.
Sorry, just mild curiosity. fromHead and fromTail would match other patterns. Also IMO the best reason for partial functions isn't even that they are an escape hatch. It is that proving a function is total is too much work so giving GHC a way to do the "right thing" when your function doesn't cover an input makes the compilers job significantly easier. All without materially impacting the significant majority of functions which are total.
Two different compilers could potentially produce different binaries from the same source, but those two binaries ought to be functionally equivalent, so that if they, in turn, are compilers, they will produce the same binary from the same source. If that doesn't happen, one of the initial two compilers is incorrect.
I read about the issue with `join`, and just have a quick question - apologies if this is obvious and I'm just not seeing something. It looks like the issue is that the `Monad` typeclass *really* wants a representational type parameter. Without having the `a` in `m a` be representational, the series of `&gt;&gt;=`s that we usually use with `do` notation and so on is weird, because suddenly the `a` begins to matter (whereas usually we have the intuition that a monad "contains" it's `a`, or at least doesn't care about what type it is). Since the `a` in `m a` is a type variable which is being given to another type variable `m`, the role inference is forced to infer that `a` is nominal rather than representational, for safety. Could this be fixed by allowing typeclasses to specify roles themselves? (Or is this already allowed?) I would imagine something like this: ``` class Monad (m :: *@R -&gt; *) where ... ``` where the `*@R` specifies that the `a` is of kind `*` and of role `R`. Then, if an instance is created where the `a` is nominal, an error would be generated at instance creation. Perhaps this would disallow some valid but weird monads? Would this solution work? Am I misunderstanding something about roles? If not, what are downsides of this approach?
*"All the info you need to test a pure function is there in its type signature.*" This is only *necessarily* true for top level functions. 
The reason I object to `head`/`tail`/`fromJust` is not because they are unsafe, but rather because they lead people astray away from idiomatic Haskell style (i.e. `fmap`, `maybe` or pattern matching, in the case of `Maybe`).
To my mind the whole role thing is a hack to the type system. It's a pity that this hack stops sensible changes like adding join to the Monad class. 
We like to stay ahead if the curve with our Haskell compiler, so we already have AMP. Time to deal with the next change. Btw, we also have fail in the MonadFail class. A very effortless change. 
Indeed, while fix the GND hole in the type system gives me a nice warm fuzzy feeling, I'm not sure if the cost is worth the few actual problems caused by GND.
*You* object to `fromJust`? Oh my. Forget what I said, please don't leave us for Agda or Idris! :)
This was an error left over in the original patch. I'm going to remove it soon.
So can trace...
For the issue the blog post describes, you could use cabal shared sandbox ``` $ cabal sandbox init --sandbox /path/to/shared-sandbox ``` It sounds to me like you are using sandboxes correctly and not missing anything (or wasting too much time).
The full quote is: &gt; As the version of libraries is fixed up until you update the stackage repo. You should never use cabal sandbox. That way, you will only compile each needed library once. *If* you follow the instructions in the post and use stackage, instead of the default of using hackage, then you don't need to use cabal sandboxes, because stackage solves the issue which sandboxes work around. If you use hackage instead (because you want a more recent version of a particular package, for example), then sandboxes are quite useful indeed.
You should definitely use cabal sandboxes. For developing multiple packages together, you can use shared sandboxes. However, there are some bugs with sandboxes that make their use with large projects (tens of packages) slow and buggy, which is the reason we don't use them at work (yet).
Ah, thank you for clarifying! I didn't actually know what "stackage" was referring to; admittedly I didn't read the post in enough detail before criticizing that particular detail. If only there was some place where I could type a term and find a bunch of websites related to it... ;)
I just discovered Nix today while searching reddit for sandboxing before posting this. Need to read more about it, but from the basic overview it sounds kinda like Docker but more related to packaging than servers. I love seeing new approaches to these problems, thanks for input!
In most cases, a statement about doing something "always" or "never" is not true. (Math is, in some sense, the search for these "always" and "never" things that are really true. Precision is often lacking.) I've found cabal sandboxing an amazing boon. But isolated package databases can't share things; that's implicit in the *isolation*. So, there is duplication at times. On the contrary, Debian frowns upon static linking and encourages using shared libraries, so that fixing security issues is easier (new library version vs. many new package versions). In this case sharing is (usually) an advantage and systems that do more isolation miss this; isolated application installation though rarely sees regressions caused by a too-quickly-released security patch. There's a tension between sharing and isolation, and it's overly simplistic to say one is right all the time. FWIW, Cabal will soon be able to handle having two flavors of the same package version installed at the same time, which will largely eliminate the breaking reinstalls and may make it more reasonable to use larger sandboxes or even go back to a single shared package database. I don't think the single shared package database is tennable for anyone working on more than 2 or 3 Haskell projects right now.
Can we not at least annotate these functions in the documentation, pointing new comers in the right direction? Currently you need to "somehow find out" that they are questionable.
It is impossible to delete a file, open a network connection or launch missiles in the Writer monad. The only thing the Writer monad can do is collect information.
Haskell does support `WARNING` pragmas, so this is totally possible.
 answer = head $ filter f1 $ filter f2 $ filter f3 $ ... $ filter fn primes I was using PE to learn Haskell too, and had the same issue... I've since moved on to writing slow interpreters. What should I do next?
Here are mine (Haskell solutions only): https://github.com/mdunsmuir/project_euler/search?l=haskell
Infecting the kinds means you can have it work forward and backwards through all of the arguments of a type. Using the typeclass machinery on the other hand is more directed. So you can use information from earlier parameters to talk about later ones, but not vice versa. The role machiney infecting the kinds requires you to either start talking about role polymorphism, etc. or just make instantiations at different roles different classes, which gets messy around Foldable, etc. which doesn't care about role.
The issue isn't the kind with polykinds the class above works with any kind (i -&gt; j). The issue is that you don't get to use `instance Representational (StateT s)` when deriving `instance Representational StateT`, you don't yet know s! I've done some work on a similar problem in http://github.com/ekmett/hask with respect to bifunctors, and naturality, so there may well be a general purpose solution, but its not yet clear what the solution is, and its complicated enough that if it isn't very clear how it applies, it'll be written off as 'too complicated' and the status quo will prevail.
Maybe warnings are a bit too much (I expect that will show 'm with `-Wall`), as you said they are not necessarily safer then their alternatives (e.g.: `head` compared to pattern matching without matching for the empty list). To have the docs point new people at the "non-idiomaticness" (I don't know a better word for it now) of these functions, would be better then to warn at `-Wall` for using them, imho.
You can sandbox whole GHC and haskell-platform installations with Nix, not just a set of packages, without using docker or anything related (althought you can).
Is that part of Noam Zeilberger's thesis or a further investigation of his results therein?
The paper [Parsing Mixfix Operators](http://www.cse.chalmers.se/~nad/publications/danielsson-norell-mixfix.pdf) by Danielsson and Norell explains how to parse mixfix operators (i.e. permitting not just infix operator overloading, but the ability to have user-defined operators like C's `_?_:_`) by using a DAG of operator precedences.
It does suck to use data types that incorrectly model the real world. It's the kind of type error that Haskell doesn't catch too. I agree that far. However, I don't like the title because you can end up with bad semantics with `type` and `data` statements too. Blaming `newtype` seems like a red herring. Plus, there is a major use of `newtype` in allowing library devs to control class instances, which is a completely different thing. 
Can you give a high-level comparison of the two checkers? I haven't been keeping up as closely with Idris lately, but that sounds interesting. The size-change checker sounds intuitive, but I'm less sure about the productivity checker.
They're checking different things. The guardedness checker ensures that every recursive call occurs immediately below a constructor, ensuring that a step of evaluation can always be taken. It is used for codata, and it is quite conservative and inconvenient. The size change checker ensures that every recursive call has a smaller argument than the current case, ensuring that it makes progress towards the base cases. This is less irritating for practical programming, and it is used for data.
Would bad things happen if `&lt;*&gt;` had fixity 0?
The text is too high on the shirt. Most t-shirts put designs lower.
Expressing precedence as a D(A)G sounds like a crazy idea. Without transitivity, you would have to specify precedence for each pair of operators separately. As you don't mention Agda in your comment, I assume it was never implemented? If so, what was the reason?
As long as you already know a little C++, this is way easier to understand than LYAH's parts on typeclasses and polymorphism. Good job!
You can actually run HUnit tests with HSpec (see Test.Hspec.HUnit). Or, take a look at tasty - it has providers for QuickCheck, SmallCheck, HSpec, HUnit, and probably a couple others.
I use a blend of HUnit and QuickCheck (mostly QuickCheck) for the actual tests. For organising the tests - I used to use test-framework, but have switched to tasty sometime in the last 6-12 months. There's an overview of tasty [here](https://ocharles.org.uk/blog/posts/2013-12-03-24-days-of-hackage-tasty.html). I know some people who use HUnit and QuickCheck via doctest rather than test-framework or tasty. On one hand I like the idea of having executable examples in the documentation and running them as part of the test cycle to keep them up to date. On the other hand, I've found that I tend to build up a decent amount of testing support code, and like to keep that all together in the test suite. Still something to consider if it sounds like it fits with your style.
I would totally wear this, and I'm pretty picky about my shirts.
Nice work. Personally I prefer light to dark: a light option might be nice too, but maybe more Haskellers prefer black?
Not OP, but yeah what he said is the first thing I thought, and yeah ik its the back.
... weird.
Please let us buy these.
I think I would buy one too (if the T-Shirts quality is any good)
I couldn't said it better. Thanks you put my words in context.
The scheme was implemented as a library, but as of the last time I checked Agda's parser did not yet use it. I haven't heard any reason why not; it might just not yet be implemented. I'd argue that it's not as crazy as it sounds. You could, for example, declare the precedence of an operator by selecting two operators that serve as the parent and child nodes for your new operator, e.g. if we declare an operator `%%` whose precedence is above `==` and below `*`, then `a == b * c %% d` is unambiguously parsed as `[a == [ [b * c] %% d ] ]`, whereas `a == b * c %% d` is rejected because neither `+` nor `%%` take precedence over one another. So it's not nearly as bad as pairwise—it just requires two points on the DAG.
You can also differentiate constructors: data OneOrTwoInts = OI Int | TI Int Int oneInt :: OneOrTwoInts -&gt; Int oneInt (OI x) = x oneInt (TI x _) = x 
Contextomy is my new word of the day. And don't worry aren't we all guilty of its usage from time to time? Furthermore it is really easy and effective to use: You stated &gt; "I have never had a single dependency conflict issue with cabal..." So why use sandboxes at first? ;-) 
I had the same experience. It was very painstaking to get it all working with OpenGL/GLFW and Netwire 5 instead of SDL and Netwire 4. I thought I'd spare everyone else the same sort of pain.
I use [HTF](https://hackage.haskell.org/package/HTF) because the preprocessing means you get exact lines where the test failures occur, and it's integrated in the EclipseFP IDE.
The answers so far conflate destructuring with pattern matching. Let's start with really simple pattern matching. Say you want to translate the first 3 positive integers to words. You could do something like this: toWords :: Integer -&gt; String toWords int = if int == 1 then "one" else if int == 2 then "two" else if int == 3 then "three" else "ERROR" But that isn't very nice. We'd like to have a way of doing different things based on the value of `i`. We'd like to say something like toWords :: Integer -&gt; String toWords int = case int of 1 -&gt; "one" 2 -&gt; "two" 3 -&gt; "three" _ -&gt; "ERROR" Here, the underscore (`_`) means "anything that isn't one of the above." This is pattern matching in a nutshell. We can do the same thing for, say, colours. Here, the `Colour` data type is defined as `data Colour = Red | Green | Blue`. hexcode :: Colour -&gt; String hexcode col = case col of Red -&gt; "#ff0000" Blue -&gt; "#0000ff" Green -&gt; "#00ff00" We don't need a "default" underscore case here, because we have covered all cases already! Pattern matching in Haskell is extra nice, because you can do it in the function definition instead of in a `case` expression. The last function can also be defined as hexcode :: Colour -&gt; String hexcode Red = "#ff0000" hexcode Green = "#00ff00" hexcode Blue = "#0000ff" We can imagine defining a "percent" function like so: percent :: Integer -&gt; Integer -&gt; Double percent part total = 100 * (fromInteger part / fromInteger total) and then using it such that λ&gt; percent 3 12 25.0 λ&gt; percent 157 422 37.2 and so on. But imagine we want to calculate the percent of our customers that have opted for the premium package? What happens if we have 0 total customers? λ&gt; percent 0 0 NaN We don't want that. If we have no customers, then 0% of our customers yet have opted for the premium package. So we can special-case the percent function, like so: percent part total = if total = 0 then 0 else 100 * (fromInteger part / fromInteger total) Or! If you remember the bit about pattern matching in the function definition, we can instead just go percent part 0 = 0 percent part total = 100 * (fromInteger part / fromInteger total) This says that if `total` is 0, just return 0. If total is anything else, call that number `total` and use it in the computation. What's so cool about pattern matching is that it also supports destructuring. That means if you have a compound structure (a value that consists of many smaller values) you can extract each of the smaller values when you do pattern matching. Pretend you have the following data type, which models your customers: data Customer = Customer { customerName :: String, isPremium :: Boolean } You want to greet the customer with a fancy greeting if they have the premium package. This could be expressed as greet :: Customer -&gt; String greet cust = if isPremium cust then "You have my warmest welcomes, " ++ customerName cust else "Hi, " ++ customerName cust and when you run it, you get λ&gt; greet (Customer "Julia" False) "Hi, Julia" λ&gt; greet (Customer "Charles" True) "You have my warmest welcomes, Charles" but instead of doing this with functions and an if expression, we can destructure the customer with pattern matching: greet cust = case cust of Customer name True -&gt; "You have my warmest welcomes, " ++ name Customer name False -&gt; "Hi, " ++ name notice how you got a variable called `name` for free, because you used it in a pattern? That's destructuring at work. Obviously, you could move all this out of the `case` expression and into the function definition, like so: greet (Customer name True) = "You have my warmest welcomes, " ++ name greet (Customer name False) = "Hi, " ++ name If you wanted to greet your daughter, whose name is Ellie, specially, you can do that with pattern matching too! greet (Customer "Ellie" _) = "Welcome, my daughter and the most intelligent person to grace this planet!" greet (Customer name True) = "You have my warmest welcomes, " ++ name greet (Customer name False) = "Hi, " ++ name (Note how we use an underscore again to show that we don't care if our daughter have chosen the premium package – we *could* care about that, but we don't in this case.) You can also extract elements from any other compound structure, such as a tuple or a list: describeT :: Show a =&gt; (a, a) -&gt; String describeT (x1, x2) = "First: " ++ show x1 ++ " and second: " ++ show x2 describeL :: Show a =&gt; [a] -&gt; String describeL (x1:x2:_) = "First: " ++ show x1 ++ " and second: " ++ show x2 What happens in the last example if the list is empty? The program will crash, because the pattern match isn't complete. You should probably add a default case for what happens with an empty list (and also what happens with a list of only one element!) It's worth mentioning that some languages have just a weak variant of pattern matching (switch statements in C, C++, Java) while others have just destructuring (let expressions in Clojure.) Some have both, but separately (Clojure can do destructuring and pattern matching, but with two different constructs in the language!) Having both combined provides more power, of course. (But some would argue that's too much power in one construct. The combination could be implemented in a Clojure library but Rich Hickey, creator of Clojure, have opted not to include that in the language because he thinks using it is often an anti-pattern.)
Always use sandboxes (unless you using nix to grab your dependencies). 
Could you give a short example of such C++ code?
You should check out Rust, it takes a similar approach to ad hoc polymorphism as Haskell while giving basically the same performance as C++ templates (specialization is guaranteed). And it unifies static and dynamic dispatch in a nice way compared to C++ (where templates and virtual methods are totally different concepts).
It's pretty annoying having these non-Haskell-related posts appear on the planet.
yes it's the back, you can buy at http://teespring.com/purelyfunctional I'm very happy you guys like it!! 
very sorry to bring up politics 1.)DO YOU REALLY TRUST Microsoft who started with CP/M Dos and/or monopoly with secret closed code along with billionaires? Yes. Gates gives his monies to charities so it is NOT about bill gates or Phill Wadler post on haskell planet or even POLITICS. 2.)it's a very unlikely circumstance and this is FICTIONAL. A BIG RUN ON THE BANK of England including Soros? again? and petroleum war in the MIDDLE EAST and 2b.)THE SCOTTISH ARMY IS CALLED upon via compulsory DRAFT and a target could be the BRITISH MILITARY NUCLEAR and BIO-chemical bases in SCOTLAND. 3.)THIS IS NOT POLITICS BUT governance, trust and THE BEST independent language without the RUBY where most code comments are in language of Japan and by THE ELITE GROUP. The stated mission is just be powerful and CONQUER the internet by Ruby on Rails. 5.)DO YOU TRUST Java which is open source? What happens to the co-FOUNDER of Java who decided to FAST LEAVE ORACLE? why are there so many odd changes and POORLY DONE PATCHES at JAVA? 8.)Is MORE JAVA in the multi-core cheap FAST SSD memory the ULTIMATE SOLUTION or just a way of dependence and SLAVERY? 9.)OK ready for nasty comments and flames? HERE IT IS. I WANT A GERMAN CAR that is handcrafted with GERMAN LOVE not a Scottish car made by the baker. I WANT ChiNESE FOOD and yes strange SCOTTISH FOOD can be made in a chinese wok with all the subtle flavors. I DO NOT WANT A CHINESE COMPILER. I want a MATH compiler GHC GLASGOW haskell as creative and innovative. ITS ABOUT balance, INDEPENDENCE, Logic tempered by practicality for Haskell is NOT ML and not LISP. It's not about MICROSOFT which is a money making CORP and NOT a standard that multiple versions of Visual Basic are incompatible and even contradictory. SIMPLE TEST of reality. Take old code snippet 5 years ago for Haskell. Add new code snippet. KEEP HACKING and even read some books. HASKELL LOGIC WORKS for it is TIMELESS. can't even do that with EJB - beans started to stink? or Java or special hotshot it to get reasonable performance. JAVA. The Python idioms framework are somewhat BROKEN also, IMHO. HASKELL MEETS the democracy criterion of TRUST QUALITY 1.)of the people. It's LOGIC is peculiar and it is NOT scala and the modules mix-in... 2.)BY the people - much of it is WAY above my head and MISSION IS RESEARCH - but sometimes I can get a better solution in Haskell than Python /C++ . this is the weakest spot 3.)FOR THE PEOPLE - there are a few bloggers thanks redditors and fpcomplete who actually SPREAD practical usage. again, sorry for metaphors on POLITICS. what a 'dirty business' with side-effects :) PS. since the USA is a 'world leader' in software LIKE HASKELL - or maybe NOT !!!!! the common core, using poorly trained teachers, teach to the test TEACH TO THE TEST in common core works only for Java job interviews who memorize the BIG BOOKS but they cannot follow LOGIC and since ONE VENDOR - you guessed it MICROSOFT - is the big player, of course ALL TOP DOWN software education will be MICROSOFT CLOUD and 'click and point.' for additional comments and even FLAMES I hereby request that the GHC IMMEDIATELY BE RENAMED - not glasgow but the INDEPENDENT NO NAME Haskell Compiler. please enjoy your MATLAB and cludgy closed proprietary NOT VERIFIED code that makes the NUCLEAR POWER PLANTS go. If I with limited knowledge can write the bindings to another C library .... maybe there will NOT be a FUKUSHIMA in the vicinity of GLASGOW SCOTLAND? fictional scenario, although of course I have engineering experience. it is NOT about politics
An other nice text for the back side would be: I'm lazy
Black Tshirt and Jeans, it's pretty much the only thing I wear.
What the fuck, mate?
I think grandparent meant something like this: struct A {}; struct B {}; struct C {}; int f(A a) {} bool f(B b) {} template&lt;typename T&gt; void g(T t) { f(t); } main() { g(A()); g(B()); g(C()); // This one fails to compile } Main differences between Haskell and C++ here: 1) The function name f is ad hoc overloaded for types A and B, but not C. There's no explicit typeclass, in fact the two overloads have different type signatures. One returns int, the other returns bool. 2) The template itself is not typechecked, only its concrete instantiations are. That means it's valid for the template function g&lt;T&gt; to call f, even though f is not guaranteed to exist for type T. If someone tries to instantiate g for a type that doesn't have a function called f, there will be a type error at the point of instantiation. Overall I think that approach is stupid, and typeclasses are much better :-)
Sure. The things I've encountered are: * When you 'add-source' lots of packages, builds become really slow (there's a ticket I think, but I can't find it). * Sandboxes never want to break anything. This means that if you reinstall some library deeper in your dependency hierarchy, it will either (1) reinstall everything that was above it, taking lots of time, or (2) fail to find a plan at all. This is my biggest issue with sandboxes currently. I've tried to use them, but always end up having to wipe them after a while because of this issue. Related tickets are [1475](https://github.com/haskell/cabal/issues/1475), [1299](https://github.com/haskell/cabal/issues/1299) and [1362](https://github.com/haskell/cabal/issues/1362), I think. * Builds that fail 'forget' that some packages are installed, causing long build times again on the next build (ticket [1375](https://github.com/haskell/cabal/issues/1375)) I've also not figured out how to use `cabal repl` in a sandbox with multiple packages loaded from source. That's off the top of my head, I'm pretty sure there was more, but I'll probably wait until cabal 1.22 until I try to switch again at work. There we have something like 60-100 packages we'd want to add source (depending on if we add forks and open sourced stuff). Are your 100+ packages all under your control, or is that all the (transitive) dependencies?
You probably already know the switch statement from C/Java/etc. To take the color example: enum Color { Red, Green, Blue } switch(myColor) { case Red: return "#ff0000"; case Green: return "#00ff00"; case Blue: return "#0000ff"; } Which is rendered in Haskell as: data Color = Red | Green | Blue case myColor of Red -&gt; "#ff0000" Green -&gt; "#00ff00" Blue -&gt; "#0000ff" But suppose you'd also like to have arbitrary RGB colors: data Color = Red | Green | Blue | RGB Int Int Int yellow = RGB 255 255 0 Then you can capture the arguments to RGB as variables, that can be used at the right hand side: case myColor of Red -&gt; "#ff0000" Green -&gt; "#00ff00" Blue -&gt; "#0000ff" RGB r g b -&gt; "#" ++ hex r ++ hex g ++ hex b Given: hex :: Int -&gt; String -- convert to zero-padded hexadecimal 
Simple and sleek, I like it. What do you think of [this](http://www.zazzle.com/2_2_where_2_2_5_t_shirt-235982249680614221) one?
Thanks for bringing this paper to my attention, it's very interesting for numerous reasons.
Maybe it is because Haskell is not the only thing on the planet. ;)
This is really easy to follow thanks. I understand it now. edit spelling
Interesting! Will the video be put online afterwards? Otherwise, someone on the front-row with a smartphone as backup, maybe? :) 
I was thinking of his paper *On the Unity of Duality*, but it should be in his thesis, too. 
Thank you! Edit: Those animal pictures made me laugh. :) Edit 2: Ok, the real wtf slide is no. 62. &gt; Use GHC’s built-in linker &gt; • Had to modify it to unload code &gt; • GC detects when it is safe to release old code &gt; • We can swap in new code while requests are still running on the old code That's damn cool! 
I like (some of) the imgur comments: &gt; I think as a rule, clothes are more object oriented than functional. &gt; If it's purely functional it doesn't need a logo or phrase. On a related note, I posted a joke to /r/ProgrammerHumor a while ago, which went like this: &gt; Do you even `(MonadTrans t, Monad m) =&gt; m a -&gt; t m a`? Granted, that would be quite nerdy already. :)
The smug remark is unnecessary. Also C++ has classes which replace typeclasses, they are also polymorphic when used with templates.
uh, please don't use so much caps, its quite annoying. And I'd even be sympathetic to the argument but damn, this is an off-putting way to do it.
The most important part of yours: &gt; I understand it is hard for communities to remain patient with beginners as they grow in size, simply because the beginner section grows fastest of them all Can't agree more. Haskell community is rather small, but seems to grow in size. And since the language is hard enough for (early) beginners to lose track, that means *many* beginner questions will appear. And it's… a good thing for us all! Note that I have not experienced hostility, but that I might have not noticed.
Perhaps it would help if people were kindly nudged towards [/r/haskellquestions](http://www.reddit.com/r/haskellquestions).
I think reddit isn't the best forum for questions like that. There's other venues like IRC, stack overflow and mailing lists that are better suited. That said, I don't downvote (but also don't upvote) the questions.
I does provide coercing for things like lists of newtypes though, which is nice. Do you think there are better alternatives for that as well?
Yes StackOverflow is a nice place for beginners right now. I too think that this is the best place to ask questions and we all can help there too! If you have enough Rep. please go there and moderate Haskell questions in a friendly way (meaning: not the way the downvote/close trolls do with C#, Java, etc.) - correct beginners spellings and formatting. Review beginner questions and close votes for Haskell and answer those few beginner questions there in a nice way. Let's make these community sites a great place for Haskellers and show the OOP masses how pathetic their communites are ;) (just kidding)
I would say that sometimes people have questions that aren't really suited for stackoverflow though. If the question is opinionated or about what library to use for some task, it can quickly get closed without the asker receiving an answer, even if it's a perfectly valid question. 
&gt; I commonly see accurate questions being almost ignored in comment threads here. Why not upvote them? They often lead to good discussion! My impression has been there is a sizable number of people who want to keep reddit for discussing articles/blogs rather than beginner questions. I can understand this tendency since there is not another popular place to go to vote on and discuss haskell articles that I know of. &gt; TL;DR: Good questions are good questions regardless of how basic they are. Stop downvoting them There is only so much room on the front page for articles and the readers have to prioritize and those priorities manifest as upvotes and down votes. &gt; it makes beginners think they are unwelcome. Which is probably not what the down voters are trying to express. I want there to be, and I think it is beneficial to the community, a specialized place to discuss articles and blogs. For a while now /r/haskell has filled that role. I recommend stackoverflow for most questions, the down side is that it is not designed for conversation. Since [/r/haskellquestions](http://www.reddit.com/r/haskellquestions) is not high traffic [haskell-beginners](http://www.haskell.org/mailman/listinfo/beginners) mailing list might be the best place to ask beginner questions. Not a very satisfying answer though. In larger communities it is easier to specialize and dedicate some communication channels to specific topics and and it seems likely that Haskell's community is just starting to feel the pressures that eventual bring on that specialization. Not a bad thing, but maybe a growing pain. I will spend more time looking/interacting in /r/haskellquestions and explaining what is going on to those that post a question to /r/haskell.
True [haskell-beginers](http://www.haskell.org/mailman/listinfo/beginners) or the haskell-cafe mailing list might be better for conversation like questions. I understand the desire to use reddit rather than a mailing list though so I will spend more time on /r/haskellquestions to help those of a similar mind.
I agree with you, believe it or not. But as long as there's nothing in the guidelines/rules I can't really fault people for not using the better alternatives. Some people are put off by the formal nature of a mailing list or StackOverflew, and prefer to ask their questions on Reddit. There's also the issue of questions arising in the middle of a discussion. Some beginners would rather ask the question right there, in the context of the ongoing discussion, rather than create an entire new submission with a link to the discussion. In the best of worlds, someone answering the question on Reddit could cross-post and self-answer it on SO too...
I use doctests, because I feel like they take less effort to write than unit tests, while fulfilling the same purpose. That is, like a unit test, a doctest can validate that an individual function returns the expected output when given a specific input. The [doctest](https://github.com/sol/doctest#readme) package supports QuickCheck tests too. -- | -- &gt;&gt;&gt; fact 3 -- 6 -- -- prop&gt; fact x &gt;= 1 fact :: Int -&gt; Int fact x = product [1..x] The major difference between doctests and unit tests is that unit tests allow you to assert different things about your output, whereas doctests can only test one thing: whether the output of an expression typed in ghci (be it a result or an exception) is the same string as expected. When I need to test something more subtle, I write a predicate and check that the answer is `True`. -- | -- &gt;&gt;&gt; fact 3 &gt;= 3 -- True
&gt; Good questions are good questions regardless of how basic they are. Stop downvoting them The purpose of voting is exactly to let people express their opinions. It doesn't make sense to tell people what to downvote or upvote. &gt; This subreddit has no rules against beginner questions in submissions. You are right. This doesn't mean that I ought to like these questions. When I don't like things, I downvote them to let people know how I feel. If the number of downvotes is greater than the number of upvotes, it lets people know that the bigger part of the audience here doesn't like this stuff. I think it's fair, and it gives people feedback on their submissions. Personally, I come here to read interesting articles. When I feel like answering questions and helping beginners, I go to stackoverflow instead. When I come here and see questions instead of articles, I get irritated and downvote them. Apparently, I'm not the only one. I think it's easy enough for these people to adjust their behavior and post to stackoverflow instead. It seems like it'd be better for both them and me (and others who feel same as me).
&gt; They often lead to good discussion! Agreed. My experience has been that, in addition to the question being directly answered by one or more people, those answers themselves tend to spark further, more comprehensive discussion about related nuances and/or complexities. So just because the initial question was beginner-level, doesn't mean that the ensuing discussion remains restricted to that level!
ahah yeah, rudeness aside those comments were pretty hilarious ( nerdy but good one ;) )
**Disclaimer:** This is click farming for a bad music video on youtube. Not related to Haskell at all. I've just created a youtube account to dislike your video. Worst thing: you probably still get money for my view... Reported as spam.
Sincerely, I wouldn't call it a design. But slapped on logo+text are not my t-shirt type. 
In C++, you can do what you wanted to do like this: template&lt;typename T&gt; enable_if_t&lt;is_arithmetic&lt;T&gt;::value, T&gt; boxSArea(T length, T width, T height) { return length * width * height; } You can also read about C++ Concepts (hopefully coming to C++1y standard) here: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3701.pdf 
Yes, teach the optimizer that, e.g., `map id == id`.
My impression is that we could use more people answering questions in /r/haskellquestions.
I don't downvote beginners questions, but if I feel it's an "accurate", but "unresearched" question I'm unlikely to answer or upvote it either. 
Those are alternatives, but I'm still not sure why reddit isn't the best of them. Its not like its a serious academic forum.
I double checked before this, but so far, nobody has said this: I'd be in support of clearly putting that in the rules, along with a suggestion that more people subscribe to /r/haskellquestions . That said, I would also observe that the _only_ hostility is that it tend to be downvoted. A posted beginner question that gets 3 answers and downmodded to -6 is, _abstractly_, a win for everybody. If it weren't for the emotional impact of that -6 there'd be no complaints, but, alas, the emotional aspect is real and irreducible.
The second example should be `a == b + c %% d`, I think. I don't agree your comment about two points, though. You'd have to say "`X` binds tighter than `$`/`$!`/`` `seq` ``" for every single `X`. Everything can be a member of a list (`:`), so that's another candidate. The result of most operators can be compared: `==`, `/=`, `&lt;=`, `&lt;`, `&gt;=`, `&gt;`. And if not, then likely it can be composed: `.`, `&lt;&lt;&lt;`, `&gt;&gt;&gt;`. And so on, and so on... Edit: &gt; You could, for example, declare the precedence of an operator by selecting two operators that serve as the parent and child nodes for your new operator Oh, if I understand this correctly, you're saying that a mixed approach is possible -- that one could define an operator either in a transitive (partial order style) or a non-transitive way (directed, possibly not even acyclic graph).
You're welcome!
&gt;If the number of downvotes is greater than the number of upvotes, it lets people know that the bigger part of the audience here doesn't like this stuff. Asking questions of a community for the first time has always been a scary thing for me; it's very hard to think rationally in such a mindset. I'm exposing myself to a bunch of strangers in a way that will show my ignorance. Therefore, to me a bunch of downvotes could signify a bunch of things: maybe the question was stupid (remember: not thinking rationally here), maybe I'm asking the wrong question, perhaps it's something that's already been answered a bunch of times (I do usually search quite a bit before asking), or maybe to you it's already been answered but I didn't have enough experience yet to apply the answer to my problem. Basically, if we want to be a community that encourages and welcomes beginners, I think the worst thing we can do is answer these questions with silent downvotes. If you downvote it, perhaps a simple comment of "Please post this to r/haskellquestions" would go a long way to making the beginner feel more comfortable with asking questions and doing so in the right place. Also, a mention in the Community Guidelines and more prominent placement of the sidebar link might help too. I didn't actually know about r/haskellquestions until this post!
Finally! A job posting that I tick almost all the boxes. I do not understand this though... * Track record in setting direction for system design, development and implementation Does this mean solution architecting large clusters ? Isn't this a pre-sales skillset rather than a back-end HPC R&amp;D engineering skill ? Or is it referring to the data analysis tool ? ... and this ... Also "well versed and experienced in risk modeling and assessment"... ? Are you guys looking for a engineering guy who moved to pre-sales, became an account manager (sales) and then went back to coding in Haskell ? :-) 
+1 for [tasty](http://documentup.com/feuerbach/tasty)
Maybe we could hide vote totals? Interesting stuff would still tend toward the top, but the emotional impact of the numbers would be reduced or eliminated. It might even prevent blind upvotes and give us greater overall quality. I'm all for /r/haskellquestions getting some love, though.
I like HSpec a lot. It has good output, integration with other libraries (HUnit, Quickcheck, etc), and handy other features (like just running tests whose description matches a certain string). 
&gt; But if we take the analogous situation in non-dependent types, would you really say that sum types are just special cases of function types? Yes. `Either String Int` is isomorphic (modulo bottom) to `forall a. (String -&gt; a) -&gt; (Int -&gt; a) -&gt; a`. All sum types can be similarly encoded by their case analysis function, but not all functions can be encoded as sum types, can they?
Related from Haskell Symposium 2014, day 2: https://www.youtube.com/watch?v=UEIHfXLMtwA&amp;list=PL4UWOFngo5DXuUiMCNumrFhaDfMx54QgV&amp;index=52 It doesn't cover HalVM, but something somewhat similar called MirageOS.
SO is better because the title can be changed later and because questions and answers there are findable by search engines. For whatever reason, searching reddit is an exercise in futility, even with an external search engine.
I think IRC is the best place for time-sensitive opinions. True, it doesn't give you a great sampling of the whole community, but these questions benefit so much from discussion, and their answers depreciate so quickly, that I think IRC really comes out ahead.
I actually think these kind of open-ended, subjective questions are best kept right here. SO comments are really quite bad for open discussion, while reddit does a pretty good job. It's also less important that those discussions be findable by a search engine in a year's time because the ecosystem may have shifted.
Also answers on SO can be edited if they contain typos, and discussions aren't automatically rendered read-only after a few months.
&gt; Good questions are good questions regardless of how basic they are. Stop downvoting them I tend to avoid the downvote button, because when I do hit it it's often for the wrong reason. For something that really has negative value, a response that exposes the wrong-ness of that something is needed *before* the downvote. That said, it's quite rare that I upvote a "beginner" question. If there's a definitive answer, and it's only a search or two away already, the discussion is rarely of any value. If there's a definitive answer, and it's not already a search or two away, I'd rather see it on SO because SO is more searchable. Low value = no vote.
That's fair. You can have your opinions and I can have mine. That's what disagreement is all about. With this discussion I primarily meant to reach the peope who casually downvote without realising what effect this has on the beginners. As long as you are aware how beginners react and still decide to downvote, you have made an informed decision and I welcome you to go right ahead. :)
Ivory itself has a compiler, the compiler is written in Haskell but it is transforming ASTs that ultimately represent C code. Thus, the code it generates is not dependent on the GHC optimizer.
&gt; There's other venues like IRC, stack overflow and mailing lists that are better suited. For some those venues are better suited, for some not so much. For me IRC is to high traffic and stack overflow to formal. I actually think /r/Haskell is very well suited for (beginners) questions. For now at least, this might change when the community grows.
So, I know that Facebook worked on a system that used applicative to asynchronously handle dependencies, would this change make it easier to write such in a more-sugared manner?
Infix pattern synonyms maybe?
I'm definitely a pragmatist, but I badly want dependent types!
&gt; If there's a definitive answer, and it's not already a search or two away, I'd rather see it on SO because SO is more searchable. From a utilitarian perspective, I think this is half of a fallacy. Sure, it would be better to have it on SO, but your voting or not voting isn't gonna affect the degree to which it ends up on SO. So either you have nothing on SO and nothing with visibility on reddit, or you have nothing on SO and something with visibility on reddit. Though you could argue that a lack of upvotes on reddit encourages posting on SO, but I personally think that is a weak argument.
&gt; Asking questions of a community for the first time has always been a scary thing for me This is really important. Even when I submit a link to a subreddit I immediately think "Oh shit, what have I done?" a millisecond after I click post. 
&gt; The purpose of voting is exactly to let people express their opinions. (...) When I don't like things, I downvote them to let people know how I feel. (...) When I come here and see questions instead of articles, I get irritated and downvote them. That is explicitly and emphatically *not* the purpose, nor an appropriate use, of voting on reddit. From the [FAQ](http://www.reddit.com/wiki/reddiquette): ###Please do (...) - **Vote.** If you think something contributes to conversation, upvote it. If you think it does not contribute to the subreddit it is posted in or is off-topic in a particular community, downvote it. ###Please don't (...) - **Downvote an otherwise acceptable post because you don't personally like it.** Think before you downvote and take a moment to ensure you're downvoting someone because they are not contributing to the community dialogue or discussion. If you simply take a moment to stop, think and examine your reasons for downvoting, rather than doing so out of an emotional reaction, you will ensure that your downvotes are given for good reasons.
Yes, but why did this get so massively downvoted on imgur?
On Reddit, I think it's perfectly okay to get karma for posting other people's stuff. I don't know how Daniel feels about this, but when I see that other people have posted links to my stuff, it makes me much happier than when I get upvotes for linking to it myself!
&gt; They might be great for those who came up while maillists were the dominant form of communication Net junkie since 1995 and BBSes before that: no, mailing lists have always sucked. Even in their heyday there were superior options for open discussion, e.g. newsgroups (which I imagine were conceived *because* mailing lists suck). The only benefit was privacy due to the exclusive nature of the list -- which of course is a *bad* thing for beginners.
I feel like beginners are going to gravitate to /r/haskell no matter what, so maybe we should accept that and be welcoming there, while having something like /r/haskellarticles as the acknowledged place to go for higher-level discussion.
I don't mind if you post them here. I had stopped doing so because most of the material in the newsletter is material that this sub has already seen.
That would basically amount to `Coercible`, right? The compiler would somehow have to derive if coercing something is safe. That depends on the type parameters it's instantiated with, just like `Coercible` currently does. Or would you propose special casing `map`?
Sounds like the April Fool's patch to StarCraft II &gt; In addition to losses, wins are now also no longer shown. Instead, players will see Jim Raynor giving them a thumbs-up after every match regardless if they win or lose.
Read the linked article - it's nice. Then search for /u/Tekmo in the comments.
Guess who wrote that answer, mostly in frustration toward the low score of the question?
That's what some really big places have done (/r/gaming and /r/truegaming I think.) Then again, some places opt to enforce their status (/r/math stays high-level, directing people to /r/learnmath for basic questions.) Either way I think is valid. What's not valid is being in a state of limbo where there's no clear community guidelines on how to react to beginner questions.
I use hslogger in the IO monad. This is for a long running server app. Messages go to local files, growl, syslog, whatever you need. Very happy with it. I use Debug.Trace very often too--for debugging, not logging. 
The first time I heard of the idea and thought about I figured you would need something like open operator families, so operators would still have a defined absolute binding precedence and transitivity. You could then say an operator is in the families X/Y/Z, and use it unambiguously with any other operator belonging to any of those families. So maybe something like: infixr 9 . in X Y Z I would worry about how brittle such a thing would be to refactoring, as well as things like implicit imports creating something like orphaned fixities.
Oops, I will admit I didn't notice that your post here had the same name. However, even though your own answer was outstanding there were two responses to the question already and another soon followed. 
Upvoted this post and this great answer by /u/kqr. I agree with the [opinion of kqr](http://www.reddit.com/r/haskell/comments/2g3em9/meta_whats_with_all_these_downvotes_on_beginner/) about beginner posts on this channel.
I don't know. *Wild speculation:* Maybe a lot of people saw it out of context while browsing imgur. If you then consider that only a fraction of those people has a programming background and again, only a fraction of them know Haskell, it might be an explanation for this drive-by-downvoting.
Pardon my ignorance, but I don't see how this is pre-sales functions. These are tasks that an IT architect would do as part of turning an R&amp;D prototype into a customer facing product. 
&gt; I recommend stackoverflow for most questions, the down side is that it is not designed for conversation The haskell community at stackoverflow is absolutely amazing. Everything I know about haskell I learned there! Maybe we could get a sticky post redirecting beginners to ask there questions on stackoverflow? This would take up very little space for regulars here, but it would also be much more visible and helpful for beginners. Sidebars often contain so much info that it's hard for a newcomer to digest it all. The sticky post could be a one-stop-shop for beginner haskellers. Links to books, important articles, stackoverflow, irc, etc. I'd be happy to write up a draft of this if people liked the idea.
Indeed, the categories are a bit of a mess. They come directly from the .cabal file, so there's not much you can personally do.
So I don't think we've come to any consensus. Which is okay. There is some awareness and I thank you /u/kqr for bringing up this topic. I was unaware of any specific statements about our community being elitist; however, I have noticed some beginner questions have gone by relatively unnoticed. For the most part, I'm fine with beginner questions being posted here. A single pane of glass does offer some nice diversity with the occasional project announcements, intermediate discussion and even the rare advanced level discussion. Too much of any one of those things, this place starts to lose the magic. /r/haskell is one the very small selections of sites that I check frequently every day. I just doubt that /r/haskellquestions is the solution. You've led the charge to get folks who are passionate about the community, who want to help. Over time, it'll be forgotten and will largely be in the state it was in yesterday. IRC is awesome, but those answers are for the *most* part lost. Yeah someone may mention the IRC transcripts but I think we all know why that doesn't work as a solution here. As much as it pains me, I don't have the time I used to, to be able to lurk on an IRC channel. SO is good, but it can be problematic when people from other disciplines try to moderate and edit posts without really understanding the problem or ecosystem. Not an insurmountable problem, as I don't think it happens very often. Out of these options, have we come up with an answer of "None of the above?" So what is the solution? Would it be putting something like [Discourse](http://www.discourse.org/) on the main site, so you could have essentially a controlled SO? Would post tags be useful here? 
&gt; If "objects to using fromJust in production code" is a shibboleth of the correctness front, I think it's clearly false to say that the pragmatists dominate (and also to call them pragmatists...). Depends on why you forbid it. Do you have lot of newbies who abuse `fromJust` by using it to avoid checking for failures, instead of the proper usage of using it to indicate that we know that the argument cannot be `Nothing`? If so, then I would say that it would be very pragmatic to ban it from your codebase. If, on the contrary, there hasn't been any issues caused by `fromJust` but it is being banned on principle, because it would be theoretically possible to call it improperly, then this would indeed be a sign of "correctness at all costs". **edit**: I have now read your other comment, and the reasons you give for banning (more than one occurrence of) `fromJust` from your codebase are clearly pragmatic rather than ideological. &gt; Moreover, I think people pushing in both directions is actually pretty useful. Those focusing on Correctness At All Costs periodically produce tools the pragmatists can beat into a convenient shape, and the sloppy tools of the pragmatists are periodically given better foundations with fewer gotchas. Hmm, that's quite convincing! Maybe I shouldn't worry so much about community division after all.
You can't have the function return different types depending on the values of the arguments (but you can depend on their types, that's why things like `head :: [a] -&gt; a` work). You would be returning `Test -&gt; Int` if `guard` is `True`, and `Test -&gt; Double` otherwise. You could probably compose `fromIntegral` with `f1` so `f` now is `Bool -&gt; Test -&gt; Double`, or something like `floor` with `f2` so `f` is `Bool -&gt; Test -&gt; Int`.
My suspicion is that you might want to look at Existential types. As I understand it, basically you can declare a type such that anything conforming to an typeclass can be put into it, as follows: data Showable = forall a. (Show a) =&gt; Showable a You can then put anything you want into the list so long as it can be shown. xs :: [Showable] xs = [Showable 1, Showable "hello world", Showable (Just 4.0)] But you can never do anything other than show elements of the list (at least not without touching unsafe casts). 
Perhaps we could propose that Hackage itself de-dupes some of these names, by e.g. automatically mapping "Crypto" to "Cryptography."
On the other hand, beginner questions can be fascinating! They can expose the things about Haskell that are hard to learn and use, things experienced Haskellers often work around without noticing, which we can improve in future iterations of the language, tools, and libraries.
and also a poor choice of tags, i think :(