You can't if quantification is parametric, because `exists a. a` is isomorphic to the unit type. You can use `exists a. (Rep a, a)`, but that involves listing all the types as constructors of `Rep`. Of course, the compiler can conceivably do the listing for you (and in, for instance, JHC, it does it anyway for internal reasons). Or you could add type case, but obviously that's evil. :)
One wonders if effects are "benign" precisely when they can be encapsulated within some embedded language with a 'runFoo' method with a pure result. Of course, I think there are benign effects that aren't as amenable to that as `ST`. For instance: is there a typing discipline that will allow you to write the delayed mutable updates that can be used to implement laziness, while making it impossible to view any breach of referential transparency outside of '`runLazy :: LRef a -&gt; a`'? (I'm sure the type won't be that simple.) Something like `unsafeInterleaveST` can probably be used to implement laziness, for instance (if it weren't already), but it can also be used to violate referential transparency in conjunction with `runST`.
For patterns, I think he's talking about something like: newtype Mu f = In (f (Mu f)) data Cons x y = Nil | Cons x y pattern C x y = In (Cons x y) head :: Mu (Cons a) -&gt; a head (C x _) = x SHE actually has this facility. You can make aliases for patterns and they behave just like matching on constructors. I'm not certain what he means by contexts, except the type class variety. For instance, in theory you could write: class Add a where (+) :: a -&gt; a -&gt; a class FromInteger a where fromInteger :: Integer -&gt; a ... context Num a = (Add a, FromInteger a, ...) foo :: Num a =&gt; ... but stuff like that isn't implemented anywhere to my knowledge.
&gt; which amount to the same thing modulo sugar +1 for awesome expression (and pertinent)
I don't believe it's a SML advocacy. On his site successor-ml.org he wrote "Standard ML, being incapable of evolution, is dead."
Well, I meant something like: `exists a. Typeable a =&gt; a`.
&gt; more often than not you'd be better served by a mutable data structure, except for the fact that you'd need dummy values for not-yet-initialized fields. And also the fact that, you know, you'd have a mutable data structure. I hardly consider that the same thing modulo sugar.
Thanks for the plug. Pattern synonyms are very handy, and SHE wouldn't be without them. Meanwhile, if you google KindFact, you may learn of a pleasingly simple plan to do something about contexts.
You're not the first to notice a similarity with size-types and "resource types" or more generally systems of descriptive complexity. However it seems quite difficult to express complexity constraints in a straightforward way with them: indeed a function of type f :: Nat^a -&gt; Nat^a is *not* linear in space (or time) usage: some subcomputation may be very expensive. The extension required to have that guarantee is still unknown, at least to myself.
You're correct on both counts; that's what I meant.
True, you can't reorder because of side effects. But even a pure strict language has one effect, namely non-termination.
I don't think I can let you get away with this sloppy argument.. &gt; Yet another example.. Yet another? What other examples of "bad" top level state can you show? Real examples from real libs please. The lib in question is also a poor example to chose to illustrate your point, considering the new API is unsafe: "Running two AcidState's from the same directory is an error but will not result in dataloss". I'll take the authors word about the data loss promise, but presumably something nasty will happen (why else is it an error?). Anyway, please show a safe implementation of *openAcidStateFrom* which reliably detects these errors and responds in whatever way you see fit. &gt;..never as good an idea.. Never? You mean really never ever ever..? If this assertion is to be taken seriously then you don't just have to eliminate "global variables" (top level state) in a lib or two to prove your point, but **all** of them, every single one. I guess expecting you to do this is asking a bit much, but here's some material for you to get started: The "standard" libs used 23 "global variables" last time I counted IIRC, that's just in the Haskell code. There'd be far more if I included those nice convenient file system and socket APIs for example, but I guess you can't really fix those so easily. The HaLVM package uses a whopping 27 "global variables". This is very strange considering it's Galois policy not to use any global state, at least if Don Stewart is to be believed. But as this is currently only a first release perhaps you needn't bother with this one. I'm sure someone at Galois must be working to fix this broken lib even as I write this. The LLVM package OTOH only uses 2 "global variables". That's much better, but still a bit odd considering Mr. "global variables are evil" himself is one of the authors. Maybe we should just blame Bryan O'Sullivan? Anyway, when you're done with all that we can talk some more if you like.
Yes, but that wasn't part of the challenge. ;-) Of course I'm not seriously proposing that this is a good list implementation.
Heh, fixed. :-)
Interesting proposal! [KindFact](http://hackage.haskell.org/trac/ghc/wiki/KindFact), linked for the lazy.
Here's some of the old code. I'm sure it can be cleaned up quite a bit: http://hpaste.org/46320/tuple_mappings As far as the printf pattern, I've used it in a few places and generally found that it works well on its own, but can cause problems in interaction with other sorts of uses of typeclasses (see, e.g., this post: http://www.serpentine.com/blog/2009/09/25/riddle-me-this/). Using typeclasses to perform induction on the arguments of a function (a la haxr) is a much neater and cleaner approach, where possible. But things like printf, I now think, are handled much more straightforwardly by judicious use of quasiquotation.
Purity and laziness are rather orthogonal, and which you see is more of a syntactical issue (taking the long view that types are ultimately syntactic, of course). In purely pragmatic terms, you can hide mutability behind an ADT. In ML you have to reason about side-effects and whether they are benign or not. Prof. Harper considers this a feature, apparently; in any case it is clearly a preference and not an advantage, as Lennart eloquently tells us.
I assume you're writing in reference to the monster "top level &lt;-" thread from 2008, and perhaps were even a participant (although I decline to speculate as to which one). For reference, it starts roughly here (http://www.haskell.org/pipermail/haskell-cafe/2008-August/046437.html) and goes on through the following month. Even "Mr. global variables are evil" (which I assume is Lennart) wrote there: "I'm not contradicting that the use of global variables can be necessary when interfacing with legacy code, I just don't think it's the right design when doing something new." And I wrote that a top level MVar is never *as good an idea as it sounds as first*, not that one is never a decent choice out of a bad bunch. The problem, as the thread noted repeatedly, is that enforcing a scope on capabilities and resources is a good thing, but that the scope that comes with top-level MVars (package or perhaps rts level) is seldom precisely the scope that one would like. As to your AcidState argument, one can get the same bad effects if one runs two completely independant binaries using the same directory (a far more likely circumstance in any case). Which is why databases and other programs tend to use lock files and the os-level `flock` functions. Absent other OS support, even the most magnificently designed runtime system can't get around issues like that... I would kindly suggest that a) you chill out a bit, and b) if so inclined, direct some of your talent and energy towards the genuinely difficult and perhaps interesting problem of designing a proposal with better semantics than "top level &lt;-" or ACIO that can provide the guarantees we actually want rather than enshrining, in essence, a mechanism that generally provides us with guarantees that we don't.
For xml, the [xml](http://hackage.haskell.org/package/xml) package is light and good. For database, [HDBC](http://hackage.haskell.org/package/HDBC) has served me well. For web programming, I think it's too soon to tell. I've mostly used [Happstack](http://happstack.com/index.html), but there are some parts that are not perfect. I haven't tried version 6 yet, perhaps it's (even) better.
Indeed. I'm curious. I don't imagine time complexity can be expressed in Type Theory itself (not that I'd rule it out). Actually I envision both being implemented in a metatheory and in practicality implemented separately. . As you might know Agda's termination checking is even implemented separately. I just have this fantasy that the perfect programming language can be expressed in a unified theory. 
You can mapToTuple to an `Either` or `Maybe` value as well. You can also write a `mapToTupleM` function which would operate with a dictionary of `a -&gt; m b`. The latter, which can be done with a `SequenceTuple` typeclass and the existing `MapToTuple` class, is probably the cleanest and most straightforward approach.
TBH the only definition of the world that matters is "the environment during the execution lifetime of the algorithm". As it stands these things remain the same for this definition of world. This issue is simply a non-problem.
A bit understated, but true. 
Time complexity *can* be expressed using certain type systems, just google "complexity + type system"
I don't get it. Could someone demonstrate in Haskell?
Sum Types allow you to have "heterogeneous" lists but with better type checking. The approach is similar to the one described using classes, except the user explicitly delineates the types held in a list and pattern-matches to extract values from the elements. Many algorithms on lists are independent of the type of the elements, and hence their type signatures have only a type variable for the element type, and are therefore "generic." I think. I am not much of a Haskell programmer, but I think I get this much. 
Isn't there `Typeable` which is similar to the special `exn` type in Standard ML? But strictly speaking, the statement is rather misleading.
http://www.haskell.org/haskellwiki/Heterogenous_collections#Existential_types
I think the article is just referring to ad-hoc product types -- i.e., tuples.
Yes http://hackage.haskell.org/packages/archive/streams/0.6.1.2/doc/html/Data-Stream-Future-Skew.html I need to actually put up a version of these for possibly-empty lists as well sometime.
Okay, I'll bite. Doesn't main = return death not actually force 'death'? Looks safe to me!
I like the approach taken by the Jane St Core library. They choose a consistent ordering of parameters (in a module with type 't, the parameter of 't is first) and then use OCaml's named parameters to do currying in any order the user desires.
And generic Num functions of the Int specific ones if you really want to be able to say your library can do this.
It was more tongue in cheek, but point taken. =) Data.Stream.Infinite.Skew provides an Integer index instead which should suffice, but doesn't provide for a bound on the length of the list.
I might let development to settle down and wait for version ω. Looks interesting though -- I might get impatient and download a finite-numbered version.
I think there's a space for a strict language which allows an opt-in to genuine call by name rather than the awkwardness of lazy-types-as-a-library as provided by ML and Scheme. That is to say, a language with a `delay` primitive but no need for an explicit `force`. Even then (and even with laziness annotations for ADTs) I'm not sure how far you get. After all, one can force something after the fact, with perhaps a bit of extra cost, but it seems impossible by definition to implement a post-hoc `delay`.
Part of a series but the previous posts are short. Explorer can't show xml, sorry for that!
Usage examples of my hackage library. You can try it online. Explorer can't show xml, sorry for that!
The lib looks really handy, and the examples are pretty compelling.
Generalized generalized abstract data types? Nice.
...what is it that you almost always want?
Lucky you. I'm clearly too stupid to tell whether it's safe or not. My point is that the fact that the damn thing typechecks is neither here nor there.
Recently Robert Harper's and others posts made me think a lot. Dynamics/Types there, syntax and homoiconicity here… All those discussions about the roots of Lisp, ML, Haskell and their comparative merits are very interesting. As for "haskell roots", something lacks here about being "lazy" (as the recent augustss reddit post pointed recently, maybe just Wadler classic "Why functionnal programming matters" paper) But as we talk about Lisp vs Haskell roots, here we have a lot about how programs as data are important, there a lot about how types are important but I feel still without firm knowledge about their relative implications. On static typing/unityping I confess I don't completely get Haper's argument which seem to me as a chicken or egg problem (to express a program you can use untyped lambda calculus, but to have a meaningful program you must have a logic beneath so implicitly a type system : so which one was first ?) In practice I prefer static types, does this condemns me to an eternal quest of better type systems (or can systems with sequant calculus like Qi address that once for all ?) On homoiconicity and lisp syntax, I don't know either. Wadler's arguments in "Why calculating is better than scheming" is relatively convincing, even though they cover more the lack of algebraic data types in scheme than true disadvantages of homoiconicity, so it's like cheating. On the second hand, a language with equationnal syntax and it's own AST as a library, templates, and quasiquotes is not that bad… but is it ultimate ? (oh, and also having agda in its repositories ;-) … but is it enough for the "next 1000 years" ?) So I wonder when I'll have a firm grasp of those damn roots and their implications…
Meh. void* heterogeneous[100]; But heterogeneous lists usually aren't a very good idea. And anybody could easily code up a Tuple&lt;A, B&gt; for C++. Doesn't hurt to have a boost library for tuples, though.
I have a question: why does MaybeBool contain 3 elements? Shouldn't it only contain nothing and jus True=just False=nothing since nothing is defined to be in MaybeBool and then just is defined to be a map into MaybeBool, which thus far only contains nothing, so that just must be the trivial map to nothing? Thanks
 main = do let foo = (1, "hello world", Maybe True) print foo
I had imagined it was discussing something more like what dons linked to. Especially since it opened up mentioning vector&lt;int&gt; and list&lt;int&gt;. And considering how trivial it is to implement tuples in C++.
There are two constructors: "nothing" and "just." "just" ~~works~~ is applied like a function in that it takes an argument. "just" can only take two values, "true" or "false" as it's argument. "just" is not a value but a constructor and requires an argument to form a value such as "just true" or "just false." Therefore, the type MaybeBool has two constructors but three values: "nothing," "just true" and "just false." Is that a clearer way of saying it?
 import Control.Monad.Cont data C r a = Val a | K (C r a -&gt; Cont r ()) invoke :: a -&gt; C r a -&gt; Cont r () invoke x (K k) = k (Val x) invoke _ _ = return () unwrap :: C r a -&gt; Maybe a unwrap (K _) = Nothing unwrap (Val x) = Just x -- In Cont r... -- do x &lt;- e1 -- e2 -- corresponds to: -- let x = e1 -- in e2 -- from a strict language with side effects m :: Cont r (Maybe Int, Maybe Int) m = do x &lt;- callCC $ return . K y &lt;- callCC $ return . K invoke 1 x invoke 2 y return (unwrap x, unwrap y) test :: (Maybe Int, Maybe Int) test = runCont m id What is test? *Main&gt; test (Just 1,Just 2) *Main&gt; Feel free to translate it to some language with native continuations.
Lovely! How are you rendering the Doc values to such pretty HTML?
That is kind of what I thought was meant, but let me just clarify one point: every type constructor is an injective map whose range is disjoint from all other type constructors (this is what I would normally expect in a programming language, but this language seems more mathematically inclined, so I was unsure what the set MaybeBool really contained (since we had only thus far defined "nothing" to be in the set) and in turn, what "just" could be since it is a function from Bool to MaybeBool)
Well no, but you're close. The ranges of the type constructors do not need to be disjoint. Example, data A : Set where foo : Bool -&gt; A bar : Bool -&gt; A so you now have four constructor forms (values), "foo true", "foo false", "bar true", "bar false." So, no, they don't need to be disjoint. I want to also point out that to describe Agda as "mathematically inclined" is an understatement of the extreme kind: Agda is an implementation of Type Theory (an alternative foundation of mathematics originally created by Bertrand Russel.) Although Agda does not adhere 100% to the theory, but it's about as close as you get in a language that anyone actually uses (see also Coq). I also want to add that Agda is mainly used by mathematicians as an interactive theorem prover and is very rarely used for production programming projects. When used in computer science, it's often used to prove implementations to be correct (or not) or to simply prototype something.
I enjoy mathematics, but I do not know much of type theory (although, I am very interested). However, are you saying it is possible for "foo true = bar false"? If so, this kind of destroys the intuitive idea I had of what was going on since I thought the type constructors were to construct new values, of type A (or belonging to A), for each of its inputs -- as in, the type constructors just wrap the data somewhat like in Haskell. Also, if this is indeed the case, then why then is it required for a type constructor to be injective? (thanks by the way, I appreciate the help)
Constructors are disjoint and injective. The injectivity gives you C A = C B -&gt; A = B. If every application of the Just constructor produced the same value, it would not be injective. Perhaps the confusion is that you are wanting to somehow reduce the constructor application? Instead of reducing, just view an application of a constructor as identifying a member of the codomain (e.g. MaybeBool). 
That helps, thank you. My confusion arose from how the set MaybeBool was defined in the sense that nothing was declared to be a member and then just was declared to be a function into MaybeBool, so i thought "just true" had to be an element of the set, but from what i could tell, nothing was the only element we had defined to be in the set so that just true=nohing. However, i see that the type constructors provide an implicit way to declare elements of the set given that we assume that the constructors be injective an disjoint (as one would expect in this context).
Sorry, again, no :\ The entire value is "foo true." Literally think of it as a string. "foo true" is distinct from "bar false" and the other two values (all ~~three~~ four are 100% disjoint.) Let's imagine how this may be compiled.. It's going to be like this A:0="foo true", A:1="foo false", A:2="bar true", A:3="bar false" just as A:0, A:1, A:2 and A:3 are clearly distinct objects and so are the "string" versions that they alias. I prefixed those all with A because you can also have data B : Set where derp : Bool -&gt; B herp : Bool -&gt; B Now note that A:0 does not equal B:0, just as their "strings" are not equal. I'm not sure what's going on in your understanding but regarding your third paragraph: no and yes... Constructors just wrap other constructors (values such as true and false) to create new (distinct) values. In this particular respect there is no difference between Agda and Haskell. You can do all of this in Haskell. In fact, I encourage you to play with MaybeBool and my A example. But yes, to quote you "the type constructors just wrap the data," yes, indeed, exactly. I cannot grok what you mean by "required for a type constructor to be injective." Required? How? I understand it as innate in the system, not an external requirement that is imposed. "a"? Singular? I'm not sure what the alternative would be. And injective, yeah, that's what we have been talking about this whole time: A type is a way of constructing values and uses type constructors to dictate how to form all possible values... is it not natural that those values are all distinct? (Yeah, there exist systems where this is not 100% true but I promise you've never heard of them.) So, at this point I want to point out that you should probably study lambda calculus itself first. In particular, what I think you want to look up is information on various types of normilisations ("normal form" is the keyword you want.) These concepts aren't unique to Agda or Type Theory but are common to [almost] all lambda calculi (yes, including Haskell and ML.) If you want to continue from there, try Wikipedia for "inductive type" or something similar. Unfortunately there's not much information on type theory for amateurs. I think the Agda material (see the Agda) wiki may be as close as you'll get. I will say that it's a rather accessible subject to learn without much background. Start doing lambda calculus with pencil and paper, read all the relevant Wikipedia and then Barendregt wrote a TON of indroductory material on the lambda calculus and type systems. All of his material is online for free (referenced from Wikipedia) and not terrible to parse. You may want to skip a lot of the proofs though if you just want to get a general idea of whats going on. His books and papers lead up to a description of a Coq-like system. Close enough to Agda but in case you *really* want to get into this stuff, what Agda is implementing derived from Martin-Löf's type theory. There's an excellent book on that (amazing really) called Programming in Martin-Löf's Type Theory. It's written by some people who have had a lot of influence in TT in general and Agda specifically. It's online free, here: http://www.cse.chalmers.se/research/group/logic/book/ Here's one of Berengredts books: ftp://ftp.cs.ru.nl/pub/CompMath.Found/lambda.pdf He has another one thats just plain huge (I think 200 pages or so) filled with everything you'd need to know building up to nearly modern type theory. Can't find a link right now though. You'll come across it. It's called "Lambda Calculi with Types." Logicomix is a nice graphic novel about the history of it all. I think Wikipedia covers all the same, but the book is more entertaining and conveys a lot more regarding motivations and frustration of it all. Anyway, good luck and have fun!
Jeesus, you are way more cut out at teaching this stuff than I. Why did I not just say "foo true" doesn't reduce instead of the whole convoluted string explanation :P haha Ok, 6:30am, time for bed... 
"type constructors provide an implicit way to declare elements of the set" Yes, now you do get it. I swear i've read that line in a paper somewhere actually :)
I like it!
Nice work. Super-useful for (web-based) teaching.
First Doc is converted to String with a simple 'show' offline, then it is injected as preformatted text (with &lt;pre&gt;). Plus some basic css formatting, nothing else!
Is it not so that existential types and virtual methods accomplish the same thing? from the haskell link &gt; This is example akin to upcasting in Java to an interface that lets you print things. vs from the c++ link &gt; True, you can use inheritance to make the containers hold different types, related through subclassing.
Edit the text after "Test&gt;" and hit enter! Maybe confusing that the textbox has no border and there is no "Submit" button.
GGADTs are nice, but that's merely second order genericity. You wait until you get higher order genericity! Generalized generalized generalized generalized generalized generalized ADTs!
Sure, but what about show_superclass* heterogenous [100]; ? I'm wondering how are existential types implemented in haskell. Do all values carry a pointer into the virtual method table, just in case? 
Minor correction: "Why functional programming matters" was written by John Hughes.
Isn't Σ the *dependant sum type*, and not the dependant product type? That latter (denoted Π in the literature) is merged into the syntax of function types in Agda.
Scala has this. There is (=&gt; A) =&gt; B for functions that take call-by-name parameters. And there is a `lazy val` declaration which allows you to `let` things lazily, I believe. The call-by-name stuff tends to be a headache, though, I think. However, I'm not sure if that's fundamental, or if it's the Scala implementation not working as well as it could.
I think existential types are more like forward declarations -- the callee doesn't know anything about the type, but has to honor its uniqueness. 
Wow ! I just tried your Diagrams example, and it is a stunning way of teaching Haskell (By the way which library is it ?) Incorporating ghci in the browser + a standard graphics/animation/plotting library with svg/canvas backends, and Haskell could have a top level interactive and teaching environment. I think I just have ideas for my next haskell hacks… Have you began to work in that direction ? (`ghci --web` interface) 
Indeed, Σ the dependent sum type. Confusingly enough it's a record with two components, and records are products. :)
Malkovich Malkovich Malkovich Malkovich!
[HLists!!](http://homepages.cwi.nl/~ralf/HList/). The way HLists are done is pretty clever. Two-parameter type classes. It is almost obvious. Also, existential types are fine if you can pin them down to a type class that is "Typeable". Anything that is at least "Typeable" can be explicitly cast and reflected on to some degree. Data.Dynamic wraps this idea. 
Back in GHC, to be precise.
And then Oleg comes along with a hack to implement vari-generalized-adic abstract data types, *in Haskell 98's type system*.
Hey that will be a separate blog post soon! Yes, it is a good way to teach Haskell because it really motivates first-year students (also). How is this delivered is a separate thing. I would like to have ghci in the browser but this is not the case here - it is just server-side computation. The point is that SVG figures are very cheap to transmit and construct on the server side!
Good to hear !! (by the way, I didn't mean ghci in the browser per se, but rather with a web interface of course) I know it was a bit off topic but I feel this is very important ! I stay tuned :-)
OK, now it is called dependent *pair* on the slides :)
\o/
Marklar marklar marklar?
&gt; I assume you're writing in reference to the monster "top level &lt;-" thread from 2008. These arguments actually go at least as far back as 2004, where the problem was first seriously discussed and Ian Stark proposed a perfectly reasonable solution. This has even been implemented in jhc. Alas, even 7 years on, the owners of the defacto "standard" Haskell compiler at ghchq evidently have no intention of doing the same, or of offering any other solution. I really don't understand why, but seeing as I can't believe anyone there actually buys into the "global variables are evil" dogma I can only assume their reasons must have a lot more to do with Haskell community politics than logic. What really annoys me most about the current stance on this issue is the sheer lack of intellectual honesty. You cannot on the one hand say it's OK to use unsafePerformIO provided you meet certain proof obligations and at the same time deliberately leave the language so defective by design that unsafePerformIO is routinely being used in circumstances where these proof obligations **cannot be satisfied**. The other equally dishonest (not to mention offensive) stance adopted by many is to dismiss this as a non-problem caused only by idiots who clearly have no right to even think they might be able to use or contribute to anything to Haskell infrastructure. It's pretty hard to find any non-trivial IO lib that was not apparently written by an idiot. Even the few unsafePerformIO hack free libs you can find generally only manage this feat either by presenting an inherently unsafe API and/or by delegating their globals to C. &gt; Even "Mr. global variables are evil" (which I assume is Lennart) wrote there: "I'm not contradicting that the use of global variables can be necessary when interfacing with legacy code, I just don't think it's the right design when doing something new." Yep, your assumption is correct. The statement you quote is a significant softening from his stated position in earlier flame wars, but is still wrong. The problem really has nothing to do with interfacing to legacy code and using "global variables" is often the right thing to do even with brand spanking new code. Even if HaLVM was written entirely in Haskell instead of being built on "legacy" Xen, it would still use many "global variables", probably far more than it does even now. The problem is that without top level (aka "global") state, I (as hypothetical IO lib implmentor) cannot write any code for managing my particulular stateful corner of "the world" that has any kind of **reliable** memory of past interactions. Many seem to think that I should take my state as a parameter somehow, but in order to do this I must expose an inherently unsafe *newPleaseMakeOnlyOneOfMe* state constructor. This is unsafe because as soon as 2 or more of these states are constructed my memory of past interactions becomes completely **unreliable**. Furthermore it's extremely inconvenient for other users, both other lib authors and end user app authors. Lib authors obviously can't (safely) invoke *newPleaseMakeOnlyOneOfMe* and so have to modify their own APIs if they want to make use of my services. So unique state creation becomes the responsibility of end user app authors (the folk ultimately in charge of main). If you really think that this or anything like it is the way to go, I say be very careful what you wish for. And no (before anyone says it), monad transformers of one kind or another won't solve this problem. &gt; b) if so inclined, direct some of your talent and energy towards the genuinely difficult and perhaps interesting problem of designing a proposal with better semantics than "top level &lt;-" or ACIO that can provide the guarantees we actually want rather than enshrining, in essence, a mechanism that generally provides us with guarantees that we don't. I'm afraid I have no idea who the "we/us" are that you're talking about, or why they object to unique "per-process" identity guarantees, or what other guarantees they might be looking for. So it'll be rather difficult for me to address their concerns. As for "we/us" unsafePerformIO hack users, there's nothing wrong with the semantics of the current proposal, it's exactly what's needed. IORefs, MVars etc are perfectly proper **immutable** values (albeit of abstract type) and there's no reason why they should not appear at the top level as long as they are given monomorpic type. Some have disputed this assertion in the past, but if they are indeed not proper values (as alleged) then I fail to see how insisting they must be lambda bound fixes anything. Furthermore, the evidence that they are actually needed in real libs is pretty overwhelming, I would say (either that, or every single one of the offending libs really was written by an idiot). 
indeed :)
If I understand well, this corresponds quite nicely to the intuitive implementation of sum types : a pair with a tag and data (which is a union). So a *sum type* is really a dependent *pair*, and Agda can typecheck it \o/
yay!
And a record is really a dependent function, from field names to values.
Any quick example of how this would work? Basically like a list comprehension? For instance: table = [("a", 1), ("b", 2)] [x + y | x &lt;- lookup "a" table; y &lt;- lookup "c" table] -- Nothing [x + y | x &lt;- lookup "a" table; y &lt;- lookup "b" table] -- Just 3 EDIT: mistakes in my code
That's a great idea! I think the way to go would be to take inspiration from Sage Notebook, including with regards to creating/saving worksheets online. One huge feature would be not only generating diagrams, graphics, plots, etc. but also, e.g., tools to format LaTex equations. This would be great not only for teaching, but for all sorts of exploratory coding -- statistics and data analysis, abstract math, etc.
Yes. Basically list comprehensions are now overloaded so that they work with any monad instead of just lists.
Also: TIL why the element of the unit type is called `tt`.
This nice paper/tutorial on parser combinators and monads use them: http://www.cs.nott.ac.uk/~gmh/monparsing.ps
Any patch in the works that make regular list syntax work for any MonadPlus? I.e. [1, 2] :: Maybe Int == Just 1 Also it would be nice if this would work: [ -x, x | x &lt;- [1..5] ], also for any MonadPlus. That would completely unify list syntax and monad comprehension syntax, where [a, b, c] is just shorthand for [a, b, c | True]. [ 1 | True ] :: Maybe Int already equals Just 1 with this patch.
Are the inductive types of Coq, Matita and Agda equivalent in expressive power, or does Agda add something over Coq, for instance? From briefly browsing the site, it appears no, but perhaps somebody knows something?
There are currently no plans to extend/generalize list syntax even further. Personally, I'm currently working on other projects and soon I'll be studying abroad for one year (after all I'm still a student) - so I won't be able to take on any bigger projects in the near future. I'm sorry. :) Maybe someone else is willing to take on this?
Well, overloaded list syntax is something I've wanted for a long time. But I think tying it to monads only would be a mistake.
Can someone give an example of what I can do now that I couldn't before?
Yeah, Sage was exaclty what i though :-) What dvip did is great. It would be cool to expand his code, and ideally to have a full haskell stack, I know hs-plugins (or it's interpreter now?) and a webserver are not in Haskell Platform, but it would be a compelling reason to have a basic one at least to fire a web interface for ghci :-) Then, to have more than text or SVG, we would need something more than the Show typeclass for different kinds of displayable object (with backends to svg/canvas/latex, with hopefully some inspiration from Conal Elliot to have something animated/interactive. /me wants to hack !
I agree, but I'd also like a unified whole. It would require quite a bit of reshuffling of the current type class hierarchy.
It's purely a change in syntax, with a simple translation to a do-block or plain function calls. Now you can write [e | x &lt;- m1, y &lt;- m2 x, pred x y] instead of do {x &lt;- m1 ; y &lt;- m2 x; guard (pred x y); return e} if you think it makes some code clearer.
If you play the "imprecise exception" games in your denotational semantics, throw is pure and only catch violates referential transparency.
Was it lazily constructed?
There's not a lot there about historical roots of Haskell though. For that there's [Being lazy with class](http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/index.htm) ([ACM](http://doi.acm.org/10.1145/1238844.1238856)) which is a fabulous read and well worth the time, though it doesn't cover recent history.
&gt; so which one was first ? I think that misses the point :) The unityped lambda calculus is Turing complete. Typed lambda calculi all introduce restrictions in one way or another: you can't do that; you can't do it that way; you can't convince me that it's safe to do it that way; etc. So in some sense the untyped lambda calculus is the end point: no restrictions, wheee! But in another important sense it's just the beginning, because without types (or the same thing by another name) we can't really say much about what on earth that blob of symbols is actually doing! Any interesting question about Turing complete systems is undecidable in general; and therefore we do not and cannot ever know those things about the untyped lambda calculus. By adding constraints, we add structure. With simple enough structures we can answer all manner of interesting questions; we can analyze the code and figure out what it does, what it *means*. Of course, simple structure isn't enough for people, it's too restrictive, it's not powerful enough. So people start exploring richer structures. Too much structure and [the thing collapses](http://www.reddit.com/r/programming/comments/gwqa2/the_real_point_of_laziness/c1qz1zn) under its own weight; too little and it doesn't have the nooks and crannies to fill with riches, it's too smooth and unspecified, it gives no insight (e.g., a sphere approximates any shape, but it's rarely the best approximation). So do we start from a lack of meaning and build understanding, or do we start with a blank chunk of marble and slowly chip away parts we don't need?
I had Javascript off for imgur, apparently they've made a change and JS is now required for the image to appear. Without it, I just got a fairly standard "spinny circle" thing forever. I thought it was a joke about laziness, but turning on JS for imgur.com forced the thunk, apparently. Nice mug.
It also supports all features currently supported by parallel list comprehensions and SQL-like comprehensions, which don't translate to do-notation that easily: [ x | x &lt;- [1,1,2,3,2,2], then group by x ] -&gt; [ [1,1], [2,2,2], [3] ] [ x | x &lt;- [1..], then take 5 ] -&gt; [1,2,3,4,5] [ x + y | x &lt;- [1,2] | y &lt;- [3,4] ] -&gt; [4,6] etc. :) *edit: code layout*
Beware: "type theory" and "Type Theory" are not the same thing, note the caps. The former describes the entire field of enquiry which explores possible theories about types; the latter is the name of one particular theory/system, namely the one Per Martin-Löf popularized (and others have extended and reinvented since then). Yes, this is an extremely obnoxious detail, but it's one that often confuses newcomers; e.g., learning MLTT and thinking that applies to all type systems, or learning other type systems and then being confused when people make flagrantly false statements (when interpreted as statements about type theory as a whole, though they're true of MLTT). This is why I prefer to refer to Type Theory as "TT" or "MLTT", to avoid confusion, and because I am interested in the entire field of enquiry and in comparing different systems rather than focusing on just one. ObTangent: There are similar reasons for why ML is called "ML" instead of "metalanguage". ([another dependent-type terminology warning](http://www.reddit.com/r/dependent_types/comments/grzfc/what_is_the_most_intuitive_dependent_type_theory/c1q5ptn))
I see, thanks for the warning, I probably would gotten lost at some point...
Catch violates equational reasoning and monotonicity, sure, but what's the example where it violates RT?
There are lots of approaches -- I wouldn't do plugins myself. Probably better to do something like `hint`, or even better just `mueval` like lambdabot now does. As far as a web interface, I have my own ideas, but I think running over raw `wai` would be the way to go. I wouldn't worry about targeting the platform at all, but just getting a nice modular tool and letting things develop from there. I completely agree about different types of displayable, but we should also consider different modes of display for the same type. You could probably use sqlite for a storage layer.
I mostly agree, at least I think so (if I'm not misunderstood). The building/sculpting analogy is interesting : maybe the exploratory process of building coherent meaning lies somewhere in between both : add this - oh no ! - remove that… On one side a pattern emerges which requires much care and thought, on the other side grounding this care in logic make types, which enable to explore other patterns. For example taking dependant pairs seen in another thread : in some sense every C programmer used a bastardized version of them, unsafely and with a lack of some deep understanding. That's why I keep my thought that it's a chicken and egg problem : I don't really believe untyped or typed has primacy ! And Harper's arguments in "Dynamic languages are static languages" seem deceptive to me (look you need types to think about anything in a language ! look chicken produce eggs !). Not to mention the reduction of data in unt(i)yped language to the problem of dynamics : I mean he lies his arguments on theory, meanwhile he attacks his foes on implementation (the one type should be lambda abstraction in theory). On a side note about this : I wonder how the Y combinator was discovered. Logical deduction from some rules ? recursive types (obviously not) ? profound intuition plus a lot of playing with what can be done with the notation ? …
Next up, taking "fail" out of the Monad class?
Thanks for the pointers ! I will look at lambdabot (and also at Sage) this WE :)
Agda has induction-recursion, which Coq doesn't. But Coq can have an impredicative Set, and Agda can't. I don't know anything about Matita.
That is a nice mug.
I just wanted to mention... The term "type constructor" is typically reserved for 'constructors' at the type level. That is, `Maybe` is a type constructor, and so is `Integer`. In the case of the former, it's a function that doesn't compute, similar to constructors at the value level. However, it is not safe in general to assume that type constructors of this sort are injective. Injectivity of value constructors can, I think, be proved using the eliminator for the relevant type. But the collection of all types is not generally taken to be inductively generated by the 'type constructors,' and has no such elimination rule. Agda used to just give you injectivity of type constructors, but it turns out that is inconsistent with classical logic interpreted in the theory, and with impredicativity. So it is now turned off by default, and it couldn't be added to Coq, for instance. GHC with extensions has both impredicativity and injective type constructors, and you can use that to write a loop (not that you can't do that 7 or 8 other ways).
What's the type of your third example?
ah, perfect. Thank you for your response :)
For imprecise exceptions, I think `catch (throw "x" + throw "y")` is defined to be a nondeterministic choice between `"x"` and `"y"`, so it can have different values throughout the program. Most strict languages would probably nail down the evaluation order too much for this to be a problem, though.
So it's not about what you can do, it's what you can do without `do`? ;)
How about treating it just like String overloading, and providing a new IsList class?
Without a fixed evaluation order, it's nondeterministic. If catch were pure let f = catch (error "A" +error "B") (\(ErrorCall s) -&gt; s) in (f, f) might evaluate to ("A","B") 
Nils Schweinsberg wrote about this on his blog: http://blog.n-sch.de/2010/11/27/fun-with-monad-comprehensions/ and I wrote a little about this on mine too: http://zenzike.com/posts/2010-12-02-comprehensive-monad-notation
Using the list monad it's: Num a =&gt; [a]
I think doliorules is asking (and I am interested in) the meaning of parallel comprehensions in *other* monads.
Oh, it depends on the monad. We introduced two new type classes *Control.Monad.Group* and *Control.Monad.Zip* which are beeing used for either group or parallel statements. Parallel statements would translate like that: [ x+y | x &lt;- listA | y &lt;- listB ] =&gt; do (x,y) &lt;- mzip listA listB return (x+y) where *mzip* is the usual *zip* function for lists.
Parallel comprehensions are supported for monads which are an instance of a typeclass MonadZip (which provides an mzip function). There are some laws MonadZip instances should obey in http://hackage.haskell.org/trac/ghc/ticket/4370#comment:20
That's my plan.
I'm curious why this didn't just use the haskell-src-exts parser, and get a nice parser for a well known language for free.
This looks very interesting, I hope we see some more about this in the future.
Do you have a goal of making this work for Map as well, eg: [("foo", 5), ("bar", 10)] :: Data.Map.Map String Int This would necessarily complicate the typeclass a bit, but could be very convenient.
With the rainbow colours and the [other uses of λ](http://en.wikipedia.org/wiki/LGBTQ_symbols), you're giving out two self-identifiers for the price of one there.
Perhaps, I don't have a design yet. But that's certainly a nice example.
Elaborations like this really are stunning. It is wonderful when you recognize how many places along the way many of us would have settled for an inferior treatment of the problem.
I really like the derivation of the linear-time algorithm in the first section. It showcases how powerful applying equational reasoning to programs can be. It's hard to imagine making the same transformations step-by-step like that in an imperative setting.
That is so amazingly cool.
How would you compile pattern match failures without fail?
Good point. Perhaps the best solutions is to only allow pattern matching when the monad is an instance of MonadPlus, and to have pattern match failures result in mzero.
I wish I had the skills and experience to try that out. *edit* Out of curiosity, what's the internship time frame?
We assume (implicitly) that programs will do something useful. That is, we assume they shouldn't crash or go into infinite loops or withdraw all our money and go on a cruise--- we can't assume they *won't* do these things, it's just that we consider it a bug if they do. Given that we assume this, there are certain programs we must categorize as "bad". Languages with static checking categorize programs as bad at compile-time; whereas dynamically checked languages categorize programs as bad at run-time. Orthogonal to the static/dynamic issue is the issue of whether the language is safe or not (i.e., the checking they do is sufficient to identify all bugs of a certain class). For example, C does type 'checking' statically, but that doesn't ensure any form of safety whatsoever. So far as I understand (and agree with) Harper's point, what he's saying is that given that we have these assumptions about how to categorize programs, the fact of the matter is that all programs are already either good or bad (or whatever other categories we've defined). That is, regardless of how the language is compiled or checked or anything else, the program has a certain behavior when executed. Our categorization of behaviors as good or bad is done a-priori to any actual program implementing that behavior. The behaviors themselves are categorized, broken out into different types of behavior. Hence, every program already has types, because "types" are how we think about what it is that programs do. The only things that can be truly untyped are the ineffable, because if we could describe them then our method of description will be to categorize them as like and unlike other things, and hence they would be of or not of those types. (Of course, "ineffable" is itself a type, namely the type of things which cannot be described other than to describe them as indescribable.) It so happens that some languages have a language-internal notion of "types", so that we can write automated tools that help categorize programs and help to maintain their categorization when manipulating them. But this language-internal notion of typing only captures a fraction of the metalinguistic and universal notion of typifying or categorizing things. The aim of type theory is to figure out how to formalize as much as we can of the universal way that we categorize things, so that we can build better tools from that formalization.
Wow, I really like how they're not looking for programmers or developers or software engineers. They want computer scientists. Thats cool! 
I was thinking about this when I was learning about RDBs. It occurred to me that a table was just a collection of functions, and the whole thing started to make sense when I started thinking of joins as a sort of functor (maps one set of functions to another, in slightly different ways), the link looks really interesting, thanks for posting it.
http://community.schemewiki.org/?latent-type-system http://lambda-the-ultimate.org/node/220#comment-7713 
I'm puzzling over this partly because I don't know the "native" analogue of unwrap, and partly because I'm too lazy/busy/whatever to remember my scheme/ml syntax and fire up guile/drRacket/smlnj and attempt to translate this, but it seems that you need delimited continuations at the least to translate this example? I usually don't mess with continuations, though I read about them, so my intuitions could be dead wrong here.
...because dons has moved on?
You don't need delimited continuations. It's true that the continuations are ultimately delimited here, and we're looking at the results outside the delimiter. But `x` and `y` are already different before that. You could print them out after the `invoke 2 y`, for instance, although that'd be an additional effect. Or you could compare them and notice they are unequal. The main change necessary is something like: data C a = Val a | K (C a -&gt; ()) or, in smlnj, something like: type 'a c = Val of 'a | K of ('a c) cont although I've never really done much of that, so I may be off on the syntax or specific types. `unwrap` is almost identical, and `invoke` returns a unit, or whatever. In Scheme, where you don't have to worry about the types, you can just do something like: (let ((x (call/cc id)) (y (call/cc id))) ...) except you'd have to check if `x` and `y` are procedures or numbers, or something. And if you don't want to have a sequence of statements, you can instead do: (\_ _ v -&gt; v) (invoke 1 x) (invoke 2 y) (unwrap x, unwrap y) Since we're in a strict language.
That is correct. Galois's job requirements include but are not limited to the ability to write faster Haskell code than dons, and evidence that you have posted more Haskell links to Reddit over the past six years than he has.
I wish I could interview there again, but they'll never let me do that after what happened last time.
These constraints are impossible to satisfy. You mean to tell me they haven't run this through a SAT solver?
This is full of much awesome. &gt; The package is full of useful iteratees and enumerators, including basic file and socket processing, parsec-like combinators, string search, zlib/gzip compression, SSL, HTTP, and "loopback" enumerator/iteratee pairs for testing a protocol implementation against itself. Wow. &gt; Super-fast LL(1) parsing is also available through seamless integration with attoparsec. Bonus! --- Note: http://www.scs.stanford.edu/~dm/
It's ok, just next time keep your sleeve rolled down over your tattoo of Bjarne Stroustrup.
I don't guess that there's any chance that this or something like it will go into ghc core?
What for? To replace the current lazy IO functions? I guess it depends if it becomes popular enough, like ByteString.
Owch! Mozilla has made you a mean person.
Yeah. I was thinking "this is awesome", but once I hit the parsing part, I was like "holy shit". Go Mazières!
Or maybe his tattoo said "I &lt;3 Mutable State" ?
I was young and impressionable when I went to Mutable State. If I could do it all over again, I'd take the purity vow.
Now I want a sweatshirt that says "Effect Masking".
...And will they let me intern if I'm an autodidact?
May I add that the documentation looks just great (from a first view). The Tutorial is [directly embedded in main doc](http://hackage.haskell.org/packages/archive/iterIO/0.1/doc/html/Data-IterIO.html). Thank you!
Wow. Looks amazing. 
Ironically, Galois didn't have any degree.
looks really great... but shouldn't it be split into separate packages to avoid unwanted library dependancies (e.g. if I don't need any ssl handling, I don't want to have to link against libssl)
seems, iterIO can also be used as a tutorial for Haddock usage :-)
As clever a reply as that is, I feel I should note that Galois *was* admitted to the somewhat prestigious Ecole Normale in Paris, and that he published a number of academic papers during his stay there. His absence of degree is more due to the fact that he died at an absurdly young age (20 !) than anything else...
rofl @ his website
Looks great, I was just thinking of using happstack for a new project, maybe I'll try to use this and acid-state instead.
Yes, looks very good indeed. Maybe I can finally drop my own private iteratee implementation now :-)
&gt; Hence, every program already has types, because "types" are how we think about what it is that programs do. The only things that can be truly untyped are the ineffable [...] This the gist of the argument, thanks for formulating it so clearly. Harper postulates only the first part which is a *very* powerful argument (I think you're adding the "ineffable" part on your own, even though I agree with it). I appreciate very much this argument : my favorite language is Haskell… Nonetheless, I think Harper is affirming it without proof and your formulation shows why : this argument is not only about logic and types but about also about how we think, about the brain. And here, about the brain, I think cognitive science can already mitigate this view of the brain as a logical and categorical machine only. Can we neglect the connexionist, probabilistic aspects of our brain which can yield what you call the "ineffable" : the fact that we don't think about everything in advance without trial/error/intuition. I think this goes even for programming. &gt; The aim of type theory is to figure out how to formalize as much as we can of the universal way that we categorize things I couldn't agree more here and with the rest of your text. Maybe type theory is especially important precisely because our brain is not as good as categorizing things as *precisely* as we wish, and types helps us refiing our fuzzy ideas.
do it in a monad and remain pure!
Snap &amp; Yesod are enumerator based. Happstack has already announced they plan to switch to using the WAI interface (that Yesod uses) which is enumerator based.
So as far as post-bachelor education, you're asking for a PhD plus "relevant experience" or 14 years experience. How are those even barely equivalent? By that standard, if the goal is to qualify for a job like this one, getting a PhD is a way to save time. I forget sometimes that there are technology jobs out there where having a PhD isn't a bad thing.
I have absolutely no problem with IO looking like shell-scripts.
Perhaps the "relevant experience" is a subtle hint that getting a PhD is a time saver if you want a job at Galois.
s/died/was killed in a duel/
This is nothing but awesome! Although I'm not a huge fan of the terminology, could have been closer to UNIX pipes. Amazing work and amazing post dons! Thanks
And wrote down as much about his theory as he could the night before because he knew that the other guy had a better aim. Actually a good candidate for some reddit memes.
And wrote down as much about his theory as he could the night before because he knew that the other guy had a better aim. Actually a good candidate for some reddit memes.
&gt; Q: Why are the adjunctions of Galois connections backwards? &gt; &gt; A: He never did get the hang of duals. *very sad trombone*
\+1. The world has enough dependency hell already... Also, among others, it depends on the `unix` library, which I guess is a big no-no for windows?!
edwardk also pointed this out to me today: callcc :: ((a -&gt; b) -&gt; a) -&gt; a callcc = undefined costrength :: (Functor f) =&gt; f (Either a b) -&gt; Either (f a) b costrength fe = callcc $ \k -&gt; Left $ fmap (g k) fe where g k (Left x) = x g k (Right y) = k (Right y) unsafePerformIO :: IO a -&gt; a unsafePerformIO = either (\_ -&gt; error "IO Exception") id . costrength . fmap Right If we keep `IO` in a monad, call/cc seems to pull it back out. Edit: I thought more about this, and I'm not sure `unsafePerformIO` really works as advertised. However, `costrength` is still problematic. Essentially, `costrength io` creates a `Left io'` such that if you ever run `io'` as part of your main `IO` action, execution will jump to the pure point where the `Either` is evaluated, and the effects of `io` will happen, yielding a 'pure' `Right x`.
I have to admit that that is a pretty cool way to go. Poor Abel had tuberculosis or something...*lame*
Yours is the majority opinion, I believe. For the minority one, see the comments in the article, specifically: [http://conal.net/blog/posts/notions-of-purity-in-haskell/](http://conal.net/blog/posts/notions-of-purity-in-haskell/)
Yes, please. That is precisely the right solution, and used to be the process used in Haskell 1.4. Except there there was even a separate `MonadZero` class. You can also detect matches like "`(a,b) &lt;- m`" that cannot fail, and not require more than `Monad`. This was also done in 1.4, but the whole thing was dubbed undesirable for some reason, and we ended up with the `fail` mess.
Is that the canonical way of doing strict types?
&gt; James Cook: I think Haskell questions on SO tend to the opposite extreme; no matter how poorly thought-out the question, the Haskell community will descend on it like a swarm of helpful piranhas. This is out of context, but anyway it seems to be true from what I've seen. So different from the Lisp community.
What happens in the monad...stays in the monad?
I actually emailed asking about the internships. I got this reply &gt; At time we do not have any internships available. Since the reply didn't say "for you", I'm assuming it's a general statement and not restricted to my inadequacies. Either way, sadface. :(
You have made a powerful enemy today, Galois!!!
Of relevance, since Okasaki's treatment omits the hard stuff: http://matt.might.net/articles/red-black-delete/
According to wikipedia [he got expelled](http://en.wikipedia.org/wiki/%C3%89variste_Galois#Final_days)..
Isn't C-H a rather poor choice for default?
Another interesting implementation by yairchu, with type-enforced RBTree invariants: https://github.com/yairchu/red-black-tree (Includes RBTree and AVLTree, both with enforced invariants).
&gt; The specific meaning and utility of the combinators in the reactive-banana library will be explained subsequently Yes, please! As a relative newbie at Haskell, I find FRP to be both intriguing and hard to approach.
probably! I'm open to suggestions.
We had an interesting discussion about the design of CmdArgs back in 2008, and again on SO today, http://stackoverflow.com/questions/5920200/how-to-prevent-common-sub-expression-elimination-cse-with-ghc 
This looks nice but I wonder if it may be easier just to create snippets for use with [snipMate](http://www.vim.org/scripts/script.php?script_id=2540).
I released the next part [see here on reddit](http://www.reddit.com/r/haskell/comments/h6ixg/online_svg_figures_with_haskell/)
You are absolute right about making online execution more clear! I posted [a new reddit entry](http://www.reddit.com/r/haskell/comments/h6ixg/online_svg_figures_with_haskell/), I hope it clarifies.
Sorry the submitted link was gone somehow, thats why it is self.haskell [instead of this](http://pnyf.inf.elte.hu/fp/Diagrams_en.xml)
Chances are that you won't be able to learn FRP before you learn idiomatic Haskell. :-) Have a look at Paul Hudak's book [The Haskell School of Expression][1], which introduces both Haskell and FRP (as "Functional Animation Language"). [1]: http://plucky.cs.yale.edu/soe/index.htm
Hi Péter - is there any reason you didn't use Conal Elliott's vector-space for scalar multiply etc.? Also point subtraction doesn't have a geometric meaning, in affine geometry at least, you really need to add vectors into the mix. Otherwise nice work! 
This has convinced me not to use CmdArgs
The hands-on examples are awesome! How exactly does your interpreter work? Any chance to see it on hackage?
Yes!
I don't know really. I rebound it to C-B, next to the normal completion.
Hi Stephen, I took a look at vector-space, migration seems easy but note that these diagrams are for first-year BSc students here in Budapest!
I am going to release the whole stuff and write a blog entry about how it works! Basically it is snap-server + hint + pandoc + xhtml + data-pprint (released 2 weeks ago) + dia-functions (released now) + gluing code + a few raws of javascript. Additionally, 2 semesters' course material is available in this system but not yet in English...
It's like "Logo 2011"!!! So cool!
I was going to say something similar.
Maybe it's time to resurrect my old command-line library.
Awesome! And I didn't know about hint, nice! This is good stuff; I'm getting ideas about making a course as well. :-)
I'm feeling pretty good about the design decisions here, but I'm still having trouble coming up with *names*. If anyone has some recommendations, please let me know.
Names for what?
I am using only barebones of yesod, completely bypassing such complicated and overengineered packages as forms, persistence etc. From my experience i know, if solution is too complex, it is not gonna last. And in the end simple marshalling data from raw sql resultsets to forms and back with a few helper functions works quite good for me. I'll be following the progress of forms package of course, but i wont' be holding my breath. There are other places for improvement in yesod that are of much more intrest to me. For example - sessionless static files and support for caching (to get rid of nginx) - improved access controll mechanism. Checking every single url in a module that can easily have 50 of them is not really fun. It would be great if there would be a way to give/deny access to the whole module with all its urls grouped by some criteria. - Initial settings (like approot, static root etc) do not belong to complilation phase. They should reside in some ini file. Why on earth would i want a different executable for my production and testing env ? 
using haskell, i can code while being drunk. now, try that with c++... :)
We are hoping for input from the community that will allow for more elegant (less complex) solutions. support for caching (to get rid of nginx) This was merged in today. Initial settings (like approot, static root etc) do not belong to complilation phase. They should reside in some ini file. Why on earth would i want different executable for my production and testing env ? I initially had a similar view. However, I now tend to think that the more that can be compiled in (all things being equal) the better. There is no need to deploy an extra internal file, and there are more static guarantees. There shouldn't be anything stopping you from loading configuration files now. In fact, I do load some configuration files in my Yesod app. We would welcome a patch from you that would better integrate that with the framework if that is your complaint. But the problem is that we have static guarantees now- if you want to have the same static guarantees from a configuration file you will need to do some programming in Template Haskell. 
From someone who should not be taken seriously because he is in the middle of a second attempt to grok it, the first being less then successful: - Because it is quintessential FP, not half-hearted like some of the alternatives. Which is I think good for learning the concepts. When you know the "proper" way to do things, you are in a better position when you are solving your problems using some other less demanding languages.
CmdArgs has two variants - pure and impure. If you use the pure variant the syntax is a bit uglier, but still nicer than any other command line processor. Both have exactly the same features. There is no requirement to use any impure code, unless you choose to. For full details see: http://neilmitchell.blogspot.com/2011/05/cmdargs-is-not-dangerous.html Update: Fix the typo, as pointed out by mosha48
Human beings suck at programming. We need all help we can get. The best help computers can provide us is a type system. Haskell has a type system that is much more powerful than in any other static mainstream language (c++, java, c#), and of course blows out of the water any dynamic language (Python, Ruby, lisps). Haskell offers the best help you can get in programming. 
Careful going down that road... you'll end up switching to Agda instead!
I think a lot of the current datatypes are misnamed. Some examples that came up in the post are FieldProfile and FieldInfo.
Is the impure device primarily to support the nicer syntax? Could a meta-programming approach (e.g quasi-quoting) be used to give the nice syntax, without the semantic concerns?
Haha, I hear that path might not be so bad ;)
I wrote a blog post with I'm vain enough to think gives a pretty good partial answer. http://cdsmith.wordpress.com/2011/03/13/haskells-niche-hard-problems/
I don't really want to get into a debate about overengineering and complexity. I *did*, however, want to respond to your last three points: * As Greg mentioned, he just wrote this code. * Access control: I *have* written code that denies/grants access to hundreds of routes, and the system scaled just fine for me (with a few helper functions of course). If you have a better system in mind, I'd be happy to implement it. * And this is the big one. You __absolutely__ want to use different executables for production and testing because of the *FileDebug stuff. But as Greg points out, you can easily use an INI file instead. Just put the settings in your foundation.
yes, neil already has some plans for that. I also started a parser that works backward from help output to construct the CmdArgs data structures. https://github.com/gregwebs/cmdargs-help-haskell The help parser is written, but I haven't had the chance to finish this project off yet.
Here's a list of things I like about Haskell: 1. The GHC compiler seems to have this magical way of compensating for the silly mistakes I make. In the end the program the compiler rubber-stamps is usually the one I would have written in the first place if I'd been smarter. 2. As a consequence of (1), I can write the stupid version of my program, refactor and be reasonably comfortable that I haven't broken anything. Even in larger programs. 3. I can use Hoogle/Hayoo to search for a function by type signature even if I don't know it's name. For example, I knew that function 'swap' (swap (a,b) = (b,a)) had to exist so I searched Hoogle for '(a,b)-&gt;(b,a)' and voila! 4. Cabal is easiest package manager I've used since Debian repos. 5. The Haskell Platform makes it extremely easy to get up and running. 6. Haskell is the only language I've worked with that is read-only. Meaning that if I can get it to compile it's much more likely that I can read and understand the source down the road. 7. Type and data families are awesome. They allow client modules to extend datatypes. 
couldn't agree more, until i learned undefined
&gt; My personal take: [...] I think the appeal of Haskell has two general sides to it, and that's an excellent summary of one side: namely, the correctness and logic side. The other side, which I tend to feel is neglected a lot in comparison, is the convenience of programming in Haskell. So I'd add these points to balance things out: * Despite very strong static typing, it doesn't get in the way due to nearly-complete inference. * It's easy to define your own notation; there's not much unnecessary syntax and nice flexibility for picking operators, using infix notation for functions, and overloading the built-in syntax that is there (e.g., do notation is for monads, not I/O) * Laziness and the deforestation / fusion transformations that are possible as a result allow you to write more declarative code without worrying about termination or building large intermediate data structures that would be a problem in other languages. * Because Haskell can directly express concepts like functors, applicative functors, monads, and Alternative; you get to learn programming models once rather than figure out how each library reimplemented them. I think what's really appealing is not just the strong logical background, or just the convenience, but the unique combination of the two into a coherent whole.
You should read CmdArgs is not dangerous: http://neilmitchell.blogspot.com/2011/05/cmdargs-is-not-dangerous.html
Yes, totally for a nicer syntax. CmdArgs already supports two alternative syntaxes - one is pure, one is impure, and there is a straightforward translation between them. CmdArgs does not require impurity. See http://neilmitchell.blogspot.com/2011/05/cmdargs-is-not-dangerous.html And yes, I have plans for QuasiQuoting: http://code.google.com/p/ndmitchell/issues/detail?id=334 - I want an even nicer syntax, and guaranteed purity.
Looks very cool, I'll add a link to the CmdArgs home page when I next get to a machine with SSH.
&gt; If you use the impure variants the syntax is a bit uglier, Didn't you mean *If you use the **pure** variants the syntax is a bit uglier*
If other programmers aren't interested in Haskell they they _shouldn't_ use it. Maybe they will find it interesting later on, maybe they won't - both paths are fine. If we must have proselytization, then let the code speak. What go me interested in Haskell was Conal Elliott's article in Dr Dobb's journal years ago - I just couldn't believe how you could do animations in two or three lines of code. 
Yes, we keep discussing it - I'd love to make CmdArgs pure, just so I can stop Lennart complaining about it! (Of course, he's totally right - code that presents an impure interface is a bad idea.) I regularly get to defend the choices made in CmdArgs, and I think it's the very strong pressure to "be safe" that means Haskell libraries are the quality they are.
Yes, fixed - thanks!
For me it's mainly two reasons: 1. I love being told about (most of) my bugs at compile-time. 2. It has introduced me to a whole other way of thinking about programming, which I'm sure has make me a better programmer in other languages too. (Although I have to admit I sometimes cringe when I think of how much simpler i would have done something in Haskell, when I am forced to use another language)
Shameless plug: http://hackage.haskell.org/package/placeholders for a version of undefined that yells at you when you forget to remove it.
Oh, i am not referring to FileDebug. I am talking about data (approot, staticroot, database connection string etc.) Besides we do not do development in test env. We do it on local devs pcs. Test env. is for users, BAs, for testing, training etc. So no FileDebug there. &gt;hundreds of routes, and the system scaled just fine for me My concern is not scaling. It's a lot of manual work. Plus prone to errors. Compare two approaches: Yesod: Access control for every url must be explicitly coded. Snap: Access control is set once on part of the url, and everything down that path falls under it. No manual coding. Yesod: Developer can simply forget about access control, when new url is added to module much later. Snap: No matter who and when adds the url, it will be automatically covered by access control for that path of url. Yes i know that this approach is tied to url paths and type-safe urls are lost. That's why i asked maybe there can be a way to group urls by some other means (not paths, modules ?).
Free source, free ideas, I am happy if you use them! :)
This isn't Haskell code, is it? I understand that this is Agda code, right? Are there any extensions for GHC in which I can do this in Haskell? 
I'm taking a second stab at Haskell after giving up on my first go. In the interval I've been learning Scala and sort of getting used to at least *starting* to think a little functionally. Now I'm back to Haskell again and I'm trying to learn it so I can do things properly. I'm getting to the point now where I can read type signatures and understand what they're supposed to represent. I'm not sure if any of the responses in here have mentioned why Haskell should be used for a project or not. Was that your original question that you wanted some answers for? Why should I use Haskell for my next project? Or was your question Why do you use Haskell (I'm making a project to collate these answers)? Whatever the question is I'm looking forward to what you come up with. I'd say from a learning perspective, I'm really enjoying the silly little aspects of creating functions that do things in the standard library. Like for example, writing your own version of Map, Filter, Zip, Drop is kinda fun. Pointless, but fun to see how these things work. 
BTW: That reddit logo rocks! :)
Ah, I can see where that question might be a little ambiguous. Honestly, an answer for either works, but I was looking for "why do you use Haskell" (and I have a project in mind involving these answers). That said, "why do you use Haskell for a project" I think is still sufficient–a lot of people I meet like the idea of Haskell (and that in itself I think is a great reason to learn it), but at the very least for pure conversation, they want to hear what some of the benefits are. I love a lot of things about Haskell and so it can be difficult sometimes to articulate why it is I enjoy it, so I'm hoping for two benefits of this question: * for my own articulation of a response to that question, and * for the hopes of this project should it work out well enough.
Thanks Deech for the reply. I don't want to go through point-by-point because I think we're on the same page on the bulk of those (especially Hoogle/Hayoo–what a neat aspect of the type system!). Though, I do have to say, I've never found cabal to be especially easy, but perhaps that's because package managers historically hate me ;) I suppose the syntax of it is simple, but dependencies in it have often lead to headaches, and particularly when trying to upgrade something.
Thanks Chris for the list of external resources–I had seen some of those, but there were quite a few I had missed in my searching as well. Appreciate the personal take too–the bullet points like this are really what I'm going after, trying to see where everyone stands on the benefits. &gt; The community combines pointy headed theory with pragmatism, and being jolly I need to highlight that last point, though, because it really deserves some recognition, and it is one of the things that I try to tell everyone about. When I came to Haskell and lurked in the IRC channel, I was beyond surprised at just how *awesomely nice* everyone there was, willing to answer a beginner question multiple times, multiple ways, and willing to question the status quo to see if the current way truly as the best. So, props to the rest of the community on this one–keep it up!
&gt; Because Haskell can directly express concepts like functors, applicative functors, monads, and Alternative; you get to learn programming models once rather than figure out how each library reimplemented them. Excellent point–has been one of the great time savers in the language. I find myself less and less needing to look at documentation or source of a module for a specific understanding of it because the type signature and the concepts employed tell so much about the program, so I can get right to some productive coding.
Well, Haskell will not last forever...
&gt; 
don't forget to add the caveat that it doesn't yet work if you link to it :)
This! ^ Learning haskell you get to know some amazing people! It's difficult to find a group of intelligent and productive people as willing to teach newcomers as haskellers from irc, reddit and stack overflow. It's also hard to find technical blogs that go into deep intellectual stuff as haskell blogs do. Planet Haskell is a freaking treasure trove of beautiful texts!!! I'm always amazed by the type of kind and brilliant people I find answering my dumb questions on haskell... It's very difficult to find a big group of brilliant people capable of such patience. These people even have the patience to take the time to explain concepts from computer science I would never understand otherwise (I'm just a physicist... all I know about CS is what some haskellers teached me.). The language is great and everything, but there are tons of great languages with nice features. If it wasn't for my perception of the excellence of the people I see using Haskell and the kind of deep stuff they talk about and the warm and welcoming atmosphere I felt around them, I would never learn it. 
Certainly that is true about the difficulties of managing dependencies in Cabal. In the end, though, I don't think that's an unusual weakness of Cabal, so much as it is another of those difficult problems that Haskell tries to solve and others just ignore. What I mean is: Haskell is largely unique in the programming language community for the number of very small (as little as a half dozen lines of code) libraries that are created and packaged and *used*; and also the massive amount of code reuse that goes on between these libraries. This means that a lot of packages have substantial and deep dependency trees. Cabal isn't perfect, and there's been a lot of discussion on ways it can improve; but it's rare to find a similar system that comes anywhere close. It's miles above the manual package management that goes on in most conventional programming languages like Java and C++... it would be a nightmare to have a project that depends on 50 to 75 libraries there; it's starting to look routine in the Haskell world. So I'd definitely say that Cabal is an awesome tool... sadly, we are starting to ask it to do so much that its limitations sometimes show through anyway.
There's certainly a need for some evangelism, though... if no one sets out to publicize the language and share its advantages, then programmers won't hear about it, and won't be given the choice of whether to like it or not. And for a language with a substantial barrier to entry, there's also sometimes a need to strengthen the motivation for people who *have* decided to learn.
This is a rather good point–I'd never actually thought about it that way in how I compared it to the systems of other languages. I guess its power was being underscored by the minor frustration that I've faced before, but it truly is a great alternative to the other options and, like you mentioned, a necessity as so many dependencies are included. Thanks for the insight.
Why use a safe language and then do something like this? I'd almost feel better using getopt() in C.
That looks beautiful! Can't wait to try it out...
When I said scale, I meant manual work wise. I basically had five functions, one for each type of permissions check. And then each time I added a new route, I made sure to call the appropriate function. It's not really so different than having a grouping system. Plus, how exactly do you handle permissions where the permission depends on the parameter? For example, you have a blog post with a post ID, and only grant write access to the author. You'll need to explicit code both sets of routes in Snap, right? I just don't see such a system saving you many keystrokes, at the cost of making a bunch of assumptions about URL structure.
Yes, i too have cases where different groups access module with different rights. But i think the number of urls that require such treatment are much less than those with general access/deny policy. Anyway, i am using authentication the yesod way. And even though i am used to different model, i hope it grows on me over the time. 
If it was practical to write "real programs" in, why not? *checks Amazon for "*Real World Agda*"...*
Agreed, I'm very impressed by the interactive examples!
I often start out with hopefully good names and describe them in a concise comment. Most of the time i find that i can use those concise comments for the actual names instead. So what are the names you want to improve and how would you describe them concisely?
See also: old functional pearl "Pickler combinators".
My C++ looks like I was drunk. Does that count?
BEWARE. There be dragons here! Anything other than plain text files have some sort of encoding/escaping scheme for their various bits and these "really simple" solutions, by abstracting away where the templated bits of text are going, invite people to be ignorant of those schemes and create cross-something-attacks. I have lost count of the number of times I've seen these schemes grotesquely misused in the real world; the case of the user input being inserted into a Erlang configuration file with no escaping, the case of the user input being inserted into a "config file" without realizing one of the fields was a shell command because that fact had been abstracted away, and so on, half a dozen others. This is one of the perennial tempting programming projects where it is easy to inadvertently actually bring negative value to the user by detaching them from what's really going on, while bringing them not all that much value. I don't mean this as discouragement; I mean it as warning. But there is a potential solution: A library that does something similar but rigidly requires a specification of how to process the input in the replacement clause. By "rigidly specify" I mean that there literally _is_ no default, you must specify _everything_. If you want to literally slap something in unencoded, you must say so `&lt;&lt;&lt;username|unencoded&gt;&gt;&gt;` or something, the idea being that typing `|unencoded` ought to make any halfway serious programmer pause for a moment and think (and if they don't, well, there's only so much you can do in the end). Yes, it's more complicated, but at least if a user screws up their encoding specification it's _their_ fault and not yours. A library of common encoding functions can be provided as well ("only ASCII", "shell safe", "no newlines or control chars", etc), to make it even more useful out-of-the-box. (Also, the vast bulk of HTML templating solutions have the exact same problems, specialized to their domain. I mean in general, Haskell templating solutions are unsurprisingly well above average on this front. It's unsurprising because once you train your brain to work with Haskell types, the problems with HTML templates in most other languages are blindingly obvious instead of insidiously subtle.)
Hamlet-style templates are designed to deal with this issue at compile time. For a html template, you must write a ToHtml instance for the types that you want to insert. We have now abstracted out the code so one can create a new template overlay on any type of file (for each type of file you need to define your conversion instances, and you can modify variable substitution syntax). We are undergoing some re-naming and shuffling now to make it more accessible for new template type authors. But I hadn't even considered a command line interface for plain text files.
I would like to emphasize the point of type signatures telling you a lot about the program. I recently used Control.Concurrent.Chan for the first time. I looked at the [readChan](http://www.haskell.org/ghc/docs/latest/html/libraries/base-4.3.1.0/Control-Concurrent-Chan.html#v:readChan) function and was wondering whether it was a blocking or non-blocking read. For a moment I was annoyed that the documentation didn't answer that question. But then I realized that the type signature did! A non-blocking read would have to return a Maybe, Either, or something similar. This isn't an earth-shattering realization, but even after several years of Haskell experience it still surprised me because it's so different from what you get in most mainstream languages.
Hstringtemplate allows nested templates and setting encoders/escapers on a per-group basis. It would make sense to construct something like newt on top of it or another full-featured template solution to get that and other features out of the box, rather than handrolling yet another substitution syntax.
[My most recent tirade](http://winterkoninkje.dreamwidth.org/72105.html)
Real World Agda is just the FFI to Haskell :-) (cf. https://github.com/larrytheliquid/Lemmachine/blob/master/src/Lemmachine/Runner.agda)
Newbie haskeller here - thanks for posting that. I especially liked this part: "Posta makes extensive use of partial evaluation for improving performance; e.g., lifting computations out of loops." When I read that sentence my mind exploded a little. That is a very specific and very cool thing about partial evaluation that had never even entered my mind before. Thanks! :D
&gt; Haskell is largely unique in the programming language community for the number of very small (as little as a half dozen lines of code) libraries that are created and packaged and used I think this comes down to language design. Non-strict semantics are better for algorithm composition. So there is little lost in just implementing idealized solutions to the general problem, composing them, and letting the asymptotics work themselves out.
Nice!
&gt; Expression-present equivalent exchange lol @ FMA reference
Calculation cannot occur without destructuring something in exchange. To force evaluation requires entering a case statement. In those days of reading core, we we really believed that to be the world's one, and only, truth...
I'm considering writing a Haskell symposium paper about it actually. I wonder if it'd be accepted...
Wow, I need to read reddit more frequently. Sclv is correct: if your compiler pass is accepted by the Coq typechecker, you are guaranteed that it will never produce ill-typed Core output. Specifically: - Every well-Coq-typed HaskStrong term is the AST of a well-Haskell-typed Haskell expression (Core, technically). Coq functions from HaskStrong to HaskStrong are guaranteed to preserve well-typedness. - Every well-Coq-typed HaskProof term is a well-formed natural deduction typing proof of a Haskell term. Formalizing the small-step dynamic semantics is actually pretty easy -- much easier than the type-correctness-guaranteeing representation; I could probably pull it off in a weekend. Unfortunately, actually proving that a compiler pass preserves the small-step semantics tends to be really really difficult, so I'm not sure it would get a whole lot of use. - a
Great. For the first problem, is it possible to solve it without a conditional? \y -&gt; if y &lt; 0 then sin (y+pi) else sin y
I haven't tried it but it looks like `sin . abs` would work.
It depends a bit on what you consider a conditional. I don't know how to get the behaviour of abs without using a conditional at some point.
 abs x = sqrt (x^2)
Aha, so obvious once you see it!
OK, thx, I'm so stupid..
Not as efficient as a conditional tho
I remember these sorts of exercises from some books I had on math for my old hp48. Plotting equations is such an immediate and fun way to learn both math and programming...
That actually strikes me as not-necessarily-true. In general, branches on modern CPUs are uncommonly expensive, such that practically any reasonable alternative that doesn't touch the program counter can be more efficient. And since floating point operations are generally implemented in dedicated hardware that may otherwise be idle... If you're sure of what you said, I believe you. But if it was just based on a vague feeling that square root calculations are hard, then I might hold off judgement.
Your post mentions an important point that hadn't been mentioned here yet: Haskell's purity makes it a powerful platform for parallelism. That is especially important given the current direction in hardware design.
Except that at the same time, even the original x87 instructions (http://en.wikibooks.org/wiki/X86_Assembly/Floating_Point#Original_8087_instructions) included a FABS instruction. I assume it works on bit twiddling.
This is really cool! Thanks for sharing it. I see that this is a mature library that you have been building on for a while, so it's probably too late to change any of the basic types. But I would have preferred a different order of the arguments for many of the functions. If more of them ended with ... -&gt; Diagram -&gt; Diagram it would be easier to build up diagram expressions. Also, it's a bit disappointing that the `render` function in the SVG backend is in IO. I see that your rendering is actually pure, which is nice. The only reason you are in IO is for a time limit and for catching errors. Seems like it shouldn't be so hard to abstract that out somehow. Then this library could have many more applications.
I'm not sure about what I said at all. :-)
Okay, true... so I was comparing sqrt (x*x) with something involving an if statement; but you're right, neither is really the best idea!
I don't see why Haskell should be particularly bad for beginners when comparing it to C and C++. At uni we start out with SML, mainly because it's type system is simpler and the code is evaluated strictly. But starting out with a functional language works great! :) Haskell is a nice language with an elegant computation model. Your main problem is likely finding a Haskell introduction that doesn't assume you already know C/Java/Python. [Learn You a Haskell](http://learnyouahaskell.com/) is quite good, but it does make references to imperative languages (like C/Java) more than once. However, it does a good job at explaining most of the language starting from a beginner's level.
If I remember my floating point stuff correctly, that would be a very simple xor operation to flip a single bit.
I could debate at length whether starting with Haskell or Python is a better idea, but there's no question in my mind that you should _not_ start with C++. Bad, bad, bad idea. Too much C++-specific stuff and not enough general lessons that you can take elsewhere, and too much work to get to the point that you can do something useful. I'm ready to declare it a legacy language. Everybody isn't there yet but we're well down the path on that. C is dubious. It used to be a good choice, because you were "so close to the machine", but that's not the advantage it used to be. While at least the lessons C will teach still apply (Haskell still allocates and deallocates memory, even if you can't see it directly, for instance), it still forces you to learn about these things at the whims of the machine, rather than in an order suitable for learning. At least the time you've spent on C will continue to be useful, but I'd move on to Haskell or Python sooner rather than later. There's a world of difference between learning the syntax and basic mechanics of the language and learning "best practices" to actually write programs in that language, and while learning the basic mechanics of C is a good idea, spending enough time with it to learn the best practices is a waste of time unless you're actually going to use it.
Consider Lisp or Scheme. Both have lesser machineries; such as type system, lazy-evaluation, purity, and gulp! monads; and thus allowing you to focus on the fundamentals of data structure and algorithms first. The other stuffs that I've mentioned are important too, but they can be overwhelming for a "begginer."
It is. But on modern architectures that could incur a very expensive move between integer and floating point registers. 
I hope so, because I'll be spending the next school year teaching Haskell programming to middle school aged (aged 12 to 13) kids at a local school! I decided to go that direction because I think Haskell is great as a programming language for beginners. In fact, it's probably a lot easier for people who don't come in with the expectation that their experience in C, Python, etc. will be immediately useful. And there are a lot of advantages to learning a declarative programming model first. The danger here is confusing math in Haskell, with everyday programming in Haskell. So you'll want to take a moment to understand that Haskell is *both* a practical programming language *and* a mathematical playground... and you'll want to be ruthless with yourself in agreeing to just ignore the second group at the beginning, and *not* let yourself give up or get discouraged by those who in the latter group. For example, if you hear the word "comonad", *stop* *listening*. You do *not* need to know what a comonad is to write programs in Haskell. Indeed, I'm scared sometimes to point beginners to the #haskell IRC channel or some similar resources, because even though people there can be very helpful and friendly, I've had the experience a half dozen or so times now of having the people I'm trying to help suddenly become rather discouraged and give up because they found themselves unable to comprehend a sentence of what was being said in sources like that. So while it's likely *not* a good idea to avoid the IRC channel (and a lot of the articles posted here), which can occasionally be a good resource, just be very resolute that you won't let that happen to you.
I think Haskell is "good" for beginners, in the sense that it teaches good programming principles and that it has an easy to follow logical structure. However, the way the language is designed, common i/o activities usually are one of the last things you learn; consequently, it will likely take several weeks of learning before you'll be able to create any kind of useful program. If you are okay with this, then go for it and good luck! Also, be sure to join the Haskell-beginners mailing list.
Actually, that would be an AND op to clear a single bit.
&gt; allow you to focus on the fundamentals of data structure and algorithms first Without algebraic data types and pattern matching? Wadler has written an essay on that.
`sqrt (x^2)` may not be less efficient. But it could be less correct, inasmuch as you want: x == abs x || x == - abs x
Yes, I know it's hard to believe, but the rumor is true that one can do data structure and algorithm without algebraic data types and pattern matching. :) Even the cult some people called MIT has taught years of programming under such heresy!
XOR to flip it, AND to clear it, you're right
\*shudders\*
&gt; but they can be overwhelming for a "begginer." There are nicer ways to correct a typo/spelling error...
But is it a good way to learn the fundamentals? 
On a modern architecture you would be using SSE registers, so at worst you would spend an extra cycle shuffling data between integer and fp SSE execution units. Agner Fog's tables say ANDPS runs in the FP unit for the architectures I checked, and anyways doesn't indicate that Sandy Bridge incurs any additional latency for moving data between functional units.
What does "language-neutral" mean in the context of FP?
Do you have a link?
If you're familiar with [Lebesgue integration](http://en.wikipedia.org/wiki/Lebesgue_integration), then you would know that while it is much superior in theory than the classical Riemannian integral, nobody teaches Lebesgue integration first because of its heavy requirement on measure theory, topology, and functional analysis.
Presumably, where the subject is "functional programming" rather than a specific language like Haskell or ML. Chris Okasaki's "Purely Functional Data Structures" is the obvious tip...
[Chris Okasaki - Purely Functional Data Structures](http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504)? 
Although the inline examples are in Standard ML and there is an appendix that includes Haskell, I think Chris Okasaki's "[Purely Functional Data Structures](http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504/)" could be considered a language neutral book on functional data structures. I'd highly recommend it.
&gt; Your main problem is likely finding a Haskell introduction that doesn't assume you already know C/Java/Python. Graham Hutton's "Programming in Haskell" is supposed to be one of the best Haskell introductions out there, and it doesn't require any previous exposure to programming.
Yes that's what I meant by language-neutral. Or subjects that are related to FP in the same way UML is related to OOP.
&gt; Does anyone believe that the difference between the Lebesgue and Riemann integrals can have physical significance, and that whether say, an airplane would or would not fly could depend on this difference? If such were claimed, I should not care to fly in that plane. -- Richard Hamming On the other hand, while I have no problems with ticket pricing engines written in Lisp, would you *fly in a plane* written in a similar fashion?
Does anyone know if "Purely Functional Data Structures" is readable on the Kindle? I know it is _available_ on the Kindle, but does it properly format source code etc for reading? I recently bought "The Reasoned Schemer" for the Kindle and the major portion of the book is set in tiny font which overrides the Kindle's font size adjustment ability. The book is practically unreadable.
I have it on my Kindle and it displays just fine.
Awesome. Totally buying this. 
The original version of Bird and Wadler, Introduction to Functional Programming.
FP is related to FP in the same way that UML is related to OOP.
is there something you don't like about the 2nd edition?
[The Algebra of Programming](http://www.amazon.com/Algebra-Programming-Prentice-Hall-International-Computer/dp/013507245X), if you can find it.
Thanks for the feedback! I am happy to modify the interface of the library if it improves its usability! I am wondering whether changing the argument orders help. The constructors behind has exactly the ordering you propose, but I changed that in the interface because it seemed better: circle 1 `move` (1,1) `scale` 2 vs scale 2 $ move (1,1) $ circle 1 Is the second better? About the render function: Yes, catching errors and timeout is not possible in pure code and I think that's right. Maybe one could use a dedicated monad just for catching error (already exists I think) and timeout. Would that help?
I'll put in a word for Abelson and Sussman's classic _Structure and Interpretation of Computer Programs_. Yes, it's in Scheme, but it's not *about* Scheme.
So, you won't fly in any plane unless its software is written in a language that supports algebraic data types and pattern matching?
The 2nd version was too mainstream.
racket has some amount of pattern matching. I don't think it has ADTs though. http://docs.racket-lang.org/reference/match.html
Sure -- the OP didn't mention types, though.
seems to be available for free as a pdf: http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf
Depending on how language-neutral you want it to be, there's [Lazy Functional Languages](http://www.amazon.com/Lazy-Functional-Languages-Interpretation-Compilation/dp/0262521601) by Geoffrey Burn. Clearly the focus is on lazy FP, though more of the ideas cross apply to strict FP than you might imagine. Also, if you want a good introduction to domain theory, that's the book for you.
I'm working through the later parts (on derivation) of this now. Great book.
I think that's the phd thesis itself. He expanded and edited it when he published the book
That's his thesis, which was the basis for the book, but doesn't have exactly the same content. The book's a bit more polished and in my view a better read.
Yes yes. #haskell is very generous with my dumb questions, which makes me very appreciative. I try to talk up the community whenever I talk up the language.
I prefer the first edition, and the second edition mentions Haskell in the title so I'd say it's less neutral.
I enjoyed ["Functional Programming: Practice and Theory"](http://www.amazon.com/Functional-Programming-Practice-Bruce-Maclennan/dp/0201137445) by Bruce MacLennan. It's from 1990. No monads, but it covers the fundamentals like Sets, Trees, HOF, infinite data structures, 2 chapters on lambda calculus and much more.
Someone tried telling me something about once I go down that path forever will it dominate my destiny. Bah, also piffle.
Purely Functional Data Structures talks about how to reason about data structures and asymptotics in a functional setting. His is pretty much the only treatise on this subtopic, which is important in analysis of algorithms, so you pretty much have to round trip through his book at some point. Richard Bird's Pearls of Functional Algorithm Design talks mostly about how to _think_ like a functional programmer. It has a laser focus on equational reasoning, and how to exploit laws and invariants to effectively calculate more efficient programs. It is much newer and hasn't had time to evolve into a classic yet, but given Bird's long history with the community and the success of his earlier material, it is only a matter of time.
You definitely should come over to the #haskell-game IRC channel on freenode, a lot of people are very interested in these topics.
Pity you can't do the same with integers.
How about *has types*? I'll take Ada or even C over an untyped language for safety-critical applications. http://en.wikipedia.org/wiki/Ariane_5_Flight_501 
You should look at the games made in Haskell already. Raincat was made by students and it's pretty cool. Another one is Frag, it's a quake-like FPS.
Also, Nikki and the Robots [appears to be open-source](http://joyridelabs.de/game/code/). Might be worth having a look at.
I don't think I found it due to evangelism. Evangelism gives me something to parrot to people when I need to justify my choices. I'm too new to have the experience to compare to, so I just trust those who do. I came to Haskell as a natural progression from scheme to Lisp to Haskell. The secret truth is that I choose Haskell because it's fun. Even when it drives me crazy it's fun. Can't exactly tell the Senior Engineer that though. 
That's fair. [Boost's Units](http://www.boost.org/doc/libs/1_46_0/doc/html/boost_units.html) is a shining example how types can be used to guard against even mistakes in implementations of mathematical formula.
[Pearls of Functional Algorithm Design](http://www.amazon.com/Pearls-Functional-Algorithm-Design-Richard/dp/0521513383/ref=pd_sim_b_2) is a pretty fun read. It happens to be in Haskell, but is not dependent on the language.
Plain old data, `destructuring-bind`, and first-class symbols are a pretty robust substitute for algebraic data types and pattern matching in a dynamic language.
richard bird's "the algebra of programming" is pretty mathematical but very interesting. a lot of the "Advanced Functional Programming" articles (proceedings from the summer schools) are very good too.
You're selling C short. It's not just that it's close to the machine, it's that it's a *lingua franca*. C is a lowest common denominator set of features that runtimes use to interact. GPU's, peripherals, and GUI toolkits all speak C, and probably speak Python or Haskell only in so far as those two speak C. I've found C *essential* to professional programming.
"For beginners". No professional can avoid learning it, until such point as it is supplanted as the system language, which I'm not holding my breath on. I'm more dubious every year about it being a good language *for beginners*, and in particular my point about C dictating the order and speed with which you learn things to get anything done vs. a language that allows you to learn things in a more appropriate didactic order is the core of my point, and not a normal criticism of C-as-first-language. Learning C from a base camp of another language that you can Get Things Done in seems like a better idea to me than trying to start there. Beginners should simply not have to deal with segmentation faults or array bounds failures in _hour two_ of their programming experience. What an fantastic way to take someone interested in programming and ensure they never want to do it again that is!
I agree with your points, except that the first thing I wanted to do as a beginner was draw on the screen, or at least make something that had a window and some buttons I could click. I find doing those things in Python to be an exercise in impedance matching with C, and I *know* C. I guess my feeling is that I always need to know C to get things done in Python or Lisp, and sooner rather than later, but I never need to know Python or Lisp to Get Things Done in C. I'm still too green in Haskell to know if it's different. YMMV for everything I've said, of course.
I'd suggest [How To Design Programs](http://www.htdp.org/) for the same reason, though it's not strictly dedicated to *functional* programming, per se.
"Why Calculating is Better than Scheming" http://www.cs.kent.ac.uk/people/staff/dat/miranda/wadler87.pdf
What do sum types have to do with ⊥? unless he solved the halting problem (unlikely), any computation of any type in any turing-complete language can be non-terminating. also, given that the principle lazy language (Haskell) has extensive sum types, he might want to reconsider his title.
Sounds like an excellent choice then!
I hope you will share your experience. I agree on declarative programming advantages, but there is also a need for a set of compelling material and tools to make the learning experience fun, interactive, visual… Recent posts by divip on a web interface and a svg generating diagrams dsl seem a great example !
Concentrate rather on the type of the results on both side of the if..then..else operator. For a lazy language with non-termination, those types are not the same on "then" and "else" branches : you can not do a unification of both branches. The trick is to add an additionnal ⊥ value to all sums to account for non-termination, and that's what Haskell does. Thus Haskell's boolean type is not real Booleans but : True | False | ⊥. In a total language like Agda, you would rather prove that your computation will terminate… 
The example he gives for sums mixes up distributivity, i.e. (a+b)*c = a*c + b*c and ends up confusing the reader. As I recall, the CBN/CBV duality states that CBN does not have proper sums even *without* being equipped with the proper products that CBN does have. Can he please explain why? 
Not sure why I'm being down-voted. I'm also not sure how well using the non-turing-complete-Agda as a counterargument to my argument about turing-complete languages performs. Also, since the author did it, I assume you'll forgive my conflation of "lazy languages" with "languages with non-strict semantics". Given this I observe the following facts: * Haskell is lazy * Haskell has sumtypes * Haskell exists --------------------------- It is not true that lazy languages don't have sumtypes **QED** Also, the whole argument is based on the postulate, which we have to accept the unjustified postulate: ((if condition then x else y), z) == if condition then (x,z) else (y,z) He then uses that postulate to disprove the existence of a programming language I use everyday. I could be living in some elaborate dream world unconnected to reality. However, a far simpler explanation is that his postulate is wrong.
`Either Int Int` is a sum in a strict language. In Haskell it is not quite a sum. test1 = 1 + (case foo of {Left x -&gt; x; Right x -&gt; x}) test2 = case foo of {Left x -&gt; 1 + x; Right x -&gt; 1 + x}) Now what happens when `foo` is `Left undefined`? Both are `1 + undefined`. But when `foo` is `undefined`? Then test1 is still `1 + undefined` and test2 is `undefined`. So the two expressions are not equivalent, whereas in a strict language, with a "genuine" sum type, they are. Note that strict data types can eliminate the `Left undefined` case, but they *cannot* eliminate the `undefined` case, which is the problematic one. 
Aye, the biggest difference is that the book has a couple of introductory chapters that I found extraordinarily helpful as an FP newbie.
As somebody who loves the second edition, but has never looked at the first, what are the major differences?
That equation for sums doesn't seem symmetric with the equation for products, and given that his equation for sums has product in it, I don't think it makes for a very compelling explanation. I think [Jeremey's comment on the issue](http://www.reddit.com/r/programming/comments/gwqa2/the_real_point_of_laziness/c1rdftt?context=3) is more clear. 
But it's easy to have a compelling example: f (if c then t else e) /= if c then f t else f e ed: Fix typo.
(1+) is not a good sample function since it is strict (typically). A non-strict function shows the difference better.
Very nice.
Haskell has sum types, that doesn't mean Haskell has sums in the categorical sense. For both sums and products you expect certain laws to hold. In Haskell they don't hold for either, in fact. :)
Granted -- working with inductive nats would probably make this more obvious.
Why would those two not be equal in a pure functional language?
Alternately, giant city-devouring ants are related to FP as much as UML is actually related to OOP :-)
I just knew haskell channel, did not know this one! Thanks
I've seen frag, but have not seen this raincat game. I'll take a look. 
 f = const 1 c = undefined
I already have an eye in this one, it seems very interesting!
I don't *think* that is the issue because in Miranda f (case x of (a,b) -&gt; e) /= case x of (a,b) -&gt; f e so if that were the issue, then Miranda wouldn't have products. I think you and the original author are mixing up ξ(?)-conversion with the issue at hand. Edit: doliorules explains how I am wrong.
Trailing 'e' should be 'c'? Perhaps: f (if c then t else c) /= if c then f t else f c 
Can you please elaborate? The way I read this, both sides may be simplified to 'const 1' provided that 'c' terminates. If 'c' doesn't terminate, then neither side does, so they are still in some sense equal.
In a lazy language, `const 1 e = 1` regardless of what `e` is. It doesn't matter if `e` is non-terminating, `const 1` doesn't look at it. However, if you examine `e` before returning `1`, which is what the rewrite does, you fail to terminate if `e` does also. Essentially, there's a difference between a function that just returns the same thing every time, and a function that looks at its argument, and then returns the same thing every time. This is because looking at the argument causes you to return bottom when the argument is bottom, so you're no longer as constant as you could possibly be.
Miranda probably doesn't have products, either. Same as Haskell.
If you're doing parallelization, I think you should take some shader or graphics algorithm which uses concurrency because of a inpure language and show how it can be done in Haskell using only parallelization. I don't know if any such exists, but I see a misconception of these terms over and over again. Depending on the game, I personally would like to see a detailed description about the logic of your game objects. link collection on games, haskell and FRP at: www.lambdor.net see Quickstarting game development in Haskell, ontopic blogs 
you can get it via print-on-demand from amazon. i seem to recall that the canadian version is much cheaper, so try amazon.ca.
I'm pretty sure Miranda's pairs are products.
It isn't *exactly* the same, quoting Turner, "Total Functional Programming", p. 754: &gt;In Miranda &gt; &gt; f (x, y) = ... &gt; &gt; is irrefutable, that is the match cannot fail, because at type A × B we have &gt; (⊥A,⊥B) = ⊥A×B &gt; whereas Haskell’s product type has an extra ⊥ below (⊥, ⊥) and &gt; therefore for it the pattern (x, y) doesn’t match ⊥. Of course he thinks all choices are absurd, partly because there are so many of them. 
I found a mailing list post from 1993 suggesting that Miranda has unlifted products, which are the correct categorical products. But that means your statement of inequality would have to be wrong, at least under the assumption that `a` and `b` are not free in `f`. The post I found also claims that `seq` breaks the eta rule, distinguishing `_|_` from `(_|_,_|_)`, which doesn't surprise me at all. If Miranda products are unlifted, it's probably because product matches are treated irrefutably, not because it uses some parallel evaluation strategy.
let `f = const 1` and let `x = undefined` const 1 (case undefined of (a,b) -&gt; e) = 1 case undefined of (a,b) -&gt; const 1 e = undefined in Miranda.
Then Miranda's products are not unlifted, and not really products.
Miranda's products *are* unlifted, and its products satisfy the categorical product law: (`fst . h` = `f` ∧ `snd . h` = `g`) ⇔ `f &amp;&amp;&amp; g` = `h`
Okay, then: eta p = (fst p, snd p) fst . eta = fst, snd . eta = snd fst &amp;&amp;&amp; snd = eta Also: fst . id = fst, snd . id = snd fst &amp;&amp;&amp; snd = id So `eta = id`. So: case undefined of (a,b) -&gt; const 1 e = case id undefined of (a,b) -&gt; const 1 e = case eta undefined of (a,b) -&gt; const 1 e = case (undefined,undefined) of (a,b) -&gt; const 1 e = const 1 (e[a := undefined, b := undefined]) = 1
:O
Yeah, one of the c should be e.
Miranda has unlifted products.
That's correct. Miranda treated tuple matching the same way Haskell treats a ~ tuple pattern.
That's wrong for Miranda. case undefined of (a, b) -&gt; ... in Miranda is treated like case undefined of ~(a, b) -&gt; ... in Haskell.
I haven't turned on my Kindle DX since getting an iPad. Technical books are much better read as PDFs if possible, which the DX botches. One can still read Kindle novels using the iPad Kindle app.
I love these materials from Diviánszky. Keep them coming!
Andre Furtado had similar interests when he wrote the (non-FRP) FunGEn game engine, now at: http://darcsden.com/simon/fungen, demo: http://www.youtube.com/watch?v=XRG9H0oC2Fw. You could ask him what he learned (and share! :-), and perhaps develop this further. Or, you could check out what he moved on to: &gt; I'm currenty exploring how domain-specific development and software product lines can be streamlined for game development. You can check it out at http://sharpludus.codeplex.com. Volunteers to port such a work to Haskell are welcome. As for FRP, you could be a test case for the newest FRP lib, [reactive-banana](http://hackage.haskell.org/package/reactive-banana) ([docs](http://apfelmus.nfshost.com/blog.html)).
I would really love to see this course put online, sounds like a fantastic introduction to Haskell and how it should be used in the real world. Can’t wait to hear more!
 type Consciousness = Stream Theme Pretty strong statement, that one!
That's perfect! The author (Andre) of FunGen is from my country (Brazil) will then be easier to talk to him. I'll get in contact with him, thanks! (Im already taking a look in Reactive-Banana and I will definitely try it if i go with the FRP-Route)
Is stanford trying to play catch up to cmu's sexy new fp course?
I'm gleefully waiting for the next installment of the CBV vs CBN wars.
In the article &gt; Don Stewart illustrated this on StackOverflow with the following diagram is mentioned... could we have a link to that SO question?
Yesterday’s [meetup](http://www.meetup.com/fp-bud/events/17699581/) was also fun!
Hi! I've updated the post to point to one [question](http://stackoverflow.com/questions/5847642/haskell-lightweight-threads-overhead-and-use-on-multicores) where Don uses the diagram. As for the informal numbers, I think I got them from his [response about sparks](http://stackoverflow.com/questions/958449/what-is-a-spark-in-haskell).
Leksah made my entire system unresponsive during the 'analyzing cabal packages' phase. Have a comprehensive kill command at the ready (`killall -9 leksah leksah-server`) when you try out this application.
There is no uninhabited type such that Either a Void -&gt; a And Either Void a -&gt; a are defined for all inputs. \_|\_ inhabits Void, since it inhabits everything of kind *. Therefore Either, which plays a coproduct-like role in Haskell, is not actually a coproduct.
I wonder if miranda-style products would be a good addition to Haskell? We already have unboxed tuples in GHC, but the rules governing them are extremely restrictive. `data unlifted UPair x y = UPair x y` could be a lightweight syntax, for example, that wouldn't guarantee unpacking but would allow it when possible by simply forcing all pattern matches on `UPair` to be irrefutable...
I've got one in Mtn. View if anyone wants to borrow it. 
Haskell's Parallel technologies, when matured, I think will be a big game changer. The way I see it right now, thread-safe code in an imperative language is extremely difficult to write for any non trivial task (call me a crappy coder if you want, but this is my experience). Providing a way to express a complex parallel computation in a simple, safe, high performance way would actually make Haskell much more desirable in industry. I see only good things in the future coming out of this.
What is the last name of "Takayuki Muranushi"? (I vaguely remember that for example the japanese have it "backwards".)
The proposal is very well written and motivating! Looks like a great project!
I think this article just describes the framework that later turned into Cloud Haskell: https://github.com/jepst/CloudHaskell So there's nothing new to see here.
Concerning FRP, have you looked into [Yampa](http://www.haskell.org/haskellwiki/Yampa)? [Here's](http://lambdor.net/?p=44) some pointers that might help get you started with it (and FRP in general)
No. Please re-read, Haskell **does not** have true sum types but only pointed sum types : given A and B it is impossible to construct A+B in the langage, only A+B+⊥. 
Sorry for the repost. I had to take it down to move some processing over to client-side. Should be much faster now and use less of my uni's bandwidth :)
Of course, since all all Haskell's types are lifted we actually have to talk about A+⊥ and B+⊥, not A and B, and that's the important point. If the data constructors are lazy then we have `Either (A+⊥) (B+⊥) = ⊥+1*(A+⊥)+1*(B+⊥)` which is problematic because it has that extra bottom in there. And if we use strict data constructors then we get A+B+⊥, which is problematic because there's no way to distinguish the bottom of A+⊥ from the bottom of B+⊥. The whole point is that we have no `f` where `f (A+⊥) (B+⊥) = (A+⊥)+(B+⊥)` *Edit:* Of course, we can define type constructors yielding `⊥+A+1*(B+⊥)` or `⊥+B+1*(A+⊥)` by making only one of the data constructors strict. But then these aren't symmetric in the way that the desired coproduct is.
Also, check this old post : http://donsbot.wordpress.com/2009/03/16/visualising-the-haskell-universe/ Updated: http://i.imgur.com/SXnLX.png 17k modules now.
BTW, you're doing great work on SO!
Hopefully the architecture portability of ghc will improve now when it can use LLVM. Somebody just needs to do the custom calling conventions for more platforms.
It would have been more interesting if the link pointed to a page that works. HWN-181 seems to have disappeared.
The reason why I write all my code in Haskell (well, except for JavaScript) is that every other language makes me wince uncontrollably. I guess it's a side effect.
&gt; While portability is not as trendy anymore as it once was, I still have the twisted opinion that high quality code in a higher level language absolutely must be portable, otherwise it’s not high quality This is a poor argument in an otherwise reasonable post. While it would be nice to be able to cross-compile to other architectures, there are many cases when portability between architectures is not necessary. If you're writing desktop software chances are that your target public is running x86. When I'm writing software for my company my boss doesn't care if it runs on Arm or MIPS, and so on. 
I sympathize with that. My only fear is that one day, i'll have to interface Haskell code with C++ or have to ship an iOS-based product.
I've heard Blogger has some extended downtime.
Doesn't -fvia-c provide some additional cross architecture portability?
I think "-fvia-c" is deprecated.
&gt; ...one day... If this isn't an actual need you have or know you're going to have in the foreseeable future, then basing decisions on it doesn't make a lot of sense.
Interfacing Haskell with C++ (in terms of calling C++) isn't that bad. You have to write a C wrapper, but other than that I've found the process straightforward. Things may be different in the windows world... As far as iOS, doesn't that pretty much lock you into Objective C anyway?
I think i would agree to your approach, if either i had a lot of sparetime or would be pretty much sure, i would not meet such a show stopper in the next 2-3 years.
No. Compiling Haskell with -fvia-c is, as mentioned, deprecated... but even when it wasn't, the result was not portable across CPU architectures. It played all kinds of evil tricks with directives to tell the C compiler to leave certain registers dedicated for certain tasks, and so on... so it was very architecture-specific C code. What you are expecting is actually there; it's called unregisterised C, and it's a perfectly supported back end for GHC, and the first place to go when porting GHC to a new architecture. It is, however, rather slower than back ends that use calling conventions better optimized for Haskell. And even so, there's some distance to go before GHC can be easily used as a cross compiler; there are also issues with the choice of runtime system to link in, per-architecture package databases, etc.
Ah, ok. I remember those points being mentioned in an article about the LLVM back end for GHC. So is unregistered C deprecated as well?
In addition, the new code generator's design is such that all compilation is cross compilation. This should make it less likely that code for other platforms than x86 bitrots, as that code won't be behind ifdefs any longer.
I think you're missing the point, which was not at all made clear in the article. The very act of making something portable often improves quality by revealing hidden assumptions, encouraging abstracting out platform/arch independent vs. dependent parts, et al.
I downloaded the test software, and I have to admit I'm pretty impressed! It seems that some formal methods people have finally gotten something right!
This makes proof of Haskell code _way_ easier to deal with than Agda. And it doesn't make me feel like my knuckles are being rapped with a ruler every time I say something. Definitely the proof assistant of choice for those who, like me, were raised in the self-esteem era.
When your horizon is clouded by iOS, then it's time for the cloud. Put differently, I guess you can get by almost entirely with client-side JavaScript and server-side Haskell these days.
Well, Haskell code is independent of processor architecture by default; it's just that GHC mainly emits x86. The only place where arch may creep in is through the foreign function interface; but all the types there are well-defined on the Haskell side.
No, unregisterised C is not deprecated. It's unlike to ever be, since it's a crucial step to porting GHC to a new architecture. (I suppose in the far-off future, it might get deprecated in favor of something like an unregisterised LLVM, if LLVM grows to support as many important platforms as GHC does.)
This is definitely way overcomplicated. I don't know what the creators were thinking. I prefer the more elegant version in Haskell: data HaskellIsTheBestAtProving proof :: HaskellIsTheBestAtProving proof = undefined
&gt; | ZFC | Falso &gt; Resistant to Gödel attacks | No | Yes I fail to see the point of this new system, but I am quite impressed that it is protected from Gödel attacks. For example, whenever I tie knots or perform meta-programming, I frequently get assaulted by a mob of philosophers that look like Kurt Gödel. Fortunately, so far I managed to avoid induced mind explosions or being sent back into the future.
It's not deprecated, but does unregisterised C actually work these days?
Also, check the [parent site](http://estatis.coders.fm/) :)
That's in the experimental code extraction backend. The current version only extracts to php, whose runtime is more suited for Falso's axiomatic system.
The claim that their system is 'stronger than' naive set theory and first order arithmetic is unverified and groundless; they should retreat to the more prudent claim that Falso is 'no weaker than' they are.
What you're saying is that making code portable has the desirable side-effect of improving the code quality. It could be so, though that's not my experience with portable C/C++ code, which usually involves a mess of #ifdefs and makefile hackery. Maybe my sample is biased, I'd be glad to be shown better examples. Anyway, what you're saying is not what he's saying. He's claiming that portability is a *necessary* condition for high quality code.
That's not true. In fact, I saw a proof of that claim in Falso just the other day...
Nice sproof. 
&gt; Almost all axiom systems are vulnerable to the Gödel process which can be used to create unprovable true statements. Worse still, such systems are also utterly unable to prove their own consistency! This is traditional mathematicians' dirty little secret: all the precious theorems they have proved in their old axiom systems silently depend on the consistency of these systems, which they have no way of proving! &gt; As you probably will have guessed by now, Falso's pioneering design ensures that it isn't vulnerable to the Gödelian process, because it can easily be used to derive a contradiction (you just have to use the axiom!) and thus fall short of the requirements of Gödel's construction. &gt; This improved security means that Falso can be used to prove its own consistency! It can even be used to study the consistency of other systems... This is pretty bad news for the users of legacy axiom systems, since we have used Falso to derive several inconsistency proofs about them which cast serious doubts about their suitability for mathematical research. Stay tuned for more information! 
The platform is where you sometimes need commercial and company support. 
We need people to help with getting it ready.
Apologies. It seems that blogger is having issues. The lost a lot of blog entries, including the one for HNW-181. They claim to be recovering things, and I'm going to give them a bit of time to do so. If they don't recover it, I'll promptly post it again. Thanks for pointing this out!
&gt; I guess it's a side effect. instance Monad Wince where...
&gt;Things may be different in the windows world... Nah, it was straightforward already when I tried it two years ago, I can't imagine it's become more complex.
&gt; Open source and pirate programs are a threat to your computers and your mathematical activity. Always use genuine versions of Estatis Inc. products. Holly shit!
Good pun ;)
Some widely used types depend on the architecture. For example, the size of Int can vary which is well-defined in the Haskell report.
I don't always write code, but when I do it's Haskell. Except when it's something else.
In my experience, writing portable Haskell code is monstrously different than writing portable C/C++ code. The structure of C/C++ is set up so that you practically *have* to use a mess of #ifdefs to get anything done, but that's a C/C++ problem not a portability problem per se. One of the problems is that C/C++ doesn't support abstractions like higher-order functions, typeclasses, a massively awesome (and user programmable) inliner, etc. Once you have those abstractions in place, it's easy to isolate the use of #ifdefs to a few places in your core libraries. In C++ you can use operator overloading to get some of the benefits of typeclasses, but that's about it. The real game-changer is the inliner, because it means you don't need to choose between abstraction and performance. Also, because Haskell is a high-level language, that makes a lot of the portability issues go away (i.e., the compiler has to solve them, not the programmer). This doesn't handle major platform portability issues like POSIX vs Windows, but it does handle most of the low-level cruft like machine word size, endianness, and similar architecture issues. A lot of the portable code I've written in C/C++ still had to deal with architecture portability...
I usually turn to OO languages when I need to implement an algorithm that is most easily described as imperative manipulation of objects with mutable state. But that may be just because I'm not familiar enough with Haskell's state monad(s).
Wow, I really am interested in this, and yet I can't shake the feeling that the self-promotional presentation makes it sound like a scam.
Wait a minute...they submitted a proof that P = NP, and then they say that in case that fails, they have a secret proof that P /= NP. wtf??? How am I supposed to take that seriously? How can you prove both that P=NP and P /= NP? ...am I just thick or is this supposed to obviously be fake?
Drat, looks like my favorite password is now compromised... &gt; adsfk;lj13248097asfdklj1324980 Better change that one ;)
IndeedWhat was I thinking? Old habits die hard.
It definitely worked in 6.12 (for x86, at least), so it's not entirely unreasonable to assume it still works in 7.0
You can prove both P=NP and P/=NP when your axiom system is inconsistent.
Personally, I found the State monad did a lot less than I was hoping it would. It carries local state through a computation, but does not add persistence globally. This is a good thing. Your best bet, I think, is to read Chris Okasaki's [Purely Functional Data Structures](http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504/ref=sr_1_1?ie=UTF8&amp;qid=1305349003&amp;sr=8-1). You can do more with pure functions than you'd think. 
Good point. But it's guaranteed to cover at least the range `[-2^29, 2^29 -1]` and you can always use `Integer` instead. Put differently, if your `Int` overflows, then you have a problem anyway, regardless of architecture. 
How?
Now, now, please don't use the i word.
I'm sorry, but this company has a big problem. Frege Gmbh holds a patent on the very method they are using for the Falso system, and I'm sure this will end up in court.
Straighforward maybe, but tedious for sure. On iOS you can use C, C++, Objective-C, Objective-C++ or languages that compile to those, like Gambit-C, and embedded languages like LUA.
Both.
Concerning point #4, the "dll hell" because of upper bounds, wouldn't it be possible to replace this upper bound by some hash of the exported signature ? Haskell beeing pure, it would be more meaningful than in other languages (and maybe it could be a hash list, to enable just adding a type to the signature). Concerning point #3, does anybody know about this "plan to fix libraries once and for all" ? 
Don't worry, I'm sure that Estatis Inc. will be able to provide a proof that their method is not equivalent to the one in the patent.
Muranushi. I had to look at the Japanese proposal to tell; sometimes people keep their name in the Japanese order, and sometimes they don't.
Slick!
This must be included into haskell-mode. Btw what's the status of haskell-mode development ? Active ? Abandoned ? 
Unlike most attempts to use math/logic in court, that might work. Falso and US patent law have similar assumptions.
Even after reading wikipedia, I still don't really understand "Reify". I read that It's supposed to make something concrete out of an abstraction, but for example in the article, &gt; One of the really neat things about the Par monad is how it explicitly reifies laziness, Laziness would be the abstraction, but ... wait, after rereading, I think I'm beginning to get it.
**Eh, it seems I'm not totally stupid yet :)** Security assessment for password "hunter2" *(not the actual password I entered, and also the one I actually entered is not a password I use)* Thanks for disclosing password "hunter2" to us! Password hunter2 Score 0 % - Insecure Assessment You just disclosed password "hunter2" to an untrusted third party (us). You have no way to find out what we intend to do with it. Maybe we logged it and intend to publish it or to use it against you? For this reason, password "hunter2" is now compromised. It is therefore insecure and should not be used in any situation. ** I don't care, it's a password that looks like one I use, but that I just made up** 
Not working for me, there is no haskell-mode-map. Using latest haskellmode from darcs.
the latest I know of is: http://permalink.gmane.org/gmane.comp.lang.haskell.emacs/108 
great script! Should be added as a hint to http://www.haskell.org/haskellwiki/Haskell_mode_for_Emacs
And here's the link to the draft ICFP 2011 paper: http://research.microsoft.com/en-us/um/people/simonpj/papers/parallel/remote.pdf
import file haskell-mode
Pygame does a good job of getting you drawing on the screen quickly. I don't know how it compares to the equivalent C/SDL, but one tutorial example has a sprite bouncing around the screen in about a dozen lines iirc.
hunter3
solved, had to put it in a hook instead of the toplevel of init.el: (defun my-haskell-mode-hook () ...keybindings here...) (add-hook 'haskell-mode-hook 'my-haskell-mode-hook)
I wish to try it, but it's seems to be too tightly integrated with ghc which puts me off :/
You might be interested in lambdascript: https://github.com/valderman/lambdascript Supposed to let you compile a subset of haskell to javascript. I personally have not used it.
Depends how much the code uses templates, really. I'd wager that a binding to CGAL would be impossible due to its extensive use of them.
Although this isn't the only use of the term, when people talk about reification in Haskell, they are often talking about representing code as data. So 1 + 2 is an expression, but: data Expr = Val Int | Op OpType Expr Expr data OpType = Plus e = Op Plus (Val 1) (Val 2) here, "e" is a reification of that expression.
The Scheme world has a big track record treating reflection/reification formally. See e.g. [here](ftp://ftp.cs.indiana.edu/pub/techreports/TR161.pdf) or [here](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.114.3275)
Any patent on this or similar systems can easily be invalidated by prior art - namely, lambdabot's `@faq` technology.
&gt;GENERAL INFORMATION &gt; &gt;a. You agree that you will comply with these Terms and Conditions to the best of your ability. &gt;b. You agree to do anything we want. You are now a slave of Estatis Inc. &gt;c. You agree to pay $ 100,000 for your use of the Estatis Free Password Security Checker if we ever ask for it. &gt;d. You agree that we may change these Terms and Conditions whenever we want, for any reason, in which case you will be automatically bound by the new version of this document. &gt;e. You agree that these Terms and Conditions shall be governed by the laws of the Intergalactic Federation. Any controversies and disputes arising out of or relating to these Terms and Conditions shall be judged by the High Court of Vogsphere, or, in the event of early universe termination, by courts of higher dimensions. 
But Frege published in 1903, which I think predates lambdabot. 
The State monads aren't really great for imperative algos; they're more suited to the (common) situation where you have a more or less functional computation that threads a variable and you want to abstract that. For imperative computations, using the ST monad is much better than State. Some Haskell beginners have some trouble with typing on the ST monad (the higher rank types degrade type inference somewhat, requiring more explicit type annotations). If you're in that situation, prototype the algorithm in IO, even if it's pure. Think of it this way: if you write it in C++ or Java or something, you're living in IO anyway, but you have a much weaker type system. In fact, porting (say) C to Haskell in the IO monad is relatively straightforward, and just having Haskell's type system will catch a huge number of simple errors. Then, once you have a working IO implementation, it should be relatively straightforward to port it to ST.
Bounded ints really highlight the need for a dependent type system.
You know what I want for Christmas? A complete Haskell compiler with close enough Hackage Coverage to GHC (i.e. can compile say 50% libs on Hackage) whose output I can put in a `configure; make all; sudo make install` tarball. This probably means a compiler that outputs to C (which is practically the only thing you can reliably put in a `configure`-able tarball) by default and doesn't depend on assembly-level hacking so much. JHC is nice but I haven't seen much recent activity...
It's already gaining ground. I know of many commercial interests that use Haskell for this purpose (although I'm not allowed to mention exactly which :( )
Unfortunate that you can't, but I'm glad it's being used *somewhere* at least, with good result.
And this patent has not expired after 108 years yet?
Sometimes I wonder whether the term "space leak" is a good analogy for the problem. To leak means: "Accidentally lose or admit contents, esp. liquid or gas, through a hole or crack". With space leaks you accidentally hold on to stuff. So space constipation would be a better term. Although it doesn't sound so nice: my program is suffering from constipation. 
I agree that "leak" seems wrong - I'd be quite happy if "space spike" or "space bubble" (cf. economic bubbles) took off as the terminology. "Space bubble" might be a bit too ethereal though.
In traditional usage in a language without GC "memory leak" makes perfect sense. You have a pool of memory to allocate from, and as the program goes on, memory is taken from the pool but never returned when it should be, and hence "leaks" away.
Hmm, I didn't look at it that way. It makes perfect sense. So it ultimately depends on which perspective you choose. For me the idea of constipation is a bit more natural. A program 'consumes' fresh memory on one end and 'dumps' used memory on the other. Failure to dump used memory can then be seen as constipation. P.S. Sorry for the mental imagery...
I can't post on SO questions, dons, what do you mean by the following: HMatrix: a custom array type with extensive bindings to linear algebra packages. Should be bound to use the vector or repa types. Are you saying you can make HMatrix interop with vector/repa, or are you saying that morally speaking that should be added to the library? :) Also, could you explain the most efficient way to interop Repa with pinned C Data? I know Repa.IO lets you copy repa arrays to ptrs, is there other support?
[Data.Bitmap](http://hackage.haskell.org/package/bitmap) is intended to serve that exact purpose; even though it's not very mature right now, it should be usable (the development version is a bit better, but it's not yet on the net). API suggestions are welcome.
* The hmatrix bindings to e.g. lapack should be used to build similar bindings for repa * Currently, repa uses only unboxed, unpinned vectors as the underlying representation. These should also be easy to convert to Storable vectors. Then things are good. Currently you have to copy.
Ah, does that mean `toVector' is constant time because all it does is give you the underlying representation?
If the array is "manifest". toVector arr = case force arr of Array _ [Region _ (GenManifest vec)] -&gt; vec and force won't do anything to an evaluated array. The library needs some complexity annotations...
Thanks, Max!
Seems like you're getting duality mixed up. "Thing retention" is dual to "space leak". If the vessel is expandable, then nothing actually has to be displaced in order for space to leak. I'd better stop here.
"Space constipation" sounds like something astronauts might suffer from if they've been in zero-g for too long.
Figure its useful enough now to put out there.
Nice!
&gt; So if 'x' is a 3D array: &gt; let x = fromList (Z :. (3::Int) :. (3::Int)) [1..9] ... This part is confusing me. Isn't that a 2D array, not a 3D one? 
No problem :-)
In case it's not entirely clear from the commit message, the idea behind these changes is to have GHC decode command line arguments, environment variables etc using the current locale. Currently we just assume these are latin-1, which means everything goes pear-shaped when you supply Unicode command line arguments or something. The wrinkle is that we have to deal with command line arguments which cannot be decoded in the current locale - for example, there might be some file names on the command line which use a different encoding. The solution is to allow roundtripping of undecodable bytes wholesale, ala Python's PEP383.
These kind of tutorials deserve all the upvotes I have. Nice work! I *now* want to play with this.
Fortunately, Haskell has some libraries that should help out with that problem: [regular](http://hackage.haskell.org/package/regular) and [digestive-functors](http://hackage.haskell.org/package/digestive-functors).
We already have irrefutable patterns, using the ~ char.
Yes, but we need to use them at every pattern-matching site. My suggestion is that to have some data-type level declaration that means the data type is *always* irrefutably matched. Think of the difference between strictness annotations in code and strictness annotations in data declarations...
that'd be great, if haskell-mode could make some progress again :-) I'm currently using `haskell-mode` + `scion.el` (and some of your extensions) for day-to-day work, which already is kinda great, but has a few rough edges...
Thanks! It's a nice introductory tutorial. A better way to show the arrays in ghci would be nice. I will try to write a new version of "show" when I get the time, but I won't be sad if someone else does it before me. :)
yes, the example is of a fold reducing a 2 D array to a 1 D array; I emended the wiki, to get both -- I hope rightly.
Are these contiguous in RAM and efficient for FFI as C arrays?
This looks like a useful resource. Small point: your email address is typoed on the first page. What would be good is more information about ThreadScope and how to interpret its profiles. I found when looking at the output that I had questions such as: why is there a white section in my program, when nothing seems to be executing -- is it where my program was scheduled out, or where time was spent calling the OS? I'm not sure where to go to for answers to that.
Thanks for a great tutorial! Is there a code typo on pg.37? Shouldn't the code be: do a1 &lt;- async $ getURL "http://www.wikipedia.org/wiki/Shovel" a2 &lt;- async $ getURL "http://www.wikipedia.org/wiki/Spade" r1 &lt;- wait a1 r2 &lt;- wait a2 return (r1,r2)
email address fixed - thanks. I'll think about adding more about interpreting ThreadScope profiles in the next version.
fixed, thanks!
Continguous in ram, yes. To interop with C arrays you need to copy them to a pointer, currently.
Beautifully clear exposition; thank you!
Needs a spell checker run on it, it has annoyingly many typos etc.
Great tutorial! Small issue on pg. 63 in the type of: forkOS :: IO () -&gt; IO () I think this should be: forkOS :: IO () -&gt; IO ThreadId
I remember asking about this in #haskell, and all it did was start a huge flamewar. I was told to not even file a bug report as it wasn't a bug by one person... so thanks for fixing it :)
Some people have asked about example images: there are a couple at http://byorgey.wordpress.com/2011/05/17/announcing-diagrams-preview-release/ and I hope to put up a gallery of more examples soon. In any case there are also some examples included with the diagrams-cairo tarball.
Thanks for this great tutorial! &gt; It is possible that a future STM implementation may use a different data structure to store the log, reducing the readTVar overhead to O(log n) Do you have any hints where i could read more about this?
I'm quite curious to see how easily I can translate a diagram of sketched out by hand into the diagrams EDSL. Looking forward to taking the library for a spin.
You'll need to add combinators for ghosts and presents :-)
Did I tell you about my idea for a "ghosts'n'presents" mode for vacuum?
With animation, I hope!
 Dependent types, The undiscovered country, from whose bourn No traveller returns, puzzles the will, And makes us rather bear those ills we have Than fly to others that we know not of.
Some ideas: * [RWH ch 14](http://book.realworldhaskell.org/read/monads.html); * [Learn You a Haskell](http://learnyouahaskell.com/a-fistful-of-monads); * [Read and use the code](http://hackage.haskell.org/packages/archive/base/4.3.1.0/doc/html/src/GHC-Base.html#Monad); * [Check SO questions too...](http://stackoverflow.com/questions/2366/can-anyone-explain-monads). In fact, this is a great question for SO.
My suggestion would be "stop trying". Seriously, just write Haskell and read other people's Haskell code. There's nothing that fundamentally _needs_ monads, but as you gain more familiarity with the type system and common repeated patterns, you'll probably start picking up hints about what they're for. I'd be willing to bet that most people try to learn them prematurely (mostly because for some reason people talk about them all the fucking time) and end up quitting Haskell in frustration (or concluding that it's a bullshit academic language).
The idea that understanding monads is necessary to be productive in Haskell is a myth. If you do not yet understand monads then it is probably not yet your time to understand monads. Despite appearances, this is not intended to be a non-answer. Just keep going with Haskell generally, learn libraries as you need them (including their `Monad` instances), and eventually you will catch on to the "monadic pattern," at which point it will be as though you had known it all along, and you will not understand what all the fuss is about.
Learn what Functors are, then Applicative Functors, then Monoids, *then* Monads (then MonadPlus). Start here, then read the next two chapters as well: http://learnyouahaskell.com/functors-applicative-functors-and-monoids After that, if you're still hungry for more Category Theoretic typeclasses, start learning about arrows (or comonads, etc etc)!
"Young man, in mathematics you don't understand things. You just get used to them." -- John von Neumann Best way is to just start using them. Trust John, not me. Use type sigs to figure out what kind of function you can apply next. Every type class (Functor, Applicative, Monad, others) comes with only a couple of essential functions you can use. Learn to keep track of those. For monads, maybe avoid do-notation at first. Then later it will seem like the mere convenience it is meant to be. The aha moments will follow quicker and without as much effort. 
In OO languages you can use other people's classes by creating instances (objects) of them, but never writing your own class. Same thing with haskell. You can use different monads created by other programmers, and never write your own instance of monad.
I found this helpful: http://blog.sigfpe.com/2007/04/trivial-monad.html
The package has a dependency on xml-types, but I don't think it actually uses that package at all. Any reason for this setup?
I couldn't wait for the library to get updated. What a nice news to begin the day with! Go go Brent! 
Sometimes I am not sure whether it's a typo or a McBrideism.
Does Horner's rule have anything to do with the strength of a Monad?
This really explained the Reader/State monad for me: http://www.haskell.org/haskellwiki/Roll_your_own_IRC_bot#Roll_your_own_monad
Er, um, sorry about that. I got to the stage where my brain refused to perceive the actual text any more.
When you don't understand monads yet, chances are that you don't have a good grasp of your previous Haskell knowledge yet. My recommendation is to know the Prelude by heart* before attempting to learn about monads. *Within reason. There is no need to know the intricacies of the `Fractional` hierarchy, but you should be able to implement any Prelude function from their type signature in 1 minute cold.
I don't think for learning, going from Functor to Applicative to Monad is very instructive. Following that route, you have to spend a lot of time in "abstract nonsense" before you get to "effects" where you can model updateable state with the State monad, partiality with the Maybe monad, logging (write-only state) with the Reader monad... Except for the multiple error Applicative, there don't seem to be any paradigmatic Applicatives (ZipList seems pretty limited) - UU-Parsing is just an Applicative and not monadic so it can be efficiently implemented, I think the same is true for Chalkboard's Active. Thus as a learning path, working through Applicative you are spending a lot of time on the funky notation and somewhat contrived exemplars. 
I agree with the other people who have suggested that you not worry so much about monads and just write some Haskell code. However, you should check out Brent Yorgey's [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia). It goes through a number of important abstractions and paints a nice picture of interrelationships that aided my understanding of monads significantly. 
I think this is where the analogy with presents and ghosts becomes unhelpful, despite the nice pictures. The point is that you can simply take the expression and evaluate it by hand f [1..4000000] 0 = f [2..4000000] (0+1) = f [3..4000000] ((0+1)+1) = ... without any understanding of how GHC implements the Haskell heap. The size of the expression is directly proportional to its memory use. You can treat the Core output as expression as well, only you have to know that `Int#` are evaluated eagerly. Likewise for the other examples. You don't even need to know about denotational semantics, even though this can be very helpful for detecting unevaluated expressions. 
What are the chances to be able to combine Diagrams with LaTeX text for use cases similar to tikZ ? Obviously something like a pgf backend is needed at least, but what else ?
I have to respectfully disagree. I will certainly grant for some people, the ghost metaphor may be unhelpful (I point this out in the introduction to the section.) But I strongly believe that there are people who have trouble with this purely syntactic notion of space usage (“But surely 0+1 is just 1! My computer should be smart enough to figure that out”), for whom making the thunks very tangible is very important to understanding. I also think the denotational semantics are very important to understanding when GHC can optimize away a leak and when it cannot. But that might just be me. :-) (reposted from blog comments)
I found this particularly helpful: http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html For me, the key was understanding the "problem" that Monads help you solve, which is nicely expressed in the signature for bind: M a -&gt; (a -&gt; M b) -&gt; M b So you've got an M a, and you really want to pass it to your function a -&gt; M b without ending up with M M b, which is what Functor would get you. Keep this in mind while looking at the definitions for various Monads, and it will click eventually. edit: Replaced Applicative with Functor. Thanks cameleon. Perhaps not all Monads are amenable to this approach, but enough are. And as others have said, don't worry about it too much. At some point it will click and you'll wonder why you thought they were so hard.
One inspiration could be Shoes. Look at the shallow native bindings https://github.com/shoes/shoes/tree/develop/shoes/native and what you can achieve with that: http://www.the-shoebox.org/
Good point, making `0+1` tangible ("it's not 1, it's something else!") is a key step to understanding lazy evaluation. However, three points: 1. Skipping the purely syntactic view is poor Haskell. After all, formal manipulation of expressions is the *raison-d'être* for Haskell. How you get there is your business, but the simple and clean formalism should be the goal. You can get by with some denotational semantics, but it's no substitute. 2. The ghosts and presents would add a lot more value if they made expressions tangible instead of replacing them entirely. (I don't see how exactly a ghost corresponds to code I wrote down.) (Also, the ghosts contain superfluous implementation details.) 3. The traditional path to tangible expressions is via graph reduction. The graphs for `0+1` and `1` are quite different. (reposted from comments. stuck in infinite duplication loop.)
Have you tried it? I think that sequence in LYAH is one of the most brilliant expositions of the whole complex I've ever seen.
Also, when you start factoring out something, and you say to yourself, "This is really simple, surely somebody has already done it", pop the type signature into [Hoogle](http://www.haskell.org/hoogle/) because you're probably right. I wrote the equivalent of [`Control.Monad.&gt;=&gt;`](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Monad.html#v:-62--61--62-), realized that somebody must already have written that, popped it into Hoogle, and realized that while I'd seen that before I had not grokked it until I needed it. A lot of Haskell works that way, really.
If you mean you want the ability to have LaTeX-typeset labels within a diagram, yes, just something like a PGF or pstex backend should suffice (and this is certainly planned). If you mean you want to be able to write a LaTeX document with code for diagrams embedded inline, this should not be too hard either, it would just require a preprocessor tool to render embedded diagrams code. I certainly plan for this to become a reality at some point hopefully not too far in the future.
I have to agree. Learn You a Haskell does an excellent job. If only it covered Monad Transformers in more depth.... 
I agree with jerf; it certainly worked for me. I've got some more details on what I found useful at http://learnmeahaskell.blogspot.com/2011/04/if-i-were-given-re-do-from-beginning.html .
That's *very* interesting, thanks for the link. Does anyone have experience with Shoes they could share? It looks like we could build a Haskell package off of its C library components without too much fuss.
Actually, to end up with an M (M b), you just need Functor and fmap, not Applicative.
Another option, that y'all will probably hate - call Haskell from C. It's really easy to learn just enough Objective-C to throw a GUI together, and then the problem on the Haskell side just becomes "write the necessary data into a buffer whenever C asks you to".
D'oh! Fixed. Thanks.
* Switched from lists to vectors and matrices from the [vector](http://hackage.haskell.org/package/vector) and [hmatrix](http://hackage.haskell.org/package/hmatrix) packages. * Now 25 times faster! * Switched from darcs to git and [hosted on github](https://github.com/basvandijk/levmar/).
This is a very well written 8-part series: http://mvanier.livejournal.com/3917.html 
As others have said, that section of LYAH is excellent, and after reading it I think I just about understand statements like "Monads are Monoids in the domain of Endofunctors" And Functors and Monoids are definitely worth learning about, quite aside from whether you use as steps to learn Monads. I'm not quite clear on the use cases for Applicative Functors, but I suppose that's probably because they didn't exist in Haskell originally, and are a subset of the capabilities of Monads (I think?), so there aren't as many examples.
Link, pretty please.
http://contemplatecode.blogspot.com/2011/05/haskell-weekly-news-issue-182.html
Here you go: [Haskell Weekly News: Issue 182](http://contemplatecode.blogspot.com/2011/05/haskell-weekly-news-issue-182.html)
What i really love about Shoes is that on any platform you can create executables for any other supported platform. The user does not need to install anything. I've tried to implement a little tool for myself using Shoes. Development was easy, especially HTML/forms-like GUIs with table-layout, input fields, rich-text etc. It is based on Cairo so graphics support is also quite good. The widgets support is deliberately simple, though, so there is no support for tabs or table/grid. I've encountered frequent random crashes, that's why i stopped finishing the implementation. Since you develop in Ruby, the platform is to be blamed. :) The Shoes manual is a nice place to get an idea: http://shoes.heroku.com/manual/Elements.html
Yep, I'll hate that option. :-) Especially since I'd like to create GUIs with [functional reactive programming](http://hackage.haskell.org/package/reactive-banana). (Hm, though it might be possible to abstract this into a few Obj-C helper classes.)
I agree.
Wasn't this the reason for the libgmp-Independence project? http://www.well-typed.com/blog/32
AFAIK that is the reason that GHC was made independent of libgmp.
Awesome. That could be haskell's "killer app"… amongst latex users at least ;-)
could you elaborate? On Windows GHC 7.0.3 statically links libgmp.
http://www.well-typed.com/blog/32
There are great examples for web-apps, but i think there are still both a lot of technical limitations (i have not seen usable web-versions of CAD programs, IDEs, Photoshop, Spreadsheets, Filesystem-Shells, etc) and economical limitations (what is the market similar in significance to iOS/Mac/Android App Store?) to be solved unless web-apps can replace every desktop application. I want technology to leverage me, not constrain me.
That's really cool. :-)
Nope. EDIT: Sadly.
Haskell has been steadily growing for the past number of years. I don't think Haskell will ever explode and take over the main 3 languages, but it has a lot more growing to do before it reaches the height of its popularity.
Wow at "wolfgang lambda"! Whose is this? Is there documentation for where its going? Really promising work. I do think that elegant web libs + elegant graphics libs + improved tools for evaluating Haskell interactively = amazing possibilities. With some work, the "wolfgang lambda" (or "haskpad") notion could not only become an awesome teaching tool, but a go-to scratchpad for me, rather than my current combination of emacs, ghci, and xview -- more interactive, better preservation of history, more structured, and with a good set of core built-in tools for reading data, basic statistics, etc.
Seconded, that thing seems to have come straight of of my dreams. I might finally be able to dump Sage and Mathematica! 
I think we're still far from that point. I think Haskell is an awesome language, but there are still big hurdles in Haskell's eco-system and around Haskell that shadow some of that awesomeness: * Despite having some of the most awesome mechanisms for polymorphism, Haskell libraries are still less polymorphic than even mainstream languages (e.g: Java). For example, see the type of the "length" function in various languages. * Haskell's ByteString API's, container API's are fragmented, duplicated, and thus not very nice to use. Having to convert from strict to lazy byte strings and back in case you want to use Binary is an example annoyance point. This of course relates to polymorphism mentioned above. * Haskell has multiple solutions to the basic-I/O problem, which means there's no "officially blessed" way to even do basic I/O! The "obvious" names in the standard library are taken by functions that use the `String` type (which should probably be removed altogether in favor of `Text`, at least when we have polymorphic functions that can work with it like they work with lists). Iteratee-based I/O has about 4 different solutions in Hackage. * Unsafety: Haskell is definitely on the "safe" side of languages. Given that's the case, it's hard to understand why partiality is so common in Haskell's libraries. I believe we should have something like: `unsafeUnjust msg (Just x) = x` with a strong convention that `msg` ought to contain an explanation of why it may never be `Nothing`. Pretty much every other currently-partial function should return a Maybe. Exhaustiveness checks should be on by default. If you are sure you want the partiality, use `unsafeUnjust` on the result of things. This is less convenient, but when using Haskell, I prefer safety over convenience. * Given the above, when you *do* encounter partiality in the wild, you want a good debugger. But compared to mainstream imperative languages, Haskell's runtime debugging tools are barely usable to non-existent. This could also be immensely useful to helping people understand GHC's operational semantics. * Lazy I/O: effects (or non-determinsm, however you prefer to look at it) are lurking behind seemingly innocuous function names such as `getContents` (would rather call it `unsafeGetContents`). This relates to unsafety. I think Iteratees are to replace lazy I/O. Haskell gave up side-effects in evaluation, and I think there's a consensus this is a *good thing*, despite seeming very inconvenient or impractical at first. I think the same should be applied to lazy I/O. * Another example of fragmentation is MTL, transformers, etc. * Haskell encourages modularity, and lots of little reusable components. This is great, but not if the package management system (cabal) cannot handle it very well. The Package-Versioning-Policy (PVP) encourages conservative version constraints. Along with the [butterfly effect](http://cdsmith.wordpress.com/2011/01/17/the-butterfly-effect-in-cabal/), the results are pretty bad. Using cabal-install requires debugging the verbose output repeatedly to figure out who forced in some old version of a package to recompile all of my base libraries. Typically I have to go and manually remove all of the PVP constraints from the packages and then everything usually compiles OK. I personally think the PVP is doing more harm than good. 
I don't think it's a spike (at least I hope not). Haskell may be too weird for mainstream adoption, but I think it will find it's niches and continue to grow there for a good while For example, some of us in HPC are deeply enamored with parallel Haskell, and its growing support for nested data parallelism and distributed computing. A colleague and I are part of the [Parallel GHC Project](http://www.haskell.org/haskellwiki/Parallel_GHC_Project) at Los Alamos, and we're looking for ways to do more of this kind of work here.
A good model for this would be a local binary (so we don't have to worry about secure code execution as much) with some sort of password/login system (so we could host for a workgroup) and leveraging darcs a la gitit to do file and change management on loading and saving worksheets. Also, of course, the ability to tell it to read a big file full of data via some upload mechanism. And then also a more secure mode for things like the interactive diagrams tutorial and soforth which we might want to share with the world.
it's mine. It's a really early preview version at the moment, and many things don't work properly. Aside from fixing lots of minor/major technical issues, these are the main things that I want to do in the short term: * persistent worksheets, with permanent urls (similar to a pastebin, click save, and you get a permanent url) * import/export of lhs files with markdown comments (this is actually how the Diagrams tutorial was imported) * Add/whitelist more packages from hackage, and document somewhere which modules are allowed * A good/documented API for interactive graphics. At the moment, only the low-level plumbing is in place. * Lots of user interface things, better error messages, cleaner css with consistent fonts, make it work in Internet Explorer (IE8+) Ultimately, I'd like to add some wiki-like functionality, for collaboratively edited worksheets in categories with a nice index and/or showcase. I hope that it could be useful for both haskell-related and non-haskell subjects. The haskell source code cells could be collapsed for items where the code is not important.
That's something Duncan was in the middle of working on two years ago. It sounds from the post like it was going well - but was the work ever finished and incorporated into GHC? Last I heard, it was in principle possible to get a GHC whose runtime wasn't intertwined with libgmp, but you still needed a special custom build of GHC. You would have to get coaching from Well-Typed to have any hope of accomplishing that. Have things improved?
I **do** think that such a thing is happening, in the sense that people will be compelled to adopt Haskell as the best *tool kit* out there in the computational universe. Other languages are said to thrive once a "killer app" is developed in them, but I don't think Haskell travel that beaten path. It may not even be Haskell, but I do think it will be a language *like* Haskell. The "killer app" might be a "killer IDE" (which is what Wolfgang is morphing into). But in the end, I suspect it will be the strongly mathematical nature of the language that allows such a tool to thrive.
I posted some ideas below. I'd also really like/perhaps be willing to work on integration with a good chart library or two. Haskell graphs aren't bad, but there are also awesome javascript tools that can provide much more interactivity (zoom, mouseover data, etc.). I'm partial to flot on that front, although its bar graph support is pretty lame. The other awesome thing would be integration with infovis: http://thejit.org/ Being able to knock out data visualizations of trees and graphs like that with minimal haskell work would be freaking awesome. The other feature I'd like (I assume a pony is already on its way) is a way to use/emulate a few of the standard ghci features (:type, :kind, :info, :browse) to find out what's in scope, play with exactly what my expression is doing, etc. Code completion shouldn't be a priority by any means, but it would certainly be neat as well.
I think great things happen when people can stand on the shoulders of giants. What are Haskell's shoulders to stand on? * Cabal/HackageDB are invaluable * I personally love Text and Binary, which makes certain tasks both easier and more efficient to code than in other languages * I am excited to read about web frameworks like Snap/Yesod etc * I was positively surprised to learn that installing Cairo bindings is now easy on Macs using Homebrew and Cabal The easier things are to use and the richer they are, the more richer things are built upon them.
Great summary of what to improve in haskell eco-system!
Interactive graphics would be awesome for me. sclv mentioned flot. I've found RaphaelJS and protovis to be good as well.
That kind of visualization is certainly one of the things I plan to add. As long as all data is entered in some declarative way, it should even be fairly simple to add a haskell front-end for it. Everything that's evaluated on the server has a type and a result string. The javascript client code checks the type and uses that to decide how to render the result. For example, there is a type for simple user interfaces (the slider in the demo worksheet), which are described by a JSON object. It's relatively easy to add different handlers for other special types. A different approach however, and possibly more productive in the long term, is to extend the Haskell side of things. The Diagrams and Wumpus libraries both support annotating graphics. It would be really great to have some standard annotations for interactive visualization, that would render to SVG files with some embedded javascript. Those files would just look like regular images to the client side code (Web.Internal.Image), and the interactivity would still work outside of wolfgang. As for ghci commands, those will probably be implemented at some point, but only after other user interface improvements.
I look forward to developing with this framework soon!
It looks like we could have a `diagrams` backend to raphaeljs (or processingjs, or even just canvas) as well, which would also be awesome!
Epic fail on my part.
The sheer number of partial functions in the Haskell standard libraries has also bothered me. Often there's not even a total equivalent provided. Also, consider the lack of a decent IDE for Haskell as another problem. No, I'm not interested in hearing how great emacs is for Haskell editing. It's not an IDE that understands your code like IntelliJ IDEA (for instance) understands your Java source.
Use continues to grow, and with that, the side effects, such as startups, and other ambitious projects. All good signs.
It was finished by the IHG quite a while ago. Even at the time it was possible to not statically link libgmp on any platform, if you want: * http://haskell.forkio.com/gmpwindows and that's a lot easier now that it used to be.
By the way, *unsafeUnjust msg = maybe (error msg) id*. By which I mean to say that maybe is really the natural way to deconstruct Maybe.
 unsafeUnjust :: Author -&gt; Address -&gt; Maybe a -&gt; a
Diagrams already generates SVG images, currently only through Cairo, which limits the possibilities, no javascript event handlers for interactivity. A native SVG backend that can do this would be very welcome indeed! (join #diagrams on freenode if you're interested in helping) I don't think outputting RaphaelJS Javascript instead of SVG would really buy us much, except for support of older Internet Explorer browsers.
Yes! This sort of thing is exactly why diagrams is backend-polymorphic.
I tried Leksah a while ago and it was incredibly buggy. &gt; I’m currently writing a decent IDE for Haskell in Emacs and I’m interested in adoption deal breakers and features you consider essential. Essential? * Syntax highlighting, bracket matching etc. * Move/indent blocks of code easily * Code completion * Go to... definition, first use, etc. * Intelligent search and replace * Tight integration with the compiler Useful: * Infer type at mouse pointer * Automatically generalise function to.. * Recognise unused imports * Auto import * Check my code for syntactic errors in the background * Generate Haddock stubs * Code folding * etc. And a million more things that you never actually explicitly notice the IDE is doing/can do when editing in the big Java IDEs but suddenly miss when you move to Haskell. Ironic, as Haskell's types should make many of these tasks easier.
`cabal install safe`
Another big must-have feature for me is automatic entity renaming and moving. This should work for at least module, function, and parameter names. It seems like a small insignificant thing, but in my experience it makes a big difference in code quality and maintainability over the long term.
hm, also isn't the 'endgame' of its type-system in sight? With typekinds and kind polymorphism apparently in the todo queue, where can it go further, without just becoming dependently typed - which would be quite a different language? Quite a few kinks to iron out around this basic structure sure, but fundamentally?
thank you. this list is a great place for some benevolent dictator to sit down and reassemble the awesome bits lying around into a blessed whole. 
`reallySafeJust :: Proof (Maybe a -&gt; a) -&gt; Maybe a -&gt; a`
&gt; It was finished by the IHG quite a while ago. Great! But what does that mean? In practice, how does one go about compiling a Haskell program without linking in GMP, on various platforms? I certainly do hope that it is indeed a lot easier now than it was when the post you linked to was written. Whew! That's what I meant - I don't think I would have risked attempting that for a real-life mission-critical software delivery without coaching. Anyway, with or without coaching, that procedure is specific to old versions from those days and is not relevant today.
http://hackage.haskell.org/trac/ghc/wiki/Building/Using#Commonbuild.mkoptions Documented in the GHC build process: &gt; INTEGER_LIBRARY &gt; &gt; By default this is set to integer-gmp, which means Integer is implemented on top of the C GMP library. If you set it to integer-simple than a simple, BSD-licensed Haskell implementation will be used instead.
I happened to be linked to this http://www.cs.kent.ac.uk/projects/refactor-fp/hare/demo.html today. This sort of thing is what is needed in any serious IDE. Unfortunately HaRe only works with 100% Haskell98 code. An adaption which supports GHC extensions would be a godsend.
&gt; Another example of fragmentation is MTL, transformers, etc. these are different packages, and mtl is defined *in terms of* transformers. See the recent proposal (and its acceptance) into the Haskell Platform in 2011. Also, I don't buy the idea that partial functions in the base library are going to hinder adoption. */me looks at every other language*. Improving the libraries are a good thing though. And we will keep doing that.
Is the GSoC proposal online somewhere?
does 'Catch' tool solve the partiality problems? (I only remember hearing about it, not even sure if it even exists for that matter)
I've been saying this [for a long time](http://www.haskell.org/pipermail/libraries/2001-May/000416.html).
I am not saying partiality in itself will hinder adoption. I am saying it is a bad choice in the context of a language geared towards safety. Also, I think it is particularly bad when combined with lack of debuggers. In other languages, when encountering partiality, I can just pop up a good debugger. In Haskell, I have to make do with some cryptic message like "Index out of bounds", or "head: empty list". No stack trace, no debuggers, nothing.
Can we get things renamed to `unsafeForkIO` and `unsafeRandomIO`, too? Probably `unsafeTakeMVar` and the like as well.
This is a good list. Some remarks: * The 'safe' library, mentioned by someone already, has safer alternatives to some common partial functions. I'd love for the Maybe versions of these to be in base (instead of the current head, read, etc). * I disagree a bit on the fragmentation in API between containers, bytestring etc. While there is no common interface, the fact that naming is consistent means that changing from one type to the other is often no more difficult than changing an import. * I think the mtl/transformers problem was more or less solved by mtl 2 (which is transformers plus the old monads-fd). I haven't had real problems here for a while. * As for the last point, about cabal: this has been a source of frustration for us as well, but I lean in the other direction. I've seen more problems due to packages not following the PVP than due to packages following it.
Where `type Proof a = a` ?
After about 2 years of being afraid of ghci debugger I tried it and I was pleasantly surprised. Just try it! google:// ghci debugger It is different than gdb fi, because it must handle evaluation. But it does the job.
Are you mocking my complaint about getContents? I think there's a difference between these -- because getContents is particularly susceptible to being treated as deterministic but dependent on evaluation order, rather than simply indeterministic. In fact, you can see stack overflow answers to newbies (by experts) that tell them to force their pure values evaluation *before* calling hClose and such so that their lazy I/O works. I think the key is that there is a difference between "consistent with a non-deterministic view of things" and actually non-deterministic. One actually exposes evaluation order, and the other is non-deterministic. Lazy I/O also sucks for many other reasons, and should probably just be abolished already.
I would also very much like a "reorganize imports" tool that simply alphabetizes existing imports, perhaps with spaces between each block from a given top-level hierarchy. I toyed with doing this with haskell-src-exts once and got it *almost* solid, but there were some irritating issues about comments and preserving source-formatting and I let it drop to get back to other projects. for reference -- the code however I left it: http://hpaste.org/46855/reorganize_imports
Even just module moving/renaming on its own would be a great feature, actually!
Looks impressive from a quick skim!
The available array libraries are nowhere near singularity level. I've spent some time digging around inside the source of repa, and attempting to use it for some projects, and it's missing a lot of features I'd expect to see from a scientific-computing-ready array library. What's more, clever as repa's implementation is, I couldn't for the life of me figure out how to add the features I wanted. This may be more a reflection on me than on repa's design, of course.
&gt; Are you mocking my complaint about getContents? Yes. Because opponents of lazy I/O too often overstate what exactly it breaks about the language (even oleg does this). It's a pet peeve. &gt; In fact, you can see stack overflow answers to newbies (by experts) that tell them to force their pure values evaluation before calling hClose and such so that their lazy I/O works. Yeah, I see that answer a lot, although I don't read stack overflow. But this is not ensuring that their lazy I/O works. It's ensuring that they don't use lazy I/O. The correct answer is to not call `hClose` on a handle involved in lazy I/O. Ever. If we want to get really operational, you should also never open a handle, pass it to another thread, and then close it in the original thread. But this is never the answer people give for lazy I/O; that is, they never suggest to use it correctly. Now, if the reason that the newbie wants to `hClose` in the first place is because of dangling resources (and associated errors), then "don't use lazy I/O" is the correct answer, because lazy I/O sucks at that. I guess forcing the whole string is an option at that point, but it's not a great one. I'm a little reluctant to get rid of lazy I/O all together because it works for some simple use cases that the alternatives don't (yet) handle anywhere near as simply in my experience. If you want to do anything remotely serious with I/O, you probably need to switch to something else, but you can do a lot in certain domains without getting very serious about I/O.
The problem is that if the language admits use of forcing evaluation in combination with lazy I/O to avoid problems with hClose/etc as correct, then I think it is game over: Haskell loses its purity and referential transparency (and for very little gain, too). As you said, it should never ever be used, and thus be *disallowed*, but unfortunately it is not only allowed, it is often depended upon and taught as a technique. Lazy I/O leads to poor code quality. That is, code that does no proper error handling. There should be no excuse for that, even not for Quick&amp;Dirty code. Not in a supposedly safe language. Lastly, I disagree about Iteratees not being "anywhere near as simple" to use for the simple cases. At most, they are slightly more complex. 
I'm currently using Scion with GHC7 with the experimental fork mentioned in http://www.haskell.org/haskellwiki/Haskell_mode_for_Emacs#Scion_Integration but I've seen that nominolo has been catching up lately in https://github.com/nominolo/scion/tree/scion-2nd-attempt As for cabal-dev I haven't tried yet (I didn't need cabal-dev yet, so I didn't bother yet), but I assume the answer would be "not yet"... :-)
I strongly disagree with these points. Lazy I/O does not lead to poor quality code when used correctly, whereas in my opinion CPS-based code in Haskell is almost always poor quality. I find it *far* easier to write beautiful, readable, maintainable code using lazy IO than with any form of iteratees. But your original point still stands. By nature, it's not obvious what the best way is to do IO in a pure language. We have made a huge amount of progress, but there is still more to learn. Nevertheless, we do have enough to be able to use Haskell comfortably for real-life general purpose programming. I do it for a living now, and so do a quickly increasing number of people. In fact, in my opinion Haskell is already much better at IO than any major impure language. So I'm eagerly anticipating a steady stream of future innovation. In the meantime, I'm happily developing in today's Haskell.
&gt; Lazy I/O does not lead to poor quality code when used correctly How do you handle I/O errors with lazy I/O, when they are not properly propagated by the libraries? (e.g: errors reflected as a premature empty list). I think Iteratee code is only unreadable due to an extremely poor choice of names. If you rename iteratees/enumerators/etc to Consumers/Producers, and replace some of the operators with meaningful names, it is pretty readable. The CPS style is really only inside the combinators -- the high-level code should just glue things together and not feel like CPS at all. In a sense, all I/O code is really CPS code, including lazy I/O. CPS can be abstracted into nice code, and "do" sugars CPS to look streamlined nicely. I'm not saying today's Haskell is bad. I think it's pretty great. But I also think it could be awesome rather than great -- if the points in my OP were addressed.
RyanT5000 described Haskell as "a powderkeg of innovation", and I think that description is pretty accurate. I'm not talking about widespread adoption, but I do think a big shift in the industry towards FP is coming, but I can't say when. I am seeing some encouraging signs, though. Now, of course it may not be Haskell. On one non-FP industry project, I'm faced with the question of how to engineer it for the future. I'm already an FP convert but no-one else in the office is, however, one of the other developers has become so frustrated with months on end of fixing bugs (an experience I went through last year on the same project), that he has borrowed my copy of Learn You A Haskell. The project is about to scale up enormously, including more emphasis on parallelism and distributed processing. To me, it just seems like the wrong technical decision to continue to base that on C++ when Haskell is up to the job and C++ (even with C++2011) simply is not. We're now reaching the point where everyone acknowledges that CPU cores are not getting faster. Companies all round the world will be facing the same technology question - "How do we scale this up?". Most will muddle on with the current inadequate technology, compounding their already exploding budgets and annoying the hell out of their customers, but it seems to me inevitable that a small minority (those who have been infiltrated by Haskellers) will be forced into the conclusion that Haskell is - in the current technology landscape - the only option. [I say Haskell is the only option for parallelism because, according to my own flawed survey of currently available technology, it is so hugely far ahead of any other mature technology, and the difficulties with parallelism are very serious ones. I know because I am faced with them every day.] Other signs: My brother manages a team of about 10 developers, which in New Zealand is considered very large. New Zealand is known in marketing and political circles as a microcosm of the western world, and so a lot of experiments are done here. He has taken functional approaches about as far as he can in Java and is considering adopting Scala, but he would really like to lock effects down, which only Haskell can do (but Haskell is not Java-friendly like Scala). If his team does formally adopt Scala, it will be the biggest Scala project in New Zealand and in my opinion that's a litmus test for the rest of the world. So, I am watching him with considerable interest.
I wonder what Haskell will look like in 2020, if people take the plunge to take the axe to the Prelude (and the Haskell Report, I suppose). Lists and string-like-things as typeclasses? No partial functions like 'head', unless their names are prefixed with 'unsafe'? Haskell has a lot of artefacts coming from the time when it was used primarily as an educational language; maybe it's time to start using it as a serious programming language instead! 
Amazing! Who did this? It's like mathematica meets haskell online.
&gt; The problem is that if the language admits use of forcing evaluation in combination with lazy I/O to avoid problems with hClose/etc as correct, then I think it is game over: Haskell loses its purity and referential transparency (and for very little gain, too). No. This is the kind of statement I'm talking about. Show me an example where referential transparency is violated. For an actual, specific definition of referential transparency. Referential transparency isn't just a fancy term for "behaves in a sensible way." It's more or less the ability to freely do CSE and expression inlining while preserving meaning. So using `unsafeInterleaveST`, I can construct an example such that: let x = &lt;e&gt; ; y = &lt;e&gt; in ... behaves differently than: let x = &lt;e&gt; ; y = x in ... for some code fragment `&lt;e&gt;`. So where is the example for lazy I/O? I can fully agree that lazy I/O makes the 'semantics' of the embedded `IO` language a bear, or useless. And maybe it's informal assumptions we make about how programs work. But that isn't sufficient to violate referential transparency or purity. I don't think we should throw around accusations of this gravity without non-hand-waving evidence behind them. &gt; Lastly, I disagree about Iteratees not being "anywhere near as simple" to use for the simple cases. At most, they are slightly more complex. The sort of case I'm talking about is where I have an algorithm written on existing, pure data structures, where the algorithm incrementally consumes the pure data structure. An example is compression of a string. It makes some sense to have `compress :: String -&gt; String`, for a naive type. With lazy I/O and this pure, lazy compression algorithm, I can read in a chunk, compress it, and write out a chunk. The process will be interleaved automatically. As far as I know, I have two options with iteratees: 1. Read the entire string into memory, and call the pure function on it. This loses the interleaving. 2. Rewrite the algorithm to incorporate iteratees into my data structures. Neither of these options seems particularly thrilling compared to the lazy I/O way of doing it. 2 can of course be avoided by having iteratees in mind from the start, so maybe I'm just not up enough with the times for that, and I need to evolve (although, having 3 or more competing iteratee implementations seems like a bad situation in this respect).
&gt; No. This is the kind of statement I'm talking about. Show me an example where referential transparency is violated. Does exposing of evaluation order count as a violation of referential transparency? str &lt;- hGetContents h evaluate (f str) hClose h -- now, in pure code, I can learn about the evaluation -- order/depth of 'f' by just looking at how much of str -- exists (empty list means it was not evaluated at all).
Consistent naming is not enough.. If you want to work with multiple libraries, and each uses a different type of ByteString, you have to manually translate back and forth. You also can't write code that is polymorphic to container types... 
I don't know if partial functions are such a big problem, but any standard library function that any newbie will encounter within their first few sessions with the language should not throw an unrecoverable exception that crashes your program without any indication from the type system that it will do so. I mean if that is the case - what is the point of all the type safety in the first place (which is so incredibly awesome!).
Exposing how? Unless you can produce an example where CSE/inlining is invalid, you probably have not violated referential transparency. And unless you can produce an expression that evaluates* to different, non-bottom values depending on the evaluation strategy used, you probably haven't broken purity (if my memory serves). Informal talk about sections of code and/or people learning about evaluation order may or may not have anything to do with referential transparency or purity. * And although I rather dislike the "execution vs. evaluation" distinction that people like to talk about in Haskell, I mean evaluation in that sense, and expression in the sense of terms that undergo transformation by said evaluation.
&gt; As far as I know, I have two options with iteratees I thought about this a little more, and it strikes me that it's not just that I have to choose between poor performance and inconvenience. There's something a little more that sticks in my craw, and I think I figured out what it is. When you read something like Why Functional Programming Matters, one of the big selling points of laziness is that you can break things down into nice, pure, logical sections, and then compose them together, with the resulting program executing everything in a nice, efficiently interleaved way. This is built into the language, and it makes things like the pure, incremental string compression algorithm behave very nice. Then, lazy I/O piggy backs on this to let you write incremental I/O processing in the same way (and eventually fails when you try to write something that really cares about high-end I/O). By contrast, iteratees throw this natural incremental processing out the window. Interleaving is not achieved by the evaluation strategy, but because algorithms are couched in terms of chunked operation, and such algorithms can be composed into larger algorithms with the same property, with interleaving. But there is no way to take a pure algorithm that naturally takes advantage of the incremental, interleaved processing afforded by lazy evaluation, and invert it into a chunk processor, so we have to throw the lazy evaluation incrementalism away and couch everything we want to interleave properly with I/O in terms of iteratees. And that rubs me the wrong way. I like pure code using pattern matching on algebraic data structures better than I like composing chunk processors.
You can use Handles that have side-effects just for reads (e.g: socket/pipe). Then you can evaluate some, and write to the other end to see if room cleared up. Also, it sounds that by the logic you're using: any side-effect that only observes mutable state and memoized for each IO invocation is pure and OK from the ref. transparency standpoint -- since in a single program the memoization will prevent CSE/inling from causing trouble. But across multiple executions of the same program, where all that's changed is evaluation strategies in pure code, you can get different results.
I don't know about a wolfram alpha clone, but this looks like an awesome blogging/teaching tool.
It would be awesome if it had the smarts to turn parametric graphs into ones that can be altered using javascript, so that the client can alter it in realtime :)
Sadly `Proof` in haskell would have to enforce totality (and termination)
It does exist but it's pretty much unusable these days cos it only works on YHC.
luite (#haskell @ Freenode) or [luite2](http://www.reddit.com/user/luite2) here on Reddit.
&gt; Also, it sounds that by the logic you're using: any side-effect that only observes mutable state and memoized for each IO invocation is pure and OK from the ref. transparency standpoint -- since in a single program the memoization will prevent CSE/inling from causing trouble. It depends what you mean by this. For instance, implementing laziness by hand using a mutable variable does not break referential transparency (in some hypothetical ML-Haskell hybrid): data Lazy a = L (Ref (Maybe a)) (() -&gt; a) delay :: (() -&gt; a) -&gt; Lazy a delay = L (newRef Nothing) force :: Lazy a -&gt; a force (L r f) = case read r of Nothing -&gt; let v = f () ; _ = write r v in v Just v -&gt; v And in the case of lazy I/O, we have: v &lt;- getContents and in practice, the reading of `v` is delayed until we look at it, but it is then fixed, and there is no way to observe any change of it. And you cannot create another name binding an expression identical to the one `v` binds, because `v` is not really a name for a particular expression in any scope, as the above is sugar for `getContents &gt;&gt;= \v -&gt; ...`. And the excuse is roughly the same as it always is for `IO`. The language term you get for: getContents &gt;&gt;= \v -&gt; ... is the same regardless of your evaluation order, and you can pull subexpressions out, or substitute them in and get the same thing. It's some representation of I/O to be done. Maybe now it's specifying that you need a crystal ball to successfully determine what occurs when you execute `getContents`. And that's bad, but it's bad because reasoning about I/O (and the embedded language we use to talk about it) is something we want to do, too, and complicating `IO` makes that more difficult. But not because it breaks referential transparency/purity of the expression language (as far as I know/have seen).
I think proposal for class aliases was motivated precisely by allowing easily changing a Prelude to some other definition. So when we get something like default superclass instances, and I gather context synonyms are to be already in 7.2, changing prelude will be just a matter of using the relevant -X flag.
&gt;It's not an IDE that understands your code like IntelliJ IDEA (for instance) understands your Java source. Not that I disagree with your main point (there should be a tool for everyone's taste, and emacs certainly requires particular taste), but doesn't CEDET indeed understand your code in the manner you're describing: http://cedet.sourceforge.net/intellisense.shtml 
hmm, I'd love to see a tool like that integrated into GHC; would sidestep the partiality in libraries problem, since you could guarantee safety even in its presence.
&gt; since you could guarantee safety small print: not a guarantee.
oh? could you elaborate? I thought the point of the tool was that it "ensures that a pattern-match error does not occur. "
I think this is great, and I'd be interested in contributing if possible. I've been using a somewhat similar-in-spirit combination of OpenGL and OpenCV from Haskell in projects for quite some time now, but have never been able to package things up. The trouble is reducing the scope of this kind of effort enough that a well-designed package comes out. One typically ends up with partial coverage of so many things that straying from the paths blessed by the package author becomes either dangerous or impossible. Perhaps sufficient automation of binding generation is an answer.
Safety is not guaranteed by a lack of pattern match errors. Furthermore, not all pattern match errors can be detected at compile time, seeing as some patterns would require evaluation of generative recursion: nonterminate :: a -&gt; Bool nonterminate x = x `seq` (nonterminate x) patternmatch_error v | nonterminate v= True | not (nonterminate v) = False Pattern match error, or no? This stuff can't be universally statically detected. 
Really nice. But there's much more in Wolfram Alpha than nice plotting. It's the really surprising ability to understand what you mean and the vast amount of real-world data available: [example](http://www.wolframalpha.com/input/?i=San+Francisco+to+New+York+at+walking+speed)
Awesome!
Oh, sry, right, I meant only safety from pattern match errors; the context of the discussion. (though I gather the rest of the guarantee of safety can be had with the coming XSafe family of flags, for those projects where the expressivity of the language restricted to trusting only safe things is enough) Well, what does Catch do in cases such as this one? I thought the idea was that if it was satisfied that p.match/use of partial functions was safe, it indeed was safe, though there could be situations where its just unsure if something is safe or not (I guess that's like generating a warning, so a programmer might know if s/he has eliminated these as well). Skimming the paper, I find pretty strong statements, like: " We have also given an argument for the soundness of our method in (Mitchell 2008), showing that if Catch declares a program free from pattern-match errors then that program is guaranteed not to crash with a pattern-match error." So, that IS a guarantee, and you COULD work to get it.
Right, but why aren't these in the Prelude? Why is the Prelude cluttered with partial functions, and why does it not provide the total alternatives itself?
&gt; Intelligent search and replace — Sounds interesting; what is it? It means that the IDE should recognise that if I've renamed a function in one module, then any other modules depending on the altered module also need to be modified to take into account my changes. You should be able to set scopes through which search and replace works. Comments should be largely ignored by search and replace, unless you force otherwise (searching and replacing "and" likely does not mean I want to change all occurrences of "and" appearing in comments). You should be able to do a "structural search and replace" which binds syntactic elements to variables which can then be used in a pattern during the replacement. etc. etc. If you're writing an IDE, get a hold of IntelliJ IDEA. It's pretty impressive what it can do with your Java source. I think there's a "community edition" that's free now.
By "understanding your code" I meant more than IntelliSense. I mean things like search and replace understanding the structure of the language, refactorings, the compiler recognising missing imports and automatically inserting them, recognising that what you've written will not compile, recognising missing functions and inserting stubs for you, and so on.
I'd like to see a language flag to give me lens declarations data Record = Record { myField &lt;: FieldType } -- random syntax I made up where `myField` has type `Lens Record Field` and `Lens` is an abstract data type that has some getter, setter, modifier, store coalgebra views and constructors and is a `Category`. How would one go forward making such a proposal? Would I have to make a GHC patch first to be taken seriously?
well, CEDET's Semantic is not just intellisense either; it looks like infrastructure for understanding your code fairly deeply. From the list, features do include recognizing that what you're written "does not fit the language", "simple navigation through the parsed language" etc... http://cedet.sourceforge.net/semantic.shtml Point is, emacs with such extensions does indeed look like an IDE reasonably ambitious in how it understands your code, so potentially awesome hs environment - for those that prefer its mode of interaction ofc, and possibly modulo some hacking of specific features... Agreed on lacking a (GHC-extension) complete refactoring tool; HaRe needs more ppl working on it apparently :) but I do like their list of transformations..
Well put. Nobody has yet come up with an example of lazy I/O that violates referential transparency. It always boils down to it violating the intuitive semantics of I/O (which it certainly does), but not referential transparency. One has to remember that every IO operation basically returns a random result, there are no further promises.
The other thing Emacs doesn't do is manage your build, integrate with version control, etc. All that boring stuff that real world engineers use.
You've missed refactoring. The refactoring potential for Haskell is huge. I'd love to be able to split out expressions into locally defined functions automatically. Or take a locally defined function and promote it to the top level. What about automatically generating type signatures on methods.
I made two separate arguments in my comment. You have conflated them. 1) Safety is not guaranteed by a lack of pattern match errors. This is obvious as IO and other things allow for non-safe actions that could result in getting stuck at runtime and crashing. Pattern match error safety is tautologically viable from the description of the project. 2) The example was meant to illustrate that sometimes pattern matching can depend on the liveness property (*not* safety property) of termination. Therefore, trivially via the halting problem, statically eliminating *all pattern match errors* is undecidable. for the record, Mitchell's argument of soundness ignores pattern guards and thus circumvents this particular problem. 
re 1), yes, I simply shouldn't have said safety; I was speaking only about the safety problem raised by the OP, namely pattern match error safety. re 2) yeah, I'm probably too confused to continue this discussion; ofc, one cannot programmatically determine if any arbitrary program has a property or not (by Rice’s Theorem)- but, one can prove an arbitrary program cannot (for eg.) 'go wrong', accepting that there will always be a class of programs which are actually ok, but nevertheless rejected by that method. This is the tradeoff any type system makes too; I thought Catch could then give a similar kind of _conservative_ guarantee about pattern matching errors, indeed proving it cannot go wrong. well, guards just get desugared away anyway, since it works on Core; he claims the thing supports full Hs98.
You are right, but catch's handling of guards is bad: Seeing as every guard must accept a boolean predicate, it is unable to evaluate any of those predicates (even `otherwise`), therefore catch suggests pattern match errors every single time a guard is used. It's not very nice. 
ohhh, I see, that's indeed quite unfortunate. Well, then, we can only hope for a tool that is both a) integrated and b) more polished :)
At first blush, this seems to be partially based on the technique for building a sentence generator illustrated by Norvig in PAIP. Edit: It appears Norvig reprints some of his sentence generator [here](http://norvig.com/python-lisp.html), for the purposes of demonstrating the differences (and similarities) between Lisp and Python.
Hugs also had a version of Eliza included in the `demo` directory.
By the way, the code I provided preserves comments and formatting perfectly (I think) for everything *except* the import block area. And of course if you're reorganizing imports there's no one "right" place to put orphaned comments. But there's probably an "almost right" way to do it, and that's what I got stuck on pushing through with.
If you implemented this in a quasiquoter and garnered support, I think that would be sufficient. Patching GHC, especially when it comes to adding syntax (and hence messing with the lexer and parser), is significantly more work. Would Record be a normal record too, by the way, or just an ADT then? I have no objection to the latter, except that you lose some neat features regarding runtime introspection from Data and regarding compile-time introspection from Template Haskell. But having the lenses themselves obviates maybe 50% of the need for these features :-) The rest of the need is probably filled by the proper Multiplate instances...
Is this a lazy post?
Don't evaluate it, or you might infinite loop!
Help! I'm stuck in a loop.
http://blog.ezyang.com/2011/05/computing-function-composition/
Cool stuff. I need to get back into Haskelling. Haven't really used it since I took Applied Logic.
&gt; What about automatically generating type signatures on methods. This is possible already (if I understand you right): [inserting type declarations in haskell-mode](http://www.haskell.org/haskellwiki/Haskell_mode_for_Emacs#inferior-haskell-type_.28C-c_C-t.29)
what do you mean by "manage your build"? ...as for integration with version control, I use [magit](http://magit.github.com/magit/) which for me personally is the best Git/IDE integration I've seen so far
Are you aware of scion and yi? There seems to be a lot of wheel-reinventing going on round these parts...
Unfortunately you have to compile every module as interpreted, which could be a problem if the actual bug you're trying to find is squirreled away in some dependency of some dependency of some dependency of your code. Cabal should introduce some kind of feature to make this easier.
Is rewriting code to use iteratees something that could in principle be automated? Would make a nice little research project if so.
According to Jon Harrop (not exactly an unbiased source) F# is a good option for parallelism. Actually he says it's a better option than Haskell. But then he has a long history of being anti-Haskell, so take that with a pinch of salt. Of course F# doesn't isolate effects in the type system. It's just that as with Ocaml, on which it's based, there's a cultural bias towards writing pure functions, because it's an explicitly functional language. (Not so much with Scala.) But of course F# still has to interface with the imperative .NET API. The JVM (and hence Scala) doesn't even have tail call optimisation yet, which arguably means that languages like Scala that get compiled to Java bytecode aren't ideal for *object-oriented* programming, let alone functional programming.
Haha, if you'd predicted that Haskell (the language) had only a few more innovations to produce at any time over the last 20 years, you'd have been wrong. Are you so sure?
Yeah, using the word "singularity" here sounds like absurd hyperbole to me. Ignition would be more appropriate.
I'm surprised no-one's mentioned safe-lazy-io yet. I foolishly imagined that it had been obsoleted by these new-fangled "iteratees" that everyone's been talking about, but reading this discussion, I realised it hasn't. &gt; The correct answer is to not call hClose on a handle involved in lazy I/O. Ever. Right, you should not call it explicitly, you should use safe-lazy-io to do it for you. &gt; Now, if the reason that the newbie wants to hClose in the first place is because of dangling resources (and associated errors), then "don't use lazy I/O" is the correct answer, because lazy I/O sucks at that. I guess forcing the whole string is an option at that point, but it's not a great one. You probably don't need to force the whole string read from the outside world - indeed, if you did, it wouldn't really be *lazy* I/O, would it? Maybe you need to force the value computed from the string that's been read in (or some intermediate value), which is what you do when you use safe-lazy-io - or rather, what it does for you. Plus, it handles dangling resources and errors. What do you think of safe-lazy-io?
&gt; Lazy I/O leads to poor code quality. That is, code that does no proper error handling. What about lazy I/O via the safe-lazy-io package?
I haven't really looked at it, to be honest. I only remember a bit from when it was first released. My vague recollection is that I'd probably prefer it to iteratees. I don't know if it covers all the things iteratees buy you, but it might at least extend the realm of 'simple cases' for which lazy I/O can be used. I think it might have a branding problem, though, since having "lazy I/O" in the name can poison the well.
XD very good point; maybe its just me running out of imagination :)
Just keep away from metaphors! I repeat: keep away from metaphors! They are evil!
&gt; According to Jon Harrop (not exactly an unbiased source) F# is a good option for parallelism. Actually he says it's a better option than Haskell. But then he has a long history of being anti-Haskell, so take that with a pinch of salt. From what I recall, his claim is entirely based on the CLR having a concurrent garbage collector. And every other collector "doesn't scale," because it isn't concurrent.
where can we see the source?
doh! looks like I can't handle my bookmarklets properly. thanks sclv for posting the link!
`tau = 2 * pi`; go nuts. I don't see a reason to rush it in, and I'm a pretty strong tauist. (And remember multiplying a floating point number by 2 simply adds one to the exponent; it is not a lossy operation.) 
Name a thing you think emacs doesn't do. You're wrong. To a first approximation, anyhow. But it certainly does the things you claim it doesn't, probably since before you started your career and possibly since before you were born.
This was a really good read. The solution to this is simple, we have a great community. Lets combine projects and actually work together to make canonical packages for every type of task (or 2-3 options at most; some choice but not overwhelming).
I.e. the Haskell platform?
Touche. Guess I did not really think about my comment. I guess the problem is really with packages that are not in the platform. There almost needs to be more 'togetherness' in the packages outside the platform too.
Exactly. The best of both worlds can be achieved. Hackage is the wild library ecosystem where multiple competing solutions can exist. The Haskell Platform is a set of community sanctioned libraries for very common tasks.
I believe the new Hackage was meant to improve on this by making it easier to see which packages are the most widely used.
My version from about a year ago: http://www.felixcrux.com/posts/hector-literate-haskell-implementation-eliza/ I don't think it's as nice as this one, but it has a bit more explanation going on.
Slight quibble. It is lossy on those with maximum exponent... Irrelevant for Pi, of course.
...what became of the *new* Hackage btw? When was it expected to be launched?
I seem to be unable to post a comment with OpenID, hence I'll comment here. Interesting survey. I think the distinction between behaviors/events and signals is not clear, though. Signals seem to be a weird mix of behaviors/events; they have only one value at each point in time, but they are also discrete. The weirdness can be seen from the fact that `Behavior` and `Event` have different notions of product: (Behavior a, Behavior b) = Behavior (a,b) (Event a, Event b) = Event (Either a b) Second, I don't agree with the second paragraph of section 3.1. The non-Haskell libraries make heavy use of effects and it is not clear to me whether they fit Conal's classic FRP semantics. Ostensibly, they use the same concepts, but from what I've read, they don't even make an attempt at spelling out their semantics precisely. Last, I think that the important work by Wolfgang Jeltsch and Gergely Patai on the Haskell/Signal side is missing. (Not to mention my own reactive-banana, but I haven't formally published anything about it yet.) 
Thanks for your FRP survey - I've found it very valuable. You or others might be interested in "Deprecating the Observer Pattern" by the Scala people. The related work section covers some interesting non-Haskell FRP implementations, and the document also has some rather good arguments against the observer pattern (potentially useful for promoting FRP to industry). http://lamp.epfl.ch/~imaier/pub/DeprecatingObserversTR2010.pdf 
Some people were wondering about the status of Hackage 2.0 which (among other things) was supposed to include dependency metrics to help decide which packages are used how much. This is what a quick search revealed: In April 2010 [Matt Gruen asked for Feedback](http://www.haskell.org/pipermail/haskell-cafe/2010-April/075906.html) on a possible GSoC project Hackage 2.0. [The corresponding Trac ticket](http://hackage.haskell.org/trac/summer-of-code/ticket/1587) has been closed as fixed three months ago (despite mentioning "Mentor: not-accepted") with a comment that, according to Matt, "the new Hackage server is pretty much code-complete, it just needs some effort to replace the existing Hackage instance". Is more recent information according to the status of Hackage 2.0 available?
It's not so much community sanctioned as it is oligarchy sanctioned.
Do you know if pi sees any special treatment or optimisation regarding performance?
Choice of two? http://hackage.haskell.org/package/sqlite http://hackage.haskell.org/package/direct-sqlite The other candidates are evidently parts of larger frameworks. If you're already using the framework then it's cool that you can use SQLite too. If you aren't using the framework, I personally wouldn't want to drag in all those dependencies... It took me less than two minutes to whittle this one down.
The source is not yet available, sorry. I'll publish it later, when the site is actually running. The current version is a very early preview on my development server, that wasn't even supposed to be made public yet, but since the url was mentioned in two reddit messages, I decided to keep it online for a few days. If you already want to help out, contributions to Diagrams would be very welcome. Apart from basic features that are still missing, like text support, gradients, it would be great to have some support for declarative interactivity in images (that would probably require svg+javascript to be generated, or something similar). Having such features wouldn't only benefit Wolfgang, but would also make Haskell more usable for other graphics tasks, like interactive web graphics with one of the great web frameworks, or high quality graphics for scientific publications.
Alas, I was reading only a selection of the papers I wanted to read. My school operates on quarters, so I only had 10 weeks instead of the usual 15 for readings, and there were certainly papers I wanted to read but didn't have time for. I think the paragraph you refer to is unclear. The implementations described fit the semantics in that they use notions of behaviors and events as first-class values, rather than abstracting to signal functions. You'd be quite right in saying that for the most part no one bothered to show proof of formal semantics for the non-Haskell/Agda implementations, except I think for RT-FRP and possibly EFRP, though almost all of the papers except the one on Frappe gave some idea that "we are trying to fit these semantics" I am following your work on reactive-banana with some interest. When you say the distinction between behaviors/events and signals do you mean Classic FRP behaviors and signal functions? Signal functions conceptually have a value at every point in time (except for signal functions carrying events, an unfortunate semantic hack which N-ary FRP is working to remove the need for). Is that what you were referring to or am I missing something? I wonder why OpenID is not working, but it's not necessarily something I can fix directly since I use Google's Blogger service. Even so, are you using a Google/Yahoo/similar OpenID or are you hosting your own?
[This example](http://www.haskell.org/pipermail/haskell-prime/2010-January/003076.html) uses `seq` to make it fairly compiler-proof. It's possible to construct something that violates referential transparency from that, for a specific compiler version.
Two columns, a choice not forced on you by an editor old enough to think everyone uses paper?
Indeed, the dependency isn't needed anymore. I removed it in the latest release 0.4.0.2
If it's possible, then let's do it. Show me the example based on this code where inlining or factoring an _expression_ causes the program to behave differently. For which compiler version are you talking about? I don't believe that the above can be used to violate referential transparency. The linked code says it's based on an example by oleg, which I've also read. And oleg's code doesn't contain a violation of referential transparency either, as far as I can tell. I can't recall if he claims it does, but if he does, I think he's wrong about that. Just to make things concrete, the fact that we cannot replace: s1 &lt;- ioAction s2 &lt;- ioAction with: s1 &lt;- ioAction let s2 = s1 is not a violation of referential transparency. And the fact that 'we know behind the scenes' that our choices in pure code affect how 'bindings' from `IO` obtain their values doesn't either. Referential transparency is about, in another sense (I think), beta conversion being valid. That is, you can replace: ... e ... e ... with: (\x -&gt; ... x ... x ...) e And vice versa (not worrying about types for this characterization). And that's about it.
Did anyone read the [linked article](http://www.winestockwebdesign.com/Essays/Lisp_Curse.html)? It comes of as *incredibly* smug. &gt;Some smug Lisp-lovers have surveyed the current crop of academic languages (Haskell, Ocaml, et cetera) and found them wanting, saying that any feature of theirs is either already present in Lisp or can be easily implemented — and improved upon — with Lisp macros. They're probably right. &gt;Pity the Lisp hackers. &gt;Dr. Mark Tarver — twice-quoted, above — wrote a dialect of Lisp called Qi. It is less than ten thousand lines of macros running atop Clisp. It implements most of the unique features of Haskell and OCaml. In some respects, Qi surpasses them. For instance, Qi's type inferencing engine is Turing complete. In a world where teams of talented academics were needed to write Haskell, one man, Dr. Tarver wrote Qi all by his lonesome. &gt;Read that paragraph, again, and extrapolate. If this is the attitude of most LISP hackers, I can easily understand why they can't stand to work together. I'm thoroughly unconvinced that multiplication of libraries are a consequence of a language being "too powerful". Scala can be said to have a (gasp!) roughly similar expressive power as Haskell, and the library duplication problem doesn't seem to be as important. Maybe the problem is simply what it appears to be, a lack of industry backing.
Concerning the semantics, what I mean is that there are a surprising number of subtly different semantics for the `Event` and `Behavior` approach. For instance, the function union :: Event a -&gt; Event a -&gt; Event a that merges two event streams may or may not keep duplicates. Also, when do `Behavior`s change: simultaneously with or after the events that were derived from. Only Conal makes this precise, whereas the non-Haskell approaches don't spell it out and likely have subtle problems. &gt; When you say the distinction between behaviors/events and signals do you mean Classic FRP behaviors and signal functions? Yes. (Strictly speaking, I consider a "signal function" to be a function that operates on "signals".) Signals are a simplification, but they come with a heavy price. Mainly, implementations are almost forced to be pull-like. ------- Ah, concerning OpenID, I was simply stupid: I didn't use the full host name. :D (I'm using myopenid.com) 
Not vaporware. The code exists. There's been a demo version of it up and running on sparky since the end of the last gsoc! I know they need more time and volunteers to make that final push, and I agree that I have very little clue as to exactly what needs to be done, but the issue is lack of resources and inertia, as well as other intervening things like the need to switch all the other hosted haskell stuff earlier than intended, etc., as far as I can tell. But certainly not vaporware. I would like to encourage the appropriate folks, by the way, to put out a call for whatever help they may find necessary to get hackage 2.0 moving again.
ieee floats don't treat it special. The spec tries hard not to introduce and difficult to predict bias, and special treatment of pi would ruin that. That said, I would expect every symbol mathematical package to represent it symbolically; not sure if Haskell does symbolic analysis or manipulation. 
I found this patched hackagedb with reverse dependency listing to be quite helpful in deciding what packages to use (and convincing coworker who likes to pick the most random of packages, what to not use). http://bifunctor.homelinux.net/~roel/hackage/packages/hackage.html
Oligarchy in a context you can't fork is different than an oligarchy in a context where you can fork. Open source runs a lot on the latter, and the oligarchy is incentivized to stay relatively honest by the virtual competition of what the forked project could do. Yes, I can name failures of that model too, but on average it actually works reasonably well and it's not a complete picture to just say "it's centralized", which does not provide an accurate mental model for how such organizations behave in practice.
If you're going the Functor route, it may be more intuitive to understand fmap, return, and join. This would actually work quite well if Monads were possible to define, alternatively, via these operations (with liftM instead of fmap).
And who sanctions the "oligarchs"? The community.
They are self appointed. No one sanctions anything. I should clarify that don't think it is wrong for Haskell Platform to be run by an oligarchy. They are of course free to do whatever they want. I just wish people would stop claiming that their views represent the views of the community. I do think the Haskell Platform is bad for the Haskell community; that having the HP is actually worse than not having HP. However, I don't have a lot of evidence for my claim. And perhaps this is a discussion for another thread.
I've been waiting for it for a long time, it would be good to see some kind of update on the status of the project, for sure.
Why not cabalize this and put it on Hackage? That's trivial to do nowadays.
Wouldn't it be simpler to partially apply the options to the functions that need them, and then pass the specialized functions to the generic ones? What am I missing here?
My problem with libraries are the proliferation of dependencies. Today I wanted to use [pointedlist](http://hackage.haskell.org/package/pointedlist) which ought to be a nice simple library for pointed lists. But look at it's dependencies: binary, derive, fclabels. I'm not interested in binary because I'm using the (much better IMHO) cereal library. I'm not interested in fclabels since I using data.accessor in my work. And look at the dependecies of derive: bytestring, containers, directory, filepath, haskell-src-exts (≥1.9 &amp; &lt;1.11), pretty, process, syb, template-haskell, transformers (0.2.*), uniplate (≥1.5 &amp; &lt;1.7). Why the hell am I depending on a pretty printing library to get a pointed list? No thank you. I'll just implement my own pointed list. This is not to pick of Jeff's project. I really like it, which is why I wanted to use it. Many other hackage packages suffer from the same problem.
Sounds like a Contestable Market from what I remember of my Economics. If there are no barriers to entry in a market, a monopoly will behave as if there were competition, because of the competition that would emerge it it did not. (Obviously this is very abstract and idealised)
It's actually not that bad. To be more explicit about what stephentetley said: 1. `cabal list sqlite` gives 11 results. 2. The name and description for `HDBC-sqlite3`, `haskelldb-hdbc-sqlite3`, `haskelldb-hsql-sqlite`, `haskelldb-hsql-sqlite3`, `hsql-sqlite3`, and `persistent-sqlite` all make it pretty clear that they're backends/drivers for using sqlite with another library, so that's 6 that can be ignored. 3. At a glance, `language-sqlite` seems to be a parser for sqlite's dialect of SQL, not a binding at all, so that's another one that can be ignored. 4. `hsSqlite3` hasn't been updated in over a year and has build failures on two versions of GHC; probably unusable. 5. `bindings-sqlite3` has no documentation whatsoever and hasn't been updated in a little less than a year. Also likely to be unusable. 6. Of the remaining two, `direct-sqlite` and `sqlite`, both have been updated in the last few months. The latter has a few [reverse dependencies](http://bifunctor.homelinux.net/~roel/hackage/packages/archive/revdeps-list.html) while the former doesn't, but that doesn't tell us all that much. It took much longer to write that out than it did to narrow the choice to those two; it's just a matter of knowing what to look for. Most of the above could be done automatically, as well; it would be nice if there were a better way to search Hackage.
Well, passing around the specialized functions has the same problems as the original idea of passing around the options: namely, that you have to pass things around. (Then again, typeclasses are a good way of passing functions around implicitly.)
&gt;In some respects, Qi surpasses them. For instance, Qi's type inferencing engine is Turing complete. Haha ... man. Funny on so many levels.
I can't tell if the article is serious about the portion you quoted, but one sad thing is that there are probably 3 or 4 people who use Qi aside from Mark Tarver himself. I've seen actual interaction between him and other lisp folk, and they have 0 interest in using his type system stuff, even though it's written in and compiles to lisp. They're mainly interested in the fact that type systems can be implemented in lisp macros so that they can claim it isn't a novel feature, and thus don't have to use them (which is a common theme in "you can do that with macros" responses, in my experience). :)
Why not just pick up the program command line and environment variables at program start time and shove em in a immutable dictionary. The program's environment doesn't change unless the program updates it and the arguments passed on the command line can't change either. Maybe I'm not understanding something about how program startup is handled. 
Isn't this what the Reader Monad is for?
That's how xmonad does it: newtype X a = X (ReaderT XConf (StateT XState IO) a) Where `ReaderT XConf` stores the immutable configuration values: data XConf = XConf { ... }
I really don't understand the urge to reinvent "huge piles of global variables" in languages that don't force them on you; there're too many unavoidably global things in Haskell as it is. The whole thing on StackOverflow had "I want to write terrible code and Haskell is getting in my way, how do I fix this?" written all over it. The two main situations I can think of where passing global configuration around would be necessary is when you have a discrete piece of data that can't be interpreted without that configuration (in which case you can wrap it as a datatype holding `GlobalConfig -&gt; SomeData`, as I said on SO) or within what truly is the monolithic core of the code, where it's pretty common to have a custom monad that you can stick a `ReaderT` onto with little trouble.
Meh. I was expecting something about how the concept of I/O cannot be captured correctly in a monad. Instead, the author simply gives a different data structure representing I/O, points out that this data structure is not a monad, then constructs a perfectly monadic IO monad out of this data structure.
Heh. There has to be a joke about lisp macros somewhere in there following the same pattern as the old "a solution exists" joke about mathematicians but I'm not feeling witty enough to try to find it.
That's right. 
Well, it is something of an implicit sanction, by virtue of enough people paying attention to what they do, and few people disagreeing or producing persuasive alternatives. It seems pretty clear that the HP is doing some good at some level. I seem to recall your arguments against it involving negative impact at larger scales over longer periods of time and that's a notoriously difficult battle to win even when backed with unequivocal fact.
An interesting read. Found a typo: &gt; you need to always need to make sure
Also... &gt; Every second or third serious Lisp hacker will roll his own implementation of lazy evaluation, **functional purity**, arrows, pattern matching, type inferencing, and the rest. Nope, sorry. You don't know what you're talking about. And of course, the big mistake the article seems to repeatedly make is taking 'teams of academics' working out how all these features should and can work, and then comparing them with random lisp hackers banging out a half-assed implementation once all the theoretical groundwork has been laid. HBC was written by Augustsson (and he wrote Cayenne, too). GHC has a handful of major contributers. JHC is mainly written by John Meacham. ... I've banged out half-assed implementations of a lot of type theories in Haskell on my own. That doesn't make me comparable to Girard, Milner, and Martin-loef combined.
This is exactly what I was thinking... Why didn't he used the adequate monad?
Actually, looking at pointedlist's source, it seems like it *is* a nice simple library for pointed lists with, unfortunately from a dependency point-of-view, precisely two extra lines in it--one line that uses derive to produce a binary instance for pointed lists, and one that makes fclabel-style labels for the pointed list data type: $(derive makeBinary ''PointedList) $(mkLabels [''PointedList]) So these dependencies aren't, clearly, used in the core implementation, but are just intended to provide pleasant integration for people who like those interfaces. The approach that people seem to take to avoid introducing too many dependencies in this kind of situation is to create separate packages (here, I guess, "pointedlist-fclabels" and "pointedlist-binary") with further dependencies, but I can understand the author's reluctance to do this when the packages in question would have just one line of code each. Given that people who want a derived binary instance or fclabels-style labels could reproduce them for themselves with such little effort, this isn't a great example of the broader pattern, but I do think there's an interesting broader issue here that you see across Hackage: people factor packages into subpackages to avoid introducing gratuitous dependencies when those dependencies only exist to adapt the package's core functionality to a related interface (see, e.g., all the \*-iteratee, \*-enumerator packages, or the data-accessor-\* packages). This works well, but it does clutter up the package namespace and generally introduce extra complexity for both package maintainer and browsers of Hackage. Just as it would be useful to have the Hackage display give some immediate visual indication of popularity/currentness/widespread use, such as through reverse dependencies, it would also be helpful to reify explicitly the notion of a package that exists to bridge two independent packages: e.g., to be able to ask, "what are the packages that bridge attoparsec to something else?" and find attoparsec-enumerator and attoparsec-iteratee, as well as to ask "what works with iteratee?" and find all the many iteratee adaptors. It would also be helpful to have a more structured package list display that pulled out all the adaptor packages, just perhaps listing them below the packages they support. This would, in itself, be a useful clue as to how widely a given interface is adopted. (It is a distinct concept from reverse dependencies, though the 'adaptor' relation is a subrelation of the dependency relation; if a package uses another one in its implementation but is not, in itself, intended to adapt its core functionality for use with that package, then it wouldn't include itself as an adaptor of that one. For example, the hypothetical "pointedlist-binary" package would be an adaptor bridging pointedlist and binary, but it would just have a regular dependency on derive.) 
I think you missed the point. Input/Output is entirely represented by a nullary data constructor (as seen in `Dialogue2`). The `IO` type in Haskell is only an extension of that core representation of Input/Output for convenience purposes (this extension being: make `Stop` polymorphic, write the obvious `Functor` instance, and generate the 'free monad'). So, what I got from this article: it feels a lot less surprising that the "`IO`" type is a `Monad` when it is so clearly and intentionally concocted into one. The article convinces me that Input/Output itself is not a `Monad`, for it is not even polymorphic in nature!
This is not a typo in some modal logics.
Well, I'm not sure I agree. Rather, I would say that the `Response` type is a concocted approximation (direct sum) to a fully polymorphic type.
I don't think it's approximation if you don't lose any expressiveness, it's just a simplification. In the end, doesn't `main :: IO ()` prove that any polymorphic variant would be just be for building convenience rather than representation?
Whenever you add a new primitive IO function (for example `newMVar`), you'd have to change the `Response` data type. The type of `main` being `IO ()` does not imply that intermediate expressions with polymorphism are useless. (That would be like saying that `map` doesn't need to be polymorphic because you always apply it to a monomorphic type at the very end.)
I might do that, I've never put anything on Hackage so it would be interesting to see how it's done. I was also planned on writing an article on my site explaining IO with infinite sequences. Next time I'm procrastinating.... 
I do own a copy of PAIP so I probably did look at his version of Eliza before writing this (it was written a few years ago for my own amusement)
&gt; The approach that people seem to take to avoid introducing too many dependencies in this kind of situation is to create separate packages (here, I guess, "pointedlist-fclabels" and "pointedlist-binary") with further dependencies I totally agree with you. But the anti-orphan-instance crowd would not only keep pointedlist the way it is but have us add every single possible utility as a dependency to pointedlist in order to put all the instances in one place.
Oh don't be modest!
This. Unfortunately, the de-facto standard solution for this is to make micro libraries "pointedlist-fclabels", "pointedlist-binary", etc, which is not something that has any extra support from Cabal (like, at the minimum, I would like to install them together); and also introduces lots of orphan instances, which it seems some people really despise (though I don't see how to avoid them precisely because of this situation). I believe we really should have some infrastructure support to solve this problem (but it looks like a nontrivial design question).
Thanks. Fixed.
One big point in favour of something like the HP is that they provide binary distribution of a set of core libraries, some of which is very hard to compile on some platforms (eg. OpenGL on Windows, and in the future hopefully some GUI toolkit(s)).
In a synchronous setting, with the traditional semantics of Event a = [(Time,a)] we have (Event a, Event b) = Event (EitherOrBoth a b) which unfortunately breaks the mathematically nice duality between `Behaviour`s and `Event`s. Because of this, for a synchronous system I adopted the semantics Event a = [(Time,[a])] (with some caveats for empty lists) which makes life more tolerable (in particular, it more-or-less restores the above formula).
Orphan instances solve the problem of instances for datatypes where neither the typeclass nor the datatype is more "canonical". If people "despise" orphans in this case they're simply wrong[*]. [*] Unless there is a really good reason that I've overlooked, but from where I'm standing I'm really struggling to see it... 
\*nods\* Which system are you using?
&gt; I seem to recall your arguments against it involving negative impact at larger scales over longer periods of time and that's a notoriously difficult battle to win even when backed with unequivocal fact. The Haskell Platform is like global warming! :D
You mean which FRP system? I periodically try to build my own; the efforts so far are probably not very far from your reactive-banana project, at least in semantics (basically, it's edsl style; builds a static event graph, so no dynamic switching; synchronous). The latest one more-or-less worked, but it was not very convenient to program it. My newest idea is a bit different: I want keep Conal style semantics, but disallow first-class events/behaviours (like arrows, but arrows are of course insufficient to do this). This one is completely unimplemented though.
&gt; Notice that there is no monad anywhere. There is are not even any type constructors around. This is why I/O is not a monad. A model of I/O is a data type, not a data type constructor. Doesn't this mean that just one of the possible "models of I/O" are data types and not constructors? For example, I could use instead: data IO a where Return :: a -&gt; IO a Bind :: IO a -&gt; (a -&gt; IO b) -&gt; IO b ReadChanIO :: IO String ... Wouldn't this be an equally valid "model of I/O" which *is* a Monad?
TLDR
&gt; TL;DR: wl-pprint-text is a pretty-printing package for lazy Text &gt; values based upon the API of wl-pprint. It however deals with the nil &gt; document better than wl-pprint does, and also has a version where all &gt; the comibnators are lifted into a Monad, primarily so that it can be &gt; used within a State Monad.
Ah, right, I'd forgotten about orphan instances problem, perhaps because I agree with you that pulling in all the dependencies required to avoid it is, in practice, a worse problem than having (this type of) orphan instances in the first place. As almafa points out elsewhere in this thread, it would be helpful if there were some tool support, or even language support, for this case. For example, if it were possible to have a 'weak' instance declaration--one that asserts that a definition exists in a named module, but doesn't actually import it into scope--then you could avoid the possibility of colliding instance declarations, as well as asking the compiler to chase down instances in random places, while still being able to compile without pulling in all the dependencies.
What's wrong about adding a new type constructor to the Response data type? The new type constructor is not used in old code, thus old code doesn't need to change.
Orphan instances have a bad reputation because in the general case they're a bad idea, but there exist specific cases where they're perfectly reasonable. It's difficult or impossible to distinguish between these cases at the language level and the "good" uses are worthwhile enough to not disallow them entirely. This is the case for quite a few features of the language, actually.
&gt; Have you used Scion? Yes, but not seriously. From poking around on github, I've found out that the original author is currently rewriting it with a new architecture, and there's a modified version which is used by Leksah. &gt; Have you used Yi? I have, and I'd like to improve it. I have commit access to the yi repository, but haven't used it much. I'll get round to it at some point. You're right that emacs is at a better place. Magit is particularly juicy. I'm dreaming of magit for yi... or something like magit but which supports darcs.
It's been running on sparky for about two years now. What we haven't done is do the db migration from old to new hackage.
I keep running into the limits of Haskell with every attempt at creating a sane FRP library. I’m starting to suspect that the EDSL approach is not really viable for ‘production-ready’ systems.
&gt; Whenever you add a new primitive IO function (for example `newMVar`), you'd have to change the `Response` data type. Is this a problem? This is something that would probably need to be done for *any* denotative model of Input/Output, rather than our current black hole / melting pot. &gt; The type of main being IO () does not imply that intermediate expressions with polymorphism are useless. I think 'useless' is a totally different game here. I'm definitely not trying to say that a polymorphic Input/Output representation is useless, because the 'container' model of a `Monad` is very useful for thinking about how to connect various Input/Output actions together. However, that mental model isn't necessary for representation, so I think the `IO ()` type of main does prove that the actual idea of Input/Output isn't higher-kinded and not a monad. (Are lists more useful than the `(a ,)` bifunctor? Yes. But they are just the free monad of the `(a ,)` bifunctor, so it might be useful to define them in that way rather than focus on the derived form's mysticism.)
The same. I'm thinking about preprocessors (and dreaming about new type systems)
The point in the article, I think, was that if all you need is "a description of some I/O to do," then `IO ()` suffices, and you don't need to be parameterized. The parameterization is used to make it more convenient to build the ultimate `IO ()` through intermediate steps. This is somewhat analogous to category theoretic handling of algebra and such (unsurprisingly), where you have a functor `F` describing the things you're interested in, those being the algebras of `F`, and the free `F`-algebra `Mu F` is a special one. However, you might also be interested in (to coin a term) `F`-expressions, with variables taken from a set `A`, and this is given by `Free F A` (still using Haskell-like names) the free monad over `F` applied to `A`. This latter object has nice substitution operations due to being a monad, but also I believe given an `A -&gt; Mu F`, we can get a `Free F A -&gt; Mu F`, filling in the 'variables'. However, there's an incongruity with the `IO` example, because `Mu F`, the initial `F`-algebra is not given by `Free F ()`, but by `Free F Void`. So for some reason, we are not using the initial algebra of the I/O-description functor, but the free monad over that functor with trivial variables. And I don't really know why. Edit: Possibly, `Stop` in his exposition shouldn't be conflated with the `Return :: a -&gt; Free f a` of the free monad; maybe the former should be part of the shape functor. Edit 2: Something like [this](http://hpaste.org/46939/io_algebra)
Ah, 'useless' was not a good word. What I meant that is that just because the output type `IO ()` is monomorphic doesn't mean that you don't use polymorphic functions in between. Basically, if you take your argument far enough, you will end up with the argument that you don't need higher order functions because every program can be defunctionalized, i.e. written without closures.
Great work! Yesod is looking to be a great framework. I'm looking to switch from a Python based framework and Yesod is continuing to look like a better option.
Are lenses and zippers two sides of the same coin? Is there something you can do with generic derivatives of data types to hit both with one stone? (The picture of a data structure with a hole in it really reminded me of zippers.)
That's both a useful fact and something I could never have found myself. Stephen Wolfram is truly a God.
Haskell can't do any symbolic analysis without explicitly being told to (like having special type (Num' = Pi | ExpE | Num' Double )that instances Num). How would it know pi = 3.141592653589793 from any other number?
I think "functional references" was the name that I first saw for them. Sorry for confusing you.
Yes, lenses and zippers are closely related. A zipper usually refers to the `(b,r)` type, so a lens would then be a bijection between some type and its zipper. Usually zippers also come with functions for moving around the data structure, which would be isomorphisms of type `(b,r) &lt;-&gt; (b,r)`. You could also define lenses using derivatives if you want. That would be something like `type Lens a b = a -&gt; (b, ∂a/∂b)`, with an implied function going back from `(b, ∂a/∂b)` to `a`. But this is not as flexible as other lens types. For example, the function `div` is a lens, taking an integer to the result of dividing by some constant, and back, while keeping the same remainder. But this can not be represented with a derivative as far as I know.
Don't you have a mobile phone?
&gt; Back then, Haskell had the following mechanism for I/O &gt; type Dialogue = [Response] -&gt; [Request] So back then, given answers, Haskell could tell you what the questions were? Whoa! If anyone has access to a Haskell 1.2 compiler, could you see what it says if you feed it 42?
I'm not so knowledgable about the theory behind lenses and zippers, but I wrote a zipper lib based on lenses from the fclabels package http://hackage.haskell.org/package/pez
Where's the code ? 
How do the garbage collectors interact? I have to assume that a loop of of pointers through both Haskell and Java would not be collected.
This is absolute madness -- but in a good way :-)
Those three extra lines certainly are a terrible burden. :) Especially since GHC can now derive `Functor`.
&gt; Especially since GHC can now derive Functor. How did I miss that fact. When did that happen?
I'll release the code when it's a bit more polished, promise.
That's a good question, that I haven't worried too much about. I think the main issue would be the other way round: Java objects being garbage collected to early while Haskell still need them. 
Haskell + Java seems like an unholy marriage to me... ;-)
I'd rather use something like http://qthaskell.berlios.de/
This is very exciting, thank you for doing this! I'm looking forward to toying around with it.
In general, I'd agree with you. What excites me about this is that it sounds like it might work on any system with a JVM installed. I can't always rely on my clients having Qt installed. __Edit__: On a related note, anyone know what happened to [HQK](http://www.haskell.org/haskellwiki/HQK)?
Since GHC 6.12. You can also derive Foldable and Traversable. See [the docs](http://haskell.org/ghc/docs/6.12.1/html/users_guide/deriving.html#deriving-typeable) for more info.
the issue is breaking existing code, before there was such a mechanism, I think. And in any case, it should be a subclass of Applicative too, and you can't derive that (yet).
Monad used to be a subclass of Functor, but it was removed.
Sad, isn't it. But then the monad laws are overrated anyway.
Oh? When? And why?
You could indeed. But if you didn't craft your response carefully the program would get annoyed and produce bottom.
Monad was a subclass when it was first put in, but it was removed before there was any official version of the Haskell Report. The reason was exactly what people have already said, namely that some people thought it was too tedious to write the few extra lines for the Functor instance for every monad.
One objection is just that monadic style is unpleasant. Haskell punishes programmers who prefer a different choice of ambient impurity to the default. I agree that, here, the monadic solution is the best way, but it's not without its downside. Could do better!
What I still don't get: How is it better to go through the mess of instructing a JVM with an imho fragile interface/dependancy to render widgets for a GUI as opposed to instruct a modern HTML5 "VM" (aka web-browser) to render the GUI?
I thought zippers only applied to recursive data types, but your use of thirst makes me pause.
I did say "crazy" :-). But more seriously, I'm not sure an interface to a HTML 5 browser would be less fragile or quicker at runtime. And some users do like the native look and feel.
&gt; or instance, Qi's type inferencing engine is Turing complete Um, that's a bad thing.
How about GHC's implicit parameters extension? Works great for me.
Why not use GHC's implicit parameters: let ?foo = bar in baz ... baz :: (?foo :: Config) =&gt; type baz = show ?foo 
Interesting, is there any code?
Hurry. hehe.
I have tried some of the haskell gui bindings, the gtk is fragile. Wxhaskell doesn't seem to have many widgets. Swing / Swt have embedded web browsers, tree widgets, etc, etc. I couldn't ever get the gtk haskell binding to install properly. Maybe this approach will be easier. Swing and SWT have 15 years of development behind their UI systems. They are used on production, high quality apps. Millions of installs.
Yeah. What we need to do is make SHE a part of the core Haskell package, and use its facilities for declaring default instances. Then it'd all work out. Or just pull that into the language proper. I could be persuaded to take that route, too. :)
Right, I almost called the library 'navigater' or something.
I'm hopeful this happens; there's a trac ticket and wiki page for them on GHC:)
which Trac ticket is it?
You missed Takusen, though. As did camccann. 
I guess its this old one: http://hackage.haskell.org/trac/ghc/ticket/2895 or the general http://hackage.haskell.org/trac/ghc/ticket/788 but I hope its more relevant due to http://hackage.haskell.org/trac/ghc/wiki/DefaultSuperclassInstances 
well, to be honest, Swing/SWT apps never felt really "native" to me on Linux anyway... :-) However, I get your point -- I just hope that (in the long-term) it won't come down to have to install Java just to have a cross-platform native GUI for Haskell programs because lacking alternatives it has become the defacto-standard for Haskell GUIs... ;-)
To be fair, it's not *necessarily* a bad thing: when working with [dependent types](http://en.wikipedia.org/wiki/Dependent_type), you can use type level computation to express very precise program semantics using a language that is, for all intents and purposes, Turing complete. However, boasting that your type system can perform arbitrarily complex computations is as useful as having 10 years of experience in brainf*ck programming.
Maybe the paper [Tackling the awkward squad](http://research.microsoft.com/en-us/um/people/simonpj/papers/marktoberdorf/mark.pdf) can help, as it explains how data created by foreign calls are garbage collected (it isn't in fact), and how to deal with that.
That makes me wonder if perhaps there isn't some practically-oriented middle ground, like a concept of "derivative with remainder". (I want to say "partial derivative" but that already means something in continuous mathematics that isn't what I want to get at.) Basically, make something derivativish that can work with div, then see if we can't unify. Maybe I'm just going unification crazy.
It is sort of a potentially messy discussion -- I don't recall if/when it came up on one of the lists -- if you could point me to a thread, I'd appreciate it. I'd also be interested in a potentially productive discussion on what exactly it would take to have a "community sanction" with what you would consider some meaning and weight, and if such a thing is possible and desirable.
I wonder if this project can be split in two: Generic java FFI that allows haskellers to interface any java libraray and maybe even automatically generate bindings. And an SWT bindings library built on top of it. 
I've long felt this way. I doubt many here would disagree.
I'll post links here as I find them: * http://www.reddit.com/r/haskell/comments/dlhn0/hackage_cabal_and_the_haskell_platform_the_second/c113ap2 * http://www.reddit.com/r/haskell/comments/bvgsq/enforcing_stability_in_hackage_some_thoughts/c0or69w
Strangely, that loop technically isn't tail recursive. go x y k | x &gt; x1 = return () | k &gt; 0 = d (x,y) &gt;&gt; go (x+1) (y+1) (k+8*x+8*y+20) | otherwise = d (x,y) &gt;&gt; go (x+1) y (k+8*x+12) The outermost function call in the recursive cases is (&gt;&gt;), not go. Now, most implementations of &gt;&gt; will make this end up tail recursive, but that's not a guarantee!
It's the lack of parallel assignment that really complicates things. If you have it, the imperative loop looks quite okay. func circleMidpoint(d func(int, int), r int) { x1 := int(math.Ceil(float64(r)/math.Sqrt(2))) x := 0 y := -r k := 5 - 4 * r for { if x &gt; x1 { break } d(x, y) if k &gt; 0 { x, y, k = x + 1, y + 1, k + 8 * x + 8 * y + 20 } else { d(x,y) x, k = x + 1, k + 8 * x + 12 } } } This variant makes it also more obvious which initial value belongs to which variable.
Nice trolling, bro.
In my experience of writing a substantial amount of numerically-intensive code in Haskell, both tail recursion and immutability typically make the code quite a bit more cluttered. Immutability leads to the introduction of many added variable names. For instance, given some code in C: if (x &lt; 4) x = x + 1; Here's the obvious translation to Haskell: x' | x &lt; 4 = x + 1 | otherwise = x Now I have both `x'` and `x` in scope, and woe betide me if I use one instead of the other (or accidentally mention `x'` on the RHS of its own binding). I also have these "do nothing" `otherwise` clauses scattered about the place. Something similar happens for repetitions of the loop body. Say I have have five variables to "mutate" from one iteration of the loop to the next, but any given re-entry to the "top" of the loop only tweaks only one or two: loop x y'' z w' v I might have a handful of invocations like that. Tricky. Oh, and quite often, two of those are typed as 'Int', and two as 'Double', so the type checker won't help me if I screw up the positioning of the parameters. (Yes, I can and sometimes do introduce `newtype` wrappers to save me from myself, but that makes my code both *more* verbose and *less* comprehensible). These aren't *horrible* problems, but they really do reduce the clarity of code. Sometimes writing loops in Haskell is a win, but quite often for an involved numerical algorithm, the equivalent in, say, Fortran 90 can be a lot easier to read.
If you find yourself frequently using some repetitive code structure, you should factor it out as a higher order function. For example: if' :: Bool -&gt; a -&gt; a -&gt; a if' b x y | b = x | otherwise = y To be fair, such a combinator really should be in prelude.
I agree -- I'm not going to try to produce the code at the moment, but I think ezyang's example could probably be neatly refactored into something using iterate and the like. That said, the compiler has a much easier time optimising these tight tail-recursive loops. So for those interested in every drop of performance, the more elegant and idiomatic solution is probably a no-go.
I ran into some of the same problems trying to implement Dijkstra's smooth sort algorithm. A straight-forward translation of imperative algorithms that use loops and mutation can lead to headaches (that is, the translation to a pure, tail-recursive implementation; translating using `ST` for your variables will probably turn out better). In some cases, you might find a better way to structure the functional algorithm, of course.
It could be nice to instantiate both at once: instance Monad MyType where -- defines Functor automatically unless using: hiding instance Functor 
I prefer the term "functional references"; lenses doesn't make much sense to me.
That is just a combinator for an if statement. 
Actually, I was intentionally ignoring any general-purpose SQL packages, such as Takusen. Several of those probably do support sqlite, but I read "looking for an sqlite binding" as wanting something narrower.
It's tricky. Things work better for me when I don't try to translate the algorithms directly and instead identify high level operations which I turn into mini-functions (hoping that the inliner will get them all.) It does require more work, but the net result is that there are much fewer intermediate variables and far more self-documenting code. Doing this and getting good performance is tricky though.
Good catch. In this particular case, I'm working in the ST monad, which will end up tail recursive.
I'm not really sure a solution using iterate would be more idiomatic. You'd end up bundling all the loop variables into a tuple and you'd end up with basically all the same code, except imposing the monadic structure is more obnoxious.
The structure here is really an unfold whose output is a lazy list of (x,y) pairs, and then you `mapM d` over them. Separating pure and impure code = always a win.
To be honest, you lost me at &gt; midpoint line-drawing algorithm What is this function doing? What do x, y, k, and d stand for?
I was rather surprised/shocked they observed the follwing: &gt; [...] we included a variant for Bryan O’Sullivan’s Data.Text. We have not investigated why it’s outperformed by String (which shares the same time-complexities for the interface we use) So far I've assumed that Data.Text and Data.Bytestring had about the same performance... what's wrong here?
I agree, and it could instantiate Applicative at the same time.
Indexing a character in a Data.Text is O(n) and involves decoding UTF-16.
The function is not tail recursive. It's the (&gt;&gt;) that is in the tail call position. It just happens that in the ST monad ghc transforms it into something that is tail recursive. Why you are really relying on is the tail call optimization. The go function will tail call (&gt;&gt;), which is passed a thunk for the recursive call to go. Now if the (&gt;&gt;) in a particular monad happens to use this thunk in a tail position then the tail call optimization will kick in again and no stack will be used. For particular monads and Haskell implementations this will be optimized nicely into a loop.
I think it's a great idea (since I and others have had similar ideas too). But it has some problems when you try to extend it to multiparameter type classes. For instance: class Foo a where foo :: ... class (Foo a, Foo b) =&gt; Bar a b where foo = ... -- Which superclass Foo does this belong to? That said, I think something reasonable could be worked out. Then it's a matter of someone implementing it.
Perhaps Data.Text needs a better internal representation that makes using Plane 0 characters fast (e.g., a flag if all characters are Plane 0, or a list of all multi-word characters.)
What should happen if there is also an `instance Applicative MyContainer` declared somewhere? Should the compiler detect this automatically and not generate the instance? Or would it be an error?
Does anyone want it for all type classes? Maybe not enabled for all type classes but perhaps a pragma just to fix up the Functor/Applicative/Monad.
It's reassuring that I'm not the first person to think of this. For the example you give, I think the best thing is to disallow providing a default value for an ambiguous instance. If we later think of a good way to specify which instance of Foo it applies to, we can then allow that without breaking old code.
I want `instance Monad MyContainer` to implicitly generate `instance Applicative MyContainer` if and only if `instance Applicative MyContainer` is not explicitly declared elsewhere. I want to avoid breaking old code.
The problem is more wide spread, for instance, it would be nice to split of the Num class. So any solution we come up with should be general enough to handle all these things.
Avoiding the automatically generated instance could be quite tricky if the other instance is an orphan. Not impossible, but I don't ghc could easily be changed to do the right thing (but it could easily detect during linking when it has done the wrong thing). 
I don't think it makes sense to 'derive' implementation of Foo a from Bar a b. if we want to provide superclass instances for Bar a1 .. an, then only constraints of the form Foo a1 .. an should be considered.
I think this would be a sufficient restriction, but it is overly restrictive.
If this is legacy code, then the modules that don't have access to the orphan instance don't use the orphan instance, so wouldn't use the automatically generated instance either, so legacy code wouldn't immediately break. It could cause maintenance difficulties though. Orphan instances seem to cause trouble anyway. Maybe we need a better way of dealing with orphan instances in general.
I'd love a better way of handling orphans, but I don't know what it is. From a pragmatic view they are essential.
What I was really meaning to say was something that could downgrade the "it has problems" from the idea to just this particular syntax. IE what if the syntax was more explicit like this: -- in Prelude class Applicative m =&gt; Monad m where -- operations for Monad (&gt;&gt;=) :: forall a b. m a -&gt; (a -&gt; m b) -&gt; m b (&gt;&gt;) :: forall a b. m a -&gt; m b -&gt; m b return :: a -&gt; m a fail :: String -&gt; m a -- default implementations for Monad return = pure ma &gt;&gt; mb = ma &gt;&gt;= \_ -&gt; mb fail = error -- default implementations for Applicative defaultImp Applicative pure = return defaultImp Applicative mf &lt;*&gt; mx = mf &gt;&gt;= \f -&gt; fmap f mx -- default implementation for Functor defaultImp Functor fmap f mx = mx &gt;&gt;= (return . f)
If you intend to use a different `Applicative` instance from another module then you could just be required to import it. You would have to do so in the absence of this extension anyway, assuming `Monad` requires an instance.
Awesome. We could then make the type of main `IOE Void`. Though now we would need to typically end our programs with an explicit halt :: IOE Void -- or maybe halt :: IOE a halt = Branch Stop Not a bad idea, but there doesn't seem to be a very big advantage over conflating `Stop` with `return` as is done now. Though it does *feel* more proper.
I don't think it's that arduous to have to specify which instance you're providing, even if it's redundant in a lot of cases, and that should clear this up, no? class (Foo a, Foo b) =&gt; Bar a b where ... default instance Foo a where foo = ... The ability to 'hide' the default instances (as in SHE) seems like it'd clear up the "what if there's a separate instance" concerns. Although, if the aim is to maintain backward compatibility, maybe it should be opt-in instead of opt-out.
You should always `return EXIT_SUCCESS` (or `EXIT_FAILURE`) at the end of `main`. :)
From early 2007 on Haskell Cafe: [IO is not a monad](http://www.haskell.org/pipermail/haskell-cafe/2007-January/021536.html), and [IO is not a monad, continued](http://www.haskell.org/pipermail/haskell-cafe/2007-February/022265.html). Kind of a different angle, though.
I think it is reasonable to look only at the point of the Monad instance. Then if an orphan instance of Applicative is defined elsewhere you get a compiler error at that point (assuming the module declaring the Monad instance is also in scope). But you can never do better then that, since if Monad became a superclass of Applicative, then the Monad-declaring-module should always declare or import an Applicative instance anyway.
We forgot our error bars! http://csks.wordpress.com/2011/05/24/doh-bars/
Another problem is with the OverlappingInstances extension, then it is not clear whether you want to reuse the general instance of the superclass or define a new more restrictive one.
[Jasper Van der Jeugt](http://csks.wordpress.com/2011/05/23/trying-some-datatypes-on-for-time-a-few-algorithmic-variations/#comment-8) is going to take our code and look further into the Data.Text issues. We're still not sure we're being fair. String index is also O(n); so I figure it's either: * that decoding is expensive * or GHC's compilation and run-time cater really well to this sort of usage of (such short) Strings
Joke, right?
Why? Modules are records and that's it. Nothing special, from a semantic point of view, at least not if you ignore typeclasses. It's syntactic sugar that can be done away with, or, even better, unified into namespacy records that work inside single files, too. And can take parameters. That is, functions.
You say "Not if you ignore typeclasses", but what about types? Also, modules are namespaces.
This would be neat for making a Haskell clone of scapy, the Python packet creation tool.
That isn't getting rid of modules. That's having good modules.
Did the title really *have* to be so long? :(
So far I've heard about [hakyll](http://jaspervdj.be/hakyll/) and [Yesod](http://www.yesodweb.com/). Has anyone used either of those for a blog setup?
No. I'd have gone with **Galois releases HaNS: a pure Haskell network stack - no OS required**. dagit's still in training though ;)
scapy seems very cool! Didn't know about it.
Heh. I wouldn't have guessed that long titles annoy people. Learn something new everyday :)
If you like [scapy](http://www.secdev.org/projects/scapy/) you'll also like [dpkt](https://code.google.com/p/dpkt/) and any other tool written by [Dug Song](http://monkey.org/~dugsong/), check out his awesome array of tools in the [dsniff](http://monkey.org/~dugsong/dsniff/) collection!
I like it that way.
&gt; The observation that I find this very-convenient is telling me something about modules Or that it's telling you something about software engineers (that we are incredibly lazy and won't do something if we don't have to).
I'm the author of hakyll, so I'm a little biased. But here's a few blogs using it: - [The french engineering company Demotera](http://blog.demotera.com/) - [ Benedict Eastaugh's extralogical](http://extralogical.net/) - [Nikolas Wu's blog](http://zenzike.com/)
Apparently you have much to learn, young padawan. :-)
Haskell modules are namespaces, more powerful modules do more.
They do.
Why not just use free blog sites like blogger.com ? Do you have to convert every task into a programming project ? Do you re-implement sorting algorithms when you need to sort some data ? 
It’s very reminiscent of Smalltalk’s approach.
Tackling a familiar task (*eg* creating a blog) in another language is a great way to learn it.
I thought OP explicitly said he wanted to use it, not to write it.
What is "antiquotation"?
Still, setting up a web framework in Haskell seems a good exercise.
You are not hearing me. I am ALL for implementing small projects to learn a new language. But that's not what OP was asking. He did not say "i'd like to write a blog to learn haskell". He said he wants to DOCUMENT his projects. In this light choosing a ready to use blog and having a documentation by the end of the day is much better than not having the work done by the end of the week. 
In my experience, don't think too hard about your blogging software. Just install Wordpress and work on the hard part: actually blogging.
I have written some very primitive stuff using Yesod and CouchDB. Grab the code here: https://github.com/fortytools/lounge But as I said, it is very simple ... you have been warned ;)
I'm not sure who's not hearing. He said he wanted to use Haskell, not blogger.
Whatever
hiya shae - I've been doing more or less the same thing for #projectaweek :) thinking that a static site generator's a pretty good way to go for a blog. you can handle comments with disqus, and if you ever get hit hard you can be certain that caching will save you. also, you can host anywhere. that said, i tried out nanoc, serious, jekyll and hakyll and found them all a bit lacking. The ruby ones didn't support Haml, which I found pretty bizarre, and hakyll needed a lot more custom code. (If jaspervdj is reading, it'd be really nice to have something like the YAML front-matter that jekyll has.)
This is Haskell that contains a (quasi)quote: someJavascriptFunction = [$jmacro|var idFunc; idFunc = function(x) {return x;};|] It is a quote because it contains Javascript (or JMacro, rather) and is embedded in Haskell. This is Haskell that contains a quote that contains an antiquote: makeAJsFun someHaskellString = [$jmacro|function giveSomeVal() {return `(reverse someHaskellString)`;|] The `reverse someHaskellString` part is an antiquote, because it is a quote of Haskell within a quote of JMacro. The point of this is that you can embed Javascript in Haskell, and inside that Javascript, you can embed even more Haskell!
As far as static generators, I'd also take a look at John MacFarlane's yst, which is yaml based. https://github.com/jgm/yst 
This post was prompted by [a discussion on #haskell](http://tunes.org/~nef/logs/haskell/11.05.19) (starting at 11:55:47) of whether IO has ever been shown to be a monad. Follows a brief discussion of what could be the meaning of the claim that Haskell IO is referentially transparent.
Python modules are an interesting thing to look at here. It's not the model Joe is grasping at, but it does a great job of showing how powerful the "nothing really special" aspect can be. They're just objects that happen to offer convenient ways to reach into them for certain things. This being Python, of course you can mutate them freely, and you can even use this to metaprogram by programmatically constructing functions or load up some data at runtime or whatever. In fact while Python modules aren't what Joe is looking for, they're probably one of the faster ways you could prototype something up if you wanted to play with some ideas along those lines before committing to anything. Python isn't functional in the Haskell sense. But it does do a pretty decent job of creating some primitives and then composing them together to create most of the language itself.
Unfortunately the stable version of Cabal doesn't have the facilities dynamically link executables.
Congrats!
cool- I didn't realize there were CouchDB bindings. Would be neat to make a Persistent backend for it.
I am a core contributor to Yesod. I used hakyll when I setup my [blog](http://blog.gregweber.info) a while ago. Although I ended up with what I wanted, including that I was able to use markdown and hamlet as I wanted, I was a bit disappointed with the effort it took to get there, including using my own fork of the library to add a feature. Some of the issues I experienced have been addressed with the new version of hakyll (including why I was maintaining a fork). But I haven't upgrades because I am fine with everything now. Jasper is a very responsive maintainer of hakyll and there is probably at least one blog example for the new version of hakyll that can get you started. You will also find a responsive Yesod community on the web-devel mail list. The [Yesod docs](https://github.com/snoyberg/yesoddocs) site has a few components, one of which is a blog. But it is pretty trivial to create yourself. A lot of this question comes down to whether you want the ease of deployment of a static site or interactive features of a dynamic site. Although we want to create a static page generator for Yesod which might start blurring the lines a bit, and there is one company that has a script to download pages from a Yesod app to then be deployed as static pages.
(In my experience,) Wordpress is a maintenance nightmare. Sure, it's all fun and games for the initial install, creating a blog and adding a few posts. But a month later or so (and every month after that...), you *will* need to upgrade wordpress (or one or more of your plugins) to plug new security holes and then the nightmare starts... (Unless you're running an FTP server that has write permissions to your wordpress installation and whatever else is required for the automatic update thingy to actually work. I'm not.) I now have a wordpress blog, but definitely will be replacing it with something else - probably a static page generator. I'm not upgrading wordpress again, but at the same time I do *not* want to have an outdated wordpress installation running on a publicly accessible server.
`mysql_escape_string`, no, I mean `mysql_real_escape_string`
I think this /r is fine for Haskell, any questions I've had people have been more than willing to help The bigger problem is people posting interesting Haskell topics into /r/programming and then cross posting them to here as a second class citizen. I know why they do it but it's a shame none the less.
I think #haskell IRC is the best option for questions, there's usually tons of people there who can (and will) help you out. Stackoverflow is an option as well, perhaps for more complicated/harder questions. I'm not a big fan of "technical" questions in this reddit, unless it's a question which interests the whole Haskell community. "Informative" questions do have a good place in the reddit IMO, such as [this](http://www.reddit.com/r/haskell/comments/his1v/what_if_instantiating_a_subclass_automatically/) or [this](http://www.reddit.com/r/haskell/comments/hezgk/haskell_singularity_approaching/).
I did ask yesterday in #haskell, the people were indeed helpful, but the comments were brief, and not of the sort "You know, using this construct is usually considered poor form because...". The thing is that I am not searching necessarily for answers to specific questions, although these are helpful as well, just comments on my code. I understand that this type of thing could be annoying for subscribers of this subreddit, and this is why I made this post. :)
Depending on your skill level, you may also find the haskell-beginners (http://www.haskell.org/mailman/listinfo/beginners) and haskell-cafe (http://www.haskell.org/mailman/listinfo/haskell-cafe) mailing lists helpful. As you can see from the [GMane](http://news.gmane.org/gmane.comp.lang.haskell.beginners) view of the beginners list, people are happy to answer silly questions there.
Thanks I was not aware of these lists :)
This is great effort - have you seen the work done by the sagemath folks? www.sagemath.org - would be great to have a haskell equivalent - there must be much to share on the web front end... though this notebook looks nicer than the sagemath already!
Maybe we could create a /r/haskell-beginners and a /r/haskell-cafe here? I find the mailing lists rather cumbersome.
I tried to do that as well, but since CouchDB has no other way to query data than building map/reduce views, this is a bit cumbersome. But there is some initial code here: https://github.com/tbh/persistent-couchdb
/r/haskell is just dons's diary
...then just use wordpress.com. I mean, if you're not willing to put in the effort to keep your software up-to-date (and Wordpress isn't that bad), I don’t think you really want to put in the effort to create and maintain software you made yourself.
I think with this reddit, [haskell-cafe](http://www.haskell.org/mailman/listinfo/haskell-cafe), [haskell-beginners](http://www.haskell.org/mailman/listinfo/beginners), #haskell on IRC and the [Haskell tag on stack overflow](http://stackoverflow.com/questions/tagged/haskell), we're pretty well covered.