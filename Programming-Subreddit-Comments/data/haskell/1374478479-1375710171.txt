&gt; The reason is that it's much clearer what's going on. This is very subjective. 'Much clearer' depends on the context of the code, what the author is used to, and the authors taste. I'm a big fan of function composition myself and think it's generally very readable, but my capabilities of grasping what's going on quickly fade when composition starts to branch (like the `maybe` in this case). Nothing wrong with an explicit case here and there. Talking about subjective and clearer, I really don't understand why people would prefer `runDB $ get imgId` or `(runDB . get) imgId` over `runDB (get imgId)`
I, for one, would love to see a short blog post where you add code to those bullet points. Or even just post some code inline that roughs out a few of the points you were discussing.
&gt; Talking about subjective and clearer, I really don't understand why people would prefer [...] Come to my FPDays talk "Practical refactoring in Haskell" and I'll tell you! http://lanyrd.com/2013/fpdays/sckzkz/ (As of posting this comment they have the wrong speaker name down) 
&gt; Let's work on making exceptions an anti-pattern first ;) Yes please!
I will! If you'll come to ours: http://lanyrd.com/2013/fpdays/sckzmq/ ;-)
`(^^)` is exact, `(**)` is for floating point types.
I agree... up until we get into threading and `throwTo`. Also, using exceptions as a "hole that I promise I will fill in later" is quite useful for incremental development and type tetris, although more direct language &amp; editor support for this would be better.
Why not just compile on the webserver? Transferring 3k lines of code shouldnt be more than a couple dozen KB.
This is something I've wanted for a long time!
Great post! Just one mistake on the `Just Int` that should be `Maybe Int`. Nice though.
I managed to follow the post until (seemingly) arbitrarily the Identity wrapper got introduced. Can anybody explain why Identity and runIdentity are needed?
Does the `Const` fold do anything other than apply the function to the first element in the traversable and ignore all the others?
`Identity` is a dummy container that can be made an instance of many typeclasses. To traverse a data structure, you need an Applicative-valued function [(see the type signature of `traverse`)](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Traversable.html#v:traverse). `Identity` is the simplest Functor/Applicative/Monad/... you could possibly think of, so if you don't really want to do anything Applicative-y but your function requires an Applicative, you wrap your value in `Identity`. To understand what the code is mostly doing, you can just ignore the `Identity` and `runIdentity` (un-) wrappers. For completeness, here's some code for `Identity` that showcases how it basically does nothing: newtype Identity a = Identity { runIdentity :: a } instance Functor Identity where fmap f (Identity x) = Identity (f x) instance Applicative Identity where pure = Identity Identity f &lt;*&gt; Identity x = Identity (f x) instance Monad Identity where return = pure Identity x &gt;&gt;= f = f x Another way of thinking about `Identity` is that it's like `Maybe` without a `Nothing` value.
Done. You've got yourself a deal :)
It is quite a bit worse than that. FYL2X compute y*log2(x), but F2XM1 which is used to do exponentiation only takes args between -1 and 1, so you need to use FSCALE to rescale when you are done, code to put it in the right range, code to set that up The costs of just these opcodes have varied considerably over the years, with figures like 50-120 cycles each being typical. This is why [I have code for computing hacky approximations to pow in my analytics codebase](https://github.com/analytics/approximate/blob/master/cbits/fast.c): Consider that *this* abomination performs about 3.3x faster than `pow`, loop and all, and is a reasonably precise, piecewise rational monotonically increasing function with errors equally distributed on either side of the correct answer: double fast_pow(double a, double b) { int flipped = 0; if (b &lt; 0) { flipped = 1; b = -b; } /* calculate approximation with fraction of the exponent */ int e = (int) b; union { double d; int x[2]; } u = { a }; u.x[1] = (int)((b - e) * (u.x[1] - 1072632447) + 1072632447); u.x[0] = 0; double r = 1.0; while (e) { if (e &amp; 1) { r *= a; } a *= a; e &gt;&gt;= 1; } r *= u.d; return flipped ? 1.0/r : r; } Then realize that it has the same structure as the peasant exponentiator, which has to do even less work.
No, it applies the function to each element in turn and `mappend`s the results.
You can use `pipes-concurrency` so that threads can always gracefully shut down without having to use `throwTo`.
We use `Identity` to get `fmap` into the same 'shape' as `traverse`. Basically its saying to `traverse` without side-effects.
I was trying the introduce the fewest new functions possible. I do find location.x `over` (+10) $ player1 easier to read than over (location.x) (+10) player1 though.
If you are interested in seeing TPG in action, you can check out [my blog](http://www.fluffynukeit.com), which has some screenshots of my hobby project that uses TPG for the interface. It's an older dev version of TPG, but it should give an idea of the library's flexibility.
So yeah the Lie group thing maybe not, but using the "log-Euclidean" Riemannian metric on R^+, ||v||_p = 1/p |v|, (**) is the Riemannian exponential map. The main point I had was that if the exponent is a non-integer, it's probably best though of as a tangent vector in one sense or another.
Oops, fixed. Thanks!
-inf / +inf are regular values.
So busybox-like behaviour, determining the action to perform based on the name of the program running? I always thought it was a nice trick.
This is why we have a package versioning policy.
Huh. I always read it as "over the focal point of `location.x` apply `(+10)`"
&gt; startled owl if you say so
Which is a policy of convention, not enforcment. What's so incredibly bizarre to me is that Haskell is a language which places functional purity on the highest pedestal and yet its packaging system is anything but. Attempting to install something *should not* depend on the state of what is already installed.
But do you have birthday already?
It sounds logical that `over clause verb subject` would be better, but somehow infixing `over` has always made more sense to me personally.
You should check out my `errors` package which does a lot of what you describe: http://hackage.haskell.org/package/errors You might also want to read my [announcement post](http://www.haskellforall.com/2012/07/errors-10-simplified-error-handling.html) where I describe several of the motivations behind the package, which mirrors a lot of what you describe in your post.
In the case of `over`, we introduced `%~` to fill that role, with the mnemonic being that `%` is read as `mod` to most people from the outside world of c/c++, etc. so its `mod-equals` to go with `+~`, `*~`, etc. but that carries you into the operator set you were deliberately avoiding. ;) *shrug* I try not to be prescriptive about the ways in which lens is used lest I find myself missing a useful interpretation. I just figured it was worth noting lest people find the phrasing strange. =)
"So you pass in a traversable and a function that returns a functor" is slightly confusing, because I can imagine a function that returns a functor as something of the kind: \*-&gt;(\*-&gt;\*). Maybe "returning a value wrapped in a functor" or just "lifted value" would be less confusing. The statement "We just made setters, which allow us to _compose functors_" is rather sudden. The only functors you singled out were Traversable and Identity. It's not clear to the reader what functors you meant in that statement.
1. Fixed. 2. What do you think would be a better phrasing for this?
Well, when it gets there and I gain the ability to seamlessly install any package without error I will admit that it is a superior system. One question I have that remains: is the implementation of such a system possible? What happens when the dependency resolution of a particular set of packages becomes undecidable?
I assume the author was trying to be more inclusive by avoiding that allusion.
A couple reasons; one is that I don't want to go through the hassle of installing my development environment on the server. Another is that my desktop machine is more powerful and can do the compiles faster (which can take a minute or two with -O2), and also isn't RAM-limited like my server is. (I'm using a cheap 512MB digital ocean instance.) I've run into problems in the past trying to run ghc in a low-memory environment. I'm inclined to believe that the 50MB binary thing is just a bug in the linker or something unless anyone can make a convincing argument that there's really 50MB worth of complexity lurking in there.
Strictly speaking, it should be "allow us to compose *natural transformations in the category of `Identity` coalgebras*", but that's a little verbose for this post! :)
Hey cool! I installed it via Git the other day to play around with it, but just deleted it because it conflicted with installing `pipes` somehow... My cabal-fu is still a bit weak, and I wasn't quite sure how to fix it. Do you have any comments on using it with Reactive Banana? FRP is something I'm hoping to play around with as I make a GUI for a program I'm writing.
I'm still not seeing it. Consider the following example: import Control.Concurrent import Control.Concurrent.Async import Control.Monad action1 :: IO Int action1 = forever $ do putStrLn "action1" threadDelay 1000000 action2 :: IO Char action2 = do putStrLn "action2" threadDelay 2000000 return 'A' main :: IO () main = do race action1 action2 &gt;&gt;= print threadDelay 10000000 How do you get the same behavior without async exceptions?
This was helpful. Thanks 
Oh! Sorry, I misunderstood. No, you can't do that using `pipes-concurrency`. I forgot about the whole canceling each other out part when they are done (mainly because I never use that feature). The best you can do is have either thread block when it tries to send the next value and then get garbage collected, but you can't interrupt them in the middle of an `IO` operation without using asynchronous exceptions.
Perhaps it would be worth defining a nonsense word (say, "fubit", by anology with my favorite proposal for the monad analog, "mobit") that means "a value whose type's outermost constructor happens to be an instance of Functor". Then you could say something like, "So you pass in a traversable and a function that returns a fubit...". Sounds like the start of a GREAT joke.
Quite possibly so, and arguing over it is not really on-topic here.
that's very impressive, enough to convince me that I need to play with this library ASAP.
Yup!
I particularly liked the example using LINQ syntax for `do`-like notation. Hadn’t seen that before. Does generalising from the list monad to IEnumerable actually give you as much power as any old monad?
`IEnumerable` *is* the list monad. It's the LINQ syntax itself that's more general. To be concrete, LINQ syntax desugars into calls to `Select` and both overloads of `SelectMany`. (Actually, there are [quite a few other extension methods](http://msdn.microsoft.com/en-us/library/bb882642.aspx) you can add to give your version of LINQ syntax some more capabilities, but none of them are strictly needed except for `Select` and both overloads of `SelectMany`.) Which of the many extension methods (or methods in general) that just happen to be named `Select` and `SelectMany` get called as a result of the desugaring is up to the compiler's treatment of the desugared code. In other words, the desugaring is completely syntactic, like `for` comprehensions in Scala, but unlike `do` notation in Haskell. Incidentally, you can actually get this type of behavior in Haskell as well, by enabling the `RebindableSyntax` extension (mind you, it rebinds a *lot* more than just `do` notation, so use it with care!). In fact, I make use of `RebindableSyntax` in this tutorial, because `IPrivilege` is not actually a `Monad` (it *is* an [`IxMonad`](http://hackage.haskell.org/packages/archive/indexed/0.1/doc/html/Control-Monad-Indexed.html#t:IxMonad), but I didn't want to depend on an entire stack of /u/edwardkmett's packages :) ), so `RebindableSyntax` is necessary to make use of `do` notation.
You have a typo tho. You're missing some parens. You write over location.x (+10) $ player1 instead of over (location.x) (+10) $ player1 which is obviously just an error, because it means (over location) . (x (+10) $ player1) Also you might want to use `g` instead of `f1` for nested functors.
could you please clarify your comment about functor vs "VALUE of a type that is an instance of the Functor class"? do you mean that it does not correspond to the mathematical notion of a functor? regarding `IO` action vs `IO` value: haskell is a lazy language. who said `IO` "actions" have to be executed at that time or at all? i read in one monad tutorial, i guess by tekmo, the interpretation, that you build a list of `IO` actions that you run by assigning to `main`. so `main` is the analogue to `runState` or whatever.
The problem is that these instances would overlap with any other instance for a two-parameter type constructor. UPDATE: I see, you are talking about just functions, without making them into a general instance declaration. So that you can use these to define particular instances.
One way they may disagree is in the order of effects (because (&amp;&amp;&amp;) is not commutative). But this doesn't make these functions any less useful. My gut feeling is that these functions should provide a valid Applicative definition for any Arrow.
In HLint 1.8.49 the hint is removed, since they are fundamentally different, and people who know more than me should make these decisions! I've also migrated to https://github.com/ndmitchell/hlint, so contributions welcome!
&gt; could you please clarify your comment about functor Just saying "functor" makes it unclear whether one means 1) the functor class, 2) a data type that is an instance of the functor class, 3) the value of data type that is an instance of the functor class. Sure, the class/type/value distinction can usually be deduced from the context, but it makes comprehension unnecessarily difficult. &gt; that you build a list of IO actions You can only build a list of values belonging to some data type. This is exactly the example of casual misuse of the language.
Actually, it's trickier than you are making it out to be, and you are also being fuzzy with your own proposed terminology. A type can't be a functor. Even a type constructor can't. A functor in Haskell is a type constructor along with fmap. A functor does not have values, either. The only truly precise way to describe f Int is as f Int. For the same reason, the made up term IO action is more honest than IO value because, again, functors don't have values. 
You *could* do it that way, but the way I've done it in the tutorial is to build an arbitrary binary tree of permissions, any leaf of which is available (possibly through multiple applications of `transport`). Then the `()` instance has nothing to do with the `(,)` instance at all, but is merely a convenient way to represent "no permissions". If I were doing this for an actual library, I would use `DataKind`-based type-level lists in Haskell and take advantage of the subtyping system in Scala. I don't know what I would do in C#, but then, I'm unlikely to write anything complex in C# any time soon. :)
Excellent! It's really good. I ordered the ebook and pbook because I like having Haskell displayed prominently on my bookshelf and don't always carry my iPad :-)
Perl has a bonus though: those operators are built into the language. In haskell, any given library is free to implement their own `|||&gt;`, `..||..` or whatever operator which could have any semantics they feel like.
As an experienced programmer, slowly toying with Haskell, the operators defined by various libraries are the most inscrutable part of learning. What does a `&gt;-&gt;` do? I have no freaking idea without diving into documentation outside of the code. I can't even guess at it. And that's a common operator, what about a `...||...` operator, or a `|||&gt;` operator? Any library can implement that, do whatever they feel, and there's not even a word I can put to it.
I didn't write about it, it was under http://comonad.com/haskell/security-policy/ mixed in with a bunch of other code.
&gt; http://comonad.com/haskell/security-policy/ mixed is it this one? http://comonad.newartisans.com/haskell/security-policy/index.html
That it would appear to be. It was something like the second or third thing I ever wrote in Haskell.
Right, it's in the spirit of `fmapDefault`, `liftA`, and things like that, providing an easy opt-in for those that want it.
&gt; True, but there are two valid instances of any applicative, for exactly the same reason. Indeed, and I recently learned that this is conveniently provided for any applicative by the newtype [Control.Applicative.Backwards](http://hackage.haskell.org/packages/archive/transformers/0.3.0.0/doc/html/Control-Applicative-Backwards.html) in `transformers`.
&gt; I recently learned that this is conveniently provided for any applicative by the newtype Control.Applicative.Backwards in transformers. And I learned it just now. Thanks!
Fixed!
Yeah, `Backwards` is very handy for reversing `Traversals`.
I don't really know anything about Traversals, but isn't that typically going to lead to large memory consumption? 
Why would it?
Instantiating, then, your teaching is that "`putChar 'a'` only becomes an action when (if) it gets executed by the RTS". This has all the profundity of "writing the word "action" only becomes an *action* when it is done". Or better: "writing the word "action" only becomes writing the word "action" when it is done" or even better: "writing the word 'action' only becomes writing the word 'action' when the word 'action' is written." It doesn't actually say anything. The mention of the RTS just makes the mess worse, as if one were to say "writing the word "action" only becomes an action when executed by the brain and muscles". -- If this means, you can't write the word "action" unless you have brain and muscle, it's true enough. Writing the word "action" doesn't become an action, it is an action; you can do it, I can do it, in fact we both just did. As soon as you introduce an expression that can capture the thing we both did, you will bump into this logico-conceptual difficulty and no amount of falsche Spitzfindigkeit with the words "RTS" "value" "execute" and "evaluate" will save you. It is clear that no matter how one interprets the words, there will a problem making sense of the idea of "something's becoming an action" so I can't see that you have yet formulated a genuine proposition.
http://en.wikipedia.org/wiki/The_Treachery_of_Images
Imagine sequencing a very long list of backwards applicatives. Isn't it going to have to walk the whole list before it can start running the actions?
Intel Labs Haskell Research Compiler -- related: http://www.leafpetersen.com/leaf/publications/icfp2013/vectorization-haskell.pdf http://www.leafpetersen.com/leaf/publications/hs2013/haskell-gap.pdf
I think it had to do with threepenny's reliance on snap, but I'll have to try installing it again to make sure.
[Reverse](http://hackage.haskell.org/packages/archive/transformers/0.3.0.0/doc/html/Data-Functor-Reverse.html) is even handier for reversing `Traversals`.
It depends on the data type being traversed. For lists, yes. For balanced binary trees, no.
I think it might only be up there for the week (during OSCON). That's what I was told anyway.
It’s a bit sad when HTML typesetting beats the ebook—or Amazon’s preview of it, at any rate. Do you know how the print version is looking?
Many of those little "be careful" paragraphs I've seen so far are of the category "this book just saved me hours of debugging". And that's after randomly reading around for 30 minutes. (Example: the lazy reversal in `TQueue.hs` in the STM chapter. "we use a `let` rather than `case` here" - guess how long you could search for that in production!)
 (2) I genuinely don't know. Since you show composition of Setters, are you calling Setters functors with 'over' playing the role of 'fmap'? A functor has a very specific meaning in Haskell. Is a Setter an instance of the Functor type class? 
taking notes as fast as I can ;-)
Enjoying what I've seen so far! Thanks! Quick tiny errata I noticed: You attribute I-Structures to pH. I'm pretty sure they go back to Id, the predecessor to pH. There's a 1991 version of the Id reference manual here which discusses I-Structures: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.4920 Haven't done a deeper dive in the research prior to that to be sure though. Edit: you also may want to expand the FFI section to briefly note the difference between safe, unsafe, and interruptable imports. I have encountered many issues due to uses of unsafe by third-party libraries where safe would have been appropriate. I also didn't know about interruptable until just now and its pretty neat!
I believe a type (at least an ordinary ADT) can only have at most one Functor instance satisfying the laws, so those should never disagree. (I am not sure if there is some subtlety about strictness qualifying this.)
This is the Big Research Paper follow up to my post last month about [GHC HEAD acquiring support for closed type families](http://www.reddit.com/r/haskell/comments/1gu2yl/ghc_now_with_closed_type_families/). In particular, I linked the extended abstract containing proofs. But I did this because I also really wanted to highlight the beauty of the units library they developed and describe briefly in Appendix A, which is vaguely familiar to the already existing dimensional package - but it is fully extensible, allowing you to add your own types of units at will! It's awesome.
This paper seems to be an EXTREMELY exciting follow up to the original work of Peng Li, [Lightweight Concurrency Primitives for GHC](http://research.microsoft.com/en-us/um/people/simonpj/papers/lw-conc/lw-conc.pdf). It fixes some relatively large problems with the prior implementation, one being that it just made a lot of things slower. The new results seem to indicate this is a promising approach, but who knows when we will see it merged! For those too lazy to read the abstract: this paper basically describes a framework that would allow you to write your own schedulers for Haskell threads, in Haskell. The new approach to scheduler activations even allows you to seamlessly communicate (e.g. with `MVar`s) between threads, which aren't necessarily owned by the same user-land scheduler. 
What's the current state of the Par monad? Last time I tried it (about 8 months ago) I ran into occasional deadlocks, and it looked like there were some open bugs that may have been related to what I was seeing. Basically, I'd do a "runPar $ parMap f xs" and every once in awhile, the function just wouldn't return. On the plus side, I did get a a very nice, predictable speedup compared to the old "parMap rdeepseq f xs" way of doing things, at least in the 99.99% of the time where I didn't hit the deadlock.
Out of interest, I tried recompiling pandoc (and all non-HP dependencies) with -split-objs. The size of the (stripped, 64-bit) binary went from 45M to 28M. One oddity, though: for some reason, I could not compile the `mime-types` package with -split-objs, and I had to remove this option to compile it. With -split-objs, I got 1 of 1] Compiling Network.Mime ( Network/Mime.hs, dist/build/Network/Mime.o ) ghc: could not execute: /usr/bin/gcc Failed to install mime-types-0.1.0.3 after a long delay. This is strange: `mime-types` is an incredibly simple package, with nothing fancy or unusual, and it was the only package that gave me problems with -split-objs. 
This is great work. I'm extremely excited about putting this stuff into practice. This seems like the missing piece to finally let type level programming feel natural and straightforward.
huh, I think i've seen a similar problem before. I should revisit it again now that I'm using clang by default
Too bad the epub doesn't look this nice, and the PDF is too clunky to read on a phone (although it's fine on my desktop). Granted, I know that much of the formatting is up to the app reading the file, and O'Reilly. Still, I'm just starting the chapter on Repa (actually have the book open on my other monitor), and I'm quite enjoying it so far. There is definitely a lack of good tutorials on this subject, and I was very excited to see a whole book on it.
That's a bit of a relief; I'm happy to have bought such a fine book from you, but as a student with few resources, I would've been a little upset if I could've gotten the book for free by waiting a few days.
https://github.com/gasche/icfp2013-papers#haskell-symposium
There have been problems with the "direct" scheduler implementation. I've just uploaded a new monad-par to Hackage that makes the trace scheduler the default again. I'm not aware of any bugs with trace.
When I was at MSR a few years ago I was working on exactly this problem, but I got hung up trying to figure out how to deal with the deadlocks that could be caused when a scheduler blackholes in a pure computation owned by a scheduler running on another HEC (or even one that is blocked). Their solution is remarkably clever--I didn't know about the blackhole-stealing feature in the GHC RTS. I'm glad to see this work going forward, although it does make me a bit sad that the core implementation of transactional memory is remaining fully embedded in the C RTS. I wanted to be able to write my own STM libraries in Haskell!
I just stumbled over the example 'sudoku3.hs', because it's not obvious, why there's no call of 'force' needed. I'm assuming, that in this case it's expected that 'solve' isn't implemented lazily, right? sudoku3.hs main :: IO () main = do [f] &lt;- getArgs file &lt;- readFile f let puzzles = lines file solutions = runEval (parMap solve puzzles) print (length (filter isJust solutions)) 
Thanks for the update.
So it's not planned to have it permanently online like the LYAH and RWH books are?
"filter isJust solutions" will reduce solutions to WHNF, which does essentially force them.
 How should this force the deep evalutation of the value of the Maybe? 
It will be, yes, but I have some work to do to make it happen. Probably not for a couple of.months or so.
I didn't know that! Thanks, I'll update the tutorial.
 Ok, I got it, the 'isJust' is forcing it, because to create a Maybe value you need to know the result of the computation, how should you otherwise know if the Maybe should be created by the Nothing or Just data constructor. But I still have a hard time to wrap my head around the parallel computation. 'parMap' uses 'rpar' to create a Sparkle for each entry of the puzzles list. After the call of 'runEval' the runtime of Haskell executes the Sparkles in parallel. But if 'isJust' forces the deep evaluation, which is called by 'filter', which just walks sequentially over the solutions list, how should then the deep evaluation be done in parallel? Ok, I still don't get it. 
Or I'm just overthinking and the WHNF of a Maybe value already enforces the deep evalutation? So after 'runEval' the Haskell runtime just computes the WHNF of the Maybe value and is done. So strictly speaking, 'filter isJust solutions' isn't forcing the parallel computation.
could you point to a specific place please?
Published paper on I-structures: http://dl.acm.org/citation.cfm?id=69562
The relevant statement from the Haskell 98 report is: "Matching an n+k pattern (where n is a variable and k is a positive integer literal) against a value v succeeds if x &gt;= k, resulting in the binding of n to x - k, and fails otherwise. Again, the functions &gt;= and - are overloaded, depending on the type of the pattern. The match diverges if the comparison diverges." 
Thanks for writing this. As someone who is just starting to get into Haskell web development I found it interesting.
Core team is still pretty small, and I imagine they have other priorities. If the functionality is important to you, and you have some systems programming know-how, I'm sure they'd be happy to help you out with the project.
Hmm.. can't install the dependencies for the example code on my mac. Off to file a bug report about `accelerate-0.13.0.5`.
Sadly, the shipping cost to Germany is $ 22.5 when ordering on O'Reilly. I could buy it on Amazon, but then I won't get a digital copy. Is there any way out of the dilemma that will still get me both versions?
Thanks. I found a paywall free version from 1987 here: http://ecommons.library.cornell.edu/handle/1813/6650
Thanks!
Ok, I tried to fix it again; how is it now? EDIT: I mean to have `x` (and `x'`), and not `n`.
I already corrected `f n` to `f x`; perhaps you need to refresh your browser cache?
Looks good now! (Didn't see this post before writing my last one below.)
Great! Thanks. :)
Yes, you had corrected it in two of three places. I wasn't looking at an old page, but apparently at an intermediate one, while you were non-atomically editing parts of the page. (Dont know whether SoH has atomic edits :-))
Can you comment on the quality of the formatting of code listings and color diagrams? I'm trying to decide whether to buy the .mobi/.epub package from O'Reilly vs the Kindle version from Amazon.
Definitely
Yes, I-structures are from I'd. 
I think the logic of `over` is more general than that of a functor. If types s and t are generated by applying some functor F to a and b, than `over` becomes `fmap`. But there is no requirement that such functor exist, so `over` is much more general. The functor in Traversable is this `f`: traverse :: (a -&gt; m b) -&gt; f a -&gt; m (f b) In `over`, `f a` and `f b` are replaced by `s` and `t` (and `m` is replaced by Identity). ((a -&gt; Identity b) -&gt; s -&gt; Identity t) The functor has disappeared!
Yes, this. I've come to truly believe in my fingers that it takes approximately 2 seconds for anything to happen online.
Thanks! Me too, haha! I added “(UTC)”. I plan on adding a menu somewhere that will let you choose the time zone to display all times in. :-)
Not seeing that here (Firefox 22 on Linux).
Thanks - I'd be grateful if you could submit errata here: http://oreilly.com/catalog/errata.csp?isbn=0636920026365 Then it'll be available for others to see, and I can make sure it gets fixed in a future revision.
It's very easy to take `n` `String`s and `n` `Int`s in alternating order: class PolyFunction t polyFunction' :: [String] -&gt; [Int] -&gt; t instance (PolyFunction t) =&gt; PolyFunction (String -&gt; Int -&gt; t) polyFunction' ss is s i = polyFunction' (s:ss) (i:is) instance PolyFunction {- insert final type here -} polyFunction' ss is = {- insert core implementation here -} polyFunction :: (PolyFunction t) =&gt; t polyFunction = polyFunction' [] [] It's significantly more complicated, but nonetheless doable, to take all the `String`s first and then all the `Int`s next: {-# LANGUAGE MultiParamTypeClasses, FunctionalDependencies, DataKinds, FlexibleInstances, FlexibleContexts #-} data Nat = Z | S Nat class PolyFunction (n :: Nat) t | t -&gt; n where polyFunction' :: [String] -&gt; [Int] -&gt; t instance (PolyFunction (S n) t) =&gt; PolyFunction n (String -&gt; t) where polyFunction' ss is s = polyFunction' (s:ss) is instance (PolyFunction n t) =&gt; PolyFunction (S n) (Int -&gt; t) where polyFunction' ss is i = polyFunction' ss (i:is) instance PolyFunction Z {- insert final type here -} where polyFunction' ss is = {- insert core implementation here -} polyFunction :: (PolyFunction n t) =&gt; t polyFunction = polyFunction' [] [] EDIT: Got my `String`s and `Int`s swapped around. EDIT 2: Swapped `n` and `S n`, as per /u/TimTravel's suggestion EDIT 3: Corrected Markdown formatting
Haha, that's great!
I switched n and (S n) in the first two instances of the second example, and it worked great. Thanks!
Looking forward to it! BBQ at Lake Zurich sounds incredibly awesome :)
Public Service Announcement: reddit's automated spam filter really, really hates URL shorteners. I strongly recommend using direct links to avoid having your comments automatically flagged and hidden until a moderator happens along to rescue them, as I just did with the above comment.
Very Cool! Would be neat to add all the various "focus" channels on http://www.haskell.org/haskellwiki/IRC_channel (ok, maybe thats just so i can pump up my loudness score :) )
I hadn't seen LIO, thanks for the link. Language-based security is the more general notion. Just as we have the privilege/obligation duality for rights, we also have a secrecy/integrity duality for security. IFC is all about ensuring that secrets don't escape; whereas things like checksums and signatures are all about ensuring that lies don't enter. Both secrecy and integrity can be built into a language, and both have been; but as I mentioned, so far, it's extremely rare to see a language which does both at once.
Just checking: You are aware that the times logged by tunes are *not* in UTC, right? 
i'd say being on the same list as lambda bot is quite the accomplishment :) 
I didn't know Intel Labs had a Haskell compiler. Is it publicly available?
No. There will be a paper available about its design next week, also from Leaf Peterson and Neal Glew. The way they approached using GHC as a frontend has fond memories for me; David Himmelstrup and I used the same approach in [LHC](http://hackage.haskell.org/package/lhc) of compiling everything to external core (including all base libraries,) and then linking all of it into one big whole-program at compile-time, and optimizing that (but the similarities seem to end there - they have a much different IR and backend.)
That makes sense. So, what if there's a function that makes sense for one type of unit but not another? Like, every unit will have a getPosition function, but a tank might have a getArmour function, which doesn't make sense for a scout without armour. What's the standard way to deal with this? Should I just have such a function throw an error if it's called on an invalid unit? 
I try to always not use type classes first, since many solutions are much simpler using just ADTs and higher-order functions. But there are definitely times when a type class is very handy. Type classes are really useful when they express a common structure (read: laws) that exist among a family of types. This makes Functor or Monoid great candidates for type classes. They both emphasize the common pattern in an *interface*. But if you are using type classes to capture the common pattern of some *implementations*, then I think you may be heading into abuse land. An ADT with HOFs can abstract over multiple implementations equally well, without some of the headache that a misused type class can cause. Just my 2c.
(Moved [to StackOverflow](http://stackoverflow.com/a/17849276/849272).)
The funny thing, that's probably less than a fifth of my chatter volume if you count all the other Haskell channels. Seriously, I barely talk on the main Haskell one compared with some of the other community channels :-)
The first solution is the idiomatic one. Avoid using typeclasses to achieve what you can use a simple function and case distinction to achieve.
Yeah, they are converted to UTC on import [by clogparse](http://hackage.haskell.org/packages/archive/clogparse/0.2/doc/html/src/Data-IRC-CLog-Parse.html#haskellConfig) from LA time. So if you compare them, you'll see that [tunes](http://tunes.org/~nef/logs/haskell/13.07.23) has 23:59:59 --- log: ended haskell/13.07.23 whereas [the corresponding line](http://ircbrowse.net/browse/haskell?id=16139035&amp;timestamp=1374649199#t1374649199) on IRC Browse is: 2013-07-24 08:59:59 +0200 ended haskell/13.07.23 (UTC +2 is the time zone of my server in Germany.) This is also why I have to wait some hours to get the logs of “yesterday” from tunes, because I have to wait for tunes's day to end. At 10:30 UTC+2 (padded for safety), we will have the 24th. Time is one of those things developers just get wrong no matter how much care they take, so you're right to check!
Angel works well. Here's [my modest config](http://lpaste.net/4453075642145046528). I use a patched version of it which uses syslog additional to stdout/stderr (I prefer to consolidate all my service logs into syslog). Tryhaskell as a service dies once in a while, due to various sandboxing factors (and I have a cronjob to kill it if it takes too long), so Angel gives me assurance that it's always running.
I tend to use typeclasses only when generating and passing around the instances manually becomes a big mess. In the code I write this is almost never. 
Throwing errors is bad, unless something really is an error (e.g., divide by zero). In general, it's better to make things explicit. If some function can fail in one way, then use `foo :: ... -&gt; Maybe Foo`. If the function can fail in a number of interestingly different ways that callers might want to distinguish, then use `bar :: ... -&gt; Either AllTheWays Bar`. For designing a game with many different units, here's what I'd do: (1) If all the different units are homogenous in nature (e.g., they all have the same sort of characteristics, modulo things like having `Nothing` for armor), then define one record type `Unit` (or `Character`, `GamePiece`,...) and project out the different components as necessary. (2) On the other hand, some games have severely different classes of units which follow different rules. If there are only a few of these (e.g., human pilots vs their mechas in Xenogears), then just define different record types for each class (e.g., `Human` vs `Mecha`). Type classes aren't necessary, but they can be useful here if for some reason you need to project the different classes all into the same "space" (e.g., you need a list/set/... of units and it's infeasible to pass separate collections for each record type; or you need functions which are polymorphic over the record types, only caring that they can "cause damage" or the like). (3) The last case is that you have a bunch of different classes of units where (1) they're not nearly homogeneous enough to smoosh into one record type, and (2) they're too many combinations to break them into different record types. The key examples here are when you have a bunch of "roles", and any given unit has a set of roles; thus you get an entire powerset lattice of different record types. When you run into this situation, that's when I'd pull out the type classes (one for each role).
In your example, the big problem with using typeclasses is that you probably want to use those types polymorphically, that is, at some point you will want a list of units, some of them being tanks, others being scouts, or you want to call one of the typeclass methods while deciding which actual type to use at runtime; that's not how typeclasses work, though. The type is resolved at compile-time, so you can't have a `[Unit]` type in your code. You *can* solve this problem with existential quantification, and I've seen it done countless times, but a better approach looks a bit like this: data Unit = Unit { maxHealth :: Int , currentHealth :: Int , unitTypeName :: String , receiveHit :: Int -&gt; Unit } newTank :: Unit newTank = Unit { maxHealth = 20 , currentHealth = 20 , unitTypeName = "Tank" , receiveHit strength = newTank { currentHealth = if strength &gt; 10 then currentHealth - 1 else currentHealth } } newScout :: Unit newScout = Unit { maxHealth = 10 , currentHealth = 10 , unitTypeName = "Scout" , receiveHit strength = newScout { currentHealth = currentHealth - 1 } } You can add other data into individual kinds of units by closing over them and carrying them through your updating "methods"; if you have more than one or two of them, it's probably wise to wrap them into another data type, e.g. `TankData` and `ScoutData`. 
Nice writeup. Completely agree. You could introduce another step between regular projection (with possible maybes) and full type classes: explicit dictionaries. Instead of storing a list of units that we can render (with some typeclass constraint for rendering), you could just store a list of render functions.
I am not sure about this. There is a misprint in the paper. On page 5 the type signatures for blockAct and unblockAct are swapped (the right ones are on page 6 in 4.7).
According to the speedup graphs even the non-vector version varies from on-par-with to much-faster-than GHC... Interesting.
Yeah, because sweeping errors under the rug is always a good idea!
This article goes into this technique in a bit more detail: http://www.gamedev.net/page/resources/_/technical/game-programming/haskell-game-object-design-or-how-functions-can-get-you-apples-r3204
Well, it is a good idea if you can check preconditions in a very cheap way. In that case, you can make the division primitive (let's call it `insaneDiv :: (SkewField a) =&gt; a -&gt; a -&gt; a`) faster. You could use the following wrappers: div :: (Eq a, Num a, SkewField a) =&gt; a -&gt; a -&gt; Maybe a div _ 0 = Nothing div x y = Just $ insaneDiv x y unsafeDiv :: (Eq a, Num a, SkewField a) =&gt; a -&gt; a -&gt; a unsafeDiv x y = fromMaybe (throw DivideByZero) $ div x y The only thing you know (i.e. should be able to assume) is that `insaneDiv` is a total function, but that using a zero denominator may lead to a nonsensical result. -edit- Arithmetic overflow is also relatively cheap to check.
Do you have any caching layer? Like Memcached? Thanks!
JHC works similarly.
The [RecursiveDo](https://www.fpcomplete.com/user/PthariensFlame/guide-to-ghc-extensions/basic-syntax-extensions#recursivedo-and-dorec) example actually throws a warning about `RecursiveDo` being deprecated, rather than `DoRec`. So does mine `ghc-7.4.1`...
Just gave GHC 6.12.3 and my extremely supped-up copy of JHC two of the programs from the nofib benchmark suite. GHC is compiling with just --make, JHC' is using clang -O0. -rwxr-xr-x 1 dumael staff 1.0M 19 Jul 18:27 blackscholes.jhc.bin -rwxr-xr-x 1 dumael staff 1.3M 25 Jul 14:31 blackscholes.ghc.bin -rwxr-xr-x 1 dumael staff 790K 24 Jul 18:57 ray.jhc.bin -rwxr-xr-x 1 dumael staff 1.3M 25 Jul 14:31 ray.ghc.bin Just some confirmation of your point. 
My simple rule : don't use typeclass unless you can't do otherwise. This is not a criticism, typeclass is a useful design, very handy when used properly. One of the best use case I can think of is : generalizing a function while keeping type safety. For example see the `sort :: Ord a =&gt; [a] -&gt; [a]` function, which is a perfect use case. In general, if you are both the only author and the only user of the typeclass, you can avoid it and use the more simple and idiomatic ADT approach.
`DoRec` is also kind of deprecated (in favor of `RecursiveDo` even afair) at least in some versions of ghc.
I usually do this with ed. E.g. echo 4,20p | ed /var/log/syslog Or with sed: sed -ne 4,20p /var/log/syslog
You can check for this at compile time using [Liquid Haskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/01/01/refinement-types-101.lhs/) so that you don't pay a price at run-time.
While I understand that it's easy to wrap it in a safer container, I don't think the default division should be unsafe like that. Maybe have an unsafeFastDiv tucked away somewhere, but making 4/0 evaluate to a nonsensical value is weird.
In my feelings, avoid type classes as much as you can. When you can't avoid them any longer, then use them.
This might be a known issue, but in the Basic Syntax Extensions article, the "Try it out!" examples in the UnicodeSyntax, RecursiveDo, LambdaCase, and MultiWayIf sections do not compile in the embedded editor.
That's not true — JHC has its own parser and typechecker.
You're right, I was referring to: &gt;compiling everything to external core (including all base libraries,) and then linking all of it into one big whole-program at compile-time. Apologies for any confusion.
Also: Intel, not Amazon.
I don't want to discourage you from exploring the furthest reaches of Haskell's type system --- I'm trying to aim this comment at any beginners who may be reading this. If you think you need to write a function like this, you can almost certainly get away with instead writing a simpler function that takes a single `[(String, Int)]` argument instead.
Hrmm. It is interesting how small the gap between those is in practice though. I confess I was describing rough orders-of-magnitude mostly from memory and for toy problems.
I find it enlightening that in functional programming, usually the best solution is to program with functions. ;)
These are very much in the spirit of the well known screed by Lindley, Wadler and Yarrop: http://homepages.inf.ed.ac.uk/wadler/papers/arrows-and-idioms/arrows-and-idioms.pdf
That's why I had `div :: (...) =&gt; a -&gt; a -&gt; Maybe a`. The current standard division operator is bad, just like `head` and `tail`, because it isn't total. But I think that the underlying `insaneDiv`, or `div#`, or whatever you'll call it in GHC Core, should be allowed to give a garbage answer to a garbage question, since that situation is already avoided through safe types.
Sure. I'd rather it isn't total than giving weird results, though. Hopefully you'll notice quicker if it blows up completely.
It's worth noting that sed can also do "ranges that are only bounded on one side": sed -ne '50,$ p' and of course you can delimit the ranges via regexp: sed -ne '/^Jul 25/,$ p' /var/log/syslog
GHC.
Yeah, this is a known issue; actually, it's two! In the cases of `RecursiveDo`/`DoRec`, `LambdaCase`, and `MultiWayIf`, it's because the School of Haskell is not up to date with the Haskell Platform. The articles are written with the latest Haskell Platform in mind, which includes GHC 7.6 rather than GHC 7.4. In the case of `UnicodeSyntax`, however, it only fairly recently broke, and I think it may have something to do with multiple versions of `containers` being installed.
It is actually the visual mode of ex, not ed.
Second example doesn't actually do what it advertises to do. It creates instances that take the same number or more Ints than Strings in any order that satisfies that every prefix has the same number or more Ints than Strings. Going for equality is easy, just replace: polyFunction :: (PolyFunction n t) =&gt; t with polyFunction :: (PolyFunction Z t) =&gt; t But it still has that prefix condition. You can eliminate it by adding a second counter: class PolyFunction (n :: Nat) (m :: Nat) t | t -&gt; n, t -&gt; m where polyFunction' :: [String] -&gt; [Int] -&gt; t instance (PolyFunction (S n) m t) =&gt; PolyFunction n m (String -&gt; t) where polyFunction' ss is s = polyFunction' (s:ss) is instance (PolyFunction n (S m) t) =&gt; PolyFunction n m (Int -&gt; t) where polyFunction' ss is i = polyFunction' ss (i:is) instance PolyFunction n n Double where polyFunction' ss is = 1.0 polyFunction :: (PolyFunction Z Z t) =&gt; t polyFunction = polyFunction' [] [] x = 1 :: Int s = "1" -- Good _ = polyFunction s s x x :: Double _ = polyFunction s x s x :: Double _ = polyFunction s x x s :: Double _ = polyFunction x s s x :: Double _ = polyFunction x s x s :: Double _ = polyFunction x x s s :: Double -- Bad --_ = polyFunction s s x :: Double --_ = polyFunction s x s :: Double --_ = polyFunction s x x :: Double --_ = polyFunction x s s :: Double --_ = polyFunction x s x :: Double --_ = polyFunction x x s :: Double --_ = polyFunction x x x :: Double Or enforce the order of arguments with this: class Down (n :: Nat) t | t -&gt; n where down :: [String] -&gt; [Int] -&gt; t class Up (n :: Nat) t | t -&gt; n where up :: [String] -&gt; [Int] -&gt; t instance Up n t =&gt; Up (S n) (Int -&gt; t) where up ss is i = up ss (i:is) instance Down (S n) t =&gt; Down n (String -&gt; t) where down ss is s = down (s:ss) is instance Up n t =&gt; Down n t where down = up instance Up Z Double where up ss is = undefined poly :: (Down Z t) =&gt; t poly = down [] [] x = 1 :: Int s = "1" -- GOOD test1 = poly s x :: Double test4 = poly :: Double test7 = poly s s x x :: Double test13 = poly s s s x x x :: Double -- Bad --test2 = poly s :: Double --test3 = poly x :: Double --test5 = poly s x x :: Double --test6 = poly s s x :: Double --test8 = poly s x s x :: Double --test9 = poly x s x s :: Double --test10 = poly x x s s :: Double --test11 = poly s x x s :: Double --test12 = poly x s s x :: Double --test14 = poly x s :: Double Last one needs -XOverlappingInstances, so I guess it's frowned upon. This is my first time playing with type level programming and I've spent quite bit of time trying to do it without -XOverlappingInstances, so I'd love to see a solution without it.
now that you mention that the second time in a few days... do you plan to write a tutorial on liquid haskell? That would be great!
I've had this issue before, and not with `accelerate`. It's definitely a GHC bug.
Right! I totally forgot that one, even though I've used that pattern a few times before.
The big issue —and this will show up if you start doing numerically intensive computations— is that all those "cheap" checks aren't so cheap when you have to do them over and over. This sort of thing destroys tight loops and also ruins the command cache.
That is, we want refinement types such as `{a:A | nonzero a}` but these should not be strictly conflated with (weak-)Sigma types, because we also want a way to non-locally discharge the refinement obligations. Thus, we want functions like div :: forall A. SkewField A =&gt; a -&gt; {a:A | nonzero a} -&gt; a but also control structures within the scope of which we can use `div` as if the second argument were simply of type `A`.
Well, there are "meadows" which are like fields except that division by zero is fully defined. Not sure we should assert that `Fractional` types are actually meadows though...
That logo is boss.
A few quick notes seeing as I intended to post it here myself when it was more polished: * This is my first Haskell project so I apologise if anything is not idiomatic. Raise an issue if anything particularly stands out. * Yes it uses Cairo and yes I'm sorry. It was used for quick prototyping of the API, eventually (v1.0.0) I will be trying for an OpenGL/GLFW backend. To be fair, it runs fine at the moment, at least on my higher-end laptop without any graphics-heavy drawing. * The current version in Hackage was not tested at all beyond a few basic demos, I've started adding in tests and I'm working on more interesting demos. I am also working on completing the guide on the website to a useful extent. The new version (v0.4) should be out by the end of the weekend.
Of all the places to encounter a Davidson allusion!
Touche. I will note that ex was derived from ed by way of em and en.
Again, «external core» is a GHC concept as far as I know. Because JHC doesn't use GHC as its frontend, there's no need in external core for JHC — it simply uses its own internal representation. JHC indeed does whole-program optimization, though.
Good stuff. How does GLFW compare to SDL anyway? (Better, worse, different? in terms of Haskell bindings I mean) 
GLFW supports multiple window handling. This would result in all the window input signals needing to take some abstract window object which is a slight annoyance I suppose. But multiple windows is theoretically more threadsafe and could allow multiple games to be running alongside each other. Note that SDL2 works like that too, but it's still in development. Plus, from what I can see the GLFW bindings seem to be more actively developed. 
Not quite, their compiler outputs a psuedo-C called [Pillar](http://www.leafpetersen.com/leaf/publications/lcpc2007/pillar-lcpc.pdf) which has support for some high level concepts like second-class continuations and transactions. In that paper, they describe Pillar as a modification of the Intel compiler. However, since then it has apparently been changed to be a source-to-source compiler which transforms to C, and can use Intel or GCC to compile the result. They say this in their [related paper](http://www.reddit.com/r/haskell/comments/1j2k4a/measuring_the_haskell_gap_pdf/) as well. But it's not clear in either whether the final result is compiled using Intel or GCC, although I bet it's Intels compiler.
[filed](http://ghc.haskell.org/trac/ghc/ticket/8094)
I thought SDL did joysticks? There's even [this](http://hackage.haskell.org/packages/archive/SDL/0.6.1/doc/html/Graphics-UI-SDL-Joystick.html).
Very cool! Any plans for loading external raster graphics yet?
I think `-` is probably better notation for the whole file than `*`, being an unbounded range and not requiring quoting in the shell.
&gt; I'm still disappointed that neither provides joystick events. GLFW doesn't generate joystick events exactly, but you can definitely get joystick input. You can see it in action in GLFW-b-demo.
Very active. I'm the maintainer, and I always like to hear any kind of feedback.
I think that spoj supports haskell
Best C and Best C compiler of course :) (to be better you must compare yourself to the best)
&gt; We attempt to provide a very careful analysis of the relative performance of C and Haskell on six of these benchmarks, using both the standard Glasgow Haskell Compiler (GHC) and our ex- perimental whole-program optimizing Haskell compiler, the Intel Labs Haskell Research Compiler (HRC). Interesting, I've never heard of Intel's Haskell Research Compiler. Can anyone tell us more about it?
Well spotted, thanks. Fixed. (Another nicety of Haskell is of course that if I had tried to run this, the compiler would have slapped me for it).
Indeed. It is easy to forget this part.
What does it make Elerea a much better option that the other available FRP libraries? I'm currently looking for a FRP library to use and this input would be very valuable for me :)
When you've finished a program section, it's a good idea to do two things: - [HLint](http://hackage.haskell.org/package/hlint) tells you about stylistic errors, e.g. "write this point-free in line ...". Install it with `cabal install hlint`, and then run it in your source folder using `hlint -c *` (`-c` is for colored output) to check all files contained (including subdirs). - Compile with -Wall and fix the warnings. For example, it will tell you that you didn't use the `action &lt;- getArgs` in `Main.hs`.
I find a bit funny that in this case the version with the most functions is also the most object oriented one.
Another way to look at the classification you mention is about how easy it is to extend the program: in option a) you fix the interface but make it easy to add new types of units while in option b) the types of units are fixed but its very easy to add new functions over the Unit ADT. I wish we had a better name for this than "expression problem" though.
This probably isn't what you are looking for but I'd strongly advise aganist using "system" or any equivalent functions since they potentially open you up to a command injection vulnerability. Think what would happen if somebody changed the modules file to read hg blah; {rm,-rf,/} The bottom line is you probably should be using an API to interact with these VCS.
I would advertise [reactive-banana][1], but I'm kind of very biased there. ;-) But I'm interested in hearing what other libraries do better, maybe I can improve the bananas. [1]: http://www.haskell.org/haskellwiki/Reactive-banana
There's also Perl; here's a guide to this category of one-liner: [Selective printing and deleting of lines](http://www.catonmat.net/blog/perl-one-liners-explained-part-six/) The use of Perl is surely hunting squirrels with an archaic model of bazooka. Yet various of my shell scripts have Perl one-liners, after I hit bugs in the OS X implementations of more basic commands such as ed.
Thanks a lot, that's exactly what it needs right now. I've only been trying it out on Linux so any efforts to test it out on other platforms are of great help. I've already found out that OSX doesn't work without a C wrapper, for example.
&gt; Can anyone tell us more about it? The paper talks about HRC a little bit on page 2, section 1.4. Summary: it intercepts Core generated by GHC, does high-level optimizations, and then generates C to be compiled with GCC.
one of the authors will starting sunday: http://www.leafpetersen.com/leaf/publications.htm
My two favourite things about elerea: 1) Simple, small codebase 2) No language extensions
&gt; When you apply a function to an argument expression, you replace all instances &gt; of name within the function's body with the argument expression. All *free* instances. Eg (\ x . \ x . x) 1 is \ x . x not \x . 1 Also, function application associates to the left, you can just state this and omit the gobs of parens. Finally, I think that mentioning the Y combinator/fixpoint friends would fit nicely into your section on nonterminating evaluation. 
Thanks - I decided to omit the discussion of free vs. bound variables because I thought it might be too much of a digression for something that's meant as a brief introduction. That's covered a bit in the comments - perhaps I'll add something to the end of the article or link to the comments. I've seen elsewhere that I'm using an outdated "encoding" - I'll have to look up how to consistently re-write things without the parens. Thanks for the feedback!
I'd mention free vs bound even informally: it's a pretty central part of the semantics of beta and alpha conversion. 
Not bad! The main thing that jumps out to me is that you could replace the lookup on your map with a function like; vc :: String -&gt; Maybe SourceControl vc "git" = Just Got vc "svn" = Just Subversion vc "hg" = Just Mercurial vc _ = Nothing
you are allowed to distribute it once.
This is fantastic! People have often claimed—and I felt inclined to believe them intuitively—that Haskell is a better language for teaching. I strongly welcome any evidence that this is the case.
The phrase "in fact the entire project didn’t compile for about 4 hours" made me laugh. It sounds miserable and nearly impossible to believe—who struggles to even get something to compile for four hours! Were you beating your head up against a semicolon error the whole time? Of course, anyone with experience in Haskell/Scala/*ML that just sounds incredibly productive—an experience of rapidly trimming off false starts and poor, incoherent invariants.
Both GLFW and SDL support polling of the joystick. But that is not the same as joystick events - e.g. you can miss button presses if you don't poll fast enough. The sad bit is that all major OS's have supported joystick events for years.
This piece was excellent. It mirrors much of the experience I had when learning to develop serious projects in Haskell. Two side notes. I still find json to be suprisingly clunky to deal with in Haskell. Maybe lenses are the answer. No time to figure it out right now. Also I would recommend to add Hutton's book to the reading list. Surprising how simple, yet thorough the approach of that book. 
Which book is that? I haven't read anything by Hutton
"Programming in Haskell" by Graham Hutton. Thin book, only generalizes after examples are understood. Nothing wrong with your recommendation BTW. Again, great piece. 
I am one of the authors. We do use the Intel C Compiler, not GCC.
&gt; "Can I call procedures inside functions?", the answer is Yes. But the global state before and after you call a function remains the same. The effects that occur inside a function are undone when the function returns a value. .. while I think the idea of using Haskell (or anything similar) in education is interesting enough, this seems like a very odd language to contrast it with. That's the weirdest stateful behaviour I've ever seen.
Schemas do not scale! &lt;/sarcasm&gt;
Even if you believe that (not saying you do, just even if), ADT schemas are unlike any schemas you've seen before. If you want just some part to be schema-free, just make a product (or coproduct, depending on your use) with an `Aeson.Value` type—instant extensibiliity and Postel's Law compliance! Tired of writing repetitive schema? Abstract them out with polymorphic schema! Use type-classes to constrain that polymorphism. Write processors which operate over your extensible-but-wrapped types in a typesafe manner. Sometimes I encode my ADTs into JSON-Schema—which, to be clear, is for most intents and purposes a pretty nice schema language—and it's just painful.
I really love the feeling of doing a huge refactoring of code spanning several modules and then loading the top-level module into `ghci` and everything type-checks. It's such a high.
Small nitpick: The example you listed under the heading "RankNTypes" is actually just an existential types. A rank-N type has a nested forall to the left of a function arrow: forall a. a -&gt; T -&gt; U -- rank 1 (forall s. s -&gt; T) -&gt; U -- rank 2 ((forall s. s -&gt; T) -&gt; U) -&gt; V -- rank 3 (I think)
Your "rank-3" type is actually just rank-2. The "rank" number is the maximum number of nested "forall"s, where there is always an implicit "forall" on the outside of the function. An actual rank-3 type would be `forall a. (forall b. (forall c. (T -&gt; c) -&gt; b) -&gt; a) -&gt; U`. EDIT: Fixed parentheses.
You are doing the job I wish I had! What kind of credentials do you have? A software engineering or education based background? Programming has been a hobby of mine from an early age (9 or so with BASIC). I feel like it gave me a huge head start over my peers when learning abstract concepts, or even more generally creating a linguistic abstraction for processes. Being able to inductively reason is very important, and Haskell makes you think much more inductively, or on a different level of induction, than a procedural language. Have you had the children implement their own versions of Prelude functions like flip, zip, zipWith, foldl, foldr? Implementing them over my own data types produced many more "Ah Ha!" moments when I first started than anything else.
I agree. I'm not the designer of this language. The effects presented by the language are only a few, so it's not very bad. It's a very simple language, that's all, and it's better than other general purpose language to teach the basics of imperative programming because of the purity of many concepts. I proposed a version as a DSL using monads in Haskell :), but I'm not sure how better it is.
The performance issue at the top of the second column of page 5 is very subtle. [Rant about the two-column format omitted.] Too subtle for me. I think I need a better understanding of exactly how lazy evaluation is implemented at a low level. Can't GHC see that `limit` is always used inside `search'`, so that it should be evaluated first when `search` is entered? Then why does `search'` need to visit a thunk on every iteration to access `limit`? And, whatever is wrong with the above logic, how does adding `seq limit` in `query` fix it? Is GHC doing lots of inlining and generating totally different code for `search'` when it knows that `limit` has already been evaluated?
Some corrections in the first part: &gt; forM_ print (lines content) should be `forM_ (lines content) print` or `mapM_ print (lines content)`. &gt; `liftM` lifts values from the IO monad into the State monad No, nononono. How is liftIO :: MonadIO m =&gt; IO a -&gt; m a a special case of liftM :: Monad m =&gt; (a -&gt; r) -&gt; m a -&gt; m r ? Where did the `(a -&gt; r)` go in `liftIO`? Where did the two different Monads come from? And I see no `State(T)` in either. `liftM` is `fmap`, it has nothing to do with `liftIO`, which is part of the `MonadIO` class and lifts values from `IO` to some `m` which is an instance of `MonadIO` (i.e. has IO as its base Monad). The real "special case" is that `m` in `liftIO` is specialized to `StateT Int IO`: liftIO :: IO a -&gt; StateT Int IO a (Edit: typo and formatting)
I'm a graduate from a carrier focused on software developing (not engineering), professor at secondary and university, and education researcher (without credentials yet). I also work at [Fundacion Sadosky](http://www.fundacionsadosky.org.ar/en/), developing activities for children of all ages. The mission is to teach Computer Science in all levels of education, similar to the work that [Computing at School](http://www.computingatschool.org.uk/) is doing.
&gt; They don't enjoy making games and having fun at school, they want to learn relevant ideas that they can use in real and even complex situations. They want to be useful and give correct answers to challenging problems. This sounds like the most important observation. What kind of problems motivate students to learn? 
You have to avoid capture too, Eg (\y.(\x.yx))x is \u.xu not \x.xx. So alpha equivalence needs to be introduced.
&gt;A comparison between guns and tractors
Do you know of any US based programs that are doing what Computing at School or you are? We (the US) seem to be quite behind in regard to teaching programming early and in depth (more than a week long course). Kids should learn concepts of how a computer works not how to use proprietary programs (I'm looking at you Excel and Word). I would really like to get involved in something even if it's an ancillary or voluntary role.
I also love the opposite, when after a big almost mindless refactor I reload the top-level module and get a big listing of exactly all the stuff needed to do to make it work again.
Does anyone have the slides in PDF form?
Err? Wikipedia does not agree: &gt; For some fixed value k, rank-k polymorphism is a system in which a quantifier may not appear to the left of k or more arrows (when the type is drawn as a tree). I think nominolo got it right.
This is fascinating! You are doing marvelous things to spread the Haskell approach to facing software development and furthering curious minds. Thanks for sharing your story! Also, I find your story particularly exciting since I'm Argentinean too, and I wasn't aware that this was happening in my own country. Thank you. I look forward to learning more about your experience and the use of Haskell throughout the country. Count me in if for any Haskell related activities that might happen in Argentina.
[Computing in the Core](http://www.computinginthecore.org) sounds like what you want.
I had to sift through the date/time libs recently and this is an example code i came up with. Maybe it helps. import Data.Time main :: IO () main = do now &lt;- getCurrentTime tz &lt;- getTimeZone now let lt = utcToLocalTime tz now let (year,month,day) = toGregorian $ localDay lt putStrLn $ "Year: " ++ show year putStrLn $ "Month: " ++ show month putStrLn $ "Day: " ++ show day 
Look at this and then look back at the library, it should help you understand how these things fit together. I am surprised there weren't any examples in the docs. impor It System.Time main = do t &lt;- getClockTime let utc = toUTCTime t month = ctMonth utc day = ctDay utc year = ctYear utc print month mapM_ print [day,year] day and year :: Int month :: Month -- Result: -- July -- 27 -- 2013 -- If you want just numbers change the last 2 lines of the main above to -- mapM_ print [fromEnum month, day, year] -- Result: -- 6 -- starts at 0==January, so add one if you need it in the standard format -- 27 -- 2013 Edit: The other answers use Data.Time, I used System.Time. Data.Time is the better choice for almost everything, but both are sufficient for returning (m,d,y)
Looks like what you want is [utctDay](http://hackage.haskell.org/packages/archive/time/latest/doc/html/Data-Time-Clock.html#v:utctDay) and [toJulian](http://hackage.haskell.org/packages/archive/time/1.4.1/doc/html/Data-Time-Calendar-Julian.html#v:toJulian). &gt; o &lt;- getCurrentTime &gt; toJulian (utctDay o) (2013,7,14) 
I also welcome any evidence to the contrary. I really want to know what language is best for teaching.
The Julian Calendar is not what most people use nowadays. The gregorian calendar is the hip, new calendar ... since about 500 years.
Thanks for writing that up! Might be worth xposting on proggit.
... oops. The old ways are the best ways.
You might find this article on [indentation](http://okasaki.blogspot.com/2008/02/in-praise-of-mandatory-indentation-for.html) interesting.
http://www.codeavengers.com/ It's a startup run by a guy in New Zealand, but he's done a couple of code camps in the US as well. I just helped out with one in California a few weeks ago for 6-8 graders. His system is very high quality, and perfectly suited for that age group.
There is a nested loop here. You are right in pointing out `limit` can be evaluated eagerly before `search'` does its job. Indeed GHC's strictness analysis is able to figure this out. But the issue as pointed out in this paper is that `limit` is kept as a thunk in the outer loop (`map`). In each iteration of the outer loop, `limit` is evaluated, and passed as a value to the inner loop (`search'`).
I dunno about you, but saying 'time is hard, suffering is part of the joy of using time in your Haskell program' doesn't make me feel warm and fuzzy. It seems reasonable (and for many applications: absolutely correct) to not care and just use features equivalent to joda-time or boost date_time... which don't track leap seconds either, and yet they somehow manage to work for many people and might even be reasonably intuitive to use. Improving our core libraries should be an explicit goal, justifying a bad design because the problem is hard isn't what we should be satisfied with.
One good reason to try supporting GHC 7.4 is that travis has only GHC 7.4 at the moment.
I can only help with some Agda code: dff : Stream Bool → Stream Bool dff inp = false ∷ ♯ inp
Indeed, that does sound like a good reason -- thanks!
Can't you just make a wrapper script to launch the executable with the proper options?
&gt; I am surprised there weren't any examples in the docs. What, really? 
Thanks, I like that, but could you explain to me why that is better? 
That is indeed a very good point. Thanks
Thank you for the suggestion! To clarify, my library does not provide executables itself but rather it provides a framework to make it easier to write programs that perform a certain class of one-off computations, and so in general these are not programs that are going to be installed somewhere.
that would still lose the help which is supposedly automatically generated from `cmdtheline`.
I interpreted as being similar to Whiley - basically, deep copy every argument and returned value, and disallow access to (or updates of) variables from the outer scopes. Not sure if Whiley does the latter. 
The github link mentioned in the article shows no changes in last two months. Why so? I really hope to see more progress: better error checking (even unknown identifiers are not caught right now, causing runtime errors in JS), more events to react. Also, is there a way not to include full elm-runtime.js? I tried using closure compiler but it only worked fine in basic mode (no removal of unused code, just whitespace reduction and some renaming). "Advanced" mode caused errors (or maybe I used it wrong).
No problem! I tried to post there as well, but it got caught in the spam filter I guess. Waiting to hear back from the mods there :)
Not only Travis, but in general Ubuntu LTS.
Uh, I think the time library is pretty good. You can do everything with it that you'd want to do on a regular basis quite conveniently. It's just that there's a lot of extra stuff in the Haddock you have to filter through because of all the fanciness you *might* have to deal with. See [tauli's code](http://www.reddit.com/r/haskell/comments/1j519n/haskell_time_libraries_are_confusing_me/cbb6pe2), and note that most of the time, you'd probably end up using formatTime rather than converting to a Gregorian (year,month,day) triplet explicitly.
I wouldn't say it's hard. Just understand what types mean and then it's all cool and prevents you from doing stupid things.
It might be functionally correct but that doesn't mean it's easy to use.
Ah, I hadn't even thought of that --- thanks!
the thing looks like an optimization monster on the benchmarks they're looking at. like 5+ times faster code generated than GHC
Thanks for the help. This lead me to the solution (it's utcToLocalTime instead of utcTimeToLocalTime, but that was easy to resolve). I wasn't confused about what the types represented -- I couldn't find the path that converts from the type I have to the type I wanted. I don't know why I didn't see localDay anywhere, but you fixed it. I appreciate the assistance.
It's not hard to use... The only thing I've had more trouble with than the time libraries is random numbers. I think it's because whoever designed it had a certain paradigm in mind, and it doesn't match my expectations. The documentation is somewhat opaque, which puts the onus on the reader to adapt to the mindset in order to do something that is not as complex in other programming environments I'm not complaining, though. It's hard to do something in any language. In this case, it's that the occasional library twists in unexpected ways and needs to be figured out. 
If I need to calculate the date of Russian Easter, now I know where to look!
Can definitely recommend this book. The examples are fantastic. 
That's cool. I don't know why you were downvoted. I do like the Data.Time library's timezone conversions, though. I did see a reference to the ctMonth, etc., somewhere, but since that had to do with the ClockTime type in System.Time, the names confused me. I.e., if you have a ClockTime, you convert it to a UTCTime (as you demonstrate above), and then you can use the accessors for a CalendarTime? The API docs do not appear to lead you in that direction.
Cool, that's basically what I have now, after help from you and roconnor above. Thanks!
As a Quartodeciman heretic, I'm finding that [`hebrew-time`](http://hackage.haskell.org/packages/archive/hebrew-time/0.1.0.2/doc/html/src/Data-Time-Calendar-Hebrew.html) really clears up my paschal calculations: &gt; import Data.Time.Calendar.Hebrew &gt; toGregorian $ fromHebrew $ HebrewDate 5774 Nissan 14 (2014,4,14)
toUTCTime :: ClockTime -&gt; CalenderTime. So even though it has that name, the ct functions are the way to go.
But with effort from the Haskell community, that could be changed, no?
Not until the next LTS release
I think you just made /u/snoyberg's day
Indeed.
&gt; The other answers use Data.Time, I used System.Time Isn't System.Time deprecated?
Buy it on amazon and register it on orally, you'll be able to buy the ebook for $5. But the registering doensn't work at the moment
In other words, in Haskell (roughly), data Stream a = Cons a (Stream a) dff :: Stream Bool -&gt; Stream Bool dff = Cons False
I have long maintained that while the `time` library is good, the API and documentation are really lacking, and a good utility layer on top would work wonders. Every time I have to deal with the library I can do what I want, but not without significant frustration and anger. A layer for doing pure date manipulation in a 'logical' way would also be fantastic.
&gt; I also don't think om is a good name for this function. I [don't think om is particularly a good name either,](http://www.reddit.com/r/haskell/comments/1i2zmq/a_useful_function_om/) nor do I think that negates from its usefulness. Oddly, `bind` is the first thing I called it, so we are at least thinking on the same page about its meaning. `bind` is quite overloaded though, I'm not sure we could steal it? &gt; would be useful. (c.f. liftM, liftM2, liftM3 etc.) I can see the logic. Is there utility for such combinators? I think `om` is also like `ap` (a name of equally dubious quality) in terms of its chainability: om :: Monad m =&gt; (a -&gt; b -&gt; m c) -&gt; m a -&gt; b -&gt; m c ap :: Monad m =&gt; m (a -&gt; b) -&gt; m a -&gt; m b If you have, e.g. whenBoth p p' m | p &amp;&amp; p' = m | otherwise = return () then you can write similar to `ap` whenBoth `om` pure True `om` pure True $ print "Hello!" or as an operator like `&lt;*&gt;`, you could write whenBoth &lt;&lt;- pure True &lt;&lt;- pure True $ print "yo" It's also a bit like `fmap`, but it brings the `m` inside the first arg.
We still like you! I also thought that discussion was unwarranted.
Why not `(&lt;!&gt;)` or `(&lt;:&gt;)` or something similar. Another approach is to use `join` + Applicative style: join $ whenBoth &lt;$&gt; pure True &lt;*&gt; pure True &lt;*&gt; print "Hello!"
&gt; I'm not sure we could steal it? `monadicBind2` is a bit long... coming up with succinct names is difficult. `mbind2` maybe? &gt; Is there utility for such combinators? I shall watch myself code for a few weeks and note when I need them. ---- I do not think `om` is like `ap` in terms of chainability. Prelude&gt; let returnIO = return :: a -&gt; IO a Prelude&gt; whenBoth `om` returnIO True `om` returnIO True $ print "Hello!" &lt;interactive&gt;:34:1: Couldn't match type `m0 () -&gt; m0 ()' with `IO b1' Expected type: Bool -&gt; Bool -&gt; IO b1 Actual type: Bool -&gt; Bool -&gt; m0 () -&gt; m0 () In the first argument of `om', namely `whenBoth' In the first argument of `om', namely `whenBoth `om` returnIO True' In the expression: whenBoth `om` returnIO True `om` returnIO True &lt;interactive&gt;:34:1: Couldn't match type `IO b1' with `IO () -&gt; IO b0' Expected type: Bool -&gt; IO () -&gt; IO b0 Actual type: Bool -&gt; IO b1 In the first argument of `om', namely `whenBoth `om` returnIO True' In the expression: whenBoth `om` returnIO True `om` returnIO True In the expression: whenBoth `om` returnIO True `om` returnIO True $ print "Hello!" Consider om :: Monad m =&gt; (a1 -&gt; a -&gt; m b) -&gt; m a1 -&gt; a -&gt; m b whenBoth :: Monad m' =&gt; Bool -&gt; Bool -&gt; m' () -&gt; m' () Then in `om whenBoth`, `m b ~ m' () -&gt; m' ()`. 
I am not sure I agree. At least a subset of commercial haskell users maintain their own time libraries including us and standard charter (i believe). I found the time library to lacking and very painful to use. Obviously others have found the same. It may just be a mismatch between the intended use of the library and the complexities of the applications being built. In either case this indicates a gap in platform capabilities.
You could parse your `-n` option and then `exec` yourself, this time with the RTS options set instead of `-n`. 
I think we actually do agree. I just think that the current `time` library could be a decent substrate for a more usable API. Since I haven't tried to write a "nice" wrapper layer over time, I could be wrong... Its also certainly true that when dealing with financial stuff, dealing with time becomes particular and complicated in a myriad of different ways (one way for day count convention stuff with fixed income, and a different way when dealing with tiny intervals for rapid equity trading).
Which good habits?
It happens to all of us from time to time. I mean I didn't expect Yitz's proposal to add my `(&amp;) :: a -&gt; (a -&gt; b) -&gt; b` to `Data.Function` to lead to anywhere near the vociferous rejection that it received from about a third of the folks who replied, and you have to admit that is a much easier to reason about combinator than `om`. ;)
Both of those are pretty common operators in existing libraries. Giving one of them up to get an admittedly fairly obscure plumbing combinator in a somewhat more privileged library position strikes me as a poor utilization of the namespace.
When someone on the street asks you for the time, do you ask them whether they mean including leap seconds? This answer is off-topic, unhelpful, and honestly a bit condescending ("it may take a bit of research for you to understand what it is that you actually want"). I don't think you meant badly—just got carried away.
It is indeed the algorithm that matters. I decided to ramp up my Haskell knowledge (and besides, I'd just read "Why Ruby is Not a Suitable Language for Programming Competitions" and wanted to demonstrate that it is indeed the algorithm that matters) and set out to solve the Code Jam qualifier "Fair and Square" problem--the qualifier was long over, but it was the principle of the thing, and I wanted to write it as clearly as I could, I got it down to about .6 seconds on a 2.8 GHz AMD Propus, and borrowing some semilattice search tree code from from a blog post by Twan van Laarhoven and making some more improvements, it's down around 0.2 seconds now. Laziness can bite you occasionally, but makes some things a lot easier to express... and if a duffer and Haskell newbie pushing 60 like me can do that on stray nights and weekends, a programming contest hotshot ought to do seriously well in Haskell.
Do you mean Data.Map's? Surely not the "map" function? Even then, I don't think there's a consensus about pattern-matching being better, especially as it doesn't work on abstract data types.
Yes, I meant Data.Map. You are right about the pattern matching, though.
&gt; now &lt;- getCurrentTime &gt; tz &lt;- getTimeZone now &gt; let lt = utcToLocalTime tz now This can all be replaced by lt &lt;- zonedTimeToLocalTime &lt;$&gt; utcToLocalZonedTime
Personally, I think mixing monadic and applicative style looks a bit ugly.
I'm reading it on my tablet, great work
From a quick glance, it's way to minimal to be considered a replacement of gtk bindings; I have to admit that the interface building is way lighter (I assume mostly in regards with my previous point), but not what I'm looking for.
&gt; $ cat twenty.txt | splitter * Seriously?
Also, to drag the discussion immediately OT in my own thread: in reading through some of the interesting feature requests on the GHC trac, and participating various other places where language or library proposals are discussed, it seems like often ideas sort of die with the sense that more discussion is needed, or because there's apparent like of enthusiasm. I wonder if the community would benefit from some central place where ideas could be floated, discussed, and sort of voted up or down (in order to gauge interest)? I get the sense that progress is slowed and interesting ideas are sort of getting lost because lots of interesting ideas are sort of spread out across these various forums, mailing lists, etc. (the above is not to suggest the feature suggestion I made above is any good; it's just something I was thinking about)
Based on what you have so far, I went through the API docs, and put together a graph for type conversions. Is this anywhere close to correct? [Jephthai's Crazy Haskell Time Graph](http://i.imgur.com/RNlvxfB.png)
Not sure if it's correct or not, but (if it is) that's awesomely helpful and it should be included in the main documentation :)
I like this suggestion: {-# INLINE (f _ _) #-} Having {-# INLINE #-} *in the middle* of your type signature is weird.
I want a name for `return ()`. I think `pass` is a good name for it.
The only thing I have against the `_`-based suggestion is that it then becomes unclear what simply `{-# INLINE f #-}` means. It could mean either inline `f` when it's completely unapplied, or it could mean what it does now. Going with this syntax consistently would force us to break backwards compatibility.
In general, what kills an idea the deadest is a lack of implementation. There's an abundance of ideas, there's a lack of resources for implementation. Unless you really think there's a large supply of Haskell hackers who sit down in front of their editor and just _don't know what to do_ with their time, it probably isn't lack of discussion preventing things from getting done. (Which of course is not to say that a bad idea or bad implementation will be accepted, I'm just saying that the primary reason things don't happen is because nobody did them.)
This needs a concrete example. Pick out two functions from the GTK API that have similar names and similar behaviors and I can suggest some specific ways to unify them into a single function. Also, you can always modify infix functions to handle more arguments by joining multiple arguments into a tuple: a `f` (b, c)
Why not just a `f` b $ c
Our commercial Haskell shop uses only the time library, and I wouldn't consider using anything else. It sounds like you are in the financial space, which we are not, so perhaps your use cases are different. But on the other hand, naively considered by someone outside that space, it seems even less likely to make sense not to use the time library for finance. Chances are, either you have completely re-invented the wheel of the time library, or your library is wrong. Either way, that could be very costly in finance. One reason I do see that it might make sense to re-invent the time library is for speed optimizations. For us the speed of the time library is far more than sufficient. But for some financial applications, even tiny optimizations can make a difference. For example, as the author of the [timezone-series](http://hackage.haskell.org/package/timezone-series) add-on to the time library, I noticed that a user in the finance space forked my library on github just to add a tiny speed optimization to one of the functions. EDIT: See the [thyme](http://hackage.haskell.org/package/timezone-series) library.
Take a look at the [time-lens](http://hackage.haskell.org/packages/archive/time-lens/0.3/doc/html/Data-Time-Lens.html) package — it may simplify some time-related tasks for you.
I disagree. I think applicative style often looks simplest when it can be used, even within a larger calculation which must be monadic here and there.
I think you meant lt &lt;- zonedTimeToLocalTime &lt;$&gt; utcToLocalZonedTime now 
I agree about the documentation. Actually, the existing documentation - type signatures, type class instance lists, and spartan comments - do technically constitute enough information to make the library completely usable. But that is no longer good enough in the context of today's Haskell user base. I'm not sure what you have in mind for the "utility layer" though. There have been attempts at that in the past. In every case, the result was either something much less generally useful than the author envisioned, or something that is just broken. Frankly, I'm fine with the API in the time library, except for the minor issue that the function names are a little wordy for my taste. But I would be very interested to see your suggestions.
Yes, it's the Paul Hudak who was part of the committee that designed the Haskell language, and who played the first Haskell-April-1st joke by announcing his "resignment" from the committee. 
&gt; especially as it doesn't work on abstract data types. -XViewPatterns could help there. But sometimes defining a "view function" for your abstract datatype doesn't make much sense.
What about PPAs providing newer GHC packages?
That *is* a ~~method~~ **function** with three arguments.
This is an obvious attempt at copying OO syntax, so I think everyone understands what I mean by "method with 3 arguments". But okay, if you prefer it that way: what about functions with 4 arguments?
&gt; Pick out two functions from the GTK API that have similar names and similar behaviors Well the thing is they don't have a similar behaviour. Take the functions the [Toolbar](http://hackage.haskell.org/packages/archive/gtk/0.12.4/doc/html/Graphics-UI-Gtk-MenuComboToolbar-Toolbar.html) module. The only end goal is to have code that doesn't read like `toolbar... toolbar... toolbar...`. In other words that would be like if there were support for something like the following in Haskell: import Graphics.UI.Gtk.MenuComboToolbar.Toolbar nameReplace 'toolbar' '' By your response, I'm rather thinking that my passing knowledge of lenses misguided me, and it may be something not possible (mind you not the import, but the overall goal).
Indeed. You can even make it look a bit more consistent: a `f` b `id` c `id` d But I still think it's awfully complicated just to have the function in second position and pretend we're doing OO.
John Wiegley brought to my attention `skip`, defined in [Just do it: simple monadic equational reasoning](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/mr.pdf). To me, both `skip` and `pass` sound a bit too much like implying “yielding”, but for lack of a better name, I'm using `skip` at the moment. 
Cool, thanks for the info! That's pretty much exactly what I was hoping would exist.
Another amusing alternative is `don't`, as in: if predicate then do ... else don't The problem with `don't` is that it drives some editors to drink.
No need to build from source, GHC HQ publishes binary tarballs. 
then the editors have to be fixed :D. i like `don't`. (and `don't` is in the `acme-dont` package. just import `Acme.Dont`.)
That is what I would expect, but the paper says &gt; So `limit` is kept as a thunk that is visited in every iteration of the inner loop (`search'`). Simply putting `seq limit` in front of `R.computeP` to force evaluating it solves the problem. I don't understand how either of these sentences can be true. Maybe I should try to reproduce these results. The full code written for this paper isn't available, is it?
There's a few constraints that are enforced by `error` - for example, calling `lineToList` on a `Music` value that wasn't produced by the function `line`. Wouldn't the author be better served by using phantom types and GADTs to ensure this statically?
library proposals: the GSoC trac is one place to look http://ghc.haskell.org/trac/summer-of-code/report/1 http://ghc.haskell.org/trac/summer-of-code/wiki/Soc2013 -------------- /r/haskell_proposals has... become asymptotic 
I don't think it's quite right to say that state is "pure" because it is transformed to a series of function applications. By the same token, ordinary haskell code would be "impure" because it is transformed to a series of assembly instructions. Those instructions, in turn, would become "pure" again if they were emulated in a state monad. If "pure" is going to have any meaning - and I'm not sure what the precise meaning is - then it has to refer to the forms that the programmer actually writes. "push 8; pop" obviously depends on evaluation order so I would say that code is not "pure".
You need to have a precise definition of what you mean by "pure" if you're going to say if something has that property or not. I think both views have merit. 
So why is every other programming language able to handle the problem then?
If you think of purity in terms of "substitution works on it", then it is pure.
Example niceness: case foo of Cons2 x y -&gt; Cons2 &lt;$&gt; f x &lt;*&gt; f y Cons1 x -&gt; Cons1 &lt;$&gt; f x Cons0 -&gt; Cons0 &lt;$ pass 
What makes you think they have handled it any better? For example, the PHP API looks pretty darn complex: http://php.net/manual/en/book.datetime.php And has less features than the Haskell time library..
&gt; GHC 7.6 introduced `setNumCapabilities` `setNumCapabilities` was introduced in GHC 7.4, although it was only possible to increase the number of capabilities ([7.4.1 changelog](http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/release-7-4-1.html)). All the 7.6 update added to it was the ability to decrease it.
I think of purity in terms of "same input, same output." If you consider slide :: Int -&gt; State Bool Int slide i = get &gt;&gt;= \case True -&gt; return (i + 1) False -&gt; return (i - 1) This is not "pure" in the sense that you do not know whether putting in `i` will result in `i + 1` or `i - 1`. You need to additionally know *what context* it will run under. So I guess that personally, when I use the term "pure", I mean "you don't need to know about any additional context." The way to get around this is to just say that the context is one of the inputs, and the modified context is one of the outputs. This explanation works for State, but might not make sense for other monads. ---- tl;dr, my modified title to the post: State is pure (when you reify context to be explicitly part of the input and output)
No, it's different `don't` 
Right, but that's just it. The "state monad" allows you to write imperative code that *implicitly* passes state around, hence the "context" stuff, so while it desugars to "pure" code, the high-level State monad code that the programmer deals with is "impure".
actually I meant lt &lt;- zonedTimeToLocalTime &lt;$&gt; getZonedTime
Indeed, but then IO is pure. But, I would say that's accurate: Haskell is a purely functional programming language. The only impure thing is seq, i guess.
If it meets your needs thats great. The existing one definitely didn't meet our needs. In fact early on I questioned whether we should even be using haskell if it didn't have a jodatime grade library for complex time manipulations. We leveraged what we could and built the rest. We do use your timezone-series package as well as the olson one under the hood. The most complex use cases aren't in our trading systems themselves but in other ones that collect and aggregate data. Deciding to build our own library was not a decision that was taken lightly. We leveraged as much external code as we could. With 7.6 we will eventually release a version that would be full featured.
[named-formlet](http://hackage.haskell.org/packages/archive/named-formlet/0.2/doc/html/Text-Formlet.html) is a trivial formlet library that I basically wrote in a rush for hpaste a while back so that I could give names to the inputs. It has a satisfyingly easy to understand API, though the docs could use a little cleaning up. [Example usage](http://lpaste.net/91281).
That depends a lot on the application itself and the platform you're distributing for. For example, is it a commandline program or a gui application? These two are typically very different in how they structure things. For instance in OSX: commandline applications install things into `//bin`, `//man`, `//lib`, `//etc`, and so on (where the `//` denotes any root tree, not just `/`). Whereas, applications are destributed as [.app bundles](https://developer.apple.com/library/mac/#documentation/CoreFoundation/Conceptual/CFBundles/BundleTypes/BundleTypes.html#//apple_ref/doc/uid/10000123i-CH101-SW13) which have a specified layout and contain all the icons, translation files, etc. Windows and each distribution of Linux has their own ways of structuring these things. The one thing I would *not* do is just dump everything into the user's home directory. The only thing that should go into ~ are the user-specific dotfiles for saving configurations. Everything else should be installed in a root tree (whether spread all over it like commandline tools do on *nix, or in some special place like `/Applications` for the bundles).
Except for two things. One: the "impurity" is captured explicitly in the type. We know, for example, that even though some `(State s a)` computation may depend on a local state `s`, it may not depend on any other state of the system. Thus, once the local state is fixed, the meaning of the stateful computation is independent of its context. Two: when Haskellers talk about "purity", what we usually mean is (something like) referential transparency— we do not (in general) mean the absence of "effects". That is, in imperative languages, things are impure in virtue of the fact that their types do not express their effects and therefore the "same" expression in different lexical contexts can mean different things. However, by explicitly capturing these effects in the type system we can avoid the problem of breaking RT. Three: monads aren't magic. All they are is syntactic sugar for a ubiquitous pattern of computing. In particular, there is *nothing* you can do with a monad that you couldn't have already done without the monad— and that includes `IO`! Because monads are just sugar, it doesn't make sense to say that the sugared version is "impure" whereas the unsugared version is "pure". They are one and the same: the unsugared version is the implementation of the sugared version! Where does this "impurity" come from if the source code is pure? That's like saying that `id` is impure whereas `(\x -&gt; x)` is pure, when we've defined `id = (\x -&gt; x)`.
To really understand what's happening, try desugaring the do notation and the replacing the monadic operators with their implementations.
What you are saying is obvious (and platform dependent, obviously), however I was more questioning it in terms of already something available that deals with all that (it would be a chore to me to invest work in all the minutiae details of this process). That came a second nature question when cabal provides the option to package applications, but I wasn't able to find anything in terms of resource management there. I guess ad hoc, DIY, externally inspired is the only "available" variant.
It's my understanding that that doesn't work with parallelism. But I'm uncertain. It was an old comonad post, so I can't search. I guess it's probably how Ermine does IO.
Maybe it's better to get away from the discussion of this word, "pure", and just ask what is useful about the various properties we might be discussing. Functional code - code which is just applications of functions to other functions or literal values - allows us to reason about it in pieces. For a complex expression we can always look at each piece of it in isolation - the function and each of its arguments - without losing any essential context. If my Haskell code is mostly state transformations in a monad, then I can't perform this kind of reasoning any longer. I have to fall back on imperative reasoning techniques - keeping track of program state through a thread of execution, analyzing preconditions, and so on. The fact that the stateful code is "really just" functional code doesn't help me reason about it any better. On the other hand, it does mean that I can transform my state-related reasoning into an equally intractable form of functional reasoning. So it really brings into question the supposed equivalence between functional form and easy decomposition or reasoning. Being able to reason independently about a function and its arguments doesn't help so much when the argument can be a very complex object which is the result of a lengthy series of subcomputations, i.e. when your functional decomposition looks like `&gt;&gt;= &gt;&gt;= &gt;&gt;= &gt;&gt;= &gt;&gt;= f g h i j k`. Personally I think that the whole idea of functional "purity" has been oversold in a highly confusing manner. Real gains in program comprehensibility come from decomposition of a problem into less complicated sub-problems, whether that is done by function application or sequential state manipulation or context-free grammar or some other compositional technique. Being able to separate stateful from stateless code is an advantage, but IMO that advantage comes from explicitness and finer-grained types, not from some sort of purification of the stateful code into a better stateless form.
To tackle only part of your question, namely &gt; Isn't the last statement in a do block always the last value to be "returned"? consider the Maybe monad. The code: x = do a &lt;- Just 2 b &lt;- Nothing c &lt;- Just 4 error "will never reach here" which desugars to x = Just 2 &gt;&gt;= \a -&gt; Nothing &gt;&gt;= \b -&gt; Just 4 &gt;&gt;= \c -&gt; error "will never reach here" will return `Nothing` instead of crashing - the last statement is never reached. The explanation for this behaviour, just as for `callCC`, lies in the monad instance: (Just x) &gt;&gt;= k = k x Nothing &gt;&gt;= _ = Nothing See how everything after `(Nothing &gt;&gt;=)` is ignored? 
The existing documentation such as it is is also spread out among a whole ton of modules, which doesn't help. I'm not sure what the utility layer would be, honestly. The general rule of thumb for time I guess is "stay in utc, please, please" and only exit for display purposes. If someone does that, things are generally ok. But even then, basic date things like "roll forward n days" "roll forward n weeks" "roll forward n months" "find the difference in months" "find the difference in weeks" etc. is somewhat more complicated than it needs to be... Holiday-aware versions of those functions (parameterized by holiday lists) would also be awesome. I'm sure there's more I can think of above and beyond, but lucky me I haven't wrestled with the datetime libs lately.
What are the primary features that your time library exhibits over `time`? It has always served me well, so I'm curious what complex time manipulations might look like.
Here, The conditional is 'when'. You can think &gt;&gt;= of continuation monad as a 'hole' passing operation. Without callcc, which explictly name the hole as 'k' in this case, the hole automatically passed from one statement to the next statement. At 'when (n' &gt; 20) $ k "over twenty" ' , the hole 'k' explicitly appear and therefore the assumption on the control flow "the hole will be placed the last and therefore executed last" is explicitly broken. That's how the branching is realized. 
Cabal helps with the data files part of this. You list `data-files` in the `.cabal` file and it installs them in the `$datdir`. Then to find these files at runtime, see the [Cabal user section on the topic](http://www.haskell.org/cabal/users-guide/developing-packages.html#accessing-data-files-from-package-code).
Consider the type of callCC. I'll use the "MonadCont" flavor for demonstration here. callCC :: MonadCont m =&gt; ((a -&gt; m b) -&gt; m a) -&gt; m a What a doozy. Okay, consider your *usage* of callCC. I'm going to break your code out into a few different exprs, and explicitly pass each bound variable around, so we have a slightly clearer view of what is going on. foo = \n0 -&gt; expr0 n0 where expr0 n = callCC (expr1 n) expr1 n = \k -&gt; expr2 k n expr2 k n = do let n' = n ^ 2 + 3 when (n' &gt; 20) $ expr3 k n return (show $ n' - 4) expr3 k _n = k "over twenty" First, are you convinced that this is a valid, though perhaps strange, refactoring? Now, let's annotate each expression with its type. In this example, I've chosen `a = String`, and `b = ()`. {-# LANGUAGE ScopedTypeVariables #-} import Control.Monad.Cont (MonadCont, callCC, Cont, runCont) import Control.Monad (when) main :: IO () main = do putStrLn (unCont (foo 0)) putStrLn (unCont (foo 30)) unCont :: Cont r r -&gt; r unCont = flip runCont id foo :: forall m. MonadCont m =&gt; Int -&gt; m String foo = \n0 -&gt; expr0 n0 where expr0 :: Int -- n -&gt; m String -- (expr0 n) expr0 n = callCC (expr1 n) expr1 :: Int -- n -&gt; (String -&gt; m ()) -&gt; m String -- (expr1 n) expr1 n = \k -&gt; expr2 k n expr2 :: (String -&gt; m ()) -- k -&gt; Int -- n -&gt; m String -- (expr2 k n) expr2 k n = do let n' = n ^ 2 + 3 when (n' &gt; 20) $ expr3 k n return (show $ n' - 4) expr3 :: (String -&gt; m ()) -- k -&gt; Int -- n (not used) -&gt; m () -- (expr3 k n) expr3 k _n = k "over twenty" The expression to pay attention to is `(expr1 n)`, because this is the argument to `callCC`. Therefore, it must have the same shape as `(a -&gt; m b) -&gt; m a`, and indeed it does! expr1 binds the "escape hatch" to the variable `k`. But there's a catch. Look again at that type signature. (a -&gt; m b) -&gt; m a You basically have to whip up an `a` no matter what. If you want to use the escape hatch, then you have to give it an `a`. If you want to ignore the escape hatch, then you have to end up with an `m a`. That's what the continuation monad *means*. The continuation is expecting you to spit out an `a` for it to use. The constraint `MoandCont m` is a promise that `m` has some way to provide such "escape hatches". Any monad can be upgraded to an escape-hatch enabled version of itself by placing `ContT` on top of it. You can see how ContT manually passes around the continuations by looking at the source for ContT.callCC: http://hackage.haskell.org/packages/archive/transformers/0.3.0.0/doc/html/src/Control-Monad-Trans-Cont.html#callCC
Thanks, this is exactly what I was looking for.
The conditional part is the fact that `k` isn't chosen until the very last moment, so with an appropriate choice of `k` you can do really weird things.
Ok, I see what you mean in your distinction between the code and the type. If you are looking at the function at its call site rather than its definition you must infer its type from its context. I now realize that it seems obvious but you've actually clarified this for me, so thank you. I did not mean to imply that -&gt; and (,) are less special than the type that I gave as an example. The difference that I was trying to illustrate is that the signatures (a -&gt; s -&gt; (b, s)) and (a -&gt; State s b) convey the same amount of information, whereas a signature like (a -&gt; App b) is intentionally hiding the state that it operates on. This is what I usually imagine when I hear the term "implicit state", but your explanation makes sense too.
It wouldn't suprise me if concurrency were one of the caveats.
Yes. See [`pure-st`](https://github.com/pthariensflame/pure-st) for a simplistic implementation that I made of a completely pure `STT` (and `ST`). I know there are others out there, too.
this is really cool, but it seems that docker now has an enormous amount of momentum and that this will overlap with docker &amp; flynn and its unclear why to choose this. Congrats on doing deployment right though.
&gt; Warning! This monad transformer should not be used with monads that can contain multiple answers, like the list monad. This is the reason. What happens is that if the underlying monad branches (like the list monad does) then each branch's state contaminates the other branches because the underlying mutation is done impurely. In contrast, if you use `StateT` using a list base monad it will behave correctly and isolate each branch's state from each other because it keeps track of state in a referentially transparent way. I'll use a simple example to illustrate this: import Control.Monad.Trans.Class import Control.Monad.Trans.State example :: StateT Int [] () example = do i &lt;- lift [1, 2] modify (+ i) main = print $ execStateT example 0 -- prints [1, 2] Had I used the `ST` monad transformer to keep track of state, the modification from the first branch (i.e. the `(+1`)`) would contaminate the second branch, which would then return 3 instead of 2. Edit: To answer the title of the post: yes you can make a correct `ST` monad transformer by simulating it using the `StateT` monad transformer, like `PthariensFlame` mentioned.
Cool! Looks like it's a StateT monad where the state is a source of ID numbers and a map from ID numbers to Any. Is that about right? Why the "zipWith (\a b -&gt; [a, b]) [0, 1 .. maxBound] [(-1), (-2) .. minBound]" as the source of names/IDs? (Assuming I have that right)
Neat but is it possible to implement a safe ST monad transformer that still has acceptable performance?
You can make a safe ST-like monad transformer, but it has to carry everything around in some kind of dictionary. You can carry that dictionary around in the state, but then it cannot be garbage collected at all. It is very tricky to get that to interact well with references and collecting the environment. You can do that, but then the code becomes an `unsafePerformIO`-riddled hellscape very rapidly, as you need to let the references themselves hold onto the data and mutate a tree of versions attached to the reference itself. You wind up with some `unsafe` operation cluttering either version though, as otherwise you have to hold onto a `Map` where the only thing enforcing the correctness of the typing discipline is that you only give out keys with types that match up with the value they reference, so there you need to `unsafeCoerce` around entries or use Typeable and partiality in uncomfortable ways. I use the simple Map-passing version of this in some unreleased revision control monad code on my github account. There I needed to support some additional versioning operations that required an efficient form of [online LCA](http://en.wikipedia.org/wiki/Lowest_common_ancestor).
I don't understand what you're talking about. What conditionals? newtype Maybe a = Maybe { runMaybe :: forall b . b -&gt; (a -&gt; b) -&gt; b } nothing = Maybe $ \n j -&gt; n just a = Maybe $ \n j -&gt; j a instance Monad Maybe where return = just ma &gt;&gt;= fmb = runMaybe ma nothing (\a -&gt; runMaybe fmb a) 
It isn't clear to me what about sharing mutation and returning 3 instead of 2 is inherently *bad*. Does this disobey monad/monadtrans laws? What if I *want* state from the first branch to contaminate the second?
They are equal. Both return the same IO value. Now, go and run that on some non-abstract machine and that's a different story. But purity isn't about what happens on particular machines. :)
Every time I run `slide` on a number, I get back the same thing -- a `State Bool Int`. You must be running it wrong if you get something different!
&gt; If you wanted to ascribe a formal semantics to getCurrentTime, you'd probably just say it nondeterministically returns an integer. Since I'm getting paid to study formal verification of cyber physical systems (and we are using Haskell, although not for this) I would have to say that no, I would not. Just because a function touches the real world doesn't mean its semantics has to be "I really don't know what this does..."
I use "run" not to mean "function application", but "usage in context". Consider, for example, ... inside a State Bool "do" block ... put True x &lt;- slide 3 put False y &lt;- slide 3 `&lt;-` is *sort of* what I mean by "run". The expression `slide 3` appears on the right hand side of the bindings for both `x` and `y`. Yet `x` will be bound to `4`, while `y` will be bound to `2`, *not* the same results! We know not to expect them to be the same, though, because the binding for `x` occurs in a different *context* than the binding for `y`, even though the binding statements are *syntactically* identical.
I really like your comment, because it touches on a lot of important ideas that I'd like to address. Overall, though I think that given the nature of responses I've gotten, that my ideas about what "purity" means are out of sync with the meaning typically used in Haskell community discourse. &gt; One: the "impurity" is captured explicitly in the type. And this is what I love about Haskell! When I want to write an imperative program using "impurities", it gets annotated in the type. &gt; That is, in imperative languages, things are impure in virtue of the fact that their types do not express their effects My notion of purity has nothing to do with types, although I find it to be a Good Thing when impurity is kept under control by leveraging the type system to annotate it and force the programmer to compose impure code correctly. &gt; we do not (in general) mean the absence of "effects". Well then, my mistake for communicating poorly with my audience, because that's exactly what I mean. There seems to be a notion that being "impure" is *bad*, but imo impure programming in a particular principled way (basically, the Haskell way) is not bad at all. Again, I think the problem I have with trying to communicate this idea is that I am using words to mean something different than what others are interpreting them to mean. &gt; it doesn't make sense to say that the sugared version is "impure" whereas the unsugared version is "pure" I think it does make sense to say exactly that. Just because I could "desugar" a C program into an explicit state-passing language, doesn't make the C code "pure". Or if I could annotate the types of C code to say "hey, this function uses this global variable", that would not make the C code "pure". But apparently, these statements are wrong, according to the definition of purity being tied to the type system.
it's a different `don't`. you are right. i missed the argument. don't _ = return ()
This looks effing cool! I wonder how we can express several distinct effects of the same type in it, e.g., *ReaderT Int (Reader Int) a* from MTL.
Is the approach similar to that of http://eb.host.cs.st-andrews.ac.uk/drafts/effects.pdfn or is it another alternative to monad transformers? Also I'm pretty sure I've seen this title before. Am I wrong or is it a new version of an old paper (or simply an old paper)?
Just glancing at the references, there are items from 2013, so I think we can rule out "simply an old paper". Not sure about the other possibilities. :)
I'd love to read these sorts of things on my e-reader, but it handles 2-column pdfs so poorly I can't manage it. Are other formats available?
You've probably seen this title, also by Oleg: [Extensible interpreters -- an alternative to monad transformers](http://okmij.org/ftp/Computation/monads.html#ExtensibleDS).
I use [cut2col](http://www.cp.eng.chula.ac.th/~somchai/cut2col/) to make 2-column pdfs suitable for my Kindle.
Indeed :)
Oh, thank you, I hadn't realized that! My confusion came from the fact that `setNumCapabilities` is exported by `Control.Concurrent` in GHC 7.6 but not in GHC 7.4, so I incorrectly concluded that the function wasn't present at all in GHC 7.4. It turns out, though, that it *is* present, but it is only exported by `GHC.Conc`. Good to know!
You can't. You'll have to indicate in some way which effect you want to target. In the case of my own `effects` package you can do that by passing around an effect identifier, but that doesn't help you in the case of readers, as it is just as much trouble as passing around the data in the first place. The obvious solution is to use `Reader (Int, Int)`. It would be nice to have a mix of their solution and my solution, so that you can use effects identifiers only when required.
I'd be really interested in learning about that too!
FWIW (don't know what e-reader you have but) mine zooms in by exactly 50%, so I just read it in quadrants. Same thing as cut2col, basically, just a bit more fingerwork. Works well enough that I haven't had the desire to try the latter.
Looking at the references is the same method I use, and I guess most people. Which begs the question: how come they don't usually put the date of publication (or submission, or whatever) up top? Edit: In this paper they *do* list it on the first page, small letters, bottom left.
Doesn't digestive functors already do that? A form contains a view, which consists of all the form elements and any errors. You can use that view to generate html, like what digestive-functors-heist does using splices.
But in that model I have to define the parser and the HTMl elements seperately. Which means defining much of the form twice.
Yes, the behavior you're describing is `ListT` done right on top of an ST monad. Then each branch shares effects.
I'm confused. You use runForm or whatever to generate a view right? So you just need to write a function "View -&gt; Html", you don't need to write the html itself. Are you expecting someone else to have written that function? I'd imagine there isn't one because it is impossible to make a correct, generic html representation of a form. Everyone will want the html to be different.
&gt; So why is every other programming language able to handle the problem then? Other languages don't handle they problem. They simply [crash](http://www.theregister.co.uk/2012/07/02/leap_second_crashes_airlines/) in the presence of leap seconds.
I think the feeling is that dating your work admits that it may not be timeless.
This isn't easy, since the efficiency of ST comes from actual effects on memory. To be able to undo these you'd have to do something similar to what STM does; i.e., some kind of copy-on-write variables. It can be made reasonably efficient, but ghc's RTS doesn't have that. 
Consider, the following `Monoid` instance is possible: instance (Monoid a, Monoid b) =&gt; Monoid (a,b) where mempty = (mempty, mempty) (a, b) &lt;&gt; (a', b') = (a &lt;&gt; a', b &lt;&gt; b') So then you can replace "write a" with "write (a, mempty)" and "write b" with "write (mempty, b)". There has to be some explicit indication of which bin you want to write to, but you can basically use a type-level list of length N to specify N writer bins. ---- Whoops, after the fact I realized you were talking about Reader, not Writer, but the same grouping applies. Converting `ReaderT Int (ReaderT Int m) r` to `ReaderT (Int, Int) m r` is just a form of "uncurrying". `get` for the first Int is replaced by `gets fst`, and `get` for the second Int is replaced by `gets snd`. The getter function also must specify its "target bin". I have the vague idea that lenses are the correct general solution for "zooming" in on the desired target bins for effects like this.
There are not recipes (every group and children are very different), but in general terms I found some that work, like teach them how to construct "their own math" (implement basic arithmetic, boolean operations, etc), ask for function signatures, discuss about representation invariants, think real world scenarios and model them with data types and functions, etc. 
There are a lot of programs and reports around, many in US. See [ACM site](http://www.acm.org/education/future-of-computing-education-summit).
&gt; `slide 3` ... always means the same thing: nothing more and nothing less than \s -&gt; case s of True -&gt; return (3 + 1) False -&gt; return (3 - 1) That is basically what I just said. &gt; abstracted over the general notion of the state of b Except I am proposing the abstract idea that the "JavaScript code" *also* means the same thing. *How it is desugared, compiled, and run* is irrelevant to its meaning. (As long as these steps preserve meaning, that is. If they don't, then you've got a bad desugarer / compiler / runtime.)
I basically want to create a library that allows me to use one set of combinators to produce `(Maybe a -&gt; Form a, View -&gt; Html)`. So that the creation of the HTML form is combined with the validating/parsing. `reform` already works somewhat similar to this, it seems, but has some disadvantages (such as no control over field names).
In the end you were right, desugaring do notation, expanding the implementation of &gt;&gt;=, and doing some algebraic-like reductions led to figuring out how a Cont computation can ignore it's continuation, and essentially ignore any further &gt;&gt;= statements. As it appeared it was not callCC that lets you ignore computations, but rather the bind operator.
Your non-monadic explanation really helped to understand the ignoring computation part! Having that in mind, and desugaring &gt;&gt;= led me to see the same operations in monadic code.
&gt; effing cool! newtype E ff a = E ff { runEff :: forall w. (a -&gt; VE w) -&gt; VE w } Not sure if coincidence or bad pun. Either way, made me smile. :)
I have a Kindle Touch. The controls for zooming in are imprecise. Then, once I've zoomed in properly, the Kindle has an exasperating tendency to flip to the next page when I am trying to scroll around. It loses its zoom settings when it does that, so I have to navigate back to the proper page, re-zoom by trial-and-error, then scroll around to the right spot again -- risking repeating the whole process. Completely unusable. By comparison, cut2col is a freaking miracle. :)
I cant find an example, but Wadler just uses `done`, which might suit you.
Ah here it is, slide 10 and then throught http://www.inf.ed.ac.uk/teaching/courses/inf1/fp/lectures/2012/lect15.pdf
I also considered `done`, but it would be arrogant for me to propose it. =P
I haven't read the whole paper yet, but all the advantages over mtl from section 2 are consequences of the removed functional dependency in classes like MonadReader. They have nothing to do with "effectfullness".
You have all of that right. Basically, I just wanted to use the whole of `Int`'s range as available, starting at 0 and alternating between negative and positive. If I were to rewrite it now, I'd just use `Word` and `[minBound .. maxBound]` instead.
I just finished this. I highly recommend this as an alternative/addition to LYAH because it has exercises and the lecture notes are clear and concise. Also, does anyone have any recommendations for a next step?
This looks fantastic. What do the actual efficiency numbers look like? They say it's sometimes a win and imply it's usually (always?) a win, but what ballpark are we talking here?
See [my comment](http://www.reddit.com/r/haskell/comments/1j912o/is_it_possible_to_make_a_safe_st_monad_transformer/cbco1ie), though all I have implemented is an ST transformer, not an STM transformer.
It looks like their approach distinguishes between polymorphic effects that have been instantiated with different types. I.E. you can have both Reader Int and Reader String. So a simple way to 'tag' multiple effects of the same type would be to newtype one or both of them.
***It's not desugared to mean this, it always meant this and it has nothing to do with sugar.*** Not in Haskell. The denotation is the same denotation, period, end of story. It's referentially transparent, 100% pure, and that's all there is to it.
Riak itself is quite good at caching. Besides Riak I'm caching most Haskell data structures (especially posts lists and read states) right inside the front end executable. So most of the data is already available in deserialized Haskell data structures after first request from user and only small portion of data is read from Riak on subsequent requests.
I know this post is a bit tongue in cheek, but it should be obvious that this `Maybe` is isomorphic to `Prelude.Maybe`: -- rename Maybe above to CMaybe, short for 'Church encoded Maybe' toChurch :: Maybe a -&gt; CMaybe a toChurch Nothing = nothing toChurch (Just x) = just x fromChurch :: CMaybe a -&gt; Maybe a fromChurch c = runMaybe c Nothing Just Exercise: Prove the monad instances in jkff and otherwun's comments are equivalent given this isomorphism. You may want to use the `maybe` function from `Data.Maybe`.
btw, in the section you linked there seems to be a broken link (which I guess would point out, that you can override `datadir` at runtime via the environment variable `&lt;pkgname&gt;_datadir`): &gt; ... (see [prefix independence](http://www.haskell.org/cabal/users-guide/developing-packages.html#prefix-independence)).
nice graph; might be useful to mark the edges that are `IO`-monadic
Didn't Oleg have a snippet demonstrating that Typeable makes Haskell98 unsound? Does this apply here? Isn't using Typeable everywhere a bad idea for this reason?
Isn't this because he made his own perverse Typeable instance, rather than deriving? (It was a long time ago.) Compare the results of his http://okmij.org/ftp/Haskell/types.html#unsound-typeable and of e.g. this [reputable paste](http://lpaste.net/91319) -- the first yields a segmentation fault, the second a well-deserved `fromJust` exception. In the [Eff.hs module](http://okmij.org/ftp/Haskell/extensible/) that goes with this paper, he does end up writing his own `typeOf1` in one complicated case. 
New GHCs (HEAD, at least) prohibit the user from defining `Typeable` instances, which leaves safety up to correctness of the compiler's generated instances. (Separately, but happening at the same time, is `PolyKind` allowing the elimination of `Typeable1` and friends; combining the two gives us a new `AutoDeriveTypeable` `LANGUAGE` directive which makes Haskell much more like "those other" languages.)
That is fixed with 7.7. You can't write your own instances any more. Sadly this breaks some code of mine.
An excellent, and dense, comment. Mind if I ask a few questions? &gt; If you intend to walk the result several times, By "walk the result" do you mean "execute the monadic action" or something else? &gt; Working with a free monad that has `Get` and `Put` constructors means you have to worry about the interpreter "cheating' and giving back weird intermediate answers that don't obey the laws. That can't happen to you if you work with `s -&gt; (a, s)`. Could you give an example? &gt; Third, things like region parameters don't work with the `Typeable` trick, and not every environment, monoidal log, or state is or can be `Typeable`. Doesn't `AutoDeriveTypeable` (or really the `PolyKind` stuff behind it) resolve the second half of this? It'd be... kind of interesting to have a `Symbol` kind for reifying phantom parameters and making them `Typeable`, but I don't know if that's actually useful. &gt; Using open unions requires the use of `OverlappingInstances` Is this really a requirement of the concept or merely their implementation? (It really bugs me that Haskell can't efficiently do something that every OO langauge on the planet has done effortlessly since the dawn of Smalltalk. ;) )
&gt; By "walk the result" do you mean "execute the monadic action" or something else? By walk the result I mean if you are going to translate each constructor into a function and execute it, then there is little benefit to be had, but if you are going to inspect it, treat it like a constructor go off do other things, rewrite it, and then come back and rewrite it again later, then you will wind up doing a lot of recomputation if you CPS. An analogy is working with Yoneda. newtype Yoneda f a = Yoneda (forall r. (a -&gt; r) -&gt; f r) It clearly is a Functor. instance Functor (Yoneda f) where fmap f (Yoneda m) = Yoneda $ \k -&gt; m (k . f) lower :: Yoneda f a -&gt; f a lower (Yoneda m) = m id Consider what happens when we fmap over that thing twice. It gets fused together into one 'fmap' over the underlying 'f'. But the fmap doesn't "happen to the elements of f" until we go to lower it. If you build one up with lots of fmaps, you pay for them all at once. But if you take the value from right before you lower it, and lower it, then modify the original and lower it again, you have to redo all those operations. It isn't stored somewhere for you. You just have to go redo all of the work again. &gt; Could you give an example? data S :: * -&gt; * -&gt; * where Get :: S s s Put :: s -&gt; S s () You can now work with `Free S` like it was a `State` monad, by using the natural transformation from `S s` to `State s`. (Of couse we could also work with `Free (State s)`, a much bigger type then the natural transformation from `State s` to `State s` is obvious!) But when we go to `runState` we can 'see' more of the structure involved. We can detect when we read from the state, detect when we write to it. We could have that interpreter lie and always give you the same state, etc. The semantic domain that we are mapping onto has distinctions we don't want it to have. &gt; Doesn't AutoDeriveTypeable (or really the PolyKind stuff behind it) resolve the second half of this? No the region `s` parameter in `ST s` remains non-`Typeable`. This is the same kind of problem we have in lens with GHC head. There we need to make a custom `Exception` instance (which requires `Typeable`) using `reflection`, which doesn't give me a region parameter that can be made `Typeable` without costing 3 orders of magnitude worth of performance. &gt; Is this really a requirement of the concept or merely their implementation? It is a requirement of the execution of them without a quadratic number of instances. I'm not sure if this one can be resolved with the new overlapping type family plumbing, but my gut says it probably can't. In practice I tend to use the lens-style of "HasFoo" or "AsFoo" classes with manual embeddings rather than rely on "a la Carte". Fewer hideous type inference woes lie down that road and the embedding isn't terribly onerous.
&gt; Finally, the sheer operational overhead of all those Typeable checks This
And makes it impossible to use build `Typeable` from Haskell98. I've occasionally used `derive` to generate instances when I wanted dynamic typing (let's be honest, I never really want dynamic typing, but some libraries seem to).
A couple comments: First, the API for users seems nicer than dealing with ordered monad transformer stacks and doing explicit lifting everywhere. Other issues aside, I want this API or something like it for dealing with multiple effects. 1. There's no reason that `Eff` has to be CPS'd that I can see. It could be an ordinary syntax tree. I guess you are just objecting that there is no One True Representation of `Eff` that is going to be suitable for all effects. For some effects, CPS/Codensity-ification is what you want, for others, you want the actual syntax tree, like if you'll be traversing it more than once. On the other hand, maybe a better way to think of this is as a uniform "container" for effectful computations of similar shape. Even if you can't cram all your effects into `Eff`, or you need both `Eff` and `EffCPS`, you might still profitably use `Eff` in programs where you're dealing with several effects of the same 'family'. 2. I am not really phased by the problem that the interpreter of `Get` and `Put` may do something totally random and stupid that breaks laws. Don't do that! There is going to be a huge amount of client code that uses some effect, and a much smaller interpreter for the effect which just needs to be audited to make sure it doesn't do something dumb, so I don't see this as a big problem. 3. I don't have any thoughts on how big a deal the `OverlappingInstances` thing is, or whether the overhead of the implementation is a problem.
&gt; I am not really phased by the problem that the interpreter of Get and Put may do something totally random and stupid that breaks laws. Don't do that! I've come over to this side, in general. It's the equivalent of "don't write an instance of MonadState that breaks the laws". We can push around the burden of enforcing the laws, but there's always some piece of code somewhere that will have some laws it needs to obey. There's really no free lunch here. I'm not sure if I like the construction in the paper or not yet -- haven't explored it carefully, and I know there are algebraic systems that aren't a "free monad in disguise". But just as a general rule, the objection to initial encodings because you have to enforce laws is silly to me, since we have to do that when we write instances of Monad or MonadTrans, or etc. anyway.
I think that's what he means by `O(1)`. All of his complexity measures are relative to a (probably small) fixed set of possible elements.
Because it's not mapping over every element, it's mapping over a model derived from those elements. I don't personally understand the math or how it's implemented (I have yet to get the library to compile on my machine because of cabal hell, I'm currently building the haskell platform from scratch to see if that helps), but I can accept that these are two different things entirely.
Ultimately this is the same thing as the Lawvere Theory stuff that [Dan Piponi](/u/sigfpe) posted. You're just moving all the choices about how layering works into the interpreter. One thing that is worrying to me about this is that there are some layerings of monadic effects where you can't freely lift one set of effects over another. Not all monad offer us coproducts. Yet all that can happen here is that we move this conundrum down to the interpreter site. This implies to me strongly that the handler-lifting technique isn't as strong as it is being sold as in the paper. Re 1) I agree that `Eff` doesn't have to be CPS'd, but then that _really_ exposes that this is just an a la carte free monad of effects. Re 2) I personally try to stick to a relative weak set of primitives just because there are fewer moving parts and fewer ways to screw it up. That said, there are often really good reasons to just work off free monads instead. e.g. Consider something like a probability monad, `newtype P a = P [(Double, a)]` -- and then work in `Free P` rather than `P`, because now you can explore the whole tree of probabilities rather than just deal with them in their fully flattened form. This is admittedly making an interpreter that "cheats", that you couldn't have written on the more "final" `P` monad directly. Ultimately this just provides us with free monad of a coproduct (which is equivalent to a coproduct of free monads), which we then have to take some quotient of by choosing an appropriate interpreter. We've gone through all the work of working in some needlessly "larger" domain, and then written a second interpreter to actually run it in the end, rather than just execute with the desired semantics directly. This means we necessarily have built up a syntax tree that has carefully preserved distinctions we don't want, and prevented ourselves from optimizing it away only to later run it in a less efficient manner with worse intermediate result sharing.
My primary objection to needlessly initial encodings is performance. I actually only really bothered to state this particular objection explicitly because I was thinking of your talk about layers of interpreters. =P
My usual rule of thumb is to deal with 3 cases. 1.) Non-GHC Typeable with a manual instance when the package can be compiled on other compilers and the data type doesn't take parameters. In practice I usually leave these up to deriving though and don't bother to give a Typeable instance on compilers without deriving and nobody complains. I've had a few people send patches to things to make them work with compilers like `hugs` or `nhc`, but they are few and far between. 2.) Alternately, the custom GHC Typeable1, Typeable2 .. with a manual instance when I need to make a custom Typeable that doesn't match GHC's deriving, and where one of the arguments isn't kind `*`. Using mkTyCon3, etc. appropriately on new enough GHCs technically makes this 2 cases, not just one. 3.) Disable generating the Typeable instance entirely on new enough 7.7+ GHCs. Not everyone is that anal retentive though. ;)
Consider the opposite though. What if we have lots of expressions like e.g. set, set, set, set, set, set, set, get? In the "final" encoding we have to go through all those sets, because the monad makes it too "opaque". In the algebraic encoding we can throw those away automatically as we construct our action. Depending, the win from doing this can outweigh the cost of the remaining interpretive overhead. So sometimes that granularity pays off...
Sure. It can definitely be a win in places to be more initially encoded. This was what I was trying to get to with the `Free P` example. In many ways thats what we're asking the compiler to do for us when we just invoke `get`/`put` methods through the `MonadState` constraint though. ;) It just has the benefit that if we _have_ chosen a concrete instance that we don't build an intermediate AST-like rep just to tear it down in the interpreter immediately thereafter. 
&gt; (I have yet to get the library to compile on my machine because of cabal hell) Are you using GHC 7.4? I see HLearn-distributions depends on containers ≥0.5, which basically means GHC 7.6. (It may be possible to install containers 0.5 on GHC 7.4, but it is a Bad Idea and will result in more work than just upgrading GHC.)
I've tried out henv, which worked OK, but I wasn't too impressed, particularly with the lack of windows support. Does cabal-dev have better cross platform support? I have a particular project that uses a windows only dll, although I so most of my development on Ubuntu, so cross platform is a big deal for me. 
Yeah, 7.6 is a requirement. Did it install?
Yeah, if there are n data points, each of which can take on k possible values, then it is O(k). Actually it's O(k log k) because it uses Data.Map and that's what mapWithKeys happens to run in. In other cases (e.g. the kernel density estimator) fmap takes O(n), but training takes O(n log n) so there's still an asymptotic improvement.
&gt; Which begs the question: how come they don't usually put the date of publication (or submission, or whatever) up top? Because the publisher says where and in which form exactly this information has to be given. In this case, bottom left, as you have noticed. 
I've never used Haskell on Windows, sorry.
Yes it does support windows. The version of cabal on github has built in sand boxing too.
I'd say that impurity (i.e., failure of referential transparency) is indeed rather bad[1], mainly because it makes things so difficult to reason about. However, I wouldn't say that effects are bad--- nor do I think most Haskellers would either. I love being able to program with state, exceptions, logging, nondeterminism, etc. There's little purpose in a program that has no effects. And much as I love pure math, I also like to see my creations have an effect on the world. I think the big problem here is your conflating the ideas of effects and purity, when really they are quite different things. For one, "purity" is, as the name suggests, very much a philosophical or aesthetic ideal and thus is always open to a certain degree of interpretation, [as augustss alludes to](http://www.reddit.com/r/haskell/comments/1j7l3a/state_is_pure/cbc2agt). Then again, the question of what constitutes an "effect" is also becoming [a subtle problem to nail down](http://www.cs.indiana.edu/~rpjames/) as well. [1] However, as a philosophical concern, I am afeared that the failure of RT is unavoidable in programming just as it is in natural language. Nothing exists outside of context. And yet, unfortunately, it is the context *not the program* which determines whether the program can be interpreted with transparency or not. Thus, purity is, of necessity, always in the eye of the beholder. But, even if, fundamentally, the notion of RT is paradoxical and thus unattainable, that does not however mean that it is not something that should be striven for. Despite Oleg's pointing out that the fact that all Haskell programs must live underneath the context of `IO` and thus cannot be truly RT; the very fact that we can push the illusion as far as we have, forcing him to resort to that topmost context, belies the fact that that the illusion of RT is very useful in itself. Moreover, as augustss pointed out in a previous thread on the topic, he wouldn't like to have a language so RT he couldn't do benchmarking on it ;)
&gt; There seems to be a notion that being "impure" is bad, Impure is bad because [it leaks](http://en.wikipedia.org/wiki/Leaky_abstraction). Impurity is what we call it when a program is allowed to break the view of the world it pretends to give us. But we believed in that world, that view. The badness is because we were lied to. For example, if someone hands us a list, we'd like to believe we have a list on hand. However, if they can go and mutate it behind our backs, or even deallocate it, then we can't possibly reason about this "list" we supposedly have. We're only holding onto vapor. &gt; but imo impure programming in a particular principled way (basically, the Haskell way) is not bad at all. But if someone hands us a pointer to a list (pick your favorite `Var` type in Haskell), then we are not misled into thinking "hey, I have a list"; it is blatantly apparent that we have only a pointer to a list. The fact that we keep track of mutation and make it explicit, means that it is no longer "impure"--- in the sense that it is not longer wildy misleading. The effect, the mutation, is the same in both cases. But the claim to purity is quite different. In the one case we have no idea what could happen; in the other, we know exactly what can happen. Isn't it obnoxious in Java (or Python, Perl, C,...) that any time you "have an object" you actually need to check whether you really have one vs whether it's `null`? Wouldn't it be nice to actually have a way to say, "no really, this is guaranteed to be a real bonafide object, none of that null pointer bait-and-switch nonsense"? The reason it's obnoxious is because we're being lied to. The language claims that variables are names for things ...except when they aren't. I wouldn't say that the presence of nullable-pointers is "impure" per se, but it gives a good idea why it is that Haskellers dislike impurity. A better example of impurity is if you're familiar with Prolog. See, Prolog presents this nice conceptual model of backtracking proof search. ...except when it doesn't. Prolog has an impure operator which allow you to prune the search space by cutting off the ability to backtrack; thus, there may be proofs that you will never find just because someone decided to put a `!` in the wrong place. Prolog also has an impure operator which allows you to test and see whether a given logic variable is bound or not; which lets you get into all sorts of tomfoolery and utterly destroys the conceptual model the language is built on. If you don't use operators like these, then it's pretty easy to say what a Prolog program means. But, like `unsafePerformIO`, once you start using these operators the only thing you can really say is "when I run this program, this is the output I get".
&gt; I think it does make sense to say exactly that. Just because I could "desugar" a C program into an explicit state-passing language No no, when I say that monads are just sugar, I really mean exactly that. You don't need to leave Haskell, change the type system, translate to another program, or any of that. As in the OP's blog post, whenever you see `do`-notation that's just sugar for explicit calls to `(&gt;&gt;=)` and `return`; but `(&gt;&gt;=)` and `return` are just plain old functions defined in Haskell, and because of referential transparency we can inline their definitions. Anything you could write using monads in Haskell, you can write *the exact same thing* without ever having defined the particular monad you were using. The two programs are exactly identical because the one is just what the compiler turns the other one into in the earliest preprocessing steps! The only thing monads do is to help hide the repetitive boilerplate code by syntactically binding it to newlines and `&lt;-`s. That boilerplate code is still there all the same. If you couldn't've written the boilerplate in the first place, then you couldn't've defined the `Monad` instance that would allow you to use the syntactic sugar.
Let's just admit that this is a quaint custom, all too common with Haskell papers, to publish two-column and not post source to Arxiv.org. Here, two of the authors have posted TeX source in the past to Arxiv.org (http://arxiv.org/find/grp_cs/1/au:+Kiselyov_Oleg/0/1/0/all/0/1 and http://arxiv.org/find/cs/1/au:+Sabry_A/0/1/0/all/0/1) but not this paper. Don't trade tricks for getting around this moronic custom; fix it. Get those editors to retire. Call attention to "conservation of originality" where the most brilliant within their specialty then display astoundingly conventional behavior (publishing only suitable for physical sheets of paper) to compensate.
Indeed. I was about to comment on how the `O(1)` claim is a lie. You shouldn't hide parameters like that because they really do matter when computing complexities that arise from composing and repeating these basic operations. Feel free to highlight the fact that `O(k*log k)` is constant in `n`; but don't claim the polymorphic function is `O(1)`.
If you look at the implementation: http://okmij.org/ftp/Haskell/extensible/Eff.hs there is a way to prevent cheating: Put the `State` code in a separate module and only export `get`, `put`, `runState`, and the `State` type, but not the `State` constructor. I wonder why they need `Typeable`. Datatypes à la carte doesn't need it.
Usually it's nowhere, though (that I can find). If that's because "the publisher says so", that's good to know! But still leaves another round of "why does the publisher say so?".
Here is another way of saying what /u/winterkoninkje is trying to say. You might choose to call `State s` impure. You are not using a well-understood definition of impurity, but sure, let's call it that. As it stands, you seem to be using impurity to mean, *has an effect*. You will notice that /u/winterkoninkje is clearly distinguishing effects and side-effects. This means that `[]` is impure, since it is just as much an effect as `State s` (and just as much a side-effect too, which is not at all). So as long you agree that `[]` is impure and `Maybe` is impure and `Cont r` is impure, then you might also say that `State s` is impure. I would argue though, that you'll only be able to use this definition with yourself and it won't be particularly useful. In short, State is pure.
Installed straight off hackage with a fresh install of ghc 7.6 and the latest haskell platform. Although I didn't get a chance to play with it yet unfortunately. 
I'll give it a look when I have the chance then. Thanks! 
Reasonable point on the cheating front. Though there are still serious optimization issues. They probably are probably abusing it so they can have multiple environments easily, etc. 
Well done to our fellow functionalists! Looks very suave. Can we also make a home page for Haskell with such a serene wow factor? :-) P.S. Open the page with [this soundtrack](https://www.youtube.com/watch?v=nuT6dTFax5o#t=57m10s) for additional inspirational feels.
[Real World Haskell](http://book.realworldhaskell.org/)?
I do this, and my reason is that, with -Wall, GHC will warn you when you don't bind a (non-()) result from a monadic action. I always compile with warnings. I suppose I could turn off this warning in particular, but actually I like it - I've certainly caught a few bugs as a result.
I prefer “`void`” to “`_ &lt;- `” because it is explicit and transportable.
Yeah - you're right. `void` is probably better style, it's just habit. I should try to switch.
I use `void` instead
I heartily welcome documentation patches to `parsers`. I haven't been able to give it the attention it deserves. (For that matter, documentation patches to any of my libraries are welcome.)
Speaking of which, when will `trifecta` be updated? Or is it defunct now?
I'm open to patches to `trifecta` as well. I've just been focused on other things. What are your pain points?
You're right. I thought that discussion would just make things more confusing, because there's really a lot to say about it and it's not really very important. n can be on the order of millions and billions, whereas m is maybe in the hundreds. I've added a couple of sentences making it clear.
I have that open, along with BOS's chapter on parsec, miscellaneous blog posts about parsec, and the docs a few other parser libraries from other languages. I need to ask Bryan if I can port parts of his csv parser for parsec 2 for the docs as well. The BSD license on Daan's examples seems to mesh with the license on parsers, but I will send him an email as well. 
It's actually just the outdated `reducers` dependency, and documentation. I'll see about submitting patches, thanks for reminding me that it's open source. :)
[Here](http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html) is a tutorial I had written for Scotty a while back. Since then I've had a few emails asking if I know of a similar tutorial for Snap. Mine covers routing, templates, an ORM, and deployment.
&gt; distinguishing effects and side-effects This is an interesting and important thing to note. In my opinion, the only difference between "effects" and "side-effects" is how hard you squint. When I look at a function of type `a -&gt; State s b`, I can look at it as an "effectful" computation, or I can squint, and it looks like a function of type `a -&gt; b` *with a (stateful) side-effect*. When I see a function of type `a -&gt; [b]`, I can squint, and it looks like a function of type `a -&gt; b` *with the non-determinism side-effect*. On the flipside, I can see a function of type `a -&gt; b` (for some concrete `a` and `b`), and un-squint. Does this function force its argument to WHNF? Can it throw an error, or loop infinitely? Will it cause memory allocation? Does it secretly invoke a C library behind its pure interface? These are "effects" that the Haskell type system doesn't capture. But we, as Haskellers, choose to squint, and look at these functions as "pure", because we have chosen a set of "acceptable" side-effects that are generally safe and convenient for us as programmers to ignore. (Assuming responsible use of unsafePerformIO, of course.) So upon reflection, I finally think y'all are right, I have used the term "impure" in a vague and confusing way. I will use the terms "imperative" and "effect" in the future, and avoid speaking in terms of "pure" and "impure" unless I first clearly define what I mean by those terms.
I think he means you can move it to the bottom of a list of do statements: main = do putStrLn "Tell me your darkest secret:" _ &lt;- getLine The above code produces a compile error: "The last statement in a 'do' block must be an expression". Using `void getLine` instead avoids this problem.
I think that both digestive-functors and reform are pretty natural points in the design space. The former gives complete control over the view, while the latter handles all of view for you (field names are a part of the view). In applications I have worked on, when I don't need view customization, I typically don't want to have to think about field names either. My first reaction to your post is to question whether you really need to control the field names. But assuming you truly do need that control, I think you're probably looking for a point in the design space that hasn't been targeted yet.
What good is it to know that two IO values are "the same" if the only way to observe them -- by running them -- produces different results? Even if I duplicated the universe, and ran the two programs beginning at the very same instant on identical machines in identical cosmic contexts down to the very atom, they would produce different results. (Unless, of course, the optimizer catches that `repeat 5 !! 1000` is the same as `5`, which GHC could with an adequate RULES pragma.) So your proposition, that they "return the same IO value", is an assertion of absolutely no value unless this sameness can tell us *something* about what happens on particular machines.
&gt; if it doesn't optimize away What would the solution to this be? I don't pretend to know anything about GHC optimizations, but it seems like it *should* be as easy as setting up a rule for optimizing `void m &gt;&gt;= f` into `m &gt;&gt;= \_ -&gt; f ()`
Or use: () &lt;$ action
"mapM space leaks". You made my day. I'm going to starting looking for some right now. 
I think the solution is already in-place [as I observed here](http://www.reddit.com/r/haskell/comments/1jcls5/why_use/cbdgs82). There is [a rule on `fmap` which is fired when it is inlined from the call to `void`](http://lpaste.net/91340). The normal monad rules probably do the rest.
Generally I don't use void because it's not in the Prelude so I have to import it. Yeah I'm too lazy to type one line of import.
Not yet. `pipes-attoparsec` is getting a major overhaul to the API so the tutorial would become obsolete very quickly.
Bravo, I look forward to it. parsec is highly deserving of some really good up-to-date docs.
Learn Haskell they said. Be lazy they said.
I still don't understand why the elegant API of `Control.Pipe` is deprecated for the way more complex `Control.Proxy` stuff. Yes, it's probably more general and therefor gives a more solid foundation, but it doesn't really seem to serve the general use case (and user). Also, I wonder if things had worked out slightly more elegantly if the `PipeC` (horizontal composition) would be called `Pipe` and `Pipe` (vertical composition) would be called `Step` (or something more meaningful). This way composition using `Category` would be the default and the other composition operators become useless. This only has the extra requirement of lifting steps into a real pipe using an additional constructor, which might be worth it considering that pipes are probably simple in most use cases. I still see pipes/conduit/io-streams as a replacement for writeHandle &lt;=&lt; liftM logic &lt;=&lt; readHandle where `logic` is pure and contains the complexity of my application and is defined elsewhere. 
My thoughts exactly! I haven't learned much of it yet, but I suspect that homotopy type theory may be of use in formalizing such a notion.
aeson is documented well enough for experienced Haskell users and I've had no trouble, but I've seen several people "in the wild" who complain that they don't understand how to define a structure that contains some other structure (of a different type).
`pipes-4.0.0` should hopefully fix many of your complaints.
I have a very simple file from when I was first learing Aeson that might be helpful as an example of parsing nested JSON. I just have to find where I archived it and make it suitable as documentation.
&gt; Makes me wonder what papers you usually look at. Maybe unpublished drafts? (Which is fine!) Possible. I don't have a large statistical sample. There's only been a few times when I wanted to check when a paper was from, and in those cases I generally couldn't except by looking at the references, which is where I got the impression that this is the usual thing. But of course I don't remember which papers those were, and it's entirely possible that they were preprints. It's good to know the general rule is actually supposed to be the opposite. I'll check back if I encounter any counterexamples.
you, sir, are quite a good samaritan. kudos.
for this nested JSON example.txt ----------------- {"menu": {"header": "menu", "items": [{"id": 27}, {"id": 0, "label": "Label 0"}, null, {"id": 93}, {"id": 85}, {"id": 54}, null, {"id": 46, "label": "Label 46"}]}} Define a record for each level of the hierarchy. In this case the levels are l1 menu l2 header l2 items l3 [id] which I named Menu, Contents, and Item below. * data Menu is rather boring. * data Contents takes care of the "listyness" and "possible nullness" in the description of items * data Item takes care of the "possibly no labelness" of each item. * The nesting is described in the records by the types. i.e. Menu {Contents} =&gt; Contents {Header Items} =&gt; Items{Id Label} * the targets for the parser are described in the FromJson instance and Aeson takes care of the rest. I really should have cleaned this up more, but I need to sleep. I will check back tomorrow. I hope this helps somewhat. {-# LANGUAGE OverloadedStrings #-} import Data.Aeson import qualified Data.ByteString.Lazy as B import Control.Applicative import System.Environment import Control.Monad import qualified Data.Text as T main = getArgs &gt;&gt;= return . head &gt;&gt;= B.readFile &gt;&gt;= print . dCode dCode :: B.ByteString -&gt; Maybe Menu dCode = decode data Menu = Menu { menu :: Contents } deriving (Eq,Show) instance FromJSON Menu where parseJSON (Object v) = Menu &lt;$&gt; v .: "menu" parseJSON _ = mzero data Contents = Contents { header :: T.Text , items :: [Maybe Item] } deriving (Eq,Show) instance FromJSON Contents where parseJSON (Object v) = Contents &lt;$&gt; (v .: "header") &lt;*&gt; (v .: "items") parseJSON _ = mzero data Item = Item { iid :: Int , label :: Maybe T.Text } deriving (Eq,Show) instance FromJSON Item where parseJSON (Object i) = Item &lt;$&gt; i .: "id" &lt;*&gt; i .:? "label" parseJSON _ = mzero Output : $ runhaskell aesonExample.hs example.txt Just (Menu {menu = Contents {header = "menu", items = [Just (Item {iid = 27, label = Nothing}),Just (Item {iid = 0, label = Just "Label 0"}),Nothing,Just (Item {iid = 93, label = Nothing}),Just (Item {iid = 85, label = Nothing}),Just (Item {iid = 54, label = Nothing}),Nothing,Just (Item {iid = 46, label = Just "Label 46"})]}})
I thought I'd link to http://stackoverflow.com/questions/12372150/using-trifectas-layout-parser/12378513#12378513, but I've just checked Hackage to see if there's been an update since then. I see trifecta-1.1 is the new version now, but it doesn't seem to include the layout-aware parsers anymore...
Nope! Never understood the point of void over _ &lt;-
If you have ideas about how I could create a similar combinator library for intrinsically-associative functions, I would be very grateful.
I'm happy to reintroduce them. There was a _serious_ bug in their implementation and I've yet to port the fixes back to trifecta in haskell from the scala implementation of [ermine](https://github.com/ermine-language). The delay is caused mostly because in the meantime I forgot what they were!
There's a difficulty in that a _single_ function can be commutative, but you need at least _two_ invocations of a function for associativity to hold... So its an interesting question, but immediately it seems pretty difficult. 
Mmh, introducing a new concept/name "the commutative monad" for a fixed monad is rather unfortunate, as there is already a well-established notion of commutative monads. There are various monads that are commutative, in a clearly defined sense, and other monads that are not commutative, just as there are binary operators that are commutative and others that are not. Saying that one specific monad is "the commutative monad" goes against this understanding. What would you (or any other mathematician) say if I introduced some operator I like and said that from now on that is "the commutative operator"? Wouldn't make much sense, right?
The monad of commutative operations? The commutativity monad?
Yes, I think the latter, "the commutativity monad", would be much better.
I always really like the correct by construction approach. It's nice to see how, with some basic assumptions, very strong properties can be encoded without full dependent types and complicated proofs. Nice work!
I would be more interested how to get from a novice to an average level first ... seems like Haskell is so far above your normal day PL that even the "brown belt" is miles ahead of me ... No really - there are great blogs and such but I do miss the book/path from LYAH to the stage where I can call me a Haskeller with some intermediate knowledge so I am really interested in the answeres here :D
It's a matter of learning some math and CS theory. Without knowing your current level of experience, it's hard to give any concrete recommendations. 
&gt;How can I be more like them? Practice.
I wonder if it would be possible to write distinguishBy' :: ((a,a) -&gt; b) -&gt; (b -&gt; Bool) -&gt; Commutative a (Either Bool (a,a)) which would take a possibly non-commutative operation, apply it to an unordered pair both ways, then observe the unordered pair of results. I haven't been able to figure out how to do it in terms of just `distinguish`. It is more general than `distinguishBy` since you should have distinguishBy = distinguishBy' fst or distinguishBy = distinguishBy' snd If you could do this, you could have things like equalSym :: (Eq a) =&gt; Commutative a Bool equalSym = distinguishBy' (==) id &gt;&gt;= either return (const $ return True) (Of course that `const $ return True` could be anything if the `Eq` instance is symmetric.) Or even more generally symmetrize : ((a,a) -&gt; b) -&gt; Commutative b c -&gt; Commutative a c which uses the given commutative operation to combine the results of applying the non-commutative operation both ways. This starts to look like composition. You might just want to compose two commutative operations compose :: Commutative b c -&gt; Commutative a b -&gt; Commutative a c You can get that by using `symmetrize` with `runCommutative`, but since the operation is commutative, you don't gain anything by applying it both ways to a pair, and you just wind up taking a diagonal of the commutative operation. You could supply two `Commutative a b`s instead compose' :: Commutative b c -&gt; (Commutative a b, Commutative a b) -&gt; Commutative a c But this is commutative in those two `Commutative a b`s, so you could have compose'' :: Commutative b c -&gt; Commutative (Commutative a b) (Commutative a c) That type signature reminds me of `fmap`, and I'd be interested to know if it means anything. Like, is there a constraint on `f` to get cmap :: Commutative b c -&gt; Commutative (f b) (f c) Something like `Functor`, but that doesn't exactly work. I think `Applicative` would be enough if `Commutative a b` were just `(a,a) -&gt; b`, but you would need the `Applicative` effects to commute?
Definitely yes: the [time](http://hackage.haskell.org/package/time) library, as discussed in this parallel [reddit thread](http://www.reddit.com/r/haskell/comments/1j519n/haskell_time_libraries_are_confusing_me/).
well-placed mutable references can be used to powerful effect, especially when writing a library or a framework. As an example, the actor model in F# uses mutable queues as ports for communication between actors; I have no idea how I'd implement that in Haskell. Of course, papers exist on the subject, so you're never quite left without a paddle, but figuring it out yourself is quite hard.
Just write code, a lot of code. I don't have any math background and don't know anything about category theory. I struggle a lot with reading formal writings about the subject and don't know how to read any of those fancy diagrams. Still, I write real-life Haskell code on a daily basis for a living. I use monads/arrows/applicatives/functors/lenses/categories/monoids all the time and love them. Not because I understand their theoretical foundation, but because they help me solve my day-to-day problems. So, although there is no good reason not to gain a bit more theoretical background, it is really no requirement for writing decent Haskell code. It is if you want to become like ekmett, or tekmo :-)
Mutability is (almost) as easy in Haskell as in F#, it just shows up in the types. Which in the case you cite, is probably exactly what you want.
A lot of that I think is just actually writing something substantial in the language. As you get past the basics, it's harder to have a single book which covers all the various things you might start wanting to know about, and it's hard to substitute reading about something for actually trying to write it and looking through the documentation for the bits that you're after at each stage. However, there is [Simon Marlow's new book](http://chimera.labs.oreilly.com/books/1230000000929/index.html) when it comes to concurrency and parallelism, and it also includes a lot of practical knowledge about how evaluation occurs which is just good stuff to know when optimising things in general. There are also a lot of papers written by various people, especially [Simon Peyton Jones](http://research.microsoft.com/en-us/people/simonpj/), [Philip Wadler](http://homepages.inf.ed.ac.uk/wadler/), [Manuel Chakravarty](http://www.cse.unsw.edu.au/~chak/papers/), [Simon Marlow](http://community.haskell.org/~simonmar/bib/bib.html), and well, there are many others, but those guys are a rather good start. :)
I prefer it because do foo void bar is nicer than do foo _ &lt;- bar return () And because this foobar (void zot) is nicer than foobar (do _ &lt;- zot; return ()) And this map void foo is nicer than map (\m -&gt; do _ &lt;- m; return ()) foo And this foo = void . bar is nicer than foo = (\m -&gt; do _ &lt;- m; return ()) . bar And this foo = zot $ bar $ \foo -&gt; void $ do yay wow is nicer than foo = zot $ bar $ \foo -&gt; do _ &lt;- do yay wow return () And finally because this main = do bob &lt;- zot _ &lt;- bar return bob personally, as a code maintainer, implies to me that the value from `bar` was once used and might be used in the future, and that it's been replaced with `_`. You can compare _most_ of the `void` examples above to to `(() &lt;$)`, but I think that looks like crap and is annoying to type—way more characters and infix is awkward—in most scenarios.
&gt; "roll forward n days" "roll forward n weeks" "roll forward n months" Yes, those could be simpler. The [time-lens](http://hackage.haskell.org/package/time-lens) package fixes those, if you don't mind using [lens](http://hackage.haskell.org/package/lens)es. &gt;"find the difference in months" "find the difference in weeks" Those are trickier. What do you do at next-larger-time-unit boundaries? How do you supply the round-off remainder? Different use cases have very different needs, and converting between them is non-trivial. &gt; Holiday-aware versions of those functions (parameterized by holiday lists) That's locale-dependent, and it's much more complicated than just lists. No one has done much yet with locales in Haskell. The time library itself still uses [old-locale](http://hackage.haskell.org/package/old-locale) in `Data.Time.Format`.
I'd probably use mutable queues in STM, with `mkActor :: (ReadQueue a -&gt; IO ()) -&gt; IO (WriteQueue a, ThreadId)` as the primitive function backing the actor system.
This very accurately summed up my initial reaction to this paper. I was struggling to see the novelty over something like à la carte.
A brief but incomplete answer is "practice." The efficient method is to find shortcuts in the process of learning. That is to say, find a method of learning, such that when you practice, you are doing so efficiently. Dedication and enthusiasm are great, rare even, but they are not enough. A lot of what you are aspiring to achieve is pattern-recoginition. The ability to find patterns is itself a skill, then recognition is by practice. On a note, I find that playing chess helps develop the same pattern-recognition skills, then play blind and with multiple opponents to do it efficiently. It is a very crude method of exercising what appear to me be the same skills. Finally, peer review of your progress and ideas. This is also about efficiency of learning. All the best in your efforts.
That's `IxContT (WriteQueue a, ThreadId) (ReadQueue a) IO ()` (ignoring `newtype` wrappers). I wonder if that formulation gives you any new ideas. I can feel the nub of one in the back of my mind, but it's not resolving itself for me. :(
&gt; I know they're comonad coalgebras in the costate comonad They're actually not, although that's another, different, formulation of lenses. The formulation used is `lens` is the same as the one used in `lens-family`, namely the (generalized) van Laarhoven representation, sometimes called the functorial lens representation. The idea is to sort-of "church-encode" (to give a very imprecise analogy) lenses via their `modF` function, which looks like this: modF :: Functor f =&gt; (a -&gt; f a) -&gt; s -&gt; Lens' s a -&gt; f s For the general case of a lens family, it becomes: modF :: Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; Lens s t a b -&gt; f t If you partially-apply away the lens parameter, you end up with a perfect representation of the lens you just passed in, in the same way that partially-applying away the list parameter of `foldr` (or `foldl`, or `foldMap`) gives you a perfect representation of the list you just passed in. --- For comparison's sake, the following are the representations of lenses/lens families that I'm familiar with: -- indexed-store coalgebroid rep., e.g., @data-lens@ type Lens s t a b = s -&gt; IxStore b a t data IxStore i o v = IxStore o (i -&gt; v) -- van Laarhoven a.k.a functorial rep., e.g., @lens@ type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t -- residual rep. (no libraries that I know of) data Lens s t a b = forall r. Lens (s -&gt; (r, a)) ((r, b) -&gt; t) -- indexed-state transformation rep. (at least one now-defunct library) type Lens s t a b = forall r. IxState a b r -&gt; IxState s t r type IxState i o v = i -&gt; (v, o) --- EDIT: Would somebody please tell me what I did wrong?
AFAICT, this derivation applies to the functorial lenses as well: http://patternsinfp.wordpress.com/2011/01/31/lenses-are-the-coalgebras-for-the-costate-comonad/ EDIT: typo
Since I haven't pushed anything on hackage yet, there is still time to rename it! Any other suggestions for the name? I like Commutative, because it made it easy to read (f :: Commutative r a) as "f is a commutative function from (r, r) to a), and (f :: Commutativity r a) doesn't have the same ring to it. "Commute"? "CommFun"?
LYAH is all I ever used. I have a coworker who keeps reading it (and RWH, which I have not read) and hoping he'll magically learn the language. Once you have the basics you just have to write a lot of code.
Thanks! I'm still not sure why what I said was wrong, though. The van Laarhoven representation is isomorphic to, but fundamentally different from, the `Store`-coalgebra representation. I read through the post you linked to several times, and I couldn't find any vestiges of any of the latter three representations at all.
Figure out an angle that will make people feel that scrz is easier and more powerful to use, like github did for code project sites, then create some buzz out there and you're good to go.
I like your `distinguishBy'`, since it is indeed more general, but I don't like your example use case. The point of Commutative is to help the programmer ensure that his functions are commutative, by checking his work. While your implementation of `equalSym` is indeed commutative, it might not do what you expect: instead of checking that your implementation of (==) is symmetric and raising a type error otherwise, it checks (==) at runtime and silently returns garbage otherwise.
Ok! :) I think you know what I meant though, right? All four of the representations I gave are *isomorphic*, but they are not *equal* (well, maybe if you bring univalence into play, but I wasn't going to). Not only do they have different efficiency and algorithmic properties, the isomorphism transport functions are nontrivial in (I think) every single case. Perhaps we're working on different definitions of "the same", but, to me, if I can state four representations and the only proofs that they are isomorphic are nontrivial, then that tells me that they are different. They serve the same purpose, and behave the same way superficially, but they are implemented differently, and that can affect not just their relative efficiencies at different tasks but also how readily they generalize to related structures, and in which "directions". Therefore, I think I'm justified in calling them "not the same" in the context in which I responded to /u/kidnapster's top-level comment.
How about CommOp (commutative operator)?
Bear in mind (just as a general rule, and much respect all around) that showing up on every thread and seeming to know everything isn't necessarily a reliable indicator of who is or isn't an expert programmer. sfvisser's advice is good though. Write lots of Haskell. Write it for everything. Whenever you think something is clumsy, get obsessed with making it less clumsy. Rewrite your code, then rewrite it again. Look for common patterns, factor them out mercilessly, then figure out what people already call the thing you just 'invented'. Stop looking at your code, then go back to it, so you can learn when you made it too clever. Read lots of code too. Subscribe to the new hackage uploads rss. Read the descriptions of modules being uploaded. If you don't understand what they do, google them. If they sound interesting, read their code. Read lots of papers. Try to read at least one a day. Start with classics (there's plenty of discussion on this reddit about that, or just hop on irc) then move up. Or find a topic that interests you and a relatively new paper on it. Fail to understand it. Look at the citations, find the interesting looking ones, try to read those. Fail to understand them. Hop on irc a bit, come back in a week, try to read it again. Try to implement it. Give up, find the package that does implement the code. Read the code, forget the code, forget the paper. Read the paper again, try to implement it again. Read Hamming's talk on you and your research: http://www.cs.virginia.edu/~robins/YouAndYourResearch.html get obsessed, basically. and simultaneously don't be daunted. if other people can learn this, you can too. it just takes time.
I think you should name the *type* Commutative, but *call it* the "commutativity monad".
I agree that associativity is much harder. However, idempotence also requires two invocations, yet it is quite easy to come up with a system for intrinsically-idempotent functions. For example, you could have a single primitive `fix :: Eq a =&gt; (a -&gt; a) -&gt; a -&gt; a` which repeatedly applies its function argument until a fixed point is reached. I'm not really fond of that particular example, since it is easy to accidentally write an infinite loop, but it demonstrates that it is not the two invocations which makes associativity hard.
The main benefit over a simple free monad is the observation that it is CPS'd. There is a nice theorem that shows you can turn any sequence of nested loops into a single nested loop. The power of CPS'ing/taking the right Kan extension of the free monad is similar, you only have to do it once, then you can deal with as many nested "loops" as you need via the devil's bargain. `Codensity (Free f)` is bigger than `Free f` precisely because it gives you that extra power, but that is a pretty old insight.
The 'costate comonad coalgebra' representation gives you a -&gt; Store b a which expands to a -&gt; (b, b -&gt; a) which can be viewed as the getter a -&gt; b and setter a -&gt; b -&gt; a packed together. The problem is that this doesn't generalize to give you the family approach in `lens` using any sort of standard categorical vocabulary. You might make up some pseudo-categorical mumbo jumbo and say that a lens Lens is an indexed costate comonad coalgebroid, but that is really just making yourself feel better and doesn't lead to the rest of the insights that make lens go. The more useful way to think about them is to note that we really want to go from some kind of p a b -&gt; p s t where s and t are related and a and b are related using the family structure I blogged about way back when. This lets us abstract over the fact that we want composition with (.) from the Prelude and id to be the identity whatsit for whatever idea we are working with. Then it naturally becomes a question of what properties should we require of p? Then if p is allowed to be any profunctor you can get an isomorphism. If p is a representable profunctor you can get a lens, If p is a representable profunctor that is represented by a functor that ignores its argument then you get a getter, etc. This leads to one viable lens library design, but not the one we use. The problem is that approach costs us `traverse` as a `Traversal`. So instead we use `p a (f b) -&gt; p s (f t)` with different constraints on p and f, and the fact that we can do so with (almost) no loss of functionality also follows from the theory of profunctors. (We lose the ability to turn around a prism twice and get a prism back using the same combinator that turns around an isomorphism twice hence "almost", but in exchange end-users can define lenses and traversals and prisms and isos and getters and folds using mere Haskell 98 dependencies.) The insights that drive lens are very much _not_ the old costate comonad coalgebra machinery, but are instead derived from just taking the properties of the van Laarhoven representation and looking at what they mean operationally and figuring out what else can be used to call those functions. The `Profunctor` insight that brought us isos, equalities, prisms, indexed lenses, index-preserving getters, etc. was derived from just trying to get at the laws from a completely different direction.
Great idea! Now I have to steal this. ;)
You have to be an expert to deal with flying crap like this: *Linking C:\Users\Gall\AppData\Local\Temp\epic-0.9.3-4368\epic-0.9.3\dist\setup\s etup.exe ... Configuring epic-0.9.3... 'make' is not recognized as an internal or external command, operable program or batch file. Failed to install epic-0.9.3 cabal: Error: some packages failed to install: Agda-2.3.2.1 depends on epic-0.9.3 which failed to install. epic-0.9.3 failed during the configure step. The exception was: ExitFailure 1* Thats why Haskell as a whole is just useless.
I'm a graduate student doing bioinformatics and I'm lucky that my professor doesn't care what language we program in, so I chose Haskell. I just started using Haskell to do everything, even the smallest of scripts. Probably the two things that contributed the most to my understanding of Haskell were: * Writing a large project (the subject of my thesis) entirely in Haskell * Writing and working on `pipes` (which was a minor offshoot of my main project that grew into something bigger) Like others mention here, practice is more important than reading. In my case I was lucky to have a lot of practice both on the application side (my protein search engine) and on the library side (`pipes`). Edit: I want to add that it doesn't take very long to get good at Haskell. The moment you do it full time you pick it up very quickly. Really, the main barrier to learning Haskell is the absence of jobs (or classes, for students).
Sounds like you don't have `make` on your system? You'd need that to compile most C programs too, you do realize?
If sclv is correct(and he probably is), this link might be helpful to you. [gnumake for windows](http://gnuwin32.sourceforge.net/packages/make.htm) 
Forget that. I've just found it requires GPL licensed crap... rm -rf ~/Haskell 
It requires libgmp, which comes with the haskell platform and does make for a few subtleties in some cases; everything else you mention has a BSD or similar license.
That's actually not true. My undergrad training is as a biomedical engineer and my graduate program is biochemistry and biophysics. Almost everything theoretical I've learned was a side effect of just being a practicing Haskell programmer. I don't have any formal training in computer science or mathematics. I think people confuse the fact that I'm more visible (because I blog) with me being more expert.
&gt; I think people confuse the fact that I'm more visible (because I blog) with me being more expert. I think it's actually more to do with `pipes` (and `mmorph`, and `index-core`, etc.) than anything else. /u/edwardkmett is also well-known primarily because of his many, many high-quality packages.
or even oleg
So long! Don't let the door hit you on the way out.
github didn't create an alternative VCS though, they completely embraced git. This project does not wrap docker, it provides an alternative.
Oh that fix example is really bad by the way. Its the equivalent of just invoking your function with different orders of arguments and throwing error if they don't match. At least if you're thinking the "haskell way" and equating nontermination and error, its the equivalent of a runtime check.
If you can find a system for associative which is just as bad as `fix`, it would already be of great help.
It's funny, as he's specifically mentioning edwardk and Tekmo, who derive much of their inspiration for Haskell programming and ideas from CT. 
Write a Haskell. 
I had to write like 4 whole Haskells before I was anywhere near a decent Haskell programmer.
This is true of the programming field in general. There is a massive gap between the average programmer and the best. The solution is the same too: massive amounts of practice.
I find a lot the early papers to be helpful, at least for getting a better understanding of type classes.
I don't think that anyone would take it up just like that to maintain it for free.
No higher order kinds (so no typeclasses)! You can use fsharp-typeclasses, but it is not a good solution. You can't write code for arbitrary monad, functors, applicatives etc.
Something like the following? liftAssoc op x y = (x, x `op` y, y) apAssocs op (w, wx, x) (y, yz, z) = let p1 = (wx `op` y) `op` z p2 = (w `op` (x `op` y)) `op` z p3 = wx `op` yz p4 = w `op` ((x `op` y) `op` z) p5 = w `op` (x `op` yz) in if all (==p1) [p2,p3,p4,p5] then (wx,p3,yz) else error "not associative sucker!" (I think because of pentagon laws etc. that this captures all we need? But I wouldn't be shocked if this was still not strong enough)
I'm writing a haskell right now.
Sounds like you're asking people to take on a liability, not an asset.
I made meager attempts to get customers: posted to 2p2, got my post deleted and was told to pay to advertise. Contacted the advertisement form box twice and never heard anything. Called one small pokerroom and tried to get ahold of the guy in charge, some floorman said I could just talk to him; I made an accoutn for them to test and he never used it. The market isn't huge; i'd say it's actually *extremely* nitch. Small pokerrooms (1-6 tables) that want to track their players by time spent at the table. Doing that by hand is possible but hard/annoying; many people will just not do it because of the headache factor even though they'd like to, and some do it by hand but spend maybe 5-25 hours a month managing that process. Also, the software doesn't really demo that well. It's nto snazzy, I never made a how-to video.. I'm a programmer not a designer (its in bootstrap), etc. So there is really more work that would probably need to be done to make it easier to sell.
You could also enter it in the [FP Haskell Competition](https://www.fpcomplete.com/blog/2013/07/competition-announcement) if you own the intellectual property....
yeah probably not. 
So you're asking people to take on a "gentleman's agreement" (as in, you don't have a contract?) for a product with zero revenue and positive opex. Seems like a bad deal.
no. i am happy to abide my my side of the gentleman's agreement forever, no matter what happens. if someone wants the software they can have it for free and do what they please, and, if they got an instance up and running and added features etc (which is why i posted to /r/haskell and not /r/startups) i would hope they would do me the kindness of letting this one guy have a free account. there is no way this is a bad deal cus it's just free with no obligation; neutral at worst.
&gt; no. i am happy to abide my my side of the gentleman's agreement forever, no matter what happens. It doesn't matter how honorable you claim to be. A "gentleman's agreement" is not a proper substitute for a legally binding contract. &gt; there is no way this is a bad deal cus it's just free with no obligation; neutral at worst. You said you're hosting the service for your customer. Surely that requires time or money? I don't understand how you can claim that it's "free" or "no obligation" when you have a "gentleman's agreement" to provide a service to a customer that you are trying to pass on to someone else. In any event, there's little point arguing about it. I am not interested.
*I* have a gentleman's agreement. You are making the assumption that I am binding the gentleman's agreement to the software; which is not the case. 
&gt; Unfortunately, an implication of the halting problem is that we cannot prove these program properties, for the general case. Sure you can, you just need to prove termination first. You can't automatically prove it, but you most certainly can prove it.
I'd potentially be interested. As long as I am in no way bound to provide maintenance for your previous customer except perhaps updates.
If we can build "has identity" by construction, then `liftAssoc` can start with a single value instead of two.
+1000 for a slicker Haskell front page
"for the general case"
Nevertheless, it is of course true, as my namesake wrote, that &gt; [Much is written nowadays about nomenclature and correct designation](http://waste.typepad.com/waste/2005/06/simplot.html), and that is quite right; it must all be worked upon and the best results obtained. Only I believe we expect too much of it and are too anxious to bestow upon things names that are an expression of their nature. The immeasurable advantage which language offers to thinking consists, it seems to me, in its constituting signs for things rather than definitions of them. ... The word should be, not a definition, but merely a sign for the definition that is always the changeable product of the collective labor of researchers; and it will always remain so with regard to such countless objects of our thinking that the thinker will grow accustomed no longer to regarding the sign as a definition and will in the end unconsciously transfer this lack of signification also to those signs that truly are definitions. And this too is, it seems to me, quite right. For, since the signs for concepts cannot be definitions of them, it is almost better to forbid any of them at all to be a definition than for the sake of a few signs that really are definitions to procure a false reputation for all those others which are not. ... Too much is expected of good words and feared from bad ones.
What a germane quote!
I think ultimately if you take this as simply an attempted counter-point to the Zed Shaw assertion that we should bother to make all of our *quantified* type variables verbose, he has a good point. reverse :: [a] -&gt; [b] and reverse :: [element] -&gt; [elemnet] contain the same amount of information but it is much easier to see that the former has a typo than that the second one does. When we use _quantified_ variables the names exist mostly for mental pattern matching. Since we need so few of them we tend to adopt other conventions, drawing them from a, b, c .. for arguments of kind *, f, g for * -&gt; * style Functors, m for Monads, etc. That is about the useful level of information we want to convey: Ease of mental pattern matching, and a bit of information about the kind of argument it is. If you start renaming them to foldMap :: (Monoid aMonoidalValue, Foldable theContainer) =&gt; (contents -&gt; aMonoidalValue) -&gt; theContainer contents -&gt; aMonoidalValue mapM :: (Monad theEffect, Traversable theContainer) =&gt; (oldContents -&gt; theEffect newContents) -&gt; theContainer oldContents -&gt; theEffect (theContainer newContents) you lose the ability to see the 'shape' of the function for all the irrelevant noise compared to foldMap :: (Monoid m, Foldable f) =&gt; (a -&gt; m) -&gt; f a -&gt; m mapM :: (Monad m, Traversable f) =&gt; (a -&gt; m b) -&gt; f a -&gt; m (f b) And that much I fully agree with him on. When I look at a type signature with quantified variables, I'm looking for things like: 1) Do they occur in positive or negative position or both? 2) Where do they 'flow' from and to? 3) What constraints do they have on them? Picking longer names for them helps me with none of these analyses and actively gets in the way of some of them. 
The term for this is [fast and loose reasoning](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html). Yes \_|\_ exists. But if you tell me a function has type `(forall a. a -&gt; a)` I can tell you it is "probably" `id`. The nice thing is you pretend to work in a total language, and the analysis you do will still carry some weight in the non-total language you are actually in! See the linked paper.
there would be nothing bound at all. 
What do you mean by positive or negative position there?
I think this makes complete sense for type variables. Often because the type signature is so generic that no meaningful name can be chosen. But for names of types and functions, meaningful names are important. 
In a function type `a -&gt; b`, `a` is in the negative or contravariant position, while `b` is in the positive or covariant position.
I'm currently reading Marlow's new book, and it is excellent. I love the way he's presented the base parallel and async libraries, by actually walking the reader through how to build the API, which is surprisingly simple and composable.
Thanks for trying, but I'm not sure I follow. Presumably, `liftAssoc` creates an intrinsically-associative function. But what is `apAssocs`? Is it a combinator for lifted functions? Or are your lifted functions also returning lifted values? I was thinking of a function runAssociative :: Associative a -&gt; (a -&gt; a -&gt; a) extracting a pure but partial associative function out of its intrinsically-associative form. Under this restriction, using the arguments to determine whether the implementation is associative on this particular input is fair game, but sneaking extra information along the output value is not. `fix` and `runCommutative` both satisfy this restriction, but I guess I should have made this requirement explicit.
And just to complete the description: The sign flips each time you are on the left hand side of an (-&gt;), so (a -&gt; r) -&gt; r has a in positive position again. Note: `a -&gt; r -&gt; r` still has a in negative position because that is `a -&gt; (r -&gt; r)` not `(a -&gt; r) -&gt; r`. and as the syntax tree goes you only down down the left branch once. They have to do with co- and contra- variance.
Besides, the interpretation of ∀a. a → a as “given an a, produces an a” is much more natural than its Haskell meaning of “given an a, doesn’t produce something that isn’t an a”.
I learned Haskell and I even have a family! Just do it.
Don't judge yourself by edwardkmett, Tekmo or anybody else. Enjoy and leverage their knowledge if you can, or save it for another day. After all, their brains are being poured into Haskell libraries :-). You will learn more by doing your own projects and later find that you can study other people's code and use it to better your own. Some day you might even want to do it *differently*. &lt;insert obligatory Newton quote here&gt; 
It needs space for collecting its results into a list, which is a leak if you are not going to use them afterward. This applies at least to the IO monad, which has to execute the entire action before using its result. Some other monads may be able to consume the resulting list lazily.
I'm always on the fence with this issue, for the examples you present, I'm inclined to prefer the single-letter names. However (to pick an example perhaps closer to home), consider: mapAccumLOf :: Conjoined p =&gt; Over p (State acc) s t a b -&gt; p acc (a -&gt; (acc, b)) -&gt; acc -&gt; s -&gt; (acc, t) Now we have a bunch of single-letter variables (a,b,p,s,t) , which presumably follow some naming scheme. I can grok `a` and `b` as traditional meta type variable names, but `p`, `s` and `t` probably come from your own set of naming rules? In terms of documentation I find this a bit impenetrable (even examining the definitions of `Over` and `Conjoined` doesn't really help). When I look at a type signature I'm trying to figure out: 1) What does this function do? 2) What do I have to give it to get it to do that? 3) How do I construct the things I have to give it? And in the above case, the most clear part is the `acc`! I know what my accumulator will be: success! As for the rest, well, it's time to solve a small mystery ;). Although they may be quantified type variables they probably still have a particular role, and that *should* be documented. Either by convention if very special/common, but usually explicitly at the site of the signature.
Any word on how it's licensed? 
Depends how specific the typeclass is, no? I use the usual `(Functor f, Monad m, Comonad w, Traversable t)` stuff in my type contexts quite happily. But when I have an application-specific typeclass, I will use a more application-specific identifier. e.g. class Monad http =&gt; Http http where ... trySomething :: (Http http, MonadPlus http) =&gt; ...
I tend to think of the old "a picture is worth a thousand words" line as accurate but missing the point. The point is that you can not pick the exact set of words and in fact very few sets of a thousand words we want to represent have pictures that represent them.
So is the author of the article in question.
I usually just adopt letters from one of the lesser used ranges and try to use it consistently throughout the API, or note that it is a special case of Monad and just use `m`.
This looks more like the Haskell Platform download page. http://www.haskell.org/platform/
I followed him on Twitter and it does seem to be his implication.
A good article! There's a minor typo though: "I create the velocity wire with pure 20 - this wire ignores its input and always produces 10." I agree with you w.r.t. `pure`, Wire's IsString and Num instances can make things very confusing.
The article says 'LiquidHaskell *infers* refinement types' in a few places. Maybe I am not familiar with the terminology enough, but this sounds misleading to me. I'd be very surprised if given the definition of `pivApp`, the system can automatically infer the refinement type given just below it. If I get this right, the refinement type depends on the use-site(s) of the definition, not merely the definition itself.
Now available.
 He is claiming other, more mundane things though.
Argh, this what happens when you change constants after you write the blog post, kids! Thanks for the feedback otherwun, I've hopefully corrected this mistake (though all the mistakes? I doubt it!)
There is also a compiler option to set default options for your compiled binaries, if all you want to do is to set it to a fixed value you could just do it that way.
Other language communities tend to care less about correctness.
It was defined for you, but the usefulness was not explained. If a type variable in some larger type expression only occurs in positive positions, that means that the value in some sense "provides" some number of values of that type (it could be 0, 1, n, or infinity). If a type variable only occurs in negative positions, that means the value in some sense "expects" some number of values of that type (again, the position alone won't tell you how many). Let's take Reader a (Cont r b), which expands to a -&gt; (b -&gt; r) -&gt; r. The a occurs in a negative position, which means intuitively that this type "expects" some number of values of type a. The b occurs in a positive position, which means this type "provides" values of type "b". To see why, consider how it's supposed to return a value of type r with only an a and a (b -&gt; r). It must either already close over some b (if b is quantified this isn't really possible though) or generate some b from the a. Finally, the r occurs in both positions, so it's a sort of "pass through" behavior. 
I agree, that is a nice way to put it. 
If you read the paper, it's backend uses icc (the intel c compiler). So even if it got open sourced, you'd still need a license for icc. That said, there are folks who are keen to port some of the good ideas here over to mainline ghc, or something similar to the good ideas. 
[Part 1](http://chris-taylor.github.io/blog/2013/02/10/the-algebra-of-algebraic-data-types/) and [Part 2](http://chris-taylor.github.io/blog/2013/02/11/the-algebra-of-algebraic-data-types-part-ii/). Really great series of articles. I don't know if part 4 is ever coming though.
Then it is sad that the article is accepted for publishing.
I don't want to be obnoxious, but let me ask a different question: I've seen a couple of guys (Monet, Picasso) who are obviously miles and miles ahead of the average painter. How can I be more like them?
There is a tremendous amount of information given in the paper. A lot of their work went into the creation of a novel intermediate representation (very low level control flow + a high level object model). Their goal wasn't to replace GHC (they mention many times the highly tuned garbage collector which they didn't attempt to displace), but rather to expose the fact that GHC could be improved by adopting a slightly more modern perspective in some of its compilation phases. Explorations like this are both valid and incredibly useful. It is incredibly common for research to be distributed sans code and, while this can be frustrating, there are usually good reasons. Perhaps their code was far from optimal or maintainable, but their ideas were sound. Code is one of many means of expression.
Speaking of blogged about way back when, any idea when/if the Comonad Reader will be back online?
Right now you can access most of the content through John Wiegley's mirror at http://comonad.newartisans.com/ For recovery, though, I may have to move to new hosting, and I'm currently trying to get access to some of the missing data. It may be more productive for me to just start anew and have historical links to the newartisans miror.
I can appreciate the position that you're in for these highly reusable functions, and that you've documented them well. I just wanted to say that just by using the name `acc` you've already conveyed a lot of information without the need for prose documentation. Using longer names also gives you a more prose-friendly handle to talk about them later. Suppose you have your `Lens s t a b`. If you now use something like `Lens src trg get set` (not claiming these as good names) at the very least you can refer to them more easily in prose documentation, such as "A `Simple Lens` is just one in which the `Lens`s `src` and `trg` are the same, as well as `get` and `set`". Of course you could do the same with the shorter names, but then it looks like a CS paper, and few people are prepared to read that style of prose. My claim is that even if these longer names are lies, if they give a newcomer the right intuition so they can be productive faster, then they are useful. Finally, please don't mistake any of the criticism for lack of appreciation for your work, I think it's really great! I just feel that the Haskell community may still have some work to do figuring out the best practices for documentation, and I am eager to engage in such discussions. (I am also very much guilty of the same "sins" I outlined above). (edit: typos)
One of the things I consider before diving into a new library is whether or not ocharles has written an intro for it.
&gt; With `src trg get set` I get no intuition that `get` and `set` are related and `src` and `trg` are related. But you get intuition that `s` and `t` are related from `s t a b`? I think we are at an impasse :D I can dig the angle where you're trying to minimize renaming; I think we may be coming at this from two different sides. Namely, writing documentation for: * the user that knows what is happening in the library and wants to combine the functions effectively * the user that is learning the library and doesn't yet know what all these things are I believe that for the latter group, longer descriptive names are better. 
Oh fantastic! I didn't know about the mirror. Glad to see it's up.
&gt; It is incredibly common for research to be distributed sans code Sad agree. &gt; and, while this can be frustrating, Sad agree. &gt; there are usually good reasons. Not so sure. &gt; Perhaps their code was far from optimal or maintainable, but their ideas were sound. I don't see these examples as "good reasons." &gt; Code is one of many means of expression. The special thing about code is that it is an *executable* means of expression. The reason code is so valuable is that you don't just have to take their word for it, you don't have to grok the research topics they are talking about, you can just run the thing. The proof is in the pudding. --- Now, I'm not *complaining* about what Intel Labs has done. I'm very grateful that they have done this much. I just don't see why they didn't also release the source. Ugly, unmaintainable source is better than no source.
Welcome to netwire-experimentators' guild! https://github.com/MaxDaten/netwire-examples
True. But my point was that you can't explain Haskell's time difficulty by saying "time is complicated", which the OP was trying to do. If time is easy in Python, perl, PHP, Java, etc, and difficult in Haskell, then the difficulty comes not from the general problems of time, but what you have to do that's unique to Haskell.
&gt; The special thing about code is that it is an executable means of expression. Plus all the bugs. If there's discrepancy between the code and the description, you don't know which is right. Also, it's easier to understand a dozen pages of detailed algorithmic description than a bunch of more or less ugly code. Having code is valuable for replicating their experiments, but not for much else.
No, the ease in the other languages comes from ignoring all those edge cases. Sure, e.g. only handling localtime and not UTC and all other timezones, no leap seconds,... seems to work most of the time if all you are writing is software for people living in the same timezone but it will break in subtle ways when those edge cases occur and in less subtle ways when the user base of your software lives in more than one timezone.
This is a nice presentation. I was hoping to get a clearer view of the advantages and disadvantages of pipes vs. conduits. But it inspired me to look into it a little more. Here are two blog posts from a few months ago that I think give a pretty good view, even though they are definitely coming from the conduits point of view. Anyway, I think they are a good contrast to this presentation which in my opinion solidly takes the pipes philosophy. Look at the section entitled "conduit's niche" [here](http://www.yesodweb.com/blog/2013/02/upcoming-conduit-1-0), and also look at [this](https://github.com/yesodweb/yesodweb.com-content/blob/master/blog/2013/02/upcoming-conduit-1-0-part-1.md). In summary, some of the important differences: * conduits provide guarantees about running finalizers promptly, while pipes provides guarantees about sequencing of finalizers * the pipes library tends to include experimental features which conduits may or may adopt later on depending on how it works out. Currently that includes in-stream exceptions and bi-directional pipes. * There is some feature called "connect-and-resume" which is claimed to be a feature of conduits and not of pipes, and is claimed to be critical for some complex use cases. Truthfully, I don't know exactly what that feature is. As an illustration of the basic difference in philosophy, Edsko in this presentation claims to have an example of how conduits do not satisfy the monad laws. Conduit users say that is simply not true - the code is not legal conduits code. The issue involves the conduit function `leftovers`. The conduit documentation clearly states that this function may not be used on a conduit unless data has already been read from it, while Edsko's code clearly violates that. If you follow that rule, and in practice it is normally trivial to guarantee it, then conduits do provably satisfy the monad laws. And if that bothers you, then just don't use `leftovers` at all; anyway it's a feature that pipes doesn't provide (I think). Whereas pipes people say that if the type system doesn't prevent you from using the type in a way that violates the monad laws, then it violates the monad laws, period. Another example: conduits does not have a Category instance. There is no intention ever to provide one either, because (it is claimed) that would significantly reduce the usability of the library in practice and detract from some of its operational guarantees. Yet conduits are completely composable - you can connect them and combine them in arbitrary ways with clear and simple semantics and performance guarantees. Whereas the pipes people say: if it isn't a category, it isn't truly composable, period. Both approaches make a lot of sense. So far, I haven't seen any reason to consider porting our conduits-based code to pipes. I didn't make the original decision to use conduits, but they work great for us.
Haha. Good recommendation for Ollie. I also concur with your enthusiasm. :-)
&gt; If there's discrepancy between the code and the description, you don't know which is right. But if the description is wrong, then without code to back it up, how will you find out? &gt; Also, it's easier to understand a dozen pages of detailed algorithmic description than a bunch of more or less ugly code. Certainly. Low-level code is no replacement for description at a higher level of abstraction. But why not provide both? Two different views, even if one is a very poor view, is better than only one view. &gt; Having code is valuable for replicating their experiments, but not for much else. Yes, but "replicating experiments" is what science is all about, because it is the most effective way we have to convince each other. In an ideal world, we'd just write papers in extremely high-level code.
Ha, thanks! I like the SDL+GL example - that's probably the next thing for me to try.
I still don't see how that make's the OP's post, which I entered to criticize, any more responsive. Instead of spending ~5 paragraphs saying "time is hard", he could have spoken to the *specific difficulties" relevant to the language the TC was asking about. His answer was: &gt;Time is hard because of big complication #1. &gt;Time is hard because of big complication #2. &gt;Time is hard because of big complication #3. &gt;Time is hard because of big complication #4. &gt;Time is hard because of big complication #5. It should have been: &gt; Time is hard because of things like [complication]. Other languages hide that by making assumptions, but it must be specifically encoded in a Haskell function [digression about functional purity]. In your case, you need to also provide ... The OP's post made it sound like no one has ever cracked the problem of time, which says nothing about the TC's difficulties or how to solve them.
I've only read the original Liquid Types paper, but that system does indeed try to infer non-trivial refinement types. Part of what makes it work is that this inference only considers predicates built in limited ways (perhaps just conjunction?) from some set of atomic predicates (stuff like &lt; in the original work).
I'd just like to echo the sentiment for it to be open sourced. Competition is good, and open competition in which ideas can be learned from (and borrowed!) is even better.
I know I, for one, can never tell the difference between reverse and tail.
Actually, if `Either a b` corresponded to `a xor b`, you couldn't give a value of type `Either () ()`, since this corresponds to `true xor true`. But surely: value :: Either () () value = Right () -- or Left () Let's have your version of Or: data Or a b = L a | R b | B a b We can show that `Either a b` holds precisely when `Or a b` (even though the representations are not isomorphic, for obvious reasons): to :: Either a b -&gt; Or a b to (Left a) = L a to (Right b) = R b from :: Or a b -&gt; Either a b from (L a) = Left a from (R b) = Right b from (B a _) = Left a -- or Right b The point is: when both `a` and `b` hold, you just pick one of them - the disjunction doesn't care. Yes, it doesn't look "nice", but consider the elimination rule for disjunction: elim :: Either a b -&gt; (a -&gt; c) -&gt; (b -&gt; c) -&gt; c elim (Left a) f _ = f a elim (Right b) _ g = g b Now, `Either` elimination is okay, but this time, you get the ugly "choose one" part on the `Or` part: elim :: Or a b -&gt; (a -&gt; c) -&gt; (b -&gt; c) -&gt; c elim (L a) f _ = f a elim (R b) _ g = g b elim (B a b) f g = f a -- or g b, but not both!
Is the non-public functional language described in this 2010 talk (http://cufp.org/conference/sessions/2010/functional-language-compiler-experiences-intel) the same the as the strict pure functional programming language compiler that was repurposed for the Haskell compillation in this paper? My curiousity has been tickled... 
&gt; Write lots of Haskell. Write it for everything. Whenever you think something is clumsy, get obsessed with making it less clumsy. Rewrite your code, then rewrite it again. Look for common patterns, factor them out mercilessly, This. Really, you can take that advice and apply it to just about any skilled field by replacing "Haskell" with the field of choice. To become an expert in anything: (a) do a lot of things, (b) do a lot of *different* things, (c) try to find patterns to minimize how much you have to do. The first point is key because, frankly, practice matters in everything. The second point is key because without challenging yourself to do difficult and uncomfortable things you will never rise above plateaux in your skill level. The third part is key because it is the compilation process that turns experience into wisdom; without it you may remember, but you'll never learn. To appear like an expert in anything: (a) become better than most people, (b) express that knowledge in public fora. N.B., you can't skimp on the first part if you want the second part to succeed. Also note, that it helps a lot if you can do the second part without annoying the hell out of people; for which, again, don't skimp on the first point. To be an "expert" you must make sure not to act like a "show off". (Which doesn't necessarily bar showing off from time to time; personality is key.)
Just adding on to ekmett's description below, it is sometimes worthwhile to distinguish between "strictly positive", "negative", and "positive". The distinction is essentially the same one that means we don't get double-negation elimination for free in intuitionistic logics. Thus, given a function like foo :: ((((a -&gt; b) -&gt; c) -&gt; d) -&gt; e) -&gt; f * `f` is strictly positive; `((((a -&gt; b) -&gt; c) -&gt; d) -&gt; e)` is negative. * `e` is negative; `(((a -&gt; b) -&gt; c) -&gt; d)` is (weakly) positive. * `d` is positive; `((a -&gt; b) -&gt; c)` is negative. * `c` is negative; `(a -&gt; b)` is positive. * `b` is positive; `a` is negative.
Just saw your comment. I have been submitting errata and other comments via the chimera site comment mechanism. I gather from your comment, the book is finalized and it's too late to correct anything? Should I stop with the comments on the chimera site, and maybe copy them to the errata site? I had been checking in at the OFPS site from time to time, to comment on the final chapters, and am a little disappointed that now that they are finally available it's too late for comments.
The example right after `which takes a ByteString and returns a kind of JSON AST` fails to compile.
Wonderfully explained, as usual in this series.
See here: http://www.twitch.tv/bethesda/b/439369577?t=1h32m
Thanks for noting this. I'm one of the co-authors, too. It was indeed a mistake in this draft. The right way to put it should be what I said above.
Yes.
Important quote: "Haskell ecosystem is not industry grade."
See reference [11] in the hrc paper. It's accepted by DTP this year, co-located with ICFP.
There's of course the issue of polymorphic values not being shared in GHC, but the bigger one for me is that `MonadIO` is kind of a hack. `MonadBase` is a bit better, but we should really have a more "composable" effect-combining system than `transformers` and `mtl` currently provide. `mmorph` gets us a bit further, but we really need something quite different. I have an idea for such a thing, but it requires both deeply integrated subtyping and enforced purity by default for it to work correctly, and as far I know no current language satisfies both of those requirements. (That's partly why I'm [designing one that does](http://pthariensflame.wordpress.com/2013/07/07/introducing-spellcode/).)
Discussion on /r/programming is [here](http://www.reddit.com/r/programming/comments/1jj9k9/john_carmacks_2013_annual_keynote_game_tech_on/).
What would you call the thing(s) people do by writing white papers that isn't science? 
fancy
That's then because the authors in question don't follow the requirements placed on them by the publishers. In almost all cases (all publishers), there will be a requirement that the authors signs in the copyright form which says something to the effect of: "You may post preprints (in many cases, even the final version) on your webpage, provided that you include a brief notice *in the document* that points to the official publication, providing its bibliographic details." 
aight mann. fixed. seriously though, i wrote this tutorial and... why is text encoding so complicated in haskell? you've got String, you've got Data.Text, you've got Data.ByteString, you've got Data.ByteString.Lazy, for some reason sometimes you have to use Data.ByteString.Lazy.Char8. i kind of glossed over it in this tutorial, but when writing the original code i fought with it a lot. am i just a noob? oh, and thanks for the feedback :)
Just white papers, probably. White papers are also used for corporate tech exposure; i.e. a company does some thing they're patenting, and so they put out a white paper extolling the virtues of they're ideas. Not really science, just PR really. That's what I'd say this piece is as well. I think we can agree on this; in CS, it's not really science until you see some code. Until then, it's just math :-)
Basically, Carmack has been using Haskell for a few years trying to wrap his head around functional programming. To that end, he's been porting Wolfenstein to Haskell. He sees some huge software engineer benefits to not having shared mutable state -- basically, you write a function and if it works, you don't have to mess with it ever again or remember all the little hidden assumptions that are usually present in imperative code. He also has been learning Lisp, and while he appreciates the elegance of Lisp, he'd really much rather have static types for large, difficult problems. He doesn't think the Haskell ecosystem is up to par with more mainstream languages (in particular, there isn't a good debugger), but it's good enough that early adopters should be able to successfully write real games. In particular, a small studio with like-minded programmers should be able to do quite well. On the other hand, forcing existing C++ programmers into Haskell against their will probably wouldn't work.
eddie does the same thing.
Not to be pedantic but it looks like you have a mistake in the bijection Category instance definition of composition. I believe it should be (MkBij g g') . (MkBij f f') = MkBij (g . f) (f' . g')
&gt; Everything that is syntactically legal that the compiler will accept will eventually wind up in your codebase. good quote
Ah yes, spellcode. I must admit, my impression of it is still, "that sounds waaaaay too good to be true." I look forward to being proven wrong.
I would have liked to have all the chapters on OFPS for comments before publication, but time was tight, I overran the deadlne as it was. The errata tool seems to be a good way to submit comments, but I'll check to see whether I can get the comments from Chimera too.
I don't understand what MonadIO is more convenient. Surely it just means you have to do a lift in a different place.
He starts talking about FP around here - http://www.twitch.tv/bethesda/b/439369577?t=1h32m0s
I'm still coming to grasp with netwire, so there's a good chance I'm wrong about this. But, I think in the definition of velocity with challenge3, integralLim_ and accumT are delayed values. So, when collisions are detected and reacted to, your velocity may be flipped at a delay of 1. If this is true, then I think there's a chance the position can be caught at the edge bouncing back and forth. I believe this can be fixed by using integralLim1_.
Interesting. Well, since I wrote HSL, I'd like to mention a few things that it does better: * performs just in time compilation when you call the program. The bootstrapper is actually python. * uses lazy byte strings for more performance. * uses type classes to make compile time decisions on how to render the output. Lists are always split across lines. Tuples are interpolated with \t. * has a really neat way to read JSON. Hope someone will check it out as I'm quite proud of it, especially the JSON part. tl;dr: employs a type system hack and a consise lookup syntax to extract typed data. I'd also love to see what improvements could be made to it by seasoned Haskellers; especially the output typeclasses as I had to use OverlappingInstances to make it work. However, the end result feels quite dynamic, its surprising how well Haskell lends itself to this sort of thing. 
i like the name. it's a hassle but not awkward.
I don't know if this can be a reason or not, but you realise for *primitives* this means you'll have to call `liftIO` in the function body, right? So, for 2 functions, you'll get something like liftIO f &gt;&gt; liftIO g Instead of liftIO (f &gt;&gt; g) (And so on for larger composition chains) This is likely to have performance implications. Rewrite rules &amp; optimisations can probably solve it, but it won't be solved just by itself. 
Suppose I have foo :: IO Thing bar :: MonadIO io =&gt; io Thing Then for `bar`, I must use `liftIO` once only, in the definition of `bar`. But for `foo`, I must use `liftIO` every time `foo` is referenced (save when it is referenced in a bare `IO` context).
:D ... no I will not ask why you would want to use this :P
I don't want to use it, it's just for fun. I could add a warning not to use it in real-world code, but that would be an insult to everyone's intelligence.
[Why?](http://i.imgur.com/hexAEvf.jpg)
For me the audio was muted in the Flash video player, just had to toggle the muting with the speaker button.
[Youtube link.](http://www.youtube.com/watch?v=eNWAcEu1jpU) EDIT: He starts talking about functional programming [here.](http://www.youtube.com/watch?v=1PhArSujR_A&amp;t=2m5s)
Why all the void nonsense and not just `()`?
Because then `goto` as the type `LabelT r m -&gt; ContT r m a` instead of `LabelT r m -&gt; ContT r m ()`. There's probably another way to do it, but this was the easiest when I wrote it.
Void is for people who want to use bottoms but don't like the dirty feeling they get from `undefined`. 
Whoops, forgot that you need direct stack access too.
Actually, I didn't use any bottoms. I used `Void` to indicate that a given path would not be taken.
I'm not sure about the implications for performance, but the thing is that I don't like the `&gt;&gt; goto label` hack. Edit: I'm talking about code quality... for an implementation of `goto`? Huh.
It can also be used with other monads: allOnes :: [Integer] allOnes = usesGoto $ do me &lt;- label b &lt;- lift [False, True] when b $ goto me return 1
[**Part 1:**](http://www.youtube.com/watch?v=eNWAcEu1jpU&amp;list=PLqSz8wYk5VJTsadQnU9EId6G0AJWA6o0q) - new console cycle - AMD hardware - game controllers [**Part 2:**](http://www.youtube.com/watch?v=w1sjRD7NSec&amp;list=PLqSz8wYk5VJTsadQnU9EId6G0AJWA6o0q) - Kinnect - Digital distribution - Portable consoles - Andriod and iOS - Cloud gaming - Creative vision vs technology - Unified memory - PowerVR and tiled rendering [**Part 3:**](http://www.youtube.com/watch?v=93GwwNLEBFg&amp;list=PLqSz8wYk5VJTsadQnU9EId6G0AJWA6o0q) - displays - head mounted display - movement tracking - sound - large scale software development - optimization - OpenGL [**Part 4:**](http://www.youtube.com/watch?v=1PhArSujR_A&amp;list=PLqSz8wYk5VJTsadQnU9EId6G0AJWA6o0q) - OpenGL - functional programming - Haskell - Lisp - Scheme - strong and weak typing - multithreading - events - garbage collection - QuakeC vs Scheme [**Part 5:**](http://www.youtube.com/watch?v=cWA_9L70moE&amp;list=PLqSz8wYk5VJTsadQnU9EId6G0AJWA6o0q) - programming Q&amp;A: - space - AMD vs Nvidia vs Intel GPUs - CPU architectures - GPU computing - id Tech 5 - id Software company [**Part 6 Q&amp;A:**](http://www.youtube.com/watch?v=CcnsJMMsRYk&amp;list=PLqSz8wYk5VJTsadQnU9EId6G0AJWA6o0q) - PC and upcoming console hardware - MegaTexture - virtual reality, augmented reality and Google Glass - voxel, ray tracing - AMDs virtual texturing - console cycle beyond Xbox One and PS4 - SSD - strobe lighting in LCD technology - control devices advancement - when single person can do a AAA game like MW3? [**Part 7 Q&amp;A:**](http://www.youtube.com/watch?v=-93gUpROf9o&amp;list=PLqSz8wYk5VJTsadQnU9EId6G0AJWA6o0q) - id Tech5 and Tango Gameworks
Still wondering what's Tim Sweeney doing now. 7 years after his talk on functional programming http://lambda-the-ultimate.org/node/1277 (ppt and pdf version) psedit: How does he manage to talk so long, so fluently without any long pause, humming, tongue or logical slip. 
is it on hackage yet? (in acme maybe)
This is why it's more like longjmp/setjmp.
He just has a lot to talk about and likes to ramble on. There's no speech to memorize. It just comes naturally. 
No. It's actually for people who don't want anything like bottom at all and want to use the type system to prove that some case is impossible to reach. It's most useful in a total or strict language since that would mean there are actually no values of type Void, but it still works alright in Haskell. 
They are equal in power, but one of them requires another dependency for both library and client code, while the other works with just the standard libraries. 
I understand that he's speaking his mind about things he's passionate about. It's mostly the physical side of things that surprises me, he can go through 2+ hours of continuous speech, I feel out of breath just watching him. 
I was going to say this would be easy with `MonadFix`, and then I saw `ContT r m` is not an instance of `MonadFix` for any `m`.
threepenny is badass, I always had problems with installing wx on different systems, but this just worked on mac and windows right out the box great job! 
It's my belief that the most active and still being improved ones are reactive-banana, Elerea, and Netwire.
just a random question, How could I send a post with some data from another webapp to the reactive-banana + threepenny-gui, assuming both are hosted locally and on the same domain? 
The only time I'll define a function in terms of the MonadIO typeclass is when I'm using other constraints as well, for example: printBytes :: (MonadIO m, MonadState ByteString m) =&gt; Int -&gt; m () Usually when I define a function like this I have a particular monomorphic monad in mind, but it might have a MonadReader component, or a MonadPlus instance, etc., and this type signature clearly expresses (and enforces) which effects are used. If I am writing a function that only uses IO, then I prefer defining it as such and lifting it with liftIO where it is referenced. Doing so clearly distinguishes IO computations from other values. I find that this is worth the added verbosity, but it is mostly a style choice.
This was exactly the thing that drew me to Ji, the precursor to threepenny. I wanted to toy with Haskell GUI on Windows and it was the only one I could get to work.
You also just called `forever` a hack while debating the merits of a `goto` implementation ;)
He chose to learn Haskell instead of some other functional language because of its "brutal purity". I agree and this just goes to demonstrate that Haskell is the most metal language.
This is what Tim Sweeney has been up to: N. Glew, T. Sweeney, and L. Petersen. A multivalued language with a dependent type system. In Dependently Typed Programming. ACM, Sept. 2013. 
You could take a look at this tutorial if you want something oriented around a bigger project: https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours If you already have the basic syntax down, definitely just pick a project and go for it. I would use [Real World Haskell](http://book.realworldhaskell.org/) as a reference for practical applications ([chapter 22](http://book.realworldhaskell.org/read/extended-example-web-client-programming.html) is basically about web scraping, for example), and ask for help on IRC or Stack Overflow or here when you need to. Not only have other beginners had success with this approach, but I feel like most people will basically stay beginners until they attempt some kind of largish project.
That's beyond all my expectations, thanks. Just found this http://arxiv.org/pdf/1307.5277.pdf Sweeney + Dependent Types = Impatience
Sure you did. Both `Void` and `main` are bottom. `main` is productive, but until Haskell gets a better representation of codata that distinction can't be represented in the type system. I agree that specified `Void` is not evaluated, but only because it's never reached.
Are you claiming that this `Void` is a type-level proof that the case is impossible to reach? If so, I have to disagree. In Haskell, `Void` seems to be more of a convention than a proof. In the ways it's commonly used (and I use it too), it works as one would like, however it's usually easy to subvert if one cares to try. I do agree however, that if one must throw around bottoms (which `Void` is), it's good to at least know where they are. I willingly concede the situation is very different in total languages though.
That's what I'm saying with the edit.
Just came back to this thread to post the same quote!
Have you looked at [Shpider](http://hackage.haskell.org/package/shpider)? I haven't used it myself, but it seems a lot more convenient than parsing the XML document yourself. Edit: That list of recommended libraries is a godsend. I'll consult that first when I need something from now on. It's exactly what the Haskell community needs to avoid having new people be confused about which library to use when there are 10 alternatives and only one of them is still maintained.
Not to my knowledge, but why would I put it on hackage?
It's the only way I learned. I only skimmed LYAH for examples and some good descriptions. The typclassopedia is essential and I keep re-reading it. My first project was a program that generated a ranomized tabula recta. It was a shitty program and npone on codereview.stackexchange basically re-wrote the whole thing; that's about when I learned what point-free programming was. My second project was a robots.txt parser that used acid-state, ixset, cmdargs, hslogger, ConfigFile, and attoparsec. I never finished it, but it was parsing command line options, reading a config file, requesting the robots.txt file, parsing it, saving the parse tree using acid-state, and answering "allowed" queries. I learned a lot of little implementation messiness with this one. My third project was a site parser for work; cmdargs, hslogger, configfile, aeson, and hxt. Given a URL it requested it and parsed relevant SEO on-page signals into a datastructure then rendered it to json on stdout. I didn't learn anything of significance here, except that you have to set your encoding properly when putting to stdout. My fourth project made the largest leap for me cognitively: it's a library that provides color representation conversion functions; SRGB to CIE L*ab, L*cH, L*UV, and XYZ. Along with color interpolation and tint/shade, darken/lighten. First project I used QuickCheck with and I was amazed at how many issues it caught; quickcheck is amazing. I also learned how to write my own Functor/Applicative/Monad instances with this project. I learned why using newtypes can be far better than just using a type synonym in cases where you have data with constants that are shaped the same but of different values that would have a completely different effect on a formula if accidentally passed in. This was also my first package to be released on Hackage. My fifth project I'm working on right now is a daemon that listens on a Redis queue and scrapes search engine's result pages with concurrent workers; I used Tekmo's excellent Pipes package for this and I gained a much deeper understanding of monads. This is also my first project using transformers and actually understanding them. I was also taught how to simplify my types using basic Algebra. Projects projects projects. Keep reading the papers, the blog posts, and keep practicing using projects that are relevant or interesting to you. [EDIT] The more I think about my program in terms of its types, the more it all seems to "flow" more easily; the type system is incredibly difficult to wrangle if you don't "go with the flow". I never really knew what people meant when they said that but I do now. This is more of an unconscious experience though (automatic) that you only really "get" once you've been doing it for a bit and learning what the idioms are.
I'm not sure what you mean when you say you "didn't use any bottoms". `absurd` is bottom, `vacuous` uses bottom (`absurd`). If by "use" you mean that `goto` never attempts to evaluate a bottom, sure. AFAICT there's no way to use this `goto` construct without creating a bottom (although I'd be happy to be shown otherwise). Also, I think it's pretty common to use `Void` to refer to both the type and the value inhabiting that type, since it's unambiguous.
Ah, OK, thanks! I'm a little surprised that the fix gains 10% (IIRC) overall runtime then, but I guess the inner loop does not actually run for very many iterations?
&gt; Haskell is the most metal language by coincidence, [this was written in Haskell](http://www.demoscene.tv/prod.php?id_prod=13772) (a warning, and a note, because apparently people can be strange: it may offend you, and don't take it seriously :)
But that only works because the effects you're using have type `IO a` rather than `MonadIO m =&gt; m a`, right...? This seems like an argument for why *your* functions should be `MonadIO m =&gt; m a`, while *everybody else's* should be `IO a`. :)
I propose the following rewrite in our everyday discourse: {-# RULES "functional programming" pure = trve #-}
This may depend on what you mean by "scrape", but in my mind, the most relevant library for actual scraping is http://hackage.haskell.org/package/tagsoup -- it gets you from any of those string-ish types to a structured stream of contents and start/end tags, so that you can write your actual logic. (If you know that the documents you're parsing are well-formed, though, you might go with a full-fledged parser; at that point, it's stretching the meaning of "scrape", but if you are in such a lucky position, by all means, do it the easy way!)
Also this quote at 1:48:20... "Languages talk about multi-paradigm as if it's a good thing, but multi-paradigm means you can always do the bad thing if you feel you really need to." 
There are quite a few gotos on hackage, [GotoT](http://hackage.haskell.org/packages/archive/GotoT-transformers/1.0/doc/html/Control-Monad-Trans-Goto.html) , [contstuff](http://hackage.haskell.org/packages/archive/contstuff/0.4.0/doc/html/Control-ContStuff.html#v:goto), etc The 'embedded' [Basic](http://hackage.haskell.org/packages/archive/BASIC/0.1.5.0/doc/html/Language-BASIC.html) is the most lunatic, to judge from the demo http://augustss.blogspot.com/2009/02/more-basic-not-that-anybody-should-care.html -- which, when we realize it is Haskell, seems like something from a horror movie.
html-conduit is a lenient parser with the intention of parsing probably-not-well-formed HTML. Tagsoup gives you a stream which is hard to work with, whereas html-conduit gives you an actual XML tree. For example, here's a challenge Snoyman brought up to compare tagsoup with html-conduit: &gt; Using tagsoup, how would you parse www.yesodweb.com to get all of the textual values inside of each &lt;dt&gt; tag inside of a &lt;section&gt; tag with the class "why"? Such basic queries are trivial with CSS and XPath In Tagsoup, that's kind of a mess, if you're using plain pattern matching, or if you're using the [partial] lame functions in *.Match. If you're using the Tree—i.e. expecting a _fairly_ well-formed source—then you might as well just use html-conduit and get proper XML functions. My answer to the challenge was that you define some combinators in Parsec. [Like this](http://lpaste.net/8903892191621939200). The nice thing is getting the full power of parsec, which means parsing trivial stuff like this, or complicated stuff, or broken stuff. It's a decent example of how Parsec can be used on any kind of token. I'd recommend this in the case that the HTML is really intractably messed up or untrustworthy. You also don't have to parse the whole document, you can just parse up to where you need, which is an advantage tagsoup has over html-conduit. The down-side is that it's not something you would readily recommend to newbies, like html-conduit. There's a lot of implicit knowledge that went into writing that small paste. Tagsoup is nice because it's simple and easy to understand. But its default APIs are not that powerful, in my opinion. I'm not sure about recommending it? There's also the new up and coming way to scrape, which is to use a headless webkit. Once the `webkit` library on hackage is updated, we'll be able to inspect the DOM directly from Haskell, no JavaScript required. Then the question of invalid HTML and such becomes "just let webkit do it for you", and it can do JavaScript-generated pages.
I will be giving a talk on F# next week and this quote will be quoted.
thanks i'm still learning haskell so sorry for the dumb question =) another random/dumb question would it be possible to load this into an iframe of another web app?
I didn't mean it's a real proof in haskell, just that it would be in a total or strict language. In haskell it at least suffices as an expression of intent. 
I do end up doing a bunch of copy/paste from libraries that only export IO versions of their functions and making ones that support MonadIO. 
Certainly. To the browser, it looks just like another website.
I hope to do exactly that, but it may take a while. :)
You could replace the `(Set Integer)` monoid with a Maybe Maximum Monoid.
*we introduced a novel approach to dependent typing, a language we call λא* How exactly would one pronounce that? 
"Lambda aleph."
*lambda-aleph* That is: **[lamða] /ˈɑːlɪf/**
Lambda aleph, maybe?
Haskell is Doomed to succeed. Literally.
Yes. Also it helps to eat your own dog food. I use xmonad and add functionality to it on a weekly basis. I built a system for tracking budgets and expenses, and turned it into a twilio web application that my family uses. I use haskell for scripts at work. Sometimes it helps to take off the floaties, jump into the deep end, and learn cuz your life depends on it.
lambdaleph ...
Yeah, I didn't know whether the estimate of accuracy should be based on the *least* accurate approximation accessed, or on the *most* accurate, so I chose not to make any assumptions.
is ghc abandonware?
I tend to export functions with a MonadIO constraint... whenever it doesn't have to take an IO-like action in negative position (as an argument). When my code does have to take another monadic action as an argument, then I usually have to stop and think about it.
That only works if you never accept an IO action as an argument.
Will he make it open source eventually? I would like to see how he writes Haskell code.
I can already feel the Haskell Quakes.
no
Haskell is all the Rage these days.
Can someone answer this guy's question already?
The basic type constructors have an algebraic interpretation: () = 1 Either a b = a + b (a,b) = a*b a -&gt; b = b^a the intuition as to why these are right can be that it is the "number of inhabitants" and a type that is polymorphic in an argument is a function of a variable Maybe a = a + 1 while recursive types are more interesting equations List a = 1 + a*List a all the regular rules of algebra apply and are just isomorphisms 2^2 = 2*2 = 2+2 so Bool -&gt; Bool = (Bool,Bool) = Either Bool Bool as witnessed by funToTuple f = (f True, f False) tupleToFun (a,b) x = if x then a else b tupleToEither (True,x) = Left x tupleToEither (False,x) = Right x eitherToTuple Left x = (True,x) eitherToTuple Right x = (False,x) it actually is cooler than this: you can take the derivative of a type and you get a zipper...
This is vaguely familiar and really cool. I need to think about this more often!
Because Carmack is so Keen on it.
Its a good discussion to be had. I'm also no designer, and have few opinions on the topic. But I understand others care about these things :-P
Another definition for Void that I think makes its properties more obvious: newtype Void = Void { absurd :: forall a. a }
"you can't expect a result when applying a function to bottom" - that certainly isn't true in Haskell, but I understand what you mean. One of Conor McBride's papers has a very nice quote about this, something like "you can't object to my shoddy merchandise if you're paying with imaginary currency." I'm not entirely sure I understand your argument that `absurd` isn't bottom. Perhaps it's that I am taking some liberties with terminology (this is reddit after all) and by `absurd` mean the result of applying absurd to an argument, and you were taking me to mean literally the value bound to the name `absurd`. If this is the case I think we're in agreement. If you mean something else, I'm pretty sure I don't understand you. Thanks for this example; it works fine. Seems obvious really, not sure how I missed it. However, I maintain you can accomplish the same thing without using `Void` at all. goto :: LabelT r m -&gt; ContT r m a goto (LabelT l) = fmap (const undefined) l data LabelT r m = LabelT (ContT r m ()) or even data LabelT r m = forall x. LabelT (ContT r m x) I agree `Void` is much nicer if you're doing any sort of reasoning, but operationally it's the same.
I'll take this opportunity to thank the people who do hard work and spend their time to be able to contribute to GHC. The project is well above my sorcery level, but I am grateful for the work being done. I'm not sure if I even would be using Haskell were it not for GHC, which really is a glorious and impressive compiler. Keep up the good work!
Does [Helm](http://helm-engine.org/) need to be added to this list?
I can do something *like* this easily: just represent the "GoTo" abstract syntax as a data type, and perform static analysis to determine the labels. main = usesGoto $ do goto "abc" lift $ putStrLn "Don't print me" label "abc" return () The catch: it's not a real monad transformer, otherwise you would be able to make labels dynamically, and and that would prevent static analysis of the labels! I'm sure there is a MonadFix version of this that could be written... but it already feels like an awful hack to use `do` syntax for this. It would be much simpler to just express it as a monoid and concatenate programs with `&lt;&gt;`. https://pastebin.com/B7VJ5iLE
Tried, 403 too unless I change the User-Agent (I put some random browser acronym like "IE") and it worked just fine.
You could try [stackoverflow code review](http://codereview.stackexchange.com/). It is usually good for reviews in haskell.
The smart render sounds quite useful. If eddie doesn't do that, then I won't even bother trying it: I'll stick with yours! I also see you took extra efforts to make the json more convenient. Maybe you should document this new convenient interface, though? I figured out what `tI` and `tS` are for, but I have not yet figured out what kind of commands are available in your custom "drill" format. Lastly, I don't think the just-in-time compilation is actually an advantage. Since the tool is used from the command-line, I think it's more important to let the user quickly try and edit his command until he gets it right than to run fast on large inputs. The docs for eddie mentions `ghc -e`, which I didn't know about, and which is much much faster than HSL. Maybe there might be a way to run the user's command over ghci instead of ghc?
That instance doesn't type check since `Bad /= Ordering`: Prelude&gt; :t compare compare :: Ord a =&gt; a -&gt; a -&gt; Ordering And those uses of `(&gt;&gt;=)` don't type check either...
Excellent! It's always bothered me a little to know that SPJ would eventually stop maintaining GHC and that there was nothing really in place to take over. This is good in the short term, because more work can be done faster, and good in the long term because there is less fear of a Cambridge Bus Error. 
The blog post was mainly to help people get a quick overview of Haskell. I'm no expert (quite far from that actually), but I hope I got the basic parts down. Criticism highly encouraged! Thanks for reading.
Helm is built on elerea, so, no, I don't think so.
א, called [aleph](https://en.wikipedia.org/wiki/Aleph), is the first letter of the Hebrew alphabet. I assume you know what lambda is.
How about sequential design http://i.imgur.com/6VbuLtV.png ? It's rough mock up, ofcourse, shadows and gradients can be added later -)
For reference, there is also the python.org redesign. http://preview.python.org/
But the `Partial`s aren’t the same—one talks about definedness of a value, while another talks about the side effects of divergence and throwing.
You may want to take a look at [this rambling](http://winterkoninkje.dreamwidth.org/86236.html) which generalizes the commutativity case. You might be able to use a similar trick for generalizing associativity.
Helm is in the FRP section of Hackage, but yeah I would call it a framework written with Elerea, not really an independent FRP library. I am considering learning Elerea though, mostly so I could contribute to a project with such a sweet logo.
If you're interested, Chris Taylor wrote a series of [blog posts](http://chris-taylor.github.io/blog/2013/02/10/the-algebra-of-algebraic-data-types/) on this. 
For what it's worth, I thought Haskell was a very difficult, cryptic puzzle until I internalized monads and functors. They are *uncomplicated* but very *unintuitive*, and they are absolutely critical to understanding anything that is not a small toy. Once you get functor and monad, the ecosystem should open right up. You should be ready to grab random hackage libraries and start gluing them together.
`$` does have some uses outside of just being a convenient replacement for brackets. It's used whenever you need to programatically apply functions: &gt; let functions = [(+ 3), (/ 2), (* 10)] &gt; let values = [6.0, 4.0, 3.6] &gt; map ($ 3) functions &gt; zipWith ($) functions values
&gt; The project is well above my sorcery level Time to work on leveling up! ;)
I like the sectioning. I find the Haskell symbols to be a bit cheesy though. I'd rename the sections to just: Learn, Use, Converse, Attend. Something like that.
For a tree of depth 16, not many comparisons indeed.
The content presentation definitely still needs to be simplified and cleaned up, but I do like the general idea, especially the big fat "download" button.
Ugh, guys, days of "business card" web sites are in the past. I personally think that the new Scala web site redesign that kicked off this discussion is ugly and sooo nineties. 
Writing on my phone so excuse any typos... data Alg = Alg { h :: a -&gt; b, z :: a -&gt; n } b^a * n^a = (b * n)^a Therefore, data Alg = Alg { h :: b, z :: n } type MyA = a -&gt; Alg Algebra of types series is woth reading. Tekmo closed the gap for me in actually using algebra to simplify the types though.
I have just written a barebone mpd client with threepenny. https://gist.github.com/rnons/6145437 I am going to update it with reactive-banana. However, I'm not sure what's the benefit I will get with the reactive-banana wrapper to threepenny. I have read some example code in threepenny and reactive-banana-threepenny repo. The reative-banana-threepenny version actually needs more code than threepenny version! CLAIM: I have no experience with FRP.
Of course, if you're locked into a single paradigm, you can't do the good things that other paradigms can do better than yours.
Sorry, but I have to say that to me it is harder the locate the download button in the mockup than in the current webpage.
The problem is not everything can take arbitrary MonadIO instances that way. Try to bracket, catch, deal with the fact that MonadIO might actually capture a continuation and jump back in, fork a thread... There are libraries that try to do this with varying degrees of success and limitations, but they need more than just `MonadIO m`. It is quite tricky to deal with a generic version in negative position correctly.
I suggest running HLint over the code - I saw at least one hint tha would be a minor improvement by eye. HLint is not a substitute for code review, but a good precursor. 
Ok, then do you like the current layout, or do you have some new ideas?
Your `applyAndConcat` example has a type error, because `head :: String -&gt; Char` but `applyAndConcat` expects a `String -&gt; String` there. I suggest replacing the example with applyAndConcat init tail "Rickon" = (init "Rickon") ++ (tail "Rickon") = "Ricko" ++ "ickon" = "Rickoickon" 
Your fibonacci `case ... of ...` example is wrong. (Edit: “wrong” is perhaps a bit strong --- it does compile and do the right thing, after all --- but certainly it is unidiomatic and I think misleading.) It should be fib n = case n of 0 -&gt; 1 1 -&gt; 1 _ -&gt; fib (n - 1) + fib (n - 2) (n.b. `case ... of ...` is not “extremely similar to pattern matching”: it *is* pattern matching.)
For simple things, the standard event-driven model used by threepenny is perfectly fine. For more complicated things like the CRUD example, FRP makes it easier to get all state updates correct because it avoids the spaghetti-code problem. In the future, I intend to integrate reactive-banana and threepenny more tightly, so that FRP is a convenient default choice even for simple examples.
 fromJust :: Maybe a -&gt; a fromJust Nothing = error "Maybe.fromJust: Nothing" fromJust (Just x) = x Part of the point of `Maybe` is that it provides a way of avoiding partial functions, so providing a partial function as an example of using it is unhelpful. I suggest instead presenting `fromMaybe`: fromMaybe :: a -&gt; Maybe a -&gt; a fromMaybe x Nothing = x fromMaybe _ (Just y) = y 
You're getting confused because you're thinking of truth conditions instead of proofs. The correspondence between Haskell and logic isn't based on truth values and truth functions in classical logic, but rather on proof, and intuitionistic (not classical) proof at that. Other than that, I second vituscze's answer.
A while ago I played around with goto and found this rather terse definition: getLabel = ContT fix goto label = lift label Does what you expect, though of course it can also only jump backwards.
[There are points that need to be agreed upon by the community](http://chrisdone.com/posts/haskell-homepage-thoughts). Put in a post so I can refer to it later.
I have to say that even if GHC were in the hands of a dozen alternative expert engineers, I would still be concerned about that Cambridge Bus Error. The human side of things...
Ooops! Fixed now, thanks.
This objection is totally valid. I had considered this possibility, but I think that in this particular case, convenience trumps correctness, because Ord is unlikely to be wrong. I, at least, always use `deriving Ord`. Still, thank you for making the assumption explicit, I'll have to add this to the documentation once I switch to winterkoninkje's version of `distinguish`.
As a matter of fact, Haskell was not the *first* language the students learned, but the *second*. Federico says clearly that he first taught Gobstones, and *then* Haskell... (I may even say that Haskell is their *third* language, being Scratch the *first*) Haskell makes a good first language when students already have some abstraction ideas. But what happens when they not? I think that using Haskell for teaching is great (I use it as the first language in my course on Data Structures in the university -- where they already learned Gobstones -- so in a way I also use it as a second language for my students), but a prior exposure to some environment where students may link their abstractions to concrete things is a good beginning! That's what Gobstones is for us at University of Quilmes.
I always thought the ocean theme (grey, blue and orange) was the new official. [hackage2](http://new-hackage.haskell.org/) uses it.
The first thing I do on a site with a big header and lots of text down the page is scroll a nudge. Most sites put the download button beneath the intro-text and above the site-map/feature overview.
Actually, what I would like to see, is a quick tour of all the &gt;&gt; $ . =&lt;&lt; -&gt; &lt;&gt; &lt;$&gt;. They're ungooglable, but hooglable.
I think you're thinking that I meant something like: catch :: forall m. MonadIO m =&gt; m a -&gt; (e -&gt; m a) -&gt; IO a What I really mean was: catch :: (forall m. MonadIO m =&gt; m a) -&gt; (forall m. MonadIO m =&gt; e -&gt; m a) -&gt; IO a The latter should be perfectly equivalent to the standard signature for catch.
Are there good resources we could be pointed to on how to begin contributing and general how-GHC-development-works context? With the team as small as it apparently is, I'm sure there is space for even non-experts to contribute.
They *are* the same; how else could a value be undefined but by diverging?
I'm okay with this. The Haskell web pages that I look at the most are by far hackage pages. I now associate that colour theme with Haskell.
I've updated the page with a link to Bootstrap themed with [the Ocean theme](http://tryhaskell.org/ocean/).
I just don't understand the platform's flower.
It would be nice to have the google analytics information from the homepage dumped somewhere, because then we could guess what people reaching the page are looking for, and where they go onwards. Then we would know for which audience we have to cater more.
Available in Hackage: http://hackage.haskell.org/package/processing-1.0.0.0
I think it will be good to define 5-10 blueish themes (since peple associate haskell with grey-blue), for example, like on https://kuler.adobe.com/blue-art-colors/ and make a poll for 2 weeks. Same thing for other controversial topics (it means scope should be defined). Also, good idea about user intentions. Since Haskell already an environment there should be logical structures. Main page should be for newcomers and link to other subsites, something like dev.haskell.org for documentations, tutorials, news; biz.haskell.org for use cases, and related info. 
Thanks. I ran HLint and it gave me one suggestion to replace (not $ ... == ...) to (... /= ..). 
Would [this variant](http://i.imgur.com/d1YkWNq.png) be ok? Because for that, I may be able to squeeze an hour or two tomorrow.
The dark blue bar should be for navigation at the top, not the search result display. Otherwise that seems good.
I'll go along with the Ocean theme if people are generally in agreement on it. Let's make a Wiki page to coordinate this effort.
That looks excellent.
The logo should probably be of another colour in addition to the blue applying to the navigation on top.
It does look better [this way](http://i.imgur.com/z5ktP2r.png), although as another commenter said the logo could also be of another colour...
Some thoughts: If we are trying to project a unified and professional veneer, then hoogle and hayoo should probably be altered to no longer be jokes about the internet of the 90's(as much as I enjoy the old switcheroo). The header and logo on Planet Haskell could change to match haskell.org. Additionally, it would be nice to add syntax highlighting or at the very least indent and block the code in the posts, there are plenty of libraries that can handle the highlighting, and an indentation and background switch is 2 lines in the css. pre{background-color:#ddd;padding-left:20px;} or something like that. The flowers on Haskell platform's website don't really make a whole lot of sense in context. The haskell platform logo with batteries uses two different styles of visual language. If we are to include batteries then they need to match the aesthetic set up by the box. Flattening them and changing the palette from gray-scale to the existing two-tone should make the image seem more whole. I don't really think that Haskell news really needs to change, maybe a logo swap, but beyond that it is just a feed and people understand what those are. Tryhaskell and lpaste have a unified theme that matches the "old" logo. They look fine to my eye, but they are out of sync with the other main sites. Hackage1 does not have a palette, but hackage 2 matches the wiki+homepage+haddocks. The language report could probably use a better section navigation interface than the one it has(collapsible by section maybe), but I don't know if in needs a theme. It is pretty much in line with what I would expect from a language report. I am interested to hear others' thoughts on this, the Haskell community seems to not care about bikeshedding websites, which I like, but Chris is right about things having become a little chaotic over the years. 
[Updated](http://i.imgur.com/z5ktP2r.png). Although the more I look at it, the actual logo doesn't necessarily need to of another colour, I certainly like it this way.
Nod, however this is a solid job. The logo can always be altered later.
Appreciate the effort, but this deviates from the [theme and layout](http://www.haskell.org/haskellwiki/Brand). Let's not do that.
[hoogle and hayoo palette switches](http://www.fallsoftware.com/haskell/haskell.png) Does anyone have a preference among those? I forgot to flip the lambda on a couple, apologies. I used the palette Chris just uploaded. I exported as png, but they are all svg.
Top-left dark background seems good! Presumably that will fit snug inside the navbar without bringing over 2em~. Would also get behind the top-left one, too, placed below the nav.
It comes from when we didn't know what to put on the layout. I suggested making it season-related, seems to have stuck. But it seems better to bring it under the Haskell umbrella now.
i did not write it. i would have chosen angular. but it's cool nevertheless :D.
I've always loved the fact that the Haskell platform page regularly changes the color palette with the season and I really like the way it looks right now with the flower.
I think we can still do that _and_ have the Haskell nav at the top to bring it under the brand. It would be good to get access to that. The design is mine, so I'm attached to it, but it's unnecessarily isolated from the rest of Haskell stuff.
Man, that ocean theme is sexy. Definitely have my support for getting that more widespread (I think it's already on Hackage?)
I like [Archlinux](https://www.archlinux.org/) home page. Very functional. It has a simple description of archlinux for newcomers. But it also has package search, recent updates, latest news, all there. It is a living community you want to stay with and you feel welcome. The problem with all these colorful facade web sites is that you have no clue if there are people out there, or is it an abandoned place. Presentation material takes too much of the central space, pushing interesting and important content way down. And the content itself is easy to miss, often just one line with no date no nothing.
Oh, I must have missed that memo! Thanks.
The problem with that is that if all you know about your negative position `m`'s is that is an instance of `MonadIO` then you have literally no extra operations you can perform on it over and above what you can do with `IO`. This gives you the worst of both worlds. You pay for the dictionary passing, but literally can't use any other operations in your monad than you could use before when the arg was just of type `IO`. `(forall a. MonadIO m =&gt; m a)` is isomorphic to `IO a` witnessed by the fact that I can choose IO for the instance one way and liftIO the other. Then to add insult to injury you've returned the final result in `IO`, the one place where MonadIO would actually help your users. As it is to call your code, I have to `liftIO` all my arguments to it and `liftIO` the final result. This is worse than if the code had been restricted to `IO` in the first place, at least there I'd only have to `liftIO` the final result.
The version that makes sense to me limited to the powers of `MonadIO` is to say something like catch :: (Exception e, MonadIO m) =&gt; IO a -&gt; (e -&gt; IO a) -&gt; m a but in practice we have packages like my `exceptions` package or `MonadCatchIO-transformers` that provide more powerful variants on `catch` that can be used in the presence of `StateT`, `ReaderT` and other transformers.
Why is isolation a bad thing? I still don't understand why we need a uniform color palette for every single Haskell site. To me that seems counterproductive because it is suggestive of inbreeding.
I find it difficult to formulate an answer under the accusation of “inbreeding”. I don't know what that's supposed to mean or when consistency becomes “inbreeding”. Can you rephrase?
Haskell’s `undefined` relies on laziness—instead of a value, you have a thunk with the side effect of throwing an exception. In that case there is no difference. However, in a strict language with `undefined`, you add the `undefined` value to all types, and there is a difference between the effect and its result.
Agreed. The only reason I put `Partial` around the function and as part of its argument is because I was assuming a lazy context. Such a function in a strict context is just `forall a. a -&gt; Partial a`.
I don't think that analogy works here. Most of the sites that are being discussed are utility and ecosystem sites, that assume a certain familiarity with haskell. Hackage, hoogle, lpaste, haskell wiki, haskell platform, and the language report never say "haskell is awesome, take it from me". The thrust of most of them is "here is a tool" and "here is what it does". And I don't think that uniformity will in any way hurt that message. I feel like many of the sites in question are eclectic simply because there was no standard when they were conceived, or that design wasn't as much of a priority as functionality when they were written. FPComplete will do its own thing regardless, they clearly have their own vision for what they want their ecosystem to look like.
EDIT: Had to add ghc-prim to Build-depends in processing.cabal. I swear I tried that and it didn't work earlier :/ I'm getting the following error when trying to install: Graphics/Web/Processing/Mid/CustomVar.hs:57:8: Could not find module `GHC.Generics' It is a member of the hidden package `ghc-prim'. Perhaps you need to add `ghc-prim' to the build-depends in your .cabal file. Use -v to see a list of the files searched for. cabal: Error: some packages failed to install: processing-1.0.0.0 failed during the building phase. The exception was: ExitFailure 1 Cabal doesn't see a ghc-prim package (it seems not to be installed so I can't just expose it). I get the same errors using cabal or cabal-dev. Is this something stupid I'm doing? Googling hasn't been too helpful yet.
What is your GHC version? Is smaller than 7.6?
7.4.1
&gt; Think of it from a psychological standpoint: are you more likely to believe 100 people telling you Haskell is awesome, or one person telling you Haskell is awesome 100 times. Try Haskell and Haskell.org are the only marketing sites. The sites that consist of individuals are blogs, and they are all different. &gt; When you create a uniform look and feel to all these Haskell sites, it feels like the same person or small group of people over and over again, whether or not there really is a large community of diverse people behind all of it. I'd say when you have a set of services that come under the same feel, your showing that you can work together and agree on things to make a coherent product. It's impressive. &gt; Also, would you consider rebranding FPComplete's site to use the same color palette and design? Think about that. I did consider it. I think it makes sense in theory. It would be like Sun's relationship with Java. It would be quite cool to go to haskell.org and see links to a fully integrated IDE with instant deployment, wouldn't it? 
I couldn't find something that documents the change, but for some reason ghc-prim was needed in the build-depends field of the cabal file before the 7.6 GHC branch, but it is not later (can someone post a link to any relevant announce for this change?). I have submitted a patch that should solve this issue.
thanks man. The hoogle results aren't bad either, but sometimes a little more explanation would help. Sometimes you just need someone to explain what an Arrow is in order to understand &gt;&gt;&gt;. 
Ahh, sadly I never realized it was seasonal. I like the page broadly, but feel the flower doesn't really fit with the Haskell professional brand so whenever I saw it I kind of wondered.
Though the explicitly recursive fibonacci function is generally about as unidiomatic as bubble sort. Terribly inefficient algorithms we still manage to teach people instead of their efficient alternatives.
Gosh, this is embarrassing. Thanks and corrected.
You're right - I forgot about that. I changed the wording a little. Thanks.
The wiki has a bunch of [commentary](http://ghc.haskell.org/trac/ghc/wiki/Commentary) on the process side of things. It must not be easy enough to find because your question is a pretty common question despite the info being available. In my opinion, GHC may very well be the best documented open source project. The Linux kernel has more books written about it, but GHC has many papers that articulate both theory and intuition. As well as commentary in the source and on the wiki that help explain the practice. I do know of one book that covers the design of GHC at a high level, ["The Architecture of Open Source Applications"](http://www.aosabook.org/en/index.html). I don't contribute to GHC yet, but I've been slowly working in that direction. I think the two most important things I've done to inch me in the right direction are: * Reading the GHC source via the [github source code view](https://github.com/ghc/ghc). * Playing with the ideas presented in [SPJ's papers](http://research.microsoft.com/en-us/um/people/simonpj/papers/papers.html). For example, I wanted to understand how to make a new backend for ghc. So I started reading papers about the intermediate representation that GHC uses and also I started doing the exercises in this [book](http://research.microsoft.com/en-us/um/people/simonpj/papers/pj-lester-book/). Of course that doesn't mean the road to "leveling up so you can contribute to GHC" is an easy one. And the path will be different for everyone. Something that is still lacking (as far as I know) is some sort of "GHC hacker mentorship". Where an experienced GHC hacker helps you find your way.
On your blog, the link is borked. It points to "http://chrisdone.com/posts/www.haskell.org/haskellwiki/Brand".
And I think there is a problem with your tree definition too. Tree is not a concrete type so it can't be used on the right hand side, you have to use Tree a. The definition should be data Tree a = Empty | Node (Tree a) a (Tree a) Same problem is also present in defnition using records, it should be data Tree a = Empty | Node { left :: Tree a, value :: a, right :: Tree a}
Have a look at [azeeem's comment about your `Tree`](http://www.reddit.com/r/haskell/comments/1jlpnu/a_quick_tour_of_haskell_syntax/cbgbini), if Reddit hasn't already drawn it to your attention.
Any non-zero cardinal number! Wow, when you generalize, you're not kidding around. The condition you gave for `f` is precisely the one I need for aspects: ∀a,a'∈A, c∈C. f a (f a' c) = f a' (f a c) When `f` has that property, let's say that `f` "commutes". I was only interested in commutativity and associativity because when `f` commutative ∧ `f` associative implies that `f` commutes: f (a (f b c) = a `f` b `f` c = b `f` a `f` c = f b (f a c) Of course, this only typechecks when `f :: a -&gt; a -&gt; a`, whereas your version only requires `f :: a -&gt; b -&gt; b`. Interestingly, when `f` has a right-identity element `i`, the implication also goes the other way: f a b = f a (f b i) = f b (f a i) = f b a a `f` (b `f` c) = f a (f b (f c i)) = f (c (f a (f b i))) = c `f` (a `f` b) = (a `f` b) `f` c I still haven't wrapped my head around your post, but I plan to read it very carefully, hunting for clues about how to implement intrinsically-commuting functions. By using a (≤k)-set, I guess?
Thanks.
Lambda expressions don't always need to be surrounded by parentheses.
Man, that looks really good. All the example pages feel instantly Haskell-y to me. Applying this to Haskell News should be 10 minutes of work right?
Nor do the LHS and RHS of operators, even if they are expressions and not just some variable name. so `(f x) : (map f xs)` = `f x : map f xs`
Cloning GHC... :)
This was intentional. For people new to Haskell, that might seem a little bit weird, so I opted to add the parentheses for clarity.
&gt; Probably the biggest waste of time in my entire life. Seems a bit extreme to claim this.
&gt; Wow, when you generalize, you're not kidding around. To be fair, I'm just assuming it works for the larger cardinals :) Presumably the tuple/fold representation will need some extra components to deal with cardinals arising from limit ordinals. &gt; When f has that property, let's say that f "commutes". Indeed. We can think of `f` as an action of `A` on `C`, and that axiom states that the action is commutative. The other axiom (about `f` and `h`) is also a sort of commutativity, which becomes an instance of the former whenever we can come up with some `c0` such that `h a = f a c0`. However, if we have such a `c0` this means that we can prove `C` even if we have no proof of `A` (via: `g c0`), hence it implicitly means we can apply the commutative operator to an empty set of arguments. Which may or may not be desirable, depending on the circumstances. As you noticed, this generalized notion of commutativity gives a certain sort of associativity. However, it doesn't give real associativity. For example, let `F` be a (&lt;κ)-commutative operator from `A` to `A`. Now the expression `F {xs, F ys}` is well-typed, since `F ys : A` and therefore we can pass it as an argument to `F`. Because `F` is commutative it doesn't matter what order we present the elements of `ys`, nor what order we present the elements of `{xs, F ys}`. However, we can't rearrange things to move elements of `ys` up into the outer set, nor can we move elements of `xs` down into the inner set. Thus, we can think of the term structure as being a (&lt;κ)-ary branching tree. Commutativity on (&lt;κ)-ary branching trees means it's more like a mobile where the branches can swing around freely, but we still can't change the hierarchical structure of the tree. The apparent associativity comes from the fact that we can convert n-ary trees into binary trees, but the laws require that all such binarizations are equivalent.
Okay then, how about this one &gt;;) distinguish :: Ord r =&gt; (a -&gt; r) -&gt; Commutative a (Either r (a,a)) distinguish f = Commutative $ \ x y -&gt; let fx = f x fy = f y in case (compare fx fy, compare fy fx) of (LT,GT) -&gt; Right (x,y) (EQ,EQ) -&gt; Left fx (GT,LT) -&gt; Right (y,x) _ -&gt; error "distinguish: the Ord instance is a lie!"
At the cost of calling `compare` twice, I think [this version](http://www.reddit.com/r/haskell/comments/1jegxb/the_commutative_monad/cbggker) guarantees correctness. Of course, ideally we'd express the possibility of failure in the type; but since we tend to assume `Ord` instances are well-defined, doing so would introduce quite a burden for very little gain.
The article itself is arguably a far stronger contender for that title...
I just glanced at it, but thanks!
I admit to liking this better.
While it's always good to have more interop, I wonder what the intended use case of this is. Anything you can do in Python you can surely do in Haskell, and porting any useful Python libraries with this method sounds like it would be a lot of work, to bridge the two very different API styles.
What about the other way around: porting Haskell libraries to Python? Right now I'm using Python's `pika` bindings to AMQP and I really wish I could use Haskell's `amqp` library instead.
If you RTFRM (read the frikken read me) you'll see this is to allow calling Haskell from Python. Still I have the same questions ... maybe people who grew up on Haskell need this but for native Pythonistas I doubt they would care about Haskell libraries as the standard Python library and eco system is so vast ... and if you have a need for speed then using a c or cuda or opencl library would easily fit the bill. For Haskellites all the beauty of Haskell's purity and type safety goes out the window once you step foot in Python land so ... Anyway ... sorry to be myopic; I'm sure it's useful to the people who wrote it and it's very generous of them to open source. I'll shut up now. 
I have updated it with reactive-banana, but I ended up using fromPoll. I find the document says "[fromPoll] is occasionally useful, but the recommended way to obtain Behaviors is by using fromChanges.". I can't find an example using fromChanges, can you show me how? Thank you very much. EDIT: I think I have figured it out, cheers.
I'm happy with that, although the Logo does need changing - I've no particular love for the current one.
While both Haskell and Python have a rich set of general-puropse libraries to work with, in real life you have pre-existing application-specific libraries and programs that need to work with each other. When those happen to be written in two different languages, bridges like this are a lifesaver. Building a fully functional cross-language bridge for two languages that are very different from each other is not at all an easy task. So the fact that there are two such Haskell-Python bridges - HaPy and MissingPy - is a hint that this is something very useful.
Excellent post Jason, thanks! EDIT: and very timely given the process of [change at GHC HQ](http://www.reddit.com/r/haskell/comments/1jky60/spj_ghcs_glorious_future/).
[haskellers.com](http://haskellers.com/) is conspicuously missing from the list of targeted sites.
And Jason just posted this great [Getting Started guide](http://dagit.github.io/posts/2013-08-03-getting-started-with-ghc-hacking.html).
Hmm. This tool looks like it would mostly be useful for text. But it deals with [ByteStrings](http://hackage.haskell.org/package/bytestring), which are designed for binary goo. Why doesn't it use [Text](http://hackage.haskell.org/package/text)?
Good point. 
Ironically it was me who suggested those colors to snoyberg, so I take responsibility for that. At any rate, it's good that pretty much everyone seems onboard with the Ocean theme and applying it.
Thanks for the suggestions! Since the library is still a moving target, I don't mind if people have to find their own way around the documentation, but I've added a note in the issue tracker for later.
Why fold f xs = case f of Fold t c -&gt; c (foldl' mappend mempty (map t xs)) mf &lt;*&gt; mx = case mf of Fold tL cL -&gt; case mx of Fold tR cR -&gt; let t x = (tL x :!: tR x) c (wL :!: wR) = (cL wL) (cR wR) in Fold t c instead of fold (Fold t c) xs = c (foldl' mappend mempty (map t xs)) Fold tL cL &lt;*&gt; Fold tR cR = let t x = (tL x :!: tR x) c (wL :!: wR) = (cL wL) (cR wR) in Fold t c ? Is it a matter of style, or is there an important difference between the two?
I was coming here to ask the same thing. My guess is that with the explicit `case`s, there's more forcing of the input values, though I would have that the alternative you proposed would result in the same thing in this case. Great post Gabriel, this is a really nice abstraction!
There's no difference in forcing here, as far as I can tell.
They are exactly the same.
Right... I know I should be using it in theory. The answer is that I probably haven't had to deal with enough unicode data for this to be an issue. Eg, ByteString doesn't know how to reverse "☺♫☂٩◔̯◔۶", where Text does. Am I missing something? I'll probably port it soonish, ideally to use strict Text.
A very similar thing has been done before: http://squing.blogspot.nl/2008/11/beautiful-folding.html (The `Applicative` instance is given in the comments.) The only difference is that was in `foldr` style, while this is in (the imho nicer) `foldMap` style. 
I love this kind of tutorial. It helped me a lot. Thank you very much.
Just wondering, how does it actually work? Are you compiling Haskell code to JS or what? Is it possible to access things from the browser environment, as you can with JS?
UNSW is in the southern hemisphere and there are a number of people with commits to GHC there (manuel chakravarty, ben lippmeier, gabi keller, etc.)
I should emphasise that we (Well-Typed) are fully committed to GHC and helping SPJ with GHC. Here's our status update on the issue: http://www.well-typed.com/blog/81
Conal wrote up ZipFold at one point, but I don't know if it's still usable.
Anything that starts to help bridge Haskell to the scientific Python world I think is a good step. There's about 10 years of community development on the SciPy ecosystem that Haskell doesn't have yet ( no reason it couldn't though ). On top of that most of the functions that libraries like SciPy and NumPy expose are pure functions and psuedo-typed. Unboxed Haskell arrays can be made ABI compatible with NumPy arrays by embedding a Python interpreter in Haskell via the FFI and so you can call out to the plethora of functions in SciPy like libraries and still use Haskell for the main logic. 
Yeah, the whole reason I wrote this post is that I plan to write this up into a library but I wanted to run it by you all first to make sure there were no obvious flaws or potential improvements that I missed.
Worth remarking that Edwardk has a lib called multipass that does a neat generalization of this that allows multiple passes when needed. Worth checking out, it's on hackage. I was meaning to spend some time helping clean it up, but I've been busy with a few other higher priority projects edit: heres a link http://hackage.haskell.org/package/multipass
I don't understand. Why is my fork of your repo getting so many more stars than yours? And it's my most popular repo yet, too! Clearly, your tool is filling a need.
Yeah, I tried it. But it looked hideous. I'm sorry to say I cannot do the red links. I hate this theme (which I don't remember anyone agreeing on, but was just implemented) but I was willing to compromise and put up with it for the sake of working together. Even the bad shade of blue and random border at the top of the nav and the depressing grey logo. But the red links, I just can't do it. I put up with reading it on Hackage and HaskellWiki because I have no choice, but not here. It doesn't look like anyone's going to give me access to Haskell.org to tidy up the messy layout anyway, stonewalled as usual, so I'm going to count my losses and go back to doing something productive.
Neat, first time I've heard of idiom brackets. Are you referring to this library? http://hackage.haskell.org/package/applicative-quoters
Haha, so it is. Weird.
Update, uses Text now.
Idiom brackets were (I think) introduced in the same paper that introduced applicatives generally. It's worth a read through.
This reminds me of a discussion I once had on Haskell Café about how the idea to use applicative syntax also works for transforming, not only for consuming, lists. Here is [some code](https://gist.github.com/sebfisch/1521467) for the discussion which can be found [here](https://groups.google.com/forum/m/#!topic/haskell-cafe/nwTOGZUXVS0) and [here](https://groups.google.com/forum/m/#!topic/haskell-cafe/ReCN1dnxEMU). I realize there has been made quite some progress regarding stream processing since back then. I wonder how the lockstep computation of incremental list transformers in applicative style compares to what can be done using pipes today (but I am not sufficiently familiar with the latter to judge). 
pattern matching in lambda is just the same as an explicit case (unlike a pattern matching in let which is irrefutable). This is how the standard idiom of matching GADTs in do notation works (which makes really pretty type safe interpreters, imo) since `do` desugars down to lambdas.
You could also lift the `num` into the applicative. instance (Num a) =&gt; Num (Fold b a) where fromInteger = pure . fromInteger (+) = liftA2 (+) (*) = liftA2 (*) etc then you get fold (sum / length) xs this trick makes many uses of idiom brackets superfluous 
Can someone explain the high-level difference between the internal representation in ghc vs the Intel research compiler? 
The way I arrived at this was from simplifying the `pipes`-based equivalent. I wanted to make it `pipes`-independent so that other libraries could use it, too.
[another version](http://img821.imageshack.us/img821/8110/vgd.png)
No magic involved. Each function calls to an internal writer monad which adds some code to a Processing syntax tree. At the end, it returns the final syntax tree. The point is to make it happen internally, so you code from a clean (and typed) Haskell API.
"If I were to awaken after having slept for a thousand years, my first question would be: Has the Riemann hypothesis been proven?" - David Hilbert
Because of wibbly-wobbly-timey-wimey reasons.
[Another version](http://i.imgur.com/6DwTqcG.jpg).
I benchmarked both and examined core and the version linked in that blog post is much more efficient and generates the same core as the hand-written fold, whereas the `Monoid`-based version does not. Also, somebody on the `haskell-pipes` mailing list pointed out that the `Monoid`-based version doesn't permit a general way to convert strict left folds, whereas the version in your linked post does. So I'm switching to the `foldl` style, which is a big performance win.
The pictures are very good. 
I wonder if you get the same results if you replace your `foldl' mappend mempty (map t xs)` to `foldMap t xs`, and also try this code on other datatypes than lists.
I think [this](http://strictlypositive.org/Idiom.pdf) is the paper you're referring to.
So, does it support arrays of the said types? 
Have you seen the implementation of `foldl` in terms of `foldMap` in the source of `Data.Foldable`? Also it has an implementation of `foldl'` in terms of `foldr`, which has an implementation in terms of `foldMap`. So I think your original data type should be able to handle everything. And it can be faster for datatypes that have a tuned `foldMap` implementation.
Hahaha! My apologies. My intention was never to be incomprehensible.
Just needs a better font.
Absolutely! (iPhones, ugh, just don't do start.)
I did check it out, but I mean something different. What I want in principle is a function of type: fromLeftFold :: (x -&gt; a -&gt; x) -&gt; x -&gt; (x -&gt; b) -&gt; Fold a b ... with the condition that the generated fold still runs in constant memory. I can't figure out a suitable `Monoid` to use that has this property. The only fold I know how to encode using a `Monoid` is a right fold, using `Endo`, but that doesn't have the property of running in constant memory.
You can easily reverse the order of a `foldMap` by just using [`Dual` from `Data.Monoid`](http://hackage.haskell.org/packages/archive/base/4.6.0.1/doc/html/Data-Monoid.html#t:Dual). I don't know if that has any favorable space characteristics, though. EDIT: You may also want to take a look at the source for [`fmlist`](http://hackage.haskell.org/packages/archive/fmlist/0.8/doc/html/src/Data-FMList.html), if you haven't already.
Well, f ((g x) y) ≡ (f . (g x)) y -- imagine if (g x) was just a function like g: f (g y) ≡ (f . g) y ≡ (f .) (g x) y -- we can do this because of how functions take their arguments: --(f .) "eats" (g x) first and becomes f . (g x), then this function gets applied to y -- so now consider (f .) (g x): (f .) (g x) ≡ ((f .) . g) x -- like before, imagine (f .) were simply f: f (g x) = (f . g) x -- now we can substitute every thing back to get p x y z = ((f .) . g) x y z -- and through repeated eta reduction p = (f .) . g This is, incidentally, a specific case of something I've noticed about function composition, where the number of compositional operators represents the number of arguments the first function "eats" before passing its result through the chain, e.g.: f . g -- g takes one argument, passes the result to f (f .) . g -- g takes two arguments, passes the result to f ((f .) .) . g -- g takes three arguments, passes the result to f And so on. [Edit: hadn't noticed your second snip] mf criteria operator list = filter criteria (map operator list) -- through a similar process, imagine (filter criteria) as f, (map operator) as g in f (g list) = (f . g) list mf criteria operator list = (filter criteria . map operator) list mf criteria operator = filter criteria . map operator -- eta reduction = (.) (filter criteria) (map operator) -- (.) in prefix notation = ((.) (filter criteria) . map) operator -- (.) (filter criteria) is f; map is g mf criteria = (.) (filter criteria) . map -- eta reduction mf criteria = (. map) ((.) (filter criteria)) -- in the same way that 2 + 3 is (+ 3) 2 mf criteria = (. map) (((.) . filter) criteria) -- (.) is f, filter is g mf criteria = ((. map) . ((.) . filter)) criteria -- (. map) is f, ((.) . filter) is g mf = (. map) . (.) . filter -- eta reduction; remove redundant parens. Hope this helped.
Is part of the problem a mixture of special Haskell syntax with ordinary rules of definition: comp k l x = k (l x) flip op a b = op b a p x y z = f ((g x) y) z p x y = f ((g x) y) -- so-called eta reduce p x y = comp f (g x) y -- definition of comp p x = comp f (g x) -- so-called eta reduce p = comp (comp f) g -- definition of comp p = (.) ((.) f) g -- definition of (.) in place of comp p = (.) (f .) g -- haskell section p = (f .) . g -- haskell infix operator mf criteria operator list = filter criteria (map operator list) mf criteria operator list = (comp (filter criteria) (map operator)) list -- def. of comp mf criteria operator = comp (filter criteria) (map operator) -- eta reduce mf criteria operator = (comp (comp (filter criteria)) map) operator -- def. of comp mf criteria = comp (comp (filter criteria)) map -- eta reduce mf criteria = flip comp map (comp (filter criteria)) -- def. of flip mf criteria = flip comp map ((comp comp filter) criteria) -- def. of comp mf = comp (flip comp map) (comp comp filter) -- def. of comp mf = (.) (flip (.) map) ((.) (.) filter) -- substituting (.) for comp mf = flip (.) map . ((.) (.) filter) -- haskell infix operator mf = (. map) . ((.) (.) filter) -- haskell flip section mf = (. map) . ((.) . filter) -- haskell infix operator
From the comments: [argoUML](http://argouml.tigris.org/)
Thank you very much,that was very helpful!
Thank you very much!
Incidentally, "tacit programming" is a weirdo APL term. Everyone else calls it point-free style (the weirdo-ism of which I leave to you to research).
As a pythonista trying to move towards haskell land, this looks quite useful to me. It could allow me to start moving chunks of pure code from python over to haskell, without having to do a complete rewrite all at once. 
“Hey kid, have people stopped complaining about Comic Sans yet?”
Headline font changed? I particularly love 'Adelle Basic', consider bring it back please. :-P
No one knows what it is is because it is used for everything.
Can you elaborate a bit on the second point for non Haskell programmers? I program in Lisp and usually stick to almost purely functional style, but it appears as though I have no intuition about function totality or its implications on a Haskell runtime.
A total function is one that returns a value for any argument in its domain. Division in Haskell is an example of a non-total function, since division by zero will not return, but will throw an exception instead. Fold is another example, since it fails to terminate for infinite lists. 
[Because it didn't annoy him as much as this](http://www.youtube.com/watch?v=AhMC2-TaDKs) (3m31s video)
He meant strong dynamic typing... /s
Could you show me, how you would write transformList ((,) &lt;$&gt; takeT 3 . chunks 3 &lt;*&gt; chunks 2 . filterT even) [1..] using pipes? (It is the last example from my first mail demonstrating parallel transformation of lists with less sharing compared to using standard types.) 
I never use `(f .) . g` for two-argument composition. I use `.:` from Data.Composition. I find it much more readable. Then p x y z = f (g x y) z -- eta reduce p x y = f (g x y) -- two-argument composition (from Data.Composition) p = f .: g Also mf criteria operator list = filter criteria (map operator list) -- two argument composition mf criteria operator list = (filter criteria .: map) operator list -- eta reduce mf criteria = filter criteria .: map -- then I'd stop there, but if you want to keep going mf criteria = (.: map) (filter criteria) -- use composition mf criteria = ((.: map) . filter) criteria -- eta reduce mf = (.: map) . filter 
Oh no, brand new comic, the Doctor is already out of date! (Well, not quite yet but needs future-proofing.)
You can get a warning about incomplete pattern matches, but I don't think there is any general mechanism to catch non-total functions. The compiler definitely cannot catch non-terminating functions, because this is a so-called "halting problem". For certain functions like head, tail, you can use total function equivalents in Safe package (http://hackage.haskell.org/package/safe). 
The problem is the wide coverage of the term 'crash'. What is a crash? a - physically disabling your system, forcing a reboot This is, and always has been, an OS issue. One can expect a modern OS to sandbox applications enough so that one faulthy application does not bring the system down. b - application runs forever, and stops responding to IO This can happen in Haskell as well. One can create an infinite loop, or can pause waiting for an IO event that will never happen. c - application does something illegal and terminates Like running out of memory (space leaks), or dealing badly with external resources. There are a classes of issues that do not happen, or should not happen, even in dynamically typed programming languages, like segmentation faults, accessing memory out of bounds, etc. But from a theoretical point of view, Haskell is no safer, than any modern programming language. It may encourage better practises, but that's a soft claim at best.
GHC can catch some self-referencing non-terminating functions: &gt; let x = x in x *** Exception: &lt;&lt;loop&gt;&gt;
I am assuming your answer was a bit tongue-in-cheek, but just to make it explicit: any program tends to be crash free if it is composed entirely of total functions, it doesn't have to be a Haskell program for that to happen. I think Haskell just makes it easier to see when you've made a partial function due to incomplete pattern warnings.
The similarity is somewhat intentional: Max's folding blog post (linked above) was an initial inspiration for the iteratee package's "zip", which I believe provided the impetus for enumerator's zip. That being said, at least in the context of stream processing this sort of fold composition arises quite naturally, as it's square in the target of the problems such systems are trying to solve. Iteratee and enumerator don't have the applicative interface for this because they're applicative parsers, but it's not at all stretch to imagine a library based around applicative folding instead. It's a large design space.
It's also available on Hackage. Actually, It used but not based on Data.Dynamic module. types which implement GeneHeukarya typeclass can be used. 
Thanks - clear and concise.
But it can't catch all of them, so some work will always be left to the programmer.
The classic example of a partial function in Haskell's Prelude is `head` - `head []` is "bottom" (what someone once called "Haskell's way of giving you the finger").
It's quite easy, actually - just check the type of each parameter, and if any of them is of the wrong type, throw a catchable error, or if that doesn't match up with your notion of totality, return `NIL` or whatever your Lisp uses to say "nope". It's kind of non-idiomatic though, just like writing explicit type checks in Python instead of catching `TypeError`s is considered un-pythonic.
The problem with returning `nil` is that you'd have to check every return value for nil, and nothing prohibits you from "forgetting" to do that just once. Particularly since it'd be a pain in chains of functions calls without intermediary bindings. And then every function above you isn't total anymore.
Lambda Calculus and Combinators, Hindley + Seldin.
Partial functions can be a pain, yes. Taking ``head`` as an example, it's trivial to turn this into a total function ``safeHead :: [a] -&gt; Maybe a``. I just had a quick look but couldn't find anything like that in the libraries that come with GHC. Is there such a thing?
Great thanks for that, I'll check it out. 
Basics is what I need, cheers.
That, plus the default non-nullable (undefined doesn't count as null because it can't be useful as an empty return value) and the fact that the vast majority of functions are total make this much more tractable in Haskell. It _is_ partially a library issue and a style issue, but the Haskell community makes decisions that render it tractable.
It doesn't catch them all, as DR6 said, but it also only works dynamically. 
http://www.haskell.org/hoogle/?hoogle=%5Ba%5D+-%3E+Maybe+a
"Comic Sans? Oh ... You mean writing." :)
The [wikipedia article](http://en.wikipedia.org/wiki/Lambda_calculus) was pretty good at some point. But I see that now it seems to have been completely rewritten, and I'm not so sure anymore. Such is wikipedia.
Very nice. I am curious; I have noted that your example only contains functions that operate on Double's. If one needed to operate on, for example, Int's, would they have to define explicitly all the functions that they would want to use, and their appropriate representation in Haskell?
I actually want to leave it as a run-time setting, but thank you very much for the suggestion! :-D
If you really want basic, there is [Alligator Eggs](http://worrydream.com/AlligatorEggs/). That's probably not what you are looking for, but it might be good if you have any younger siblings.
You can use a bracket abstraction algorithm from combinatory logic.
Correct. What you'd end up doing is, basically, implementing an ad-hoc type checker.
I've used both but nowadays I prefer `m ()` because it documents the intent of the API better and it can catch a wider class of mistakes at compile time. Having to sometimes add a `void` isn't that much trouble, and I've found it's needed quite rarely in practice.
I agree that `m ()` is preferable. I'd rather have a type that describes what the function is doing more accurately than save a couple keystrokes. 
Sure, it's OK, but the function isn't total. You can't prove in general that the function will terminate on all inputs. The parent comment was answering the question "what's a total function?".
Straightforwardly applying the code from `Data.Foldable` seems to work fine: https://gist.github.com/sjoerdvisscher/6155858 Edit: Ah, it works fine for one fold created by `fromLeftFold'` but not for one that is created from two `fromLeftFold'`s like `average`.
dynamic's symbol representations are just used for saving result to file at this time, so it can be anything in type Text. Just make sure that every symbol should be used only once in the same function's collection. but if you want to print available Haskell code, use Haskell codes as representations is better choice. Because Data.Dynamic didn't support polymorphic, Any function must be defined with type explicitly. But I still reserved a interface for advanced typing. for example, if you want a Int and Double typing (+) in the same list , a better function list should be [ toDyn "((+)::Int -&gt; Int -&gt; Int)" ((+)::Int -&gt; Int -&gt; Int), toDyn "((+)::Double -&gt; Double -&gt; Double)" ((+)::Double -&gt; Double -&gt; Double) ] by the way, using Monad and Arrow are also possible with Dynamic, but you have to declare all possible sort of typing separately which is really silly for me. I'll try to use other packages such as "plugins" and "haskell-src" for evolving Haskell code directly in the future. Since "plugins" is lack of advanced typing functionality, I need to implement it myself before that. It will take time.
The fact that `b` appears only in the input values in `finally :: IO a -&gt; IO b -&gt; IO a` also documents that `b` isn't being used. I think `IO ()` is better, though, because it's more clear as you say.