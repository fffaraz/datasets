vlc works too. vlc mms://mschnlnine.wmod.llnwd.net/a1809/d1/ch9/0/Beckman_OnMonoids_NoFear_s_ch9.wmv 
He lost me at the bind :-( Totally lost, what in the world is he talking about the symetry of the thing. He never told me what Ma was.
I watched this a while back when I was learning F# and had no haskell background. I think this time around, I will get wtf he is talking about :-) Can't wait to check it out again. Also he has some other vids on channel 9 where he talks about functional programming. Additionally, look at some of Erik Meijer stuff - this guy inspired me to get into haskell.
Scripts have been stealing my Karma this whole time? Blah.
Pretty - screenshots anyone? 
Yeah, I just mean FFI on the normal platforms.
At about 25 minutes in, at the Monoid discussion, I think he really should have put more emphasis on the fact that (f . g) is not automatically equivalent to (g . f), and that it is a constraint you have to enforce yourself when defining f and g if you want associativity. f = map (* 2) g = map (+2) list = [1,2,3,4] not_true = (f . g) list == (g . f) list 
Any other videos like this out there that can give some good insight into FP concepts?
There's a whole YouTube [channel](http://www.youtube.com/user/TheCatsters) dedicated to category theory.
&gt; if you want associativity. No, that's commutativity. (.) is an associative operator, i.e. (f . g) . h == f . (g . h), for all f, g and h. A monoid operation is an *associative* (not necessarily commutative) one, with an identity.
Neat but I don't agree with the claim that Erlang isn't fun to write code in. I do it every day and have been for the last couple of years, and sometimes it's excellent, though if I had my way, I'd do more Haskell than Erlang. 
Woops, you're right.
Yeah, people complain about PDF links, but this is really pushing it. I tried cabal installing it and was told I needed to install cairo, glade, and gtk. Ah well, just as well I suppose, back to work.
Is it common to abbreviate "Sir" as "Sr."? It doesn't seem like it actually saves you any characters.
Those are some beautiful slides. Enough Agda-isms to make use of conventional syntax, but not so much that it becomes impossible to follow.
Nice blurb, but calling this an "article about the Haskell Hack web server library " is way misleading. It says no more about Hack than the Hack README.md file. Nice to see it get attention; I've started playing with it recently, my brain numbed by what passes for "views" in Happstack. More information about Hack would be very welcome. Right now, I'm trying to figure out how I can read in a template file from disk, interpolate some stuff, and send it off as the Application body value. Basically, I'd like to find (or write) the Ramaze/Nitro equivalent for Haskell. 
I mean no offense, but maybe you should try installing gtk2hs? :)
Hack is *not* a high level framework; it is a low-level method of talking to different backends. Also, things are very young right now. Hopefully, there will be some proper code samples in not too long. For now, you can take a look at Wordify if you're interested (http://wordify.snoyman.com/)
I didn't vote this up in proggit because it's kinda gross looking. But I'll vote it up here because it's kinda gross looking. Maybe I'm just not used to C# - is it normal to have piles and piles of code like that? This is a poor sell for the "functional way." But maybe it's just a poor sell for C#.
I just tried it and i learned that foldr (&gt;&gt;) (return ()) $ map is the same as just mapM_ This seems to be interesting for people learning Haskell. edit: formatting
Kudos to the AProVE guys for a great research work. Unfortunately, AProVE itself is written in Java ! Just try the example. &gt; INTERNAL ERROR: &gt; java.lang.NullPointerException
The web interface seems to have a problem when selecting modules. Please select "--none--" (only uses the Prelude) for the moment.
Double penetration? Wut?
how can I download it?
He probably meant it lets you focus on the what, not on the how, rather than vice versa... Also, I like Conal Eliott's ["Semantic Editor Combinators"](http://conal.net/blog/posts/semantic-editor-combinators/) post, where he explains a generalization of the (.) . (.) pattern. If you use: result = (.), then it becomes (result . result), and you can think of it as traversing a "path" inside the type. So left-side result takes an (a -&gt; b -&gt; c) and "traverses" into the (b -&gt; c). The next "result" traverses into the "c". And that's where a function is applied. So (result . result) has the type: (old -&gt; new) -&gt; (a -&gt; b -&gt; old) -&gt; (a -&gt; b -&gt; new) But conal's idea is that this generalizes to also editing other stuff. For example, you can edit arguments too: arg = flip (.) (result . arg . result) would take (a -&gt; ((b -&gt; c) -&gt; d)) and traverse all of its way into the "c" there. Likewise, for tuples: (first . second . first) would take ((a, (b, c)), d) and traverse all of its way into the "b" in there. So: Prelude Control.Arrow&gt; (first . second . first) (+100) ((1,(2,3)),4) ((1,(102,3)),4) This also works for "fmap" (traversing into functors), "cofmap" (covariant functors), and various other stuff. Very general and suddenly point-free combinators of this kind become very readable :-) conal's post is highly recommended.
I just tried it and learned to use list comprehensions just for the conditions. Instead of if cond then [item] else [] One can do [item | cond] 
just learned that sameSize a b = numDigits a == numDigits b is the same as sameSize = (==) 'on' numDigits 
I hate the name "Dynamic Programming". It is completely meaningless and does not actually describe what's happening. Its a great technique and deserves a better name :-) Some suggestions: * Bottom-up computation (as opposed to top-down recursive computations) * Shared bottom-up computation (to emphasize the whole point of DP, avoiding revisiting computations) * De-recursive programming Can anyone think of more names?
The problem is fixed now. All modules can be used again.
Nope, I meant the how and not the what. As in, focus more on how the data is processed as opposed to what the data is. Am I making sense? :)
The author needs to check out the standard prelude and use those functions. And learn pattern matching.
Traditionally, the "how" is considered the imperative program running on the CPU, and the "what" is considered a declarative/functional explanation of that program. A point-free combinator is a declarative build up of a data transformation -- it specifies what should be transformed to what. It can be seen as drawing an elaborate pipe/graph. The "how" of these combinators is a very difficult-to-follow imperative program, probably :-)
Contracursion!
Then by your terminology, yes you're right :) that's what I meant. Cheers mate!
It looks like it's good for more than just understanding - it's good for finding function-level patterns that can be abstracted.
Thanks for the link. "Hack is not a high level framework; it is a low-level method of talking to different backends." That's just about right for me. I want to understand what the code is doing, and being able to hook my own stuff into something that bridges the gap from Web server to app is about perfect right now. I came across Loli recently (http://github.com/nfjinjing/loli/tree/master) which also looks very interesting.
* Memoization * Non-redundant evaluation * Unique evaluation 
Bird Meertens Formalism. ;-)
Great talk, but he says haskell isn't a panacea because "you can't just stick in a printf to see what's happening cause a printf has a side effect... you know have to rip it apart to put it in the io monad..." Haskell isn't a panacea, but hello, Debug.Trace :) 
For all that's said against point-free style, to me it's just like a function call: sum = foldr (+) 0 says that if you know what fold (+) 0 does, you know what sum does.
I noticed that the link claiming explicit import lists speeding up compilation was to GHC 6.0's documentation, which is comparatively ancient. Finding the equivalent page (http://www.haskell.org/ghc/docs/6.10.3/html/users_guide/sooner-faster-quicker.html) in a recent GHC's documentation seems to leave out the explicit import list as an optimization. Does that mean it isn't an issue anymore? I know that it can significantly affect performance to have explicit export lists.
Point-free is a nice style to work in. I appreciate it more for stack-based languages (such as Factor or Cat). 
I'm also curious how *much* it affects compile time. I'd gladly take a small hit in compile time if it made the code appreciably cleaner, which qualified imports can. I wouldn't be surprised if it was still there, but perhaps not as much of a slowdown as in previous versions, making qualified imports a stylistic choice.
I set it to proving a specialization of Fermat's Last Theorem: import qualified Prelude data Nat = S Nat | Z add Z n = n add (S m) n = S (add m n) mult Z _ = Z mult (S m) n = add n (mult m n) pow Z Z = Prelude.undefined pow _ Z = S Z pow m (S n) = mult m (pow m n) equal Z Z = Prelude.True equal (S m) (S n) = equal m n equal _ _ = Prelude.False interleave (x Prelude.: xs) ys = x Prelude.: interleave ys xs cart (x Prelude.: xs) ys = interleave (Prelude.map ((,) x) ys) (cart xs ys) allPositive = Prelude.iterate S (S Z) allTriples = cart allPositive (cart allPositive allPositive) three = S (S (S Z)) cubic (x, (y, z)) = (pow x three `add` pow y three) `equal` pow z three and testing the term `Prelude.any cubic allTriples`, and it answered "MAYBE". =)
I hope you know that this is NOT the typical/intended use case of a termination checker (for any language).
Proving theorems about termination of your program and proving theorems about number theory are just a minor quibble apart.
This is indeed a very interesting example from a number theoretic point of view. However, the goal of AProVE is to be applicable to algorithms used in "real world" programs and not to be a general automatic theorem prover. The vast majority of those programs do not terminate for complicated number theoretic arguments. So while we certainly would love to be able to prove FLT automatically, it is questionable if this would help us in our main goal of analyzing real programs.
 x.tolower() -&gt; map toLower x (not toLower x) Doesn't printf do what % does? rstrip = reverse . dropWhile isSpace . reverse x.split = probably has implementations in the new Split package? 
Pedobear approves! too bad, that demo isn't quine.
Note that the comparison for Python's `x[i:j]` is slightly bogus is that we typically do not use integer indexes but instead use substrings. So typically you would not be trying to do the span between i and j because the operations that would give you i and j would instead just give you the substrings. For example, span and break. The point about operations that use predicates on substrings is perfectly valid of course. I can't comment on many of the other python examples as I don't know what they mean.
Worst name ever.
It truly is unfortunate.
I disagree!
agreed
From [this page](http://erlang-factory.com/conference/London2009/talks) where there is also a presentation.
Haskell seems so useless if you need a 1h video to explain such a stupid thing like direct IO ... 
My expierence with Haskell is that it is usefull, if you need a small prototype to get a good solution, but you won't use it for REAL programs which need often a fast responsiv GUI and fast algorithms where you can ask the OS directly by callig a system function. Other software may need fast web connections and other things like sound and so on. For languages like C/C++/Java you find a big bunch of such libarys. Where is the use of redeveloping all this stuff again to make it accessible via slow connection libarys or in case you need it quick and dirty making unpure Haskell libarys, leaving behind the good idea of Haskell's functional transperancy. Maybe it would be better to develop translation patterns for good code fragments in Haskell to other more common languages...
I use both of these together at work, and have been for the last 6 months... nice! :-)
ta
The best part is I am now tagging delicious bookmarks as 'loli'. 
This was fixed a year ago. Downvoted.
[FLV direct link](http://209.73.191.206//s1snfs06r23/023/videosearch/88457071.flv?StreamID=88457071&amp;pl_auth=f03a11c9476a8ffbce6d0c4545331d9e&amp;ht=21600&amp;b=8j4j32p4pdvrs4a4b3bdd&amp;s=792795605&amp;br=300&amp;mdt=unknown&amp;mid=14253845&amp;nid=14253846&amp;pg=MTA2OTA5MzA0OTRhNGIzYm&amp;q=FhqYKPTiKXv3bIvmjDz35X&amp;rd=video.yahoo.com-offsite&amp;sl=3062&amp;so=%252Fvideosearch%252Fvideosearch%252Fyvs5411131) for those who prefer to save the file and watch it offline.
Why? It seems appropriate. It's a web DSL that's young, small, tender, innocent, soft, yielding... ahem, what was I saying again?
[Arch Package](http://aur.archlinux.org/packages.php?ID=27866), but how does it work?? 
An FAQ? 
SYB = Scrap Your Boilerplate. This is a feedback request on a paper by Herr Lämmel.
From README: Invoke like `hspresent /path/to/your/presentation`. Left and right arrow keys move between slides, and hitting `q` or `Ctrl-C` quits the presentation. The file format is really simple right now. Slides are separated by lines consisting of the characters `--`. That's it. Here's an example presentation: the title of the first slide this is really cool -- the title of the second slide hooray for hspresent -- look at how fancy the title to this slide is * bullet point one * bullet point two
my new lesson: mapMaybe :: (a -&gt; Maybe b) -&gt; [a] -&gt; [b] mapMaybe f = catMaybes . map f 
cross-posted to [http://www.reddit.com/r/prolog](http://www.reddit.com/r/prolog).
I think that ought to be either `mapMaybe = (catMaybes .) . map` or `mapMaybe f = catMaybes . map f`.
what happens when you HLint HLint? Is it worse than when they switch on the Large Hadron Collider?
Woah, this is cooler than I expected. I ran it on some of my own code and it found a lot of good things to fix.
what is so special about this (for those of us who don't breathe with fixpoint functors)? and for patterns themselves, wouldn't view patterns allow this?
VERY VERY GOOD YES MORE OF THAT PLEASE &lt;3
I don't know if you are being sarcastic or not.. but if you are serious then your request is quite possible :)
I see a disadvantage. Say I have a complex data type where the sort order depends on what I need the resulting list for. With sortOn, I need to define a new data structure and an instance of Ord for each or the different sorts I want to do. While this is functionally equivalent to sortBy, it creates complexity and increases the amount of development, testing, and maintenance required. I can see augmenting the "Bys" with "Ons" but not replacing them. Even that doesn't seem like it will buy too much.
I hate using transformer stacks. This is like Optimus Prime. Seriously, more pls. Get it into Haskell' :P
And what exactly is your work ?
I'll write something soon about [generators](http://github.com/yairchu/generator/blob/f0f23d7a1c5ff85b6162f4e85355038b3e666865/testProducers.hs): an alternative to lazy IO and Iteratee :) Basically if you are familiar with Python's generators - this let's you "yield" inside a ProducerT and get the next item with "next" in a ConsumerT. An interesting use case is to use Producers of the list monad, to decouple search trees from their pruning. You can't do this with Python's generators. 
This is really cool, and I use way to many redundant brackets apparently.
 pow Z Z = S Z ;)
Yeah, heh. I think we've even had this argument on #haskell, and I remember you convincing me one way or the other... I just never remember which way. All I remember is that lim_{x -&gt; 0} 0^x /= lim_{x -&gt; 0} x^0
For natural numbers, you have that n^m is the number of functions from a set of size m to a set of size n. Since we want sets to form a category, there must be a function from the empty set to the empty set -- namely id (and it is unique).
A newtype adds a bit of complexity, but its also explicitly specifying that you're abiding the preconditions. 
It says 2008 on the linked page too, but I could swear this is 2009... maybe even 2010 if I slept in longer than intended.
Definitely 2009...
&gt; A newtype adds a bit of complexity... Good point. I didn't think of that; I was thinking of creating a new data type that contains the information necessary for the comparison. Your way would be more efficient. &gt; ...its also explicitly specifying that you're abiding the preconditions. What is the benefit of this? Some data, like HTTP requests, don't have an innate sort order and any comparison will be done on a part of the request. Is much clarity gained by creating and comparing the following types: HttpRequest\_OnFrom, HttpRequest\_OnAccept, HttpRequest\_OnAcceptEncoding, HttpRequest\_OnAcceptLanguage, HttpRequest\_OnUserAgent, etc.? The permutations of each subset of this list? My impression is that all of this is encapsulated within the function used in the first parameter to the "By" functions to handle the comparison. Breaking the encapsulation to bring out the internal complexities feels like a step backward. Constructs like Monads and Arrows provide benefit by hiding internal complexity and it's counterintuitive to go the other way.
Well, the (value-) Nat-indexed type syntax in the other recent post looks nice.
I'll admit I'm not a fan. Overlapping instances that pretty much instantly turn me off to the idea, because they don't play nice if someone defines another base-like case you weren't expecting. Whenever I see overlapping instances its a good sign that 'here be dragons'.
btw, a private case for this can be made to simplify MonadIO instances. instead of having per-transformer instance definition of MonadIO: instance (MonadIO m) =&gt; MonadIO (MaybeT m) where liftIO = lift . liftIO we can have one to rule them all (without OverlappingInstances): {-# LANGUAGE FlexibleInstances, UndecidableInstances #-} class MonadIO m where liftIO :: IO a -&gt; m a instance MonadIO IO where liftIO = id instance (MonadTrans t, Monad (t m), Monad m, MonadIO m) =&gt; MonadIO (t m) where liftIO = lift . liftIO to try this with current mtl one needs to rename it into "MyMonadIO" (name is already taken etc) this replaces the current O(N) code length for MonadIO with O(1) code length. this can be done because IO is always the last monad in the transformers stack.
Very nice tool.
This one I have no problem with because the compiler can figure out what to do with the instance head unambiguously. =)
an XUL link? not everyone runs firefox here :P
&gt; As with any good tool, HLint has been used in the development of itself, and all suggestions have been fixed. apparently we're safe for now
too many redundancies is a meta-redundancy
doesn't even work well on firefox. some letters are stuck at the left.
...honestly, I felt a little embarrassed uploading it since opening a POSIX serial port is really file input/output, and the manager code around it is a fairly trivial use of MVars.
What disadvantage exactly do you have in mind? I mean, you don't need to create a new data type each time, you can project to an existing one: data Awe = Flop | Top | Uber deriving (Eq, Ord) data Language = L { name :: String, awesomness :: Awe } langs = [L "Haskell" Uber, L "BASIC" Flop] &gt; sortOn awesomness langs &gt; sortOn name langs In general, we have sortOn f = sortBy (comparing f) and weighing in with Luke's argument, the latter form is the most common use of `sortBy` anyway.
Can you similarly express `sortBy` in terms of `sortOn`? If both are related by a natural isomorphism (`comparing` and its "converse"), it seems to me it would be less controversial to pick `sortOn` as the preferred, more "abiding" building block.
This is almost exactly (though see below) what is in a paper that I wrote with Sam Lindley and Jeremy Yallop, to appear at the Haskell Symposium this year: [Unembedding Domain Specific Languages](http://homepages.inf.ed.ac.uk/ratkey/unembedding/). We use a different definition of HOAS to the webpage linked. To represent untyped lambda terms we effectively use the type: forall a. ((a -&gt; a) -&gt; a) -&gt; (a -&gt; a -&gt; a) -&gt; a The advantage of this type over the GADT that is used in the webpage that I have a proof that (in idealised System F) this type admits all and only the untyped lambda terms; i.e. the representation is adequate. There is no need to have an additional 'Tag' constructor that introduces partiality into some of the definitions. The proof of this result is in a TLCA'09 paper I wrote: [Syntax For Free: Representing Syntax with Binding using Parametricity](http://homepages.inf.ed.ac.uk/ratkey/parametricity). The result also extends neatly to typed lambda calculi, as long as we restrict the types we use in the embedded language to a known set. To present the type above, we use type classes which neatly allows modular construction of embedded domain-specific languages. In the Haskell Symposium paper, we show how to exploit this proof to provide a well-typed translation from the adequate HOAS representation to de Bruijn, and hence to allow intensional analysis of the terms. There is a small problem because the well-typing of this conversion relies on parametricity, which we have no access to in the Haskell type checker, so we have to cheat slightly by missing a case from a pattern match. Happily though, there is a proof that this is OK. 
 View patterns as implemented in ghc 6.10 do not help with this.
Yes, it's possible to express `sortBy` in terms of `sortOn`, as Edward Kmett shows in [comment 12](http://lukepalmer.wordpress.com/2009/07/01/on-the-by-functions/#comment-845). Basically, the idea is to make a special type that carries the comparision function around, like this data Ordy a = Ordy { cmp :: (a -&gt; a -&gt; Ordering), unOrdy :: a } instance Ord (Ordy a) where compare (Ordy f x) (Ordy _ y) = f x y sortBy f = map unOrdy . sortOn id . map (Ordy f)
Mrs Lopsided wants you to know that she has the loli.
You have shown another good way to do it for simple cases. How would you use this method to sort by both at the same time? E.g., if awesomeness is equal, then sort by name? What if you only wanted to sort by a portion of a string? E.g., a URL can be sorted by protocol, host, filename, etc. What if you wanted an unusual sort order? E.g., strings starting with 0 through 5 come before the letters and 6 through 9 come after. These could be done through newtype but there will be the downside of having a type for each comparison.
 sortOn (awesomeness &amp;&amp;&amp; name) langs
Its likely that HttpRequest_OnUserAgent will sort that way for a *reason*, and perhaps you can abstract away that comparison into a properly-named newtype, rather than "expose an internal complexity". I am not sure this is a good example, but if you consider ZipList, the Sum and Product monoids, you see that they don't "expose internal complexities" but rather expose data-types similar to those they newtype, but with different properties. 
[Monatron](http://www.cs.nott.ac.uk/~mjj/monatron/) also avoids the combinatorial explosion that you mention. It additionally comes with the notion of *uniform liftings* and iirc uses quite few Haskell extensions.
veryUnsafeKernel32
SICP seems to be frequently recommended as a functional programming design book. Real World Haskell also offers a pretty good set of practical tasks in Haskell.
Once again, you don't need a newtype for that; all of your sorting criteria can be implemented by projecting to the data type you sort on, maybe in conjunction with tuples protocol = takeWhile (/= ':') sortOn protocol urls projectUnusual xs = let (ds,_) = span isDigit xs in case ds of [] -&gt; (2,xs) (d:_) -&gt; if d `elem` ['0'..'5'] then (1,xs) else (3,xs) unusual = sortOn projectUnusual Not that defining a newtype is actually a bad thing. For instance, the latter example is probably for sorting filenames, and it makes much sense to not use a vanilla `String` but a newtype `Filename` for that. As soon as your data type takes "a life of its own", making it abstract is the right thing to do.
Looks like he's designed some neat little languages
Need a mirror...
I've always liked this site.
Ah, Chris Pressey. Awesome guy.
From comments: &gt; You can also write dsuByFunc in a more point-free style: &gt; &gt; dsuByFunc f = map fst . sortBy (comparing snd) . (zip `ap` map f) I love the `a -&gt;` monad. So mind bendy.
[http://sebfisch.de/fp-overview.pdf](http://sebfisch.de/fp-overview.pdf)
This is a good game. I'll probably end up adding some more levels. If anybody's interested in writing a solver for it, that was exactly the problem in the "Trojan Badger" problem in the South African Computer Olympiad 2007: https://olympiad.cs.uct.ac.za/contests/saco-2007/ Haskell is not an official language of the SACO, but we do accept Haskell solutions with a .hs extension. The compiler is GHC 6.6.1. We don't have model solutions in Haskell (except for parrots, which I did to learn Haskell I/O), so I don't know how much low-level optimization is needed to solve the problem in time (optimization at the level of choosing the right algorithm *is* needed: that's the way we like it). BTW, if you're interested in doing other contests in our [archive](https://olympiad.cs.uct.ac.za/old/), they may not have Haskell enabled, but I can enable it for any contest from late 2005 onwards: just ask.
One thing worth considering is explaining how one types →, ◦, λ, etc. You don't want programmers thinking "how can I learn this language if I can't even type it".
Awesome monad!
Very well explained! I think, the *PList* monad is almost the same as the implementation of breadth-first search by Mike Spivey in his chapter in "The Fun of Programming" or his paper [Algebras for combinatorial search](http://spivey.oriel.ox.ac.uk/mike/search-jfp.pdf). Your *join* function performs diagonalisation and Spivey uses the function *wrap* instead of *penalty* which has a slightly different interface but both are inter-definable. Also, I think if you use a continuation monad as in [Reinventing Haskell backtracking](http://sebfisch.de/Reinventing.pdf) you don't need to implement the complex diagonalisation function. [edited for correcting typos]
yes good point. I guess I should explain notation in a preliminary chapter yet to be written. edit: for the impatient who don't know: those symbols are *minus-greater*, *period*, and *backslash*, respectively
I'd say it's beyond awesome. Sigfpe never ceases to amaze me.
Not a bad idea, a cool thing to do would be to maybe make a server that's focused on WSGI apps, that way Django or whatever can be plugged into it without a problem.
Sounds like [HaPPs](http://happs.org/)
I don't understand why you'd want a haskell server to run python programs. Stick with lighttpd or code your webapp in haskell with [happstack](http://happstack.com).
I think this has been pointed out before but, choice of project name, despite all the "reasoned and technically minded" people in FOSS community, is pretty important for it's success. That said if the author doesn't care about that then it doesn't much matter.
It depends on your goals- if your goal is to implement a django app this would be a huge waste of time. If your goal is to learn Haskell while making a web app, using an existing haskell framework would be a much braver solution. Also remember that Apache is not your only options for webservers (I have been happy using Nginx). If your goal is to learn some haskell, then implementing an interface from django to an existing (haskell) web server in haskell might be a reasonable way to do that, but it is really decoupled from the need to wrie the web app.
Like I sort of said, I'm groping around in the dark. The app is finished and done as related to this part of the project. The idea is basically, build a server in haskell that is optimized, secure, and stable for serving django apps I'd run into the term WSGI, but wasn't aware of any details. thanks for the suggestions.
Loli's are small though. But in this case, I agree - not only will it be impossible to find information on the subject, you'll end up looking like a pedo in the process. 
omg
I read the title as "Functional Perl" and went WTF.
I read the title as "Functional Perl" and went WTF.
Knuth's dancing links/algorithm X does sudoku and other exact cover problems well
Nope, it illustrates everything that is right with functional programming, e.g. taking a clearly correct solution and rewriting it using small, sound equational reasoning steps to produce a program that is denotationally the same but with (hopefully) better operational characteristics. Functional programming makes programming as easy as algebra. 
BTW, I prefer Conor McBride's Sudoku data structure: data Triple a = Triple a a a type Square a = Triple (Triple a) type Grid a = Square (Square a) Basically a Sudoku grid is a 3x3 square of 3x3 squares. It is easy to write out the appropriate symmetries of the Sudoku by doing "transpose" lifted to various levels of the `(Triple (Triple (Triple (Triple a))))` type. This allows you to write code that checks rows and columns using a symmetry to reduce the problem to checking the nine 3x3 boxes.
Thanks for the link! I've looked for the paper version of Bird's [talk](http://icfp06.cs.uchicago.edu/bird-talk.pdf) for ages now. Added to the big [list of functional pearls](http://www.haskell.org/haskellwiki/Research_papers/Functional_pearls).
Is there a generalisation of this data structure to n x n Sudoku boards?
Have you seen [Higher-Order Perl](http://hop.perl.plover.com/)?
Hrmm, that encoding is a Church encoding of the Boxes go Bananas catamorphic elimination form. So, forall a. ((a -&gt; a) -&gt; a) -&gt; (a -&gt; a -&gt; a) -&gt; a and the elimination form: forall a. (T a -&gt; a) -&gt; a where data T a = Abs (a -&gt; a) | App a a encode the same thing. I griped about the BgB encoding on my blog a year or so back. =) http://comonad.com/reader/2008/rotten-bananas/
&gt; The type-level version of data is made by declaring dummy type constructors the same arity as your data constructors: `she` prepends SheTy (sorry, Shilpa) to the names of your prefix constructors LOL
HAppS has been renamed [Happstack](http://happstack.com).
Proving some of these laws are fun. &gt; `filter (p . f) = map f . filter p . map f` when `f . f = id` by the free theorem for `filter` we know that map f . filter (p . f) = filter p . map f compose `map f` on the left on both sides map f . map f . filter (p . f) = map f . filter p . map f now `(map f . map f) = id` when `f . f = id` map f . map f = map (f . f) = map id = id so we get filter (p . f) = map f . filter p . map f The more general theorem: `filter (p . f) = map g . filter p . map f` when `g . f = id`.
How easy would it be to play with these kind of proofs in a system such as coq or alice? Do you know of any work on programs which try to find and apply these "semantic reductions" at the source level automatically? I imagine similar (albeit more complex) reductions already happen in a compiler pass.
Why are you searching Google Images for info about a web framework, anyway? And... why don't you have a seat over there?
I had to do a "cd" to run bootstrap.sh, but otherwise it run fine. Thanks for the link, very useful for lazy people like me. EDIT: Failed because of Codec.Compression.GZip, sigh. Installing zlib manually from Hackage fixed the error.
&gt; cols :: Matrix a → Matrix a &gt; cols [xs] = [[x] | x &lt;- xs] &gt; cols (xs : xss) = zipWith (:) xs (cols xss) could use: cols = transpose 
What was the problem with zlib exactly? The version that the bootstrap script tries to use is the same as the one from hackage.
Yes, I know. The "Church style" version was also proposed ages ago in 1985 by Coquand and Huet; it is also basically the untyped version of the finally tagless representations of Carette, Kiselyov and Shan. As to the problem with case analysis that you mention in your blog posting; that's the main problem we are trying to deal with in our Haskell'09 paper. We show how to convert from the Church/BGB/Finally tagless version to explicit de Bruijn in order to do intensional analyses. There are other options too. One is to use the type forall a. (String -&gt; a) -&gt; ((a -&gt; a) -&gt; a) -&gt; (a -&gt; a -&gt; a) -&gt; a which represents "locally nameless" HOAS: the first constructor is used for free variables. With this representation it is possible to represent open terms, and to do (inefficient) case analysis on them. Another option is to use weak HOAS: forall v a. (v -&gt; a) -&gt; ((v -&gt; a) -&gt; a) -&gt; (a -&gt; a -&gt; a) -&gt; a In my TLCA paper I show that this is isomorphic to the original Church encoding. Since this no longer has any negative occurences of type variables, it can be represented as a normal algebraic datatype: data PreTm v = Var v | Lam (v -&gt; PreTm v) | App (PreTm v) (PreTm v) type Tm = forall v. PreTm v Now it is possible to do some case analysis, but you have to take care about what you use for the variables. [edit: formatting]
I just found out that you should uncomment the line -- documentation: False in the file ~/.cabal/cabal. And change it to true. Otherwise you won't get any documentation for the packages you download. Unless you use the undocumented flag --enable-documentation every time you install a package. Seems to me that this is the opposite of the setting you want by default, at least for programming. And who is installing things with cabal besides programmers? If they had to go through what I did, then nobody. 
Bootstrap said "zlib installed and correct version", but even so there was an error of "Could not find Codec.Compression.GZip". I don't have a clue on what caused the problem.
Perhaps you should start a poll. If the majority opinion is to have the default be on then we can switch it. It's currently off because: * it was off before the feature was added, so no change in behaviour for existing users * it's not a zero-cost option (argument for it being opt-in) * it's possibly somewhat annoying during development
Sounds like the Haskell zlib was indeed already registered, but that all the files for it had been deleted. This will happen if e.g. you do `rm -rf ~/.cabal` because the db package of registered packages does not live under `~/.cabal`.
Yeah. I wandered off and read your paper after making that comment and realized how obvious my statement sounds in retrospect. I also realize the problem in that post can be solved by shifting to a different representation, but I was mostly annoyed that I had to. ;) I constructed a 'finally tagless' version of that style of weak hoas a little while back when exploring the design space for my own purposes. But in the end, I'm still not sure which style of HOAS representation I prefer. In the end I guess the BgB encoding has the benefit that it seems to have the least offensive boiler plate syntactically when it comes to defining passes (no explicit lambda terms wrapped in constructors for all of your analyses), so things that you can define in that style are pretty clean and working with the catamorphism means you don't wind up making up new types just to shoe-horn things into a signature, but then you have to deal with the syntactic noise of the explicit fixed point, and you have a hard time dealing with more than one layer of the structure at a time. The last option you gave above looks like it is just a weak version of the Fegaras/Sheard encoding, which of course makes sense, (right up to limited case analysis) because we aren't doing much by pushing all these symbols around except eliminating the negative occurence to make things like coq happy.
This probably won't matter in the long run, but why the bizarre license? Is the author really that concerned about a business taking these modifications and running with them?
If you're learning Haskell you might want to try the [Haskell paltform](http://hackage.haskell.org/platform/) as it includes cabal. Haskell Platform 2009.2.0.1 requires GHC 6.10.3 and under Kubuntu 9.04 it installed flawlessly.
Just a tip; personally I never use the ubuntu haskell debs, it's much easier to install ghc and cabal in your HOME. This is likely to be the same thing as to install the haskell platform, as mentioned above, but I've never tried that.
I'll admit the limited license until this gets contributed to GHC is a little strange. Ryan will be at the Boston Haskell User Group on the 16th, so if nothing else I'll be able to ask him about it there.
Ok, what I was looking for is a tightly linked server for a django application, [this](http://en.wikipedia.org/wiki/Application_server). I'm perplexed why there isn't one - seems a way to a more production level of performance, but there are reasons I'm not seeing at this minute. Happstack looks interesting, but also looks very raw. Django appears to have more of what I need (Batteries included, e.g. session managment, pagination, etc.) at my skill level but I'm intrigued and going to start playing with Happstack.
I think the "adjustor pool" is an important restriction not listed in the notes. From http://projects.haskell.org/ghc-iphone/patches/16-rts-adjustor-pool.patch (the file is in Macintosh encoding) &gt; A declaration for an “wrapper” callback looks like this: &gt; &gt; foreign import ccall safe "wrapper" &gt; mkDelegate :: IO () -&gt; IO (FunPtr (IO ())) &gt; &gt; To implement these, GHC normally generates a small piece of executable code at &gt; runtime, called an “adjustor”. The purpose of these “wrapper” declarations is &gt; to generate a C-callable function pointer that executes a Haskell closure. &gt; &gt; Apple has a code-signing policy, that says that all code that runs on the iPhone &gt; must be signed by Apple. GHC wants to generate code at runtime, and this can't &gt; be signed by Apple. This is enforced by the iPhone's kernel, so GHC's standard &gt; implementation is impossible. &gt; &gt; This iPhone port solves the problem by pre-compiling a pool of functions, and &gt; allocating from it. &gt; &gt; If the pool is too small for a given application, you can increase it by using a &gt; {-# POOLSIZE x #-} pragma, which must appear after the "wrapper" token. e.g. &gt; &gt; foreign import ccall safe "wrapper" {-# POOLSIZE 100 #-} &gt; mkDelegate :: IO () -&gt; IO (FunPtr (IO ())) &gt; For me this is more evidence that adjustors should be avoided when possible. Adjustors create unportable code relying on runtime code generation. They are only needed if you're using a poor API that doesn't let you pass a data parameter with your callback such as qsort(). In GTK all callbacks use a data parameter, and in gtk2hs there is a custom closure that avoids adjustors: http://code.haskell.org/gtk2hs/glib/System/Glib/hsgclosure.c .
Do you have a link to Conor McBride's solution?
Here's me crossing every appendage I have for them to fix the prelude typeclass hierarchy (Num and Monad/Applicative/Functor in particular!) ...but I sort of doubt it'll happen :(
Is it actually OK to relicense a patch against a BSD tree? Shouldn't the patches also have the same license? I'm not sure.
Nope, I was talking to him at a dinner or on a bus or something. I seem to recall the transposition was done with some sort of `traversable` something or other.
For Coq, in some sense the proofs would be better in that these reasoning steps have unstated side conditions about the data being total and finite. These side conditions are trivially true in a total system such as Coq. On the otherhand, Coq does not natively have extensional equality, so this reasoning couldn't be done without appealing to an axiom or using setoids or such.
Hahaha, yes, Haskell' coming real soon now. It's been like that since Haskell' was first suggested.
I don't have an opinion on adjustors, but surely a sandboxed platform could provide a facility for building adjustors that just bind a data parameter.
I think that's out of scope: libraries (presumably including the Prelude) are now within the purview of the libraries@haskell.org mailing list.
too bad they did it without getting ARM working
It could be something like it, since my home directory is from an old Fedora installation (where I had Cabal working properly).
I want to see a proof-driven development tutorial for the layman
Why would a layman want to carry out proof-driven development? Who was that chap that said things should be as simple as possible and no simpler?
Let N be the number of monad transformers: * In regular mtl, the length of the source code for the auto-lifts implementation is O(N^2) * In Monatron, it's O(N) * With [SuperMonad](http://www.reddit.com/r/haskell/comments/8xkak/ask_haskell_is_supermonad_a_bad_idea/), it's O(1) Both Monatron and SuperMonad use `OverlappingInstances`. Regular mtl does not. 
The world doesn't divide into hardcore academicians and PHP monkeys. Plenty of smart programmers out there who don't have formal CS training in proof, and would be interested.
Correct URL is http://www.cs.uu.nl/wiki/bin/view/Center/AspectAG
(After hitting submit, I wonder if "Imperative" was the wrong word...) I was thinking this past weekend about the similarities and differences of FPL's and more traditional imperative langauges (C#, Java, Python even) and was thinking about the task of translating an imperative program to a functional one. Specifically I was thinking python to haskell. It occured to me that, in general, it would NOT be possible for such a thing (Am I wrong?) because of globals at the module level, but for the class of program that did not feature globals, it should be possible. By occured I mean *intuitively seems* right. Are these two conjectures correct? Has there been any interesting work done to this kind of topic? Note: I did google around, but I don't know solid enough names for this kind of thing for my google-fu to be effective. Also, I don't actually want to make a full-scale something like this, but it would be neat to make an automated python-haskell translator at the interpretor level for newbies\me.
You might just start with the history of those languages? Who were the creators? Why did they create this language with this syntax. It all makes sense. And I am curious on what you find in terms of 'advantages/disadvantages' for X language. I also don't like the term functional programming language or imperative language or dynamic language because no one language is functional or imperative. They can be both. I just like saying here is a 'set' of languages that have some similar characteristics and here are some that don't. I think Scala and Haskell have more in common than C and C++. For example. Also, Haskell and Scala are at an advantage because they seemed to have been created in a Research lab, University. The creators wanted to introduce new language features and interesting syntax. Java for example was created by Sun and meant to be put on Sun servers. I bet good enough for the marketing campaign was all that was needed. Yea, start with the history. Edited: "thinking about the task of translating an imperative program to a functional one. Specifically I was thinking python to haskell." Like I said, I don't know if I would put too much emphasis on these monikers. If you look at Scala, Scala has similar features to Haskell and runs on the JVM. Objected Oriented Code written in Java has state. You can call most Java code from Scala. Also for Java, there is a functional programming library, Functional Java that is a pure Java library.
Each direction is "just" compilation.
I should clarify: I don't mean to imply that one language is better than the other. They are all good for different tasks; even QBasic has its place. And your point is well taken that no one language exemplifies only one paradigm, or even exemplifies one paradigm completely. I was just wondering if there was any kind of algorithm for these types of transformations. The best example of what I'm thinking of -- which isn't to say a *good* example -- is that of tail call optimization. For a certain class of recursive function it is provable that you can rewrite it as an iterative statement without changing the outcome, almost completely regardless of what the function actually does. The question then is: What is the widest class of imperative program that can be provably rewritten into a purely functional language? (and what is the algorithm for that rewrite? This is an important question since it is trivial to show that everything is a turing machine and so all [lets say] python programs have a feasible representation in [lets say] haskell.) Is there a particular group of a academia that attempts at answering these kinds of questions?
I want more details, because what you said makes no sense to me.
Is this the kind of thing you're looking for? If we look at just evaluation order in the lambda calculus, then we have the following: The theorem of Computational Adequacy: If a program has *some* particular evaluation order that terminates and gives a result, then lazy evaluation (i.e. left outermost first) is guaranteed to also terminate and give the same result. This means that any imperative algorithm can be evaluated lazily and we will get the exact same result, while the converse isn't true.
No, you're not correct. Imperative programs are mechanically translated to functional ones all the time, by way of conversion to [single static assignment form](http://en.wikipedia.org/wiki/Static_single_assignment_form). Richard Kelsey proved in 1995 that SSA form is equivalent to CPS (continuation passing style) form, which represents computations by passing "return values" into explicit continuations.
Also, even for those of us with formal training, approachable tutorials facilitate idle consumption. It's rather nasty to think _too_ hard during afternoon tea, despite it being a wonderful time for intellectual exploration.
Exactly the kind of thing I'm looking for, Thank you. I've got some reading to do. (Why does wiki not have a topic for this when dictionary.com does? Bizzare.)
Can someone go into detail on how the (lift . runCod) "correction function" works? It seems to be related to the statement in section 5.1 that given any functor S and monad M, there is a 1-1 correspondence between functions forall a. S (M a) -&gt; M a, and functions forall a. S a -&gt; Cod M a. Is there an easy proof of this correspondence? I don't quite follow. This is clearly the real innovation of the library, and I'd like to understand it.
There are a number of core typeclasses which are generally agreed upon to have been incorrectly specified. Changing them would be easy, the "right" way is now well know in each case. It would just break lots of old code. The two examples sited are Num and Monad/Applicative/Functor. Num, the number typeclass, has several methods which are _far_ to assumptive about what numbers are, and severely interupt a good deal of fun algebra that people would like to write code for. As for Monad/Applicative/Functor. Well, Monads should always be instances of Applicative, which should always be instances of Functor. Except teh current defs don't require that.
How many people really do proof-driven development? It seems like it would be terribly time consuming.
Oh, the right way to do the numeric classes is known? That's news to me. I can certainly see how to fix some of the problems, but not all. 
Very cool. I'm actually extremely surprised how trivial this seems to be; especially if you *assume* no global state to manage (My understanding is that it would be impossible not to have to carry around that global state explicitly, which would make things more challenging -- and uglier). You seem knowledgeable on this kind of thing; how naive is it to think that a toy python to haskell translater would be doable as a quick side project? 
anyone managed to launch the video ? it doesn't seem to work on firefox 3.0.11
Up until recently at least there was a proposal on the Haskell' page for reorganizing the prelude, but it was still undecided whether they would include it. About "fixing" the typeclasses, I think it would ideally need "context synonyms" and "associated contexts" for the best outcome. Then we could give a reasonably mathematically sound hierarchy of numerical typeclasses, and make Num a synonym of the (rather odd set of) contexts that compose it. Making the Functor -&gt; Applicative -&gt; Monad hierarchy would also not be hard, but to make it super nice it would be nice if those typeclasses could be given an associated context so that we could have, among other things, restricted monads as part of the language (see the rmonad package on hackage, or http://blog.omega-prime.co.uk/?p=61 and the referenced proposal from John Meacham). This would also allow us to make something like UArr from uvector an instance of Array, for example. But as I said, that's a lot of work for this iteration so I doubt it'll happen :/
I have 3.0.11 and it works for me.
yes, it worked after reloading the page 10 times. weird.
It seems that Mauro is using the MonadPrompt trick, to state it in your terms, though I don't understand the details. The functor `S` is the signature of the possible effects, so for the state monad we'd have data S a where Get :: S s Put :: s -&gt; S () so that `put` and `get` take the form put :: S () -&gt; M () get :: S s -&gt; M s respectively. Or something like that, it doesn't quite add up with `forall a. S a -&gt; M a`. Then, `S (M a)` seems to mention the monad `M a` because just the `a` is not enough. This is necessary for putting something like mplus :: M a -&gt; M a -&gt; M a into an effect data type. Now, I think the correspondence is nothing more than using MonadPrompt (in the guise of `Cod`, also known as `ContT`) to implement the effects. &gt; I don't quite follow. This is clearly the real innovation of the library, and I'd like to understand it. Since I don't quite understand it either, I think that the paper needs significant changes for clarity. Less comprehensiveness for the `mtl` part which is going to be overthrown anyway, more crispness in chapters 4 and 5. When do I have to lift what, how do I do that, and why? I'm lost trying to distinguish `handleX`, `handleM`, `liftHandle`, `lift`, `handleExcT`. And where does this `tmap` function suddenly come from, and what exactly does it do? 
&gt; My understanding is that it would be impossible not to have to carry around that global state explicitly, which would make things more challenging -- and uglier The compiler can keep track of all of the free variables a lot better than a person can. A complete implementation of python would be too much for a side project probably. A subset of python mightn't be too bad at all, even with an LLVM native code backend. Check out [Lennart Augustsson's blog](http://augustss.blogspot.com/), he writes a lot about interpreters &amp; LLVM.
Is there a competition between language designers to come up with the most ungooglable names? Haskell' can't be googled at all. At least C#, C, C++ and even D can be searched to some degree.
I completely agree. proog-driven development is intensely time consuming and needs a lot more effort on part of the programmer. However, while it's pretty clear to me that virtually nobody would want to jump through such hoops for simple, non-critical "everyday programming", I can spontaneously name a lot of scenarios where it would be enormously beneficial, i.e. everywhere where guaranteed correctness is important. In an environment like nuclear power plants I'd be happy to use a programming environment which, using internal logic, forces you to write correct code (as long as the statements you're trying to prove are also correct, of course).
Oh. *reloads 6 more times*
proving an entire program does seem like it would be tedious, but then so does achieving 100% coverage in TDD (I haven't done TDD). I wonder if it's possible to construct partial proofs that are as viable as partial test coverage.
I think it's a temporary name until people can agree on a better one :)
Günther, the post was pretty sparse on details, especially about how much data you're trying to process. But you described a few conditions which sound problematic: 1) You're using SQLite for "data warehousing". 2) You're knowingly using no indices for your table lookups. 3) You're assuming your data can't fit into memory. First of all, how large does your dataset get? StackOverflow's using a machine with 48GB of RAM, and I think that I saw a Dell with 192GB of RAM. And buy a SSD, and then you'll get 100x more random seeks per second. If you *really* can't fit it all in RAM, and you need to drop back to a db, and you need to denormalize your data, it's not a temporary table, is it? You need that table to do your work. So create the table, and index it, and chuck your data in with indices. What sorts of things are you doing where having as much of the page cache in memory (assuming you've increased the default_cache_size from the 2MB default) is too slow? Last, SQLite is a fully-functional database, and I love it, but http://www.sqlite.org/whentouse.html says explicitly that it's not your best bet on "Very Large Datasets". That sounds like what you've got, especially since you're describing that as "data warehousing". Try postgres or Oracle instead of SQLite. It sounds like your db layer is giving you trouble.
i've had to do large data processing in haskell before, csv files were fine for me
IMHO, a poorly written post with weak justifications indicating a lack of undestanding.
I think this is harsh. I disagree with most of what he says. Most of the reasons he gives for not liking haskell are reasons that I like it. However, they do all have drawbacks. My main gripes with haskell are that debugging is difficult (which he doesn't mention), and it is difficult to understand how lazy evaluation will cause a program to be run, and whether it might cause space leaks. I wouldn't be surprised if the article author found that most of his issues disappeared with a bit of practice, and that he ended up loving haskell.
The poorly written part is harsh because the writer appears to be German. However, the rest of TheColonial's statement is certainly true. If the writer had voiced these concerns on a mailing list or on IRC, issues with the REPL, and whitespace would have been resolved. Also, the misconception that monads are only related to state could have been clarified as well. If he doesn't like the syntax, just an opinion, and we can't help that, but most of his issues are simply due to a lack of investigation.
He doesn't like Clojure either. I guess he doesn't like much. He says he likes functional programming, but apparently he doesn't like much of anything functional programming brings. I'm not sure he really understands the whole point of functional programming.
Works on both Intel and PPC. Yay! Thanks to Greg Wright for his hard work whipping the ghc port back into shape for Tiger.
there doesn't seem to be anything here
I think HTML format with some anchors and wider formatting could really help make it more readable. One of the superficial barriers of "laymen programmers" into this world, I think, is that not only do the concepts and syntax seem weird, but they're even distributed as academic papers to read in PDF form, rather than being ordinary blogs or such that people are used to reading :-)
This site fails without javascript.
Use this link instead: http://gist.github.com/47991
Another angle: * *Denotational semantics* maps imperative programs to functional programs; and * *Compilers* map functional programs to imperative ones. Not always, of course. Only for imperative languages in the first case and functional languages (and imperative machines) in the second case.
 cd ghc-6.10.3/ ./configure sudo make install One should not do that in a debian-based system. Raw 'make install' into /usr or /usr/local pollutes your system and has no upgrade/cleanup path. Debian users should checkinstall or stow self-built packages.
&gt; Raw 'make install' into /usr or /usr/local pollutes your system and has no upgrade/cleanup path What do you mean it pollutes your system? THe GHC package from ubuntu's own repositories install into /usr/local/lib with links from .usr/local/bin. I get the same results with the instructions for ghc 6.10.3
I mean that the files it installs are not tracked, so they might overwrite something else, or be overwritten by another install, or you won't be able to track down everything when uninstalling, or something will break on upgrade. The typical problems that existed before package management, I guess.
Ah yes OK - I assumed I was on my own anyway - thanks for introducing me to checkinstall.
How do you build a deb file of GHC? I've tried to make one before following generic instructions on building deb packages, but it always fails. Anyone have some specific successful instructions?
before getting laid off, i used it for analysis of sensor data
Why Haskell over R or SAS? They seem to be pretty common in that field.
&gt; approval how would you go about seeking approval?
I mainly use it for writing quick scripts to do boring stuff. Not in any customer-facing code (sadly).
Text processing. Usually logs, csv's, database exports. Pretty much everything I would have previously done with Perl and the various unix tools (sort, uniq, etc.). Good fun but using Parsec feels pretty heavyweight when compared with regular expressions. I'm used to pcre rather than posix regular expressions. Any suggestions as to which pcre library to use? I find Strings ridiculously slow but ByteString is cumbersome. I probably need to learn some GHC extensions to make them behave more like Strings. My usual pattern is something like: main = do args &lt;- getArgs content &lt;- C.getContent mapM_ putStrLn $ (various transformations here...) $ map C.unpack $ C.lines content I'm sure I'm doing it wrong : ) Tried to use HDBC today (instead of relying on database dumps), only to find it doesn't compile with ghc 6.10.3. Bah! 
&gt; HDBC cabal update. The latest release of HDBC works fine.
I do a lot of black box testing of multimedia programs (e.g., Flash) and, while there is a lot of repetition in the UI, programs are rarely refactored well. I use Haskell to generate test scripts in XML.
That worked! Thanks dons!
Should one have a preference between HDBC and takusen?
I'm writing a gadget to build PO (foreign language translation) files from html, and then generate translated html using the PO files. Pretty easy stuff. Basically I'm using it for micro projects where I'd usually use python or whatever. 
nah, they weren't making any money go begin with, and i was first in
i had a huge data set, thousands of files and millions of points. initially i wrote a tool in java but it was bug prone and unreliable so i reimplemented it in Haskell which ended up running slower, but had better results plus i don't know sas or R and they don't have a built in function for what i needed
&gt; reimplemented it in Haskell which ended up running slower, Slower? Why? (I'm curious, I assumed Haskell would be better with larger datasets because it can aggressively manage memory, but I don't know enough to say for sure)
When no one is watching.
What were the other three langauges? 
well, there was alot of reasons it ran slower, first of which was it just returned some data to the emacs buffer which i just saved to the disk later (and my workstation didn't have alot of memory). i'm sure if i did it correctly with proper database connections and better memoizaion it would have been fine.
Takusen, if you like the types. HDBC is a reliable fall back.
I think that when the Honda engineers developed CVCC, they did so without approval. Sometimes when the overall organization (or person in charge) is averse to new ideas, it is better to fly under the radar. It also helps to not be smug when you succeed.
I have to use R for some of my projects at work. It's very inconsistent in its syntax, has weak typing (unsafe), and poor performance. I wish I could use haskell instead but what R really has going for it are the stats libraries and great built in graphing support.
I generally use haskell the same way you do since ocaml is the language of choice at my company. For regexes I use Text.Regex.PCRE. It works well and the =~ is a pleasant interface (though I often end up using makeRegex and match for precompiled regexes. I typically don't bother with byteStrings until I determine I actually need to optimise for some reason.
If he's insinuating Google, C, C++, and Python.
At Galois we have to advocate strongly not to use Haskell (i.e. if we wanted to use Coq :) Here's some background on the kinds of projects: http://www.galois.com/blog/2009/04/27/engineering-large-projects-in-haskell-a-decade-of-fp-at-galois/ 
You probably want the `OverloadedStrings` extension (requires at least GHC 6.8.) It is the equivalent of the `Num` typeclass for Strings: all types which are regularly inferred to be `String` are instead inferred more generally as `IsString a =&gt; a`. This allows you to write code like this: ~/t $ cat overloaded-strings.hs {-# LANGUAGE OverloadedStrings #-} import Data.ByteString.Char8 as B str = "hello world" main = B.putStrLn str ~/t $ ghci overloaded-strings.hs GHCi, version 6.10.2: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer ... linking ... done. Loading package base ... linking ... done. [1 of 1] Compiling Main ( overloaded-strings.hs, interpreted ) Ok, modules loaded: Main. *Main&gt; main Loading package bytestring-0.9.1.4 ... linking ... done. hello world *Main&gt; :t str str :: ByteString *Main&gt; import Data.String *Main Data.String&gt; :i IsString class IsString a where fromString :: String -&gt; a -- Defined in Data.String instance IsString [Char] -- Defined in Data.String instance IsString ByteString -- Defined in Data.ByteString.Char8 *Main Data.String&gt; 
I don't think Google works with mysterious committees that are far away. As far as I know, their corporate culture among programmers is pretty transparant.
Very well, thank you.
He can't tell us. The Big Three are Yahoo, Microsoft, and Google. If it's Java, that rules out Microsoft. If the other languages match the Google list, we'll know it's Google. If they don't, it's Yahoo.
&gt; Text processing. Usually logs, csv's, database exports. Ditto. In addtion, I use Haskell for parsing binaries (ugh), making charts, and scaring colleagues... oh no.
The Big 4 at Google are C++, Java, Python, and JavaScript. Nearly all remaining C code has been ported to C++. And I doubt it's Google, because GMail is written in Java and development on that began in 2001. Ron Garret's blog also mentions doing the initial version of AdWords in Java in 2000. From his comment history, I'd guess Microsoft.
sounds more like yahoo than google. 
I'm using Haskell for developing financial models at the moment. Including some actuarial stuff, as it happens. The current system I'm working on uses a simulator based on [this code](http://contracts.scheming.org/), but with a Monte Carlo back end.
I'll start: I first started tinkering in November or December of 2008, because my girlfriend was taking a course in Haskell and looking over her shoulder piqued my curiosity. Until then, I was mostly a ruby person, and used c when I needed performance. What really drew me in was the elegance of it. Even in ruby I liked to avoid naming things and made big chains of .map and .inject, so getting cleaner syntax to do the same thing (and much, much more) was attractive. My only experience with type systems before haskell was in the usual c/c++/java context, and I associated them with everything bad, so seeing how powerful types could be without getting in the way was also refreshing. I've stuck with it because unlike any other language I've tried, it feels like a bottomless pit of new things for me to learn. I've worked with it as much as I could fit in with grad school since I started and I still feel like I'm only scratching the surface of the language. It's also introduced me to entire fields I was unaware of, like type theory and category theory. As for applications, I've mostly been doing some machine learning-related number crunching in it so far, although I tinker with other things in my free time.
I met a guy online who claimed to be from Princeton. He was he was an arrogant prick who stated that testing programs was useless and they should be proven instead. But, hey, I figured if he was that competent and going to a good school, I might as well check it out. The promises and the differences were quite enticing. After that, as [godofpumpkins said](http://www.reddit.com/r/haskell/comments/90zlp/when_how_and_why_did_you_learn_haskell_and_why_do/c0b2dqi): &gt; I've stuck with it because unlike any other language I've tried, it feels like a bottomless pit of new things for me to learn. I've worked with it as much as I could fit in with grad school since I started and I still feel like I'm only scratching the surface of the language. It's also introduced me to entire fields I was unaware of, like type theory and category theory. Though I'm not going to grad school... I've mostly been looking at it from a gaming perspective as well as doing some automatic content generation for work. It's been amazingly easy to duplicate major libraries (e.g., the BeOS Messaging API) and language features (e.g., Prolog-style backtracking). I still have a long way to go (I consider myself intermediate level) but I've done stuff that I hadn't imagined possible before.
I'd been doing an undergrad in mathematics at the time. I started with O'Caml after hearing from the other CompScis in my year about what an impossible language ML is — it's the first language they learn — which naturally piqued my interest: I like a challenge. Started off writing simple numerical code for my computational maths modules: I liked the idea of being able to succinctly express mathematics at a much higher level than previously possible. I'd been hacking code for almost a decade at that point (yes, wasted teenage years), and it was a pleasure to be able to program at a much higher level with such ease. In particular considering that I'd been doing the same thing with e.g. function pointers in C and getting frustrated when I make mistakes that I can't see. One of my old bosses (hi Tony &amp; Marty) commented that in Perl, you can write any program as a composition: map ◦ filter ◦ map, and I largely subscribed to that view. It was refreshing to see a programming paradigm that encouraged this sort of compositionality, as opposed to some esoteric technique used by grand masters of the Tao. Migrated to Haskell through Paul Hudak's book — I remember skipping lectures the morning after I'd found out about the book, and getting my copy from the local bookstore. Having witnessed how much more elegant a programming language could be, while still remaining practical, I can't see myself working in industry using anything lesser. So in that sense, don't learn Haskell, because it'll make you rule yourself out of most jobs. Now I'm doing a PhD in the area and have moved on to harder substances, e.g. Agda and Coq for fun and profit. Let this be a warning to you kids.
think about using haskell and taking the time to write the statistical code yourself. i doubt you need to use an equations that would be difficult to implement
I honestly can't even remember any more. I didn't study it at university, and I remembering hating SML and failing to see the point of functional programming entirely. But at some point I came across Haskell and I remember being quite take with list comprehensions. They seemed too cute for words. And it's grown on me a lot since then. And as others have mentioned, opened up whole vistas of knowledge that I never suspected were there or that I'd be interested in. 
A wise C programmer once joked, "if I were going to write my own implementation of visual basic 6, I would do it in Haskell rather than C", which piqued my interest. I'm not sure that I've really "learnt" Haskell, I've been writing stuff in Haskell for over a year now, but I have not waded into deeper waters of trying to understand monads. I like Haskell for it's type safety, the great community of people who contribute to the compilers, libraries, the mailing lists, blogs, the wiki, research papers and the irc channel.
nooooo!
After a bit of research I've contacted someone at Yale who host haskell.org and Neil who wrote Hoogle. In the meantime I'm using [hayoo](http://holumbus.fh-wedel.de/hayoo/hayoo.html) which seems equally good for my needs.
Why not just cabal install hoogle?
"Wherefore" means "why", not "where".
Plain old ignorance - I didn't know you I could install it locally. I like the fact that the web site links to the source where available.
its back :D
A few months ago; through reddit; I like it because its an efficient paradigm, parts I've touched on are fun and its a new way to stretch my thinking. edit: not educated as or working as a professional programmer
I first noticed it after an offhand comment in #ruby-lang about how annoying it was to have to write `list.map { |var| var.method }` when Scheme programmers were able to write `(map function list)`. Someone said "Don't forget Haskell," showed me the syntax, and I replied with "Hmm, that's nice" and shortly forgot about it. Then I started hanging out in /prog/, where all the *popular* trolls know Haskell, and decided I'd better learn it.
You learn something new everyday!
I learned Haskell because I learned new languages all the time and decided to further broaden my horizons. I like Haskell because it is the least stupid language that I know. That's really how I feel. To try to explain everything more concretely that I like about Haskell compared to other languages would be futile because there are just *so many* reasons. [Edit: Apparently I am incapable of proofing what I write.]
I read about it on reddit :) about 3 years ago. I was frustrated by all the languages I knew that time (C, (C++), Python, Maple, assembly), so that was the motivation. It took two or three tries to really get started with Haskell, but haven't looked back since. And, as already mentioned, it is bottomless (sic): I still learn new things about it pretty often.
I learned Haskell in 1990 so I could write the first Haskell implementation. 
Haskell was my first language. The Evergreen State College used to use it as the introductory language in their CS program.
actually, haskell isn't total, so there's plenty of bottoms.
I found Haskell through Reddit. I was hooked pretty instantly. I enjoy coding in it due to the "lsd effect." I feel like the more Haskell I learn, the better I get at programming in general, due to the huge amount of mind-expanding things that I learn while coding in it.
I actually got interested in Haskell because of reddit, there was a lot of material on haskell being posted and at the time I was kind of getting interested in Erlang because of the concurrency features. There were some blogs and articles on STM that caught my attention, and I decided to look deeper into Haskell. That was about a year and a half ago. I have not looked at Erlang since. Within the same time frame there was all the talk about monads (applied in other languages). So I wanted to go to the source and learn monads in the context of haskell where they actually make the most sense. Then watching a few SPJ talks and reading some great papers got me hooked. I have to hand it to the people in the haskell community, but a lot of them are very enthusiastic communicators. What I like the most about the language is that makes no compromises on purity. I love the idea of explicit effects that are documented and checked via the type system. The language has a steep learning curve when you come from non FP mainstream OO languages, but the reward is worth it in my opinion. 
This is exactly the same reason I became interested in Haskell; it started popping up in the programming subreddit a lot and looked really interesting. I was also completely sold after watching the SPJ videos and non-stop postings here have done nothing to help that. Getting my copy of RWH in the mail made the whole language a lot easier to grasp. I've always preferred having an actual book to learn from as opposed to a PDF or website.
I'm friends with John Meacham, he started evangelizing it to me and a few others around 1999 or so. Outside of puzzle-solvers, I have since then used it mostly as "a better imperative language", for various problems in my graduate work.
Wow! He spends the talk saying that fundeps are a nice idea and give nice notation. But then at about 33:00 there's a charged Q&amp;A and SPJ explains the problems he's found with fundeps. The speaker just about admits that type families are equally powerful and have more theoretical and practical support. So what is the relevance of fundeps? 
I like to tell people to think of it as the opposite of "therefore".
Exactly. In the Shakespeare play, Juliet isn't saying "Where are you, Romeo?" (i.e., "Why aren't you here?"), she's saying "Why are you Romeo?" (i.e., "Why in the name of mother-loving *Christ* did I have to go and fall in love with one of my family's sworn enemies?"). It makes all the difference in the world when you read it correctly.
actually, that's exactly what i was trying to refer to...
Yeah I forgot to mention I bought RWH too. Great book.
I learned Haskell in 1999 as the language of my initial computer science training in Sydney. I got back into it in 2002 when working on a JVM backend to GHC as part of an honors degree project. I like the balance of conciseness, performance and expressive power. The things that are exciting me now are parallelism, hackage, the new GHC backend, and new type system features.
I'd never heard it put that way. Nice.
I'm confused. It *is* the Galois I thought it would be.
Haskell subreddit? Why? Totally appropriate for the math subreddit.
We did an SICP study group last year and several members would compare exercise solutions with Haskell idioms. I try to learn a new language every 18 months. I'm reading RWH and still very much a newb, but I like the community and tools, not to mention the literature. Some languages I've studied, I've shipped production systems with and kept using them. Other languages have been a valuable learning experience, but I didn't actually ship any real code on them. I'm amazed at the potential of the language for getting real work done. Maybe a year or two from now I'll be able to use it in a real project. Either way, it's a great brain food.
I started Haskell a few months ago (its on hold while I work on getting my WPF knowledge up to speed). I got into F#, liked it and saw interviews with Erik Meijer and he always talked about the purity of Haskell. I wanted to see for myself, hence I started learning it for my own benefit. Thus far it has been enjoyable and nothing would make me happier if I can just get back into it. But I am looking for a job, and Haskell alone isn't going to cut it for me, hence, WPF.
I found Haskell through Reddit. There is a traditional split in computer science. On one side is the practical people, who in the limit case are "merely" programmers, interested primarily in doing things and how to do them better. On the other are the theoreticians, who care about the math and theory. Both sides have a problem. The practitioners (especially the programmers) would benefit from many things the theoreticians are promoting, but the theoreticians have historically done a terrible job of verifying that their theories actually have any relevance to the real world, either. So frequently, the theoreticians solved clean problems with clean code, but in the real world we rarely get clean problems and so the relevance has been limited. (Yes, some of that is brought on by the lack of theoretically sound practices by the practitioners, but there is also an irreducibly complex aspect to the problems in the real world.) I have always been right in the middle, caring about theory but also down in the trenches programming. What intrigues me about Haskell and the community is that it seems like Haskell is the best attempt the theoreticians have ever made in reaching out to the practical side and making a theoretically clean language with a minimum of compromises on that front (though I gather not nil), while still producing a language that can do real work. (Note that while there have been other languages from academia with the goal of doing real work, few or none of them have also gone for such a level of purity, and I don't think any of those have the same amount of support and momentum.) This is the same thing SPJ says in one of his presentations, albeit from a different perspective, when he shows Haskell in the lower right headed up towards practicality, with C# in the upper left heading right towards purity. I am drawn to Haskell as a bridge between my practical side and my theoretical side. There isn't much competition on that front. (Unfortunately, my "real" work picked up and I've had to shelve my Haskell project for a while, but I will get back to it sometime.) Haskell still has some work on the practicality front to do; for evidence I hold up the fact that every GHC release still has a lot of really good work coming out on low-hanging fruit like UTF-8 support, whereas the mature practical languages have had solutions to that for years. But it's getting there fast, and without the same sorts of compromises. I'm very intrigued by Haskell's future. (I think right now, I'm squarely in the part of the field that Haskell needs to impress to keep going forward. I've been tough on Haskell on reddit, but consider it a compliment, as there is no point being tough on something worthless. It's getting better.)
I learned Haskell in 1992 for my PhD in Natural Language Processing. We converted our NLP system (Originally callled General Inference Machine, but then named Large-scale, Object-based, Linguistic Interactor, Translator and Analyzer) from Miranda to Haskell because hbc written by augustss ran much faster. Most of our workstations has 486 levels of performance... slow... Then we moved to ghc because it ran faster and we could parallelize it with the help of the Ghc guys who lived a little further north in Edinburgh. We were in Durham (UK). Haskell's wonderful for anything algorithmic, because you only say what you need to say. Think compilers, parsers, text manipulation, NLP, etc. Even if my target language is x86 assembly, I'll often write the algorithm in Haskell, simplify it to its bare essence, and then convert it to x86. Having multiple ways of expressing a problem helps distinguish what is incidental (the class design) from what is essential (the data being manipulated). Read http://research.microsoft.com/~simonpj/papers/history-of-haskell/history.pdf for more info on Haskell's history and applications.
Same here.
Well, whoop de freakin' doo.
Was it in Soviet Russia? Cause it sounds like Haskell learned you!
Dons, you have also made a significant contribution to my interest in Haskell. If it wasn't for all the (sometimes rather random) submissions you've made to the subreddit, my interest in the language would never have reached the level it is at now. I'm probably not the only one who wants to say this: Thanks!
I clearly said that these were my first expieriences with Haskell.
I (the author) like StandardML and Ocaml, even though I dont really use them often. I also like scheme, even though I dont use it often. I have my reasons for not really liking clojure the way it is now, but I also dont like the pages-long hate-wars why clojure is better or worse that scheme or common lisp - its another language, it goes new ways, and going new ways is good. I mostly use common lisp at the moment (which is multi-paradigm) and most of my code shold be sort of "functional", i.e. avoiding side-effects where possible, and when using them, doing it explicitly, using methods (and therefore some kind of type-system), etc. Well, I think I know what functional programming is about - and I never said that I like it. I like it for some purposes, but I also see its limits.
&gt; whereas the mature practical languages have had solutions to that for years but they haven't had solutions to things like easy parallelism or say, true concurrency support, which Haskell /has/ had for years. The research-driven development model of the 90s.
I'm sure your Clojure hatred is directly linked to the JVM, seems like it always is. I'm a Clojure zealot, but I don't care to press the issue, if people don't like it, they don't like it.
dons works for a company here in Portland called Galois. They're a fairly well-known Haskell shop. Edit: Spelling.
I went to University in September last year. I used to program in Common Lisp and Scheme at the time. My university ran a Haskell course as one of the first courses. After I had learnt the basics of Haskell, I decided to stick with it (ie: spend my free time on it) because the community around it is working on the most modern problems, and making good progress - verification, concurrency and parallelism.
I started playing with Haskell in high school in 1997, although I didn't get serious until late 1998, I didn't get good at it until 1999, and it wasn't until some time in 2000 that I was fluent enough that I no longer felt the need to run back to imperative programming when things got tricky. I was at a stage in my life when I was deeply unsatisfied with Pascal and C++ and was looking around desperately for something good. I had become attached to Modula-3, and had noticed that most universities that taught Modula-3 also taught ML. When I ran across the book "Programming in ML", I read it over about 2 days and was immediately sold by pattern matching, map, fold, and the fact that I could write my own higher-order functions if map and fold didn't do it for me. I started learning SML and Haskell in parallel, although I found Haskell easier going, in part thanks to Hugs, better tutorials, and the community. I moved to GHC and GHCi with the 5.00.0 release, and never really looked back.
Historically? They've been around a lot longer than type families, and they were one of the motivations for beginning research on type families and associated types. Primacy aside, I think the take-home message of this talk is that TFs have a pretty syntax and that syntax can be used to capture the user-facing side of FDs--- something Mark rightly mentions has always been a thorn in the side of FDs. In the broader scope of things, ultimately the relevance of FDs is a research question: nobody knows. Are FDs actually different from TFs once all the syntax is stripped away? This is a question that deserves an answer, but so far as I know noone has one yet. The database community is renowned for their poor interaction with the programming language community, despite the domain overlap and many failed crossovers. The DB community has spent a lot of research on expanding and understanding relational algebra and if that research can be taken and funneled into PL research, that would be a good thing since it would allow PL researchers to draw on that vast literature for tackling type theory, and it could allow for DB researchers to finally start writing query languages that make sense from a PL perspective. If FDs and TFs turn out to be the same (or their similarity and difference can be documented explicitly) then that means there's a common language for being able to cross-apply the work of logicians and DBists--- another pair of communities which have always failed to communicate meaningfully despite shared domains. Unfortunately, Mark, who invented FDs, is one of the few people still in support of them as an area for research. This talk discusses why they've fallen into disfavor, and I think this sort of historiography is important for the Haskell community to know where to put FDs in their mythos. But unless there's a resurgence of interest in FDs or in trying to connect the theories of FDs and TFs, the ability to tie DB research together with PL and logic will be lost. Communication failure is, alas, the biggest impediment to research and progress in academia.
College, 2003-2004ish. I'd just read Paul Graham's essay on Blub programmers, and then I ran across a comp.lang.lisp post by him where he proclaimed that Lisp was all you really needed and Haskell was just a "weird, academic language." I knew then I'd found something even higher than Lisp on the Blub continuum...
i learned haskell in 06 for an undergraduate project. i like it because it lets you reason about programs that you can hold entirely in your head.
A bit of irony... I picked up Haskell at a C/C++ meetup. Brian O'Sullivan himself was presenting at an ACCU get-together at Symantec, and it really struck me the way Haskell was so similar, and yet so different from the MLs that I was familiar with. After that, I started checking out RWH online, eventually bought a copy, and I've never looked back since. The way that the language differs from all others is so subtle-- what a difference outright purity makes! --and so genuinely exhilarating. At about the same time, I was also realizing that the web forced a separation of concerns. The important consequence of this was that the high-context programming enabled by OO languages could no longer be exploited for anything other than prototyping. Everything had to be serialized to the backend so you *had* to do MVC anyway. For this reason, I feel that Haskell is a superior choice when building the backend for your services.
oh, and most of your work is really just getting stuff into a haskell list, the rest is cake
Any possibility of a link to the slides for those of us without access to [ACM](http://portal.acm.org/citation.cfm?id=1543134.1411292)?
* [SmallCheck](http://www.cs.york.ac.uk/fp/smallcheck/) * [slides](http://www.cs.york.ac.uk/fp/smallcheck/Haskell08talk.pdf) (pdf)
I have HDBC working with 6.10.3 both on Kubuntu 9.05 and centos 5.3. What's your setup ? edit : nevermind, I see that you solved your problem :)
well, I do intend to start using haskell for more computational stuff but I'm no statistician (couldn't even play one on TV!) so it takes me a while to figure out the underlying mechanics of matrix factorizations etc. I would love to contribute to a haskell stats project but to directly compete with the way R is used (maybe thats not event the right goal) it would have to have a nice interactive environment with graphing and it would have to be all integrated in one easy to use environment. 
What hatred? I dont hate clojure. I just dont like some previous versions much, and the current one doesnt seem to be very different. And I dont like the fact that though it seems not really well-optimized, it is yet considered "version 1.0" which is much too early I think. But still, I would thankfull use it instead of Java. Indeed, it would be a good thing if clojure worked on other JVMs than Sun's VM (actually, I heard that it meanwhile does, is that correct?), especially on gcj (because a huge problem with sun's VM is the fact that it needs a lot of memory which is bad on a small vps) but the JVM as such is - I think - the best choice. Why does all criticism always have to end in that type of discussions? I mean, what I wrote about Haskell is not even real criticism - I have stated a few things I noticed about Haskell the first time I really used it - I clearly said that. And I also wrote about a few things I found interesting about Haskell. And seems like most people who commented directly on my blog understood that. But here on reddit, flaming seems to be more popular.
Quite a strange implementation. choose_from space = fst $ minimumBy (comparing snd) [(code, maximum $ map (length) (space `pivot_on` code)) | code &lt;- space] This takes a loong time on the initial search space, and will always return the same choice. EIther way, playing against this would be frustratingly slow. A backtracking strategy using monads would have been faster and easier to implement..?
let me repost then my recent favourite here: series :: [Int] -&gt; [Integer] series [] = 1 : repeat 0 series (k:ks) = xs where xs = zipWith (+) (series ks) (replicate k 0 ++ xs) it computes the power series expansion of 1 / ( (1-x^a[1])\*(1-x^a[2])\*...\*(1-x^a[n]) ) [markdown always bite me...]
`powerSet = filterM (const [True, False])`
Well, hatred, dislike, whatever. It's not a big deal man, I didn't mean anything by it. I'm not trying to flame you. I just figured your dislike for clojure was linked to the JVM. I'm sorry I came off the wrong way. As for your question, I'm not sure if it runs on other JVMs. You could ask on the Google group or IRC channel if you're really wondering though. It's all good. I come in peace.
That's amazing! I feel very stupid...
Could you explain what this does for us newbies?
A great one: import Control.Arrow( (&amp;&amp;&amp;) ) import Data.List( group ) rleEncode = map (head &amp;&amp;&amp; length) . group 
Have a look at the corresponding page on the [Haskell Wiki](http://haskell.org/haskellwiki/Blow_your_mind)
You can do many sorts of awesome things using the combinators in the `Control.Arrow` module. For instance, if you have a list of numbers, instead of writing things like this to get both the least and the greatest elements: minmax :: [Int] -&gt; (Int, Int) minmax xs = (l, g) where l = minimum xs g = maximum xs By using `&amp;&amp;&amp;`, you can alternatively write a one-liner: minmax = minimum &amp;&amp;&amp; minimum 
The power set of a set S is the set of all subsets of S. So now you know what it does. I'll leave it for someone else to explain how it does it.
In all fairness, you could write the first version as minmax xs = (minimum xs, maximum xs) but yes, the Control.Arrow combinators are quite useful.
The basic idea of filter is that it iterates through each item in the provided list, deciding if that item is going to be in the result or not. Using filter in the list monad allows you to decide both yes and no at each iteration. The magic is that the list monad keeps track of these exponentially growing results and bundles them up for you in a single list at the end of the computation.
I still didn't understand and in case anyone else is trying to, [wikipedia to the rescue](http://en.wikipedia.org/wiki/Power_set). &gt;If S is the set {x, y, z}, then the complete list of subsets of S is as follows: &gt; * { } (also denoted \emptyset, the empty set) &gt; * {x} &gt; * {y} &gt; * {z} &gt; * {x, y} &gt; * {x, z} &gt; * {y, z} &gt; * {x, y, z} &gt; and hence the power set of S is {{}, {x}, {y}, {z}, {x, y}, {x, z}, {y, z}, {x, y, z}}. [A filter in a functional lanugage is](http://en.wikipedia.org/wiki/Filter_%28higher-order_function%29): &gt;a higher-order function that processes a data structure (typically a list) in some order to produce a new data structure containing exactly those elements of the original data structure for which a given predicate returns the boolean value true. [An explanation of filterM](http://byorgey.wordpress.com/2007/06/26/deducing-code-from-types-filterm/), which I don't fully understand, probably because I'm not comfortable with the type system and monads yet.
I always put it like this: saying that "A is a subset of B" is equal to "A is an element of the power set of B". A ⊆ B ↔ A ∈ ℘(B)
Something that went to actual production.
when: some time in late 2007/early 2008. how: started with yaht, then lots of papers including the one on universality of the fold operator, why FP matters and a few others. interaction with #haskell why: that's a long story, but since you asked.. One of my professors had recommended that i implement an algorithm I came up with for a sat solver (similar to Bucket Sat) in Haskell, but being the die hard C++ fan i was back then, i did it in C++ and due to a bug in the verifier was not able to submit a solution on time. What really opened my eyes was that another guy simply modified the java version of walksat and implemented my algorithm and scored more points!! That quarter opened my eyes!! I got my most important education right there, it doesnt matter if you are good at using a language and come up with a good algorithm, if you don't think "big picture" and design well before you implement it you are well on your way to doom and gloom. And standing on the shoulders of giants is actually a good thing [when your focus is on implementing an algo, there is no point in starting from the nuts and bolts if you dont have to]. My prof or some one else had said that Haskell facilitated that "big picture" thinking. That course was in 2003. after i graduated, and got reasonably stable in personal life and job wise, one of the first things I wanted to do was look at Haskell. I'm glad i did. I was able to understand the concepts easily enough but i cant stress the fact that it was help from the community which kept me interested in the language[ especialy caleg, ddarius, dons etc..] . 
Somewhat ironically, version 1.0.0 has since been superseded by 1.0.0.1.
So what does it do?
Along those same lines, I like this one: truthTable n = replicateM n [True,False]
now = unsafePerformIO getDateTime The .cabal file is longer than the source.
&gt; actual production I.e. used for the task it was built for? Or sold/released to others to do that task?
 sudo pacman -Syu *drums fingers* sudo pacman -Syu *drums fingers*
A bit of recent #haskell golf turned up this cute snippet to turn phone numbers into mnemonics: m=mapM((words"0 1 abc def ghi jkl mno pqrs tuv wxyz"!!).read.(:[])) m "278" 
For those too lazy to download and appreciate the joke: -- | The unchanging \"now\". If you get any errors or unexpecting results from using -- this value, it's because your assumptions about the nature of time are wrong. -- I have a proof of this, but the margin isn't large enough to contain it. now :: UTCTime now = unsafePerformIO getCurrentTime 
One implementation of the inverse: rleDecode = concatMap (\ (x,n) -&gt; replicate n x)
Polygon clipping... clip :: Integral a =&gt; (a,a,a,a) -&gt; [(a,a)] -&gt; [(a,a)] clip (x0,y0,x1,y1) = clipEdge (\(x,y) -&gt; x &gt;= x0) (interpx x0) . clipEdge (\(x,y) -&gt; y &gt;= y0) (interpy y0) . clipEdge (\(x,y) -&gt; x &lt; x1) (interpx x1) . clipEdge (\(x,y) -&gt; y &lt; y1) (interpy y1) where swap (a,b) = (b,a) interpx z (x0, y0) (x1, y1) = (z, ((z - x0) * (y1 - y0)) `div` (x1 - x0) + y0) interpy z a b = swap $ interpx z (swap a) (swap b) clipEdge inside intersec polygon = ce (last polygon) polygon where ce prev (cur:rem) = let loop = ce cur rem in case (inside prev, inside cur) of (True, True) -&gt; cur:loop (True, False) -&gt; intersec prev cur:loop (False, False) -&gt; loop (False, True) -&gt; intersec prev cur:cur:loop ce _ [] = [] rect = (100,80,420,260) :: (Int,Int,Int,Int) p = [(77,276),(46,32),(261,47),(380,24),(490,77),(443,204),(440,298),(235,196)] :: [(Int,Int)] -- clip rect p 
You need a script for that :-)
I'm not sure I'd call this beautiful... more like monad golf. After various transformations (explode filterM into do notation, desugar it, translate(&gt;&gt;=) into (flip concatMap) cause we're in the list monad) you get, roughly: powerSet [] = [[]] powerSet (x:xs) = let pxs = powerSet xs in (concat . map (\px -&gt; [ x : px ]) $ pxs ) ++ pxs which to my mind is a lot clearer ;)
could you post code that uses this in practice, to calculate something useful?
This post inspires a certain new twist on bash.org.
The DWIM sequence completer: {-# LANGUAGE PostfixOperators #-} (...) [] = [0,0..] (...) xs@(h:t) = scanl1 (+) $ h : (zipWith (-) t xs...) main = do print $ take 10 ([5,6,7,8]...) print $ take 10 ([1,4,9,16]...) (reconstructed from memory, the original may have been more awesome)
 powerSet [] = [[]] powerSet (x:xs) = map (x:) pxs ++ pxs where pxs = powerSet xs Redundant concat and slightly odd style removed.
 rleDecode = concatMap ((uncurry . flip) replicate)
 foldl f z ys = foldr (\x xs y -&gt; xs (f y x)) id ys z
Of course the best thing to do is limit or eliminate data contention. Both STM and locks don't scale beyond some point due to each CPU invalidating each others cache, but if you can find a way (or use an existing way) to eliminate the contention then that's wonderful. I've recently been exposed to some fun and useful [data structures (PDF!)](http://www.cs.pdx.edu/~walpole/class/cs533/papers/joshrpe.pdf) that use [relativistic programming](http://wiki.cs.pdx.edu/rp/) techniques. At first glance I'd swear you need to be in the IO monad to perform any such actions (as this requires destructive updates) which is unfortunately counter to much of the goodness Haskell has to offer to multicore systems (purity!), short of amazing advances in strictness analysis.
What do you mean by useful? Clearly this line of code will never drive web servers; on the other hand, it is pretty useful mathematics. My original motivation to write this function was a recreational math problem, the so-called [postage stamp problem](http://en.wikipedia.org/wiki/Postage_stamp_problem). For example suppose that you have three types of coins, a $2 coin, a $3 coin and a $5 one. If you have unlimited supply of these coins, this function tells you how many ways you can pay `n` dollars with these types of coins. The postage stamp problem deals with a *limited* supply of coins, and you can use this stuff as a building block to attack this problem. Of course all this is not very useful for the general population. However, a quite large portion of combinatorial problems (which *are* useful) can be rephrased as counting integral points in (rational) convex polyhedra. If you have some parameters in the problem, which you typically have (a single isolated problem is not very interesting), very often the polyhedra you are interested in are slices of (convex, rational) cones. Now, a general cone can be decomposed as an (almost) disjoint union of [*simplicial*](http://en.wikipedia.org/wiki/Simplicial) cones, where simplicial means that it has exactly as many edges as the dimension is. And for simplicial cones, the answer is calculated by this function. So you can reduce (at least in theory) very general (and useful) counting problems to this function (using the [inclusion-exclusion principle](http://en.wikipedia.org/wiki/Inclusion-exclusion_principle)); and I personally think that this implementation is both elegant and fast, and it uses lazyness in an essential way, which makes it relevant to this thread. ps.: I'm drunk. 
Or you can see it [here](http://hackage.haskell.org/packages/archive/acme-now/1.0.0.1/doc/html/src/Acme-Time-Now.html#now) without much hassle.
I love [Conal Elliott's paper on functional reactive programming](http://conal.net/papers/simply-reactive/). It is so easy to read and yet points towards things that are so tantalizingly powerful. The implementation still needs work for efficiency, but it is under [active development](http://hackage.haskell.org/package/reactive).
Any papers you'd recommend on them? I read the standard haskell wiki thing on them; where to go from there?
Honestly, I haven't yet taken advantage of Control.Arrow's full generality; I simply apply them to functions. In this context, they aren't all that difficult to understand. The ones I make most frequent use of are (&gt;&gt;&gt;), i.e. forward composition, (* * *), and (&amp;&amp;&amp;). You might like Conal Elliott's post on [semantic editor combinators](http://conal.net/blog/posts/semantic-editor-combinators/) 
The final version: rleEncode = map (length &amp;&amp;&amp; head) . group rleDecode = concatMap (uncurry replicate) 
Built from scratch for a task, sold as part of a licensed product. I'm guessing (I'm on vacation) it's still live :)
vacation... 
Ace, looking forward to an updated haskeline in ghci
And the subject of his talk will be...?
Re-posted from the proggit thread: I recently wrote this algorithm (which I think is elegant) in haskell. It is related to the [postage stamp problem](http://en.wikipedia.org/wiki/Postage_stamp_problem), and solves this sub-problem: *you may use up to (n) different denominations of stamps to allow you to apply the exact postage (p) to a letter; enumerate all the unique sets of stamp denominations that can be used to apply the postage (p) to a letter. Each set should be ordered.* sumLists :: Int -&gt; Int -&gt; [[Int]] sumLists = f 0 where f _ 1 n = [[n]] f i c n = do a &lt;- [i.. n `div` c] as &lt;- f (i+a) (c-1) (n-a) return (a : dropWhile (==a) as) 
which are also [fairly efficient](http://coder.bsimmons.name/blog/2009/05/run-length-encoding/), compared to more explicit definitions.
short primes generator: nubBy (((&gt;1).).gcd) [2..] Edit: fixed
... for him to decide, but likely very interesting for Haskellers :-)
It helps to look at the definition of **filterM**: filterM pred [] = return [] filterM pred (x:xs) = do cond &lt;- pred x xs' &lt;- filterM pred xs if cond then return (x:xs') else return xs' So, for each element **x** in the list, **filterM** uses the *monadic* predicate function **pred** to determine whether **x** should be included in the result list or not. Since we are executing inside the List monad, **pred** can return more than one value, and the subsequent code is computed for each answer, in this case [True,False]. The same thing happens for the line "xs' &lt;- filterM pred xs". **filterM** returns all the possible filtered lists, using the filter function **pred**. So, the if-expression will be evaluated for the product of all the possible values of **cond** (either True or False), and all the possible values of **xs'** (all the possible subsets of **xs**). The result is the power set of the list **(x:xs)**. [Edit: Edited for accuracy]
Hmm, unbalanced parentheses.
I can recommend Hughes' paper ["Generalizing Monads to Arrows"](https://wiki.ittc.ku.edu/lambda/images/9/95/Hughes_Generalizing_Monads_to_Arrows.pdf)
Wadler's [functional pearl on arrows](http://homepages.inf.ed.ac.uk/wadler/papers/arrows/arrows.pdf) has a really beautiful notation for arrows; much better than the `&gt;&gt;&gt;` or `proc` business.
I can't fit in any of the code examples I am thinking of, but I am enamored with code where Haskell has allowed you to define _the right abstraction_ and everything else just falls from there. Tricky little one-liners just confuse me. 
&gt; I'm not sure I'd call this beautiful... more like monad golf. If you learn to understand the higher-level abstractions, it becomes possible to make use of them, in rather powerful and elegant ways. Until then, it's going to seem weird to you. Just as your version would seem weird to someone who's more familiar with loops than recursion.
cd ghc-6.10.3/ ./configure --prefix=~/haskell make install
I don't think I can post the code. But with a combination of fmap and Maybe's and mplus. I've managed to write "if free" or "case free" code when it comes to error handling in an event-driven textual IO based emulator of another system. Not having tons of ifs or cases or even where clauses with a bunch of pattern matching actually seemed to help the readability of this code, in that on any given line where an alternative to a (Just a) form of a Maybe lives it's counterpart for handling the case when the expression is evaluating a Nothing. This style gives me the ability to ensure every line produces a real value, and to guarantee that the function I'm writing is total. Example might be (shortened a bit) M.put $ fromJust ((expression) `mplus` Just (someDefault)) expression might be "fmap function maybeVal". (Note I'm not sure how to produce backquotes... :-( ) fmap returns the Nothing if the maybeVal is not a (Just a) instance, which hands it off to `mplus` which for Maybe's will evaluate the right hand side if the left hand side is Nothing. I love this kind of coding... 
Be careful, you might [upset Euclid](http://www.crystalinks.com/euclid.html): &gt; All accounts of Euclid describe him as a kind, fair, patient man who quickly helped and praised the works of others. However, this did not stop him from engaging in sarcasm. One story relates that one of his students complained that he had no use for any of the mathematics he was learning. &gt; Euclid quickly called to his slave to give the boy a coin because "he must make gain out of what he learns." 
upvoted for... euclid had slaves!?
very interesting, thanks :)
It dawns on me that my dog has a functional notion of time. "What time is it, Dolly?" "Now!"
I believe they're called "research assistants" now.
and if its going to be like his talk about dsls from last week (?), I'd say uninteresting for non-haskellers (because they don't get it). edit: but I thought it was great!
http://www.reddit.com/r/programming/comments/8rwjb/sending_null_to_devnull/c0a915v That is all
So what is it good for?
winterkoninkje gave a great response, my only addition is that on the practical side fundeps are much simpler to use for the most common needs. At least, thats my opinion after having used both.
Since reification of folds as data structures works so well in terms of being able to optimize arbitrary compositions, can the same idea be extended to things like fmap and others? I'm even thinking of stream fusion, which relies on rewrite rules and seems to be a little fragile at the moment. Is anyone working on things in this area?
Seems awesome! :-)
I am quite unworried that his talk might become uninteresting, for Haskellers or others in the PEPM audience.
Regarding the [final couple of slides](http://projects.tmorris.net/public/haskell-parsers/artifacts/0.9/chunk-html/ar01s23.html), one can make it even less noisy by writing it in an applicative style, giving in a syntax that resembles BNF grammars. I'm sure there's a few pointers in the docs for [Control.Applicative](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Control-Applicative.html).
We actually did that in the tutorial since one of the students intuited it so I showed him how.
Why do they use (&gt;&gt;&gt;) instead of (&gt;&gt;)? The types match those of (&gt;&gt;) not (&gt;&gt;&gt;)...
So as not to require the students to import Prelude hiding ((&gt;&gt;)) Also to prevent their confusion when they see &gt; :type (&gt;&gt;) (&gt;&gt;) :: (Monad m) =&gt; m a -&gt; m b -&gt; m b
How did you create the slide show HTML, I need to use that format for a project I am working on.
Although that's sort of more confusing, what with `Control.Arrow` and all...
The students are largely unaware of Control.Arrow but this thought did cross my mind.
Docbook Here is the source http://projects.tmorris.net/public/haskell-parsers/trunk/src/docbook/ There are other formats besides sliced HTML. Jump back a directory.
Well, thanks for the link. I was planning to post a link to reddit when I was nearer the half way mark, but looks like dons beat me to it. I'd really appreciate any feedback about better solutions (either here or on the blog) as I try to get my head around this mind-bending language; if anyone has the time or inclination [here's a better link for the Real World Haskell entries.](http://barryallison.wordpress.com/tag/real-world-haskell/) So far in I've found this book to be pitched at just the right level for me - as an experienced programmer - with a couple of years experience in the functional paradigm. I really like the way scope and motivation is given to the reader to go off and research some other parts of haskell and library functions. Thanks.
My translation of a C program to generate cnf clauses for the nqueens program. i wrote this when i first started learning Haskell. http://ramonhaskell.blogspot.com/
Every time I've need a parser, it's made me really happy. It'd be kind of lame to paste the code to a parser here though. Combining Applicative with Parsec works wonders. Haskell also seems to be pretty good at whacking large datasets. The laziness saves you a good bit of trouble, and the built in thread ops take a little getting used to but are neater and easier-to-maintain than the corresponding code in C++ or Java. Statistical NLP applications come to mind, with this sort of thing. Brian O'Sullivan pitched it for doing bio-research work. Haskell has been a pretty good language for writing backends, but so are OCaml and Python (of course with Python, you don't get the compiler checking things for you and it's sloooowww), so there's nothing leaps and bounds special there. That Haskell can sit in as a practical imperative language is worth noting though because you usually don't get that with research languages.
So, this *article* doesn't even explain what STM is..
the only example given: main = print . take 2 . extracts . mkRandR (1, 6) =&lt;&lt; getStdGen doesn't use anything of Comonad. so it isn't clear what is the benefit of it being a Comonad / why is it one. 
&gt;One thing I’m enjoying about the book is the attitude of throwing in extra parts of the language or libraries and encouraging us to go off and read about it ourselves for example introducing the Either data type and related Left and Right constructors. Having said that I couldn’t find a way of using Either without writing verbose (and ugly) code as I have to de-construct and reconstruct values throughout. It also seems like it will a maintenance headache as changing something minor in its use will require a lot of careful fiddly changes to the code. I hope late I’ll find a more elegant or robust way of using it but I’m already wary. Ah, Monads. As sigfpe said, you could have invented them yourself!
I forgot to mention how deeply satisfying is to get down and dirty and *impure* and mutate state all over the place. To pig out on state. From time to time, of course, we're all nice and proper here. Don, if you ever come to Argentina, be sure to email me up and I'll invite you to a bottle of wine.
&gt; To pig out on state That's awesome :)
I have just uploaded a minor version with an actual comonadic example: The following function generates an infinite list of dice throw sums with `n` dice: rolls :: RandomGen g =&gt; Int -&gt; g -&gt; [Int] rolls n = extracts left . -- Extract an infinite list of the sums. fmap (sum . take n) . -- Sum the first n values of each list. extend (extracts next) . -- Group them into lists of die values. mkRandR (1,6) -- Generate random die values.
What's the other kind of ping?
The duck. Also, actual ICMP echo requests.
Simple, but to the point. Haskell actually has a great set of libraries that makes small network operations fairly easy. Now if I could just get my head around building a threaded server.
It would be nice to see the relationship to value-supply [here : http://hackage.haskell.org/packages/archive/value-supply/0.5/doc/html/Data-Supply.html] spelled out....
That may be a good idea. Here are some of the differences I can think of right off the bat. I believe value-supply's results are not always reproducible in the presence of parallelism because all derivatives of one Supply rely on the mutation of a single IORef. This package requires that the generating function be implemented as a RandomGen instance. This happens to provide for more flexibility as far as how splitting works, but may limit its applications anyway. value-supply doesn't provide the Comonad interface, of course, although category-extra's Control.Comonad.Supply does. I would use value-supply or Control.Comonad.Supply if I only care that the values are unique, but I don't know if I would want to use it if I care about anything else due to the reproducability problem.
I have very mixed feelings w/r/t the appearance of an Acme category on hackage.
Slightly silly, but a great example of the scaffolding needed to write a server.
EDIT: I should clarify. This code isn't that much longer than the OPs (still fits in a reddit comment), but it adds threads. I had it lying around so I thought it might be encouraging to share. Threaded import Geography import System.IO import Control.Concurrent import Control.Concurrent.STM import Network import System.Environment ( getArgs ) import Control.Monad ( liftM ) data Command = Say String | Travel Direction deriving (Show, Read, Eq) forever :: IO a -&gt; IO () forever a = loop where loop = a &gt;&gt; loop rooms :: IO String rooms = readFile "rooms.txt" main :: IO () main = do rs &lt;- rooms case (roomsFromString rs) of Left e -&gt; putStrLn e Right (r, w) -&gt; withSocketsDo $ server r w serverSocket :: IO Socket serverSocket = do (portNumArg:_) &lt;- getArgs let portNum = fromIntegral (read portNumArg) listenOn (PortNumber portNum) getClientLn :: Handle -&gt; IO String getClientLn = liftM chompCR . hGetLine where chompCR "\r" = "" chompCR "" = "" chompCR (a:as) = a:chompCR as server :: Room -&gt; World -&gt; IO () server r w = do sock &lt;- serverSocket forever $ do (h,_,_) &lt;- accept sock forkIO $ client h r w client :: Handle -&gt; Room -&gt; World -&gt; IO () client h r w = hSetBuffering h LineBuffering &gt;&gt; client' r where client' r = do hPutStrLn h (description r) clientIn &lt;- getClientLn h case parseCommand clientIn of Just (Travel d) -&gt; case go r d w of Just r' -&gt; client' r' Nothing -&gt; hPutStrLn h "Can't go that way" &gt;&gt; client' r Just (Say s) -&gt; sendMessage s &gt;&gt; client' r Nothing -&gt; hPutStrLn h "Invalid command." &gt;&gt; client' r sendMessage :: String -&gt; IO () sendMessage = const $ return () parseCommand :: String -&gt; Maybe Command parseCommand "n" = Just $ Travel North parseCommand "s" = Just $ Travel South parseCommand "w" = Just $ Travel West parseCommand "e" = Just $ Travel East parseCommand "u" = Just $ Travel Up parseCommand "d" = Just $ Travel Down parseCommand "nw" = Just $ Travel NorthWest parseCommand "ne" = Just $ Travel NorthEast parseCommand "sw" = Just $ Travel SouthWest parseCommand "se" = Just $ Travel SouthEast parseCommand ('s':'a':'y':' ':xs) = Just $ Say xs parseCommand _ = Nothing Let me know if you want Geography.hs.
Lovely example of a threaded server. However, when I've made similar servers, I've had problems with the process dying when a client behaves strangely. I had a simple web server, and when I would have several browsers hit it at the same time and disconnect before the full response in sent, process dies. I tried a pile of different 'catch' combinations, and nothing I tried (I must admit, I didn't try for too long) worked. Does your server work with misbehaving clients?
Thank you; added to the list.
The photo was taken during a presentation by Ryan Trinkle about why his game company is developing using Haskell.
Yknow the really funny part of that is it really is a good reason for a company to start using Haskell... if it is true...
That's one way to avoid success at all costs...
Haha, this picture loaded in really slowly, so I actually laughed out loud when the third item eventually came up
Well, first we had programming "evangelists" and "computational theologists" (Sun's terminology, relating to Java people, from the late 90s), and now we have "fundamentalists". I'm not sure this new term is appealing or helpful, from a marketing perspective.
I don't know. This is/was a toy, never put it under much duress.
Well, you need to take into account who the speaker is. Erik is not really known for his moderate positions ;)
Do you know if this presentation is available online?
Galois is hiring. We need more Haskell programmers. http://www.haskell.org/pipermail/haskell/2009-June/021401.html *Please send us your resume*
Love the last point. I enjoyed learning and playing around in Haskell, and I realized that unless I was there when Haskell was first made (or at least have 10 yrs exp playing with it), I will not be programming in Haskell in a job setting.
You guys are still hiring? I lack confidence in my abilities, and so haven't applied, but if you're having trouble finding people...
We've hired, but now we need more!
Needs bettur whyte balanz!
Any possibility Galois might hire an undergrad, even if only for fairly menial tasks?
Generally you'll need an undergraduate degree in computer science.
Something like this would be nice for Arch as well
A harder one (that I half-understand): powerset = filterM (const [True, False])
seems nice, but fails to update from 6.10.2 to 6.10.3 (using paludis), maybe it's a good moment to help
Thanks. 
There was another thread where this came up, and someone there gave a rather neat explanation, which I will try to paraphrase: The `const [True, False]` here in the `filterM` basically says that for every element in the argument list, both select it *and* reject it. Because both actions are taken for every element in the list, you end up with every possible sublist, ranging from all elements rejected (the empty list) to all elements accepted (the original argument list) and everything in between.
Note that there is an adjoint pair `pair . unpair = id` such that his function is mapEveryOther f = pair . (map f *** id) . unpair I like to think of it as a (real, physical) zipper.
See [here](http://byorgey.wordpress.com/2007/06/26/deducing-code-from-types-filterm/).
I don't think it is, but next time I see Ryan I can ask him if he'll post it. Most of the talk was on porting GHC to the iPhone.
That only works for lists of even length. At least, I don't see a way to make it work in general.
Or see [this picture](http://cale.yi.org/share/filterM.png)
Yeah. Given the general competence of Haskellers and their desire to get paid for using Haskell, it's actually a pretty good reason. For a lot of companies it should offset the smaller hiring pool, too. 
&gt; both select it and reject it In case anyone finds it weird that we can do *both*, remember that the monad in question is the list monad, otherwise known as the [non-determinism monad](http://www.hvergi.net/2009/02/the-non-determinism-monad/).
I believe you are mistaking `(map f *** id) :: ([a], [b]) -&gt; ([c], [b])` for `map (f *** id) :: [(a,b)] -&gt; [(c,b)]`.
That works for all lists. That's why I didn't say "isomorphism" but "adjoint pair": unpair [] = ([], []) unpair [x] = ([x], []) unpair (x : y : zs) = (x : xs, y : ys) where (xs, ys) = unpair zs pair ([] , []) = [] pair ([x], []) = [x] pair (x : xs, y : ys) = x : y : pair (xs, ys) Proving the adjointness of `(pair, unpair)` is trivial.
After I wrote that I vaguely recalled that `Arrow.first f = f *** id`, no? Hence mapEveryOther f = pair . first (map f) . unpair 
Wow, thanks. I have a few examples. The part I need to work on is really understanding the inter-process communication via MVars and STM. Bookmarked for reference.
&gt; version 1.6 of the Cabal library includes support for parsing the output of “ghc-pkg check”, so we’re able to use this to have haskell-updater find these broken packages for you as well! Almost, but not quite. Cabal version 1.6 supports reading the ghc-pkg database into a package graph and it provides a function to find packages in that graph with missing dependencies. In other words that Cabal lib is doing all the work, not calling `ghc-pkg check`. It makes sense to do it this way because if Cabal called ghc-pkg all the time then things would be really slow. Cabal calls ghc-pkg once, gets all the data and then does everything else purely in memory.
This is probably a bad example then, I was mid putting in/ripping out the chat mechanism with `Chan`s (and trying to switch to STM) , so it doesn't actually do any interthread communication at the moment.
I find all the explanations given, including the replies to this post, lacking. There is a fairly simple explanation though, just look at the definition of (&gt;&gt;=) for lists, and it all falls out. filterList :: (a -&gt; [Bool]) -&gt; [a] -&gt; [[a]] filterList _ [] = [[]] filterList p (x:xs) = concatMap foo $ p x where foo flg = concatMap bar $ filterList p xs where bar ys = [if flg then x:ys else ys] 
I'm not sure I understand his comment about module overhead. Wouldn't this be a perfectly fine Haskell file if he left out the "let"?
That is, there is a pool of good programmers who are (probably) ready to be hired.
That'd be awesome if it's not too much of a bother. I haven't heard much about Haskell in that area.
I'll be honest it is why most of the programmers I have hired lately even for non-Haskell work are Haskellers. The skillset and personality it selects for are valuable in other languages as well.
How about opening a Canadian bureau?
Unemployed? I have yet to find a haskell hacker in the Boston area. Seriously, we require three skillsets: Django/Python, C and Haskell. How hard can that possibly be?
usually we use something like an 'evens' or 'odds' function for a list
If you think of a monad as a "box" with a value in it, the list monad is like Schroedinger's cat box.
module SomeModule where ...
Thanks. The overview paper is very interesting.
Like mentioned by eridius, expanding the definitions of the monadic combinators helps less than a high level view on their meaning in a non-determinism monad. What would you think does the following function do: sublist = filter (\_ -&gt; True or False) If you replace "or" by the "?" operator, then this is actually valid [Curry](http://www.curry-language.org/) code. (Curry is a language similar to Haskell with built-in non-determinism so you don't need a monad).
&gt; Why Haskell? Why not?
Pretty hard if other C programmers are like me and have never been motivated to even investigate Django, never mind properly learn Python.
There are quite a few of us in the Boston area actually. You might try showing up at the Boston Haskell User Group some time to find talent. That said, most of us (in this area anyways) are actually employed. ;) http://groups.google.com/group/bostonhaskell
Functions are a well known example of exponentials in category theory. You can view the types categorically and this derivation drops right out. An example of a paper using that terminology: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.1.7380 But the notion goes back a lot farther than that. See: http://en.wikipedia.org/wiki/Exponential_object 
&gt; A pair colleagues of mine and I have been staring at an interesting riddle, which I’m guessing exists in the literature somewhere. Yes, these are known as [exponential objects](http://en.wikipedia.org/wiki/Exponential_object).
Awesome. And comparing to the [wp article](http://en.wikipedia.org/wiki/Exponential_object) people are linking too this post is much more readable and insightful. It explains the reasoning and not just that "this is how it is defined".
Right, but that's optional.
will you be my mentor?
Why stop at exponentation? What are [tetration](http://en.wikipedia.org/wiki/Tetration) types, and how does it work with the rest of the [hyperoperator](http://en.wikipedia.org/wiki/Hyperoperation) heirarchy?
Well we're looking for Python and Haskell programmers that are comfortable with C, not C programmers comfortable in the other languages.
Yes I should have specified unemployed haskell programmer. Good to know about the group. I was looking on meetup.com for a haskell group (more to participate than recruit) and was depressed to find none.
Looks cool. What are they useful for? Not immediately obvious from link nor google. I'm guessing it makes it easy to construct tuples with some defaults e.g. map (10,,100) [1..10] But can't immediately think of a use for that...
If nothing else it's nice to have the consistency. You can already take sections of nearly all other infix operators. The tuple constructors are basically a family of n-ary infix operators. Right now you can't even take sections of pairing like (,x) and (x,) even though they are morally no different than (+x) or (x+). This n = 2 case will no doubt be the most common in practice due to the prevalence of pairs. It's funny that you happened to use [1..10] in your example. That's another infix operator that doesn't allow sectioning. The meaning assigned to [n..] makes that impossible to change. You could allow sectioning of the left operand but the asymmetry would be criminal. As [n..] is very suggestive notation for what it currently does, not to mention that it represents a more frequent use case than sectioning, this is not a great loss.
I first encountered Haskell round about 1996. The occasion was the first time the Scottish Go Congress was held in Glasgow. John O'Donnell was organising it, and on the Friday evening several of us were in a bar. I asked him what Haskell was like, but he seemed more interested in playing Go than describing it in any detail. So I did some probing on the internet later that week. Back in November 2008, I took another look at the language.
Nice analogy.
I'm usually on the freenode #haskell irc channel as edwardk and I'm pretty open to answering questions/directing people towards resources, 
Cool! I've often found myself doing: (,) x . foo . bar and even changing code so that I can even make the above composition, when perhaps I would prefer the 'x' to be snd in the tuple. will this be an extension requiring a flag, or just work?
I was expecting something like [this magic](http://stackoverflow.com/questions/1160702/values-inside-monads-nested-in-data-structures): &gt; reduce [(["ab", "c"], "12")] :: [(String, String)] [("ab","12"),("c","12")] &gt; reduce [(["ab", "c"], "12")] :: [(Char, Char)] [('a','1'),('a','2'),('b','1'),('b','2'),('c','1'),('c','2')] &gt; reduce [("ab", "12"), ("cd", "3")] :: [(Char, Char)] [('a','1'),('a','2'),('b','1'),('b','2'),('c','3'),('d','3')] 
 {-# LANGUAGE TupleSections #-} It might make it into a new Haskell standard, but probably not the next one (Haskell 2010, if that happens).
I don't think it's that unreasonable. The syntax of infix binary operators lends itself easily to partial application. The same is not true of other syntactic forms like `if then else`, enumeration `[n..m]`, list comprehensions, etc etc. Tuples are somewhat on the boundary.
Not to be a boor, but I submitted my resume and never even received a "Dude, you are so under qualified." Is that normal, or did my resume get lost in the shuffle?
OMG. I've wanted this feature for *such* a long time.
What did I say was unreasonable? Enumeration seems like a pretty clear-cut example of a morally infix operator. Syntactically it is exactly analogous to the 2-tuple constructor syntax (x,y) in that there's a separator token and two bracket tokens. If not for the wish to support unbounded enumeration with the same syntax (a good call, in my opinion) you could have used (..) :: Enum a =&gt; a -&gt; a -&gt; [a] and it wouldn't have looked substantially different from what there is now. That would not cover mixed list construction and enumeration but I cannot remember when I last saw that used in practice. Anyway, the convenience of unbounded enumeration is for me significantly greater than enumeration sectioning. The syntactic incompatibility of the two I see as no great loss. (In an APL-derived language you could have supported both with the same .. operator by having distinct monadic and dyadic denotations. APL traditionally denotes enumeration by Greek iota, crudely transliterated into ! in some later languages like K with ASCII-based syntax, and the monadic version enumerates up from 0. With separate left monadic and right monadic denotations it would be simple to support ranged enumeration, enumeration from below and enumeration from above over arbitrary enumerable types, all with the same syntactic operator. But I digress.)
I thought the term was already [defined](http://en.wikipedia.org/wiki/Magic_(programming\)). The RoR example of a function accepting a string is particularly weird for me : why say it's magic while "unspecified" or "not documented" would be accurate ?
I remember years ago when it took a full 3 minutes for Mathematica to calculate Pi to 50,000 digits on a new PC.
&gt; I forgot to mention how deeply satisfying is to get down and dirty and impure and mutate state all over the place. To pig out on state. Ocaml programmers are such perverts!
But this is still limited to Haskell 98 right?
Anyone know of a mirror? This one just won't play through. Thanks.
Don't let Haskell into the hands of Russians or we are all doomed.
When will it compile with ghc-6.10?
I just installed it with ghc-6.10.3...
like that oleg guy, huh?
Does it support tabs yet? I've not used an IDE in some time and was interested in using Leksah for Haskell development, but its dismal tab support drove me back to Vim. I find it strange that an editor for a whitespace-delimited language would lack this (maybe just to me?) important feature. Perhaps the Leksah developers use braces and semicolons instead?
Bear in mind you may need to first run: cabal update
cabal is good but you could do something like adding -lglut somewhere for the GHC 6.8 users like me who have to build manually : ghc --make -lglut defend.hs
I find it strange that a programmer for a white space delimited language would want tabs, which are displayed as a different number of spaces in many editors ;-) . I think Leksah devs probably considered this a feature, so you don't get compile errors when things are visually lined up.
If you use vim for editing haskell, it's worth noting the excellent [vim-haskellmode](http://projects.haskell.org/haskellmode-vim/).
Variable width is the reason I use tabs -- the indentation width I prefer might be too wide for somebody else, or vice versa. In an editor that properly supports tabs, mis-alignment does not happen. That's what I mean by "dismal tab support" -- Leksah is behind even Notepad.
There is Haskell code that actually changes meaning depending on the width of tabs. The Haskell standard says tab is eight spaces, you're asking for very confusing bugs if you choose to display them in any other way.
The only reason tabs might be confusing is if the editor renders them poorly, eg as blank space.
Interesting point. The sense I get when I hear people complaining about magic code is that, although the intent might be that the underlying behavior and implementation need not be of concern to the developer, that often ends up being not the case. So, all magic code becomes suspect, because in the end there are too many people for whom that magic is voodoo.
Visible or not, if you render tabs at any width other than 8, how would the user know the meaning of the code without working out in there head what it would look like with 8 character tabs? If you fix that by converting the tabs to spaces when you save the file how is some other user going to open it with different tab width (the benefit of tabs you were asking about)? Personally I am pro-tabs in other languages (at the start of lines only). In Haskell though, I think they are a recipe for disaster. Having said that Leksah still needs work to make indentation easier. For instance deleting a space at the start of a line should automatically delete back to the last sensible indentation point (it should examine previous lines to work this out).
I know that there have been a number of posts in the Haskell reddit relating to dependent types, so I thought there would be some interest in this new reddit.
I want to try out Leksah, but `gtk2hs` and its gtk dependencies always fail to install on my MacPorts installation. :-/ By the way, how do I install Leksah via MacPorts? There doesn't seem to be an official portfile, does there?
using -lglut doesn't work on my comp (mac). is it specific to linux? specific distro, etc?
Try out http://leksah.org/Leksah-0.6.1.0.dmg it includes everything you need to get started except GHC (you can download the Haskell Platform dmg for this). Alternatively if you want to use MacPorts there are instructions in the manual on how to install with MacPorts. There is no Portfile for leksah itself instead (once gtk and gtk2hs are installed with MacPorts) run... cabal install leksah
Thanks for the link to the dmg, but unfortunately I'm on PPC / Tiger. Installing gtk2hs with MacPorts is the problem; some components just fail for no apparent reason, like `gnome-keyring` throwing linker errors today. Upgrading gtk from scratch will probably solve that but it always takes half a day to compile... Is there a precompiled gtk for PPC / Tiger out there?
I think it's pretty sweet. It's always nice to avoid having to write boilerplate code, and using this, you are able to use the type system to tell exactly where in your data structure you want to apply your function. I think I'm going to use this myself :). The only problem I see is that you need to write 2^n - 1 instances for each n-tuple type. I'm not sure this is bulletproof, but I think it can be solved this way: {-# LANGUAGE FlexibleInstances, MultiParamTypeClasses, OverlappingInstances #-} class InnerMap a b fa fb where innerMap :: (a -&gt; b) -&gt; fa -&gt; fb instance InnerMap a b a b where innerMap = ($) instance ( Functor f , InnerMap a b c d ) =&gt; InnerMap a b (f c) (f d) where innerMap func x = fmap (innerMap func) x -- Added: An instance for InnerMap where we don't apply the function instance InnerMap a b a a where innerMap x = id instance ( InnerMap a b c e , InnerMap a b d f ) =&gt; InnerMap a b (c, d) (e, f) where innerMap func (c, d) = (innerMap func c, innerMap func d) instance InnerMap a b d e =&gt; InnerMap a b (c, d) (c, e) where innerMap func (c, d) = (c, innerMap func d) -- Added: An instance for a three-touple. instance ( InnerMap a b c f , InnerMap a b d g , InnerMap a b e h ) =&gt; InnerMap a b (c,d,e) (f,g,h) where innerMap func (c,d,e) = (innerMap func c, innerMap func d, innerMap func e) instance InnerMap a b e f =&gt; InnerMap a b (c, d, e) (c, d, f) where innerMap func (c, d, e) = (c, d, innerMap func e) I had a weird compile-time error, where the compiler complained about overlapping instances when **func** is only applied to the last element in the tuple. It is therefore necessary to create two instances for each n-tuple type: one where **innerMap** is applied to each element in the tuple, and one where it is only applied to the last.
Well, this is how the regular expression library works. The matching operator (=~) returns different things depending on the type you give. 
&gt; Visible or not, if you render tabs at any width other than 8, how would the user know the meaning of the code without working out in there head what it would look like with 8 character tabs? Because the meaning is the same, no matter how widely the tabs are rendered.
yes, according to what I read on http://www.haskell.org/haskellwiki/FieldTrip (end of page) They talk about lenny but with jaunty I had to do it too
Specific to this discussion: What I understand you saying is it doesn't matter because each tab will line up with the other tabs. What I understand others saying is that it does matter because source code invariably mixes the use of tabs and 8 spaces, so if an editor _displays_ tabs as four spaces then you might manually change the line indented with spaces to be four spaces so its visually lined up - which would be wrong, you'd get a compile time issue! To do with Leksah: Leksah handles tabs properly, imo, by not inserting tabs at all and instead inserting a variable number of space characters when you press the tab key. As Hami said - the thing Leksah doesn't get right is intelligent indentation on tab or shift-tab. Edit: A code example of "four space tabs" causing an issue - just to beat the dead horse and make sure there isn't a communication breakdown. func a b = do let x = 3 y = 2 return (a + x, b + y) Pretend line 2 is tab-indented (but rendered with a width of 4 spaces) and the "let " on that line is obviously four characters. If you indent "y = 2" using two tabs then this issue comes up (2 tabs = 16 spaces according to GHC, but rendered as properly lined up / indented 8 spaces)
There are many ways to do automated deriving. Via DrIFT, via an SYB-generics script (e.g. for JSON or Data.Binary (http://code.haskell.org/binary/tools/derive/ )). Via the 'derive' tool: http://hackage.haskell.org/package/derive
Sure if you leave the tabs in the file sent to the compiler it will always have the same meaning. But it will be hard for the user to work out what that is if the tabs are rendered as 4 (since the compiler uses 8). I think the Tabs section in the following explains the issues better than I have http://urchin.earth.li/~ian/style/haskell.html
to solve the weird error you need to remove the functor instance (and make a list instance instead + every other functor of interest..). GHC doesn't check the contexts when finding matching instances, and so it take "InnerMap a b (f c) (f d)" as fitting when f is the "(,) a" of your tuple (even though "(,) a" is not a functor, as it is ignoring contexts). I think this solution make this not work with tuples inside tuples. But I'm not sure about this. Imho it would have been cool if tuples of more than 2 were just syntactic sugar so that (a,b,c,d) is (a,(b,(c,d))). That would had removed the need for the 2^n instances as well as removing the need for zip3, zip4, zip5, etc. 
I know I can specify in cabal compilation options to be specific to linux, but do you happen to know if there is a way for me to specify in the cabal file that this flag is only for debian? 
Ironically, that page contains an exact description of how Leksah should behave! When tabs and spaces were highlighted as on that page in an editor, they become obvious and easy to work with.
&gt; because source code invariably mixes the use of tabs and 8 spaces Only if the editor doesn't support tabs properly. In an editor which does, such as Vim, Emacs, Eclipse, GEdit, Visual Studio, etc, tabs and spaces are rendered so that the programmer may see them directly.
Jesus you guys are wizards.
Personally, I don't like this style because it's somewhat inconvenient to use: basically you have to write a type most of the time when you use "innerMap" otherwise you get a type error. But "abusing" the type system seems like a fairly strong way to put it. In some contexts, it might make sense.
I don't think it's possible but I can be wrong. However, I think a lot of linux distrib will have this problem between glut and ghc6.8 (I don't know about ghc6.10) so I think this problem is distrib-independant (the webpage I mentioned shows what happened but not all what may happen).
heh woops, guess i missed that one
Did you read their conclusion? &gt; So, we have eliminated both options; what is left? Simple: **Do not use tabs. At all.** My real fear if you allow tabs at all is that someone will end up mixing them. For instance imagine the "buy" line started with a tab, but the others spaces... f=case 1 of 0-&gt;case 1 of 0-&gt; "" 1-&gt;"buy" _-&gt;"sell" Even if they can see the tab the user will have to remember that though they are displaying 4 char tabs the compiler will use 8 and the result of the function will be "sell" not "buy". Worse still if you share the code with someone who uses an editor that does not show tabs. This is not to say that your requirement for changing the indentation amount should be impossible. It is just that using tab characters is not the way to do it in Haskell. We need a Haskell aware tool to reformat the code without changing the meaning or breaking compilation.
I see, thanks for the explanation :). Is there a reason for this behavior, or can it be turned off by a compiler directive? Creating instances for each Functor of interest is also a lot of repetitive work, but maybe if the compiler could derive instances for us, this would be a good solution (like **Typeable**). It would also allow innerMap to be used on one's own data types. I don't see a reason why it can't work with tuples inside tuples, but there may be something that I have missed. I'm going to try it out later. I've always thought zip3, zip4, zip*, etc. and the like was a bit hackish, so yes, a more general way of representing tuples would be really great! Something like what you describe could probably be created using phantom types, encoding the size of the tuple in the type, but I'm not sure.
I think Conal's [Semantic Editor Combinators](http://conal.net/blog/posts/semantic-editor-combinators/) are a simpler solution that is basically just as short. If I make a few imports and aliases: import Control.Monad(join) import Control.Arrow((***), first, second) each :: (a -&gt; b) -&gt; [a] -&gt; [b] each = map I can now: &gt; (each . second . each) ord [('a', "bc"), ('d', "ef")] [('a',[98,99]),('d',[101,102])] &gt; each (ord *** each ord) [('a', "bc"), ('d', "ef")] [(97,[98,99]),(100,[101,102])] I think this is nicer, as it is more explicit about the "path" being taken, doesn't require overlapping instances.
&gt; basically you have to write a type most of the time when you use "innerMap" otherwise you get a type error This is true for the examples given. But I think that sometimes when it is used inside a bigger piece of code you will not need to specify the type because the types of its input and output may already be inferred. 
Thanks, for some reason I totally overlooked the derive tool and DrIFT in my search.
Perhaps this (extending the deriving mechanism) should be a chapter in ... RWH II: The Type Error Strikes Back
&gt; you will not need to specify the type because the types of its input and output may already be inferred. True enough, but my experience has been that such things happen less often than you would like...
what do you mean?
Dons is one of the authors of [this book](http://book.realworldhaskell.org/read/), My suggestion was that -- if there is to be some sort of sequel, given that this is a real-world topic, it would be good to put such a thing in said book. The title is a pun, based on the title of the second (actually fifth, but second of the original three) Star Wars movies.
The mouse positioning seems a bit off
Yeah cool, this actually works!
Jesus, man. That's less of a cheatsheet and more of a cheatpamphlet! That said, I'm sure it will find lots of use and appreciation.
I've added a link to this from the haskell.org main page. Great work.
"Snow Leopard is reported to fail due to a 32/64-bit incompatibility between GHC and XCode." This is in a way spectacular news, if it forces everyone's hand to make Haskell on OS X 64-bit by default, as it should be.
See, haskellers are cheaters!
Yep. A couple of idea to improve: * remove all commentary text * drop alternative syntax (eg. ['a','b','c'] for string...) * write a "table of contents"
Not a bad bunch of exercises, though I do have an issue with the names used. If someone doesn't know what words like 'monad' are, I don't see how substituting more meaningless words like 'Misty' helps any. Especially when functions like 'banana' operate on them. At least use analogies that make some sense! Otherwise you may as well just use math-style cryptic variables like many tutorials do.
so what? almost all linux users will use package management to install the system. that can come from any of a thousand mirrors 
Sure, the comparison to Linux is misleading for the reason you just gave. It's important to be aware of that. But still, the fact remains that there are a lot of Haskell users on Windows.
Just to be fair, I use Haskell at my universities computer labs and every day I make at least one download. 
Could you possibly keep (at least the installer) on a usb stick?
There are a lot of Windows users. There are lots of random bloggers stating Haskell is a silver bullet, so some percentage is sensitive to that. What you do is extrapolating that to number of users based on nothing. When you want to distribute anything remotely related to the truth, you might want to parse the whole mailing list archive and find unique user names on all the Haskell mailing lists and filter out all the complete beginner- and students questions, assuming you are referring to both academic and industrial use. 
Ah that'd necessitate buying a USB disk, but I could always email the installer to myself that way it'd only use network traffic within the university. I hadn't thought of the issue before.
Oh, very interesting. You mean you download the full platform toolchain every day?
Could you elaborate on why should it be 64 bit by default? I would guess that most OSX users uses laptops with less than 4 gigs of ram, where using 64 bit pointers is just a waste of resources (or so I would believe)
Do I sense some Windows hatred? 
Well I do this: 1. Download the installer 2. Download Cabal 3. Grab whatever I need that day from cabal So I usually end up with the latest versions of everything. 
I'd love it if Haskell Platform built profiling enabled libraries.
A good estimate might be the number of "cabal install hlint"s, if Hackage provides these statistics..
With regard to multicore parallelism, I'm curious whether the changes going into GHC 6.11 will come close to making Haskell the champion of performance as well as I imagine presently the cleanest, and most maintainable.
I decided to learn Haskell because I kept seeing it mentioned when peopled talked about functional programming. It didn't seem quite as common as Lisp, but people talked about it with the same reverence. I also noticed that when people talked about functional programming they seemed to regard Haskell as being the closest to the ideal and the "most functional" language. I figured if I was going to hop in and learn a new functional language, I should go all the way and just go learn the most functional of them all. So I grabbed Real World Haskell and dove in, and I'm glad I did. Sounds like I wasn't the only one to notice.
The Haskell Platform installer contains cabal. I'm confused...
Oh apologies. I had a brain snap, and was thinking about [GHC's release](http://www.haskell.org/ghc/download_ghc_6_10_4.html). That's what I download, then cabal, then configure it myself. I guess I've been doing it the hard way out of ignorance.
Relatedly, sort of, I've one can use GHC.Generics and derivable type classes: http://www.haskell.org/ghc/docs/latest/html/users_guide/generic-classes.html I have, however, never seen any actual example of this being used anywhere! (So, if you have one, please share)
On page 24, above Section 1.2.4, you said *head $ permute [1,2,3]* would only give three results, without duplicates. However, I just tested it in Muenster Curry Compiler 0.9.11, and I did get six results with duplicates?
I agree that Haskell IDE's should provide the power of the dot. However, I think that this approach is the wrong one, and I don't think syntactic concerns such as resolving names should use higher-layer information such as types. I think this would make type inference unnecessarily complex. I think the right approach is to integrate Hoogle into the IDE. If the IDE knows the type of the current expression (and even the type the result should be), it can use a Hoogle query to create possible completions. The Hoogle query will be more complete, more flexible and more accurate than the typical "dot resolution" of OO environments.
getAllValues is undefined?
A fairly good article for the most part, but there are a couple of bad mistakes: &gt; Because it is based on Lambda calculus, Haskell programs can be provably reliable &gt; Hackage [a Haskell wiki] &gt; Haskell98 was the first full implementation of the language to be released. This is also an odd way of looking at things: &gt; Variables in Haskell, for example, cannot be changed; rather, developers pass functions to those variables, and receive new variables as the output.
If they are sticking with STM then I'd say no. Share-nothing architectures (e.g. Erlang) will always beat STM.
I think Haskell needs an overhaul of the record system and something like this is very desirable. But this proposal is not it. It's just creeping featurism. It introduces a new way to do overloading which is similar to the existing class system, but yet different. Being based on knowing actual types it also breaks Haskell's existing nice abstraction facilities. E.g., ... foo.x.y ... bar.x.y ... cannot be changed to let getXY a = a.x.y in ... getXY foo ... getXY bar ... 
Haskell isn't "sticking with STM", Haskell offers STM as *one* option (share-nothing threads is another, but even that is not "it"). Languages which pretend that there's a single paradigm suitable for all concurrency/parallelism problems will always lose to languages that offer a wider variety of tools for all the different kinds of scenarios that pop up.
&gt; Languages which pretend that there's a single paradigm suitable for all ... problems will always lose to languages that offer a wider variety of tools You could use the same argument to conclude that Python is better than Haskell (because you can do functional programming in Python too, but you can also do other paradigms), and to conclude that "goto" should be added to Python to make it even better. The competing philosophy of pureness claims that there is probably a single paradigm that is awesome for everything, a "unified theory" if you wish. And I think this philosophy is what brought to the development of Haskell. 
I don't think it's the same. The difference is that it's been fairly well demonstrated by now that ad hoc side effects are not needed (Haskell still has them, only isolated), but for concurrency we're not even close to knowing what paradigms are best for what problems, and what are sufficient. Arguing for message passing as the only concurrency paradigm today strikes me more like as arguing for purely functional programming as a practical solution in the early eighties. It turned out that the main problems were (eventually) overcome and Haskell is the embodiment of that research, but there was no way of knowing that back then! So if you were looking for a general programming solution in the eighties, you probably wouldn't have gone with pure FP. Message passing is not even semantically suitable for all kinds of problems, so even before we attempt to prove that it's viable implementation-wise, and IMO there's a lot of work ahead to do that, you have to show how to improve it so it's even desirable for things which don't fall out in nice "actor" metaphors (it has the exact problem "goto" has, incidentally, in that control flow "jumps" all over the place to different threads in a non-structured, difficult to follow, fashion, except via messages rather than gotos - where's the declarative/FP equivalent for concurrency? Join patterns? I have no idea, but it's clear nobody else really does either which is why, for now, any practical language will need to support multiple paradigms). And, finally, the biggest difference is that ad-hoc side effects actually interfere with purity, whereas having the option of STM, at least in a principled way like in Haskell, doesn't interfere with message passing (you can easily choose to do one or the other, or both in the same app - a thread without any STM variables passed to it can not share data with any other thread, so as far as that thread is concerned, it's a share-nothing world).
Ok, so I think you are saying: * Practically at the present: It would be best for a language to support several paradigms. * Theoretically (and perhaps practically but in the future): There might be one single paradigm which works great for everything. If that's the case, then I have no criticism on this opinion, and maybe I even agree. 
Yes, this shows why tech journalists should get their work checked over by their interviewees, if they are not confident about the subject.
Yep, that's pretty much it. There may be a single paradigm in the future, but we're not anywhere close to that day right now. I happen to think that it won't be message passing, at least not in any form recognizable to us today. Concurrency/parallelism really is a scary outlier when it comes to technology advancement. E.g. when people starting seeing the complexity of software becoming untenable, they could go to academia and say "how do we fix this?" and academia would say "higher level of abstraction, less state, automatic memory management, formal proofs, strong typing" etc. from which the industry could pick and choose to improve the state of the art. This time around people are going "how do we program multiple cores?" and academia is going pretty much "we don't know lol". The state of the art, bleeding-edge, research into concurrency/parallelism is really not nearly as far ahead of the mainstream as in other areas. Which is pretty scary. Anyway, the point is that we need solutions today that are many years away from being proven, which means we pretty much have to take our best guess from what we can get, use it in anger, and see what sticks.
That last one just seems like a typo, surely me means "pass those variables to functions"?
Why not just introduce the function (#) :: a -&gt; (a -&gt; b) -&gt; b rec # field = field rec Using this, we can write **rec#** in the IDE, and the IDE can infer the type of the second argument to (Rec -&gt; b), effectively resulting in a list of fields.
I don't think there should only be *one* option, I just think STM is flawed. It's certainly better than manual thread management, just as a simple ref counting GC is better than manual memory management. But I'd rather have something more advanced.
I'm confused. from Breakout.lhs: ballPos ← integralVec ballPos0 ballVel ballVel ← storeJust ballVel0 $ toMaybe &lt;$&gt; (ballCollHorz ||@ ballCollVert ||@ ballCollPlayer) &lt;*&gt; (adjustVel &lt;$&gt; ballCollHorz &lt;*&gt; ballCollVert &lt;*&gt; ballCollPlayer &lt;*&gt; ballVel &lt;*&gt; ballNewVelX) How is ballVel in scope in the first line here? &gt; Elerea lets us express all these circular dependencies in a natural way, by simply referring to the respective signals by their names. (Breakout.lhs) What does Patai mean by "names"? the variable names? 
Like what? What's the alternative right now for multiple agents interacting with an ad-hoc, dynamically changing, subset of shared data? Transactions and optimistic concurrency seems to be the only viable solution for that so far (see databases), and for SMP machines that means STM (or HTM).
Haskell has two kinds of declarative pure-FP concurrency. The par / Control.Parallel.Strategies stuff, which is IMO wildly underhyped, and nested data parallel arrays.
I think any share state strategy is going to suffer scaling problems. I would look into things like Message passing, Futures, implicit parallelism, and so on. I'm sure we can get quite far with H/STM, certainly further then the horrid method we use now (locks) but I don't see it as having a good end game.
Sometimes you really do need shared state though. Eveyrone agrees that as much as possible you should avoid shared state, but this isn't something that's possible to do in general. If you can you should, but if you can't then STM is the most attractive solution I know of. STM and message passing (which Haskell has), futures (not needed in a lazy language), implicit parallelism (which Haskell has), etc. are not in competition. They're doing different things. STM is competing with locks and condition variables, not message passing. Saying STM is flawed and you should use message passing instead is a category error, IMO. Message passing doesn't do shared state, so it doesn't even try to compete with STM - it falls back to the old busted locks and condition variables (in disguise) whenever your problem calls for shared state. You can simulate locks with message passing (which is pretty much the way you solve these problems in Erlang and other message passing languages - big "server" threads which guard any shared state, brokering transactions), but they're still just locks and have the same problems. So really, if all you have is message passing the "end game" for these types of problems amounts to just using coarse grained locking again, and you already agree that this is horrid and STM is a better solution.
Neither of those deal with concurrency, they deal with parallelism. The distinction is important. How do we do coordination of genuinely concurrent events in a declarative way? I don't think there's a good solution to this yet, so we're back to using the concurrency-equivalent of "goto" (threads and messages).
I think you are focusing on code completion, but this proposal also is intended to solve a problem of overloading, or at least of duplicate identifiers coming from different modules. How does the function (#) that you propose solve that?
I came up with something similar some time ago, a generalized version of fmap. It ended up being useless, due to the 3 pages long, clueless type errors it tended to generate. Sometimes it is just better to be explicit. ---------------------------------------- -- The brilliant FunctorN class -- supercedes all fmap, fmap2, fmap3.. -- but produces headache type errors ---------------------------------------- fmap2 = fmap . fmap fmap3 = fmap2 . fmap class FunctorN a b c d | a b c -&gt; d where gfmap :: (a -&gt; b) -&gt; c -&gt; d instance FunctorN a b a b where gfmap = id instance (Functor f, FunctorN a b c d) =&gt; FunctorN a b (f c) (f d) where gfmap f = fmap (gfmap f) 
Note that we're in MonadFix (mdo expression), so you can refer to names defined later in the source.
I'm not convinced that you *ever* actually *need* shared state. &gt;futures (not needed in a lazy language) Why? Lazy languages do calculations when needed. Futures run code when values are available. One is the reverse of the other. &gt;STM is competing with locks and condition variables, not message passing. All forms of concurrency are competing with each other. STM is one way to deal with concurrency, manual locking another, futures another and so on. &gt;if all you have is message passing the "end game" Which, as I said, I don't. I just wouldn't personally spend a lot of time on STM in particular. I think there already exist better alternatives.
How many sock puppets does Jon Harrop need on reddit?
In a lazy language you don't need to wrap up computations in special packages since *everything* is lazy (and therefore already "wrapped up"). You "force" it once at the "top" and things will evaluate automatically with all the dependencies worked out (possibly in parallel if that's the evaluation strategy you've specified for an expression). The only solution message passing can offer to the problem that STM addresses is "emulating locks and condition variables", which we've already agreed suck. STM doesn't attempt to deal with share-nothing threads, and message passing doesn't attempt to deal with shared memory threads. They're doing two completely different things. I agree that you should try to use share-nothing threads as much as possible, but if your problem doesn't fit, it doesn't fit. A lot of times your problem can be expressed as lots of independent threads sending messages, but sometimes your problem requires a shared "working set" with lots of agents updating (parts of) it. Message passing offers nothing over standard locks for these situations. For example, imagine a big giant game world that needs to stay consistent with multiple clients interating with it, each client will need to update its tiny part of the game world (say 5-6 objects at a time), but each such update must be atomic, and consistent, and ideally these should happen concurrently. Message passing's solution is basically to wrap up the whole world in a big thread that "owns" the game world data and does the modifications on the behalf of the client, one at a time, or possibly into multiple threads with some complicated and error prone "exclusion protocol" to ensure a client gets exlusive access to any parts of the world it may need to touch - sound familiar? That's just locks again! So it's not that I'm knocking the things you've listed, I think they're great, it's just that for this problem they don't even *attempt* a solution. They're just not designed for that particular problem (and arguably these kinds of problems are the ones that are actually difficult to solve - if things happen to break up cleanly into concurrent threads you're halfway home, a lot of the time they don't though). STM is, which is why I'm saying that STM and message passing aren't competing. The problems they solve are orthogonal.
A couple of these things showed up in the draft sent to me after the interview and I removed them. Didn't seem to take, though many of my other changes made it through. It's a tall order for a non-specialist to provide concise background to flesh out an interview on pure functional programming. 
File a bug on the tracker: http://trac.haskell.org/haskell-platform and assign it to "gregorycollins". I'll get to it in the next release.
`#` seems a nicer name than `.`, though, since it won't be confused with function composition, so if the magic semantics were added to `#` maybe that would be nicer. I don't particularly like making the spaces around the `.` significant, especially since it might even break old code in subtle ways. `#` is used as a function in [some libraries](http://hackage.haskell.org/packages/archive/haskelldb/0.12/doc/html/Database-HaskellDB.html#v%3A%23), but you could easily alias it. It's also used in a few other language extensions, but those wouldn't coincide with this, I think.
Overloading should be resolved with type-classes (or maybe a more elaborate module system?) and not by type-selecting the best match from a list of ambiguous names. 
oh, I didn't know about mdo expressions. interesting
chans, don't forget.
Sweet stuff! Deriving instances is a really cool language feature.
So, you believe these numbers say something and are not manipulated in the first place? Did you personally keep count? There is plenty of reason to inflate the number. If you look at the "average of 650" users in #haskell for example, you will know that this number is inflated too. It is not inflated by more than 50%, but it is inflated. Who says that you are not a sock puppet? yairchu presents a different option to measure it, but still this number is only available to those that control Hackage, and thus cannot be trusted. The option I suggest cannot be manipulated easily, but by all means, if you are not a puppet, keep believing in these interpretations of numbers you cannot even verify. 
Came across something similar in haskell-cafe: [fmap for lists of lists of lists](http://www.haskell.org/pipermail/haskell-cafe/2006-March/014931.html)
It's type signature is shown on page 25 (first line). The function cannot be defined in Curry and is usually provided externally. (In MCC it is defined in terms of getSearchTree which is itself defined in terms of primitive functions). Maybe I should state clearly that it is an external feature. edit: to call getAllValues in MCC you need to import the module AllSolutions.
Which definition of insert do you use? The one one page 17 is strict while the one on page 18 is lazy. So when using the lazy version of insert you should get only three results without duplicates. Thanks for pointing that out, I should clarify the difference.
Since they have mentioned about RPN and stack-based languages, can someone whip something up in Factor or Joy? I would really like to see an example. Edit: found one but could be improved i guess. http://paste.factorcode.org/paste?id=805
I'm not a huge fan of pdf links. I'd rather see an abstract or something that tells me what's in the link. Could you please link to: http://themonadreader.wordpress.com/ next time. For example, Issue 14 consists of the following three articles: * Fun with Morse Code by Heinrich Apfelmus * Hieroglyph 2: Purely Functional Information Graphics Revisited by Jefferson Heard * Lloyd Allison’s Corecursive Queues: Why Continuations Matter by Leon P Smith Feel free to browse the source files. You can check out the entire repository using darcs &gt; darcs get http://sneezy.cs.nott.ac.uk/darcs/TMR/Issue14 
Ah, I see the difference now. Your article is interesting, but some code is not readily executable. Have you published the code on the web? Is this chapter the end of the story, or is there a book being written? I'm hoping there's a book!
It seems to me that vim config files are a lost art in the modern world. It's always nice to see snippets explaining how to do non-trivial things. That is, non-trivial but also not assuming a whole library of infrastructure as well.
Thanks for your interest! Which code is not readily executable? Which would you like to see on the web? edit: the [simple](http://gist.github.com/158634) and [efficient](http://gist.github.com/158635) implementations of the n-queens solver are now available online.
Note that this is effectively a fork of the standard .NET runtime and there's no indication of when we can expect to see it in mainline. Still pretty cool though, their scalability looks really nice even if you do pay a significant initial constant factor (roughly 4x) for using STM.
 /** * maybeColors is a list of 6 {@link Maybe@} values. * Values of the Maybe type are used to extend a type with an additional * {@link Nothing@} value. */ maybeTrigs :: [Maybe (Double -&gt; Double)]; maybeTrigs = [Just sin, Nothing, Just cos, Nothing, Nothing, Just tan]; what is this CAL? is it Haskell with semicolons and C-style comments? 
anyone tried this yet?
too verbose, according to shootout
I've played quite a lot with it: http://www.kablambda.org/blog/category/computer-science/cal/ I live very much in the Java ecosystem, and I like static typing, so CAL suits me well. It's like Haskell 98 with a bit less syntactic sugar.
Why are there so many projects to make languages run on the JVM? I mean you can't really benefit from all the nicer features of modern languages when you shackle yourself to Java libraries.
Wouldn't it be better if haskell could just parse -34 as the number negative thirty four instead of the partial application of 34 to the function `(-)`? It is what is expected 99% of time, and to get the latter meaning all you need to do is insert a space between the `-` and `34`.
yes! haskell can do that!
It does sort of seem silly, but I guess there are a lot of people who work at shops that are all Java based and so they're already tied to the JVM. They program in Haskell at home and, well, Java is exceedingly painful after Haskell, so...
I remember learning this method in school! Ah, happy days.
Good to see more functional goodness being added to .Net.
This post makes the HAppS project seem active and interesting. It's a pity the [HAppS website](http://happs.org/) doesn't reflect any of this activity, everything on there is sparse and very out-of-date. 
Nice to see STM gaining some traction. I only wish that we were seeing pure languages gain traction along with it. 
check out their demo videos of [Gem Cutter](http://openquark.org/Open_Quark/Gallery.html). looks interesting. kinda (yet not quite) like [Subtext](http://subtextual.org/subtext2.html) but runs on jvm (so could be used for "real" stuff?). 
http://resources.businessobjects.com/labs/cal/tutorial_calintro.html
JVM bytecode has a ton of optimizations performed on it, so by targeting the JVM--for strongly typed languages at least--you get: A: performance (java, contrary to what it was in the 90's is very [fast](http://shootout.alioth.debian.org/u64q/benchmark.php?test=all&amp;lang=all&amp;box=1) ) B: a massive standard library C: the ability to interface with thousands of other quality libraries.
HAppS has been superceded by http://www.happstack.com/
A might be true for languages reasonably similar to Java, I am pretty sure different paradigms need different kinds of optimizations, in particular stuff like tail call optimization, copy on write semantics for variables, lazy evaluation,... have different optimization opportunities. As for B and C after seeing quite a bit of the Java standard library and the inconsistency in there as well as the sheer number of Java projects that simply fail because the libraries were over-engineered because someone thought "enterprise" means you need more levels of abstraction than in other fields I don't think using those libraries directly in a new language is an adequate substitute for writing libraries fitting your language's programming style.
Thank you. I didn't realise. I'll have a look at Happstack. Maybe a final news item to that effect on happs.org would be good. 
&gt; Why are there so many projects to make languages run on the JVM? In many enterprise environments, JVM deployment is favoured because Java is memory safe. Programs in "native code" might require lengthier, more expensive reviews, for example.
I guess people want Haskell on the JVM badly enough for someone to plagiarize it...
Last time I checked Java still has NullPointerExceptions which is the exact equivalent of a segfault, that said there are plenty of languages out there that do away with those, take your pick.
Lazy + side effects = scary, right? For example, in the intro they use (\list -&gt; javaSort list `seq` list) to do an in-place sort on a list.
can you write programs that catch segfaults?
The point is that Java is still on the wrong side of having to deal with that kind of issue, not whether or not you can work around them.
A `NullPointerException` is not dangerous to other programs/processes. General memory unsafety means potential trouble for the ops team. In other words, a `NullPointerException` is a problem that is nicely localized in the engineering department whereas general memory unsafety spreads responsibility across at least two departments. The enterprise isn't sold on the notion that we can prove the safety of a natively compiled program (by region inference, for example). It's relatively easy to get behind the idea that the JVM intercepts everything and protects the rest of the processes from misbehaving programs.
A segfault can't affect other processes -- on modern computers each process has its own address space. The problem with unsafe languages like C/C++ is not segfaults -- it's that you may make an invalid memory access (read/write past the end of an array, read/write from a pointer to memory which has been deallocated) and *NOT* get a segfault, just incorrect behaviour and memory corruption. CAL doesn't throw null pointer exceptions -- there's no concept of 'null', but, like Haskell you can still get runtime errors, e.g. by asking for the head of an empty list.
OMFG. Please at least use the language before making such statements. OutOfMemoryExceptions are more dangerous than those type of RuntimeExceptions. OutOfMemory means just that, it is an end game, you can't continue if you haven't set your heap properly or added some kind of fail over. NullPointerException is just a runtime exception that can be easily caught if you are careful in your coding style.
What? I am not understanding what you want a program to do if you can't determine the state of your application. If you attempt to reference an object that isn't available. What do you want the app to do? Like I said, Out of Memory Exception is normally a type of error where you can't recover from. And you can recover depending on how you design your application and if you are using any kind of clustering. Python suffers from the same problem. &lt;pre&gt; b = None b.run() $ python a.py Traceback (most recent call last): File "a.py", line 2, in &lt;module&gt; b.run() AttributeError: 'NoneType' object has no attribute 'run' &lt;/pre&gt;
One solution is to not allow "null" as a valid value for 99% of references (only references where you expect them to be invalid in normal program execution would be "nullable references", which would be a different type from a regular reference). If a problem can not happen, as proved by the compiler, you don't have to worry about it.
I am not a 'language' person, but wouldn't this only work for more strong typed/static/compile ahead of time languages like Scala or Haskell. With Python and even Java (where Java isn't type safe always), they would have to change entirely to fix the issue of the nulll reference. Edited: Here is a common idiom in Java. I guess you could do away with it, somehow. And that would get you 99%. String s = null; for (int i = 0; i &lt; 10; i++) { s = service.doSomething(i); s.run(); } ... ... How would the compiler get to the 99% with the way Java works? 
On Windows, yes, in theory. (use SEH to catch EXCEPTION\_ACCESS\_VIOLATION) I don't know if it actually works any more and there's not much you can sensibly do at that point, given that you probably also have memory corruption. OTOH, depending on how your program is written a NPE might also suggest that you have corrupted state. (But "exact equivalent" it is not.)
That is the whole point. The JVM was designed for Java, other languages, especially Haskell-like ones (as this whole post is about) don't have some of the flaws Java has. They have to deal with them anyway by using Java libraries and they can't optimize away code that isn't necessary (like a null pointer check) because the JVM isn't designed for that kind of language. So what do those gain from using the JVM?
&gt; NullPointerException is just a runtime exception that can be easily caught if you are careful in your coding style. Segfaults can be easily avoided if you are careful in your coding style. This is the old "but it is possible to avoid that kind of bug given a sufficiently smart programmer" argument. It doesn't mean the language is not flawed though.
If a "String" couldn't be null, you'd have to declare the variable inside the loop and let he compiler handle reusing the slot on the stack.
Anyone else getting a request for authentication ?
all of openquark.org seems to be behind a .htaccess lock
I found the "cabal test" tweak to be really useful!
Which is exactly how this is handled in Cal, if something can not exist then it's wrapped in a "Maybe"
Just some modules had to be imported. It wasn't clear when reading the chapter.
Which Curry implementation would you recommend?
The link isn't working.
[Mirror](http://goto.cwru.edu/~left/DataTypesALaCarte.pdf)
thanks for the link. i'm starting to really get into haskell, and was looking for a nice project. working out rubick's cube properties might be a solid one.
Comments please!!
Is GHC able to fuse the original code as well? If not, why not?
GHC has no built-in fusion, all optimizations of this nature are added in libraries as RULES pragmas. The rules that come with the standard library are based on a system known as "build/foldr fusion" or "shortcut deforestation". This system will fuse any list constructed with `build` and any consumer expressed in terms of `foldr`. In the original: sum [x | x &lt;- [3..999], x `mod` 3 == 0 || x `mod` 5 == 0] GHC will fuse the construction of `[3..999]` with the filter in the list comprehension because `enumFromTo` is defined in terms of `build` and `filter` is defined in terms of `foldr`. However, since `sum` is defined as a `foldl`, the filtered list will not fuse with the summation. If you're counting, that's one list constructed with GHC's optimizations versus two constructed in a naive compiler. Other fusion systems, such as stream fusion, can remove all the lists in this example. [The list of fusible functions in the base library.](http://www.haskell.org/ghc/docs/latest/html/users_guide/rewrite-rules.html#id539906)
The relevant part of the article: &gt; using a list for this is cumbersome, so I cooked up the «quad». The «quad» is devoid of any intrinsic meaning because it is intended to be a general data structure, so I looked for the best meaningless names for the slots/accessors, and decided on QAR, QBR, QCR, and QDR. The quad points to the element type (like the operator in a sexpr) in the QAR, the parent (or back) quad in the QBR, the contents of the element in the QCR, and the usual pointer to the next quad in the QDR While the post is relatively new, he apparently used this when he was involved with SGML.
Sounds like a doubly-linked tree structure, rather than a zipper in Huet's sense. Zippers are rooted in pointer-reversal techniques, where exactly one child pointer is *replaced* by a backpointer. Doubly-linked structures are harder to maintain in a purely functional setting without loss of sharing. Of course, the real dispute in inventing the zipper is between Newton and Leibniz...
In my 1st (or 2nd?) year of CS studies, I've seen how to remove recursion from a tree traversal, involving pointer edges to the parent nodes. (What do you know, a Zipper...) This was back in 1996, and the teachers presented it as well known stuff. Huet's paper is a pearl, also suggesting that the Zipper was folklore when he wrote it. So I think you'd have to search for the first traces of zippers in the eighties at least.
This is a threaded tree, not a zipper.
Could somebody remind us why stream fusion did not make it in GHC/libraries? 
Are those things I'm guessing are variables longer than they need to be? Would something like this be equivalent and easier to read? c1 = (-9223372036854775807) c2 = (-9223372036854775806) c3 = (-9223372036854775808) $s$wfold :: Int# -&gt; Int# $s$wfold = \ (sl :: Int#) -&gt; case modInt# c1 3 of s2 { __DEFAULT -&gt; case modInt# c1 5 of s3 { __DEFAULT -&gt; $wfold sl c2; 0 -&gt; $wfold (+# sl c1) c2 }; 0 -&gt; $wfold (+# sl c1) c2 } $wfold :: Int# -&gt; Int# -&gt; Int# $wfold = \ (w1 :: Int#) (w2 :: Int#) -&gt; case &gt;# w2 999 of w3 { False -&gt; case w2 of w4 { __DEFAULT -&gt; case modInt# w4 3 of s2 { __DEFAULT -&gt; case modInt# w4 5 of s3 { __DEFAULT -&gt; $wfold w1 (+# w4 1); 0 -&gt; $wfold (+# w1 w4) (+# w4 1) }; 0 -&gt; $wfold (+# w1 w4) (+# w4 1) }; c3 -&gt; case modInt# c3 3 of w5 { __DEFAULT -&gt; case r1 of w6 { False -&gt; $s$wfold w1; True -&gt; $s$wfold (+# w1 c3) }; 0 -&gt; $s$wfold (+# w1 c3) } }; True -&gt; w1 } 
So it could fuse just by replacing sum with foldr1 (+) ?
It's worth mentioning that one of the Timber guys is doing an internship at GHC HQ implementing super-compilation... Who knows if that will work out, but if it does (and ends up in there by default) that should do a whole bunch of fusion. This is all according to a Simon PJ interview posted a few weeks back which I'm too lazy to find!
Holy shit I've waited for this! Off to try. EDIT: Oh. Forgot it's gtk-based. :-\ Non-native file open boxes and all that.. Have to manually navigate to the folder (doesn't open to home folder by default), no spotlight search etc. Changing preferences (editor font) had no effect. It asked me to manually locate the folder where Haskell packages are installed (shouldn't there be a default/standard location?), couldn't find any itself. Fonts are also a bit messed (after 'candifying' I get square boxes where the unicode symbols should be). Also, it still seems to be built for 1600x1200 resolution monitors. I wish more apps used wxHaskell instead. It's good to see Leksah progressing, but I'll stick with Vim for now.
If that's true, its an example of how explicit strictness in code harms modularity.
The original version needed a new optimization added to GHC to make concatMap (and thus nested list comprehensions) fuse well.
I agree. Leksah would benefit greatly from an overhaul to make it look more native since I have to keep dragging things around to get to various parts of tabs, but this is definitely a step in the right direction. 
I've been waiting for this for literally weeks now! My comments as I try using it: * I can understand needing some paths to search for packages, but wouldn't it be a safe bet to check the default Haskell Platform location at least? * It looks for a file 'welcome.txt' in '/home/jutaro/Develop/leksah/data'. Somehow I doubt most people will have that folder. * The 'candy' doesn't work on my machine, presumably due to a font issue. It might be a good idea to bundle a font that contains those characters. Since my windows dev environment is a VM with minimal customizations, I'd interpret this to mean that windows by default doesn't have the necessary fonts. * It would be nice if packages could be automatically configured when you try to build them. * There seems to be a real drive throughout the IDE to use the cabal package as the atomic unit of a project. With that in mind, I would expect to open a package and get a list of modules in the pane titled 'Modules'. It looks like that's what happens in the screenshots, so I'm not sure what exactly I'm doing wrong. * Okay, I have my package open and built. I know for a fact it works under windows. Now how exactly do I get a GHCi prompt to open with this library loaded? * Assuming that some of these issues were a result of my failure to RTFM, I decided to try that. [The manual](http://leksah.org/leksah_manual.pdf) appears to be password protected. I understand that this IDE is still a work in progress, and am seriously impressed with the work done so far. Sadly, it doesn't look like it's very well suited to my style of workflow yet. Back to the daily toil with vim and a console, I guess.
I wish wxHaskell was easier to install so that more apps would use it :-). There is progress though. I hope to see a split from the wxc wrapper around wxWidgets from wxcore, and a wxcore which is purely Haskell and which can be mindlessly installed in the ~/.cabal directory
&gt; Changing preferences (editor font) had no effect did you then click "Apply"? the norm in windows (and leksah seems to follow it) is that the setting don't change unless you also choose to "apply" them. 
Yes I did, I also closed the editor window and opened a new source file, in case the prefs are applied only to new editor tabs.
Yes, it would fuse. However, note that foldr1 (+) will use linear stack space, thereby negating the benefits of fusion.
another possible concern about packages: * should it be split into smaller parts? * does it contain "junk-dna"? an example: mtl has Control.Monad.List.ListT which is a broken list monad transformer, unlike [ListT done right](http://www.haskell.org/haskellwiki/ListT_done_right). machine can assist in determining if a package carries deadweight by checking for each function if anything in hackage uses it. 
&gt; Have to manually navigate to the folder (doesn't open to home folder by default) You can click on the "paper and pencil" icon in top left. This opens an input box where you can enter the path. &gt; after 'candifying' I get square boxes where the unicode symbols should be Are you sure your font actually has these symbols?
&gt;You can click on the "paper and pencil" icon in top left. Yep :) I'm also a linux user so Gtk is not unfamiliar to me. It's just a minor annoyance when I'm on OS X (I tend to use spotlight a *lot*). &gt;Are you sure your font actually has these symbols? Considering that I wasn't able to change any fonts, Leksah is using a default font (presumably bundled with it). It's using `Sans 10` right now, and it looks like DejaVu/Bitstream Monospace to me. Whichever default font it picked out-of-the-box should have the symbols, right?
This is puzzling. Can you try running /Applications/Leksah.app/Contents/MacOS/Leksah in a terminal to see if anything is logged there.
No we have not bundled a font. I switched candy off by default on the OSX version (forgot to on Windows). Mainly because it is probably not helpful for users new to Haskell, but partly also because there are no default fonts on Windows or OSX that have all the symbols.
I am most concerned about the last point most, as I think some of your questions are indeed answered in the manual. Was the password prompt coming from your browser or pdf reader?
Native file open may be a way off (although there are more OS X Leksah users now so perhaps someone will be inspired to have a go). I think the default path is the package location (once one is open). It's no substitute for spotlight in the file open dialog, but you can grep the files in you package using from the find bar. I normally open files through the Modules tree or by clicking the source button in the info pane. It should persist the layout of windows as part of the session, so you should only have to adjust it to your liking once. This might help too http://www.nabble.com/Re%3A-ANN%3A-Leksah-0.6-p24647247.html 
PDF reader. It may be the fault of the reader, but it's still never happened before, and I can't help but think that other people would encounter the same problem.
I was able to read the manual perfectly fine.
I think it must be a compatibility issue. What OS (and version) and PDF reader (and version) are you using? In the mean time you can google "site:leksah.org leksah manual pdf" and click "View as HTML" or you could go straight to the source http://code.haskell.org/leksah/doc/leksah_manual.lyx
I tried again and, weirdly, now it works. File this under 'spoke too soon'. It however doesn't seem to remember the font across instances. Btw it was using "Monospace 14" by default. Changing to Monaco I can now see -&gt; changed into unicode arrow (but other symbols are still boxes, I assume it's because Monaco doesn't have them). In that case 2 bugs out of the way.
Is there anything preventing it from replacing foldr/build fusion now?
&gt;In a lazy language you don't need to wrap up computations in special packages since everything is lazy (and therefore already "wrapped up"). You "force" it once at the "top" and things will evaluate automatically with all the dependencies worked out (possibly in parallel if that's the evaluation strategy you've specified for an expression). This isn't enough. Futures would be more analogous to Monads: When you create a new promise it is "contaminated". Normal code is either never able to touch it (e.g. how E works afair) or upon touching it becomes contaminated itself. &gt;They're doing two completely different things. I agree that you should try to use share-nothing threads as much as possible, but if your problem doesn't fit, it doesn't fit. I think this is our disagreement point. I'm not yet convinced that there are situations that message passing "just doesn't fit". Certainly doing things via message passing will likely require a totally different architecture, but I haven't yet seen any compelling cases where shared-state is the only option. &gt;For example, imagine a big giant game world that needs to stay consistent with multiple clients... I would indeed expect one process that owns a given zone, but I wouldn't expect any locking protocol, but rather the clients send some action message, the game tries to apply it and then response with a success or failure. Or better yet, the zone is just sending updates of what changes to all clients and the clients just send what actions they want to take. Many of the places I've seen where someone claims locks are required, the example they give makes lock a part of the design. 
No, just an opinion, based on truth.
"almost all linux users" All linux users? I have never once used the apt or the other system. I needed the tar zipped download to build the source and run. Sorry all == 100% == false.
nice.
Don, I don't see any examples in your structure. While I think Ruby's page goes overboard with examples, they are nice to see for a Haskell-neophyte. One elegant way to handle too much detail, would be to create new pages, and make the main page have only a summary, as Python's page does for syntax and semantics: http://en.wikipedia.org/wiki/Python_syntax_and_semantics Your section on Typing (3.3) seems like a good candidate for the same technique. Otherwise, the outline is excellent and an improvement on what's out there. 
This is already fixed in GHC 6.12.
I don't know how relevant [this](http://www.mat.uab.es/~kock/cat/polynomial.pdf) is, but it seems to be a lot of fun. (*Edit:* the chapter on containers seems yet to be written).
Is there any chance you could replicate this somewhere with LateX support (a blog perhaps)? It's very interesting, but quite hard to follow currently.
For comparison, here's [March 2009](http://www.galois.com/~dons/hackage/popularity.html).
Every number of downloads is even.
Hmm.... :/ Grr. Bugs.
For comparison, here's [March 2009](http://www.galois.com/~dons/hackage/popularity.html). (And thanks to roconnor for spotting a bug earlier!)
How would people like this data presented in future?
you need a better type system. :P
Anyone have a good reference for type differentiation? Does anyone do type integration? Do generating functions shed any light on this stuff?
You seem to have some difficulty understanding the term "almost".
The vids all seem to be down. :(
Sorry, I'm so used to pseudo-LaTeX ASCII math that I forget some people find it unreadable. [Here's a LaTeX version.](http://mathbin.net/22535) 
The obvious question is: how long until xmonad is in the Haskell platform?
But note, xmonad has changed little since March (see below), while HTTP has nearly doubled. xmonad is getting pulled in from distro packages now.
Isn't the Haskell platform mainly about fulfilling dependencies, rather than just putting popular software all in one package?
Look at some of Conor McBride's publications. This is closer to the theory of combinatorial species for polynomial ("tree-like") structures. Generating functions are what you get when you forget structure and focus only on enumeration. That is, there's a forgetful functor from the category of species to the category of generating functions. The functor preserves a bunch of operations (including sums, products, exponentials, derivatives and integrals) so that's why the manipulations appear so closely analogous. I would say that species shed light on generating functions rather than vice versa, though generating functions were the original inspiration for the theory. Integration makes sense as anti-differentiation. Differentiation is about pointing ("digging holes") and integration is about co-pointing ("filling holes"). That is, if you interpret every 1 in a polynomial data type as a hole then integration replaces it by x, thus filling it with a slot of type x. For x^n you have holes adjacent to each of the n different x slots. There's 1 x ... x, x 1 x ... x, ..., x ... x 1, which adds up to n+1 different holes. Filling a given hole replaces a 1 in x^n by an x, yielding x^(n+1), so filling any one of the n+1 holes gives (n+1) x^(n+1). Lo and behold, it's the rule for integrating monomials. I haven't read anything about this in the literature but I suspect that if you work with virtual species (the Grothendieck K-completion of species, where -x is interpreted as a kind of anti-particle that upon collision with an x annihilates the pair) you can develop a species analogue of the calculus of finite differences. The classical theory needs a notion of rising and falling powers which seems tricky to give an intuitive meaning for species. What's the intuitive meaning of x - n? It's that which reacts with n (the species of natural numbers less than n) and produces x. Not too intuitive in non-virtual terms. However, much as the classical calculus of finite differences is applied in combinatorics without regard for the meaning of the intermediate expressions so long as the end result has a useful interpretation, the species version of the calculus should also be useful as a tool for calculations. One thing I've been wanting to work out is what would be the chain theory (in the sense of homology) dual to all this stuff when interpreted as a weird kind of cochain theory. I wonder if anyone has any hunches on that? For all I know it's a totally barren line of thinking. But the interpretation of differentiation in terms of holes is way too suggestive not to set off my algebraic topologist radar. What gives me hope is that I know the combinatorist Richard Stanley has already successfully deployed some of the big guns of homological algebra like spectral sequences.
Yes, yes, it was just a joke.
You rule the school! thanks http://trac.haskell.org/haskell-platform/ticket/87
There are many factors affecting the download count. For example a package which is updated very often is probably downloaded many times by the same user. So there could be some ad-hoc normalization scheme(s), like the download count divided by the number of versions, and the normalized "score(s)" displayed in addition to the total download count.
These statistics are great. Keep these coming. When we (company) evaluate a technology, we need to know how much usage/downloads the software is getting. 
I am sorry, I don't think it matters. There are no perfect stats, but some stats are better than none at all.
Well, a question was asked. A suggestion was posted. Which by the way suggested an *addition*, not a replacement...
I couldn't find the PDF on that page but it's easy to find on google: http://www.cs.umd.edu/~avik/papers/cmllch.pdf
The use of the heart (`'♥'`) to indicate mutual synchronization is a great touch. It's worth pointing out that the paper makes a stronger statement than the abstract: &gt; In Section 8, we compare the performance of &gt; our library against OCaml’s standard library &gt; of CML-style primitives. Our implementation &gt; consistently outperforms the latter, even &gt; without possible optimizations. 
The reduction rule `SEL COMM` is a little puzzling -- what if `a, ā` are also in the sets of actions?
Never mind -- it's cleared up on page 4.
The [source code](http://hackage.haskell.org/packages/archive/cml/0.1.1/doc/html/src/Control-Concurrent-CML.html) for this package [which is available on Hackage](http://hackage.haskell.org/package/cml) is somewhat unfortunate when it comes to readability: atsync :: Synchronizer -&gt; Abort -&gt; IO () -&gt; IO () atsync r a x = do { (t,s) &lt;- takeMVar r; forkIO $ fix $ \z -&gt; do { (t',s') &lt;- takeMVar r; forkIO z; putMVar s' Nothing }; c &lt;- newEmptyMVar; putMVar s (Just c); b &lt;- takeMVar c; if b then do { putMVar t (); fix $ \z -&gt; do { (tL,f) &lt;- takeMVar a; forkIO z; if elem t tL then return () else f } } else x } (It's indented incorrectly, uses way too many single-letter variables whose meaning isn't really clear from context and (in other places) abuses functions like `maybe` where a simple case-expression would be clearer.) It seems to me that based on their descriptions in the Haddock documentation, the CML primitives ought to admit fairly straightforward implementations, but perhaps I'm wrong. One thing that I really don't understand in the paper is their term 'higher-order concurrent programming', which they don't really define so clearly versus 'first-order'. If someone happens to know what they mean by that, it would be nice to know. They seem to make it sound like it's surprising that the CML primitives are possible to implement in terms of MVars, but it doesn't really surprise me at all that it's possible, just looking at the Haddock documentation, and the types involved. (It would surprise me much more to see a system which is utterly impossible to implement in terms of MVars and IO actions.) What is surprising though is that their implementation manages to be faster than the output of a compiler specifically designed to implement those primitives.
&gt; What is surprising though is that their implementation manages to be faster than the output of a compiler specifically designed to implement those primitives. Ocaml is not specifically designed to implement these primitives -- in fact it actually has a very slow implementation, because it uses Unix threads as processes. It just used the CML interface because it's a good interface. A fast implementation can be found in the SML/NJ implementation: there, thread creation and context switch are only slightly slower than ordinary function calls. 
Pick from things in [Category:Math](http://hackage.haskell.org/packages/archive/pkg-list.html#cat:math). There are 60 libraries at the moment. I think you'll have to glue together what you need from each of them.
Ah, right you are, sorry. The paper mentioned CML using low-level, language specific details to implement the operations, but then uses the O'Caml implementation in the benchmarks. It would be interesting to see the benchmarks extended with SML/NJ results.
hmatrix ? 
Excellent, Haskell is dire need of good example game code (albeit simple games like tetris). Hopefully he will release the source soon. 
&gt; Special mention goes to Saizan for his everlasting patience. Hear hear. Saizan has helped me so many times, too! Thanks Saizan!
Personally, I'd be more interested in Mathematica for Haskell. I chose Haskell for my mathematics work because I hope to take advantage of it's wonderful support for infinite lists (which underlie the objects I'm studying). 
Hi, yeah I will release it as soon as I am finished with what I need it for in my studies. In the mean time there are some great examples here: http://hackage.haskell.org/cgi-bin/hackage-scripts/package/#cat:game
Sounds good. I've actually gone over the code for the majority of games in that section. Unfortunately, however, there seems to be a dearth of complete, well written and interesting projects. A good number of the projects are very skeletal nature and contribute very little, aside from being someone's pet project. On the other hand, there are the larger projects such as Frag, but these tend to be re-imaginings of existing games. To be frank, there's nothing that people actually want to play. Haskell needs an original, polished game to proves its viability as a language for games development (which surprises me, as it seems very capable). 
Well, I will be honest with you. The game here is also a pet project of mine. I am however using it as a case study. Haskell is mostly (not totally though) a research language and althought more and more people are being attracted to it (so it seems to me at least), it still requires even more people using it in order for there to be enough "gamer" types which write more and better games. Such people usually go for more mainstream languages such as ActionScript or if developing something more substantial go for C++. I agree with you however - I think Haskell is a very viable language for games development (as this game has taught me). Hopefully someday soon a fully-fledged, interesting game is created with Haskell.
just downloaded and tried it out. looks pretty good.
C++ templates were inspired by ML's type system. 
Hey, let's not pick at details. At least he's seeing the light.
Really? Do you have a source (named Bjarne) on that? It seems to me that in copying ML's structures, they managed to lose all the really useful features: a module system, signatures for module-level typechecking, etc. To me, C++'s template system seems more like a compile-time (rather than preprocessor time) macro system.
&gt; I downloaded the platform and proceeded to read through the online book. And was blown away by the time I got to Chapter 2 and 3! Wow, it's going to be crazy once he gets to the point where he can imagine writing a non-trivial program. I'm expecting rich blog posts and papers describing how wonderful that imagined program will be.
I can see how that might work. Templates are parametrically polymorphic on unconstrained arguments after all. Polymorphism is just compiled by full specialization.
This is very stupid, this problem is trivial using the [wheel](http://www.haskell.org/haskellwiki/Prime_numbers#Prime_Wheels) method, or any of the methods faster than the wheel.
&gt;I'm expecting rich blog posts and papers describing how wonderful that imagined program will be. So why did he bother telling us that he bought a book? What a pathetic post and why did anyone find it interesting?
I want proof for that. I don't think I've ever seen Stepanov mention the ML languages even though the comparisons are obvious. I can't tell you how often I've read about something in the C++ world and said, "you know, you guys really should look at what people are doing in other language communities." 
Along those lines, I wrote this snippet today as part of some matrix algebra: identity = iterate (0:) (1 : repeat 0) identityN n = (map (take n) . take n) identity
Very nice, but you really don't want to use a list of lists as your matrix representation. ;-)
I have an implementation of Gaussian elimination in a few lines of Haskell that depends on a list-based matrix zipper. It has the same asymptotic complexity as a mutative array-based implementation and it expresses and exploits all the symmetries inherent in the algorithm in a way that I haven't quite seen in any mutative implementation. I'm pretty fond of it. I also have a one-liner for Laplace expansion of determinants and few-liners for a number of other linear algebra algorithms based on this zipper I designed. I wouldn't use it for serious numerical work. Then again, I wouldn't use Haskell for that in the first place. Here's a sample: eliminateEntry (x:xs) (y:ys) = 0 : (ys `subtract` (y `scale` xs)) eliminateColumn m = case findPivot m of | Just (pivotRow, rows) -&gt; pivotRow : map (eliminateEntry pivotRow) rows | Nothing -&gt; m triangularize n = iterateN n (go Down . go Right . updateBlock LowerRight eliminateColumn) gaussianEliminate n = flipDiagonally (triangularize n) . triangularize n That's the entirety of the implementation that isn't a general purpose library function. It's compact and yet reads like prose. Upper triangularization can be expressed as mirrored lower triangulation without any abstraction penalty. There are no indices to muddle the structure. The function `gaussianEliminate` works directly on zippers. You pass it a zipper for an augmented matrix. If you wanted to simply determine the rank of a matrix you'd apply count (/= 0) . diagonal . fromZipper . gaussianEliminate n . toZipper 
[This might suffice](http://www.research.att.com/~bs/bs_faq.html) &gt; Standard C++ and the design and programming styles it supports owe a debt to the functional languages, especially to ML. Early variants of ML's type deduction mechanisms were (together with much else) part of the inspiration of templates. Some of the more effective functional programming techniques were part of the inspiration of the STL and the use of function objects in C++. On the other hand, the functional community missed the boat with object-oriented programming, and few of the languages and tools from that community benefited from the maturing experience of large-scale industrial use. I remember that he discussed the idea in the [C++ programming lanugage](http://www.amazon.com/Programming-Language-Special-3rd/dp/0201700735/) book, but I cannot find the right passage on the interwebs. 
I'd refactor his: where lenPreviousLines = foldr ((+).length) 0 . take y . terminatedLines to: where lenPreviousLines = sum . map length . take y . terminatedLines :-) Good quote: "when you remove side effects you lose all barriers to composability" 
&lt;nit-picking&gt; I wouldn't name a function "gauss". because then you would need to name several other functions gauss too: normal distribution, least squares regression, his fft algorithm, etc. 
I just renamed it to `gaussianEliminate` in my code before I posted that snippet. There's a lot of other stuff I'm refactoring. Maybe later tonight when it's in better shape I'll post the whole thing in self-contained form. It's only around 70 lines for the general purpose vector, matrix and zipper algebra and then very few lines per algorithm (Gaussian elimination, Laplace expansion of determinants, Gauss-Seidel algorithm, etc).
&gt; I wouldn't use it for serious numerical work. Then again, I wouldn't use Haskell for that in the first place. Well, I'll admit I don't do numerical work very often, and that Haskell isn't the first thing that comes to my mind either. However, you should be able to do reasonably well using unboxed ST arrays, unsafeFreezing them once they've been initialized. (Or using runSTUArray) &gt; I have an implementation of Gaussian elimination in a few lines of Haskell that depends on a list-based matrix zipper. It has the same asymptotic complexity as a mutative array-based implementation and it expresses and exploits all the symmetries inherent in the algorithm in a way that I haven't quite seen in any mutative implementation. I'm pretty fond of it. I also have a one-liner for Laplace expansion of determinants and few-liners for a number of other linear algebra algorithms based on this zipper I designed. In some ways but not in others, this sounds similar to some stuff I've written. It also sounds like you've done more with it. I'd be interested in taking a look... I'm wondering if there isn't a way to tweak your definitions just slightly to enable a more efficient array-based solution, without sacrificing too much code prettiness or conciseness.
I designed most of the code yesterday but just spent a few hours banging out some expository text. This is very rough but I thought I'd post it to gauge interest. Aside from the things specific to matrix zippers, there is some nice stuff in there about symmetries and conjugations and how you can use it to get "code for free". The next thing I'm going to add is how to use this matrix zipper to implement many of the standard algorithms of linear algebra in very few lines of code, in an index-free fashion, with the same asymptotic complexity as mutative array-based implementations. I already have all this working (and more) but it takes about ten times longer to write it up than to code it, and I'm also trying to refactor and clean up the code as I go. Right now it's 4 AM and I'm going to sleep. :)
This is sweet! I've had a passing interest in exploring exactly this topic ever since I started with Haskell 2-3 years ago (has it really been so long?), but I never got around to really trying.
This is really nice looking. I think a monad reader article could be created from this without much difficulty.
Well, it's a *huge* work to create Mathematica- or Maple-like functionality in Haskell. But imho Haskell is really well-suited for that, you are not the only one wanting it, and there are people working on stuff in that direction. Check out the math category on Hackage. As an example, I would say that regarding combinatorics, Haskell is already on par, if not better, than Maple or Mathematica: Check out the packages [combinat](http://hackage.haskell.org/package/combinat), [permutation](http://hackage.haskell.org/package/permutation), [species](http://hackage.haskell.org/package/species), [HaskellForMaths](http://hackage.haskell.org/package/HaskellForMaths).
Thanks for taking the time to write it up - it really helps having the explanations right there with the code.
Doesn't work for me. Something about certificates from twitter. I tried to download and add a cert for curl to use, but it didn't work. Not sure why. Seems like it'd be an interesting application.
It may not be appropriate for `[a]`. It might be interesting for other structures, such as uvector. Only one way to find out.
&gt; It may not be appropriate for [a]. Indeed. I don't see how a Haskell list timsort could be lazy, or do the array reverses and destructive updates in any highlevel fashion. Might be doable on Data.Sequence though - I've long thought it underused.
[a] -&gt; STArray ix a -&gt; sort -&gt; [a]?
no sort can be lazy. 
Uhm...why?
 [1..] There is a lazy sort for this. What it is, is left as an exercise for the reader.
When you're sure what the least item is, you must have examined the values of every other item.
A function which sorts only [1..] is not a sort.
Oh? And why not? Are you working off some peculiar definition of function that excludes any partial functions? There's plenty of laziness possibilities with sorts. Apfelmus has a nice posting about one particular lazy sorting task: http://apfelmus.nfshost.com/quicksearch.html &gt; If the sort function is lazy enough, this will take O(n) time instead of the expected O(n log n) that would be needed to first sort the list and then extract the first element.
But the sort still can be lazy. For example this stupid O(n²) algo: sort [] = [] sort (x:xs) = first : sort rest where (first, rest) = min_ ([], x, xs) min_ (hd, p, tl) = if (all (p&lt;=) (hd ++ tl)) then (p, hd ++ tl) else min_ (hd ++ [p], head tl, tail tl) This is a lazy sorting... Sure it examines all values, but it computes the sorted list lazily.
The entire input list must be consumed before any output can be produced however you don't have to do all the n log n work up front. You can construct a heap in linear time and lazily extract elements from the heap at a log n cost for each element.
That's the post i was looking for, thx!
When people say that something like insertion sorting is lazy, they don't mean that it is non-strict. They are comparing the evaluations of a lambda expression with call-by-need versus call-by-value when forcing parts of the returned structure. These patterns of forcing can be defined by a family of observers like `nth`. Insertion sorting is lazy because to extract the first element of the returned list (the minimum of the initial list) requires only O(n) evaluations with call-by-need but the full O(n^2) with call-by-value. This is a natural definition if you think about it. The usual definition of strictness only cares about convergence and divergence. Any finite reduction sequence is considered no better or worse than any other. What matters is finite versus infinite. But the essence of normal-order reduction is not merely that it finishes in finite time whenever that is possible but that it does so in the most economical fashion, never reducing anything unless forced. A non-lazy function is one on which this frugality is wasted.
It's lazy until you force any one item in the result list. Then the `all` has to force all the elements of the list to do the comparison.
Less obvious is that in an instance where treesort ("functional quicksort") would sort the full list in O(n log n) time you can extract the minimum in O(n) time by unconsing. The argument goes like this. For treesort to finish in O(n log n) time, the split at each level must be even up to a constant factor, so for asymptotic analysis we can assume it is exactly even. To find the leftmost element by unconsing, we need only descend into the leftmost segment of the partition at each level. Thus there are n elements at the first level, n/2 at the second level, and so on. At a level with k elements we only do k comparisons for the left partitioning. Therefore the total number of comparisons to find the leftmost element is n + n/2 + n/4 + ... = n (1 + 1/2 + 1/4 + ...) &lt;= 2n = O(n)
I often wish for greater abstraction along these and other lines. A good first step for abstract algebraic data types would be non-horrible view patterns. There's also the problem that the current type signature syntax does not really scale in human usability terms when you get too many type classes jammed in there and everything is a quantified type variable. I've mentioned it before but I'd really like some notion of partial type signatures. It would be great if I could take a two-parameter type class `Seq a b` and write cons :: a -&gt; Seq a -&gt; Seq a and it would generate fresh and quantified type variables for each occurrence of Seq a: cons :: (Seq a b, Seq a b') =&gt; a -&gt; b -&gt; b' This kind of type signature with embedded type classes would be partial: The compiler would only use it as an upper bound in type inference, reducing it further to a principal type signature based on the definition and contextual uses of cons. In this example it would look at cons's definition and force the type variables b and b' to be unified: cons :: Seq a b =&gt; a -&gt; b -&gt; b Is there anything obviously harebrained with this idea? Note that it is not intended as a replacement for the current syntax but as syntactic sugar for type classes that represent abstract data types.
I do not see any more strictness requirements in Timsort than in mergesort.
What is this function? Can it sort other infinite lists?
"sure it examines all values" I couldn't say it more clearly. Please read psykotics post above mine ;) I just thought this ultra simple algorithm points the lazynesss out...
So, back to the point: you can do the exact same thing with timsort right? I understand it's basically a mergesort with a special pass to construct "small sorted runs" efficiently.
As far as I understand you can do this with timsort as well.
 cons :: MonadPlus m =&gt; a -&gt; m a -&gt; m a cons = mplus . return 
It's not a purely functional algorithm. You could maybe transfer some of the insights on optimizing for partially sorted runs but in the end it would be a very different algorithm. Of course, you could implement it directly by mutation in the ST monad but then you lose laziness.
Note that are already some relevant classes in existence (like Functor, Monoid, Foldable, MonadPlus). I like to use them when applicable. I agree that there are definitely missing ones, for example one to unify Map and IntMap etc. Many prelude functions could have been generalized too. For example "null" could had been made to work for all Foldables: null :: Foldable f =&gt; f a -&gt; Bool null = all (const False) "filter" could had been made to work for all MonadPluses: filter :: MonadPlus m =&gt; (a -&gt; Bool) -&gt; m a -&gt; m a filter cond = (&gt;&gt;= f) where f x | cond x = return x | otherwise = mzero etc
Master Po said, "Cons without uncons is non-cons." empty :: Seq a cons :: (a, Seq a) -&gt; Seq a uncons :: Seq a -&gt; Maybe (a, Seq a) uncons empty == Nothing uncons (cons (x, xs)) == Just (x, xs) maybe xs cons (uncons xs) == xs 
Nice, but a dead paradigm. Modern paradigm is to pull data into browser via JavaScript, generating DOM on the fly on the browser-side.
I'd say a lot of it is historic hacks.
I second this. The code and the writing are beautiful.
Can someone link a description of Timsort?
I hope this sees the light of day then http://hackage.haskell.org/trac/haskell-prime/ticket/97
Also maybe of interest: [mathlink](http://hackage.haskell.org/package/mathlink)
Let's also look an example where this fails to understand the issues better. Here's a functional merge sort: merge xs [] = xs merge [] ys = ys merge (x:xs) (y:ys) = if x &lt;= y then x : merge xs (y:ys) else y : merge (x:xs) ys halve xs = halve' xs xs where halve' xs [] = ([], xs) halve' (x:xs) xs' = let (left, right) = halve' xs (drop 2 xs') in (x:left, right) mergeSort [] = [] mergeSort [x] = [x] mergeSort xs = merge (mergeSort left) (mergeSort right) where (left, right) = halve xs This is worst-case O(n log n) time for a full sort. What about finding the minimum by unconsing? Let T(n) be the worst-case time to find a minimum element by unconsing from a merge-sorted list of length n. Our goal is to derive a recurrence relation for T(n) and then solve it. For the top level we first have to cut the list in two halves, which takes O(n) time. We then recursively sort the two halves and merge. Since we're only unconsing the first element, for merging we need only compare the first two elements of the sorted halves. But that's exactly what T(n/2) measures! So we have the recurrence relation T(n) = 2 T(n/2) + O(n). Its solution is T(n) = O(n log n), which is too large. To disentangle the problem a little bit, we can break it down into examining a specific algorithm for finding the minimum in a list based on binary chopping: minimum [x] = x minimum xs = min (minimum left) (minimum right) where (left, right) = halve xs It's easy to see that this takes O(n log n) time using the same recurrence analysis as before. The problem is the halving, which has no equivalent in imperative merge sort or the imperative version of the above minimum-finding algorithm. There halving is a simple matter of dividing the length by 2, a constant time operation. For functionally merge sorting a whole list the linear cost of the merge means that the linear cost of halving is absorbed and therefore we are not left behind in the asymptotic race. I am nearly certain that no variant of functional merge sort for lists could get rid of the linear time cost in the halving. I've figured out how to shave off a linear term (compared to my current halve function) by using a length-annotated zipper but that still leaves a single linear term. Even for something like finger trees (essentially tree zippers) rather than lists you are looking at O(log n) to move the fingers when you halve. Then you get the recurrence T(n/2) = 2 T(n/2) + O(log n) with the slightly better solution O(n log log n) but it's still not linear the way we'd want. As a corollary, if this observation is true then anything remotely like timsort cannot be lazy. If you care deeply about sorting laziness, my suggestion would be to examine the optimizations that go into it which aren't specific to merge sort (dealing with partially sorted runs) and see if they can be transferred to other algorithms. By the way, here's the minimum algorithm that corresponds to unconsing from tree sort. First let's recap what tree sort looks like: treeSort [] = [] treeSort xs = (treeSort left) ++ middle ++ (treeSort right) where pivot = pickPivot xs (left, middle, right) = (filter (&lt; pivot) xs, filter (== pivot) xs, filter (&gt; pivot) xs) The corresponding minimum-by-unconsing algorithm is this: minimum [] = [] minimum xs = if null leftMin then [pivot] else leftMin where pivot = pickPivot xs leftMin = minimum (filter (&lt; pivot) xs) When pickPivot is O(1) and makes a good choice at each level this is an O(n) time algorithm. Of course, the problem is how to always make a good choice. Any linear-time median selection algorithm for pivot picking would make this O(n log n) time, and you can't even make a constant-time randomized choice the way you can with arrays.
Qualified imports provide namespaces for identifiers so that everyone doesn't have to come up with unique names for their functions and types. Not using qualified imports has given raise to ad-hoc namespacing in the function names themselves (e.g. foldU, traverseA, etc). These short suffixes provide little information (what is U?) and you only have so many letters before you need to start using two letter suffixes. Also, avoiding using qualified imports leads to obscure functions names (intercalate) as people try to avoid name clashes. Trying to find unique names doesn't scale as the code base gets bigger.
http://paste.pocoo.org/show/133352/
DOesn't that one haskell fork solve this problem? http://www.haskell.org/haskellwiki/DDC
&gt; I've mentioned it before but I'd really like some notion of partial type signatures. How does this actually a help in this case? It seems that `cons` is easy enough to define without it...
In most languages you've got one major collection interface, providing length measure, indexing, iteration, sorting and others. In Haskell, I don't think that's what'll happen -- instead, we'll have `Lengthable c`, `Indexable c`, `Foldable c` and so forth. It takes some doing to sort out which classes are necessary given its going to be factored like that.
On a different tangent, I have always wondered how natural a lazy array sort based on the [Select](ftp://db.stanford.edu/pub/cstr.old/reports/cs/tr/73/349/CS-TR-73-349.pdf) algorithm would look in Haskell. So instead of getting the minimum element in O(n) comparisons, you could get the median, or any other single element in O(n).
Aren't there formulas where you can simply calculate any arbitrary digit of pi? http://en.wikipedia.org/wiki/Pi And there are formula that get MORE accurate, rapidly, as you carry out more calculations.
As I said, it's a syntactic convenience. When you have long and complex type signatures where everything is an abstract type variable (a -&gt; b -&gt; c -&gt; d, etc) then the current syntax is not very readable. The dissertation on Discipline tackles the same problem from a related but different angle.
By the way, MonadPlus seems to have more structure than you need. You only need a kind of functorial family of monoids with constructor-like `box :: Monoidal m =&gt; a -&gt; m a`. cons = mappend . box MonadPlus is of course an instance where `mappend = mplus` and `box = return`. I'm not sure what these are called in the literature. 
can you explain how it solves it? I read that page and didn't see anything about it.
Open a ticket!
Qualified imports are certainly better than unqualified imports (at least open ones), I don't think anyone argues with that. But it would be nice if code didn't have to be specific to ByteString.null vs. Prelude.null, but instead could use some typeclass's null and then be general to both String and ByteString. A type-class would guarantee that ByteString and lists adhere to the same names and semantics, etc.
but do consider that it's not "all gain no pain". The following program has problems: import Control.Monad import Data.Foldable import Prelude hiding (all, filter, null) null :: Foldable f =&gt; f a -&gt; Bool null = all (const False) filter :: MonadPlus m =&gt; (a -&gt; Bool) -&gt; m a -&gt; m a filter cond = (&gt;&gt;= f) where f x | cond x = return x | otherwise = mzero main :: IO () main = print . null . filter (&gt; 7) $ return 5 because the compiler can't determine which is the Foldable MonadPlus produced by return and given to filter (for example it can be a list and can be a Maybe) This works fine with Preludes's null and filter. 
I don't understand your type. Is Seq a type-class or a type?
I think the effect is minor (Adding type signatures is cheap) I also think the examples of this happening are rare enough that you had to use a non-useful one :-) Can you find a useful example?
&gt; Can you find a useful example? No. But maybe the best way to check is to get all of hackage and build it with alternative null and filter, and see if it brakes anything. 
It may be that we want something in between modules and classes. The problem with many modules implementing the same "interface" is that there is no specification of that interface so there's nothing to enforce that they really do implement the same interface. That makes switching harder. The problem with using classes is that as soon as you do make several types implement the same interface then people want to be able to parameterise by that interface. But doing so may either be inappropriate or impose performance penalties that negate many of the advantages of some of the instances. (I'm thinking particularly of a string-like class here. Parameterising by the type of string usually doesn't make sense and it can certainly impose a big performance penalty.) Something intermediate might be much like a type class but less first class, where you have to pick the instance at compile time, so it's more like a template or ML functor.
A type class that represents an interface to an abstract data type.
There are a few typos I should have fixed but the pastebin format makes it a bit hard. :) &gt; If a &gt;&gt;&gt; b = a &gt;&gt;&gt; c then a = b Typo. I meant b = c. &gt; What are the closest thing to lists while having a commutative kind of concatenation? Sets of course! I actually meant multi-sets, or bags. Unfortunately GHC has no Data.Bag. Absent that, you can easily implement a bag monoid by sorting the list monoid: data Bag a = Ord a =&gt; Bag [a] deriving (Show, Eq) bag = Bag . sort unbag (Bag xs) = xs instance Ord a =&gt; Monoid (Bag a) where mempty = bag [] xs `mappend` xs' = bag (unbag xs ++ unbag xs') instance Functor Bag where fmap f = bag . fmap f . unbag GHC complains about this functor definition but it's _morally_ correct. My use of existentials in defining Bag should ensure that fmap can only be called for ordered types yet GHC seems unable to pick up on that. I could work around that by sticking the relevant function from the Ord dictionary into the bag and passing it around but that seems grotesque. Is there anything I can do to make this work with existentials without having to do manual labor along those lines? In my post from a few days ago I was talking about conjugation. The above definition could be simplified with some higher-order functions for conjugation: conjugate1 a2b b2a f = b2a . f . a2b conjugate2 a2b b2a f a a' = b2a (f (a2b a) (a2b a')) Then you could write mappend = conjugate2 bag unbag (++) fmap f = conjugate2 bag unbag f (Digression: Even better, define a type of invertible, left-invertible and right-invertible functions. Conceptually, they're a pair, and when you invert you simply swap the entries. Conjugation would take something of this type as an argument. This reminds me that it would be cool if all of the syntax and library functions for functions were generalized to function-like types. This is actually something languages like Scala got right in my opinion. One can dream...) A replay of the example with bags: *Main Data.List&gt; let a = inject (bag [1]) *Main Data.List&gt; let b = inject (bag [2]) *Main Data.List&gt; let c = inject (bag [3]) *Main Data.List&gt; a &gt;&gt;&gt; (b &gt;&gt;&gt; inverse b) &gt;&gt;&gt; c == a &gt;&gt;&gt; e &gt;&gt;&gt; c True Bag is the free commutative monoid functor. Set would be the free _idempotent_ commutative monoid functor. That's a lot of adjectives. :)
I think what Peaker meant is there's a syntax problem in your type definitions: &gt; cons :: (Seq a b, Seq a b') =&gt; a -&gt; Seq a b -&gt; Seq a b' * "(Seq a b, Seq a b') =&gt;": Seq is used as a type-class in the context * "a -&gt; Seq a b -&gt; Seq a b": Seq is used as a type 
Oh sorry, that was a typo. I meant to write cons :: (Seq a b, Seq a b') =&gt; a -&gt; b -&gt; b'. I'll go back and fix it.
I agree with this. To expand on this idea, I think that the one-dimensional nature of source code is a poor way to represent structure.
You can generalize those even further, to monotyped containers as well. http://hackage.haskell.org/packages/archive/monoids/0.2.0.1/doc/html/Data-Generator-Combinators.html
But, it's two dimensional, though, isn't it?
Mulling it over, I've decided to list a few properties of containers that are a reasonable basis for classes. * `Finite`: The container has a finite length and strict folds can be expected to terminate. * Supports `length`. * Supports `reverse`. * `Ordered`: This property basically says there is a first element, a second element and so on. For a `Set` or `Map`, this property doesn't hold (Axiom of Choice not withstanding). Why do we care about this property? Well, a `List` of bytes can be UTF8 decoded but that doesn't make sense with a `Set` of bytes. * `Indexed`: Associates a key to a value as, for example, lists associate a natural number to a value. * `Membered`: Allows membership testing. This is relevant for "Hotel California" containers like Bloom Filters. As long as a container is a `Functor` it will have `fmap`. This excludes some things that we might reasonably call containers -- `ByteString`s come to mind. I'm not sure what to do about that. Note that any kind of `map` over `ByteString`a will have a much more restricted type than `fmap` -- it only looks like a map. I'm going to keep adding to this list (and I invite you to do the same). 
we may present it to ourselves as two dimensional because we can read more of it in a glance that way, but for the most part the significance of placement of any particular character right-to-left is much more than the significance top-to-bottom. That was a particularly poor explanation. To take another approach, you could describe a program as an AST. But most languages don't diagram this tree structure, they snip it up and smush the fragments up into single lines with terrible inventions like infix operators and parenthesis. Maybe my opinion exists more as intuition than as some sort of expressible argument. I'll have to think about it further.
I'll reuse my argument I posted in progit. The problem with names is that sometimes there isn't one as good as the code itself. 
reddit should have a "merge discussions" option for link reposts in different sections
What representation would you advocate then? S-expressions with structure editors?
In other words, don't post comments about what colo?r to paint the bikeshed.
Please don't do that. In general, hierarchies with too many root nodes are fragmented and unmanageable. For example, you will ruin the party for those who want to navigate the Haddock tree by starting with a simple, fully-collapsed tree and drilling down. Yes, there are other navigation methods that would not be affected, but why make this mess in the first place? It is analogous to putting all of your directories off the root of your Unix file system. True, modern FSes might not drag down your system performance anymore if you do that. And true, there are Unixy tools like "locate" and "find". But - why make this mess in the first place?
Please don't pick a new tiny part of the top level namespace. This makes it hard to categorize your package using automated tools.
How does sticking `Data.` or `Control.` in front of everything help?
&gt; For example, you will ruin the party for those who want to navigate the Haddock tree by starting with a simple, fully-collapsed tree and drilling down. But they get the same mess as soon as they open up `Data.`. I don't see how making them press one button before seeing a mess helps anyone. Worse even, one could end up looking through all of `Data.` before you find out it isn't there and you have to start all over again with `Control.`.
I just found this thread which might be relevant: [Revamping the module hierarchy](http://www.nabble.com/Revamping-the-module-hierarchy-%28was:-ANNOUNCE:-OpenGLRaw-1.0.0.0%29-td24005833.html)
I've been meaning to add Timsort to my package of sorting functions (among other things) for uvector for a while now (I have parts implemented in a previous experimentation directory, but never finished it). Some parts make sense for [a]. For instance, one thing Timsort does is find runs of non-decreasing/non-increasing elements in the original array. This has already been suggested for GHC's sort, because it was in the YHC sort, and performed better. Timsort also sets a lower bound on the size of the smallest initial chunks. This means that if the above doesn't provide a big enough chunk, it grabs some unsorted elements and sorts them using insertion sort. The chunk size is based on the size of the input array, which isn't readily available for lists, though. I'm not sure if that'd be a win or not. It also tries to arrange for merges between relatively equally sized arrays, but I'm unsure whether that'd make a difference, or be worth the overhead with lists. In the merging phase, Timsort uses techniques for finding bulk-copies. This is done by "galloping" over elements by checking positions at 2^i for i starting from 0, and then binary searching between 2^(i-1) and 2^i for the right place. However, it only starts doing this after seeing situations where it'd be a win in the process of normal merging, and switches back to normal merging when it thinks it's no longer a win. Needless to say, none of this makes sense for lists. So, it's not obvious that Timsort would be the best choice for lazy, immutable lists. There are parts that would be a good idea (and should be implemented, which, I think the YHC sorting thing never produced any results), but the algorithm as a whole isn't even readily implementable for Haskell's lists.
Not to rain on any parades, but this page is pretty old. My name's on that list, but the most recent e-mail I have relating to it is from April 2007. I don't know if anyone out there's been working on related stuff, but I haven't heard about it. Of course, if someone wants to dig into AI related stuff in Haskell, I won't stop them. :)
&gt;Is there anything I can do to make this work with existentials without having to do manual labor along those lines? [GADT-style](http://www.haskell.org/ghc/docs/latest/html/users_guide/data-type-extensions.html#gadt-style), perhaps? :) &gt; define a type of invertible, left-invertible and right-invertible functions [Twan van L's FRef post](http://twan.home.fmf.nl/blog/haskell/overloading-functional-references.details) talked about that. I'd like to see it, if only because I'm getting into species stuff, and I'd like to see some more related type families developed. 
http://www.shirky.com/writings/ontology_overrated.html
Prelude Data.Number.CReal&gt; showCReal 200 (10 * log 10) "23.02585092994045684017991454684364207601101 48862877297603332790096757260967735248023599 72050895982983419677840422862486334095254650 82806756666287369098781689482907208325554680 843799894826233198528393505" Anyway, might be good to get someone to record a video of it. :)
isn't "import qualified Data.Colour as Colour" the same thing?
I'm not really sure it makes sense to package a GHC backend thingy up for Hackage. :)
Aye but this is slightly more general than that.
His example about differentiating the type of cycles to get the type of lists reminds of topology. (In fact, a similar argument is used in introductory topology courses to prove that the circle and the line are topologically distinct, obvious as that may sound.) There you're also trying to count things like the ranks of homology groups. The excision theorem for homology groups is all about how you can puncture a space, solve the problem on the simpler punctured space, and then unpuncture the solution by extending it to the original space. More generally, you might have to puncture by a bigger subspace than a point to get useful information. For example, you can calculate the homology groups for a double torus by puncturing by the connective circle between the two holes, which leaves a pair of individually point-punctured tori each with homology groups Z^2 in degree 1. You must then undo the puncturing by figuring out how those individual homology groups are combined in the original space. Here it turns out they're simply multiplied; it's as if the toroidal components lead independent lives as far as homology is concerned. Thus the homology group of the double torus in degree 1 is Z^4.
Right, you probably shouldn't put it directly under Data, either. It should be even further down the hierarchy. You didn't provide a link, so I don't know exactly what this module is supposed to do.
Shirky's essay is highly overrated. Well, at least it is now. At the time that he wrote it, ontologies were all the rage. Some people thought they would somehow provide a magical AI solution to every information management problem. But now, partially under the influence of Shirky's essay, the pendulum has swung too far the other way. Ontologies are indeed useful and powerful. Not perfect, usually far from a complete solution, but very worthwhile.
Even beter: "import qualified Data.Colour as Color".
the specific example of "Colour" doesn't belong in Control.
Hey, hands off the bikeshed, OK?
Wow, that is relevant. And surprisingly, the author doesn't even mention Conor, despite an extensive history section and references. I emailed him.
By the same logic, at least 50 other packages could remove the prefix, and then the top-level would be extremely cluttered. The current ad-hoc solution is in serious need of revamping, I agree with full heart, but this sounds like a very bad idea. (I also agree that Data is overused). 
But it needs to be blue!
Then we agree. There are some abstractions (i.e. type classes) that don't exist today that could unify things. The interface shared between String and ByteString is "Sequence" or perhaps "Stream" (and *not* StringLike. Bytes != Unicode code points).
Yes, but `import qualified Colour` is less verbose.
For what it is worth: http://hackage.haskell.org/package/colour
I do agree that ontologies are not useless. However, I find it instructive to consider how I (and I assume most of us?) find packages on Hackage. I go to the big list of organized packages with the list of categories at the top that link to sections on the page, ignore all that, and use Firefox's in-page search.
Regrettably, that text still needs a lot of filling-in done to it, and it seems semi-abandoned.
If they press Network, and not Data, they don't get that mess.
I don't think one should be too bothered about the verbosity of the import statements.
It helps, just not enough. One might argue that Colour is a specific enough concept to image processing, graphics and UI design, that it belongs under the existing Graphics namespace as something like Graphics.Colour. This would get you out of the cesspool that is the root of Data. http://www.haskell.org/haskellwiki/Hierarchical_module_names In the end, the existence of the hierarchical namespaces is a bikeshedding issue, but it is one, kind of like the package versioning policy, that works best if everyone follows the same general standard.
Oh yes, Graphics.Colour seems *the* solution to me. (although I've never used the package :) )
One option for porting timsort would be to execute it with strict chunks and a lazy(?) spine, like a lazy bytestring. Timsort finds short monotonic runs, flipping reversed monotonic runs in place, and then extending them via insertion sort up to the chunk size, that seems like a fairly natural division of labor. In fact, we might even be able to win over the python implementation by lighting sparks for these tasks if the chunk size is large enough. I'm doubtful however, since locality effects will probably dominate parallelism opportunities here. That is then followed by a series of neighbor merges, which mutate the data in place using a temporary store the size of the smaller neighbor and some galloping tricks for dealing with long runs of identical data. These could be implemented in ST s fairly readily. The general issue is the merge passes at the end seem to destroy the laziness that you usually get from an insertion sort, so the question is, is there a compromise that permits lazy or chunkwise evaluation of the result list. Ultimately, every chunk will have to be touched to generate the head chunk of the result list, so a straightforward translation of timsort will not be lazy.
http://nothingmuch.woobling.org/talks/yapc_na_2009/haskell_brain.txt
wow, that noise idea is great (but syntax isn't that great). how hard is it to use these macro things (I mean things like error messages)?
&gt; It helps, just not enough. I'm slowly coming around to the opinion that the module/package system needs some fixing along the lines of [the thread that I mentioned](http://www.reddit.com/r/haskell/comments/99ole/removing_my_data_prefix/c0bxk9f). Dropping the `Data.` prefix is insufficient, at least without a change in the general module/package system.
I read the thread in question, and I loved wren's suggestion as to how you could build package-local namespaces, but it sounds like it would be severely invasive to all sorts of things inside of GHC. I am considering a very similar approach for my own toy language project, having seen the general growing pains of the existing Haskell hierarchical namespaces, however.
I didn't put it into `Graphics.` because the main module and datatype of `colour` is in principle physiological rather than graphical. I do admit that in practice many of the submodules (particularly sRGB) are graphics related ... hmm.
Wouldn't idiom brackets be pretty much trivially expressible via quasiquotes these days? many p = ps where ps = [$idiom| (:) p ps |] &lt;|&gt; pure [] or even many p = ps where ps = [$idiom| p : ps |] &lt;|&gt; pure [] 
Why not just use a spigot algorithm -- digits do not get emitted until they are known to be correct, and # of digits emitted only limited by the available memory?
IIRC it inserts {-# LINE #-} pragmas that tell GHC to report the right line in error messages. I haven't used it, so I can't vouch for its overall friendliness though.
Is there any discussion of GHC sort vs. YHC sort?
Let me just try a bit of expectation management. SHE makes no attempt to support error handling via LINE pragmas. Mea culpa: it's an omission, but not difficult to rectify. I had other priorities. ghc -E happens all too often. We're at the very early prototype stage, so the emphasis is on experimenting with functionality. Syntax for noise has varied: I used {..} at one point, but I had to change that when I started using {..} to support dependent types. By all means, bikeshed away!
I expect you're right, but as I had the machinery anyway, it wasn't hard to manufacture the round-bracket version. many p = (| p : many p | [] |) SHE is a mixture of Epigram-hacking conveniences and experiments with candidate features for Haskell. For the latter, it's nice to try to get the look, albeit with incomplete success. For the record, I wouldn't dare suggest that a feature SHE implements is "implemented" in the sense required for candidacy to Haskell Prime. I'm just having (a lot of) fun.
&gt; Even better, define a type of invertible, left-invertible and right- invertible functions. Conceptually, they're a pair, and when you invert you simply swap the entries. I see what you did there. What *does* one get by completing **fns** to **K fns fns**, anyway?
You would need an algebra of functions corresponding to a commutative monoid for that to work. In that case I would expect some kind of super-exponential type, isomorphic to a plain exponential type. Basically, it would be something like Z^(A^A) = (A -&gt; A) -&gt; Z. This suggests something involving Church numbers, doesn't it? By the way, I seriously did not think of Grothendieck completion when I wrote that comment. I can hardly believe that myself! But the pair view of invertible functions is kind of inevitable when you take a constructive approach to mathematics. I think I'll use this as further motivation the next time I have to explain Grothendieck completion to someone with a constructive bent. Instead of thinking of an invertible monoid as a monoid with a function for computing inverses, it's intuitionistically natural to think of an invertible element as an element paired with its inverse (technically you'd also need to include a proof that they are indeed inverses). Once you get that, Grothendieck's construction seems almost inevitable.
One of the interesting features of the better languages in the APL family (I'll nominate K) is that many point-free idioms are so concise that if you were to package them up into reasonably named functions in a standard library you'd increase their lengths by a significant percentage. K is something of an extreme case and it can certainly be a mixed blessing.
Many papers on this project. (could someone add the rest from the nicta link?) * http://haskell.org/haskellwiki/Research_papers/Program_development#Operating_systems This is a significant coup for Haskell in large scale verification.
how about {- -} for noise? if it's possible it would look great (dimmed font or whatever for comments) in every haskell-aware editor out there
I might be mistaken, but it looks like there is something wrong with the ordering of the elements: *Main&gt; goUpperRight (([[1]], [[2]]), ([[3]], [[4]])) (([],[]),([[2,1],[4,3]],[])) *Main&gt; goLowerLeft (([[1]], [[2]]), ([[3]], [[4]])) (([],[[3,4],[1,2]]),([],[])) *Main&gt; goLowerRight (([[1]], [[2]]), ([[3]], [[4]])) (([[4,3],[2,1]],[]),([],[])) It might not be want you want, but here is an attempt for a fix: [http://haskell.pastebin.com/m726324fd](http://haskell.pastebin.com/m726324fd). O(n^2) instead of O(n), but it seems to give the right results: *Main&gt; goUpperRight (([[1]], [[2]]), ([[3]], [[4]])) (([],[]),([[1,2],[3,4]],[])) *Main&gt; goLowerLeft (([[1]], [[2]]), ([[3]], [[4]])) (([],[[1,2],[3,4]]),([],[])) *Main&gt; goLowerRight (([[1]], [[2]]), ([[3]], [[4]])) (([[1,2],[3,4]],[]),([],[])) Maybe also concatVertically and concatHorizontally are interchanged? *Main&gt; goDown $ (uncurry concatHorizontally) $ splitVertically (([[1]], [[2]]), ([[3]], [[4]])) (([[1]],[[2]]),([[3]],[[4]])) *Main&gt; goRight $ (uncurry concatVertically) $ splitHorizontally (([[1]], [[2]]), ([[3]], [[4]])) (([[1]],[[2]]),([[3]],[[4]])) 
Who verifies the verifier?
some [previous discussion](http://www.reddit.com/r/haskell/comments/95xdi/cal_haskellish_language_compiled_to_jvm/) about it.
Why is it significant for Haskell again? The microkernel is written in C, and Isabelle, the proof assistant, is written in SML if I recall correctly. Do we mean that it's significant as far as the methodology of provably correct software development goes?
The kernel is implemented in Haskell, a tool translates it to Isabelle and proves that Haskell impl correct. C is then extracted. Haskell as the EDSL used to verify the first full scale kernel -- yeah, that's good.
This is #haskell. That joke isn't funny here.
Oh, is that what they mean by the executable specification that's written in Haskell? I took that to mean that it was a sort of kernel ABI emulator which implemented the kernel's interface, so that a test suite could be written against that interface then tested against a real kernel to ensure identical results were obtained.
The seL4 kernel was designed using an executable prototype in Haskell (seL4 is a derivative of L4, not the exact same kernel). That prototype was good enough to run moderately complex software on the kernel (which was important to ensure that the kernel interface is practical). The C implementation is a manual re-implementation of the behaviour (and to a certain extent structure of the Haskell prototype). Finally, the abstract model against which the C code was verified was extracted from the Haskell model (after it was semi-automatically translated to Isabelle). In other words, the Haskell model —and hence, Haskell— played a pivotal role in that project! For details, see our 2006 Haskell Workshop paper describing the Haskell work: http://www.cse.unsw.edu.au/~chak/papers/DEKC+06.html
Use [split](http://hackage.haskell.org/packages/archive/split/0.1.1/doc/html/Data-List-Split.html)?
[Here](http://neilmitchell.blogspot.com/2008/03/sorting-at-speed.html) is a Neil Mitchell blog post on the subject, which also links to some mailing list discussion on the topic (which may be the discussion I'm remembering). The important part is the `risers` function there, and (I suppose) the note that the YHC sort is a tuned version by Lennart Augustsson, so it isn't surprising that it's better than GHC's, which is a completely unremarkable bottom-up mergesort, as I recall. Timsort uses a slightly fancier version of `risers`, which starts with two (three?) possible cases: for (x:y:zs@(z:_)) 1) If x &lt; y, then look for a non-decreasing run 2) If y &lt; x, then look for a non-increasing run 3?) If x == y, repeat with y and z and until one of the above two cases applies Non-increasing runs become non-decreasing runs via reversal, which should be faster than running any sorting algorithm on the chunk.
Hm, I see. Did anywhere in that extremely long thread did anyone explain why GHC doesn't use the YHC sort?
Ah. God. Damnit. for some reason I had the idea that Hoogle was searching hackage, not just the standard libraries, so I figured a package like this didn't exist yet. fuck me. Well thanks for the link anyway.
That wasn't a joke.
Isabelle (the proof assistant used) has a core theory. Isabelle proofs can be exported in that core theory, which can then be verified by an independent prover. The core theory is small enough that it's implementation could be verified manually (not sure whether anybody actually ever did that). The core logic itself has of course been thoroughly studied.
Not sure it's a good idea to steal comment syntax for code which has effects even if it doesn't contribute values. It's not crucial that noise delimiters nest, which may open other possibilities. I just considered (-..-), but (-3-) is an operator section! It's just occurred to me that {..;..;} might be possible and have a certain sick appeal. Wadler's law strikes again!
-fancy: gayest suffix ever
Not that I recall. But it's probably because no one was aware that it was so superior, or had bothered caring much about its performance. And since it still hasn't been added (to my knowledge), it's the usual situation where someone points out something that should be done in the libraries, and then everyone forgets to do it afterward. :)
how about making it easy to configure in the source?
I agree. Now you need to pick the color space. YCbCr, YCgCo, RGB, HSV... so many to choose from, so little time.
any updates on this work? it seems very interesting and I'd like to see more :)
I was feeling unsatisfied with the Enum class and wrote this instead. It's not really intended to be practical (the function equality and enumerable functions in particular are silly) but was a fun experiment. I was thinking it could be fun to make applicative parsers (as they're forced to be context-free) an instance of this, but I don't really have much experience with the parsing libraries. Does anyone know if any existing parsing frameworks allow the kind of introspection I'd need to enumerate the strings the parsers accept?
&gt; instance (Enumerable a, Eq b) =&gt; Eq (a -&gt; b) where This instance seems wrong. It should be: instance (Finite a, Eq b) =&gt; Eq (a -&gt; b) where or [better yet](http://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/): instance (Compact a, Eq b) =&gt; Eq (a -&gt; b) where 
Yes, I've been having a little experience of that myself with Control.Monad.ignore. It's quite distressing. I guess I'll put this on my TODO list for after 'ignore' is settled one way or another.
I don't think you can do it with Parsec, as the parsers are opaque functions (IIRC).
I guess it wouldn't work anyway, as I'd need to reify each grammar I wanted to enumerate into a type before being able to make an instance for it.
The [enum](http://hackage.haskell.org/packages/archive/emgm/0.3.1/doc/html/Generics-EMGM-Functions-Enum.html) function in [EMGM](http://www.cs.uu.nl/wiki/GenericProgramming/EMGM) does this generically.
Can you give me a link to an editor for wxHaskell?
[wxGlade](http://wxglade.sourceforge.net/) may be of interest [DialogBlocks](http://www.anthemion.co.uk/dialogblocks/) too. I haven't tried either of these yet. I don't remember what the status of the XRC code is (if it has been released yet), but I do know it's there.
Thank you very much!
There is a vi/emacs clone written in haskell already it's called yi http://haskell.org/haskellwiki/Yi
Yup. I'm doing it just for the sake of learning.
Alright, I made it so it's Finite. Still not sure how I feel about it being "wrong", but it definitely doesn't seem useful unless it's finite.
Ah, that's nice! Would it be possible to enumerate functions with that though?
Great work Petr!
odd, website doesn't work with Opera: GET / HTTP/1.1 User-Agent: Opera/9.64 (Windows NT 6.0; U; en) Presto/2.1.1 Host: lhc.seize.it Accept: text/html, application/xml;q=0.9, application/xhtml+xml, image/png, image/jpeg, image/gif, image/x-xbitmap, */*;q=0.1 Accept-Language: en-AU,en;q=0.9 Accept-Charset: iso-8859-1, utf-8, utf-16, *;q=0.1 Accept-Encoding: deflate, gzip, x-gzip, identity, *;q=0 Referer: http://lhc-compiler.blogspot.com/2009/08/status-update-new-integer.html Connection: Keep-Alive, TE TE: deflate, gzip, chunked, identity, trailers HTTP/1.1 406 Not Acceptable Content-Length: 0 Content-Type: text/plain Date: Sun, 16 Aug 2009 03:16:44 GMT Server: Happstack/0.2.1
I've added a more complete version of this to hackage at http://hackage.haskell.org/package/enumerable, if anyone's interested :)
I don't believe it. Blogspot does not run on happstack! :-) When I wget -S that page the server reports itself as `Server: GFE/2.0`.
I wonder how it performs next to the old Integer.
I think he is talking about the LHC wiki, which does run on Happstack. Look at the host field in his headers.
How do I shoot?
Middle clicking.
you may want to look at SmallCheck for depth-bound enumeration -- basically you want to do the same thing without a depth bound.
oh, to work there... out of curiosity, how low is the dumbness bar set up? are there any non-phds working there? maybe even some people without [formal] education? if so, are they doing ok?
&gt; and a good computer science background is required If you have "a good CS background" without formal education, you probably know it ;)
I notice that the previous message says that Jane's St. is also hiring for OCaml dev: http://www.haskell.org/pipermail/haskell/2009-August/021566.html Good times for functional programmers?... well, maybe, but that's only two places. 
[Not necessarily](http://en.wikipedia.org/wiki/Dunning-Kruger_effect)... 
Initially, I was going to say no, because EMGM only supports datatypes isomorphic to sums-of-products. However, now that I think about it, it might be possible to simply give a class instance for Rep Enum to do it. Hmm...
See, I can't figure this out. Perhaps I read your comment wrong, but is it just that working with functional languages is laughable to most of the programmer multiverse? I think I see one of these[1] in every "Haskell achieves X" or "Ocaml achieves Y" or "Acme Lambdas is hiring functional programmers" post. 1. It could be that I just assume people look at FP with disdain in the typical work place--it may not actually be the case.
Is Galois only considering folks willing to relocate to Portland?
"Enginners work in small team settings, and must successfully interact with clients, partners, and other employees in..." Hmm, I don't have a comment.
Sorry, there was a typo.
It wasn't really relevant. 
And perhaps I read your comment wrong as well... because I can't figure it out. Maybe I should restate my first comment: it's nice that people are getting hired to do FP. But it's not taking the world by storm yet. Maybe someday it will, but so far that doesn't seem to have happened.
The same kind of reflection is also seen in one-dimensional list zippers. It's what allows efficient motion and access to the elements neighboring the focus. When the focus is in the upper right, the nearest element to the focus is 2, so you should be able to access it in O(1) time. With my original version that's possible but with yours it takes O(n) time. I didn't emphasize this reflection property since I assumed the reader already had some familiarity with zippers. When I turn this into a full article, I'll be sure to make this point more explicitly.
Ah, that's better. I guess I'm pre-primed to read that sort of statement as a slight. Thanks for clarifying. Sorry for any offense. 
Boo. Emacs! vi is for the devil etc etc. Looks nice!
The parsimony library I'm putting together should allow it, since it is Applicative, and there is a zipper for the grammars available. I'll follow up on IRC.
That is what the reflection package is for. =)
Very good read dons. I contacted you earlier via messaging, I'm not sure if you saw it, just weren't interested, or don't have time. Let me know which if you can. I love the scrabble analogy. For the reference of others, and if I remember correctly, H (quaternions) are a division algebra (and hence a division ring). Octonions (O) form a nonassociative division ring. As far as I've heard, there are rather easy ways to restore commutativity to quaternions, and even return associativity to octonions using monoids. That's one of the reasons I'm interested in Haskell specifically to study them. There is a friend of mine who is studying (and generating) quite a bit of mathematics in formulating physics in terms of a 4d wave equation based in quaternions. Most of his mathematics is sound (as I have checked out with Mathematica, but I am reproducing in Haskell), but he uses a few unusual tricks to accomplish his goals. You can see some of results of this at [visual physics](http://visualphysics.org). For more details on the mathematics behind those visualizations, see [physics with quaternions](http://world.std.com/~sweetser/quaternions/qindex/qindex.html). There are hour long lectures where he steps through his work, trying to explain it to viewers. He is ... eccentric. 
Hello all, Firstly, apologies for the self promotion. Any comments you have on the code much appreciated, please feel free to make them here or ask questions. (Especially if you wanted to try it out then something changed your mind or you tried and something stopped you.) TIA
I think no one quite acknowledged the important point raised (by Igloo, I think? don't recall). Namely, hierarchical packages mean that someone looks for OpenGL doesn't need to look in the Network hierarchy. Yeah, sure there's some ambiguity, but that's not a fatal problem, since we're talking about human beings browsing API documentation. We aren't talking about trying to automate things. We're talking about trying to organize things by topic, acknowledging that there will necessarily be some overlap. Libraries have faced a similar problem for centuries, but they didn't give up and just arrange all the books alphabetically by title. If they did, they would be massively less useful. Anyone looking at placing each package in a separate top-level module should go re-read statistics on how many new packages are uploaded to Hackage every week.
Awesome! It made quite a comeback there in the middle.
I've been a fan of L4 and Haskell for a long time. They just make sense. Stuff like this makes one want to look more into hOp and House again.
I find that a surprising UI choice.
i have a rec
&gt; Very good read dons. Don did not write that article. He submitted it. I'm not sure what you mean by "restore commutativity and associativity". The non-commutativity of the quaternions is fundamental to their usefulness. Rotations in spaces of dimension greater than 2 combine in a non-commutative way. As for octonions, Frobenius proved that the only finite-dimensional associative division algebras over R are R itself, C and H. There is no room for an associative version of O. There are useful graded associative algebras like Grassmann algebras and Clifford algebras but they lose other desirable properties and are not generally commutative. They are certainly not division algebras in general; Grassmann algebras are riddled with zero divisors from the start and Clifford algebras begin to have them when you get to dimension 3.
Thanks, this comes at an opportune time for me. Only last week I was kludging together some special purpose code for doing column-aligned text layout.
One good argument (there are of course many) for effect monads more fine grained than IO is that it makes this kind of "mocking" much cleaner. If you do that then you can statically check that a test is guaranteed free of external effects when run over a mock instance of the appropriate monad type class (e.g. a file system monad). You could then run a test suite in parallel, distributed, etc. 
My thanks to the rest of the steering committee for their hard work.
The only problem with the library as-is is that the Alignment and Box type constructors are not exported, so you can't actually write any type annotations for code that uses the library. ;)
Yes :) However it doesn't always manage to do so ... gotta keep fixing and tweaking :)
Those were some pretty interesting slides.
Thanks =)
Posting about these seemingly awesome tech talks is almost a tease for all of us non-Oregonians. ;-) We want videos! It seems like it would benefit Galois, the guest speakers, and the FP community.
I recommend reading Martin Escardo's papers, and his guest-posts on [Andrej Bauer's](http://math.andrej.com/2008/11/21/a-haskell-monad-for-infinite-search-in-finite-time/) [blog](http://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/). I would even be so bold as to draw your attention to my comments on the first-mentioned post, where I point out that his structures, which are precisely your 'J's, form a monad.
I'm sorry to bother you, but it seems that you intended both links to refer to different posts, whereas they refer to the same URL (and I couldn't discern a comment related to monads as you mention). I can't seem to find a related post in Mr. Bauer's blog. Would you be so kind to add the link you intended?
Thanks. Martin Escardo was the person talking about this monad at lunch. I got the definition of the monad, and some of the logical properties it has, but no functional properties. Ah, I see now that Escardo's search monad `S` is `J Bool`.
My bad; the follow-up post is here: http://math.andrej.com/2008/11/21/a-haskell-monad-for-infinite-search-in-finite-time/ Thanks for the catch.
Awesome! I had a short correspondence with Dr Escardo, where I pointed out that his 'forsome' and 'forevery' functions are argument-flipped isomorphic forms of folds (as in Haskell's Foldable class), just without the kludge of inventing the aliases for disjunction and conjunction.
+1 For god's sake, record and publish those tech talks.
No idea why you'd want to do that, given the flexibility of the functions in System.Process.
Personally, I find that Escardo's S monad is more readable if you flip the arguments around and use function names that mimic the corresponding operations from Data.List. That yields something like: import Prelude hiding (any,all,and,or,elem,notElem) import Control.Applicative hiding ((&lt;*),(*&gt;)) newtype Search a = Search ((a -&gt; Bool) -&gt; a) choose :: (a -&gt; Bool) -&gt; Search a -&gt; a choose p (Search f) = f p -- Escardo's search has the signature of Data.List.find find :: (a -&gt; Bool) -&gt; Search a -&gt; Maybe a find p xs | p x = Just x | otherwise = Nothing where x = choose p xs any, all :: (a -&gt; Bool) -&gt; Search a -&gt; Bool any p = p . choose p all p = not . any (not . p) and, or :: Search Bool -&gt; Bool and = all id or = any id elem, notElem :: Eq a =&gt; a -&gt; Search a -&gt; Bool elem = any . (==) notElem = all . (/=) instance Functor Search where fmap f xs = Search $ \p -&gt; f $ choose (p . f) xs instance Monad Search where return = Search . const _ &gt;&gt; ys = ys xs &gt;&gt;= f = Search $ \p -&gt; choose p . f $ choose (any p . f) xs instance Applicative Search where pure = Search . const fs &lt;*&gt; xs = Search $ \p -&gt; let f = choose (\g -&gt; any (p . g) xs) fs in f $ choose (p . f) xs -- search is a semigroup under union union :: Search a -&gt; Search a -&gt; Search a union xs ys = Search $ \p -&gt; let x = choose p xs in if p x then x else choose p ys -- and intersection intersection :: Search a -&gt; Search a -&gt; Search a intersection xs ys = Search $ \p -&gt; let x = choose p xs in if p x then choose p ys else x doubleton :: a -&gt; a -&gt; Search a doubleton x y = Search $ \p -&gt; if p x then x else y bit :: Search Int bit = doubleton 0 1 cantor :: Search [Int] cantor = sequence (repeat bit) -- see libraries@ proposal to merge into Applicative class Applicative' f where (&lt;*) :: f a -&gt; f b -&gt; f a (*&gt;) :: f a -&gt; f b -&gt; f b instance Applicative' Search where a &lt;* _ = a _ *&gt; b = b em :: Search (Either a (a -&gt; Bool)) em = Search (\p -&gt; Right (\l -&gt; p (Left l))) peirce :: Search (((a -&gt; Bool) -&gt; a) -&gt; a) peirce = Search (\k l -&gt; l (\m -&gt; k (\_ -&gt; m))) toCont :: Search a -&gt; Cont Bool a toCont (Search z) = Cont (\k -&gt; k (z k)) Also note the existence of intersection, which probably breaks some invariant of Escardo's code, but which was too concise to resist writing.
&gt; Nix is a purely functional package manager. It allows multiple versions of a package to be installed side-by-side, ensures that dependency specifications are complete, supports atomic upgrades and rollbacks, allows non-root users to install software, and has many other features. It is the basis of the NixOS Linux distribution, but it can be used equally well under other Unix systems. http://nixos.org/
Oh, factoring that out to obtain the J monad might make the dependencies on Bool clearer. import Prelude hiding (any,all,and,or,elem,notElem) import Control.Applicative hiding ((&lt;*),(*&gt;)) import Control.Monad.Cont newtype S r a = S ((a -&gt; r) -&gt; a) type Search = S Bool choose :: (a -&gt; r) -&gt; S r a -&gt; a choose p (S f) = f p find :: (a -&gt; Bool) -&gt; Search a -&gt; Maybe a find p xs | p x = Just x | otherwise = Nothing where x = choose p xs any :: (a -&gt; r) -&gt; S r a -&gt; r any p = p . choose p all :: (a -&gt; Bool) -&gt; Search a -&gt; Bool all p = not . any (not . p) or :: S r r -&gt; r or = any id and :: Search Bool -&gt; Bool and = all id elem, notElem :: Eq a =&gt; a -&gt; Search a -&gt; Bool elem = any . (==) notElem = all . (/=) instance Functor (S r) where fmap f xs = S $ \p -&gt; f $ choose (p . f) xs instance Monad (S r) where return = S . const _ &gt;&gt; ys = ys xs &gt;&gt;= f = S $ \p -&gt; choose p . f $ choose (any p . f) xs instance Applicative (S r) where pure = S . const fs &lt;*&gt; xs = S $ \p -&gt; let f = choose (\g -&gt; any (p . g) xs) fs in f (choose (p . f) xs) -- search is a semigroup under union union :: Search a -&gt; Search a -&gt; Search a union xs ys = S $ \p -&gt; let x = choose p xs in if p x then x else choose p ys -- and intersection intersection :: Search a -&gt; Search a -&gt; Search a intersection xs ys = S $ \p -&gt; let x = choose p xs in if p x then choose p ys else x doubleton :: a -&gt; a -&gt; Search a doubleton x y = S $ \p -&gt; if p x then x else y bit :: Search Int bit = doubleton 0 1 cantor :: Search [Int] cantor = sequence (repeat bit) -- see libraries@ proposal to merge into Applicative class Applicative' f where (&lt;*) :: f a -&gt; f b -&gt; f a (*&gt;) :: f a -&gt; f b -&gt; f b instance Applicative' (S r) where a &lt;* _ = a _ *&gt; b = b em :: S r (Either a (a -&gt; r)) em = S $ \p -&gt; Right $ \l -&gt; p (Left l) peirce :: S r (((a -&gt; r) -&gt; a) -&gt; a) peirce = S $ \k l -&gt; l $ \m -&gt; k $ \_ -&gt; m toCont :: S r a -&gt; Cont r a toCont (S z) = Cont $ \k -&gt; k (z k) 
well it fixes by design many gentoo bugs, too bad it's not source based and still too small to use I think...
It is source based, but with binary packages as an optimization.
then it sounds too good to be true.
It's written by Haskellers @ Utrecht...
Ah-hah, that makes sense.
Slick!
The Fink package system on OS X is another example of such a system (ie. both source and binary).
One more step, we can add toSet and toList coercions that try to extract the elements of the list one at a time assuming equality or order is sound for the type. toSet :: Ord a ⇒ Search a → Set a toSet = toSet' Set.empty where toSet' ys xs | p x = toSet' (Set.insert x ys) xs | otherwise = ys where p y = y `Set.notMember` ys x = choose p xs toList :: Eq a ⇒ Search a → [a] toList = toList' [] where toList' ys xs | p x = x : toList' (x:ys) xs | otherwise = [] where p y = y `List.notElem` ys x = choose p xs Note that the extraction into list form, while unordered, is lazy enough for infinite sets -- if not exactly efficient.
Heck, most binary distributions are source-based at their cores. It's too tedious to create the binary packages without a source-based system of some sort.
Another related link: http://www.cs.bham.ac.uk/~mhe/papers/selection-escardo-oliva.pdf
I put together a [module with a lot of boilerplate](http://coder.bsimmons.name/blog/2009/08/haskell-boilerplate-for-google-codejam/), which should provide a good framework for any of the problems, as long as they follow the same format as last year. Hope it encourages people to use their language of choice and participate if they wouldn't have otherwise :)
The thing to do would be to partner this with nilfs, a file system that acts like a purely functional data structure.
I'd like to see cabal-install to incorporate the ideas of Nix so at the very least we could uninstall cabal packages.
and use hurd kernel (it also doesn't have any [side] effects)
Me too :-) Patches gratefully accepted! There's plenty of other good ideas in Nix and we're slowly tring to steal more of them in Cabal. One of the differences is that with Cabal we do not have to deal with loads of random old build systems so we don't have to go to quite the same effort in making a clean build environment.
I never really liked the few-hour long programming contest format (probably because I'm not very good at it). The winning code is usually terribly from an software engineering perspective (oh, I'll just allocate a 1000000 entry array upfront, that way I don't need to worry about memory management). That's why I sort of prefer the ICFP contest, which seems to strike an acceptable compromise in contest duration. Of course, it is good to have contests of all sorts of different natures.
It is. ;-) There are a few situations where it's not immediately obvious what to do in a purely functional style. One of them is GHC's package registration. Basically, if you install a new package (Haskell library), you can't update the package database `package.conf` via `ghc-pkg --register`, you'd have to rebuild the database from scratch. But then, it's not clear who "owns" it. The nix package containing the GHC compiler doesn't, because it depends on the libraries. No single library does either, because it depends on other libraries. Making GHC depend on the libraries is not a good idea, for that would mean rebuilding GHC every time you want to install a new library. 
(from the link in the article) &gt; The outcome is the result of four years’ research by Dr Klein’s team of 12 NICTA researchers, NICTA/UNSW PhD students and UNSW contributed staff. They have successfully verified 7,500 lines of C code and proved over 10,000 intermediate theorems in over 200,000 lines of formal proof. The proof is machine-checked using the interactive theorem-proving program Isabelle. It is one of the largest machine-checked proofs ever done. 7500 lines of C explodes into 10,000 theorems, explodes into 200,000 lines of proof? That strikes me as a strong argument against C for secure systems design (as if there needed to be another one). 
There is also support for scion in Yi head, but it doesn't have as many features as the emacs code yet, as far as I can see.
There's another Rake-like tool called "hake". HackageDB: hake-1.3.3 http://hackage.haskell.org/package/hake
&gt;Known bugs: &gt;... &gt;- linux make files are busted Ooh! Ooh! Sign me up! Sounds like these guys could be the next micro-$oft! 
did anyone else think it said scrotum at first?
What retards; as if C isn't both crazy and unportable. Actually, I have no idea how you could consider Haskell even remotely unportable.
[And being more popular :-)](http://qa.debian.org/popcon-graph.php?packages=xmonad%2C+scrotwm&amp;show_installed=on&amp;want_legend=on&amp;want_ticks=on&amp;from_date=&amp;to_date=&amp;hlght_date=&amp;date_fmt=%25Y-%25m&amp;beenhere=1) But yes, this kind of attitude is precisely what holds our industry back. Doesn't matter if xmonad is 1/4th the size, comes with 100% core test coverage, has Coq proofs of its correctness... doesn't matter how good the engineering. So: * make sure your distro bundles the Haskell Platform by default * don't rewrite things in C for "portability" Rewriting a perfectly good, popular Haskell app in C "for portability" should never happen. Our community needs to make sure this never seems like a sensible idea.
I'm pretty sure that's intentional.
If it's not in C, it's not software (just a script, mayyybe) :-) So many people still believe this (even though it is not true, and never was, of course)…
Is xmonad-lite still maintained somewhere? It would do something to cure his complaint of a language dependency.
[Details here](http://braincrater.wordpress.com/2008/08/28/announcing-xmonad-light/). Honestly, we could have a whole suite of configuration parsers in xmonad.hs that are entirely not dependent on having GHC at runtime. YAML, JSON, Haskell Read/Show, ConfigFile, Sexp.
I've actually considered Haskell somewhat unportable at times! Haskell assumes certain things about the available environment - such as basic [IO](http://www.haskell.org/onlinereport/io-13.html) (putStr / getChar) and a [file system](http://www.haskell.org/onlinereport/io.html) (openFile etc). So frustrating!
Yes; I've always wanted to run a window manager on a machine without basic IO or a filesystem.
The guy does know, surely, that GHC can compile via C?
http://hackage.haskell.org/package/atom
I'd like to see JSON configuration.
it is hard to see how hake is like rake. Nemesis is a good copy of the rake DSL except instead of a hash syntax :task_name =&gt; [:dependency] there is a simple string "task_name: dependency"