Still quite good. I really enjoyed learning FP via Caml Light, LP with Prolog and had a little taste of SML. So you can imagine how long ago this was, Miranda was still a thing. So even though I dabbled in other FP languages since then, my only means to do FP at work is via Java, C#, JavaScript and C++. Which makes me jump of joy every time they adopt a little bit of FP, even if I don't get the full package.
&gt; Headaches [for Java 8]: A bolted-on feeling makes us want to jump in with both feet and use Scala (see below). Suggesting the epitome of bolted-on-ness in response to a feeling of bolted-on is kinda ironic.
Ok, here is an example implementation: {-# LANGUAGE DeriveFunctor #-} module Main where import Control.Monad.Free (Free, liftF, iter) import Control.Foldable (traverse_) type Player = String data DecidedMatch = DecidedMatch { winner :: Player, loser :: Player } data MatchF a = MatchF Player Player (DecidedMatch -&gt; a) deriving (Functor) type Tournament = Free MatchF match :: Player -&gt; Player -&gt; Tournament DecidedMatch match player1 player2 = liftF (MatchF player1 player2 id) playerNamed :: String -&gt; Player playerNamed name = name runTournament :: (Player -&gt; Player -&gt; Bool) -&gt; Tournament a -&gt; a runTournament decide = iter (\(MatchF player1 player2 continue) -&gt; if decide player1 player2 then continue (DecidedMatch player1 player2) else continue (DecidedMatch player2 player1)) doubleEliminationGroup :: Tournament [Player] doubleEliminationGroup = do match1 &lt;- match (playerNamed "1") (playerNamed "2") match2 &lt;- match (playerNamed "3") (playerNamed "4") winnersMatch &lt;- match (winner match1) (winner match2) losersMatch &lt;- match (loser match1) (loser match2) decidersMatch &lt;- match (loser winnersMatch) (winner losersMatch) return [ winner winnersMatch, winner decidersMatch, loser decidersMatch, loser losersMatch] main :: IO () main = do let decider player1 player2 = case (player1,player2) of ("1","2") -&gt; True ("1","3") -&gt; True ("1","4") -&gt; True ("2","3") -&gt; True ("2","4") -&gt; True _ -&gt; False traverse_ putStrLn (runTournament decider doubleEliminationGroup) I hope this is helpful and not scary. If you'd want to run a tournament with probabilities I'd use [`FreeT`](https://hackage.haskell.org/package/free-4.12.1/docs/Control-Monad-Trans-Free.html#t:FreeT) over a [distribution monad](https://hackage.haskell.org/package/probability-0.2.5/docs/Numeric-Probability-Distribution.html#t:T). 
He is pretty harsh on Haskell. Attacking it for the type system hole that GeneralisedNewtypeDeriving left, for example. Or its "unusable cost model". Also harsh. Or attacking it for its lack of modules. But these aren't unreasonable positions. I think these are actually constructive criticism, though they're not phrased that way. So now GHC has roles to fix GND. It'll get strict-by-default modules. And Backpack. 
This looks very promising, I'm gonna go read up on `Free` monads.
All mutually recursive families of types can be expressed as a single GADT with a type level tag: -- mutually -- data Expr = EInt Int | ELam Pat Expr -- data Pat = PAs String Expr -- as GADT data ExprType = ExprTy | PatTy data Expr (t :: ExprType) where EInt :: Int -&gt; Expr ExprTy ELam :: Expr PatTy -&gt; Expr ExprTy -&gt; Expr ExprTy PAs :: String -&gt; Expr ExprTy -&gt; Expr PatTy deriving instance Eq (Expr t) deriving instance Show (Expr t) Similarly, we write the pattern functors for mutual types as a single GADT: data ExprF (k :: ExprType -&gt; *) (i :: ExprType) where EIntF :: Int -&gt; ExprF k ExprTy ELamF :: k PatTy -&gt; k ExprTy -&gt; ExprF k ExprTy PAsF :: String -&gt; k ExprTy -&gt; ExprF k PatTy deriving instance (Show (k PatTy), Show (k ExprTy)) =&gt; Show (ExprF k i) deriving instance (Eq (k PatTy), Eq (k ExprTy)) =&gt; Eq (ExprF k i) However,`ExprF` is not a functor anymore; it has kind `(ExprType -&gt; *) -&gt; (ExprType -&gt; *)`. We can define a new class for indexed functors: class IxFunctor (f :: (k -&gt; *) -&gt; (k -&gt; *)) where imap :: (forall i. a i -&gt; b i) -&gt; (forall i. f a i -&gt; f b i) instance IxFunctor ExprF where imap f = \case EIntF i -&gt; EIntF i ELamF pat expr -&gt; ELamF (f pat) (f expr) PAsF str expr -&gt; PAsF str (f expr) Now, we can build our recursion schemes, but we use `IxFunctor` instead of `Functor`: newtype IxFix f i = In (f (IxFix f) i) out (In f) = f deriving instance Show (f (IxFix f) t) =&gt; Show (IxFix f t) deriving instance Eq (f (IxFix f) t) =&gt; Eq (IxFix f t) deriving instance Ord (f (IxFix f) t) =&gt; Ord (IxFix f t) cata :: IxFunctor f =&gt; (forall i. f a i -&gt; a i) -&gt; (forall i. IxFix f i -&gt; a i) cata phi = phi . imap (cata phi) . out [I wrote up a bit of a self-contained example for this](https://gist.github.com/AndrasKovacs/af856be6cf816e08da95). There are a lot of things in Haskell such that we can build their indexed versions: - Indexed monad (can support type-changing state and other fancy stuff) - Indexed lenses (in particular, indexed `Control.Monad.Plated` would be nice for mutual types) - Indexed uniplate (again, works with mutual types) - Indexed cofree comonad / indexed free monad - Indexed `bound`: binding variables over typed/mutual AST-s. - Indexed `data-reify` for observable sharing and turning AST-s into graph. These libraries either don't exist or exist scattered around in different packages, with ample code duplication, which is the reason I sometimes wish for a standardized "indexed ecosystem" in Haskell. For AST manipulation, [`compdata`](https://hackage.haskell.org/package/compdata) and [`multirec`](https://hackage.haskell.org/package/multirec) already provide support for indexed/mutual types. However, these libraries aren't particularly accessible (tutorials, docs plz!) and are both bundled with additional stuff that make them even less accessible: `compdata` has "Data Types á la Carte"-style open sums, and `mutirec` has a generics-based API. The [`syntactic`](https://hackage.haskell.org/package/syntactic) package also overlaps with the aforementioned ones. I think a lot of useful work and reasearch could be made regarding an "ultimate" AST transformation library that tries to synthetize the best parts of the existing libraries, and also provide an unified solution for most plausible AST-manipulation tasks. So we could have something that works on mutual and non-mutual data, can bind variables, does annotation, provides generic traversals and folds, can reify into graphs, lets us swap features in-and out as needed without rewriting everything, maybe even let us work with generic signatures and types á la Carte, and also cook our dinner. Now, this may seem like an overly bloated thing, but I see no other way to rectify the current sitatuation, where libraries solve some problems separately but for non-trivial cases we end up rewriting functionality from scratch because we need to solve most of those problems simultaneously. 
So are you saying that OCaml would be better choice for the sake of platform independence? 
Hmm, this is intriguing. How is type inference for this stuff? And does the type checker take full advantage of the fact that kind `ExprType` is only inhabited by types `ExprTy` and `PatTy`?
That wasn't my point. I consider F# a better ML than Scala, specially given the syntax. Since on my line of work we are only allowed to use platform first class languages, I get to use F# when on .NET projects even it if is only for my own scripts, because usually IT makes a full VS install. When working on Java projects, the devenv made available by customers only allows for Java, so no alternative languages, given that we don't have always root rights. Also even with the half-heart approach from Microsoft to F#, the language has quite a good tooling and libraries vs other MLs, specially on Windows. How is OPAM support on Windows nowadays? 
This is quite enlightening. I have been wondering for years how can a genius like [Mike Bostock](http://bl.ocks.org/mbostock) program daily in Javascript, and be happy and write such awesome code. I will come back to your post and think again about it. Unfortunately, i am used at having my fingers glued to some keyboard, and unfortunately a professional software developer gets often asked about writing large, ambitious systems. In this sense i think that technology still plays a crucial role
I wish they'd do it for F*'s sake :)
GHC can't infer anything about `ExprType` inhabitation, as far as I know. At least the situation is not *worse* with the indexed GADT-s, for example, I could derive instances for `ExprF` with roughly the same amount of boilerplate as would be necessary for mutual types: deriving instance (Show (k PatTy), Show (k ExprTy)) =&gt; Show (ExprF k i) deriving instance (Eq (k PatTy), Eq (k ExprTy)) =&gt; Eq (ExprF k i) As to the many rank-2 types versus inference... it's acceptable. Sometimes we're bitten by the fact that `a -&gt; forall b. t` types are a bit of a lie, since GHC floats foralls out to the top of the scope, and we have to introduce typed let-bindings to correctly generalize. The lack of support for impredicative types sometimes makes `newtype Nat f g = Nat (forall i. f i -&gt; g i)` a necessity. `Nat` also makes type inference straightforward, but always using it instead of rank-2 types is a bit of a hassle, since then we'd also need different newtype wrappers for `forall i. a -&gt; f i`, `forall i. a i -&gt; f (t b i)` etc. 
I was referring to the last sentence about tooling. Since Microsoft has conflict of interest, it is hard to believe that NET/F# will be well supported on other platforms. I can install Ocaml with single command at my convenience - it would be nice if I could do the same for F#, but I would have to install ...Windows as well, because there is no tooling for other platforms.
[Previous discussion](https://www.reddit.com/r/haskell/comments/3rh9tn/burritos_for_the_hungry_mathematician)
I have a real-life example I was previously doing with `Free2`, a "two-parameter free monad" constructor. I'll try the GADT method!
I think you're right. If you could go from Gödel numbers to a typed representation, you could reproduce the standard diagonalization argument (as you say), but I think that's it.
I learned about most of this in two Stack Overflow threads, they're nice examples, if you're interested: - [This one is about indexed Free](http://stackoverflow.com/questions/27676294/working-out-the-details-of-a-type-indexed-free-monad/27695354#27695354) - [This one is about indexed uniplate and its lens version](http://stackoverflow.com/questions/25355570/simplifying-a-gadt-with-uniplate)
Right. And compared to [my later comment](https://www.reddit.com/r/haskell/comments/3sj85d/designing_data_structures_that_are_dependent_on/cwyb7lz) the downside is that the you must get the order right when running the rounds (the tournament structure isn't encoded in the structure, but depends on you to get that order right). However the code is simpler than the later example (at least it's got that going for it). Also with the later example you can encode any type of tournament (single/double/etc) I guess... but I'm not familiar with all the tournament formats to really state that as a fact.
I am running F# and MonoDevelop without major issues on Ubuntu installation. Any F# PCL that is Mono compatible will run on all platforms targeted by Mono. The upcoming Xamarin release is improving their F# support. OCaml is great, if one doesn't care about Windows and is happy with Emacs/VIM as devenv.
The role of `+1` is simply "take an argument and produce something *different*", and `\x` can fulfill that role, without requiring the language to support integers. One thing I don't get, though, is how to type p_u — what are the "suitable type annotations for x,y" that the authors omit? Don't they want to make the two sides have the same type? But they they'd have a type satisfying T = A -&gt; T, which is impossible unless you use an FOmega with equirecursive types. I think that if you add integers to FOmega you can use `+1` again, and the proof of 3.3 would still go through; if you use `+1` with Church-numerals, showing their contradiction is probably more involved (I'd guess it's trivial semantically, but you need to rely on correctness of Church-encoding of numerals which is far from trivial).
&gt; The role of `+1` is simply "take an argument and produce something different", and `\x` can fulfill that role, without requiring the language to support integers. Sure, but they're trying to convince us that a style of proof does not work. It's not very convincing if they choose *one* possible way of "producing something different" and say that does not work. They need to convince us it doesn't work for *all* reasonable ways of "producing something different"! 
Well that certainly sucks... In any case, thank you for the heads up. I guess I won't be using the REPL on this project.
&gt; As a hopeless nerd, I'm hoping to write my own language with subtyping, structural (i.e. non-nominal) typing, linear typing, dependent typing, and a codata/data distinction. I was thinking of the same, though I'm not entirely sold on subtyping; I'd have to play around with LiquidHaskell and F* to see how that works in practice. Unfortunately, I'm still struggling to wrap my head around Martin-Löf's type theory, so I'm a long way from actually implementing something. Wish you luck!
That's not an acceptable solution for me.
Yeah, the crucial bit is that `fmap` (aka operator `&lt;$&gt;` or `liftM`), when used with a function of arity 2 or more, creates a list of functions, or in general a Functor of functions. Then `&lt;*&gt;` (aka `ap`) applies each function of the lhs list to each element of the rhs list (or in the general case uses the Applicative definition). For larger arities you can just keep applying `&lt;*&gt;` until you get the desired values.
This is really, really cool. Thanks! Have a free monad /u/changetip
Fantastic! Haskell never stops surprising me. But I am not familiar with GADT stuff—I did consider the possibility of putting all stuff in one sum type but that without GADT (your approach) would lose type safety—so now I have two choices: either I can be more practical or I can take this as a chance to learn them… I have a bad feeling about my poor brain.
I don't think they really feel Haskell is "terrible," just that it has theoretical inconsistencies (some of which have to do with `seq` for example). And since they are theorists, they actually care about such inconsistencies.
Hmm, I thought you can still spoof a `Typeable` instance if you really *insist*, with some black magic courtesy of [Reflection](https://www.fpcomplete.com/user/thoughtpolice/using-reflection): {-# LANGUAGE RankNTypes, ScopedTypeVariables #-} module Main where import Data.Typeable import Unsafe.Coerce data Foo = Foo data Bar = Bar class MyTypeable t where myTypeRep :: Proxy t -&gt; TypeRep instance MyTypeable Foo where myTypeRep = typeRep instance MyTypeable Bar where myTypeRep = typeRep newtype MagicTypeable a r = MagicTypeable (MyTypeable a =&gt; a -&gt; r) spoofTypeable :: forall a r. TypeRep -&gt; a -&gt; (MyTypeable a =&gt; a -&gt; r) -&gt; r spoofTypeable rep val k = unsafeCoerce (MagicTypeable k :: MagicTypeable a r) (const rep) val printTypeRep :: forall t. MyTypeable t =&gt; t -&gt; String printTypeRep _ = show (myTypeRep p) where p :: Proxy t = Proxy main = print $ spoofTypeable (typeRep p) Foo printTypeRep where p :: Proxy Bar = Proxy The above works and prints "Bar", but after replacing `MyTypeable` with real `Typeable` it segfaults. I'm not sure if the class is different from any other, or if I'm just doing something wrong. I do realize that it probably wouldn't count, as it's essentialy a contrived way to implement `unsafeCoerce` with `unsafeCoerce`, but apparently this kind of magic still compiles to legal `Core`, so I'm not sure what to think about this. 
How do I write an indexed function of type forall i. a i -&gt; b i where `i :: ExprType`? 
I've been keeping pretty close track of who has issues with what and I've spent the year since traveling around to conferences and user group meetings talking to folks. So, yes. After some people expressed concerns about `length` the polling started trending in the limit towards 82%. Before that it was trending towards 84-85%. Considering that poll was between an actual concrete proposal and literally doing anything else, that is still a rather ridiculously strong super-majority. Allowing for a further drift as people have used the compromises that result, something in the 78-80% range would fit with my understanding of past drift and observations since. Consider that the bulk of the voices who are agitating against the Foldable/Traversable proposal today are the same voices that were agitating against it before and that there have been no new viable alternative proposals that weren't covered in the original FTP FAQ. That said, I'll freely admit if you take it apart more piecemeal there is some rather wild fluctuations in opinions about individual details of the FTP. e.g. you can pull a larger percentage who are upset over the presence of generalized combinators in `Data.List` and who think we should have monomorphized or removed them, and of people would prefer to ungeneralized `length` and `null`, or who think it was a mistake not to take more names from the Prelude to bring `foldl'` into scope, or who would have preferred a slimmer `Foldable` at the expense of more silent changes to the semantics of existing code. Some of this we may be able to address through the Haskell Report process.
Sure, it's not finished yet though it works, what's missing is pure UI detail. https://github.com/alvare/tateti-tateti I've used a Monad Trans approach, with lots of State and Lens. A thing I'm struggling with is colors. I have a very tight loop where I don't want to use newColorID, but having them predefined is complicating things up :/
Style guides in Haskell make all sorts of random proscriptions that I've never been willing to bring myself to follow.
Also it might not do *all* you need to do here. Creating new entries in such a structure requires the `At` typeclass, which is not implemented for list-like things (only for maps).
SML, in the eighties, was my first foray into this family of languages. I moved on; it hasn't. I would try SML again if there was a version with critical mass to its community, and modern libraries. Unicode? If I can't use all cores, theory is pointless. This is trivial in Haskell. So I looked again. SML of NJ is no longer dormant, but how active? MLton's author now works at Jane Street. I would venture that even Coq has a bigger community than either of these? What is the canonical choice for programming in SML in 2015?
IIUC, you estimate the throughput by dividing the data size D by the empirical mean of the running times. This is not the same as the average of the reciprocal running times (which is proportional to the average throughput) (D is constant), in fact by Jensen's inequality it's just a lower bound. Just how big is the discrepancy might be more noticeable in longer jobs (depending on how the RTS behaves if the machine resources fluctuate in time), or GHC might make all this analysis irrelevant; no idea. Still, good to keep in mind.
&gt; If I get it right you'd like to interpret the ExprType tags, and not just simply throw them away with K or pass them along. I want to pass them along, I think. I want an equivalent of the free monad, but with types indexed by `ExprType` at the leaves, not just one type at the leaves. Anyway, I think I have enough to go on for now. Thanks for introducing me to this interesting area!
Isn't a closure just an anonymous inner class?
I think people are very divided on whether row polymorphism / subtyping is useful/disciplined.
One tricky thing is that in practice the deletions and additions to the arrays will always happen at the end. This is because the algorithm detects added and removed keys and for arrays the keys are indices. I.e. for a list such as `[1,2,3]` and `[2,3]` is will regard index `0` as modified, index `1` as modified and index `2` as deleted. So the deletion can only happen at the end. As far as I can tell this makes using a type class difficult because for arrays you only need push and pop. But actually guaranteeing that is hard. I'm not sure if the above explanation makes sense. Let me know if it is unclear.
You can define some types as `newtype` and make them instance of `Show` type class instead of using those format* functions. It seems like you can use `Text.Printf` module to get rid of some formatting like `padL`. You should also benefit by representing your database as `IntMap` assuming that bar code is unique. 
Alcoholic? I don't associate FP with downers. Now if initially learning Haskell had driven you to drink, that I'd understand.
How long was this refactoring, exactly? Would be I correct in guessing that it was one of those 72-hour sessions during which disassociation sets in?
I don't understand why Facebook is doing this with rule-based classification instead with something more sophisticated. Instead of writing rules about every single thing they missed maybe there are high-level patterns that are easier to catch with a statistical model.
Because of precision. You want to target very specific content. All fully automated classifiers are imprecise. E.g., even with gmail often legit email ends up in spam. 
Definitely refinement types are big (like LiquidHaskell). You can specify much richer properties and have them proven using an SMT solver. Still limited but huge potential there. In general, any form of formally embedding lightweight contracts into code are very exciting IMHO.
In fact, there is currently work on impredicativity [underway](https://ghc.haskell.org/trac/ghc/wiki/ImpredicativePolymorphism/Impredicative-2015) as we speak. We'll see how this story turns out.
I submitted a patch.
I didn't say it wasn't on purpose. But it does make it less nice to work in when building something new.
what are the issues with row polymorphism? (correct me if I'm wrong: that's the one where you can have a list of Point3 (x,y,z fields) or Point2 (x,y fields) and give either the type [{x :: Int}]. and then map over the x field, like incremeneting. but, not mix Point2s and Point3s in the same list, OO-style. I thought I'd read that one had fewer issues, and it seems simpler to me, in a vague way.).
Julian Arni delivers a marathon talk on [Servant](http://haskell-servant.github.io/) for the [Boston Haskell](http://www.meetup.com/Boston-Haskell/events/226374552/) users group. http://www.meetup.com/Boston-Haskell/
`isSubsequenceOf "Ial" "I really like Haskell." == True` so an example does exist it just may not be as obvious.
Julian managed simultaneous live-coding and live-responding-to-ekmett, all while on a borrowed laptop. Very impressive!
makes no sense, precision is exactly what you are training for, there's no reason why a rule based classifier would be more precise than something else... recall might be affected, on the other hand, but the ability to find spam that isn't in rules is close to none... 
I'm a big fan of session types, although I haven't had the time (and still need to level up a bit) to see if the types can be made easier to work with. Edit: probably worth pointing out that this probably doesn't require a new language or a new language extension.
Even if this is tail recursive, the gc still needs to allocate for each item of the list during the traversal. However they are immediately garbage collected. So the maximum memory used by your program is less than this. Also, ghci runs in not optimized mode, so a lot of optimization / simplification are not done.
It's great that they are promoting Haskell, but I would like to hear about something other than Haxl&amp;Sigma.
&gt; concern themselves only with flat lists like strings. That's true, but it's easy to generalise to trees by recursively using it on each nested array. In fact, that's what the existing [aeson-diff](https://github.com/thsutton/aeson-diff) library does.
Hello there, I am sorry I have taken so long to get back to you. I am using manifold learning to research domain adaptation in computer vision/ image processing. I am hoping that some of haskell's more "advanced' abstract frameworks will impose/influence a nice relationship between coding and understanding the problem domain.
what are session types? I found https://hackage.haskell.org/package/sessions but it's old is it like Servant? it seems to be defined as type safe communication between server and client.
Yes, or you could just ignore him/her. This person seems to have no idea how to learn and is desperately outsourcing their homework.
&gt; the presentation is really condescending. In what way?
They're a way of providing a type for an entire protocol. It's a bit like Servant, in that you can get your client and server from the one definition, but it covers more of the back and forth, you can enforce deadlock freedom, and, in the more advanced forms, you can deal with more than 2 parties. They've got links to linear logic and various concurrency formalisms, and I'd really like to see a) how far the usability could be pushed and b) how far you could extend existing things (like STM? or any number of fun things) with this extra level of information. [This](http://users.eecs.northwestern.edu/~jesse/pubs/haskell-session-types/) and [these](http://groups.inf.ed.ac.uk/abcd/) are good places to start digging.
Yeah, I didn't get it either, but it's happened multiple times now...
I know I shouldn't enable but goddamn do I want to answer east questions. hint: compose two prelude functions. 
Is what right? You didn't post a function.
I'm curious what made you think he was being *really* condescending? I didn't get that sense from the presentation, so I wonder if my perception was skewed. FWIW I thought he was being quite modest in his presentation. I don't recall him speaking as if he was superior. He was asked if Haskell should be used elsewhere inside FB and I thought he was being conservative in his response. He recommended using the right tool for the job which might not be Haskell in all cases.
Could you briefly explain to me how the library function `f` is less extensible than the library function `g`, in my example above?
Is this a joke?
Good job! My main comment is that you might do well to study [the `Data.List` module](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html) and try and simplify your code by using functions from there. Take for example this function: formatLines :: [ (Name,Price) ] -&gt; String formatLines list = concat [formatLine (name,price) ++ "\n" | (name, price) &lt;- makeBill barCodes)] Setting aside the fact that the `list` argument is unused, this seems like it's basically the same thing as this (using [the `unlines` function from `Data.List`](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:unlines)): import Data.List (unlines) formatLines :: [ (Name,Price) ] -&gt; String formatLines _ = unlines (map (formatLine . makeBill) barCodes)) (**EDIT:** I got a type error there I think, but you get the idea.) Another general remark is that you are using list comprehensions very heavily. There's nothing wrong with list comprehensions, but... in my experience, newcomers lean a bit too heavily on them because they're not familiar with the large repertoire of functions in the [the `Data.List` module](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html) (have I mentioned [the `Data.List` module](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html) yet?). So for example, this function: look :: Database -&gt; BarCode -&gt; (Name,Price) look dBase barcode | null find = ("Unknown Item", 0 ) | otherwise = head find where find = [(name, price) | (barcodeProduct, name, price) &lt;- dBase, barcode == barcodeProduct ] ...I would rewrite to something like this, using [`find` from the `Data.List` module](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:find): import Data.List (find) look :: Database -&gt; BarCode -&gt; (Name,Price) look dBase barcode = -- `find predicate list` = find first element of `list` that satisfies `predicate` case find matchesBarcode dBase of Nothing -&gt; ("Unknown Item", 0 ) Just (_, name, price) -&gt; (name, price) where matchesBarcode (barcodeProduct, _, _) = barcode == barcodeProduct Think of this this way: list comprehensions can only do three things: 1. Take cartesian products of the elements of input lists (which is like the `concatMap` function). 2. Filter out elements of these cartesian products (which is like the `filter` function). 3. Apply a function to all elements of the result list (which is like the `map` function). So if you're leaning very heavily on list comprehensions, you're limiting yourself to three operations out of dozens that are available! I did notice you threw a `replicate` in there, so this suggestion is basically to go further down the utility functions route. Oh, and before I forget, I should mention [the `Data.List` module](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html)!
Slides here: https://snoyberg.github.io/generated/2015-11-10-twitter-haskell-fast-concurrent-robust-services.html#/ And the talk starts around at [8:52](http://www.youtube.com/watch?v=6bkWvfI7QDQ&amp;t=8m52s) (Thanks to William Sewell for that).
Let's hope this gives the committee more confidence to do the right thing and break stuff ... and stop more people from getting off the Haskell train ༼ຈل͜ຈ༽
Perhaps it's a style thing. I found the slow, half baked story-like approach to be the condescending part. 
Thanks, The exercise was a exercise to practice with list comphrension, so that is why I did it this way. Is there a way I can take care that the MakeBill function is not called and executed three times on different functions. 
Data.List.NonEmpty
 newtype First a = First a instance Semigroup (First a) where a &lt;&gt; _ = a newtype Min a = Min a instance Ord a =&gt; Semigroup (Min a) where Min a &lt;&gt; Min b = Min (min a b)
Yes.
Indeed. For the record, as someone who has been vocal about the social consequences of generalizing list functions like maximum and length, I do generally support having Foldable and Traversable in universal scope. I'm absolutely delighted with AMP and I don't personally care whether we have MRP or not - afaict, I'll just have to delete my redundant return definitions. I fit into the description of having been around a long long time, (Orwell, Gofer, Hugs, ghc), but my only contribution to the community was answering questions on stack overflow. My point is that I've made a lot of noise recently over a part of one change but that's completely out of proportion to my position in the community and my feelings about recent changes generally. TL;DR little fish realises big splashing gives skewed impression 
What tools were used in creating this slides?
Not at all. Not sure why I was downvoted. Leading tech companies take that strategy all the time with standards, platforms, languages, etc. Facebook seems to be at the "extend" phase with Haskell. And they are somewhere between extend/extinguish phase with html/css/js via React. Apple wants apps instead of the open web. Same thing. When a company is no longer the leader, they fall back to "embrace" stage. I've seen people express surprise for a couple years that Microsoft has been so "open source friendly" lately. I understand that this talk is validating for Haskell users in a way, but I like to keep that (E.E.E.) cycle in mind when I see what technologies companies are "supporting." Scare quotes because the relationship isn't necessarily simple or benevolent. Not sure whether that perspective is irrelevant, stupid or just unpopular. Or maybe people just don't like links with no explanation? I don't know. What made you think it was a joke?
Learn them both so you'll be able to complain vocally about the design limitations of either.
A bounding box has a natural union but no natural empty. 
A statistical model will by definition have false detections, with the order of magnitude of request, they would be hitting a large number of authentic users. But they can still be useful filtering or analyzing.
Nice and fresh demo. I like these kind of presentations since they teach much more than more formal ones. Really I don´t understand the fascination with Servant. All that I see is a framework that manages low level thing without any kind of high level abstraction. It is like if I use Haskell for creating a type safe assembler. is that cool? yes for assembler geeks, but claiming that the result is useful for general programming, it is a bad use of Haskell. EDIT: Well I'm unjust above. it has the right level for defining APIs, but not for web applications.
Since when isn't 0 the unit of addition? edit: Sorry, I forgot that there are other definitions of natural numbers than zero : ℕ; succ : ℕ -&gt; ℕ. I blame it on too much Agda.
Any statistical model can be trained to reduce the number of false positives (increase the precision and decrease recall). Writing rules is much more error prone. They probably don't have language agnostic rules, or at least not efficient ones. For example, I published a blog post to myself (only I could see it) to see how facebook displays it. After doing that for about 5 times, just making the blog post visible to myself, I decided to publish it to every one of my friends, Facebook blocked it and said it was dangerous content, by doing simple language analysis they could have deduced it was not spam, I guess this kind of situation wasn't in their rules. The same kind of situation can happen with a statistical model, but you could model features far higher up the abstraction chain with it than with using english based rules.
For these sort of easy questions, I recommend #haskell-beginners on irc.freenode.org. The people there are very friendly and helpful.
It's not that easy. You may be hitting the edge of a ROC and the only way to decrease false detections is to reduce true detection. You have also to factor the time to train, adapt and test the model. Also, on a statistical model it's much harder to tell what it will target, and when it does wrongly, how to patch it away. Something I bet Facebook wants to have at hand to either unblock a rule too stringent or react quickly to a new kind of attack, with a precise scope.
If all you have is module Id where newtype ProjectId = ProjectId { unProjectId :: Int } newtype StoryId = StoryId { unStoryId :: Int } that might be mildly useful, but not very useful. However, if you have something like this: module Project (ProjectId, StoryId, getStory, getProject) where newtype ProjectId = ProjectId { unProjectId :: Int } newtype StoryId = StoryId { unStoryId :: Int } getProject :: IO ProjectId getProject = undefined getStory :: ProjectId -&gt; IO StoryId getStory = undefined then you might have something useful. Another use of `newtype`s is to take advantage of typeclass instances. [Here's an example](http://stackoverflow.com/questions/22080564/whats-the-practical-value-of-all-those-newtype-wrappers-in-data-monoid) although the popularity of someone who commented "I can't say I ever use them" shows that this might not always be useful. In your case I would use `newtype`, as it offers some benefits beyond eliminating mixups between `ProjectId` and `StoryId`. Mostly I think it clarifies the API documentation. If I see an `Int` I see a value that can span the range of `Int` and that you can do math on. You probably aren't doing math on your `Id`s. By using the `newtype` you can strictly define what you can and can't do with an `Id`, where to me if you used a `type` synonym that would suggest that an `Id` can be added, subtracted, negated, etc. On the other hand a `type` synonym can be useful when you really do have values that are interchangeable, but you just want to have a synonym to help the reader through the documentation. Sometimes it is quite intentional that the values be interchangeable even though it can sometimes be useful for humans to view them as being different in some way. Both [Pipes](https://hackage.haskell.org/package/pipes) and [lens](https://hackage.haskell.org/package/lens) use this to great effect. That huge diagram on the `lens` page is a tower of type synonyms. Good luck figuring it out; even seasoned `lens` users don't necessarily grasp all of it. `pipes` is a much more accessible example of this sort of thing. A `Producer` and a `Consumer` are both type synonyms for a `Proxy`. The [tutorial](https://hackage.haskell.org/package/pipes-4.1.7/docs/Pipes-Tutorial.html) is great. But both `pipes` and `lens` are rather advanced usage of polymorphic types with synonyms. As for reader monads, in my view they add a layer of abstraction that does not offer enough benefit to offset the added layer of boilerplate, but I would call this bikeshedding so I could go either way. For instance there could be times where you are building your own `Monad` and the use of a reader monad in a stack could result in less work for you (sometimes you can write a huge stack, put it in a `newtype`, and then use `GeneralizedNewtypeDeriving` to save yourself from having to write your own `Monad` instance.) But if I'm not doing something like that, I don't bother with a `Reader` monad.
iirc that type will be part of GHC 8.0!
What do you mean *more typical*?
I think saying "natural numbers (positive integers)" instead of just "positive integers" is unnecessarily confusing, given that people reading it will tend to associate "natural numbers" with non-negative integers.
Now that you ask that, I don't know how to answer it. I guess I mean something like "similar to how someone would do things in C++ or whatever mainstream language people usually use for 3d applications".
&gt; Its GUI support is about as bad as Haskell's. Hear, hear.
Here's their (quite gentlemanly) discussion of pipes and conduit: https://www.youtube.com/watch?v=Qn2Oc4vWoGg
It is certainly possible to get Frege code running on Andoid (I have a test app on my phone ....). I'm not sure about the current state of Java support on Android. But even if it's still Java6, it is defnitly possible to compile Frege to Java 6 code. It's just not maintained, despite the apparent high interest for Android. To get this going, all that would be needed is a concerted effort of some people that are really interested and knowledgeable in that stuff. (I personally am not.) 
&gt; All that I see is a framework that manages low level thing Considering how rare that is, that's a significant part of it right there. &gt; it is a bad use of Haskell. How so? &gt;it has the right level for defining APIs, but not for web applications. It is missing some of the things needed for writing web apps, but it isn't a question of being the wrong level, just motivation.
If you want to be really nitpicky, 0 being positive or not is also cultural.
The Haskell community is lucky to have these two professional, brilliant stalwarts.
So... can we create a monoid for every semigroup by making the value optional? newtype FirstExisting a = FirstExisting (Maybe a) instance Monoid (FirstExisting a) where mappend (FirstExisting (Just a)) _ = FirstExisting (Just a) mappend _ &lt;&gt; other = other mempty = FirstExisting Nothing
I know you can cheat because both authors are there, but conduit and pipes aren't really good libraries to present to people just learning, their documentation is scary.
wt actual f
FWIW, [Ur/Web](http://impredicative.com/ur) supports both ML-style modules and type classes. I've found the combination pleasant to use. Their interaction even enables a new feature: "closed" type classes that cannot have new instances added. The type classes work a bit differently than Haskell's, though. I don't know the details, but perhaps they're more like a variation on implicit arguments.
Great talk, but I wish haskell videos didn't nearly all have crummy audio &amp; barely visible/cut off slides.
Do "counting numbers" include transfinite ordinals too?
I only use type synonyme to alias type, ie because the type is too long to type (like `lens`) or because I'm not sure what I need and I want to be able to change it later (e.g. `type Amount = Double` because I might change to `Float` or `Decimal`). Using type synonyme to add semantic is IMHO a bad idea. The first reason is , is give you a false security. You think you add something to the documentation, but it's not checked by the compiler, so you can mix for example function using `ApiToken` and `String` without any problem. That just confused the user. The second reason is, unless your type is really opaque (as `ApiToken`) could be. The user needs to know what is the real type. I'm speaking for example of `type FirstName = String`. You need to know it's a string (or a text) to capitalize for example. If the user needs to know the real type, is better not adding an extra layer with a type synonyme. If the type is really opaque, then a newtype is perfect. However, I recently find that you can use `newtype` to add semantic without adding boiler plate. The idea is to use the newtype in every function, but never store or manipulate them as newtype, but just build them each time they are needed. Let's take the example of `Firstname` and `Surname`. You could have `user` record data User = User { firstname :: String, surname :: String} with a smart constructor newUser :: Firstname -&gt; Surname -&gt; User newUser (Firstname firstname) (Surname surname) = User firstname surname When you need to create a user (given two strings), you just call newUser (Firstname firstname) (Surname surname) That way, you can't call `newUser` with the argument in the wrong order. Of course you can still do thing like newUser (Firstname surname) (Surname firstname) but then, the bug should be easier to spot. Also, using typeclass it would be possible to define `newUser` so that it takes arguments in any order (I'm not sure it's necessary though). 
(Aside: your definition doesn't use the semigroup structure, since it discards the second argument to `mappend` if the first one isn't `FirstExisting Nothing`, so you don't need the semigroup assumption and can make a monoid this way out of any type.) /u/phadej [provides](https://www.reddit.com/r/haskell/comments/3srhit/something_that_is_semigroup_but_is_not_monoid/cx00bhi) a [link](https://hackage.haskell.org/package/semigroups-0.18.0.1/docs/Data-Semigroup.html#t:Option) to a definition that does make use of the underlying semigroup. The existing Monoid instance for Maybe has a Monoid context, so provides `Nothing` as a new `mzero`. This new monoid contains the newly-non-identity element `Just mzero`, which loses the battle with `Nothing` over who does less: `mappend (Just mzero) Nothing` is `Just mzero`.
I tried to run this in ghci. (last $ [1..100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000]++[1..10])::Integer It has been running for hours and memory usage did not change. https://www.dropbox.com/s/qfgkm6xldsii74c/Screenshot%202015-11-14%2009.37.59.png?dl=0 Now I should find a way to get same information without waiting 12 hours. GHC.Stats seems like a good place to start.
Modifying OverloadedLists and OverloadedStrings to dispatch more stuff at compile time is a very good idea that has come up a couple of times, but nobody has proposed an actual concrete plan to get us there. e.g. you might have something where "foo" desugars to `$(fromStringExp "foo")` letting us catch more of these cases at compile time, but we'd need to add to `fromStringExp` to the `IsString` class, figure out how to match up the type in the splice to the one we expect in the code, etc. This would permit failure when injecting into a non-empty string/list type, but it doesn't address the similar issues around existing manual calls to `fromString` in the class today. [Edit: formulated this one as a response to the now deleted thread, this thread has more content, but i haven't updated the response.]
&gt; [Edit: formulated this one as a response to the now deleted thread, this thread has more content, but i haven't updated the response.] The only substantial change is the title (use -&gt; integrate with), Reddit doesn't allow to edit the topic title, so I had to delete it and create a new one.
What about this then? This version replaces numbers with `(fromInteger &lt;number&gt;)` instead of lifting them, but validates the integer first. {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE ExplicitForAll #-} {-# LANGUAGE ScopedTypeVariables #-} module NumTH where import Data.Proxy import Numeric.Natural import Language.Haskell.TH.Syntax (Lift) import Language.Haskell.TH.Lib (TExpQ) data IsValid = Valid | Invalid String class ValidatedNum a where validNumInteger :: Proxy a -&gt; Integer -&gt; IsValid instance ValidatedNum Natural where validNumInteger _ x = if x &gt;= 0 then Valid else Invalid "Natural numbers must be greater than or equal to zero" instance ValidatedNum b =&gt; ValidatedNum (a -&gt; b) where validNumInteger _ = validNumInteger (Proxy :: Proxy b) fromIntegerTH :: forall a . (Num a, ValidatedNum a) =&gt; Integer -&gt; TExpQ a fromIntegerTH i = case validNumInteger (Proxy :: Proxy a) i of Valid -&gt; [|| fromInteger i ||] Invalid reason -&gt; fail reason 
Nice fix. Not very easy to package this up though.
Why don't you compile and run it with ‘RTS -s' option?
Out of curiosity, which features would you like to see that ghci doesn't have?
It would help if you mentioned specifically what about ghci you don't find satisfactory. /u/chrisdone is writing a "next generation" repl based on ghci here: https://github.com/chrisdone/ghci-ng
Which twitter office is this? 
Firstly I'm very new to it, so I hope I wasn't putting the horse before the carriage! Some of the things I want may already be in ghci, such as the easy pulling up of documentation and definitions. Of course pry is built around a different programming paradigm, but I quite liked being able to "ls" into objects to see methods defined on them, and it has much cleaner multi-line support than ghci's :{. Including a very nice way to manipulate the input buffer while doing multi-line input. Nice syntax coloring and all that jazz too, which ghci may be able to do, I haven't gotten into it yet. 
People would try harder to find a solution on their own. I can't say it for sure, but I'm pretty sure he don't want to learn it. 
Can't say I didn't expect the downvotes. Perhaps you haven't noticed yet, but a education system that is forcing people to learn something they don't want to learn is a broken education system.
As acow has mentioned, I've got a library that actually aspires to be just what you describe: an in-memory Haskell relational database. I've got natural joins working properly. There are two main obstacles now: persistence and efficiency. Optimizing joins and restrictions is definitely a non-trivial affair, but I'm hoping to have time to give it a go at some point. Glad to hear someone else is thinking about this kind of thing though.
Heh, I watched it at 1.5 and 2x speed so I didn't find it slow at all ;) I find most presenters speak too slowly allowing my mind to wander. I tend to focus better at faster speeds because a lapse in attention caused me to miss something. It's a video, so I can always rewind and play back at a slower speed if something's complex.
This is a great example. I ran across this while working on an image recognition project a few years ago. One of our fundamental types was a collection of objects on a canvas together with their bounding box. A `Monoid` instance for this type was very useful, but to create it we had to bolt on an artificial extra constructor to provide `mempty`. This quickly spewed gobs of ugly, meaningless code all over our application to deal with myriads of cases involving `mempty` that could never actually happen. When we switched to `Semigroup` instead, our code returned to being elegant, clear, and readable. `Semigroup` is awesome!
Yes, but don't do that. It will bloat your code with "special case" handling of meaningless `Nothing`s. If your type is naturally a `Semigroup` and not a `Monoid`, use `Semigroup`.
Have you seen [IHaskell](https://github.com/gibiansky/IHaskell)? Might not be what you want, but from what I understand it's pretty cool anyway.
Thanks, that looks interesting. Though I admit I'm pretty much sold on using [Julia](http://julialang.org) for data processing and numeric stuff. More because of the libraries than the language itself, of course, but I can't help but to notice an amazing amount of good design hidden underneath the dynamic typing and superficial similarity to Matlab. I was really caught off guard when I found (applicative) functors in there: &gt; s = slider(0:.1:1,label="Slider X:") &gt; display(typeof(signal(s)); Reactive.Input{Float64} &gt; xsquared = lift(x -&gt; x*x, signal(s)) &gt; color = lift((x, y, z) -&gt; RGB(x, y, z), r, g, b) where the last command can be abbreviated with a macro: color = @lift RGB(r, g, b) 
Read about integration with hoogle. It lets you search for functions by type in a way that I think is much more powerful than listing methods because of it's ability to generalize your query.
Map and set union give you monoids; the zero is the empty map/set. Map and set intersection gives you semigroups, but monoids only for finite key types; the zero would be the set with all elements of the type, the map with every element as a key and zeroes for values.
Justification and use-case starts at https://github.com/ghc/ghc/commit/46a03fbec6a02761db079d1746532565f34c340f#diff-02e7842997523ff8a5ad0312df2edfe2L12435 if anyone was wondering.
This was a very interesting read for me, and perfect in time as well. I just finished a course in algorithms and have just started my first course in functional programming, in haskell. 
As far as I understand it, one of haskell's strongest optimization methods is graph reduction (also used on the reduceron). So if you can translate a complex graph to a simple one you save CPU time by either decreasing the number of reductions you have to do to get to normal form or swapping the types of reductions you have to do for more efficient ones. &gt;Also I don't immediately see how graph isomorphisms are that important to haskell optimization so imho there is no impact. Aren't all optimizations just isomorphisms and reductions? edit: for those who don't know about the reduceron. [The Reduceron: Widening the von Neumann Bottleneck for Graph Reduction using an FPGA](https://www.youtube.com/watch?v=0wT5pobsIaM)
&gt; in comparison with that other commitee designed static typed language ALGOL?
Is adding bang patterns everywhere the same thing as call by value evaluation? It's not clear to me.
If you have hoogle set up you could also do ":!hoogle Type -&gt; a" to get a list of functions that consume a Type. 
San Francisco
It isn't clear to the rest of us either. ;)
Nothing comes to mind: I've been chewing on it since the result was initially announced.
Does that means GHCJS could now yield thunksless JS with -XStrict is given?
In theory this is only possible if all your modules are `-XStrict`. In practice I don't think this will happen in the near future, `base` is still full of laziness. We could probably have a `base-strict` though. (exactly the same as `base`, except compiled with `-XStrict`)
There are two main algorithms we do on graphs in a type theory / lambda calculus like environment. We use a topological sort during type checking to maximize the amount of generalization we can get out of inferred signatures, inferring once per strongly connected component, and we perform a form of graph reduction for evaluation of call-by-need lambda calculus terms. So, we do a form of graph reduction, yes, but Babai's result gives a way to check for the isomorphism of graphs, which is a completely unrelated problem. We don't ever actually do that during evaluation.
ghci has a killer feature over python / ruby / lisp REPLs, which is type checking. Other than that, I have a simple macro to load the module currently edited in vim, and beyond that, never felt the need for anything more fancy. If I want to see definitions, I do :tag Module, fold up the file, and start paging around or use /. Then :L to load the file and experiment with the functions. Tab completion will also list the contents of modules, and :i will get info on class methods, declared instances, etc. For documentation of external packages I have a browser pointed at the haddock. It will also load compiled code quickly, so I can almost instantly jump into some module in the middle of a 500 module project and start running functions or tests. I'm not very sophisticated with python REPLing, but it's way better than anything I ever used in python. It also probably has a lot to do with functional code being a lot "shallower" than imperative style, where just instantiating all the objects you need is likely to be a big deal.
Actually that is a great idea. I just learned about RTS options from @fread2281 's comment. I need to read on them to figure out how to use it though. Thanks for the suggestion.
What about `(,)`
This is not my area of expertise but the [Servant paper](http://alpmestan.com/servant/servant-wgp.pdf) discusses this in more detail. From the abstract, it seems to say that one important feature is open type classes and open type families.
In `Dynamic`, we just use `Typeable`.
No
Since 7.10 `Typeable` is available for all concrete types. You have to explicitly ask for the constraint to use it though. On the other hand, RTTI is available for all classes and you can use it without asking. 'isInstanceOf' is a thing you're permitted to ask for all the time, which makes a bit of a joke of things like the Liskov substitution princople. `Typeable` is a constraint that lets you talk about the type of a given thing and do typecase. That is it. You can compare one concrete type against another concrete type. `Data` and `Generic` have to be generated by you explicitly for each data type you want to support either by hand (mostly in the former case) or by `DeriveDataTypeable` and `DeriveGeneric`. `Data` lets you walk inside of a structure and replace parts you can look for using `Typeable`. This is pretty much entirely a runtime affair. That said, some folks have been playing with ways to move it to compile time, e.g. Michael Adams et al's work in ["Optimizing SYB is Easy!"](http://michaeldadams.org/papers/syb-opt/) moves more of it back into compile time. &gt; Can Haskell type check generics at compile time? Continuing the theme above, `Generic` moves a bit more introspection to compile time. `Generic` stuff pushes through the type system rather than at the value level like `Data`, so you can have things that e.g. can only handle product like structures just fine and get type errors when it fails. I've also written code that runs at compile time using `template-haskell` to introspect on the structure of a data type and generate code appropriately. This is probably the closest to the reflection story from the java ecosystem. I can generate code from meta information about my existing code. However, we lack a generic annotation mechanism we can inspect through TH. So we have a whole continuum of solutions from fully-runtime (`Data`) to fully-compiletime (`template-haskell`). Pick your poison.
You can. This is standard `zip` from `base`: zip [] _bs = [] zip _as [] = [] zip (a:as) (b:bs) = (a,b) : zip as bs With `-XStrict`, it becomes: zip [] !_bs = [] zip !_as [] = [] zip (!a:!as) (!b:!bs) = (a,b) : zip as bs `[1..]` is just a syntactic sugar for `enumFrom 1`. Let's assume we're using `Enum` instance of `Int` here. With `-XStrict` it'd be defined like this: (This is the code from `base` made strict) enumFrom (I# !x) = eftInt x maxInt# where (I# maxInt#) = maxInt eftInt :: Int# -&gt; Int# -&gt; [Int] eftInt !x0 !y | isTrue# (x0 &gt;# y) = [] | otherwise = go x0 where go !x = I# x : if isTrue# (x ==# y) then [] else go (x +# 1#) Note that `(:)` etc. would also become strict. This code just loops without adjustments but it's perfectly valid, strict code. --- I think what most people are missing is that if you add bang annotations everywhere, WHNFs of types become normal form of the same type. Then by adding bangs to binders you make everything strict. Since functions will also have bangs in arguments, functions calls become call-by-value.
Wow, your explanation was lovely. I especially liked how you decomposed the expression (and displayed the steps) in such a way that error become more clear. Could you please recommend a good editor for writing/debugging Haskell? Currently, I'm using Sublime text 3 with the "SublimeHaskell" plugin. Thanks again!
I'm not too advanced, and I know there are strong opinions out there about typeclasses that are currently over my head...but if the goal is to define a guaranteed interface A, and then add a data type that wraps any other data type that implements A and is also implements A itself, doesn't it need to be a typeclass? Even if it didn't implement the Transport typeclass, ZlibTransport would still need to place a Transport typeclass constraint on what it wraps, otherwise you'd have to write a new wrapper for every simple transport, and new transports are written fairly often. It strikes me as a good way to guide implementations, but good chance I'm misunderstanding/overlooking something (and sorry for my OO-esque way of speaking)
Is this just an exercise? Seems like it'd be more useful (and less confusing to define: f n = enumFromTo 1 n Or point-free: f = enumFromTo 1 Or if you prefer terse syntax: f n = [1..n]
I like it despite its opinionated colour/font settings (replaced some with my own preferences). Quick question: how to upgrade it in future? Execute the same one line command again? 
Try `:i`, which is tremendously intuitive once you get familiar with the common type classes. &gt; :i String type String = [Char] -- Defined in `GHC.Base` That means it's a type synonym. If we put in the "real" type, we get more info: &gt; :i [] data [] a = [] | a : [a] -- Defined in `GHC.Types' instance Eq a =&gt; Eq [a] -- Defined in `GHC.Classes' instance Monad [] -- Defined in `GHC.Base' instance Functor [] -- Defined in `GHC.Base' instance Ord a =&gt; Ord [a] -- Defined in `GHC.Classes' instance Read a =&gt; Read [a] -- Defined in `GHC.Read' instance Show a =&gt; Show [a] -- Defined in `GHC.Show' instance Applicative [] -- Defined in `GHC.Base' instance Foldable [] -- Defined in `Data.Foldable' instance Traversable [] -- Defined in `Data.Traversable' instance Monoid [a] -- Defined in `GHC.Base' and &gt; :i Char data Char = GHC.Types.C# GHC.Prim.Char# -- Defined in `GHC.Types' instance Bounded Char -- Defined in `GHC.Enum' instance Enum Char -- Defined in `GHC.Enum' instance Eq Char -- Defined in `GHC.Classes' instance Ord Char -- Defined in `GHC.Classes' instance Read Char -- Defined in `GHC.Read' instance Show Char -- Defined in `GHC.Show' Which (indirectly) tells you a whole lot about what functions are available. You can look up each typeclass, but you'll quickly learn to recognize them and have the functions memorized. It's easy because it's always the same functions, for most types--e.g. `mappend` from `Monoid` shows up everywhere, so you get used to it quickly. Also super helpful is `:t`, which lets you find the type of any expression. When I see some code that I don't understand, I'll often get the types of various chunks until I understand how it fits together. &gt; :t fmap (+1) fmap (+1) :: (Functor f, Num b) =&gt; f b -&gt; f b 
Surprise. Hip thing (reveal.js) is what actually powers slides.com :) https://github.com/hakimel/reveal.js/#online-editor
I see. The [1..n] part got me confused, The filter is redundant here. Thanks.
I've never used Thrift, so maybe a typeclass really is needed to reduce boilerplate, but at the very least there should be a record *as well*. It would be data Transport { tIsOpen :: IO Bool tClose :: IO () tRead :: Int -&gt; IO LBS.ByteString tPeek :: IO (Maybe Word8) tWrite :: LBS.ByteString -&gt; IO () tFlush :: IO () tReadAll :: IO LBS.ByteString } If passing this around manually really gets too much than also add instance HasTransport a where transport :: a -&gt; Transport 
Well, the "ls into objects" thing wouldn't be a fit for Haskell, simply because there are no objects. The next best thing, listing all the functions defined in a module, already exists (`:browse`). GHCI also has `:info` and `:type`, and it can integrate with a locally-installed hoogle search. There may also be integration points for haddock and such, but I haven't investigated that path yet, simply because my workflow isn't very REPL-centric. Better input manipulation; agree, that would be a nice thing to have. Syntax highlighting; I don't personally miss it a lot, in fact, I think that sometimes it distracts more than it helps, but I can certainly see the appeal.
Thanks. I'll look into that paper.
The questions have moved from being related to random haskell problems found via Google to question from the CIS194 course. Given the form of the questions and replies on reddit and the fact that lots of the Haskell material that gets recommended is in English, it could just be a language fluency problem. Does anyone know of a German translation of Learn You A Haskell For Great Good or the CIS194 material? 
This looks great. Thanks for taking over this important task, Gershom!
Let me rephrase my comment...it is valid code, it just does something unintended, namely it loops forever to evaluate the infinite list.
Won't take 5 [1..] just loop around forever?
Chalmers University of Technology in Gothenburg, Sweden 
What version will support this?
8.0
In what way?
For good laughs look up the "robot monkey operator"
It's a bit incomplete, if you ask me. Stackage's indexes more packages.
Oh indeed, fixed that comment
By the way, in case my comments were too vague, I'm not trying to blame you for anything at all. Any mistake (and reasonable people disagree on the matter) is purely on the part of the Thrift devs.
He asked for test cases of packages binding to msys libraries, not general C bindings. `text-icu` is a particularly difficult case because it binds to a large third-party C library that's distributed in at least two different ways (native binaries from the ICU people; msys2 system package) with different results (arbitrary location with no pkgconf configuration; system location with pkgconf files). Working out the include and linking paths is intractable: the Haskell package doesn't use cabal's pkgconf support because it's frequently not available, and who knows where people install the darn thing. All that said, It'd be *great* if somebody (or some bodies) work out how to get `text-icu` building reliably on Windows. AFAIK, it's one of the last remaining major blockers for Windows compatibility, setting aside remaining GHC issues (e.g. dynamic linking).
Yes, I know Haskell doesn't have objects, it was more the ... hmm... "spirit" of that tool that I enjoyed. Easily taking on contexts/environments, though I guess that's essentially state and is also absent in Haskell. `:browse`, `:info`, `:type` are indeed very nice and basically handle that sort of thing. 
I know there is a lot of overlap between Haskell and NixOS users, so I thought many people in this subreddit would be interested in this. If you are a Haskell user and haven't checked out Nix or NixOS, I highly recommend you take a look! Even if you don't use it as your day-to-day OS, NixOS can change the way you think about dependencies and build systems. Also, there is special support for Haskell packages in the Nix package ecosystem (/u/ocharles talks a lot about how he develops Haskell w/ Nix. He also did a talk in this conference!)
Just want to second this. You should check it out if you haven't. It's referential transparency applied to the entire OS. And surprisingly it works!
&gt; EDIT: Well I'm unjust above. it has the right level for defining APIs, but not for web applications. Exactly! For me, the excitement is that it provides a way to create an type-checked interface that two or more independent processes will use to communicate without imposing a lot of restrictions on how those processes are implemented. For web 1.0 style websites, it is not that exciting. But when you are trying to write a server, a separate client-side html/ghcjs UI, mobile applications, etc, it is awesome to be able to have this bit of shared code that ensures everyone is talking the same language. Especially because it allows all the teams to get started at the same time and work independently. Best of all, it does not impose a lot of constraints on how those things are implemented. If a servant API is provided, and I have to implement the client and server portions, I still have tons of flexibility. I can use warp, snap, or happstack on the server-side. I can use blaze-html, HSX, lucid, etc in the ghcjs side of things. I can use whatever database I want, etc. `servant` itself is very much tied to HTTP. But I think the ideas have potential to be applied elsewhere. 
Thanks! I assumed it was a requirement. Good to know.
Ya, you could have worked with the infinite list [1...] instead (i'm pretty sure that's the syntax). 
What do you mean by packaging it up and why isn't it easy?
I mean taking this idea in its raw form and getting it into a form that would be acceptable for GHC as a concrete proposal, and getting other folks to accept that it is a worthwhile goal is a long long road. ["And miles to go before I sleep."](http://www.poetryfoundation.org/poem/171621)
Contact Johan Jeuring (J.T.Jeuring@uu.nl), he'll be happy to help out.
Well, directed graphs are a generalization of categories, aren't they? Since this one does happen to have identity morphisms in the form "go from point A to point A in zero time" you might as well go one further and call it a category.
Thanks! I appreciate the help. I guess I'm used to Ruby's meta programming where you're just reading text and not an AST.
I think you mean transfinite cardinals, if you're counting.
Can anyone give me some quick advantages of using Nix rather than using stack, or even using them together (I saw something about nix support in stack's release notes I believe). What does the workflow look like? What does the workflow look like when you don't have a package that is supported? Are environments still the way to do things in nix? I always found them fairly confusing. I got a little bogged down having to write nix expressions so much, and even got some build errors until I started using [cabbage](https://github.com/acowley/cabbage)
Does this mean with `-XStrict`, bangs will force laziness? If not, what is the lazy annotation?
Oh, no problem, didn't take it that way at all. I'm coming to understand that a lot of Thrift's problems stem from the need to cater to the lowest common denominator language (and when it comes to Haskell, probably compounded by lack of manpower). Anyways, it looks like my ZlibTransport is working, and after some heavier testing I'll probably try getting it merged into the main Thrift repo. Thanks for the help!
The lazy annotation is to put a ~ before the variable.
I feel as though _browsers_ are enforcing the adoption of the key/value system now, and it would be wise for us to adopt it as well. The `urlDecode` function from http-types seems to be the best bet, from what I've seen, as a low-level percent un-escaping function. It also lets us flag whether or not we should un-escape `+` as a space, too, which makes it a complete tool for decoding. My solution so far has been to use `urlDecode`, `Data.ByteString.UTF8.toString` from utf8-string, and `splitOn` from the split package. I also feel as though `Maybe String` is more explicit in terms of possibly existent values than matching an empty string. To create our list of pairs with potential elements, it's as simple as: import Data.ByteString as BS import Data.ByteString.UTF8 as BS import Data.List.Split import Network.HTTP.Types (urlDecode) -- | Turns '+' to ' ' as an example decodeUrl :: ByteString -&gt; [(String, Maybe String)] decodeUrl xs = let string = BS.toString (urlDecode True xs) getVal c = case splitOn "=" c of [k] -&gt; (k, Nothing) [k,v] -&gt; (k, Just v) _ -&gt; error "more than one value bound to a key" in map getVal (splitOn "&amp;" string) This handles unicode symbols, too, which is awesome :)
Not that compelling so far. All I care about is good tooling and support. What's the Haskell ecosystem like on Nix?
I think it's quite good, maybe have a read of the Haskell support in our manual: https://nixos.org/nixpkgs/manual/#users-guide-to-the-haskell-infrastructure
You might want to start by having a read through the documentation in the nixpkgs manual, which outlines a few "flows" as to how you can use the Haskell support that Nix comes with: https://nixos.org/nixpkgs/manual/#users-guide-to-the-haskell-infrastructure
Yeah, right. Thanks.
To me, that was his tone of voice, what made it condescending. Just the speaking manner. It's good to be proud of your accomplishments but one should speak about it without sounding patronizing. In contrast, watch any of the Simon Marlow's talks-there's none of that.
Your code is wrong though, and defeats the whole purpose of percent-encoding, as it decodes `"key1=val1&amp;key%3D&amp;key%263=val+3"` to `[("key1",Just "val1"),("key",Just ""),("key",Nothing),("3",Just "val 3")]`. What you are looking for is [Network.HTTP.Types.parseQuery](http://hackage.haskell.org/package/http-types-0.9/docs/Network-HTTP-Types.html#v:parseQuery): import Control.Arrow ((***)) -- why this isn't in Prelude already!? import Data.ByteString import Data.ByteString.UTF8 as B8 import Network.HTTP.Types decodeQuery :: ByteString -&gt; [(String, Maybe String)] decodeQuery = fmap (decodeToStr *** fmap decodeToStr) . parseQuery where decodeToStr = B8.toString . urlDecode True -- &gt;&gt;&gt; decodeQuery $ fromString "key1=val1&amp;key%3D&amp;key%263=val+3" -- [("key1",Just "val1"),("key=",Nothing),("key&amp;3",Just "val 3")]
How does one get an internship or job here?
By the standard `;` is a "reserved character" and thus compliant implementations have to percent-escape it. YOLO!
&gt; How would you check for this? It's not possible to write a function `isFinite :: [a] -&gt; Bool`, as you might already be aware. Using `length` is also a non-starter, as it will hang on infinite lists. If you give a maximum length above which is considered too long, however, then yes, you can: bailoutReverse :: Int -&gt; [a] -&gt; Maybe [a] bailoutReverse _ [] = Just [] bailoutReverse 0 _ = Nothing bailoutReverse max (x:xs) = fmap (++ [x]) (bailoutReverse (max - 1) xs) &gt; Is there a name for this kind of bug I would call it an infinite loop. &gt; Where does it actually hang? Consider a definition of `reverse`. This definition is not exactly the same as the one in `base` but it works for now: reverse [] = [] reverse (x:xs) = reverse xs ++ [x] If we give `reverse` an infinite argument, then it will match the second case (`reverse (x:xs)`). So then we start by attempting to reverse the tail of this infinite list (`reverse xs`). But the tail of an infinite list is itself infinite, so the same thing happens again, and again, and so on. Hence, infinite loop.
The `[0..]` syntax is sugar for `enumFrom 0`, which is equivalent to `iterate succ 0`. Iterate is defined like: iterate :: (a -&gt; a) -&gt; a -&gt; [a] iterate f x = x : iterate f (f x) Haskell's laziness means that the list is generated lazily. `reverse` is defined in `base` as: reverse l = rev l [] where rev [] a = a rev (x:xs) a = rev xs (x:a) So when you pass in `reverse [0..]`, it does: -- substituting the `iterate` definition for `l`: reverse (x : iterate succ (succ x)) = rev (x : iterate succ (succ x)) [] -- pattern match on the first element rev (x : xs@(iterate succ (succ x)) [] = rev xs (x : []) When we go to pattern match on the next element of the list, we cause the `iterate` function to evaluate one more element: rev (succ x : xs@(iterate succ (succ (succ x)))) [x] = rev xs (succ x : [x]) And again... rev (succ (succ x) : xs@(iterate succ (succ (succ (succ x))))) [succ x, x] = rev xs (succ (succ x) : [succ x, x]) Etc... Laziness is really cool, in that you *can* generate these infinite lists and use them almost normally, but it can bite you sometimes. Infinite looping errors, generally, can't be checked for (halting problem). GHC can probably check to see how a list was defined, and if it isn't finite, complain if you use `foldl` or other functions that demand finite lists.
Wait a minute - that have a bit of practical implications. Now compiler can apply more optimization patterns, in a first because it is cheap, second - now it safety, because we can approve isomorphisms of graphs. For example problem - not all recursion are optimized by memory. Why? Maybe because graph have not usual form and it is too expensive looking for way to optimize it.
I'm not sure I see use on this. You could just write a database of lambda terms with their reductions cached anyway. The set of possible graphs is prohibitively huge. Just imagine the amount of different terms a tuple of 4 ints can have. You'd need to store a graph for each one of those billions of terms for a `(Int,Int,Int,Int) → Int` function, for example...
*sigh* Kinda makes one appreciate and miss Jon Harrop's trolls.
[removed]
y'all i found the poster child for #fragilemasculinity
So in case you are wondering why [0..] prints out but reverse [0..] doesn't, it is tied to weak-head-normal-form: `[0..] = 0:1:2:....` where : is a data constructor, which the next function can match and consume the data. `reverse [0..] = reverse [n..] ++ ([n-1]++([n-2]++...))` where the next function can never match on the list data constructor because it always have to evaluate the next iteration of reverse. Notice that the next function has to match on the first data constructor or it can't extract a value to print. Since it doesn't exist we search for the impossible, thus a loop.
My god you should make your flame bait less obvious. 
Am I the only one to appreciate the irony that this guy is using Twitter (that relies on Scala in their backend) to troll FP...?
What you should do is look inside each of the most common monad transformers, learn them, understand them, and understand how they compose together. Then, once you've done that, you can write your own monads with the same functionality, without the overhead of the transformers.
That's because the actual definition of Typeable is in [Data.Typeable.Internal](http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-Typeable-Internal.html#t:Typeable), and the actual parameter it takes is a [Proxy#](http://hackage.haskell.org/package/ghc-prim-0.4.0.0/docs/GHC-Prim.html#t:Proxy-35-) a, which is an unboxed, 0-byte value. If you try to stuff a normal-sized value into that, it'll crash horribly.
Checking if a list is infinite would require solving the halting problem, so it's not possible in Haskell or any other language. As a proof, suppose we write a Turing machine simulator `simulateTM :: TuringMachine -&gt; Input -&gt; [TMStep]` where `TuringMachine` is some description of a TM, `Input` is some description of TM input, and `TMStep` is a list of the steps that the TM takes until halting (i.e. move left, move right, write value to tape, change state, etc). Now suppose we have some value `m` that describes a TM and `i` that describes input to it. Now computing: isFinite $ simulateTM m i is equivalent to solving the halting problem.
They also hired Gabriel Gonzalez! 
Not really more inference, but more information. If you don't specify sufficient information on the type-level about what types your handlers should have, then there's no way to magically recover this, because it is simply not available statically. Then you can only work with dynamic checking. But I would not go as far as calling the `f'` approach an anti-pattern. Using type-level features of Haskell extensively typically has a price. You're ending up using lots of language extensions which may change over time, and you are possibly making your code less accessible. Also, there are situations where you simply have to do extra work to prove to the compiler that you're always "correct" in your assumptions. So it is a tradeoff, and should be judged on a per-situation basis.
Infinite loop isn't necessarily error if you use it to enumerate sequence of unknown length - eg. number of user inputs. Only solution is to make sure that there is way out of the loop.
Isn't there a FiniteSequence type class or similar? I would expect that those algorithms that do require finite sequences to only accept finite sequences as parameters. Sometimes one doesn't know, so MaybeInfinite would also be needed. In this case two functions that returns a Finite/Inifinite Sequence from a MaybeInfinite Sequence would be required for the user to specify what is going on.
I don't get why this argument about the halting problem is used as an excuse that analysing a program's behavior is impossible. The way I understand it, the halting problem means that you can't figure out if a program will halt or not, for all programs. It doesn't prevent trying. After all, if we humans can figure out in most cases that a program will stop or not, there is no reason that a program couldn't do the same, isn't it ? 
If a particular question is reducible to the halting problem, it means that it some cases it will take an infinite amount of time to find an answer. If you have an algorithm running that is trying to figure out if a function halts, you can never tell if the algorithm is just taking a long time, or if it will never stop. It's not an excuse: saying that a question is undecidable means that **it is impossible to give an answer in every case**. You can have heuristics that can try to make a guess in some particular cases, but the general case **cannot be solved by any means in a finite amount of time**. It's even worse than e.g. trying to break strong encryption: most modern encryption systems can be broken in finite time --- but maybe not until after the heat death of the universe. An undecidable problem is so hard that no amount of time and resources can ever solve it for an arbitrary input.
Thanks for the clarification. Would a program (that checks if another program halts) having 3 outcomes : - Yes it will halt - No it won't - I can't figure out. Be usefull at all ? 
It won't really help much to have laziness in just a few places since the values will likely already have been evaluated fully by all the strict code around them.
Yes, that's very often useful. For an example, check out the totality checker in Idris. Idris requires that functions used in type signatures are total (finite and well defined on all inputs) in order to typecheck.
Not in general, no. terminates :: Computation -&gt; Maybe Bool terminates = const Nothing Nonetheless, this is more or less what total-by-default languages do. But you can't slap a totality checker on top a language designed without such in mind. For starters, it will reject most of the standard library. And totality checking is not exactly a silver bullet. It's easy to construct a program that provably terminates, but certainly not in your lifetime. See /u/capitalsigma's example.
No, it's not possible in general. Consider just as an example myreverse n = reverse (collatz n) where collatz :: Integer -&gt; [Integer] collatz n = n : collatzNext n collatzNext :: Integer -&gt; [Integer] collatzNext 1 = [] collatzNext n | even n = collatz (n `div` 2) collatzNext n = collatz (3 * n + 1) Will `myreverse` terminate for all `n`? We currently don't know (https://en.wikipedia.org/wiki/Collatz_conjecture) ... But you expect a compiler to be able to check?
Because termination does not depend on any input. Both `[1..]` and `while(1);` have no inputs yet they will not terminate. But if you have lazy evaluation it will not even start infinite loop as long as you don't try to evaluate whole expression.
It certainly won't terminate for `(-1)`. :-) By the way, why don't we have nats in the standard library?
I mean, yes and no. The undecidability of the halting problem is a hard upper bound on how well program analysis can do, in the same way that the speed of light is a hard upper bound on what engines can do. It happens that it's much easier to hit that upper bound in computer science than in physics. GHC does detect infinite loops sometimes, but it's limited. Your favorite Java IDE probably warns you when you write a `while` loop that doesn't look like it will terminate. I don't know much about how well you can do in terms of heuristics. Checking for termination is harder than `P`, `NP`, `EXPTIME`, `EXPSPACE` and all of your favorite complexity classes. If you could solve everything in `NP` in constant time it wouldn't get you any closer to deciding the halting problem.
Let me get this; with `-XStrict` on, also infinite streams become strict, so in this case GHC first tries to fully evaluate `[1..]` before taking 5 elements? So, no code with streams if the strictness extension is turned on?
Seriously, what happened? Who hurt you? If you don't like to program functionally, don't. Nobody is forcing you. But please stop acting like an idiot around here.
No, [it's not possible in general](https://en.wikipedia.org/wiki/Halting_problem). Some specific cases can be detected, and GHC(I) actually does that (this is what happens when you see the `&lt;&lt;&lt;loop&gt;&gt;&gt;` error), but there is no general solution to the Halting Problem.
Only 3 when I looked just now; he's on a downward trend!
From that so question: &gt; isInfinite x = length x \`seq\` False simple :P
I was thinking of JAVA...
Why was Traversable "left behind"? Also, could you give an example? I've tried a few of the examples in this paper and, while on one hand the results are very pretty and informative, my only gripe is that many don't work without explicit type annotations..
If you manage to compile everything with -XStrict it's quite possible you'll get call-by-value semantically (I'd have to look at the details to be absolutely sure). But operationally you'll not get it. GHC will not be aware that every single function is strict so when you have a function application `f e` it will still build a thunk for `e` (which `f` will force at once). If `f` is a known function the thunk construction might be avoided, but in the higher order case `f` is not known, and GHC will construct the thunk just as it always has. To get the full advantage of call-by-value you must teach the compiler that everything is strict and that it should not construct thunks.
As someone that is half beginner, but uses Haskell everyday for my thesis, this makes me feel two things: - I'm really happy, as in the beginning I really wanted everything to be strict, as laziness confused me a lot. - I feel also that making things selectively strict is easier than making things selectively lazy. With time I learned how much laziness helped me have programs that work in constant memory if I manage to pipe all my process in the appropriate. And I learned that because of default laziness, that made me fight through my problems. I hope beginners do not overuse it, laziness is complicated but it's beautiful too.
The slides are linked in the video comments
&gt; but in the higher order case f is not known, and GHC will construct the thunk &gt; just as it always has No it won't. Here's a higher-order function: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map _ [] = [] map f (x : xs) = f x : map f xs With `-XStrict` it becomes: map :: (a -&gt; b) -&gt; [a] -&gt; [b] map !_ [] = [] map !f (!x : !xs) = f x : map f xs Note that `(:)` also becomes strict in it's first and second arguments. I don't see any thunks consutructed here. `f x` is evaluated before the cons cell is constructed, becuase `(:)` is now strict in both arguments. Again, do you have a counterexample? 
Haskell has [mutable hashtables](https://hackage.haskell.org/package/hashtables) too. But yeah, haskell performance can be unpredictable. I'm surprised that Text is slow though.
[removed]
Since we don't care about totality, what's the problem?.. And after AMP I don't see extracting a superclass as being a huge problem. Surely Monad instances are more common than Num instances in Haskell code!
[removed]
Can you provide an example of a startup where functional programmers invaded the workplace and bankrupted the company?
Had I known that ocharles gave a talk in Germany, I might have come. But I only heard about NixCon yesterday :/
Thanks. Interesting read. I didn't know GHC could emit SIMD instructions now! I have done some work with regard to stream fusion for union of two sorted sequences [here](https://github.com/hunt-framework/hunt/blob/cx3/hunt-searchengine/src/Data/IntSet/Packed.hs#L288). As I understand from the paper it is not possible to exploit two representations of the bundle without performance loss? The U2 and U3 cases do some elementwise copying which could benefit from the chunked representation I guess.
apparently I missed the joke :-P
Thanks!
Graph isomorphism is one of those things that was already nearly this "cheap" in practice for most practical cases. All this result did is say that the hairy cases can be handled too, by precisely classifying the hairy cases and showing that they are the only hard things you have to deal with. But regardless of it being "cheap" enough to be closer to the P end of the P vs NP spectrum, it is still outside of what we know of in P proper. The time bound here is still annoyingly large. And graph isomorphism isn't subgraph isomorphism, so you don't immediately get an ability to spot the sub-graphs that fall into your pathological shapes without walking everything and raising the price even further.
Just downvote and hide and move on. 
Here's a prototype for what I mean: https://www.reddit.com/r/haskell/comments/3sfngj/haskellnative_spreadsheets/cwxitjp You need an editor for writing the function code, inline, but nothing stops (or should stop) a spreadsheet from importing other modules (written in a full IDE environment). I think spreadsheets cater to simple/experimental use and having negligible learning time.
It's a public forum; besides, it's all very amusing.
I like how you assume that literally no one here has any knowledge about "threads" in "real languages". After all, realizing that it's false would seriously harm your imagined argument of you being clearly right and everyone using FP being clearly dumb.
&gt;&gt;Yes, I know this has been done before. :-/ &gt;Not to my knowledge https://github.com/chrisdone/hell
I think what irks me most about these posts is the impassioned defense for the current state of the industry, aka the status quo. I especially love the part where he decries the fact that FP devs ask for more money; he's literally arguing for de-skilling and commoditization.
Funny, I literally answered a question on Stack Overflow today saying [that GHC doesn't really support impredicativity](http://stackoverflow.com/questions/33734129/vector-containing-gadt/33734604#33734604).
It's funny, I've taught university courses on "threads" in "real languages", and I was once hired by a big company doing work with "threads" in "real languages", sometimes running on _multiple machines_! And yet I program in Haskell in my free time. Something doesn't add up in his logic.
I think Julie and I did alright on audio and visuals in our [Stack tutorial](https://www.youtube.com/watch?v=sRonIB8ZStw), but it's pretty hard to naildown AV for a meetup/talk without a fair bit of prep &amp; gear. I keep a mic in my backpack now, but I still don't have a good solution for recording video.
I don't think this is right. Let's say I have this: let !a :: T = ... in &lt;body&gt; Where `T` is a strict type. `a` can't be bottom in `&lt;body&gt;`.
Or gamma rays could be the cause of his state of mind.
Awesome, thanks. Can you give me a concrete example of a space leak due to lazy data structures?
This is right, but only if we use `-XStrict` everywhere, including `base`. I think I explained this in one of my previous posts, but `[1..]` is just a syntactic sugar. If you desugar it and look at the expression, it involves some recursive functions, applications of `(:)` etc. all of these need to be `-XStrict`. If, for example, `(:)` is not strict(which is defined in `base` so it's not strict at the moment), then you still get your infinite lists(streams). In practice we would probably want to have both lazy and strict versions of lists so that we can still have infinite streams when we need them.
&gt; definition your your datatype i.e. `Tree a = Node !a !(Tree a) !(Tree a) | Leaf !a` can still be a bottom, even though it can't "contain" a bottom (note that I am using a different definition of contain here than jonsterling).
Take an RB tree and apply a rebalancing operation. Most rebalancing operations don't walk the tree after transformation, only before transformation. That means that as you are doing your rebalance you are creating thunks which reference the original version that won't be evaluated until you need that part of the tree.
The denotational semantics for lifted types (like a fully-banged `Tree`) contains bottom in addition to the values of the types. That's just how the semantics work! Now, there's another proposed extension to allow the definition of unlifted types, I believe, but that's a separate matter. (It seems a bit complicated sadly, but in the broad strokes I think it is really promising! Whereas in ML, you could simulate Haskell-style laziness from day one, this should allow the simulation of aspects of ML-style programming in Haskell, where the intended induction principles hold. That is, you'll finally be able to define the natural numbers, the lists, the trees, and other inductive types in Haskell, which has been impossible so far.) Now, what happens in a banged let statement is totally irrelevant and orthogonal from the question of what are the actual members of a type. The meaning of program typing is defined in the denotational semantics.
On that note, I have found it an effective learning technique to study existing Haskell code, then put it away and write my own code that does the task, trying as hard as possible not to look at the original. I'd very much recommend octatoan have a look at hell before starting; I don't think it's "cheating", I think it's an effective use of time. Plus if there's something you don't understand, you can ask specific questions about a concrete code base, which is often better than generic questions.
I can't comment on the denotational semantics part(where can I learn about Haskell types' denotational semantics?), but what I'm saying that in this program `a` can't be bottom, which is what's important here I think. `-XStrict` removes lots of bottoms here and gives us a program that works like it would work in a call-by-value language.
I completely agree with that. Types don't guarantee that(unless it's a hash-kinded type like `Int#`). What I was trying to say is `-XStrict` every let binding becomes strict too, and as a result programs evaluate like they would in a call-by-value language. --- EDIT: So in other words, the type contains one bottom, but with bang pattern on let-binding we have a bottom-less `Tree`. --- EDIT 2: In other words, we can still have `undefined :: Tree` :)
Using this extension will give you the ability to simulate call-by-value in the broad strokes, but this is orthogonal to the question of what your datatypes actually contain, and what induction principles are valid for them. This extension does not address the fact that mathematical induction is false for any type of "natural numbers" defined in Haskell, for instance (or likewise, any type of lists or trees, etc.).
I think that lots of people are using caution because `-XStrict` doesn't cause your dependencies to be strict so you need to still keep laziness in mind.
That would be nice to have. I hope GHC gets unlifted types at some point.
Me too! :) It would be lovely, even if it would decrease the number of cheap shots I can take.
[removed]
Coming from someone that is not so experienced. I came to create the image that my programs have two parts, one that deals with the flow of information and another that treats this information. I like the flow of information to be lazy, on demand. And I like the treatment to be strict, that is, when it is asked for, it is completely done. But I work with numerical analysis of data of a very specific kind, so this may be a very biased point of view.
Could you provide some concrete feedback on how to improve [the conduit tutorial](https://github.com/snoyberg/conduit#readme)? Hearing "it's scary" doesn't provide information on how to improve. (I'm sure Gabriel would be interested in the same for pipes.)
[removed]
Well, I'm glad we agree. :) I said I could believe that -XStrict gives call-by-value semantically, but it certainly doesn't operationally. And that's what you said too. (But your description of what happens is slightly wrong. The `foo` function does not return, instead it tail calls `g`, but that's a minor point.) To be totally convinced that -XStrict really gives call-by-value semantics I'd have to look both at the compiler and the runtime system. The compiler program transformations might sneak in some laziness somewhere, as well as the runtime. The fact that we can get temporary thunks is bad for performance. And if the motivation for -XStrict is performance, then this means we're not getting as much of an improvement as we could. 
I have what may be a beginner's question. How does neural network learning work if you don't know whether a specific choice is good or bad? I'm trying to train a neural network to play [2048](https://gabrielecirulli.github.io/2048/), but (a) I don't know which move would be optimal (if I did, I wouldn't be training a network to play it), (b) I don't want to punish or reward the wrong thing, and (c) I will have no idea which moves contributed to the final defeat. Do you have any advice?
To be constructive, this post shed the light on something very important, .i.e the ability of new comers to learn Haskell and to see its advantages in real world, the "why". This kind of reaction is the consequence of the failure to learn the language. Concerning the community, except for beginners staff, you see often "I let you find by yourself the answer as an exercise !" . The community should know that for a regular programmer "in production mode" he hasn't time to do exercises, when he asks for help he should find a solution and move on. He will acquire knowledge from these "working" pieces of code as much as he receives. For instance, check other communities c++, c# and java the amount of complete answers (not exercises) is huge, which makes the programmer comfortable of choosing one of these languages. Second thing, it's rare to find a complete application, 1) because real world application are rare and 2) I'm sorry the community don't give away such code if any (I'm not talking about libraries). 3) A complete IDE 4) GUI applications is just a dream: time consuming, in windows machine don't even think.... This makes the process of learning Haskell hard, tedious vs benefits. The sad truth if the community do not react: more video tutorials explaining in details, more books (currently few 50% outdated and 0% intermediary and advanced), etc. the language will not survive in the upcoming years. 
Haha, yeah, that too. "I founded a startup, but it didn't succeed. That's odd."
I would say that you want it roughly as often as you would use the equivalent in other languages (e.g. an `int` instead of an `Integer` in Java, an `int` instead of an `int*` in C). A pointer to a heap-allocated small value is such an inefficient representation that even languages which wanted "everything to be an object" made concessions to have integers (and doubles, etc) that are not, just out of performance concerns. I talked a bit about this during my Haskell eXchange 2015 talk: http://blog.johantibell.com/2015/10/video-of-my-haskell-exchange-2015-talk.html
Probably true. Most core libraries do and some people at companies I've talked to have switched to make every field strict by default (and even use -funbox-strict-fields).
&gt; Checking if a list is infinite would require solving the halting problem, so it's not possible in Haskell or any other language. This is only true for a pretty narrow definition of either "list" or "language". It is not true for, e.g., strictly constructed lists or total languages. It is certainly true of Haskell lists per se.
[Here](https://www.reddit.com/r/haskell/comments/3rnbqw/an_apology_to_the_functional_programming_community/cwr46ka)'s my retelling of his first post, reading between the lines about what happened, and rephrasing it to avoid the worst of the abusive language and hyperbole.
So someone who has everything working with OverloadedString has to change everything to support this new version? Who is to say there isn't a way to avoid the duality? If there is a way to avoid the duality GHC would be closing that door or eventually making this extension defunct. Not saying these are problems, just noting the kinds of things you need to think about before adding an extension to GHC. That is what he is talking about when he says it isn't easy to package up.
See my other response clarifying this statement. 
Well, as in any such situation, you may need to write your own prelude/(non)standard basis library. I'm only talking about what is *actually possible* given the language's semantics (and contrasting this with what is currently *not possible*, which is to simulate ML in Haskell); it's gnarly enough to actually do it that you probably wouldn't want to (in fact, you pretty much never need to!). It could be patched up with language extensions (there have been some in the past IIRC), but it's not so urgently needed that anyone has bothered to come up with a good user interface for the translation. 
Your troll game has been getting soft. This would force you to step it up :-p.
Standard ML [doesn't really have much of a standard library anyway](https://www.quora.com/What-useful-features-of-Standard-ML-dont-have-an-equivalent-in-Python/answer/Robert-Smith-9/comment/3683847), so, if you want to write portable code, you'll be implementing lots of basic convenience stuff, strict of lazy. Using laziness in SML isn't any more impractical than using any other abstractions that the SML Basis Library doesn't provide. An abstract type of lazy suspensions is easy to implement in SML, requiring just 20 lines of code: signature LAZY = sig type 'a lazy (* the type language is RPN-ish *) val pure : 'a -&gt; 'a lazy val delay : ('a -&gt; 'b) -&gt; ('a -&gt; 'b lazy) val force : 'a lazy -&gt; 'a end structure Lazy :&gt; LAZY = struct datatype 'a cell = P of 'a | D of unit -&gt; 'a type 'a lazy = 'a cell ref val pure x = ref (P x) fun delay f x = ref (D (fn _ =&gt; f x)) fun force (ref (P x)) = x | force (l as ref (D f)) = let val x = f () in l := P x ; x end end Note that `Lazy.delay` only works on functions of one argument - you can `Lazy.delay f (x,y)`, but you can't `Lazy.delay f x y`. In this situation, SML's convention of not currying first-order functions turns out to be very convenient. `Lazy.lazy` is a `Monad`, and almost a `MonadFix`, so, if desired, the following functions can be added to the signature: val map : ('a -&gt; 'b) -&gt; ('a lazy -&gt; 'b lazy) val ap : ('a -&gt; 'b) lazy -&gt; ('a lazy -&gt; 'b lazy) val bind : ('a -&gt; 'b lazy) -&gt; ('a lazy -&gt; 'b lazy) val fix : ('a lazy -&gt; 'a) -&gt; 'a (* `fix f` diverges if `f` forces its argument *) With implementations: fun map f = delay (f o force) (* `o` : function composition *) fun ap f = map (fn x =&gt; force f x) (* can't be eta-reduced, why? *) fun bind f = map (force o f) fun fix f = f (delay fix f) This should be straightforward to use for an MLer, but as I remarked above, you must remember to use `Lazy.delay` and `Lazy.force` in the right places, because a value of type `foo` isn't the same thing as `foo Lazy.lazy`.
No, I don't want to write a bash clone. I'd like to make a kind of toy shell that allows one to use Haskell-like idioms at the command line, if that makes any sense.
&gt; No; in order to compute the length, you must traverse the whole list. But you don't need to know the exact length - only whether it's within some bound... noLongerThan :: Integer -&gt; [x] -&gt; Bool noLongerThan n [] = (n &gt;= 0) noLongerThan n (x:xs) = if n &gt; 0 then (noLongerThan (n-1) xs) else false I'm just pedanting a detail, of course - not claiming this is a good idea. 
There's nothing right about your code, to be honest. 
Indeed. In cases like this it is good to have explicit type signatures. Something like: anyBigNumbers :: Int -&gt; [Int] -&gt; Bool Then the compiler is able to give you better feedback on what about your expectations is wrong.
What do you expect your code to do? Why do you expect it to do that?
&gt; anyBigNumber 100 [ 4,5, 600 ] true Judging from this example looks like it should check if there exist a number in the list that is bigger than the threshold number (100) 
stop taking that course, start taking this one instead https://www.cis.upenn.edu/~cis194/spring13/ The reason I give this advice is because your question shows that you lack some fundamental knowledge of Haskell syntax and types, which you really ought to know before doing a homework like the one you posted. The link I gave will prepare you more adequately.
I know what the result should be, I want to know why the OP thinks their code should work. They're more Socratic questions than anything.
I thought that if you encoded the length/depth/etc in the type that you could ensure that you had a finite listlike/treelike/etclike thing, since infinite types don't typecheck.
An infinite list is not bottom! You can use your technique to ensure that your data is finite if it is not bottom, but that technique cannot be used to ensure that the data *is not bottom*. (That is, bottom is a *value* of every lifted type as opposed to merely a *program* of that type, whereas in ML, the bottoms get squeezed out as part of the meaning explanation for membership; in the latter setting, divergence is a computational effect rather than a value.) 
&gt; find to me other way The other way is to read any one of the books we have repeatedly linked to you, or any of the resources in the side bar of this subreddit. A lot of us didn't learn Haskell in school (and many here haven't even taken a computer science class, or have a formal education whatsoever). Programming requires a very high tolerance for frustration and an ability to solve problems. It's clear that by the difficulty of the problems you are posting that you haven't put much effort into learning Haskell. Most people who use Haskell are very nice, and would be happy to help you if you demonstrated a solid effort. However, it seems that you don't even get the code in your questions to compile, and you don't read the error messages that surely come up when that fails. You can't expect to learn anything substantial by asking others for help whenever you are slightly stuck. 
I appreciated the title of this post because I am one of those people who find template Haskell a bit intimidating. I also liked the flow and explanations in this tutorial, including all the thought-processes that it took to get from one step to another. One comment, the second example was a bit more dense and I started to lose it a bit. It might be worth breaking it up into two different posts. In addition, I have an extremely minor comment, but if you are interested in writing/editing with a general audience in mind, you may wish to distinguish "it's" from "its". The latter is possessive while the former is a ~~conjunction~~ contraction: Possessive example: "Separate the function from its type signature." ~~Conjunction~~ Contraction example: "It's very windy here today." "It's" can be substituted for "it is". I saw two examples where the possessive was intended but the contraction was used. A minor point for an overall very interesting post.
Oh my goodness. You actually did go and write an article on it! I know what I'll be reading in bed tonight.
It does, and I figured that's what you meant. I'd take a look at [hawk](http://melrief.github.io/HawkPresentation/) in addition to other projects mentioned here. It's not a shell, but I feel it's still relevant.
Thank you for taking the time to write this out!
I often use GHCI itself as a shell. It might sound weird, but if you think about it, you have full access to your Haskell functions and a well thought-out semantics sans the "convenient" "features" that riddle bash. I fail to see the benefit of some arcane, imperative language whose only purpose is to slightly expedite dealing with IO. That comment aside: the size of such a project heavily depends on how feature-rich you want this shell of yours to be. The basic model would only have to feature a fixed set of commands (e.g. move, copy, delete, echo, help, cat) and a REPL in which they are embedded. For this, you'd only need IO. HOF present a more serious challenge, as they complicate parsing. Whereas scanning for the name of a command can be accomplished with a `lookup :: String -&gt; [(String, Command)] -&gt; Maybe Command`, a line of user input that, say, glues a series of commands together needs a serious little parser. Would these HOF look like regular functions, or would they be some special operators (like `|` in bash)? After that, you get into esoterica like input/output redirection (+ the syntax for that), string escaping, control structures (loops), closures. The sky's the limit, really.
If it's in `sub-delims` (which is an RFC 3986 thing iirc, I don't recall seeing that in any of the http specs), it's reserved, end stop.
Thanks, but the problems I had on Windows weren't only related to shared libraries. Namely: * I've got linker errors about some basic mathematical functions like `atanh`, and I had to prune significant amount of code from one of HMatrix's C files to make anything work * Stack won't install HsQML on Windows, because fuck me, apparently. (I've filled an issue) * Figuring out how to use MSYS to build stuff that is accustomed to Linux toolchain is a pain in general.
what are you talking about? Mathematical induction works just fine for data Nat = Z | S !Nat a term of type Nat has exactly the same possible inhabitants as you would get in ML. Any technical distinction is just a distinction in the style of the semantics, not what the languages actually mean, as it would not be observable. You could construct a semantics for haskell where the meaning of types was, in general, unlifted (say, as pre-domains) but type constructors inserted liftings and it would be equivalent to the definition which interprets types as lifted.
Muphry's law: &gt;The latter is possessive while the former is a ~~conjunction~~ contraction
Yep, re-run the installer to upgrade. I guess that should be mentioned more prominently in the readme.
Great post. I don't think TH is especially scary, but (like many more advanced Haskell topics) the official documentation is pretty beginner-hostile.
&gt; Then come all the extensions and optimizations for distributed computation :D challenge accepted? I would be the really wrong candidate to for this exercise. 
Thanks. I think that if you are going to write something like this, it would better to choose some other GUI library than Qt, as at least when it's coupled with QML it's rather rigid and hard to integrate with `transient` well.
I don't see the point of starting with Ruby and textual metaprogramming. I'm pretty sure people used to Lisp-style syntactic macros would also find TH scary. Anyway, I've always considered TH as something akin to a plunger. It solves uninteresting problems in an inelegant way, but it's still good to have one in your bathroom...
By no means an expert, but this guy is: http://colah.github.io/posts/2015-09-NN-Types-FP/ *edit* Earlier conversation here: https://www.reddit.com/r/haskell/comments/3jiqcf/haskells_map_fold_zip_and_their_deepneuralnetwork/
Haha. Beautiful. &gt; ~~Muphry's~~ Murphy's law: &gt; &gt;The latter is possessive while the former is a ~~conjunction~~ contraction It's turtles all the way down!
I don't have any experience with machine learning, but this is Reddit, so I'll comment anyway. Some chess AIs study past games. You could use boardstate and eventual winner as your training parameters. Iterate over the possible moves and their resultant boardstates, then choose the move that leads to the boardstate that has the highest probability of winning. You could run a bunch of random games to generate initial training data.
Instead of destructors we usually use regions. Basically a resource is valid only inside a block of code (region), and it is destructed on exit from the block. There are different implementations of regions. The mentioned `bracket` function (and its variants like `withFile`) is an example. Indeed it is possible for the resource to escape `bracket`. Other possible region implementation can be found in `regions` package (http://hackage.haskell.org/package/regions ) It uses type-level black magic to ensure the resource can't escape the region. It also supports nested regiions, so you can explicitly transfer the resource to outer region. The downside is that you can't easily destroy the resource early. Other downside is a complexity of type-level machinery. Other approach: `resourcet` (see http://hackage.haskell.org/package/resourcet ) It is probably the most popular. Alternative approach: `io-region` package (see http://hackage.haskell.org/package/io-region ) Here region is a first class citizen, so one can pass region to othe function, store in mutable cells, use in data types etc.
Will this finally allow support for pre-initialised CAFs (similar to what C++ supports)?
Thanks! I fixed the typos. I know ghc-mod can expand template haskell for you, but in Vim at least it fails to expand if there's an error, and that's well.. precisely when I need to expand it :P That would definitely be helpful! I'd love to have a nice go-to comprehensive resource for Template Haskell related stuff. I know 
If you want a language with deterministic resource management, functional features and a strong type system, let me recommend Rust instead. 
Look in the [System.Mem.Weak](https://hackage.haskell.org/package/base-4.8.1.0/docs/System-Mem-Weak.html) module. There you'll find a concept called finalizers which will do what you want. Be wary about their limitation in a referentially transparent language though; you might want to only use them with IORefs or MVars, or they might run too early. Check module haddocks for more info.
I still want to read/write files and feel confident doing so, GC or not... Does Haskell still have concrete advantages over Rust? If not, why don't haskellers let go of Haskell, leave it to rest in peace and move on to Rust, Haskell isn't particularly popular and so isn't Rust AFAIK, so why not promote the best safe language there is? I still kinda like Haskell's pure functions and lightweight syntax...
Now that there is a pragma for default strictness I suppose it would make sense to have an annotation to make things lazy when needed. We already have the tilde on the value level and it would be natural to have it on data constructors as well.
Also [managed](https://hackage.haskell.org/package/managed-1.0.1/docs/Control-Monad-Managed.html) would be suitable; it gives you a `newtype Managed a = Managed { (&gt;&gt;-) :: forall r . (a -&gt; IO r) -&gt; IO r }` that takes a `bracket`ed action as input. 
If you have a resource your want scoped there are a variety of mechanisms from bracket (https://wiki.haskell.org/Bracket_pattern) to resourcet (http://www.stackage.org/package/resourcet). There may be others in common use but those are the two I use. I would say I am promoting the best safe language there is. 
That's disingenuous: there are plenty of elegant solutions to this in Haskell: [ResourceT](https://www.fpcomplete.com/user/snoyberg/library-documentation/resourcet) and [Managed](https://hackage.haskell.org/package/managed) to name a few.
Please see my and other answers to this thread.
Thanks for flagging this up. I've now put together a [patch](https://phabricator.haskell.org/D1486) that makes DRF and Generics work together.
It's not an either-or world. Besides, the notion of "best" isn't an absolute one.
This certainly the closest thing to C++ destructors in Haskell. However, the OP really wants RAII which is best represented with other tools in Haskell. See the other answers.
In programming languages, “safety” is never an absolute term - it's always relative to what kind of misbehaviors your type system is designed to prevent. Haskell's type system, in particular, doesn't even attempt to prevent resource mismanagement. So, if you're trying to manage multiple resources that depend on each other in complicated ways, I'd say Rust is the better choice. On the other hand, Haskell is much better than Rust at performing pure data transformations, because the type system lets you identify and exploit algebraic structures, which are the ultimate foundation of reusable generic algorithms. To a certain extent, this is doable in Rust as well, but you will run into trouble if you try to abstract over type constructors.
Wow, transient looks like some amazing work!
It's a bit off to say that "Haskell's type system was not designed to prevent resource mismanagement." Haskell's type system was (and is being) designed to allow the programmer to express as many constraints as he wants to in the type system. We're not there yet, of course.
I don't want exactly RAII, I want something that lets me use scarce resources without fear of losing track of them, guaranteed by type system. And C++ stuff can be easily circumvented as well: File* function() { std::shared_ptr&lt;File&gt; file = std::make_shared&lt;File&gt;("some.txt"); return file.get(); }
Don't get me wrong, it isn't my intention to put Haskell down. Haskell is a very beautiful language (much more so than Rust!), and it's often my tool of choice for solving complex problems. However, I don't see how the resource management problem can be solved *elegantly* without substructural types (ideally, linear, but affine is “good enough” for most cases). Regions (a generalization of `ST`) are a theoretically interesting solution, but they require too many manual programmer annotations (in the form of “enter this region”, “transfer this resource to that other region”) to be viable in complex scenarios, *and* they are still less powerful than Rust's compile-time ownership and lifetime checks. Unfortunately, retrofitting substructural types into Haskell requires a major revision of the language and standard libraries, which makes it not viable from a software engineering point of view.
Recommending what you think is a better language for the task is a perfectly legitimate answer in my opinion.
I don't think are any active organizers. :-(
Nothing is wrong with that, but it isn't strictly answer to that question which should be either solution in Haskell or explanation why it is hard/impossible to do in Haskell. Sometimes it is just matter of tolerance to offtopic or misunderstanding, but down-votes should be explained in general.
Because not everyone really needs deterministic resource management, and also because Haskell's type system is more powerful for most things. I personally prefer Rust for system-level programming and Haskell for application-level programming.
I don't disagree, but it's certainly not a complete answer.
That's fair. I didn't realize the extent of the safety you were after.
Good to know. I thought I had seen attempts in Haskell to capture these types of thing in the type system. Do you know of other prior work?
I think interaction nets solve his particular counterexample. `2^n` takes linear time to store in interaction nets on its normal form, and I'd argue inets are *the* natural representation of lambda terms. You could just use interaction nets instead of textual representations for communication. I'm not sure how many use cases it solves.
finally a breaking change that makes sense / which I can get behind. Stuff like this is a lot more important, IMO, than whether you have the optimal signature hierarchy.
There is no best. I like Denommus' answer, but my own points: - Rust does not let you infer the type of functions - Rust does not yet have higher kinded types - Currying is not the default - Returning heap allocated closures is harder in Rust than Haskell - RAII infects everything, so you are stuck thinking about resource management even when you did not want to. Whether this is good depends on the user. I like it, others do not. - There are no custom operators. There is operator overloading, however. - There is no effect tracking system. - There is no lightweight task runtime. - partial matches are a compiler error. In Haskell it is convenient to leave out some cases and let it throw an exception on unexpected cases. In Rust this requires boilerplate. - There are no orphan instances for typeclasses/traits. - "Slice patterns" are not yet stable, so if you want to program over sequence like data structures functionally by pattern matching on multiple elements at a time, you will be disappointed. Your mileage will vary depending on what you want and prefer. There is an existing resource somewhere about linking to Rust code from Haskell via C bindings as a lingua franca.
`regions` seems to be abandoned. Are there any other libraries that prevent accidental use of a resource after it was released?
/u/Tekmo can you confirm/clarify?
Sorry, didn't read the type of `runManaged`... But I still can use IORef to circumvent it: bad = do ref &lt;- newIORef Nothing runManaged $ do handle &lt;- managed (withFile "some.txt" ReadMode) liftIO (writeIORef ref (Just handle)) fmap fromJust (readIORef ref) `runManaged` only being able to return `Managed ()` adds 0 safety, just annoyance when you have to return something from your computation... 
Certainly, it's always possible to circumvent a system if you try hard enough. It's a bit antagonistic to say this adds "0 safety." The docs for Managed describe the intent: http://hackage.haskell.org/package/managed/docs/Control-Monad-Managed-Safe.html
We added tilde for constructors.
What an exciting week for GHC HEAD! :)
I'm not familiar with the entire literature on region-based resource management. But the monadic encoding of nested regions is due to [Fluet and Morrisett](http://www.cs.cornell.edu/people/fluet/research/rgn-monad/index.html) (Matthew Fluet is one of the developers of the whole-program optimizing Standard ML compiler [MLton](http://mlton.org/)), as well as [KIselyov and Shan](http://okmij.org/ftp/Computation/resource-aware-prog/region-io.pdf) (Oleg expands on this topic on [his website](http://okmij.org/ftp/Haskell/regions.html)). The basic approach is to use a monad transformer stack with a layer for each nested region. Region-based memory management itself was originally developed by [Tofte and Talpin](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.5498), for use in the MLKit Compiler, but it builds on prior work on [type-and-effect systems](https://en.wikipedia.org/wiki/Effect_system), rather than on the monadic encoding of effects. Pierce's book “Advanced Topics in Types and Programming Languages” dedicates chapter 3, “Effect Types and Region-Based Memory Management”, to this topic. Rust's notion of lifetimes is also based on this work.
It does to some extent. It still means I can't export it in a private module for internal does, but at least the "fully private" stuff can stay fully private.
He might be talking about the MRP.
`MonadFail` constraints won't be generated by the desugaring by default until at least 3 releases from now. In 8.0 we add the class and give you an explicit `MonadFailDesugaring` extension you can turn on to try out the new desugaring. In 8.4 we warn about the impending change. In 8.6 we turn on the extension by default. That said, we _have_ been talking about releasing an official `fail` package, that could be used to get compatibility with manual `fail` calls with older GHCs for folks who currently don't use the desugaring, but do manually call `fail`. That would let folks who want to be early adopters define instances early and get them out of the way.
Can you actually show me a bit of code that `do` would ever generate that only needs `Applicative` and `fail` that doesn't result from you calling `fail` yourself?
Great article! When I learned about event sourcing, I immediately thought that would be a great storage model for a purely functional application. I maintain a large scientific desktop GUI in Python for work, and I would love to move to Haskell. Articles about models and inter-model communication are very informative to me.
At the moment, `base-compat` [deliberately avoids](https://github.com/haskell-compat/base-compat#what-is-not-covered) redefining data types and type classes to avoid issues with potential breaking changes in the future. However, it just occurred to me that `MonadFail` will be a part of `Prelude` some day, which will make backporting the latest `Prelude` much trickier. [Hm...](https://github.com/haskell-compat/base-compat/issues/36)
&gt; The lines ... don't really mean anything. Sure they do. Just not what OP wants them to mean. Don't give people answers to homework questions. It doesn't help. 
&gt; That said, we have been talking about releasing an official `fail` package, that could be used to get compatibility with manual `fail` calls with older GHCs for folks who currently don't use the desugaring, but do manually call `fail`. That would let folks who want to be early adopters define instances early and get them out of the way. Right, this is what I want. I don't find myself using the desugaring much, but there are some places where I find manual `fail` calls useful (Template Haskell error messages being an example), and having `fail` be its own package would make it much easier to reduce CPP-related overhead.
Really interesting indeed. Regarding the event sourcing, whenever you change something you have to recompile and run your API which mean deserialize all the events. I wonder how you do it when you test your application because it means to me that you have to compile &amp; run multiple time to see your changes. If you don't work with a lot of data it's fine but if you have to deserialize tons of data then I wonder if the trade off is really interresting compared to a traditional database.
They then delete their submissions once they get an answer, presumably so they don't show up in their history. Bad faith, that.
The reason you [repeatedly](https://www.reddit.com/r/haskell/comments/3t3j5u/in_haskell/cx2t0wh) ask questions that show a fundamental lack of understanding of the basics of Haskell and the given task is that people keep repeatedly giving you answers while you don't do any work on your own. In this way, you move on to later exercises without ever understanding the previous exercises, which will cause you to be continuously frustrated and to continuously post these awful submissions to /r/haskell and, more to the point, prevent you from ever actually learning Haskell. The only way to learn is to *do the work yourself*. Stop doing this, start over again, and try your best to learn these things on your own. The only person you are cheating is yourself. 
Type safety (progress and preservation) *is* about making wrong things impossible to do. The problems with monadic regions are: * They require rank-2 types, which complicates inference. In fact, GHC simply does not infer rank-2 types. The type behind the rank-2 quantifier (the region's `s`) is never actually instantiated, because its sole purpose is to have the property of being unusable outside of the region. * They require extremely cumbersome annotations: “enter this region”, “store this object in this region”, “transfer this object to that other region”. * The notion of ownership is too coarse-grained - each monadic region owns a possibly large collection of resources, because the API makes finer-grained resource management impractical - who would want a 50-layer monad transformer stack? * Because monad transformer stacks are intrinsically... well... *stacks*, they can't model what I often call “braided resource acquisition”: acquire A, acquire B, release A, acquire C, release B, acquire D, release C, etc. This pattern is fundamental in the implementation of [concurrent mutable collections](http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA232287). (Link requires download, sorry.) Rust has none of these problems.
Haskell is pretty well taken care of in Nix land. Other languages like Python, Perl etc are as well to varying degrees; I'd say Haskell probably has the most complete and full package set of any language in Nix, as well as lots of active users... But I maintain python and perl and C things and like ~50 other Nix packages, and none of them are too troublesome. The problem with 'tooling and support' is that you shouldn't be surprised when you need to write your own... Everyone who uses Nix pretty much eventually becomes a contributor, just due to the size of the community. You inevitably run into something you need or want. Also, some software in the wild is kind of... actively hostile to sensible build systems it seems... so some software takes more 'cajoling' than others to get it to play nice. And other things can get in your way. I have my own kernel patchset for example with some modifications, and it's not really very nice to develop the patchset on NixOS because { long reasons this is the case }. Using it, however, is wonderfully easy for anyone. Working around this is OK enough. And I'm using it more than anything, so, yeah. However, the list of advantages is significant IMO and more than worth some labor. Especially with NixOS, you can have a computer with properties like "I can undo any upgrade transactionally" or "installing new applications does not break old ones", among a lot of other great ones (like, I copy one file from laptop A to laptop B, copy my `$HOME` over, and run one command, and I have exactly replicated my laptop instantly). It's almost horrifying we've worked ourselves into a place where this stuff isn't the case for major consumer software systems. Like... "undo" is pretty useful. Why don't most systems have that in 2015?
Excellent! A very welcome change. 
Minor, possibly pedantic correction: at most once.
Sorry, I meant linear, not constant time. If I understand the paper, `zipWith` and `min` are at best quadratic on the Calculus of Constructions, since pattern matching on church-encoded values itself is linear. I could be mistaken, though.
I think almost all frameworks allow the use of a custom template language. Which one are you using?
How about this? data These a b = This a | That b | These a b thisThatThese = do This f &lt;- fooOp That g &lt;- barOp return $ These f g could be desugared into thisThatThese = (\(This f) (That g) -&gt; These f g) &lt;$&gt; fooOp &lt;*&gt; barOp ~~Edit: Actually, the above example doesn't need a `&gt;&gt;=` either, it's just `(\(Foo f) (Bar b) -&gt; f b) &lt;$&gt; fooOp &lt;*&gt; barOp`.~~ Nope, that relies on an ordering of effects. Thank you to /u/twanvl for pointing that out.
We have a USER GUIDE and you think about responsive themes?!?!
Pretty much all modern web development allows for use of templating. Surely the use of popular templating engines is supported? None of them are language-specific.
Alternate theory: Moderators are removing them?
Most frameworks let you pick a templating engine. Some of them force you to write templates within haskell, others don't. Example: [ede](http://hackage.haskell.org/package/ede) (and [its servant integration](http://hackage.haskell.org/package/servant-ede)).
Good luck! Even if you find the source, you'll probably have a hard time compiling it because it's so old. [Reactive](http://hackage.haskell.org/package/reactive), by the same author, is more recent than Fran and yet I gave up trying to make it compile with a recent version of GHC. Instead of studying a real-world, but early implementation of FRP, why not study this [toy implementation](http://gelisam.blogspot.ca/2014/07/homemade-frp-study-in-following-types.html) I blogged about? It's much much simpler than real implementations, of course, but it sounds like a simple implementation is what you're looking for.
Perhaps, although they say "deleted" rather than "removed".
Indeed your suggestion looks pretty good to me. The current theme is my attempt at hacking the theme used by LLVM for their documentation but I was never really pleased with the outcome. /u/alt_account10, would you be interested in trying your hand at integrating a new theme into the users guide?
If it is the MRP, the merits of this would be about the same; the idea behind both center around "making invalid states unrepresentable" (to use the language of MRP). AMP and FTP had somewhat stronger cases as they were about generalization as well.
You can definitely use the type system to enforce this. Only export a `Managed` or `Acquire` resource from your library and don't export the corresponding `open` and `close` functions.
What implementation of `min` is linear? I still don't comprehend / am not convinced `zip` can't be linear. But couldn't [this](http://staff.computing.dundee.ac.uk/pengfu/document/talks/mvd-2012.pdf) solve the performance issue? It looks like a very simple and natural addition to CC - much more simple than CIC. What do you think about it?
How would I do “braided resource acquisition”? That is: acquire A, acquire B, process A-B, release A, acquire C, process B-C, release B (...) acquire Y, process X-Y, release X, acquire Z, process Y-Z, release Y, release Z. Not that Java or Go can handle this either. It requires substructural types.
I actually don't think it's necessary, the moment I've made the comment I've checked with the in-browser resize tool for responsiveness, but actually on mobile on double tap it zooms in nicely and it centers the relevant content.
&gt; Can't monad transformers be extended in the other direction as well? I have no idea. I suspect not. &gt; What's a substructural type, by the way? A [substructural type system](https://en.wikipedia.org/wiki/Substructural_type_system) lets you control how many times values are used. For instance, if `File` is a linear type, then any value of type `File` has to be used *exactly once*, so our file manipulation API could look like this: open :: String ~&gt; File seek :: Int -&gt; File ~&gt; File read :: Int -&gt; File ~&gt; (File, String) -- Int says how many characters we want to read at most write :: String -&gt; File ~&gt; File close :: File ~&gt; () Where `~&gt;` means “effectful function”. (You can think of it as `Kleisli IO` as an approximation.) Linearity provides the following guarantees: * We can't forget to use a `File`. In particular, we can't forget to close a `File`. * We can't use a `File` twice. In particular, we can't use a `File` after closing it. In a language with linear types and first-class functions, some care must be taken, because now we have a distinction between `Foo -&gt; Bar`, the type of functions that can be called as many times you want, and `Foo -o Bar`, the type of functions that must be called exactly once (because they're closures that capture some linearly typed variable).
I'll definitely find an excuse to come down. Now I just need to figure out a topic I'll still be interested in in 3 months. =)
By the way, it isn't on the page yet, but Eugenia Cheng (who you may know from this: https://byorgey.wordpress.com/catsters-guide-2/ and from this: http://www.amazon.com/How-Bake-Pi-Exploration-Mathematics/dp/0465051715) will be giving a keynote.
The idris docs linked are themed via readthedocs (https://readthedocs.org/) -- I think we can probably just hook up a readthedocs instance for ghc now that we have this doc build method, and get that _on top of_ whatever we now have, tied to different tags, etc. I understand agda is moving to use that system as well.
One of the best times we had at my meetup was when I brought 20 minutes of live-demo material, and then we went way off-script with it. I just wanted to show how I had stumbled blindly into a fairly idiomatic version of the MaybeIO monad transformer (as verified by several in #haskell) using a very light form of hole-driven Haskell. I could repeat this (in about 20 minutes), but I still didn't understand monad transformers, or even how to use the one I'd "written," so I was just going to show how to at least write it. I started demoing from my laptop to the big screen on the wall, and then people asked questions, and I'd tweak it, or redo bits to show others ways to think about or solve things. I started to ask questions and wait to let them solve parts, and eventually was having them do everything; I was just taking dictation :) They went a different way than I had at one point, and we got stuck, so I eventually said "Want to see how I did this part?" and undid back and showed my solution. Sometimes I'd get myself confused, though, and the group would discuss and help me through it again. One guy knew a bit more than me, and started having me write things I didn't understand to show how to use what we'd made. I was lost, but still picking up bits that I'm sure will be useful later. We went for 2 hours, collectively playing like this, and everyone really enjoyed it. The host said he'd wanted us to do something just like that for awhile, and wants us to think up more halfway-solutions that we can share and pick apart together. I wish I could do this with a fun group for an hour every weekday. I'd learn *so much*. Just an idea!
You can get it from [here](http://research.microsoft.com/en-us/downloads/c9eff407-ce59-4a2a-90cb-de099a9bacbd/), but it is written in very old Haskell (clearly before the time that we had hierarchical modules) that also has lots of bitrotted Windows stuff mixed in. The download there is a self-extracting Windows executable, but Debian's unzip was able to extract it for me. It'd probably be a somewhat reasonable project to try to resuscitate it as the pure side of it at the very least is an object of historical value. A lot of the pure code will likely require just small adjustments if any, but the IO interface will need to be thrown out and rewritten completely I expect. You might be able to use [gloss](http://hackage.haskell.org/package/gloss).
So it's pretty clear to me that `a` and `b` are your two input lists of type: a :: List a b :: List a ... which are church encoded with the `Cons`-replacement as the first argument and the `Nil`-replacement as the second argument, except that I think you have the arguments to the `Cons` constructor swapped from the traditional order: type List a = forall x . (x -&gt; a -&gt; x) -&gt; x -&gt; x The reason I think you have them in that order is that the first continuation you pass to `a` is: λ p c . (f (p c)) ... which would not be parametric in the type parameter `a` if `p` were the list element. It looks like `c` is supposed to be the list element. The part where I got stuck is that I thought that `f` is the comparison function used for comparing individual elements: f :: a -&gt; a -&gt; Ordering However, if I do type inference from that point forward I get a type error, because that would imply that: x :: Ordering p :: a -&gt; Ordering c :: a p c :: Ordering f (p c) :: &lt;Type Error&gt; ... so that's where I need some more explanation of how your function is designed to work.
FWIW, [MRP](https://ghc.haskell.org/trac/ghc/wiki/Proposal/MonadOfNoReturn) is indirectly related to generalising `Monad` to `Applicative` as it also makes sure `(&gt;&gt;)` is defined in a sound way to allow finishing up generalising (and de-duplicating) the remaining verbs in `Control.Monad`/`Foldable`/`Traversable` without risking performance bugs. 
&gt; Not taking enough care to keep compilation time low What we did was – for local development we use `-O0`, while for production it's `-O2`. It's easy to configure it this way using `stack` tool, which can easily pass flags to not only main, but also dependant packages you're building. Did you try this technique? It made a huge difference for us, converting multi-minute build into 15-seconds one.
&gt;GHC **7.8.4** Users Guide This is a mistake, isn't it? 
code is here: https://github.com/suhailshergill/extensible-effects but it doesn't yet include results from the last " Freer Monads, More Extensible Effects" paper. There is an issue that tracks the adaptation here: https://github.com/suhailshergill/extensible-effects/issues/56
I'm in mobile so I can't conveniently have look myself... Can this be used as a library and embedded into another program?
Well, the readme says "As with most Haskell packages, this package can either be used as a library or as a binary package."
I don't know whether to be annoyed or elated! Every xmas I get myself a new xmas project, something un-related to other things I've been working on, a new green field project. My plan for this year's project was exactly this, to start work on re-implementing the Tor in Haskell and to see how far I could get.
There were quite some attendees from outside Leuven: we were 5 from Ghent, and I understood from some other people that they also travelled from East or West Flanders. While I would love to have a Haskell or functional programming group in Ghent (traffic to Leuven is hell during rush hour), I am a bit wary of fragmenting the relatively small community in Belgium. Maybe some sort of simulcast could be an option in the future, but of course one of the nice things of these meetings is to be able to meet in real life and discuss.
Duly noted. I'll try to fix that.
Good work! Seems to be written in a very clear style. I'm sure I'll learn some things by reading through the code base :D.
I don't understand it either but here is what GHC has to say: type Natural = forall x . (x -&gt; x) -&gt; (x -&gt; x) min :: Natural -&gt; Natural -&gt; Natural min = (\ a b f x -&gt; (a (\ p c -&gt; (f (p c))) (\ c -&gt; x) (b (\ p c -&gt; (c p)) (\ c -&gt; x)))) Occurs check: cannot construct the infinite type: t0 ~ (t0 -&gt; x) -&gt; x Relevant bindings include c :: t0 -&gt; x (bound at Main.hs:42:64) p :: (t0 -&gt; x) -&gt; x (bound at Main.hs:42:62) x :: x (bound at Main.hs:42:16) f :: x -&gt; x (bound at Main.hs:42:14) In the first argument of ‘c’, namely ‘p’ In the expression: (c p) If I omit the type signature GHC accepts it and infers the following type: min :: (((t4 -&gt; t3) -&gt; t4 -&gt; t2) -&gt; (t5 -&gt; t9) -&gt; t1 -&gt; t) -&gt; ((t7 -&gt; (t7 -&gt; t6) -&gt; t6) -&gt; (t8 -&gt; t9) -&gt; t1) -&gt; (t3 -&gt; t2) -&gt; t9 -&gt; t 
Here is my quick [attempt](http://ghc.readthedocs.org/en/latest/index.html) at hacking the user's guide into readthedocs. There is still a bit of work to me done here, * The PDF build appears to be broken; I'm actually not sure there is an easy way to fix this as readthedocs uses standard `pdflatex` whereas we specifically use `xelatex` in the GHC build system due to lacking Unicode support in the former. * The links to Haddocks are currently broken. I don't believe we can host them on RtD so they'll have to remain elsewhere. * The build isn't actually derived from the `ghc` repository but instead generated from another [repository](http://github.com/bgamari/ghc-users-guide) which I generated by script. This was necessary due to the need to modify `conf.py`.
Fair enough; I'll admit I'm not much of a designer. Suggestions certainly welcome!
That is because Web development is so low level nowadays that an assembler programmer of the 50s would feel at home. All these routes, all these bits in headers, caching request and responses, user session state, configurations... All that mess is there an functional programming has done nothing to change it. it is a complete failure. What is worst, things are going backwards rather than forward, as brainlesly people are introducing ugly alien concepts like MVC and routes to functional languages. These OOP minded people hare doing a great harm to Haskell by burying nice and wonderful functional developments under their hyperactivity and their over-engineered solutions for trivialities. There is more type safety and level of abstraction in some functional web frameworks from 10 years ago like WASH than in Servant.
I've [improved](http://downloads.haskell.org/~ghc/master/users-guide//) the contrast a bit (although this may take a while to propagate through the CDN). Does this look any better?
I'm not sure I understand. Servant provides a few ways to build abstractions to make everything feel less low-level. Servant-out-the-box is raw, but I've been (ab)using the abstraction capabilities to write high-level code for a year.
Have you tried using [MonadRandom] (https://hackage.haskell.org/package/MonadRandom-0.4)? It should let you have the bonuses of using the IO monad but without allowing all the full range of effects that IO does.
(This comment will probably be obvious to anyone who understands the topic, but I'll leave it here anyway, because figuring it out was a learning experience for me.) These "free" constructions are quite tricky. They aren't just any expression trees corresponding to the typeclass operations, they must also obey the laws. For example, take this simple typeclass: class Transformable a where transform :: a -&gt; a At first blush, it seems easy to define a "free" construction for it - don't apply the operation, but build an expression tree to remember how many times we applied it: data Free a = Initial a | Transformed (Free a) instance Transformable (Free a) where transform = Transformed But wait! That construction doesn't obey any laws, it just stacks operations forever. What if the typeclass had a law saying the operation must be idempotent? Then the "free" construction shouldn't stack operations more than once: data Free a = Initial a | Transformed a instance Transformable (Free a) where transform (Initial x) = Transformed x transform (Transformed x) = Transformed x Or what if we had a law saying the operation must be its own inverse? Then the same must be true for the "free" construction: data Free a = Initial a | Transformed a instance Transformable (Free a) where transform (Initial x) = Transformed x transform (Transformed x) = Initial x Similarly, we all know that a free monoid is a list, but that fact hinges on the law saying the monoid operation must be associative. Without that law, a free monoid would be a binary tree (i.e. a fully parenthesized sequence). With Monad it's even more complicated. At first I thought you could get the freest possible Monad by just recasting the monad operations as different constructors of an unevaluated expression tree (GADT style), but now I see that it's not quite that simple. The expression tree needs to also obey the monad laws by construction, and that's why the result is not trivial.
Uhm you're right; I linked it off the top of my head but it definitely became more complex since last time I saw it (few months ago). There weren't any indexed functors, lenses or pipes, IIRC. Sorry /u/tailanyways , I didn't mean to scare you off. Have a look here for a way of making your datatype an instance of `Random` : https://www.fpcomplete.com/school/starting-with-haskell/libraries-and-frameworks/randoms
Fair enough. For a minute there I thought you were part of the decade-long false flag operation conducted by the upper echelons of the Python cabal to spread the notion that Haskell is impossibly weird and aimed only at Ph.D. types. People call me paranoid but I've... seen... things.
Check out [Heist](https://hackage.haskell.org/package/heist) from the [Snap](http://snapframework.com) web framework. It's designed for exactly what you describe: handing front-end development off to colleagues who are not yet Haskellers. Heist has nice [documentation](http://snapframework.com/docs/tutorials/heist) and has been the subject of some blog posts, two from Chromatic Leaves [[1](http://chromaticleaves.com/posts/the-great-template-heist.html), [2](http://chromaticleaves.com/posts/compiled-heist-the-walkthrough.html)] and one from /u/ocharles [[3](https://ocharles.org.uk/blog/posts/2013-12-11-24-days-of-hackage-heist.html)]. Heist templates are valid xml fragments. They would be valid html5 too, except for the fact that not many of them are enclosed in `&lt;html&gt;` tags - instead they get recombined into one big page later. But the idea is that your normal html editing tools will not get confused because there's no special syntax. Lack of special syntax means that Heist templates lack some of the things that you'd get in other template systems, like `for` loops and conditionals. To achieve things like branching and listing, you would just include a named tag which the Haskell side may splice out, or replicate. Another thing that Heist gives up, when it provides templates as xml fragments, is some type safety. It's harder for the system to know that your Haskell code and your templates match up perfectly than it would be for a template-Haskell based solution. Some of my bugs have come from this. I hear that you can namespace your tags and the namespaces allow Heist to know if you have forgotten/misspelled tag names in splices... that sounds like something I should be doing :) The last bit of info I should say about Heist is that there are two different mechanisms for declaring and executing the splicing - 'interpreted' and 'compiled'. The first one is moderately easy to understand (although you will still have to acclimate yourself to the somewhat quirky Heist abstractions), but a bit slow, because the page generation happens on-demand. Compiled splices are several orders of magnitude faster, but figuring out how to get them to work can be difficult. That trade-off can cause some cognitive dissonance. If this is interesting to you, you can learn all about them from Heist's author in this [video](https://vimeo.com/59215336) and in the Heist [wiki](https://github.com/snapframework/heist/wiki/Compiled-Splice-Formulations) Come say hi on #snapframework irc channel if you try heist get stuck.
Thanks. It looks great! One very annoying UI problem: on a touch device, clicking on the `next` link often becomes a click on the `Glasgow Haskell Compiler Users Guide` header because it extends across the entire width of the page (and so you loose where you were and go to the beginning...). It would be better if the "clicable" area only covered the `Glasgow Haskell Compiler Users Guide` text. I agree with /u/mirpa that there's a lot of gray though. But the red warning box stands out nicely (e.g., one on [GHCi page](http://downloads.haskell.org/~ghc/master/users-guide//ghci.html). Another problem I see is that the background of headings (H1, H2, ..) is the same as the background of the code block. I am not sure that the headings need a background at all, in fact. They are already bold and large...
So, is this an approach that is best suited for large core libraries, where the performance improvement due to fmap fusion would be non-negligible? 
&gt; One very annoying UI problem: on a touch device, clicking on the next link often becomes a click on the Glasgow Haskell Compiler Users Guide header because it extends across the entire width of the page (and so you loose where you were and go to the beginning...). It would be better if the "clicable" area only covered the Glasgow Haskell Compiler Users Guide text. Do you mean the "next" link on the right side of the header? What browser are you using? I'm having trouble reproducing this behavior in Firefox's responsive mode. &gt; Another problem I see is that the background of headings (H1, H2, ..) is the same as the background of the code block. I am not sure that the headings need a background at all, in fact. They are already bold and large... This is an interesting point. I'll play around with this.
Well, the core 'Coyoneda' trick is suitable when you are going to do lots of `fmap` calls over some functor, but don't get to know which one it is. As for the viability of `Freer`, that is a bit more of a mixed bag. It is a better reflection without remorse, but the constant factors remain somewhat daunting. The asymptotic story is better when it applies, which is that you are going to do multiple inspections of the structure and continue binding, but the constant factors are high, and not every monad fits.
There's already some good answers, and I don't want to just repeat them, so I'm going to make a different suggestion. Why not write it so your basic operations are on a state space rather than specific values, and then compose your state spaces rather than using concrete values, only selecting a concrete item from that space at the very end with an interpreter function? Lazyness lets you define enormous trees, and so long as you only visit one or a few branches, your whole operation will be still be tiny and fast.
I suppose it depends on the interpreter. But, a type like that certainly suggests the only operation is identity. However, the interpreter might do something arbitrary between each "step".
Using the definitions in http://comonad.com/reader/2015/categories-of-structures-in-haskell/ you get: data Cofree1 σ f a = forall g. σ g =&gt; Cofree1 (g ~&gt; f) (g a) Cofree (Cofree1 Functor f) a ~ (a, exists g. Functor g =&gt; g ~&gt; f, g (Cofree (Cofree1 Functor f) a))) ~ exists g. Functor g =&gt; (a, g ~&gt; f, g (Cofree (Cofree1 Functor f) a)) data Cofreer f a = Cofreer :: Functor g =&gt; a -&gt; (forall x. g x -&gt; f x) -&gt; g (Cofreer f a) -&gt; Cofreer f a
&gt; Safety is not about making wrong things hard to do, but it is about making correct things easier to do. No. Safety is exactly about making wrong things hard (ideally *impossible*) to do. For example: http://ehs.ucsc.edu/programs/safety-ih/lockout-tagout.html (with NSFW [pics](http://ehssafetynewsamerica.com/2012/06/11/safety-photo-of-the-day-why-lock-out-tag-out-is-vitally-important/)).
oh, that's even better.
Depending on what you're doing, try the [async](https://hackage.haskell.org/package/async) package and read Simon Marlow's (free) [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929).
Up for revision right now! https://phabricator.haskell.org/D1495
Yes, I almost certainly will.
Awww! Sorry to run your Xmas! But be of good cheer: there are a bunch of features I didn't get done in time for the talk at QCon. Hidden service support, flow control, ... And I love pull requests.
Thanks!
What have you tried so far?
This is what I have written so far: sumDifference :: Int -&gt; Int -&gt; (Int,Int) sumDifference x y | x + y = | x - y = I'm not sure if I'm suppose to be using guards or not and I'm not sure how to output as a tuple. Edit: Sorry for bad formatting
I know a bit from university classes, but I'm not familiar enough with the online resources to be able to recommend anything. Last time I asked for some resources, I've been pointed to a series of video lectures on Type theory foundations by Robert Harper, which can be downloaded from [here](http://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html) and can also be found [on youtube](https://www.youtube.com/watch?v=9SnefrwBIDc&amp;list=PLGCr8P_YncjXRzdGq2SjKv5F2J8HUFeqN).
I'm morbidly curious; what sorts of "... things"?
Thanks, your comment helped me understand it. Extremely simple and I feel stupid for not getting it immediately. Thanks again!
 randomRIO :: (a, a) -&gt; IO a the pair are the limits http://hackage.haskell.org/package/random-1.1/docs/System-Random.html#v:randomIO Don't use monadRandom. It is worthless to introduce a new monad just for generating random numbers. abusing monads are to bad functional design what abusing objects is to bad OOP design. They both encapsulate things for no reason and you have to clutter your code without any additional functionality, while losing composability
I can help out with this. I have a Mac for personal use and I've gone through their contribution process before. If you'd like me to give getting it on homebrew a shot, let me know. My email address [is on my github](https://github.com/bitemyapp/).
Yes, that is more readable.
I don't know what you mean by defining on a "state space." Do you mean the solution of putting everything into a do block? Ideally, I would be map fitness over a generation of creatures (scores) every time I want to create a new generation (10K times? 100K times? I'm not sure what my termination condition should be yet (fitness of x vs. number of generations)). Is that in line with what you're saying? 
Ok. So, I'm assuming that takes a similar approach to the thing I have commented out, which is: randomIOKey :: IO Int randomIOKey = getStdRandom (randomR (0,87)) But now I have IO Ints... and I don't know how to use them throughout my program. I know how to print them, and put them in a do block. Can I cast them to an Int somehow?
Dammit. What you're saying sounds really practical, but it's still over my head.
Thanks.
You are missing the point. I'm talking about a completely different thing. Imaging you have to apply 100 lockout devices to swap a fuse. Will you really apply all of them? Most people will ignore the procedure because it is too complex. And what if you need to apply only one lockout device? Most people will follow the procedure because it is simple. If your safe guard makes peoples worke harder, thay will disable the guard. If you make it impossible to disable, people will just use alternative (unsafe) tool to do their work. It is actually about real life, not about CS.
It would suffice to add qualifier as prefix. import qualified Data.ByteString.Char8 as C ... f :: C.ByteString -&gt; Int and show `f` as `f :: C.ByteString -&gt; Int` in haddock and perhaps list qualified imports too at the beginning. As workaround you can hover over type with mouse to see where it leads. But you are right, two types should be distinguishable. 
How close to done would you say this is overall? Are those features relatively smally compared to whats already written, or is there a long way to go? I run a relay, I'm going to see if I can get this running and see if I run into any problems. I'd love to try to contribute to this. A fully featured haskell tor implementation is like my dream haskell application, thanks for building this. 
I replied. The Homebrew people might ask for you to pop in and verify that you want the project posted. First I'll try to get the recipe hammered out.
Something I do fairly often is look at the url which pops up when you hover over a link in order to find out which package/module it comes from. Firefox displays it in the bottom left usually, I'm not sure about other browsers.
Oh, I see, you meant the `min` of two `Nat`s. That's a pretty clever trick, but there's no way to translate it to System F as you wrote it. `a_continuator` alone cannot be implemented in System F. My recommendation is to use the coinductive encoding of natural numbers if you want linear-time `min`: data CoNat = forall x . CoNat x (x -&gt; Maybe x) The other benefit of the coinductive encoding is that you can also implement the identity of the `min` function, which is infinity: infinity = CoNat () (\_ -&gt; Just ()) ... which you wouldn't get if you used the inductive encoding.
 [1,2] [1,4] == false [1,2] [1,2] == true that what has to be 
Nice! I just discovered [Guard](https://github.com/guard/guard) as well. Looks to be a very mature project but I'm not at all plugged into the Ruby community so I hadn't heard of it. I wonder what features it has over Steel Overseer.
I tried to do it other way, but gave me error setEqual :: [Int] -&gt; [Int] -&gt; Bool setEqual [] [] = True setEqual length (x:xs) = if x /= length then False else setEqual length xs 
You should take some time to read the error messages you get. If you read the error message the compiler would have told you that `x` and `length` are not the same type, so you can't test them for equality.
 1 cmp :: Eq a =&gt; [a] -&gt; [a] -&gt; Bool 2 cmp _ [] = False 3 cmp [] _ = False 4 cmp xs ys 5 | ((length xs) /= (length ys)) = False 6 | otherwise = all (==True) ( zipWith (==) xs ys )
That's just normal equality. The (==) function on lists will do that for you. `compare = (==)` If you want to define it yourself using a recursively defined function you can do so as well. compare [] [] = true compare (x:xs) (y:ys) = x == y &amp;&amp; compare xs ys compare _ _ = false
This is a pretty well known phenomenon in computer security as well. Requiring your users to follow ultra-detailed and annoying protocol with things like passwords (at least 24 chars, new unique pass every month, must contain at least 4 punctuation characters, etc) causes them to invent shitty workarounds like incrementing a number every time you ask them for a new password... or just writing it on their desk.
&gt;mandelbrotness
Any hint as to when registration will open? 
I guess you mean a mix of stack and cabal ? How will it be different from stack ? 
Ah I understood it as a reference to `lens` ...
Could you be more specific?
Why don't you like `[1 .. 4]` ?
Just for fun. sumDifference x y = ($x) &lt;$&gt; [(+), (-)] &lt;*&gt; [y] I tried a pointfree version, but that's not readable at all ;)
Cool. If you use the command line a lot, you might be interested in [Hawk](https://github.com/gelisam/hawk)
Fun!
liftA2: Couldn't match type 'Complex Double' with '[Complex Double]' 
It is too bad that Gabriel Gonzalez has not made a web client library since then there would have been great documentation. Haskell would really shine if there existed a well documentred, well-designed web client library that worked with ghcjs, Windows, Mac &amp; Linux (CEF?). 
So this can be seen in at least two different ways: sampling uniformly a very high dimensional space of configurations (notes * time ticks) and filtering according to these many rules (however what you describe above is a set of pairwise or N-wise rules, for N &lt; 5 or so). The other way is a highly targeted form of sampling, i.e. conditioned on the previous samples (chords at previous times); the technical underpinnings might be described with Markov Chain sampling (finite-dimensional chord-chord transition matrix), but a musical composition is not really a random process, by any measure. It's limited in time and frequency, not periodic nor completely random. In this sense rejection sampling could be extremely inefficient: a musical composition is an extremely rare object in such a configuration space.
Sorry, I was very unclear. I meant an API for creating HTML based UI:s.
To get a picture of just how slow GHCi is, the JavaScript code produced by Haste is generally 1.5-25x times faster than running the same program with GHCi for the nofib subset I'm currently playing with. Haste code is generally 5-10 times slower than native, optimized code produced by GHC, so the difference between compiled and interpreted GHC Haskell is pretty huge.
Excelent example, thanks. I wonder why people agree with you and disagree with me :) Probably because of my poor English.
This looks very elegant and i am looking forward to try it myself, however, i had troubles installing `OpenGLRaw` because of a `Missing C library: GL`. I could not find a simple way to install OpenGL with Nix as well. I would be glad if anybody could point me to the Nix package name, so that i can roll out a simple `default.nix` to work in a shell. I guess that i should also modify `LD_LIBRARY_PATH` in my `nix.default`
Nice!
I meant to get rid of the type signature just long enough to infer the type and then insert it as the new type
Ah that makes more sense. Thanks for the tip, I'll definitely be doing that from now on
I'm pretty sure that Luite has a diagrams -&gt; canvas backend lying around somewhere.
I solved the issue in 2 seconds using `stack solver --modify-stack-yaml`, but I agree with you that it is painful.
This is very cool. I would love it if GHC had this behavior for error messages. I don't see why it can't, but I do not have much insight into GHC either :p. In other words, feature request this ^
He does, and I tried to use it, but I failed to get it to work. Everything type-checked but produced no output and there's no documentation at all.
&gt; no documentation at all OMG, are you OK...? ;-)
Lovely ^_^ too bad the Haskell Center is shutting down, the possibility of hosting a two-click demo with editor , interpreter and graphical output as well was a real advance.
I need to figure out the proper way to host this sort of thing going forward.
In defense of GHC, Elm is a much smaller language that lends itself to much easier error reporting by virtue of not trying to implement most of modern Haskell type system features. Error reporting in the presense of GADTs, type families, promotion, etc can pretty quickly turn into a research problem where it's not at all obvious where to even trace the provenance of the error too. Working in a simple extension of HM (like Elm), the problem is much more tractable.
Coloring the diff would be possible now I would think. 
The PureScript compiler has a similar approach, but just links to the compiler wiki on GitHub. This has proved to be quite useful for distributing the task of collecting real-world examples of each error.
We at Snowdrift.coop are using Hamlet and Cassius. We have a designer with no Haskell experience who is managing ok. I hope the tooling continues to improve (a standalone hamlet --&gt; html tool would be great... and I even wrote one once but lost it somewhere and haven't had the chance to rewrite it). It's workable, though. And Hamlet is actually really nice imo.
However, the issue with GHC's error handling is more basic: from what I hear the internal representation of error messages is just strings and not a more structured data type. This makes it harder to plug in automated transformations to improve error messages.
Elm 0.17: Compilers as stalkers
It'll tell you you should really phone your mother before her birthday is over ;)
Once the types get more complicated the error messages can become quite confusing. But I agree the error messages for small types are helpful.
Haha! :P [We traced the type error, it's coming from inside the house.](https://www.youtube.com/watch?v=rkcGm-pWwsQ)
There are probably some other things to be done, and even if the complexity of the type system does not make still other improvements impossible, it makes them _less obvious_. As a datapoint, when people were looking at making specifically beginner-friendly errors in Haskell with the Helium project, they found that omitting some type system features was key to their success: http://www.open.ou.nl/bhr/HeliumCompiler.html That doesn't mean that you _can't_ have better messages in combination with all the type system bells and whistles -- it just means that it isn't _straightforward_ to do so, and more research along those lines would be very welcome. One place where good progress has been made is in type error localization: http://cs.nyu.edu/wies/publ/finding_minimum_type_error_sources.pdf Location reporting itself can be improved pretty simply as well, if we manage to implement the type error provenience stuff as shown by lennart in 2014: https://www.youtube.com/watch?v=rdVqQUOvxSU
I did a little experiment to see how GHC fares for an example similar to the one in the post. Here is the code I tested: data Foo = Foo { field1 :: String , field2 :: String , field3 :: String , field4 :: String , field5 :: String , field6 :: String , field7 :: String , field8 :: String , field9 :: String } foo :: Foo foo = Foo { field1 = "Foo" , field2 = "Foo" , field3 = "Foo" , field4 = "Foo" , field5 = "Foo" , field6 = "Foo" , feild7 = "Foo" , field8 = "Foo" , field9 = "Foo" } ... and here is the error message: test.hs:21:7: ‘feild7’ is not a (visible) field of constructor ‘Foo’ That's actually a pretty decent error message and it points exactly to the line and column number of the error. It would benefit from: * the typo suggestion as in Elm * displaying the code context and a color highlight of the error
You can use the SVG output, at least you could when I wrote my little UI with threepenny to create a curve going through a few point (with tangents) and output pstricks, Tikz and SVG from diagrams (which I used to offer a preview).
Is there a plan to have shorter error messages suitable for e.g editor integration?
I'm not sure about shorter, but you can currently get the errors and warnings in JSON for editor use using `elm-make --report=json`, which is what the [Sublime](https://packagecontrol.io/packages/Elm%20Language%20Support), [Atom](https://atom.io/packages/linter-elm-make) and [LightTable](https://github.com/rundis/elm-light) plugins use, I believe.
Do you want to write bindings to C libraries? Or need mutable variables? Or both? There's a chapter in Real World Haskell dedicated to the FFI, which however uses hsc. I had a better experience with c2hs for writing type mappings and simple signatures, but eventually switched to `inline-c`. Re. mutable arrays `Data.Vector` is your friend. Its Storable interface gives you access to pointers, and you can pass packed data to/from C this way. At a higher level you'll definitely want to hide the details of mutation within the ST monad.
It really depends on what you mean by numerical code. However, I'd check out these libraries if I were you: * `hmatrix` - Haskell bindings to BLAS and LAPACK * `vector` - High-performance arrays * `repa` - High-performance parallel arrays of arbitrary rank * `accelerate` - GPU-based computations Those cover the most common high-performance use cases. I'm taking this mostly from my section on [Numerical programming](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#numerical-programming) from the Haskell State of the Ecosystem post.
Yes, all of that is good. I did a little of everything you cited here, but good references are lacking. When we want to make bindings of anything more complicated than C base types I felt the best was just open other sources and try to copy stuff.
that airpair article looks great but "sign in with github to see unobfuscated code samples" ?! srsly? Anyway, thanks /u/agocorona for this great work, interactive browser applications in Hs will attract a lot of good mindshare
OH. Yes, that makes more sense, thanks.
You can use modules `System.Directory` and `System.FilePath`. fmap (&lt;/&gt; "myFile") getCurrentDirectory -- same as "myFile" Most basic solution would be global variable with absolute path. But configuration file in fixed location (eg. $HOME/.myProg or /etc/myProg on Unix) would be more robust. Environment (shell) variable is viable too.
Usually if memory is leaking that's usually an issue with the code you wrote and not with the `hmatrix` and `vector` libraries. The most common source of space leaks is doing some sort of fold and not keeping the fold's accumulator strict.
Fixed.
Due to time constraints, I didn't get a chance to cover the application of propagators to linear programming, incremental computation, unification, type error slicing, etc. or to really dive into the various wait-free algorithm tricks at the end, but overall I think this talk gives a good summary of what Propagators / LVars / CRDTs can do and how to steal a bunch of results from different problems that are almost-the-same to make the rest fast.
It's the kind of stuff I was looking for, thanks!
I'd like to see the connection to LP!
Seconded. Are there any papers that discuss the connection between propagators and LP?
an optimization in performing union-find on type variables.
So if I am understanding this, instead of building up a church numeral that runs the whole fold from the ground up, do some evaluation first so that the next continuator is just as complex as the last?
This is to be expected seeing as an OOP object isn't just a record type, it's a recursive record type `μx. { f1 : x -&gt; A, f2 : x -&gt; B, ... }`. Inferring types with a general least fixpoint (μ) is undecidable IIRC. You'll have to do something a little more complicated than just records to fix this.
B-blog posts, please?
Wouldn't it be possible to define the default implementation of `&gt;&gt;` to be `*&gt;` instead of of `&gt;&gt;= const`? I'm in favor of MRP, but I don't see why would that require moving `&gt;&gt;` out of the class.
It also reminded me of an inference process; if the intervals are all supported on the same base interval, adding more can only narrow their intersection (where "interval ~ distribution" and "adding ~ Bayes averaging"). In this case "Top" would correspond to mismatched supports. Furious handwaving going on here, though. 
Thanks.
Is there a good reason for not using typed errors rather than strings? 
The code gets stuck in eternally comparing increasing values of `i` with the length of the list. Imagine an infinite stream of `Nope`, `Nope`, `Nope` ... it doesn't terminate because you didn't _ask_ it to terminate
Why can't objects be simple records of functions? You can accomplish self-reference via closures, it doesn't have to be mentioned in the type.
The problem has already been explained in enough detail by other comments, but I wanted to mention: You probably want `takeWhile` for this kind of thing.
If the source language has a syntax for "this" (or "self", etc.) how do you know which closure it refers to? I'm not sure what you mean
YES! That was the command I was looking for. Thanks!
Nice! Especially if you are able to show the code like that! I have been thinking about trying to get the double underline working. Would be pretty cool :)
The `i &lt;= length list` part filters for every `i` in `[1..]`, which are infinitely many. I.e. `[i | i &lt;- [1..10], odd i]` does also not stop and return `[1]` only. You can remove the guard form the list and prepend `take (length list) [...]` to make it work. Another possible solution: `posTuple list = zip [1..length list] (repeat list)` or `posTuple list = take (length list) (zip [1..] (repeat list))`
Next time you're in Portland let me know. You need to do a Galois tech talk at some point :)
Yeah. I mean, it could definitely get better, and it would be nice if some of ideas from Elm and what /u/augustss mentioned could be ported into GHC, but still, it's not *all that bad*. I'm recently playing with Julia, and its way of reporting type errors is, I kid you not: LoadError: MethodError: `convert` has no method matching convert(::Type{SubArray{Float64,2,P&lt;:AbstractArray{T,N},I&lt;:Tuple{Vararg{Union{AbstractArray{T,1},Colon,Int64}}},LD}}, ::Array{Float64,2}) This may have arisen from a call to the constructor SubArray{Float64,2,P&lt;:AbstractArray{T,N},I&lt;:Tuple{Vararg{Union{AbstractArray{T,1},Colon,Int64}}},LD}(...), since type constructors fall back to convert methods. Closest candidates are: call{T}(::Type{T}, ::Any) convert{T}(::Type{T}, !Matched::T) while loading In[2], in expression starting on line 7 edit: In case you're wondering, Julia's subarrays (views of parent arrays) differ in type based on what was used to index the parent array, and I got that type wrong.
Is that part of the weekly meeting? Would you say those are worth the drive down from Seattle?
EK as-a-service
This is exactly what shameless homework posts deserve. But it's not really pointfree unless you strip out the x and y, is it? sumDifference = (&amp;&amp;&amp;) &lt;$&gt; (+) &lt;*&gt; (-)
Thanks for writing up your thoughts on this. I've actually thought a lot about the same topic. A few of my own thoughts: - I really like using `free`/`operational` for making API calls. - It's really only possible to mock a database if you're using it as a key-value store (in the article, you refer to this as "crud operations"). Mocking the relational parts of a database basically amounts to building a relational database. - You mentioned using a feature of sqlite that allows it to run in-memory. While postgres does not have an equivalent feature, it can be [tuned to trade durability and consistency for performance](http://stackoverflow.com/questions/9407442/optimise-postgresql-for-fast-testing/9407940#9407940). For tests (not for production), this could be useful.
What's the TL;DW (at least until I have the time)?
You can also use more traditional dependency injection, i.e. take the IO action as a parameter. Recently we've gone for this approach, mainly for simplicity and because it was much easier to work into our existing codebase.
Posts like this are incredibly important for the adoption of Haskell in industry. Thanks for the writeup!
&gt; GHCJS can now be used with stackage snapshots via the new compiler field. awww man this is really exciting
Just in case you're not already aware of it, it may also be worth looking at the Rust language for this kind of work.
after months (of daily haskell), it still throws me off. also, I'd prefer a different word choice, maybe involving Inferred: 
Nice! I would suggest marking the `n` in the else branch as well. As is, I was initially confused because the condition shouldn't affect the types of the branches. It does in this case, but only because `n` is returned in the else branch.
The Haddock bug was the last reason I was keeping `cabal` around. If you find bugs in `stack`, report them; the team is fixing them pretty quickly.
In what way are you knocked off balance? Do the words mean something else to you, or do they just not hold any/enough meaning for you?
just going to add my voice here to say that expected/actual did in fact throw me off for many months of daily haskell usage. I can't pinpoint the exact reason why, but I'm leaving this here to let y'all know that this is a real phenomenon confirmed with many people :) 
I like this idea. "Expected" and "actual" still trip me up.
[Alexey Radul's thesis](http://dspace.mit.edu/bitstream/handle/1721.1/49525/MIT-CSAIL-TR-2009-053.pdf?sequence=1) describes an improvement to propagator networks which acts via combination of partial information in the network cells. This "combination of information" property can be attacked from many sides ranging to techniques used in SAT solvers, in CRDTs, or in Datalog and Ed attacks efficient implementation of large propagator networks from all of these angles and more.
That could be arranged.
I mentioned very briefly during the talk that you can view the polytope given by a particular linear programming problem as a lattice under intersection of polytopes, so long as you don't make a cut that affects the actual answer. Basically you gain information on where the actual answer can be. (Mixed) ILP is usually solved by using branch and cut techniques. You use the relaxation of the ILP problem to a normal LP problem, get a conservative lower bound, and then start enforcing the integrality constraints. This requires a form of choice, which goes back to the provenance tracking machinery early in the talk. e.g. your initial LP solution says x = 2.4, so you turn this into two sub problems, one where x &lt;= 2, and one where x &gt;= 3, then solve one of them completely, taking the solution from that and the formula for the valuation function to make a cut on the other. Given all of this we can see that we can view the linear program itself as a lattice, and the orchestration of it as a series of LVish-style steps. Why do I care? Well, we can do (mixed) integer linear programming this way. But we can do it in a way that can readily share variables with any other form of integer constraints, e.g. all-different constraints. This leads to CILP -- constraint integer linear programming, or hybrid optimization. There is a whole conference of CPAIOR stuff out there and a hybrid operations research community devoted to studying these things. Notably other relaxations work nicely on linear programs, such as Lagrangian relaxation which uses Lagrange multipliers to lift large portions of the constraints into the cost function, making them soft. Next up a few other problems start to fit in nicely, in particular when working with interior-point methods (to get good polynomial time bounds) for the LP sub-problems mentioned above, it is beneficial to have access to derivatives. I care a good deal about 'matrix-free' methods for this sort of thing, as there are certain things where the moment I write out Ax &lt;= b, I've already lost. You're paying an O( n^2 ) no matter what once you've given up and gone there. A few months back Dan Piponi pointed me to an article on [forward adjoint oracles](http://stanford.edu/~boyd/papers/pdf/abs_ops_iccv.pdf) which give a way to evaluate the primal, derivative, adjoint, etc. for these matrices without actually forming the matrix. I just need AD to make that work. This can be useful for things where you could factor A = BC, where the intermediate rank is small. In the middle of the talk I mentioned briefly that if you switch from idempotence to a distributive lattice to get your evaluation guarantees that you can make automatic differentiation fit into a propagator-like setting. That can be useful or evaluating the forward adjoint oracles. Basically I'm fishing around for solutions where I can abuse the fact that I'm in a functional language to work with functions rather than matrices. Trying to play to our strengths, rather than have to go yak shave decent BLAS bindings. ;)
Yes, this is great!
I had the same experience. Here is some *actual* `ghci` output: λ&gt; "" :: Int &lt;interactive&gt;:2:1: Couldn't match expected type ‘Int’ with actual type ‘[Char]’ In the expression: "" :: Int In an equation for ‘it’: it = "" :: Int I find that my brain processes the following with much more ease: λ&gt; "" :: Int &lt;interactive&gt;:2:1: Type mismatch: ‘""’ has type ‘[Char]’, but is expected to have type ‘Int’. In the expression: "" :: Int In an equation for ‘it’: it = "" :: Int This also shows the subject term `‘""’` *before* its actual and expected types. Note that I'm not a native English speaker. Maybe this is part of my confusion.
Check out [SBV](http://leventerkok.github.io/sbv/) - it is absolutely awesome! That could be a reason for you to use Haskell. 
I recently did some work in there to categorize errors by importance, and it didn't seem too bad to me. I'd say the basic problem is that someone needs a specific plan. Once you have that probably the most complicated thing is plumbing in extra information you might want, like source context, but that's independent of the messages starting out unstructured. Actually they're already fairly well structured by the function that generates each fragment. Then the most annoying thing is that if you change the messages, thousands of tests break. If you put it behind a flag you can at least put off that problem.
Check out how Elm is doing it: The type annotation is saying: [[ some type ]] But I am inferring that the definition has the type: [[ some other type ]] Incredibly clear and informative. http://elm-lang.org/blog/compilers-as-assistants
Thank you very much for coming back and saying so, this'll make Julie's day! Would you be willing to write up a little blurb for the [reviews page](http://haskellbook.com/feedback.html)?
Thank you. It did make my day to hear from you again. Please let us know if you have any comments, questions, suggestions, etc :) 
It is pretty effortless now, but takes a long time to install: cabal install http://ghcjs.luite.com/master.tar.gz ghcjs-boot --dev 
Okay, that's explains it. Thanks everyone for helping me out, and for the suggestions for alternatives. Really appreciated.
Very handy, really appreciate your help.
Thanks, thats helps, and shows me a new function to boot.
Just today I was thinking of how skeptical I was of Stackage and Stack. Seemed to me there wasn't much wrong with `cabal` and raw Hackage. Now I realize there was a lot wrong with it; I had just grown used to fixing the problems. But I suddenly realized how much of an improvement `stack` was when I was (recursively enough) building `stack` using `stack`. The dependency graph had over 100 packages in it. In the `cabal` and raw Hackage days, I would have crossed my fingers and hoped something like this could build. Some things would probably break along the way, and sometimes I could fix it, sometimes not. Now, with `stack`, I don't cross my fingers. I expect it to build. And it does. So in a short time I went from a skeptic to someone who can't remember how I ever got along without these tools. If you're wondering if you should get on board, do it. oh - and `stack upgrade` worked--not by downloading a binary, but by fetching the `stack` code from somewhere and building it using the old `stack`.
Definitely. I still find "actual" confusing if it's a tricky error that's not immediately obvious looking at the referenced line number.
Yes it should be, go for it
&gt; It's really only possible to mock a database if you're using it as a key-value store (in the article, you refer to this as "crud operations"). Mocking the relational parts of a database basically amounts to building a relational database. You definitely have a good point here. Mocking something that is complicated can take a lot of time! It may not make business sense to take the time to do something like this. Thanks for the info about postgres! I'm sure some people would be able to use this for their tests.
What if there is no type annotation?
I would prefer it downloaded a binary. What is the advantage of compiling locally?
Note that you need `cabal-install &gt;= 1.22.4` in order for ghcjs-boot to work properly. This is one of the things that `stack setup` ensures.
To prove a point
I have no issues with the actual words used. But I feel that the order of types should be reversed. Something like - &gt; True == (True, "abc") Couldn't match type ‘(Bool, [Char])’ with expected type ‘Bool’ In the second argument of ‘(==)’, namely ‘(True, "abc")’ In [..omitted..] Instead of - &gt; True == (True, "abc") Couldn't match expected type ‘Bool’ with actual type ‘(Bool, [Char])’ In the second argument of ‘(==)’, namely ‘(True, "abc")’ In [..omitted..] 
Putting this in my stack.yaml just works for me (with preinstalled ghcjs binaries on my path) - compiler: ghcjs-0.2.0_ghc-7.10.2 
Yes, but you could also try Prolog!
Agreed! I have several old private projects with multiple old dependencies. With stack I am always able to build each project independently (and not by deleting all ghc packages and starting from scratch). Once you get used to it, going back to cabal is difficult, and unnecessary. 
The single biggest improvement that would help me, would be to have the actual types coloured differently when printed in the terminal. A wall of text with potentially complicated types included inline is very difficult to parse quickly.
My understanding is that: - "Expected" means "the type inferred from the environment that the expression exists in" and - "Actual" means "the type inferred from the components that make up the expression". Still, I always have to think twice when I read the error messages, perhaps because the two words aren't too "contrasting" in my lexicon. In an ideal world, I would prefer if the error also stated the *origin* of the given type construct that conflicts with the other type construct. For example, in [hexagoxel's example](https://www.reddit.com/r/haskell/comments/3tm3lv/proposal_expectedactual_requiredfound/cx7g580), the expected type `Bool` arises from the `True` on the left-hand side, while the actual pair type `(_, _)` arises from the pair constructor `(_, _)` on the right-hand side. &gt; True == (True, "abc") Type mismatch in expression: True == (True, "abc") ^~~~~~~~~~~~~ Expected: ‘Bool’ True == (True, "abc") ^~~~ Actual: ‘(Bool, [Char])’ True == (True, "abc") ~ ^ ~
To get a prereleased version of stack. 
I agree that changing the structure might help more than any rewording. However the challenge is finding something that remains readable when all involved types/expressions are large - it is hard to reproduce a full example; currently the output in one such case roughly looks like: Couldn't match type ‘Foo’ with ‘Bar’ Expected type: [.. 9 lines in total ..] Actual type: [.. another 9 lines ..] In the first argument of ‘someFunction’, namely ‘someOtherFunction’ In [..] `Foo` and `Bar` are short here, but they are only one little part of the type for the involved expression. You certainly would not want to repeat the 9 lines, and restructuring this case in general seems to make it worse. Now, you still could 1) Special-case the "short" cases. I am not sure about this one, because consistency is probably more important than any wording/structuring of the errors. 2) Make the connection more clear. One idea is adding a binding, i.e. `Actual type: t_found = [long stuff]` and then `namely ‘someOtherFunction’::t_found`, just to make this connection more clear. I like the /u/peterjoel suggestion of color-coding this as well. Or maybe there is a different structure that still works with complex cases.
Thanks for posting this! When writing this blog post, I was hoping someone would call me out on this. I'm wondering what other people think about this? I think from previous discussions that /u/Tekmo is a big proponent of no lawless typeclasses? I'm wondering how he and others think about this?
I am not sure about this one. Apart from any personal preference, the problem really is that this change would be confusing to all the people used to the current order (while replacing one/two words probably requires much less mental change for the users). And you can make arguments for the current order, e.g.: The "expected" part contains the more interesting information, as the "found" type can often be inferred by the user already; So it should come first.
I like the idea of colour as well. Maybe just coloring the actual type and the problematic expression should be enough ...
It also means that if you built your own `stack` binary on a platform that we don't provide binaries for, you can still use `stack upgrade`. That said, there are plans to provide a binary upgrade path as well: https://github.com/commercialhaskell/stack/issues/1238.
You can have laws without typeclasses; you can have typeclasses without laws. [Arbitrary](http://haddock.stackage.org/lts-3.14/QuickCheck-2.8.1/Test-QuickCheck.html#g:10) has no laws but it's still useful. I view typeclasses as offering ad-hoc polymorphism, nothing more. You can attach laws to a typeclass but that's not always necessary.
I think a good complement to "Inferred" would be "Constrained" ghci&gt; "" :: Int Error: "" has been inferred to have type [Char], but has been constrained to have type Int And as long as I'm dreaming ghci&gt; "" == True Error: "" has been inferred to have type [Char] and True has been inferred to have type Bool, but == constrains its arguments to be of the same type. 
The reason that type classes should have laws is you can't reason about code that is polymorphic over that type class without those laws. Let's take this code, for example: example :: MonadError e m =&gt; (e -&gt; m b) -&gt; m b example f e = throwError e `catchError` f Let's use the `MonadError` class as an example. You intuitively expect that the above code is equivalent to: example = f e ... but the only reason you know that is because of the laws for the `MonadError` class. Without those laws you would have absolutely no way to prove that the above code transformation is valid without inspecting the source code to every single `MonadError` instance ever written. The reason that type classes should have **mathematical** laws, specifically laws from category theory and abstract algebra, is that those laws lend themselves to much simpler proofs about code. I [wrote up a post](http://www.haskellforall.com/2014/07/equational-reasoning-at-scale.html) and [gave a talk](http://www.techcast.com/events/bigtechday8/maffei-1450/?q=maffei-1450) about how these mathematical laws make it much easier to compose smaller proofs into larger proofs. The `MonadError` class is also an example of a mathematical set of laws. The laws happen to be: throwError e `catchError` f = f e m `catchError` throwError = m (m `catchError` g) `catchError` h = m `catchError` (\x -&gt; g x `catchError h) If you define: (f &gt;|&gt; g) e = f e `catchError` g ... then the three `MonadError` laws are just identity and associativity laws: throwError &gt;|&gt; f = f f &gt;|&gt; throwError = f (f &gt;|&gt; g) &gt;|&gt; h = f &gt;|&gt; (g &gt;|&gt; h)
No, current release snapshots still missing XHR. That's my point. There are still too many bugs in ghcjs to rely on releases. 
"Like any other language, there is no straightforward way in Haskell to test code that does IO" I'm considering learning Haskell, and this took me by surprise, because it seems like an obvious necessity for building good software. Is it true? If so, why? Edit: I develop in Python primarily, I'm not sure what's not straightforward about testing IO there. Also, in the specific case of DynamoDB, which is mentioned in the article, Amazon provides an emulator you can run locally on your machine, which works very well.
What happened to 0.1.7.0?
It would be more convenient if there were: * a stack new template that sets up a yesod project with ghcjs on the client side * some way to call "stack test" once on such a project and have it configured to compile everything and do all the right things * some way to run yesod devel such that code changes, including the ghcjs portion, are hot swapped with near-instant compile times For example, with clojure and clojurescript this is possible like so: * lein new figwheel # new project template * lein test # to test * lein uberjar # to compile to a .jar * lein figwheel # very fast live code reload [edit] Another amazing convenience that comes with lein figwheel is a clojurescript repl that automatically connects to your browser. You can write (js/alert "hi") in the repl and the browser will pop up that alert. You can inspect the state of refs and manually invoke callbacks. It's amazing.
Like /u/Fylwind said: there is context (top expression) like function, type constraint, `if` statement which expects/requires some type and then there is it's argument (bottom expression) like function argument, constant, `then` + `else` which represents type that was actually supplied to the context. Maybe emphasis on what is top/bottom expression would help more than simple change of words expected/actual.
What is the feeling on generating compile messages that are machine parsable, such as with json. Then we simply write filter scripts to present those messages in as comprehensive or simple or alternative a way as we like. Would be easier to integrate with editors and ide's too.
Looks like this post linked to the changelog rather than the [release announcement](https://github.com/commercialhaskell/stack/releases/tag/v0.1.8.0), but in there it says: &gt; Note that, starting with v0.1.8.0, releases of Stack will always have an even-numbered second-to-last version component. Odd second-to-last version components are reserved for unstable builds.
Do you find Hugs message better? Main&gt; True == (False, "abc") ERROR - Type error in application *** Expression : True == (False,"abc") *** Term : True *** Type : Bool *** Does not match : (Bool,[Char]) 
&gt; I read in the documentation of vector that "unboxed vectors of pairs are represented as pairs of unboxed vectors." and it let me imagine that it exists a way to have an actual memory representation different from what the type describe. For example, I'll be really happy to switch implementation from structure of array to array of structure during compilation. [Unbox](http://hackage.haskell.org/package/vector-0.11.0.0/docs/Data-Vector-Unboxed.html#t:Unbox) instances are customarily built on top of [Prim](http://hackage.haskell.org/package/vector-0.11.0.0/docs/Data-Vector-Primitive.html#t:Prim) which doesn't export its methods, so you can't write new instances without forking `vector`. You could in write your `Vector Unbox foo` and `MVector Unbox foo` instances without using `Data.Vector.Primitive` `Vector` and `MVector`, but if you do want them to be implemented like instances from the library, the only way you can do this is by combining existing `Primitive` vectors in a structure. On the other hand, `Storable` forces you to use arrays of structures, as the instance must be able to pack the whole data structure into contiguous memory, and can't "know" that you're using it to implement vectors later on.
Agreed. Actual is vague and doesn't clarify much. I'd love to see Expected and Found. (I'm not so fond of Required though; it feels wrong to me.)
Effort, mostly. If somebody took ownership of the problem and made a type hierarchy for the errors it could be fixed in GHC 8.2.
There are also cases of useful Applicatives that do admit a Monad instance, but one that is kind of useless. I'm thinking of [L'](http://hackage.haskell.org/package/folds-0.6.3/docs/Data-Fold-L'.html) from **folds** and the [Fold](http://hackage.haskell.org/package/foldl-1.1.2/docs/Control-Foldl.html#t:Fold) datatype from **foldl**. And I suspect the [folds with monadic effects](http://hackage.haskell.org/package/foldl-1.1.2/docs/Control-Foldl.html#t:FoldM) in foldl do not admit a monad instance at all. (Edited for clarity.)
I was thinking of [FoldM](http://hackage.haskell.org/package/foldl-1.1.2/docs/Control-Foldl.html#t:FoldM), the "fold with effects in a monad". They are Applicatives themselves, but I don't see how to define a monad instace for them.
My usual approach is to start with `Applicative`. Show how well they compose. Give examples of how you can use them, like `Validation`, `ZipList`, etc. Then show how these compositions are limited by not letting you pick what to do next based on what you've seen so far. This gives a motivation for considering monads, where the `(&gt;&gt;=)` can change out the entire future of computation based on past results. Now I can start to show how certain examples drop away. (`Validation`, `ZipList`, `Const m` `Compose f g`) and this then motivates the exploration of transformers and effect systems to address the composition problems. Jumping off the theory deep-end: If the audience is category-theory inclined I can trot out that Applicative can be built in both a covariant form like it usually is, but also contravariantly (unlike Monad). If the audience is REALLY category-theory-savvy then I can show how they are "monoids in the category of endofunctors" just like monads, but with respect to a different tensor. http://hackage.haskell.org/package/contravariant-1.3.1/docs/Data-Functor-Contravariant-Divisible.html 
You already seem to be most of the way there. If you are using `System.Random`, you are doing it in a pretty idiomatic way. There are better ways, my favorite of which is `random-fu`, but that has a significant learning curve behind it (probably worth going through if you want to work on random heuristic searches like genetic algorithms though). The most suspect thing is `chordFromKey`, passing a list in to consume only exactly four elements of it loses type safety compared to simply having a four argument function. Also, you probably want to make `Chord` strict in its fields (it might be better if `Note` isn't). Cale Gibbard's old library is easy to pick up though, `MonadRandom`. Checking in on it, it is still being maintained by Brent Yorgey. Using `MonadRandom`, you can implement `randomNote` something like this: --Choose 1 of 88 piano keys randomKey :: (RandomGen g) =&gt; Rand g Int randomKey = getRandomR 1 88 --Choose 1 of the 12 keys on the first octave randomBassKey :: (RandomGen g) =&gt; Rand g Int randomBassKey = getRandomR 1 12 --Choose a random Note randomNote :: (RandomGen g) =&gt; Rand g Note randomNote = fmap noteFromKey randomKey --Restrict the search space by specifying only consonant chords --Randomly generate a chord via mapping to piano keys randomChordKeyOffsets :: (RandomGen g) =&gt; Rand g (Int,Int,Int,Int) randomChordKeyOffsets = uniform [ --These probably need to be padded with some octaves to get into the right ranges (0,4,7,12), -- Major triad (0,3,7,12) -- Minor triad --More chords here ] randomChord :: (RandomGen g) =&gt; Rand g Chord randomChord = do (b,t,a,s) &lt;- randomChordKeyOffsets k &lt;- randomBassKey return (chordFromKeys (k+s) (k+a) (k+t) (k+b)) -- you would need to change chordFromKeys, but this is better anyway To use `Rand` you can either supply a seed with `runRand` or get a seed from the environment with `evalRandIO`: randomChordFromSeed :: Chord randomChordFromSeed = runRand (mkStdGen 42) randomChord randomChordInIO :: IO Chord randomChordInIO = evalRandIO randomChord Generally though, you only want to use these at the last minute. It is better to just `fmap` functions over your random values to generate new random values. To progress further down this line of thinking, you are probably going to want efficient shuffles of low level arrays. This is not trivial for a Haskell newcomer used to other languages. I would recommend you look into the `vector` and `random-fu` libraries at some point and get comfortable with them. I would share some of my old code for stochastic searches in Haskell but I don't think I have it anymore. Eventually, I switched to C (mostly because of CUDA).
This is contrasted with testing pure code in Haskell where it's generally speccy/unit tests, property tests, model checking, or a proof. There are a lot of options for stuff that has IO (it has no denotation, so the former options are narrowed down to unit tests), many of them are still very good though. I _love_ testing code in Haskell vs. what I had in Python.
The changes to Core are the basis of the kind equalities work by /u/goldfirere, as part of the plan to allow dependently-typed Haskell. See the [wiki page](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell/Phase1) and [code](https://phabricator.haskell.org/D808).
I've been wondering for a while, but why does stack install binaries in `.local/bin`? As far as I can find no other program uses that path, so why not just put the bin dir under `.stack/`?
For database testing in Haskell would you tend to mock the database (which essentially seems to be one of the approaches in the article) and then at some point also run the tests against the actual DB you'll be using in production? It seems you have to always do the latter, while the former maybe gives you some speed advantages, at the cost of not being a "real" test. If it's not really easy to do both I'd probably err towards just testing against a real DB.
The install guide has [a section on paths](https://github.com/commercialhaskell/stack/blob/master/doc/install_and_upgrade.md#path), which links to [issue #153 where this was discussed](https://github.com/commercialhaskell/stack/issues/153).
I use `stack` rather than `cabal-install`. I agree that is better for most use cases, but your comment is not in good taste. Besides, `stack` still uses `cabal-install` for dependency solving when you use packages that aren't on stackage, so you're likely still using that solver whether you appreciate it or not.
Can I ask a question? As I understand, stackage is a coherent subset of hackage that works well together. Being a subset, shouldn't cabal be able to reproduce this stability using the dependency solver, at least in a single install basis? Or is there something else I missed?
Obviously, in the case of monads, the tensor is functor composition. Which is it in the case of applicatives?
Agreed, though GHC's extensions are still a bit overwhelming for me due to their abundance and inter-extension interaction. On a related note, the thing that's so appealing to me about Idris is that its type system is pretty simple and really consistent, yet allows really powerful expression. I only wish it was at a similar level of maturity to GHC in terms of optimizations and libraries. People frequently ask (in the talks I've seen) if it would be possible to have a Haskell &lt;-&gt; Idris bridge, and the answer is, sadly, not in any reasonable way. However, I have heard that compiling Idris to Haskell would be relatively easy, and then one could manually plumb things together.
[Have some refactoring! Of a single function. Because I was bored.](http://lpaste.net/145747) The first version is yours. The do block opened in line 24 is in the Maybe monad, to rid you of those matches against Nothing. (sequenceA_ Nothing is return (), sequenceA_ (Just (print 2)) is print 2.) The third version gets rid of all those pesky names. (It requires that pixelize's main argument be passed last, and suggests that it should be!) All is untested. (&gt;&gt;&gt;) was used instead of (.) to preserve the general look of the snippet. Feel free to turn the operators around and stuff.
You can also do `stack install` instead of `stack build` to have the executables copied to `~/.local/bin`, so you don't have to go spelunking in `.stack-work`.
Control.Monad.filterM operates on a list though, perhaps maxigit wants to filter a Seq rather than use its monad instance?
`filter` isn't part of Foldable or Traversable. Foldable is far too weak because the only thing it affords you is to iterate over the structure once, consuming it in the process. Traversable is a bit more powerful, as it preserves your structure as-is. Both are too weak to capture the notion of filtering, however, as that would involve selectively dropping elements. To my knowledge, no typeclass exists which captures the notion of a "filterable structure", thus the best you can do is hope that the library author has implemented a filter for whatever you're using. That being said, you can write filterM for Seq yourself, using `|&gt;` - just re-create the data structure as you iterate over it. Adapting from `filter`: {-# LANGUAGE LambdaCase #-} filter :: (a -&gt; Bool) -&gt; Seq a -&gt; Seq a filter p = foldl (\ xs x -&gt; if p x then xs |&gt; x else xs) empty filterM :: Monad m =&gt; (a -&gt; m Bool) -&gt; Seq a -&gt; m (Seq a) filterM p = foldlM checkPred empty where checkPred xs x = (\case{True -&gt; (xs |&gt; x); False -&gt; xs}) &lt;$&gt; p x 
Awesome! I've been wanting something just like that.
One can implement filter with `MonadPlus`.
A generalized `filterM` does sound like a good idea. A Traversable or Foldable constraint, however, would not be sufficient, since Traversable cannot alter the shape of the container nor the number of elements, and Foldable can extract elements but cannot reconstruct a container out of individual elements. I'd go with [IsList](https://ghc.haskell.org/trac/ghc/wiki/OverloadedLists) instead: {-# LANGUAGE OverloadedLists, TypeFamilies #-} import Control.Monad (filterM) import GHC.Exts (IsList(..)) -- | -- &gt;&gt;&gt; import Data.Sequence ((&lt;|), empty) -- &gt;&gt;&gt; myFilterM (return . even) (1 &lt;| 2 &lt;| 3 &lt;| 4 &lt;| Data.Sequence.empty) -- fromList [2,4] -- -- &gt;&gt;&gt; import Data.Set (insert, empty) -- &gt;&gt;&gt; myFilterM (return . even) (insert 1 $ insert 2 $ insert 3 $ insert 4 $ Data.Set.empty) -- fromList [2,4] -- -- &gt;&gt;&gt; import Data.Char -- &gt;&gt;&gt; import Data.Text -- &gt;&gt;&gt; myFilterM (return . isAlphaNum) (pack "hello world") -- "helloworld" myFilterM :: (Monad m, IsList fa, Item fa ~ a) =&gt; (a -&gt; m Bool) -&gt; fa -&gt; m fa myFilterM p = fmap fromList . filterM p . toList The constraint is the complicated-looking `(IsList fa, Item fa ~ a)` instead of the simpler `IsList f` because `IsList` needs to support monomorphic containers, like Text which can only contain Char.
Thanks for the ideas! &gt; a stack new template that sets up a yesod project with ghcjs on the client side That should be imminently feasible, PRs welcome. Though, to work out of the box, the project template mechanism would probably need to be extended so that "stack init" doesn't pick up the client cabal file and add it to the server's stack.yaml. There is already a stack template, but there's a bit of a hiccup getting it to init: https://github.com/commercialhaskell/stack/issues/1390 &gt; some way to call "stack test" once on such a project and have it configured to compile everything and do all the right things This should just work. If you have a GHCJS test-suite, it will get run. &gt; some way to run yesod devel such that code changes, including the ghcjs portion, are hot swapped with near-instant compile times Well, it's far from near-instant, but this is supported by https://github.com/fpco/stackage-view/blob/master/server/Yesod/GHCJS.hs , which really ought to get packaged up. &gt; Another amazing convenience that comes with lein figwheel is a clojurescript repl that automatically connects to your browser. Yes, the [ghcjsi branch](https://github.com/ghcjs/ghcjs/tree/ghcjsi) supports some of this, it'll be great when it's a standard part of GHCJS.
Is be very interested indeed to see what it comes out with, when you get there. Please do spill on /r/haskell when you have partial results. 
In fact, this already exists in `Control.Monad` as [`mfilter`](https://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Monad.html#v:mfilter): mfilter :: MonadPlus m =&gt; (a -&gt; Bool) -&gt; m a -&gt; m a mfilter p ma = do a &lt;- ma if p a then return a else mzero It's not hard to generalize that to a monadic predicate: mfilterM :: MonadPlus m =&gt; (a -&gt; m Bool) -&gt; m a -&gt; m a mfilterM p ma = do a &lt;- ma b &lt;- p a if b then return a else mzero
I've never tried teaching this, but how I think of it in my head is that `Functor` provides `fmap`, `Applicative` provides `pure`, and `Monad` provides `join`. Is this too simplified? Oh wait... `&lt;*&gt;` has no default definition, so it is an integral part of `Applicative`, right? And the reason for that namesake. The above simplification seems to work if you're just talking about the parts relative to `Monad`...? Also, at this point I wish `ap` wasn't specialised to `Monad` - I like having words for all operations.
When you expose the type system to the programmer they have the ability to describe what their data should look like and, depending on the language, describe what their program does. Then the compiler has some idea of what is "wrong" with the program and can show the difference between the implications of the description and the code. This comparison is critical in giving meaningful errors. Unfortunately, all languages that do not give you a way to describe what your program does besides the implementation will not be able to give clear answer to what is "wrong" with your program. This is also true if writing that description is difficult or not required. Java has the ability to describe what your program does, yet is still criticized for non-descriptive errors since you have to go out of your way to use those features. Haskell's type system, in my opinion, does an incredible job of integrating the description of your program with your code. That way you don't feel like you are describing what your code does on top of writing the implementation. This is the magic of type systems. Since Haskell is side effect free and pure, you can always derive the type and make the assertions you are looking for. However, you would be forfeiting the compiler's ability to give helpful errors.
Haskell does an admirable job. I’m pretty squarely in the static-types camp, so I’m not *really* arguing against type systems, but I’d certainly like to see an even tighter notational integration between specifications and implementations, and QuickCheck is one good example of that. With term-level assertions that the compiler can verify, you could still infer types and present warning messages accordingly. For example: &gt; This line may cause runtime errors because these assertions don’t fully specify the type of `f`, which I inferred to be `…`. You can cover this case by adding the assertion `…`. (Similar to `case` exhaustivity analysis.) &gt; This line may cause runtime errors because I can’t verify `length xs = length ys` at compile-time. Use `debug_assert` or `always_assert` to specify whether this assertion should be run only in debug mode, or always. 
Day convolution, which can be written as data Data f g a where Day :: ((a, b) -&gt; c) -&gt; f a -&gt; g b -&gt; Day f g c or since Hask is closed, you can get something that looks a bit more like (&lt;*&gt;) than liftA2. data Day f g a where Day :: f (a -&gt; b) -&gt; g a -&gt; Day f g b 
I think Template Haskell might allow you to query the compiler about the module and then generate the required lenses. If not, you can parse the Thrift file and generate the lenses separately, but that's a little extra work. You can use my [language-thrift](http://hackage.haskell.org/package/language-thrift) library to parse the Thrift files if you end up going that way. "Export everything in this module" is done by leaving the import list out. module Foo where -- exports everything defined in Foo If you need to re-export things you imported as well, you can do, module Foo (module Foo, module Bar) where import Bar -- exports everything defined in Foo and everything imported from Bar.
Super exciting stuff! Glad to see all the hard work Matt has put into ghc-exactprint is starting to pay off concretely!
How is it possible that compiling Idris to Haskell is an easy task, while a bridge between the two is almost impossible? It seems to me that the two are basically equivalent. I actually don't know the Idris language, but I am curious about your statement.
It's be nice to have a screenshot in the GitHub README.
I'm a native English speaker. Your way is much more natural/fluent than the compiler's, making it easier to follow. 
I wasn't necessarily trying to generalize it. I was just surprised `filterM` wasn't defined for `Seq`, therefore I wondered if it was defined somewhere else as a generalized way. However as /u/mstksg pointed, it is `filterA` from the `wither` package (with a Seq instance). Anyway, generalizing it is a good idea. In which package should we put it ?
`filterM` works on list, not on `Seq`. I need `Monad m =&gt; (a -&gt; m Bool) -&gt; Seq a -&gt; m (Seq a)`.
+1 on the yesod scaffold, would be great to see you folks who know what you're doing set the basic "best practice" template for the rest of the users Also, isn't yesod devel deprecated with stack for the time being or did it get fixed?
I'm slightly confused that there is no mention of the [Spock](https://www.spock.li/) web framework, but several others that I've never heard of. On the other hand, I never thought there was that much Haskell happening in Munich!
&gt; How is it possible that compiling Idris to Haskell is an easy task, while a bridge between the two is almost impossible? Compiling Whitespace to Haskell is likely trivial. Now try the other way around.
I agree with you. I'm not pushing to get a generalized `filterM`. I'm just suprised that vector provides it but seq doesn't. I then thought it wasn't implemented for `Seq` not because nobody needed it, but more because it was already available somewhere else.
Of course. Fixed .
&gt; For database testing in Haskell would you tend to mock the database (which essentially seems to be one of the approaches in the article) and then at some point also run the tests against the actual DB you'll be using in production? I think there are trade-offs that will be made between each approach. &gt; If it's not really easy to do both I'd probably err towards just testing against a real DB. This will probably give you tests that more reliably catch bugs. However, it's always possible that you are using a database that you can't easily test against.
Thanks for this! I'll update the blog post.
&gt; It's not hard to generalize that to a monadic predicate True. But I'm getting tired of pasting that over and over again into my code.
Super cool. Excellent work.
Thanks for pointing this out. I did not know about it, but it looks better than `filterable`, which I had previously used for this kind of thing.
Editor here. People working on Spock never submitted an entry. I would be delighted to include it in the next edition, if they send something. :)
Interestingly, I just saw that Ch. 5.2 of A.Radul's thesis mentions probabilistic programming and the canonical "lawn sprinkler" example from the directed graphical model tradition : (https://projects.csail.mit.edu/church/wiki/Simple_Generative_Models#Bayes_Nets_in_Church). 
Most Windows developers I know don't do that. It's true that Windows tools generally do fine with Unix line endings, as they do with Unix path separators. But that's not what the tools do by default, so that's not what most people do by default either.
Are you thinking that somehow "type classes" in the Idris sense are indisputably more sophisticated than Haskell type classes? 
We write loads of selenium tests at work using one of the js frameworks (mocha, jasmine, stuff like that) and they use ANSI color codes (I think) to delineate lines, which is actually kinda nice. When we had a wrapper in Python to run this stuff, I found that it swallowed a lot of output, so I wrote a handler that looks for the colored lines and spits them out, but I always wondered how universal this is. It looks great on terminals in OSX or Ubunty, but on Jenkins, for instance, it displays in a web interface as plain text and is actually harder to read. I like this suggestion and it is does not sound overly difficult (but I really have no idea), but it would be criminal to make the error messages harder to parse in certain environments.
Type classes in Idris are different than type classes in Haskell. Idris allows multiple instances of the same class for the same type and allows you to use names do disambiguate which. Haskell, baked really deep into the semantics, tries to ensure that there is only one instance, and behaves very weirdly if it manages to compile when that's not the case (because it made assumptions that were violated).
I'd say the [authentication changes](https://www.spock.li/2015/08/23/taking_authentication_to_the_next_level.html) are pretty exciting, but I guess the authors missed/forgot about the report.
I'll try to ping them for the next report in March but I might forget (I'm human in the end) so it would be better if more would ping them to send updates :)
Wrong sub.
Well in the case of that particular typeclass, it's not very difficult. You just need: class OrdWRT (w :: k) t | t -&gt; k where compareBy :: p w -&gt; t -&gt; t -&gt; Ordering ... You could also remove the functional dependency if you want the kinds of ordering that are possible to be open, but when transpiling that doesn't matter really. I'd think it would be much harder going the other way, because there are valid Haskell programs that are not valid Idris programs.
I am pretty sure he won't (implement such a typeclass), but abstract differently.
I proposed the AMP and I also wrote that sentence. I had the conversation of how Applicative and Monad are related numerous times on IRC over the course of roughly 3 years now. These were mostly "advanced beginners" who had read about monads and applicatives already, and were a bit confused by when to use which one. When teaching (on IRC and in person) I introduce Functor, because the "map" operation is sufficiently well-known among non-functional programmers now. I show how many other things than just lists have a related "map" operation, and Functor abstracts over that. I then skip Applicative and go to Monad, just briefly mentioning that there's this intermediate class that we'll not concern us with for the time being. Then it's monads time; afterwards, I usually talk about how Monad is often "too much" and we can use a sometimes more efficient class that allows useful abstractions without the full monad power, and that's Applicative.
To be honest, I wasn't sure about posting it or not. Coq is written in OCaml so the Ocaml subreddit might have been more appropriate ;-). However, lots of Haskeller are aware of language surch as Idris, Agda and Coq and might be interested in how they can actually be used in real mathematical proof and there is not much happening on /r/haskell today ...
I got the impression that he never really nailed all that stuff down and made it rigorous. It is hard to describe the lattice involved from propagating the observation to update you from a prior to a posterior distribution. You can do something similar to emulating a CmRDT with a CvRDT by tagging all the observations and tracking if you've updated the distribution with respect to each to get back into a lattice formulation though.
This is a sub with a high concentration of people that would be interested. A lot of people in r/math wouldn't care, ocaml is less active, and there is no r/coq as far as I am aware. So it is perhaps the wrong sub but there isn't a better sub and I'm glad it was posted.
/r/coq does exist but that does not mean that coq material cannot be mentioned anywhere else on reddit. ;)
The implementation language is not a strong argument (otherwise almost all software would be discussing in the C or C++ subreddits), but next time feel free to post in r/haskell *and* in r/ocaml -- everybody can have the nice things.
I think that's a failing of most Windows developers - I certainly have a script that detects and throws errors on \r in my source files, and develop almost exclusively on Windows. Source should be cross-platform as far as possible, and given the tools out there, that means picking \n as the line ending.
I think that it's plain that Idris's type classes are less magical.
I appreciate it having been posted here.
This is really cool! Now that Haskell is moving towards dependent types, we need to be thinking about what can be done with DT and what kinds of techniques there are. So, thanks for posting this!
Could be. But I'm not here to evaluate Windows users. I just want to be able to use cool tools like this, so I'm looking at reality. As long as we have even a single developer on our team who happens to use Windows, and happens to use tools in the usual way for that platform, this tool is useless if it wrecks all source files created by that developer. And in fact, we have more than one developer like that. Some of those developers are not part of the Haskell team - yet. But they have learned some Haskell on their own and gotten up the courage to make some small changes to the Haskell code. I want to encourage that, and make them feel good about it. I don't want to nitpick them about how they use their tools, or back out every change they commit.
Just write one and then you will know where it is.
Thanks. Just to clarify this paper is a few years old. I just stumbled upon it today and thought it might be of interest.
I know MS was pretty bad about that back in e.g. VC++ 6 days. If you had unix line endings and added a line in their IDE that line was the only line in the file with a windows line ending. I believe they fixed that in more recent versions though but it was a pain back then.
Sure thing. I'll send you something once I'm finished reading it.
What exactly happens with CPP? [ghc-exactprint](https://github.com/alanz/ghc-exactprint) has been described as "Does not process CPP properly [should be sorted soon]" for a while, and this blogpost talks about "a few known problems with CPP". Does that mean CPP mostly works (by which I mean is reproduced faithfully) with some edge cases that don't, or that it doesn't work at all?
The short answer is: free monad The longer answer is: none of the above. I feel like any abstraction that explicitly exposes the notion of database is too low-level because you can't equationally reason about operations over databases. I'd prefer something that deliberately oversimplifies the database API but provides some guarantees that restore equational reasoning.
I was under the impression that, because Haskell isn't total, and because there's no universe polymorphism, that Haskells type system is inconsistent. These are compromises that make it practical for real life programming and verifying code, but probably not for purely mathematical proofs. 
Leksah does something like this. Put the cursor in some code HLint has suggested changing (or select a block of code containing more than one suggestion) and press Ctrl+R (Command+R on OS X) to "resolve errors". If there are no errors anywhere to resolve it will apply the selected HLint suggestion. I think apply-refact will work better, but the naive text replacement approach Leksah currently uses works fairly well.
Thank you :)
But if we take `m ~ Seq`, `mfilterM` is surely not what he wants. The predicate should return a Bool packed into a second, independent monad.
Somewhere in the middle. There are two problems really, 1) In order to process CPP correctly you need lots of non-local information. For instance, the location of header files, user-defined macros etc. This makes processing individual source files hard. 2) Even with all this information, CPP can do weird things to source files which makes it very hard to put all the processing directives back in afterwards. We make an effort to try to run the preprocessor which can resolve simple macros but in most cases it will need some additional information such as the "cabal_macros.h" file to do the preprocessing correctly. So I would say that if you definitely want to be able to use tooling then try to avoid using CPP. The best way in my opinion are the `*-compat` packages as then the CPP is limited to just one file you won't touch very often. It really is a shame that CPP is so prevalent, I second /u/kamatsu's suggestion that it is one of the worst things about Haskell.
Couldn't you just run unix2dos after running this tool?
Very cool ! Could you elaborate a little on the magic sauce? Where is GHC running? How do you make it talk to the browser? How does the page react to user input?
This is a great library! However, if the author (OP) is looking for a bit more challenge then it's not so suitable. For a program verification course we had design and implement a verification engine for Dijkstra's Guarded Command Language (GCL) with it's 'weakest liberal precondition' algorithm. This was also doing SMT problems, which we tackled with writing out a file that can be fed into Z3: https://github.com/Z3Prover/z3/wiki Doing something likes this could be a nice intermediate approach for the OP.
thanks (-: Note that we are generally open to remote work, physical presence is just our preference. I would still be interested in your cv!
I can think of more Haskell developers in Berlin who'd be interested in a job than I have fingers on one hand!
&gt; unix2dos Yeah, I suppose. The more rubber bands and paper clips you have to stick on, the less likely that people - including me - will get around to setting it all up and actually using it. 
Looks pretty awesome. I'd apply in a sec if I were in the market.
Just describe it recursively and hope for the best :P --- More seriously, it's found in [GHC.List.foldl](https://hackage.haskell.org/package/base-4.8.1.0/docs/src/GHC.List.html#foldl). The documentation is not visible however. Possible solutions: - `GHC.List` could be made visible, or - simplified versions of the definitions could be added as documentation for `Foldable`, or - it could be redefined in terms of something else?
* GHC is running via [mueval](https://github.com/gwern/mueval) on a server, which is [tryhaskell.org](http://tryhaskell.org/). The service was already running so I just used that. * The browser just has a little bit of JavaScript to get the inputs and produce a Haskell expression (`let decl1 = ..; decl2 = .. in (decl1,decl2)`) which it sends to the server to evaluate and then with the result it updates the display. I've been learning [Elm](http://elm-lang.org/) over the past couple weeks to use for the beta implementation of this. This just a throw-away prototype. The real thing will have much more first-class support for rendering data structures visually (lists, vectors, maps, matrices, records, etc.).
Yes but I think that studying the design of SBV will be very enlightening to OP. As I understand, they are new to Haskell and SBV, which demonstrates Haskell's power of supporting embedded DSLs, could motivate them to consider Haskell. 
Let's face it. `Foldable.foldl` is no longer a generalization of `foldl` for lists; rather, `GHC.Lists.foldl` is a type-specialization for `foldl`. It's time to eat our own dogfood. The documentation here must describe the general `foldl` as a primitive `Prelude` function, not bootstrap it off something else. We *can* use lists as an example. There must be at least one example here with some other type. Are there any non-trivial `Foldable` instances in the `Prelude` now, besides lists and `Maybe`? If not - this is a bit embarrassing. Also, since `foldl` is a very general function now, we should include some laws for it. Or at least refer to the laws for `foldl` if the they are already explicitly stated elsewhere, such as in the documentation for the `Foldable` class. Would it be helpful to note that for `Maybe` we have: foldl = maybe id . flip Not sure. 
I have also been working on a GUI REPL for over a year. I will release the initial version in a few months, and hopefully get some of you to join me in further development. Cheers.
That's an interesting idea, but unfortunately, I'm not sure if it's feasible. AFAIK, data types with derived `Enum` instances cannot have any arguments to any constructors because otherwise, the deriving machinery wouldn't be able to tell what `toEnum` or `fromEnum` would produce in certain scenarios. For instance, the generated code for `data Letter = A | B deriving Enum` looks (roughly) like this: instance Enum Letter where toEnum 0 = A toEnum 1 = B toEnum _ = error "outside of enumeration range" fromEnum A = 0 fromEnum B = 1 That is, `deriving Enum` relies on a one-to-one correspondence between the `Enum` instance and a subset of the natural numbers. Now let's suppose you relax the aforementioned restriction on derived `Enum` instances. What code would be generated for this? data Foo a = Bar a | Baz deriving Enum Intuitively, you'd expect something of the form `instance Enum a =&gt; Enum (Foo a)` to be generated, but just knowing `Enum a` isn't enough to tell you what behavior it has, i.e., instance Enum a =&gt; Enum (Foo a) where fromEnum (Bar a) = undefined fromEnum Baz = undefined It would be difficult to infer what would go in place of the `undefined`s, since you don't know which subset of the natural numbers `Bar a` occupies. Unless you could figure that out somehow, I don't see how this would work.
Probably not, since `generic-deriving`'s `GEnum` doesn't work quite like `Enum`: &gt; :set -XDeriveAnyClass -XDeriveGeneric &gt; :m + Generics.Deriving.Enum GHC.Generics &gt; data ABC = A | B | C | D deriving (Enum, Generic, GEnum, Show) &gt; [A ..] [A,B,C,D] &gt; genum :: [ABC] [A,C,B,D]
Having gone down the road of using Elm, I highly recommend learning or at least checking out Reflex. Elm is woefully underpowered, not just the language but it's version of FRP, which lacks recursive signals and dynamic event switching.
Could you explain how this actually works? I think it has something to do with partial functions. Kinda how this works: `sum = (+)` However I can't do `sumDiff = ((+),(-))` (I understand why this doesn't work). Thanks!
Very cool! Servant + PureScript/Halogen is a really awesome stack to be using. 
In what cases does one need these features?
I really liked the intro, as it spoke well to my personal background. My previous favorite language before Haskell was Ruby, and I loved Ruby's capacity for metaprogramming. While I love Haskell,, I've tried to use Template Haskell in the past but never quite gotten comfortable with it.
AFAIK the behavior hasn't changed. however `local-bin-path` is still interpreted from the command-line or a `stack.yaml` even without a `--copy-bins` option being supplied.
Thanks. That's a helpful starting point. I'll take this line of attack.
do you want (1) to be able to use the `[x..y]` syntax, or (2) simply access all values in a type? if (2), I wrote https://github.com/sboosali/enumerate to generalize Enum to non-sum types. it supports: {-# LANGUAGE DeriveGeneric, DeriveAnyClass #-} import Data.Enumerable (Enumerable(..)) import Data.Generics (Generics) data CrudOp = Add | Edit | Delete | View deriving (Eq,Ord,Enum,Bounded) data Route = Home | Person CrudOp | House CrudOp deriving (Eq,Ord,Generic,Enumerable) &gt;&gt;&gt; enumerated :: [Route] [Route, Person Add, Person Edit, Person Delete, Person View, House Add, House Edit, House Delete, House View] the documentation is detailed, but the package isn't on hackage yet. [edit: documentation at http://sboosali.github.io/documentation/enumerate/index.html , uploaded to https://hackage.haskell.org/package/enumerate-0.0.0 ] for (1), currently, no support for deriving Bounded or Enum (though there is support for deriving an Enumerable from an existing Bounded or Enum). but I can try to add them, if something like the following is too inconvenient (should be efficient): import qualified Data.Array as Array import Data.Array (Array, (!)) import qualified Data.Map as Map import Data.Map (Map) instance Bounded Route minBound = arrayRoute ! 0 maxBound = arrayRoute ! (cardinalityRoute - 1) instance Route Enum where toEnum i = arrayRoute ! i fromEnum x = fromJust (Map.lookup x tableRoute) -- as a monomorphic CAF, should only be run once arrayRoute :: Array Int Route arrayRoute = Array.listArray (0, cardinalityRoute - 1) enumerated -- CAF tableRoute :: Map Route Int tableRoute = Map.fromList (zip enumerated [0 .. cardinalityRoute - 1]) cardinalityRoute = cardinality ([] :: [Route]) some inconvenience is inescapable (without Template Haskell) since the standard library classes don't provide DefaultSignatures, only automatically derived instances. although, since you'll build upon `Enumerable`, not `Enum`, you might not need the latter directly. i.e. if you have a parent type like `data My = MyRoute Route | ...` you can simply repeat `... deriving (Generic,Enumerable)`. 
The problem is the one inherent to code reviews: https://twitter.com/iamdevloper/status/397664295875805184 I keep complaining about this but I don't know a better solution myself. Personally what I'm trying to do when reviewing is that I try not to request changes even if I don't like the code/comments/etc. Unless there's an obvious violation of some conventions or a bug etc. I just don't comment. I should admit that I'm a bit impatient :) So YMMV. --- About commentable Hackage pages: They're great but they solve a different problem I think. We still need documentation in the code because 1) not all modules have Haddock pages 2) not all code are uploaded to Hackage 3) seeing the documentation for the code you're just editing in your IDE/editor is very useful.
[**@iamdevloper**](https://twitter.com/iamdevloper/) &gt; [2013-11-05 09:58 UTC](https://twitter.com/iamdevloper/status/397664295875805184) &gt; 10 lines of code = 10 issues. &gt; &gt; 500 lines of code = "looks fine." &gt; &gt; Code reviews. ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
My attempt at contributing to GHC had me write a trac ticket, a wiki page, a dozen emails, comments on phabricator, and the expected code, documentation, research into potential conflicts with code in the wild, and tests. After all that, it was frozen unless I switched to a core dev's preferred syntax and ran it through the mailing lists again. I'm impatient, too, but that process could teach the IRS a thing or two about being user-unfriendly. At least now I have the four(?) accounts needed to potentially contribute!
we certainly need documentation in the code. comments in the haddock is just about making it easier to nudge slowly in a positive direction.
Maybe `Enum` could be generalized to (a subset of) ordinal numbers? Then we could ask for the `ω·2+5`th element. We wouldn't ever reach ω^ω , would we?
my guess, versus a repl, at least: * non-textual output (eg show a type as a Diagram, rather than a String) * non-textual input (drag a slider to change a constant, rather than typing the value) * some editor features (undo, history, shortcuts) 
Are you looking for interns too by any chance? 
Thank you but I am not solving the particular example I gave. See my edit.
Well, you can still enforce more complex ordering with GADTs and some other extensions, but it’s much more straightforward in my experience to just define these as separate data types—CST, AST, IR, and so on.
oh I see, you want the whole module hierarchy generated automatically. I'm not familiar with Thrift, can you give a simple example of some Thrift declaration that would result in two Haskell modules? you can also look at what the AWS packages do (or what the gl package does), as they generate dozens of modules. but, they might probably specify the module hierarchy manually. having said that, I would avoid nonstandard Setup files, because of the fragility, unless necessary. also, the user now has to import one module per type (as you can't reexport modules qualified). 
what's the source of the haskell program? also, in your Java, aren't you calling points on an empty Points object?
http://puu.sh/lwAih.png
I am not sure if I understood your question correctly, but I think [base-noprelude](https://hackage.haskell.org/package/base-noprelude) is doing something similar. It is worth having a look at its source code. In the recent versions, it's using a newly added feature of cabal to reexport the modules. But, in older versions, it relied on a custom [Setup.hs](https://hackage.haskell.org/package/base-noprelude-4.7.0.2/src/Setup.hs).
And yeah, I am calling it on an empty points object. I just made a dummy toString method inside that class declaration that returns the string I specified above. 
No problem. By the way, looking at the [documentation](http://docs.oracle.com/javase/7/docs/api/java/lang/Runtime.html), it looks like `Runtime.exec` is overloaded to accept an array as well, so try something like: Runtime.getRuntime().exec(["/bin/processPoints",points.toString()])
Hey that worked! http://puu.sh/lwBZP.png Thanks for the help!
Looks like you have extra quotes around the string in the java output. Compare: putStrLn "[Point ...]" putStrLn "\"[Point ...]\"" 
Okay, ignore my previous post, it's actually useless. It's changing the type you're operating on, rather than constraining it, which doesn't help at all and probably makes things worse (you would have to marshal things too and from the modified type, which it does not make safe). I blame sleep deprivation. This will take the constructor you are pattern matching on *out of* the actual `Option2` and let you match on subsets of it. data Option2 (f :: OptionForm) a where Constr :: Con f a -&gt; Option2 f a data OptionForm = ZeroC | OneC | TwoC data family Con (f :: OptionForm) a data instance Con 'ZeroC a = Zero data instance Con 'OneC a = One a data instance Con 'TwoC a = Two a a data OptionFull (f :: OptionForm) where ZeroF :: OptionFull 'ZeroC OneF :: OptionFull 'OneC TwoF :: OptionFull 'TwoC data OptionNonZero (f :: OptionForm) where OneNZ :: OptionNonZero 'OneC TwoNZ :: OptionNonZero 'TwoC class GetNZ f where getNZ :: OptionNonZero f instance GetNZ 'OneC where getNZ = OneNZ instance GetNZ 'TwoC where getNZ = TwoNZ firstOp :: OptionNonZero f -&gt; Option2 f a -&gt; a firstOp OneNZ (Constr (One a)) = a -- patterns not exhaustive warning without... firstOp TwoNZ (Constr (Two a _)) = a -- and no warning with. -- So we can write first = firstOp getNZ The OptionNonZero parameter is providing the constrained pattern matching, and locking the associated `Option2` value to having the same constructor so you can just match on it. Note that like all GADT based solutions to this problem this is putting some information about the constructor into the type signature, which is going to make other things more difficult. An `Option2 'ZeroC Int` is not the same type as an `Option2 'OneC Int`. That's information you have to hide if you want it in a list with other `Option2 k Int`s, for example, and it's not super convenient to get it back once hidden (though if you are sufficiently clever, you don't need it back). You are also not really going to be able to avoid this: you can't statically ensure anything not represented in the types, and representing something in the types makes them different types. Also, re: your question about why it shows the error at `main` instead of `fun`, it's because you've assured the case expression that you have a `Show a` instance for your `Some a` via the constraint in the type signature. The fact that `X` does not have one does not matter because you've declared that whatever `a` is given, that `a` *will* have one.
Could you override [`readDesc`](https://downloads.haskell.org/~ghc/7.8-latest/docs/html/libraries/Cabal-1.18.1.5/Distribution-Simple-UserHooks.html#v:readDesc) and modify the `PackageDescription` inside that? As a side note, it sounds like you're trying to integrate "Run Thrift compiler" and "Build Haskell library" into the same command as part of your build process. It may not be advisable to do that. Your Setup.hs ships with your library. That means that you're requiring all consumers of your library to have the Thrift compiler installed too, instead of just the library. It also means that code will be re-generated at compile time, so it might fail to compile if the code generator included a breaking change in some version. An easier, albeit manual approach would be to keep the generate step separate (maybe use a local Makefile to avoid copy pasting commands). You could use a simple template for your Cabal file into which you generate the list of generated exposed modules.
That is a fair point. I was referring to the concept of playgrounds as introduced by Apple together with the Swift programming language. For an overview, have a look at this introductory video (of WWDC 2014): https://developer.apple.com/videos/play/wwdc2014-408/
When are we getting these? :) Seems a bit more straightforward to ship than dependent types
I suggest taking a look at Apache Commons Exec (https://commons.apache.org/exec/). It will simplify external process execution and most importantly will help to avoid common pitfalls.
Apple re-introduced them, which is still great nonetheless. But you can have a feeling of a primitive version of them when looking at how Lisp Machines used to work. https://www.youtube.com/watch?v=o4-YnLpLgtk Or Smalltalk https://www.youtube.com/watch?v=JLPiMl8XUKU&amp;index=9&amp;list=PL56HHwgheCQRHYESUgjDu26SRPW4JG0jP Or Mesa/Cedar http://www.computerhistory.org/revolution/input-output/14/347
&gt; It's really only possible to mock a database if you're using it as a key-value store (in the article, you refer to this as "crud operations"). Mocking the relational parts of a database basically amounts to building a relational database. Not necessarily true. You could also tell the mock database what to deliver as a result when confrontend with the query you expect or even on the first call, second call,... in your test itself.
There are a number of different ways to represent this situation at the type level, to enforce statically that you don't try to examine values that don't exist at the current stage. But you want something much more specific - you want to use the GHC pattern matching exhaustivity test. Keep in mind that GHC's exhaustivity testing is only a "best effort" convenience feature. It is not always possible even in theory for GHC to be able to prove that a set of patterns is exhaustive. And even in cases where it is possible, GHC doesn't always succeed. It's a hard problem. Trivially, you could have a completely different version of the type for each stage, where the only constructors available are the ones that make sense at that stage. Each transformation would then need to copy all values from one type to the next. You could avoid some of the boilerplate using generics, but you need the copy. But I would recommend that you not focus so much on the exhaustive pattern match mechanism. Focus on creating a type design which gives you the static guarantees you need.
It would be awesome to have entire **stackage snapshot**'s documentation. That's what I do locally with https://github.com/philopon/haddocset but why not have it done once ?
That was the way we did it when Applicative was newer and less widely used. It was also less widely understood then. We were worried that Applicative would just confuse people. But now that AMP is a reality - thanks! - we don't need to think of Applicative as something unusual or esoteric anymore. I think [Albert's incremental approach](https://mail.haskell.org/pipermail/haskell-cafe/2015-November/122282.html), as linked by /u/Mob_Of_One elsewhere in this thread, is better nowadays.
If your application is a thin layer over the db then mocking responses is bad. If you have dense logic to test then it makes complete sense.
Neat concept! I think this will be useful for running a random code snippet. Ideally though the author of the snippet would include information (i.e. what stack supports as comments at the top of the file) on how to reproduce the snippet. So it might be useful to instead focus on making that workflow easier. Similarly as an end goal I would want to be able to output a stack.yaml and .cabal file rather than just ephemerally running code (many snippets don't have a main anyways). The node dependency is going to be a non-starter for many.
&gt; despite people's hate for it, labview has had this for decades. If only LabView was this nice at **run time**, instead of using hex codes for errors, or my personal favorite - "generic error", which just makes me shout expletives in National Instruments' general direction...
Isn't that a relatively old paper? One thing is the theory of mapReduce and another his practical realization. It is a pure computation and therefore it should be easy to express as a composition of pure computations. But since the whole purpose of mapReduce is to make it distributed, It should use impure effects to distribute data and communicate results. Maintaining composability with all these impure effects is very difficult. The paper does not attempt to maintain composability, and the mapReduce is a monolitic function, presumably with a lot of unsafe IO under the hood, if it would be realized anytime in the real world: mapReduce :: Ord k2 =&gt; Int −− Number of partitions −&gt; (k2 −&gt; Int) −− Partitioning for keys −&gt; (k1 −&gt; v1 −&gt; [(k2,v2)]) −− The MAP function −&gt; (k2 −&gt; [v2] −&gt; Maybe v3) −− The COMBINER function −&gt; (k2 −&gt; [v3] −&gt; Maybe v4) −− The REDUCE function −&gt; [Map k1 v1] −− Distributed input data −&gt; [Map k2 v4] −− Distributed output data For anyone interested, [I demonstrate here a composable map-reduce](https://www.fpcomplete.com/user/agocorona/estimation-of-using-distributed-computing-streaming-transient-effects-vi-1#distributed-datasets), where partition among nodes, map and reduce are different computations. This is the example that multiply all the dataset by 3 and count the number of odd/even numbers do runNodes nodes let cdata = distribute [1 .. 10000 :: Int] let cdata' = cmap (*3) cdata r &lt;- reduce (sumOddEven 0 0) sumIt (0,0) cdata' liftIO $ print r (An one-liner version would be more impressive, but not as clear as this one) [Apache spark](http://spark.apache.org/) does also a nice job on expressing distributed map-reduce as a composition, but it need an additional infrastructure of job schedulers and monitors to do it (in essence spark is a distributed platform that handle job scheduling and asynchronous communications under the hood, using akka and scala futures), while mine is just a library, and all is done by the monad and the EDSL primitives. No extra code or processeses are necessary. Other mapReduce implementations are more monolithic. spark is composable but pre-set for map-reduce computations. while mine is composable and admit any distributed architecture. In the example, the the data is partitioned in different simulated nodes (that can be made real by changing `runNodes` by the appropriate code) . In the genuine mapReduce process the map generate key-value pairs. The reduction phase is done in a different node for each key. Currently In my [transient library in hackage](http://hackage.haskell.org/package/transient) (module `Transient.DDS`) the mapping and reduction are done in the same node. A second final stage of reduction is made in the node that scheduled the calculation (that is the reason for the two reduction functions `summIt` and `sumOddEven`. But this is not general enough. I did, but still not tested a reduction stage where the results of the map go to a different reduction node for each key. I use vectors instead of lists in that new version, since the efficient use of memory is critical. I said that one thing is the theoretical formulation and another is the real implementation with the real world constraints. But I've shown that Haskell could do it significantly better than other languages/platforms in the real case. There is a huge potential for Haskell in this territory! 
I hate this. It's awful and you should be ashamed! Something like that?
No. I would prefer something more constructive
That sounds like a fun project to work on. I can't think of a compelling reason to use it, though. The only time I need to compile to a dynamic language is if my target is JavaScript, and I can use PureScript, GHCjs, Elm, etc. if I want to. Compiling to Ruby or Lua.. why not just use Haskell? Going to a different language incurs a lot of costs -- a primitive static type system is IMO worse than dynamic typing. Dealing with an FFI is always annoying. If you have to debug the compiled code, then you have a lot of trouble coming if the target source isn't usable. There are performance issues. New/immature languages don't have the libraries and tooling that older, more established languages have. "I can compile this to JavaScript *and* Ruby!" doesn't.. really... get me very excited. If I'm going to be selling a new language to an employer or client, it has to have *significant* technical advantages. I can compile Haskell (or OCaml, or Scala, or F#, or Clojure, etc..) to JS already, and if I can use one of those, then why am I using Ruby at all?
Perhaps a good use could could be when you have a large, existing code base in Ruby or Lua or something else, and want to incrementally port it to a statically typed language.
You should also look into rewriting the bash portion in [Turtle](https://hackage.haskell.org/package/turtle-1.2.3/docs/Turtle-Tutorial.html).
&gt; PS is too complicated Too complicated for what? Also, from whose the point of view - the user? I don't understand why this is a problem. In particular, I think the "complex" features are often indispensable for writing reliable or maintainable software. But even so, if you want, there's nothing stopping you sticking to a simpler set.
check out [Jane Street Capital](https://www.janestreet.com/join-jane-street/internships/), they do a ton of work with OCaml
Why would it be synonyms? I don't see any benefits.
You just use type's name type FooBar = {foo::Int,bar::String} x = {1, "text"} :: FooBar you construct variable x with type FooBar
I'm based in London atm, but I'm more than willing to move anywhere else. I probably should have stated more clearly that I am still an undergraduate (3rd year currently). As such I don't care too much about salary; so long as the pay is fair and I'm doing haskell I'm a happy man =)
One thing you can do is use GADTs. So your definitions could be data Nullable data NotNullable data Option2' k a where Nothing :: Option2' Nullable a One :: a -&gt; Option2' k a Two :: a -&gt; a -&gt; Option2' k a type Option2 = Option2' Nullable type NotNull = Option2' NotNullable As you can see from the GADT declaration, the Nothing constructor can never be typed as NotNull since that would cause a mismatch between Nullable and NotNullable. On the other hand, One and Two could be typed as either, because they are polymorphic in the second type argument to Option2'. Hope that helps :)
I tried to do this and failed. Let me tell you what I would do, now that I thought about it more. You're going to need to get creative and network the shit out of it. I suppose you also need to know your shit (expected CS curriculum knowledge) for the interviews! So, try Jane Street for the Ocaml, that would be a great position. Spam all of the companies posting [here](https://functionaljobs.com/). Hit LinkedIn, Angelist and any haskeller who is employed (they normally list it?) on Twitter to get a foot in the door. Keep begging :) I hope you get it done!
&gt; It's just a flexibility of the language. Such type could be possible in dynamic language, so it should be in static too. I'd suggest a different perspective. Once upon a time, some languages were "untyped", in that they literally did not know what "type" of thing was in a given memory address. Assembly code doesn't even blink at treating a pointer to chars as an int because it can't even tell. Nowadays by that standard everything is "strongly typed", in that language runtimes, one way or another, always know what "type" of thing is somewhere, at least to some degree. So in that sense the term "strong typing" has become useless; all modern languages are strongly typed, which is why you'll hear a Pythonista claim they have a "strongly typed" language. What is one of the things that distinguishes a "strong" type language from a "weak" one today, then? I'd submit the primary characteristic is _how hard the language tries to automatically coerce things_. Perl is dangerous because you can just run merrily along with `my $x = "hello $obj world $array" + 8` and end up screwed up with no indication until runtime. foo :: (Bool -&gt; Int) | (String -&gt; Double) foo true = 1 foo "text" = 20.0 is opening a door directly on to that world. This is true despite the fact you can trivially represent that as packing the request into an input datatype `(Bool, String)` and an output datatype `(Int, Double)` and code that is statically guaranteed to not pick the wrong answer. (All automatic conversions can always be so represented, after all.) You're eliding differences between Bool and String and Int and Double, and the language is going to afford more and more of those elisions until despite wearing the trappings of a strongly-typed language, you're going to have a weakly typed language. The next code chunk does something similar, where it tries to elide for convenience the difference between `x` and `Maybe x`. I'd suggest this is actually how most languages approach the problem, but I think Haskell came up with a better solution by sticking to it and pounding on the problem for nigh unto ten years, which is the whole Applicative type class and associated operators; instead of hiding the fact that something is Maybe, they make it relatively easy to work with Maybe values. Those two things may not sound too different in English, but they are two fundamentally different approaches to the problem. I understand the desire to hide this sort of complexity from the user. I do not merely mean that rhetorically, I _really do_ get it. But I'd suggest it's not the right way to go. For study, have a look at Python's seam where it tries to integrate its generator-based streams and character strings, in particular in that iterating over a string yields things that are still strings, which makes it impossible to know whether a given string is supposed to be a "character" or another "string". This actually grunks up the APIs around strings in interesting ways. &gt;&gt;&gt; print type("abc") &lt;type 'str'&gt; &gt;&gt;&gt; for x in "abc": ... print x, type(x) ... a &lt;type 'str'&gt; b &lt;type 'str'&gt; c &lt;type 'str'&gt; Even if you don't agree, you ought to spend some time looking at that until you know what I mean and why that causes problems. (Also I'd say that among the first ten questions your users would ask is "how can I extend `foo` from my own modules?" and holy cow is that a world of pain waiting to happen to you if you don't know why that's a bad idea already.)
This post shows the actual flow of downloading torrents using the methods I described in previous articles, so it should be more interesting. The code in on [github](https://github.com/farnoy/torrent).
Ahah spot on! I do include a link to my github in my CV. Short answer is because I love it. I love how concise and elegant the coded solution ends up being, most of the times anyway!
We aren't specifically looking for an intern, but send your CV over to jobs@skedge.me
Very glad you've continued with this, it's a great series :)
A fold reduces a certain container of data such as a list down to a single summary value; it takes a binary function `(b -&gt; a -&gt; b)`, an initial element of type `b` and a list of `a` and recursively applies the function to the "carry" of type `b` and the head of `[a]`. `foldl` does the list recursion for you, so the outer function doesn't need to. The `l` and `r` suffixes denote the direction in which they are associative. A "scan" instead returns [f a, (f (f a)), ..]`, so its use range is somewhat different from folds.
I'd love to attend next time I'm at strange loop :)
So... why not call it "toroid"?
It seems like you've added a ton of syntax sugar in a way that reminds me of Coffeescript: &gt; Tuple is a record with anonymous fields &gt; &gt; `{1, "text"} :: {Int, String}` &gt; &gt; you can drop {,} if it's possible &gt; &gt; `1 "text" :: Int String` But I don't really see why. It just looks like it will make the language harder to read. Explicit is better than implicit! Also, are there any plans for generics? Do you want to give up `Set a` for tuple sugar?
while as a solution it might be better, the problem of generating typed code from untyped code seems harder. unless, you have a universal Object type, where method calls take arbitrary strings, etc.
I guess most currying in the target language could be done away with with a sufficiently smart compiler. That is, inline and specialise by how many arguments are known at call sites, or have according wrapper functions. It is, after all, not like GHC wouldn't be doing that, too.
Many of you have mentioned Jane Street, so I did apply. Fingers crossed now!
You might be interested in [packman](https://github.com/jberthold/packman)
As far as I know, no. Take a screenshot or screen capture.
How does this compare to [servant](https://haskell-servant.github.io/)?
Because of languages like [PHP](http://sandbox.onlinephpfunctions.com/code/f2a30c26387b127f98b8d99fe64d1454e5b1e20f).
Well, I guess there's Morte for a maximally explicit language. :)
&gt; What about something in an equally awesome functional language? Well obviously no language is equally as awesome as Haskell... but Clojure and Scala are pretty neat and, being JVM languages, have had an easier time catching on industrially.
Haha, I had forgotten about that bug. I worked on a PHP implementation for a little while, and there’s all kinds of weird syntactic and semantic issues. But really, it’s fascinating to me how people talk (and think?) about these two notations very differently, even though grammatically they’re very similar.
It's nice to have the syntax highlighting. If you are unable to help, move on.
Just had to do this actually. Build like so stack build --executable-profiling --library-profiling Then run your binary with `+RTS -p -RTS` (add any other options between the RTS's). Beware that running stack exec with +RTS will profile stack exec itself and not your binary.
http://i.imgur.com/Ng9eY4u.png
Yeah, definitely. I was a bit nervous. It'll improve as I give the talk a few more times. 
Thanks for the info! I had a play with ghc-exactprint on the darcs source code and it has about 90 failures and 144 successes, which isn't bad. I got another 30 to pass by hacking in an include path to the source code of 'roundtrip'. I guess ideally this would be integrated with cabal so it knows where to find things properly, but that sounds like hard work. A lot of the CPP in darcs is to be able to report source line numbers in error messages - hopefully the new stuff for stack traces in recent GHCs can replace that in time.
I asked the same question on StackOverflow: http://stackoverflow.com/questions/33111789/efficient-data-serialization-in-haskell To sum it up, I ended up using the new aeson with the toEncoding function (HEAD - not the version that is on hackage, it's buggy - I wonder what happened to the maintainer, there have not been any updates for more than a month)
Yeah, I usually don't like to fall into the dichotomy - everything is a gradient. I was pressed for time when making that comment (which is always a mistake!).
I must have missed the generics example in the readme. How does the syntax for generics interact with the tuple syntax? Do you need tuples more than arrays? It's all a tradeoff!
&gt;How does the syntax for generics interact with the tuple syntax? If there is any syntax conflict, you can always use full tuple's syntax {a,b} &gt;Do you need tuples more than arrays? Every function with more than one argument use a tuple, so yes.
I have added small changes, so empty type should be more clear to understand https://github.com/dotneter-/aeiar/commit/530706dbad6dfe0d1eb044b282c00374dc7f6354?short_path=04c6e90#diff-04c6e90faac2675aa89e2176d2eec7d8
&gt; I would find it less confusing if Yes, I have changed it to type Nothing = {} // {} - empty tuple's type {nothing :: Nothing} nothing cannot have type void directly otherwise every empty type would be void and it would be really confusing.
Hmm. Then I don't know. Use `-v` and see what GHC is doing. Try to reproduce without using stack.
torrentoid?
There has to be an idiomatic way, like sticking a `WriterT` on top that appends to a list of frames, which can then be rendered separately.
/u/dcoutts ?
Yes, packman sounds like a great fit for this. 
Here at Suite Solutions the work is not like that at all. But it's not profuntors, either. It's web applications on the front end, and processing highly-structured textual content on the back end. All engineered for the enterprise, in scale, features, and high availability.
&gt; But I've shown that Haskell could do it significantly better than other languages/platforms in the real case. Link to your benchmarks?
I reject your typeface and substitute my own with an OCR reader
If your haskell object is big, maybe it has some common internal references. if you do not take care, A serializer will inline them by default, so the referenced data will be serialized multiple times . Maybe by removing repetitions would achieve better compression. Moreover, when the data is deserialized, the memory footprint will be reduced too. I use RefSerialize for that purpose. 
Oh, BTW, my more recent talk was at ZuriHac this year: https://github.com/meiersi/HaskellerZ/blob/master/meetups/20150529-ZuriHac2015_Duncan_Coutts-Better_Faster_Binary_Serialization/binary.pdf
So using `binary-serialise-cbor` (https://github.com/well-typed/binary-serialise-cbor) compared to `binary`, you might expect: * 1/2 size of serialised output * 6-8x faster serialisation * 2-2.5x faster deserialisation I am surprised that `protobufs` gives you 1/4x the size of the output vs `binary`, you must have really been hitting the worst case (perhaps lots of small integers). If so, perhaps `binary-serialise-cbor` will also give a greater improvement for your data vs `binary`, since it also uses compact representations like `protobufs` does.
I know [Peter Thiemann](http://www2.informatik.uni-freiburg.de/~thiemann/) has a great PL group at U. Freiburg, perhaps graduates from their courses would be interested in applying. I'm not available and I don't speak German anyway, but I highly recommend Freiburg as a location. It's really a beautiful place and most people there are friendly (a notable difference from when I visited Berlin..) 
Thank you.
I had some benchmarks for this: https://github.com/osa1/serialization-bench It's comparing binary, cereal and packman. The library versions may be a bit old now but feel free to bump version numbers in cabal and run benchmarks again. Pull requests would also be appreciated if you do that :p
Oh, happy to know I was wrong.
Of course, but any anecdote is like that. I know one thing: When I was in Freiburg looking lost, someone kindly came up to me and gave me directions. When I was carrying heavy luggage, people kindly moved to let me on the tram. People were very understanding when I struggled to speak German. In Berlin, in the space of three days: I got nearly pushed out of a bus onto the snow as someone pushed past me without so much as an "Entschuldigung", someone who was smoking blew smoke in my face while I was suffering from a cough (I think they might've thought that I was being passive-aggressive about them smoking, but it's still dickish behaviour), and a child started mocking the race of my partner and the parents of the child did nothing to stop it.
Not in terms of performance but in terms of expressiveness and necessary machinery. It does not make sense to test performance before switching to vectors instead of lists etc. But I expect similar performance.
There doesn't actually *have* to be. You never actually get the frames to work with anywhere in Gloss so far as I can tell. They never actually exist in haskell-land. I went looking so I could render part of a smaller frame to a bitmap and then tile that to reduce overhead, but I could never find a way to do that.
I doubt strictness would help here. I believe this is O(n) because it first does the first sum, then the second one. But since it has to do the second one it doesn't start garbage collecting because it keeps a reference to the head.
I'm probably going to abuse some terminology, so you'll need to forgive me for that, but I'll try my best to convey how this differs from `servant`. Servant doesn't use haskell-style dependent types (GADTs as singletons). It's basically closed type families with typeclasses. And it's really cool because it basically avoids the need for any template haskell. If you think about data being structurally vs nominally typed, routes in servant are more on the structural side (plus being indexed by position). Here's an example route from the `servant` tutorial: type API = "position" :&gt; Capture "x" Int :&gt; Capture "y" Int :&gt; Get '[JSON] Position :&lt;|&gt; "hello" :&gt; QueryParam "name" String :&gt; Get '[JSON] HelloMessage :&lt;|&gt; "marketing" :&gt; ReqBody '[JSON] ClientInfo :&gt; Post '[JSON] Email None of these routes have names at the type level. They get their names when you do something like this (again taken from the tutorial): position :&lt;|&gt; hello :&lt;|&gt; marketing = client api (BaseUrl Http "localhost" 8081) Similarly, you do dispatch like this: server :: Server API server = positionHandler :&lt;|&gt; helloHandler :&lt;|&gt; marketingHandler Here are several odd things that I noticed about building routes this way: - In an application with many routes, if you accidentally switch the ordering of two of the dispatch functions (or two of the render functions), you don't get an error if the types of the `Capture`s are the same. You are required to correctly order the list of auxiliary route functions. - Generating documentation is completely typeclass driven. Since the routes don't have type-level names, you can't assign documentation on a per-route basis. You have to do it based on things like the names of the `Capture` parameters. Also, you have to use orphan instances to do this. The approach I've taken is to give each route a name. In a sense, I would say that it is a more nominally typed approach than servant. To anyone familiar with dependent types, I'm using the universe pattern, where the universe is the list of routes names. Then, you provide a function to map this universe of routes to a type level representation of their paths. So, if we assume similar routes to the servant examples I gave earlier, an example of using my rendering function is: &gt; renderMyRoute = render (Proxy :: Proxy PlanMyRouteSym0) sPlanMyRoute &gt; renderMyRoute HomeR ("Hey_People",()) /hello/Hey_People &gt; renderMyRoute PositionR (44,(22,())) /position/44/22 I haven't tried doing anything with querystring parameters yet. But, basically, the fundamental difference is that with my approach, you have functions provided by the library that take your routes (and captures) as arguments. With servant, the functions provided by the library generate a big tuple (or something isomorphic to a tuple) with a function that does something for each route. You need to bind names appropriately to this resulting tuple. I'm trying to getting a write-up on routing done, and hopefully I can explain better there. And just to make one thing clear: I don't think that `servant` does a bad job. I think it's a very cool library that cleverly avoids using any template haskell, but I'm really just exploring a different point in the design space. 
I do not. But I'm working on writing some stuff up on routing, so I'll probably have one soon. If you know of a similar library though, I would be interested to hear about it.
Ya, I was being a smartass
Yes, my answer is irrelevant. I misunderstand the original question. Though see the other answer I wrote.
See http://www.haskellforall.com/2013/08/composable-streaming-folds.html?m=1 I think this solution is packaged on Hackage somewhere, but cannot remember where.
Do you understand why it helped?
Does it count if I am currently learning German? Still at a rather basic level, though.
&gt; What’s with this one parameter rule? Not sure where you got this idea; Python lambdas allow multiple parameters. &gt;&gt;&gt; (lambda x, y: x + y)(1,2) 3
&gt; I think this solution is packaged on Hackage somewhere, but cannot remember where. * https://hackage.haskell.org/package/foldl * https://hackage.haskell.org/package/folds
yeah, which should I use or is "better" ?
Yep! The code using `foldl` would be: &gt;&gt;&gt; import Control.Applicative &gt;&gt;&gt; import qualified Control.Foldl as Fold &gt;&gt;&gt; Fold.fold ((,) &lt;$&gt; Fold.sum &lt;*&gt; Fold.sum) [0..1000000] (500000500000,500000500000) That runs efficiently in constant space using a fully strict accumulator
Well, it depends. GHCJS (usually with reflex and reflex-dom, which in my opinion is the best combination of libraries for the frontend) generate a huge JavaScript, but has all the power of GHC. Elm generates smaller JavaScript, but it's less powerful.
Yes, my use case is somewhat special. It has lots of text and small numbers and I think protobuf is specially good at small number because of it's base 127 encoding scheme.
In an English subreddit you might want to call that language "German". (Just like in a German subreddit you would call English "englische".)
I'd suggest tweaking the title to say "German speaker required" instead of "German required", if that's what you meant.
This is weird that JSON is both faster and smaller than a binary format! By the way, where can I find the head? https://github.com/mailrank/aeson is broken!
&gt; My thought was that it would be a practical to be able to reuse the same UI (based on web technologies) in a web browser or in any native OS version of the application. The DOM API supports at least Linux &amp; Mac, only Windows support is unclear for me. And that is not the most important thing right now. It may be also possible to develop the same widgets over an standard GUI like GTK. &gt; I'm wondering if there are any technical showstoppers to complete the ghcjs ports of perch and hplayground or is it just the scarce resource of time that is lacking? It is just a matter of scarce resources. [Perch has been ported already to GHCJS by geraldus](https://github.com/geraldus/ghcjs-perch). Besides porting hplayground I want a seamless interoperation with the server using websockets and transient. hplayground is a set of widgets over an early version of transient. Transient can execute applications in different cloud nodes. I want to permit hplayground to invoke procedures in server nodes and vice-versa, so that the browser becomes a more node in the cloud, with additional display capabilities. That interoperability is the part that is taking more time.
I'll look into these suggested projects. I understand the hesitance to mix the file-generation process into the cabal framework, but it's important that we eliminate the manual aspects of updating the thrift files and propagating the changes to each language we use. 
Tell us a bit about what you want to build. Because like this you are soliciting "it depends" answers.
You would normally just make a view for this, if you wanted to really do this "on the left". data View = BarView Int (String,String) | BazView view :: Foo -&gt; View view (Bar x y z) = BarView x (y,z) view Baz = BazView f :: Foo -&gt; Whatever f (view -&gt; BarView x p) = ... this is a really silly use for a view, since you can just use a let on the right, or use a where.
Going off of /u/psygnisfive 's answer, you can make a "pattern synonym" to get something similiar without making an entire extra data type, but it's just as silly :) pattern FooView x (y, z) = Bar x y z And now you can use it: &gt; let FooView a b = Bar 1 "string1" "string2" in (a, b) (1, ("string1", "string2")) &gt; FooView 1 ("string1, "string2") -- you can go backwards too Bar 1 "string1" "string2" It's equally silly, but it's also probably not what you want, right? You want syntactic sugar for something like this that happens automatically. I don't think we have that in Haskell or Haskell GHC quite yet. And people already complain that Haskell is a language with waaaaay too much sugar and syntax :p But I don't think I've ever come across a situation where something like that would be helpful. Can you show an example where it might be worth the extra overhead of adding "yet another" syntactic sugar?
...and [Haste](http://haste-lang.org/) provides a middle ground, producing fast, small code with all the power of GHC behind it, but with some compatibility kinks still to be worked out when compared to GHCJS.
[nested-routes](https://hackage.haskell.org/package/nested-routes) uses a "predicative" lookup table to accomplish routing - we can directly embed parsers or regular expressions as predicates during the lookup phase. There's also [path-extra](https://hackage.haskell.org/package/path-extra), which provides type-safe representations of path fragments (thanks to Chris Done's [path](https://hackage.haskell.org/package/path) library), and adds other useful utilities for creating stringless routes. For an example server, check out this simple [address book app](https://github.com/athanclark/contact-logger). &gt; Warning - I developed these libs :x
Can you actually do that tho? I didn't realize that pattern synonyms can also *manipulate* data like that.. also your code markup is all fucked
Yes, that's what I meant. I tried to change it but it's not possible. https://www.reddit.com/wiki/faq#wiki_i_made_a_mistake_in_my_submission_title.2C_how_can_i_edit_it.3F I'll do better next time!
I hope so, too! If everything works out, we'll have international customers next year. We'll have to change a few things then because nobody of us speaks any danish and that would be a good point to change the company language to English. We'll keep you posted!
There's also a benchmark in the binary-serialise-cbor comparing that with several other packages. Pull requests welcome.
You can always apply for an internship! You should have basic Haskell skills or good programming skills in a different language. Just send us your resume and we'll get in touch and see, how it could work out.
Yeah, but then I have to remain interested in a topic long enough to write an actual paper.
Brilliant! Okay, so the main issue you're facing is in debrujin enumeration, and how you could do it while retaining the distributed model, and without a global counter? Sorry if I'm wrong here, it's great to know that you've been working on this and have so much material, but I'm just not up to speed yet.
Well, it is more that in the presence of additions or deletions from the environment I'd like to get the ability to reuse other typechecking results. It is hard to figure out how to do that in a deterministic enough manner that the type error slicer can readily check for the 'exact same error' in a shrunk context.
Correct. The data is ultimately passed off to OpenGL - gloss doesn't render the `Picture` type.
/u/dcoutts Do you have any idea of when you are able to push it to Hackage?
I don't fully understand what you're talking about but the substitution instead of modifying sounds quite like the editing in [Unison](http://unisonweb.org/), you might wanna check them out (they're also just a really awesome project in general).
Exercise: does this definition improve something? f z x = z || take n x == needle 
I think it counts. The article that phitrion linked to has an example of using the routes in a bidirectional way (for both rendering and parsing).
Thanks for sharing. Glad to see that other people are interested in the same topic. Based on my reading of the tutorial, nested-routes handles dispatch in a type safe way but doesn't have a type safe way to render routes. Is this accurate?
It's still a little annoying they don't seem to even be open to accepting pr's in other languages.
Whoah, I must have missed that part. That's excellent.
Yup. We do cool ass stuff. 🙃
I think if you are aiming for a "single page app" then, to my understanding, Elm is more geared towards that use case. As for regular old client-side scripting, you'll be fine with either one; I'd lean towards ghcjs personally. Try both out for yourself and see which style you prefer.
I'd love to see the [binary-serialize-cbor](https://github.com/well-typed/binary-serialise-cbor) package included in this.
If you use "getParams" somewhere in code to get all GET-params – then it would not be possible to determine which one of them were used later. It's also hard to make sure your API-call will go through all possible branches which call getParam.
&gt;if I had a data type Foo, and wanted to add a new sum-type data constructor to it I don't want to sound pedestrian, but isn't this particular use case better solved by polymorphic variants or restricted dynamic types? I mean, I admit I've always seen using regular algebraic data types for modeling problem domain (as opposed to control structures like `Maybe` or `Either`, that aren't ever going to need new fields or constructors) is a kludge encouraged by the ease of declaring AlgDTs and abstruseness of dealing with better solutions. But if you're going to turn a language upside down, then it's all a bit different.
Oh wow I would love that. The biggest problem I see is that I'm not a EU citizen; I've recently graduated with a five-year degree in computational linguistics, been coding and studying FP on my own for a long time. Can we talk? It would be great to get even an internship, relocate for a while, put my German to use, if that's a possibility.
I'll keep you in my notes. Next semester will be my bachelors thesis, but I guess I can do an internship while going for the masters degree... :-)
I really ought to write a post that explains why we *have to* use type-level API descriptions. Basically we want some types (like `Server`) to be different depending on the API description they get as input, and until we get proper dependent types... we can only make this happen if API descriptions are types themselves.
Super cool! I've wanted to do something with ncurses for a while, and I think this gave me the last push :) Regarding making the game online, I have no idea what your plans are but I would: * Make a server that would matchmake people from a lobby (you enter the lobby, challenge someone and they can accept or decline) * The game functions as normal, with the addition that the move the player makes is sent to the server, and likewise the player is waiting on the server to respond with his opponents move. * To see if people are connected, the server could do a PING/PONG like IRC does. If using HTTP, this would probably need to be the client that PINGs and the server that PONGs. I would only send the move itself, because the server would start with its own game state when a game is begun, and from there it would only need to move itself and check if that particular move is legal or not. The server could just be a HTTP server, and you would do normal HTTP calls, responding with 200 for a "accepted" move and 403 for a "forbidden" move.
I periodically try to get GHCJS working on my Windows machine. I haven't managed yet. I do get a bit further every time though, so it's a definite sign of progress. I've been playing around with PureScript and so far I have yet to reach any show stoppers. It wasn't really a smooth process getting it up and running, but all the problems at least had reasonable workarounds. PureScript produces significantly smaller JS files. I'm guessing part of the reason is because it doesn't have to simulate lazyness, and part because it doesn't by default include a large base, but only the things you use. That being said, even a basic project using halogen produces a 100kb minified JS file. I do think that's acceptable though. I can't comment on Elm and Idris. The reason I chose PS over Elm is because PS is closer to Haskell, feature-wise.
I took a look, and opted for TypeScript, partly because I want very predictable Javascript and had a legacy code base. I've been very happy with it, code size and performance are both exactly as you'd expect, since it's based on Javascript. Certainly could be more functional, but it's a worthy point in the design space.
We do have plans for generating HATEOAS behaviour automatically. The work I started doing on [verdict](https://github.com/jkarni/verdict) was partly motivated by that. That being said, my impression is that in reality documenting URIs à la Swagger, API Blueprint, and current Servant is quite useful to most people, whereas the idea of having clients navigate from an entry point via HATEOAS hasn't really caught on (even if other REST aspects have). I have in the back of my mind some thoughts about how to optionally aggregate requests with simple dependencies between them into a single one, which might help with one of the issues of this discovery mechanism of REST (i.e., more requests than a URI-aware approach), but this does start to become its own protocol that one can't assume all clients understand. &gt; What needs documenting is the media types exchanged between client and server (e.g. XML, HTML, JPG, your own custom types, etc.). Servant does describe the possible request and response media types. Do you mean also providing schemas (e.g. JSON and XML schemas) for the requests and responses? If so, again part of the idea behind verdict is to head in that direction.
That makes total sense, though obviously the logical conclusion of the argument for value-level is that you just need to describe the use of headers in your value DSL, same as you have to do in a type DSL (so no using raw WAI functions!).
I'd love to read it!
&gt; simple tasks such as ~~folding over binary trees~~ rendering a DOM might be a more representative frontend benchmark?
There's also [Haste](http://haste-lang.org), which produces code which is significantly smaller (as in, at least 5x smaller) and usually also faster (by 1.5-3x in my benchmarks) than what GHCJS produces. On the flip side, Haste doesn't support weak references, is not API compatible with Control.Concurrent (preemptive concurrency uses a slightly different API), and still has Template Haskell on the TODO-list. EDIT: oh, and it has binary installers available for Linux, Windows and OSX. 
You can automate documentation-creation in runtime, obviously. Python has tools for that, for example. The thing is: - you'd need to basically build your own ad-hoc value-level runtime type-system - you'd lose all the things compiler gives you (for example, warnings that some parameters are not used anymore) So, since you know which values you want to use at compile-time anyway, converting that to runtime in a way to let you work with it in compile-time-like fashion doesn't have a lot of reason behind it anyway.
[MFlow](http://github.com/agocorona/mflow) has the most elegant and idiomatic implementation of type safe routes in Haskell that I know. It is a monadic parser of the REST route with backtracking. The elements of this navigation parser are page parsers/rendererers (formlets), which parse POST requests. Any change of the REST path in a page response activates the navigation parser, that goes forward or backward (using backtracking) to the appropriate page parser. With this two level parsing hierarchy it is possible to create any web application with safe routes and safe pages in a single and composable expression. Web Services are simpler to express in this way. NOTE: If anyone want to implement the parser scheme for routing, backtracking is not really necessary if the navigation does not manage state.
As of yesterday: - `stack-run-auto` and all other scripts on the repository are Haskell programs - Everything that can be done in parallel will be done in parallel (though it could be more stream-like)
I sort of removed the bash portion now :) Thanks for the link, hadn't seen that project before.
When I talk about type safe routing, I'm thinking specifically of two features: 1. Rendering routes 2. Parsing routes (or possibly just dispatch)
Just to finish pinning it down, what do you mean by "routes"?
That's definitely a possibility. We're a small company and don't have any experience what it takes to get all the legal work done, but we're used to tackle challenges. Please send us your resume!
&gt; Well, this can be argued in the fact that the most common REST client is the web browser which works exactly this way Right. I should have been more precise - clients in the sense of e.g. other web services, where there isn't continual human input, have tended not to adopt entry-point plus HATEOAS discoverability. Part of that, to be sure, is that HATEOAS support from the *server's* side is also not that widespread; another part of it is the multiple requests issue I mentioned; and I think there are other issues besides. But none of this is to say I'm against HATEOAS - as I mentioned, getting it to happen more or less automatically with servant has been a goal of mine for a while. I'm still a little unclear on your media type suggestions. Creating new media types should, I would imagine, be a rare occurrence; unregistered media types are discouraged by RFC 6838, and code-on-demand to deal with them, while sensible for the browser, aren't usually what you'd like other clients (other web services, curl, etc) to have to deal with. 
For this I've also considered PureScript, as it seems to have the predictability benefits (size, speed) of TypeScript but you can program in a decent ML instead of, well, JavaScript. But having used Google Closure for serious use on codebases something like TypeScript seems indispensable now that it's here, for the situation you describe.
Routing is just interpreting the path of an URL: blah1/blah2/blah3 routes in Haskell Web development is like a fetish problem. Apparently detecting that an element in the path is an Integer or a String is the most important problem that web development has. And rejecting a route because a segment does not type match is the greatest achievement of the Haskell language if not the one of the entire civilization. (together with lenses)
I've run into issues with the maximum path length/maximum command line length on Windows. As far as I understand, this is an issue in the Node side of the story.
Also related https://github.com/gilligan/hoobuddy
This is awesome, and along the same motivation to what I'm proposing. Basically, here's my train of thought: - Say you have a codebase, and you open a file to edit in lambdu. This file defines types used in other modules. - Now, you make a pseudo-breaking change in one of the types (represented as a diff of the old version to the new one, in an AST - like a AST-level patch) - We __cannot__ commit this change yet, because the breaking change borks the other modules that depended on the old type - Thus, we need to build a _set_ of patches to submit atomically, s.t. all expressions dependent on the breaking change are also patched. - In the case of sum types and enumerated pattern matches (no wildcards), a new sum type would just require an additional matched expression. I'm not sure if this is along the same goal, but I plan to take this concept very far in a Haskell interpreter, to allow a running program to be modified via these atomically submitted patches.
Sounds great, I'm gonna have a look at Hoogle 5, maybe I can contribute something useful
A few notes regarding PureScript: - I installed purescript by cloning the repo and then building with stack - fairly easy IMO - You probably also need to install node.js and npm, bower and pulp (I think pulp is great) - I haven't used it yet, but there's seem to be some editor support with [psc-ide](https://github.com/kRITZCREEK/psc-ide) - The book [PureScript by Example](https://leanpub.com/purescript/) which is available for free is pretty thorough and up-to-date - There is a [this short tutorial](http://www.purescript.org/learn/getting-started/) about how to get started with PureScript
I love using scopedtypevariables like that, rather than having to include the monad type when you write it on the right side of the expression. Can you elaborate on why you find it hard to follow if possible (I know these things are often hard to articulate)?
Awesome, all contributions welcome! I should point out Stack has a ticket for Hoogle integration, which might end up being roughly akin to your script: https://github.com/commercialhaskell/stack/issues/55
I didn't run into any issues of that sort.
Hi, Thanks for the book recommendation. My professor actually recommended that as well, but I wanted to give it a shot myself before looking at the book.
I think the more relevant question here would be what the operation would be. Is it commutative?
So the unit-problem is "no problem" and the multiplication is just collecting problems to make bigger problems? This way you fulfill the laws as "no problem" really behaves like unit. But i would like to see the arrows between Problems and their concatenation .. and especially the category of Inter-Problem-Arrows.. as i would like to use a natural transformation to convert them to a category i can solve and then transform them back to the solved problem.
That's actually a very good suggestion. I haven't encountered these functors yet, I'm actually writing these articles while learning Haskell myself. I might just do that when I get there :)
I've been learning about these three things lately and while I seem to 'get' how they work, I don't yet understand their usefulness/purpose. Anyone care to enlighten me? To me they look like really fancy syntactic "sugar"
I liked the one about inventing ballistic algebra that runs in the type system before you can shoot yourself in the foot more.
[Image](http://imgs.xkcd.com/comics/hyphen.jpg) **Title:** Hyphen **Title-text:** I do this constantly [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/37#Explanation) **Stats:** This comic has been referenced 3006 times, representing 3.3481% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cxe9fw9)
I thought I was going to read a medium article that wasn't total garbage, but was immediately proven wrong when it started with the classic "functors are boxes" analogy. 
I just tried this out with the latest master on hoogle and I got this curious message: [181/237] xml... 0.02s [182/237] zlib... 0.03s Packages not found: abstract-deque abstract-par arb-fft attoparsec-enumerator blaze-html blaze-textual bytestring-builder bytestring-mmap cassava cgi criterion cryptohash curl diagrams enumerator erf extensible-exceptions Glob hastache hostname hslogger hspec hspec-core hspec-discover hspec-expectations HUnit ieee754 math-functions mime-mail monad-par monad-par-extras MonadCatchIO-transformers monads-tf multipart nats network-info postgresql-libpq quickcheck-io regex-base regex-posix regex-tdfa-rc rts setenv statistics tasty tasty-quickcheck test-framework test-framework-hunit test-framework-quickcheck2 time-locale-compat timezone-series transformers-compat unbounded-delays vector-binary-instances vector-th-unbox Found 58 warnings when processing items Reodering items... 0.02s Writing tags... 0.12s Writing names... 0.23s Writing types... 0.83s Took 12.91s I opened [an issue #142](https://github.com/ndmitchell/hoogle/issues/142) about it.
I'm gonna steal /u/ForTheFunctionGod's comment from [another thread about motivating AMP in teaching](https://www.reddit.com/r/haskell/comments/3tpom7/amp_how_do_you_motivate_this_in_teaching/) since that is what helped me "get" the point of it in the end :) --- You can naturally extend the intuition of a functor to Applicative, and from there to a monad. An explanation might go as follows: &gt;Let's say I have a list of numbers and want to add 2 to each. I'd write fmap (+2) [1..10]. But what if I had two lists of numbers and wanted to add each number from the first to each number from the second? Well, with Applicative, you can do that - it's like fmap, but with multiple arguments. You just write (+) &lt;$&gt; [1..10] &lt;*&gt; [11..20]. From there you, you can also motivate Monad by asking &gt;What if I wanted to abort midway through - say, if I encountered a negative number? I could write add xs ys = do x &lt;- xs if x &lt; 0 then [] else do y &lt;- ys if y &lt; 0 then [] else return (x+y) The three thus form a natural hierarchy of features: 1. Functor: apply a function to a container. 2. Applicative: apply a multi-argument function to multiple containers. 3. Monad: the same as Applicative, but I can decide what to do next after each step. --- EDIT: cleared up the formatting a bit (copying a comment wasn't as straightforward as I hoped, heh)
I suspect that your confusion is related to "what is a type class". I would recommend reading [the wiki page on Type class'](https://en.wikipedia.org/wiki/Type_class) or some other resource (I'm sure there are better ones). A small, but hopefully helpful extract from that: Type class' (which Functor is one of) are a way to guarantee a set of properties about everything that is under that typeclass. For example, a common type class is `Eq` - if something is an instance of the `Eq` typeclass, it means that it implements `==` and `\=`. The type class for `Eq` is given by, class Eq a where (==) :: a -&gt; a -&gt; Bool (/=) :: a -&gt; a -&gt; Bool which specifies the "requirements" of the type class. An instance would then describe how these two methods are implemented. For example instance Eq Integer where x == y = x `integerEq` y could specify how `Integer` is compared when using `==` by some internal `Integer` specific function `integerEq`. The inequality `/=` can typically be specified as the inverted result of `==`. Now, if we wanted to use the type class in our function, we would define it as a type constraint in the type signature like so, f :: (Eq a) -&gt; a -&gt; a -&gt; a f a b = a == b which simply states that the function takes in any type that has an instance of `Eq`. This all applies for `Functor` as well - it is a way to guarantee a set of properties. If you are familiar with other languages, they are kind of a way to provid overloaded functions (parametrically polymorphism).
With HAL there is a spec (in real-world use) that supports including (partial) additional results in one request.
re 2: That's why you have `rel`. You are free to invent new `rel`s. In fact: you will most likely have to. I do concur with your conclusion, that non HATEOS-resty APIs are easier to use. The fact that you will have to write your own tooling is not exactly nice. But if there were a, say, angular router that exposed a HATEOS service, it won't be more difficult.
Thanks. I think I understood now.
That adds nothing over what the Haddocks provide and is worthless for helping newcomers build any sort of intuition for what a functor is.
omg thank you very much for such a detailed answer. i couldn't run your example yet as i'm having trouble installing gloss-rendering but i thank you in advance for the trouble &lt;3
I always like to think of Functors/Applicatives/Monads as "providing an extra feature in addition to computing a value". The most basic thing a function can do is to compute a value: `f :: a`. If one adds a context like `f :: m a` (for a given Functor m), then the function will still compute value(s), but it will also do "something extra": in the case of Maybe, it might return 0-1 values, in the case of [] 0-*, in the case of IO, it might produce some side-effect, etc. I agree with you that the box-analogy isn't too helpful, but how would you judge the "extra feature"-analogy?
This is true, but if someone comes to you for help with functor, I suspect they've already tried reading the documentation and failed to understand it. At that point I would suggest using the following type signatures (among others) instead. fmap :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe b fmap :: (a -&gt; b) -&gt; [a] -&gt; [b] fmap :: (a -&gt; b) -&gt; IO a -&gt; IO b fmap :: (a -&gt; b) -&gt; Parser a -&gt; Parser b After some time with those, the more general pattern should emerge and the abstract type signature can be understood.
I think its more than this though as languages that can not express a monad as a concept still seem to benefit from creating abstractions that are monads. i.e. there is some value in functors/applicative/monads as a pattern as well 
I'm not the organizer, but I do attend most meetings. If you have any questions, feel free to ask.
It was simply outside my pool of known language extensions so far; one never stops learning! 
ghc -O2 -threaded Main.hs Main.hs:8:18: Could not find module `Graphics.Gloss.Rendering' Use -v to see a list of the files searched for. cabal install gloss-rendering Resolving dependencies... cabal: Could not resolve dependencies: trying: gloss-rendering-1.9.3.1 rejecting: base-4.6.0.1/installed-8aa... (conflict: gloss-rendering =&gt; base==4.8.*) rejecting: base-4.8.1.0, 4.8.0.0, 4.7.0.2, 4.7.0.1, 4.7.0.0, 4.6.0.1, 4.6.0.0, 4.5.1.0, 4.5.0.0, 4.4.1.0, 4.4.0.0, 4.3.1.0, 4.3.0.0, 4.2.0.2, 4.2.0.1, 4.2.0.0, 4.1.0.0, 4.0.0.0, 3.0.3.2, 3.0.3.1 (global constraint requires installed instance)
/u/funfunctional asked it crudely, but I honestly have the same question: Why does type-safe routing matter so much? It seems like there's a very large effort going into it, and I don't see a lot of benefit vs just having handlers that only take Text parameters.
People often refer to parsers and functions as "boxes" containing their result?
Interesting article. Thank you.
My particular use-case is frontend app in GHCJS with Reflex and backend in haskell. I used Servant to generate the web API these 2 parts use to communicate. The critical code is shared between these 2 projects so I get a 'guarantee' that the calls will work together with practically zero boiler-plate code. Ultimately it is reinventing SOAP, but it seems to me it works really well and cut my REST API coding practically to zero.
Case in point: [F*](https://fstar-lang.org) and baked-in monadic effects.
Impressive!
To anyone who can't wait for Parts 2 and 3 just read the chapters from Learn You A Haskell For Great Good in this order. * [Functors](http://learnyouahaskell.com/making-our-own-types-and-typeclasses#the-functor-typeclass) * [Applicative Functors](http://learnyouahaskell.com/functors-applicative-functors-and-monoids#applicative-functors) * Should also read about [Monoids](http://learnyouahaskell.com/functors-applicative-functors-and-monoids#monoids) here to really get the full picture * [Monads](http://learnyouahaskell.com/a-fistful-of-monads#the-monad-type-class)
Awesome. I've been thinking about doing something like this myself after doing my 2D version.
This is awesome, great work :) You can embed GIFs into GitHub READMEs.. the video would look good as one!
please be careful of the [monad tutorial fallacy](https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/).
That's a good idea. Thanks.
Just read that post, really eye-opening! I'll keep this in mind.
I can't really speak for everyone, but I'll share a little bit about my own experience. When I refactor an application and realize I need to edit half a dozen routes to change the type of resource they take, it's super helpful to have the compiler tell me everything that broke. Like many things, it's something whose value only shows up as an application gets larger.
`stack build` does create the binary but in a project-local directory. You can either (A) use `stack exec` to run the binary (as /u/Cliksum suggested) or (B) use `stack install` to copy the binary to something like the `~/.local/bin` directory.
Built and ran flawlessly on Windows. I looks great.
Ah nice one, I did a reasonably sized lambda calculus post/tutorial when I first learned about it a while back; its a beautiful thing!
Thanks for pointing that out. The `update a record` idiom is basically designed for the exact situation that you're describing. I really don't think there's enough complexity yet to warrant using `lenses`...
There's no reason you'd need to prevent allocations. That is, just because GHC may normally run GC during allocation doesn't mean that it's necessarily inextricable.
This is commonly called "signatures" (e.g. "module signatures) instead of "versions" in other material, and there is work being done to gradually move Haskell in this direction (see e.g. http://blog.ezyang.com/2014/08/whats-a-module-system-good-for-anyway/ ) 
yeah that's a cool idea. given that Haskell has static types (and "static names"), this should be possible. even today, with the GHC API, one should be able to get a list of all symbols used by a module (their types and fully qualified name). then simply index that by package. still, official support by Cabal would be nice. like a command that printed out this information $ cabal version text - Data.Text.pack :: String -&gt; Data.Text.Internal.Text ... or something. I also think this is related to backpack? where a cabal package can depend on "any package with these modules that have these names with these types" rather than "any package in this numerical range". 
I don't know of a way to do that, though it sounds like a neat feature request? There's the opposite: an explicit way to _perform_ gc: http://hackage.haskell.org/package/base-4.8.1.0/docs/System-Mem.html#v:performGC One could maybe run the executable with a huge memory allocation so GC basically only happens on demand, and then call the explicit collection function at designated points?
I don't think the documentation should be hidden. but as it says, it's unsafe/unstable.
I guess I thought that GC was only triggered when the heap is full. but you're saying that it is run more frequently? is that what the "minor cycle" does? 
As a side note, there is also the [Tokyo Haskell Meetup](http://www.meetup.com/ja/Tokyo-Haskell-Meetup/events/226787936/) (A different one) on December 13th.
I am not aware of a way to turn the GC off in OCaml. How do you do it?
In the web world, there's this "feature detection" idea instead of versioning
You can always post the permalink to comments: https://www.reddit.com/r/haskell/comments/3tpom7/amp_how_do_you_motivate_this_in_teaching/cx8an8b
You might find this note of oleg's amusing. https://mail.haskell.org/pipermail/haskell-cafe/2010-December/087842.html 
as of right now this is a little over my head because I'm only up to the end of the higher order functions section of learn you a haskell but I'm sure it'll make sense soon
There is an important principal in type theory called parametricity. Roughly, it states that if a function is parametrized over a type, then it ought to act uniformly over all types. In particular, as long as a type system wishes to maintain parametricity, we are not allowed to perform arbitrary typecase. That is, we cannot say, "If x is of type A, then do this, otherwise, do that instead." There are all sorts of reasons to love parametricity. It does wonderful things in the theory, giving us "free theorems" for all parametric functions. As a consequence, parametric functions are also severely limited in what they might be. A total function `a -&gt; a` in Haskell *must* be the identity function. A function `[a] -&gt; [a]` might throw elements away or reverse the elements, but any reordering or selection it makes *cannot* depend on any feature of the elements inside. Another nice practical consequence is that compilers may opt to generate uniform code for such functions. This is in contrast to what C++ does, where templated functions are potentially compiled once for every concrete type they are instantiated with. As long as a proper boxed notation is used for all objects, the same object code for `reverse` works on any sort of list. Lastly, by not relying on type information at runtime, this information may be completely erased at the end of compilation. So rather than trying to solve this problem, it is better to think about ways to avoid it. From a Haskeller's point of view, I would be very suspicious of code which needed nested lists without a fixed amount of nesting. Such notions are possible in languages with more sophisticated type systems. (This would be a good candidate problem for what we sometimes call [generic programming](http://www.seas.upenn.edu/~sweirich/ssgip/main.pdf)). But alternatively, you can lower your ambitions a little, and rather than try to parametrize over all possible circumstances, write the function which maximizes the product of convenience and feasibility in the type system. 
This is actually a part of the beauty of Haskell's type system -- it enforces **parametric polymorphism**. That means, if you say that the type of your function is `foo :: a -&gt; a`, or `foo :: [a] -&gt; [a]`, that means that it has to be implemented **identically** for *any* type `a`...no arbitrarily branching for different types. For example, there's only one meaningful function of type `foo :: a -&gt; a`. It's foo x = x But, what about foo :: a -&gt; a foo x = x + 1 ? Well, not all functions have a `+` function defined on them. This doesn't work; it's not legal; the single implementation has to work for a value of *any* type you put in, without being able to distinguish. Things like this are a part of what gives Haskell's type system such strength. You're actually allowed to make extremely powerful proofs and statements about your programs *just* from the type of your functions alone. For example, consider the classic "sequence" function that takes a list of IO actions and returns a big fatty IO action that does all of them one after the other: sequence_ :: [IO a] -&gt; IO () But let's say you're using a library and someone offers you something of that type. Can you trust it, based only on the types? Well, a function that is written as an `[IO a] -&gt; IO ()` returns an `IO ()` that can do *any* arbitrary `IO`. A badly written `sequence_` implementation might sequence all your actions, and then launch a missile. Or it might ignore all your actions, and just wipe your hard drive. All implementations are possible, given that type signature. Now, what if you had written: sequence_ :: Monad m =&gt; [m a] -&gt; m () Now, you can feel a bit more comfortable using this on your `[IO a]`. That's because you know that whatever `IO ()` you get from using `sequence`, it can't do arbitrary `IO`. It can't launch any missiles. The worst it can do is perform none of your actions, or do all your actions backwards. Because the function is written with *parametric polymorphism*, you know that it can't check if it's being called on an `[IO a]`, inject in a `launchMissiles :: IO ()`, and leave you none the wiser: sequence_ :: Monad m =&gt; [m a] -&gt; m () sequence_ (x:xs) = if (type? x = IO a) then launchMissiles else return () This looks like a silly example, and you're not always going to be working against evil co-workers trying to launch missiles. But you're always going to have to re-factor, be re-reading your own code, trying to guess what your code does. And because of parametric polymorphism, you have greater tools to reason about what your code can't or can't do. Also, on a practical level, types are erased at compile-time, so there is no type information to read at run-time anyways :) By the way, there's a typeclass that simulates "pairing" your data with a run-time tag of the type. It involves a typeclass: foo :: Typeable a =&gt; a -&gt; String The `Typeable` typeclass means that the value of type `a` has implemented a way to allow you to retrieve its type information at run-time. Of course, though, if your function has a `Typeable` constraint, you lose a lot of guarantees about what it can or can't do. Consider the only implementations of a function `foo :: a -&gt; String`: foo :: a -&gt; String foo _ = "hello!" The only possible implementation of a `foo :: a -&gt; String` is a constant function. It has to ignore the parameter. That's a surprising requirement of parametric polymorphism! But what about `foo :: Typeable a =&gt; a -&gt; String`? You could have `foo` get the type of `a`, and then print out something different based on the type! In fact, many typeclasses in general give you a "slightly less powerful" parametric polymorphism. The more typeclasses you add, the less conclusions you can make. But the best typeclasses hit the sweet spot where you can still make a lot of meaningful conclusions (like with `Monad` above). For an example of another typeclass that lets you write non-constant `a -&gt; String`s, consider: foo :: (Num a, Eq a) =&gt; a -&gt; String foo x = if x == 0 then "I am zero!" else "I am not zero :("
The Typeable type class is the class of precisely the types that can be compared for type equality, so this could help you determine if the element of the list is itself a list. However, your implementation is unnecessary checking each term to see if it is a list. You only need to check once, if any item in a list is another list then **all** items in that list are also lists. This is guaranteed by the type system. That is, the list [1,[2,3]] is a type error and you will not be able to create it at all. You will either have [1,2,3] (a list of numbers) or [[1],[2,3]] (a list of lists of numbers). Also your Racket code is bugged. You've used a cons where you want an append. You are consing the whole flattened list onto the flattened rest of the list, and since this is in fact the key part of the function, your flatten function actually does nothing at all. Incidentally, an enforced list type would have caught this error. You're trying to do the same with the Haskell code, but it won't let you.
As a not very serious answer: if you only use [primops](https://hackage.haskell.org/package/ghc-prim-0.4.0.0/docs/GHC-Prim.html), primitive arrays, unboxed tuples and tail recursion, then you can make quite sure that GC won't happen (heap/stack allocation serves as entry point for GC and interrupts).
I'm mostly posting it here to get a "fact check" on what I'm saying. While it's certainly no tutorial on how to use these type class, I cannot help but feel responsible in sharing what finally (at least to me) helped me understand Monads a lot better. Or at least the purpose of them. Hopefully it is also a help to someone else than me. In short, any feedback is welcomed :)
What is it that you don't like about the offered solutions though? Several of them factor out the flattening to one place, whereas your suggestion violates the "don't repeat yourself" principle. Is it the NestedList type you like?
There is some fun to be had with atom + the Haskell plugins. Sometimes it complains that ghc-mod has crashed and stops working. Eclipse useres may feel home there. :-)
There is [EclipseFP](http://eclipsefp.github.io/), but its development has been [discontinued](http://jpmoresmau.blogspot.de/2015/05/eclipsefp-end-of-life-from-me-at-least.html). There is [Haskforce](https://github.com/carymrobbins/intellij-haskforce) for IntelliJ Idea, but I haven't tried it yet. Also [Leksah](http://leksah.org/) seems to make progress with a standalone Haskell IDE. But most people here are vim or emacs users in general, who have added Haskell IDE functionality to their editors via plugins, which rely on certain command-line programs (like ghc-mod) for richer IDE support. There is also [spacemacs](https://github.com/syl20bnr/spacemacs), an emacs customized to behave like vim, coming with a rich preconfigured [layer of haskell emacs plugins](https://github.com/syl20bnr/spacemacs/tree/master/layers/%2Blang/haskell). I recommend using an emacs/spacemacs setup, because the emacs plugins are actively developed and the most mature to my knowledge.
Another way to characterize Monad: it is an interface allowing "flattening" operations that transform double layers of a "type constructor" into a single layer. In Haskell, this is the &gt; join :: Monad m =&gt; m (m a) -&gt; m a function. For example, if you have a list of lists, you often want to flatten it into a single list. If you have an IO action that returns another IO action, it is natural to want to sequence both into a single IO action. If you have a function from X that returns another function from X, you may want to construct a single function from X by passing the same argument twice. And so on. This flattening operation must respect some [intuitive laws](https://wiki.haskell.org/Monad_laws). If you have a list of lists of lists (three layers) and want to flatten it into a single list, you have two options: start flattening "from the outside", or begin by flattening the "inner lists" first. Both approaches should give the same result (this is an associative law, kinda like the one monoids must satisfy). The "flattenable" view of monads is related to the "choose what to do next" view. When you "choose what to do next", what you are really doing is mapping over a monadic action using a function that produces another action (instead of a pure value) and then joining the result. The "what to do next" view: &gt; readStr &gt;&gt;= putStr "I decide what to print to console based on the result of the previous readStr computation" The "flattening" view: &gt; join (fmap putStr readStr) "I map over the result of readStr using putStr, getting a nested **IO (IO ())** action, which I then sequence using **join**".
Sorry to nitpick, but don't think you meant to include foo :: [a] -&gt; [a] as a function that has a single implementation. a -&gt; a certainly has only a single implementation. 
Win 10, 64 bit. Maybe problems with the OpenGL version?
It's a bit sad that Vim and Emacs are the go-to solutions for writing Haskell. Neither is particularily user friendly, to put it mildly, and learning them distracts from learning Haskell.
I could use a second job. Of course, I'd have to get the transcription thing down first. And stop killing computers that have the relevant documents on them.
Well, you know, that's just, like, your opinion, man ;-)
I can wholeheartedly recommend spacemacs.
I never thought I would see the day where someone puts “Eclipse” and “Quality” in the same sentence.
Make sure you get your graphics driver directly from the vendors site (ie nvidia, amd or intel). The driver provided by windows update doesn't include OpenGl support.
Same here. I noticed I had `stack 1.6`, I'm upgrading to see if it fixes something edit: `stack 1.8` get me less warnings about the build, but same error trying to execute. I'll check what /u/tobbebex said and see if my nvidia driver has opengl stripped. 
Really? I'd find that surprising, doesn't windows update uses the driver provider by the vendor, just lagged behind? edit: this is strange, system viewer from nvidia only gives me a directx version, and I'm quite sure I had upgraded the gpu driver recently. edit: GLView tells me my intel driver is only GL 3.1 compatible, and boids.exe expects GL 3.3. Damn!
Fantastic post. This was the post that taught me how to sit down and work out the type of a nontrivial expression step-by-step. Well done! 
Haskell's type system will let you do it without that. newtype Mu a = Roll { unroll :: Mu a -&gt; a } fix f n = helper (Roll helper) n where helper h'@(Roll h) n = f (h h') n
I dont like how the solution such as the foldr one flattens an element into a list of a single element and concatenates a bunch of 1 item lists as the last step, I'd rather cons the element itself onto the list. It seems like the way it's being done is sort of almost flattening it and then using some trick to get around the fact that youre left with not the element itself but a list of it
I actually did mean to include it as an example of parametric polymorphism -- you must give one uniform implementation for any `a` :) no branching on whether or not `a` is a list, like in OP's suggestions
As long as your types implement `Typeable` and you're ok constraining to that, you can. But any solution that uses `Typeable` is the wrong solution.
You can do this using typeclasses and some extensions. Note this is a hack and not recommended for real use! Because it uses UndecidableInstances, you have to provide a type signature. {-# LANGUAGE MultiParamTypeClasses, FunctionalDependencies, FlexibleInstances, UndecidableInstances#-} class Deepflatten a b | a -&gt; b where deepflatten :: [a] -&gt; [b] instance Deepflatten a b =&gt; Deepflatten [a] b where deepflatten = concatMap deepflatten instance Deepflatten a a where deepflatten l = l λ&gt; deepflatten [[1, 2], [3, 4]] :: [Int] [1, 2, 3, 4] λ&gt; deepflatten [[[1, 2]], [[3, 4]]] :: [Int] [1,2,3,4] λ&gt; deepflatten [[1, 2], [3, 4]] :: [[Int]] &lt;interactive&gt;:20:1-11: Overlapping instances for Deepflatten [Int] [Int] arising from a use of ‘deepflatten’ Matching instances: instance Deepflatten a b =&gt; Deepflatten [a] b -- Defined at /tmp/test.hs:5:10 instance Deepflatten a a -- Defined at /tmp/test.hs:8:10 In the expression: deepflatten [[1, 2], [3, 4]] :: [[Int]] In an equation for ‘it’: it = deepflatten [[1, ....], [3, ....]] :: [[Int]]
So, the logical conclusion here is that you're going to be making a monad tutorial soon and I feel I should give some advice because you're falling into some similar traps that I've seen other tutorials fall into. Please focus more on why one should learn this stuff in the first place. You're explaining Applicatives with base case examples, but that's just making the concept of an Applicative more confusing because now the reader is trying to understand what is so awesome about this: pure (+) &lt;*&gt; Just 3 &lt;*&gt; Just 4 And this: (+) &lt;$&gt; [2,3] &lt;*&gt; [4,5,] Anyone learning Haskell would look at that and think "Wait, so what's the point of this Applicative thing, how is that solving any actual problem? Why is this worth the trouble of learning?" Basically, the classic algebra classroom question: "When will we ever actually use this in our actual lives?" Sure, your example above is something that would be awkward to write out without Applicatives, but it doesn't show why Applicatives have so much utility in a purely functional language and are totally worth the learning curve. I recently encountered the [optparse-applicative](https://hackage.haskell.org/package/optparse-applicative-0.12.0.0) library while working on a command line interface for a project I'm engaged in right now. I had originally written the interface in Python a year ago using Python's argparse library, but I found optparse-applicative to be very useful and productive for making what was a relatively complex command line frontend - it also turned out to be a glorious example of applicatives (as well as monads) in action. The library works basically by composing together lots of little computational descriptions of parsers in relation to each argument, subcommand, and flag passed by the user to the program. The result is a single composite parser that can correctly parse rather complex commands and pass off the extracted information to the relevant event handling code. If it can't parse what it's given, it will throw a wonderfully formatted help message pertaining just to the invalid argument or flag the user tried to pass. I'd recommend it to anybody, the author included lots of helpful examples that were essential to picking up the library quick and using it for real things. I could give similar praise to Parsec and Attoparsec. My point is, that library was amazing because of Applicatives and Monads. It let me do cool things easily in Haskell that WOULD NOT WORK if Haskell wasn't capable of creating EDSLs as effortlessly as it does. And the reason those EDSLs can exist is because Applicatives and Monads allow us to play around with computational descriptions of whatever we want via function composition independent of execution time. I have found from writing actual Haskell code that such a capability, though general as hell, is pretty useful to have and makes a bunch of things pretty straightforward that otherwise wouldn't be in a purely functional language. And I'm not sure if such a conclusion can be obtained without actually using Applicatives and Monads in the field or at least experiencing what it's like to program in Haskell without such features. They're pretty simple at the end of the day, but I distinctly remember them NOT being simple back when I was trying to learn them. I over-complicated them and I feel many do the same. Perhaps the best way to explain them would be to explain, through careful and exhaustive examples, a library that makes heavy use of them. The examples you *do* give would be good in that context, because they isolate Applicatives in they're most basic use case, but you need be able to zoom out and show them as part of a larger solution to realer problems. Hope this was coherent and good luck on the Monad part!
I'm sad that the e-book you recommend at the end is paid :( Would love to read that one
Yes and no! But first, let's get rid of those dynamically-typed habits. In Racket, lists are heterogeneous: they can contain elements of different types, such as a list of ints, followed by an int, followed by a string: `(list (list 1 2) 3 "foo")`. In statically-typed languages, we prefer not to use heterogeneous lists, because we want to know at compile time which types the elements of the list could be, in order to make sure our code covers every possible case. If we receive a list of type `[Int]`, we'll know for sure that the elements will all be ints (something like `[1,2,3]`), and if we receive a list of type `[String]`, we'll know for sure that the elements will all be strings (something like `["foo","bar","baz"]`). It's not technically possible to have a Haskell list which contains elements of different types. Each expression must have a type which is known at compile time, but for the list `[1,2,"foo"]`, there is no type `A` we could write which would allow the expression to have type `[A]`, so such a heterogeneous list is illegal in Haskell. Of course, sometimes we do want to pack elements of different types inside a single list. One common trick to achieve this is to create a new wrapper type with one alternative for each type of element we want to put in the list, and to use a list of wrapped elements instead of a list of elements. For example, if we want a list whose elements could either be ints or strings, we'd create a wrapper type with one alternative for Int and one alternative for String: data Wrapped = WrappedInt Int | WrappedString String And now we can write our list as `[WrappedInt 1, WrappedInt 2, WrappedString "foo"]`, and it will have type `[Wrapped]`. It's a bit verbose, but in exchange we have gained a new ability: we can now use pattern-matching to determine which type each element has. For example, suppose we wanted to compute the sum of the int elements and of the length of the string elements. In Racket, we'd write something like this: (define (heterogeneousSum xs) (cond ((empty? xs) 0) ((string? (first xs)) (+ (string-length (first xs)) (heterogeneousSum (rest xs)))) (else (+ (first xs) (heterogeneousSum (rest xs)))))) When I see code like that, I worry about what will happen in the `else` part if the list contains a value which is neither a string nor an int. In Haskell, I can be sure that I have covered all the cases, because the type `Wrapped` ensures that there are only two cases: heterogeneousSum :: [Wrapped] -&gt; Int heterogeneousSum [] = 0 heterogeneousSum (WrappedInt n : xs) = n + heterogeneousSum xs heterogeneousSum (WrappedString s : xs) = length s + heterogeneousSum xs So yes, it is possible to tell which type an element has, by pattern-matching on the `WrappedInt` and `WrappedString` constructors we've created to distinguish the two types. And no, there is no `string?` predicate which can tell you whether an arbitrary value is a string or not, you need to have a type such as `Wrapped` which already has the information you need. All right, let's implement `flatten` now. Clearly, this function expects an heterogeneous list: some of the elements will be lists, and others will be atomic values. So let's begin by creating a wrapper type for those different types of elements: data NestedList = WrappedAtom Int | WrappedList [NestedList] For simplicity, I created a type in which the atomic values are always ints. For the case in which the element is a list, I used the type `[NestedList]`, because this nested list might itself contain more deeply-nested lists. An element of type `NestedList` could look like this: `WrappedList [WrappedList [WrappedAtom 1, WrappedAtom 2], WrappedAtom 3]`. Finally, we can now use pattern-matching to implement `flatten`. We still don't have a `list?` predicate, but we don't need to: when we have a value of type `NestedList`, we simply use pattern-matching to determine if the value is a wrapped list or a wrapped int. flatten :: [NestedList] -&gt; [Int] flatten [] = [] flatten (WrappedAtom n : xss) = n : flatten xss flatten (WrappedList xs : xss) = flatten xs ++ flatten xss Oh, and one final thought. Since we can use pattern-matching to tell whether a `Wrapped` value is an int or a string, we *can* implement a `string?` predicate, but it won't be very useful. isString :: Wrapped -&gt; Bool isString (WrappedInt _) = False isString (WrappedString _) = True If we try to use `isString` on a value `x` of type `Wrapped` and `isString` returns `True`, it won't be very useful because `x` still has type `Wrapped`, so it still won't be possible to use `x` as a string: -- Couldn't match expected type ‘[Char]’ with actual type ‘Wrapped’ -- In the first argument of ‘(++)’, namely ‘x’ -- In the expression: x ++ "!" addExclamationPoint :: Wrapped -&gt; String addExclamationPoint x = if isString x then x ++ "!" else "not a string" In a dynamically-typed language, you the programmer know that if the `(string? x)` branch executes, you can safely treat `x` as a string because it must be a string. In a statically-typed language, it is not enough for you the programmer to know that `x` is a string, the compiler must also know, otherwise it's a type error. Pattern-matching is better than if statements in that regard because it gives type information both to you and to the compiler.
In the end I just chose to keep the syntax highlighting and resort to external tools for everything else with Atom and the Haskell plugins. On the bright side they get very frequent updates so the problems may be fixed soon.
People like to dump on Eclipse, but for the sheer amount of stuff it allows you to do I think it's a quite a good piece of engineering. I've been using it for Java development since it first came out and I have encountered my share of irritants, but overall I've been quite happy with it. Now, for everything else, including Haskell development, I use Emacs. But for Java, there's no contest. (Feel free to substitute NetBeans, etc.)
Great comment, I will definitely keep this in mind for the monad article!
Even agreeing that they are more efficient, for a newbie they become yet another difficult thing to learn while learning Haskell. 
Ah, I see what you mean now. I thought you were referring to the space of implementations.
Subtype constraint systems type the Y-combinator, no problem; I guess you can argue that they're not sane, though.
Haskellers tend to prefer investing energy up-front to save energy in the long run. Learning vim/emacs is part of that -- they're the best editors, and learning them sucks, but once you do it, the payoff is huge. Haskell is similar, in that it is difficult to learn up front and takes a while to get productive, but once you are productive, you're doing a lot better than you were previously. At least, this is *my* experience with learning vim and Haskell...
Without `pure`, you cannot create `f (a -&gt; b)`. So the only reason you can use `Just (+)` is because `Maybe` could be given an applicative instance. Without `pure`, you could not make an `IO (a -&gt; b)`. That is something that you cannot do with just `Functor`. 
No.
Stop teasing me. I don't have a way to pay for it
I'll need about a billion examples before I finally figure out what `Applicative` is about. Can someone give me real world examples of applicatives that aren't monads?
The `Validation` Applicative is the most useful -- `data Validation e a = Correct a | Errors [e]`. The `Applicative` instance does this: Errors e &lt;*&gt; Correct _ = Errors e Correct _ &lt;*&gt; Errors e = Errors e Errors e &lt;*&gt; Errors e' = Errors (e &lt;&gt; e') You can't write a Monad instance for this that keeps the accumulating errors.
how does luminance compare to GPipe? It looks like it has been resurrected recently.
Thanks! What's `&lt;&gt;`?
That's an operator for `mappend`, for lists it is `++`
(&lt;&gt;) = mappend
When I read "not that one" I thought this was about Paul Graham's company. Doesn't everyone think of the lambda calculus y combinator first? I guess not.
One classic example is formlets/reform. With traditional web forms (before javascript, etc), you had three things you needed to do: 1. display a form 2. validate the form data submission and extract the data 3. if there is a validation error, render the form with the submitted data, plus validation errors If you try to implement a `Form` as a monad you can end up with something like: do foo &lt;- inputText Nothing bar &lt;- inputText (Just foo) But this is problematic -- if the first input field fails to validate, it won't produce a result. That means we can not evaluate the second `inputText` function, which means that we can not rerender the form with submitted data + validation errors. One solution is to simply not have a monadic instance for `Form`. That gets rid of the `&lt;-` (aka `&gt;&gt;=`), so you can no longer express the troublesome computation. Instead you write: pure (,) &lt;*&gt; inputText Nothing &lt;*&gt; inputText Nothing There is no way the second `inputText` can depend on the result of the first `inputText` hence, the problematic situation can not be written. Error accumulation is another example. Along the same lines as the above example, imagine you are implementing some RESTful server and you need to check that a client passed all the query parameters to some API call. If you use a monadic instance like: do x &lt;- getVar "x" y &lt;- getVar "y" return (x,y) You could have `getVar` automatically return an error when a parameter like `x` or `y` is missing. However, it is going to stop at the first missing parameter and only return a single error. If you the caller was missing both `x` and `y` you'd like to return an error message that indicates that. If you restrict yourself to `Applicative` then you'd have: pure (,) &lt;*&gt; getVar "x" &lt;*&gt; getVar "y" And now it would be possible to check for the existence of all the vars, and then return a list of missing values. 
Every value representing an applicative computation is also a value representing a monadic computation (where you ignore the result to compute the next effect) So you won't find real world applicative which are not monadic.* But when you *declare* a value to be of (the more restrictive) applicative kind that means that you have more ways to use it as you can *assume* it has fixed, and not dynamic, effects. It allows for more static computation, optimization etc.. See applicative parsers for this. There are on the other hand monadic computations which are not applicative. Edit : * By that I mean at term level. at instance level things are reversed.
Did you try Scala.js? It's a way better language than JavaScript/TypeScript, has better IDE support than JavaScript/TypeScript/Haskell, doesn't require JavaScript's insane build setup, emits small JS files, is stable and mature, and is used in production.
Very cool; thank you for sharing! I'm trying to learn how to use gpipe (or whatever will let me do shader writing the easiest), so it's great to have an example. Particularly because you've provided top-level type annotations. The gpipe tutorials I was looking at didn't do that, and it makes it a lot harder for me to learn. I noticed a use of forM in makeBoids (Boid.hs) that could easily be replaced with replicateM; slightly cleaner. That's about the extent of my Haskell knowledge, though. One thing the other gpipe tutorials do, that this program also does, is write the main loop to recurse explicitly. How hard is it to separate that out from the body of the loop? I spent some time trying on a gpipe tutorial but I've had trouble doing it.
Do you compare with pure ghcjs or after closure compilation ?
I did not say that today so let me say it : stack is awesome
Indeed that's the same need of updating the state of things, collecting opinion which has value in and of itself
&gt; Instead, use packCStringLen after massaging your data into a CString of some sort. May or may not be relevant depending on your actual use case. &gt; &gt; Re-implement the createAndTrim function in your own code, using only stable functions, thus ensuring that it stays stable for the near future. Well, that will be slower than `createAndTrim`... as I understand it, `packCStringLen` always copies, while `createAndTrim` doesn't.
It's interesting to know of other attempts in the same space than optparse and friends. Thanks for sharing your opinion on the Python experiments 
Pre minification, Haste code is generally 6-12 times smaller. Post minification, the difference shrinks to 3-10 times. For larger programs, the difference tends towards the lower end of the scale. For performance, the figures are without minification. GHCJS programs break if you run Closure with advanced optimizations over them, so I haven't been able to compare. However, I wouldn't expect minification to make a dramatic difference in execution times. Haste programs are generally more or less unaffected (WRT performance) by minification. Some programs become faster by at most ~10%, while some actually become slower by a few percent.
its just one way to use the monadic interface : One that short circuit. Monadic is saying that you **can** potentially use the value returned to determine the next effect, for instance to short circuit. But of course another instance of monadic is if you don't use the value that you can use.that is also of course a monadic computation. **stating** that you don't use the value is what makes your computation applicative
[ZipList](http://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Applicative.html#v:ZipList) can't be extended compatibly to a `Monad`. [Validation](https://hackage.haskell.org/package/either-4.4.1/docs/Data-Either-Validation.html), can't be extended compatibly to a `Monad` and lets you extract _multiple_ errors not just one. [Concurrently](https://hackage.haskell.org/package/async-2.0.2/docs/Control-Concurrent-Async.html#t:Concurrently) can't be extended compatibly to a `Monad` but lets you run a number of computations in parallel. (There is a `Monad` instance that is broken, but [it is being removed](https://github.com/simonmar/async/pull/26).) Notably, you know the issue with how arbitrary monads don't compose and so you need transformers to hack around this? Well, Applicatives do: instance (Applicative f, Applicative g) =&gt; Applicative (Compose f g) We can make [parsers](https://www.fpcomplete.com/user/edwardk/heap-of-successes) that take advantage of the limitations of `Applicative` to parse more efficiently, even if they do extend to become a full fledged Monad, the Applicative can be vastly more efficient. You can use observable sharing to build an `Applicative` CYK or [Valiant](http://www.cse.chalmers.se/~bernardy/PP.pdf) parser, extending the reach of parsing combinators from LL(*) to arbitrary context free grammars. Many syntax tree forms can be built as an Applicative in such a way that you can exploit the ability to introspect on the structure of what will be computed. The [haxl project at Facebook](https://www.youtube.com/watch?v=IOEGfnjh92o&amp;index=5&amp;list=PLnqUlCo055hXArE00SkORNiK9fk54de2a) takes advantage of this extra introspection to find exploitable parallelism. A contravariant form of Applicative powers my work on [`discrimination`](https://www.youtube.com/watch?v=cB8DapKQz-I). data Moore a b where Moore :: (s -&gt; b) -&gt; (a -&gt; s -&gt; s) -&gt; s -&gt; Moore a b forms a nice `Applicative` and can be made into an efficient way to fuse together multiple left-folds over a data structure. It admits a `Monad` structure, but the `Monad` is grossly inefficient, the `Applicative` is awesome, though. [`optparse-applicative`](https://hackage.haskell.org/package/optparse-applicative) uses the ability to introspect on an `Applicative` to provide the option parsing, help messages, autocompletion, etc. all from one Applicative computation.
One of my favorite uses for applicative style is constructing things only if _all_ fields really exist, e.g. data Thing a = Thing Int Bool a thing :: Maybe Int -&gt; Maybe Bool -&gt; Maybe a -&gt; Maybe (Thing a) thing a b c = Thing &lt;$&gt; a &lt;*&gt; b &lt;*&gt; c And parsing is quite nice in applicative style as well: parens p = char '(' *&gt; p &lt;* char ')'
-8 it must be bullshit. If only one of those silent voters could be so kind as to enlighten the world as to how it is so...
What do you mean by "subtype constraint systems"? My first thought were refinement types a'la LiquidHaskell or F*, but I don't see how would that help.
It is a phenomenal book. Even as a vaguely intermediate Haskeller, I've been picking up some great stuff.
* [Spacemacs haskell settings](https://github.com/syl20bnr/spacemacs/tree/master/layers/%2Blang/haskell) * [This helped me personally, because I had issues with the path were the cabal things are stored](https://touk.pl/blog/2015/10/14/getting-started-with-haskell-stack-and-spacemacs/) I don't know if they fixed it but they had some problems at master, but the issues where fixed in develop branch which is also good.
It's not quite the same. If your `Parser` type is monadic then it offers the capability for the user to "sequence" things with `(&gt;&gt;=)`. The environment analysis would then have to stop each time such a sequencing occurred. By offering the ability to sequence, we throw away the promise of complete analysis. That's the tradeoff. As another example, this is how Haxl works. It offers (I think) a monadic interface, but whenever you have a chunk of computation which only uses applicative combinators it explores it statically and tries to execute things in parallel. It's a little confusing since it's hard to see where parallelism will occur, but it's nice in that semantically, excepting slowdown, there is no difference in the result. Another another example is parsing with something like UU's parser library. This one actually offers different semantics based on whether or not it can accomplish sufficient static analysis in the "applicative-only" parts of your parser.
I tried the TypeScript support, but wasn't too excited about it. It kinda-worked, but not reliably. And dragging all this npm mess along didn't create a good impression for me either. I went with Scala.js after that. Just works, leveraging the standard tools that already exist. Not being restricted to the front-end/extremely small ecosystem is a benefit, too. Being able to take existing backend code and saying "this would be a cool as a web app, let's replace some of the IO stuff, and put some HTML in front" is pretty amazing.
This is the setup I'm currently using, but I've avoided integrating the build system that comes with haskell-ide because as I understand it it still depends on cabal, and I want stack.
Correct. But being applicative is all about reflecting *at type level* that you don't need to inspect the value. You can build monadic computation which never inspect the value but the consumer will have to assume you do if it wants to stay type safe. So the situation is actually reversed : some monadic computation are applicative but all applicative computation can be declared monadic. At instance level it goes the other way around as every monadic instance (standing for all the terms where you *can* inspect the result to determine the next step) gives rise to a applicative instance where you *force* the terms to ignore the result of the computation to determine the next step. Happy to be corrected constructively ..
There has just been a bit of a lack of exceptionally well written material in the space. While LYAH is certainly good, I don't think the teaching style is that good, and Real World Haskell is getting quite outdated, although most should of course still apply. Ideally it would be free, but if you have the oppurtunity and can buy just one book, this one is definitely the one I recommend (and I'm not even getting any affiliate or anything, just super enthusiastic :).
&gt; If you try to implement a Form as a monad you can end up with something like: &gt; &gt; do foo &lt;- inputText Nothing &gt; bar &lt;- inputText (Just foo) &gt; &gt; But this is problematic No problematic in essence. What is problematic is his interpretation and to find a possible use of that. The monad instance of formlet is what MFlow uses for creating interactive server side pages which change the layout and content depending on the responses of the user. see the section: Single page applications at https://themonadreader.files.wordpress.com/2014/04/mflow.pdf In MFlow the formlets are not a mere addition in order to create forms, but the way to create self contained composable page components. 
It was also my understanding that it could capture the idea that sequencing does not matter. But in the presence of IO, or event State, can we ever say that sequencing does not matter ? It would make sense to try to comply to this idea, but that's not in the type. On one hand as soon as you want to work for any effect, order matters, on the other it makes sense to say that it does NOT since we are stating that the result is not threaded, so that would be sort of a semantic-ish contradiction. It's a part which I am not clear about, but I did see quite a few people are not either actually. Must be some interesting thing going on :) There are some clear leeway in the implementations in Haskell, not forcing you to abide by the laws of X. I guess we have a tension here between what those laws would be for a general effect which would mandate sequencing, and what we intend to express which would mean non sequencing. I feel that's a more subtle point than what I made though.
Which one ?
&gt;We can make parsers that take advantage of the limitations of Applicative to parse more efficiently, even if they do extend to become a full fledged Monad, the Applicative can be vastly more efficient. Also: https://hackage.haskell.org/package/Earley
You have the relation exactly backwards. Every monadic computation is an Applicative computation, but not every Applicative computation is Monadic. Monadic contexts have to support `join`. Join lets you do *more* than just `pure` and `fmap`, but not every context that supports `pure` and `fmap` supports join. There are multiple examples of Applicatives with real world value that are not Monadic. Edward Kmett listed three further up the page.
All of them? * return a &gt;&gt;= k = k a * m &gt;&gt;= return = m * m &gt;&gt;= (\x -&gt; k x &gt;&gt;= h) = (m &gt;&gt;= k) &gt;&gt;= h I don't see how would you write a `&gt;&gt;=` satisfying any of these laws if you ignore what is inside the first argument.
What is a monadic computation for you? Is it an instance or is it a term? Every monadic *instance* gives rise to a applicative instance that we agree with. But not all monadic *term* give rise to an applicative term, only the one which do not inspect the outcome of previous computation can be turned into an applicative one. I called on mr Kmett (which I agree with as well ) to clarify exactly because of that confusion. 
It's interesting AFAIK those laws are about making the Haskell monad, monads in the sense of the Hask category. If category is the concern, there must be some category for which ignoring the argument is a monad. constant functions as arrows and * as object ? 
Perhaps a different definition of applicative would help. Haskell base defines it in terms of pure and &lt;*&gt;. Traditionally it is defined in terms of two functions like this: unit :: f () pair :: f a -&gt; f b -&gt; f (a,b) Basically an applicative can have produce () without effects (which can then be fmapped to get pure), and you can attach two applicatives together. Exercise: Define unit and pair in terms of pure and &lt;\*&gt;. Define pure and &lt;\*&gt; in terms of unit and pair.