You can with [base-unicode-symbols](http://hackage.haskell.org/package/base-unicode-symbols-0.2.2.4/docs/Prelude-Unicode.html).
Is called "bottom" if you want to google it more effectively 
Thanks to @cdsmith for giving a talk on this yesterday. Even as someone already familiar with fixpoints, it was thoroughly enjoyable. It was mentioned in the blog post, but if anyone in the Atlanta area wants to come to a Haskell hackathon (Haskathon), let me know - I organise it. It's held once a month, and we look for people of all experience levels. There is an emphasis on hacking. We have a twitter account: twitter.com/haskathon
I do love Googling bottoms.
If we just think in terms of 1-element lists: f &lt;$&gt; [x] == [f x] f &lt;$&gt; [x] &lt;*&gt; [y] == [f x y] f &lt;$&gt; [x] &lt;*&gt; [y] &lt;*&gt; [z] == [f x y z] [f] &lt;*&gt; [x] == [f x] [f] &lt;*&gt; [x] &lt;*&gt; [y] == [f x y] In other words, `&lt;*&gt;` allows you to apply a function to an argument in which both the function and argument are under applicative functor. Contrast that with `&lt;$&gt;`, where the function is *not* under the functor.
Should be [https://www.haskell.org/ghc/download\_ghc\_8\_4\_4.html](https://www.haskell.org/ghc/download_ghc_8_4.4.html)
The observation that `fix error` does not in fact fix any errors is hilarious.
It's generally used like (+) &lt;$&gt; readLn &lt;*&gt; readLn Which reads two numbers from stdin and adds them. 
I think the similarity to lens can be explained via conal's referenced paper -- the twanvl lens representation is a yoneda transform of a standard lens representation, just as reverse mode ad can be seen as a yoneda transform of forward ad?
I do agree that my post is basically a small rewording of a subset of Conal‚Äôs paper, putting it into I hope familiar terms. The twanvl lens is what I call a fancy lens. I don‚Äôt believe that the relation between forward and reverse mode is analogous to the relation between twanvl rep and the simple rep. I used both the twanvl and simple rep interchangeably for reverse mode. Perhaps from some different perspective forward and reverse mode are related by the yoneda lemma, but I do not know it. It sounds plausible.
Not sure if it really matters to you, but from the hacktoberfest website, as far as I can tell, it seems like the total number of submissions have been reached.
Fixed point is the initial object in the category of F-algebras. What's the problem? /s
I think this was exactly what I needed a few months ago! üòÄ
Yeah, for what? I feel like more concrete uses for it would be handy.
Thank you!
This hasn't been a problem for a long time now.
Thank you, I stand corrected. I‚Äôll edit that part later tonight. I assumed my human confusion would be correlated with the compiler‚Äôs confusion. Is there still a performance difference if I implement the van Laarhoven lenses via the simple form and then built them using the ‚Äúlens‚Äù function? Would there be performance benefits to implementing them originally as van Laarhoven lenses? I still think that the simple lens makes sharing of results between the forward and backward pass clearer to the eye. I am unclear how much of that ghc will find on its own. The simpler form only uses tuples and composition. The newtype unwinding is really that superior?
I have never ever had one single problem in years of using CRAN, very willy-nilly at times ("woo new topic of interest, let's just grab 10 libraries and check them all out!") but I'm unsure if this is a guarantee on their part or just a consequence of how they handle things. Of all the ecosystems I've looked at, well, it's not that many I suppose, but CRAN stands out to me as the gold standard. 
``` f :: Maybe a -&gt; Maybe a f Nothing = Nothing f (Just a) = Just a ``` This function doesn't actually do anything, but that's what I got from your question. 
&gt; sum type classes is that a typo or remnant of trying to write something else?
I think the idiom you want here is "as recently as yesterday". "No later than" usually indicates a deadline.
Hmm, I tried it out and it reduced my binary size from 40MB to 8MB, which is nice. But it also increased the startup time from ~0 to 100ms. Thanks for the suggestion though.
Hey man, I just want to say thanks for writing this out. This helped to see the light 
Formality aims to combine dependent types, inductive datatypes and elementary affine logic. That makes it a language that features formal proofs (like Coq, Agda, Idris), but that could be evaluated with the oracle-free fragment of Lamping's optimal reduction algorithm (which you're probably tired of hearing from me!). Previously, most of my work consisted of experiments, unusable syntaxes, etc. Now I'm trying to turn all that into an usable language. Formality, in theory, can have good performance characteristics that languages like Agda, Coq and Idris can't, due to its affine type system. That includes beta-optimality (obviously), but also parallelism opportunities and no garbage collection. This, in turn, makes it suitable for resource-scarse environments like the Ethereum virtual machine. [Preliminar benchmarks](https://github.com/MaiaVictor/symmetric-interaction-calculus-benchmarks) show that my Rust interpreter is roughly 6x slower than Node.js in a binary tree benchmark; i.e., a raw-allocation/matching test that has no sharing at all. That shows that the algorithm is not fast only when sharing is present (such as [those cases](https://medium.com/@maiavictor/solving-the-mystery-behind-abstract-algorithms-magical-optimizations-144225164b07) where it outperforms Haskell by orders of magnitude), but also has potential to be efficient in general. By optimizing the runtime further (having a proper compiler rather than an interpreter, perhaps using native arrays/numbers, and exploiting the parallelization possibilities), I expect it to surpass Haskell's overall performance. Of course, that's speculative and still to be seen, but the fact that an interpreter is about an order of magnitude slower in the worst case is at least encouraging, IMO.
Typo / misspelling / etc. I'll fix it.
My guess is let-generalization broke instance resolution somehow.
I think going through the `lens` function does lose you some sharing. Writing the lens directly using fmap lets you take advantage of sharing, so it should be nicer than using the `lens` builder. You are right though that the simple lens makes the back-and-forth more immediately clear, though. If it helps, maybe look at this, the type signature of a lens: (y -&gt; f dy) -&gt; (x -&gt; f dx) Remembering the positive-negative rules, we see that `x` and `f dy` are in the *negative* position (you "give these" to the lens) and `f dx` and `y` are in the *positive* position (they are "given to you" by the lens). We can rearrange this a bit: x -&gt; (y -&gt; f dy) -&gt; f dx You give the lens `x` and how you would produce the `dy` from the `y` result, and the lens uses *that* `dy` that you give back in order to produce the `f dx` (the backpropagation). w.r.t. how much ghc can find, remember what you're comparing. You're comparing "tuples and composition" to just "composition", or normal function application. The newtype is just a way to *write the function you were originally writing*, and have it be interpreted differently depending on the newtype instance. `view _1` literally compiles down to just `fst`, because that's what the Functor instance of `Const a` is. Note however that a lot of this does depend on things all inlining properly; if you compile `view _1` "all the way", you get literally `fst` if you compile down all the newtypes and inline applications of `id`. You do run into some issues some time if things don't inline properly, which sometimes happens during long compositions of lenses; there are still some lens combinators like `fusing` that can manually force inlining though.
Sounds really cool, I'll be following this. I understand not having a garbage collector should improve performance, but without one how will memory be managed at the user level?
Would it use a borrow system like Rust? 
Other way around? `good`, the one which uses `let`, did compile and `bad` didn't compile.
Similarly to Rust, variables passed by value are linear, which means you collect them when they go out of scope. Opposed to Rust, there is no immutable/mutable burrow, just an explicit copy operation. Catch is, that duplication operation is performed lazily and optimally shared, so making copies is not expensive, thus you don't need references. For example, you can copy a huge vector and send it to a `head()` function, it will only actually copy the parts of the structure you read.
&gt; how is it avoiding doing so? With laziness. For example, when fixing `add3 xs = 3 : xs`, you don't actually need to use `xs`, since to compute `add3 (add3 (add3 (add3 (...` you just need to compute `3 : (3 : (3 : (3 : ( ...`.
That was my takeaway from the way conal presented things at icfp -- that given his very general setup, reverse mode was induced by a cps transform... (yoneda being just a fancy categorical way of saying that).
I can work out which posts are from /u/SrPeixinho just from the titles :D
This is very useful! :) Would it make sense to move `withServantServer` from servant-quickcheck to another pakage, maybe servant-client? Its a really useful combinator. In the first example, the forkIO and Warp combination doesn't wait for the Server to be ready, right? 
Apologies for the delay; I was away - I've made some amendments per your suggestions, I found \[this\]([https://github.com/commercialhaskell/stack/blob/master/doc/GUIDE.md#writing-independent-and-reliable-scripts](https://github.com/commercialhaskell/stack/blob/master/doc/GUIDE.md#writing-independent-and-reliable-scripts)) explaining a little about the "runghc" -&gt; "script" change.
Apologies for the delay; I was away - I've made some amendments per your suggestions, I found \[this\]([https://github.com/commercialhaskell/stack/blob/master/doc/GUIDE.md#writing-independent-and-reliable-scripts](https://github.com/commercialhaskell/stack/blob/master/doc/GUIDE.md#writing-independent-and-reliable-scripts)) explaining a little about the "runghc" -&gt; "script" change.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [commercialhaskell/stack/.../**GUIDE.md#writing-independent-and-reliable-scripts** (master ‚Üí 7574e33)](https://github.com/commercialhaskell/stack/blob/7574e33f119125f7d2517a13e26489b89696f0ed/doc/GUIDE.md#writing-independent-and-reliable-scripts) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e7uvwwr.)
Hi all, this is a small, lightweight vim plugin to speed up refactoring Haskell code. &amp;#x200B; It's still a WIP. Any suggestsions, PRs + contributions, comments, feedback most welcome!
Which ones are as good as (or better than) the Haskell Book?
Yes, it does increase the startup time since it needs to decompress the binary. If the startup time is very important to you then, this is not a great option.
Could you show how the explicit duplication looks like in README? 
&gt; The billion-dollar mistakes were more serious fundamental flaws ‚Äî notably, its omission of sum types and pattern matching ‚Äî that drive Java‚Äôs users to abuse tagged bottoms for lack of sufficient means to express themselves properly within the normal confines of the type system. Well, that and the fact that the standard library itself uses and abuses "tagged bottoms" as you call them. I haven't done much Java myself, but I've done a lot of F#, and that's something we feel a lot: F# code is generally null and exception-free, except at the boundary with C# where they're inevitable.
Can you expand a bit on what exactly is going on there in the return type of `tail`?
Don't waste your time on creepto support. Tether is blowing up and will pull down half the exchanges with it. Ethereum is still shedding value.
It is still not present in the implementation, but is simple. I'm also not decided on how it'll look like, but should be something like: ``` copy [t] as (a,b) in e ``` This creates two copies of `t` and substitutes them as `a`, `b` in `e`. `[t]` "boxes" a term, which allows it to be duplicated. Only boxed terms can be duplicated. Type rules are something like: ``` ctx |- T : * ctx |- t : T --------------------------- ctx |- [t] : !T ctx |- T : * ctx |- t : T ctx, a : T, b : T |- e : T' --------------------------------------------------------- ctx |- copy (a, b) = t in e : T' ``` With the stratification checks I've explained on other posts (boxes have no free variables, duplication variables must have 1 box between declaration and occurrence). One reason I've not included it yet is that I'm considering if we could avoid explicit annotations and instead infer if a term is EAL-typeable. There are a few papers doing that.
Is it wrong? I was very tired when I coded that, let me check it.
Well thanks Ethereum and its shedding value for this being possible at all.
I know that, when not using a build tool, you just need the "--static" switch for GHC, e.g. ghc --make --static &lt;src files&gt; Is that not the case with the stack and co?
To a much smaller degree, yeah.
This is the corresponding Coq definition: ``` Inductive True : Type := | unit : True. Inductive Nat : Type := | succ : Nat -&gt; Nat | zero : Nat. Inductive Vec : Type -&gt; Nat -&gt; Type := | cons : forall A : Type, forall n : Nat, forall x : A, forall xs : Vec A n, Vec A (succ n) | nil : forall A : Type, Vec A zero. Definition tail_ret (A : Type) (n : Nat) := match n with | succ pred =&gt; Vec A pred | zero =&gt; True end. Definition tail (A : Type) (n : Nat) (xs : Vec A (succ n)) := match xs in Vec A n return tail_ret A n with | cons A n x xs =&gt; xs | nil A =&gt; unit end. ``` Does that make sense? Let me know if there is any problem with it. 
I think this is really cool, but when is it more practical than plain recursion? [The documentation](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Function.html#v:fix) uses the factorial function as an example which is arguably easier to read as a regular recursive function.
The lens function takes a separate (s -&gt; a) and (s -&gt; b -&gt; t) arguments. This is suboptimal. Why? Because you must do the work of pattern matching on `s` twice. The Van Laarhoven or costate comonad coalgebra versions both have the benefit that you only ever flay open `s` once and keep the remaining parts you'll need to reassemble it in the environment.
Something I have wondered: how does this relate to, or how does it differ from, reference counting? With reference counting one also "makes a copy", but "lazily" because it merely increments the count rather than actually copying anything.
Not nearly as much: There are very few instances where bottom is actually a valid value for a datum (if any). The problem with null-as-bottom is that you don't know which it is: Just a missing value or bottom? Of course the problem in Java largely stems from *tradition* -- if there had been a Maybe-esque thing from the start then perhaps the problem of NPEs wouldn't be so pervasive. You see much fewer NPEs in e.g. Scala and those that you do see tend to be because of either interfacing with Java code, or because initialization order can become *really* tricky what with the interactions of val, lazy val, etc. and traits, classes, etc.
You need to be very careful, though. I'm very nervous of Conal's paper. It is quite elegant, but check the asymptotic performance of `x^y` in "CPS'd" reverse mode! You want to be very sure that it doesn't blow up to take x^32 to 4 billion operational steps via composed functions for the derivative! This is why my original stack overflow reply on whether it was possible to implement reverse mode AD in haskell said 'no' until tricks using observable sharing or lifting mutable Wengert lists into constraints via reflection came along. Compare this with the difference between doing a monoid via reflection vs doing one with newtype M a = M (a -&gt; (a -&gt; a -&gt; a) -&gt; a) and I expect you'll see the exact same issue. I know you do in the version of this that Tiark Rompf described in an unpublished paper right before Conal came out with his version. You share functions, not values and suffer for it if you try to write against them as if they are arbitrary numerical types. If you are the compiler, or are "compiling to categories" you have more control and can lift this transformation over the (^) function, though.
What happens to `t`, after you split it into two? Can you still use it? Why introduce new variables, when formality can do it for you? use t 2x in e wold make more sense. 
You mean because it's a tighter type than `(x, dx) -&gt; (y, dy)`?
Care to share the solution?
Not for stack, the github discussion above is exactly about adding such a flag there. Afaik it's not completely trivial, since you need musl to replace glibc, and I guess all the dependencies need to be compiled statically too.
&gt; but then you'd have two types of variables, i.e., exponential and linear variables How does this relate to linear logic's `!` and LinearHaskell's `Unrestricted a`? 
Have you seen this: https://vadosware.io/post/static-binaries-for-haskell-a-convoluted-approach/ ?
`!` is how we call a box on it, which closes a term that can be duplicated. I'm pretty sure anything with a linear system should not need a GC if done properly, but I don't know much about LinearHaskell.
Avoiding this can be done by changing `{-# INCOHERENT #-}` to `{-# OVERLAPPING #-}` and `{-# OVERLAPPABLE #-}`. But, the code I post is "narrowed down" version from what I originally worked on, and in the original version, overlapping instance was not a solution. (And tried to use incoherent instances then failed.)
If the nil case is unreachable, shouldn‚Äôt you be able to finish the program/proof by contradiction?
I saw the link, but I was not courageous enough to look at it yet.I think it's the words 'convoluted' and 'docker' that scared me off, but I will definitely read through it, it looks educational at first glance.
Scissors! ‚úå I win
Nice lifted banana (^) :^^)
Some books that are highly recommended on the standard textbook side are Bird's "Thinking Functionally with Haskell" and Hutton's "Programming in Haskell." On the more "commercial" side I've heard good things about Kurt's "Get Programming with Haskell" on Manning press. There are plenty of other good books as well.
Right I don't think the claim is that the _actual_ asymptotics are necessarily efficient -- rather that the "yoneda" view is a way to understand the _meaning_ of reverse mode AD, or to give a model of it, or the like.
Bad bot.
bad good bot bad human bot
That's a good suggestion. One of the things I struggle with when writing these is trying to find a good mix between a) what would someone new to the subject need to know, b) what do people typically do in this scenario, and c) how we do not introduce too many concepts so as to explicitly highlight the above. I ended up including some things that were not germane to the primary topic in order to cut corners (`lens-aeson`, for example).
Thanks for the feedback! For the first question, I don't know. Possibly one of the Servant devs may have an opinion. For the second point, I think that's probably an important detail to include. Should we include a note on that in the recipe? By the way, I posted this here hoping to get some feedback on how to improve it. I was happy with it when I wrote it because it's material that doesn't seem to be otherwise present, but then I worried it was needlessly complex in places or that it strays from its introductory bullet points. Anyway, this is just to say, again, thanks for reading and thanks for the feedback.
I think the difference between null and bottom for me is that null is an explicit tool in java whose purpose is *control flow*; it was explicitly introduced as a control flow mechanism. You're meant to "match" on null, use it to signify information, explicitly return it was a valid result. So i suppose to me, checking for null isn't the meta-code -- it is the explicit code, since null is usually treated as an explicitly chosen returned value, chosen with deliberate intent (and not as a mistake in programming). Null doesn't mean "mistake", it means "this other option that i am declaring to you, for you to use as a return value". In Haskell, the role of bottom isn't control flow, and in fact it can't even reliably be used as control flow (even in IO). Bottom isn't meant to signify meaning to the caller. For that, we have nullable types, `Maybe`. For what it's worth, I do consider bottom to be a value -- one that inhabits every type; the only value of type `forall a. a`. But null to me is more like `Nothing`, because it's used in the same way -- `Maybe`, `Just`, and `Nothing` are in practice used as *control flow mechanisms* in Haskell, just like null is used in Java. All of this does work to support your final point, I feel. Null's position as an important control flow tool in Java probably would have never happened if Java supported sum types in a nice way.
That is an interesting point about the dependency. I love a more constraining type. Probably not worth the cost in readability when you have to go point free though (unless there is a way around that?)
One option that I think could work for parameters is using a function p -&gt; Lens s t a b that just takes the parameter p. If you don‚Äôt need to differentiate with respect to it, it doesn‚Äôt need to be in the point free pipeline.
Are you referring to a power function built from many applications of duplicate and multiply? Are there a significant performance issue with the lensy style I was suggesting? I‚Äôm not very proficient in performance in Haskell. Were observable sharing, etc what was necessary to write a reverse AD that can be used like a regular haskell functions rather than point free and staying out of a monad? I think something got mislaid in your comment. I‚Äôm guessing a caret made the right parenthesis a superscript? 
Not really. Even things like Map.put in java end up being "meta-code" as you put it: if key is null then it does something special (inserts value into the special "null-key" slot) instead of raising an exception as it should if it respected the "tagged nulls" discipline. As a result you can have your program print a wrong answer (instead of crashing) just because you introduced a null somewhere, even without you explicitly writing any meta-code yourself.
Btw, in Smalltalk, null is an Object; it is an instance of the singleton class `nil`. This enables many cool patterns and above all guarantees that you will always deal with an object in smalltalk, no matter what. null in Java is... dunno what it is. One cool pattern: If you send `nil` any message, it will call the message `doesNotUnderstand:`, which throws an detailed error and allows for live debugging (meaning you can fix the bug, step back one step and rerun the system). Conceptually, this is Java's nullPointerException, but it is so much more sane.
Nice skittles theme ! Take my upvote. 
I've gotten to the point of the final linking stage, but running into errors due to libpq missing functions. The errors are similar to this: https://github.com/lpsmith/postgresql-libpq/issues/17 I gave up due to a lack of time, due this seems like a starting point: https://www.postgresql.org/message-id/e1200439-c336-0d60-e384-ee86941dd551%40iki.fi 
Why thank you! It's this theme: [https://github.com/rakr/vim-one](https://github.com/rakr/vim-one)
You do need to compute gradients for the parameters though, by parameters I mean weights. In your example, it's already quite a bit of work to thread the weights through the network - for a more complicated network it becomes a burden you don't really want to expose to your user. Perhaps with some newtypes the complexity of the `Weight` type can be hidden, otherwise you end up with some huge spaghetti type synonym which will pop up in error messages! Using `concat` you'd also solve the plumbing issues. I think this could be a really promising approach! &amp;#x200B;
&gt; null in Java is... dunno what it is. You can certainly think of it as an Object. It just throws an NPE for any method or field access.
Nix may have a high learning curve but it's your best bet for reliably being able to build static binaries, especially across multiple platforms.
I mostly agree with this. Except that bottom is a value. It is the very definition of *not* being a value. Bottom is when you encountered a (non-user-defined) exception, e.g. when the operational semantics got stuck, or reduction *doesn't* halt, e.g. doesn't reduce to a value. It's not generally possible to match on bottom, otherwise the halting problem would be decidable. Hence the entire discussion of considering bottom something you inspect or explicitly construct is rather unfitting. You can construct values (such as null) that may 'evaluate' to bottom in certain conditions, but that's in no way similar to passing unevaluated non-termination around. Nor is it possible in a strict language like Java.
But how to subclass it for example? Not possible. How to change how null reacts to a method? Not possible w/o dirty hacks.
I have written this sort of thing and it's wonderful in Haskell. 
Nice, I'm glad it worked well for you! Would you mind sharing some resources you found useful for the endeavor?
Sweet that looks useful, are there any good html parsing libraries for haskell? Ideally I do all the parsing in Python with bs4 and regex but if there are some libraries you like in haskell they'd be worth looking at!
Not really. It's a magic value that happens to be an instance of every class, including `Object`. In Smalltalk, IIUC, it is the single instance of a specific class, and no other. This is like Python's `None`. Yes, this makes static types trickier (you need lots of unions), but it is a cleaner system.
Haskell is phenomenal at parsing. Since its origins are in academia, where people love writing compilers, people have come up with countless ways to write parsers in the language. As far as I know, [tagsoup](http://hackage.haskell.org/package/tagsoup-0.14.7/docs/Text-HTML-TagSoup.html) is the goto library specifically for html but if you'd like to write your own, my personal favorite is [megaparsec](http://hackage.haskell.org/package/megaparsec-7.0.1#readme). 
Haskell has [green threads](https://en.m.wikipedia.org/wiki/Green_threads) and [software transactional memory](https://www.schoolofhaskell.com/school/advanced-haskell/beautiful-concurrency/3-software-transactional-memory). Concurrency in Haskell is very performant. Simon Marlow, the ghc runtime author, even has a [book](https://simonmar.github.io/pages/pcph.html) on concurrent programming in Haskell. 
If it's 350 http requests, Haskell will handle that no problem. [mapConcurrently](https://hackage.haskell.org/package/async-2.2.1/docs/Control-Concurrent-Async.html#v:mapConcurrently) will probably be your friend.
Beautiful, I'll have to pick that up
It's free online :)
Tagsoup looks like it's a level of abstraction below what I'm used to using but maybe I'm misreading it. Is there a library that you pass html into and can do things like selectAll 'a' hmtlresponse To return a list of all 'a' tags?
You don't subclass objects in Java anyway. That's like asking how to I subclass `"foo"` or `Integer.valueOf(42)`. You also don't change how an object reacts to a method; that's all in the method body of the (possibly anonymous) class. The specific results might vary based on the fields of the object, which you might be able to change by mutating them or calling other methods -- and you can do the same thing to `null`. ;)
Maybe [this function](http://hackage.haskell.org/package/tagsoup-0.14.7/docs/Text-HTML-TagSoup-Match.html#v:getTagContent). I haven't used \`tagsoup\` before so I'm not really familiar with it at all.
I wrote [a blog post](https://vaibhavsagar.com/blog/2018/01/03/static-haskell-nix/) about doing it the Nix way if that helps.
Use Scalpel! If should be able to do a lot of stuff with a high level API. If you're stuck with something, look at one of the open issues first, and in the meantime, you could use taggy-lens as a workaround for that small bit.
Check out the example for `universeTree` in [Text.HTML.TagSoup.Tree] (http://hackage.haskell.org/package/tagsoup-0.14.7/docs/Text-HTML-TagSoup-Tree.html).
If you're doing this as a learning exercise then go right ahead -- Haskell is great for concurrent programming and the task at hand is no exception. **However**, expect some pain in parsing and extracting data out of ill-formed HTML that you are bound to find on the Wild Wild Web. If this is mission critical, consider using something like https://80legs.com (there are more such products). At scale, you'll start hitting IP restrictions, request throttling, etc and these products have already figured all this out.
I think there might be some confusion because there are two separate applications of the Yoneda lemma. One application is the proof that the van Laarhoven rep is the same as the costate comonad rep. Another application has something to do with cps and the reverse-forward mode relation. I think a type for reverse mode ad more in line with Conal's description is `forall dz. (x, dx -&gt; dz) -&gt; (y, dy -&gt; dz)` which is pretty cool in how it looks so much like the forward mode type `(x,dx) -&gt; (y,dy)` plus some sort of cps transformation. The `dx -&gt; dz` could still be interpreted as a Jacobian transpose, where dz becomes the eventual differential input to the completed AD function chain. I believe this type is isomorphic to the lens type via `to f x = f (x, id)` `from g (x,j) = let (y,j') = g x in (y, j . j')` But I haven't personally seen lens written in this form before.
That makes sense, nothing is mission critical really. I have scraped the websites for about 350,000 small businesses that I sell to. I scraped them from a directory where I didn't have to issue too many requests (around 30k) so great concurrency wasn't an issue. Now I want to crawl the 300k for contacts and contact info. I'll end up having to crawl probably 20 million pages total.
That book was one of the most important resources in my haskell upbringing. Extremely impactful.
I am just surprised why the return type of `tail` is not just `Vec A n`. If its argument is type `Vec A (succ n)`, then why can't you get the `n` from that just structurally, instead of going through this partial predecessor function and handling its failure case with `True`?
The same argument can be made for any language with great flexibility, say changing the evaluation order of arguments in LISP. If programmers are not disciplined or do not know what they are doing, you end up with an unmantainable mess. In our case, the added behavior was transparent to the user; it involved looking up the object in a database and restoring it if found. Another possible, transparent usecase would be adding some kind of logging.
Meh... obviously I meant "subclassing a class" instead of "subclassing an instance of an class". Don't get confused by terminology, "How an object reacts to an message" in Smalltalk is precicely "What method body get's executed" in Java. Show me a way to change any method of null ;)
An object without a class that I can adapt to my needs? That is not an object for me *for practical purposes*. Some things I have done in the past are not possible this way without breaking a clear OO paradigm.
Raising an error wouldn't help much, unless you make clear why it was raised in the first place. 
I used [taggy]( http://hackage.haskell.org/package/taggy ) before: It's nice because of [lenses](http://hackage.haskell.org/package/taggy-lens), which are ideal for scraping.
Hmm, it might be useful indeed, but it would have to be in a package that depends on both servant-server and servant-client then, since BaseUrl comes from the latter and HasServer from the former. We can't make servant-client bring in all the server side packages. As a reaction to a tweet announcing this cookbook recipe, S√∂nke Hahn (one of two other original authors of servant) [pointed us](https://twitter.com/soenkehahn/status/1052200295810441216) to a function in warp, `testWithApplication`, which accomplishes roughly the same thing: you just replace the `Server api` with an `Application` (so it's just a `serve` call away) and it gives you a `Port` instead of a `BaseUrl`. This might really be the best route, since you're usually going to have `warp` around when you run a servant-server app.
scalpel-core is what I usually use. It's almost like writing query selectors in JS.
Yes. With the existing `ad` library, it uses the `(^)` function from the Prelude. However, if it didn't do the sort of evil tricks that it does behind the scenes it wouldn't be able to do so and have the correct asymptotics. The usual test is to check if `a^32` 32 steps in a graph or 4 billion steps in a tree. In your case, you can't use any of the Prelude functionality, so you're already stuck building a bespoke edsl for everything. Your lenses are not Num instances. (Though, they could be, I suppose, with appropriate newtyping.) There you'd see the problem if you used (^) on them via the Num instance.
Hi. I do it by adding the following to [package.yaml](https://github.com/haskus/haskus-system-examples/blob/4d17f1dfbacae605b4d49c88dd89633e2733406e/package.yaml#L24-L27) (or .cabal file): ``` ghc-options: -Wall -O2 -static -threaded cc-options: -static ld-options: -static -pthread extra-lib-dirs: ./.system-work/lib ``` The extra-lib-dirs is used to find my own build of libgmp.a (not available in ArchLinux's libgmp packages for example). I am still using ``glibc``, not ``musl``.
I'm generally open to that. However I usually hit a roadblock somewhere pretty quickly. This time around, I'm hitting good old `cc' failed in phase `Linker'. But feel free to tell me what I'm doing wrong: https://github.com/2mol/pboy/blob/dev/pboy.nix, you can see the error in the CI logs: https://travis-ci.org/2mol/pboy/builds/442555465 Another attempt even broke on CI because the log was too long for travis! Since I'm not running anything important, I'm pretty amused by that, I should be sad for the time I invested and the fact that I can't debug it or even find a workaround for the linking issue. 
Transparent... as long as the database is responding. Ditto for logging to a remote destination.
I'm surprised by this failure of type inference with `QuantifiedConstraints` and functional dependencies: {-# LANGUAGE QuantifiedConstraints, FlexibleContexts, GADTs #-} import Control.Monad.Reader data T x = T { anX :: x, anInt :: Int } control1 :: (forall x. MonadReader (T x) (m x)) =&gt; m y y control1 = asks anX control2 :: MonadReader (T x) (m y) =&gt; m y Int control2 = asks anInt foo :: (forall x. MonadReader (T x) (m x)) =&gt; m y Int foo = asks anInt bar :: (forall x x'. MonadReader (T x) (m x')) =&gt; m y Int bar = asks anInt baz :: (forall x x'. x ~ x' =&gt; MonadReader (T x) (m x')) =&gt; m y Int baz = asks anInt Loading this in GHC 8.6.1, the controls, `bar`, and `baz` are accepted, but `foo` doesn't typecheck: BadQC.hs:14:7: error: ‚Ä¢ Could not deduce (MonadReader (T x0) (m y)) arising from a use of ‚Äòasks‚Äô from the context: forall x. MonadReader (T x) (m x) bound by the type signature for: foo :: forall (m :: * -&gt; * -&gt; *) y. (forall x. MonadReader (T x) (m x)) =&gt; m y Int at BadQC.hs:13:1-53 The type variable ‚Äòx0‚Äô is ambiguous Relevant bindings include foo :: m y Int (bound at BadQC.hs:14:1) These potential instance exist: one instance involving out-of-scope types (use -fprint-potential-instances to see them all) ‚Ä¢ In the expression: asks anInt In an equation for ‚Äòfoo‚Äô: foo = asks anInt I would have thought that `foo` would be legal for the same reason as `control2`, i.e. `MonadReader`'s functional dependency resolving the ambiguity (the error message shows that GHC knows \`MonadReader\`'s second parameter must be \`m y\` in \`foo\`, so it should do something like unify \`x\` in the quantified context with \`y\`, and then know \`x0 \~ y\` from the context + the fundep). Given that that's not happening, I have no idea what's going on.
Awesome, that is exactly the kind of thing I was looking for. It doesn't seem to fully compile statically, since I'm still seeing some linked libraries with `otool -L` (like `ldd` but for mac), but it's progress. I'll do a comparison later when I have time.
Obviously. When that particular database would be down, there would be other problems tho. I don't feel like explaining the whole scenario right now, just so much: The data we needed became so big that we had to use some scalable solution. We had to store the objects *somewhere*. Without the database, there would be no sane way to re-create the wanted object. Point is: It made sense to hook up our object database to an scalable database and being able to change the implementation of `doesNotUnderstand:` was a pretty elegant solution. This change was as transparent as could be. In your scenario, the other alternative would have been some kind of error because the developer attempted to try to retrieve the object himself instead of indirectly.
Ahh yes of course, good spot! And thank you :) 
I think they are the same thing - in the linear Haskell paper IOL is a graded monad, where the grade is given by the linearity of the arrow.
&gt; Show me a way to change any method of null Show me a way to change any method of `"foo"`. The method body is fixed at the time the object is constructed. You don't construct `null`, it's an already existing object, so you can't change any of it's method bodies, just like `"foo"`.
&gt; An object without a class that I can adapt to my needs? Yeah, like `"foo"`. It's always a `java.lang.String`, and you can't subclass that since it's a `final` class. It's far from the only `final` class on the JVM, too.
I don't care what Java people consider to be an object. I just gave my definition. This discussion is over for me.
&gt; If programmers are not disciplined You mean always? &gt; or do not know what they are doing ... yeah, you definitely mean always. Seriously, "disciplined" programmers that "know what they are doing" don't get undefined behavior and bugs in C. If 50 years of industrial programming has taught us anything, it's that we can't count on programmers to be disciplined or knowledgeable *as a group*.
&gt; it's that we can't count on programmers to be disciplined or knowledgeable as a group. Which I never claimed. Discussion with you seems fruitless.
&gt;expect some pain in parsing and extracting data out of ill-formed HTML &amp;#x200B; Are you or anyone else aware of something like BeautifulSoup for Haskell, which was designed for "real world" (read: broken) HTML? Or something that implements the HTML5 recommendations for parser recovery? &amp;#x200B; Or more generally, a parser library made for approximate parsing? I have a few non-html uses in mind for such a thing. &amp;#x200B;
I imagine so, or if you use the type family "instance chain" trick instead of overlapping instances.
You did claim &gt; If programmers are not disciplined or do not know what they are doing, you end up with an unmantainable mess. Presumably, you believe that the code have in mind was not an "unmaintainable mess", so you mean to say that the programmers working on it were disciplined and knew what they were doing. Congratulations! You must have had an amazing team, with a great process to help new members get up to speed and protect against the occasional brain fart. Presumably though, you're pointing out this powerful feature of Smalltalk because you'd consider using it again. I feel like that's saying "I won the lottery, so I'm going to play again, because it's clearly a good investment." You see, Haskellers tend to be deeply distrustful of anything that requires constant vigilance; we know that we are stupid and lazy some of the time. So we prefer to put our trust in the compiler: it's never lazy; it catches our mistakes, and the price is less freedom to do really strange things. I prefer my designs to be impossible to use incorrectly; when that's out of the question, I try to have something that fails as clearly and as soon as possible. Something that is invisible and complex behind the scenes and works most of the time so people are tempted to forget what it's doing and then fails in a confusing way is the worst. I have a printout of [this](http://thecodelesscode.com/case/116) on my desk. I would be terrified of working on a code base with the change you suggested. Maybe there's a good reason?
&gt; I mostly agree with this, except with bottom being a value. The word "value" is overloaded; to clarify, I [like to say](https://www.reddit.com/r/haskell/comments/4zm2gc/notes_on/d75o4wt/) that bottom is a denotational value which corresponds to many operational values but to no WHNF values nor any total value.
&gt; Actually, doing it in a subclass of nil does a lot to allay my worries. Now it only affects code where you have produced your special nil value rater than potentially all code running in the same vm. Yes, my memory was inprecise. &gt; And I can think of several other ways to solve your problem off the top of my head, each with different downsides. All of them will lead further astray from the beautiful, paraphrased-in-my-own-words "Every objects knows how to handle the things it is supposed to model", I would guess. For example, introducing a new class responsible for serialization and deserialization of objects. That absolutely works, but it is not 100% pure to the OO paradigma - shouldn't any object know how to serialize and deserialize itself? Why introduce an artificial class for that?
Scalpel is the way to go. 
I've put links to this in [https://github.com/ndmitchell/hlint/blob/master/README.md#customizing-the-hints](https://github.com/ndmitchell/hlint/blob/master/README.md#customizing-the-hints)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ndmitchell/hlint/.../**README.md#customizing-the-hints** (master ‚Üí 123ab00)](https://github.com/ndmitchell/hlint/blob/123ab00c342978251c5d789c852669d81122fb19/README.md#customizing-the-hints) ---- 
Just out of curiosity, are there any real-world uses for the second definition, such as pausing it in a debugger, or maybe trying to reproduce infinite-looping behavior out of some other code that's suspected of it? 
True, though non-termination is eventually addressed: https://imgs.xkcd.com/comics/halting_problem_2x.png
Something like https://github.com/emk/rust-musl-builder for Haskell would be great.
It‚Äôs easy to understand what `fix` does if you think of graphs instead of linear text (which is what many Haskell implementations do). It‚Äôs just a small graph with a cycle. (Sorry, I can‚Äôt draw a picture on my phone.)
I suspect as soon as someone [PRs it](https://github.com/Homebrew/homebrew-core/edit/master/Formula/ghc.rb) :)
You can start following at https://github.com/lucasdicioccio/servant-http2-client (currently not a library, it's a PoC).
To write to logs on Heroku you just write to `stdout` or `stderr`. Have you tried profiling your app locally, to see if anything looks off? Also check out [EKG](http://hackage.haskell.org/package/ekg) and [wai-middleware-metrics](http://hackage.haskell.org/package/wai-middleware-metrics).
Take a look at the Sidebar, "Learning Material".
Oh didn't saw that! Thanks :)
Also: * https://github.com/bitemyapp/learnhaskell * https://github.com/hzlmn/haskell-must-watch * https://github.com/cohomolo-gy/haskell-resources/blob/master/README.md * https://github.com/soupi/haskell-study-plan * https://github.com/sassela/haskell-learning
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [cohomolo-gy/haskell-resources/.../**README.md** (master ‚Üí 2195aa3)](https://github.com/cohomolo-gy/haskell-resources/blob/2195aa3b08e3424bb4e681533b8ba7b666837c94/README.md) ---- 
Thanks for the answer. I found that the error was in the library `pwstore-fast` which was recommending to use strength 17 for hashing passwords which was too much for the Heroku dyno to handle but no problem for my computer. I will check out the libraries you recommended for the future though!
The type `Set ()` only has one value -- `Set [()]` -- and `Set Void` has no values.
Using `Fix` from [recursion-schemes](http://hackage.haskell.org/package/recursion-schemes-5.0.3/docs/Data-Functor-Foldable.html#t:Fix): type PureSet = Fix Set You should be able just `Fix . op_s . unFix` or `embed . op_s . project` for a lot of your op_p functions. Depending on how annoying you find that, you might even try: type PureSet2 = Set (Fix Set) Then, each op_s *is* the op_p. But, the *users* of your functions will need to `Fix` / `embed`.
`Set Void` has one value: `Set { elems_s = [] }` (aka `empty_s`) `Set ()` has two values: `Set { elems_s = [] }` (aka `empty_s`) and `Set { elems_s = [()] }` (aka `singleton_s ()`).
Ah, I get what you're saying, but it's not quite true -- `Set Void` is inhabited by `Set []`, via `empty_s`. However, what I didn't think about was the type of one, which would be represented by `[[]]`, would be `Set (Set ())` or `Set (Set Void)`.
Use Hoogle. It lets you type the definition of a function and search for a function. For example, if I want a function that does this: (a -&gt; b) -&gt; [a] -&gt; [b], I can Hoogle it and it will say that the "map" function does just that!
Oh yes, good point. I guess I was (accidentally) referring to only non-trivial values then.
Welcome! :)
This is interesting, thanks. I'm struggling to make an `empty` `PureSet` with either method though. Well I might have made one with `Set (Fix Set)`, but it won't display without an instance for `Data.Functor.Classes.Show1 Set`, which is hidden. Even writing a manual instance for `Show (Set a)` doesn't help. I suppose the real aim is to defined `PureSet` in such a way that for each pair of functions `op_s` and `op_p`, we can define an `op` which can be applied to both `Set a` and `PureSet`.
Haskell is really the sweet spot for this both because of concurrency and parsing. I've written a \[ugly\] framework for doing simpler scraping jobs: [https://github.com/grafted-in/web-scraping-engine](https://github.com/grafted-in/web-scraping-engine) &amp;#x200B; If I had time to go back to this I think there is a very elegant solution waiting to be codified (Haxl/Fraxl might be that solution). But regardless, you can do some pretty neat things with just \`async\` package and continuations.
&gt; I suppose the real aim is to defined PureSet in such a way that for each pair of functions op_s and op_p, we can define an op which can be applied to both Set a and PureSet. `type PureSet = Set (Fix Set)` is what you are describing. Then, `PureSet ~ Set a` &lt;-&gt; `a ~ Fix Set`. But, like I said, then the callers of something like `add` will need to call `Fix` on the thing being inserted (but not the thing being inserted into). You might experiment with be `Mu`/`Nu` fix points to see if their encoding is any more palatable. --- Recursive aliases are not allowed because they aliases are effectively expanded as part of type checking, and a recursive alias would never stop expanding so type checking would never complete. Because of this, I don't think you'll be able to avoid adding/removing a newtype layer when moving out of/into the `PureSet` (or any self-recursive structure). Note that newtype wrapping and unwrapping are *generally* erased before code generation (so they are "zero cost"), and when they aren't you should be able to use an explicit, safe coercion.
I like taggy-lens for this sort of thing, but if you're just getting started it might be worth deferring lens until later...
&gt; Nix may have a high learning curve but it's your best bet for reliably being able to build ... ... just about anything. Yesterday I had a friend ask me if there was a way to get a taffybar compiled with GHC 8.2 easily under nix. The answer: yes, pass in a different ghcWithPackages into the taffybar expression and wait for it to cook.
You can build an entire fresh linux installation and run it in a VM just to test new versions of an init system. If you have 2 devices, you can transparently parallelize builds between machines. Nix often feels like magic.
Directly indexing `Set` by the element type causes a loop when you try to define `PureSet`. Another way than `Fix` to break the loop is to use an uninterpreted symbol as the index, from which the element type can be computed via a type family. {-# LANGUAGE FlexibleContexts, TypeFamilies #-} newtype Set0 s = Set0 { elems :: [Elt s] } data SET a data PURE type family Elt (s :: *) :: * type instance Elt (SET a) = a type instance Elt PURE = PureSet type Set a = Set0 (SET a) type PureSet = Set0 PURE Full snippet: https://gist.github.com/Lysxia/9d87852ad731019b23ae435b489f9e78
Nix's build parallelism is incredible. I was building packages 12-at-a-time to test builds with musl libc at one point, and you can even build across architectures. When I was an undergrad, I wanted to do a research project on package management, but didn't have the imagination to conceive of something like nix. I think I'd seen GoboLinux at the time but hadn't put the pieces together in my brain. It's incredible what can happen when people re-evaluate things carefully from first principles.
Just curious, what is your build/deploy setup like?
[My take on it](https://gist.github.com/viercc/525baa84a0f52388160064aabe048ae9#file-question-sets-hs)
This is exactly what I've been searching for, thank you! I knew type families had to be involved somehow, but working out that clever `Elt` trick never would have come to me. Great work.
&gt; the only caveat (that I can see so far) is that you need to be very strict with explicit typing *Might* be able to get better inference with a closed type family. /u/viercc's solution of putting the operations in a MPTC would also work for the `Fix` solution, though you might not get a `deriving` for free.
It's getting late here, but I'll experiment with MPTC+Fix tomorrow. Thanks for all the tips.
Using DerivingVia it should be possible to do away with the extra layer.
Let me try another answer, in case it helps. Haskell being a non-strict language means that you can think of expressions as being evaluated from the outside in, rather than from the inside out. So, for instance, given the expression `f x + y`, a strict language *first* evaluates the subexpressions, `f x` and `y`, and only then proceeds to the top-level expression by applying the `+` operator. But Haskell attempts to apply the `+` operator *first*, and only evaluates the subexpressions if and when they are only needed. So when Haskell evaluates `fix f`, it first tries to evaluate x, by replacing it with `f x`. In a strict language, you'd expect it to then try to evaluate `x` again, so that it can pass the argument to the function `f`. But it doesn't! Instead, it just passes the *partially* *evaluated* term `x` into `f`. If the result actually depends on `x`, then `x` can be evaluated later. The upshot of this is that you end up being able to evaluate the recursively expanded expression `f (f (f (f ...)))` with no problems, because in a non-strict language, you don't need to start from the innermost expression. You start from the outermost expression, and that's readily available. In the specific case of `threeAnd x = 3 : x`, you end up building up the expression `3 : 3 : 3 : 3 : 3 : ...`, which is `[3, 3 ..]`. Any time you force the expression `x` during evaluation, it gets replaced by `3 : x`, which is good enough to pattern match one more layer deep. There's another trick here, which is that actual *laziness* also guarantees that you don't even allocate an infinitely long list here! In lazy evaluation, a variable name (with no type class constraints) is only evaluated only once, and automatically memoized so that further evaluations get back the same object on the heap. Because `x` is such a variable name, its uses all point to the same place on the heap. That means that instead of incrementally building up an infinitely long linked list as you might expect from the infinite expansion of the term, Haskell compilers will usually automatically turn it into a *circular* list instead. IIRC this isn't actually guaranteed anywhere in the Haskell specification, and it's definitely not detectable from referentially transparent code. But it happens.
Honestly, I'd be in favor of a mixin-based solution if I was committed to an OO lang with pervasive mutation. That way, there's no business with nulls: you mixin a proxy that loads the real object from the DB whenever a message is received; you do this for every class you want backed by the DB (yes, I'm thinking with classes rather than objects; sorry, I'm tired).
Can anyone from Digital Asset comment? What's the team and job like?
&gt; A Smalltalk with strong static analysis would be hard to resist for me There's [Strongtalk](http://strongtalk.org/), but it doesn't seem to be active. There's also [Crystal](https://crystal-lang.org/) which is a statically typed Ruby that seems promising. &gt; ML type inference with subtypes anyone? Sadly, subtypes _really_ break things when it comes to ML-style type inference. It's novel research to get something that's nice and solid in that design space. &gt; Then again I love Haskell, because it get's many things right and I consider FP to be a superior paradigm to OO To me, the most salient difference is mutability vs immutability by default, and the latter just seems right.
For noncommutative monads like parsers or IO, the ordering of effects is important. In `mf &lt;*&gt; mx`, the effect associated with `mf` will be sequenced before the effect associated with `mx`. Hence, in general, (\ x y -&gt; (x, y)) &lt;$&gt; mx &lt;*&gt; my is not the same as (\ y x -&gt; (x, y)) &lt;$&gt; my &lt;*&gt; mx unless the monad is commutative. For example, if `mx` parses a string and `my` parses a integer, both expressions will yield a `(String, Integer)` after a successful parse, but the former would consume the string first from the input, then the integer, whereas the latter will consume the integer then the string.
I'm using Heroku with this buildpack https://github.com/mfine/heroku-buildpack-stack. Then in my package.yaml I have the following optimisations: ``` name: myapp ghc-options: -threaded -O2 -rtsopts -with-rtsopts=-N tests: spec: main: Setup.hs source-dirs: - src/backend - test/backend dependencies: - hspec ghc-options: -main-is Setup.main executables: myapp: main: Main.hs source-dirs: - src/backend generate: main: Generate.hs source-dirs: - src/backend - src/integration ghc-options: -main-is Generate.main ``` and my Procfile is simply: ``` web: myapp ``` 
&gt; (yes, I'm thinking with classes rather than objects; sorry, I'm tired) Haha no problem! &gt; Honestly, I'd be in favor of a mixin-based solution if I was committed to an OO lang with pervasive mutation. That way, there's no business with nulls: you mixin a proxy that loads the real object from the DB whenever a message is received; you do this for every class you want backed by the DB That sounds like a viable approach too. IMHO, the scenario we had at hand is that most of the objects had to be persistable (basically everything that was referenced by another to-be-persistent object) so you would end up with mixin in proxies for basically every class object. This comes really close to inheriting from the proxy directly. I concede that it is better to not make changes to subclasses of all objects, but mixin in some class is likely not much cleaner. Thanks for the alternative!
For IDE I highly recommend vscode with 'simple haskell plugin'. It is by far the easiest one to setup! 
Have a look at the `p` functions in the [Opaleye type families tutorial](https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/Doc/Tutorial/TutorialBasicTypeFamilies.lhs). You parametrise your datatype on a type symbol (HKD style) and write a generic traversal like function once and for all, e.g. https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/Doc/Tutorial/TutorialBasicTypeFamilies.lhs#L222 (this can be generated with TH or Generics, but I haven't got round to it) and then you can use it to do many things, including constructing records containing Applicative fields. This example is for ProductProfunctors, but it's much the same idea: https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/Doc/Tutorial/TutorialBasicTypeFamilies.lhs#L236 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tomjaguarpaw/haskell-opaleye/.../**TutorialBasicTypeFamilies.lhs** (master ‚Üí b407b9a)](https://github.com/tomjaguarpaw/haskell-opaleye/blob/b407b9adfe288922a55d3d17f5a17b2127dcf88b/Doc/Tutorial/TutorialBasicTypeFamilies.lhs) * [tomjaguarpaw/haskell-opaleye/.../**TutorialBasicTypeFamilies.lhs#L222** (master ‚Üí b407b9a)](https://github.com/tomjaguarpaw/haskell-opaleye/blob/b407b9adfe288922a55d3d17f5a17b2127dcf88b/Doc/Tutorial/TutorialBasicTypeFamilies.lhs#L222) * [tomjaguarpaw/haskell-opaleye/.../**TutorialBasicTypeFamilies.lhs#L236** (master ‚Üí b407b9a)](https://github.com/tomjaguarpaw/haskell-opaleye/blob/b407b9adfe288922a55d3d17f5a17b2127dcf88b/Doc/Tutorial/TutorialBasicTypeFamilies.lhs#L236) ---- 
Allocate space for a variable `x`, which is just a pointer to a thunk. When that thunk is evaluated, it will call `f` and pass the *same* pointer `x` as an argument. The function can inspect `x` (e.g. with a `case` expression), which implicitly results in another recursive call to `f x`‚Äîor not, in which case it may return a value (which is cached in the thunk). Typically `x` has a function type like `a -&gt; b`, so the recursive call may have additional arguments such as an accumulator parameter, and the call to `fix` has type `((a -&gt; b) -&gt; a -&gt; b) -&gt; a -&gt; b`.
Oddly enough, rust slows down the process of rusting. Rust is oxidation.
Going to channel my inner-Wittgenstein and say that it depends wholly on how you use the word "functional". &amp;#x200B; That said, I like what the article does in breaking the question down into *specific* features of languages. Anything else is just meaningless. &amp;#x200B; &amp;#x200B;
The problem with general tail recursion optimization is that it depends on garbage collection, which Rust intentionally doesn't provide. Consider the following program: struct List&lt;'a&gt; { depth: u32, prev: Option&lt;&amp;'a List&lt;'a&gt;&gt;, } fn happy(depth: u32, mut prev: Option&lt;&amp;List&gt;) { if depth == 0 { while let Some(current) = prev { println!("{}", current.depth); prev = current.prev; } } else { let node = List { depth, prev }; happy(depth - 1, Some(&amp;node)); } } fn main() { happy(100_000, None); } This program creates an on-stack linked list. The stack has limited size, but the program only allocates on a stack. The program appears to be tail recursive, but due to a requirement to keep stack allocation as long we are within a function, it couldn't be optimized.
Worth more than just an upvote, so I will add, "well written, informative, easily understood and showing excellent insight into what I needed and wanted to know." And you did it without telling me to press the 'Duh' key.
ATS provides TCO without GC. I believe Ur/Web does too.
It also have a link to sources, which may give more clues to what particular function does or how.
Or you could add 'no stack allocation outside of the function prelude' to the tail recursion definition.
Go easy on yourself. The volume of questions surrounding multiline input in GHCi suggests it is not a 'duh' issue. Also, if someone calls type inference a duh issue then I do wonder why we all know the names Hindley and Milner or at least why their publications were accepted.
GHC is slowly moving into direction of replacing `*` kind notation to more helpful `Type`. The problem of outdated old books and tutorials is still relevant. But it would be really nice if new blog posts started to use this `Type` notation more often!
Thank you for pointing that out, now it's clear to me. Your article is really good :-)
Amazing article! This is exactly the sort of intermediate-advanced post I'd like to see more of. I have a feeling I'll be referring back to it a lot. 
And it seems to run well off of the most minimal heroku instance? Been wanting to shift some things I run over to Haskell but I attempted a Yesod/Keter setup and it seems to require compiling elsewhere if I want to deploy to a tiny Amazon lightsail instance and I'd didn't want to spend the effort yet. 
Forall a referrs to every a in the type signature, including the output type. In fact, when you write normal type signatures, the forall is implied, so `id :: a -&gt; a` is really `id :: forall a. a -&gt; a`. With rankN types, forall a referrs to every a in that scope, so foo :: forall a. a -&gt; (forall a. a -&gt; a) Has two different a's that may be the same but don't have to be (although that type signature makes essentially zero sense).
It includes the necessary infrastructure plans around depreciating `*`, what compiler warnings to print out, and so on. The only major consequence would be the ability to free up the star to be used as an operator at the type level, although I'm not sure if the proposal explicitly does that.
It can also refer to output arguments. See for example a function like fromInteger: fromInteger :: forall a. Num a =&gt; Integer -&gt; a This is exactly the same (not considering some extensions) as writing: fromInteger :: Num a =&gt; Integer -&gt; a 
Having just chased down a `null` in Scala, this post is missing one huge difference between `null` and `???`/`undefined`: Java does not throw an exception when it sees `null`, it just chugs along until someone tries to call a method or get a field of that value. The Haskell analogue is an `undefined`/`error` that doesn't get evaluated (and consequently barf) until a long time after it was created. _That_ is a billion dollar mistake.
The GHC developers are looking for community feedback to help guide our development priorities for the coming year. We would be quite grateful if you could take a few minutes to fill out this survey and let us know what problems are most urgent for you. Thanks! - Ben, Simon, Simon, Alp, Tobias, and Omer
No noticeable problem so far except that password hashing was extremely slow and was hard to profile. Before going with my Scotty setup I tried with the scaffolded Yesod side + postgres and that worked fine as well.
&gt; A functional language is one in which functions (or whatever you name your native callables) are first-class values. But if we imagine a Haskell without first class functions what you have is a language with a seemingly arbitrary and very annoying _restriction_ (at least to someone who hasn't written a compiler). To me that tells us that to the extent that haskell is the epitome of a "functional" language (which I think most people agree), the has-first-class-functions definition is not the most useful (but you may be right w/r/t historical accuracy).
&gt; Definitely Python and likely C#, hmm. Agreed, although doesn't C# still make you jump through hoops with delegates? I haven't done any serious C# programming since .Net 3.0. Lisp, too. It's not pure, lazy, total, or productive nor does it have pattern matching (though it can be added with macros, though I don't think it does coverage checking) and it's static typing is usually lacking. But, if Lisp isn't functional, the word doesn't mean *anything* anymore. I'm absolutely willing to give Python and Javascript the adjective "functional" -- they earned it. Doesn't mean that I don't think the purity, laziness, and type-inference of Haskell or the purity, totality, and dependent types of Idris isn't better. &lt;sarcasm&gt;I can still be an ivory-tower elitist and give them the word functional.&lt;/sarcasm&gt;
&gt; It seems to me the most meaningful definition is "functions like in math". Turns out, that's *purity* not *functional*.
Respectfully, I disagree. This definition has been historically used quite a bit, but that seems like an arbitrary restriction of "procedural" languages, or otherwise not acknowledging that "procedural" and "functional" are different paradigms. It doesn't need "ultimate" purity (not even Haskell has that, it just strongly encourages it), but a "functional" language that doesn't deal in _functions_ as opposed to _procedures_ is hardly _functional_.
To answer your specific question, that error has nothing to do with the code you're showing. It's appearing because you typed `100 101` into GHCi, which Haskell understands to mean "apply the function `100` to the argument `101`. Because of a quirk of how Haskell handles numbers the error message is a bit cryptic.
Huh? Your example doesn't have anything to do with GC. You're keeping references to the stack yes, so obviously it can't be shrunk. But that doesn't have anything to do with GC or no GC, that's just an example of a function that isn't TCO'able.
If your functions aren't functions, then where's the function part of functional coming from?
If that's the well-understood definition then it seems we should stop using the word, talking about "FP", or describing haskell using the term. Maybe haskell is just a "pure language". The fact that it's "functional" by your definition is just necessary and expected and not particularly notable.
I guess I'm not exactly sure where or if we disagree. I think my broad point is that when people talk about Functional Programming now (in this article and elsewhere) they _actually mean_ something like "the constellation of interrelated characteristics that make haskell nice, at the base of which is purity". I think you're arguing that this is a lumping together of ideas that isn't useful or is an abuse of terminology.
As other answers already suggested, type parameters can refer to output types. But it is also worth mentioning that the type you wrote cannot soundly have any inhabitant, because it is not possible to construct values of arbitrary types from nowhere.
That last example isn‚Äôt a higher-rank type though: `forall a. a -&gt; (forall a. a -&gt; a)` = `forall a. a -&gt; forall a. a -&gt; a` = `forall a. a -&gt; forall b. b -&gt; b` = `forall a b. a -&gt; b -&gt; b`. You can always float quantifiers and typeclass constraints up like this in non-dependent Haskell, iirc‚Äîe.g. you can write `take :: Int -&gt; forall a. [a] -&gt; [a]` or `Foo -&gt; Num a =&gt; Bar a -&gt; Baz a`. If you want a rank-2 (or higher) type, you put a quantified type as the *input* of a rank-1 (or higher) function type, like `(forall a. ‚Ä¶) -&gt; ‚Ä¶`
&gt; The fact that it's "functional" by your definition is just necessary and expected and not particularly notable. It (my "functional") does set it (Haskell) apart from some languages (C and C++ [at least when I was learning it in '99]). But, my main point is I do wish people would refer to the extra bits that Haskell has over other languages: static, inferred types, parametric polymorphism, ad-hoc polymophism, laziness, pattern-matching (w/ coverage analysis), immutability, etc. using their specific terminology, not under some new-ish catch-all meaning of "functional".
Proposals for explicit TCO in Rust (E.g. using `become` instead of `return` (implicit in that example), like `become happy(depth - 1, Some(&amp;node))`) end up outlawing that sort of code with rules like: any lifetimes in arguments to the become call must outlive the function call (essentially, they must be derived from the caller's arguments). As with most things in Rust, one can then opt in to the more expensive schemes required to make that sort of code work, such as more copying, or more allocation (costs that are there, but generally implicit, in other languages).
Your github link is inaccessible.
Same abuse of terminology that has C (and predecessors, though not all they way back to Algol68, IIRC) call it's subroutines "functions". Lisp and scheme also used the word "function" for things that were only called for their effects, not because they mapped the given element of the domain to a particular element on the co-domain. I would like to think I would have railed against that abuse of teminology, too. And, I try and make the distinction, when it's important. But, it was well established before I realized it. So, "function" in programming languages, is basically any callable thing, be it a (pure, mathematical) function or (just a named sequence of instructions) a nullary subroutine with no return value (or returning "void"). And functional languages treat these functions (read: callable things) as first-class values, which is certainly not the case in all languages.
&gt; not even Haskell has that Haskell 98 did. You couldn't define impure functions in it, and it didn't have a standard FFI, so you couldn't import them from another language either. Once we got the standard FFI, which was incorporated into Haskell2010, we were allowed to import things that were not pure without an `IO` wrapper.
&gt; Maybe haskell is just a "pure language". It's pure, functional, and lazy. In fact its *the* lazy, pure, functional language.
Thank you stack devs!
If you're learning Rust from a FP background, I suggest just considering it an imperative language. Fundamentally it's a different take on C++. &gt; I'm left a little unsure what things are expressions with types, Basically everything *inside* a function is an expression. Even something like `return` is an expression; it just has type `!` (`undefined`) and never evaluates to a value at runtime. Some expressions are also statements, like `x = y;`. &gt; what `mut` exactly means (I've also heard the phrase "interior mutability" as something distinct. [this](http://smallcultfollowing.com/babysteps/blog/2014/05/13/focusing-on-ownership/) [...]) There are two places `mut` is used. One is as an annotation on a name, like `let mut x;`. This means you are allowed to get a mutable borrow of the data at that location, or any of its accessible fields, and that you're allowed to move a different value into that location or any of its accessible fields. Without it, you can only directly write to that location once, and the only time you can take a mutable reference to it is during `Drop::drop`. The other is to annotate a pointer, like `&amp;mut`. You typically get these pointers by taking references to `mut`-annotated values, or deriving them from other `&amp;mut` pointers, such as getting a pointer to a field or to an element of a vector. These pointers also allow moving a new value onto the data at that location, or any of its accessible fields. The invariant upheld is roughly that only one active `&amp;mut` exists to any given location at a time, so the owner of that pointer is free to use it in ways that would be unsafe if unsynchronized. Interior mutability is any time a type allows you to modify its value without requiring that you have a `mut` value or pointer. They do this by only allowing safe actions through some other mechanism; `Cell`s can only be used with types for which a shallow copy is a complete copy, and cannot be accessed across threads, so it knows writes never break type invariants. `Mutex` will give you an `&amp;mut` across threads, but manually enforces the invariant of that pointer type. Etc. &gt; what "name shadowing" even means anymore Consider let x = 1; let x = 2; Both `x` are semantically live at the same time, yet the first cannot be accessed through the name `x`; it lives "in the shadow of" the second declaration. Any time this happens is name shadowing. Shadowing may be a local phenomena. &gt; Looping constructs are introduced nearly all of which rely on mutability Yes, this is the intent. &gt; the odd `let if` syntax is sort of like `when` i.e. useful when the body performs an effect. Not really. `if let Pattern = value { body }` is shorthand for `match value { Pattern =&gt; { body }, _ =&gt; () }`. 
&gt; Honestly, I'm not really sure what "functional" means Perhaps, then, you should defer to the people that do have a specific definition for functional, that's been in use for some time? If you mean ML-style, just *say that*, don't steal the word functional that already had a perfectly good meaning! I don't honestly have a lot of love for ML-style, though my experience is rather limited to a small application in F# and the ML from Okasaki's PFDS, and of course whatever gets borrowed around by other languages. I *think* I'd rather something more homoiconic, though I'm not in love with S-expressions, either.
Frat name?
I wish there were an empty box in the survey where I could just express my gratitude, but I'll say it here- thanks for all the work you do!
&gt; tail recursion optimization TCO is a red herring anyway. Haskell's laziness often makes it better to *not* be tail recursive, but rather to be productive. Yes, TCO is very practical to have in a functional language, especially if you encourage a pure style through recursive calls instead of, well, anything else that doesn't do recursive calls. It's not mandatory though -- other approaches to not "blowing the stack" are certainly acceptable and really they adjust the quality of the implementation, not necessarily the language.
Added my response. It's hard to work out my wishlist for what GHC things to work on next. Thanks for all your hard work on GHC!! &amp;#x200B;
What are some acceptable answers to the first question? It would be a lot easier to answer if it was instead written as, "here are ten things we could be working on, please rank their importance".
Plenty of js guys are fans of functional programming and think they can do it just fine from there. And then you have the lisp people, who think that they are doing proper functional programming and that the haskell types are masochists.
A functional programming language is a language where lambda calculus is the primary evaluation model.
Not a compiler optimisation pro, but read about rewrite rules in GHC. My guess is that this would depend upon some rewrite rule getting fired.
Did you mean `head $ sort mylist`? Or maybe using `last` instead of `tail`? `tail` is unlikely to change the complexity. As a reminder, `tail` gives you back a list without the first element. In your example the list still has to be sorted in its entirety which is likely O(n log(n)). :-) For the version with `head` this would become equivalent to the function `minimum`. Another comment mentions rewrite rules, I doubt there are rewrite rules for these cases around `sort` (but I could be wrong). Depending on the sorting algorithm used, the rest of the list (the `tail`) might not necessarily need to be sorted thanks to laziness. That is, if the first element of the sorted list is known before the rest of the list is sorted, Haskell will "wait" before sorting the rest of the list until it's needed, which is never in the `head`version. Hope this helps! :-) Tell me if you have any questions.
Thanks for the pointer, I'll check them out.
oops, I renamed the file. link fixed now
Thanks for the input! While that may be true for, say, selection sort, is it really generally true though? [Some Googling yields](https://softwareengineering.stackexchange.com/questions/180417/fastest-haskell-library-sort-implementation) a claim that Haskell uses merge sort, which would need to run the full O(nlogn) algorithm to get the first element, which doesn't sound very lazy. Also, I meant to write `last`, not `tail` - sorry about that. In the case of `last` instead of `head`, even selection sort wouldn't benefit from lazy evaluation. So is there something else at play here?
I prefer the terms Purely Functional (Haskell, Idris, et al.), Highly Functional (Erlang, OCaml, Elixir, et al.), Functional (Rust, Java, C++, et al.) and Non-Functional (everything else that doesn't apply). The difference between Highly Functional and Functional is the interesting distinction in this crowd. Strong support for recursion is necessary, which rules out Python, Java, and apparently Rust (haven't used it much). Highly Functional languages still have mutation, but it's not at their core. It's mostly as an aside from every thing else in the language. Erlang and Elixir have ETS and OCaml has ref's. Elixir's 'reassignment' of variables is really just hidden in the syntax itself and is static single assignment under the hood. Functional languages can still have immutability like a const, but when mutability is a consistent factor in how you program in the language; the language is not Highly Functional. That's how I like to think about it anyways.
Of course, which really argues in favour of the Wittgensteinien thesis that the meaning of a word is just its use. I know other JS people who say that it is not functional, but instead is OO. To them that is true because their use of the word "functional" is different to your use. I guess that's also why it's probably more fruitful to talk about specific language features. 
The problem is different people have different uses of the word and there is no privileged viewpoint here, which makes it impossible to say whose is "right". It's better just to talk about specific language features, otherwise we are just arguing about nonsense. The thing to remember is that the word is not a mathematical term, so it's not well defined. It's just a word of English and so contains multiple layers of ambiguity and vagueness, as all English words do.
In my humble opinion, functional programming is wathever that makes computer programming having a good foundation in some abstraction, so that a program can exhibit the properties and laws that the problem has under this abstraction. If the program has the property A, then the program has such property A. In this sense, functional programming exist since FORTRAN (formula traslation) which was the first language in which numerical formulas could be used directly. Previously, such formulas needed to be coded in long imperative sequences of machine code, involving IO between the processor registers and the memory that destroyed the properties of the problem. That can be applied to other parts of a program: maps preserve properties while loops don't. monads preserve properties of algorithms, ad-hoc imperative sequencing don't. pure structures preserve a lot of mathematical properties, inpure structures generally don't, and so on.
I don't know if this is supposed to be a survey without "hints" from other people, if yes ‚Äì please remove this comment, but otherwise I wanted to emphasize that there's one particular feature that I think would make Haskeller's experience way better than it is. It's "Attach stacktrace information to SomeException" https://ghc.haskell.org/trac/ghc/ticket/12096 As I understand, if that is done, we would finally be able to get rid of the nasty "error: something is bad somewhere" style of one-line errors and will get proper call stacks (if we mark our code with HasCallStack).
[removed]
1) Developments which help the cross compiling to mobile teams, since mobile has for a fair while now actually dominated the actual 'people using things' stuff, and using the unbridled awesomeness of Haskell on a new thing can only make things better. 2) Stopping to make sure that the compiler is not getting radically slower. The REPL turnaround on Haskell is pretty solid, but could be better and should never allowed be worse! 3) More cool type things! 4) Making the same algorithms faster. That is, smarter and better optimizations to make the same programs run faster. 5) More of these studies!
Wow, this was very good! I've known about type-level programming in Haskell, but now I kinda know how you would implement it!
Wow, this just keeps getting better and better! I am looking forward to seeing how you model contracts! In my opinion, purely functional languages are not well-suited for the domain of contracts. Purely functional languages work with data (which is finite) in finite time. But [contracts unfold infinitely in time](https://dl.acm.org/citation.cfm?id=1177178). They are objects that interact with other objects. While it is certainly useful that we can prove properties about our functions, it is this interaction between objects that I would like to prove properties about. This is not to criticize your work, which I'm a big fan of, just some random thoughts.
`Data.List.sort` does indeed use a merge sort. If we look at the algorithm, We can conclude that to find the first element, we need O(n) time. There is only a single bang pattern in the mergesort algorithm in `base`: when we merge two lists, the resulting list is strictly evaluated to weak head normal form. That means that the first element of each and every sublist to be merged is evaluated the moment you query the first element of the final sorted list. Note that the remainder of the list is left to be lazily evaluated later as required. The final sorted list is the result of merging two big "1/2 lists" together. To find the minimum of the final list, mergesort only needs the minimum element from each of the "1/2 lists" (we know that this minimum has been strictly evaluated as well). You compare the minimum element from each "half list" to get the final result. In order to get the minimum of a "1/2 list", you need to find the minimum of the two "1/4 lists" and compare them and so on and so forth. Finding the minimum element in this way takes O(n) comparisons, since every time you do a comparison of a "1/n list", you eliminate a single list from the running, and there are O(n) lists of any length. To find the original sublists in the mergesort algorithm, you must run over the entire unsorted list once to find ascending and descending sequences. This operation takes O(n) time. So in the current implementation (base-4.12.0.0), It takes O(n) operations to generate sub lists from the unsorted list (for which we already know the minimum), and takes O(n) time to find the minimum value of all of the sublists merged together. Therefore the total running time in O(n). Note, to find the *second* element, you only need O(log n) time, which I will leave as an exercise to the reader. Quicksort has similar big-o performance characteristics as mergesort, but the computational complexity is not deterministic, and may perform as slow as O(n^2 )
I think perhaps I have a different view on what Smart Contracts are, but, to me, they are just programs that use resources to compute, that have their state stored forever. Not all contracts need communication, but that can be seen as just IO. APIs are part of most real world software. Ethereum, to me, is just a decentralized computer. I don't plan to model contracts much differently than I'd modeling regular IO programs...
Programming language Clean is a counterexample. It's definitely functional, but its evaluation model is based on graph reduction rather than lambda calculus. 
I think the historical usage should be privileged, unless a non-fallacious connection between the work and meaning can be established by newer usage.
"Functional" is a **specific language feature**. It's first-class functions and *nothing else*.
I see, that's really interesting! I had never heard of a recursively lazy merge sort like this before. I think I'm starting to understand why my friends are so fond of Haskell.
Thanks for the in-depth response. You basically hit all the beats I was wondering about. Basically, it seems like while GHC is very smart, it's not magical, and while Haskell is a lot more declarative than your traditional imperative languages, it's not completely declarative - there's no substitute for good software engineering. The fact that GHC's behavior and lazy evaluation can be consistently reasoned about like you say makes me a lot more excited for learning Haskell in the future.
Historical for which group of people?
Given that most modern languages support first-class functions, describing something as "functional" ceases to have much meaning, then. Not that I disagree with that assessment. 
Why are you singling out *that* group of English speakers? What makes their use of the word privileged?
Agreed. But, certainly at the time the term was coined, and even for quite a while afterward, many languages used in industry (if not acedemia) did not support first-class functions. I still write C and Java 6 for work, so I'm still envious of lambda terms and first-class functions. I think mostly what people are talking about is *purity*, though there's definitely a raft of those features they like (pattern matching for one). And, I can understand wanting to avoid that word when "proselytizing", since "pure" and "impure" have, to me, stronger value connotations than "functional" and "non-functional".[1] I'm certainly open for a new vocabulary, for either specific features or a vague collection of features; I just don't want use cannibalizing "functional" when there are still books in print using the existing meaning. [1] Even the later has been a source of confusion in my own communication attempts, where the person I am talking to applies the "providing a function or in operation" meaning that we use in the phrases like "functional machinery". The conversation about VB6 being functional was very surreal until I made sure we went back and defined our terms.
They provided the initial definition that can be used as an adjective for programming languages. Plus, what we are trying to do is *categorize programming languages* so, for ease of reuse of existing work, we should follow established jargon and other terminology by default, and only revolutionize it for an advantage that outweighs being able to easily reuse the existing body of work.
&gt; I do think this whole thread illustrates that you will get a different answer to the question "what is functional" from each person you ask. I completely disagree. You get random answers from the uninformed, but if you have an established history in categroization of programming languages, the term "functional" is well-established. &gt; perhaps it would just be better to talk explicitly about I agree that we should talk about separate language features separately. But, I'm not (yet) willing to "yield" the terminology "functional" and go back to saying/writing "first-class functions" instead.
 (:) :: a -&gt; [a] -&gt; [a] Do you see your error?
So (:) expects a list as the second argument for some seconds I thought about thart, too, but changing it [n mod 2] didn't do anything
&gt;As a rule of thumb, how do you go about reasoning about compiler optimizations like this? This example is not a question of compiler optimization but rather laziness. There's no simple rule of thumb, but Okasaki on functional data structures gives a method for reasoning about the performance of lazy programs. Generally, I wouldn't rely on the compiler to rewrite `last . sort` to `maximum`, but you might look through `base` to see if any rewrite rules are in place to do this. 
Because you plug in \`toBin n \\\`div\\\` 2\` into \`(:)\` you get your error. It now expects as the second argument a list of lists, because the first argument is of type list. Go to r/https://hoogle.haskell.org, search for a function with type signature of \`\[a\] -&gt; \[a\] -&gt; \[a\]\`, turn the second argument into a single element list and apply the function you'll find on hoogle to both arguments.
&gt; Functional (Rust, Java, C++, et al.) But... those languages aren't remotely functional. 
Indeed we intentionally left it open-ended to avoid bias. We really want people to think about the problems that they have.
Perhaps start by reflecting on the things that you find slow your workflow. We'd love to hear from users what papercuts they routinely encounter.
Thanks!
&gt; functional programming is wathever that makes computer programming having a foundation in some good abstraction This makes no sense. 
MS defined [some](https://docs.microsoft.com/en-us/dotnet/api/system.func-2?view=netframework-4.7.2) [delegates](https://docs.microsoft.com/en-us/dotnet/api/system.action-1?view=netframework-4.7.2) for .Net 3.5 (?), so fewer delegates to juggle. I enjoyed writing the statically typed "functional" uglies with them.
Ok, I'll spell it out because it might be hard to see for you: You basically have your arguments to `:` flipped, try ``` toBin n = (n `mod` 2) : (toBin (n `div 2)) ``` instead. Now of course your list will probably be reversed compared to what you were expecting, but you can probably fix that (you also are missing the case for `toBin 0`).
Feels a lot like what Java 8 did, in you know a more Java-y way.
.Net 3.5 predates Java 8 by ~7 years, so I guess they just learned the mentality from Java, and then Java people did as they do. Also AFAIK Java's generics are broken in ways I can't comprehend, so some functional uglies aren't even possible that are in C#.
why?
Don't we already have that? module Main where import Test.DocTest import GHC.Stack -- | -- &gt;&gt;&gt; foo1 -- *** Exception: oops -- CallStack (from HasCallStack): -- error, called at src/Main.hs:20:8 in main:Main -- foo3, called at src/Main.hs:17:8 in main:Main -- foo2, called at src/Main.hs:14:8 in main:Main -- foo1, called at &lt;interactive&gt;:... in interactive:Ghci3 foo1 :: HasCallStack =&gt; IO () foo1 = foo2 foo2 :: HasCallStack =&gt; IO () foo2 = foo3 foo3 :: HasCallStack =&gt; IO () foo3 = error "oops" main :: IO () main = doctest ["./src/Main.hs"] 
Aha fair enough. I guess my views come from my views on the philosophy of language, so aren't likely to change either :) 
I would also love to see what people are interested in.
The type of `return` can be thought of as `forall a. a -&gt; (forall Monad m. m a)`. We'd never write it was way because the forall and constraint can be "floated" out, but `return` has a polymorphic return type that is not fixed by it's argument. Similarly `def` (from Data.Default) and `mempty` (from Data.Monoid) are polymorphic values; they don't have any (value) arguments, so their "forall" is definitely applied to the (return) value. `def :: Default a =&gt; a` and `mempty :: Monoid a =&gt; a`. Finally, part of the reason that all of these have a constraint, is that an unconstrained `forall` in the terminal location is equivalent to `Void`, the uninhabited type. {-# LANGUAGE RankNTypes #-} data Void absurd :: Void -&gt; (forall a. a) absurd x = x `seq` absurd x drusba :: (forall a. a) -&gt; Void drusba x = x (absurd's inferred type is more general; drusba has a type that won't be inferred.) theAnswer :: Int theAnswer = absurd . drusba $ error "How many roads must a man walk down?" 
Hey guys, im new to Haskell and im trying to make a morse code converter of sorts, this is what i have: data Morse = Dot | Dash | Gap deriving Show encode :: String -&gt; [Morse] encode [] = [] encode "A" = [Dot, Dash] encode "B" = [Dash, Dot, Dot, Dot] ... encode "Z" = [Dash, Dash, Dot, Dot] encode (x:xs) = encode x : encode xs main :: IO () main = putStrLn( show( encode ["ABC"] ) ) however i get a few errors, all surrounding the last line of my encode function. I will show the errors main.hs:35:17: error: ‚Ä¢ Couldn't match type ‚Äò[Morse]‚Äô with ‚ÄòMorse‚Äô Expected type: [Morse] Actual type: [[Morse]] ‚Ä¢ In the expression: encode x : encode xs In an equation for ‚Äòencode‚Äô: encode (x : xs) = encode x : encode xs main.hs:35:24: error: ‚Ä¢ Couldn't match type ‚ÄòChar‚Äô with ‚Äò[Char]‚Äô Expected type: String Actual type: Char ‚Ä¢ In the first argument of ‚Äòencode‚Äô, namely ‚Äòx‚Äô In the first argument of ‚Äò(:)‚Äô, namely ‚Äòencode x‚Äô In the expression: encode x : encode xs main.hs:35:28: error: ‚Ä¢ Couldn't match type ‚ÄòMorse‚Äô with ‚Äò[Morse]‚Äô Expected type: [[Morse]] Actual type: [Morse] ‚Ä¢ In the second argument of ‚Äò(:)‚Äô, namely ‚Äòencode xs‚Äô In the expression: encode x : encode xs In an equation for ‚Äòencode‚Äô: encode (x : xs) = encode x : encode xs This is really bugging me as i cannot see how in the first and second errors im getting the wrong type, and for the last error why does it expect \[\[Morse\]\], im so confused.
&gt; Can you recommend what the learning progression should be? I'm going to hesitantly recommend [Haskell Programming from First Principles](http://haskellbook.com/). After that, probably [Parallel and Concurrent Programming in Haskell](https://web.archive.org/web/20180108044627/http://chimera.labs.oreilly.com:80/books/1230000000929/index.html) from /u/simonmar. Only then do I think you'll be ready for [Real World Haskell](http://book.realworldhaskell.org/), not because it is complicated, but because the text (particularly code examples) hasn't aged well, so you'll have to bring it up to date as you work through it. Any time after the first half of HPFP, I recommend going off on your own small projects, and stopping back in to /r/haskellquestions or here, for more detailed recommendations. After the 3 books above you should be able to go in your own directions entirely, and only check in with the community when you want to collaborate or to clarify some specifics. Finally, I'm going to recommend [TDD in Idris](https://www.manning.com/books/type-driven-development-with-idris) even if you don't write a line of Idris (though some of the exercises will be... difficult in current GHC Haskell). It gives some good ideas about how to leverage a type system, and over time more of it's techniques will be applicable to advanced GHC Haskell.
4 spaces before each line of code yields preformatted text that is easier to read
&gt; A functional language is one in which functions (or whatever you name your native callables) are first-class values. They can be passed as arguments, returned, and created at runtime, as well as anything else you can do to other values This isn't actually a precise definition, and it could exclude all strict functional languages in some interpretations. 
Suggesting a new verb: "to applicate". Spread the word :-)
&gt; I'm absolutely willing to give Python and Javascript the adjective "functional" -- they earned it. In what world is Python functional? 
&gt; I personally find that definition unsatisfactory since it includes JavaScript and Python, things which are ostensibly not functional. Python is definitely not functional, but JavaScript has libraries for recursion schemes and such. 
&gt; it could exclude all strict functional languages How so? Lisp is strict and functional.
Not to mention, most of the people calling Rust "functional" are doing so for reasons wholly unrelated to technical merit. 
&gt; haskell is the epitome of a "functional" language (which I think most people agree), Most of this is because lots of functional programming paradigms developed around/in Haskell due to GHC. That's not a coincidence, but some of them can be ported to strict languages rather easily. 
&gt; A haskell guy is probably going to talk about type systems and type system enforced purity. That's a shortcoming of the community, to a large extent. &gt; A ruby guy is going to mostly just focus on ruby's map/filter/reduce equivalents. Every one of them will think that he is promoting functional programming. How on Earth is Ruby functional programming? It's an object-oriented language. There's more to FP than maps and filters. 
&gt; "Functional" means "has first-class functions". But apparently not optimizing tail recursion? That's frankly nuts. 
&gt; If you're learning Rust from an FP background, I suggest just considering it an imperative language. Fundamentally it's a different take on C++. Agreed. And moreover treating it like Haskell will make your code significantly slower. 
I not extremely concerned about their motivations; I care much more about the accuracy of the statement. Just skimming https://doc.rust-lang.org/1.8.0/book/closures.html doesn't make me think of Rust as functional, though I do recognize that at least some of these issues can also come up for other first-class values.
&gt; And functional languages treat these functions (read: callable things) as first-class values You keep repeating this but it isn't true. 
&gt; The problem with general tail recursion optimization is that it depends on garbage collection One does not need *general* tail call optimization, but rather, one would like to be able to replace while loops with simply writing functions. Hence, *functions* suffice where a language like Rust has to rely on compiler builtins like `while` loops. 
Thanks a lot! How well has HPFP aged, being published in 2016? All still current? Why hesitant in recommending it? Bonus question, just cuz I'm polling everyone I can now: Haskell or Clojure and why?
There's no way to know how much of that is a result of ghc, and how much is a result of qualitatively more code being generated/linked by the upgrades of all the attendant libraries -- which is quite a significant jump!
"first-class value" means that you can do the same things with functions that you can with other first-class values; there are no restrictions on what you can do with functions that make them "second-class citizens" among values. What those "things you can do" are can vary, and what other values exist definitely varies. At least least, "first-class" means they can be passed as arguments, returned, and created at runtime. Once you define / pick the language, this definition gets less vague since it has more specific behaviors in the other values to look at. But, until you know what other values can do (other than the 3 minimal requirements), you can't decide is functions are simply not values, or are some how restricted (aka "second-class") values. --- Evaluation models is immaterial to whether a language is functional or not. It can be always strict, it can support call by name or call by need, it could use call by push-value internally, or whatever. The evaluation model doesn't matter when determining if a language is functional or not.
It depends on how you use ruby. If you mostly write pure functions using [persistent data structures](https://github.com/immutable-ruby/immutable-ruby), how is that not functional programming? Ruby isn't ideally suited to that style of programming, but it can support it well enough. Also, map/filter/reduce are still functional programming. Sure, idiomatic ruby generally focuses on more imperative/oo stuff for large scale design, but any time you use map, you are using functional programming. You are using a pure function to compute a new data structure without mutating the old one -- that's functional programming in a nutshell. You just using it within a single function instead of designing your entire app using those principles. Saying "I like functional programming, so I'm using bits and pieces of it in ruby" isn't unreasonable.
\[Typed holes!\]([https://www.youtube.com/watch?v=0oo8wIi2qBE](https://www.youtube.com/watch?v=0oo8wIi2qBE))
&gt; Why hesitant in recommending it? IIRC, there's been some drama in the production, but I don't want to go into details. &gt; Haskell or Clojure and why? Depends on what you want. I was specifically looking for purity / referential transparency when I found Haskell. If you do stuff on the JVM a lot, I'd recommend Eta instead. If you live on the CLR, probably stick with Clojure. GHC Haskell is probably the most brain-bending though, depending on your background. I think most people find it easier to learn in groups, so if you can find a group for one language but not the other, that could aid your decision.
&gt;IIRC, there's been some drama in the production, but I don't want to go into details. Ok, but nothing to do with accuracy then, right
&gt; Ok, but nothing to do with accuracy then, right Right. I am unaware of any specific problems with the content, and in general I think any errors are probably rather minor.
Using maps is not the be-all and end-all of functional programming. &gt; JS doesn't push fp over oo/imperative programming, but it still supports fp if you want to go in that direction. Sure, but it also has things like recursion schemes. 
&gt; "first-class value" means that you can do the same things with functions that you can with other first-class values; Right, which you can't, in a strict language. But strict languages are still FP.
Brilliant. Thank you.
So you design a ruby program with a single mutable reference to the current state, where the state is composed entirely of immutable/persistent data structures and everything that changes the state is basically a pure state -&gt; state function. How is that not functional programming? Yeah, sure, you have to be a bit more careful about recursion, but I don't actually use direct tail recursion all that often in "real" fp languages. Seriously, I call `recur` all of twice in my current ~13k line clojure project. Converting those two functions to use while doesn't mean that the rest of my code base suddenly shouldn't qualify as functional programming.
Laziness is both a blessing and a curse. It does enable some super beautiful usages like this though.
Really, the word functional doesn't mean much. Some take it to mean supports HOFs, some mean it to mean purity, some take it to mean an expressive type system, it's just not a well defined term.
&gt; haskell types lul
What are you talking about? Strictness has nothing to do with what you can do with function values. List is strict and function values are first-class there.
A follow up on this issue: enabling split-objs does reduce the binary size to 65M. Unfortunately this increases the compiling/linking time considerably. Aparently there is a new option split-sections that should be preferable to split-objs but I haven't figured out how to enable it.
What would explicitly doing that look like?
Thanks for your reply! That's what I thought too, but when I reverse the order of the first two guards I get a non-exhaustive search error. Which doesn't make any sense because there is an otherwise guard.
That reminds me of something that i didn't think of when i submitted my response earlier, it would be nice if the compiler had a communication protocol for interaction with editors like Idris and Agda do.
For those on old reddit: matches :: Integer -&gt; [Integer] -&gt; [Integer] matches _ [] = [] matches n (x:xs) | x == n = x:matches n xs | otherwise = matches n xs
Yeah it helps a lot thank you! 
&gt; `forall a. a -&gt; (forall Monad m. m a)` Should be: forall a. a -&gt; forall m. Monad m =&gt; m a In order to be valid Haskell. 
"Type" is overloaded. `Maybe` *is* a type..of kind `* -&gt; *`. Just as `'True` is a type of kind `Bool`.
And since this is Haskell, you can write a *section* instead of a lambda: `(==n)`.
So the library should work but I yet have to write a large test-suites for covering the many combinations of Request representations and possible errors when servers have issues (contributors welcome). Many thanks to Alp Mestanogullari for pointing me at the relevant piece of codes in the other packages.
Haskell for Mac is ok for beginners, although its Libraries slightly outdated.
Just use safe package or Control.Lens.Cons if u familiar Lens library.
Thx for your answer !
Thanks for all your work, I'll look asap.
My pleasure. I'm especially interested in benchmarks in setups representative of seen-in-the-wild deployments: reverse-proxy terminating http2 to an http1 backend vs. when the backend directly exposes http2 ; but also across a WiFi-LAN vs. within-a-cloud etc.
I concur. I've found `Protolude` to be well designed and documented. You will bump into issues where `String` needs to be converted to `Text` while also tagging `"quoted text" :: Text` but other than that, a great alternative. `RIO` is another one worth noting among several others.
Emacs + [dante](https://github.com/jyp/dante)
A language is functional if its creator and community consider it functional. That's it. Your colleagues are going to be the ones reading your code and deciding if it ends up in the codebase at the end. And if they are not used to functional features your code is not going to be accepted no matter how much time you spend explaining "Oh C# has first class functions so that means it's okay for me to implement something with a Free monad and my own makeshift Functor interface". On the other hand, if you start rambling about functors and monads to JavaScript people, there's a big chance they will understand you because FP has become a huge influence on client-side application design in the last few years and more people are familiar (and have a liking for) these concepts.
What are your thoughts on [Relude](https://github.com/kowainik/relude) (and, by extension, Protolude vs. Relude)? I tried Protolude briefly, and am trying Relude currently, but I lack the experience to draw any solid conclusions on which is better.
Thanks. I so rarely use the explicit `forall` that I just assume it's supposed to read like maths.
Here's one: `error "fromInteger @Void"`. The one the code I provided uses is slightly simpler: `undefined`.
&gt; they do allow for functions as first class citizens in a language and as such are functional. "first class citizens" is not well-defined and in fact not optimizing tail recursion (or even being strict) could be seen as not being a first-class citizen. &gt; It doesn't really matter though. It's all just worthless jargon no
Why no love for Delta: Base Change?
Are remote work from EU possible?
Thanks for the kind words :)
Thank you. I was indeed thinking of using protolude, but I find it a bit annoying that, when I add a trace somewhere in my program, it raises a warning at compilation: it prevents gchid from running the program. (I like to use : ghcid "--command=stack ghci myProgram" "--test=:main myArgs", which automatically runs myProgram whenever I save it, provided there is no error or warning) This is what I meant when I said I was looking for a solution that did not make additional changes to Prelude. (Maybe there is a way to disable these warnings)
Hi, I'm a maintainer of `relude` and I can give you a rough comparison :) It's based on `protolude` but has a huge number of differences. 1. `Protolude` supports older GHC versions (from GHC 7.6.1) while `relude` only supports from GHC 8.0.2. So if you aim ancient GHC versions, `protolude` might be a better choice. But because of that it contains a lot of CPP, code is ugly in some places as a consequence and it's more difficult to add, remove or change things there. 2. `relude` has much better documentation: * [high-level overview of internal module structure](http://hackage.haskell.org/package/relude-0.3.0/docs/Relude.html) * 100% Haddock coverage * Almost every function has usage examples and all examples are tested with `doctest` (which also sometimes hard to do because of multiple GHC versions support, but we try really hard) * [Tutorial + migration guide](https://github.com/kowainik/relude#structure-of-this-tutorial) from `Prelude` and just general description of the whole package and libraries it depends on. 3. `relude` has custom HLint rules specific to it: you can use them to remove redundant imports or find hints how to use functions from `relude`. Moreover, the HLint rules are generated using Dhall and there is [a blog post about used technique](https://kowainik.github.io/posts/2018-09-09-dhall-to-hlint). This allows to maintain HLint rules much easier because it's already not an easy task... 4. `relude` has less dependencies and slightly lighter because of that but still very powerful and useful. 5. One minor difference: `head` in `protolude` returns `Maybe a` while in `relude` it works with `NonEmpty`. 6. Minor feature: `relude` uses type-level magic to forbid `elem` and `notElem` functions for `Set` and `HashSet` (because `elem` from `Foldable` run in O(n) time and you can accidentally use `elem` from `Foldable` but with `relude` you can't). 7. `relude` is opt-in oriented and has a notion of `Extra.*` modules that are not exported by default from the `Relude` module. So we don't spoil global namespace but still have a lot of useful features like polymorphic functions to work with every `newtype`, `Enum/Bounded`-related useful utilities, functions to take a name of any type as `Text` and much more. It's very easy to make them accessible package-wide with `base-noprelude` trick! Btw, you can use [`mixins` cabal feature](https://twitter.com/ChShersh/status/1053205244438503429) to replace default `Prelude` with `relude`. So, basically, we in Kowainik try to push very hard to give the best possible UX for the `relude` users :) For example, since GHC 8.6.1 the `base` library reexports `Contravariant` functors and `relude` already has PR merged that adds these reexports, so you can enjoy all new features!
&gt; In standard Haskell, all inhabited types (types that have at least 1 value) are of kind `*` Is this Just The Way It Is‚Ñ¢, or could we imagine a scenario where it would make sense to have more than one kind for inhabited types? Perhaps a kind for types can be inhabited by all values except ‚Äúbottom‚Äù?
So this would be good if I had to hit the same API over and over fairly rapidly?
this over anything else (intero, language server variants, ...)
There's a pretty sizable Haskell Discord community here: https://discord.gg/FvT2Y5N
vscode and the Haskell ide engine is an excellent choice (https://github.com/haskell/haskell-ide-engine/)
Correct. HTTP2 is the underlying transport-protocol of gRPC from Google (marketed for its high performance). There are two situations where HTTP2 will make a great difference: heavy load in datacenters where initiating a TCP connection is a non-negligible overhead. Similarly, you'll quickly amortize the TCP-dial up when suffering high latencies (e.g., residential connections). If you research HTTP2, you'll find that HTTP2 also has an interesting feature: server-push. Server-push was designed to hastily distribute static assets before web browsers ask for them (hence, server push is only marginally relevant for HTTP APIs).
&gt; Looking at the alternative Prelude, it looks like String and partial functions (e.g. head) should be avoided. Strings are an unfortunate reality in Haskell, but partial functions are not bad enough that it's worth the added dependency burden of using an alternative prelude. I'd suggest just depending on the `text` package for that reason. You might consider banning partial functions in the project's `.hlint.yaml` file or just using a per-project prelude that depends on `base`. You could use backpack + cabal to make your project prelude the default if you *really* wanted. 
`lens` is not in any way a replacement for the prelude.
1. Use `Text` in all your code. 2. Discover that all code interacting with the outside world uses `String`s or `ByteString`s. 3. Shove a bunch of `toFoo` and `fromBar` calls everywhere. 4. Discover that there's no standard usage of strict vs. lazy `ByteString`s in these libraries. 5. Shove some more `toBaz` and `from Quux` calls in there. 6. Oh no, turns out you actually needed utf16 and the conversion is dropping characters! 7. Drink.
Lazy vs. strict is a performance issue, so I can't imagine how it could be standardized. IMO, you accept lazy bytestrings and produce strict bytestrings whenever you efficiently can. Because if you ask for a strict bytestring, your caller might have to force the lazy bytestring they have, thereby causing it to be copied, but the other way round is no problem. And while returning, if you are already producing a single block of memory with bytes in it, it would be wasteful to forget this fact and wrap it as a lazy bytestring, since your caller might actually need that (maybe they'll use it as a C array).
Any thoughts on adding witherable to relude? The functions I hide most often from `Prelude` are `filter` and `catMaybes`, so I would love for a Prelude-replacement to handle that for me.
More of a blessing than a curse IMO. See [here](https://www.reddit.com/r/haskell/comments/5xge0v/today_i_used_laziness_for/deia53t).
Only works for new Reddit / redesign. Doesn't work for old Reddit.
``` test ```
Yes. I switched back and forth today, actually; testing it.
You've skipped two compiler releases and upgraded libraries? If this is a significant problem, I'd suggest tracking it down so that you can report bugs with the appropriate amount of information. 
I think that's a little unfair. Rust has taken a lot of influence from functional programming languages. It contains a lot of approaches which have been popularised by functional languages. Namely Option and Result, but a lot of other things too. That's why people say it's functional.
The source on Lucid‚Äôs Hackage page is cross-linked for me. Maybe something‚Äôs been fixed since you posted this?
Huh, interesting.
&gt; Namely Option and Result, but a lot of other things too. Those have basically nothing to do with functional programming.... &gt; It contains a lot of approaches which have been popularised by functional languages. Well, several. It has folds, maps, and zippers because what language doesn't? They don't make a language functional. 
Well the word I used was 'popularised'. Do you really disagree? If you go back say 10 years then the functional language were pretty much the only ones pushing those ideas. The mainstream non-functional languages only gained them through copying.
ADTs aren't a functional language feature. 
I never said they were.
I like `witherable` but I don't think we can add it to dependencies of `relude`... But I'm open to discussion! You can open issue in the `relude` and I will try to put all my thoughts about `witherable` there: * https://github.com/kowainik/relude/issues
Add `skip-msys: true` and `system-ghc: true` to `%APPDATA%\stack\config.yaml`.
Thank you ! I'm going to use Relude. I like its design goals, its support, and its extended documentation. I realize you put a lot of effort in it: please keep going !
The lambda calculus is a mathematical formalism, nor an evaluation model
I totally agree. The ergonomics around strict/lazy `Text`/`Bytestring` is quite poor. It would be great to have a type class in the `Prelude` that has `toStrict` and `toLazy` (with a law that this is an isomorphism) methods without depending on `text` or `bytestring`, so that we can use them everywhere without constantly thinking about what's imported and qualified as what.
I'll give this a shot, thanks.
As above, thanks! I'll take a look at that. 
(duplicating my comment from HN) I threw this together a while back. I've been meaning to clean up the code a bit before sharing but that keeps not happening so I figure I'll try the other way 'round. Big thanks to Matt Halverson for getting queryparser freed at Uber and to Heli Wang for more recent maintainership, as well as for their contributions to that project in the first place!
&gt; I thought you meant Void does indeed have valid instances for both Floating and Integral. No. Sorry for the confusion. You can get type-correct bottom-free instances of both classes for `()` (unit) though. I think they are even law-abiding, even if they aren't terribly useful. You are right that neither the `Void` in base, nor any uninhabited data type, can have bottom-free instances of both types, for the reason you state.
I think the easiest and most reliable setup is your preferred text editor + [ghcid](https://github.com/ndmitchell/ghcid) + (optionally) [hlint](https://github.com/ndmitchell/hlint). No editor plugins or GHC versions to worry about; it just works, and it should be enough to get started. You can worry about fancier features later on.
&gt; you had to worry about which packages worked together on which versions, and when you could update without breaking things. If only there were a way to encode this information in the .cabal file then we could have something like a dependency solver automatically find sets of packages that work together for you...
I don't see how that would help avoiding to deal with module imports unless you collapse everything into a single module namespace. But then you'll still need to deal with with at least one module prefix to disambiguate verbs such as `length` or `map` unless you introduce yet another type class to abstract over operations applicable to non-polymorphic containers.
In my opinion simply using the isomorphic `Bool` instead of `Bit` is fine too and you can directly reuse logical operations such as `&amp;&amp;` or `not`. 
 There is also a pretty decent Intellij plugin which we use at work, install the latest beta (not directly from the intellij marketplace, the version is rather old): https://github.com/rikvdkleij/intellij-haskell/releases The first start will be long as the plugin downloads and compiles its own dependencies like intero, hindent, stylish... But then it works nicely. One of the best features IMHO for beginners is "navigate to definition" which works also for dependencies/prelude, it helps a lot to understand what's going on. You also get things like jump to test/source, type info, exe runner, etc.
Good luck to you if one package needed `blah == 1.2.*` while another needed `blah == 1.4.*`. Not only could those two packages not both be in a dependency graph (they still can't); package installs were global, and you can't have multiple versions of `blah` installed. This is the problem solved by stack, and later by cabal new-*.
&gt; It may be well known that you can use Applicative instead of Monad to indicate independence but I haven't seen it written down. For this, I can recommend [Jared Tobin's blog post](https://medium.com/@jaredtobin/encoding-statistical-independence-statically-ec6a714cf24a).
This looks like a step in the right direction. Did you read the BitML paper? Or try it?
I would recommend separating the desired JSON structure from the underlying db representation. Regardless of the specifics, having the two decoupled will be better as they can evolve separately. Something like this: ``` data Something = Something (Entity Event) (Entity Venue) [Entity Performers] instant ToJSON Something where ... -- whatever you want someHandler :: EventId -&gt; Handler Value someHandler eventId = do event &lt;- get404 eventId venue &lt;- get404 $ eventVenue event performers &lt;- selectList [PerformerId &lt;-. eventPerformers] [] sendResponseJSON status200 $ Something (Entity eventId event) (Entity eventVenue venue) performers ``` I personally like to start with multiple queries and not prematurely optimize, but you could also use esqueleto do query this data in one query using joins. (Apologies if this formats wrong, I'm on my phone at.) 
You‚Äôll want to use Esqueleto to do SQL joins for this. Esqueleto can be a little intimidating, but it‚Äôs pretty easy to copy paste an example query to modify it for your needs. You‚Äôll get the hang of it pretty quick. This is what I imagine the schema looking like: 1. A table with venues 2. A table with artists 3. A table with events. This will have a foreign key to its one venue. I‚Äôm assuming there can be multiple artists, so I would make a 4th table that consists of a unique pair of (artist_id, event_id) to represent the many to many relationship
&gt; It seems that in persistent I have two options, either store relation as a foreign key (i'm doing this now) or as a character varying (json) in the db. I don't understand the json option. Are you saying that 1. you have both an `Event` table and a `Venue` table, the `Venue` table has an `id` column, and the `Event` table has a `venue` column which stores the id of the venue, but encoded in json somehow? 2. you only have an `Event` table, one of its columns is `venue`, and it stores a json object containing a bunch of fields describing the venue? 3. something else? &gt; Storing the foreign entity as JSON does not work because the id is not included. Assuming interpretation (2): don't you decide which fields you want to include in your json blob? Since there is no `Venue` table in which to lookup a venue by id, why do you want an id? &gt; Storing the foreign key as a foreign key also has issues because it's hard to get anything else than the id from the foreign key. With only persistent's API, you need to make one query to fetch the event's fields, and a separate query to fetch the venue's fields. If you are using a relational database, you can use [`rawSql`](https://hackage.haskell.org/package/persistent-2.8.2/docs/Database-Persist-Sql.html#v:rawSql) to make a more complex SQL query returning fields from more than one table, or [http://hackage.haskell.org/package/esqueleto](esqueleto) if you want type-safe SQL queries. I would start with the two persistent fetches, it's the simplest approach. &gt; I cannot perform a monadic action here to fetch eventVenue properties? Presumably you mean performing a persistent query? No, you can't, and the fact that you can't is an important feature of Haskell. The language is preventing you from hiding side-effects inside functions which should not perform any. Here, `toJSON` is a function which formats a value into json, it would be very surprising if such a function was to return a different value depending on the state of the database. If you are used to working in other languages, this might feel limiting at first, but I find that it not only helps me while debugging, it also helps me to structure my thoughts about how I should organize my program. In this case, the types suggest the following structure: 1. create a new type `Result` which includes all the information which you want to include in the json 2. make some persistent calls on order to obtain all the events, venues, and performers which you want to include in the json 3. construct a value of type `Result` using all that data 4. write a `ToJSON` instance for `Result`
ahh ok I see yes this is a good idea. And in the toJSON I could then have toJSON (Something event venue performers) = I think this is exactly what I need. Thank you so much!!
The problem with Esqueleto is that it gives me joins in the form of pairs and I cannot represent it like that. I have to represent the JSON in a certain way because it's being pulled by another service. So unfortunately they need to be nested but not pairs. Thank you though Esqueleto is otherwise a very good solution
You can use the pairs to create a new, nested data structure. Once you have a list of (event, artist) you can group them by the event ID, to get to (event, [artist]) (You can do this with Map from the containers package). 
So to clarify. What I meant is that if I do this Event venue Venue persistent seems to store the Venue as json. Even if it is an entity in the database with its own db persistent stores the venue without id because it's just taking the Venue datatype and dumping it to json. The Venue data type does not include id only Entity Venue does. "Here, toJSON is a function which formats a value into json, it would be very surprising if such a function was to return a different value depending on the state of the database. If you are used to working in other languages, this might feel limiting at first, but I find that it not only helps me while debugging, it also helps me to structure my thoughts about how I should organize my program. In this case, the types suggest the following structure" I totally agree and I kind of knew the answer to this stupid question of mine. Believe me I'm very thankful for the limitations of Haskell vs say Python. I have looked at esqueleto and in general it's a very good library. The problem is again representation where I need to make these fields nested since they are being pulled by another service which require their representation to be this way. Your solution with Result is exactly what i think i need to do. Thank you!
&gt; That last example isn‚Äôt a higher-rank type Ah, darn, I had a small feeling in the back of my mind about that. Thanks for the correction :)
Is [yesod-auth-account](http://hackage.haskell.org/package/yesod-auth-account) broken? When I try to compile it I get many errors complaining about ambiguous types and non-injective type classes.
I read it, didn't try it though. It looks like a good direction that Bartoletti et al. are exploring. The big question is, however, how to do an analysis that reveals whether a (Bitcoin) smart contract will behave as expected. I think that they are looking into that right now (not sure though), but that is the real interesting matter. My language here is just a nice thing to have I guess, nothing major. BitML is definitely more interesting, but in my opinion the real interesting matter concerns the formal analysis of smart contracts w.r.t. their possible effects.
OP here, just wanted to follow up with a post with a request for comments -- if you think I've missed something huge here (I almost certainly have), or if you find parts of the post (or the whole thing) outright harmful, please feel free to let me know -- I update my blog posts by putting a large blue banner @ the top in the case of egregious errors or going back and just updating the copy. I'm no Haskell expert but I like to think I know a little bit more than a total beginner might and think there's a gulf in what gets covered by many of the introductory more practical blog posts when it comes to writing web servers with haskell (in particular REST-ish ones). Also, half the time talks are either too introductory or are for type magicians only (I introduce the concept of a strong typing tophat in the post which I think is novel/funny or a good way to refer to those types of antics, so this is a call to that). I originally meant for this to be a single post, but about a week into writing this first bit I realized I should just split it up into 4 parts -- this is the first of 4 posts. Also, if you think the TODO idea is done to death and want me to build some other kind of application I'll take some suggestions on that too, assuming it doesn't explode the project complexity so I have some hope in actually finishing this series of blog posts.
Awesome, can‚Äôt wait to read this!
Seriously Haskell needs an easy IDE for beginners! I just hope people stop suggesting Emacs or Vim
&gt; i would like to use the latest version of aeson (1.4), but telegram-api requires an older version (&gt;=1.0 &amp;&amp; &lt;1.3). Is it possible to install both packages without conflicts? Short answer: See is the cabal `--allow-newer` option can work for you. --- Long answer: You might *think* you want this. You do not. While linking in two different versions of the same library is possible under certain conditions, it allow functions from one version of the library to work with/generate data from the other version of the library. Assuming the two versions never interact, this might work; it might also very confusing errors of the form "Type error: expected type: X; actual type: X". Haskell should pick the vast majority of those up at compile time, unless you use a lot of Typeable/Dyn. IME with Java/Maven it's not caught as often at compile time as I'd like, as shows up too often at runtime. Your best approach is to get a variant of telegram-api that supports the version of aeson you want to use. Contacting the telegram-api developers is one approach, as is forking the code privately. I believe cabal also provides a way to ignore upper bounds, which might work if the current source of telegram-api happens to be compatible with the new version of aeson, but the telegram-api developers couldn't verify that at the time the package dependencies were written.
The name sounds like the pharmaceutical industry's dream drug.
The name made me hungry every time I was working on it. 
&gt; Let‚Äôs take some time to make our main function and the code in app/ more robust ‚Äì there are a few things that almost every executable for a server should do so let‚Äôs set them up before we get down to the nitty gritty of actually building the service. That's why I wrote [wai-cli](https://github.com/myfreeweb/wai-cli) :) And [magicbane](https://github.com/myfreeweb/magicbane) for adding logging/metrics/stuff and having less imports.
Few more images: &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
I have a pretty stupid question about sum types: Let's say that I have a type that can either be A or B: data One = A | B Let's say that I then want to make another type that extends this so that the result is a type that can either be A, B or C. This is how I thought it would work: data Two = One | C However this doesn't seem to work: &gt; B :: Two &lt;interactive&gt;:19:1: error: ‚Ä¢ Couldn't match expected type ‚ÄòTwo‚Äô with actual type ‚ÄòOne‚Äô ‚Ä¢ In the expression: B :: Two In an equation for ‚Äòit‚Äô: it = B :: Two How would I achieve this? Thanks! 
Yeah, you need an extension for the `forall` syntax but the type itself is rank-1. The error message for that is a bit misleading, come to think of it.
Amazing, great job!
Awesome! I will probably order one as soon as that student loan arrives. Maybe you and Bartosz can look into getting this published by a publishing company? For example, the book I'm currently reading on abstract algebra is published by orthogonal publishing: http://orthogonalpublishing.com/ They seem interested in publishing high quality free books.
Purchased, great work!
Amazing stuff! I wish the monome was more affordable. :(
It's just how blurb display it, I guess. I made the book, but it says Bartosz is the author, so I guess that's good!
Is Bartosz getting paid for this print run? I hope so.
In the second example `One` is not a keyword, it is a data constructor for the `Two` type (and yes, unrelated to the type `One` presented just prior).
First book i ever order
Yeah. This is kind of what I said yeah? Or am i missing something?
I also want to know the answer to this.
Just bought one! Also... This link should get someone 20% off theirs, too: [https://share.blurb.com/x/pHX5d2](https://share.blurb.com/x/pHX5d2). &amp;#x200B;
You called `One` a keyword. The term `keyword` means something quite particular in the context of programming languages - I'm just being pedantic.
Does the original author know you are doing this and paying for it? Was this commissioned buy the author?
Yes yes yes thank you, this is exactly the kind of comment I was hoping would come! Going to look at both of the libraries today and update the post! I'm going to link to this comment of yours if you don't mind! Felt bad when I was writing that section because I knew there had to be *something* out there I could recommend to people but I was too lazy to go search.
This didn‚Äôt work for me, instead it offered me $20 to use on making a book of my own.
Ah true. Thanks for pointing that out.
Much more readable! Though you lose some location information by summing the call sites, I'm not sure you often want/need that detail.
This isn't a Haskell role?
Super cool. When's your first performance video? :)
Sure! I got Bartosz' permission to publish the book. He also contributed a whole bunch of fixes to the PDF!
No, unfortunately all the money is going to blurb - the price is a bit high because it's a full color print. This is the lowest I could find from checking with different print-on-demand services. The one at Lulu would have cost more than $75 for a color print! Blurb was the lowest I could find as they offer an economy color print option, which is suitable for textbooks.
Cool post! I found it really useful and valuable :+1: You cover a lot of different things: from configuration parsing to monadic stack and overall architectures design. I have a couple of comments, since you've requested for the feedback :) &gt; There‚Äôs a servant template supported by stack-templates, but I am more comfortable using no template and going through things in a bit more of a manual fashion Shameless plug: if you prefer more doing things manually you can try to use `summoner` tool to create projects in interactive way with cool and fancy features. Saves a lot of keystrokes and time for me usually. * https://github.com/kowainik/summoner I see that you're using Partial Options monoid pattern and Higher Kinded Data pattern to distinguish phases of configuration. I can share our solution in `summoner` to this problem which involves `DataKinds` and `TypeFamilies` to remove some boilerplate and get rid of `Identity`: * https://github.com/kowainik/summoner/blob/master/src/Summoner/Config.hs I see that you're using TOML format for the configuration and I really like your choice! I'm working on bidirectional TOML parsing library, you may find it interesting as well: * https://github.com/kowainik/tomland/blob/30358695304f95e073a1306107b8c0432a18fb48/examples/Pl
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [kowainik/summoner/.../**Config.hs** (master ‚Üí 57fa18b)](https://github.com/kowainik/summoner/blob/57fa18b0188737c76e13511fcd5053d4191396be/src/Summoner/Config.hs) ---- 
I guess you could just reupload and print the PDF from Github... u/hmemcpy could even tell you exactly which version they used.
Maybe this year...
Sure, everything is on github on the releases tab (including the print version, the cover embeds the git commit of the printed edition).
I especially like you try to mention new things appearing in the code and also include full context instead of showing a part of snippets. As a beginner, it is frustrating when new keywords show up especially the extensions. Of course as a beginner, I don't expect to understand everything but I would like to know why it is there. One minor thing is that I wish there were type annotations for every functions including the ones defined in `where`. Sometimes it is not clear what the type is. Anyway, thank you for an awesome post. Invaluable information for beginners like me. I just wanted to say this. OH and (&gt;&gt;=) over do syntax is growing on me. 
&gt; I especially like you try to mention new things appearing in the code and also include full context instead of showing a part of snippets. I did that very much on purpose, thanks for noticing -- I have the *same* exactly problem when I read other people's posts! Sometimes I have absolutely *no* idea how someone made a leap. I actually felt like I skipped a bunch of explanation in this article, but the separation of simple/straightforward approaches and complicated (extension/advanced type heavy) approaches hopefully helped! I will go back and definitely add type annotations for all those functions! I will try to get back to it by tonight or maybe tomorrow and will edit this comment when I do!
&gt; The last Github blob link isn't working, could you post another? Yes, sorry for that. I've fixed the link. &gt; I don't think I understand what's going on in there well enough Feel free to ask any questions! But I will try to explain general ideas here. Instead of parametrising over type variable `f :: Type -&gt; Type` our `Config` is parametrised over `p :: Phase` where `Phase` is a simple enum: data Phase = Partial | Final It's like _a tag_ or just closed set of types. The `DataKinds` extension allows to lift constructors to type-level, so `Final` is a value that has `Phase` type but `'Filan` is a type that has `Phase` type (or `Phase` kind if you prefer). Later we use `:-` type family to map those tags to types. So `:-` is a type family ‚Äî function from types to types in ‚Äî in a form of operator (because operators look cooler). This function takes type of type `Phase` and arbitrary type and pattern matchers on `Phase` to decide what to return: infixl 3 :- type family phase :- field where 'Partial :- field = Last field 'Final :- field = field So in case or partial config we wrap selected fields into `Last` monoid (to be able to `mappend` configs easily) and return just field for `Final` configuration. I'm and an advocate of explicit type signatures, so this code can be slightly clearer if written like this: type family (phase :: Phase) :- (field :: Type) :: Type where Somehow the type signature for type family is missing... But this should be done and this can be a useful addition to the existing code! 
Thanks for this explanation! I will try to digest it and see if I can expand on the code in the post -- I haven't done any type family stuff recently (the first time I saw it was in `servant` actually), and though I've watched a bunch of stuff about the progress (the addition of `Type`, approaches to dependent typing in haskell, etc), I haven't gotten to the point of actually using it in my own code. Thanks very much, I think I've understood your explanation but I will go over it more carefully tonight. The ultimate test will writing something similar :)
In most cases you don't really need to use type-level tricks to write programs and be productive. So you can live happily without Type Families for a long time :) But sometimes they can be useful and they allow to explore wider design space.
Purchased, thank you! Using the coupon code ‚ÄúQUICK25‚Äù gave me a 25% discount.
You've always been able to have multiple versions of blah installed. The restriction in a single package database was that if foo-1.0 depends on blah &gt;= 1.2 &amp;&amp; &lt;1.5, you couldn't have two copies of foo-1.0 installed, one using blah 1.2 and one using blah 1.4. This in turn becomes a problem if your packages requiring blah 1.2.\* and 1.4.\* respectively both also require foo-1.0. &amp;#x200B; Cabal sandboxes and cabal new-\* solve it properly (the former with private package databases and the latter with full hashing of dependencies to store packages in a single database); I'm not quite sure what stack does to solve it but presumably it sets up different package databases for each snapshot which is similar to the sandbox solution but less flexible.
I don't think this is a bug (except perhaps in tooling), it's more likely to be because the compiler/linker just includes all code from libraries even if it is not actually called from my code and the more recent libraries contain more functionality. It's not (yet) a problem but this runs in a VM with 2Gb and 2-4 instances of this process, so I would like to avoid binary bloat if possible. 
How approachable is this for a self taught programmer with minimal (read high school) math knowledge.
Do they ship to europe?
It's a bit hard to take your argument seriously if you don't explain why _your_ definition of functional is? For example, if a language had nulls, runtime exceptions, no ADTs or GADTs, no match-destructuring, no Maybe/Option, no Either/Result, no type classes, no type-families/fun-deps, _but it did have_ purity &amp; inescapable immutability it _would_ be functional? Could it be functional without HKTs? What is your definition?
&gt; many languages used in industry (if not acedemia) did not support first-class functions. I still write C C supports first-class functions, has done since the very beginning. Look at the definition of `qsort` in `&lt;stdlib.sh&gt;`, its third parameter is a function-pointer. C did not, and still does not have currying of course, which significantly the utility of passing functions around.
I just want to say that you did an awesome job with the PDF. It looks better than all technical books from professional publishers that I have on my e-reader.
Amazing, ordered one :) I've really liked Bartosz's youtube series but have only read his CT blog entries superficially. 
I'm going to grab this and mess about: https://itunes.apple.com/ca/app/touchosc/id288120394#?platform=ipad Thanks for the pointer.
It's well explained, I can follow the book without a strong math background. Also, the original content is public available online on the author's blog &amp; youtube, check it out before buying it!
For those who were (like myself) concerned about Bartosz's involvement, it's endorsed by the man himself: https://twitter.com/BartoszMilewski/status/1054272962889568256
Is this an inappropriate question for this sub? Too elementary?
...or poorly presented?
[https://en.wikipedia.org/wiki/Prefix\_sum#Parallel\_algorithm](https://en.wikipedia.org/wiki/Prefix_sum#Parallel_algorithm)
How does such an arrangement differ from that governing the print-on-demand versions currently available? 
I know I'm likely to get downvoted for this but I feel some duty of care here: I've used Yesod, Servant, and Snap in production so far. Yesod's the only framework that has handled exceptions correctly.
Want to learn more about the basics of functional programming? &amp;#x200B; Our 2-day course is aimed at industry participants with a working knowledge of programming in some (not necessarily functional) programming language (Java, C#, Python, ‚Ä¶) who want to get a better understanding of functional programming. &amp;#x200B; The course takes place on February 7-8, 2019 at the KU Leuven Department of Computer Science and is taught by Tom Schrijvers, professor of Functional Programming.
How is your server configured? You can look [in here for configuration settings](https://hackage.haskell.org/package/snap-server-1.1.0.0/docs/Snap-Internal-Http-Server-Config.html) -- by default logging is to [log/access.log and log/error.log](https://hackage.haskell.org/package/snap-server-1.1.0.0/docs/src/Snap-Internal-Http-Server-Config.html#defaultConfig), but you can change it to whatever you want -- I usually just do an IO log to std error.
&gt; function-pointer Not a function. Also, functions are second-class because they cannot the created at runtime -- no lambda form equivalent, even a limited one. They also can't be passed or returned -- function pointers can, but they are distinguished in the C standard. C is not, nor has ever had first-class functions. `qsort` and `bserach` though, are C's attempt at higher-order functions, and serve as mild examples of how to do higher-order programming in limited languages. C++11 lambda forms get very close to first-class functions.
Maybe. I'm unclear how your question deals with **Haskell**, so it feels off-topic to me.
Wow, you did a great job gathering feedback from other people and putting it into your blog post! First time I see such amazing update after feedback :heart: And thanks a lot for mentioning `summoner`!
Yes, it logs to these files, but the text decoding errors did not show up in there. This might be because it was inside `snap-websocket`, maybe?
You may find a more receptive audience here if you were to post some haskell code of what you're describing in words.
&gt; the central unit of computation is the **function**, whose outputs are uniquely determined by its inputs I think you mean "completely determined"
I work in exactly this field! I agree that the literature remains very embryonic, because the field is small and has so far not delivered on its promises (when was the last time *you* ran a parallel functional program that was actually fast?). The most accessible literature probably remains the writings by Guy Blelloch, which defined/popularised the concepts that Acar is also using today. It's unfortunate that no real foundation of textbooks and tutorials have grown up yet, and it will probably take a while before it arises (although two of my colleagues are working on it, as are of course the people at CMU).
Ya, that sounds like a good possibility. I haven't used websockets recently, but there is [some discussion on Github that sounds related](https://github.com/jaspervdj/websockets-snap/issues/9), but it's super old, so who knows.
Nobody mentionned Atom yet, so there you go :) [https://atom.io/](https://atom.io/)
Did you raise any issues for Snap or Servant to that effect?
They'd already been raised and in both cases the developers refused to fix it, believing the behavior was correct. I don't do holy wars any more.
Well, Dragon Book (a. k. a. Aho, Ullman ‚ÄúCompilers...‚Äù) to the rescue!
&gt; Firstly, my supervisor said "ML type checker" I assume he means Standard ML ML is a family of languages. The most popular languages of that family are Standard ML, OCaml and F#. It is possible that your supervisor meant SML (or possibly OCaml) or he could mean your own simplified dialect and/or a dialect that introduces non-standard features that you and your supervisor talked about to make things more interesting. Or maybe that's for you to decide. You should really clarify that with your supervisor. &gt; I've read a lot about Haskell parsing including parsec but also parser generators such as Happy - I'm not sure what approach to use That's a really subjective thing. Just try both and see which one you like better. I feel like most people find parser combinators easier to use, but again, that's subjective. Either way works fine - there's no right or wrong here. &gt; I've only found small examples of calculator type grammars or JSON. Did you understand those examples? I feel like if you understand how to parse JSON and arithmetic expressions, you should at least have a good idea of where to start parsing something like ML. I don't think it'd be a good idea to use more expansive/complicated examples in tutorials - it'll just make the concepts harder to understand. &gt; Any help would be appreciated if anyone could point me in the direction of how I should get started with this I would recommend to start with the type checker (which I understand is the main point of the project, right?) and worry about the parser later. So at first you should read up on how the Hindley-Milner type system works (which if you're already familiar with Haskell, you might know already know) and Algorithm W (the algorithm that's commonly used to typecheck Hindley-Milner type systems). Then I'd start playing around with code to implement it. To test that code you can call it with hand-created ASTs directly, so you don't need a parser. You can even go the test-driven route and write those tests before you write the actual type checker. And when that all works would I bother with the parser. But that's just a suggestion.
u/dllthomas
I do like the notion of putting tasks in the repo, and also the word "toodles"!
https://ruslanspivak.com/lsbasi-part1 Cycle all the way to part 14. Of course, Dragon book is always the base, as mentioned
Looking juicy, can't wait to get my hands on one of these! I'll have to wait until November 6th though ::::]
This is good advice. ["Write You a Haskell" by Stephen Diehl](http://dev.stephendiehl.com/fun/) might also be helpful.
Can anyone explain what the potential benefits of using linear types are? 
Your description of how `scan` works is not quite accurate. For a good diagram and (and `scan`'s implementation), you can see the second page of [this](https://www.cs.cmu.edu/afs/cs/academic/class/15210-s18/www-s18/recitations/scan-reloaded.pdf) document from CMU.
I installed classy prelude, went into ghci and typed `:i zip`: ClassyPrelude&gt; :i zip class Functor f =&gt; chunked-data-0.3.1:Data.ChunkedZip.Zip (f :: * -&gt; *) where ... zip :: f a -&gt; f b -&gt; f (a, b) ... -- Defined in `chunked-data-0.3.1:Data.ChunkedZip' ClassyPrelude clearly reexports some functions from chunked-data, but **why the hell** does ghci show me this information? I have two problems with this: * The type definition is too damn long and noisy. * I cannot type `:i Zip` because it is not part of ClassyPrelude. I also cannot `:m Data.Chunked`!
[Read the motivation section here](https://github.com/tweag/ghc-proposals/blob/linear-types2/proposals/0000-linear-types.rst#motivation)
To state the obvious, you can either add `chunked-data` to your dependencies or maybe better yet not use any alternate prelude to avoid running into these kinds of issues.
Congrats! Is it like http://hackage.haskell.org/package/lentil, but web UI instead of CLI ?
Here's a [presentation](https://www.youtube.com/watch?v=t0mhvd3-60Y) from SPJ. Towards the end, it also compares the linearity designs of Haskell and Rust.
WOOOOOHOOOOO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1
What?
My comment was made when the post had been up for several hours with no activity, when later posts had seen many. Thats mostly what i was referring to
Oh absolutely no problem -- it's the least I can do, after all you guys made the libraries! Tried to summarize what they did the best I could to give others an idea of what they did and link people back here for more info!
Because tabs are the whitespace of the devil, and using them shall damn your immortal soul for all eternity. Yea verily, be not seduced by their seeming convenience, for their ruse is cunning and wicked indeed, but each press of this sinistermost key brings you closer to perdition! Shun tabs, abjure the \011th deadly sin!
üéâüíØüéâüíØüéâüíØüéâüíØüéâüíØ GIVE üëè IT üëè UP üëè FOR üëè LINEAR üëè TYPES :pwarrrrrrrrrrrrrrrr air sirens:
holky shitt
Yeah this is my biggest problem with Rust. Lifetimes really really complicate the API. `AsRef` and family are somewhat ugly hacks to work around it imo. I trust they‚Äôll fine a good solution in time but it does make work tedious in the meantime.
And I see that maxInt is 2\^29, which is less than 10\^18; is there a 64bit Int, or do I need to go to Integer? And I now see that memo-izing all that repeated exponentiation will help a lot.
Hmm - I guess this is already a problem in Haskell. It becomes another dimension in designing your program/library which you might think about! 
The Haskell report mandates that `Int` must cover *at least* up to 2^29 -1. In practice, the Int type is often larger than that. If you know you need 64-bit integers , use `Data.Int.Int64` - or `Data.Word.Word64` if you need unsigned 64-bit integers.
Cool I assume this is based on linear logic since it assumes resources may only be used once? Makes me think of linear logic proofs. (I'm still learning types and programming languages so appreciate any details on bridging the Gap from linear logic to linear types).
&gt; I wonder how the typed hole functionality could make use of linearity information. https://twitter.com/edwinbrady/status/1054014121828589568
This will simplify the API for programming with mutable arrays to a great extent. And subsequently allow (hopefully) more efficient implementation of a number of high performance and numerical computing algorithms. So quite excited by this!!
Thank you so much for the kind words, it's a real honor to hear :)
Conal Elliott has quite a few [articles on parallel scans](http://conal.net/blog/tag/scan) on his blog. I recommend checking out [the slides following this link](http://conal.net/papers/generic-parallel-functional/) for some pretty pictures.
Thank a lot for less GC pauses, efficient arrays, less latency, less memory usage, more speed when needed, making Haskell a real alternative for system programming, checking more errors at compile time!!!
I would have posted a mildly negative reaction, because I have no use for linear types, find their applications dubious, and think they are an unwarranted addition of complexity - and I say that as someone who has implemented a compiler that supports uniqueness typing in its core language. However, since I have never contributed to GHC, I don't think it's my place to say what other people should or should not work on - I've certainly benefited plenty from their work, so I don't want to be a party pooper. But I won't be excited over it.
I know this is "just syntax", but I would like it if the name of the extension would be `-XLinearArrows` or `-XLinearFunctions` instead of `-XLinearTypes`. Just in case somebody extremely out-of-this world smart comes along which happens to be into Haskell and linear types and somehow figures out how to do linearity constraints/annotations on arbitrary types, instead of "just" the arrow type. Also, `-XLinearArrows` or `-XLinearFunctions` clearly highlights how this corner of the linear types design space differs from other linear type design avenues. I don't want to derail the discussion in the official proposal repo on the more important aspect of the proposal, the semantics, so I'm just posting it here on reddit instead.
Check out [tweag.io](https://www.tweag.io/blog)'s blog posts on the subject. They're excellent.
I wonder if a better syntax is possible. Rust is easily readable, this is‚Ä¶ barely.
That's really cool, but is linearity being used there in one of the examples?
\&gt; That's really cool, but is linearity being used there in one of the examples? [https://twitter.com/edwinbrady/status/1054025898800898049](https://twitter.com/edwinbrady/status/1054025898800898049) :)
anyone have a good air siren/vuvuzela for [TidalCycles](https://tidalcycles.org/)?
That's a better link!
See below. It's used to guide proof (implementation) search, but I have yet to see the practical value of this. IMO in practice, the search space seems to big for the compiler to find the right implementation.
Woohoo! Great work thanks. 
Can we get the same safety by using indxeed monads and monadic regions? 
I think so---so long as you're doing it in a monadic context. What you can't get is the sweet compiler optimizations.
So do you find uniqueness types more useful? They are the dual of linear types, right?
I've just been reading through the proposal and I don't understand why it's possible to define the multiplicity of the record fields at all? Should they not always have a multiplicity of 1 by the fact that when we define an argument to be consumed that we define a data type to be consumed when each of the fields are consumed exactly once?
Perhaps leave this comment on the issue?
 instance FromJSON MyType where parseJSON value = do result &lt;- genericParseJSON defaultOptions{ fieldLabelModifier = map toUpper . drop 1 } value yourFunction result Is this what you mean?
Bump
I find that uniqueness types *barely* carry their weight in a very focused (small and performance-oriented) language, where the significant complexity burden on the implementation and users is offset by making it possible to write more efficient code in some cases. Specifically, we use them to implement efficient array updates in a pure functional setting. I don't think uniqueness types are convincing in a more general-purpose language, like Clean. Sure, they are notationally more lightweight than monads, but monads do not need any fancy language features to work (except higher-kinded polymorphism, but those are *much* simpler than uniqueness types).
Sorry on mobile, no links. Lambdacast Co-recursive Reason town Functional geekery
Functional Geekery is somewhat established podcast on functional programming. Also just discovered Code Recursion which recently hosted Philip Wadler.
[http://www.magicreadalong.com/](http://www.magicreadalong.com/) is very casual in style deep in fp concepts
just looked at its website. its not a regular show huh?
Yes, but it's an uphill battle really. I doubt you can make your type system precise enough to make this actually a useful feature in real life.
You used an idiom! &gt; uphill battle A struggle against very unfavourable circumstances.
What‚Äôs unreadable about it?
I highly recommend my buddies Brandon Williams and Stephen Celis‚Äôs podcast [Point Free](https://www.pointfree.co). They use Swift but with a lot of inspiration coming from Haskell.
Lambda cast is very nice for beginners. 
Please comment on the issue about what makes it hard to read for you! Knowing what users like for syntax is an integral part of any proposal like this.
The best alternative I've seen so far, proposed by a commenter in the GitHub PR, is \`(:) :: a #p -&gt; \[a\] #q -&gt; \[a\]\`. But if you have better ideas, do chip in! I must say, pinning down an even half decent syntax has been a tough one for us.
&gt; why spaces over tabs? For much the same reasons you shouldn't use tabs in any language, but magnified in Haskell by its layout-sensitive syntax. Haskell [language report](https://www.haskell.org/onlinereport/haskell2010/haskellch10.html#x17-17800010.3) states that &gt; Tab stops are 8 characters apart. &gt; A tab character causes the insertion of enough spaces to align the current position with the next tab stop. If you mix tabs with spaces, it may easily happen that your code looks aligned on screen but the compiler rejects it. In other languages it's only other developers you're going to annoy with your tab characters. 
Possibly, just don't have a new syntax at all? Like treat it as a magic new type operator `:: Type -&gt; Type -&gt; LinearityThing -&gt; Type` As in `(:) :: (a -+ [a]) p -&gt; [a]`
What about treating the linear arrow as just a new type operator, without a special syntax? As in `(:) :: (a -+ ([a] -+ [a]) q) p`
Looks very hard to read to me. In your example, the last (i.e. outermost) multiplicity pertains to the first arrow, while the first multiplicity pertains to the second arrow. That should get pretty confusing by the time you're trying to give a type to `foldr`!
As a C++ programmer looking to get started with functional programming this is exactly what I was looking for. Just placed my order.
You're into extension territory anyway, I half wonder if you might as well just use unicode syntax: f :: A -&gt;‚ÇÅ B
 Prelude&gt; (error "1" + error "2") :: Int *** Exception: 1 CallStack (from HasCallStack): error, called at &lt;interactive&gt;:7:2 in interactive:Ghci1 Prelude&gt; (error "1" + error "2") :: Integer *** Exception: 2 CallStack (from HasCallStack): error, called at &lt;interactive&gt;:8:14 in interactive:Ghci1 &amp;#x200B;
What is the purpose of the `vacuous` function in `Data.Void`? Its documentation says &gt; If Void is uninhabited then any Functor that holds only values of type Void is holding no values. However, length (vacuous [undefined :: Void]) == 1 Shouldn't that be of length 0? What's a typical use of `vacuous`?
I'm using Template Haskell as part of Yesod, and have {-# LANGUAGE TemplateHaskell #-} enabled in my header (as part of the suite of extensions), but using it produces an error which recommends I use Template Haskell. https://gist.github.com/tavrinky/a4576edf8b15d299fbe304f5346d2a88 with the error of https://gist.github.com/tavrinky/eaf7c23e3f8df66d8839c85e7be1e883
wow
I thought LANGUAGE pragmas had to be before the `module` line.
So, if not general TC elimination / optimization, then trampolining (not always an optimization)?
&gt; I guess this falls down with variables, as you can't use unicode to subscript arbitrary words (can you?) [Apparently you can](https://en.wikipedia.org/wiki/Unicode_subscripts_and_superscripts#Latin_and_Greek_tables), as long as your identifier contains only numbers and the letters ‚Çê ‚Çë ‚Çï ·µ¢ ‚±º ‚Çñ ‚Çó ‚Çò ‚Çô ‚Çí ‚Çö ·µ£ ‚Çõ ‚Çú ·µ§ ·µ• ‚Çì, ·µ¶ ·µß ·µ® ·µ© ·µ™ and ‚Çî... No q :(
I find that awesome too, though the proposal mentions: &gt; The keyword in the above examples is safety. This proposal is not about improving the performance of the compiler's generated code. It is not about new runtime support. It is about enabling programmers to build safer API's that enforce stronger properties, thereby bringing possible but otherwise high-risk optimization techniques, like managing memory manually, into the realm of the feasible. Is performance merely not the focus of the initial proposal, or are there any fundamental problems (in particular, with respect to implementing semantically pure functions with mutation under the hood. I vaguely remember someone here mentioning that there is some gotcha with mutation)? 
`vacuous` is `fmap absurd` where `absurd :: Void -&gt; a`. You can define `absurd` by case analysis: `Void` has zero cases. absurd :: Void -&gt; a absurd v = case v of {} So we can explain the length as follows: `length (vacuous [undefined]) = length (fmap absurd [undefined]) = length [absurd undefined] = 1`. You can think of `f Void` as a type of containers, and `vacuous` allows you to coerce those containers to any other element type. Alternatively, it is the conversion from `f Void` to `forall a. f a`, both of which represent empty containers; the latter is generally more flexible, but there are a few occasions where the former being monomorphic makes it more convenient (you will have trouble trying to store `forall a. f a` in a list).
Please no. Or at least have some 7-bit ASCII safe syntax as well.
I believe once this gets in, there's already a "mutable array" library waiting in the wings. Also, IIRC, one of the problems is that even if a function is linear (or affine) in an argument, that's not enough to let it mutate it in place, since it might already be shared. You'd still have to make a mutable copy, and if you make those in the wrong places, you get worse performance not better. (In particular, an affine function might not make sure of the copy, if you go down a "drop" path.) There's nothing in the conditions are explicitly prevents the proposal from improving performance, it's mainly a list of things that *must not break* in its "final form".
Semantically, both of the results are supposed to be identical (bottom). Any way you distinguish them would be outside of the denotational semantics of haskell; they are meant be indistinguishable, and any distinguishability would be from implementation/IO.
&gt; I doubt you can make your type system precise enough to make this actually a useful feature in real life. IME with Idris and Agda, this is *probably* the case. Term inference is a nice trick, but it tends to fail apart for functions with much "real" logic. I still believe in TDD (type driven development), but more for checking the programmer rather than generating whole programs. (I was pleasantly surprised that Idris was able to give me the whole implementation for `zipWith : (a -&gt; b -&gt; c) -&gt; Vect n a -&gt; Vect m b -&gt; Vect (min n m) c`, but I honestly think that might have been near the limit.)
&gt; indxeed monads IS the approach for nice syntax for these still just `RebindableSyntax` or is there something better?
A fun alternative could be to surprise your supervisor and implement a [compositional type checker](https://gergo.erdi.hu/talks/2016-06-compty/CompTy.pdf) for SML.
This is definitely not Snap's fault. This can be seen by adding the following handler: `("foo", writeText $ decodeUtf8 "\xc3")` ...and then hitting the `/foo` endpoint. When you do that you get the following in `error.log`: ``` [23/Oct/2018:20:50:29 -0400] Exception leaked to httpSession during phase 'sending-response': During processing of request from 127.0.0.1:50818 request: "GET /foo HTTP/1.1\naccept: */*\nuser-agent: curl/7.58.0\nhost: localhost:8000\n\nsn=\"localhost:8000\" c=127.0.0.1:50818 s=127.0.0.1:8000 ctx=/ clen=n/a" A web handler threw an exception. Details: Cannot decode byte '\xc3': Data.Text.Internal.Encoding.decodeUtf8: Invalid UTF-8 stream ``` You are seeing this behavior because the websockets library is catching exceptions so the websocket thread will not die when it receives a bad message.
I think /u/sepp2k has the best answer. I would just like to mention [Modern Compiler Implementation in ML](https://www.cs.princeton.edu/~appel/modern/ml/) as an alternative to the "Dragon Book". You can specifically refer to the chapters on abstract syntax trees and type checking. It is done in SML, which is similar enough to Haskell. You can see a functional approach to compilers / type checkers / etc.
I had guessed that was dead. Looking around it seems like Chris Forno hasn't done anything publicly Haskell related since last year. Maybe /u/AlpMestan can weigh in on the status? 
To work around the warning issue you can create your own version of `trace` that doesn't emit a warning and re-export `protolude` (or `relude`) hiding the warning-laden version.
haskell is painful learn python ans haskell at the same time
I enjoy the channel of Tsoding on youtube: https://www.youtube.com/channel/UCEbYhDd6c6vngsF5PQpFVWg
This is one of the best features of Haskell. I'm completely serious. Getting rid of the concept of evaluation order is how we approach declarative programming - saying *what* to do, not *how* to do it.
Tangential curiousity: Is there any legal instance of `fmap` that isn't linear in its second argument?
When it comes to pure ADTs, FunLists is somewhere at the top. data FunList a b t = Done t | More a (FunList a b (b -&gt; t)) The thing about FunLists is that they uniquely represent a Traversable. You can make an applicative instance for FunLists, as well as these two functions: runFunList :: Applicative f =&gt; (a -&gt; f b) -&gt; FunList a b t -&gt; f t runFunList _ (Done b) = pure b runFunList f (More s r) = f s &lt;**&gt; runFunList f r makeFunList :: Traversable t =&gt; t a -&gt; FunList a b (t b) makeFunList t = traverse (\a -&gt; More a (Done id)) t The Applicative instance is pretty wacky, too: instance Applicative (FunList a b) where pure = Done Done f &lt;*&gt; fa = fmap f fa More s r &lt;*&gt; fa = More s (flip &lt;$&gt; r &lt;*&gt; fa) When a ~ b, FunList is even a Comonad: instance (a ~ b) =&gt; Comonad (FunList a b) where extract (Done a) = a extract (More s r) = extract r s extend f (Done a) = Done (f (Done a)) extend f (More s r) = More s (extend (\r' s' -&gt; f (More s' r')) r) 
The [Tardis monad](http://hackage.haskell.org/package/tardis-0.4.1.0/docs/Control-Monad-Tardis.html). Don't ask me how it works, it's magic.
Because floating point values lose precision with nearly every operation. 
I believe this is the same as: import Control.Applicative.Free data Mono x y a where Mono :: x -&gt; Mono x y y type FunList a b = Ap (Mono a b) Which I used to do sorting of arbitrary traversables: https://elvishjerricco.github.io/2017/03/23/applicative-sorting.html
Ghc already supports curly braces and semicolons as alternative to indents.
Of course, since you even can FFI to assembler right now without linear types.
Not the most exotic one, but definitely interesting: data Stream a = a :&gt; Stream a You can do a lot of interesting things with infinite streams. And the possibility of defining such data type is amazing by itself.
Well, FFI has nothing to do with it. I just meant that the only difference between our existing mutable collections and linear variants are that you know you're the only owner of a linear collection. Performance-wise, they will be utterly identical.
Figured it out. It is corecursive. 
I have moved back to standard Prelude. The type errors were too nasty D:
What does that operator mean?
Lately I've been going back to solve projecteuler.net problems in Haskell, and find myself constantly using /u/edwardkmett's natural number tree: https://stackoverflow.com/a/3209189/904219 data Tree a = Tree (Tree a) a (Tree a) deriving Functor index :: Tree a -&gt; Integer -&gt; a index (Tree _ m _) 0 = m index (Tree l _ r) n = case (n - 1) `divMod` 2 of (q, 0) -&gt; index l q (q, 1) -&gt; index r q nats :: Tree Integer nats = go 0 1 where go !n !s = Tree (go l s') n (go r s') where l = n + s r = l + s s' = s * 2 memoize :: ((Integer -&gt; a) -&gt; Integer -&gt; a) -&gt; Integer -&gt; a memoize f = fix $ \g -&gt; index (fmap (f g) nats) Basically it lets you easily memoize any function of natural numbers. For any openly recursive function `f :: (Integer -&gt; a) -&gt; Integer -&gt; a`, you can simply map it over `nats` to get a memoized version. You can also define a lens that lets you kind of treat it like a bad IntMap: atNat :: Integer -&gt; Lens' (Tree a) a atNat 0 f (Tree l a r) = fmap (\b -&gt; Tree l b r) $ f a atNat n f (Tree l a r) = case (n - 1) `divMod` 2 of (q, 0) -&gt; fmap (\l' -&gt; Tree l' a r) $ atNat q f l (q, 1) -&gt; fmap (\r' -&gt; Tree l a r') $ atNat q f r type IntMap a = Tree (Maybe a) lookup :: Integer -&gt; IntMap a -&gt; Maybe a lookup = view . atNat insert :: Integer -&gt; a -&gt; IntMap a -&gt; IntMap a insert i a = set (atNat i) (Just a) There's even an applicative instance you can use to zip two different solutions: instance Applicative Tree where pure a = let t = Tree t a t in t Tree tf1 f tf2 &lt;*&gt; Tree ta1 a ta2 = Tree (tf1 &lt;*&gt; ta1) (f a) (tf2 &lt;*&gt; ta2)
What about `Const`? instance Functor (Const m) where fmap _ (Const v) = Const v
In its *second* argument, not the first. The `Const v` is properly consumed in the result.
It's a constructor, so you're seeing the **definition** of that operator. As you probably know, constructors always start with a capital letter. And as you may know, operators are just "identifiers consisting of only operator symbols". Now, the missing puzzle piece that you probably don't know: You are allowed to define operator constructors. But which operator symbols are capital letters? We have (fairly arbitrarily) defined `:` to be the only capital letter operator symbol. So: Any operator starting with : **must** be a constructor. So the example you replied to is basically `data Steam a = Cons a (Stream a)` (so a list without `Nil`), except that `Cons` is replaced with the operator constructor `:&gt;`.
Don't want to be pessimistic about this, but I doubt you will find the right answer to what will suit you and your user cases universally without interacting with these languages. Both Haskell and Clojure are liked and designed by intelligent people. If you have a concrete use case and know what you prefer yourself, then it might be easier to choose. I think it is possible to get the same functionalities in both languages, but: 1. What will be the time and resources needed in each case? &lt;-&gt; 2. How safe/fast/etc will it has to be in each case? Both these questions can be answered only in pragmatic ways, so there is no universal truth to be followed until dealing wit the concrete problem. In no way am I criticising you for asking here, you will definitely might some useful feedback.
Try 'em both? Why don't you keep those two languages and use them where you feel they are the most efficient?
I think this is Russell's paper that was referenced in the podcast; it introduces the theory of types and Philip Wadler mentioned it as an example of a very readable paper: [Mathematical Logic as Based on the Theory of Types](https://fi.ort.edu.uy/innovaportal/file/20124/1/37-russell1905.pdf)
Right. Thanks! I briefly thought the `v` would have to be consumed too.
where can i find "official" documentation describing using : as capital letter in operator constructor?
In the Haskell report. 
 data Proxy t = Proxy instance Functor Proxy where fmap _ _ = Proxy
Could you please elaborate on what you mean by &gt; `FunLists` [...] uniquely represent a `Traversable` Do you mean that there is a single `Traversable` instance possible for `FunLists`, or something else?
A cool one I encountered recently was [snowleopard's algebraic graph representation](https://hackage.haskell.org/package/algebraic-graphs-0.2/docs/Algebra-Graph.html) (corresponding [paper](https://github.com/snowleopard/alga-paper/releases)): data Graph a = Empty | Vertex a | Overlay (Graph a) (Graph a) | Connect (Graph a) (Graph a) It's correct-by-construction in the sense that you can't declare edges to points that don't exist on your graph, and seems to work nicely with [formal verification](https://github.com/algebraic-graphs/agda), too.
Any answer I give is almost definitely going to be a [non-regular data type](https://www.cs.ox.ac.uk/richard.bird/online/BirdMeertens98Nested.pdf). One example is data Term v = Var v | App (Pair (Term v)) | Lam (Term (Incr v)) data Incr v = Zero | Succ v which models the lambda calculus with correct-by-construction de bruijn indexing. This is the same approach taken by [`bound`](http://hackage.haskell.org/package/bound).
Yes, long ago though; IIRC liquid haskell doesn't allow you to prove arbitrary theorems nor to manipulate and verify proofs, you just write programs normally in a restricted DSL where many invariants can be automatically proven (correct?).
In the Haskell report it says that: The constructor ‚Äú:‚Äù is reserved solely for list construction; like [], it is considered part of the language syntax, and cannot be hidden or redefined. It is a right-associative operator, with precedence level 5 (Section 4.4.2). Doesn't that mean that : is only used for list construction and not other constructor?
&gt; Faster ‚Äî Compiles to native code and no JVM overhead &gt; JVM ‚Äî Vastly more mature libraries. Stability and ubiquity vs &gt; ClojureScript ‚Äî Apparently much better than GHCJS? So, is your target platform the javascript or server backend?
It's in the lexical definition part of the Haskell Report. 
Yes : is only for lists, but : as a prefix has wider use.
Note that it says the constructor `:` is reserved. It doesn't say anything about other constructors that happen to start with `:` (which are completely legal, per the lexical spec). 
Something else. I mean that `FunLists` may be used to characterize the behaviour of an instance of `Traversable`. When you use `makeFunList`, what you're basically doing is partially applying `t a` to `traverse`, and the rest - the `forall f. Applicative f =&gt; (a -&gt; f b) -&gt; f (t b)` part - is preserved "in amber", which may then be recovered with `runFunList`. Indeed, a `FunList a b t` is in fact *isomorphic* to `forall f. Applicative f =&gt; (a -&gt; f b) -&gt; f t`. See also [Bazaar](https://hackage.haskell.org/package/lens-4.17/docs/Control-Lens-Internal-Bazaar.html#t:Bazaar), from `lens`, which is a FunList encoded in the `forall f. Applicative f =&gt; (a -&gt; f b) -&gt; f t` way. (Well, Bazaar is generalized over all profunctors, not just (-&gt;)). [Here's some extra reading if you're interested.](https://bartoszmilewski.com/2018/10/12/trading-funlists-at-a-bazaar-with-yoneda/)
For many here the appendices alone are a goldmine.
Yes \`(:)\` specifically is only list constructors, but \`(:&lt;)\`, \`(:::)\`, etc. can be a non-list constructors.
It's not the answer that either Chris or myself want to write, it's not the answer that the Haskell Cast listeners want to hear, it's also not very original, but here it is: we've both been quite busy and haven't really had the bandwidth to work on a new episode.
Balanced-by-construction 2-3 trees: data TwoThree a = Two a a | Three a a a data Tree a = Leaf a | Branch (Tree (TwoThree a)) You can implement an `O(log n)` comparison-based persistent insert/delete/query 'set' in it. Implementation not as elegant as type, but still quite a neat trick.
&gt; if our only thought was reducing keystrokes we would have gone with a Lisp, right? Lisps reduce the difficulty of parsing and manipulating ASTs, not the number of keystrokes. Compare `(+ (* 1 2) (* 3 4))` (19 characters) to `1 * 2 + 3 * 4` (13 characters) or even `1*2+3*4` (7 characters) for example.
&gt; I'm not even asking for bracket scoping here (but wouldn't mind it either in Haskell) We have that too! First, here's the block layout (whitespace-sensitive) version: -- | -- &gt;&gt;&gt; main -- hello -- 1 -- world main :: IO () main = do putStrLn "hello" do let x = 1 print x putStrLn "world" -- x is no longer in scope And here is the less-common semicolons-and-braces syntax: -- | -- &gt;&gt;&gt; main -- hello -- 1 -- world main :: IO () main = do { putStrLn "hello" ; do { let x = 1 ; print x } ; putStrLn "world" } Which I'm guessing still looks uncomfortable, since the semicolons are vertically-aligned and used as statement separators, not statement terminators. Turns out you can use them as statement terminators too: -- | -- &gt;&gt;&gt; main -- hello -- 1 -- world main :: IO () main = do { putStrLn "hello"; do { x &lt;- pure 1; print x; }; putStrLn "world"; } But I've never seen anyone use them as line terminators in Haskell, I'm surprised the syntax is even accepted.
That's really cool. What are some realworld uses for it?
This is a specific instance of a memoizing trie (for `[Bool]`), which can be generalized a bunch! (I think it *is* generalized in one of /u/edwardkmett's packages somewhere). You first take a representable functor: {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE TypeFamilyDependencies #-} {-# LANGUAGE DeriveFunctor #-} {-# LANGUAGE RankNTypes #-} import Control.Comonad.Cofree import Control.Comonad (extract) import Control.Lens (Lens') class Functor f =&gt; Representable f where type Rep f = r | r -&gt; f tabulate :: (Rep f -&gt; a) -&gt; f a index :: f a -&gt; Rep f -&gt; a And then `memoize` looks like this: memoize :: Representable f =&gt; (Rep f -&gt; a) -&gt; Rep f -&gt; a memoize = index . tabulate Then you stick in your instances: data Pair a = Pair a a deriving Functor instance Representable Pair where type Rep Pair = Bool tabulate f = Pair (f False) (f True) index (Pair x _) False = x index (Pair _ y) True = y instance Representable f =&gt; Representable (Cofree f) where type Rep (Cofree f) = [Rep f] tabulate = unfold (\f -&gt; (f [], tabulate (\x -&gt; f . (:) x))) index = flip (foldr f extract) where f x xs ys = xs (index (unwrap ys) x) And you get the memoizing function you want: memoizeBool :: ([Bool] -&gt; a) -&gt; [Bool] -&gt; a memoizeBool = memoize The lens can be generated as well, with a class kind of like `Ixed`: class Representable f =&gt; TotalIndexed f where index_ :: Rep f -&gt; Lens' (f a) a instance TotalIndexed Pair where index_ False f (Pair x y) = fmap (flip Pair y) (f x) index_ True f (Pair x y) = fmap (Pair x) (f y) instance TotalIndexed f =&gt; TotalIndexed (Cofree f) where index_ = foldr (\x xs -&gt; _unwrap . index_ x . xs) _extract
Job security.
[This](http://www.cs.bham.ac.uk/~drg/bll/steve.pdf) seems to be a nice and simple introduction to this topic.
I like the variant of this where every `More` constructor has an existential type variable so you can actually get heterogeneous functions. You can then parametrize it by a typeclass constructor and retain knowledge of the inputs to a function.
Here's a reformatted version (indented by 4 spaces): myAsm (Label l) = do lbls &lt;- getFuture (position, _) &lt;- getPast sendPast (Map.insert l position lbls) myAsm (Jump l) = do -- l may be in the future labels &lt;- getFuture (pos, insns) &lt;- getPast sendFuture (pos + 1, insns ++ [LowLevelJump (Map.lookup l labels)]) myAsm ... = ... doAssemble = runTardis (mapM myAsm ops) (0, []) Map.empty 
Incredible, thanks. My curiosity is now completely satisfied.
Vector spaces have a degenerate structure where tensor and par coincide. It was named in analogy more than anything else, with the hope of a good linear algebraic semantics to come.
This is actually something that is really interesting about Haskell! It has *imprecise exceptions* in pure code. There's an [accessible and easy to read paper](https://www.microsoft.com/en-us/research/wp-content/uploads/1999/05/except.pdf) that goes into detail on how they work and, more importantly, *why* the designers of Haskell chose to go with this approach.
The multiplicative disjunction of linear logic.
I'm a big fan of l√∂b: https://github.com/quchen/articles/blob/master/loeb-moeb.md &gt; Feeling smart? Let's change that, here's loeb: &gt; ``` &gt;loeb :: Functor f =&gt; f (f a -&gt; a) -&gt; f a &gt;loeb x = go &gt; where go = fmap ($ go) x &gt;``` It is the entire concept of Excel in a single function. Speaking purely data types, you can't go wrong with ZigZag: &gt; data ZigZag a b = ZNil | Zig a (ZigZag a b) | Zag b (ZigZag b a) 
Thank you for this. What I personally am concerned about is that the course the OP is taking is mine. We have lot's of teaching resources, slack channel, personal instruction sessions by the TA's and supervised group study sessions, all of which would IMO solve any of the posted issues a lot faster than reddit posting. 
I agree with your concern regarding the nomenclature. It is a potential source of confusion, if not a missed opportunity to be more precisely descriptive of what to expect from the extension. - E
Wait, I thought that fully homomorphic encryption was still an open problem. Has it really been solved?
`Traversable` is interesting. Its output is supposed to have the same shape as its input, so it seems somewhat reasonable that it *must* be linear.
‚Öã, the de Morgan dual of ‚äó.
Well that‚Äôs inaccurate, historically speaking, it was called linear logic before Seely et al really sorted that out. 
No need to put "official" in quotes.... we actually have real official standards documentation :)
You might get more responses if you mentioned a specific thing or example you don't understand :) We don't really know exactly what you get and what you don't, so it's hard to give useful advice.
How about a constraint based approach? (:) :: ( Mul (:-&gt;) p, Mul (:=&gt;) q) =&gt; a :-&gt; [a] :=&gt; [a] 
How about a constraint based approach? ``` (:) :: ( Mul (:-&gt;) p, Mul (:=&gt;) q) =&gt; a :-&gt; [a] :=&gt; [a] ```
How about a constraint based approach? ``` (:) :: ( Mul (:-&gt;) p, Mul (:=&gt;) q) =&gt; a :-&gt; [a] :=&gt; [a] ```
The adjective *linear* reflects on that in linear logic assumptions (facts or judgments) are considered as resources and used (consumed) in a linear fashion exactly once. For more details see Philip Wadler's paper, [A Taste of Linear Logic](https://homepages.inf.ed.ac.uk/wadler/papers/lineartaste/lineartaste-revised.pdf). From the abstract: "This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of ‚ÄòPropositions as Types‚Äô. "
The adjective *linear* reflects on that in linear logic assumptions (facts or judgments) are considered as resources and used (consumed) in a linear fashion exactly once. For more details see Philip Wadler's paper, [A Taste of Linear Logic](https://homepages.inf.ed.ac.uk/wadler/papers/lineartaste/lineartaste-revised.pdf). From the abstract: "This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of ‚ÄòPropositions as Types‚Äô. "
The adjective *linear* reflects on that in linear logic assumptions (facts or judgments) are considered as resources and used (consumed) in a linear fashion exactly once. For more details see Philip Wadler's paper, [A Taste of Linear Logic](https://homepages.inf.ed.ac.uk/wadler/papers/lineartaste/lineartaste-revised.pdf). From the abstract: "This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of ‚ÄòPropositions as Types‚Äô. "
The adjective *linear* reflects on how in linear logic assumptions (facts or judgments) are considered as resources and used in a linear fashion exactly once, kind of consumed. For more details see Philip Wadler's paper, [A Taste of Linear Logic](https://homepages.inf.ed.ac.uk/wadler/papers/lineartaste/lineartaste-revised.pdf). From the abstract: "This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of ‚ÄòPropositions as Types‚Äô. "
Ah, you beat me to it. I've been calling this type `Expression` or `Expr`, as [it's the type of lambda-expressions in a lexically-scoped interpreted language](http://semantic.org/post/how-to-write-an-interpreter-for-a-lambda-calculus-based-language/). data Expr a = ClosedExpr a | OpenExpr Name (Expr (Value -&gt; a)) Intuitively, it's "if you can give me these, I can give you this". For example, the lambda-expression `x (x y)` is "if you can give me `x` and `y`, I can give you `x (x y)`". In my current project to implement a strongly-typed interpreted language using a variation of Hindley-Milner, I actually use a generalisation of your `FunList` twice: for expressions, and also to do type unification (where it's "if you can give me a unification of types `a` and `b`, I can give you a unification of `List a` and `List b`"). Switching it around, you can use something similar to do pattern-matching in your language: data Pattern a = ClosedPattern (Value -&gt; Maybe a) | OpenPattern Name (Pattern (Value, a)) This says, "if you give me a value, I can maybe give you values corresponding to these names".
The adjective *linear* reflects on how in linear logic assumptions are considered as resources and used (kind of consumed) in a linear fashion exactly once. For more details see Philip Wadler's paper, [A Taste of Linear Logic](https://homepages.inf.ed.ac.uk/wadler/papers/lineartaste/lineartaste-revised.pdf). From the abstract: "This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of ‚ÄòPropositions as Types‚Äô."
Ah, you beat me to it. I've been calling this type `Expression` or `Expr`, as [it's the type of lambda-expressions in a lexically-scoped interpreted language](http://semantic.org/post/how-to-write-an-interpreter-for-a-lambda-calculus-based-language/). data Expr a = ClosedExpr a | OpenExpr Name (Expr (Value -&gt; a)) Intuitively, it's "if you can give me these, I can give you this". For example, the lambda-expression `x (x y)` is "if you can give me `x` and `y`, I can give you `x (x y)`". In my current project to implement a strongly-typed interpreted language using a variation of Hindley-Milner, I actually use a generalisation of your `FunList` twice: for expressions, and also to do type unification (where it's "if you can give me a unification of types `a` and `b`, I can give you a unification of `List a` and `List b`"). Switching it around, you can use something similar to do pattern-matching in your language: data Pattern a = ClosedPattern (Value -&gt; Maybe a) | OpenPattern Name (Pattern (Value, a)) This says, "if you give me a value, I can maybe give you values corresponding to these names".
Ah, you beat me to it. I've been calling this type `Expression` or `Expr`, as [it's the type of lambda-expressions in a lexically-scoped interpreted language](http://semantic.org/post/how-to-write-an-interpreter-for-a-lambda-calculus-based-language/). data Expr a = ClosedExpr a | OpenExpr Name (Expr (Value -&gt; a)) Intuitively, it's "if you can give me these, I can give you this". For example, the lambda-expression `x (x y)` is "if you can give me `x` and `y`, I can give you `x (x y)`". In my current project to implement a strongly-typed interpreted language using a variation of Hindley-Milner, I actually use a generalisation of your `FunList` twice: for expressions, and also to do type unification (where it's "if you can give me a unification of types `a` and `b`, I can give you a unification of `List a` and `List b`"). Switching it around, you can use something similar to do pattern-matching in your language: data Pattern a = ClosedPattern (Value -&gt; Maybe a) | OpenPattern Name (Pattern (Value, a)) This says, "if you give me a value, I can maybe give you values corresponding to these names".
Some people are doing machine learning on homomorphically encrypted data, today! For example this blog post is a very nice introduction: http://iamtrask.github.io/2017/03/17/safe-ai/ it goes about implementing one such neural network from scratch.
Some people are doing machine learning on homomorphically encrypted data, today! For example this blog post is a very nice introduction: http://iamtrask.github.io/2017/03/17/safe-ai/ it goes about implementing one such neural network from scratch.
The adjective *linear* reflects on how in linear logic assumptions are considered as resources and used (kind of consumed) in a linear fashion exactly once. For more details see Philip Wadler's paper, [A taste of linear logic](https://homepages.inf.ed.ac.uk/wadler/papers/lineartaste/lineartaste-revised.pdf). From the abstract: "This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of ‚ÄòPropositions as Types‚Äô."
Sure, this is true for their definition of linearity. I don't think this matches most people's expectations though. If I have a variable that the type system says is linear, I expect it to be consumed. For resource management, what about other resources like sockets? I'd prefer a solution that doesn't require special cases. 
Sure, this is true for their definition of linearity. I don't think this matches most people's expectations though. If I have a variable that the type system says is linear, I expect it to be consumed. For resource management, what about other resources like sockets? I'd prefer a solution that doesn't require special cases. 
Sure, this is true for their definition of linearity. I don't think this matches most people's expectations though. If I have a variable that the type system says is linear, I expect it to be consumed. For resource management, what about other resources like sockets? I'd prefer a solution that doesn't require special cases. 
For those who are looking for a PDF: https://github.com/hmemcpy/milewski-ctfp-pdf This is compiled via scraping Bartosz's website and versioned accordingly. (I have purchased the hardcover and have the PDF, as well.)
For those who are looking for a PDF: https://github.com/hmemcpy/milewski-ctfp-pdf This is compiled via scraping Bartosz's website and versioned accordingly. (I have purchased the hardcover and have the PDF, as well.)
For those who are looking for a PDF: https://github.com/hmemcpy/milewski-ctfp-pdf This is compiled via scraping Bartosz's website and versioned accordingly. (I have purchased the hardcover and have the PDF, as well.)
Sure, this is true for their definition of linearity. I don't think this matches most people's expectations though. If I have a variable that the type system says is linear, I expect it to be consumed. For resource management, what about other resources like sockets? I'd prefer a solution that doesn't require special cases. 
For those who are looking for a PDF: https://github.com/hmemcpy/milewski-ctfp-pdf This is compiled via scraping Bartosz's website and versioned accordingly. (I have purchased the hardcover and have the PDF, as well.)
For those who are looking for a PDF: https://github.com/hmemcpy/milewski-ctfp-pdf This is compiled via scraping Bartosz's website and versioned accordingly. (I have purchased the hardcover and have the PDF, as well.)
How is `ZigZag a b` any more useful than `[Either a b]`?
For those who are looking for a PDF: https://github.com/hmemcpy/milewski-ctfp-pdf This is compiled via scraping Bartosz's website and versioned accordingly. (I have purchased the hardcover and have the PDF, as well.)
For those who are looking for a PDF: [https://github.com/hmemcpy/milewski-ctfp-pdf](https://github.com/hmemcpy/milewski-ctfp-pdf) &amp;#x200B; This is compiled via scraping Bartosz's website and versioned accordingly. &amp;#x200B; (I have purchased the hardcover and have the PDF, as well.)
One of the most remarkable application of CPS is turning a non tail-recursive function into a tail-recursive one. Take this simple example, which is far from complete, but is a good starting point. Generally speaking in computer science you may want to get rid of recursion because whenever you make a recursive call you are allocating a record inside the stack of your program which stores information about what to do next. It's similar on how you would make any recursive call by pen and paper: you store information aside with partial computations and store in which order you make them. Unfortunately stack space is not usually big, and making recursive function calls is a serious concern. However, when your function is tail-recursive (i.e. the recursive call is the very last operation) this problem can be easily optimized by the compiler, or by the interpreter, but such optimization is not possible in the most general case. When you programming language supports functions as first class objects (such as functional programming languages as we intend today), instead of polluting the stack, you can use CPS to convert any non-tail-recursive function into a tail-recursive one, which gets optimized. The idea is to store "what to do next" as argument of the last call, this is what gets called "continuation". To get the idea right, take the standard factorial function ``` fact :: Int -&gt; Int fact 0 = 1 fact n = n * (fact $ n - 1) ``` The function `fact` is not tail-recursive, since the last operation is the multiplication. However you can do it differently by storing one additional parameter ``` fact_handle_tail :: Int -&gt; Int -&gt; Int fact_handle_tail 0 m = m fact_handle_tail n m = factor_handle_tail (n - 1) (n * m) fact_tail :: Int -&gt; Int fact_tail n = fact_handle n 1 ``` This is an easy way to turn the function `fact` into `fact_tail'` using `fact_handle`, which is tail-recursive. We can write `factor_handle_tail` in a slightly different way which brings CPS into the scene. What we store as `m` inside the arguments of `factor_handle_tail` can be interpreted as the "multiplication by m" function, which in lambda terms can be written as `\x -&gt; m * x`. In this way the function becomes ``` fact_handle_cps :: Integer -&gt; (Integer -&gt; Integer) -&gt; Integer fact_handle_cps 0 cont = cont 1 fact_handle_cps n cont = fact_handle_cps (n - 1) (\x -&gt; cont $ n * x) fact_cps :: Integer -&gt; Integer fact_cps n = fact_handle_cps n (\x -&gt; x) ``` This seemingly innocuous change opens the gate to a wide variety of examples which gives an overview of what CPS is about (but this is not the whole story..). For instance you can do the same for fibonacci ``` fib :: Integer -&gt; Integer fib n = if n &lt; 2 then n else (fib $ n - 1) + (fib $ n - 2) fib_handle_cps :: Integer -&gt; (Integer -&gt; Integer) -&gt; Integer fib_handle_cps 0 cont = cont 0 fib_handle_cps 1 cont = cont 1 fib_handle_cps n cont = fib_handle_cps (n - 1) (\x -&gt; fib_handle_cps (n - 2) (\y -&gt; cont (x + y))) fib_cps :: Integer -&gt; Integer fib_cps n = fib_handle_cps n (\x -&gt; x) ```
One of the most remarkable application of CPS is turning a non tail-recursive function into a tail-recursive one. Take this simple example, which is far from complete, but is a good starting point. Generally speaking in computer science you may want to get rid of recursion because whenever you make a recursive call you are allocating a record inside the stack of your program which stores information about what to do next. It's similar on how you would make any recursive call by pen and paper: you store information aside with partial computations and store in which order you make them. Unfortunately stack space is not usually big, and making recursive function calls is a serious concern. However, when your function is tail-recursive (i.e. the recursive call is the very last operation) this problem can be easily optimized by the compiler, or by the interpreter, but such optimization is not possible in the most general case. When you programming language supports functions as first class objects (such as functional programming languages as we intend today), instead of polluting the stack, you can use CPS to convert any non-tail-recursive function into a tail-recursive one, which gets optimized. The idea is to store "what to do next" as argument of the last call, this is what gets called "continuation". To get the idea right, take the standard factorial function ``` fact :: Int -&gt; Int fact 0 = 1 fact n = n * (fact $ n - 1) ``` The function `fact` is not tail-recursive, since the last operation is the multiplication. However you can do it differently by storing one additional parameter ``` fact_handle_tail :: Int -&gt; Int -&gt; Int fact_handle_tail 0 m = m fact_handle_tail n m = factor_handle_tail (n - 1) (n * m) fact_tail :: Int -&gt; Int fact_tail n = fact_handle n 1 ``` This is an easy way to turn the function `fact` into `fact_tail'` using `fact_handle`, which is tail-recursive. We can write `factor_handle_tail` in a slightly different way which brings CPS into the scene. What we store as `m` inside the arguments of `factor_handle_tail` can be interpreted as the "multiplication by m" function, which in lambda terms can be written as `\x -&gt; m * x`. In this way the function becomes ``` fact_handle_cps :: Integer -&gt; (Integer -&gt; Integer) -&gt; Integer fact_handle_cps 0 cont = cont 1 fact_handle_cps n cont = fact_handle_cps (n - 1) (\x -&gt; cont $ n * x) fact_cps :: Integer -&gt; Integer fact_cps n = fact_handle_cps n (\x -&gt; x) ``` This seemingly innocuous change opens the gate to a wide variety of examples which gives an overview of what CPS is about (but this is not the whole story..). For instance you can do the same for fibonacci ``` fib :: Integer -&gt; Integer fib n = if n &lt; 2 then n else (fib $ n - 1) + (fib $ n - 2) fib_handle_cps :: Integer -&gt; (Integer -&gt; Integer) -&gt; Integer fib_handle_cps 0 cont = cont 0 fib_handle_cps 1 cont = cont 1 fib_handle_cps n cont = fib_handle_cps (n - 1) (\x -&gt; fib_handle_cps (n - 2) (\y -&gt; cont (x + y))) fib_cps :: Integer -&gt; Integer fib_cps n = fib_handle_cps n (\x -&gt; x) ```
One of the most remarkable application of CPS is turning a non tail-recursive function into a tail-recursive one. Take this simple example, which is far from complete, but is a good starting point. Generally speaking in computer science you may want to get rid of recursion because whenever you make a recursive call you are allocating a record inside the stack of your program which stores information about what to do next. It's similar on how you would make any recursive call by pen and paper: you store information aside with partial computations and store in which order you make them. Unfortunately stack space is not usually big, and making recursive function calls is a serious concern. However, when your function is tail-recursive (i.e. the recursive call is the very last operation) this problem can be easily optimized by the compiler, or by the interpreter, but such optimization is not possible in the most general case. When you programming language supports functions as first class objects (such as functional programming languages as we intend today), instead of polluting the stack, you can use CPS to convert any non-tail-recursive function into a tail-recursive one, which gets optimized. The idea is to store "what to do next" as argument of the last call, this is what gets called "continuation". To get the idea right, take the standard factorial function ``` fact :: Integer -&gt; Integer fact 0 = 1 fact n = n * (fact $ n - 1) ``` The function `fact` is not tail-recursive, since the last operation is the multiplication. However you can do it differently by storing one additional parameter ``` fact_handle_tail :: Integer -&gt; Integer -&gt; Integer fact_handle_tail 0 m = m fact_handle_tail n m = factor_handle_tail (n - 1) (n * m) fact_tail :: Integer -&gt; Integer fact_tail n = fact_handle n 1 ``` This is an easy way to turn the function `fact` into `fact_tail'` using `fact_handle`, which is tail-recursive. We can write `factor_handle_tail` in a slightly different way which brings CPS into the scene. What we store as `m` inside the arguments of `factor_handle_tail` can be interpreted as the "multiplication by m" function, which in lambda terms can be written as `\x -&gt; m * x`. In this way the function becomes ``` fact_handle_cps :: Integer -&gt; (Integer -&gt; Integer) -&gt; Integer fact_handle_cps 0 cont = cont 1 fact_handle_cps n cont = fact_handle_cps (n - 1) (\x -&gt; cont $ n * x) fact_cps :: Integer -&gt; Integer fact_cps n = fact_handle_cps n (\x -&gt; x) ``` This seemingly innocuous change opens the gate to a wide variety of examples which gives an overview of what CPS is about (but this is not the whole story..). For instance you can do the same for fibonacci ``` fib :: Integer -&gt; Integer fib n = if n &lt; 2 then n else (fib $ n - 1) + (fib $ n - 2) fib_handle_cps :: Integer -&gt; (Integer -&gt; Integer) -&gt; Integer fib_handle_cps 0 cont = cont 0 fib_handle_cps 1 cont = cont 1 fib_handle_cps n cont = fib_handle_cps (n - 1) (\x -&gt; fib_handle_cps (n - 2) (\y -&gt; cont (x + y))) fib_cps :: Integer -&gt; Integer fib_cps n = fib_handle_cps n (\x -&gt; x) ```
Because the rendering of the text that you type with tabs is dependent on the program that the reader chooses to view the file with. Hence, the visual indentation of your code cannot be relied upon to match the actual indentation of your code in the event that you included spaces. It is much easier for you as an author to configure your editor to use a fixed number of spaces when you hit the tab key than it is to ensure all future readers of your code are using the same method to view it. In other languages, this is a minor matter that's usually seen as a matter of occasionally inconveniencing some people some of the time. In indentation sensitive languages such as Haskell, it makes desk-checking your code almost impossible in some contexts. Personally, I consider GHC accepting tab characters in source files at all to be a serious design flaw and I'd prefer that it just issue an unrecoverable parse error. 
Ah, you beat me to it. I've been calling this type `Expression` or `Expr`, as [it's the type of lambda-expressions in a lexically-scoped interpreted language](http://semantic.org/post/how-to-write-an-interpreter-for-a-lambda-calculus-based-language/). data Expr a = ClosedExpr a | OpenExpr Name (Expr (Value -&gt; a)) Intuitively, it's "if you can give me these, I can give you this". For example, the lambda-expression `x (x y)` is "if you can give me `x` and `y`, I can give you `x (x y)`". In my current project to implement a strongly-typed interpreted language using a variation of Hindley-Milner, I actually use a generalisation of your `FunList` twice: for expressions, and also to do type unification (where it's "if you can give me a unification of types `a` and `b`, I can give you a unification of `List a` and `List b`"). Switching it around, you can also use something similar to do pattern-matching in your language: data Pattern a = ClosedPattern (Value -&gt; Maybe a) | OpenPattern Name (Pattern (Value, a)) This says, "if you give me a value, I can maybe give you values corresponding to these names".
Oh, it's not *useful* by any means. It's just loads of fun to have people write type signatures out on paper for values you give them and *see them squirm*.
How's this? `a -&gt;{p} b -&gt;{q} c`
I don't know for sure, but I think the naming comes from some of Girard's earlier work, e.g. [https://ac.els-cdn.com/0168007288900255/1-s2.0-0168007288900255-main.pdf?\_tid=3bcceb6b-715d-43db-949f-75dbb55a7eb9&amp;acdnat=1540411856\_f7e77aff418213f735c4937b21f2290b](https://ac.els-cdn.com/0168007288900255/1-s2.0-0168007288900255-main.pdf?_tid=3bcceb6b-715d-43db-949f-75dbb55a7eb9&amp;acdnat=1540411856_f7e77aff418213f735c4937b21f2290b) &amp;#x200B; Here, he is studying certain functors of the form \`Set\^A\` where \`A\` is a finite set. These are sort of like an abstraction of vector spaces, where \`A\` takes the place of the basis vectors and \`Set\` takes the place of the field of scalars. In 2.19, he looks at the question "given sets \`A\` and \`B\`, can we find a set \`C\` such that \`Set\^C\` is isomorphic to the category \`Set\^A -&gt; Set\^B\`?" And he finds that yes, indeed you can; and in fact, the application map \`Set\^C x Set\^A -&gt; Set\^B\` is linear in a certain sense (you can decompose it into a bunch of things where there is a notion of coefficient and exponent, which both end up being 1). &amp;#x200B; This paper also has this interesting footnote to the introduction: &gt;Added in print. It appears now (October 1986) that the main interest of the paper is the general analogy with linear algebra. The analogy brought in sight new operations, new connectives, thus leading to 'linear logic'. The treatment of the sum of types (Appendix B) contains implicitly all the operations of linear logic. What has been found later is that the operations used here (e.g., linearization by means of 'tensor algebra') are of logical nature. &amp;#x200B;
I try to build my project using nix. I have a `default.nix` generated by the `cabal2nix` utility and a simple `project.nix` that builds my project. Now I want to create a `shell.nix` file as well where -apart from the project dependencies- it will include additional dependencies (e.g. `hlint`) which will help in the development process. How can I specify these extra dependencies inside `shell.nix`? 
We just don't have any implementations of the standard, anymore. ;) (GHC is, of course, close enough for any purpose I've needed or imagined, *except* teaching to the spec.)
Strong agree: we're using `algebraic-graphs` in production at my day job, and it‚Äôs been a really great experience, especially compared to the graph type that comes with `fgl`. `Graph` also has a particularly beautiful instance for `Num`: ``` instance Num a =&gt; Num (Graph a) where fromInteger = Vertex . fromInteger (+) = Overlay (*) = Connect signum = const Empty abs = id negate = id ``` which lets you build graphs with `*` and `+`.
I think compact regions are the thing that's most likely to make Haskell usable for stuff like game and physics engines. If you're careful with them, you can make your pause times pretty darn good. Dunno if this works with GHCJS or not
The $ operator is function itself: &gt; f $ x = f x If you have a function (\f -&gt; f x) you can reduce it to ($ x). In other words map ($ x) fs is equal to map (\f -&gt; f x) fs.
`$` is an infix function (or operator): [here on hackage](https://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#v:-36-) The type is basically `(a -&gt; b) -&gt; a -&gt; b`. It takes a function as a first argument, a value as a second argument and returns the result of applying the function to the value. In your example with `map`, what you're seeing is a [section](https://wiki.haskell.org/Section_of_an_infix_operator). It is an operator where you only apply one of the arguments to it (either the first or the second argument depending on which side is used). If it helps you could rewrite that same function with a lambda instead: `map (\f -&gt; f a)` Hope this helps! :-) PS: Sorry about possible formatting issues, I'm on my phone. 
You don't know you are the only owner of the collection unless the allocator is written in CPS like the array example. You can call linear functions on non-linear values.
Personally, I'm very interested to learn that algebraic graphs are used in production. If you're able to, could you please describe what are you using them for and which parts do you enjoy the most?
[removed]
Yep. I was referring to the CPS trick.
Many schemes have existed. RSA is actually homomorphic for some operations. However this was mitigated by introducing a padding scheme. Homomorphism isn't a desirable property in all cases. RSA is used for signatures and malleability of signatures is not a good property to have :)
This one is not even affine, though I'm cheating to make it a "legal" instance... but maybe someone can improve: {-# LANGUAGE GADTs #-} data FreeAnswer b where Fmap :: (a -&gt; b) -&gt; FreeAnswer a -&gt; FreeAnswer a -&gt; FreeAnswer b instance Functor FreeAnswer where fmap f fa = Fmap f fa fa instance Eq (FreeAnswer b) where _ == _ = True 
That ended up being exactly it, thanks!
I personally wouldn't consider that a legal instance. There is an observable difference between `fmap id` and `id`
This instance seems weird to me. \`\`\`(fromInteger 1) - (fromInteger 1) == (fromInteger 1) + (fromInteger 1) (fromInteger 1) + (fromInteger 1) /= (fromInteger 2)\`\`\` I could see it working a little better with \`negate = fmap negate\`, so \`(fromInteger 1) - (fromInteger 1) == (fromInteger 1) + (fromInteger (-1))\`. \`abs = fmap abs\` could work too, but now we're really mixing concepts. It just seems like a typeclass that doesn't fit. (Welcome to \`Num\`, I guess.) 
Something I've only played around with a little: data G f g h n k a where Node :: a -&gt; f k (G f g h n k a) -&gt; G f g h (Apply g n) (Apply h k) a where Apply is type family Apply (f :: *) (n :: Nat) :: Nat data SF; data IdF --Successor type instance Apply SF n = S n --Identity type instance Apply IdF n = n where G is a generic version of indexed cofree comonads and k-ary trees: type BinTree n a = G Vec SF IdF n (S (S Z)) a type KaryTree n k a = G Vec SF IdF n k a type GenKary f n k a = G f SF IdF n k a type ICofree f n a = G f SF SF n n a (Indexed cofree comonads are just indexed monads, but for the cofree comonad, so as below) data ICofree f n a where (:&gt;&gt;) :: a -&gt; f n (ICofree f n a) -&gt; ICofree f (S n) a 
We are just using GHCJS-DOM + PixiJS, so running the client via GHC is going to be fairly difficult. But thanks for the info!
Hmm ok, hopefully they respond. Do you know how to play around with the GC / disable it and run it manually?
Tangent: This is defined as `(??)` in `Control.Lens`, and it's also `traverse fmap` if you fix `f` to be `Identity`. You can write that relatively transparently as f fs = fmap runIdentity . traverse fmap fs . Identity It's possible to get that explicit `fs` out of there, but it's very ugly.
Does a constraint-based approach work? Something like: ``` (:) :: (Mul p (:-&gt;), Mul q (:=&gt;)) =&gt; a :-&gt; [a] :=&gt; [a] ```
Thanks for letting us know.
I feel that it would be more useful to see timings for problems in the Computer Benchmarks game or some other standard set of benchmarks.
Indeed! How about running some real program? Pick some simple programs from the nofib suite. 
Are you familiar with operator sections? (+ 3) is sugar for: \x -&gt; x + 3 You can imagine it as a function waiting for a value to fill in the "hole" on the left side: map (/ 3) [3,6,9] -- [1.0, 2.0, 3.0] map (3 /) [3,6,9] -- [1.0, 0.5, 0.33333333] So, remember that `($)` is for the most part not any special syntax...it's just a normal function: f $ x = f x So we can see how things expand: applyFuns fs v = map ($ v) fs is the same as applyFuns fs v = map (\f -&gt; f $ v) fs because of section sugar, and we can expand the definition of `$`, since `f $ x = f x`: applyFuns fs v = map (\f -&gt; f v) fs
I'm not an expert, but this could be a simplified version. {-# LANGUAGE QuantifiedConstraints, FlexibleContexts #-} {-# LANGUAGE MultiParamTypeClasses, FunctionalDependencies #-} class C x y where c :: x -&gt; y class D x y | x -&gt; y where d :: x -&gt; y -- These should type check good1 :: C (f Bool) Bool =&gt; f Bool -&gt; Bool good1 = c good2 :: D (f Bool) Bool =&gt; f Bool -&gt; Int good2 = const 1 . d {- -- This should not type check bad :: C (f Bool) Bool =&gt; f Bool -&gt; Int bad = const 1 . c -} -- This should type check good1QC :: (forall x. C (f x) x) =&gt; f Bool -&gt; Bool good1QC = c -- This is expected to type check, but doesn't. -- Rejected while reporting similar error message to badQC good2QC :: (forall x. D (f x) x) =&gt; f Bool -&gt; Int good2QC = const 1 . d {- -- This should not type check for same reason @bad@ doesn't. badQC :: (forall x. C (f x) x) =&gt; f Bool -&gt; Int badQC = const 1 . c -}
I more or less agree, although its worth noting there are some serious issues with many of the Computer Benchmarks game benchmarks. &amp;#x200B; One major issue is that an idiomatic Haskell implementation of the binary tree benchmark actually gets optimized out too effectively, and doesn't allocate the memory expected by the benchmark. &amp;#x200B; Another issue is that k-nucleotide requires you to use hashtables specifically, rather than just any general purpose map structure. Hashtables are more used and optimized in procedural languages, whereas Trie's and Tree's and others are more optimized in functional languages, so it artificially biases in favor of imperative languages.
The problem is that Formality doesn't support machine integers yet, which are needed for all of those. Meanwhile, I think it is worth measuring how it performs on things such as memory allocation, pattern-matching, high-order functions and so on (which are the things that consume most of your resources anyway!).
How do you implement tail recursive foldr with CPS? 
Why do you use lambda encoded data types in Haskell code? That's inefficient. 
hm, ok, I was thinking more like `+ 1 2 3` vs `1 + 2 + 3` but I see your point.
I'll ask in /r/haskellquestions next time, thanks. Hm, didn't know pointfree was a thing. Would've been nice if my book explained that. So by "just being passed through" you mean these arguments will always be passed through to the very end of the expression after the = correct?
The above pattern applied to the usual definition of `foldr` in a straightforward manner would give this GHCi, version 8.0.2: http://www.haskell.org/ghc/ :? for help Prelude&gt; :{ Prelude| myFoldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b Prelude| myFoldr f n xs = myFoldrCont f n xs (\x -&gt; x) Prelude| myFoldrCont :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; (b -&gt; b) -&gt; b Prelude| myFoldrCont f n [] cont = cont n Prelude| myFoldrCont f n (x:xs) cont = myFoldrCont f n xs (\y -&gt; cont $ f x y) Prelude| :} Prelude&gt; myFoldr (:) [] [1,2,3] [1,2,3] but you of course wouldn't usually want this definition for lists because of its bad lazyness/strictness properties. In general, for data structures that are not like lists, or when aiming for a strict right fold (which is not all that useful for lists either though) a CPS style definition like the above *can be reasonable* though. In fact, when we look closely, we can see, that the CPS style definition suddently has the structure of a left fold (i.e. traverses the list with an accumulator, the variable `cont`) - watch this: First step, swap parameters: myFoldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b myFoldr f n xs = myFoldrCont f xs (\x -&gt; x) n myFoldrCont :: (a -&gt; b -&gt; b) -&gt; [a] -&gt; (b -&gt; b) -&gt; (b -&gt; b) myFoldrCont f [] cont n = cont n myFoldrCont f (x:xs) cont n = myFoldrCont f xs (\y -&gt; cont $ f x y) n Second step, "eta-reduce" the `n`: myFoldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b myFoldr f n xs = myFoldrCont f xs (\x -&gt; x) n myFoldrCont :: (a -&gt; b -&gt; b) -&gt; [a] -&gt; (b -&gt; b) -&gt; (b -&gt; b) myFoldrCont f [] cont = cont myFoldrCont f (x:xs) cont = myFoldrCont f xs (\y -&gt; cont $ f x y) Third step, avoid the explicit propagation of `f`: myFoldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b myFoldr f n xs = myFoldrCont xs (\x -&gt; x) n where myFoldrCont :: [a] -&gt; (b -&gt; b) -&gt; (b -&gt; b) myFoldrCont [] cont = cont myFoldrCont (x:xs) cont = myFoldrCont xs (\y -&gt; cont $ f x y) Forth step, use `foldl` instead: myFoldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b myFoldr f n xs = myFoldrCont xs (\x -&gt; x) n where myFoldrCont :: [a] -&gt; (b -&gt; b) -&gt; (b -&gt; b) myFoldrCont xs cont = foldl (\cont x -&gt; (\y -&gt; cont $ f x y)) cont xs Fifth step, refactor: myFoldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b myFoldr f n xs = (foldl f' (\x -&gt; x) xs) n where f' cont x y = cont $ f x y and when you look it up, you can indeed find a very very similar definition in the `Foldable` type class for providing a [default implentation for a strict right fold](http://hackage.haskell.org/package/base-4.12.0.0/docs/src/Data.Foldable.html#foldr%27): foldr' :: (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b foldr' f z0 xs = foldl f' id xs z0 where f' k x z = k $! f x z A very similar, perhaps more popular and well-known construction (more popular since it's *actually useful* for lists) is used for implementing `foldl'` using `foldr`. Or if someone tells you *"hey, did you know you can implement* `foldl`*using only* `foldr`*?"*, what they mean is something like this.
Do you plan to add machine integer support to Formality lang?
&gt; One major issue is that an idiomatic Haskell implementation of the binary tree benchmark actually gets optimized out too effectively, and doesn't allocate the memory expected by the benchmark. Oh. That's why. Can I do anything to make [this](https://github.com/MaiaVictor/symmetric-interaction-calculus-benchmarks/blob/master/benchmarks/bintree.hs) code allocate what I'd expect it to? 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [MaiaVictor/symmetric-interaction-calculus-benchmarks/.../**bintree.hs** (master ‚Üí 40d73d3)](https://github.com/MaiaVictor/symmetric-interaction-calculus-benchmarks/blob/40d73d33acdc951821bf82f1d2652c66c9d20e2f/benchmarks/bintree.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e8f03u4.)
Well, hopefully it‚Äôll be fused away (I was careful to only use `foldr` and `unfoldr`). Still, though, it shouldn‚Äôt be much worse than the overhead of the tree, right?
Soon‚Ñ¢
Well that's because there is no optimization. It uses constant memory because the IO is consumed lazily similar to lists... 
&gt; It has long been understood that homomorphic evaluation of symmetric-key primitives like PRFs is a very useful tool in the theory and practice of FHE (see, e.g., [26, 28, 33]). For example, it allows a client to encrypt its data using symmetric-key encryption‚Äîwhich is much faster and more compact than FHE encryption‚Äîwhile still allowing a worker to compute on the data homomorphically. Can someone explain to a crypto layman like myself what exactly is alluded to here? If I have a FHE implementation of a symmetric cypher, it will still mean the other side only has homomorphic access to the result of the (homomorphic) symmetric decryption, right? So all operations will still need to happen in the homomorphic space?
It may have something to do with lineal equations since variables are used a single time if we consider that a^2 == a*a , non lineal equations are "used" more than one time.
This is the most satisfying explanation so far
I agree, it's a bit weird (although, formally it does meet the only law of `Num`). I wish we had a better type class hierarchy for number-like things in Prelude. Ideally, I'd like to have `FromInteger`, `Additive` with operator `+`, and `Additive =&gt; Multiplicative` with operator `*` and distributivity over `+`, all in separate classes.
Could you also benchmark the allocations? 
This is a useless benchmark. You use terrible data structures to represent completely standard things and then for some reason you don't use GHC's built-in list?
I have a lot of problems with the Computer Benchmarks Game but you're right: benchmarks are hard, and you need to actually solve a task and argue that it's representative of code users will write. This is not. 
The isomorphism only holds if you ignore infinite traversals, like the one we _started_ with, for []. Bazaar allows more programs to yield answers than FunList!
This. Please use native data structures promoted by the language. 
For sure, I'm not claiming that the Computer Benchmark games are a good representative set of benchmarks, but at least they're standard. Right now, the benchmarks being used are neither standard nor obviously representative of real use. I believe the TechEmpower benchmarks for web frameworks are more representative of real work, but seems like setting it up would require a LOT more work.
Because it isn't terrible. It is actually the fastest way to do it. You could use anything else and you'd not be nearly as fast. If you use GHC's data structures with native ints, it'd be faster, but Formality would still be arbitrarily faster, which is the point! But if I did native ints for Haskell and encoded ints for Formality, I'd be comparing two different programs. **That** would be, IMO, useless. Please let me know how you would do it instead?
Have a look at the type signatures for your functions. `flip :: (a -&gt; b -&gt; c) -&gt; (b -&gt; a -&gt; c)` `(-) :: (Num a) =&gt; a -&gt; a -&gt; a` `flip (-) :: (Num a) =&gt; a -&gt; a -&gt; a` `flip (-) 2 :: (Num a) =&gt; a -&gt; a -- consumed the first a` When you do `flip (-)`, you get back a function. Then `flip (-) 2` applies that function to 2, which makes the subtraction by 2 happen. But you still need that second `a` to get the final result.
Because the point of that benchmark is to test ADTs, pattern-matching and fusion. And it wouldn't make any difference if I used native data structures in Haskell. The program below does the same thing, and takes the same 30 seconds. ``` list :: [Int] list = map (const 0) [0..1000] apply_pow2n_times :: Int -&gt; ([Int] -&gt; [Int]) -&gt; [Int] -&gt; [Int] apply_pow2n_times 0 = \ f x -&gt; f x apply_pow2n_times n = \ f -&gt; apply_pow2n_times (n - 1) (\ x -&gt; f (f x)) main :: IO () main = print $ apply_pow2n_times 20 (map (+ 1)) list ``` But I wanted to benchmark identical programs because **otherwise** that would be meaningless. I want to compare how well the compiler deals with the same input in different languages, don't you?
I replied below. I'm honestly surprised so many people agree it is useless. It is not useless, this demonstrates we can fuse millions of intermediate calls at runtime, something Haskell will never do! This demonstrates Formality is clearly much faster than GHC when programming in a highly functional style, with lots and lots of maps, filters, reduces. Perhaps the problem is the code is huge and ugly? But that's only because it includes the implementation of lists, map, etc.; that'd be abstracted away in libs. In practice, you'd just import it. I don't see how that would be useless.
What do you mean? I also did that...
I couldn't disagree more; how is mapping over lists and incrementing numbers not something users would write? 
Why would anyone use `data Nat = S Nat | N` for naturals? Why would they use a linked list for bitstrings? Why would they not use the builtin list type? 
I don't see in the readme how much memory is needed for each language to run it. 
&gt; And no, it wouldn't make any difference if I used native data structures in Haskell. But... you didn't. Surely you could write the benchmarks more sensibly and benchmark that? I would also argue that a list of length 1000 is *not* a sensible data structure. For such things one would want to use a `Vector`. &gt; benchmark the raw performance of ADTs and pattern-matching Why? Also, that's not quite what you're doing: you are implicitly benchmarking the allocator and garbage collector as well. &gt; But I wanted to benchmark identical programs because otherwise that would be meaningless. We're interested in comparing how well the compiler deals with the same input in different languages, don't you agree? I'm interested in finding places where GHC's optimizations are suboptimal and fail actual users, but I remain uconvinced this is the place. Moreover, GHC is lazy and has a runtime which provides things that this doesn't. So it doesn't seem quite the same - in any case, the compiler optimizations may not be applicable to GHC.
&gt; Perhaps your problem is that the benchmark includes the implementation of lists, which is ugly? For one thing, fusion in Haskell is accomplished by rewrite rules, and your list implementation doesn't bother to add any. 
&gt;But I wanted to benchmark identical programs because otherwise **that** would be meaningless. We're interested in comparing how well the compiler deals with the same input in different languages, don't you agree? Um, no? Different compilers expect different typical inputs and hence their optimizers are set up differently. For example, if you start using laziness in Rust everywhere, GHC could very well beat Rust for non-trivial code.
Oh, I see. Yes I didn't do that. Thanks for the suggestion.
The reason I used that encoding is that it is actually the fastest way to do things, and I wanted test programs to be identical. But your argument makes sense, so I've now included two sets of benchmarks: "identical", where tested programs are the same to Formality up to the most minimal details, and "native", where it should solve the "same problem" in the most idiomatic language for each language. Can you see the repository again and let me know if it is better that way?
Well, I still disagree... I'm really interested in measuring how GHC / Formality behaves in identical inputs, that's the number I'm looking for. An example, if GHC performs better with Scott Lists, and Formality performs better with Church Lists, that'd be a useful information to me! Testing different programs won't me that. But I had an idea. I updated the benchmarks with two versions, "identical", where I use the exact same program as Formality, and "native", where I use the most natural / best possible implementation of the same problem in said language. Does that make more sense to you?
I think Haskell encourages a lack of shared mutable state, which naturally reduces locking / synchronization overhead. Where there *is* shared mutable state, Haskell *as a language* doesn't even prevent races, much less output lock- and race-free code automatically. The type system might allow a library that allow you to combinatorialy build structures that support lock-free mutation, but I'm not aware of one. `STM` generally avoids locks and races, especially when there's no write/read or write/write conflicts at runtime. IIRC, there's some locks in the implementation but they aren't exposed to the user, so they are always locked/unlocked correctly; unfortunately, it does mean that if there are a high number of transactions (or especially, a right number of retries due to write/read or write/write conflicts), lock contention can be an issue. I think the Async library and the Par monad are a bit friendlier than dealing with threads directly, but there's concurrency libraries for other languages that also hide the thread details. I think you still get the best performance out of concurrent data structures that are designed for lock-free, safe mutation from multiple threads. In my *extremely* limited experience, such data structures are non-persistent, so not the easiest structures to model in Haskell (loads of `ST` or `IO` inside the structure, and explicit strictness in core functions that operate on them).
&gt; I would also argue that a list of length 1000 is not a sensible data structure. For such things one would want to use a Vector Seems a hell of a blanket statement, especially when Haskell and its fondness for _infinite_ lists comes into the mix. It's an arbitrary size list, fairly large, sure, used to test performance of -- wait for it -- lists. Things would likely get lost in the noise when you're using lists of, let's say size 10. 
I'm thrilled to see [my suggestion from another thread](https://www.reddit.com/r/haskell/comments/9mm05d/2018_haskell_survey_results/e7ka8xn/) to improve the survey becoming reality! 
Yes, and Rust would beat GHC in other kinds of code. I wanted to see in which cases Formality performs better, and in which cases GHC performs better. Doesn't that make sense?
&gt; especially when Haskell and its fondness for &gt; infinite &gt; lists comes into the mix You seem to be completely misunderstanding the purpose of infinite lists. &gt; It's an arbitrary size list, fairly large, sure, used to test performance of -- wait for it -- lists. Things would likely get lost in the noise when you're using lists of, let's say size 10. Do you have any evidence to support this? Benchmarking lists of large lengths is simply meaningless. It tells you nothing about useful optimizations for real-world code. 
&gt; But this is disingenuous. When you use a custom list, which means GHC's default rewrite rules do not fire. Again, I understood that. The purpose of identical benchmarks is to identify cases where Formality performs well / poorly relative to GHC (and, thus, improve it!). I've added what you asked - native implementations - and you didn't comment on it. Did you see those updates? Perhaps you feel like I'm claiming Formality is faster than GHC, which I'm not! There is a reason the second benchmark listed is one where GHC destroys Formality.
I absolutely agree, and I even added some notes on the README due to your comment, including how I'm performing a benchmarks. I'm doing my best to keep comparisons meaningful and I believe they are, but you see any obvious flaw in this benchmark, please let me know!
In my opinion, chaining `.map`s is common enough to be representative of real use, but I agree it is a micro benchmark that doesn't cover nearly as many situations as desirable. I'll keep writing them as I develop Formality libs, and I'd love to eventually have more standardized benchmarks, but that's what we have for now!
Lately, every time I've seen a non-regular data type, I start thinking about doing it with GADTs instead. For example, Matthew Brecknell [represents 2-3 trees](https://github.com/mbrcknl/btree-gadt/blob/master/src/BTree.hs) as: data Nat = Z | S Nat data N n a = T1 (T n a) a (T n a) | T2 (T n a) a (T n a) a (T n a) data T n a where BR :: N n a -&gt; T (S n) a LF :: T Z a data Tree a where Tree :: T n a -&gt; Tree a His implementation has values at the branches, not the leaves, but it's possible to tweak this to match how your representation works. I've been wondering if every non-regular data type can be turned into a GADT or if it's only a certain set of special cases.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [mbrcknl/btree-gadt/.../**BTree.hs** (master ‚Üí e5f5907)](https://github.com/mbrcknl/btree-gadt/blob/e5f590757dd8ba2e4ec57813a3a047cda8851415/src/BTree.hs) ---- 
Ok, I get you. But then I would call it: Formality Runtime benchmark, since you compare how well different language run the execute (run) the same input. Formality benchmarks on the other hand should describe how much faster will my algorithm / application be, if I rewrite it into formality. 
Do you have a reference? I could not google it. 
Fun fact: I wound up dusting off this trick just last night to build a nice-ish functional encoding of permutations: https://github.com/ekmett/coda/blob/5af7219f1c220b5543e6c4091b4792e8ff030a19/wip/Perm.hs
I can call it that, yes. But from seeing this example you do not feel at all Formality could make your application / algorithm faster? Even aware of how "aggressively" it optimizes high-level idioms such as maps, folds, filter? Or is it that you 1. don't think those idioms are common enough or 2. aren't convinced yet Formality aggressively optimizes those?
[Non-404 link](https://www.parsonsmatt.org/2018/10/24/tchan_vs_tqueue.html).
Oh no! I have fixed the 404 -- looks like I don't understand Jekyl all that well.
1... I do not think scott encoded data structures are common enough.
If anything, I think the benchmarks should include GHC's builtin \[\] and the canonical 'data List a = Nil | Cons a (List a)\`. That way, you can see how it performs against \[\] with and without fusion/rewrite rules.
Just to be sure: did you see that the scott encoded Formality code is faster than the native Haskell code in the task of applying multiple maps, right? (I'm just trying to understand your reasoning!)
What kind of algorithm did you write? A typical divide-and-conquer sort like mergesort or quicksort should parallelize almost linearly without any locking (in Java or Haskell).
It was a 2 way bubble sort as an extra credit project for class. The problem was incrementing the state as one end finishes, a new one starts. Due to the setup, atomicintegerarray was insufficient in preventing race conditions. It would still occasionally execute the next increment before the get command in my callable. I switched to a wait notify scheme and found it completely bastardized my execution time. Eventually I settled on using an intermediate cache which gave me superiority at array length 1000. I was using the main thread and a worker from a single thread executor to repeatedly call a sift going in optional directions until sorted. I didn't want to create a bunch of temporary variables to feed the callable so this worked pretty nicely. I got the parallel version within about 12 ms of the traditional for all lengths in which the traditional alg was faster. of course the parallel is asymptotically twice as fast in terms of execution time. I just wondered if it was possible to make it so there is never a case where the traditional out performs the parallel on any array size (costless parallelism). I thought just maybe Haskell might somehow come close.
I was impressed by their interview questions, when I briefly interacted with them.
&gt; formally it does meet the only law of Num What about the ["customary laws"](http://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#t:Num) added in the latest `base`?
It is. Though it has it's particularities. Specifically our Linear Haskell is a variant of intuitionistic linear logic (where we don't have an involutive dualisation). It also has more equivalences (and, in fact, isomorphisms). In particular (!A)‚äó(!B) is isomorphic to !(A‚äóB), they are not even equivalent, in general, in linear logic (wether intuitionistic or not).
Use the Source, Dude: http://hackage.haskell.org/package/base-4.12.0.0/docs/src/GHC.Base.html#unIO RealWorld# is unlifted, so it's always strict; same with are strict pairs (#,#).
It doesn't satisfy these: &gt; fromInteger 0 is the additive identity &gt; negate gives the additive inverse &gt; fromInteger 1 is the multiplicative identity `empty` is the additive identity (and multiplicative) identity for graphs. Graphs do not have additive inverses, in general.
&gt; I've been wondering if every non-regular data type can be turned into a GADT or if it's only a certain set of special cases. It works in general. W-types can encode any (well-founded) recursive type, including non-uniformly recursive ones. `A` is the GADT, the `b` the product of all your indexes; parameters are uniform, so we can just roll them into `A`. M-types can handle the co-recursive cases, but have a dual construction.
 Four spaces before code on the reddit; makes it easier to read
Drop the `words` call, change the string patterns (e.g.: "+", "ln") into pattern matches on your Op constructors. Should be 85% of the way there.
&gt; Why would anyone use data Nat = S Nat | N for naturals? Easier to write proofs to satisfy (design by) contracts. Some GMP-ish library would be required as an alternative; machine words are big enough for most of the naturals.
You can pattern match on ADTs: case x of True -&gt; 1 :: Int False -&gt; 0 This matches on the value of `x :: Bool` (of type `Bool`) and specifies a value for each case. All branches have the same type! You can use this in functions as well: not :: Bool -&gt; Bool not False = True not True = False fromMaybe :: a -&gt; Maybe a -&gt; a fromMaybe x Nothing = x fromMaybe _ (Just y) = y This should be enough to solve your task. You might want to: * Post your beginner questions here: https://www.reddit.com/r/haskellquestions/ * Have a look at this tutorial to learn more: http://learnyouahaskell.com/chapters
I say this every year but I continue to mean it! Thanks soooo much for the work you do on this survey Taylor, and of course on Haskell Weekly as well! I really really appreciate it!
Is this remote means anywhere in US or worldwide remote?
You need to do three things: 1. change the type signature for `solveRPN` 2. update the patterns in `foldingFunction`. Instead of matching on "*", you match on `Mul`, instead of matching on `numberString`, you match on `(Val x)` (and then you don't even need to use `read`), etc. 3. change the implementation of `solveRPN` slightly - you no longer need to use `words`, since your input is already neatly divided into operators and numbers.
Code segments for those on old reddit (paragraph break in triple-backticks doesn't work): fact :: Integer -&gt; Integer fact 0 = 1 fact n = n * (fact $ n - 1) --- fact_handle_tail :: Integer -&gt; Integer -&gt; Integer fact_handle_tail 0 m = m fact_handle_tail n m = factor_handle_tail (n - 1) (n * m) fact_tail :: Integer -&gt; Integer fact_tail n = fact_handle n 1 --- fact_handle_cps :: Integer -&gt; (Integer -&gt; Integer) -&gt; Integer fact_handle_cps 0 cont = cont 1 fact_handle_cps n cont = fact_handle_cps (n - 1) (\x -&gt; cont $ n * x) fact_cps :: Integer -&gt; Integer fact_cps n = fact_handle_cps n (\x -&gt; x) --- fib :: Integer -&gt; Integer fib n = if n &lt; 2 then n else (fib $ n - 1) + (fib $ n - 2) fib_handle_cps :: Integer -&gt; (Integer -&gt; Integer) -&gt; Integer fib_handle_cps 0 cont = cont 0 fib_handle_cps 1 cont = cont 1 fib_handle_cps n cont = fib_handle_cps (n - 1) (\x -&gt; fib_handle_cps (n - 2) (\y -&gt; cont (x + y))) fib_cps :: Integer -&gt; Integer fib_cps n = fib_handle_cps n (\x -&gt; x)
`fmap id (undefined :: Proxy Int)` is `Proxy` on the first one and `_|_` in the second one. So, if a lawful `fmap id` is supposed to be `id`, only the second is lawful.
 eval :: PExp -&gt; Int eval = head . foldl foldingFunction [] . where foldingFunction Plus = (x + y):ys foldingFunction Minus = (y - x):ys foldingFunction Mul = (y * x):ys foldingFunction IntDiv = (y / x):ys foldingFunction Val x = x This is what i have so far...
Just use the definitions, the equations: subtract2 = flip (-) 2 -- then, subtract2 3 = flip (-) 2 3 = (-) 3 2 = (3 -) 2 = (3 - 2) = 1 so it makes sense. but if you were to write subtract2 3 4 = flip (-) 2 3 4 = ..... = (3 - 2) 4 = 1 4 then it wouldn't make sense (the types wouldn't fit).
 eval :: PExp -&gt; Int eval = head . foldl foldingFunction [] . where foldingFunction Plus = (x + y):ys foldingFunction Minus = (y - x):ys foldingFunction Mul = (y * x):ys foldingFunction IntDiv = (y / x):ys foldingFunction Val x = x This is what I have so far...
So, in short, Formality has the best `.map` performance by more than an order of magnitude, but it uses Scott encodings under the hoods (which can easily be hidden in a library in the same way Haskell's rewrite rules are), yet it isn't good enough! I guess some people are just too hard to please! : )
Okay, two questions: * Do you know what the `.` does? For example in `head . foldl foldingFunction [] . words`? * Do you know how pattern matching works? 
Barely know pattern matching, Im taking an online Haskell course thats mainly just the text book that I have not been understanding well. No i do not know what it does. I find Haskell very confusing to read and implement coming from C# Python Java background...
Without giving anything away, why were you impressed?
Thanks! My suspicion has been confirmed.
http://hackage.haskell.org/package/MemoTrie/docs/Data-MemoTrie.html
Through my university 
Okay, then I have no idea how you arrived at the `String -&gt; Float` version of your code :P Let's start with the pattern matching. In your original code, you had this line: foldingFunction (x:y:ys) "+" = (x + y):ys It's doing a bunch of things. The `(x:y:ys)` part takes the first input parameter (which is a list), and splits it up into its first element (x), second element (y), and rest of the list (ys). This is what allows you to use the names x, y, and ys on the right-hand side of the `=`. So since you've deleted that part from your new version of the code, it won't work. You need to put that back in. Then there's `"+"` - this is what we're going to replace. The second argument to `foldingFunction` is no longer a String, it's an `Op`. You seem to be doing fine with that, except for the `Val x` case, which needs to have parentheses around it: `foldingFunction xs (Val x) = ...` Okay, now to the `.` operator. It's defined like this: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c f . g = \x -&gt; f (g x) What it does is take two functions and chain them together into a new one. So `f . g`, where f and g are functions, becomes a new function which will first apply g to the input, then apply f to the result of that. So `head . foldl foldingFunction [] .` doesn't make a lot of sense - you need to remove the `.` at the end, so you're saying "first do the fold, then take the first element". Alternatively, you can rewrite to something simpler: eval :: PExp -&gt; Int eval pexp = head (foldl foldingFunction [] pexp) where foldingFunction ... = ...
Ouch, I haven't seen these customary laws before. Indeed, as /u/bss03 points out, some of these laws are not satisfied. 
The String -&gt; Float code came from the example given and a little from the previous problem I was able to finish. Thank you very much for your explanation I‚Äôve learned a ton from this alone. Did I do the stuff to the right of the equal signs correct?
It was a fun, very short problem, yet challenging and room for lots of creativity, and also relevant to their business. It was obvious that they had put deep thought into what sample problem they selected. The only thing I would change is to increase the time allotted slightly or pair down the problem a little bit. This was a year ago, so who knows what problems they are using now.
Maybe you could add haddocks on the instance that indicate which laws aren't satisfied. Many types in `base` now have disclaimers like that. See e.g. `Float` or `Natural`.
What do you mean ? Where do you study ?
I see. I don't disagree with anything that you said now, then. I agreed that the native versions are fairer comparisons, and was trying to argue that the identical versions are still useful. Anyway, thanks for the criticism, I believe the repository is much better after thinking about some of those feedbacks! 
I think the most surprising part for me is javascript being faster than Haskell! 
This literally sounds like my dream job, am currently using (or intending to use) every single language and technology you mentioned in my current project. Unfortunately I am very lacking in commercial experience at the moment but it's cool to know that opportunities like this exist. 
This is actually correct! There is no official endorsement yet, and there's a lot of discussion about what such an endorsement brings to the table, which is actually "not much" outside of maybe encouraging a bit more uptake. So I think it is likely this will happen, but I don't see why people are particularly excited about it, since it is a relatively small thing in the scheme of things :-)
I‚Äôm just getting an error on the Val case line now... do I still need to read in?
Post your code and the full error message, otherwise I have no way of knowing what's wrong. 
 eval :: PExp -&gt; Int eval pexp = head (foldl foldingFunction [] pexp) where foldingFunction (x:y:ys) Plus = (x + y):ys foldingFunction (x:y:ys) Minus = (y - x):ys foldingFunction (x:y:ys) Mul = (y * x):ys foldingFunction (x:y:ys) IntDiv = (y `div` x):ys foldingFunction xs (Val x) = read (Val x):xs error Couldn't match type ‚ÄòOp‚Äô with ‚Äò\[Char\]‚Äô Expected type: String Actual type: Op ‚Ä¢ In the first argument of ‚Äòread‚Äô, namely ‚Äò(Val x)‚Äô In the first argument of ‚Äò(:)‚Äô, namely ‚Äòread (Val x)‚Äô In the expression: read (Val x) : xs
Okay! Yeah, don't use read. read is for turning strings into other things, but (Val x) is not a string. The right hand side should just be `x:xs` - we're building a list of integers, and x is an integer. 
Shit that makes sense!!!!
Yay :D
Soooo if i had these data types data RPNError = DivByZero | InvalidInput deriving ( Show , Eq ) data Either a b = Left a | Right b deriving ( Show , Eq ) type RPNResult = Either RPNError Int and I want to error check the eval for dividing by 0, and i change the parameters to \`eval :: PExp -&gt; RPNResult\` ...how would I go about adding that?
Give it a go on your own first ;) 
 foldingFunction (x:0:ys) IntDiv = RPNError foldingFunction (0:y:ys) IntDiv = RPNError I came up with this to add, thinking it would catch the cases x or y was 0?
 data RPNError = DivByZero | InvalidInput deriving ( Show , Eq ) data Either a b = Left a | Right b deriving ( Show , Eq ) type RPNResult = Either RPNError Int evalSafe :: PExp -&gt; RPNResult evalSafe pexp = head (foldl foldingFunction [] pexp) where foldingFunction (x:y:ys) Plus = (x + y):ys foldingFunction (x:y:ys) Minus = (y - x):ys foldingFunction (x:y:ys) Mul = (y * x):ys foldingFunction (x:y:ys) IntDiv = (y `div` x):ys foldingFunction xs (Val x) = x:xs foldingFunction (x:0:ys) IntDiv = RPNError foldingFunction (0:y:ys) IntDiv = RPNError With this code, why do i get this error: &amp;#x200B; **Ambiguous occurrence ‚ÄòEither‚Äô** **It could refer to either ‚ÄòPrelude.Either‚Äô** &amp;#x200B;
But any smart compiler knows that there is no reason to carry the bricks and thus chooses not to. Having idiomatic code actually carry the bricks is more of an insult to the compiler. It's pointless to have a benchmark that only cares about one single type of data structure if there are data structures that can completely replace it. Haskell does provide them but they are quite rarely used and thus there is much less reason to optimize them. Whereas in other languages they are the primary data structure and thus quite focused on. 
Relevant ticket: [https://ghc.haskell.org/trac/ghc/ticket/15351](https://ghc.haskell.org/trac/ghc/ticket/15351) (QuantifiedConstraints ignore FunctionalDependencies) Although I don't think the second failure you found is related to this first problem.
Interesting. But I think it is missing laws for `fromInteger`, `abs` and `signum`. `fromInteger` (ring homomorphism): 1. `fromInteger (a * b) = fromInteger a * fromInteger b` 2. `fromInteger (a + b) = fromInteger a + fromInteger b` `abs` and `signum` (These are not as nice, mainly because the codomain of `abs` is wrong and we can not say `abs (a + b) ‚â§ abs a + abs b`): 4. `a = abs a * signum a` 2. `abs (a * b) = abs a * abs b` 3. `signum (a * b) = signum a * signum b` 4. `signum (signum a) = signum a` 3. `‚àëabs an = 0 ‚áí ‚àÄn an = 0` 3. `abs ‚àëabs an = Œ£abs an` I suppose they didn't add the laws for `abs` and `signum` because `abs` and `signum` don't really fit into `Num` in the first place, which is basically a ring class and because they are not as nice. However the missing laws for `fromIntegral` seem like an oversight.
i find paradigm fundamentalism to be tiresome, tbh.
OOP is a lie, but only because it's extremely poorly defined. Let's play Good Idea, Bad Idea with some of the various constituent ideas that make up Java, since it's generally agreed that Java is the poster child for OOP. Strongly Typed Structs: Good Idea, somewhat defeated in implementation by everything being inhabited by null regardless of type. Hierarchical Single Inheritance: Bad Idea Types that Restrict Behaviors : Good Idea Abstract Types with no inhabitants : Mostly a good idea, implementation is severely lacking in ergonomics. Parametric Polymorphism (generics) : Good Idea Implict, inherited context : Terrible idea Structs with functions attached : See implicit inherited context, and also the prior point about everything being inhabitable by null Code 'encapsulation' via private fields and methods : Good idea, failed execution The rest of Java is less OOP, and more just various flavors of idioms designed to work around having a language that doesn't have strong support for functions as first class objects. Seems like there is nothing terribly damning about OOP as a concept, just a few bad idea and odd omissions here and there. 
Hey, sorry, Reddit wouldn't let me log in for a while there... So the error is because the Either type is already defined in haskell's standard library. That means you can remove the line `data Either a b = Left a | Right b deriving ( Show , Eq )` Now, your code still won't work, and... a proper explanation that covers everything is kinda out of scope for a reddit comment. But I can give you some keywords that you can ask your teachers about, or look up on the internet or in whatever book you hopefully have to learn from. First of all, you should find out the difference between a *type*, a *type constructor*, and a *value constructor*. Second, you should learn what `foldl` actually does, and how your `foldingFunction` fits into that. Then you can think about what type signature the `foldingFunction` should have, and how you'll need to change the `foldl` call to take that into account.
Are you also looking for interns, or just full-time?
There's some good answers here, but I'm just going to address one part of your question which seems to have been glossed over. &gt; My question is why does this work? **Isn't the '$' a replacement for brackets?** Or does this function just need two brackets surrounding it? Yes but also no. As /u/AleksejsIvanovs points out, `$` is simply defined as `f $ x = f x`. This represents function application as an operator, which is really handy. The definition also carries the important annotation `infixr 0`. `infixr` means that the `$` operator associates to the right. In other words `f $ g $ x` will do `g $ x` first, then apply `f` to the result. `0` means the operator precedence. 0 is very very low, so `$` will be last in the order of operations. The upshot of both these things is that the right hand side of $ will get first precedence, as if it had been in brackets. Also, because Haskell is lovely, we can use "sections" with our infix operators (as described by /u/Boom_Rang and /u/mstksg). The upshot being that `($ x)` means "apply some function to x" and `(f $)` means "apply function f to something".
I‚Äôve tried for awhile, no luck. I understand that I need to do a different kind of check over the whole list but I can‚Äôt implement it right
I definitely think that's an exaggeration, but it has some truth. The truthful part is that a lot of OOP "design patterns" which can be cumbersome to program in Java have analogues that are pretty quick to program in Haskell and are in fact just the way you do things normally. Things that come to mind: visitor (pattern matching), factory (closures), decorator (some wrapper function). On the other hand, http://haskellbook.com/ could be considered a primer on "FP design patterns" -- after working through it you should be able to write functional analogues for OOP designs, or at least accomplish things in Haskell to a similar degree you would in Java, but that book is most definitely not paper thin. For the teaching at school bit, it seems as if Dijkstra would have agreed: https://www.cs.utexas.edu/users/EWD/transcriptions/OtherDocs/Haskell.html
Hmph. So this won't actually address the selection bias problems *at all*
&gt; Some GMP-ish library would be required as an alternative; machine words are not big enough for most of the naturals. Given that there are less than 64 bits of addressable memory on a computer, there's quite simply nothing that will fit in a `data Nat = S Nat | N` that will not fit in a Word64.
I think the RSLT[1] is cool. It's like a graph. In a graph, relationships are ordered, binary and labeled. In an RSLT, relationships are ordered but can involve any number of things, including other relationships, and also including relationship labels (which I called "templates" because they are shared across many relationships). Full disclosure: I wrote it, a couple years ago. The tooling around it leaves almost everything to be desired, although the demo[2] is pretty neat. Later, I think, I discovered it's equivalent to HypergraphDB, but I can't easily find evidence of the equivalence. [1] https://github.com/JeffreyBenjaminBrown/digraphs-with-text/blob/master/introduction/Minimal_Types.hs [2] https://github.com/JeffreyBenjaminBrown/digraphs-with-text/blob/master/introduction/demo.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [JeffreyBenjaminBrown/digraphs-with-text/.../**Minimal_Types.hs** (master ‚Üí 34e47a5)](https://github.com/JeffreyBenjaminBrown/digraphs-with-text/blob/34e47a52aa9abb6fd42028deba1623a92e278aae/introduction/Minimal_Types.hs) * [JeffreyBenjaminBrown/digraphs-with-text/.../**demo.hs** (master ‚Üí 34e47a5)](https://github.com/JeffreyBenjaminBrown/digraphs-with-text/blob/34e47a52aa9abb6fd42028deba1623a92e278aae/introduction/demo.hs) ---- 
I don't trust haskell.org at all. Too much fuckery and bad faith around Stack.
You should try implementing a parallel quick sort. That will give you the speedup, as long as the array is large enough (creating and synchronizing threads has some overhead, so for small enough arrays, a single-threaded version will almost always be the winner)
Perhaps that‚Äôs true. I‚Äôm certainly finding that enterprise Java development tends to complexify things.
I wouldn't go that far. Additional help in dispersing the survey could get a wider response rate. But selection bias isn't something that you can just "remove" from surveys -- all surveys have some form of bias in who they reach and who responds. You just have to understand the demographics of the results sufficiently to interpret the information bearing those elements in mind. I think the whole linked discussion is pretty rich on this, and goes into these sorts of things in greater detail.
I think I cut that one short, so I never found out one way or another...I used to have a rule where if it took more than a few days between correspondences, then I would move on and not look back. The rule is kind of useful because it helps distinguish between companies that are really excited about you and get your background, and ones that are neutral.
Not a bad policy at all. Being strung along applies just as much to job interviews as other relationships.
For as long as there are an overwhelming number of jobs being offered for OO work, there will be schools teaching it for everything. Yeah sure it may be problematic in a lot of different situations. It may lead to bottlenecks or code duplication (despite their claims) or higher levels of maintainability (also despite their claim), et al. The key fact is that it exists and companies have invested a lot of money in using it. Universities can only really guarantee jobs if they teach skills companies want to hire. People want jobs to make money and live.
&gt; For as long as there are an overwhelming number of jobs being offered for OO work, there will be schools teaching it for everything. My university started me on functional programming, and I couldn't be more grateful.
An FP design patterns book would be commentary on a selection of libraries.
That's great. I started with python and almost immediately (3 months later) transitioned to functional stuff. It's still my preference, but quite frankly we are a small sample size.
FizzBuzz enterprise edition: https://github.com/EnterpriseQualityCoding/FizzBuzzEnterpriseEdition 
I think you should run each benchmark multiple times (~100+) and then take an average... 
How did they do that? Can you point us to any examples?
Still, from what I've read and heard of Girard, and what I know of him, I'm pretty sure that linear logic is named as an analogy to linear algebra. (the presence of ‚äó and ‚äï as operators seem s like a big giveaway). Girard probably knew quite early of the fact that vector spaces formed a model of linear logic (though linear logic was born out of the study of the coherence space models of Œª-calculus). Even if the category theory details hadn't be carried out yet. (btw, only the category of finite dimensional vector space form a model of linear logic, as, for general vector space, dualisation is not involutive)
&gt; The only reason I am not applying now is because I crave algorithm intense work in a company building a compiler or database, where it might be justified to spend lots of time considering tradeoffs between different algorithms for a problem. Unfortunately most work - regardless of the language - is more one the less algorithmic side. It's the main reason why I still have a C++ job and I think it still is a lot easier to find this kind of work in C++. I'm having big hopes for Rust in this regard, but for Haskell it might be still difficult in the future, because if you need good control about the performance and memory characteristics of your program, then Haskell just isn't the best fit. 
I have a question about writing back-end applications in Haskell. More specifically, how to combine all of the side-effect creating things when writing an endpoint for a web app. Consider the following example: &amp;#x200B; requestBody :: ActionM ByteString getCurrentTime :: IO UTCTime insertUser :: User -&gt; Action IO Value &amp;#x200B; I have 3 different types of side effects (web framework, db and IO) and the whole thing should be combined into a single function that returns `register :: ActionM ???`. Also I make user of non-IO monads like Maybe and Either so at the end should the function type be something like `register :: ActionM (IO (Action IO Value))`, looks really wrong to me.
One way to do it is to use : ``` mkShell { name = "puppet-unit-test-shell"; buildInputs = [ exec language-puppet ]; }; ```
You mean `Zero | Succ (Incr v)` I guess ?
Unlike custom types Either is widely known, standard, and allows seamless integration between library code without forward knowledge. Using custom type instead of either is like reinventing the wheel.
There is a pretty good talk out there by Scott Wlaschin about this topic of "How do design patterns translate to FP" and it touches a lot on the point of why OOP is more cumbersome than helpful. https://www.youtube.com/watch?v=E8I19uA-wGY
Thanks! Why is disabling not possible? Can I somehow make it so that a GC will only occur in truly dire circumstances? I will have to ditch GHCJS entirely if these GC pauses continue to occur randomly throughout the game.
I've been using Rust for the past year. Prior to that, I used C and C++ almost exclusively. I've been getting into solving problems in a functional way, and I tend to find now that roughly 50% of problems end up with an elegant FP solution, and 50% with a procedural solution. As of yet, I've not found any problems that are solved more elegantly by OOP design than procedural or functional - even when writing a UI toolkit.
I don't think I do - that's copied directly out of the paper. The important bit is `Lam (Term (Incr v))` rather than just `Lam (Term v)`.
For example in `megaparsec` parse function returns `Either ParseError a`. I can use it in any `MonadError` from `mtl` without any additional work to integrate them. There is no spefic code for `mtl` in `megaparsec` and vice versa. Basically any library that uses Either could be used with any other library that uses Either, too. If i'd defined my own type instead of using Either i would have to write additional code to transform my type into Either and back.
It is true that the Gang of Four's book had a lot of patterns that are simply syntax elements of languages in the ML family, e.g. Command pattern is just a lambda, Visitor pattern is arguably composition. However that doesn't mean that there are no patterns in ML languages, it just means that there are different patterns. For example in Haskell we have the ReaderT pattern for state propagation, the Monoid pattern for configuration; the free-parameter pattern for state annotation (e.g. define data TodoCmd f = NewItem f a | EditItem f a | DeleteItem f a where f could be Maybe, Either or Identifity); the Effectful style for monad statcking (e.g. MonadReader m r, HasFoobar r); and more The phrase "OOP is a lie" is a non sequitur. OOP is a perfectly fine way to develop single-user, single-core, desktop applications and did a much better job of facilitating code-reuse than the other languages of the 80s and early 90s were doing. Nowadays we typically develop multi-user, multi-core web-apps, and OOP's idea of mutating states is no longer optimal. Meanwhile languages in the ML family -- from Haskell to F# to even Rust or arguably Swift -- have advanced to the point where they're solid options for building large apps in teams. It is true that the original version of OOP introduced by Smalltalk was a lot closer to what we now call the actor method of concurrency; but what we know consider OOP was fixed by C++ in the 80s; arguing _that_ point is just pedantry.
I think the defining feature of oop is subtyping - and by extension programming to the interface. Always defining a public interface first is powerful but also easy to get wrong and hard to fix. Maybe that's why oop likes proven patterns. Haskell has similar issues with type classes - which is why we use so many proven patterns from math - but in Java every class definition runs into this. On the other hand, Java doesn't need Backpack.
It looks like [ActionM](https://hackage.haskell.org/package/scotty-0.10.2/docs/Web-Scotty.html#t:ActionM) is just a type alias for `ActionT Text IO`. Since [ActionT](https://hackage.haskell.org/package/scotty-0.10.2/docs/Web-Scotty-Internal-Types.html#t:ActionT) is a monad transformer, you should be able to `lift` actions from the inner monad (in this case, `IO`) into the transformed monad (in this case, `ActionT Text IO`. This sort of pattern in general is the basis of the [mtl](https://hackage.haskell.org/package/mtl) and [transformers](https://hackage.haskell.org/package/transformers) packages -- transformers contains the actual transformers, and mtl contains typeclass patterns. For example, `ReaderT` is the actual reader monad transformer, while `MonadReader` is any monad which "can behave like a reader," i.e., it supports "ask"-ing for the environment.
Ok, so I should check out monad transformers. Is there anything else I need to know?
I wouldn't say that's my primary goal. And what I should have said was that I think having separate "Stack surveys" and "Cabal surveys" (to pick one example of how to split the community) doesn't do any of us any good. In fact, later in the thread I said: &gt; I didn‚Äôt mean to be exclusionary with my language before, and I thank y‚Äôall for correcting me there. ‚ÄúWe‚Äôre doing this together for the benefit of all‚Äù is an excellent way to say what I‚Äôm shooting for here. 
Thanks Chris! I always look forward to reading your content. Keep up the great work! 
Love me a good dijkstra rant.
Depends on what you want to do -- the ecosystem is pretty big! Anyway ya, transformers can be a bit tricky to wrap your head around at first, although once they "click", you'll find yourself using them everywhere. But for now, if you just want your code to run, you can probably just get by with `liftIO`, which will take any `IO` action like `putStrLn "hello" :: IO ()` and allow you to run it in your `ActionM`, like this: example :: ActionM () example = do body &lt;- requestBody ... liftIO $ putStrLn "hey, I can use IO! here's the request body:" liftIO $ print body ... pure ()
The endorsement had not yet happened by the time you wrote this comment, but it did happen a few hours ago: &gt; Yes, we're happy to support [the survey] from Haskell.org. https://mail.haskell.org/pipermail/haskell-community/2018-October/000342.html
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [eckyputrady/haskell-scotty-realworld-example-app/.../**src** (master ‚Üí d14f533)](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/tree/d14f53362e96dd3730524764306fc56d76f0f285/src) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e8hilwk.)
Can you elaborate on how continuations can subvert the lack of composability problems in FP?
Right now we're only looking for full-time, but we'll likely begin looking for interns in 2-3 months months.
Good advice just in general as well. :)
If people at your company form their opinions of a group of people from the exposure to 1 person there are more problems at your company than that one guy being a jerk. 
Yeah, now they also have mvp partially written in untyped scripting language, lol.
Having just finished the Haskell Book, I'd recommend it to you as a good starting point. Pretty much everything written in it is of interest to the average haskeller: it goes from the basics (pattern matching, recursion, higher order functions, currying, function composition, etc.), through the intermediate content (algebraic data types, parametric polymorphism, ad-hoc polymorphism/typeclasses, etc.) to the advanced stuff (the typeclass ladder: Functor, Applicative, Monad, Monoid, Foldable, Traversable; all of them with specific instances and examples). The book ends with the subject of monad transformers, widely used in pretty much all Haskell libraries. It also has a chapter dedicated to parsing using parser combinators, which is a neat and useful concept that allows you to hack together a parser very quickly.
ayy lmao
„ÉΩ‡ºº ‡∫àŸÑÕú‡∫à‡ºΩ Ôæâ Raise ur dongers! ^^Dongers ^^Raised: ^^39010 ^^Check ^^Out ^^/r/AyyLmao2DongerBot ^^For ^^More ^^Info
Not that I disagree with the sentiment, but it's curious to me that a company would base such an assessment on the disagreeableness of a single programmer. One assumes the powers that be are unfamiliar with the broader Haskell community, which I gravitated toward in part because I found to be uniquely friendly and helpful. So how did they land on Haskell in the first place? And after making a sweeping judgment call based on a single person, why would they rewrite MVP in a language with such different features? Changing course completely suggests that they didn't choose either language for their technical merits, which is a little troubling. At any rate, I'm sorry to hear about your experience. Best of luck, and I'll take this reminder to always do my best to be nice.
Honestly if they make such ridiculous and childish generalizations based on a single person then they're probably not worth caring much about
&gt; Changing course completely suggests that they didn't choose either language for their technical merits, which is a little troubling. I'd rather choose a language based on the availability of developers than on any perceived inherent technical merit. Doesn't help to choose Purescript for your front end if you can't find developers willing to use it. 
&gt; It really hurts Haskell adoption when people with a bad personality use it. I don't see it like that. Think about it for a moment. Just because there are people who are annoying, I would not reject a programming language (or software, or an idea or what ever). This is not professional. Your colleagues and the management does not act professionally and *they* are to blame. Generalizations and guilt by association are not reasonable.
&gt; Doesn't help to choose Purescript for your front end if you can't find developers willing to use it True, though one would assume this would have factored into the decision making before choosing Haskell as well. OP didn't say his company changed course because there wasn't the necessary critical mass of Haskell developers in place, though that certainly could have guided their thinking. And, yeah, Purescript. Ugh. I love Purescript, but most its frameworks are abandoned and it remains quite slow. I was so hopeful for it just a couple years ago. At least Typescript papers over some of the cracks, but it can be so painful.
Why is this downvoted? He's right, if they think this way of Haskell programmers because of one person they're part of the problem. Like you can't find difficult people to work with in Python, C++, Java or whatever.
I've answered this question a few times. I'll copy/link to a previous answer. [https://www.reddit.com/r/programming/comments/61kt36/functional\_programming\_design\_patterns\_by\_scott/dffy4b5](https://www.reddit.com/r/programming/comments/61kt36/functional_programming_design_patterns_by_scott/dffy4b5) &gt;I think this critique has been overstated to some extent. There are certainly patterns in FP, but the uppercase Pattern is usually reserved for GoF. Most of GoF is not needed because they are techniques for modeling principled composable abstraction in a very freewheeling environment. FP attempts to make composition first class. Because of this, base language semantics favor composition. You don't need to fight the environment to favor composable entities. Add to this the emphasis on lawful abstractions via the influence of Peter Landin and you get lowercase patterns that have very little resemblance to uppercase Patterns. &amp;#x200B;
This is great! Can't wait for the next part :)
I think *maybe* they are referring to a period of time when the Stack method of getting a Haskell development environment was hardly mentioned on the website. IIRC, there was even some period of time where the more promoted ways were actually broken, particularly on MS Windows. But, I wasn't that interested, so I count be remembering it poorly or wrongly.
Innate readability is a technical merit, it‚Äôs one of the tenants python is based on, and one thing a lot of functional languages lack. Sometimes you trade off the hopes of fast programs and optimization for the ability for your team to actually produce some code.
You criticise them for making such a leap, and then conclude with a leap of your own ("they're probably not worth caring much about").
&gt; instead of something meaningful? For better or for worse, `Left` and `Right` already _do_ have meaning in Haskell: "Left" means "please short-circuit the rest of the computation", usually because an error occurred, and "Right" means "please continue the rest of the computation", usually because everything is fine. `data Result e a = Error e | Ok a` might have been better names, but if you define a new `Result` datatype you won't benefit from the wealth of reusable code which has already been written for `Either`.
bad bot
bad human
Well, if we generalized all C programmers based in Linus Torvalds, nobody would use C any more.
If their opinions are based on such nonsense then why care about their opinion at all? I didn't say disregard them entirely, but on this subject they clearly know nothing.
There is no such thing as innate readability -- it's all familiarity. Haskell is more familiar to me than Python and I find it much easier to read.
You can write unreadable code in anything, but I also find the typical Haskell I encounter easier to read than the typical Python. I suspect this has something to do with the languages but more to do with me.
Non-Mobile link: https://en.wikipedia.org/wiki/Python_(programming_language) *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^223015
**Python (programming language)** Python is an interpreted high-level programming language for general-purpose programming. Created by Guido van Rossum and first released in 1991, Python has a design philosophy that emphasizes code readability, notably using significant whitespace. It provides constructs that enable clear programming on both small and large scales. In July 2018, Van Rossum stepped down as the leader in the language community after 30 years.Python features a dynamic type system and automatic memory management. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Someday, maybe companies will have a special slot for hackerrank competitors similar to how some places sponsor an open source developer on staff.
no need to fly off the handle, buddy
Writing a lot on the internet is a good distraction from emotional trouble, sorry if I‚Äôm annoying.
Right -- language absolutely has something to do with it (eg Whitespace is going to be unreadable for technical reasons), but it's silly to pretend that prior experience and exposure aren't dramatically more important. I started with Java in school, then did JavaScript and Ruby. I started learning Haskell about a year in to programming at all, so I didn't really have much in the way of familiarity. I've done more Haskell than anything else at this point, so it's familiar, so it's easier for me to read. I can point out bits of syntax and claim that *that's* the reason it's easier for me to read, but that's post hoc reasoning -- I am familiar with the syntax, so it is easy to read.
That's unfortunate. I do hope that the company considers that ornery and difficult programmers exist in every language, and that this is an individual problem and not a community one. Most Haskell developers that I know love to share knowledge and cooperate.
I know that it can be difficult in times of emotional duress to consider the feelings of others, but writing angry and insulting comments on the internet is not a good outlet for negative feelings. Please take care of yourself in a way that doesn't harm others.
Sure, but the Haskell world is smaller. It definitely isn't reasonable for this company to make a decision about Haskell based on one person... but it's also not just one person, if we're being honest. Doesn't hurt to remind ourselves now and then that in a smaller community, a few people being rude or unkind can reflect badly on the whole community. It's not as if this isn't relevant to Haskell, sadly.
This sounds like an inaccurate representation of the decision. What if the company made that decision to accept Haskell as an implementation language in the first place because they were counting on a few experienced Haskell programmers to bring others up to speed? If that's not happening, the situation has changed. It doesn't matter that there might be some amazing friendly and outgoing Haskell programmers in the world (and of course there are) if a substantial part of the Haskell talent at that company isn't willing to do the knowledge-sharing they need.
Is `Maybe a` an instance of `Show`? It obviously depends on what type `a` is. How does this work? How is this defined? I believe only "fully qualified types" (is that the right term?) can be members of a typeclass, and `Maybe a` is not fully qualified. That much makes sense. How does it work syntactically? I know the language syntax to make a type a member of a typeclass, and I know how to make a type not a member of a typeclass (just don't declare an instance), but how do I say "it depends" as appears to be the case with `Maybe`?
If it's only slower for small collections, it sounds like you just over-parallelized. I.E. you could probably make the algorithm non-parallel on the smaller sub tasks. Haskell won't solve any of these problems automatically. If you use pure data structures like Data.Map, it'll be easier to avoid locks, but it might be too much overhead. You'll still need just as much locking an synchronization if you use mutable data structures though.
I used to be a generous, magnanimous person. I donated to charities, volunteered, and helped old ladies across the street. Then one day I picked up a book about Haskell and as soon as I installed ghc something changed. Now I steal candy from children.
That's the one. If you can't afford it right now, you should start with the free resources available to you. Before this book, I had already started with "Learn You A Haskell". It's a nice introduction to the language (not as comprehensive as the Haskell book, of course), that can teach you fairly well the basics. Most importantly, it's free!
It should just work. When you call the method with a Month instead of a UTCTime, it'll find the right instance at compile time and use that one.
&gt; Since Œª-encoded structures fuse So... you didn't add rewrite rules for GHC, and they fused in Formality but not Haskell? That really doesn't sound that impressive to me. In fact it sounds misleading. 
I'd still like `Vectors` tbh. And given that there are no rewrite rules in the Haskell code it seems kind of dishonest (though perhaps it's just mistaken) - fusion in Haskell is accomplished precisely through rewrite rules. Of course if you don't bother to do that in Haskell then Formality is faster. Benchmarking fusion without using rewrite rules is just a bad benchmark.
&gt; But any smart person knows that there are plenty of situations which require the bricks to be carried, and pretending otherwise is an insult to others. This is an argument for a better benchmark suite.
I've know a huge dick who liked sandwiches a lot. I am now deeply distrustful of anyone who eats sandwiches.
Oh really?????? I will email them right away! I feel bad inquiring about it though... but oh well.
What are the "shapes" of the functions in MyClass? No matter what they are, a simple solution should be available (typically "extract the Month value, use MyFunction on it, put it back in"), but some more information might allow us to help you better.
Even if they are difficult people, I would say it's a good thing not a bad. Linus is the best example, you need a minimum level of abrasiveness to keep the shit gunking the wheels. In my experience it actually helps keep the codebase more maintaible and overall tech debt lower. 
 &gt; How does it work syntactically? I know the language syntax to make a type a member of a typeclass, and I know how to make a type not a member of a typeclass (just don't declare an instance), but how do I say "it depends" as appears to be the case with Maybe? Using standalone deriving it looks like so: -- | @since 2.01 deriving instance Show a =&gt; Show (Maybe a) An explicit instance could look like so: instance Show a =&gt; Show (Maybe a) where show Nothing = "Nothing" show (Just x) = "Just (" ++ show x ++ ")" -- for exemplification only &gt; Next question, is Maybe a an instance of Eq? Yes if there is an instance `Eq a` just as with `Show`.
Whenever you pick a nonstandard framework for anything, you're opting to provide infrastructure. Your colleagues thus become your customers. If your colleagues don't feel as though they are in control, they are going to look to other frameworks to regain it. If you want an effort like this to succeed, you really need to commit to providing your team with everything they need to own the framework. That means being available for advice, review feedback, and providing all the background information that your colleagues would need to get things done without you. Pair programming is also pretty useful.
Here's a brief initial reaction: Your math background has the potential to serve you well in programming and perhaps even more so in Haskell. But programming is also quite a bit about wanting to create/build/engineer things. If your aim is to be a "Haskell Programmer" and not some kind of specialist in system proofs, chip design, algorithms or whatever other highly mathematical application, my most general advice would be to become productive in Haskell. Anything you can do to demonstrate that you can produce real-world, operational code in Haskell will let you meet the minimum barrier you'd need to get consideration for most Haskell programming jobs. If, however, you want to focus more on the math/algebraic/scientific computing aspects, then it'd be less about Haskell and more about your ability to reach the needed levels of abstraction in systematizing. Haskell-wise, it'd probably help you then to go deeper into more advanced type system features (dependent types, etc.) and get practice in their practical applications (EDSLs, compilers, etc.).
I'm just wondering about the general way. ElvishJerricco's answer is sufficient.
&gt; I believe only "fully qualified types" (is that the right term?) can be members of a typeclass Nah. `Maybe` is a `Monad`. The Haskell type class system is fine with higher-kinded types, which is one advantage it has over other type systems, with parametric polymorphism. I'm not exactly sure what the right term is. I like "value types", but I think that may be already used for something unrelated. I like that term because if you have *any value*, then that value is of *some* type `t`, and that `t` is of kind `Type` (or `*` in other GHC). HKTs (like `Maybe` or kind `Type -&gt; Type` [or `* -&gt; *`]) don't have values, there's no *expression* (not even `undefined`) that has that as a type! Still, they are useful to talk about -- `Functor`, `Applicative`, `Alternative`, `Monad`, `Comonad`, `Bind`, `Apply`, `Extract`, `Foldable`, `Traversable`, etc. are all really about these HKTs. Without HKTs, you get stuff like `MonoTraversable`. ;)
For example the MVC pattern can be susbstituted by functions with the same interaction patter, but without handlers/callbacks using continuations. There are literature about that. Look for web frameworks and continuations. Futures are a form of continuations that also avoid callbacks. Futures use transactional mutable variables and these things use continuations at the runtime level
bad good bot bad admins bot
That's very true. But it doesn't change the fact that this sort of thing happens anyway. There's multiple parties to blame here and much of that blame still lies with the jerk Haskell programmer. Unless you're in management, the only contribution you can make to this problem is to not be a jerk Haskell programmer.
FWIW I found [George Wilson's Next Level MTL talk](https://www.youtube.com/watch?v=GZPup5Iuaqw) particularly enlightening around the time when I was trying to understand the same things you are now.
It looks like you got it sorted out, but if you run into that type of problem again in the future the solution is to add an entry for the version of the package you want to the `extra-deps` field of `stack.yaml`. In this case the entry would be something like `- elm-compiler-0.18`, however according to [hackage](https://hackage.haskell.org/package/elm-compiler) it looks like they stopped publishing the elm compiler after version 0.15 so you might've been out of luck anyway. 
Sorry for the delay. This is pretty straightforward to implement with streamly. See the race section in https://github.com/composewell/streamly/blob/master/docs/streamly-vs-async.md#race . Just replace async's race_ primitive in the above code with one of those implementations described there.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [composewell/streamly/.../**streamly-vs-async.md#race** (master ‚Üí d918833)](https://github.com/composewell/streamly/blob/d9188336a366adc430b0ea6fba29b3868100687b/docs/streamly-vs-async.md#race) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e8ibybd.)
Can you elaborate on the capability pattern and how it relates to FP? All Google gives me is a post about Java where the author describes it as an object being able to cast itself to its subclasses, depending on what it supports.
It is related to Layer 2 in the [Three layer cake](http://www.parsonsmatt.org/2018/03/22/three_layer_haskell_cake.html). The core idea is to have granular permissions for what operations are permitted.
I see the usual 'but you can write horrible code bases in every language' answers coming.
While that is true, some languages an paradigms make it easier than others.
They're not reject Haskell because that person is annoying. They're rejecting the code (which is written in Haskell), because nobody understands it and the annoying guy won't help anyone to understand. That's not unprofessional, it would be unprofessional for a company to keep code they can't maintain for the sake of pleasing one guy no one likes. 
A lot of programmers just lack social skills in general. Our industry is hard too, some problems have one clear answer while others have a lot of possible answers and the best is matter of opinion. My advice learn to recognize both and pick and choose your battles.
Yeah, I'd tried a yesod site with sqlite and compiling on a Lightsail instance looked like it was going to be too ridiculous. Maybe I'll just try out that stack / docker image thing too. 
I've been thinking about this before you posted this, but the OP said directly that everyone in the company thinks Haskell *devs* are difficult, because of this guy. Your interpretation is probably correct if the people in the company are reasonable. OP could tell us more, if this is true.
 &gt;Generalizations and guilt by association are not reasonable. Sure. That doesn't prevent people to draw those conclusions. One have to separate what is true about logic from what is true about people's behavior. It's true that concluding about the behavior of someone just because someone who uses the same programming language is a jerk isn't reasonable. But the fact is that people do. Even people that think that's unreasonable. We are way less rational than we think we are. Another example: if you things A is better than B and B is better than C, it's very reasonable that you would think A is better than C. But repeatedly in experiments people exhibit behavior incompatible with transitive preferences. So we should take this lack of rationality and reasonableness into account when trying to predict what will happen when we do something.
That's simply not true. In my experience, being kind, understanding and open leads to more people caring about your opinions and doing what you consider best practice. Software engineering is as much about the people that write the code than it is about the code. If you ignore any of those parts you'll end up with a bad situation.
I know right. I'm looking for a position in NYC, preferably not remote. But damn this place looks like a great place to work from the ad alone
I picked up the first couple of chapters from the haskell book. I saw that there are actually some resources offered by this subreddit as well So I will get busy with those next :)
I think this is more a question of syntax and language ergonomics than it is about paradigm or high level feature sets. My experience with enterprise Java is that the actual complexity of the code usually isn't so different, but the part where all the identifiers are 30 characters long and a given business process stretches across 8 different files or more is what makes it nasty. The bad naming and 'one class per file' idioms aren't endemic to the paradigm, or, as far as I can tell, a natural evolution of it, so much as it's an overcorrection from bad C conventions. If I trace a given Haskell function around, it might reference 8 other functions and several classes, but they'll all be terse and in 2-3 files, which makes a big difference for my cognitive overhead. There is nothing OO about Java's naming schemes or one class per file, that's just some dudes ideas about good code structure stretched beyond the point of reason.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc-proposals/ghc-proposals/.../**0018-quantified-constraints.rst#overlap** (master ‚Üí 601466d)](https://github.com/ghc-proposals/ghc-proposals/blob/601466df45889efc45a8a059dfdbf4deaa3b9f8c/proposals/0018-quantified-constraints.rst#overlap) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e8ipfef.)
The code is important too, and the *comments* are arguably the most important of all. Poorly organized code, code with unclear control flow, poor separation of concerns, poorly named functions, variables, and types, insufficient type signatures, etc., can make it difficult or impossible for someone else to make changes. The (common) assumption that the precise purpose and operation of some piece of code should be apparent from its implementation (so it doesn't need to be documented) is *rarely* valid. The correct way to operate: (try to) write the code as though the documentation doesn't exist, and then document the hell out of it anyway.
Ah, his solution does work. I was confused by his wording, but it makes sense now that you've clarified it.
They are `Right` as in correct and complete. and `Left` and in left-over. ;)
You can *not* fuse that in Haskell because it goes inside a loop: list = [...] for i in 0..range(x): list = map(f, list) Usual rewrite rules can not fuse that because there is just one map in code. Formality can fuse things during runtime, in a much more fluid manner. The whole point of the benchmark is to demonstrate this ability. Highlighting something that Formality clearly does better than Haskell is being misleading? 
Relevant to question 2: My first non-trivial Haskell project was a Heroku-hosted reddit bot. It used postgres for some persistent data (subs it was allowed in, time of last poll so it knew not to respond to anything older) and made web requests to another server to fetch data, which it then formatted as a response. (Roughly along the lines of wikipedia bot.) I came into it with only the roughest grasp of monad transformers (never mind `mtl`), outright fear of `lens`, and no significant prior experience with HTTP APIs or raw postgres requests (or Heroku). IIRC it took about a month, part-time. This project led directly to my first full-time programming job.
You might find more stimulation if you aim to become a lecturer in math, but going into engineering or the intersection of math and engineering (like a proof engineer in formal methods or applied math heavy parts of engineering) will pay you more and afford a better lifestyle.
Still no performance but here's a [video walkthrough](https://youtu.be/qKsCnfOo0No).
Speaking as a PhD in math that now works as a Haskell programmer full time, I think your background in math will definitely be a plus. The ability and training in thinking rationally and abstractly about problems is sought after. I would say that another thing employers want to see is some evidence that you can program (e.g. open source work) and that you can do that as part of a team. We get a lot of applicants whose only programming experience seems to be small hobby projects on github. And those are cool, they just don‚Äôt really show evidence that you can do software engineering in the large. I would suggest with your 3 months to find some open source Haskell project that seems interesting and dive in. Good luck!
I can't vouch for your affirmation, but every Haskell code I've seen doesn't mount on top of multiple layers of generic close-to-useless abstractions; they are much more straightforward. I very often see OOP coders jumping into the first pattern they can find, forcing their classes into brittle abstractions that tie them and break composability in many different ways. Also, it bothers me that very often patterns are an excuse for skipping static analysis of safety and composability, leading to choices based on pure aesthetical preferences and filled with boiler plate. If only immutability and explicit state were the norm, things would be much better. 
There are plenty of highly talented people who are on record as having been driven away from contributing to Linux by Linus‚Äô "minimum level of abrasiveness", and even more who didn't even try in the first place because they didn't want to be exposed to it. Linus is abrasive because it strokes his ego. It's a power trip to be aggressively rude to people and have a legion of fawning fans tell you how great you are for being an asshole. In reality, though, there's literally nothing he accomplished by being rude that he couldn't have accomplished more effectively through diplomacy and empathy.
Statistics and TDA are great places to get employment at the moment. Your background there will serve you well. A lot of jobs in that sort of field do tend to use python because it has all the libraries. But there's definitely room for haskell there, and I know some shops have been moving in that direction. What you will find however is that _most_ haskell employment won't be so much in that direction and will instead lean towards other sorts of stuff -- focusing more on dsls, concurrency, high-performance data processing, and verification, among other things (i.e. also just plain webapps, etc). My advice is to pick a project you're interested in -- perhaps in TDA -- and try to code it up in Haskell. Start simple, and then build out some more complex functionality on top. Start with the computational core, then think about all the details of how you'd wrap it up in a nice command line app -- exploring options parsing libraries and command-line help, settings files, considering maybe parallelizing it when possible, and interaction with different data formats at the endpoints -- json, csv, etc. So take something you care about, and then just keep _writing the heck_ out of your one project, improving it in different ways and as you add features, learning different libraries and techniques. For example, as you start you'll require virtually no IO. But now maybe you want to make an interactive component, so you need to figure out how to handle that. And if you want to parse commands from the user, maybe you need to learn a bit about parsing libs, etc. In the end, you'll have done something interesting you care about, and along the way learned about a _ton_ of the core software engineering techniques you need to build nontrivial projects in Haskell. Talking about a project like that and what you learned from it is also a great way to make an impression in interviews. You'll note I didn't recommend any particular book or tutorial. That's deliberate -- just learn as you go, and ask questions about each concrete step, finding the best resources to solve each problem as it arises. That's also a key part of being a self-directed learner you'll need in a future programming career. 
I would implement typeclasses in an entirely second pass once the main inference engine is through. That works fine until you hit fundeps at least. This old article from doel might be of some interest: http://comonad.com/reader/2012/natural-deduction-sequent-calculus-and-type-classes/ Also there's the classic typing haskell in haskell: https://web.cecs.pdx.edu/~mpj/thih/thih.pdf Neither addresses bidirectional checking per-se. But to a first approximation, my guess is: it shouldn't matter much!
Thanks! I'll have to take a closer look at both articles, but what do you mean by placing it in a second pass? Do you mean typing `(+)` as `forall t, t-&gt;t-&gt;t` and then verifying the types later? I could probably try doing that and seeing how it goes. Am I correct in assuming that the only rule in which type classes actually matter is in function application with an overloaded function? If so, I might be able to figure out some kind of modified inference rule and see if it works.
Imagine integers modulo 2, where x - y == x + y.
Do you have any resources on implementing functional dependencies?
Maybe look at PureScript? I think it's bidirectional but incorporates typeclasses. 
This has also been my experience with the Haskell community. I remember being so excited by the language that I looked up the next Haskell-related gathering, and when it was across the ocean in the UK, I bought a plane ticket and flew there. That weekend in Cambridge, if I'm remembering right, I met Neil Mitchell, Don Stewart, Simon Peyton-Jones, Simon Marlow, Duncan Coutts, and Dan Piponi. We chatted about Haskell for a day, then went rowing together on the Cam. But it's important to remember that not everyone has the same experience. The Haskell community is now large, and unfortunately the culture varies quite a lot. I've met people lately who have had very different, and far worse, experiences with our language community. They've found it to be hostile and angry, elitist and exclusionary, and a place to experience harassment and demeaning attitudes. Thankfully, that's \*also\* not a fair characterization of the community as a whole. The point, I suppose, is that I no longer feel comfortable with bragging about the community. Instead, let's agree that we \*want\* to be inviting, accepting, and helpful. And also that it takes constant vigilance to remain that way.
[I wrote a guide](http://reasonablypolymorphic.com/blog/algorithmic-sytc/) on implementing typeclasses in Haskell 98. Might be of some use.
I have a function that causes the compiler to blow up with 'pattern match checker exceeded 2000000 iterations'. I've already extracted it from a larger set of pattern matches it was part of, but can't see how to simplify it further while retaining its meaning. (It has only two non-catch-all alternatives, each of which matches one of two arguments ‚Äî both from a complicated recursive family of big ADTs ‚Äî using a pattern synonym. One alternative in particular is causing the problem ‚Äî commenting it out stops this, commenting the other out doesn't ‚Äî but I don't see a difference between them.) The best workaround I've found is to use `GHC_OPTIONS` to set `-fmax-pmcheck-iterations=1000` so it at least doesn't take several seconds to compile, but I still get a warning on every compilation. Ideally: is there a way to disable pattern-match checking locally? If not... I've tried to disable it for the whole file but haven't been able to make that work either ‚Äî based on [https://ghc.haskell.org/trac/ghc/ticket/13464](#13464) I tried setting `-Wno-incomplete-patterns -Wno-overlapping-patterns` but that had no effect. Is there any good workaround here?
Huh, this kind of seems like something I could reasonably patch into a bidirectional checker. Not sure how it would fare with polytypes (such as verifying `(forall x, Num x =&gt; x -&gt; x) -&gt; ...`) and fundeps in the mix; do you have any advice to that front?
It doesn't play nicely with _inference_ of polytypes, but I see no reason that the dispatch approach wouldn't work. You just push the burden of dictionary/proof onto whomever ends up providing the concrete implementation of the polytype. eg. you desugar `(forall x. Num x =&gt; x -&gt; x) -&gt; ...` into `(forall x. @Num x -&gt; x -&gt; x) -&gt; ...` and then the caller of this function is responsible for providing the dictionaries.
That makes sense. I don't think D-K supports inference of polytypes anyway, so that's not too bad. I think this approach seems pretty promising, but there are a few kinks I'd have to work out in the implementation. I'm also not entirely sure how to fit the list of constraints into the bidirectional framework.
TBH I don't understand bidirectional typechecking. Can you point me at any references?
There are a few "tutorials" that are easily found on Google, but the essence is that it adds a checking function so it can better deal with type annotations (which allows both checking and inference to be implemented straightforwardly). At least that's how I understand it.
That's unfair. Linus Torvalds has no problem with sharing his information. In fact he is very vocal about it!
[Complete and Easy Bidirectional Typechecking for Higher-Rank Types](https://www.cl.cam.ac.uk/~nk480/bidir.pdf). The gist of it is that it's (supposedly) easier to deal with more complicated type system features (in this case, rank-N polymorphism) by letting the type checker take advantage of known types, instead of just guessing a bunch.
&gt; Can you share the original Cabal file that produced these conflicts? https://github.com/tomjaguarpaw/haskell-opaleye/blob/09880b9412e32719b2ef17b02466069918c43371/opaleye.cabal#L30
Previously: https://www.reddit.com/r/haskell/comments/8x5ppy/the_simple_essence_of_automatic_differentiation/
I've proposed recently an idea for better error messages in this case, so this is definitely an area where things can be improved: * https://github.com/haskell/cabal/issues/2858#issuecomment-409830885 
That's right. This snippet: ``` rejecting: binary-0.8.6.0/installed-0.8... (conflict: containers==0.5.11.0, binary =&gt; containers==0.6.0.1/installed-0.6...) trying: binary-0.8.6.0 next goal: text (dependency of opaleye-0.6.7003.0) rejecting: text-1.2.3.1/installed-1.2... (conflict: binary==0.8.6.0, text =&gt; binary==0.8.6.0/installed-0.8...) ``` means 1. I cannot use the installed version of binary-0.8.6.0, because I have chosen containers-0.5.11.0 already and the installed version of binary is built against the installed version of containers-0.6.0.1 2. I'm going to try to re-build binary-0.8.6.0 against other dependencies. 3. I cannot use the installed version of text-1.2.3.1, because that depends on the installed version of binary-0.8.6.0, but I've now chosen an uninstalled version of binary-0.8.6.0. The primary problem, however, is that the error shown is just a minor trace of the solver output hitting the very first backtracking point, and it often has little to do with where the real problem is. The full search tree is huge, so printing that is difficult, which is why we instead print a set of packages that seem to trigger most of the problems (that are involved in most of the conflicts). Not even that's always showing the true culprits though.
Yes, your suggestion is a good one, and something like this should probably be implemented. It still retains the problem that the part of the search tree shown is either possibly irrelevant or too large. I think one can possibly do even more by finding better heuristics to guess the right problem.
Oh you were right! Thanks for the thorough explanation. 
if you do this, don't forget to opt out of Discord's forced arbitration clause: [https://kotaku.com/you-should-opt-out-of-discords-new-policy-changes-1829867491](https://kotaku.com/you-should-opt-out-of-discords-new-policy-changes-1829867491)
Let's have a data type for terms/expressions: `Expr`. We can parametrise it by "type", i.e. `Expr ty`. `Expr ()` is "untyped", `Expr (Maybe Ty)` is some types known, and in `Expr Ty` there are all types known. Similarly we can parametrise `Expr` by a dictionary evidence, so `Expr Consraint ty` is a term/expression with constraints to be instantiated. `Expr Dict Ty` would be a type with dictionaries synthed. In summary: - From `Expr () (Maybe Ty)` *partially annotated term* - **first pass** to `Expr Constraint Ty` *annotated term without dictionaries filled in* - **Second pass** to `Expr Dict Ty`. (This is kind-of *Trees that Grow* Haskell98 -version).
Yeah, should be a concise functional code of conduct: be . nice
You clearly don't understand how humans work do you? Humans are not machines, we like to think we base things on logic, but the reality is decisions are made up of lots of things and sometimes we backward rationalise things post the event. I don't disagree with your logical statement, but your assumption is simply false, humans remember are not machines.
I'm... huh? Not sure what you're going for here. I'm _aware_ that humans make stupid judgements for stupid reasons, otherwise I wouldn't be making the comment in the first place. What I'm saying is that being expected to accommodate that is silly.
I‚Äôm learning parsing with happy/alex, but the documentation only provides simplistic examples without covering more advanced features. I have little experience in parsing, so I‚Äôd like to find some real world projects which use these tools to learn the best practice. Any recommendations on that? Thanks
I just wanted to second the suggestion of trying `--reorder-goals`: it fixes the problem for me at least 80% of the time. However, given the mention of installed packages, I do not believe it will fix the problem in this case.
I believe the above explanation is correct. In situations like this, I usually take a look at the version constraints on the packages Cabal is highlighting for me. It may be that you just need to increase the upper bound constraint on `containers`. Doing so will almost certainly require a lot of packages to be rebuilt^1, so you might also have to provide `--force-reinstalls`. ^1 Hopefully, you‚Äôre using a sandbox. Although building with sandboxes takes longer and uses more space (unless you‚Äôre using the new build stuff, but I‚Äôm not too familiar with that), it‚Äôs worth it.
Voice? Dunno if this channel is set up that way, but voice is maxdef a plus.
Voice? Dunno if this channel is set up that way, but voice is maxdef a plus.
What's the slack channe?
Is idling or sharing space expensive for you? Some people like having their own spaces of various size and community bent. There does not need to be a centralized authority on community. We can all be a part of all of them.
If you do end up implementing an engine like this, writing a short tutorial on the topic would be helpful to others! There's a real lack of learning materials about this important topic. Research papers are great, but they often have goals that differ from what a beginner needs to know.
Apart from its awful memory use and interface, I associate slack with work, so I'd most definitely not want to use it in this context. I know many here do meet on slack though; IRC (#haskell on Freenode) is where it's at.
How do you want the function to behave when the inner lists don't all each have the same element repeated? E.g. what do you want the function to return if you give it `["aba","cdcdc"]`?
Sorry, I frogot to mention that there are only the same elements next to each other. So it doesn't work with \["aba","cdcdc"\] 
With that signature you aren't working over a list of lists, but instead a list of tuples. The head of ["aa","bbb"] is "aa". How would you get (2,'a') from "aa"? Can you do that same process recursively for the rest of the list? If you can how would you stitch those back together into the list you want as your final result? 
I was thinking of concat but I can't construct that "complicated" function yet. I barely understood the recursion. 
Well, ignoring all cases where the elements in the inner lists are different, `encode . concat` probably does what you want. Or did you mean you do not know how to define concat?
I understand concat and I have defined it before but we haven't worked with that "." thing before in class (I mean: encode . concat ). It's a homework, I was struggling writting encode function and now I'm lost with this. 
Actually a bigger issue is: how should the function behave if an inner list is empty? I don't think there's a sensible way to resolve that unless you treated it as a "validation" issue and wrapped the return type in `Maybe` or `Either` to express the error condition. Perhaps that makes the most sense with the other issue of inner lists composed of unequal elements.
`.` is for function composition: `f . g = \x -&gt; f (g x)` if you know `\ `. So in this case function xs = encode (concat xs) Though that might not be what you want on inputs like `["aa","aaa"]` since that would turn into `[(5,'a')]`.
I shouldn't use these types of functions yet. I don't know how should I define list of lists. 
With languages like Haskell it's often very important to think about these "doesn't work with" cases and what they mean for your function. Do you mean that you don't care at all what the result of `function ["aba", "cdcdc"]` would be? Alternatively you might want the program to crash if that ever happened, or you might want to express this uncertainty with a `Maybe` return type. If this is a very minimally specified homework question without much context then maybe it's appropriate to just not care about what's left unspecified, but taking care in choosing how to handle badly-formed inputs is a really good habit to develop.
I wouldn't use `zip`, I would use `concat`.
I‚Äôm working on it but wherever I put concat it gives me error.
You want to find a `function :: [[a]] -&gt; [(Int,a)]` You already have a function `encode :: Eq a =&gt; [a] -&gt; [(Int, a)]`. So if you have a some other function of type `[[a]] -&gt; [a]` then you can apply it before `encode` and the resulting function will be the type that you need. This function is `concat`. Alternatively you could call `encode` on each inner list individually, using `map` (`map encode :: Eq a =&gt; [[a]] -&gt; [[(Int, a)]]`), and then call `concat` on the result. Doing it this way actually results in slightly different behavior ‚Äî if you give it `["aa","aaa"]` it will return `[(2,'a'),(3,'a')]` instead of `[(5,'a')]`. Conversely, `zip :: [a] -&gt; [b] -&gt; [(a, b)]`. So you might find some strange way of redefining `encode` with `zip`, but you already have a perfectly good implementation of `encode` :)
Here's a small Alex lexer that finds identifiers in Haddock comments: https://github.com/sjakobi/ghc/blob/fe4701957023bbdf9b77a37522a36c80c3a11f8d/compiler/parser/HaddockLex.x
It prevents two Right's or Left's in a row
You said it's "weird" in regards to "guilt by association", but really it's not weird, it's imperfect humans being human, does it justify the wrong action? Answer No. But in this imperfect world, we need to (a) understand it's exists (b) be adaptable and find ways to cope and adjust our behaviour such that we get the outcome we want, but one that works in the current broken context
You can disable the Prelude by writing `{-# LANGUAGE NoImplicitPrelude #-}` at the top of your file. The definitions from `Prelude` will then not be automatically imported any more. If you need some of them, you can write `import Prelude (x, y, z)` or `import qualified Prelude`.
this is beautiful!
does the HiDPI support come from `FLTK` itself or from `fltkhs`? 
The tuple type (a, b) is often called the *anonymous product*. It's not very informative, and you don't really ever want to use more than a three-tuple before defining your own data type. Sometimes you should make your own type instead of taking or returning a tuple (when the situation might be confusing). But it's useful in a pinch or when you don't care about the extra benefit. It's also useful because a lot of higher order functions use tuples, because the lack of info makes it as generic and meaningless as possible. Either is the same. It's the "anonymous sum". Not super informative, and you shouldn't really chain them instead of making your own type. Sometimes you might make your own informative type instead of using an Either, but it's useful in a pinch and you don't want to go through the hassle of making your own type for the extra benefit. It's also used by a lot of higher order functions because the lack of info makes it as generic and meaningless as possible.
Oh yeah, that's actually the much more sensible option!
There's nothing regarding bidirectionality, but but here are some references I found helpful: Jones, M. P. (2003). Qualified types: theory and practice (Vol. 9). Cambridge University Press. Jones, M. P. (1992, February). A theory of qualified types. In European symposium on programming (pp. 287-306). Springer, Berlin, Heidelberg. Peterson, J., &amp; Jones, M. (1993, August). Implementing type classes. In ACM SIGPLAN Notices (Vol. 28, No. 6, pp. 227-236). ACM. Jones, M. P. (1995). A system of constructor classes: overloading and implicit higher-order polymorphism. Journal of functional programming, 5(1), 1-35.
Thanks! I'll take a look at them. Currently my plan is just to bolt predicates onto the types after inference/checking and see if I get anything useful.
How so? Zig 1 . Zig 2 . Zag "testing" $ Zag 3 ZNil :: ZigZag Int String That zigs twice in a row and zags twice in a row.
Ah, thanks for the correction. I was thinking of a different type I had seen before, essentially the same thing without the Zig constructor.
Yeah thanks, it‚Äôs working fine when I add Eq a =&gt; but I can‚Äôt do this. Also in my school assignment encode function should be done with flatten function and with F function we are looking for. 
Exactly. In my opinion an ideal benchmark is one where you will be given random input x and have to give back output f(x), but where there are basically no other rules (besides no FFI). That way various compiler optimizations like avoiding unnecessary brick carrying or even algorithmic restructures are not cheating. You are also not penalized for design decisions of the language as long as you can efficiently do the task at hand. 
Absolutely! Thanks to strong typing, refactoring should be mostly mechanic.
It's still very much a work in progress, but I think it's at the point where it could be useful! In addition to everything you would expect (typeclasses, monads, etc) it also has discussion on stack configurations, testing with hspec and quickcheck, GADTs and datakinds, and other nice things. On the agenda is: much more revision for wording and clarity, a chapter on the pipes library, an example project making an IRC client, a chapter about how to write optimized haskell, and a chapter on concurrency/parallelism. Possibly ones on Parsec and Reflex-DOM, depending on interest. 
https://reddit.com/r/haskellquestions/comments/9rw3xr/_/e8kkroe/?context=1 This cannot be written without the Eq constraint. As for flatten, unless something very weird is going on, it should be identical to concat, so just use flatten wherever concat has been suggested and you‚Äôll be fine.
If you find your `encode` function confusing with your explicit recursion, it might be easier to understand with more idiomatic Haskell, using higher order functions (`group` is from `Data.List` in the prelude): encode :: Eq a =&gt; [a] -&gt; [(Int, a)] encode = map (\xs -&gt; (length xs, head xs)) . group As for how to make this function work with lists of lists, you want to use function composition to make a function that first concatenates the lists into a single list, and then applies your `encode` function to the result: concatEncode :: Eq a =&gt; [[a]] -&gt; [(Int, a)] concatEncode = encode . concat If you'd rather not use the function composition function `(.)`, this can also be written as concatEncode :: Eq a =&gt; [[a]] -&gt; [(Int, a)] concatEncode xss = encode (concat xss)
All done in pandoc!
&gt; always3 = 3 &gt; [...] But in reality, it‚Äôs a function that doesn‚Äôt have any parameters and just returns 3. The proper name would be a nullary function. No, [constants are just constants](http://conal.net/blog/posts/everything-is-a-function-in-haskell).
Looks friendly, I am doing the Learn Haskell for the greater good but I will check this as well. A part about space and time complexity of certain data operations and algorithms would be nice.
Literally both articles I linked :-)
Just went through the chapter on stack, definitely useful for a beginner. Will check out the rest
But wouldn't it be the right way to think of it abstractly?
No, for the many reasons listed in the article I linked to. What do you suppose would be gained by pretending that 3 is a function?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [conal/concat/.../**RAD.hs** (master ‚Üí ecc78a7)](https://github.com/conal/concat/blob/ecc78a7f0c5d3dd117de3e847cb9afc7bad7a326/examples/src/ConCat/RAD.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e8l9gm0.)
It's not. The "right way" of thinking about functions in Haskell is to think of them as "all functions have exactly 1 input and exactly 1 output". A function always has a function type, written in the form `a -&gt; b`, where `a` is the function's input type, and `b` is the function's output type. You may say "what about a function with type `a -&gt; b -&gt; c`? Doesn't that have two arguments?" The answer, of course, is no! This is syntactic sugar for `a -&gt; (b -&gt; c)`. The single input of this function is an `a`, and the single output of this function is another function, of type `b -&gt; c`. Multi-arg functions in Haskell are just an illusion. Rather than thinking of defining constants as a special case of defining functions, the "right way" is to turn that around and think of defining functions as a special case of defining constants. When you write f :: Foo -&gt; Bar f x = ... This is just syntactic sugar for defining the constant `f` like so f :: Foo -&gt; Bar f = \ x -&gt; ... Instead of thinking of all values as functions, think of all functions as values.
No, functional programming doesn‚Äôt mean that everything is a function, even though it‚Äôs rooted in lambda calculus, where everything is indeed a function. The only things that are functions in Haskell are those that have `-&gt;` in their type, and *all* of them are unary‚Äîwe think of a type like `A -&gt; B -&gt; C -&gt; D` as a function of three arguments, but it‚Äôs really a curried ‚Äúchain‚Äù of functions `A -&gt; (B -&gt; (C -&gt; D))`. There are languages in which every *term* denotes a function, such as concatenative languages (which are based on combinatory logic, a relative of lambda calculus), but even these languages still have *values* that aren‚Äôt functions. 
Please don‚Äôt perpetuate the false idea that there are functions with no arguments. All functions have exactly one argument. If something does not have an arrow type, then it‚Äôs not a function. 
Please, split it into separate pages per chapter.
Some kind of reference to the existing material on zippers would probably be useful here.
The idea is not that *everything* is a function, but that functions are a value like any other, that can be bound to names, be returned from other functions, be passed around as arguments, etc.
Define them as `even'` or `even_`
I see you have a bold goal of replacing Org-mode with your own Haskell-based tool. It must be a lot of fun! &amp;#x200B; I don't think I'll ever give up on Emacs, though. It's an amazing interactive environment. I'd rather bring some types to it (there'is an [attempt](https://github.com/emacs-elsa/Elsa) to do it which looks promising) instead of replacing it with something rigid.
&gt; Use some version control system to track changes in your source code. I disagree. At this point pretty much every developer knows git. Unless there is a good reason to use a different tool, can we please just stick with it? I don't want to have to search for some hg/darcs/whatever cheat sheet every time I want to submit a small fix to your project.
"value-oriented" does not convey this any better, imho, precisely because it does not convey that functions are just values. If you come from any background where functions are second-class, then "value-oriented" won't give you the right intuition. From that perspective, I feel that this whole criticism of "nullary functions" is a bit over the top: If a newcomer learns that "everything is a function" and knows that we have lists of things, then a natural conclusion already is that you can also have lists of non-nullary functions. Saying "everything is a function" does a fine job of destroying the mental barrier that might exist between the concepts of "value" (first class) and "function" (second class). The rest depends on the definition of "function". I concur that defining function any other way than exactly-one-argument is a bit risky in haskell, because the function _type_ has a precise definition where "functions of different arity" can never unify. But it is just a definition, and definitions are never wrong, and most readers will be able to understand that definitions can be local/clash (function types that are exactly unary versus the more abstract function concept that may have any arity). If the abstract concept helps breaking the mental barrier, it might still be useful to some beginners, no? (I really would prefer if we'd talk about the structure of the material. Although I too prefer adding to this thread because it is easier than arguing whether the way do-notation is introduced leaves room for improvement. Not to say that there is something wrong with the structure, I just think that the structure might be way more important than a single definition.)
I'm a new haskeller trying to follow along and it's not working properly, specifically in your use of optparse-applicative. I've gotten as far as the "SUPPORTING RUNNING SPECIFIC DIRECTLY COMMANDS" section. This doesn't seem to match up with the code either in the repo or later in the file, and I don't see where you change it. Is this section out of date?
Thank you!
Both HiDPI and SVG support come from `FLTK` itself.
For the last six months I‚Äôve been working on a screencast video editor called *Komposition*, and it‚Äôs now released and open source. This is an experience report, based on a talk from Lambda World C√°diz 2018, that‚Äôll give an overview of Komposition‚Äôs design, implementation, testing, and planned future work.
That's correct, it doesn't. I could have added something like 'left as an exercise for the reader', but honestly I don't know how I would do it. 
Komposition uses ffmpeg cli as the editing tool and gtk for the UI. Would it be possible to do the UI with HTML 5?
&gt; What does a programmer do when faced with a repetitive task? Spend way more time on automating it! I'm doing the exact same thing for my Haskell videos :) Sadly, my workflow is quite different, so I'm writing yet another specialized video editor, I can't reuse yours.
Would you mind posting which part is breaking for you? The code in the repository should definitely work, while the code in the post might be slightly out of date. Which part are you stuck on?
The code in that one section "SUPPORTING RUNNING SPECIFIC DIRECTLY COMMANDS (AKA CLI BLING)" The statement `import Options.Applicative (Parser, subparser, execParser, info, argument, str, idm, command)` didn't contain `Mod` or `CommandFields`, and ghc was complaining about that. Once I fixed that, I got: Main.hs 22 8 error [-Wdeferred-type-errors] ‚Ä¢ Couldn't match type ‚ÄòCommand‚Äô with ‚ÄòIO ()‚Äô Expected type: Parser (IO ()) Actual type: Parser Command ‚Ä¢ In the expression: subparser commands In an equation for ‚Äòopts‚Äô: opts = subparser commands where serverAction :: IO (IO ()) serverAction = pure server serverCmd :: ParserInfo (IO ()) serverCmd = info serverAction idm .... (intero) Main.hs 30 24 error [-Wdeferred-type-errors] ‚Ä¢ Couldn't match expected type ‚ÄòParser (IO ())‚Äô with actual type ‚ÄòIO (IO ())‚Äô ‚Ä¢ In the first argument of ‚Äòinfo‚Äô, namely ‚ÄòserverAction‚Äô In the expression: info serverAction idm In an equation for ‚ÄòserverCmd‚Äô: serverCmd = info serverAction idm (intero) Main.hs 33 18 error [-Wdeferred-type-errors] ‚Ä¢ Couldn't match type ‚ÄòIO ()‚Äô with ‚ÄòCommand‚Äô Expected type: Mod CommandFields Command Actual type: Mod CommandFields (IO ()) ‚Ä¢ In the expression: command "server" serverCmd In an equation for ‚Äòcommands‚Äô: commands = command "server" serverCmd In an equation for ‚Äòopts‚Äô: opts = subparser commands where serverAction :: IO (IO ()) serverAction = pure server serverCmd :: ParserInfo (IO ()) serverCmd = info serverAction idm .... (intero) 
It's true that not everything is a function, but it's useful to know that more things are functions than you might think. Just seeing `[1,2,3]` as the repeated application of the constructor function rather than a single monolithic chunk of data goes a hell of a long way toward understanding laziness. Not sure I'm still on the train when it comes to constants though...
I'd personally generalize this whole thing with a list of lists (since you are using lists anyway) and than instead of: ``` ListCursor { previous = "xet yM" , next = "tual example" } ``` I'd just have: ``` ListCursor ["xet yM", "tual example"] ``` And ofc. multiple cursors can be represented as: ``` ListCursor ["xet yM", "tual ", "example"] ```
Do you mean like in an embedded WebKit view, or a client-server webapp?
Oh! That's cool, but I'm curious: what's your workflow like?
Honestly that article isn't really very compelling and it leaves out what is, in my opinion, the most obvious reason people think this. Syntactically and operationally for the programmer, non function values behave exactly as nullary function values would be expected to. If you think about it for a few minutes you end up with "well if application requires no extra syntax, then applying a function to nothing should be automatic!" Then you ignore the weird feeling you get in the back of your brain that says, "but when do you stop?" and everything just works.
That's nice! Maybe you want to try Komposition if you do more screencasts? Looks like it would fit the style of your video. And yeah, Premiere Pro has something like subsequences also. But I found it cumbersome to work with, I want something that's quick to organize with. Maybe I'm just using it wrong, but it felt like a misfit.
Thanks for the excellent guide! Talking about typos, there's one in the GADT section. `where` should be `Const` :) in below. \&gt; We still define two data constructors, End and `Where` &amp;#x200B;
thanks, I'll get that fixed!
I plan to do that
You should probably use `NonEmpty` instead of the outer list, tbh.
I agree and this is why I wrote it that way. I think the intuition that everything is a function is actually useful for beginners
Link that supposed to lead to github issues page is actually leading to https://wickstrom.tech/programming/2018/10/26/github.com/owickstrom/komposition/issues which is forbidden.
Fixed now, thank you!
I solved it without Eq function(x:xs) = (length x , head x) : function xs 